{
  "name" : "1705.05940.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Subregular Complexity and Deep Learning",
    "authors" : [ "Enes Avcu", "Chihiro Shibata" ],
    "emails" : [ "enesavc@udel.edu", "shibatachh@stf.teu.ac.jp", "heinz@udel.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n70 5.\n05 94\n0v 1\n[ cs\n.C L\n] 1\n6 M\nay 2\n01 7\ning how formal language theory can shed light on deep learning. We train naive Long Short-Term Memory (LSTM) Recurrent Neural Networks (RNNs) on six formal languages drawn from the Strictly Local (SL) and Strictly Piecewise (SP) classes. These classes are relevant to computational linguistics and among the simplest in a mathematically well-understood hierarchy of subregular classes. SL and SP classes encode local and long-distance dependencies, respectively. The results show four of the six languages were learned remarkably well, but overfitting arguably occurred with the simplest SL language and undergeneralization with the most complex SP pattern. Even though LSTMs were developed to handle long-distance dependencies, the latter result shows they stymie naive LSTMs in contrast to local dependencies. While it remains to be seen which of the many variants of LSTMsmay learn SP languages well, this result speaks to the larger point that the judicial use of formal language theory can illuminate the inner workings of RNNs."
    }, {
      "heading" : "1 Investigating Deep Learning",
      "text" : "This paper proposes that formal language theory provides a systematic way to better understand the kinds of patterns deep learning networks (LeCun et al., 2015) are able to learn. The main ideas of this approach are illustrated with experiments testing how well LSTM networks (Hochreiter and Schmidhuber, 1997) can learn different formal languages. The formal languages are drawn from subclasses\nof the regular languages. These are wellunderstood theoretically to form a complexity hierarchy (McNaughton and Papert, 1971; Rogers et al., 2010; Rogers and Pullum, 2011). This complexity of a language is not based on automata-theoretic measures, such as the size of the minimal deterministic automaton, but instead has a model-theoretic basis (Enderton, 2001). It is determined by the kind of logic and model needed to specify it (Rogers et al., 2013).\nThis approach is not entirely without precedent. Earlier research used formal languages to probe the learning capabilities of neural networks. Section 2 highlights some of this work to make clearer our own motivation and contribution.\nWe are particularly interested in understanding how well LSTMs can learn SP languages. As will be explained in §3, SP languages are simple regular languages which encode only certain types of long-distance dependencies. LSTMs were specifically designed to model long-distance dependencies within temporal sequences more accurately. We compare the performance of LSTMs on SP languages with their performance on learning SL languages, another simple class of regular languages, but one which only encodes local dependencies in sequences. We trained LSTMs on both positive and negative examples of SL and SP languages. We controlled the training and testing data for word length, varied the amount of training data, and varied the LSTMs with respect to vector size. The experimental details are explained in §4.\nThe results, presented in §5, are somewhat unexpected. While LSTMs performed above chance in all of our experiments, LSTMs showed both overfitting and undergeneralization. Specifically, overfitting was observed in the simplest SL language and undergeneralization was observed in the most complex SP language. Implications of these results are explained in §6. These results are\nunexpected because it is known that LSTMs can learn some context-sensitive formal languages exhibiting long-distance dependencies with uncanny precision (Prez-Ortiz et al., 2003)."
    }, {
      "heading" : "2 Motivation and background",
      "text" : "In the 1990s, many studies aimed to learn formal languages with neural networks. When the aim was to predict the next symbol of a string drawn from a regular language, first-order RNNs were used (Casey, 1996; Smith, A.W., 1989). The target languages here were based on the Reber grammar (Reber, 1967). When the aim was to decide whether a string is grammatical, second-order RNNs were used (Pollack, 1991; Watrous and Kuhn, 1992; Giles et al., 1992). Here the target languages were the regular languages studied by Tomita (Tomita, 1982). Later research targeted nonregular languages (Schmidhuber et al., 2002; Chalup and Blair, 2003; Prez-Ortiz et al., 2003). However, this line of research has not been pursued recently to our knowledge.\nThe reasons for making formal languages the targets of learning are as valid today as they were decades ago. First, the grammars generating the formal languages are known. Therefore training and test data can be generated as desired. Thus, the scientist can run controlled experiments to see whether particular generalizations are reliably acquired under particular training regimens.\nImportantly, the relative complexity of different formal languages may provide additional insight. If it is found that formal languages of one type are more readily learned than formal languages of another type in some set of experiments then the difference between these classes may be said to meaningfully capture some property that the RNNs in the experiments are incapable of capturing. Subsequent work may lead to proofs and theorems about which LSTMs can reliably infer formal languages from certain classes and which cannot. It may also lead to new network architectures which overcome identified hurdles.\nThe primary difference between the present paper and past research, beyond the development in neural networks, is that the regular languages chosen here are known to have certain properties. The Reber grammars and Tomita languages were not understood in terms of their abstract properties or pattern complexity. Even though\nsubregular distinctions had already been studied (McNaughton and Papert, 1971), it went unrecognized how that branch of computer science could inform neural network learning."
    }, {
      "heading" : "3 Subregular Complexity",
      "text" : "Figure 1 shows proper inclusion relationships of well-studied classes of subregular languages. The Strictly Local (SL), Locally Testable (LT), and Non-Counting (NC) classes were studied by (McNaughton and Papert, 1971). The Locally Threshold Testable (LTT) class was introduced and studied by (Thomas, 1982). The Piecewise Testable (PT) class was introduced and studied by (Simon, 1975). The Strictly Piecewise (SP) class was studied by (Rogers et al., 2010). As many authors discuss, these classes are natural because they have multiple characterizations in terms of logic, automata, regular expressions, and abstract algebra. Cognitive interpretations of these classes also exist (Rogers and Pullum, 2011; Rogers et al., 2013).\nSL is the formal language-theoretic basis of ngram models (Jurafsky and Martin, 2008) and SP models aspects of phonology (Heinz, 2010). We only provide characterizations for SL and SP.\nLet Σ denote a finite set of symbols, the alphabet, and Σ∗ the set of elements of the free monoid of Σ under concatenation. We refer to these elements both as strings and as words. The ith symbol in word w is denoted wi. Left and right word boundary markers (⋊ and ⋉, respectively) are symbols not in Σ. A stringset (also called formal language) is a subset of Σ∗.\nIf u and v are strings, uv denotes their concatenation. For all u, v, w, x ∈ Σ∗, if x = uwv then then w is a substring of x. If x ∈ Σ∗w1Σ ∗w2Σ ∗ . . . wnΣ ∗ then w is a subsequence of x. A substring (subsequence) of length k is called a k-factor (k-subsequence). Let factork(w) denote the set of substrings of w of length k. Let subseq\nk (w) denote the set of subse-\nquences of w up to length k. The domains of these functions extend to languages in the normal way.\nA stringset L is Strictly k-Local (SLk) iff when-\never there is a string x of length k − 1 and strings u1, v1, u2, v2 ∈ Σ\n∗, such that u1xv1, u2xv2 ∈ L then u1xv2 ∈ L. We say L is closed under suffix substitution. L is SL if L ∈ SLk for some k (Rogers and Pullum, 2011).\nAs discussed in\n(McNaughton and Papert, 1971; Rogers and Pullum, 2011), SLk languages can also be characterized by a finite set of kfactors as follows. A SLk grammar is a set of k-factors G ⊆ factork({⋊}Σ ∗{⋉}). Symbols ⋊ and ⋉ denote left and right word edges, respectively. The language of G is the stringset L(G) = {w | factork(w) ⊆ G}. The grammar G is the set of permissible k-factors. Any k-factor w in factork({⋊}Σ ∗{⋉}) which is not in G is thus forbidden and consequently all strings containing w as a substring are not in L(G). Since for each k, factork({⋊}Σ\n∗{⋉}) is finite, SL stringsets can be defined with grammars that only contain forbidden k-factors as we do in §4.\nA stringset L is Strictly k-Piecewise (SPk) iff\nsubseq k (w) ⊆ subseq k (L) implies w ∈ L. L is SP if there is a k such that it belong to SPk; equivalently, L belongs to SP iff L is closed under subsequence (Rogers et al., 2010). SPk stringsets can also be defined with a finite set of k-subsequences (Rogers et al., 2010). In fact the parallel to SLk is near perfect. A SPk grammar is a set of ksubsequences G ⊆ subseq k (Σ∗). The language of G is the stringset L(G) = {w | subseq k (w) ⊆ G}. The grammar G is the set of permissible k-subsequences. Any k-subsequence w in subseq k (Σ∗) which is not in G is thus forbidden so strings containing w as a subsequence are not in L(G). Since for each k, subseq k (Σ∗) is finite, SP stringsets can be defined with grammars containing forbidden k-subsequences as in §4.\nSL and SP classes form infinite hierarchies of language classes based on k (Rogers and Pullum, 2011; Rogers et al., 2010). In particular for all k ∈ N, SLk ( SLk+1 and SPk ( SPk+1. Consequently, for any SL (SP) stringset, the smallest k value for which it is SLk (SPk) is another measure of its complexity. Additionally, for given k, the SLk and SPk classes are learnable in the limit from positive data (Garcia et al., 1990; Heinz et al., 2012). These learners are efficient in time and data (de la Higuera, 1997)."
    }, {
      "heading" : "4 The Experiments",
      "text" : "Here we describe the target languages, the training data, the test sets, and the LSTM architecture.\nIn this study, six formal target languages were defined in order for training and testing purposes. Languages are referred to by the class they belong to. SL2, SL4 and SL8 were defined according to the following forbidden substrings {⋊b, aa, bb, a⋉}, {⋊bbb, aaaa, bbbb, aaa⋉} and {⋊bbbbbbb, aaaaaaaa, bbbbbbbb, aaaaaaa⋉}, respectively. SP2, SP4 and SP8 were defined according to the following forbidden subsequences {ab}, {abba} and {abbaabba}, respectively. In each case, we let Σ = {a, b, c, d}.\nThese grammars were implemented as finitestate machines using foma, a publicy available, open-source platform (Hulden, 2009). The number of states in the minimal deterministic automata recognizing SL2, SL4, SL8, SP2, SP4, and SP8 languages were 3, 7, 15, 2, 4 and 8, respectively.\nTraining data was generated with foma. For each language L we generated three training data sets, which we call 1k, 10k, and 100k because they contained 1,000, 10,000, and 100,000 words, respectively. Half of the words in each training set were positive examples (so they belonged to L) and half were negative examples (so they did not belong to L). Training words were between length 1 and 25. For the positive examples there were 20, 200, and 2,000 words of each length. These were generated randomly using foma, so training sets contained duplicates. For the negative examples, we wanted to provide 20, 200, and 2,000 words of each length, respectively. However, as the k value increases, there is no negative data for shorter words since all shorter words belong to L. In this case, we generated 20×k, 200×k, and 2,000×k of words of length k and 20, 200, and 2,000 words for the lengths between k+1 and 25.\nFor each language L and each training regimen T for L, we developed two test sets, which we call Test1 and Test2. Test1 and Test2 contain 1,000, 10,000, or 100,000 words depending on whether T is 1k, 10k, or 100k, respectively. Half of the test words belong to L and half do not. Test1 and Test2 only contain novel words. Novel positive words belong to L but do not belong to the positive examples in T. Novel negative words do not belong to L and do not belong to the negative examples in T. The difference between the two test sets has to do with word length. Test1 words are no longer\nthan 25. Test2 words are of length between 26 and 50. Thus while both sets test for generalization, Test2 importantly checks to what extent the generalizations are independent of word length.\nFor Test2, we used foma to randomly generate 20, 200, and 2,000 positive words and negative words for each length between 26 and 50.\nFor Test1, we used foma to randomly generate words whose length was less than 26. Again, we wanted to generate 20, 200, and 2,000 positive words and negative words for each length. However, there may not be positive or negative words for some of the shorter lengths. This is because either they do not exist for the target language L, or because they do exist for L but they also exist in the training data in which case they would not be novel. For positive (negative) words, we did the following. If there was at least one word of a particular length, we randomly generated 20, 200, or 2,000 words of that length depending on T. We also generated extra words of length 25 (specifically 200, 2,000, or 20,000). We concatenated these together ordered by length into one file and then selected the first 500, 5,000, or 50,000 words to be the positive (negative) words in Test1. The order of the words in the test sets was randomized.\nWe constructed simple LSTM networks to test the capability of the LSTM itself. We used a package for RNNs called Chainer (http://chainer.org) to implement them.\nFor each input string, the output of the network is the probability of it belonging to the target language. For each string, the network can be represented as a connected graph as shown in Figure 2. Among the possible modifications of LSTMs (Greff et al., 2015), the standard architecture with forget gates and without peepholes was used.\nThe output of the ‘embed’ layer in Figure 2 is known as the distributed representation of the symbol, which is equivalent to a linear layer (with\nno bias) that maps the one-hot vector to a realvalued vector. The outputs of the LSTM layer corresponding to each symbol except for the last one are ignored. The output corresponding to the last symbol is mapped to a two dimensional vector through the softmax layer, whose elements represent the positive and negative probabilities.\nEven with the simple structure shown above, there are many combinations of hyperparameters. To reduce the combinations, the vector sizes of the LSTM and the embed layer were made identical in this study. We used three LSTMs, which we call v10, v30, and v100 because the vectors are of size 10, 30, and 100, respectively.\nEach value of the weights in the ‘embed’ layer are independently initialized according to the normal distribution. The LSTM unit includes four fully-connected layers called the the input gate, the block input, the forget gate, and the output gate. The weights of those layers are initialized in the same manner as the ‘embed’ layer except for the weights of the forget gate, which are initialized according to the normal distribution with mean 1 and variance 1.\nWhen training the network, the training data are divided into batches, and the gradient is calculated for each batch. The strings in each batch are usually chosen uniformly and randomly to avoid biased gradients. However, calculation becomes inefficient if strings are chosen completely randomly since the differences of lengths of strings are large. Thus, we sorted all strings first in order of their lengths, re-order partially, block them and choose those blocks randomly. The LSTMs processed the training data 100 times or ‘epochs.’\nOther parameters for training were set up as follows. The batch size is 128, the L2 norm of the gradient is clipped with 1.0, and the optimization algorithm called Adam (Kingma and Ba, 2014) is applied. The lengths of strings in each batch are aligned through padding an additional symbol whose embedding is fixed to the zero vector."
    }, {
      "heading" : "5 Results",
      "text" : "After each epoch, the LSTMs were tested on the test sets. Inspection of the accuracy trajectories indicate that accuracy had stabilized in all cases well before the 100th epoch. Therefore, accuracy results after the 100th epoch of training are given in Table 1 (accuracy less than 80% is in bold; note that the chance level is 50%.).\nThere are three observations worth reporting. First, on the experiments run targeting the SL4, SL8, SP2, and SP4 stringsets, the accuracy is incredibly good across nearly all LSTM types and training regimens.\nSecond, on the SL2 experiments the accuracy of the v10 and v30 LSTMs improved from 1k to 10k, but then significantly worsened from 10k to 100k. The v100 LSTM significantly worsened from 1k to 10k and then slightly improved. These results suggest the LSTMs, especially the v10 and v30 versions, overfit the data.\nThird, across all LSTM types and training regimens, the accuracy on Test2 in the SP8 experiment was between 50% and 70%. While the Test1 scores are better, they do not come close to the accuracy in the SP2 and SP4 experiments. This suggests the LSTMs failed to generalize correctly."
    }, {
      "heading" : "6 Discussion",
      "text" : "What explains the poorer results in the SL2 and SP8 experiments? Generally, there are two possibilities: the training data was poor and/or the LSTMs were too naive.\nConsider the possibility that the training was\ninsufficient. Casual inspection revealed that the data generation was not truly random because some word types were disproportionately generated more than others. Grammatical inference algorithms can be used to determine data quality. For example, RPNI (Oncina and Garcia, 1992) provably identifies any regular language with positive and negative data if the data is of sufficient quality. Thus RPNI can tell us whether a training set contains all the information needed to discriminate any regular language from any other one. In this way, it is worthwhile to know how RPNI performs on the experiments described above.\nAnother possibility is the LSTMs were too naive. With respect to overfitting, we did not apply the dropout method nor other techniques which are known to help reduce it.\nWith respect to undergeneralization, of course we should use larger vector sizes, multi-layer LSTMs, the dropout method, Kalman filters, different initial weighting schemes, and so on. So this is one avenue of future research. But this goes hand-in-hand with varying the complexity of the target languages at different levels of abstraction, such as adding more forbidden strings to the grammars, increasing k, or moving up the subregular hierarchy to more complex classes. The goal here is not one-upmanship, but to instead get a better understanding of how properties of RNNs relate to properties of formal language classes.\nThese experiments showed that naive LSTMs have difficulty with learning a formal language defined by a forbidden subsequence of length 8 but no difficulty with learning a formal language defined by forbidden substrings of length 8. The difference between substring and subsequence— which reduces logically to the difference between the successor and precedence relations (Rogers et al., 2013)—is thus significant for naive LSTMs."
    }, {
      "heading" : "7 Conclusion",
      "text" : "We believe the experiments presented here help show how formal language theory can reveal the advantages and disadvantages of various RNN models more clearly than testing with real-world datasets. The primary reason is that we can control the nature of the target patterns and their complexity. This was illustrated here with the comparison of SL and SP languages which encode local and long-distance dependencies, respectively,\nand with the k value. Moreover, as shown through Test1 and Test2, we also can control the test data to better understand how the RNNs generalize. Finally, we identified future research goals with less naive RNNs, more complex formal languages, and the integration of grammatical inference to validate the sufficiency of the training data."
    } ],
    "references" : [ {
      "title" : "The dynamics of discrete-time computation, with application to recurrent neural networks and finite state machine extraction",
      "author" : [ "M Casey" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "Casey.,? \\Q1996\\E",
      "shortCiteRegEx" : "Casey.",
      "year" : 1996
    }, {
      "title" : "Chalup and Alan D",
      "author" : [ "StephanK" ],
      "venue" : "Blair.",
      "citeRegEx" : "Chalup and Blair2003",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "A Mathematical Introduction to Logic",
      "author" : [ "Herbert B. Enderton" ],
      "venue" : null,
      "citeRegEx" : "Enderton.,? \\Q2001\\E",
      "shortCiteRegEx" : "Enderton.",
      "year" : 2001
    }, {
      "title" : "Enrique Vidal",
      "author" : [ "Pedro Garcia" ],
      "venue" : "and José Oncina.",
      "citeRegEx" : "Garcia et al.1990",
      "shortCiteRegEx" : null,
      "year" : 1990
    }, {
      "title" : "G Z Sun",
      "author" : [ "C L Giles", "C B Miller", "D Chen", "H H Chen" ],
      "venue" : "and Y C Lee.",
      "citeRegEx" : "Giles et al.1992",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "Jan Koutnı́k",
      "author" : [ "Klaus Greff", "Rupesh Kumar Srivastava" ],
      "venue" : "Bas R. Steunebrink, and Jürgen Schmidhuber.",
      "citeRegEx" : "Greff et al.2015",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Anna Kasprzik",
      "author" : [ "Jeffrey Heinz" ],
      "venue" : "and Timo Kötzing.",
      "citeRegEx" : "Heinz et al.2012",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Learning long-distance phonotactics",
      "author" : [ "Jeffrey Heinz" ],
      "venue" : "Linguistic Inquiry,",
      "citeRegEx" : "Heinz.,? \\Q2010\\E",
      "shortCiteRegEx" : "Heinz.",
      "year" : 2010
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Hochreiter", "Schmidhuber1997] S Hochreiter", "J Schmidhuber" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Hochreiter et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Hochreiter et al\\.",
      "year" : 1997
    }, {
      "title" : "Foma: a finite-state compiler and library. In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, pages 29–32",
      "author" : [ "Mans Hulden" ],
      "venue" : null,
      "citeRegEx" : "Hulden.,? \\Q2009\\E",
      "shortCiteRegEx" : "Hulden.",
      "year" : 2009
    }, {
      "title" : "Speech and Language Processing: An Introduction to Natural Language Processing, SpeechRecognition, and Computational Linguistics",
      "author" : [ "Jurafsky", "Martin2008] Daniel Jurafsky", "James Martin" ],
      "venue" : null,
      "citeRegEx" : "Jurafsky et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Jurafsky et al\\.",
      "year" : 2008
    }, {
      "title" : "Adam: A method for stochastic optimization. CoRR, abs/1412.6980",
      "author" : [ "Kingma", "Ba2014] Diederik P. Kingma", "Jimmy Ba" ],
      "venue" : null,
      "citeRegEx" : "Kingma et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kingma et al\\.",
      "year" : 2014
    }, {
      "title" : "Yoshua Bengio",
      "author" : [ "Yann LeCun" ],
      "venue" : "and Geoffrey Hinton.",
      "citeRegEx" : "LeCun et al.2015",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Identifying regular languages in polynomial time. In Advances In Structural And Syntactic Pattern Recognition, Volume 5 Of Series In Machine Perception And Artificial Intelligence, pages",
      "author" : [ "Oncina", "Garcia1992] Jose Oncina", "Pedro Garcia" ],
      "venue" : null,
      "citeRegEx" : "Oncina et al\\.,? \\Q1992\\E",
      "shortCiteRegEx" : "Oncina et al\\.",
      "year" : 1992
    }, {
      "title" : "The Induction of Dynamical Recognizers",
      "author" : [ "Jordan B. Pollack" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Pollack.,? \\Q1991\\E",
      "shortCiteRegEx" : "Pollack.",
      "year" : 1991
    }, {
      "title" : "Douglas Eck",
      "author" : [ "Juan Antonio Prez-Ortiz", "Felix A. Gers" ],
      "venue" : "and Jrgen Schmidhuber.",
      "citeRegEx" : "Prez.Ortiz et al.2003",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Implicit learning of artificial grammars",
      "author" : [ "Arthur S. Reber" ],
      "venue" : "Journal of Verbal Learning and Verbal Behavior,",
      "citeRegEx" : "Reber.,? \\Q1967\\E",
      "shortCiteRegEx" : "Reber.",
      "year" : 1967
    }, {
      "title" : "Aural pattern recognition experiments and the subregular hierarchy",
      "author" : [ "Rogers", "Pullum2011] James Rogers", "Geoffrey Pullum" ],
      "venue" : "Journal of Logic, Language and Information,",
      "citeRegEx" : "Rogers et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Rogers et al\\.",
      "year" : 2011
    }, {
      "title" : "David Wellcome",
      "author" : [ "James Rogers", "Jeffrey Heinz", "Gil Bailey", "Matt Edlefsen", "Molly Visscher" ],
      "venue" : "and Sean Wibel.",
      "citeRegEx" : "Rogers et al.2010",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Dakotah Lambert",
      "author" : [ "James Rogers", "Jeffrey Heinz", "Margaret Fero", "Jeremy Hurst" ],
      "venue" : "and Sean Wibel.",
      "citeRegEx" : "Rogers et al.2013",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "and D",
      "author" : [ "J. Schmidhuber", "F. Gers" ],
      "venue" : "Eck.",
      "citeRegEx" : "Schmidhuber et al.2002",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Piecewise testable events",
      "author" : [ "Imre Simon" ],
      "venue" : "In Automata Theory and Formal Languages,",
      "citeRegEx" : "Simon.,? \\Q1975\\E",
      "shortCiteRegEx" : "Simon.",
      "year" : 1975
    }, {
      "title" : "Smith",
      "author" : [ "D Zipser" ],
      "venue" : "A.W.",
      "citeRegEx" : "Smith. A.W.1989",
      "shortCiteRegEx" : null,
      "year" : 1989
    }, {
      "title" : "Classifying regular events in symbolic logic",
      "author" : [ "Wolfgang Thomas" ],
      "venue" : "Journal of Computer and Systems Sciences,",
      "citeRegEx" : "Thomas.,? \\Q1982\\E",
      "shortCiteRegEx" : "Thomas.",
      "year" : 1982
    }, {
      "title" : "Learning of construction of finite automata from examples using hill-climbing",
      "author" : [ "Masaru Tomita" ],
      "venue" : "Proc. Fourth Int. Cog. Sci. Conf., pages",
      "citeRegEx" : "Tomita.,? \\Q1982\\E",
      "shortCiteRegEx" : "Tomita.",
      "year" : 1982
    }, {
      "title" : "Induction of Finite-State Automata Using Second-Order Recurrent Networks",
      "author" : [ "Watrous", "Kuhn1992] Raymond L Watrous", "G M Kuhn" ],
      "venue" : "Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Watrous et al\\.,? \\Q1992\\E",
      "shortCiteRegEx" : "Watrous et al\\.",
      "year" : 1992
    } ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "This paper presents experiments illustrating how formal language theory can shed light on deep learning. We train naive Long Short-Term Memory (LSTM) Recurrent Neural Networks (RNNs) on six formal languages drawn from the Strictly Local (SL) and Strictly Piecewise (SP) classes. These classes are relevant to computational linguistics and among the simplest in a mathematically well-understood hierarchy of subregular classes. SL and SP classes encode local and long-distance dependencies, respectively. The results show four of the six languages were learned remarkably well, but overfitting arguably occurred with the simplest SL language and undergeneralization with the most complex SP pattern. Even though LSTMs were developed to handle long-distance dependencies, the latter result shows they stymie naive LSTMs in contrast to local dependencies. While it remains to be seen which of the many variants of LSTMsmay learn SP languages well, this result speaks to the larger point that the judicial use of formal language theory can illuminate the inner workings of RNNs. 1 Investigating Deep Learning This paper proposes that formal language theory provides a systematic way to better understand the kinds of patterns deep learning networks (LeCun et al., 2015) are able to learn. The main ideas of this approach are illustrated with experiments testing how well LSTM networks (Hochreiter and Schmidhuber, 1997) can learn different formal languages. The formal languages are drawn from subclasses of the regular languages. These are wellunderstood theoretically to form a complexity hierarchy (McNaughton and Papert, 1971; Rogers et al., 2010; Rogers and Pullum, 2011). This complexity of a language is not based on automata-theoretic measures, such as the size of the minimal deterministic automaton, but instead has a model-theoretic basis (Enderton, 2001). It is determined by the kind of logic and model needed to specify it (Rogers et al., 2013). This approach is not entirely without precedent. Earlier research used formal languages to probe the learning capabilities of neural networks. Section 2 highlights some of this work to make clearer our own motivation and contribution. We are particularly interested in understanding how well LSTMs can learn SP languages. As will be explained in §3, SP languages are simple regular languages which encode only certain types of long-distance dependencies. LSTMs were specifically designed to model long-distance dependencies within temporal sequences more accurately. We compare the performance of LSTMs on SP languages with their performance on learning SL languages, another simple class of regular languages, but one which only encodes local dependencies in sequences. We trained LSTMs on both positive and negative examples of SL and SP languages. We controlled the training and testing data for word length, varied the amount of training data, and varied the LSTMs with respect to vector size. The experimental details are explained in §4. The results, presented in §5, are somewhat unexpected. While LSTMs performed above chance in all of our experiments, LSTMs showed both overfitting and undergeneralization. Specifically, overfitting was observed in the simplest SL language and undergeneralization was observed in the most complex SP language. Implications of these results are explained in §6. These results are unexpected because it is known that LSTMs can learn some context-sensitive formal languages exhibiting long-distance dependencies with uncanny precision (Prez-Ortiz et al., 2003). 2 Motivation and background In the 1990s, many studies aimed to learn formal languages with neural networks. When the aim was to predict the next symbol of a string drawn from a regular language, first-order RNNs were used (Casey, 1996; Smith, A.W., 1989). The target languages here were based on the Reber grammar (Reber, 1967). When the aim was to decide whether a string is grammatical, second-order RNNs were used (Pollack, 1991; Watrous and Kuhn, 1992; Giles et al., 1992). Here the target languages were the regular languages studied by Tomita (Tomita, 1982). Later research targeted nonregular languages (Schmidhuber et al., 2002; Chalup and Blair, 2003; Prez-Ortiz et al., 2003). However, this line of research has not been pursued recently to our knowledge. The reasons for making formal languages the targets of learning are as valid today as they were decades ago. First, the grammars generating the formal languages are known. Therefore training and test data can be generated as desired. Thus, the scientist can run controlled experiments to see whether particular generalizations are reliably acquired under particular training regimens. Importantly, the relative complexity of different formal languages may provide additional insight. If it is found that formal languages of one type are more readily learned than formal languages of another type in some set of experiments then the difference between these classes may be said to meaningfully capture some property that the RNNs in the experiments are incapable of capturing. Subsequent work may lead to proofs and theorems about which LSTMs can reliably infer formal languages from certain classes and which cannot. It may also lead to new network architectures which overcome identified hurdles. The primary difference between the present paper and past research, beyond the development in neural networks, is that the regular languages chosen here are known to have certain properties. The Reber grammars and Tomita languages were not understood in terms of their abstract properties or pattern complexity. Even though Regular NC PT SP LTT LT SL ✭✭✭✭ ✭✭✭ ◗ ◗◗ Figure 1: Subregular language classes with inclusion shown left-to-right. subregular distinctions had already been studied (McNaughton and Papert, 1971), it went unrecognized how that branch of computer science could inform neural network learning. 3 Subregular Complexity Figure 1 shows proper inclusion relationships of well-studied classes of subregular languages. The Strictly Local (SL), Locally Testable (LT), and Non-Counting (NC) classes were studied by (McNaughton and Papert, 1971). The Locally Threshold Testable (LTT) class was introduced and studied by (Thomas, 1982). The Piecewise Testable (PT) class was introduced and studied by (Simon, 1975). The Strictly Piecewise (SP) class was studied by (Rogers et al., 2010). As many authors discuss, these classes are natural because they have multiple characterizations in terms of logic, automata, regular expressions, and abstract algebra. Cognitive interpretations of these classes also exist (Rogers and Pullum, 2011; Rogers et al., 2013). SL is the formal language-theoretic basis of ngram models (Jurafsky and Martin, 2008) and SP models aspects of phonology (Heinz, 2010). We only provide characterizations for SL and SP. Let Σ denote a finite set of symbols, the alphabet, and Σ∗ the set of elements of the free monoid of Σ under concatenation. We refer to these elements both as strings and as words. The ith symbol in word w is denoted wi. Left and right word boundary markers (⋊ and ⋉, respectively) are symbols not in Σ. A stringset (also called formal language) is a subset of Σ∗. If u and v are strings, uv denotes their concatenation. For all u, v, w, x ∈ Σ∗, if x = uwv then then w is a substring of x. If x ∈ Σ∗w1Σ ∗w2Σ ∗ . . . wnΣ ∗ then w is a subsequence of x. A substring (subsequence) of length k is called a k-factor (k-subsequence). Let factork(w) denote the set of substrings of w of length k. Let subseq k (w) denote the set of subsequences of w up to length k. The domains of these functions extend to languages in the normal way. A stringset L is Strictly k-Local (SLk) iff whenever there is a string x of length k − 1 and strings u1, v1, u2, v2 ∈ Σ ∗, such that u1xv1, u2xv2 ∈ L then u1xv2 ∈ L. We say L is closed under suffix substitution. L is SL if L ∈ SLk for some k (Rogers and Pullum, 2011). As discussed in (McNaughton and Papert, 1971; Rogers and Pullum, 2011), SLk languages can also be characterized by a finite set of kfactors as follows. A SLk grammar is a set of k-factors G ⊆ factork({⋊}Σ ∗{⋉}). Symbols ⋊ and ⋉ denote left and right word edges, respectively. The language of G is the stringset L(G) = {w | factork(w) ⊆ G}. The grammar G is the set of permissible k-factors. Any k-factor w in factork({⋊}Σ ∗{⋉}) which is not in G is thus forbidden and consequently all strings containing w as a substring are not in L(G). Since for each k, factork({⋊}Σ ∗{⋉}) is finite, SL stringsets can be defined with grammars that only contain forbidden k-factors as we do in §4. A stringset L is Strictly k-Piecewise (SPk) iff subseq k (w) ⊆ subseq k (L) implies w ∈ L. L is SP if there is a k such that it belong to SPk; equivalently, L belongs to SP iff L is closed under subsequence (Rogers et al., 2010). SPk stringsets can also be defined with a finite set of k-subsequences (Rogers et al., 2010). In fact the parallel to SLk is near perfect. A SPk grammar is a set of ksubsequences G ⊆ subseq k (Σ∗). The language of G is the stringset L(G) = {w | subseq k (w) ⊆ G}. The grammar G is the set of permissible k-subsequences. Any k-subsequence w in subseq k (Σ∗) which is not in G is thus forbidden so strings containing w as a subsequence are not in L(G). Since for each k, subseq k (Σ∗) is finite, SP stringsets can be defined with grammars containing forbidden k-subsequences as in §4. SL and SP classes form infinite hierarchies of language classes based on k (Rogers and Pullum, 2011; Rogers et al., 2010). In particular for all k ∈ N, SLk ( SLk+1 and SPk ( SPk+1. Consequently, for any SL (SP) stringset, the smallest k value for which it is SLk (SPk) is another measure of its complexity. Additionally, for given k, the SLk and SPk classes are learnable in the limit from positive data (Garcia et al., 1990; Heinz et al., 2012). These learners are efficient in time and data (de la Higuera, 1997). 4 The Experiments Here we describe the target languages, the training data, the test sets, and the LSTM architecture. In this study, six formal target languages were defined in order for training and testing purposes. Languages are referred to by the class they belong to. SL2, SL4 and SL8 were defined according to the following forbidden substrings {⋊b, aa, bb, a⋉}, {⋊bbb, aaaa, bbbb, aaa⋉} and {⋊bbbbbbb, aaaaaaaa, bbbbbbbb, aaaaaaa⋉}, respectively. SP2, SP4 and SP8 were defined according to the following forbidden subsequences {ab}, {abba} and {abbaabba}, respectively. In each case, we let Σ = {a, b, c, d}. These grammars were implemented as finitestate machines using foma, a publicy available, open-source platform (Hulden, 2009). The number of states in the minimal deterministic automata recognizing SL2, SL4, SL8, SP2, SP4, and SP8 languages were 3, 7, 15, 2, 4 and 8, respectively. Training data was generated with foma. For each language L we generated three training data sets, which we call 1k, 10k, and 100k because they contained 1,000, 10,000, and 100,000 words, respectively. Half of the words in each training set were positive examples (so they belonged to L) and half were negative examples (so they did not belong to L). Training words were between length 1 and 25. For the positive examples there were 20, 200, and 2,000 words of each length. These were generated randomly using foma, so training sets contained duplicates. For the negative examples, we wanted to provide 20, 200, and 2,000 words of each length, respectively. However, as the k value increases, there is no negative data for shorter words since all shorter words belong to L. In this case, we generated 20×k, 200×k, and 2,000×k of words of length k and 20, 200, and 2,000 words for the lengths between k+1 and 25. For each language L and each training regimen T for L, we developed two test sets, which we call Test1 and Test2. Test1 and Test2 contain 1,000, 10,000, or 100,000 words depending on whether T is 1k, 10k, or 100k, respectively. Half of the test words belong to L and half do not. Test1 and Test2 only contain novel words. Novel positive words belong to L but do not belong to the positive examples in T. Novel negative words do not belong to L and do not belong to the negative examples in T. The difference between the two test sets has to do with word length. Test1 words are no longer",
    "creator" : "LaTeX with hyperref package"
  }
}