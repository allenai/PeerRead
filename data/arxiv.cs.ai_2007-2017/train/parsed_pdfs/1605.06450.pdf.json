{
  "name" : "1605.06450.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Query-Efficient Imitation Learning for End-to-End Autonomous Driving",
    "authors" : [ "Jiakai Zhang", "Kyunghyun Cho" ],
    "emails" : [ "zhjk@nyu.edu", "kyunghyun.cho@nyu.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "We define end-to-end autonomous driving as driving by a single, self-contained system that maps from a sensory input, such as an image frame from a front-facing camera, to actions necessary for driving, such as the angle of steering wheel and braking. In this approach, the autonomous driving system is often learned from data rather than manually designed, mainly due to sheer complexity of manually developing a such system.\nThis end-to-end approach to autonomous driving dates back to late 80’s. ALVINN by Pomerleau [13] was a neural network with a single hidden layer that takes as input an image frame from a front-facing camera and a response map from a range finder sensor and returns a quantized steering wheel angle. The ALVINN was trained using a set of training tuples (image, sensor map, steering angle) collected from simulation. A similar approach was taken later in 2005 to train, this time, a convolutional neural network to drive an off-road mobile robot [11]. More recently, Bojarski et al. [3] used a similar, but deeper, convolutional neural network for lane following based solely on a front-facing camera. In all these cases, a deep neural network has been found to be surprisingly effective at learning a complex mapping from a raw image to control.\nA major learning paradigm behind all these previous attempts has been supervised learning. A human driver or a rule-based AI driver in a simulator, to which we refer as a reference policy drives a car equipped with a front-facing camera and other types of sensors while collecting image-action pairs. These collected pairs are used as training examples to train a neural network controller, called a primary policy. It is however well known that a purely supervised learning based approach to\nar X\niv :1\n60 5.\n06 45\n0v 1\n[ cs\n.L G\n] 2\n0 M\nay 2\nimitation learning (where a learner tries to imitate a human driver) is suboptimal (see, e.g., [7, 16] and references therein.)\nWe therefore investigate a more advanced approach to imitation learning for training a neural network controller for autonomous driving. More specifically, we focus on DAgger [16] which works in a setting where the reward is given only implicitly. DAgger improves upon supervised learning by letting a primary policy collect training examples while running a reference policy simultaneously. This dramatically improves the performance of a neural network based primary policy. We however notice that DAgger needs to constantly query a reference policy, which is expensive especially when a reference policy may be a human driver.\nIn this paper, we propose a query-efficient extension of the DAgger, called SafeDAgger. We first introduce a safety policy that learns to predict the error made by a primary policy without querying a reference policy. This safety policy is incorporated into the DAgger’s iterations in order to select only a small subset of training examples that are collected by a primary policy. This subset selection significantly reduces the number of queries to a reference policy.\nWe empirically evaluate the proposed SafeDAgger using TORCS [1], a racing car simulator, which has been used for vision-based autonomous driving research in recent years [9, 6]. In this paper, our goal is to learn a primary policy that can drive a car indefinitely without any crash or going out of a road. The experiments show that the SafeDAgger requires much less queries to a reference policy than the original DAgger does and achieves a superior performance in terms of the average number of laps without crash and the amount of damage. We conjecture that this is due to the effect of automated curriculum learning created by the subset selection based on the safety policy."
    }, {
      "heading" : "2 Imitation Learning for Autonomous Driving",
      "text" : "In this section, we describe imitation learning in the context of learning an automatic policy for driving a car."
    }, {
      "heading" : "2.1 State Transition and Reward",
      "text" : "A surrounding environment, or a world, is defined as a set of states S. Each state is accompanied by a set of possible actions A(S). Any given state s ∈ S transitions to another state s′ ∈ S when an action a ∈ A(S) is performed, according to a state transition function δ : S ×A(S)→ S. This transition function may be either deterministic or stochastic.\nFor each sequence of state-action pairs, there is an associated (accumulated) reward r:\nr(Ω = ((s0, a0), (s1, a1), (s2, a2), . . .)),\nwhere st = δ(st−1, at−1).\nA reward may be implicit in the sense that the reward comes as a form of a binary value with 0 corresponding to any unsuccessful run (e.g., crashing into another car so that the car breaks down,) while any successful run (e.g., driving indefinitely without crashing) does not receive the reward. This is the case in which we are interested in this paper. In learning to drive, the reward is simply defined as follows:\nr(Ω) = { 1, if there was no crash, 0, otherwise\nThis reward is implicit, because it is observed only when there is a failure, and no reward is observed with an optimal policy (which never crashes and drives indefinitely.)"
    }, {
      "heading" : "2.2 Policies",
      "text" : "A policy is a function that maps from a state observation φ(s) to one a of the actions available A(s) at the state s. An underlying state s describes the surrounding environment perfectly, while a policy often has only a limited access to the state via its observation φ(s). In the context of end-to-end autonomous driving, s summarizes all necessary information about the road (e.g., # of lanes, existence of other cars or pedestrians, etc.,) while φ(s) is, for instance, an image frame taken by a front-facing camera.\nWe have two separate policies. First, a primary policy π is a policy that learns to drive a car. This policy does not observe a full, underlying state s but only has access to the state observation φ(s), which is in this paper a pixel-level image frame from a front-facing camera. The primary policy is implemented as a function parametrized by a set of parameters θ.\nThe second one is a reference policy π∗. This policy may or may not be optimal, but is assumed to be a good policy which we want the primary policy to imitate. In the context of autonomous driving, a reference policy can be a human driver. We use a rule-based controller, which has access to a true, underlying state in a driving simulator, as a reference policy in this paper.\nCost of a Policy Unlike previous works on imitation learning (see, e.g., [7, 16, 5]), we introduce a concept of cost to a policy. The cost of querying a policy given a state for an appropriate action varies significantly based on how the policy is implemented. For instance, it is expensive to query a reference policy, if it is a human driver. On the other hand, it is much cheaper to query a primary policy which is often implemented as a classifier. Therefore, in this paper, we analyze an imitation learning algorithm in terms of how many queries it makes to a reference policy."
    }, {
      "heading" : "2.3 Driving",
      "text" : "A car is driven by querying a policy for an action with a state observation φ(s) at each time step. The policy, in this paper, observes an image frame from a front-facing camera and returns both the angle of a steering wheel (u ∈ [−1, 1]) and a binary indicator for braking (b ∈ {0, 1}). We call this strategy of relying on a single fixed policy a naive strategy.\nReachable States With a set of initial state Sπ0 ⊂ S, each policy π defines a subset of the reachable states Sπ. That is, Sπ = ∪∞t=1Sπt , where Sπt = { s|s = δ(s′, π(φ(s′))) ∀s′ ∈ Sπt−1 } . In other words, a car driven by a policy π will only visit the states in Sπ .\nWe use S∗ to be a reachable set by the reference policy. In the case of learning to drive, this reference set is intuitively smaller than that by any other reasonable, non-reference policy. This happens, as the reference policy avoids any state that is likely to lead to a low reward which corresponds to crashing into other cars and road blocks or driving out of the road."
    }, {
      "heading" : "2.4 Supervised Learning",
      "text" : "Imitation learning aims at finding a primary policy π that imitates a reference policy π∗. The most obvious approach to doing so is supervised learning. In supervised learning, a car is first driven by a reference policy while collecting the state observations φ(s) of the visited states, resulting in D = {φ(s)1, φ(s)2, . . . , φ(s)N} . Based on this dataset, we define a loss function as\nlsupervised(π, π ∗, D) =\n1\nN N∑ n=1 ‖π(φ(s)n)− π∗(φ(s)n)‖2. (1)\nThen, a desired primary policy is π̂ = arg minπ lsupervised(π, π ∗, D).\nA major issue of this supervised learning approach to imitation learning stems from the imperfection of the primary policy π̂ even after training. This imperfection likely leads the primary policy to a state s which is not included in the reachable set S∗ of the reference policy, i.e., s /∈ S∗. As this state cannot have been included in the training set D ⊆ S∗, the behaviour of the primary policy becomes unpredictable. The imperfection arises from many possible factors, including sub-optimal loss minimization, biased primary policy, stochastic state transition and partial observability."
    }, {
      "heading" : "2.5 DAgger: beyond Supervised Learning",
      "text" : "A major characteristics of the supervised learning approach described above is that it is only the reference policy π∗ that generates training examples. This has a direct consequence that the training set is almost a subset of the reference reachable set S∗. The issue with supervised learning can however be addressed by imitation learning or learning-to-search [7, 16].\nIn the framework of imitation learning, the primary policy, which is currently being estimated, is also used in addition to the reference policy when generating training examples. The overall training set\nused to tune the primary policy then consists of both the states reachable by the reference policy as well as the intermediate primary policies. This makes it possible for the primary policy to correct its path toward a good state, when it visits a state unreachable by the reference policy, i.e., s ∈ Sπ\\S∗. DAgger is one such imitation learning algorithm proposed in [16]. This algorithm finetunes a primary policy trained initially with the supervised learning approach described earlier. Let D0 and π0 be the supervised training set (generated by a reference policy) and the initial primary policy trained in a supervised manner. Then, DAgger iteratively performs the following steps. At each iteration i, first, additional training examples are generated by a mixture of the reference π∗ and primary πi−1 policies (i.e.,\nβiπ ∗ + (1− βi)πi−1 (2) ) and combined with all the previous training sets: Di = Di−1 ∪ { φ(s)i1, . . . , φ(s) i N } . The primary policy is then finetuned, or trained from scratch, by minimizing lsupervised(θ,Di) (see Eq. (1).) This iteration continues until the supervised cost on a validation set stops improving.\nDAgger does not rely on the availability of explicit reward. This makes it suitable for the purpose in this paper, where the goal is to build an end-to-end autonomous driving model that drives on a road indefinitely. However, it is certainly possible to incorporate an explicit reward with other imitation learning algorithms, such as SEARN [7], AggreVaTe [15] and LOLS [5]. Although we focus on DAgger in this paper, our proposal later on applies generally to any learning-to-search type of imitation learning algorithms.\nCost of DAgger At each iteration, DAgger queries the reference policy for each and every collected state. In other words, the cost of DAgger CDAggeri at the i-th iteration is equivalent to the number of training examples collected, i.e, CDAggeri = |Di|. In all, the cost of DAgger for learning a primary policy is CDAgger = ∑M i=1 |Di|, excluding the initial supervised learning stage.\nThis high cost of DAgger comes with a more practical issue, when a reference policy is a human operator, or in our case a human driver. First, as noted in [17], a human operator cannot drive well without actual feedback, which is the case of DAgger as the primary policy drives most of the time. This leads to suboptimal labelling of the collected training examples. Furthermore, this constant operation easily exhausts a human operator, making it difficult to scale the algorithm toward more iterations."
    }, {
      "heading" : "3 SafeDAgger: Query-Efficient Imitation Learning with a Safety Policy",
      "text" : "We propose an extension of DAgger that minimizes the number of queries to a reference policy both during training and testing. In this section, we describe this extension, called SafeDAgger, in detail."
    }, {
      "heading" : "3.1 Safety Policy",
      "text" : "Unlike previous approaches to imitation learning, often as learning-to-search [7, 16, 5], we introduce an additional policy πsafe, to which we refer as a safety policy. This policy takes as input both the partial observation of a state φ(s) and a primary policy π and returns a binary label indicating whether the primary policy π is likely to deviate from a reference policy π∗ without querying it.\nWe define the deviation of a primary policy π from a reference policy π∗ as\n(π, π∗, φ(s)) = ‖π(φ(s))− π∗(φ(s))‖2 . Note that the choice of error metric can be flexibly chosen depending on a target task. For instance, in this paper, we simply use the L2 distance between a reference steering angle and a predicted steering angle, ignoring the brake indicator.\nThen, with this defined deviation, the optimal safety policy π∗safe is defined as\nπ∗safe(π, φ(s)) = { 0, if (π, π∗, φ(s)) > τ 1, otherwise , (3)\nwhere τ is a predefined threshold. The safety policy decides whether the choice made by the policy π at the current state can be trusted with respect to the reference policy. We emphasize again that this determination is done without querying the reference policy.\nLearning A safety policy is not given, meaning that it needs to be estimated during learning. A safety policy πsafe can be learned by collecting another set of training examples:1 D′ = {φ(s)′1, φ(s)′2, . . . , φ(s)′N} . We define and minimize a binary cross-entropy loss:\nlsafe(πsafe, π, π ∗, D′) = − 1\nN N∑ n=1 π∗safe(φ(s) ′ n) log πsafe(φ(s) ′ n, π)+ (4)\n(1− π∗safe(φ(s)′n)) log(1− πsafe(φ(s)′n, π)), where we model the safety policy as returning a Bernoulli distribution over {0, 1}.\nDriving: Safe Strategy Unlike the naive strategy, which is a default go-to strategy in most cases of reinforcement learning or imitation learning, we can design a safe strategy by utilizing the proposed safety policy πsafe. In this strategy, at each point in time, the safety policy determines whether it is safe to let the primary policy drive. If so (i.e., πsafe(π, φ(s)) = 1,) we use the action returned by the primary policy (i.e., π(φ(s)).) If not (i.e., πsafe(π, φ(s)) = 0,) we let the reference policy drive instead (i.e., π∗(φ(s)).)\nAssuming the availability of a good safety policy, this strategy avoids any dangerous situation arisen by an imperfect primary policy, that may lead to a low reward (e.g., break-down by a crash.) In the context of learning to drive, this safe strategy can be thought of as letting a human driver take over the control based on an automated decision.2 Note that this driving strategy is applicable regardless of a learning algorithm used to train a primary policy.\nDiscussion The proposed use of safety policy has a potential to address this issue up to a certain point. First, since a separate training set is used to train the safety policy, it is more robust to unseen states than the primary policy. Second and more importantly, the safety policy finds and exploits a simpler decision boundary between safe and unsafe states instead of trying to learn a complex mapping from a state observation to a control variables. For instance, in learning to drive, the safety policy may simply learn to distinguish between a crowded road and an empty road and determine that it is safer to let the primary policy drive in an empty road.\nRelationship to a Value Function A value function V π(s) in reinforcement learning computes the reward a given policy π can achieve in the future starting from a given state s [19]. This description already reveals a clear connection between the safety policy and the value function. The safety policy πsafe(π, s) determines whether a given policy π is likely to fail if it operates at a given state s, in terms of the deviation from a reference policy. By assuming that a reward is only given at the very end of a policy run and that the reward is 1 if the current policy acts exactly like the reference policy and otherwise 0, the safety policy precisely returns the value of the current state.\nA natural question that follows is whether the safety policy can drive a car on its own. This perspective on the safety policy as a value function suggests a way to using the safety policy directly to drive a car. At a given state s, the best action â can be selected to be arg maxa∈A(s) πsafe(π, δ(s, a)). This is however not possible in the current formulation, as the transition function δ is unknown. We may extend the definition of the proposed safety policy so that it considers a state-action pair (s, a) instead of a state alone and predicts the safety in the next time step, which makes it closer to a Q function."
    }, {
      "heading" : "3.2 SafeDAgger: Safety Policy in the Loop",
      "text" : "We describe here the proposed SafeDAgger which aims at reducing the number of queries to a reference policy during iterations. At the core of SafeDAgger lies the safety policy introduced earlier in this section. The SafeDAgger is presented in Alg. 1. There are two major modifications to the original DAgger from Sec. 2.5.\nFirst, we use the safe strategy, instead of the naive strategy, to collect training examples (line 6 in Alg. 1). This allows an agent to simply give up when it is not safe to drive itself and hand over the control to the reference policy, thereby collecting training examples with a much further horizon without crashing. This would have been impossible with the original DAgger unless the manually forced take-over measure was implemented [17].\n1 It is certainly possible to simply set aside a subset of the original training set for this purpose. 2 Such intervention has been done manually by a human driver [14].\nAlgorithm 1 SafeDAgger Blue fonts are used to highlight the differences from the vanilla DAgger. 1: Collect D0 using a reference policy π∗ 2: Collect Dsafe using a reference policy π∗ 3: π0 = arg minπ lsupervised(π, π\n∗, D0) 4: πsafe,0 = arg minπsafe lsafe(πsafe, π0, π\n∗, Dsafe ∪D0) 5: for i = 1 to M do 6: Collect D′ using the safety strategy using πi−1 and πsafe,i−1 7: Subset Selection: D′ ← {φ(s) ∈ D′|πsafe,i−1(πi−1, φ(s)) = 0} 8: Di = Di−1 ∪D′ 9: πi = arg minπ lsupervised(π, π\n∗, Di) 10: πsafe,i = arg minπsafe lsafe(πsafe, πi, π\n∗, Dsafe ∪Di) 11: end for 12: return πM and πsafe,M\nSecond, the subset selection (line 7 in Alg. 1) drastically reduces the number of queries to a reference policy. Only a small subset of states where the safety policy returned 0 need to be labelled with reference actions. This is contrary to the original DAgger, where all the collected states had to be queried against a reference policy.\nFurthermore, this subset selection allows the subsequent supervised learning to focus more on difficult cases, which almost always correspond to the states that are problematic (i.e., S\\S∗.) This reduces the total amount of training examples without losing important training examples, thereby making this algorithm data-efficient.\nOnce the primary policy is updated with Di which is a union of the initial training set D0 and all the hard examples collected so far, we update the safety policy. This step ensures that the safety policy correctly identifies which states are difficult/dangerous for the latest primary policy. This has an effect of automated curriculum learning [2] with a mix strategy [20], where the safety policy selects training examples of appropriate difficulty at each iteration.\nDespite these differences, the proposed SafeDAgger inherits much of the theoretical guarantees from the DAgger. This is achieved by gradually increasing the threshold τ of the safety policy (Eq. (3)). If τ > (π, φ(s)) for all s ∈ S, the SafeDAgger reduces to the original DAgger with βi (from Eq. (2)) set to 0. We however observe later empirically that this is not necessary, and that training with the proposed SafeDAgger with a fixed τ automatically and gradually reduces the portion of the reference policy during data collection over iterations.\nAdaptation to Other Imitation Learning Algorithms The proposed use of a safety policy is easily adaptable to other more recent cost-sensitive algorithms. In AggreVaTe [15], for instance, the roll-out by a reference policy may be executed not from a uniform-randomly selected time point, but from the time step when the safety policy returns 0. A similar adaptation can be done with LOLS [5]. We do not consider these algorithms in this paper and leave them as future work."
    }, {
      "heading" : "4 Experimental Setting",
      "text" : ""
    }, {
      "heading" : "4.1 Simulation Environment",
      "text" : "We use TORCS [1], a racing car simulator, for empirical evaluation in this paper. We chose TORCS based on the following reasons. First, it has been used widely and successfully as a platform for research on autonomous racing [10], although most of the previous work, except for [9, 6], are not comparable as they use a radar instead of a camera for observing the state. Second, TORCS is a light-weight simulator that can be run on an off-the-shelf workstation. Third, as TORCS is an open-source software, it is easy to interface it with another software which is Torch in our case.3\nTracks To simulate a highway driving with multiple lanes, we modify the original TORCS road surface textures by adding various lane configurations such as the number of lanes, the type of lanes.\n3 We will release a patch to TORCS that allows seamless integration between TORCS and Torch.\nWe use ten tracks in total for our experiments. We split those ten tracks into two disjoint sets: seven training tracks and three test tracks. All training examples as well as validation examples are collected from the training tracks only, and a trained primary policy is tested on the test tracks. See Fig. 1 for the visualizations of the tracks and Appendix A for the types of information collected as examples.\nReference Policy π∗ We implement our own reference policy which has access to an underlying state configuration. The state includes the position, heading direction, speed, and distances to others cars. The reference policy either follows the current lane (accelerating up to the speed limit), changes the lane if there is a slower car in the front and a lane to the left or right is available, or brakes."
    }, {
      "heading" : "4.2 Data Collection",
      "text" : "We use a car in TORCS driven by a policy to collect data. For each training track, we add 40 cars driven by the reference policy to simulate traffic. We run up to three iterations in addition to the initial supervised learning stage. In the case of SafeDAgger, we collect 30k, 30k and 10k of training examples (after the subset selection in line 6 of Alg. 1.) In the case of the original DAgger, we collect up to 390k data each iteration and uniform-randomly select 30k, 30k and 10k of training examples.\n4.3 Policy Networks\nPrimary Policy πθ We use a deep convolutional network that has five convolutional layers followed by a set of fully-connected layers. This convolutional network takes as input the pixel-level image taken from a front-facing camera. It predicts the angle of steering wheel ([−1, 1]) and whether to brake ({0, 1}). Furthermore, the network predicts as an auxiliary task the car’s affordances, including the existence of a lane to the left or right of the car and the existence of another car to the left, right or in front of the car. We have found this multi-task approach to easily outperform a single-task network, confirming the promise of multi-task learning from [4].\nSafety Policy πsafe We use a feedforward network to implement a safety policy. The activation of the primary policy network’s last hidden convolutional layer is fed through two fully-connected layers followed by a softmax layer with two categories corresponding to 0 and 1. We choose τ = 0.0025 as our safety policy threshold so that approximately 20% of initial training examples are consid-\nered unsafe, as shown in Fig. 1. See Fig. 6 in the Appendix for some examples of which frames were determined safe or unsafe.\nFor more details, see Appendix B in the Appendix."
    }, {
      "heading" : "4.4 Evaluation",
      "text" : "Training and Driving Strategies We mainly compare three training strategies; (1)Supervised Learning, (2) DAgger (with βi = Ii=0) and (3) SafeDAgger. For each training strategy, we evaluate trained policies with both of the driving strategies; (1) naive strategy and (2) safe strategy.\nEvaluation Metrics We evaluate each combination by letting it drive on the three test tracks up to three laps. All these runs are repeated in two conditions; without traffic and with traffic, while recording three metrics. The first metric is the number of completed laps without going outside a track, averaged over the three tracks. When a car drives out of the track, we immediately halt. Second, we look at a damage accumulated while driving. Damage happens each time the car bumps into another car. Instead of a raw, accumulated damage level, we report the damage per lap. Lastly, we report the mean squared error of steering angle, computed while the primary policy drives.\n5 Results and Analysis\nIn Fig. 2, we present the result in terms of both the average laps and damage per lap. The first thing we notice is that a primary policy trained using supervised learning (the 0-th iteration) alone works perfectly when a safety policy is used together. The safety policy switched to the reference policy for 7.11% and 10.81% of time without and with traffic during test.\nSecond, in terms of both metrics, the primary policy trained with the proposed SafeDAgger makes much faster progress than the original DAgger. After the third iteration, the primary policy trained with the SafeDAgger is perfect. We conjecture that this is due to the effect of automated curriculum learning of the SafeDAgger. Furthermore, the examination of the mean squared difference between the primary policy and the reference policy reveals that the SafeDAgger more rapidly brings the primary policy closer to the reference policy.\nAs a baseline we put the performance of a primary policy trained using purely supervised learning in Fig. 2 (a)–(b). It clearly demonstrates that supervised learning alone cannot train a primary policy well even when an increasing amount of training examples are presented.\nIn Fig. 3, we observe that the portion of time the safety policy switches to the reference policy while driving decreases as the SafeDAgger iteration progresses. We conjecture that this happens as the SafeDAgger encourages the primary policy’s learning to focus on those cases deemed difficult by the safety policy. When the primary policy was trained with the original DAgger (which does not take into account the difficulty of each collected state), the rate of decrease was much smaller. Essentially, using the safety policy and the SafeDAgger together results in a virtuous cycle of less and less queries to the reference policy during both training and test.\nLastly, we conduct one additional run with the SafeDAgger while training a safety policy to predict the deviation of a primary policy from the reference policy one second in advance. We observe a similar trend, which makes the SafeDAgger a realistic algorithm to be deployed in practice."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this paper, we have proposed an extension of DAgger, called SafeDAgger. We first introduced a safety policy which prevents a primary policy from falling into a dangerous state by automatically switching between a reference policy and the primary policy without querying the reference policy. This safety policy is used during data collection stages in the proposed SafeDAgger, which can collect a set of progressively difficult examples while minimizing the number of queries to a reference policy. The extensive experiments on simulated autonomous driving showed that the SafeDAgger not only queries a reference policy less but also trains a primary policy more efficiently.\nImitation learning, in the form of the SafeDAgger, allows a primary policy to learn without any catastrophic experience. The quality of a learned policy is however limited by that of a reference policy. More research in finetuning a policy learned by the SafeDAgger to surpass existing, reference policies, for instance by reinforcement learning [18], needs to be pursued in the future."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank the support by Facebook, Google (Google Faculty Award 2016) and NVidia (GPU Center of Excellence 2015-2016)."
    }, {
      "heading" : "A Dataset and Collection Procedure",
      "text" : "We use TORCS [1] to simulate autonomous driving in this paper. The control frequency for driving the car in simulator is 30 Hz, sufficient enough for driving speed below 50 mph.\nSensory Input We use a front-facing camera mounted on a racing car to collect image frames as the car drives. Each image is scaled and cropped to 160× 72 pixels with three colour channels (R, G and B). In Fig. 4, we show the seven training tracks and three test tracks with one sample image frame per track.\nLabels As the car drives, we collect the following twelve variables per image frame:\n1. Ill ∈ {0, 1}: if there is a lane to the left 2. Ilr ∈ {0, 1}: if there is a lane to the right 3. Icl ∈ {0, 1}: if there is a car in front in the left lane 4. Icm ∈ {0, 1}: if there is a car in front in the same lane 5. Icr ∈ {0, 1}: if there is a car in front in the right lane 6. Dcl ∈ R: distance to the car in front in the left lane 7. Dcm ∈ R: distance to the car in front in the same lane 8. Dcr ∈ R: distance to the car in front in the right lane 9. Pc ∈ [−1, 1]: position of the car within the lane\n10. Ac ∈ [−1, 1]: angle between the direction of the car and the direction of the lane 11. Sc ∈ [−1, 1]: angle of the steering wheel\n12. Ib ∈ {0, 1}: if we brake the car\nThe first ten are state configurations that are observed only by a reference policy but hidden to a primary policy and safety policy. The last two variables are the control variables. All the variables are used as target labels during training, but only the last two (Sc and Ib) are used during test to drive a car."
    }, {
      "heading" : "B Policy Networks and Training",
      "text" : "Primary Policy Network We use a deep convolutional network that has five convolutional layers followed by a group of fully-connected layers. In Table 5, we detail the configuration of the network.\nSafe Frames\n0.987078 0.975920 0.965883 0.960750 0.957986\n0.954337 0.951110 0.948736 0.946532 0.944643\nUnsafe Frames\nSafety Policy Network A feedforward network with two fully-connected hidden layers of rectified linear units is used to implement a safety policy. This safety policy network takes as input the activations of Conv5 of the primary policy network (see Fig. 5.)\nTraining Given a set of training examples, we use stochastic gradient descent (SGD) with a batch size of 64, momentum of 0.9, weight decay of 0.001 and initial learning rate of 0.001 to train a policy network. During training, the learning rate is divided by 5 each time the validation error stops improving. When the validation error increases, we early-stop the training. In most cases, training takes approximately 40 epochs."
    }, {
      "heading" : "C Sample Image Frames",
      "text" : "In Fig. 6, we present twenty sample frames. The top ten frames were considered safe (0) by a trained safety policy, while the bottom ones were considered unsafe (1). It seems that the safety policy at this point determines the safety of a current state observation based on two criteria; (1) the existence of other cars, and (2) entering a sharp curve."
    } ],
    "references" : [ {
      "title" : "Curriculum learning",
      "author" : [ "Y. Bengio", "J. Louradour", "R. Collobert", "J. Weston" ],
      "venue" : "Proceedings of the 26th annual international conference on machine learning, pages 41–48. ACM",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "End to end learning for self-driving cars",
      "author" : [ "M. Bojarski", "D.D. Testa", "D. Dworakowski", "B. Firner", "B. Flepp", "P. Goyal", "L.D. Jackel", "U.M. Mathew Monfort", "J. Zhang", "X. Zhang", "J. Zhao", "K. Zieba" ],
      "venue" : "arXiv preprint arXiv:1604.07316",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Multitask learning",
      "author" : [ "R. Caruana" ],
      "venue" : "Machine learning, 28(1):41–75",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Learning to search better than your teacher",
      "author" : [ "K.-w. Chang", "A. Krishnamurthy", "A. Agarwal", "H. Daume", "J. Langford" ],
      "venue" : "In Proceedings of the 32nd International Conference on Machine Learning",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2015
    }, {
      "title" : "Deepdriving: Learning affordance for direct perception in autonomous driving",
      "author" : [ "C. Chen", "A. Seff", "A. Kornhauser", "J. Xiao" ],
      "venue" : "Proceedings of the IEEE International Conference on Computer Vision, pages 2722–2730",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Search-based structured prediction",
      "author" : [ "H. Daumé Iii", "J. Langford", "D. Marcu" ],
      "venue" : "Machine learning, 75(3):297–325",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Deep sparse rectifier neural networks",
      "author" : [ "X. Glorot", "A. Bordes", "Y. Bengio" ],
      "venue" : "International Conference on Artificial Intelligence and Statistics, pages 315–323",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Evolving large-scale neural networks for vision-based reinforcement learning",
      "author" : [ "J. Koutník", "G. Cuccu", "J. Schmidhuber", "F. Gomez" ],
      "venue" : "Proceedings of the 15th annual conference on Genetic and evolutionary computation, pages 1061–1068. ACM",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "The wcci 2008 simulated car racing competition",
      "author" : [ "D. Loiacono", "J. Togelius", "P.L. Lanzi", "L. Kinnaird-Heether", "S.M. Lucas", "M. Simmerson", "D. Perez", "R.G. Reynolds", "Y. Saez" ],
      "venue" : "CIG, pages 119–126. Citeseer",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Off-road obstacle avoidance through end-to-end learning",
      "author" : [ "U. Muller", "J. Ben", "E. Cosatto", "B. Flepp", "Y.L. Cun" ],
      "venue" : "Advances in neural information processing systems, pages 739–746",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Rectified linear units improve restricted boltzmann machines",
      "author" : [ "V. Nair", "G.E. Hinton" ],
      "venue" : "Proceedings of the 27th International Conference on Machine Learning (ICML-10), pages 807–814",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Alvinn: An autonomous land vehicle in a neural network",
      "author" : [ "D.A. Pomerleau" ],
      "venue" : "Technical report, DTIC Document",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 1989
    }, {
      "title" : "Progress in neural network-based vision for autonomous robot driving",
      "author" : [ "D.A. Pomerleau" ],
      "venue" : "Intelligent Vehicles’ 92 Symposium., Proceedings of the, pages 391–396. IEEE",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "Reinforcement and imitation learning via interactive no-regret learning",
      "author" : [ "S. Ross", "J.A. Bagnell" ],
      "venue" : "arXiv preprint arXiv:1406.5979",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "A reduction of imitation learning and structured prediction to no-regret online learning",
      "author" : [ "S. Ross", "G.J. Gordon", "J.A. Bagnell" ],
      "venue" : "arXiv preprint arXiv:1011.0686",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Learning monocular reactive uav control in cluttered natural environments",
      "author" : [ "S. Ross", "N. Melik-Barkhudarov", "K.S. Shankar", "A. Wendel", "D. Dey", "J.A. Bagnell", "M. Hebert" ],
      "venue" : "Robotics and Automation (ICRA), 2013 IEEE International Conference on, pages 1765–1772. IEEE",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "G",
      "author" : [ "D. Silver", "A. Huang", "C.J. Maddison", "A. Guez", "L. Sifre" ],
      "venue" : "Van Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, et al. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484–489",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Reinforcement learning: An introduction",
      "author" : [ "R.S. Sutton", "A.G. Barto" ],
      "venue" : "volume 28. MIT press",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 1998
    } ],
    "referenceMentions" : [ {
      "referenceID" : 11,
      "context" : "ALVINN by Pomerleau [13] was a neural network with a single hidden layer that takes as input an image frame from a front-facing camera and a response map from a range finder sensor and returns a quantized steering wheel angle.",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 9,
      "context" : "A similar approach was taken later in 2005 to train, this time, a convolutional neural network to drive an off-road mobile robot [11].",
      "startOffset" : 129,
      "endOffset" : 133
    }, {
      "referenceID" : 1,
      "context" : "[3] used a similar, but deeper, convolutional neural network for lane following based solely on a front-facing camera.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 5,
      "context" : ", [7, 16] and references therein.",
      "startOffset" : 2,
      "endOffset" : 9
    }, {
      "referenceID" : 14,
      "context" : ", [7, 16] and references therein.",
      "startOffset" : 2,
      "endOffset" : 9
    }, {
      "referenceID" : 14,
      "context" : "More specifically, we focus on DAgger [16] which works in a setting where the reward is given only implicitly.",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 7,
      "context" : "We empirically evaluate the proposed SafeDAgger using TORCS [1], a racing car simulator, which has been used for vision-based autonomous driving research in recent years [9, 6].",
      "startOffset" : 170,
      "endOffset" : 176
    }, {
      "referenceID" : 4,
      "context" : "We empirically evaluate the proposed SafeDAgger using TORCS [1], a racing car simulator, which has been used for vision-based autonomous driving research in recent years [9, 6].",
      "startOffset" : 170,
      "endOffset" : 176
    }, {
      "referenceID" : 5,
      "context" : ", [7, 16, 5]), we introduce a concept of cost to a policy.",
      "startOffset" : 2,
      "endOffset" : 12
    }, {
      "referenceID" : 14,
      "context" : ", [7, 16, 5]), we introduce a concept of cost to a policy.",
      "startOffset" : 2,
      "endOffset" : 12
    }, {
      "referenceID" : 3,
      "context" : ", [7, 16, 5]), we introduce a concept of cost to a policy.",
      "startOffset" : 2,
      "endOffset" : 12
    }, {
      "referenceID" : 5,
      "context" : "The issue with supervised learning can however be addressed by imitation learning or learning-to-search [7, 16].",
      "startOffset" : 104,
      "endOffset" : 111
    }, {
      "referenceID" : 14,
      "context" : "The issue with supervised learning can however be addressed by imitation learning or learning-to-search [7, 16].",
      "startOffset" : 104,
      "endOffset" : 111
    }, {
      "referenceID" : 14,
      "context" : "DAgger is one such imitation learning algorithm proposed in [16].",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 5,
      "context" : "However, it is certainly possible to incorporate an explicit reward with other imitation learning algorithms, such as SEARN [7], AggreVaTe [15] and LOLS [5].",
      "startOffset" : 124,
      "endOffset" : 127
    }, {
      "referenceID" : 13,
      "context" : "However, it is certainly possible to incorporate an explicit reward with other imitation learning algorithms, such as SEARN [7], AggreVaTe [15] and LOLS [5].",
      "startOffset" : 139,
      "endOffset" : 143
    }, {
      "referenceID" : 3,
      "context" : "However, it is certainly possible to incorporate an explicit reward with other imitation learning algorithms, such as SEARN [7], AggreVaTe [15] and LOLS [5].",
      "startOffset" : 153,
      "endOffset" : 156
    }, {
      "referenceID" : 15,
      "context" : "First, as noted in [17], a human operator cannot drive well without actual feedback, which is the case of DAgger as the primary policy drives most of the time.",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 5,
      "context" : "Unlike previous approaches to imitation learning, often as learning-to-search [7, 16, 5], we introduce an additional policy πsafe, to which we refer as a safety policy.",
      "startOffset" : 78,
      "endOffset" : 88
    }, {
      "referenceID" : 14,
      "context" : "Unlike previous approaches to imitation learning, often as learning-to-search [7, 16, 5], we introduce an additional policy πsafe, to which we refer as a safety policy.",
      "startOffset" : 78,
      "endOffset" : 88
    }, {
      "referenceID" : 3,
      "context" : "Unlike previous approaches to imitation learning, often as learning-to-search [7, 16, 5], we introduce an additional policy πsafe, to which we refer as a safety policy.",
      "startOffset" : 78,
      "endOffset" : 88
    }, {
      "referenceID" : 17,
      "context" : "Relationship to a Value Function A value function V (s) in reinforcement learning computes the reward a given policy π can achieve in the future starting from a given state s [19].",
      "startOffset" : 175,
      "endOffset" : 179
    }, {
      "referenceID" : 15,
      "context" : "This would have been impossible with the original DAgger unless the manually forced take-over measure was implemented [17].",
      "startOffset" : 118,
      "endOffset" : 122
    }, {
      "referenceID" : 12,
      "context" : "2 Such intervention has been done manually by a human driver [14].",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 0,
      "context" : "This has an effect of automated curriculum learning [2] with a mix strategy [20], where the safety policy selects training examples of appropriate difficulty at each iteration.",
      "startOffset" : 52,
      "endOffset" : 55
    }, {
      "referenceID" : 13,
      "context" : "In AggreVaTe [15], for instance, the roll-out by a reference policy may be executed not from a uniform-randomly selected time point, but from the time step when the safety policy returns 0.",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 3,
      "context" : "A similar adaptation can be done with LOLS [5].",
      "startOffset" : 43,
      "endOffset" : 46
    }, {
      "referenceID" : 8,
      "context" : "First, it has been used widely and successfully as a platform for research on autonomous racing [10], although most of the previous work, except for [9, 6], are not comparable as they use a radar instead of a camera for observing the state.",
      "startOffset" : 96,
      "endOffset" : 100
    }, {
      "referenceID" : 7,
      "context" : "First, it has been used widely and successfully as a platform for research on autonomous racing [10], although most of the previous work, except for [9, 6], are not comparable as they use a radar instead of a camera for observing the state.",
      "startOffset" : 149,
      "endOffset" : 155
    }, {
      "referenceID" : 4,
      "context" : "First, it has been used widely and successfully as a platform for research on autonomous racing [10], although most of the previous work, except for [9, 6], are not comparable as they use a radar instead of a camera for observing the state.",
      "startOffset" : 149,
      "endOffset" : 155
    }, {
      "referenceID" : 2,
      "context" : "We have found this multi-task approach to easily outperform a single-task network, confirming the promise of multi-task learning from [4].",
      "startOffset" : 134,
      "endOffset" : 137
    }, {
      "referenceID" : 16,
      "context" : "More research in finetuning a policy learned by the SafeDAgger to surpass existing, reference policies, for instance by reinforcement learning [18], needs to be pursued in the future.",
      "startOffset" : 143,
      "endOffset" : 147
    } ],
    "year" : 2016,
    "abstractText" : "One way to approach end-to-end autonomous driving is to learn a policy function that maps from a sensory input, such as an image frame from a front-facing camera, to a driving action, by imitating an expert driver, or a reference policy. This can be done by supervised learning, where a policy function is tuned to minimize the difference between the predicted and ground-truth actions. A policy function trained in this way however is known to suffer from unexpected behaviours due to the mismatch between the states reachable by the reference policy and trained policy functions. More advanced algorithms for imitation learning, such as DAgger, addresses this issue by iteratively collecting training examples from both reference and trained policies. These algorithms often requires a large number of queries to a reference policy, which is undesirable as the reference policy is often expensive. In this paper, we propose an extension of the DAgger, called SafeDAgger, that is query-efficient and more suitable for end-to-end autonomous driving. We evaluate the proposed SafeDAgger in a car racing simulator and show that it indeed requires less queries to a reference policy. We observe a significant speed up in convergence, which we conjecture to be due to the effect of automated curriculum learning.",
    "creator" : "LaTeX with hyperref package"
  }
}