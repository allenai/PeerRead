{
  "name" : "1708.05603.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Nonnegative Restricted Boltzmann Machines for Parts-based Representations Discovery and Predictive Model Stabilization",
    "authors" : [ "Tu Dinh Nguyen", "Truyen Tran", "Dinh Phung", "Svetha Venkatesh", "Dinh Nguyen" ],
    "emails" : [ "tu.nguyen@deakin.edu.au." ],
    "sections" : [ {
      "heading" : null,
      "text" : "Keywords parts-based representation · nonnegative · restricted Boltzmann machines · learning representation · semantic features · linear predictive model · stability.\nTu Dinh Nguyen, Truyen Tran, Dinh Phung, Svetha Venkatesh Center for Pattern Recognition and Data Analytics, Deakin University, Australia. Corresponding E-mail: tu.nguyen@deakin.edu.au.\nar X\niv :1\n70 8.\n05 60\n3v 1\n[ cs\n.L G\n] 1\n8 A\nug 2"
    }, {
      "heading" : "1 Introduction",
      "text" : "Learning meaningful representations from data is often critical to achieve high performance in machine learning tasks [8]. An attractive approach is to estimate representations that best explain the data without the need for labels. One important class of such methods is the restricted Boltzmann machine (RBM) [47,17], an undirected probabilistic bipartite model in which a representational hidden layer is connected with a visible data layer. The weights associated with connections encode the strength of influence between hidden and visible units. Each unit in the hidden layer acts as a binary feature detector, and together, all the hidden units are linearly combined through the connection weights to form a fully distributed representation of data [24,8]. This distributed representation is highly compact: for K units, there are 2K−1 non-empty configurations that can explain the data.\nHowever, the fully distributed representation learned by RBMs may not interpretably disentangle the factors of variation since the learned features are often global, that is, all the data units must play the role in one particular feature. As a result, learned features do not generally represent parts and components [49]. For example, the facial features learned by RBM depicted in Fig. 1a are generally global; it is hard to explain how a face is constructed from these parts. Parts-based representations, on the other hand, are perceptually intuitive. Using the same facial example, if individual parts of the face (e.g. eyes, nose, mouth, forehead as in Fig. 1c) are discovered, it would be easy to construct the face from these parts. In terms of modeling, detecting partsbased representation also improves the object recognition performance [3].\nOne of the best known techniques to achieve parts-based representation is nonnegative matrix factorization (NMF) [35]. In NMF, the data matrix is approximately factorized into a basis matrix and a coding matrix, where all the matrices are assumed to be nonnegative. Each column of the basis matrix is a learned feature, which could be sparse under appropriate regularization [26]. The NMF, however, has a fundamental drawback: it does not generalize to unseen data since there is no mechanism by which a new data point can be generated from the learned model. Instead, new representations must be learned from the expensive “fold-in” procedure. The RBM, on the other hand, is a proper generative model – once the model has been learned, new samples can be drawn from the model distribution. Moreover, due to the special bipartite structure, estimating representation from data is efficient with a single matrix operation.\nIn this paper, we derive a novel method based on the RBM to discover useful parts whilst retaining its discriminative capacity. As inspired by the NMF, we propose to enforce nonnegativity in the connection weight matrix of the RBM. Our method integrates a barrier function into the objective function so that the learning is skewed towards nonnegative weights. As the contribution of the visible units towards a hidden unit is additive, there exists competition among visible units to activate the hidden unit leading to a small portion of connections surviving. In the same facial example, the method could achieve\nparts-based representation of faces, which is, surprisingly, even better than what learned by the standard NMF (cf. Fig. 1). We term the resulting model the nonnegative restricted Boltzmann machine (NRBM).\nIn addition to parts-based representation, there are several benefits with this nonnegativity constraint. First, in many cases, it is often easier to make sense of addition of new latent factors (due to nonnegative weights) than of subtraction (due to negative weights). For instance, clinicians may be more comfortable with the notion that a risk factor either contributes positively to a disease development or not at all (e.g., the connections have zeros weights). Second, as weights can be either positive or zero, the parameter space is highly constrained leading to potential robustness. This can be helpful when there are many more hidden units than those required to represent all factors of variation: extra hidden units will automatically be declared “dead” if all connections to them cannot compete against others in explaining the data. Lastly, combining two previous advantages, the NRBM effectively gathers related important features, e.g., key risk factors that explain the disease, into groups at the data layer and encourages less hidden units to be useful at hidden layer. This helps to stabilize linear predictive models by providing hidden representations for them to perform on.\nWe demonstrate the effectiveness of the proposed model through comprehensive evaluation on three real-world applications using five real datasets of very different natures.\n– The first application is parts-based discovery. Our primary targets are to decompose images into interpretable parts (and receptive fields), e.g., dots and strokes in handwritten digits (MNIST dataset [34]), and facial components in faces (CBCL [13] and ORL [4] databases); and to discover plausible\nlatent thematic features, which are groups of semantically related words (TDT2 text corpus [12]). – The second application is feature extraction for classification. Here we apply our models on MNIST, TDT2 and heart failure datasets. The learned features are then fed into standard classifiers. The experiments reveal that the classification performance is comparable with the standard RBM, and competitive against NMF (on all images, text, and medical records) and latent Dirichlet allocation (on text) [11]. – The last one is linear classifier stabilization. The goal is to enhance feature and model stabilities in clinical prognosis. The experimental results on heart failure patients show significant improvements of stability scores over linear sparse model and the RBM, and better prediction performances than those of RBM and NMF. This application in healthcare analytics is the main extension of our previous model introduced in [40].\nIn short, our main contributions are: (i) the derivation of the nonnegative restricted Boltzmann machine, a probabilistic machinery that has the capacity of learning parts-based representations; (ii) a comprehensive evaluation the capability of our method as a representational learning tool on image, text and medical data, both qualitatively and quantitatively; and (iii) a demonstration of stabilizing linear classifiers in clinical prognosis.\nThe rest of the paper is structured as follows. Section 3 presents the derivation and properties of our nonnegative RBM, followed by its applications in linear predictive model stabilization in Section 4. We then report our experimental results in Section 5. Section 6 provides a discussion of the related literature. Finally, Section 7 concludes the paper."
    }, {
      "heading" : "2 Preliminaries",
      "text" : "We first describe the restricted Boltzmann machine (RBM) for unsupervised learning representation. An RBM [47,17,22] is a bipartite undirected graphical model in which the bottom layer contains observed variables called visible units and the top layer consists of latent representational variables, known as hidden units. Two layers are fully connected but there is no connection within layers. The visible units can model the data while the hidden units can capture the latent factors not presented in the observations. The hidden units are linearly combined through connection weights. A graphical illustration of RBM is presented in Fig. 2. As a matter of convention in the literature of RBM, we shall use the term “unit” and “random variable” interchangeably.\n2.1 Model representation\nLet v denote the set of visible variables: v = [v1, v2, ..., vN] > ∈ {0, 1}N and h indicate the set of hidden ones: h = [h1,h2, ...,hK] > ∈ {0, 1}K. The RBM\ndefines an energy function of a joint configuration (v,h) as: E (v,h;ψ) = − ( a>v + b>h + v>Wh ) (1)\nwhere ψ = {a,b,W} is the set of parameters. a = [an]N ∈ RN,b = [bk]K ∈ RK are the biases of hidden and visible units respectively, and W = [wnk]N×K ∈ RN×K represents the weights connecting the hidden and visible units. The model assigns a Boltzmann distribution (also known as Gibbs distribution) to the joint configuration as:\np (v,h;ψ) = 1\nZ (ψ) e−E(v,h;ψ) (2)\nwhere Z (ψ) is the normalization constant, computed by summing over all possible states pairs of visible and hidden units:\nZ (ψ) = ∑ v,h e−E(v,h;ψ)\nSince the network has no intra-layer connections, the Markov blanket of each unit contains only the units of the other layer. Units in one layer become conditionally independent given the other layer. Thus the conditional distributions over visible and hidden units are nicely factorized as:\np (v | h;ψ) = N∏ n=1 p (vn | h;ψ) (3)\np (h | v;ψ) = K∏ k=1 p (hk | v;ψ) (4)\nwherein the conditional probabilities of single units being active are:\np (vn = 1 | h;ψ) = sig ( an + wn·h ) p (hk = 1 | v;ψ) = sig ( bk + v >w·k )\n(5)\nwith the logistic sigmoid function sig (x) = [1 + e−x] −1 .\n2.2 Parameter learning\nThe parameter learning of RBM is performed by maximizing the following data log-likelihood:\nL (v;ψ) = log p (v;ψ) = log ∑ h p (v,h;ψ)\nThe parameters are updated using stochastic gradient ascent as follows:\nwnk ← wnk + η (Ep̃ [vnhk]− Ep [vnhk])\nwherein η > 0 is the learning rate, p̃ (v,h;ψ) = p̃ (v;ψ) p (h | v;ψ) is the data distribution with p̃ (v;ψ) representing the empirical distribution, p (v,h;ψ) is the model distribution defined in Eq. (2). Whilst the data expectation Ep̃ [·] can be computed efficiently, the model expectation Ep [·] is intractable. To overcome this shortcoming, we use a truncated MCMC-based method known as contrastive divergence (CD) [22] to approximate the model expectation. This approximation approach is efficient since the factorizations in Eqs. (3,4) allow fast layer-wise sampling.\n2.3 Representation learning\nOnce the model is fully specified, the new representation of an input data can be achieved by computing the posterior vector ĥ = [ ĥ1, ĥ2, ..., ĥK ] , where ĥK is shorthand for p (hk = 1 | v;ψ) as given in Eq. (5)."
    }, {
      "heading" : "3 Nonnegative restricted Boltzmann machine",
      "text" : "We now present our main contribution – the nonnegative restricted Boltzmann machine (NRBM) that integrates nonnegativity into the connection weights of the model. We then present the capability of NRBM to estimate the intrinsic dimensionality of the data, and to stabilize linear predictive models.\n3.1 Deriving parts-based representation\nThe derivation of parts-based representation starts from the connection weights of the standard RBM. In the RBM, two layers are connected using a weight matrix W = [wnk]N×K in which wnk is the association strength between the hidden unit k and the visible unit n. The column vector w·k is the learned filter of the hidden unit k. Parts-based representations imply that this column vector must be sparse, e.g. only a small portion of entries is non-zeros. Recall that the activation of this hidden unit, also known as the firing rate in neural net-\nwork language, is the probability: p (hk = 1 | v;ψ) = σ ( bk + ∑N n=1 wnkvn ) . The positive connection weights tend to activate the associated hidden units whilst the negative turn the units off. In addition, the positive weights add up to representations whereas the negative subtract. Thus it is hard to determine which factors primarily contribute to the learned filter.\nDue to asymmetric parameter initialization, the learning process tends to increase some associations more than others. Under nonnegative weights, i.e. wnk ≥ 0, the hidden and visible units tend to be more co-active. One can expect that for a given activation probability ĥk and bias bk, such increase must cause other associations to degrade, since vn ≥ 0. As the lower bounds of weights are now zeros, there is a natural tendency for many weights to be driven to zeros as the learning progresses. For an illustration, Fig. 3 shows the histograms of model’s weights at initialization (epoch #0) and learned at epoch #1 and #2. It can be seen that, the number of weights close to zeros increases after each learning epoch.\nRecall that learning in the standard RBM is usually based on maximizing the data likelihood with respect to the parameter ψ as:\nL (v;ψ) = p (v;ψ) = ∑ h p (v,h;ψ) (6)\nTo encourage nonnegativity in W, we integrate a nonnegative constraint under quadratic barrier function [41] into the learning of the standard RBM. The objective function now turns into the following regularized log-likelihood:\nLreg = logL (v;ψ)− α\n2 N∑ n=1 K∑ k=1 f (wnk) (7)\nwhere:\nf (x) =\n{ x2 x < 0\n0 x ≥ 0\nand α ≥ 0. The penalty of sum squared of negative weights regularizes the optimization to encourage nonnegativity in W. The learning procedure now performs minimizing the penalty term and maximizing the data log-likelihood logL (ψ;v) simultaneously to achieve the optimal solution for the regularized function. The degree of nonnegativity is controlled by the hyperparameter α. If we set α to zero, the barrier function will vanish and the model will revert to the ordinary RBM without nonnegative constraint. The larger α is, the tighter the barrier restriction becomes.\nFinally the parameter update rule reads: wnk ← wnk + η ( 〈vnhk〉p̃ − 〈vnhk〉p − α dwnke − )\n(8)\nwhere η > 0 is the learning rate, p̃ (v,h;ψ) = p̃ (v;ψ) p (h | v;ψ) is the data distribution with p̃ (v;ψ) representing the empirical distribution, p (v,h;ψ) is the model distribution defined in Eq. (2) and dwnke− denotes the negative part of the weight. Following the learning of standard RBMs [22], p̃ (v;ψ) is computed from data observations and p (v,h;ψ) can be efficiently approximated by running multiple Markov short chains starting from observed data at each update.\n3.2 Estimating intrinsic dimensionality\nThe fundamental question of the RBM is how many hidden units are enough to model the data. Currently, no easy methods determine the appropriate number of hidden units needed for a particular problem. One often uses a plentiful number of units, leading to some units are redundant. Note that the unused hidden units problem of RBM is not uncommon which has been studied in [9]. If the number of hidden units is lower than the number of data features, the RBM plays a role of a dimensionality reduction tool. This suggests\none prominent method – principal component analysis (PCA) which captures the data variance using the principal components. The amount of variance, however, can help the method specify the number of necessary components. The nonnegativity constraint in the NRBM, interestingly, leads to a similar capacity by examining the “dead” hidden units.\nTo see how, recall that the hidden and visible units are co-active via the connection weights:\np (vn = 1 | h) = σ ( an +\nK∑ k=1 wnkhk\n) (9)\nSince this probability, in general, is constrained by the data variations, the hidden units must “compete” with each other to explain the data. This is because the contribution towards the explanation is nonnegative, thus an increase on the power of one unit must be at the cost of others. If K∗ < K hidden units are intrinsically enough to account for all variations in the data, then one can expect that either the other K−K∗ hidden units are always deactivated (e.g. with very large negative biases) or their connection weights are almost zeros since wnk ≥ 0. In either cases, the hidden units become permanently inoperative in data generation. Thus by examining the dead units, we may be able to uncover the intrinsic dimensionality of the data variations."
    }, {
      "heading" : "4 Stabilizing Linear Predictive Models",
      "text" : "This section presents an application of NRBM in stabilizing linear predictive models. Model stability is often overlooked in prediction models which pay more attention to predictive performances. However, model stability is as important as the prognosis in domains where model parameters are interpreted by humans and are subject to external validation. Particularly, in the medical domain, the model stability increases reliability, interpretability, generalization (or transferability) and reproducibility [6,31]. For example, it is desirable to have a reliable model that can discover a stable set of risk factors to interpret the causes of the disease. The generalization of the method is the capability to transfer knowledge from one disease to another. This also helps to improve clinical adoption. Finally, stability enables researchers to reproduce the results easily.\nA popular modern method to learn predictive risk factors from a highdimensional data is to employ `1-penalty, following the success of lasso in linear regression. Lasso is a sparse linear predictive model which promotes feature shrinkage and selection [50]. More formally, let X = [xmn]M×N ∈ RM×N denote the data matrix consisting of M data points with N features and y = [y1, y2, ..., yM] > ∈ {0, 1}M denote the labels. Consider a predictive distribution:\np (y | x;w) = p ( y |\nN∑ n=1 wnxn\n) (10)\nLasso-like learning optimizes a `1-penalized log-likelihood:\nLlasso = 1\nM M∑ m=1 log p ( ym | xm·;w ) − β ‖w‖1 (11)\nin which w = [wn]N×1 is the parameter vector and β > 0 is the regularization hyperparameter. The regularization induces the sparsity of weight vector w.\nThe lasso, however, is susceptible to data variations (e.g. resampling by bootstrapping, slight perturbation), resulting in loss of stability [5,55]. The method often chooses one of two highly correlated features, resulting in only a 0.5 chance for strongly predictive feature pairs. To overcome this problem, our solution is to provide the lasso with lower-dimensional data representations whose features are expected to be less correlated. Here we introduce a twostage framework which is a pipeline with the NRBM followed by the lasso. The first stage is to learn the connection weights W = [wnk]N×K of NRBM. Then the machine can map the data onto new representations, i.e. the hidden posteriors: X = [xmn]M×N 7→ X̂ = [x̂mk]M×K (cf. Eq. (5)). The representations are in K-dimensional space and used as the new data for the lasso. At the second stage, the shrinkage method learns K weights for K features (i.e. hidden units) to predict the label. Suppose we obtain weight vector ŵ = [ŵ1, ŵ2, ..., ŵK] >\n, we have:\np (y | x̂; ŵ) = p ( y |\nK∑ k=1 ŵkx̂k\n)\n= p ( y |\nK∑ k=1 ŵk N∑ n=1 xnwnk\n)\n= p ( y |\nN∑ n=1 xn K∑ k=1 ŵkwnk\n)\nIt can be seen that we can sum over the multiplications of the weights of two methods to obtain a new weight vector w̄ = [w̄1, w̄2, ..., w̄N] > as below:\nw̄n = K∑ k=1 ŵkwnk (12)\nThese weights connect original features to the label as the weights of lasso in Eq. (10). Thus they can be used to assess the stability of our proposed framework."
    }, {
      "heading" : "5 Experiments",
      "text" : "In this section, we quantitatively evaluate the capacity of our proposed model – nonnegative RBM on three applications:\n– Parts-based discovery : unsupervised decomposing images into parts, discovering semantic features from texts, and grouping clinically relevant features; – Feature extraction for classification: discovering discriminative features that help supervised classification; and – Linear predictive model stabilization: stabilizing feature selection towards rehospitalization prognosis.\nWe use five real-world datasets in total: three for images, one for text and one for medical data. Three popular image datasets are: one for handwritten digits – MNIST [34] and two for faces – CBCL [13] and ORL [4]. For these datasets, our primary target is to decompose images into interpretable parts (and receptive fields), e.g. dots and strokes in handwritten digits, and facial components in faces. The text corpus is 30 categories subset of the TDT2 corpus1. The goal is to discover plausible latent thematic features, which are groups of semantically related words. Lastly, the medical data is a collection of heart failure patients provided by Barwon Health, a regional health service provider in Victoria, Australia. We aim to investigate the feature stability during the rehospitalization prediction of patients. For prognosis, we use logistic regression model, i.e, p (y | x;w) in Eq. (10) is now the probability mass function of a Bernoulli distribution, for 6-month readmission after heart failure.\nImage datasets The MNIST dataset consists of 60, 000 training and 10, 000 testing 28×28 images, each of which contains a handwritten digit. The CBCL database contains facial and non-facial images wherein our interest is only on 2, 429 facial images in the training set. The images are histogram equalized, cropped and rescaled to a standard form of 19 × 19. Moreover, the human face in each image is also well-aligned. By contrast, the facial images of an individual subject in ORL dataset are captured under a variety of illumination, facial expressions (e.g. opened or closed eyes, smiling or not) and details (e.g. glasses, beard). There are 10 different images for each of 40 distinct people. Totally, the data consists of 400 images with the same size of 92× 112 pixels. Images of the three datasets are all in the grayscale whose pixel values are then normalized into the range [0, 1]. Since the image pixels are not exactly binary data, following the previous work [21], we treat the normalized intensity as empirical probabilities on which the NRBM is naturally applied. As the empirical expectation 〈vnhk〉p̃ in Eq. (8) requires the probability p (v;ψ), the normalized intensity is a good approximation.\nText dataset The TDT2 corpus is collected during the first half of 1998 from six news sources: two newswires (APW, NYT), two radio programs (VOA, PRI), and two television programs (CNN, ABC). It contains 11, 201 on-topic documents arranged into 96 semantic categories. Following the preprocessing in [12], we remove all multiple category documents and keep the largest 30 categories. This retains 9, 394 documents and 36, 771 unique words in total.\n1 NIST Topic Detection and Tracking corpus is at http://www.nist.gov/itl/.\nWe further the preprocessing of data by removing common stopwords. Only 1, 000 most frequent words are then kept and one blank document are removed. For NRBM, word presence is used rather than their counts.\nHeart failure data The data is collected from the Barwon Health which has been serving more than 350, 000 residents. For each time of hospitalization, patient information is recorded into a database using the MySQL server of the hospital. Each record contains patient admissions and emergency department (ED) attendances which form an electronic medical record (EMR). Generally, each EMR consists of demographic information (e.g. age, gender and postcode) and time-stamped events (e.g. hospitalizations, ED visits, clinical tests, diagnoses, pathologies, medications and treatments). Specifically, it includes international classification of disease 10 (ICD-10) scheme [28], Australian invention coding (ACHI) scheme [2], diagnosis-related group (DRG) codes, detailed procedures and discharge medications for each admission and ED visit. Ethics approval was obtained from the hospital and research ethics committee at Barwon Health (number 12/83) and Deakin University.\nFor our study, we collect the retrospective data of heart failure patients from the hospital’s database. The resulting cohort contains 1, 405 unique patients with 1, 885 admissions between January 2007 and December 2011. We identify patients as heart failure if they had at least one ICD-10 diagnosis code I50 at any admission. Patients of all age groups are included whilst inpatient deaths are excluded from our cohort. Among these patients, 49.3% are male and the medium age is 81.5 at the time of admission. We focus our study on emergency attendances and unplanned admissions of patients. The readmission of patients is defined as an admission within the horizons of 1, 6 and 12 months after the prior discharge date. After retrieving the data, we follow the one-sided convolutional filter bank method introduced in [52] to extract features which are then normalized into the range [0, 1].\nTo speed up the training phase, we divide training samples into “minibatches” of B = 100 samples. Hidden, visible and visible-hidden learning rates are fixed to 0.1. Visible biases are initialized so that the marginal distribution, when there are no hidden units, matches the empirical distribution. Hidden biases are first set to some reasonable negative values to offset the positive activating contribution from the visible units. Mapping parameters are randomly drawn from positive values in [0, 0.01]. Parameters are then updated after every mini-batch. Learning is terminated after 100 epochs. The regularization hyperparameter α in Eq. (7) is empirically tuned so that the data decomposition is both meaningful (e.g. by examining visually, or by computing the parts similarity) and accurate (e.g. by examining the reconstruction quality). The hyperparameter β in Eq. (11) is set to 0.001, which is to achieve the best prediction score.\n5.1 Part-based discovery"
    }, {
      "heading" : "5.1.1 Decomposing images into parts-based representations",
      "text" : "We now show that the nonnegative constraints enable the NRBM to produce meaningful parts-based receptive fields. Fig. 4 depicts the 100 filters learned from the MNIST images. It can be seen that basic structures of handwritten digits such as strokes and dots are discovered by both RBM and NRBM. However, the features that NRBM learns on Fig. 4b are simpler whilst the ones learned by RBM on Fig. 4a are more difficult to interpret.\nFor the CBCL dataset, the facial parts (eyes, mouth, nose, eyebrows etc.) uncovered by NRBM (Fig. 5c) are visually interpretable along the line with classical NMF [35] (Fig. 5b). The RBM, on the other hand, produces global facial structures (Fig. 5a). On the more challenging facial set with higher variation such as the ORL (cf. Sec. 1), NMF fails to produce parts-based representation (Fig. 1b), and this is consistent with previous findings [26]. In contrast, the NRBM is still able to learn facial components (Fig. 1c).\nThe capacity to decompose data in NRBM is controlled by a single hyperparameter α. As shown in Fig. 6, there is a smooth transition from the holistic decomposition as in standard RBM (when α is near zero) to truly parts-based representations (when α is larger)."
    }, {
      "heading" : "5.1.2 Dead factors and dimensionality estimation",
      "text" : "We now examine the ability of NRBM to estimate the intrinsic dimensionality of the data, as discussed in Section 3.2. We note that by “dimensionality” we roughly mean the degree of variations, not strictly the dimension of the data\nmanifold. This is because our latent factors are discrete binary variables, and thus they may be less flexible than real-valued coefficients.\nFor that purpose, we compute the number of dead or unused hidden units. The hidden unit k is declared “dead” if the normalized `1-norm of its connection weight vector is lower than a threshold τ : ∣∣w·k∣∣1 N−1 ≤ τ , where N is the dimension of the original data. We also examine the hidden biases which, however, do not cause dead units in this case. In Fig. 7, the number of used hidden units is plotted against the total number of hidden units K by taking the average over a set of thresholds (τ ∈ {0.01; 0.02; ...; 0.06}). With the NRBM, the number of hidden units which explicitly represents the data saturates at about 150 whilst all units are used by the RBM."
    }, {
      "heading" : "5.1.3 Semantic features discovering on text data",
      "text" : "The next experiment investigates the applicability of the NRBM on decomposing text into meaningful “parts”, although this notion does not convey the same meaning as those in vision. This is because the nature of text may be more complex and high-level, and it is hard to know whether the true nature of word usage is additive. Following literature in topic modeling (e.g., cf. [11]), we start from the assumption that there are latent themes that govern the choice of words in a particular document. Our goal is to examine whether we can uncover such themes for each document, and whether the themes are corresponding to semantically coherent subset of words.\nUsing the TDT2 corpus, we learn the NRBM from the data and examine the mapping weight matrix W. For each latent factor k, the entry to column w·k reflects the association strength of a particular word with the factor, where zero entry means distant relation. Table 1 presents four noticeable semantic features discovered by our model. The top row lists the top 15 words per feature, and the bottom row plots the distribution of association strengths in decreasing order. It appears that the words under each feature are semantically related in a coherent way.\n5.2 Feature extraction for classification\nOur next target is to evaluate whether the ability to decompose data into parts and to disentangle factors of variation could straightforwardly translate into better predictive performance. Although the NRBM can be easily turned into a nonnegative neural network and the weights are tuned further to best fit the supervised setting (e.g., cf. [21]), we choose not to do so because our goal is to\nsee if the decomposition separates data well enough. Instead we apply standard classifiers on the learned features, or more precisely the hidden posteriors.\nThe first experiment is with the MNIST, the 500 factors have been previously learned in Section 5.1.1 and Fig. 4. Support vector machines (SVM, with Gaussian kernels, using the LIBSVM package [14]) and k-nearest neighbors (kNN, where k = 4, with cosine similarity measures) are used as classifiers. For comparison, we also apply the same setting to the features discovered by the NMF. The error rate on test data is reported in Table 2. It can be seen that (i) compared to standard RBM, the nonnegativity constraint used in NRBM does not lead to a degradation of predictive performance, suggesting that the parts are also indicative of classes; and (ii) nonlinear decomposition in NRBM can lead to better data separation than the linear counterpart in NMF.\nThe second experiment is on the text data TDT2. Unlike images, words are already conceptual and thus using standard bag-of-words representation is often sufficient for many classification tasks. The question is therefore whether the thematic features discovered by the NRBM could further improve the performance, since it has been a difficult task for topic models such as LDA (e.g., cf. experimental results reported in [11]). To get a sense of the capability to separate data into classes without the class labels, we project the 100 hidden posteriors onto 2D using t-SNE2 [54]. Fig. 8a depicts the distribution of documents, where class information is only used for visual labeling. The separation is overall satisfactory.\nFor the quantitative evaluation, the next step is to run classifiers on learned features. For comparison, we also use those discovered by NMF and LDA. For all models, 100 latent factors are used, and thus the dimensions are reduced 10- fold. We split TDT2 text corpus into 80% for training and 20% for testing. We train linear SVMs3 on all word features and low-dimensional representations provided by LDA, NMF and NRBM with various proportions of training data. Fig. 8b shows the classification errors on testing data for all methods. The learned features of LDA and NMF improve classification performance when training label is limited. This is expected because the learned representations are more compact and thus less prone to overfitting. However, as more labels are available, the word features catch up and eventually outperform those by LDA/NMF. Interestingly, this difficulty does not occur for the features learned by the NRBM, although it does appear that the performance saturates after seeing 20% of training labels. Note that this is significant given the fact that computing learned representations in NRBM is very fast, requiring only a single matrix-vector multiplication per document.\n5.3 Stabilizing linear predictive models\nIn this experiment, we evaluate the stability of feature selection of our framework introduced in Section 4. More specifically, we assess the discovered risk factors of heart failure patients by predicting the future presence of their readmission (i.e., ym in Eq. (10)) at a certain assessment point given their history. It is noteworthy that this is more challenging as the EMR data contain rich information, are largely temporal, often noisy, irregular, mixed-type, mixedmodality and high-dimensional [38]. In what follows, we present the experiment setting, evaluation protocol and results."
    }, {
      "heading" : "5.3.1 Temporal validation",
      "text" : "We derive the cohort into training and testing data to validate the predictive performance of our proposed model. Two issues that must be addressed during\n2 Note that the t-SNE does not do clustering, it only reduces the dimensionality into 2D for visualization while still try to preserve the local properties of the data.\n3 SVM with Gaussian kernels did not perform well.\nthe splitting process are: learning the past and predicting the future; ensuring training and testing sets completely separated. Here we use a temporal checkpoint to divide the data into two parts. More specifically, we gather admissions which have discharge dates before September 2010 to form the training set and after that for testing. Next we specify the set of unique patients in the training set. We then remove all admissions of such patients in the testing set to guarantee no overlap between two sets. Finally, we obtain 1, 088 unique patients with 1, 415 admissions in the training data and 317 patients with 360 admissions in the testing data. The removing steps and resulting datasets are illustrated in Fig. 9. Our model is then learned using training data and evaluated on testing data."
    }, {
      "heading" : "5.3.2 Evaluation protocol",
      "text" : "We use Jaccard index [45] and consistency index [32] to measure the stability of feature selection process. The Jaccard index, also known as the Jaccard similarity coefficient, naturally considers both similarity and diversity to measure how two feature sets are related. The consistency index supports feature selection in obtaining several desirable properties, i.e. monotonicity, limits and correction for chance.\nWe trained our proposed model by running M = 10 bootstraps and obtained a list of feature sets S = {S1,S2, ...,SM} where Si is a subset of original feature set v. Note that the cardinalities are: |Si| = T and |v| = K with the condition: T ≤ K. Considering a pair of subsets Si and Sj , the pairwise\nconsistency index C (Si,Sj) is defined as:\nC (Si,Sj) = RK− T2\nT(K− T)\nin which |Si ∩ Sj | = R. Taking the average of all pairs, the overall consistency index is:\nC = 2\nM(M− 1) M−1∑ i=1 M∑ j=i+1 C(Si,Sj) (13)\nWhilst most similarity indices prefer large subsets, C provides consistency around zero for any number of features T [32]. The consistency index is bounded in [−1,+1].\nJaccard index measures similarity as a fraction between cardinalities of intersection and union feature subsets. Given two feature sets Si and Sj , the pairwise Jaccard index J (Si,Sj) reads:\nJ(Si,Sj) = |Si ∩ Sj | |Si ∪ Sj |\nThe Jaccard index evaluating all M subsets was computed as follows:\nJ = 2\nM(M− 1) M−1∑ i=1 M∑ j=i+1 J(Si,Sj) (14)\nJaccard index is bounded in [0, 1]. For prediction, we average the weights of 10 models learned after bootstrapping to obtain the final model. Then the final model performs prediction on testing data. The threshold 0.5 is used to decide the predicted outcomes from\npredicted probabilities. Finally the performances are evaluated using measures of sensitivity (recall), specificity, precision, F-measure and area under the ROC curve (AUC) score with confidence intervals based on Mann-Whitney statistic [10]."
    }, {
      "heading" : "5.3.3 Results",
      "text" : "Section 6 shows that the basis matrix of NMF plays the same role as NRBM’s connection weights. Using the same derivation of NRBM in Section 4, it is straightforward to obtain the weight vector w̄ in Eq. (12) for the NMF. Thus we can compare the stability results of our proposed model against those of NMF. In total, we recruit three baselines: lasso, RBM and NMF. The numbers of hidden units, latent factors of RBM, NRBM and NMF are 200.\nTable. 3 reports the prediction performances and Fig. 10 illustrates the stability results for different subset sizes. Overall, the NRBM followed by lasso achieve better AUC score than the RBM and NMF followed by lasso and worse yet acceptable (1.8% lower) than the sole lasso. However, the NRBM followed by the lasso outperform the sole lasso and RBM+Lasso with large margins of consistency and Jaccard indices. The worst stabilities of RBM+Lasso are expected because the standard RBM is not designed with properties that promote a steady group of features. Comparing with the NMF followed by lasso, the NRBM performs worse at first but then catches up when the subset size reaches about 150 and slightly better after that.\n6 Discussion\n6.1 Representation learning\nRepresentation learning has recently become a distinct area in machine learning after a long time being underneath the subdomain of “feature learning”. This is evidenced through the recent appearance of the international conference on learning representation (ICLR4), following a series of workshops in NIPS/ICML [8]. Existing approaches of learning representation can be broadly classified into three categories: learning kernels, designing hand-crafted features, and learning high-level representations.\n4 http://www.iclr.cc/doku.php\nIn kernel learning approach, the algorithms may use fixed generic kernels such as Gaussian kernel, design kernels based on domain knowledge, or learn combined kernels from multiple base kernels [46]. These methods often use “kernel tricks” to substitute the kernel functions for the mapping functions that transform data into their new representations on the feature space. Thus the kernel machines do not define explicit mapping functions, and as a result do not directly produce representations for data.\nThe second approach is to directly use either raw data or features extracted from them. These features are extracted from feature selection frameworks with or without domain knowledge. The original data and extracted features can be preprocessed by scaling or normalizing, but not projecting onto other spaces [37,15,42,7]. Normally, the feature selection frameworks are designed by hand. Hand-crafted feature extraction relies on the design of preprocessing pipelines and data transformations. This approach, however, suffers from two significant drawbacks. First, it is labor intensive and normally requires exhaustive prior knowledge. Thus only domain experts can design good features. Second, the feature engineering process may create more data types, especially for complex data, leading to more challenges for fundamental machine learning methods.\nThe last approach is to automatically learn parametric maps that transform input data into high-level representations. The models that learn such representations typically fall into two classes: one is non-probabilistic models, the other is probabilistic latent variable models. The common aim of the methods in the first class is to learn direct encoding, i.e., parametric maps from input data to their new representations. The classic linear techniques are principal component analysis (PCA) [29], latent semantic indexing (LSI) [16], and independent component analysis (ICA) [27]. These methods support\ncapturing data regularity, discovering latent structures and latent semantics of data. They, however, face difficulty in modeling data which follow a set of probabilistic distributions. This drawback can be overcome by the second class of probabilistic models.\nProbabilistic latent variable models often consist of observed variables x representing data and latent variables h which coherently reflect the distributions, regularities, structures and correlations of data. The common idea is to define a joint probability p (x,h) over the joint space of observed and latent variables. The learning is to maximize the likelihood of training data. Once the model has been learned, the latent representation of the data is obtained by inferring the posterior probability p (h | x). Most methods in this part are probabilistic versions of the ones in the first class including probabilistic PCA [51], probabilistic LSI [25], and Bayesian nonparametric factor analysis [43]. As a fully generative two-layer model, our proposed model can also be categorized into this class.\n6.2 Nonnegative data modeling\nOur work was partly motivated by the capacity of nonnegative matrix factorization (NMF) [35] to uncover parts-based representations. Given a nonnegative data matrix V ∈ RN×M, the NMF attempts to factorize into two low-rank real-valued nonnegative matrices, the basis W ∈ RN×K and the coefficient H ∈ RK×M, i.e., V ≈WH. Thus W plays the same role as NRBM’s connection weights, and each column of H assumes the “latent factors”. However, it has been pointed out that unless there are appropriate sparseness constraints or certain conditions, the NMF is not guaranteed to produce parts [26]. Our experiment shows, on the contrary, the NRBM can still produce parts-based representation when the NMF fails (Fig. 1, also reported in [26]).\nOn the theoretical side, the main difference is that, in our cases, the latent factors are stochastic binary that are inferred from the model, but not learned as in the case of NMF. In fact this seemingly subtle difference is linked to a fundamental drawback of the NMF: The learned latent factors are limited to seen data only, and must be relearned for every new data point. The NRBM, on the other hand, is a fully generative model in that it can generate new samples from its distribution, and at the same time, the representations can be efficiently computed for unseen data (cf. Eq. (5)) using one matrix operation.\nRecently, there has been work closely related to the NMF that does not require re-estimation on unseen data [36]. In particular, the coefficient matrix H is replaced by the mapping from the data itself, that is H = σ ( W>V ) , re-\nsulting in the so-called autoencoder structure (AE), that is V ≈Wσ ( W>V ) , where σ(x) is a vector of element-wise nonlinear transforms and W is nonnegative. A new representation estimated using σ ( W>V ) now plays the role of the posteriors in NRBM, although it is non-probabilistic. The main difference from the NRBM is that the nonnegative AE does not model data distribution, and thus cannot generate new samples. Also, it is still unclear how the new\nrepresentation could be useful for classification in general and on non-vision data in particular.\n6.3 Feature discovery\nFor the semantic analysis of text, our proposed model is able to discover plausible thematic features. Compared against those discovered by topic models such as latent Dirichlet allocation (LDA) [11], we found that they are qualitatively similar. We note that the two approaches are not directly comparable because the notion of association strength between a latent factor and a word, as captured in the nonnegative weight wnk, cannot be translated into the properly normalized probability P (vn = 1 | zn = k) as in LDA, where zn is the topic that generates the word vn. Nevertheless, the NRBM offers many advantages over the LDA: (i) the notion that each document is generated from a subset of themes (or semantic features) in the NRBM is an attractive alternative to the setting of topic distribution as assumed in the LDA (cf. also [19]); (ii) inference to compute the latent representation given an input is much faster in the NRBM with only one matrix multiplication step, which typically requires an expensive sampling run in the LDA; (iii) learning in the NRBM can be made naturally incremental, whilst estimating parameter posteriors in the LDA generally requires the whole training data; and (iv) importantly, as shown in our experiments, classification using the learned representations can be more accurate with the NRBM.\nThis work can be considered along the line of imposing structures on standard RBM so that certain regularities are explicitly modeled. Our work has focused on nonnegativity as a way to ensure sparseness on the weight matrix, and consequently the latent factors. An alternative would be enforcing sparseness on the latent posteriors, e.g., [23]. Another important aspect is that the proposed NRBM offers a way to capture the so-called “explaining away” effect, that is the latent factors compete with each other as the most plausible explanatory causes for the data (cf. also Section 3.2). The competition is encouraged by the nonnegative constraints, as can be seen from the generative model of data p (vn = 1 | h;ψ) = σ (an + ∑ k wnkhk), in that some large weights (strong explaining power) will force others to degrade or even vanish (weak explaining power). This is different from standard practice in neural networks, where complex inhibitory lateral connections must be introduced to model the competition [24].\nOne important question is that under such constraints, besides the obvious gains in structural regularization, do we lose representational power of the standard RBM? On one hand, our experience has indicated that yes, there is certain loss in the ability to reconstruct the data, since the parameters are limited to be nonnegative. On the other hand, we have demonstrated that this does not away translate into the loss of predictive power. In our setting, the degree of constraints can also be relaxed by lowering down the regularization parameter α in Eq. (7), and this would allow some parameters to be negative.\n6.4 Model stability\nThe high-dimensional data necessitates sparse models wherein only small numbers of strongly predictive features are selected. However, most sparsity-inducing algorithms lead to unstable models due to data variations (e.g., resampling by bootstrapping, slight perturbation) [55]. For example, logistic regression produces unstable models while performing automated feature selection [5]. In the context of lasso, the method tends to keep only one feature if two are highly correlated [58], resulting in loss of stability.\nModel stability refers to the consistent degree of model parameters which are learned under data changes. For classifiers with embedded feature selection, a sub-problem is the stability of selected subsets of features. The importance of stability in feature selection has been largely ignored in literature. A popular approach in initial work is to compare feature selection methods basing on feature preferences ranked by their weights [33,30]. Another approach targets on developing a number of metrics to measure stability [31]. Recently, model stability has been studied more widely in bioinformatics. The research focus is to improve the stability by exploiting aggregated information [44,1,48] and the redundancy in the feature set [57,56].\nIn this paper, our work addresses the stability problem via readmission prognosis of heart failure patients using high-dimensional medical records. The heart failure data is studied in [20] but with only a small subset of features. Our recent work employs graph-based regularization to stabilize linear models [18, 53]. This paper differs from our previous work in that no external information is needed. Rather, it is based on self-organization of features into parts which are more stable than original features. This application in healthcare analytics continues our on going research on modeling electronic medical records which are high-dimensional and heterogeneous data [39]."
    }, {
      "heading" : "7 Conclusion",
      "text" : "To summarize, this paper has introduced a novel variant of the powerful restricted Boltzmann machine, termed nonnegative RBM (NRBM), where the mapping weights are constrained to be nonnegative. This gives the NRBM the new capacity to discover interpretable parts-based representations, semantically plausible high-level features for additive data such as images and texts. Our proposed method can also stabilize linear predictive models in feature selection task for high-dimensional medical data. In addition, the NRBM can be used to uncover the intrinsic dimensionality of the data, the ability not seen in the standard RBM. This is because under the nonnegativity constraint, the latent factors “compete” with each other to best represent data, leading to some unused factors. At the same time, the NRBM retains nearly full strength of the standard RBM, namely, compact and discriminative distributed representation, fast inference and incremental learning.\nCompared against the well-studied parts-based decomposition scheme, the nonnegative matrix factorization (NMF), the NRBM could work in places where the NMF fails. When it comes to classification using the learned representations, the features discovered by the NRBM are more discriminative than those by the NMF and the latent Dirichlet allocation (LDA). For model stability, the performance of the proposed model surpasses the lasso, the RBM and is comparable to the NMF. Thus, we believe that the NRBM is a fast alternative to the NMF and LDA for a variety of data processing tasks."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "The success of any machine learning system depends critically on<lb>effective representations of data. In many cases, it is desirable that a represen-<lb>tation scheme uncovers the parts-based, additive nature of the data. Of current<lb>representation learning schemes, restricted Boltzmann machines (RBMs) have<lb>proved to be highly effective in unsupervised settings. However, when it comes<lb>to parts-based discovery, RBMs do not usually produce satisfactory results. We<lb>enhance such capacity of RBMs by introducing nonnegativity into the model<lb>weights, resulting in a variant called nonnegative restricted Boltzmann machine<lb>(NRBM). The NRBM produces not only controllable decomposition of data<lb>into interpretable parts but also offers a way to estimate the intrinsic nonlin-<lb>ear dimensionality of data, and helps to stabilize linear predictive models. We<lb>demonstrate the capacity of our model on applications such as handwritten<lb>digit recognition, face recognition, document classification and patient read-<lb>mission prognosis. The decomposition quality on images is comparable with or<lb>better than what produced by the nonnegative matrix factorization (NMF),<lb>and the thematic features uncovered from text are qualitatively interpretable<lb>in a similar manner to that of the latent Dirichlet allocation (LDA). The sta-<lb>bility performance of feature selection on medical data is better than RBM<lb>and competitive with NMF. The learned features, when used for classifica-<lb>tion, are more discriminative than those discovered by both NMF and LDA<lb>and comparable with those by RBM.",
    "creator" : "LaTeX with hyperref package"
  }
}