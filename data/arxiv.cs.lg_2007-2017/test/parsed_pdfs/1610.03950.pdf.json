{
  "name" : "1610.03950.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Compressing Neural Language Models by Sparse Word Representations",
    "authors" : [ "Yunchuan Chen", "Lili Mou", "Yan Xu", "Ge Li", "Zhi Jin" ],
    "emails" : [ "chenyunchuan11@mails.ucas.ac.cn", "doublepower.mou@gmail.com,", "xuyan14@pku.edu.cn", "lige@pku.edu.cn", "zhijin@pku.edu.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Neural networks are among the state-ofthe-art techniques for language modeling. Existing neural language models typically map discrete words to distributed, dense vector representations. After information processing of the preceding context words by hidden layers, an output layer estimates the probability of the next word. Such approaches are time- and memory-intensive because of the large numbers of parameters for word embeddings and the output layer. In this paper, we propose to compress neural language models by sparse word representations. In the experiments, the number of parameters in our model increases very slowly with the growth of the vocabulary size, which is almost imperceptible. Moreover, our approach not only reduces the parameter space to a large extent, but also improves the performance in terms of the perplexity measure.1"
    }, {
      "heading" : "1 Introduction",
      "text" : "Language models (LMs) play an important role in a variety of applications in natural language processing (NLP), including speech recognition and document recognition. In recent years, neural network-based LMs have achieved significant breakthroughs: they can model language more precisely than traditional n-gram statistics (Mikolov et al., 2011); it is even possible to generate new sentences from a neural LM, benefiting various downstream tasks like machine translation, summarization, and dialogue systems (Devlin et al., 2014; Rush et al., 2015; Sordoni et al., 2015; Mou et al., 2015b).\n1Code released on https://github.com/chenych11/lm\nExisting neural LMs typically map a discrete word to a distributed, real-valued vector representation (called embedding) and use a neural model to predict the probability of each word in a sentence. Such approaches necessitate a large number of parameters to represent the embeddings and the output layer’s weights, which is unfavorable in many scenarios. First, with a wider application of neural networks in resourcerestricted systems (Hinton et al., 2015), such approach is too memory-consuming and may fail to be deployed in mobile phones or embedded systems. Second, as each word is assigned with a dense vector—which is tuned by gradient-based methods—neural LMs are unlikely to learn meaningful representations for infrequent words. The reason is that infrequent words’ gradient is only occasionally computed during training; thus their vector representations can hardly been tuned adequately.\nIn this paper, we propose a compressed neural language model where we can reduce the number of parameters to a large extent. To accomplish this, we first represent infrequent words’ embeddings with frequent words’ by sparse linear combinations. This is inspired by the observation that, in a dictionary, an unfamiliar word is typically defined by common words. We therefore propose an optimization objective to compute the sparse codes of infrequent words. The property of sparseness (only 4–8 values for each word) ensures the efficiency of our model.\nBased on the pre-computed sparse codes, we design our compressed language model as follows. A dense embedding is assigned to each common word; an infrequent word, on the other hand, computes its vector representation by a sparse combination of common words’ embeddings. We use the long short term memory (LSTM)-based recurrent neural network (RNN) as the hidden layer of\nIn Proc. ACL, pages 226–235, 2016.\nar X\niv :1\n61 0.\n03 95\n0v 1\n[ cs\n.C L\n] 1\n3 O\nct 2\n01 6\nour model. The weights of the output layer are also compressed in a same way as embeddings. Consequently, the number of trainable neural parameters is a constant regardless of the vocabulary size if we ignore the biases of words. Even considering sparse codes (which are very small), we find the memory consumption grows imperceptibly with respect to the vocabulary.\nWe evaluate our LM on the Wikipedia corpus containing up to 1.6 billion words. During training, we adopt noise-contrastive estimation (NCE) (Gutmann and Hyvärinen, 2012) to estimate the parameters of our neural LMs. However, different from Mnih and Teh (2012), we tailor the NCE method by adding a regression layer (called ZRegressoion) to predict the normalization factor, which stabilizes the training process. Experimental results show that, our compressed LM not only reduces the memory consumption, but also improves the performance in terms of the perplexity measure.\nTo sum up, the main contributions of this paper are three-fold. (1) We propose an approach to represent uncommon words’ embeddings by a sparse linear combination of common ones’. (2) We propose a compressed neural language model based on the pre-computed sparse codes. The memory increases very slowly with the vocabulary size (4– 8 values for each word). (3) We further introduce a ZRegression mechanism to stabilize the NCE algorithm, which is potentially applicable to other LMs in general."
    }, {
      "heading" : "2 Background",
      "text" : ""
    }, {
      "heading" : "2.1 Standard Neural LMs",
      "text" : "Language modeling aims to minimize the joint probability of a corpus (Jurafsky and Martin, 2014). Traditional n-gram models impose a Markov assumption that a word is only dependent on previous n − 1 words and independent of its position. When estimating the parameters, researchers have proposed various smoothing techniques including back-off models to alleviate the problem of data sparsity.\nBengio et al. (2003) propose to use a feedforward neural network (FFNN) to replace the multinomial parameter estimation in n-gram models. Recurrent neural networks (RNNs) can also be used for language modeling; they are especially capable of capturing long range dependencies in sentences (Mikolov et al., 2010; Sundermeyer et\nal., 2015). In the above models, we can view that a neural LM is composed of three main parts, namely the Embedding, Encoding, and Prediction subnets, as shown in Figure 1.\nThe Embedding subnet maps a word to a dense vector, representing some abstract features of the word (Mikolov et al., 2013). Note that this subnet usually accepts a list of words (known as history or context words) and outputs a sequence of word embeddings.\nThe Encoding subnet encodes the history of a target word into a dense vector (known as context or history representation). We may either leverage FFNNs (Bengio et al., 2003) or RNNs (Mikolov et al., 2010) as the Encoding subnet, but RNNs typically yield a better performance (Sundermeyer et al., 2015).\nThe Prediction subnet outputs a distribution of target words as\np(w = wi|h) = exp(s(h,wi))∑ j exp(s(h,wj)) , (1)\ns(h,wi) =W > i h+ bi, (2)\nwhere h is the vector representation of context/history h, obtained by the Encoding subnet. W = (W1,W2, . . . ,WV ) ∈ RC×V is the output weights of Prediction; b = (b1, b2, . . . , bV ) ∈ RC is the bias (the prior). s(h,wi) is a scoring function indicating the degree to which the context h matches a target word wi. (V is the size of vocabulary V; C is the dimension of context/history, given by the Encoding subnet.)"
    }, {
      "heading" : "2.2 Complexity Concerns of Neural LMs",
      "text" : "Neural network-based LMs can capture more precise semantics of natural language than n-gram models because the regularity of the Embedding subnet extracts meaningful semantics of a word\nand the high capacity of Encoding subnet enables complicated information processing.\nDespite these, neural LMs also suffer from several disadvantages mainly out of complexity concerns.\nTime complexity. Training neural LMs is typically time-consuming especially when the vocabulary size is large. The normalization factor in Equation (1) contributes most to time complexity. Morin and Bengio (2005) propose hierarchical softmax by using a Bayesian network so that the probability is self-normalized. Sampling techniques—for example, importance sampling (Bengio and Senécal, 2003), noise-contrastive estimation (Gutmann and Hyvärinen, 2012), and target sampling (Jean et al., 2014)—are applied to avoid computation over the entire vocabulary. Infrequent normalization maximizes the unnormalized likelihood with a penalty term that favors normalized predictions (Andreas and Klein, 2014).\nMemory complexity and model complexity. The number of parameters in the Embedding and Prediction subnets in neural LMs increases linearly with respect to the vocabulary size, which is large (Table 1). As said in Section 1, this is sometimes unfavorable in memory-restricted systems. Even with sufficient hardware resources, it is problematic because we are unlikely to fully tune these parameters. Chen et al. (2015) propose the differentiated softmax model by assigning fewer parameters to rare words than to frequent words. However, their approach only handles the output weights, i.e., W in Equation (2); the input embeddings remain uncompressed in their approach.\nIn this work, we mainly focus on memory and model complexity, i.e., we propose a novel method to compress the Embedding and Prediction subnets in neural language models."
    }, {
      "heading" : "2.3 Related Work",
      "text" : "Existing work on model compression for neural networks. Buciluǎ et al. (2006) and Hinton et al. (2015) use a well-trained large network to guide the training of a small network for model compression. Jaderberg et al. (2014) compress neural models by matrix factorization, Gong et al. (2014) by quantization. In NLP, Mou et al. (2015a) learn an embedding subspace by supervised training. Our work resembles little, if any, to the above methods as we compress embeddings and output weights using sparse word representations. Existing model\ncompression typically works with a compromise of performance. On the contrary, our model improves the perplexity measure after compression.\nSparse word representations. We leverage sparse codes of words to compress neural LMs. Faruqui et al. (2015) propose a sparse coding method to represent each word with a sparse vector. They solve an optimization problem to obtain the sparse vectors of words as well as a dictionary matrix simultaneously. By contrast, we do not estimate any dictionary matrix when learning sparse codes, which results in a simple and easyto-optimize model."
    }, {
      "heading" : "3 Our Proposed Model",
      "text" : "In this section, we describe our compressed language model in detail. Subsection 3.1 formalizes the sparse representation of words, serving as the premise of our model. On such a basis, we compress the Embedding and Prediction subnets in Subsections 3.2 and 3.3, respectively. Finally, Subsection 3.4 introduces NCE for parameter estimation where we further propose the ZRegression mechanism to stabilize our model."
    }, {
      "heading" : "3.1 Sparse Representations of Words",
      "text" : "We split the vocabulary V into two disjoint subsets (B and C). The first subset B is a base set, containing a fixed number of common words (8k in our experiments). C = V\\B is a set of uncommon words. We would like to use B’s word embeddings to encode C’s.\nOur intuition is that oftentimes a word can be defined by a few other words, and that rare words should be defined by common ones. Therefore, it is reasonable to use a few common words’ embeddings to represent that of a rare word. Following most work in the literature (Lee et al., 2006; Yang et al., 2011), we represent each uncommon word with a sparse, linear combination of com-\nmon ones’ embeddings. The sparse coefficients are called a sparse code for a given word.\nWe first train a word representation model like SkipGram (Mikolov et al., 2013) to obtain a set of embeddings for each word in the vocabulary, including both common words and rare words. Suppose U = (U1,U2, . . . ,UB) ∈ RE×B is the (learned) embedding matrix of common words, i.e., Ui is the embedding of i-th word in B. (Here, B = |B|.)\nEach word in B has a natural sparse code (denoted as x): it is a one-hot vector withB elements, the i-th dimension being on for the i-th word in B.\nFor a wordw ∈ C, we shall learn a sparse vector x = (x1, x2, . . . , xB) as the sparse code of the word. Provided that x has been learned (which will be introduced shortly), the embedding of w is\nŵ = B∑ j=1 xjUj = Ux, (3)\nTo learn the sparse representation of a certain word w, we propose the following optimization objective\nmin x ‖Ux−w‖22 + α‖x‖1 + β|1>x− 1|\n+ γ1>max{0,−x}, (4)\nwhere max denotes the component-wise maximum; w is the embedding for a rare word w ∈ C.\nThe first term (called fitting loss afterwards) evaluates the closeness between a word’s coded vector representation and its “true” representation w, which is the general goal of sparse coding.\nThe second term is an `1 regularizer, which encourages a sparse solution. The last two regularization terms favor a solution that sums to 1 and that is nonnegative, respectively. The nonnegative regularizer is applied as in He et al. (2012) due to psychological interpretation concerns.\nIt is difficult to determine the hyperparameters α, β, and γ. Therefore we perform several tricks. First, we drop the last term in the problem (4), but clip each element in x so that all the sparse codes are nonnegative during each update of training.\nSecond, we re-parametrize α and β by balancing the fitting loss and regularization terms dynamically during training. Concretely, we solve the following optimization problem, which is slightly different but closely related to the conceptual objective (4):\nmin x L(x) + αtR1(x) + βtR2(x), (5)\nwhere L(x) = ‖Ux −w‖22, R1(x) = ‖x‖1, and R2(x) = |1>x−1|. αt and βt are adaptive parameters that are resolved during training time. Suppose xt is the value we obtain after the update of the t-th step, we expect the importance of fitness and regularization remain unchanged during training. This is equivalent to\nαtR1(xt)\nL(xt) = wα ≡ const, (6)\nβtR2(xt)\nL(xt) = wβ ≡ const. (7)\nor\nαt = L(xt)\nR1(xt) wα and βt =\nL(xt)\nR2(xt) wβ,\nwhere wα and wβ are the ratios between the regularization loss and the fitting loss. They are much easier to specify than α or β in the problem (4).\nWe have two remarks as follows. • To learn the sparse codes, we first train the\n“true” embeddings by word2vec2 for both common words and rare words. However, these true embeddings are slacked during our language modeling. • As the codes are pre-computed and remain\nunchanged during language modeling, they are not tunable parameters of our neural model. Considering the learned sparse codes, we need only 4–8 values for each word on average, as the codes contain 0.05–0.1% nonzero values, which are almost negligible."
    }, {
      "heading" : "3.2 Parameter Compression for the",
      "text" : "Embedding Subnet\nOne main source of LM parameters is the Embedding subnet, which takes a list of words (history/context) as input, and outputs dense, lowdimensional vector representations of the words.\nWe leverage the sparse representation of words mentioned above to construct a compressed Embedding subnet, where the number of parameters is independent of the vocabulary size.\nBy solving the optimization problem (5) for each word, we obtain a non-negative sparse code x ∈ RB for each word, indicating the degree to which the word is related to common words in B. Then the embedding of a word is given by ŵ = Ux.\n2https://code.google.com/archive/p/word2vec\nWe would like to point out that the embedding of a word ŵ is not sparse becauseU is a dense matrix, which serves as a shared parameter of learning all words’ vector representations."
    }, {
      "heading" : "3.3 Parameter Compression for the",
      "text" : "Prediction Subnet\nAnother main source of parameters is the Prediction subnet. As Table 1 shows, the output layer contains V target-word weight vectors and biases; the number increases with the vocabulary size. To compress this part of a neural LM, we propose a weight-sharing method that uses words’ sparse representations again. Similar to the compression of word embeddings, we define a base set of weight vectors, and use them to represent the rest weights by sparse linear combinations.\nWithout loss of generality, we let D = W:,1:B be the output weights of B base target words, and c = b1:B be bias of the B target words.3 The goal is to use D and c to represent W and b. However, as the values ofW and b are unknown before the training of LM, we cannot obtain their sparse codes in advance.\nWe claim that it is reasonable to share the same set of sparse codes to represent word vectors in Embedding and the output weights in the Prediction subnet. In a given corpus, an occurrence of a word is always companied by its context. The co-occurrence statistics about a word or corresponding context are the same. As both word embedding and context vectors capture these co-occurrence statistics (Levy and Goldberg, 2014), we can expect that context vectors share the same internal structure as embeddings. Moreover, for a fine-trained network, given any word w and its context h, the output layer’s weight vector corresponding to w should specify a large inner-product score for the context h; thus these context vectors should approximate the weight vector of w. Therefore, word embeddings and the output weight vectors should share the same internal structures and it is plausible to use a same set of sparse representations for both words and target-word weight vectors. As we shall show in Section 4, our treatment of compressing the Prediction subnet does make sense and achieves high performance.\nFormally, the i-th output weight vector is estimated by\nŴi =Dxi, (8) 3 W:,1:B is the first B columns of W .\nThe biases can also be compressed as\nb̂i = cxi. (9)\nwhere xi is the sparse representation of the i-th word. (It is shared in the compression of weights and biases.)\nIn the above model, we have managed to compressed a language model whose number of parameters is irrelevant to the vocabulary size.\nTo better estimate a “prior” distribution of words, we may alternatively assign an independent bias to each word, i.e., b is not compressed. In this variant, the number of model parameters grows very slowly and is also negligible because each word needs only one extra parameter. Experimental results show that by not compressing the bias vector, we can even improve the performance while compressing LMs."
    }, {
      "heading" : "3.4 Noise-Contrastive Estimation with",
      "text" : "ZRegression\nWe adopt the noise-contrastive estimation (NCE) method to train our model. Compared with the maximum likelihood estimation of softmax, NCE reduces computational complexity to a large degree. We further propose the ZRegression mechanism to stablize training.\nNCE generates a few negative samples for each positive data sample. During training, we only\nneed to compute the unnormalized probability of these positive and negative samples. Interested readers are referred to (Gutmann and Hyvärinen, 2012) for more information.\nFormally, the estimated probability of the word wi with history/context h is\nP (w|h;θ) = 1 Zh P 0(wi|h;θ)\n= 1\nZh exp(s(wi, h;θ)), (10)\nwhere θ is the parameters and Zh is a contextdependent normalization factor. P 0(wi|h;θ) is the unnormalized probability of the w (given by the SpUnnrmProb layer in Figure 2).\nThe NCE algorithm suggests to take Zh as parameters to optimize along with θ, but it is intractable for context with variable lengths or large sizes in language modeling. Following Mnih and Teh (2012), we set Zh = 1 for all h in the base model (without ZRegression).\nThe objective for each occurrence of context/history h is\nJ(θ|h) = log P (wi|h;θ) P (wi|h;θ) + kPn(wi) +\nk∑ j=1 log kPn(wj) P (wj |h;θ) + kPn(wj) ,\nwhere Pn(w) is the probability of drawing a negative samplew; k is the number of negative samples that we draw for each positive sample.\nThe overall objective of NCE is\nJ(θ) = Eh[J(θ|h)] ≈ 1\nM M∑ i=1 J(θ|hi),\nwhere hi is an occurrence of the context and M is the total number of context occurrences.\nAlthough setting Zh to 1 generally works well in our experiment, we find that in certain scenarios, the model is unstable. Experiments show that when the true normalization factor is far away from 1, the cost function may vibrate. To comply with NCE in general, we therefore propose a ZRegression layer to predict the normalization constant Zh dependent on h, instead of treating it as a constant.\nThe regression layer is computed by\nZ−1h = exp(W > Z h+ bZ),\nwhereWZ ∈ RC and bZ ∈ R are weights and bias for ZRegression. Hence, the estimated probability by NCE with ZRegression is given by\nP (w|h) = exp(s(h,w)) · exp(W>Z h+ bZ).\nNote that the ZRegression layer does not guarantee normalized probabilities. During validation and testing, we explicitly normalize the probabilities by Equation (1)."
    }, {
      "heading" : "4 Evaluation",
      "text" : "In this part, we first describe our dataset in Subsection 4.1. We evaluate our learned sparse codes of rare words in Subsection 4.2 and the compressed language model in Subsection 4.3. Subsection 4.4 provides in-depth analysis of the ZRegression mechanism."
    }, {
      "heading" : "4.1 Dataset",
      "text" : "We used the freely available Wikipedia4 dump (2014) as our dataset. We extracted plain sentences from the dump and removed all markups. We further performed several steps of preprocessing such as text normalization, sentence splitting, and tokenization. Sentences were randomly shuffled, so that no information across sentences could be used, i.e., we did not consider cached language models. The resulting corpus contains about 1.6 billion running words.\nThe corpus was split into three parts for training, validation, and testing. As it is typically timeconsuming to train neural networks, we sampled a subset of 100 million running words to train neural LMs, but the full training set was used to train the backoff n-gram models. We chose hyperparameters by the validation set and reported model performance on the test set. Table 2 presents some statistics of our dataset."
    }, {
      "heading" : "4.2 Qualitative Analysis of Sparse Codes",
      "text" : "To obtain words’ sparse codes, we chose 8k common words as the “dictionary,” i.e., B = 8000.\n4http://en.wikipedia.org\nWe had 2k–42k uncommon words in different settings. We first pretrained word embeddings of both rare and common words, and obtained 200d vectors U and w in Equation (5). The dimension was specified in advance and not tuned. As there is no analytic solution to the objective, we optimized it by Adam (Kingma and Ba, 2014), which is a gradient-based method. To filter out small coefficients around zero, we simply set a value to 0 if it is less than 0.015 ·max{v ∈ x}. wα in Equation (6) was set to 1 because we deemed fitting loss and sparsity penalty are equally important. We set wβ in Equation (7) to 0.1, and this hyperparameter is insensitive.\nFigure 3 plots the sparse codes of a few selected words. As we see, algorithm, secret, and debate are common words, and each is (sparsely) coded by itself with a coefficient of 1. We further notice that a rare word like algorithms has a sparse representation with only a few non-zero coefficient.\nMoreover, the coefficient in the code of algorithms—corresponding to the base word algorithm—is large (∼ 0.6), showing that the words algorithm and algorithms are similar. Such phenomena are also observed with secret and debate.\nThe qualitative analysis demonstrates that our approach can indeed learn a sparse code of a word, and that the codes are meaningful."
    }, {
      "heading" : "4.3 Quantitative Analysis of Compressed Language Models",
      "text" : "We then used the pre-computed sparse codes to compress neural LMs, which provides quantitative analysis of the learned sparse representations of words. We take perplexity as the performance measurement of a language model, which is de-\nfined by\nPPL = 2− 1 N ∑N i=1 log2 p(wi|hi)\nwhere N is the number of running words in the test corpus."
    }, {
      "heading" : "4.3.1 Settings",
      "text" : "We leveraged LSTM-RNN as the Encoding subnet, which is a prevailing class of neural networks for language modeling (Sundermeyer et al., 2015; Karpathy et al., 2015). The hidden layer was 200d. We used the Adam algorithm to train our neural models. The learning rate was chosen by validation from {0.001, 0.002, 0.004, 0.006, 0.008}. Parameters were updated with a mini-batch size of 256 words. We trained neural LMs by NCE, where we generated 50 negative samples for each positive data sample in the corpus. All our model variants and baselines were trained with the same pre-defined hyperparameters or tuned over a same candidate set; thus our comparison is fair.\nWe list our compressed LMs and competing methods as follows. • KN3. We adopted the modified Kneser-Ney\nsmoothing technique to train a 3-gram LM; we used the SRILM toolkit (Stolcke and others, 2002) in out experiment. • LBL5. A Log-BiLinear model introduced in\nMnih and Hinton (2007). We used 5 preceding words as context. • LSTM-s. A standard LSTM-RNN language\nmodel which is applied in Sundermeyer et al. (2015) and Karpathy et al. (2015). We implemented the LM ourselves based on Theano (Theano Development Team, 2016) and also used NCE for training. • LSTM-z. An LSTM-RNN enhanced with\nthe ZRegression mechanism described in Section 3.4. • LSTM-z,wb. Based on LSTM-z, we com-\npressed word embeddings in Embedding and the output weights and biases in Prediction. • LSTM-z,w. In this variant, we did not com-\npress the bias term in the output layer. For each word in C, we assigned an independent bias parameter."
    }, {
      "heading" : "4.3.2 Performance",
      "text" : "Tables 3 shows the perplexity of our compressed model and baselines. As we see, LSTM-based LMs significantly outperform the log-bilinear\nmodel as well as the backoff 3-gram LM, even if the 3-gram LM is trained on a much larger corpus with 1.6 billion words. The ZRegression mechanism improves the performance of LSTM to a large extent, which is unexpected. Subsection 4.4 will provide more in-depth analysis.\nRegarding the compression method proposed in this paper, we notice that LSTM-z,wb and LSTM-z,w yield similar performance to LSTM-z. In particular, LSTM-z,w outperforms LSTM-z in all scenarios of different vocabulary sizes. Moreover, both LSTM-z,wb and LSTM-z,w can reduce the memory consumption by up to 80% (Table 4).\nWe further plot in Figure 4 the model performance (lines) and memory consumption (bars) in a fine-grained granularity of vocabulary sizes. We see such a tendency that compressed LMs (LSTMz,wb and LSTM-z,w, yellow and red lines) are generally better than LSTM-z (black line) when\nwe have a small vocabulary. However, LSTMz,wb is slightly worse than LSTM-z if the vocabulary size is greater than, say, 20k. The LSTM-z,w remains comparable to LSTM-z as the vocabulary grows.\nTo explain this phenomenon, we may imagine that the compression using sparse codes has two effects: it loses information, but it also enables more accurate estimation of parameters especially for rare words. When the second factor dominates, we can reasonably expect a high performance of the compressed LM.\nFrom the bars in Figure 4, we observe that traditional LMs have a parameter space growing linearly with the vocabulary size. But the number of parameters in our compressed models does not increase—or strictly speaking, increases at an extremely small rate—with vocabulary.\nThese experiments show that our method can largely reduce the parameter space with even performance improvement. The results also verify that the sparse codes induced by our model indeed capture meaningful semantics and are potentially useful for other downstream tasks.\n4.4 Effect of ZRegression\nWe next analyze the effect of ZRegression for NCE training. As shown in Figure 5a, the training process becomes unstable after processing 70% of the dataset: the training loss vibrates significantly, whereas the test loss increases.\nWe find a strong correlation between unstableness and the Zh factor in Equation (10), i.e., the sum of unnormalized probability (Figure 5b). Theoretical analysis shows that theZh factor tends to be self-normalized even though it is not forced to (Gutmann and Hyvärinen, 2012). However, problems would occur, should it fail.\nIn traditional methods, NCE jointly estimates normalization factor Z and model parameters (Gutmann and Hyvärinen, 2012). For language modeling, Zh dependents on context h. Mnih and Teh (2012) propose to estimate a separate Zh based on two history words (analogous to 3-gram), but their approach hardly scales to RNNs because of the exponential number of different combinations of history words.\nWe propose the ZRegression mechanism in Section 3.4, which can estimate the Zh factor well (Figure 5d) based on the history vector h. In this way, we manage to stabilize the training process (Figure 5c) and improve the performance by\na large margin, as has shown in Table 3. It should be mentioned that ZRegression is not specific to model compression and is generally applicable to other neural LMs trained by NCE."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this paper, we proposed an approach to represent rare words by sparse linear combinations of common ones. Based on such combinations, we managed to compress an LSTM language model (LM), where memory does not increase with the vocabulary size except a bias and a sparse code for each word. Our experimental results also show that the compressed LM has yielded a better performance than the uncompressed base LM."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This research is supported by the National Basic Research Program of China (the 973 Program) under Grant No. 2015CB352201, the National Natural Science Foundation of China under Grant Nos. 61232015, 91318301, 61421091 and 61502014, and the China Post-Doctoral Foundation under Grant No. 2015M580927."
    } ],
    "references" : [ {
      "title" : "When and why are log-linear models self-normalizing",
      "author" : [ "Andreas", "Klein2014] Jacob Andreas", "Dan Klein" ],
      "venue" : "In Proceedings of the Annual Meeting of the North American Chapter of the Association",
      "citeRegEx" : "Andreas et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Andreas et al\\.",
      "year" : 2014
    }, {
      "title" : "Quick training of probabilistic neural nets by importance sampling",
      "author" : [ "Bengio", "Senécal2003] Yoshua Bengio", "JeanSébastien Senécal" ],
      "venue" : "In Proceedings of the Ninth International Workshop on Artificial Intelligence and Statistics",
      "citeRegEx" : "Bengio et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2003
    }, {
      "title" : "A neural probabilistic language model",
      "author" : [ "Bengio et al.2003] Yoshua Bengio", "Réjean Ducharme", "Pascal Vincent", "Christian Jauvin" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Bengio et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2003
    }, {
      "title" : "Strategies for training large vocabulary neural language models. arXiv preprint arXiv:1512.04906",
      "author" : [ "Chen et al.2015] Welin Chen", "David Grangier", "Michael Auli" ],
      "venue" : null,
      "citeRegEx" : "Chen et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2015
    }, {
      "title" : "Fast and robust neural network joint models for statistical machine",
      "author" : [ "Devlin et al.2014] Jacob Devlin", "Rabih Zbib", "Zhongqiang Huang", "Thomas Lamar", "Richard M Schwartz", "John Makhoul" ],
      "venue" : null,
      "citeRegEx" : "Devlin et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2014
    }, {
      "title" : "Sparse overcomplete word vector representations",
      "author" : [ "Yulia Tsvetkov", "Dani Yogatama", "Chris Dyer", "Noah A. Smith" ],
      "venue" : "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguis-",
      "citeRegEx" : "Faruqui et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Faruqui et al\\.",
      "year" : 2015
    }, {
      "title" : "Compressing deep convolutional networks using vector quantization",
      "author" : [ "Gong et al.2014] Yunchao Gong", "Liu Liu", "Ming Yang", "Lubomir Bourdev" ],
      "venue" : "arXiv preprint arXiv:1412.6115",
      "citeRegEx" : "Gong et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Gong et al\\.",
      "year" : 2014
    }, {
      "title" : "Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics",
      "author" : [ "Gutmann", "Hyvärinen2012] Michael Gutmann", "Aapo Hyvärinen" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Gutmann et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Gutmann et al\\.",
      "year" : 2012
    }, {
      "title" : "Document summarization based on data reconstruction",
      "author" : [ "He et al.2012] Zhanying He", "Chun Chen", "Jiajun Bu", "Can Wang", "Lijun Zhang", "Deng Cai", "Xiaofei He" ],
      "venue" : "In Proceedings of the 26th AAAI Conference on Artificial Intelligence,",
      "citeRegEx" : "He et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2012
    }, {
      "title" : "Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531",
      "author" : [ "Oriol Vinyals", "Jeff Dean" ],
      "venue" : null,
      "citeRegEx" : "Hinton et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2015
    }, {
      "title" : "Speeding up convolutional neural networks with low rank expansions",
      "author" : [ "Andrea Vedaldi", "Andrew Zisserman" ],
      "venue" : "In Proceedings of the British Machine Vision Conference",
      "citeRegEx" : "Jaderberg et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Jaderberg et al\\.",
      "year" : 2014
    }, {
      "title" : "On using very large target vocabulary for neural machine translation",
      "author" : [ "Jean et al.2014] Sébastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1412.2007",
      "citeRegEx" : "Jean et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Jean et al\\.",
      "year" : 2014
    }, {
      "title" : "Visualizing and understanding recurrent networks. arXiv preprint arXiv:1506.02078",
      "author" : [ "Justin Johnson", "Fei-Fei Li" ],
      "venue" : null,
      "citeRegEx" : "Karpathy et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Karpathy et al\\.",
      "year" : 2015
    }, {
      "title" : "Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980",
      "author" : [ "Kingma", "Ba2014] Diederik P Kingma", "Jimmy Ba" ],
      "venue" : null,
      "citeRegEx" : "Kingma et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kingma et al\\.",
      "year" : 2014
    }, {
      "title" : "Efficient sparse coding algorithms",
      "author" : [ "Lee et al.2006] Honglak Lee", "Alexis Battle", "Rajat Raina", "Andrew Y Ng" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Lee et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2006
    }, {
      "title" : "Linguistic regularities in sparse and explicit word representations",
      "author" : [ "Levy", "Goldberg2014] Omer Levy", "Yoav Goldberg" ],
      "venue" : "In Proceedings of the Eighteenth Conference on Natural Language Learning,",
      "citeRegEx" : "Levy et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Levy et al\\.",
      "year" : 2014
    }, {
      "title" : "Recurrent neural network based language model",
      "author" : [ "Martin Karafiát", "Lukas Burget", "Jan Cernockỳ", "Sanjeev Khudanpur" ],
      "venue" : "In INTERSPEECH,",
      "citeRegEx" : "Mikolov et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2010
    }, {
      "title" : "Strategies for training large scale neural network language models",
      "author" : [ "Anoop Deoras", "Daniel Povey", "Lukas Burget", "Jan Cernocký" ],
      "venue" : "In Proceedings of the IEEE Workshop on Automatic Speech Recognition",
      "citeRegEx" : "Mikolov et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2011
    }, {
      "title" : "Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781",
      "author" : [ "Kai Chen", "Greg Corrado", "Jeffrey Dean" ],
      "venue" : null,
      "citeRegEx" : "Mikolov et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Three new graphical models for statistical language modelling",
      "author" : [ "Mnih", "Hinton2007] Andriy Mnih", "Geoffrey Hinton" ],
      "venue" : "In Proceedings of the 24th International Conference on Machine learning,",
      "citeRegEx" : "Mnih et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Mnih et al\\.",
      "year" : 2007
    }, {
      "title" : "A fast and simple algorithm for training neural probabilistic language models. arXiv preprint arXiv:1206.6426",
      "author" : [ "Mnih", "Teh2012] Andriy Mnih", "Yee-Whye Teh" ],
      "venue" : null,
      "citeRegEx" : "Mnih et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Mnih et al\\.",
      "year" : 2012
    }, {
      "title" : "Hierarchical probabilistic neural network language model",
      "author" : [ "Morin", "Bengio2005] Fréderic Morin", "Yoshua Bengio" ],
      "venue" : "In Proceedings of the International Workshop on Artificial Intelligence and Statistics,",
      "citeRegEx" : "Morin et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Morin et al\\.",
      "year" : 2005
    }, {
      "title" : "Distilling word embeddings: An encoding approach",
      "author" : [ "Mou et al.2015a] Lili Mou", "Ge Li", "Yan Xu", "Lu Zhang", "Zhi Jin" ],
      "venue" : "arXiv preprint arXiv:1506.04488",
      "citeRegEx" : "Mou et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Mou et al\\.",
      "year" : 2015
    }, {
      "title" : "2015b. Backward and forward language modeling for constrained natural language generation",
      "author" : [ "Mou et al.2015b] Lili Mou", "Rui Yan", "Ge Li", "Lu Zhang", "Zhi Jin" ],
      "venue" : "arXiv preprint arXiv:1512.06612",
      "citeRegEx" : "Mou et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Mou et al\\.",
      "year" : 2015
    }, {
      "title" : "A neural attention model for abstractive sentence summarization",
      "author" : [ "Sumit Chopra", "Jason Weston" ],
      "venue" : "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Rush et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Rush et al\\.",
      "year" : 2015
    }, {
      "title" : "A neural network approach to context-sensitive generation",
      "author" : [ "Michel Galley", "Michael Auli", "Chris Brockett", "Yangfeng Ji", "Margaret Mitchell", "Jian-Yun Nie", "Jianfeng Gao", "Bill Dolan" ],
      "venue" : null,
      "citeRegEx" : "Sordoni et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Sordoni et al\\.",
      "year" : 2015
    }, {
      "title" : "SRILM—An extensible language modeling toolkit",
      "author" : [ "Andreas Stolcke" ],
      "venue" : "In INTERSPEECH,",
      "citeRegEx" : "Stolcke,? \\Q2002\\E",
      "shortCiteRegEx" : "Stolcke",
      "year" : 2002
    }, {
      "title" : "From feedforward to recurrent LSTM neural networks for language modeling",
      "author" : [ "Hermann Ney", "Ralf Schlüter" ],
      "venue" : "IEEE/ACM Transactions on Audio, Speech and Language Processing,",
      "citeRegEx" : "Sundermeyer et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Sundermeyer et al\\.",
      "year" : 2015
    }, {
      "title" : "Robust sparse coding for face recognition",
      "author" : [ "Yang et al.2011] Meng Yang", "Lei Zhang", "Jian Yang", "David Zhang" ],
      "venue" : "In Proceedings of the 2011 IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Yang et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 17,
      "context" : "In recent years, neural network-based LMs have achieved significant breakthroughs: they can model language more precisely than traditional n-gram statistics (Mikolov et al., 2011); it is even possible to generate new sentences from a neural LM, benefiting various downstream tasks like machine translation, summarization, and dialogue systems (Devlin et al.",
      "startOffset" : 157,
      "endOffset" : 179
    }, {
      "referenceID" : 4,
      "context" : ", 2011); it is even possible to generate new sentences from a neural LM, benefiting various downstream tasks like machine translation, summarization, and dialogue systems (Devlin et al., 2014; Rush et al., 2015; Sordoni et al., 2015; Mou et al., 2015b).",
      "startOffset" : 171,
      "endOffset" : 252
    }, {
      "referenceID" : 24,
      "context" : ", 2011); it is even possible to generate new sentences from a neural LM, benefiting various downstream tasks like machine translation, summarization, and dialogue systems (Devlin et al., 2014; Rush et al., 2015; Sordoni et al., 2015; Mou et al., 2015b).",
      "startOffset" : 171,
      "endOffset" : 252
    }, {
      "referenceID" : 25,
      "context" : ", 2011); it is even possible to generate new sentences from a neural LM, benefiting various downstream tasks like machine translation, summarization, and dialogue systems (Devlin et al., 2014; Rush et al., 2015; Sordoni et al., 2015; Mou et al., 2015b).",
      "startOffset" : 171,
      "endOffset" : 252
    }, {
      "referenceID" : 9,
      "context" : "First, with a wider application of neural networks in resourcerestricted systems (Hinton et al., 2015), such approach is too memory-consuming and may fail to be deployed in mobile phones or embedded systems.",
      "startOffset" : 81,
      "endOffset" : 102
    }, {
      "referenceID" : 18,
      "context" : "The Embedding subnet maps a word to a dense vector, representing some abstract features of the word (Mikolov et al., 2013).",
      "startOffset" : 100,
      "endOffset" : 122
    }, {
      "referenceID" : 1,
      "context" : "We may either leverage FFNNs (Bengio et al., 2003) or RNNs (Mikolov et al.",
      "startOffset" : 29,
      "endOffset" : 50
    }, {
      "referenceID" : 16,
      "context" : ", 2003) or RNNs (Mikolov et al., 2010) as the Encoding subnet, but RNNs typically yield a better performance (Sundermeyer et al.",
      "startOffset" : 16,
      "endOffset" : 38
    }, {
      "referenceID" : 27,
      "context" : ", 2010) as the Encoding subnet, but RNNs typically yield a better performance (Sundermeyer et al., 2015).",
      "startOffset" : 78,
      "endOffset" : 104
    }, {
      "referenceID" : 11,
      "context" : "Sampling techniques—for example, importance sampling (Bengio and Senécal, 2003), noise-contrastive estimation (Gutmann and Hyvärinen, 2012), and target sampling (Jean et al., 2014)—are applied to avoid computation over the entire vocabulary.",
      "startOffset" : 161,
      "endOffset" : 180
    }, {
      "referenceID" : 3,
      "context" : "Chen et al. (2015) propose the differentiated softmax model by assigning fewer parameters to rare words than to frequent words.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 8,
      "context" : "(2006) and Hinton et al. (2015) use a well-trained large network to guide the training of a small network for model compression.",
      "startOffset" : 11,
      "endOffset" : 32
    }, {
      "referenceID" : 8,
      "context" : "(2006) and Hinton et al. (2015) use a well-trained large network to guide the training of a small network for model compression. Jaderberg et al. (2014) compress neural models by matrix factorization, Gong et al.",
      "startOffset" : 11,
      "endOffset" : 153
    }, {
      "referenceID" : 6,
      "context" : "(2014) compress neural models by matrix factorization, Gong et al. (2014) by quantization.",
      "startOffset" : 55,
      "endOffset" : 74
    }, {
      "referenceID" : 6,
      "context" : "(2014) compress neural models by matrix factorization, Gong et al. (2014) by quantization. In NLP, Mou et al. (2015a) learn an embedding subspace by supervised training.",
      "startOffset" : 55,
      "endOffset" : 118
    }, {
      "referenceID" : 5,
      "context" : "Faruqui et al. (2015) propose a sparse coding method to represent each word with a sparse vector.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 14,
      "context" : "Following most work in the literature (Lee et al., 2006; Yang et al., 2011), we represent each uncommon word with a sparse, linear combination of com-",
      "startOffset" : 38,
      "endOffset" : 75
    }, {
      "referenceID" : 28,
      "context" : "Following most work in the literature (Lee et al., 2006; Yang et al., 2011), we represent each uncommon word with a sparse, linear combination of com-",
      "startOffset" : 38,
      "endOffset" : 75
    }, {
      "referenceID" : 18,
      "context" : "We first train a word representation model like SkipGram (Mikolov et al., 2013) to obtain a set of embeddings for each word in the vocabulary, including both common words and rare words.",
      "startOffset" : 57,
      "endOffset" : 79
    }, {
      "referenceID" : 8,
      "context" : "The nonnegative regularizer is applied as in He et al. (2012) due to psychological interpretation concerns.",
      "startOffset" : 45,
      "endOffset" : 62
    }, {
      "referenceID" : 27,
      "context" : "We leveraged LSTM-RNN as the Encoding subnet, which is a prevailing class of neural networks for language modeling (Sundermeyer et al., 2015; Karpathy et al., 2015).",
      "startOffset" : 115,
      "endOffset" : 164
    }, {
      "referenceID" : 12,
      "context" : "We leveraged LSTM-RNN as the Encoding subnet, which is a prevailing class of neural networks for language modeling (Sundermeyer et al., 2015; Karpathy et al., 2015).",
      "startOffset" : 115,
      "endOffset" : 164
    }, {
      "referenceID" : 25,
      "context" : "We adopted the modified Kneser-Ney smoothing technique to train a 3-gram LM; we used the SRILM toolkit (Stolcke and others, 2002) in out experiment. • LBL5. A Log-BiLinear model introduced in Mnih and Hinton (2007). We used 5 preceding words as context.",
      "startOffset" : 104,
      "endOffset" : 215
    }, {
      "referenceID" : 25,
      "context" : "We adopted the modified Kneser-Ney smoothing technique to train a 3-gram LM; we used the SRILM toolkit (Stolcke and others, 2002) in out experiment. • LBL5. A Log-BiLinear model introduced in Mnih and Hinton (2007). We used 5 preceding words as context. • LSTM-s. A standard LSTM-RNN language model which is applied in Sundermeyer et al. (2015) and Karpathy et al.",
      "startOffset" : 104,
      "endOffset" : 345
    }, {
      "referenceID" : 12,
      "context" : "(2015) and Karpathy et al. (2015). We implemented the LM ourselves based on Theano (Theano Development Team, 2016) and also used NCE for training.",
      "startOffset" : 11,
      "endOffset" : 34
    } ],
    "year" : 2016,
    "abstractText" : "Neural networks are among the state-ofthe-art techniques for language modeling. Existing neural language models typically map discrete words to distributed, dense vector representations. After information processing of the preceding context words by hidden layers, an output layer estimates the probability of the next word. Such approaches are timeand memory-intensive because of the large numbers of parameters for word embeddings and the output layer. In this paper, we propose to compress neural language models by sparse word representations. In the experiments, the number of parameters in our model increases very slowly with the growth of the vocabulary size, which is almost imperceptible. Moreover, our approach not only reduces the parameter space to a large extent, but also improves the performance in terms of the perplexity measure.1",
    "creator" : "LaTeX with hyperref package"
  }
}