{
  "name" : "1305.7111.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Test cost and misclassification cost trade-off using reframing",
    "authors" : [ "José Hernández-Orallo" ],
    "emails" : [ "cemadj@gmail.com", "jorallo@dsic.upv.es" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Many solutions to cost-sensitive classification (and regression) rely on some or all of the following assumptions: we have complete knowledge about the cost context at training time, we can easily re-train whenever the cost context changes, and we have technique-specific methods (such as cost-sensitive decision trees) that can take advantage of that information. In this paper we address the problem of selecting models and minimising joint cost (integrating both misclassification cost and test costs) without any of the above assumptions. We introduce methods and plots (such as the so-called JROC plots) that can work with any off-the-shelf predictive technique, including ensembles, such that we reframe the model to use the appropriate subset of attributes (the feature configuration) during deployment time. In other words, models are trained with the available attributes (once and for all) and then deployed by setting missing values on the attributes that are deemed ineffective for reducing the joint cost. As the number of feature configuration combinations grows exponentially with the number of features we introduce quadratic methods that are able to approximate the optimal configuration and model choices, as shown by the experimental results. Keywords: test cost, misclassification cost, missing values, reframing, ROC analysis, operating context, feature configuration, feature selection."
    }, {
      "heading" : "1 Introduction",
      "text" : "The feature space (including both input and output variables) characterises a data mining problem [29]. In predictive (supervised) problems, the quality and availability of features determines the predictability of the dependent variable, and the performance of data mining models in terms of misclassification or regression error. Good features, however, are usually difficult to obtain. It is usual that many instances come with missing values, either because the actual value for a given attribute was not available or because it was too expensive (e.g., in medical domains, where attributes usually correspond to diagnostic tests). This frequently represents a utility or cost-sensitive learning dilemma [35, 11] between misclassification (or regression error) costs and tests costs, both being integrated into a joint cost.\nOne possible option is known as missing value imputation [37], but this approach is not usually appropriate when test costs are considered. First, imputing missing values “is regarded as unnecessary for cost-sensitive learning that also considers the test costs” [36]. Second, expensive attributes (e.g., in diagnosis) are usually missing for many other instances as well and it is difficulty to infer them from other instances or attributes.\nThe most common option is to train models that are able to do reasonably good predictions with the available attributes. However, a more powerful approach is to find a trade-off (in terms of minimising joint cost) about how many (and which) attributes need to be used. Retraining with all the attribute subsets (possibly using feature selection) does not seem to be a good option, because for n attributes we typically have 2n possible combinations. As a result, one common option is to use techniques that lead to models that can use any subset of attributes. Decision trees are the usual choice [30, 36, 31] because the use of attributes can be customised in many different ways. Similarly, we could also try —if not already done— to design costsensitive versions of many other families of techniques, such as Bayesian models, neural networks, logistic\nar X\niv :1\n30 5.\n71 11\nv1 [\ncs .L\nG ]\n3 0\nM ay\nregression, kernel methods, etc., with varying success. This would lead to two problems. One one hand, we would need to have a library of specific cost-sensitive algorithms for classification and regression, which would also limit our range of options and the use of the ultimate learning techniques (until cost-sensitive versions appear and are implemented). On the other hand, even assuming that this is possible, we would require some tools to properly select which model is better, as we do not know in advance (in training time) what the misclassification (or regression error) cost and test cost context will be during deployment. In fact, each instance may have a different subset of missing values and a different cost context, so any choice performed during the training stage will be specific and biased.\nIn this paper, we explore an alternative, more general approach that can use off-the-shelf machine learning methods. The procedure is simple: we use any data mining technique that accepts missing values during training and prediction and learn a predictive model with our training data as usual. Then, we evaluate the model (on a validation dataset) by exploring the lattice of attribute subsets, with a very straightforward mechanism: we set missing values on purpose for each combination in the lattice. As a result, we know how well our model behaves for any attribute subset. From here, once the model needs to be deployed on unlabelled data, and whenever a new instance appears (with a possibly particular cost context) we decide which attributes the model requires to get the lowest expected joint cost1. This is done by calculating the expected joint cost for each point in the lattice. In this sense, each prediction is associated with a possibly different operating condition, and the best attribute subset is chosen.\nInterestingly, we can use the previous approach for more than one model, and see that some models dominate for some operating conditions over the rest. This is exactly the way ROC analysis works (for classification [34, 19, 24, 13, 32] and for regression [23]). We will introduce graphical plots and procedures to make this selection and also to reduce the number of combinations in the lattice that need to be explored in order to make a good selection.\nThe goal of the paper is then to introduce new methods to make optimal choices in terms of joint cost (i.e., considering both misclassification and test costs) when using off-the-shelf data mining models. An optimal choice is understood as selecting the right model with the right subset of attributes. In this paper we will focus on classification, but many of the ideas could be extended to regression as well.\nSection 2 reviews the notion of misclassification and test costs, cost context, and motivates the possibility of solving the problem of cost minimisation and model selection in a different way. Section 3 proposes the idea of reframing an existing model to a new cost context by setting (or letting) some of the attributes as missing. Several examples show that good results can be obtained by setting most of the attributes missing during deployment, and show that the approach can be applied to any kind of predictive technique, including ensembles. Section 4 introduces a more effective way of analysing and finding the trade-off between test cost (TC) and misclassification cost (MC), by plotting TC on the x-axis and MC on the y-axis, and finding the optimal feature and model configuration using isometrics on these plots. The notion of convex hull and dominance are also introduced. After realising that there are 2m feature configurations for m features, section 5 explores hull approximations by performing a quadratic selection on the number of configurations that need to be explored. Section 6 evaluates these approximations on several datasets and cost context. Section 7 closes the paper with some recommendations and take-away messages about how to use the methods and plots introduced here. Several extensions are proposed as future work."
    }, {
      "heading" : "2 Motivation",
      "text" : "We will focus on classification problems, characterised by a multivariate input domain X, i.e., a tuple of elements of sets X1, X2, . . . , Xm, where m is the number of features or input attributes, possibly containing the null value, and a univariate output domain Y ⊂ {l1, l2, . . . , lc}, where c is the number of classes or labels of the output attribute. The domain space D is then X × Y. Examples or instances are just pairs 〈x, y〉 ∈ D, and datasets are subsets (actually multi-sets) of D. The length of a dataset will usually be denoted by n. A crisp or hard classification model f̂ is a function f̂ : X → Y. We just represent the true value by y and the estimated value by ŷ. Subindices will be used when referring to more than one example\n1Given an example with some non-missing and some missing values we may decide increase the number of non-missing values. Given a case for which we have not still retrieved any of the attributes (tests) we decide how many (and which) attributes we are going to ask for.\nin a dataset. Given an example i, the values of the m input attributes are denoted by xi,1, xi,2, . . . , xi,m. Throughout the paper we will use several classifiers from Weka [22]. In this paper we are especially interested in using the techniques as they are, being able to use techniques that are, in principle, inattentive to the use of all the attributes, such as kernel methods, ensembles, etc. In particular, we will use SMO (a support vector machine), IBk (a k-nearest neighbour), J48 (a decision tree), Adaboost (an ensemble method with J48 decision trees) and Bagging (an example method with J48 decision trees). All of them will be used with their default parameters.\nOnce this common setting for classification is set, we may wonder how models are created and deployed. In fact, models are usually learned under some contextual information but possibly deployed several times under changing conditions. Reuse of learned models is of critical importance in the majority of knowledgeintensive application areas, particularly because the operating context can be expected to vary from training to deployment and we need to make the best decision according to that context [34, 19]. One kind of context is related to the way inputs (i.e., features) can vary from training to deployment. Among these changes, we can mention two important ones: attributes may not be available (missing values) or may have different test costs. Another type of context depends on how class distribution and misclassification costs affect the output variable. Note that these context changes may happen for each problem instance individually. For instance, in a medical domain, some tests may not be applicable to some patients (as can be contraindicated or risky), and other tests may be more or less expensive depending on the patient (her insurance policy). Also, for the output variable, a wrong diagnosis usually has asymmetric costs, as a false negative is usually worse (and economically more expensive in the long term) than a false positive. Again, these costs may be different for each example.\nThese two types of costs (test costs and misclassification costs) are highly intertwined. In fact, as Turney [35] points out, we can only rationally determine whether it is worthwhile to pay the cost of test when we know the cost of misclassification errors. If the cost of misclassification errors is much greater than the cost of tests, then it is rational to purchase all tests that seem to have some predictive value. But if the cost of misclassification errors is much less than the cost of tests, then it is not rational to purchase any tests.\nLet us define these types of cost formally:\nDefinition 1. A misclassification cost function is any function M : Y×Y→ R which compares elements in the output domain. For convenience, the first argument will be the estimated value, and the second argument the actual value.\nAs Y is a discrete set, typically we refer to M as the misclassification cost matrix. We will assume that the diagonal of the matrix is zero (i.e., ∀y : M(y, y) = 0) and that the other elements of the matrix are greater than or equal to 0.\nWe can have a different matrix for each example, denoted by Mi. From above, we define the misclassification cost MC of an example i as MCi ,Mi(ŷi, yi). Only when the matrix is the same for all the examples, we can just calculate the average MC as the Frobenius product between the confusion matrix for the whole dataset and the cost matrix, divided by n.\nDefinition 2. The test cost vector is a real vector of size m, i.e., (t1, t2, . . . , tm), where m is the number of attributes. The test cost function Tj is any function as follows:\nTj(x) , { tj if x is not null 0 otherwise\nWe can have a different test cost function for each example and attribute, denoted by Ti,j . From above, we define the test cost TC of an example i as TCi , ∑m j=1 Ti,j(xi,j). Only when Ti,j are independent of the example i we can just calculate the average TC as the dot product between the use vector (how many times each attribute has been used for the dataset) and the test cost vector, divided by n.\nWe want to integrate both the misclassification cost and the test cost in one single measure of cost:\nDefinition 3. The joint cost for example i is:\nJCi , α ·MCi + (1− α) · TCi\nwith α ∈ [0, 1].\nThe value α will be better explained later on, but clearly sets more relevance to misclassification or test costs. If α = 1 only the misclassification cost matters, and if α = 0 only the test cost matters. M , T and α configure the cost context or operating condition. With m attributes and c classes, there are m+ c(c− 1)− 1 degrees of freedom (assuming the cost matrix has a zero diagonal).\nExample 1. Consider the iris dataset [1], created by R.A. Fisher, which is composed of four attributes: SL, SW , PL and PW and three classes: setosa, versicolour and virginica.\nAssume that we have an example where the test cost vector is (3, 2, 10, 5) and the misclassification cost matrix M is defined as follows:\nsetosa versicolour virginica setosa 0 20 15\nversicolour 5 0 15 virginica 30 15 0\nwhere columns represent the actual value and rows the predicted value. Consider also that we have three models to be applied to the same instance. Model 1 requires attributes SL and PL and predicts virginica, model 2 requires attributes SL and SW and predicts setosa, and model 3 requires all attributes and predicts versicolour. If the true label is versicolour, then we have JC = MC + TC = 15 + (3 + 10) = 28 for model 1, JC = MC + TC = 20 + (3 + 2) = 25 for model 2, and JC = MC + TC = 0 + (3 + 2 + 10 + 5) = 20 for model 3.\nIn the previous example, model 3 is better than the other two for this example. Of course, in general, we need to make the decision of which model to use without knowing the actual label, and that will depend on the reliability of the models and the class frequencies. This is then a decision problem that can be solved by determining the model with lowest expected cost.\nInterestingly, we may wonder what would happen if we removed attribute PW for model 3. Even if we are told that model 3 was trained to work with that attribute, it is not difficult to guess what the model could do without it and still give a prediction. As we will see in more detail below, there are (at least) two ways to do it. First, we could set PW to null (i.e., make it missing) and see what happens. Second, we could consider as range values for the attribute and get the most frequently predicted class. Clearly, none of these methods actually requires the attribute but allows us to use model 3. Imagine that, by using any of these two methods, model 3 still predicts versicolour. Our cost would have been lowered down to 15.\nSo, the question we want to address in this paper is not only what model to choose but also the subset of attributes that we will use (‘buy’). How can we analyse this problem systematically? Can we use any kind of technique in machine learning, statistics and data mining, including ensembles, kernel methods, etc., where the tests costs are originally high."
    }, {
      "heading" : "3 Reframing the model with missing values on purpose",
      "text" : "There has been an extensive work in the past decades on how the performance of a predictive technique evolves with different feature subsets. This is the core of feature selection techniques. In fact, model performance can even be increased by using a subset of the original attributes. Also, if we think about costs, most works on minimising costs have taken this approach [30, 36, 31].\nHowever, we can also consider that the model has already been trained (with possibly all the attributes) and we may just want to apply the model with fewer available attributes, e.g., when missing values appear or when we cannot afford ‘buying’ some of the tests included in the model. It is important to say that we consider models that may have been developed by experts or by automated predictive analysis tools, or both. Re-training can be a bad choice on many occasions: when we have an expert (human-made) model, when we are using ensembles or other techniques with high training costs, when the training data is no longer available, or when the cost context changes recurrently, even for each example.\nWhat can we do instead of re-training? What we do is to reframe the original model to a situation with fewer attributes, a different feature configuration. But, how do models behave when we remove attributes from them?\nFirst, we need to clarify how we can get predictions from a model that takes m attributes when we only provide m′ < m. There are two possible ways of reframing a model in order to do this:\n1. Setting the attribute to null. Many models can just work with missing values for test instances. However, on some occasions the model cannot take null values (e.g., logistic regression is usually one of these techniques). Nonetheless, it highly depends on the implementation of the technique (or the model).\n2. Instead, we can invent or negotiate over the attribute [4]. This means that if it is a nominal attribute, we can just ask the model to give a prediction for all the possible values for the attribute, get the predictions, and calculate the most frequently predicted class. If it is a numerical attribute, we can just use a sampling or discretisation and then behave similarly. If we have information about the attribute value distribution, we can also use it, as in missing value imputation.\nThis second approach is more powerful (and related to missing value imputation and feature selection). In fact, on occasions, we may even realise that we get the same prediction for whatever value of the attribute (i.e., this is said to be a non-negotiable attribute in terms of [4]) so we can clearly save the cost of getting the value for this attribute. However, for simplicity, we will work with the first way, as using a null value works for many DM/ML techniques and libraries, without further modification of our models. In our case, it just worked smoothly with Weka [22].\nOnce we know a simple procedure to reduce the attribute set, let us analyse how models behave. Figure 1 shows the evolution of accuracy2, for all the possible subsets of the iris and the diabetes dataset (the subset lattice). Models are trained for 2/3 of the data and evaluated with the rest. We see many interesting things here. First, the general pattern is to get more accuracy as more attributes are used. But, obviously, some attributes are more important than others, leading to a sawtooth picture. Second, and more interestingly, the minimum is found at the majority-class classifier, i.e., if we are not given any information about any attribute, the best thing that we can do is to predict the majority class (or the class with lowest expected loss if misclassification costs are taken into account). Third, now surprisingly, we see that for some models and problems (Figure 1, right), the maximum is not obtained with all the attributes. In fact, it is obtained at several other places, one of them with six attributes removed (of the possible eight).\nWe can show the specific values of MC, TC and the aggregate JC for a given context of M , T and α.\n2We show accuracy, but we could show other measures such as AUC or MSE [16, 26]\nWe will first consider a ‘uniform’ operating context:\nDefinition 4. The uniform operating context θU is defined by a uniform test cost vector (1/m, 1/m, . . . , 1/m) and a uniform misclassification cost matrix ∀y1, y2 : M(y1, y2) = c/(c− 1) if y1 6= y2 and 0 otherwise. Also, α = 0.5.\nThe parameters of this context have the property that given a problem whose classes are perfectly balanced, the expected MC of a random classifier is 1 and the expected TC of a classifier using all the attributes is 1. As a consequence, JC = 1. For this context, any model with JC > 1 is clearly a model to be discarded. In fact, as a random classifier does not need to use any of the attributes, any JC > 0.5 is also discardable for this context. It is easy to see that if T is the uniform test cost vector and M is the uniform misclassification cost matrix, we have that ∑ T = 1 and ∑ M = c2. This property will be known as a context being normalised. Figure 2 shows the evolution of MC, TC and the aggregate JC for the uniform context described above. We see that the in formation shown is very similar to that evolution of Figure 1. However, for other operating contexts, things might be different. Let us see this.\nThe new (non-uniform) operating context is defined as follows for the problems “iris” and “Pima Indian diabetes”.\nThe operating context θ1 for “iris” is just the one in example 1. The operating context θ2 for “Pima Indian diabetes” is defined as a test cost vector is (2, 50, 5, 5, 20, 3, 10, 1), which means that the most expensive tests correspond to ‘plasma glucose concentration’, ‘2hour serum insulin’ and ‘diabetes pedigree function’. The misclassification cost matrix M is defined as follows:\nnegative (0) positive (1) negative (0) 0 200 positive (1) 50 0\nwhere columns represent the actual value and rows the predicted value. The value of α is 0.5. With these operating contexts, Figure 3 shows the same plots as Figure 2, with a different result."
    }, {
      "heading" : "4 The MC/TC trade-off: JROC plots",
      "text" : "The plots seen in the previous section are very informative for a given operating context. If the plots are drawn on a validation set, we will just choose the model and feature configuration that minimises the JC. However, there are some problems with the previous plots: if we have several models, the plot gets too crowded. Also, the curves are usually too sawtooth. Finally, we need to change the curves whenever we change the operating context.\nWhile some of the above problems are difficult to solve completely, most especially because we have m+ c(c−1)−1 degrees of freedom, we can see a more convenient alternative that minimises these problems. The alternative is based on a graphical visualisation of the MC/TC trade-off that we call JROC plots.\nDefinition 5. A JROC plot shows the test cost (TC) on the x−axis and misclassification cost (MC) on the y−axis.\nFigure 4 shows JROC plots for iris and Pima Indian diabetes. For iris, as it has four attributes, we see 24 × 3 points, 24 for each model. For diabetes, as it has eight attributes, we see 28 × 3 points, 28 fore each model. Those models and configurations which go closer to the bottom left corner are better than those that are placed on the top right area of the plot. There is always a point with 0 TC and a usually high misclassification cost, frequently matching the majority class model. However, as mentioned earlier on, the minimum MC is not always achieved with maximum TC. In this particular case, the minimum MC for diabetes is obtained with a TC of 0.22.\nFigure 5 shows a similar plot with different cost contexts. Here, we also see how the points are now located in different places. Even though the classifiers are the same, the distribution of the points is very different.\nIntentionally, we have not shown α on the plots, even though, according to definition 3, we cannot calculate JC unless this value is fixed. The following lemma shows that the same plot can be used to calculate JC for any value of α, with the notion of cost isometrics.\nProposition 1. Given a value of α the points which are connected by a line with slope 1−α−α have the same JC.\nProof. From definition 3 we have that JC = α ·MC + (1− α) · TC. Consequently, two points a and b have the same JC iff α ·MCa + (1− α) · TCa = α ·MCb + (1− α) · TCb. Operating with this equation, we get:\nα ·MCa + (1− α) · TCa = α ·MCb + (1− α) · TCb\nMCa + 1− α α · TCa = MCb + 1− α α · TCb\nMCa −MCb = 1− α α · (TCb − TCa) MCa −MCb TCa − TCb = 1− α −α\nAs the last expression is the change in y divided by the change in x, the expression 1−αα is the slope of this line.\nIf α = 1 only the misclassification cost matters and the slope is 0, and if α = 0 only the test cost matters and the slope is infinite. It is clear that α only represents one of the m+ c(c− 1)− 1 degrees of freedom, but it is able to consider the most important one: the relative relevance between misclassification and test costs.\nAs in classical ROC analysis, if we slide an isometric line given by a value of α from the point (0, 0) in the opposite direction (towards the top-right part of the plot), we will eventually find one point (or more) on the plot. This is the best point according to the operating condition.\nFigure 6 shows three different isometrics given by operating conditions α = 0.03, α = 0.5 and α = 0.9 and where they touch on the cloud of points. As we can see, different feature configurations and models are chosen for each operating condition.\nFinally, if we consider all possible values of α ∈ [0, 1] we see that some points are never chosen. This is exactly the notion of convex hull:\nDefinition 6. A JROC convex hull of a model is the convex hull of the set of points on the JROC space that are defined using all the attribute subsets.\nFigure 7 shows the convex hull for each of the three models. We can also see the regions of dominance. For diabetes, IBk dominates for high values of α, while the decision tree dominates for low values of α.\nFrom here we can calculate the regions of dominance for α and choose the best model accordingly, in the very same way as in ROC analysis."
    }, {
      "heading" : "5 Approximating the JROC hull",
      "text" : "The previous procedure allows us to determine the best model and configuration given the operating condition. We only need to calculate where all the points lie, compute the convex hull and find the one that corresponds for each possible α in application time. While this looks easy to do, there is one big issue. As the number of attributes increase, the number of points for a model grows exponentially: a lattice for m attributes has 2m nodes. For instance, for a model with 16 attributes, we would have 216 = 65536 points. Navigating the complete lattice of attribute subsets, and calculating their expected TC and MC would be infeasible. So we need to explore some ways to reduce the number of configurations that are evaluated, while still having a good approximation of the JROC hulls in order to do the correct decisions and get the optimal cost.\nWe will consider how to reduce the number of configurations from an exponential growth (O(2m)), given by a full method, to a quadratic growth (O(m2)). We consider four possible3 methods:\n• Backward MC-guided (BMC): we start with one case with the m attributes, we evaluate with the m cases removing one attribute, and choose the best one in terms of MC, then we evaluate the m − 1 cases removing one attribute from the previous one, and so on. This leads to exactly 1 + (m) + (m − 1) + (m− 2) + · · ·+ 1 = m(m+ 1)/2 + 1, which has an order of O(m2).\n• Backward TC-guided (BTC): as BMC but using TC instead. It has the same order and number of points.\n• Backward JC-guided (BJC): as BMC but using JC instead. It has the same order and number of points.\n• Monte Carlo (RND): a random sample over the lattice. In order to make comparison fair, we will also consider the same number of elements.\n3There would also be the forward versions as well. We rule these possibilities out here for the simplicity of exposition, and also because we think that the results would be similar, but they could also be considered in practice.\nIt is easy to show that if the misclassification cost matrix is uniform, then BTC and BJC are equivalent. If the test cost vector then BMC and BJC are equivalent. If both the misclassification cost matrix and the cost vector are uniform (i.e., the uniform operating context θU ) then BMC, BTC and BJC are equivalent.\nFigure 8 shows the results for the BMC method for our two datasets and operating contexts θ1 and θ2. Similarly, we have the results for the BTC, BJC and RND methods on figures 9, 10 and 11. If we compare with Figure 7, we see that the hulls are much worse for BTC, and notably worse for BJC and RND. However, we see that the results for BMC are good, almost identical to Figure 7. Does this observation hold in general? This (and other things) are explored in the experiments."
    }, {
      "heading" : "6 Experiments",
      "text" : "Now we are going to explore whether the JROC plots are effective, and also whether their quadratic approximation suffers from a degradation. In order to do that, we consider six datasets of the UCI repository, with number of attributes between 4 and 11, as shown in Table 1. We could not use larger datasets in this first experiment because the Full method is too slow as the number of elements to explore in the lattice grows exponentially.\nWe consider two different contexts: a uniform context θu and a random context where each value of the misclassification cost matrix and test cost vector are obtained by multiplying the original value of the\nuniform context by k, where k = eβ×(k0−0.5), k0 is obtained as a random number between 0 and 1 using a uniform distribution, and β is a factor of how irregular we want the vector and matrix to be. We set β = 10 for the following experiments. Once this function is applied, the test cost vector T and the misclassification cost matrix M are normalised such that ∑ T = 1 and ∑ M = c2.\nFor each dataset of size n, we split it into a work dataset (2n/3 of the data) and the remaining data (n/3) for test. With the work dataset, we perform a split of the work dataset into two halves. We train four models (SMO, IBk, Adaboost with J48, Bagging with J48) with the first half of the data (n/3) and calculate all the points (i.e., TC and MC) according to the full method, and the BMC, BTC, BJC and RND methods with the other half. Next, we choose 5 values of α ∈ {0.1, 0.3, 0.5, 0.7, 0.9} and determine the best configuration of model and feature subset for each of the five methods. We use the configurations given by the five methods (Full, BMC, BTC, BJC and RND) for the test set and calculate the JC.\nWe repeat the experiment 4 times. This gives us, 4 × 5 = 20 results for each of the 4 methods for each of the 6 datasets."
    }, {
      "heading" : "6.1 Uniform context",
      "text" : "First we give the results for the uniform context. Table 2 shows the mean and standard deviation of the results for each dataset and method. We see that Full cannot be improved by the other methods, as it explores all the possibilities. In general, the RND method is worse than the backward methods, except for dataset 1 (the smaller one, iris, where the number of explored configurations is 4 × 5 + 1 = 11 in front of a total of 16, which is not a big difference). In fact, for the big datasets, where the difference in explored configuration grows exponentially, we see that the backward methods get close to the Full methods, which gives support to these approximation.\nTable 3 shows the results aggregated for all datasets but showing each value of α. This means (8 datasets with 10 repetitions). In this case, we can see that the backward methods are consistently better than the RND method and are reasonable close to the Full method. The influence of α is not particularly clear, the approximation is similar for all of them.\nFinally, we want to see the whole picture and perform a statistical test. In order to assess the significance of the experimental results we will use a custom procedure, following [28] and [18, ch.12], which in turn is mostly based on [9]. Since we will not have any baseline method, we will use a Friedman test to tell whether\nthe difference between several methods is significant and then we will apply the Nemenyi post-hoc test. We agree with [21] that the Nemenyi test is a “very conservative procedure and many of the obvious differences may not be detected”, but we prefer to be conservative given our experimental setting and the use of a 0.95 confidence level. In some result tables we will show the means (even though in many cases they are not commensurate) and in some other tables we will show the average ranks (from which the Friedman and Nemenyi tests are calculated). We will also include the critical difference for the Nemenyi test, so we will be able to simply tell whether the difference between two algorithms is significant if the difference between their average ranks is greater than the critical difference.\nTable 4 shows the results of several data, where we are particularly interested in knowing which of the three backward methods is best. As we can see, the three methods behave almost equally. In fact, BMC and BJC are exactly equal, which is a consequence of the use of a uniform context."
    }, {
      "heading" : "6.2 Variable context",
      "text" : "In order to see what happens in a more realistic situation, let us see the results for the non-uniform context. Table 5 shows the mean and standard deviation of the results for each dataset and method. Here we see that not all backward methods are equivalently, but interestingly we see that BMC is now consistently better than RND for all datasets.\nAgain, Table 6 shows the results aggregated for all datasets but showing each value of α. This means (8 datasets with 10 repetitions). Now we see that the results are not especially different according to α.\nFinally, if we look at the whole picture and using a statistical test, we see in Table 7 that the backward methods are better than the RND method, but now we find difference between them. In fact, BTC is significantly better than BMC and BJC.\nAlthough some more definitive conclusions of which method is best in general would require more datasets and repetitions (although the results are significant enough here), these experiments show the potential of the backward methods."
    }, {
      "heading" : "7 Conclusion",
      "text" : "In the introduction we argued that we were looking for a flexible approach that could be used in a variety of circumstances. In fact, we were motivated by the following considerations:\n1. The method must work for any kind of predictive model, either human-made or trained from data using any off-the-shelf predictive modelling technique.\n2. Each example may have a different subset of missing values.\n3. Retraining the model for each example (using a subset of the examples with similar feature subsets) is not an option (because of 2 above or other reasons).\n4. Both misclassification cost (MC) and test cost (TC) must be considered.\nWe have presented some graphical tools and an optimisation method that meets these requirements. In fact, this has to be compared to the usual approach which is specific on decision trees, with several approaches according to [36, 31]: (a) KV, a tree is rebuilt when missing values are found, (b) Null strategy: replace by an extra label (model is not rebuilt), (c) Internal node: creates nodes for examples with missing values (model is not rebuilt), and (d) C4.5 strategy: probabilistic approach (model is not rebuilt). Option (a) is infeasible if the situation 2 holds.\nAll the above options are specific to decision trees, so they are not able to take advantage of many other off-the-shelf techniques of our preferred data mining suite or machine learning library. This is an important limitation as many of the most powerful machine learning techniques used today, such as ensemble methods (using or not decision trees as base classifiers), support vector machines, etc., are much more difficult to adapt for minimising test costs.\nThe take-away message of this paper is that we can use any machine learning technique, train a model on a dataset with the available attributes and possibly containing missing values, and reframe it for a different deployment context where we have fewer available attributes, a different distribution of missing values, a different misclassification matrix and test cost vector. While the Full approach is intractable in general, we have introduced some approximations that are just quadratic, which are feasible for hundreds of attributes, which is already a high number of attributes if we are considering test costs. Also, during all the process we can explore the performance of several models using JROC curves. In fact, these curves are not specific for\nthe methods we have introduced here; they could be used for the traditional methods used for decision trees or for the analysis of any cost context considering both MC and TC.\nThis work opens many new avenues of future work. For instance, in section 3 we discuss that an alternative to the use of missing values is the use of ranges (see bullet 2). The approach presented here could also be compared or explored in combination to the mimetic technique to get models that use fewer attributes [17, 12, 6, 7, 5]. Another interesting idea would be the problem of quantification with test costs, which could be applied to both classification and regression [2, 3]. We have also been suggested4 to use decision stump ensembles, where the elements in the ensemble could be pruned a posteriori when the test cost is known.\nMore comparison with the area of feature selection could lead to a better understanding of the possibilities of reframing and better methods. For instance, the use of the attribute correlation can be used to an approximate notion of dominance (e.g., if two attributes have high correlation, the cost is expected to be related to the lowest test cost for any of them). As for the relation to other problems, we could also consider that the output domain may be null, as in abstaining classifiers [15, 33] and the notion of delegation [14] could be applied to this case. In fact, a missing value on purpose can be seen as the parallel of a reject option or abstention for the output value.\nThe notion of JROC curve could be further explored and extended. For instance, we could figure out other ways of drawing these curves, by using attribute correlation or some other order on the attributes. The issue of representing operating conditions when the the matrix and vector are not fixed could lead to more dimensions, or the inclusion of the cost matrix. At least in the case of binary classifiers we could have 3D surfaces, using, e.g., TPR, FPR and TC. As for any curve representing cost for each operating condition, we wonder whether the area over the JROC curve means something, as in ROC analysis [8, 20]. Also we could ask the question of whether we can draw cost plots as in [10, 25, 27].\nFinally, there are more more ambitious ideas. We could investigate which attributes to use for each example. We could use reliability measures (especially in probabilistic classifiers) to make better decisions on whether to remove an attribute or not. We could analyse what to do when new attributes appear, using, e.g., the correlation to other attributes to derive the old attributes, or thinking about more general ways of representing the feature space. Finally, we think that there is no reason why most of the ideas introduced here could not work equally well for regression, combining the test cost with any regression loss."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This work was supported by the MEC/MINECO projects CONSOLIDER-INGENIO CSD2007-00022 and TIN 2010- 21062-C02-02, GVA project Prometeo/2008/051, the COST - European Cooperation in the field of Scientific and Technical Research IC0801 AT, and the REFRAME project granted by the European Coordinated Research on Long-term Challenges in Information and Communication Sciences & Technologies ERA-Net (CHIST-ERA), and funded by the respective national research councils and ministries."
    } ],
    "references" : [ {
      "title" : "Quantification via probability estimators",
      "author" : [ "A. Bella", "C. Ferri", "J. Hernández-Orallo", "M.J. Ramı́rez-Quintana" ],
      "venue" : "In 2010 IEEE International Conference on Data Mining,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2010
    }, {
      "title" : "Aggregative quantification for regression",
      "author" : [ "A. Bella", "C. Ferri", "J. Hernández-Orallo", "M.J. Ramı́rez-Quintana" ],
      "venue" : "Data Mining and Knowledge Discovery,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2013
    }, {
      "title" : "Ramı́rez-Quintana. Using negotiable features for prescription problems",
      "author" : [ "A. Bella", "C. Ferri", "J. Hernández-Orallo", "M.J" ],
      "venue" : null,
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2011
    }, {
      "title" : "Ramı́rez-Quintana. Estimating the class probability threshold without training data",
      "author" : [ "R. Blanco-Vega", "C. Ferri-Ramı́rez", "J. Hernández-Orallo", "M.J" ],
      "venue" : "ROC Analysis in Machine Learning, ICML2006 workshop,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2006
    }, {
      "title" : "Analysing the trade-off between comprehensibility and accuracy in mimetic models",
      "author" : [ "R. Blanco-Vega", "J. Hernández-Orallo", "M.J. Ramı́rez-Quintana" ],
      "venue" : "In Discovery Science, 7th International Conference,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2004
    }, {
      "title" : "Knowledge acquisition through machine learning: minimising expert’s effort",
      "author" : [ "R. Blanco-Vega", "J. Hernández-Orallo", "M.J. Ramı́rez-Quintana" ],
      "venue" : "Fourth International Conference on Machine Learning and Applications,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2005
    }, {
      "title" : "The use of the area under the ROC curve in the evaluation of machine learning algorithms",
      "author" : [ "A.P. Bradley" ],
      "venue" : "Pattern Recognition,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1997
    }, {
      "title" : "Statistical comparisons of classifiers over multiple data sets",
      "author" : [ "J. Demšar" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2006
    }, {
      "title" : "Cost Curves: An Improved Method for Visualizing Classifier Performance",
      "author" : [ "C. Drummond", "R.C. Holte" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2006
    }, {
      "title" : "The foundations of cost-sensitive learning",
      "author" : [ "C. Elkan" ],
      "venue" : "In International Joint Conference on Artificial Intelligence,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2001
    }, {
      "title" : "Simple mimetic classifiers",
      "author" : [ "V. Estruch", "C. Ferri", "J. Hernandez-Orallo", "M. Ramirez-Quintana" ],
      "venue" : "Machine Learning and Data Mining in Pattern Recognition,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2003
    }, {
      "title" : "An introduction to ROC analysis",
      "author" : [ "T. Fawcett" ],
      "venue" : "Pattern Recognition Letters,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2006
    }, {
      "title" : "Delegating classifiers",
      "author" : [ "C. Ferri", "P.A. Flach", "J. Hernández-Orallo" ],
      "venue" : "Machine Learning, Proceedings of the Twenty-first International Conference (ICML",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2004
    }, {
      "title" : "Cautious classifiers",
      "author" : [ "C. Ferri", "J. Hernández-Orallo" ],
      "venue" : "Proceedings of the 1st International Workshop on ROC Analysis in Artificial Intelligence",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2004
    }, {
      "title" : "An experimental comparison of performance measures for classification",
      "author" : [ "C. Ferri", "J. Hernández-Orallo", "R. Modroiu" ],
      "venue" : "Pattern Recognition Let.,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2009
    }, {
      "title" : "Ramı́rez-Quintana. From ensemble methods to comprehensible models",
      "author" : [ "C. Ferri", "J. Hernández-Orallo" ],
      "venue" : "Discovery Science,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2002
    }, {
      "title" : "Machine Learning: The Art and Science of Algorithms that Make Sense of Data",
      "author" : [ "P. Flach" ],
      "venue" : null,
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2012
    }, {
      "title" : "Decision support for data mining",
      "author" : [ "P. Flach", "H. Blockeel", "C. Ferri", "J. Hernández-Orallo", "J. Struyf" ],
      "venue" : "Data Mining and Decision Support,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2003
    }, {
      "title" : "A coherent interpretation of AUC as a measure of aggregated classification performance",
      "author" : [ "P. Flach", "J. Hernández-Orallo", "C. Ferri" ],
      "venue" : "In Proceedings of the 28th International Conference on Machine Learning,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2011
    }, {
      "title" : "An extension on statistical comparisons of classifiers over multiple data sets for all pairwise comparisons",
      "author" : [ "S. Garćıa", "F. Herrera" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2008
    }, {
      "title" : "The weka data mining software: an update",
      "author" : [ "M. Hall", "E. Frank", "G. Holmes", "B. Pfahringer", "P. Reutemann", "I.H. Witten" ],
      "venue" : "ACM SIGKDD Explorations Newsletter,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2009
    }, {
      "title" : "A graphical analysis of cost-sensitive regression problems",
      "author" : [ "J. Hernández-Orallo" ],
      "venue" : null,
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2012
    }, {
      "title" : "The 1st workshop on ROC analysis in artificial intelligence (ROCAI-2004)",
      "author" : [ "J. Hernández-Orallo", "C. Ferri", "N. Lachiche", "P. Flach" ],
      "venue" : "ACM SIGKDD Explorations Newsletter,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2004
    }, {
      "title" : "Brier curves: a new cost-based visualisation of classifier performance",
      "author" : [ "J. Hernández-Orallo", "P. Flach", "C. Ferri" ],
      "venue" : "In Proceedings of the 28th International Conference on Machine Learning,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2011
    }, {
      "title" : "A unified view of performance metrics: Translating threshold choice into expected classification loss",
      "author" : [ "J. Hernández-Orallo", "P. Flach", "C. Ferri" ],
      "venue" : "Journal of Machine Learning Research (JMLR),",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2012
    }, {
      "title" : "ROC curves in cost space",
      "author" : [ "J. Hernández-Orallo", "P. Flach", "C. Ferri" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2013
    }, {
      "title" : "Evaluating Learning Algorithms: A Classification Perspective",
      "author" : [ "N. Japkowicz", "M. Shah" ],
      "venue" : "Cambridge Univ Pr,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2011
    }, {
      "title" : "Feature space theory - a mathematical foundation for data mining",
      "author" : [ "H.X. Li", "L.D. Xu" ],
      "venue" : "Knowledgebased systems,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2001
    }, {
      "title" : "Decision trees with minimal costs",
      "author" : [ "C.X. Ling", "Q. Yang", "J. Wang", "S. Zhang" ],
      "venue" : "In Proceedings of the twenty-first international conference on Machine learning,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2004
    }, {
      "title" : "A survey of cost-sensitive decision tree induction algorithms",
      "author" : [ "S. Lomax", "S.l Vadera" ],
      "venue" : "ACM Computing Surveys (CSUR),",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2013
    }, {
      "title" : "Selecting features in microarray classification using ROC curves",
      "author" : [ "H. Mamitsuka" ],
      "venue" : "Pattern Recognition,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2006
    }, {
      "title" : "Optimizing abstaining classifiers using ROC analysis",
      "author" : [ "T. Pietraszek" ],
      "venue" : "In Proceedings of the 22nd international conference on Machine learning,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2005
    }, {
      "title" : "Better decisions through science",
      "author" : [ "J.A. Swets", "R.M. Dawes", "J. Monahan" ],
      "venue" : "Scientific American,",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2000
    }, {
      "title" : "Types of cost in inductive concept learning",
      "author" : [ "P. Turney" ],
      "venue" : "Canada National Research Council Publications Archive,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 2000
    }, {
      "title" : "Missing is useful: missing values in cost-sensitive decision trees",
      "author" : [ "S. Zhang", "Z. Qin", "C.X. Ling", "S. Sheng" ],
      "venue" : "IEEE transactions on knowledge and data engineering,",
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 2005
    }, {
      "title" : "Missing value estimation for mixed-attribute data sets. Knowledge and Data Engineering",
      "author" : [ "X. Zhu", "S. Zhang", "Z. Jin", "Z. Zhang", "Z. Xu" ],
      "venue" : "IEEE Transactions on,",
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 27,
      "context" : "1 Introduction The feature space (including both input and output variables) characterises a data mining problem [29].",
      "startOffset" : 113,
      "endOffset" : 117
    }, {
      "referenceID" : 33,
      "context" : "This frequently represents a utility or cost-sensitive learning dilemma [35, 11] between misclassification (or regression error) costs and tests costs, both being integrated into a joint cost.",
      "startOffset" : 72,
      "endOffset" : 80
    }, {
      "referenceID" : 9,
      "context" : "This frequently represents a utility or cost-sensitive learning dilemma [35, 11] between misclassification (or regression error) costs and tests costs, both being integrated into a joint cost.",
      "startOffset" : 72,
      "endOffset" : 80
    }, {
      "referenceID" : 35,
      "context" : "One possible option is known as missing value imputation [37], but this approach is not usually appropriate when test costs are considered.",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 34,
      "context" : "First, imputing missing values “is regarded as unnecessary for cost-sensitive learning that also considers the test costs” [36].",
      "startOffset" : 123,
      "endOffset" : 127
    }, {
      "referenceID" : 28,
      "context" : "Decision trees are the usual choice [30, 36, 31] because the use of attributes can be customised in many different ways.",
      "startOffset" : 36,
      "endOffset" : 48
    }, {
      "referenceID" : 34,
      "context" : "Decision trees are the usual choice [30, 36, 31] because the use of attributes can be customised in many different ways.",
      "startOffset" : 36,
      "endOffset" : 48
    }, {
      "referenceID" : 29,
      "context" : "Decision trees are the usual choice [30, 36, 31] because the use of attributes can be customised in many different ways.",
      "startOffset" : 36,
      "endOffset" : 48
    }, {
      "referenceID" : 32,
      "context" : "This is exactly the way ROC analysis works (for classification [34, 19, 24, 13, 32] and for regression [23]).",
      "startOffset" : 63,
      "endOffset" : 83
    }, {
      "referenceID" : 17,
      "context" : "This is exactly the way ROC analysis works (for classification [34, 19, 24, 13, 32] and for regression [23]).",
      "startOffset" : 63,
      "endOffset" : 83
    }, {
      "referenceID" : 22,
      "context" : "This is exactly the way ROC analysis works (for classification [34, 19, 24, 13, 32] and for regression [23]).",
      "startOffset" : 63,
      "endOffset" : 83
    }, {
      "referenceID" : 11,
      "context" : "This is exactly the way ROC analysis works (for classification [34, 19, 24, 13, 32] and for regression [23]).",
      "startOffset" : 63,
      "endOffset" : 83
    }, {
      "referenceID" : 30,
      "context" : "This is exactly the way ROC analysis works (for classification [34, 19, 24, 13, 32] and for regression [23]).",
      "startOffset" : 63,
      "endOffset" : 83
    }, {
      "referenceID" : 21,
      "context" : "This is exactly the way ROC analysis works (for classification [34, 19, 24, 13, 32] and for regression [23]).",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 20,
      "context" : "Throughout the paper we will use several classifiers from Weka [22].",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 32,
      "context" : "Reuse of learned models is of critical importance in the majority of knowledgeintensive application areas, particularly because the operating context can be expected to vary from training to deployment and we need to make the best decision according to that context [34, 19].",
      "startOffset" : 266,
      "endOffset" : 274
    }, {
      "referenceID" : 17,
      "context" : "Reuse of learned models is of critical importance in the majority of knowledgeintensive application areas, particularly because the operating context can be expected to vary from training to deployment and we need to make the best decision according to that context [34, 19].",
      "startOffset" : 266,
      "endOffset" : 274
    }, {
      "referenceID" : 33,
      "context" : "In fact, as Turney [35] points out, we can only rationally determine whether it is worthwhile to pay the cost of test when we know the cost of misclassification errors.",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 28,
      "context" : "Also, if we think about costs, most works on minimising costs have taken this approach [30, 36, 31].",
      "startOffset" : 87,
      "endOffset" : 99
    }, {
      "referenceID" : 34,
      "context" : "Also, if we think about costs, most works on minimising costs have taken this approach [30, 36, 31].",
      "startOffset" : 87,
      "endOffset" : 99
    }, {
      "referenceID" : 29,
      "context" : "Also, if we think about costs, most works on minimising costs have taken this approach [30, 36, 31].",
      "startOffset" : 87,
      "endOffset" : 99
    }, {
      "referenceID" : 2,
      "context" : "Instead, we can invent or negotiate over the attribute [4].",
      "startOffset" : 55,
      "endOffset" : 58
    }, {
      "referenceID" : 2,
      "context" : ", this is said to be a non-negotiable attribute in terms of [4]) so we can clearly save the cost of getting the value for this attribute.",
      "startOffset" : 60,
      "endOffset" : 63
    }, {
      "referenceID" : 20,
      "context" : "In our case, it just worked smoothly with Weka [22].",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 14,
      "context" : "2We show accuracy, but we could show other measures such as AUC or MSE [16, 26] 5",
      "startOffset" : 71,
      "endOffset" : 79
    }, {
      "referenceID" : 24,
      "context" : "2We show accuracy, but we could show other measures such as AUC or MSE [16, 26] 5",
      "startOffset" : 71,
      "endOffset" : 79
    }, {
      "referenceID" : 26,
      "context" : "In order to assess the significance of the experimental results we will use a custom procedure, following [28] and [18, ch.",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 7,
      "context" : "12], which in turn is mostly based on [9].",
      "startOffset" : 38,
      "endOffset" : 41
    }, {
      "referenceID" : 19,
      "context" : "We agree with [21] that the Nemenyi test is a “very conservative procedure and many of the obvious differences may not be detected”, but we prefer to be conservative given our experimental setting and the use of a 0.",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 34,
      "context" : "In fact, this has to be compared to the usual approach which is specific on decision trees, with several approaches according to [36, 31]: (a) KV, a tree is rebuilt when missing values are found, (b) Null strategy: replace by an extra label (model is not rebuilt), (c) Internal node: creates nodes for examples with missing values (model is not rebuilt), and (d) C4.",
      "startOffset" : 129,
      "endOffset" : 137
    }, {
      "referenceID" : 29,
      "context" : "In fact, this has to be compared to the usual approach which is specific on decision trees, with several approaches according to [36, 31]: (a) KV, a tree is rebuilt when missing values are found, (b) Null strategy: replace by an extra label (model is not rebuilt), (c) Internal node: creates nodes for examples with missing values (model is not rebuilt), and (d) C4.",
      "startOffset" : 129,
      "endOffset" : 137
    } ],
    "year" : 2013,
    "abstractText" : "Many solutions to cost-sensitive classification (and regression) rely on some or all of the following assumptions: we have complete knowledge about the cost context at training time, we can easily re-train whenever the cost context changes, and we have technique-specific methods (such as cost-sensitive decision trees) that can take advantage of that information. In this paper we address the problem of selecting models and minimising joint cost (integrating both misclassification cost and test costs) without any of the above assumptions. We introduce methods and plots (such as the so-called JROC plots) that can work with any off-the-shelf predictive technique, including ensembles, such that we reframe the model to use the appropriate subset of attributes (the feature configuration) during deployment time. In other words, models are trained with the available attributes (once and for all) and then deployed by setting missing values on the attributes that are deemed ineffective for reducing the joint cost. As the number of feature configuration combinations grows exponentially with the number of features we introduce quadratic methods that are able to approximate the optimal configuration and model choices, as shown by the experimental results.",
    "creator" : "LaTeX with hyperref package"
  }
}