{
  "name" : "1605.06676.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Learning to Communicate with Deep Multi-Agent Reinforcement Learning",
    "authors" : [ "Jakob N. Foerster", "Yannis M. Assael", "Shimon Whiteson" ],
    "emails" : [ "jakob.foerster@cs.ox.ac.uk", "yannis.assael@cs.ox.ac.uk", "nandodefreitas@google.com", "shimon.whiteson@cs.ox.ac.uk" ],
    "sections" : [ {
      "heading" : null,
      "text" : "We consider the problem of multiple agents sensing and acting in environments with the goal of maximising their shared utility. In these environments, agents must learn communication protocols in order to share information that is needed to solve the tasks. By embracing deep neural networks, we are able to demonstrate endto-end learning of protocols in complex environments inspired by communication riddles and multi-agent computer vision problems with partial observability. We propose two approaches for learning in these domains: Reinforced Inter-Agent Learning (RIAL) and Differentiable Inter-Agent Learning (DIAL). The former uses deep Q-learning, while the latter exploits the fact that, during learning, agents can backpropagate error derivatives through (noisy) communication channels. Hence, this approach uses centralised learning but decentralised execution. Our experiments introduce new environments for studying the learning of communication protocols and present a set of engineering innovations that are essential for success in these domains."
    }, {
      "heading" : "1 Introduction",
      "text" : "How language and communication emerge among intelligent agents has long been a topic of intense debate. Among the many unresolved questions are: Why does language use discrete structures? What role does the environment play? What is innate and what is learned? And so on. Some of the debates on these questions have been so fiery that in 1866 the French Academy of Sciences banned publications about the origin of human language.\nThe rapid progress in recent years of machine learning, and deep learning in particular, opens the door to a new perspective on this debate. How can agents use machine learning to automatically discover the communication protocols they need to coordinate their behaviour? What, if anything, can deep learning offer to such agents? What insights can we glean from the success or failure of agents that learn to communicate?\nIn this paper, we take the first steps towards answering these questions. Our approach is programmatic: first, we propose a set of multi-agent benchmark tasks that require communication; then, we formulate several learning algorithms for these tasks; finally, we analyse how these algorithms learn, or fail to learn, communication protocols for the agents.\n†These authors contributed equally to this work.\nar X\niv :1\n60 5.\n06 67\n6v 2\n[ cs\n.A I]\n2 4\nThe tasks that we consider are fully cooperative, partially observable, sequential multi-agent decision making problems. All the agents share the goal of maximising the same discounted sum of rewards. While no agent can observe the underlying Markov state, each agent receives a private observation correlated with that state. In addition to taking actions that affect the environment, each agent can also communicate with its fellow agents via a discrete limited-bandwidth channel. Due to the partial observability and limited channel capacity, the agents must discover a communication protocol that enables them to coordinate their behaviour and solve the task.\nWe focus on settings with centralised learning but decentralised execution. In other words, communication between agents is not restricted during learning, which is performed by a centralised algorithm; however, during execution of the learned policies, the agents can communicate only via the limited-bandwidth channel. While not all real-world problems can be solved in this way, a great many can, e.g., when training a group of robots on a simulator. Centralised planning and decentralised execution is also a standard paradigm for multi-agent planning [1]. For completeness, we also provide decentralised learning baselines.\nTo address these tasks, we formulate two approaches. The first, named reinforced inter-agent learning (RIAL), uses deep Q-learning [2] with a recurrent network to address partial observability. In one variant of this approach, which we refer to as independent Q-learning, the agents each learn their own network parameters, treating the other agents as part of the environment. Another variant trains a single network whose parameters are shared among all agents. Execution remains decentralised, at which point they receive different observations leading to different behaviour.\nThe second approach, which we call differentiable inter-agent learning (DIAL), is based on the insight that centralised learning affords more opportunities to improve learning than just parameter sharing. In particular, while RIAL is end-to-end trainable within an agent, it is not end-to-end trainable across agents, i.e., no gradients are passed between agents. The second approach allows realvalued messages to pass between agents during centralised learning, thereby treating communication actions as bottleneck connections between agents. As a result, gradients can be pushed through the communication channel, yielding a system that is end-to-end trainable even across agents. During decentralised execution, real-valued messages are discretised and mapped to the discrete set of communication actions allowed by the task. Because DIAL passes gradients from agent to agent, it is an inherently deep learning approach.\nOur empirical study shows that these methods can solve our benchmark tasks, often discovering elegant communication protocols along the way. To our knowledge, this is the first time that either differentiable communication or reinforcement learning (RL) with deep neural networks have succeeded in learning communication protocols in complex environments involving sequences and raw input images1. The results also show that deep learning, by better exploiting the opportunities of centralised learning, is a uniquely powerful tool for learning communication protocols. Finally, this study advances several engineering innovations, outlined in the experimental section, that are essential for learning communication protocols in our proposed benchmarks."
    }, {
      "heading" : "2 Related Work",
      "text" : "Research on communication spans many fields, e.g. linguistics, psychology, evolution and AI. In AI, it is split along a few axes: a) predefined or learned communication protocols, b) planning or learning methods, c) evolution or RL, and d) cooperative or competitive settings.\nGiven the topic of our paper, we focus on related work that deals with the cooperative learning of communication protocols. Out of the plethora of work on multi-agent RL with communication, e.g., [4–8], only a few fall into this category. Most assume a pre-defined communication protocol, rather than trying to learn protocols. One exception is the work of Kasai et al. [8], in which tabular Q-learning agents have to learn the content of a message to solve a predator-prey task with communication. Another example of open-ended communication learning in a multi-agent task is given in [9]. Here evolutionary methods are used for learning the protocols which are evaluated on a similar predator-prey task. Their approach uses a fitness function that is carefully designed to accelerate learning. In general, heuristics and handcrafted rules have prevailed widely in this line of research. Moreover, typical tasks have been necessarily small so that global optimisation methods,\n1This paper extends our own work in [3], which proposes a variation of DQN for protocol learning.\nsuch as evolutionary algorithms, can be applied. The use of deep representations and gradientbased optimisation as advocated in this paper is an important departure, essential for scalability and further progress. A similar rationale is provided in [10], another example of making an RL problem end-to-end differentiable.\nFinally, we consider discrete communication channels. One of the key components of our methods is the signal binarisation during the decentralised execution. This is related to recent research on fitting neural networks in low-powered devices with memory and computational limitations using binary weights, e.g. [11], and previous works on discovering binary codes for documents [12]."
    }, {
      "heading" : "3 Background",
      "text" : "Deep Q-Networks (DQN). In a single-agent, fully-observable, RL setting [13], an agent observes the current state st ∈ S at each discrete time step t, chooses an action ut ∈ U according to a potentially stochastic policy π, observes a reward signal rt, and transitions to a new state st+1. Its objective is to maximise an expectation over the discounted return, Rt = rt + γrt+1 + γ2rt+2 + · · · , where rt is the reward received at time t and γ ∈ [0, 1] is a discount factor. The Q-function of a policy π is Qπ(s, u) = E [Rt|st = s, ut = u]. The optimal action-value function Q∗(s, u) = maxπ Qπ(s, u) obeys the Bellman optimality equation Q∗(s, u) = Es′ [r + γmaxu′ Q∗(s′, u′) | s, u]. Deep Qlearning [2] uses neural networks parameterised by θ to represent Q(s, u; θ). DQNs are optimised by minimising: Li(θi) = Es,u,r,s′ [(yDQNi −Q(s, u; θi))2], at each iteration i, with target y DQN i = r+γmaxu′ Q(s ′, u′; θ−i ). Here, θ − i are the parameters of a target network that is frozen for a number of iterations while updating the online network Q(s, u; θi). The action u is chosen from Q(s, u; θi) by an action selector, which typically implements an -greedy policy that selects the action that maximises the Q-value with a probability of 1 − and chooses randomly with a probability of . DQN uses experience replay: during learning, the agent builds a dataset of episodic experiences and is then trained by sampling mini-batches of experiences.\nIndependent DQN. DQN has been extended to cooperative multi-agent settings, in which each agent a observes the global st, selects an individual action uat , and receives a team reward, rt, shared among all agents. Tampuu et al. [14] address this setting with a framework that combines DQN with independent Q-learning, in which each agent a independently and simultaneously learns its own Q-function Qa(s, ua; θai ). While independent Q-learning can in principle lead to convergence problems (since one agent’s learning makes the environment appear non-stationary to other agents), it has a strong empirical track record [15, 16], and was successfully applied to two-player pong.\nDeep Recurrent Q-Networks. Both DQN and independent DQN assume full observability, i.e., the agent receives st as input. By contrast, in partially observable environments, st is hidden and the agent receives only an observation ot that is correlated with st, but in general does not disambiguate it. Hausknecht and Stone [17], propose the deep recurrent Q-networks to address single-agent, partially observable settings. Instead of approximating Q(s, u) with a feed-forward network, they approximate Q(o, u) with a recurrent neural network that can maintain an internal state and aggregate observations over time. This can be modelled by adding an extra input ht−1 that represents the hidden state of the network, yielding Q(ot, ht−1, u). For notational simplicity, we omit the dependence of Q on θ."
    }, {
      "heading" : "4 Setting",
      "text" : "In this work, we consider RL problems with both multiple agents and partial observability. All the agents share the goal of maximising the same discounted sum of rewards Rt. While no agent can observe the underlying Markov state st, each agent receives a private observation oat correlated to st. In each time-step, the agents select an environment action u ∈ U that affects the environment, and a communication action m ∈M that is observed by other agents but has no direct impact on the environment or reward. We are interested in such settings because it is only when multiple agents and partial observability coexist that agents have the incentive to communicate. As no communication protocol is given a priori, the agents must develop and agree upon such a protocol to solve the task.\nSince protocols are mappings from action-observation histories to sequences of messages, the space of protocols is extremely high-dimensional. Automatically discovering effective protocols in this space remains an elusive challenge. In particular, the difficulty of exploring this space of protocols is exacerbated by the need for agents to coordinate the sending and interpreting of messages. For\nexample, if one agent sends a useful message to another agent, it will only receive a positive reward if the receiving agent correctly interprets and acts upon that message. If it does not, the sender will be discouraged from sending that message again. Hence, positive rewards are sparse, arising only when sending and interpreting are properly coordinated, which is hard to discover via random exploration.\nWe focus on settings with centralised learning but decentralised execution. In other words, communication between agents is not restricted during learning, which is performed by a centralised algorithm; however, during execution of the learned policies, the agents can communicate only via the limited-bandwidth channel. While not all real-world problems can be solved in this way, a great many can, e.g., when training a group of robots on a simulator. Centralised planning and decentralised execution is also a standard paradigm for multi-agent planning [1, 18]."
    }, {
      "heading" : "5 Methods",
      "text" : "In this section, we present two approaches for learning communication protocols."
    }, {
      "heading" : "5.1 Reinforced Inter-Agent Learning",
      "text" : "The most straightforward approach, which we call reinforced inter-agent learning (RIAL), is to combine DRQN with independent Q-learning for action and communication selection. Each agent’s Q-network represents Qa(oat ,m a′ t−1, h a t−1, u\na), which conditions on that agent’s individual hidden state and observation. Here and throughout a is the index of agent a.\nTo avoid needing a network with |U ||M | outputs, we split the network into Qau and Qam, the Q-values for the environment and communication actions, respectively. Similarly to [19], the action selector separately picks uat and m a t from Qu and Qm, using an -greedy policy. Hence, the network requires only |U |+ |M | outputs and action selection requires maximising over U and then over M , but not maximising over U ×M . BothQu andQm are trained using DQN with the following two modifications, which were found to be essential for performance. First, we disable experience replay to account for the non-stationarity that occurs when multiple agents learn concurrently, as it can render experience obsolete and misleading. Second, to account for partial observability, we feed in the actions u and m taken by each agent as inputs on the next time-step. Figure 1(a) shows how information flows between agents and the environment, and how Q-values are processed by the action selector in order to produce the action, uat , and message m a t . Since this approach treats agents as independent networks, the learning phase is not centralised, even though our problem setting allows it to be. Consequently, the agents are treated exactly the same way during decentralised execution as during learning.\nParameter Sharing. RIAL can be extended to take advantage of the opportunity for centralised learning by sharing parameters among the agents. This variation learns only one network, which is\nused by all agents. However, the agents can still behave differently because they receive different observations and thus evolve different hidden states. In addition, each agent receives its own index a as input, allowing them to specialise. The rich representations in deep Q-networks can facilitate the learning of a common policy while also allowing for specialisation. Parameter sharing also dramatically reduces the number of parameters that must be learned, thereby speeding learning. Under parameter sharing, the agents learn two Q-functions Qu(oat ,m a′ t−1, h a t−1, u a t−1,m a t−1, a, u a t ) and Qm(·), for u and m, respectively, where uat−1 and mat−1 are the last action inputs and ma ′\nt−1 are messages from other agents. During decentralised execution, each agent uses its own copy of the learned network, evolving its own hidden state, selecting its own actions, and communicating with other agents only through the communication channel."
    }, {
      "heading" : "5.2 Differentiable Inter-Agent Learning",
      "text" : "While RIAL can share parameters among agents, it still does not take full advantage of centralised learning. In particular, the agents do not give each other feedback about their communication actions. Contrast this with human communication, which is rich with tight feedback loops. For example, during face-to-face interaction, listeners send fast nonverbal queues to the speaker indicating the level of understanding and interest. RIAL lacks this feedback mechanism, which is intuitively important for learning communication protocols.\nTo address this limitation, we propose differentiable inter-agent learning (DIAL). The main insight behind DIAL is that the combination of centralised learning and Q-networks makes it possible, not only to share parameters but to push gradients from one agent to another through the communication channel. Thus, while RIAL is end-to-end trainable within each agent, DIAL is end-to-end trainable across agents. Letting gradients flow from one agent to another gives them richer feedback, reducing the required amount of learning by trial and error, and easing the discovery of effective protocols.\nDIAL works as follows: during centralised learning, communication actions are replaced with direct connections between the output of one agent’s network and the input of another’s. Thus, while the task restricts communication to discrete messages, during learning the agents are free to send real-valued messages to each other. Since these messages function as any other network activation, gradients can be passed back along the channel, allowing end-to-end backpropagation across agents.\nIn particular, the network, which we call a C-Net, outputs two distinct types of values, as shown in Figure 1(b), a) Q(·), the Q-values for the environment actions, which are fed to the action selector, and b) mat , the real-valued message to other agents, which bypasses the action selector and is instead processed by the discretise/regularise unit (DRU(mat )). The DRU regularises it during centralised learning, DRU(mat ) = Logistic(N (mat , σ)), and discretises it during decentralised execution, DRU(mat ) = 1{mat > 0}, where σ is the standard deviation of the noise added to the channel. Figure 1 shows how gradients flow differently in RIAL and DIAL. The gradient chains for Qu, in RIAL and Q, in DIAL, are based on the DQN loss. However, in DIAL the gradient term for m is the backpropagated error from the recipient of the message to the sender. Using this inter-agent gradient for training provides a richer training signal than the DQN loss for Qm in RIAL. While the DQN error is nonzero only for the selected message, the incoming gradient is a |m|-dimensional vector that can contain more information, here |m| is the length of m. It also allows the network to directly adjust messages in order to minimise the downstream DQN loss, reducing the need for trial and error exploration to learn good protocols.\nWhile we limit our analysis to discrete messages, DIAL naturally handles continuous protocols, as they are part of the differentiable training. While we limit our analysis to discrete messages, DIAL naturally handles continuous message spaces, as they are used anyway during centralised learning. DIAL can also scale naturally to large discrete message spaces, since it learns binary encodings instead of the one-hot encoding in RIAL, |m| = O(log(|M |). Further algorithmic details and pseudocode are in Appendix A."
    }, {
      "heading" : "6 Experiments",
      "text" : "In this section, we evaluate RIAL and DIAL with and without parameter sharing in two multi-agent problems and compare it with a no-communication shared parameters baseline (NoComm). Results presented are the average performance across several runs, where those without parameter sharing (-\nNS), are represented by dashed lines. Across plots, rewards are normalised by the highest average reward achievable given access to the true state (Oracle).\nIn our experiments, we use an -greedy policy with = 0.05, the discount factor is γ = 1, and the target network is reset every 100 episodes. To stabilise learning, we execute parallel episodes in batches of 32. The parameters are optimised using RMSProp with momentum of 0.95 and a learning rate of 5× 10−4. The architecture makes use of rectified linear units (ReLU), and gated recurrent units (GRU) [20], which have similar performance to long short-term memory [21] (LSTM) [22, 23]. Unless stated otherwise we set σ = 2, which was found to be essential for good performance. We intent to published the source code online.\n6.1 Model Architecture\nRIAL and DIAL share the same individual model architecture. For brevity, we describe only the DIAL model here. As illustrated in Figure 2, each agent consists of a recurrent neural network (RNN), unrolled for T time-steps, that maintains an internal state h, an input network for producing a task embedding z, and an output network for the Q-values and the messages m. The input for agent a is defined as a tuple of (oat ,m a′ t−1, u a t−1, a). The inputs a and uat−1 are passed through lookup tables, and m a′\nt−1 through a 1-layer MLP, both producing embeddings of size 128. oat is processed through a task-specific network that produces an additional embedding of the same size. The state embedding is produced by element-wise summation of these embeddings, zat = (\nTaskMLP(oat ) + MLP[|M |, 128](mt−1) + Lookup(uat−1) + Lookup(a)\n) .\nWe found that performance and stability improved when a batch normalisation layer [24] was used to preprocess mt−1. zat is processed through a 2-layer RNN with GRUs, h a 1,t = GRU[128, 128](zat , h a 1,t−1), which is used to approximate the agent’s action-observation history. Finally, the output ha2,t of the top GRU layer, is passed through a 2-layer MLP Q a t ,m a t = MLP[128, 128, (|U |+ |M |)](ha2,t).\n6.2 Switch Riddle\nThe first task is inspired by a well-known riddle described as follows: “One hundred prisoners have been newly ushered into prison. The warden tells them that starting tomorrow, each of them will be placed in an isolated cell, unable to communicate amongst each other. Each day, the warden will choose one of the prisoners uniformly at random with replacement, and place him in a central interrogation room containing only a light bulb with a toggle switch. The prisoner will be able to observe the current state of the light bulb. If he wishes, he can toggle\nthe light bulb. He also has the option of announcing that he believes all prisoners have visited the interrogation room at some point in time. If this announcement is true, then all prisoners are set free, but if it is false, all prisoners are executed. The warden leaves and the prisoners huddle together to discuss their fate. Can they agree on a protocol that will guarantee their freedom?” [25].\nArchitecture. In our formalisation, at time-step t, agent a observes oat ∈ 0, 1, which indicates if the agent is in the interrogation room. Since the switch has two positions, it can be modelled as a 1-bit message, mat . If agent a is in the interrogation room, then its actions are u a t ∈ {“None”,“Tell”}; otherwise the only action is “None”. The episode ends when an agent chooses “Tell” or when the maximum time-step, T , is reached. The reward rt is 0 unless an agent chooses “Tell”, in which case it is 1 if all agents have been to the interrogation room and −1 otherwise. Following the riddle definition, in this experiment mat−1 is available only to the agent a in the interrogation room. Finally, we set the time horizon T = 4n− 6 in order to keep the experiments computationally tractable.\nComplexity. The switch riddle poses significant protocol learning challenges. At any time-step t, there are |o|t possible observation histories for a given agent, with |o| = 3: the agent either is not in the interrogation room or receives one of two messages when he is. For each of these histories, an agent can chose between 4 = |U ||M | different options, so at time-step t, the single-agent policy space is (|U ||M |)|o| t = 43 t\n. The product of all policies for all time-steps defines the total policy space for an agent: ∏ 43 t = 4(3 T+1−3)/2, where T is the final time-step. The size of the multi-agent policy space grows exponentially in n, the number of agents: 4n(3 T+1−3)/2. We consider a setting where T is proportional to the number of agents, so the total policy space is 4n3 O(n)\n. For n = 4, the size is 488572. Our approach using DIAL is to model the switch as a continuous message, which is binarised during decentralised execution.\nExperimental results. Figure 4(a) shows our results for n = 3 agents. All four methods learn an optimal policy in 5k episodes, substantially outperforming the NoComm baseline. DIAL with parameter sharing reaches optimal performance substantially faster than RIAL. Furthermore, parameter sharing speeds both methods. Figure 4(b) shows results for n = 4 agents. DIAL with parameter sharing again outperforms all other methods. In this setting, RIAL without parameter sharing was unable to beat the NoComm baseline. These results illustrate how difficult it is for agents to learn the same protocol independently. Hence, parameter sharing can be crucial for learning to communicate. DIAL-NS performs similarly to RIAL, indicating that the gradient provides a richer and more robust source of information.\nWe also analysed the communication protocol discovered by DIAL for n = 3 by sampling 1K episodes, for which Figure 4(c) shows a decision tree corresponding to an optimal strategy. When a prisoner visits the interrogation room after day two, there are only two options: either one or two prisoners may have visited the room before. If three prisoners had been, the third prisoner would have finished the game. The other options can be encoded via the “On” and “Off” position respectively."
    }, {
      "heading" : "6.3 MNIST Games",
      "text" : "In this section, we consider two tasks based on the well known MNIST digit classification dataset [26].\nColour-Digit MNIST is a two-player game in which each agent observes the pixel values of a random MNIST digit in red or green of size 2 × 28 × 28, while the colour label, ca ∈ 0, 1, and digit value, da ∈ 0..9, are hidden. For each agent, reward consists of two components that are antisymmetric in the action, colour, and parity (odd, even) of the digits. Only one bit of information can be sent, so agents must agree to encode/decode either colour\nor parity, with parity yielding greater rewards. The game has two steps; in the first step, both agents send a 1-bit message, in the second step they select a binary action ua2 . The reward\nfor each agent is r(a) = 2(−1)aa2+ca+da ′ + (−1)aa2+da+ca ′\nand the total cooperative reward is r2 = r(1) + r(2).present results for 5 time-steps with a 1-bit of information exchanged per step and for 2 time-steps with 4 bits exchanged per step.\nMulti-Step MNIST is a grayscale variant that requires agents to develop a communication protocol which integrates information across many time-steps: Each step the agents send a message, mat , and take an action uat ∈ {0, . . . , 9}. Only at the final step, t = 5, is reward given, r5 = 0.5 for each correctly guessed digit, ua5 = d\na′ . As only 1-bit is sent per step, agents must find a protocol that integrates information across the four messages they exchange (the last message is not received). The protocol can be trained using gradients in DIAL, but also needs to have a low discretisation error.\nArchitecture. The input processing network is a 2-layer MLP TaskMLP[(|c|×28×28), 128, 128](oat ). Figure 5 depicts the generalised setting for both games. Our experimental evaluation showed improved training time using batch normalisation after the first layer.\nExperimental results. Figures 6(a) and 6(b) show that DIAL substantially outperforms the other methods on both games. Furthermore, parameter sharing is crucial for reaching the optimal protocol. In multi-step MNIST, results were obtained with σ = 0.5. In this task, RIAL fails to learn, while in colour-digit MNIST it fluctuates around local minima in the protocol space; the NoComm baseline is stagnant at zero. DIAL’s performance can be attributed to directly optimising the messages in order to reduce the global DQN error while RIAL must rely on trial and error. DIAL can also optimise the message content with respect to rewards taking place many time-steps later, due to the gradient passing between agents, leading to optimal performance in multi-step MNIST. To analyse the protocol that DIAL learned, we sampled 1K episodes. Figure 6(c) illustrates the communication bit sent at time-step t by agent 1, as a function of its input digit. Thus, each agent has learned a binary encoding and decoding of the digits. These results illustrate that differentiable communication in DIAL is essential to fully exploiting the power of centralised learning and thus is an important tool for studying the learning of communication protocols. We provide further insights into the difficulties of learning a communication protocol for this task with RIAL compared to DIAL in the AppendixB.\n6.4 Effect of Channel Noise\nThe question of why language evolved to be discrete has been studied for centuries, see e.g., the overview in [27]. Since DIAL learns to communicate in a continuous channel, our results offer an illuminating perspective on this topic.\nIn particular, Figure 7 shows that, in the switch riddle, DIAL without noise in the communication channel learns centred activations. By contrast, the presence of noise forces messages into two different modes during learning. Similar observations have been made in relation to adding noise when training document models [12] and performing classification [11]. In our work, we found that adding noise was\nessential for successful training. More analysis on this is provided in Appendix C."
    }, {
      "heading" : "7 Conclusions",
      "text" : "This paper advanced novel environments and successful techniques for learning communication protocols. It presented a detailed comparative analysis covering important factors involved in the learning of communication protocols with deep networks, including differentiable communication, neural network architecture design, channel noise, tied parameters, and other methodological aspects.\nThis paper should be seen as a first attempt at learning communication and language with deep learning approaches. The gargantuan task of understanding communication and language in their full splendour, covering compositionality, concept lifting, conversational agents, and many other important problems still lies ahead. We are however optimistic that the approaches proposed in this paper can play a substantial role in tackling these challenges."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This work was supported by the Oxford-Google DeepMind Graduate Scholarship and the EPSRC. We would like to thank Brendan Shillingford, Serkan Cabi and Christoph Aymanns for helpful comments."
    }, {
      "heading" : "A DIAL Details",
      "text" : "Algorithm 1 formally describes DIAL. At each time-step, we pick an action for each agent -greedily with respect to the Q-function and assign an outgoing message\nQ(·),mat = C-Net ( oat , m̂ a′ t−1, h a t−1, u a t−1, a; θi ) . (1)\nWe feed in the previous action, uat−1, the agent index, a, along with the observation o a t , the previous internal state, hat−1 and the incoming messages m̂ a′\nt−1 from other agents. After all agents have taken their actions, we query the environment for a state update and reward information.\nWhen we reach the final time-step or a terminal state, we proceed to the backwards pass. Here, for each agent, a, and time-step, j, we calculate a target Q-value, yaj , using the observed reward, rt, and the discounted target network. We then accumulate the gradients, ∇θ, by regressing the Q-value estimate\nQ(oat , m̂ a′ t−1, h a t−1, u a t−1, a, u; θi), (2)\nagainst the target Q-value, yat , for the action chosen, u a t . We also update the message gradient chain µat which contains the derivative of the downstream bootstrap error ∑ m,t′>t ( ∆Qa ′ t+1 )2 with respect to the outgoing message mat . To allow for efficient calculation, this sum can be broken out into two parts: The first part,∑ m′ 6=m ∂ ∂m̂at ( ∆Qa ′ t+1 )2 , captures the impact of the message on the total estimation error of the next step. The impact of the message mat on all other future rewards t ′ > t + 1 can be calculated using the partial derivative of the outgoing messages from the agents at time t+ 1 with respect to the incoming message mat , multiplied with their message gradients, µ a′\nt+1. Using the message gradient, we can calculate the derivative with respect to the parameters, µat ∂m̂at ∂θ .\nHaving accumulated all gradients, we conduct two parameter updates, first θi in the direction of the accumulated gradients, ∇θ, and then every C steps θ−i = θi. During decentralised execution, the outgoing activations in the channel are mapped into a binary vector, m̂ = 1{mat > 0}. This ensures that discrete messages are exchanged, as required by the task.\nIn order to minimise the discretisation error when mapping from continuous values to discrete encodings, two measures are taken during centralised learning. First, Gaussian noise is added in\nAlgorithm 1 Differentiable Communication (DIAL)\nInitialise θ1 and θ−1 for each episode e do s1 = initial state, t = 0, ha0 = 0 for each agent a while st 6= terminal and t < T do t = t+ 1 for each agent a do\nGet messages m̂a ′ t−1 of previous time-steps from agents m ′ and evaluate C-Net: Q(·),mat = C-Net ( oat , m̂ a′ t−1, h a t−1, u a t−1, a; θi ) With probability pick random uat , else u a t = maxaQ ( oat , m̂ a′ t−1, h a t−1, u a t−1, a, u; θi\n) Set message m̂at = DRU(m), where DRU(m) = { Logistic(N (mat , σ)), if training, else 1{mat > 0}\nGet reward rt and next state st+1 Reset gradients∇θ = 0 for t = T to 1, −1 do\nfor each agent a do yat = { rt, if st terminal, else rt + γmaxuQ ( oat+1, m̂ a′ t , h a t , u a t , a, u; θ − i ) Accumulate gradients for action: ∆Qat = y a t −Q ( oaj , h a t−1, m̂ a′ t−1, u a t−1, a, u a t ; θi\n) ∇θ = ∇θ + ∂∂θ (∆Q a t ) 2 Update gradient chain for differentiable communication:\nµaj = 1{t < T − 1} ∑ m′ 6=m ∂ ∂m̂at ( ∆Qa ′ t+1 )2 + µa ′ t+1 ∂m̂a ′ t+1 ∂m̂at Accumulate gradients for differentiable communication: ∇θ = ∇θ + µat ∂∂mat DRU(m a t ) ∂mat ∂θ\nθi+1 = θi + α∇θ Every C steps reset θ−i = θi\norder to limit the number of bits that can be encoded in a given range of m values. Second, the noisy message is passed through a logistic function to restrict the range available for encoding information. Together, these two measures regularise the information transmitted through the bottleneck.\nFurthermore, the noise also perturbs values in the middle of the range, due to the steeper slope, but leaves the tails of the distribution unchanged. Formally, during centralised learning, m is mapped to m̂ = Logistic (N (m,σ)), where σ is chosen to be comparable to the width of the logistic function. In Algorithm 1, the mapping logic from m to m̂ during training and execution is contained in the DRU(mat ) function."
    }, {
      "heading" : "B MNIST Games: Further Analysis",
      "text" : "Our results show that DIAL deals more effectively with stochastic rewards in the colour-digit MNIST game than RIAL. To better understand why, consider a simpler two-agent problem with a structurally similar reward function r = (−1)(s1+s2+a2), which is antisymmetric in the observations and action of the agents. Here random digits s1, s2 ∈ 0, 1 are input to agent 1 and agent 2 and u2 ∈ 1, 2 is a binary action. Agent 1 can send a single bit message, m1. Until a protocol has been learned, the average reward for any action by agent 2 is 0, since averaged over s1 the reward has an equal probability of being +1 or −1. Equally the TD error for agent 1, the sender, is zero for any message m:\nE [ ∆Q(s1,m1) ] = Q(s1,m1)− E [ r(s2, a2, s1) ] s2,a2 = 0− 0, (3)\nBy contrast, DIAL allows for learning. Unlike the TD error, the gradient is a function of the action and the observation of the receiving agent, so summed across different +1/−1 outcomes the gradient updates for the message m no longer cancel:\nE [∇θ] = E [( Q(s2,m1, a2)− r(s2, a2, s1) ) ∂ ∂m Q(s2,m1, a2) ∂ ∂θ m1(s1) ] <s2,a2> . (4)"
    }, {
      "heading" : "C Effect of Noise: Further Analysis",
      "text" : "Given that the amount of noise, σ, is a hyperparameter that needs to be set, it is useful to understand how it impacts the amount of information that can pass through the channel. A first intuition can be gained by looking at the width of the sigmoid: Taking the decodable range of the logistic function to be x values corresponding to y values between 0.01 and 0.99, an initial estimate for the range is ≈ 10. Thus, requiring distinct x values to be at least six standard deviations apart, with σ = 2, only two bits can be encoded reliably in this range. To get a better understanding of the required σ we can visualise the capacity of the channel including the logistic function and the Gaussian noise. To do so, we must first derive an expression for the probability distribution of outgoing messages, m̂, given incoming activations, m, P (m̂|m):\nP (m̂|m) = 1√ 2πσm̂(1− m̂) exp\n( − ( m− log( 1m̂ − 1) )2 σ2 ) . (5)\nFor any m, this captures the distribution of messages leaving the channel. Two m values m1 and m2 can be distinguished when the outgoing messages have a small probability of overlapping. Given a value m1 we can thus pick a next value m2 to be distinguishable when the highest value m̂1 that m1 is likely to produce is less than the lowest value m̂2 that m2 is likely to produce. An approximation for when this happens is when (maxm̂ s.t.P (m̂|m1) > ) = (minm̂ s.t.P (m̂|m2) > ). Figure 8 illustrates this for three different values of σ. For σ > 2, only two options can be reliably encoded using = 0.1, resulting in a channel that effectively transmits only one bit of information.\nInterestingly, the amount of noise required to regularise the channel depends greatly on the benefits of over-encoding information. More specifically, as illustrated in Figure 9, in tasks where sending more bits does not lead to higher rewards, small amounts of noise are sufficient to encourage discretisation, as the network can maximise reward by pushing activations to the tails of the sigmoid, where the noise is minimised. The figure illustrates the final average evaluation performance normalised by the training performance of three runs after 50K of the multi-step MNIST game, under different noise\nregularisation levels σ ∈ {0, 0.5, 1, 1.5, 2}, and different numbers of steps step ∈ [2, . . . , 5]. When the lines exceed“Regularised”, the test reward, after discretisation, is higher than the training reward, i.e., the channel is properly regularised and getting used as a single bit at the end of learning. Given that there are 10 digits to encode, four bits are required to get full rewards. Reducing the number of steps directly reduces the number of bits that can be communicated, #bits = steps− 1, and thus creates an incentive for the network to “over-encode” information in the channel, which leads to greater discretisation error. This is confirmed by the normalised performance for σ = 0.5, which is around 0.7 for 2 steps (1 bit) and then goes up to > 1 for 5 steps (4 bits). Note also that, without noise, regularisation is not possible and that with enough noise the channel is always regularised, even if it would yield higher training rewards to over-encode information."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2016,
    "abstractText" : "We consider the problem of multiple agents sensing and acting in environments<lb>with the goal of maximising their shared utility. In these environments, agents must<lb>learn communication protocols in order to share information that is needed to solve<lb>the tasks. By embracing deep neural networks, we are able to demonstrate end-<lb>to-end learning of protocols in complex environments inspired by communication<lb>riddles and multi-agent computer vision problems with partial observability. We<lb>propose two approaches for learning in these domains: Reinforced Inter-Agent<lb>Learning (RIAL) and Differentiable Inter-Agent Learning (DIAL). The former uses<lb>deep Q-learning, while the latter exploits the fact that, during learning, agents can<lb>backpropagate error derivatives through (noisy) communication channels. Hence,<lb>this approach uses centralised learning but decentralised execution. Our experi-<lb>ments introduce new environments for studying the learning of communication<lb>protocols and present a set of engineering innovations that are essential for success<lb>in these domains.",
    "creator" : "LaTeX with hyperref package"
  }
}