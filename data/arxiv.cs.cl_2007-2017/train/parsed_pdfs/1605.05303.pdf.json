{
  "name" : "1605.05303.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Fuzzy Sets Across the Natural Language Generation Pipeline",
    "authors" : [ "A. Ramos-Soto", "A. Bugarı́n", "S. Barro" ],
    "emails" : [ "alejandro.ramos@usc.es", "alberto.bugarin.diz@usc.es", "senen.barro@usc.es" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Index Terms\nfuzzy sets, computing with words, linguistic description of data, natural language generation, data-to-text systems\nI. INTRODUCTION\nData Science has traditionally relied on analytics and visualization techniques to make sense of large volumes of data. Data scientists employ different techniques such as statistics, signal processing, pattern recognition, data mining or machine learning among others to extract relevant information from such amounts of data. However, communication of the extracted information after the analytics process is usually made through graphics or visualization techniques that in general demand interpretation efforts from the user side and sometimes require a rather extensive academic development or expertise for its actual comprehension (Fig. 1 shows an example of this problem).\nThis issue arises the interest of using other kind of complementary descriptive techniques which help fill the gap between data and users in a more human-friendly way, so that the obtained information can be grasped by a wider range of people regardless of their expertise. In this regard, approaches such as linguistic description of data (LDD) or natural language generation (NLG), which provide information expressed in terms of natural language, have emerged as feasible complements which, while still exploiting the full potential of standard Data Science analytics, allow for a better understanding of what underlies in such data. In this regard, recent studies [1] indicate that non-specialized users actually strongly demand textual descriptions of data as a means for better understanding of graphics and visualizations.\nThe discipline of linguistic description or summarization of data (LDD) [4] has become in recent times a very promising approach to capture the essential information residing in numerical data. It allows to easily obtain linguistic information structured in protoforms able to describe relationships between the values of the different input variables along time and even space dimensions. Linguistic description of data is based primarily on the use of fuzzy quantified sentences and its main contribution is that it allows to manage the uncertainty and vagueness present in the human language concepts which are used to summarize data. This has led to an apparition of a very diverse collection of use cases in very different domains (a rather representative review of such approaches can be found in [5], [6]).\nAt the same time, the natural language generation (NLG) field [7], which addresses the creation of systems able to automatically deliver texts indistinguishable from those produced by humans, is currently experiencing a bursting scientific, technical and commercial expansion in its data-to-text (D2T) specialty [5], [8] due to the rise of the Big Data era. The more data is available, the more time experts and users need to make sense of it and, while it may be a mundane task, the creation of reports that describe in a few paragraphs what in origin were huge amounts of data is usually necessary in any organization. In this regard, D2T solutions help analysts, experts and users in general in saving time by performing data analysis and deliver relevant information as high quality texts.\nIt is safe to assume that the D2T solutions provided by NLG companies do not include any uncertainty or vagueness management [8]. In fact, although NLG (and D2T by extension) excels in terms of generating texts whose quality is optimal\n1This paper encompasses general ideas that emerged as part of the PhD thesis “Application of fuzzy sets in data-to-text systems”. It does not present a specific application or a formal approach, but rather discusses current high-level issues and potential usages of fuzzy sets (focused on linguistic summarization of data) in natural language generation.\nar X\niv :1\n60 5.\n05 30\n3v 1\n[ cs\n.A I]\n1 7\nM ay\n2 01\n6\n2\nfrom a linguistic perspective, the problem of how to address vagueness is still an open issue which is being actively researched in this discipline [9]–[12].\nIn this context, the current state of both LDD and NLG fields has led to a climate of mutual interest. LDD approaches may use NLG techniques to convert linguistic protoforms into information in an even more human-friendly state, which allows the delivery of high quality texts. Likewise, NLG systems may use LDD and other fuzzy-related techniques to address the problem of uncertainty at different levels within the distinct tasks involved in an NLG process. Examples from both trends include the GALiWeather [13] system, conceived as a LDD approach but made use of several NLG techniques to generate short-term weather forecasts; and the model presented in [14] that involves fuzzy temporal constraint networks and experimental data from three languages to address the problem of generating uncertain temporal expressions from an NLG point of view.\nThe previous examples show that collaboration between both fields is slowly but steadily starting to take place. However, it is yet unclear which and how many forms this potential cross-fertilization process will take. In this situation, the aim of this paper is to study from a detailed and practical perspective several potential convergence points for LDD and NLG which may open new research lines in the context of such collaboration. Furthermore, we will also resume the discussion opened by Kacprzyk and Zadrożny in [15], [16], where a relationship between LDD and NLG was discussed for the first time at a conceptual level, and which was later expanded by [5], [6], which provided additional insights and state of the art reviews for both fields.\nThis paper is organized in three main sections. Section II explores the challenges that LDD currently faces and how its usage\n3 together with NLG can help in addressing them. Section III studies in depth how the use of LDD techniques (namely fuzzy quantified statements) relate to the NLG tasks described by the well-known and widely accepted NLG system architecture proposed by Reiter and Dale in [7], as well as potential implications derived from such usage. Finally, Section IV provides further insights into some of the topics of most interest in the context of the collaboration between LDD and NLG and summarizes the main contributions of this paper."
    }, {
      "heading" : "II. LDD AND NLG: MAIN CHALLENGES",
      "text" : "Linguistic descriptions of data can be used to extract relevant information from numeric input data sets. Such descriptions are usually based on the concept of fuzzy quantified statement [4], which is usually classified using Zadeh’s notion of protoform [17]. In this regard, two basic protoforms are distinguished\n“Q Xs are A” (1)\n“Q DXs are A” (2)\nwhere Q is a fuzzy quantifier, A is a summarizer and D is a qualifier. These protoforms are also a representation of fuzzy quantified statements commonly referred to as type-1 (Eq. 1, “a few researchers are young” or “some of the humidity values were high”) and type-2 (Eq. 2, “most of the cold days were very humid”).\nAlthough protoforms have also been used in the context of fuzzy queries [15], [16], we will focus on their application in the automatic extraction of relevant linguistic information, that is, approaches applied to practical use cases in particular knowledge domains, where the structure of the linguistic descriptions which are obtained is known a priori. Following this definition, fuzzy quantified statements in the literature have been used to a great extent as a means to summarize and describe time series of data [6].\nIn this sense, LDD emerges as a discipline with a similar purpose to that of D2T approaches: to provide an understandable interface between the data and the human users in the form of information expressed in terms of natural language. However, data-to-text is aimed at the production of actual texts, while linguistic description of data remains in a more conceptual level. Figure 2 illustrates this contrast between both fields: while it is feasible to obtain a linguistic description which summarizes a data set properly, it is arguably useful for a human user if such linguistic information is not given in a way that matches the language used in the user’s specific application domain.\nThus, the main downside of linguistic descriptions of data is that they are restricted to protoform structures and, although they are flexible enough to be extended into more sophisticated forms (including time and spatial dimensions, for instance), their usage in real applications is not feasible when provided as is. This issue was identified by Kacprzyk and Zadrożny in [16], where they stated that “Zadeh’s protoforms are very powerful and convenient in CWW but may be a limiting factor in many real-world applications as their structure is too restricted, notably as compared to the richness of natural language”.\nAt the same time, this necessity for a means to effectively use the fuzzy techniques developed in LDD in practical application domains has also opened up the possibility of resorting to NLG. In this regard, Kacprzyk and Zadrożny proposed in [16] to define new types of protoforms “to make a full use of the power of NLG tools”. In their opinion, to be able to generate statements that provide distinct and richer expressiveness than standard protoforms would expand the potential usages of LDD\n4 and, at a general level, NLG techniques could be used to merge the best candidate statements obtained by a search algorithm into a proper text.\nIn our opinion, expanding the current collection of protoforms would entail a significant advance for LDD in terms of expressiveness, which in turn could probably bring LDD a step closer to NLG, but not necessarily closer to its usage in real systems. In this regard, the idea of NLG systems producing texts solely based on fuzzy protoforms, however complex these may be, seems unrealistic for the needs of actual systems.\nFor instance, the textual weather forecast generator GALiWeather [13] employs type-1 fuzzy quantified sentences to perform a global description of the cloud coverage variable, but it also uses different crisp approaches to extract the relevant information from other variables such as precipitation or temperature. The usage of such distinct techniques responds to the needs of the domain experts, who provided both the linguistic specifications and most of the domain knowledge required to build the system. Thus, LDD provides powerful tools to manage uncertainty and imprecision in the generation of linguistic expressions, but first it should be determined when their usage is appropriate. This issue is directly related to a relevant concept also noted in [16], namely the domain-modeling.\nThus, the perspective should shift from “how to use standalone LDD in real applications” to “when and how to use LDD as part of real NLG (or D2T) systems”. In a general sense, the analysis of the texts of a specific application domain (or the linguistic requirements of the experts if no example texts are available) will shed light on this issue2. If certain expressions in the language requirements match (directly or indirectly, from a content perspective) any of the protoforms currently defined and the linguistic terms to be used in such expressions allow for a fuzzy numerical definition (the domain experts cannot provide specific crisp definitions of such terms or there is not a consensus definition), then LDD could be properly applied.\nThis directly leads to another challenge that restricts the usage of LDD in real applications and which has not been previously considered, namely the problem of building fuzzy definitions of linguistic terms based on expert knowledge. In general, the problem of mapping the intuitive notion of a subjective concept from the application domain into a fuzzy set or relationship has not been a primary concern in the literature in LDD, as theory and use cases had to be developed first in order to show the potential applications of this kind of techniques. Even in more recent applied approaches which generate actual texts, linguistic variables were defined by authors and the quality of each solution as a whole was checked through an evaluation process by experts, e.g. [13], [21].\nWhile to impersonate an expert domain in order to fill knowledge gaps for the application of LDD techniques can be considered admissible and plausible to a certain extent, this practice seems to be in conflict with the purpose of a domainmodeling process: in order to be able to use LDD, the author creates fuzzy definitions for linguistic terms based on selfjudgments about the application domain, rather than capturing this meaning from the domain itself. In this sense, NLG has traditionally used empirical techniques to assess the meaning of words and terms in an as accurate as possible way. For instance, for the development of the NLG system SumTime-Mousan [18], Reiter et al. analyzed a parallel set of textual wind forecasts by five different experts and their corresponding data in order to achieve a coherent definition of temporal expressions such as “by evening” or “by midday”. Subsequent evaluations of the system showed that overall forecast readers preferred the wind texts generated by the system over human-written wind texts. In other cases experiments were run in order to study how human subjects use linguistic expressions in different domains (e.g. [14], [20]).\nIn this regard, the main challenge resides in bringing such empirical approaches into LDD and adapting them for achieving a proper definition of fuzzy linguistic terms. This, depending on the kind of LDD statements which could be used, also opens up the possibility of performing experimentation to determine, for instance, which operators could be used effectively for combining different summarizers or properties (e.g. as in “most of the students are short and fast”), such as the compensatory\n2This technique is commonly known in NLG as “corpus” analysis and is performed systematically in the early conception stages of an NLG system [18].\n5 operators proposed by Zimmermman [22] or the OWA operators by Yager [23]. In a general sense, this would imply the instantiation of the theoretical models developed in fuzzy sets based on standard NLG empirical approaches for different application domains.\nAlthough LDD faces important challenges (Table I summarizes the main aforementioned issues and hints for addressing them), there is an important consensus in this field about its practical viability based on the use of NLG tools [5], [6], [16], [24]. This trend has been reinforced in recent years, as more work in LDD has adopted the use of template-based NLG approaches [25] to generate actual texts (e.g. [13], [21]).\nThe remaining question, now it is clear how LDD should be approached effectively in conjunction with NLG, resides in clarifying what benefits and implications LDD (and fuzzy sets in general) may bring to NLG systems. This was also noted by Kacprzyk and Zadrożny in [16]: “...it seems that NLG can benefit from the approach and solutions that are adopted in our approach by finding the conceptually and numerically operational means to grasp and handle the problem of imprecision of meaning that is so characteristic for natural language but has not been appropriately considered in NLG despite an urgent need”.\nImprecision and uncertainty in NLG have not been fully addressed, but it is a subject of interest and research, as stated in Section I (e.g. [9]–[12]). However, it is not clear why fuzzy sets and derived disciplines such as LDD have not been considered by the NLG community until very recently [14], despite being intuitively appropriate for this task. This unawaraness may be partially explained by the opposition between the traditional theoretical nature of the fuzzy field, more focused on its logical aspects, and the more applied nature of NLG, focused on linguistic problems of a more empirical weight.\nNowadays, thanks to the effort and work made from both fields, we have a better understanding of where LDD fits in an NLG process and what benefits this relationship may bring in the task of textually describing large data sets while also taking into account the management of uncertainty and imprecision. In this regard, Section III deals with this question in a thorough way from an NLG perspective."
    }, {
      "heading" : "III. FUZZY SETS AND LDD ACROSS THE NLG PIPELINE",
      "text" : ""
    }, {
      "heading" : "A. The NLG Pipeline",
      "text" : "NLG systems can be generally depicted as systems tasked with the conversion of some input data into an output text. The most widely accepted classification of this task division is the architecture proposed by Reiter and Dale in [7], where NLG systems are characterized as a pipeline composed of several parts which deal with different aspects of the NLG process. In this pipeline NLG systems are informally structured as a process divided into three principal modules: i) text or document planning, ii) microplanning and iii) realization. Document planning is focused in the production of a specification of the text’s content and structure, microplanning addresses the problem of choosing appropriate expressions for the content and other fine-grained tasks and realization produces the actual text by applying grammatical, syntactical and ortographical rules.\nA more detailed division of such architecture distinguishes six different activities (see Fig. 3). These subtasks focus alternatively on two different dimensions of the NLG process, namely content and structure:\n• Content determination. As the name indicates, this process identifies and isolates the relevant information which will be communicated in the text from the source input data in the form of input messages. • Document structuring. Organizes the set of messages according to a certain order and structure, including how to group messages together thematically, their order or their correspondence to high level document structures such as paragraphs and sections.\n6 • Lexicalization. This process involves deciding which specific words and expressions shall be used to express the information obtained in the content determination task. • Aggregation. Similar to document structuring, this task is concerned with deciding how the information is mapped into low level structures such as sentences. • Referring expression generation. This task deals with the identification of entities in a discourse. This implies selecting the words or expressions that allow to identify such entities. • Linguistic realization. Converts sentence representations into actual text. • Structure realization. Converts the high level structures into mark-up symbols understood by the document presentation\ncomponent (for instance, if paragraphs or sections are part of an HTML file which is used to display the output text). Of all the subtasks described in [7] by Reiter and Dale, content determination emerges as the more intuitively related to LDD. In this sense, the current consensus in LDD is that this kind of techniques allow to perform a “fuzzy” content determination task. Kacprzyk and Zadrożny in [16] introduced this notion in terms of the more general concept of text planning, while [5], [6] explored in more detail the role of LDD as a means to perform content determination while managing at the same time the problem of imprecision in natural language.\nThis has opened up new ways for further exploration of the relationship between both fields, which allow to consider an even wider usage of fuzzy sets and LDD beyond content determination. In this regard, we are also considering lexicalization, aggregation and referring expression generation as subtasks which can be addressed or influenced by means of LDD (Fig. 3). We will illustrate this through a use case inspired by both the WEATHERREPORTER design study provided in [7] and the LDD example presented in [26], where a generic model to approach LDD in the context of performing content determination tasks was proposed."
    }, {
      "heading" : "B. An Illustrative Use Case",
      "text" : "Let us suppose there is a meteorology agency which provides weather reports based on observational data series obtained from several meteorological sensor stations spread across 100 different locations in a specific region. This agency is interested in automatizing the creation of reports, as this task involves a lot of analysis time and effort from the meteorologists due to the high number of reports which must be produced (one per location).\nA team composed of NLG experts is then tasked with the development of a system which generates meteorological textual reports. After a few preliminary meetings and studying some examples of parallel human-produced texts and corresponding\n7 data, the experts determine that the specifications provided by the meteorologists are incomplete and imprecise: in some cases there is not a clear semantic definition of the linguistic terms to be used in the text generation process and meteorologists seem to diverge in their definitions.\nA few experts in fuzzy sets and their application in LDD join the NLG system development team to help them address and model the imprecision found during the initial stages of the domain-modeling process. The team then designs several experiments, which are performed by the meteorologists, focused on the semantics of the terms related to the meteorological variables of interest, namely the temperature, humidity and precipitation, and other terms such as quantifiers. Based on the obtained empirical data, the team is able to build a fuzzy knowledge base such as the one shown in Fig. 4. This allows to start the design and development of the NLG system, which will include expressions produced through the use of fuzzy and LDD techniques.\nIn a general sense, the NLG system will receive as input a data set composed of time series data for several variables from a given meteorological station in a given time period and generate a textual report as output. The reports generated by the system will include both general facts and more specific details about the behavior of the variables of interest (see Fig. 5). In this context, we will study how fuzzy sets and LDD may interact with the NLG tasks."
    }, {
      "heading" : "C. Across the NLG Pipeline",
      "text" : "1) Content determination: This task is tied to the specific application domain, as it involves employing different techniques to extract relevant information from the source data. The problem of which techniques to use depends on the nature of the source data and the content requirements of the texts to be produced. For instance, in the present use case the NLG system development team could apply signal processing or pattern recognition techniques, as the system will have to deal with time series data for several variables (see Fig. 6). Although the content requirements in this use case are more restricted for illustration purposes, actual NLG systems do include a wide variety of this kind of approaches. This is especially true in the case of D2T systems, such as BabyTalk [27], [28] or SumTime-Turbine [29], [30]).\nIf we focus on the content which must be included in the “general information” part of the meteorological report, three main elements can be identified (Fig. 5). The first and second ones provide information about the predominant trend for temperature and rain in a given time period, while the third one is just a count of the number of days where precipitation was registered. Following the concept of message described in [7], Fig. 7 shows a possible content determination message for the rainy days count.\nLet us suppose then that the first two elements which describe the temperature and rain trends in the human-produced reports are a vague description based on a subjective quantification performed by the meteorologists. The fuzzy experts determine that, based on the fuzzy definitions shown in Fig. 4, these elements can be modeled using type-I fuzzy quantified statements, such\n8\nas “most of the days the temperature was cold” or “a few days were wet”. This increases both the flexibility and complexity of the content determination task for temperature and rain, which is developed by the fuzzy experts using the following strategy:\n• All posible combinations of fuzzy quantifiers and labels are computed for each content element (e.g., from “few, very cold” to “nearly all, very hot”). This involves determining the fulfillment degree (FD) of each quantified statement and additional criteria, such as the coverage degree (Cov) of the quantifiers [31]. • The best candidate statements are selected based on the chosen criteria. In our use case, the fuzzy experts decide to choose the minimum number of candidate statements with a high fulfillment degree while covering as much data as possible at the same time. • More than one statement can be selected depending on the computed criteria. For instance, a “nearly all, cold” statement with a 0.9 fulfillment degree would suffice to describe the temperature, since “nearly all” is the quantifier with highest coverage and the fulfillment degree is high. However, both “some, almost dry” with a 0.6 fulfillment degree and “some, wet” with a 0.4 fulfillment degree would be selected as content messages, since the sum of both fulfillment degrees is the highest and the added coverage of both quantifiers is compatible with 100% of the data.\nThe chosen fuzzy quantified statements are then included as part of the content determination messages, such as the rainy days count message (Fig. 7). This process is illustrated by Fig. 8, which also shows an example of a content determination message produced as a result of the fuzzy quantification algorithm. In general terms, such kind of messages should include at least the fulfillment degree associated to the obtained piece of content, as well as any other element which characterizes this content (e.g. linguistic quantifier and label in the case of type-I quantified sentences), as these will play an important role in subsequent NLG tasks (see Section III-C2).\n9 There are also further additional fine-grain issues which must be considered. In this use case, for instance, to choose a fuzzy quantification model is another decision the fuzzy experts had to make in order to calculate the fulfillment degree of the fuzzy quantified sentences. Particularly, in the previous calculations, Zadeh’s proposal was used [32]\nFD(“Q Xs are A”) = µQ\n( 1\nn n∑ i=1 µA(xi)\n) (3)\nwhere µQ is the function associated to the fuzzy quantifier Q (e.g. “nearly all”), µA is a function associated to the fuzzy label A (e.g. “cold”), and X is the input time series data.\nHowever, it is not clear which models might be the most appropriate for usage in real applications. Although we are aware of their theoretical properties [33], which can be useful in this regard, this remains an open problem which should be solved in each specific case [34]. This issue also applies to t-norm or t-conorm operators, which can be used to aggregate different properties (e.g. as in “most of the days were cold and wet”). Minimum and maximum are perhaps the most commonly used, but there is a wide variety of operators which make such decisions harder to make (this will be further discussed in Section III-C3).\nIn general terms, using LDD or fuzzy approaches in content determination will imply creating a higher number of content messages which account for the imprecision defined in the fuzzy knowledge base: a discourse element such as the temperature or the rain will not be necessarily characterized by just one piece of information, but by several which contain fuzzy-related information (e.g. fulfillment degree, quantifier coverage, etc.) and which must be carried on to the following subtasks of the pipeline in order to be properly exploited. Particularly, lexicalization will be highly influenced by the decisions made in this initial task.\n2) Lexicalization: If content determination focuses on obtaining relevant information from the input data, the purpose of the lexicalization task is to move such content closer to an actual text. It is in this stage where the imprecision modeled with fuzzy techniques in content determination can actually show its highest usage potential.\nResuming our use case, the content determination task has now been succesfully developed by the fuzzy and NLG experts. As a result, the current system is able to extract content messages which contain both the general and extended information for the target reports (Fig. 5). Furthermore, the NLG experts have implemented the document structuring stage, and the system is also able to organize the content messages into different high level structures (such as paragraphs) according to the general structure described in the report sample (Fig. 5).\nThe current challenge for the NLG system developers is to decide how to lexicalize the content messages, i.e., which words and expressions should be used to communicate the information contained in the content determination messages. For some content parts this should be rather direct, such as the rainy days count message (Fig. 7), but the introduction of the fuzzy content messages expands the possibilities and choices which can be made in this regard.\nFor instance, retaking the general temperature message from the content determination example in Fig. 8, which essentially consists in a type-I quantified sentence, the system developers may choose several alternatives. The first and most direct alternative is, if we obviate the domain language requirements, to express it as “nearly all the days of the period were cold”.\n10\nIn this sense, a fuzzy quantified statement which follows a standard protoform can easily be matched with a natural language expression whose subject is “Q of Xs” and predicate is “are A”. Thus, fuzzy quantified statements can also be considered as “proto-lexicalization” messages, since they are built using linguistic concepts which can already be directly expressed as natural language sentences.\nHowever, as it was shown in Fig. 2 in Section II, in real applications the expressions must be adapted to the domain language requirements, and converting a protoform-like statement into an actual appropriate natural language expression is not trivial in many cases. Thus, the NLG system developers of our use case decide to lexicalize the general temperature message as “the period was predominantly cold”3. Moreover, the lexicalization task is implemented so that every time a “nearly all” quantifier appears in the temperature message, the expression “was predominantly” is used systematically, as long as the fulfillment degree of the message is high (which is usually determined by establishing a certain threshold).\nIn fact, without taking into account the fulfillment degree, the lexicalization task in our use case would not be too different from standard lexicalization approaches. The semantics of the general content messages for temperature and rain is a combination of the linguistic terms which characterize the fuzzy quantified statements and their associated fulfillment degrees, and everything must be taken into account when performing lexicalization. Another simpler, but perhaps more illustrative example of this is the lexicalization process for exceptional temperature periods (which deviate from the general trend) in the extended information part of the report in Fig. 5.\nLet us suppose that the NLG system development team has implemented as part of the content determination task a fuzzy algorithm which extracts relevant temperature periods from the input data. Each period is characterized by its start and end dates, a temperature label and the average fulfillment degree calculated by evaluating the label against the input data. Suppose now that for a given input data set, the system obtains two messages for a same temperature label, as Fig. 9 shows.\nThe most obvious solution in this case would be to lexicalize both messages as “warm periods”. However, this way we would be omitting the semantic contribution of the fulfillment degree and nullifying the flexibility that fuzzy sets provide. Once more, to grasp the contribution of a fulfillment degree to the semantics of a linguistic term such as “warm” is a problem the NLG system experts have to solve. For instance, “warm” periods with an average fulfillment degree in the range [0.5,0.75] could be lexicalized as “warmish” (not entirely warm), while periods in the range (0.75,1] are lexicalized as “warm”. Content messages with an average fulfillment degree lower than 0.5 would already be discarded as part of the content determination task algorithm.\nIt is also possible that additional criteria should be taken into account in the case of Fig. 9, such as the fulfillment degree of contiguous labels for each message. Focusing on the “warm FD = 0.55” message, one could consider computing the fulfillment degrees for contiguous “cold” and “hot” fuzzy labels for the same temporal period. This increases the possibilities, as a “cold FD = 0.35” or a “hot FD = 0.4” could be used in conjuction with “warm FD = 0.55” to produce expressions such as “warm/coldish” or “warm/hottish”.\nThe previous examples show how individual messages can be lexicalized in the context of using fuzzy approaches and illustrate some of the possibilities these techniques open in this regard. However, to simply convert single messages into natural language expressions is not enough in most cases to produce actual texts. For instance, if the NLG system experts fed the linguistic realizer with the current lexicalization structures, the system would generate something similar to “The period was cold in general. The period was wet in general. There were 77 days with rain. There was a warm period from the 14th to the 19th of December. There was a warmish period from the 20th to the 26th of January...”, which is perfectly correct in\n3Note that we are using actual natural language expressions to illustrate lexicalization, but this task actually involves creating new sets of structures which define an intermediate syntax between the original content messages and the actual text [7].\n11\nterms of ortography, syntax and grammar, but is repetitive and still lacks readability. The aggregation task plays an important role in addressing this issue.\n3) Aggregation: In general, the aggregation task in NLG systems involves the creation of more complex sentences by merging simple phrase specifications [7]. This task is usually depicted from a structural perspective (see Fig. 3), and is usually addressed by using mechanisms such as simple conjunction, conjunction via shared participants, conjunction via shared structures, etc. For instance, the NLG experts in our use case may add several aggregation rules in order to merge some of the sentences which have been lexicalized, e.g. the general temperature and rain descriptions. Thus, “the period was predominantly cold” and “the period was predominantly wet” could be merged via shared participants (they share the same subject, “the period”) as “the period was predominantly cold and predominantly wet”. Alternatively, the previous messages could also be merged through a shared structure mechanism as “the period was predominantly cold and wet”.\nWhile these mechanisms operate at an strictly syntactical or structural level, they resemble similar content aggregation mechanisms which are present in fuzzy sets theory and LDD. These are usually modeled by means of t-norm operators, such as the minimum or the product. For example, if we backtrack the generation of the “the period was predominantly cold and wet” sentence, two alternatives now emerge. The first one, which has already been described, is a conjuctive aggregation operation via shared structures. The second one, however, could have been performed during the content determination task by obtaining a single quantified sentence which aggregates both temperature and rain properties (“Q Xs are A and B”), such as “near all the period days were cold and wet’. However, “and” in this case would not be a simple conjunction which merges two sentences, but rather a t-norm operator.\nThis arises some questions. Consider the case where the sentence “the period was predominantly cold and wet” is generated using the two distinct aforementioned approaches (see Fig. 10). To which extent their meaning would be equivalent? And to which extent does using a specific fuzzy aggregation operator influence the semantics of the obtained stamement and its equivalence with the other sentence? Again, to decide which specific operator to use will be a non-trivial decision the experts will have to make if they choose to aggregate properties using fuzzy operators, especially when other kind of nonstandard operators have been shown to better resemble the way humans aggregate information [22], [23]. Thus, using such fuzzy aggregation techniques at a content determination level may imply additional issues, but this can also help alleviate the complexity of the subsequent lexicalization and aggregation subtasks.\nAnother interesting case of aggregation is also introduced by the use of LDD techniques within the NLG pipeline. Let us consider a situation in our use case where the general temperature trend message is composed of two submessages (which means that the fulfillment degree is spread among more than one quantifier), as Fig. 11 shows. The second message in Fig. 11 is composed of two different statements, namely “some of the period days were cold” and “many of the period days were cold”. Although they are not incompatible from a fuzzy logic perspective (both fulfillment degrees are close to 0.5), to lexicalize both independently would probably lead the reader to an important confusion state. Likewise, if we aggregate them using the aforementioned conjunctive linguistic mechanisms, a sentence like “some and many of the period days were cold” would be generated, which does not improve the situation much.\nIn this problem the key lies in providing an expression which reflects the semantics of the message in a proper way. Intuitively, the whole message could be expressed as “between some and many of the period days were cold” or “some or many period days were cold”, among others, which are far from the domain language requirements. Thus, in order to cut corners, the experts in our use case decide to lexicalize it as “the period was cold in general”. Other semantic interpretations are possible, since it can also be considered that the quantifier “some”’ is more inespecific than “many” and, in terms of their fuzzy sets interpretation it should hold that some ⊃ many. Under these considerations consistent lexicalizations can be either to state the more inespecific quantifier some = some ∪many if the aim is to cover as much data as possible or\n12\nmany = some∩many if providing the most specific information is preferred to data covering. One could say that this latter specific problem is more related to lexicalization (choosing a natural language expression for a given more complex message) than aggregation, but even the well-defined NLG architecture by Reiter and Dale admits a high flexibility of opinions in similar task placement issues [7].\n4) Referring Expression Generation: The task of generating referring expressions emerges as a beast of its own and may well be the less understood and most actively researched subtask within NLG. Although it is directly to related to lexicalization, it focuses on a more specific problem. In short, its purpose is to identify entities (thing, being, event...) within a discourse and generate expressions which provide such identification (referring expressions). These are usually expressed in the form of noun phrases, or surrogates for a noun phrase.\nIf we follow the previous definition, some referring expressions can be identified in the report example from Fig. 5, such as “the period”, or “a coldish interval from the 2nd to the 6th of January” (Fig. 12). The case of “the period” is perhaps one of the simplest referring expression examples which can be found, as the reader of the report will certainly know which time interval the report is referring to (there is only one period and thus no features are needed to further identify such entity). However, in “a coldish interval from the 2nd to the 6th of January” we find a more interesting referring expression, which identifies an specific event by means of a fuzzy property (cold, expressed as “coldish”) and a temporal expression (which in this case is crisply defined, but could also be a fuzzy temporal expression in other situations [14], e.g. “towards the beginning of January”).\nFuzzy sets and LDD allow for the inclusion of fuzzy properties which can be used for the task of referring expression generation. In this regard, another interesting open research problem in NLG is the generation of geographical referring expressions which help identify within a single discourse relevant events through the use of geographical descriptors. For instance, consider the highlighted region in Fig. 13 a). Could this region be described as “NW Spain”, “Galicia, Asturias and part of Castilla y León”, “to the north of Portugal” or simply “near Galicia”?\nSuch geographical referring expressions have been generated through crisp approaches classically, such as the ones produced by the RoadSafe system [35], [36], which is able to generate complex expressions from geographical data like “in some far southern and southwestern places” [37]. However, the use of crisp boundaries between the descriptors leads to situations where even if one data point is only slightly below another point, one could be computed as “north” and the other one as “south”. Fuzzy sets and LDD can help address such problems by modeling fuzzy geographical descriptors which can be combined to generate similar geographical referring expressions which consider the graduality of such geographical concepts [38].\nTo characterize discourse entities using fuzzy properties for the task of generating referring expressions will also have a\n13\ndeep impact in how referring expression algorithms should be approached (in a similar manner to how the other tasks within the NLG pipeline are affected by this, since increasing flexibility by using gradual properties also increases the complexity which must be managed). It must also be noted that this complexity will be given by the kind of domain entities which must be referred to. For instance, in D2T systems which generate texts from time series data, language tends to be simpler [8] and relevant information such as events can be clearly identified through the use of temporal expressions. In others, such as the aforementioned geographical NLG systems or visual systems in general [39], this complexity will be higher (see Fig. 13 b)).\nIV. DISCUSSION AND CONCLUSIONS"
    }, {
      "heading" : "A. Additional remarks",
      "text" : "We have studied how fuzzy sets and LDD can play an important role in bringing imprecision and uncertainty management to NLG systems. While they seem to be directly usable for content-related tasks, we have also explored how a structure-related task, such as aggregation, could also be approached in some cases through fuzzy means.\nIn this regard, we would like to retake the idea proposed by Kacprzyk and Zadrożny in [16] about the creation of new kinds of protoforms [40], which was discussed in Sec. II. Although we did not describe in this paper document structuring as an NLG task which could be directly approached by LDD or fuzzy means, we believe that this could serve as inspiration to design richer protoforms.\nWhile document structuring deals with the organization of content messages into higher level structures such as paragraphs, this sorting operation also takes into account other kind of relationships among these messages, namely discourse relations, such as contrast, elaboration or narrative sequences. It is not difficult to imagine extended protoforms which model somehow these discourse relations. Consider, for instance, the following protoforms\n“Q Xs are A, but R Y s are B” (4)\n“Q Xs are A, especially R Y s are B” (5)\nwhich model a contrast (4) and an emphasizing (5) relation between two basic protoforms. To obtain protoform-like statements such as “Most of the days in 2015 were warm, but most of the days in November 2015 were very cold” or “Most places in Southern France suffered from strong precipitations, especially most of the SW coast suffered from intense floods”. Such kind of protoforms would also have an impact in how the text should be structured, as a relation holds between their more basic elements.\nAlthough this kind of contrast and emphasizing relationships between different content elements were considered in [41] from a fuzzy perspective, further research could be made to adapt this kind of discourse relations into new types of protoforms. This would also mean that the semantics of such relations could be approached in a fuzzy way (e.g. a fuzzy measure of contrast among different labels of a same linguistic variable, based, for instance, on a standard measure of antonimy)."
    }, {
      "heading" : "B. Evaluation of NLG systems using fuzzy approaches",
      "text" : "The evaluation of NLG systems which include uncertainty management through fuzzy sets will be another interesting research scope, as the flexibility such techniques provide implies that additional decisions will have to be taken (e.g. as in the examples shown in Sections III-C1, III-C2 and III-C3). To assess all these design decisions during the conception and development of\n14\nan NLG system will probably be unfeasible, which opens the possibility of including such assessment during the subsequent standard evaluation process of the system [42].\nIn this regard, we would like to differentiate the concept of “evaluation” used in a LDD perspective from the one used in an NLG context. Evaluation of LDD approaches has traditionally consisted in the usage of different criteria, such as the fulfillmente degree or coverage used in the use case in Sec. III-C, which allow to rank and select the most appropriate candidate statements generated by an LDD algorithm. Thus, such evaluation would be performed as part of the content determination process.\nTo evaluate an NLG system is a task which is usually done once the system is fully functional and is able to generate actual texts. In this regard, two different types of evaluations may be performed: i) intrinsic evaluations, which focus on the quality and appropriateness of the texts generated by the system, and ii) extrinsic evaluations, which try to measure the extent to which the system is actually useful and impactful for its users [5].\nIn our opinion, many of the decisions taken as part of the NLG system development process due to the use of fuzzy approaches could be assessed inherently through intrisinc evaluations (which usually consist in obtaining feedback from the domain experts in the form of questionnaires or text-free comments). In some cases, it could also be interesting to create different versions of a same NLG system and assess their differences (for instance, to study the problem of aggregation at a content determination level using fuzzy operators and the aggregation at a structural level)."
    }, {
      "heading" : "C. Terminology",
      "text" : "Another issue, which is indirectly related to the research questions explored here but may very well become a problem in the future, is the terminology used in the literature to refer to each discipline involved in the tasks here described. We have tried to keep here a clear distinction between LDD, NLG and D2T and, although the two latter terms are well established in terms of usage, LDD has been named differently or used with distinct meanings in the literature. LDD was originally conceived as “linguistic summarization of data” [4] and this name is still used in many fuzzy sets research papers. While we believe that it represents well the purpose of the field which represents, this terminology may confuse readers from other disciplines, as summarization is a well-known discipline in NLG and NLP (generating texts which summarize larger documents), which is totally unrelated to LDD.\nOther authors are considering LDD as an alternate approach which actually reunites NLG/D2T and what we understand as LDD [6]. This is an interesting proposal which fits well the idea of reuniting both paradigms, but a consensus should be achieved to avoid further confusion. Perhaps, the most surprising fact in this terminology discussion is that none of the names used until now (“linguistic summarization of data”, “linguistic description of data”) explicitly emphasizes the fuzzy nature of the techniques and operations that this discipline encompasses."
    }, {
      "heading" : "D. Conclusions",
      "text" : "The use of fuzzy sets and linguistic descriptions of data to provide imprecision and uncertainty management capabilities in NLG systems is a promising research line which, as we have discussed in this paper, has many ramifications. Although this kind of techniques seem to fit primarily in content-related tasks, the diversity of problems involved in such tasks allows for many possibilities. Furthermore, even more structure-focused NLG tasks such as aggregation or document structuring could also benefit from fuzzy sets and LDD. Table II summarizes the most relevant potential applications of these techniques in NLG.\nLikewise, the research on NLG systems will also help bring fuzzy sets and LDD closer to real applications where uncertainty plays a key role. In this regard, as discussed in Section II, one of the main challenges will be to embrace and adapt the empirical approaches usually performed during the conception of NLG systems.\nThe final purpose of this strong collaboration between these both major fields in the artificial intelligence community is to provide better systems which, in the context of Data Science, produce more human-friendly information in the form of natural language texts while managing the vagueness and imprecision included in the semantics underlying such information. Within this context, D2T/NLG systems, either alone or as a complementary support to visualization, will allow to improve the understanding of large data sets in many application domains and bring data closer to people. Empiric studies [1] show that visual information alone is not always capable of adequately communicating relevant information about data to users. In this regard NLG and D2T are descriptive approaches that, combined with sound analysis techniques, are starting to prove to be valuable complementary informative tools in the Data Science realm.\nAs future work, the authors will follow the tips here provided and research the use of fuzzy sets in the research and development of NLG systems. Particularly, we will focus on i) researching the generation of geographical referring expressions by means of fuzzy sets, ii) exploring how to better integrate LDD within D2T in terms of content determination and lexicalization tasks and iii) identifying new potential application domains where the use of LDD+NLG may prove useful.\n15"
    }, {
      "heading" : "ACKNOWLEDGMENTS",
      "text" : "This work was supported by the Spanish Ministry for Economy and Competitiveness (grant TIN2014-56633-C3-1-R) and by the European Regional Development Fund (ERDF/FEDER) and the Galician Ministry of Education (grants GRC2014/030 and CN2012/151). Alejandro Ramos Soto (A. Ramos-Soto) is supported by the Spanish Ministry for Economy and Competitiveness (FPI Fellowship Program) under grant BES-2012-051878."
    } ],
    "references" : [ {
      "title" : "A case study: NLG meeting weather industry demand for quality and quantity of textual weather forecasts",
      "author" : [ "S.G. Sripada", "N. Burnett", "R. Turner", "J. Mastin", "D. Evans" ],
      "venue" : "INLG-2014 Proceedings, June 2014.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Softlearn: A process mining platform for the discovery of learning paths",
      "author" : [ "B. Vázquez-Barreiros", "M. Lama", "M. Mucientes", "J.C. Vidal" ],
      "venue" : "14th International Conference on Advanced Learning Technologies (ICALT 2014). IEEE Press, 2014, pp. 373–375.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "A new approach to the summarization of data",
      "author" : [ "R.R. Yager" ],
      "venue" : "Information Sciences, vol. 28, no. 1, pp. 69 – 86, 1982.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 1982
    }, {
      "title" : "On the role of linguistic descriptions of data in the building of natural language generation systems",
      "author" : [ "A. Ramos-Soto", "A. Bugarı́n", "S. Barro" ],
      "venue" : "Fuzzy Sets and Systems, vol. 285, pp. 31–51, 2016.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "On generating linguistic descriptions of time series",
      "author" : [ "N. Marı́n", "D. Sánchez" ],
      "venue" : "Fuzzy Sets and Systems, vol. 285, pp. 6 – 30, 2016, special Issue on Linguistic Description of Time Series.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Building Natural Language Generation Systems",
      "author" : [ "E. Reiter", "R. Dale" ],
      "venue" : null,
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2000
    }, {
      "title" : "An architecture for data-to-text systems",
      "author" : [ "E. Reiter" ],
      "venue" : "Proceedings of the 11th European Workshop on Natural Language Generation, S. Busemann, Ed., 2007, pp. 97–104.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Generating referring expressions that involve gradable properties",
      "author" : [ "K. van Deemter" ],
      "venue" : "Comput. Linguist., vol. 32, no. 2, pp. 195–222, Jun. 2006.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Utility and language generation: The case of vagueness",
      "author" : [ "——" ],
      "venue" : "Journal of Philosophical Logic, vol. 38, no. 6, pp. 607–632, 2009.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Generating numerical approximations",
      "author" : [ "R. Power", "S. Williams" ],
      "venue" : "Comput. Linguist., vol. 38, no. 1, pp. 113–134, Mar. 2012.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Towards a possibility-theoretic approach to uncertainty in medical data interpretation for text generation",
      "author" : [ "F. Portet", "A. Gatt" ],
      "venue" : "Knowledge Representation for Health-Care. Data, Processes and Guidelines, ser. Lecture Notes in Computer Science, D. Riaño, A. ten Teije, S. Miksch, and M. Peleg, Eds. Springer Berlin Heidelberg, 2010, vol. 5943, pp. 155–168.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Linguistic descriptions for automatic generation of textual short-term weather forecasts on real prediction data",
      "author" : [ "A. Ramos-Soto", "A. Bugarı́n", "S. Barro", "J. Taboada" ],
      "venue" : "Fuzzy Systems, IEEE Transactions on, vol. 23, no. 1, pp. 44–57, Feb 2015.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Multilingual generation of uncertain temporal expressions from data: A study of a possibilistic formalism and its consistency with human subjective evaluations",
      "author" : [ "A. Gatt", "F. Portet" ],
      "venue" : "Fuzzy Sets and Systems, vol. 285, pp. 73 – 93, 2016, special Issue on Linguistic Description of Time Series. [Online]. Available: http://www.sciencedirect.com/science/article/pii/S0165011415003590",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Protoforms of linguistic database summaries as a human consistent tool for using natural language in data mining",
      "author" : [ "J. Kacprzyk", "S. Zadrozny" ],
      "venue" : "International Journal of Software Science and Computational Intelligence (IJSSCI), vol. 1, no. 1, pp. 100–111, 2009.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Computing with words is an implementable paradigm: Fuzzy queries, linguistic data summaries, and natural-language generation",
      "author" : [ "——" ],
      "venue" : "Fuzzy Systems, IEEE Transactions on, vol. 18, no. 3, pp. 461–472, June 2010.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "A prototype-centered approach to adding deduction capability to search engines-the concept of protoform",
      "author" : [ "L.A. Zadeh" ],
      "venue" : "Intelligent Systems, 2002. Proceedings. 2002 First International IEEE Symposium, vol. 1, 2002, pp. 2–3.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Choosing words in computer-generated weather forecasts",
      "author" : [ "E. Reiter", "S. Sripada", "J. Hunter", "I. Davy" ],
      "venue" : "Artificial Intelligence, vol. 167, pp. 137–169, 2005.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Reducing the complexity in genetic learning of accurate regression tsk rule-based systems",
      "author" : [ "I. Rodriguez-Fdez", "M. Mucientes", "A. Bugarin" ],
      "venue" : "Fuzzy Systems (FUZZ-IEEE), 2015 IEEE International Conference on, Aug 2015, pp. 1–8.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Automatic linguistic reporting in driving simulation environments",
      "author" : [ "L. Eciolaza", "M. Pereira-Fariña", "G. Trivino" ],
      "venue" : "Applied Soft Computing, vol. 13, no. 9, pp. 3956 – 3967, 2013.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Latent connectives in human decision making",
      "author" : [ "H.-J. Zimmermann", "P. Zysno" ],
      "venue" : "Fuzzy Sets and Systems, vol. 4, no. 1, pp. 37 – 51, 1980.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 1980
    }, {
      "title" : "On ordered weighted averaging aggregation operators in multicriteria decisionmaking",
      "author" : [ "R. Yager" ],
      "venue" : "Systems, Man and Cybernetics, IEEE Transactions on, vol. 18, no. 1, pp. 183–190, Jan 1988.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 1988
    }, {
      "title" : "Fuzzy sets and systems + natural language generation: A step forward in the linguistic description of time series",
      "author" : [ "N. Marı́n", "D. Sánchez" ],
      "venue" : "Fuzzy Sets and Systems, vol. 285, pp. 1 – 5, 2016, special Issue on Linguistic Description of Time Series. [Online]. Available: http://www.sciencedirect.com/science/article/pii/S0165011415005758",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Real versus template-based natural language generation: A false opposition?",
      "author" : [ "K. Van Deemter", "E. Krahmer", "M. Theune" ],
      "venue" : "Computational Linguistics,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2005
    }, {
      "title" : "A model based on computational perceptions for the generation of linguistic descriptions of data",
      "author" : [ "A. Ramos-Soto", "M. Pereira-Farina", "A. Bugarin", "S. Barro" ],
      "venue" : "Fuzzy Systems (FUZZ-IEEE), 2015 IEEE International Conference on, 2015, pp. 1–8.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Automatic generation of textual summaries from neonatal intensive care data",
      "author" : [ "F. Portet", "E. Reiter", "A. Gatt", "J. Hunter", "S. Sripada", "Y. Freer", "C. Sykes" ],
      "venue" : "Artificial Intelligence, vol. 173, no. 7-8, pp. 789–816, May 2009.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Automatic generation of natural language nursing shift summaries in neonatal intensive care: Bt-nurse",
      "author" : [ "J. Hunter", "Y. Freer", "A. Gatt", "E. Reiter", "S. Sripada", "C. Sykes" ],
      "venue" : "Artificial Intelligence in Medicine, vol. 56, no. 3, pp. 157 – 172, 2012.",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "An approach to generating summaries of time series data in the gas turbine domain",
      "author" : [ "J. Yu", "J. Hunter", "E. Reiter", "S. Sripada" ],
      "venue" : "Proceedings of IEEE International Conference on Info-tech & Info-net (ICII2001), Beijing, 2001, pp. 44–51.",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Sumtime-turbine: A knowledge-based system to communicate gas turbine time-series data",
      "author" : [ "J. Yu", "E. Reiter", "J. Hunter", "S. Sripada" ],
      "venue" : "Developments in Applied Artificial Intelligence, ser. Lecture Notes in Computer Science, P. Chung, C. Hinde, and M. Ali, Eds. Springer Berlin Heidelberg, 2003, vol. 2718, pp. 379–384.",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "On the role of fuzzy quantified statements in linguistic summarization",
      "author" : [ "F. Dı́az-Hermida", "A. Ramos-Soto", "A. Bugarı́n" ],
      "venue" : "Proceedings of 11th International Conference on. Intelligent Systems Design and Applications (ISDA), 2011, pp. 166–171.",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "A computational approach to fuzzy quantifiers in natural languages",
      "author" : [ "L. Zadeh" ],
      "venue" : "Comput. Math. Appl., vol. 9, pp. 149–184, 1983.",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 1983
    }, {
      "title" : "Fuzzy quantification: a state of the art",
      "author" : [ "M. Delgado", "M.D. Ruiz", "D. Sánchez", "M.A. Vila" ],
      "venue" : "Fuzzy Sets and Systems, vol. 242, no. 0, pp. 1 – 30, 2014, theme: Quantifiers and Logic.",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Characterizing quantifier fuzzification mechanisms: a behavioral guide for practical applications",
      "author" : [ "F. Diaz-Hermida", "M. Pereira-Fariña", "J.C. Vidal", "A. Ramos-Soto" ],
      "venue" : "2016. [Online]. Available: https://arxiv.org/abs/1605.03506”",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Using spatial reference frames to generate grounded textual summaries of georeferenced data",
      "author" : [ "R. Turner", "S. Sripada", "E. Reiter", "I.P. Davy" ],
      "venue" : "Proceedings of the Fifth International Natural Language Generation Conference, ser. INLG ’08. Stroudsburg, PA, USA: Association for Computational Linguistics, 2008, pp. 16–24.",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Selecting the content of textual descriptions of geographically located events in spatio-temporal weather data",
      "author" : [ "——" ],
      "venue" : "Applications and Innovations in Intelligent Systems, vol. XV, pp. 75–88, 2007.",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Generating approximate geographic descriptions",
      "author" : [ "R. Turner", "S. Sripada", "E. Reiter" ],
      "venue" : "Empirical Methods in Natural Language Generation, ser. Lecture Notes in Computer Science, E. Krahmer and M. Theune, Eds. Springer Berlin Heidelberg, 2010, vol. 5790, pp. 121–140.",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Application of fuzzy quantifiers in image processing: A case study",
      "author" : [ "I. Glöckner", "A. Knoll" ],
      "venue" : "Proceedings of the Third International Conference on Knowledge-Based Intelligent Information Engineering Systems, 1999, pp. 259–262.",
      "citeRegEx" : "38",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Describing images via linguistic features and hierarchical segmentation",
      "author" : [ "R. Castillo-Ortega", "J. Chamorro-Martı́nez", "N. Marı́n", "D. Sánchez", "J.M. Soto-Hidalgo" ],
      "venue" : "Fuzzy Systems (FUZZ), 2010 IEEE International Conference on, 2010, pp. 1–8.",
      "citeRegEx" : "39",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Information Processing and Management of Uncertainty in Knowledge-Based Systems: 15th International Conference, IPMU",
      "author" : [ "A. Wilbik", "U. Kaymak" ],
      "venue" : "Proceedings, Part II. Cham: Springer International Publishing,",
      "citeRegEx" : "40",
      "shortCiteRegEx" : "40",
      "year" : 2014
    }, {
      "title" : "Automatic linguistic descriptions of meteorological data: a soft computing approach for converting open data to open information",
      "author" : [ "A. Ramos-Soto", "A. Bugarı́n", "S. Barro", "F. Diaz-Hermida" ],
      "venue" : "8th Iberian Conference on Information Systems and Technologies, June 2013, pp. 1–6.",
      "citeRegEx" : "41",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Introducing shared tasks to nlg: The tuna shared task evaluation challenges",
      "author" : [ "A. Gatt", "A. Belz" ],
      "venue" : "Empirical Methods in Natural Language Generation, ser. Lecture Notes in Computer Science, E. Krahmer and M. Theune, Eds. Springer Berlin Heidelberg, 2010, vol. 5790, pp. 264–293.",
      "citeRegEx" : "42",
      "shortCiteRegEx" : null,
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "In this regard, recent studies [1] indicate that non-specialized users actually strongly demand textual descriptions of data as a means for better understanding of graphics and visualizations.",
      "startOffset" : 31,
      "endOffset" : 34
    }, {
      "referenceID" : 2,
      "context" : "The discipline of linguistic description or summarization of data (LDD) [4] has become in recent times a very promising approach to capture the essential information residing in numerical data.",
      "startOffset" : 72,
      "endOffset" : 75
    }, {
      "referenceID" : 3,
      "context" : "This has led to an apparition of a very diverse collection of use cases in very different domains (a rather representative review of such approaches can be found in [5], [6]).",
      "startOffset" : 165,
      "endOffset" : 168
    }, {
      "referenceID" : 4,
      "context" : "This has led to an apparition of a very diverse collection of use cases in very different domains (a rather representative review of such approaches can be found in [5], [6]).",
      "startOffset" : 170,
      "endOffset" : 173
    }, {
      "referenceID" : 5,
      "context" : "At the same time, the natural language generation (NLG) field [7], which addresses the creation of systems able to automatically deliver texts indistinguishable from those produced by humans, is currently experiencing a bursting scientific, technical and commercial expansion in its data-to-text (D2T) specialty [5], [8] due to the rise of the Big Data era.",
      "startOffset" : 62,
      "endOffset" : 65
    }, {
      "referenceID" : 3,
      "context" : "At the same time, the natural language generation (NLG) field [7], which addresses the creation of systems able to automatically deliver texts indistinguishable from those produced by humans, is currently experiencing a bursting scientific, technical and commercial expansion in its data-to-text (D2T) specialty [5], [8] due to the rise of the Big Data era.",
      "startOffset" : 312,
      "endOffset" : 315
    }, {
      "referenceID" : 6,
      "context" : "At the same time, the natural language generation (NLG) field [7], which addresses the creation of systems able to automatically deliver texts indistinguishable from those produced by humans, is currently experiencing a bursting scientific, technical and commercial expansion in its data-to-text (D2T) specialty [5], [8] due to the rise of the Big Data era.",
      "startOffset" : 317,
      "endOffset" : 320
    }, {
      "referenceID" : 6,
      "context" : "It is safe to assume that the D2T solutions provided by NLG companies do not include any uncertainty or vagueness management [8].",
      "startOffset" : 125,
      "endOffset" : 128
    }, {
      "referenceID" : 1,
      "context" : "Snapshot of the learning analytics dashboard SoftLearn [2], [3], which displays metrics and data plots a teacher must interpret to assess how students perform in a course.",
      "startOffset" : 60,
      "endOffset" : 63
    }, {
      "referenceID" : 7,
      "context" : "from a linguistic perspective, the problem of how to address vagueness is still an open issue which is being actively researched in this discipline [9]–[12].",
      "startOffset" : 148,
      "endOffset" : 151
    }, {
      "referenceID" : 10,
      "context" : "from a linguistic perspective, the problem of how to address vagueness is still an open issue which is being actively researched in this discipline [9]–[12].",
      "startOffset" : 152,
      "endOffset" : 156
    }, {
      "referenceID" : 11,
      "context" : "Examples from both trends include the GALiWeather [13] system, conceived as a LDD approach but made use of several NLG techniques to generate short-term weather forecasts; and the model presented in [14] that involves fuzzy temporal constraint networks and experimental data from three languages to address the problem of generating uncertain temporal expressions from an NLG point of view.",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 12,
      "context" : "Examples from both trends include the GALiWeather [13] system, conceived as a LDD approach but made use of several NLG techniques to generate short-term weather forecasts; and the model presented in [14] that involves fuzzy temporal constraint networks and experimental data from three languages to address the problem of generating uncertain temporal expressions from an NLG point of view.",
      "startOffset" : 199,
      "endOffset" : 203
    }, {
      "referenceID" : 13,
      "context" : "Furthermore, we will also resume the discussion opened by Kacprzyk and Zadrożny in [15], [16], where a relationship between LDD and NLG was discussed for the first time at a conceptual level, and which was later expanded by [5], [6], which provided additional insights and state of the art reviews for both fields.",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 14,
      "context" : "Furthermore, we will also resume the discussion opened by Kacprzyk and Zadrożny in [15], [16], where a relationship between LDD and NLG was discussed for the first time at a conceptual level, and which was later expanded by [5], [6], which provided additional insights and state of the art reviews for both fields.",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 3,
      "context" : "Furthermore, we will also resume the discussion opened by Kacprzyk and Zadrożny in [15], [16], where a relationship between LDD and NLG was discussed for the first time at a conceptual level, and which was later expanded by [5], [6], which provided additional insights and state of the art reviews for both fields.",
      "startOffset" : 224,
      "endOffset" : 227
    }, {
      "referenceID" : 4,
      "context" : "Furthermore, we will also resume the discussion opened by Kacprzyk and Zadrożny in [15], [16], where a relationship between LDD and NLG was discussed for the first time at a conceptual level, and which was later expanded by [5], [6], which provided additional insights and state of the art reviews for both fields.",
      "startOffset" : 229,
      "endOffset" : 232
    }, {
      "referenceID" : 5,
      "context" : "Section III studies in depth how the use of LDD techniques (namely fuzzy quantified statements) relate to the NLG tasks described by the well-known and widely accepted NLG system architecture proposed by Reiter and Dale in [7], as well as potential implications derived from such usage.",
      "startOffset" : 223,
      "endOffset" : 226
    }, {
      "referenceID" : 2,
      "context" : "Such descriptions are usually based on the concept of fuzzy quantified statement [4], which is usually classified using Zadeh’s notion of protoform [17].",
      "startOffset" : 81,
      "endOffset" : 84
    }, {
      "referenceID" : 15,
      "context" : "Such descriptions are usually based on the concept of fuzzy quantified statement [4], which is usually classified using Zadeh’s notion of protoform [17].",
      "startOffset" : 148,
      "endOffset" : 152
    }, {
      "referenceID" : 13,
      "context" : "Although protoforms have also been used in the context of fuzzy queries [15], [16], we will focus on their application in the automatic extraction of relevant linguistic information, that is, approaches applied to practical use cases in particular knowledge domains, where the structure of the linguistic descriptions which are obtained is known a priori.",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 14,
      "context" : "Although protoforms have also been used in the context of fuzzy queries [15], [16], we will focus on their application in the automatic extraction of relevant linguistic information, that is, approaches applied to practical use cases in particular knowledge domains, where the structure of the linguistic descriptions which are obtained is known a priori.",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 4,
      "context" : "Following this definition, fuzzy quantified statements in the literature have been used to a great extent as a means to summarize and describe time series of data [6].",
      "startOffset" : 163,
      "endOffset" : 166
    }, {
      "referenceID" : 14,
      "context" : "This issue was identified by Kacprzyk and Zadrożny in [16], where they stated that “Zadeh’s protoforms are very powerful and convenient in CWW but may be a limiting factor in many real-world applications as their structure is too restricted, notably as compared to the richness of natural language”.",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 14,
      "context" : "In this regard, Kacprzyk and Zadrożny proposed in [16] to define new types of protoforms “to make a full use of the power of NLG tools”.",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 11,
      "context" : "For instance, the textual weather forecast generator GALiWeather [13] employs type-1 fuzzy quantified sentences to perform a global description of the cloud coverage variable, but it also uses different crisp approaches to extract the relevant information from other variables such as precipitation or temperature.",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 14,
      "context" : "This issue is directly related to a relevant concept also noted in [16], namely the domain-modeling.",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 16,
      "context" : "When to use fuzzy/LDD techniques as part of an NLG system? Corpus analysis [18]: Determine which expressions in the language requirements can be modeled with protoforms.",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 16,
      "context" : "How to build fuzzy sets based on empirical knowledge? Distinct approaches: • Analysis of parallel corpora of texts and corresponding data [18].",
      "startOffset" : 138,
      "endOffset" : 142
    }, {
      "referenceID" : 17,
      "context" : "Alternatively, automatically learn such definitions (for instance, as it is done in TSK rule-based systems [19]).",
      "startOffset" : 107,
      "endOffset" : 111
    }, {
      "referenceID" : 12,
      "context" : "• Perform experiments to obtain empirical definitions from experts or human users [14], [20].",
      "startOffset" : 82,
      "endOffset" : 86
    }, {
      "referenceID" : 11,
      "context" : "[13], [21].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 18,
      "context" : "[13], [21].",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 16,
      "context" : "For instance, for the development of the NLG system SumTime-Mousan [18], Reiter et al.",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 12,
      "context" : "[14], [20]).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 16,
      "context" : "2This technique is commonly known in NLG as “corpus” analysis and is performed systematically in the early conception stages of an NLG system [18].",
      "startOffset" : 142,
      "endOffset" : 146
    }, {
      "referenceID" : 19,
      "context" : "operators proposed by Zimmermman [22] or the OWA operators by Yager [23].",
      "startOffset" : 33,
      "endOffset" : 37
    }, {
      "referenceID" : 20,
      "context" : "operators proposed by Zimmermman [22] or the OWA operators by Yager [23].",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 3,
      "context" : "Although LDD faces important challenges (Table I summarizes the main aforementioned issues and hints for addressing them), there is an important consensus in this field about its practical viability based on the use of NLG tools [5], [6], [16], [24].",
      "startOffset" : 229,
      "endOffset" : 232
    }, {
      "referenceID" : 4,
      "context" : "Although LDD faces important challenges (Table I summarizes the main aforementioned issues and hints for addressing them), there is an important consensus in this field about its practical viability based on the use of NLG tools [5], [6], [16], [24].",
      "startOffset" : 234,
      "endOffset" : 237
    }, {
      "referenceID" : 14,
      "context" : "Although LDD faces important challenges (Table I summarizes the main aforementioned issues and hints for addressing them), there is an important consensus in this field about its practical viability based on the use of NLG tools [5], [6], [16], [24].",
      "startOffset" : 239,
      "endOffset" : 243
    }, {
      "referenceID" : 21,
      "context" : "Although LDD faces important challenges (Table I summarizes the main aforementioned issues and hints for addressing them), there is an important consensus in this field about its practical viability based on the use of NLG tools [5], [6], [16], [24].",
      "startOffset" : 245,
      "endOffset" : 249
    }, {
      "referenceID" : 22,
      "context" : "This trend has been reinforced in recent years, as more work in LDD has adopted the use of template-based NLG approaches [25] to generate actual texts (e.",
      "startOffset" : 121,
      "endOffset" : 125
    }, {
      "referenceID" : 11,
      "context" : "[13], [21]).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 18,
      "context" : "[13], [21]).",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 14,
      "context" : "This was also noted by Kacprzyk and Zadrożny in [16]: “.",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 7,
      "context" : "[9]–[12]).",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 10,
      "context" : "[9]–[12]).",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 12,
      "context" : "However, it is not clear why fuzzy sets and derived disciplines such as LDD have not been considered by the NLG community until very recently [14], despite being intuitively appropriate for this task.",
      "startOffset" : 142,
      "endOffset" : 146
    }, {
      "referenceID" : 5,
      "context" : "The most widely accepted classification of this task division is the architecture proposed by Reiter and Dale in [7], where NLG systems are characterized as a pipeline composed of several parts which deal with different aspects of the NLG process.",
      "startOffset" : 113,
      "endOffset" : 116
    }, {
      "referenceID" : 5,
      "context" : "Natural language generation task pipeline according to Reiter and Dale [7], and potential LDD connections.",
      "startOffset" : 71,
      "endOffset" : 74
    }, {
      "referenceID" : 5,
      "context" : "Of all the subtasks described in [7] by Reiter and Dale, content determination emerges as the more intuitively related to LDD.",
      "startOffset" : 33,
      "endOffset" : 36
    }, {
      "referenceID" : 14,
      "context" : "Kacprzyk and Zadrożny in [16] introduced this notion in terms of the more general concept of text planning, while [5], [6] explored in more detail the role of LDD as a means to perform content determination while managing at the same time the problem of imprecision in natural language.",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 3,
      "context" : "Kacprzyk and Zadrożny in [16] introduced this notion in terms of the more general concept of text planning, while [5], [6] explored in more detail the role of LDD as a means to perform content determination while managing at the same time the problem of imprecision in natural language.",
      "startOffset" : 114,
      "endOffset" : 117
    }, {
      "referenceID" : 4,
      "context" : "Kacprzyk and Zadrożny in [16] introduced this notion in terms of the more general concept of text planning, while [5], [6] explored in more detail the role of LDD as a means to perform content determination while managing at the same time the problem of imprecision in natural language.",
      "startOffset" : 119,
      "endOffset" : 122
    }, {
      "referenceID" : 5,
      "context" : "We will illustrate this through a use case inspired by both the WEATHERREPORTER design study provided in [7] and the LDD example presented in [26], where a generic model to approach LDD in the context of performing content determination tasks was proposed.",
      "startOffset" : 105,
      "endOffset" : 108
    }, {
      "referenceID" : 23,
      "context" : "We will illustrate this through a use case inspired by both the WEATHERREPORTER design study provided in [7] and the LDD example presented in [26], where a generic model to approach LDD in the context of performing content determination tasks was proposed.",
      "startOffset" : 142,
      "endOffset" : 146
    }, {
      "referenceID" : 24,
      "context" : "This is especially true in the case of D2T systems, such as BabyTalk [27], [28] or SumTime-Turbine [29], [30]).",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 25,
      "context" : "This is especially true in the case of D2T systems, such as BabyTalk [27], [28] or SumTime-Turbine [29], [30]).",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 26,
      "context" : "This is especially true in the case of D2T systems, such as BabyTalk [27], [28] or SumTime-Turbine [29], [30]).",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 27,
      "context" : "This is especially true in the case of D2T systems, such as BabyTalk [27], [28] or SumTime-Turbine [29], [30]).",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 5,
      "context" : "Following the concept of message described in [7], Fig.",
      "startOffset" : 46,
      "endOffset" : 49
    }, {
      "referenceID" : 28,
      "context" : "This involves determining the fulfillment degree (FD) of each quantified statement and additional criteria, such as the coverage degree (Cov) of the quantifiers [31].",
      "startOffset" : 161,
      "endOffset" : 165
    }, {
      "referenceID" : 29,
      "context" : "Particularly, in the previous calculations, Zadeh’s proposal was used [32]",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 29,
      "context" : "Fulfillment degrees were calculated using Zadeh’s approach [32] from the data shown in Fig.",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 30,
      "context" : "Although we are aware of their theoretical properties [33], which can be useful in this regard, this remains an open problem which should be solved in each specific case [34].",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 31,
      "context" : "Although we are aware of their theoretical properties [33], which can be useful in this regard, this remains an open problem which should be solved in each specific case [34].",
      "startOffset" : 170,
      "endOffset" : 174
    }, {
      "referenceID" : 5,
      "context" : "3Note that we are using actual natural language expressions to illustrate lexicalization, but this task actually involves creating new sets of structures which define an intermediate syntax between the original content messages and the actual text [7].",
      "startOffset" : 248,
      "endOffset" : 251
    }, {
      "referenceID" : 5,
      "context" : "3) Aggregation: In general, the aggregation task in NLG systems involves the creation of more complex sentences by merging simple phrase specifications [7].",
      "startOffset" : 152,
      "endOffset" : 155
    }, {
      "referenceID" : 19,
      "context" : "To which extent their meaning would be equivalent? And to which extent does using a specific fuzzy aggregation operator influence the semantics of the obtained stamement and its equivalence with the other sentence? Again, to decide which specific operator to use will be a non-trivial decision the experts will have to make if they choose to aggregate properties using fuzzy operators, especially when other kind of nonstandard operators have been shown to better resemble the way humans aggregate information [22], [23].",
      "startOffset" : 510,
      "endOffset" : 514
    }, {
      "referenceID" : 20,
      "context" : "To which extent their meaning would be equivalent? And to which extent does using a specific fuzzy aggregation operator influence the semantics of the obtained stamement and its equivalence with the other sentence? Again, to decide which specific operator to use will be a non-trivial decision the experts will have to make if they choose to aggregate properties using fuzzy operators, especially when other kind of nonstandard operators have been shown to better resemble the way humans aggregate information [22], [23].",
      "startOffset" : 516,
      "endOffset" : 520
    }, {
      "referenceID" : 5,
      "context" : "One could say that this latter specific problem is more related to lexicalization (choosing a natural language expression for a given more complex message) than aggregation, but even the well-defined NLG architecture by Reiter and Dale admits a high flexibility of opinions in similar task placement issues [7].",
      "startOffset" : 307,
      "endOffset" : 310
    }, {
      "referenceID" : 12,
      "context" : "However, in “a coldish interval from the 2nd to the 6th of January” we find a more interesting referring expression, which identifies an specific event by means of a fuzzy property (cold, expressed as “coldish”) and a temporal expression (which in this case is crisply defined, but could also be a fuzzy temporal expression in other situations [14], e.",
      "startOffset" : 344,
      "endOffset" : 348
    }, {
      "referenceID" : 32,
      "context" : "Could this region be described as “NW Spain”, “Galicia, Asturias and part of Castilla y León”, “to the north of Portugal” or simply “near Galicia”? Such geographical referring expressions have been generated through crisp approaches classically, such as the ones produced by the RoadSafe system [35], [36], which is able to generate complex expressions from geographical data like “in some far southern and southwestern places” [37].",
      "startOffset" : 295,
      "endOffset" : 299
    }, {
      "referenceID" : 33,
      "context" : "Could this region be described as “NW Spain”, “Galicia, Asturias and part of Castilla y León”, “to the north of Portugal” or simply “near Galicia”? Such geographical referring expressions have been generated through crisp approaches classically, such as the ones produced by the RoadSafe system [35], [36], which is able to generate complex expressions from geographical data like “in some far southern and southwestern places” [37].",
      "startOffset" : 301,
      "endOffset" : 305
    }, {
      "referenceID" : 34,
      "context" : "Could this region be described as “NW Spain”, “Galicia, Asturias and part of Castilla y León”, “to the north of Portugal” or simply “near Galicia”? Such geographical referring expressions have been generated through crisp approaches classically, such as the ones produced by the RoadSafe system [35], [36], which is able to generate complex expressions from geographical data like “in some far southern and southwestern places” [37].",
      "startOffset" : 428,
      "endOffset" : 432
    }, {
      "referenceID" : 35,
      "context" : "Fuzzy sets and LDD can help address such problems by modeling fuzzy geographical descriptors which can be combined to generate similar geographical referring expressions which consider the graduality of such geographical concepts [38].",
      "startOffset" : 230,
      "endOffset" : 234
    }, {
      "referenceID" : 6,
      "context" : "For instance, in D2T systems which generate texts from time series data, language tends to be simpler [8] and relevant information such as events can be clearly identified through the use of temporal expressions.",
      "startOffset" : 102,
      "endOffset" : 105
    }, {
      "referenceID" : 36,
      "context" : "In others, such as the aforementioned geographical NLG systems or visual systems in general [39], this complexity will be higher (see Fig.",
      "startOffset" : 92,
      "endOffset" : 96
    }, {
      "referenceID" : 14,
      "context" : "In this regard, we would like to retake the idea proposed by Kacprzyk and Zadrożny in [16] about the creation of new kinds of protoforms [40], which was discussed in Sec.",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 37,
      "context" : "In this regard, we would like to retake the idea proposed by Kacprzyk and Zadrożny in [16] about the creation of new kinds of protoforms [40], which was discussed in Sec.",
      "startOffset" : 137,
      "endOffset" : 141
    }, {
      "referenceID" : 38,
      "context" : "Although this kind of contrast and emphasizing relationships between different content elements were considered in [41] from a fuzzy perspective, further research could be made to adapt this kind of discourse relations into new types of protoforms.",
      "startOffset" : 115,
      "endOffset" : 119
    }, {
      "referenceID" : 39,
      "context" : "an NLG system will probably be unfeasible, which opens the possibility of including such assessment during the subsequent standard evaluation process of the system [42].",
      "startOffset" : 164,
      "endOffset" : 168
    }, {
      "referenceID" : 3,
      "context" : "In this regard, two different types of evaluations may be performed: i) intrinsic evaluations, which focus on the quality and appropriateness of the texts generated by the system, and ii) extrinsic evaluations, which try to measure the extent to which the system is actually useful and impactful for its users [5].",
      "startOffset" : 310,
      "endOffset" : 313
    }, {
      "referenceID" : 2,
      "context" : "LDD was originally conceived as “linguistic summarization of data” [4] and this name is still used in many fuzzy sets research papers.",
      "startOffset" : 67,
      "endOffset" : 70
    }, {
      "referenceID" : 4,
      "context" : "Other authors are considering LDD as an alternate approach which actually reunites NLG/D2T and what we understand as LDD [6].",
      "startOffset" : 121,
      "endOffset" : 124
    }, {
      "referenceID" : 0,
      "context" : "Empiric studies [1] show that visual information alone is not always capable of adequately communicating relevant information about data to users.",
      "startOffset" : 16,
      "endOffset" : 19
    } ],
    "year" : 2016,
    "abstractText" : "We explore the implications of using fuzzy techniques (mainly those commonly used in the linguistic description/summarization of data discipline) from a natural language generation perspective. For this, we provide an extensive discussion of some general convergence points and an exploration of the relationship between the different tasks involved in the standard NLG system pipeline architecture and the most common fuzzy approaches used in linguistic summarization/description of data, such as fuzzy quantified statements, evaluation criteria or aggregation operators. Each individual discussion is illustrated with a related use case. Recent work made in the context of cross-fertilization of both research fields is also referenced.",
    "creator" : "LaTeX with hyperref package"
  }
}