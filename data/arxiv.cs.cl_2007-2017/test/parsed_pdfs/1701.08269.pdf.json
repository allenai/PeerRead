{
  "name" : "1701.08269.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Systems of natural-language-facilitated human-robot cooperation: A review",
    "authors" : [ "Rui Liu", "Xiaoli Zhang" ],
    "emails" : [ "xlzhang}@mines.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "submitted to Knowledge-based Systems, January 2017\nshare knowledge between a human and a robot for conducting intuitive human-robot cooperation (HRC), is continuously developing in the recent decade. Currently, NLC is used in several robotic domains such as manufacturing, daily assistance and health caregiving. It is necessary to summarize current NLC-based robotic systems and discuss the future developing trends, providing helpful information for future NLC research. In this review, we first analyzed the driving forces behind the NLC research. Regarding to a robot’s cognition level during the cooperation, the NLC implementations then were categorized into four types “NL-based control, NL-based robot training, NL-based task execution, NL-based social companion” for comparison and discussion. Last based on our perspective and comprehensive paper review, the future research trends were discussed. Keywords: robotic systems, natural language, human-robot cooperation, control, task execution, social companion\n1. Introduction\n1.1 Advantages\nNatural-language-facilitated human-robot cooperation (NLC), in which a human uses either spoken or written instructions to communicate with a robot for task cooperation [1][2][3][4], has received increasing attention in humaninvolved robotics research. By using natural language (NL), human intelligence at high-level mental planning and robot capability at low-level physical execution are combined to perform an intuitive cooperation [5][6].\nCompared with human-robot cooperation (HRC) using tactile indications such as contact location [7] and force strength [8][9][10], and visual indications such as body pose [11][12][13] and motion [14][15][16], HRC using NL indications has several advantages. First, NL makes the HRC natural. For the traditional methods mentioned above, the human involved in HRC needs to be trained to use certain actions/poses for making themselves understandable [17][18][19][20][21] . While in NLC, even non-expert users without prior training can cooperate with a robot by using human-like communication [3][22][23] . Second, the cooperation request is described accurately. The traditional methods using actions/poses only provide limited patterns to roughly describe cooperation requests due to the information loss in the action/pose simplification (such as using markers to simplify the actions) [24][25][26][27]. While in NLC, cooperation requests related to action, speed, tool and location are already defined in NL expressions [5][28][29][30]. With these expressions, cooperation requests for various task executions are described accurately. Third, NL transfers cooperation requests efficiently. The information-transferring method using actions/poses requires the design of informative patterns for different cooperation requests [24][25][26][27]. While existing languages, such as English, Chinese and German, already have standard linguistic structures, which contain abundant informative expressions to serve as patterns [31][32]. NL-based methods do not need to design specific informative patterns for various cooperation requests, making HRC efficient. Lastly, since the instructions are delivered orally instead of being physically involved, human hands are set free to perform more important executions [33]. Attracted by the above advantages, NLC has been widely explored in areas, including daily assistances [1][33][35], medical caregiving [36][37][38][39], manufacturing [5][40][41], indoor/outdoor navigation [2][42][43], social accompany [44][45][46] etc. Typical areas using NLC systems are shown in Fig. 1.\nFrom a realization perspective, the pushing forces for recent NLC developments are concluded as natural language\nprocessing (NLP) developments and HRC developments.\n1.2. Pushing force one: development of NLP\nRecently, supported by machine learning technics in classification [47], clustering [48] and feature extraction [49], NLP has been developed from simply syntax-driven processing, which builds syntax representations of sentence\n___________________________ *Corresponding author: Tel.:+1-303-384-2343; fax:+1-303-273-3602, email:xlzhang@mines.edu.\nsubmitted to Knowledge-based Systems, January 2017\nstructures, to semantically-driven processing, which builds semantic networks for sentence meanings [50].\nAt the early stage (1950-2000) of NLP [51], a word-based understanding method was developed to enable a naïve word-symbol understanding by identifying single/multiple keywords [52][53], lexical affinities [54][55], and word/affinity occurrences [56][57]. The word-based NLP method separately understood word meanings, and sentence meaning was unknown. For example, with the word-based method, a robot understood when a human mentioned the word “cup”, but it did not understand the related requests such as “I need a cup of water”[15]. Moreover, this method relied on training samples. If the available training samples are limited, thereby leading to the ignorance of some keywords, the meaning understanding will be poor [51]. These two drawbacks limited robots’ understanding to a shallow level where only symbolic NL expressions were analyzed [58][59].\nCompared with the word-based understanding method, which enabled a shallow-level understanding, a conceptbased method allowed a comprehensive understanding of NL expressions. The concept-based method modeled the meanings for sentences by exploring the embedded concepts, which mainly included implicit NL indications [60][61], hierarchical ontologies [62][63], and semantic correlations [64][65]. The concept-based method not only understood explicit facts such as involvements of action/object/events/persons, but also understood the implicit indications such as action purpose, object usage, event meaning, and human intentions. The concept-based method was widely used in research such as [5][6][23] for complicated NL expression analysis. Compared with the understanding supported by the keyword-based method, the concept-based method endowed a robot with a relatively in-depth understanding of the meanings in NL expressions. However, due to the limited consideration of implicit logic correlations in NL expressions, the knowledge represented by the concept-based method cannot model a structural knowledge [5][17][40] This drawback limited robots’ understanding towards task procedures and task-world correlations and further limited NLC in practical situations in the real world.\nTo support a practical implementation of NL understanding, a narrative-based method was developed to create a more sophisticated knowledge representation in a decision-making-focused [68], real-world-aware [69] and humancognition-imitated manner [70][71]. In this method, the mechanisms of human reasoning and planning, knowledgeto-world mapping, and logic-based human understanding & learning were focused on in the NLP process. Supported by this method, knowledge was practically used in NLC, further improving robot’s knowledge scalability and humanrobot-cooperation flexibility.\nThe developments of NLP techniques are shown in Fig. 2, with detailed time labels.\n1.3. Pushing force two: development of HRC\nIn an early period (about 1940s’[72]), humans started to cooperate with robots by using remote controllers, developing an initial HRC, in which the cooperation requirements for action mapping, task goal mapping, and cooperation naturalness/effectiveness were not considered. As the tasks became complicated, both the robot and the human in HRC were assigned with different roles, such as leader-follower and cooperator-cooperator, to perform different parts of a task with considerations of task goal accomplishments, human-robot communications, robot/human statuses and physical/mental capabilities. Compared with HRI, which focuses on general interactions (detailed HRI reviews are in [4][73][74][75]) for physical/mental assistances without task-goal constrains, HRC focuses on specific cooperation for task fulfillment with task-goal constrains such as task planning and adjusting (detailed HRC reviews are [72][76][77]). In this paper, we emphasized on HRC, specifically exploring the state-of-the-art robotic systems\nsubmitted to Knowledge-based Systems, January 2017\nusing NL to facilitate HRC in various research domains.\nComprehensively, HRC has been developing: it began largely from a low-cognition-level action research where actions were designed and selected according to human instructions, grew to a middle-cognition-level interaction research where shallow-level understanding of human motions, activities, tasks and safety concerns were conducted, and is expanding to a high-cognition-level human-centered engagement research where human’s psychological and cognitive statuses, such as attention, motivation, emotion and user models, are considered to improve the HRC effectiveness and naturalness. Increasing human involvements in HRC is shown in Fig. 3.\nFor action-based HRC, which started from motor-control-based action design, a robot followed simple human instructions to adjust its actions [78][79]. To make the robot actions natural, human-like motion style was then adopted in robot action design [80][81]. Though robots’ motion behavior was similar to that of a human, robots’ cooperation performances were still poor due to the limited action-understanding being insufficient to support its adaptations towards users/environments [82][83] . To improve robot’s adaptability, action interpretations were added for a robot in HRC. For example, ‘cup manipulation’ in ‘drinking’ activity meant ‘containing liquid’”[40]. Though the understanding was still limited into action level, robots’ understanding towards human behaviors in HRC was improved [84]. Overall, action-based HRC was targeting the design of actions in HRC with consideration for cooperation efficiency and naturalness. Due to the lack of human behavior understanding, action-based HRC was still at the imitating level and lacked goal/execution motivation interpretations. Therefore, action-based HRC had a lowlevel cognition requirement towards robots and could not support an intuitive cooperation.\nFor interaction-based HRC, which started from action-understanding-based movement imitation [85][86], a robot was required to learn from human demonstrations, understand human movements and develop its own movements. To improve the understanding of human movements, robots were provided with various informative motion data such as human action trajectories [87], hand/body poses [88], and bio-signals [89]. To further improve robots’ cooperation performance, robots were trained to explore the mutual influence between a human and his/her surrounding environment [90][91][92][93][94]. Overall, interaction-based HRC improved the cooperation from the naïve predefinition stage to the current intuitive interaction stage. Robot understanding toward human behaviors was improved by using various informative data and considering multiple influential factors from environments. With a comprehensive understanding, a robot was closely associated with a human by correctly identifying the cooperation requests and appropriately providing the assistance.\nFor engagement-based HRC, individual-level factors, such as individual attentions [95][96], personalities [97][98], emotions [99][100] and safety factors [83][101] , were considered by robots in the cooperation. With the engagement-based HRC method, robot cognition was further improved by adapting various individuals. This method is widely used in present-day HRC research. Given the cognition requirements towards robots in HRC, a natural and efficient cooperation manner such as NLC is in urgent need.\n1.4. Systematic overview of NLC research\nWith cross-disciplinary technique supports, NLC has been developed from low-cognition-level symbol matching control, such as using “yes/no” for controlling robotic execution, to high-cognition-level task understanding, such as “go straight and turn left at the second cross”.\nAs a result of NLC research, a substantial number of projects were launched, including “collaborative research:\nsubmitted to Knowledge-based Systems, January 2017\njointly learning language and affordances” in Cornell University [102], “robots that learn to communicate with humans through natural dialog” in the University of Texas at Austin [103], “collaborative research: modeling and verification of language-based interaction” in MIT [104], “language grounding in robotics” in University of Washington [105], “semantic systems” in Lund University [106] etc. NLC research is regularly published in international journals such as IJRR [107], TRO [108], AI [109] and KBS [110], and international conferences such as ICRA [111], IROS [112] and AAAI [113]. The publication trend is shown in Fig. 4, where the increasing significance of using NL to facilitate HRC is reflected by the steadily increasing publication numbers in the recent decade.\nCompared with the existing review papers about HRC using gesture/pose [114][115], action/motion [116], and tactile [117], a review paper about NLC is lacking. Given the promising potential and increasing attention of NLC, it is necessary to make a summary of the state-of-the-art robotic systems in wide-range domains, revealing the current research progress and signposting future NLC research. The organization of this review paper is shown in Fig. 5.\n2. Typical systems\nAs a human-robot-combined decision-making method, the NLC is widely implemented in various robotic systems. According to robot cognition level during the cooperation, NLC-based robotic systems are categorized into four main types: NL-based control, where only the NL-format control symbols are given and comprehensive instruction understating is not involved; NL-based robot training, where comprehensive instruction understanding is required and intuitive task execution is not conducted; human-guided task execution, where comprehensive understanding of human instructions, practical situation conditions and human intentions are required, and intuitive task execution is conducted; and NL-based social companion, where the understanding of social norms is required in addition to the NL-based execution conducting. Summary of the typical NLC systems is shown in Table 1.\nsubmitted to Knowledge-based Systems, January 2017\nTo set human hands free for other tasks and reduce human’s physical burden, NL was initially used to replace physical control means, such as joysticks and remote controllers, which required the use of human hands [33]. During the control, NL played a role as information-delivering media, which contains the human-desired robot executions,\nsubmitted to Knowledge-based Systems, January 2017\nshown in Fig. 6. A human user mainly made decisions in the control process, while a robot was controlled by detecting and following the controlling commands in human NL requests. From a burden-assessment perspective, during the whole cooperation process, the human mainly took the cognitive burdens, while the robot mainly took the physical burdens.\n3.1. Typical robotic systems\nControl using symbolic words. The idea of using human NL for robot control was proposed at the year 1992. NL was initially used in robot teleoperation [118], in which the correct robot actions were selected by a human to guide it to a desired destination. The system in paper [119] used NL instructions to plan task-execution procedures for a robot. The NL-based control for execution procedure planning was a word-to-action mapping process, which was discrete and the word-action associations were predefined in the robot’s database. In this process, a human was restricted to give symbolic and simple NL commands to a robot. The robot needed to accurately detect the symbolic words in human speech and then associate them with the predefined actions or action sequences. Typical NLC systems using symbolic word control include manipulation control [119][120] , motion trajectory control [121][122], navigation location & behavior control [88][123]etc. These works addressed the challenges in speech recognition, word disambiguation and word-action mapping. NL-based control was used to specify the detailed action-related parameters such as action direction, movement amplitude, motion speed, force intensity, and hand pose status. The NL-based control for action specification was the word-to-value mapping process, which was continuous and its value ranges were predefined in robots’ database. During the development of NL-based control, the mapping rules such as fuzzy [124]/strict [118] mappings were designed.\nControl using semantic correlations. To further improve the robustness of NL-based control during HRC, semantic correlations among the controlling symbols were explored by analyzing the linguistic structures of the NL commands. Controlling symbols include different types of actions, hand poses, objects etc. By exploring the semantic correlations, such as “grasp-cup” and “go-To-left” among these symbols [125], human commonsense was initially involved in robot control process, increasing the naturalness of the robot executions in scenarios such as navigations and manipulations. Moreover, the execution flexibility of a robot was also improved by extracting the general cooperation patterns such as “grasp(object)” [119][126] and “goTo(Location)” [2][127], improving robots’ adaptability towards instruction variety.\nControl using logic structures. Even though semantic correlation improved the flexibility of NL-execution mapping, the control performances in dynamic situations was still limited due to the ignorance of control logics among these semantic correlations. The logic ignorance made a robot incapable of adjusting itself to environmental changes and incapable of intuitively reasoning about the execution plans. For example, the NL instruction “fill the cup with water, deliver it to human” includes logics “search cup, then use water pot, last deliver cup”. The ignorance of the logics in the controlling process will lead to the wrong executions such as “use water pot, then search cup” or will restrict the correct executions such as “deliver cup, then use water pot” in dynamic environments, which will cause the removing/adding execution steps. To solve this problem, the logic correlations, including temporal logic, spatial logic and ontology logic among the controlling symbols, were explored to enhance the adaptability of the NL-based\nsubmitted to Knowledge-based Systems, January 2017\ncontrol method. Typical robotic systems using logic to facilitate robot control include modifying robot movement trajectory in different situations [127][131], designing robot manipulation post according to fuzzy action type/speed requirements [88][124], serving meals by considering “foodType–vesselShape” relations [130][132], and assembling industrial parts by considering the spatial matching relations [129][133] etc.\nControl using environmental conditions. Besides the logic relations among the control commands, it is necessary to consider environmental conditions for a practical NL-based control in real-world situations. The balance among human commands, robot knowledge and environment conditions should be made for an intuitive NLC. With the NLC systems, typical applications that consider practical environment conditions include grasping with considering constraints in safety, temporal relations and human preferences [58][134], navigating by considering location/building matching [20][135], serving food with consideration of user locations [136], food vessel shapes and path conditions [119] etc. In these systems, if NL commands and robot knowledge were supported by real world conditions, NLC tasks are highly likely to be successful, or NLC tasks are likely to fail.\n3.2 Open problems\nFor the NL-based robot control, a human interacted with a robot in a verbal manner. Simple NL control commands were given separately, or in a hybrid manner where the NL commands were combined with visual/haptic cues. A human was the only information source, guiding the whole control process. A robot was designed to simply map the human NL commands to the knowledge structure in robot databases, or to the real-world conditions perceived by robots’ sensors. With physical/mental work assignments for robots and humans, current efforts in NL-based control focus on improving control accuracy, decreasing humans’ cognition burdens and increasing a robot’s cognition burdens. However, the cognition burden of humans in NL-based control was still at a high level and robot cognition was still at a low level. A human user was required to lead the cooperation and the robot was required to follow the human with an understanding of control commands. The big cognition-level difference between a human and a robot restrained the intuitiveness and naturalness of the current NLC systems in the cooperation. Summary of NL-based control systems is shown in Table 2.\nTo support a robot in complex task planning and intuitive decision making, NL was used to train robots for task executions with a spoken/written manner. During the training, knowledge was transferred from robot/human experts to targeted robots, shown in Fig. 7. With consideration of robot physical capabilities such as force/strength/physical structure/speed, human preferences such as motion/emotion, and real-world conditions such as object availability/distribution/location, executable knowledge was specified for a robot. With the executable knowledge, robots’ capabilities in task understanding, environment interpretation and human-request reasoning are improved. Different from NL-based control, where a robot was not involving in advanced reasoning, in NL-based robot training, robots were required to reason about human cooperation requests from NL instructions. According to the knowledge transferring manners, systems using NL-based robot training are mainly categorized into four types: training using\nsubmitted to Knowledge-based Systems, January 2017\nhuman instruction, training using human demonstration, training using human feedback and training using proactive robot querying. In both the training using human instruction and human demonstration, robots passively learn from a human. The difference is that in instruction humans only orally describe and do not physically participate, while in demonstration, humans need to physically participate. In the training both using human feedback and proactive robot querying, robots proactively learn from a human. The difference is that in the training using human feedback a human decides what knowledge to learn, while in the training using robot querying a robot decides what knowledge to learn.\n4.1. Typical robotic systems\nTraining using human instruction. Given that human NL instructions are informative and natural, robot training initially started with using NL instructions to define task execution methods [137]. In the early stage of instruction training, these instructions were used to perform low-level knowledge grounding, in which short NL expressions given by humans were mapped into separated knowledge entities in the real-world. With the NLC systems, typical applications include using the NL instructions to identify an object [132][138], object physical properties [139][140], and action associations [141][142]. Low-level knowledge grounding endowed robots with a shallow understanding of the motivations and logics in the instructed task-execution procedures due to which the knowledge was mapped from a database to the real world in a point-to-point manner. Instead of correlating the execution procedures to form a semantic network for comprehensive execution understanding, the knowledge correlations in the low-level knowledge grounding were mutually independent. As information/automation techniques improved, the low-level knowledge grounding method was then evolved into the high-level knowledge grounding method, in which complex NL expressions were grounded into hierarchical knowledge structures for motivation/logic understanding. With the hierarchical knowledge, the instruction-based training for sophisticated task execution was enabled. With the NLC systems, typical applications include using the NL instructions to describe daily common sense such as precise object manipulation [143][144], to describe spatial/temporal correlations for intuitive navigations [145][146], and to model human cognition for daily activity performing [147][148] . During the high-level knowledge grounding, human cognition processes on task planning and performing were modeled. The advantage of the training using human NL instructions is that robots’ reasoning mechanisms during NLC is initially developed; the disadvantage is that the learned execution methods are still abstract in that the sensor-value-level specifications for the NL commands are\nsubmitted to Knowledge-based Systems, January 2017\nlacking, limiting the knowledge implementations in real-world situations.\nTraining using human demonstration. To improve robot training in practical perceiving and acting, a human demonstration method was developed to train a robot in real task-execution situations. With training in a humandemonstration manner, theoretical knowledge such as actions, action sequences and object weight/shape/color was associated with sensor data and data patterns. This theory-practice association enabled a straightforward, sensor-databased interpretation towards the abstract task-related knowledge, improving robots’ understanding and cooperation by practically implementing the learned knowledge. A general demonstration process was that a human physically performs a task and meanwhile orally explained the execution intuitions for a robot. The robot was expected to associate the NL-extracted knowledge with sensor data to specify the task executions. With the NLC systems, typical applications using the demonstration-based robot training method include: learning object-manipulation methods by associating human NL expressions with sensor data such as touching force values, object color/shape/size and visual trajectory [144][149]; learning human-like gestures by associating human hand gesture with speech context [150][151]; learning object functional usages by simultaneously considering human voice behaviors, motion behaviors and environment conditions [147]; learning abstract interpretations of environmental conditions by combining human operations, human NL explanations, and the corresponding sensor data patterns [21][151]; adapting new situations by replacing NL-instructed knowledge with real-world-available knowledge [36][152] etc. Human demonstration enabled a robot with a practical understanding of real-world task executions. Compared with robot training using instructions, robot training using demonstrations specified the abstract theoretical knowledge with the real-world, making the learned knowledge executable in real-world situations. However, robots’ reasoning capability was not largely improved since demonstration-based training was actually a sensor-data-level imitation of human behaviors and ignored the “unobservable human behaviors” such as human’s subjective interpretation towards real-world conditions, human’s philosophy in execution and human’s cognitive process in decision making.\nTraining using human feedback. To improve robots’ reasoning capability in NLC, human NL feedback was used to directly tell the robots “the unobservable human behaviors”. With human NL feedback, robot behaviors in cooperation were logically modified by adding/removing some operation steps [40][96][153] or subjectively emphasizing on executions [154][155][156]. With the NLC systems, typical applications include: indicating the human-desired locations/objects with NL instructions during HRC [153]; assessing robot execution performances and correcting robots’ undesired manipulation behaviors such as hand poses and object selections by using real-time NL instructions [96]; analyzing robots’ execution failures and helping a robot to learn from failures with NL conversations [40]; and emphasizing/ignoring robot behaviors in complex task execution by subjective NL rewards and punishments such as “joy, anger” [155]. Compared with training using human demonstrations, training using human feedback proactively and explicitly indicated a robot with operation logics and decision-making mechanisms, enabling better robot understanding towards human cognitive process in cooperation. Based on both human cognition understanding and environment perceiving, robots’ surrounding environments in NLC were interpreted as a human-centered situation. In this human-centered situation, task cooperation was interpreted in a human perspective, improving robots’ reasoning capability in cooperating with a human. However, the feedback-based learning required frequent human involvements, imposing a heavy cognition burden on a human. Moreover, the knowledge learned by human feedback was given by a human without considering the robot’s actual knowledge needs, limiting robots’ adaptation to new environments where robots’ knowledge shortage was waiting to be compensated for successful NLC.\nTraining using proactive querying. To solve the new situation adaptation problem for further improving a robot’s reasoning ability, a proactive-querying method was developed for robot training. In the querying process, a robot used NL to proactively query humans about its missing knowledge about human-intention disambiguation, environment interpretations, and knowledge-to-world mapping. After the training, a robot was endowed with a more targeted knowledge to adapt the previously-encountered situations, thereby improving a robot’s environment adaptability. With the NLC systems, typical applications using querying-based training include: asking for cognitive decisions on trajectory/action/pose selections in tasks such as “human-robot jointly carrying a bulky bumper” [157]; asking for knowledge disambiguation of human commands such as confirming the human-attended object “the blue cup” [158]; asking for human physical assistances to deliver a missing objects or to execute robot-incapable actions such as “deliver a table leg for a robot” [40]; asking for additional information such as “the object is yellow and rectangle” from a human to assisting robots’ perceiving [153][159] etc. Compared with the training using human instructions, demonstrations and feedback, in which a robot was trained passively, in the querying method a robot was trained proactively and knowledge was more situation-specific. Robots were endowed with an advanced self-improving capability. Supported by a never-ending mechanism, robot performances in NLC were improved in the long term by\nsubmitted to Knowledge-based Systems, January 2017\ncontinuous knowledge acquiring and refining [91].\n4.2. Open problems\nDuring the development of training methods starting from instruction training to querying training, human cognition burden was gradually decreased and robot cognition level was gradually improved. Robot trainings using the above-mentioned methods are suffering the shortcomings of robot knowledge scalability and adaptability. The knowledge scalability problem is caused by limited knowledge from limited information sources such as available humans/robots experts. The robot knowledge is hard to be largely scaled up with an economic manner. Knowledge adaptability problems are caused by the current shallow cognition models. It is hard for a robot to adapt to a specific user/situation and meanwhile adapt to the general types of users/situations. The potential solutions for the knowledge scalability problem and adaptability problem will be discussed in section 8.3 and 8.4. A summary of NL-based robot training systems is shown in Table 3.\nDifferent from NL-based robot training systems, in which human NL was helping a robot with its task understanding, in NL-based robot task execution systems, human NL was helping a robot with its task execution. In NL-based training, a robot created a structure-completed and execution-specified knowledge representation. But in NL-based task execution, including understanding the task, the robot was also required to interpret the surrounding environments, predict human intentions, and make optimal decisions satisfying the constraints from the environment, the task execution method and the human requirements. Typical robotic systems using NL-based task execution are shown in Fig. 8. Given that the reasoning was strictly requested in NL-based task execution, robots’ cognition levels in NL-based task execution were higher than that in NL-based robot training. With respect to whom is leading the execution, systems of NL-based execution are categorized into human-centered task execution, in which a human is mentally leading the task executions and a robot is providing appropriate assistances for facilitating human executions, and robot-centered task execution, where a robot is mentally leading the task executions and the human is providing oral reminders or physical assistances for facilitating robot executions.\nsubmitted to Knowledge-based Systems, January 2017\n5.1. Typical robotic systems\nHuman-centered task execution. Given the technology limitations in perceiving, reasoning and acting, a robot is still not fully automatized and human intelligence is still necessary in HRC. To integrate human mental intelligence and robot physical execution, NL-based execution systems were designed to be human-centered. NL expressions in task execution deliver information such as explanation of human’s cooperation requests, descriptions of human’s execution plans and indications of human’s urgent needs. With this information, a robot provides appropriate assistances timely. With NLC systems, typical applications using human-centered executions include performing tasks such as “table assembly”, during which the human sets up task goal (assembly a specific part), makes plans (action steps, pose and tool usages) and partially executes tasks (assemble the parts together), and the robot provides humandesired assistances (tool delivery, part delivery, part holding) [162][163]. During the cooperation, human took both cognitive and physical responsibilities, and the robot took partial physical responsibilities. Comprehensive humancentered execution was developed so that a human was only burdened with cognitive responsibilities such as explaining the navigation routine [164][165], describing the needed objects and location/pose [66][166] and guiding the fine/rough processing [5][167]. Correspondingly, a robot took on only physical responsibilities such as grasping/transferring the fragile/heavy objects [168][169]. Both the human and the robot performed independent subtasks by considering the same high-level task goal. The robot received less instructions for its tasks and meanwhile was expected to monitor human’s task processes so that the robot provided appropriate assistances when the human required it. This cooperation proposed a relatively-high standard towards robot cognition on providing appropriate assistances at the right location/time. Overall, in human-centered NL-based task execution, the human was leading the execution at the cognition level, and a robot provided the appropriate assistance for saving the human’s time and energy, thereby enhancing the human’s physical capability.\nRobot-centered task execution. To further improve the cooperation intuitiveness and decrease human’s mental/physical burdens, systems using robot-centered task execution were developed. Different from humancentered systems in which a human mainly took the cognitive/physical burdens and a robot gave human-needed assistances to facilitate human execution, in robot-centered systems, a robot mainly took the cognitive/physical burdens, and a human gives robot-needed assistance physically to facilitate the robot execution. NL expressions in the robot-centered systems were used for a robot to ask for assistances from a human. With the NLC systems, typical applications using robot-centered task execution include: robot lead industrial assembly, in which a human enhanced robot physical capability by providing robot with physical assistances such as grasping [170] and fetching [171]; robot executed tasks such as heavy object moving and elderly navigation in unstructured outdoor environments, in which a human analyzed and conquered the environment limitations such as objects/space availability [22][172][173][174]. Compared with robots in human-centered execution, where a robot was required to comprehensively understand human behaviors, robots in robot-centered applications were required to comprehensively understand the limitations on robot knowledge, real-world conditions, both humans’ and robots’ physical capabilities etc. The advantage was that the human was less involved and his/her hands/mind were partially set free.\nsubmitted to Knowledge-based Systems, January 2017\n5.2. Open problems\nAn open problem for human-centered task execution is that the human’s cognition burden is relatively heavy. Even though a robot become more autonomous to prove timely assistances, a human is still leading the NLC mentally/physically. An open problem for robot-centered task execution is that robot-centered applications set a high standard towards the robot capability on situation analyzing, plan making, assistance asking, and knowledge updating. While these robot capabilities are still immature [175]. Therefore, a NLC-based system involving human cognitive process modeling, intelligent robot decision-making, autonomous robotic task-execution, and human/robot physical capability consideration is in urgent need. Summary of NL-based task execution systems is shown in Table 4.\nTo make NLC natural and intuitive, NL-based social cooperation was developed by involving human’s social behaviors such as social speeches and social motions in NLC. Different from NL-based task execution systems, which focused on objective task information such as task goals and task execution procedures, NL-based social cooperation systems focused on subjective social norms such as social speech or motion behaviors, shown in Fig. 9. With social norms, robots were naturally integrated with a human in NLC and, meanwhile, robots’ social acceptances were increased. According to social norm types, robotic systems using NL-based social cooperation are categorized into social communication in which social NL expressions are used for facilitating communication, and social execution in which social NL expressions are used for facilitating execution.\n6.1. Typical robotic systems\nNL-based social communication. In a natural and intuitive NLC, robot-human communication needs to be both information-correct and social-norm appropriate. Capturing social norms from humans’ NL expressions was helpful in certain aspects such as detecting human preferences in cooperation, specifying cooperation roles such as “leader, follower, cooperator”, and increasing social acceptance. NL in social communications served as an information source, from which both the objective execution methods and the subjective human preferences were extracted. With the NLC systems, typical applications using NL-based social communication include: a receptionist robot increased its social acceptance in conference arrangements by using social dialogs with pleasant prosodic contours [180]; cooperative machine operations used social descriptions considering human action preferences [181][182]; health-caregiving robots searched and delivered objects by considering user speech confidences, user safety and user roles such as “primary user, bystander” [183]; adapted unfamiliar users by using NL expressions with fuzzy emotion statuses such as “fuzzy happiness/sadness/anger” [184]; modeled social NL communications in NLC by defining human-robot relations such as “love”, “friendship” and “marriage” [185]; designed robotic companion by using friendly NL conversation [67][186][187] etc.\nNL-based social execution. In a natural and intuitive NLC, the communication and the execution need to be socially appropriate. To introduce social execution norms into NLC, NL-based social execution systems were designed. NL was used to indicate socially-preferred executions for robots, enhancing robots’ understanding of social motivations behind task executions and further making robot executions socially-acceptable. With the NLC systems,\ntypical applications using NL-based social executions include: a navigation robot autonomously modified its motion behaviors (stop, slower, faster) by considering human density (crowded, dense) with the reminding of human NL instructions (“go ahead to move”, “stop”) [188]; a companion robot moved its head towards the human speaker according to human’s NL tunes [189]; a storytelling robot depictd stories by mapping NL expressions with human’s body motion behaviors to catch humans’ attention [178][190].\nNL-based social communication and NL-based social execution focused on two different aspects of NLC. To\ndevelop socially intuitive NLC systems, the two aspects need to be focused on simultaneously.\n6.2. Open problems\nSocial norms in both communications and executions are hard to model. First, social norms are implicit. It is challenging to summarize social norms from human behaviors. Second, social norms vary. Different regions, countries, cultures, races and personalities form different social norms. A social-norm model that considers the above influential factors is challenging to create because the representative norms are difficult to extract. Last, social norms are currently non-evaluable. It is challenging to assess the correctness of social norms because there are no clear standards to judge the correctness of social norms, and different persons have different levels of social behavior acceptance/tolerance degree. Summary of NL-based social cooperation systems is shown in Table 5.\nsubmitted to Knowledge-based Systems, January 2017\n7. Methods for realizing NLC\nDeveloping NLC systems are suffering from three main challenges. First, it is challenging to comprehensively understand human NL instructions. Understanding NL instructions is not about precise speech recognition, but instead precise semantic analysis, by which the meaning, logic and human cognitive process embedded in NL are extracted. Second, it is challenging to represent robots’ understanding to support robots’ decision-making in NLC. The representation of a robot’s understanding is expected to include the task-related knowledge contents and knowledge implementation manners. With the representations, a robot is able to measure the applicability of its knowledge in a situation and decide the correct contents and manners of using its knowledge. Third, it is challenging to accurately map a robot’s knowledge into real-world situations. In the mapping process, the theoretical items in knowledge databases are expected to be associated with the corresponding practical objects or relations in the real world, and the incomplete/real-world-inconsistent knowledge is expected to be autonomously corrected.\nTo address these three challenges, three types of research were conducted: NL understanding, knowledge representation, and knowledge-world mapping. With the NL understanding research, human verbal instructions were processed and the task-related knowledge, such as task goal, execution steps, and the execution parameters “speed, tools, locations, human requirements” were extracted to perform a comprehensive semantic analysis [5][172][192] . With the knowledge representation research, task knowledge was constructed by algorithms such as Bayesian Network [91][193], Support Vector Machine (SVM) [194][195], and Hidden Markov Model (HMM) [118][196]. By using these algorithm structures, the knowledge was capable of supporting robot decision making with various logic/semantic/spatial/temporal manners. With the knowledge-world mapping research, the theoretical information patterns were grounded into real-world situations correctly, and the incomplete/inconsistent knowledge gaps were filled to ensure the success of NLC. In theoretical knowledge grounding, the grounding methods aimed to explore the semantic correlations among the theoretical knowledge and the real-world by associating the temporal/spatial/visual/physical features [197][198] . In the gap filling, the methods aimed to detect the missing knowledge, which was needed in real-world situations but ignored in theoretical knowledge representations, and the inconsistent knowledge, which was instructed by a human but was not available in the real world [58][199] [200]. The complete NLC realization process by using the above-mentioned researches is shown in Fig. 10.\n8. Emerging trends\nNLC has been developed to improve the effectiveness and naturalness of HRC. Due to the limitations of NLCrelated techniques such as NLP, machine learning and robot design, NLC performances in dealing with complex tasks, various users, and dynamic/unstructured environments still needs to be improved. Based on our comprehensive review, the future trends for future NLC research are summarized as follows.\n8.1. NLP’s contributions to NLC\nComprehensively, NLP is undergoing a deep-neural-network revolution to create sophisticated computational sematic models, including the word embedding method for comprehensive meaning modeling by adding in extra\nsubmitted to Knowledge-based Systems, January 2017\nmeanings such as “cats and dogs are animals” in data preparation stages [201], sequence-to-sequence language understanding/translation by sequentially outputting the meaning-modeling results based on the previous semantic context [202], attention-based NL understanding in which the relatively important words/expressions are valued by increasing the weights of the important expressions and decreasing the weights of the unimportant expressions [203], unsupervised long-term meaning modeling [204] etc.\nWith the advantages brought by the NLP revolution, NLC is correspondingly benefited in areas including: complex cooperation request understanding by adding task-specific knowledge such as manufacturing common sense; daily assistance common sense and caregiving common sense into the input NL data; real-time and context-aware task execution by aligning NL expressions with knowledge related to tasks, robots, environments, and humans; humandesired execution priority analysis by analyzing human’s verbal focuses in communication; self-improving of robot understanding ability during task executions.\n8.2. HRC’s contributions to NLC\nThe current HRC research has two trends: generalization and specification. In the generalization trend, a robot is endowed with broad commonsense knowledge to support the general NLC under various situations [33][94][205]. Massive commonsense is essentially the general principle during typical task executions. The objective of generalization is to make a robot adapt to a wide range of tasks/situations/users. To effectively summarize the representative features shared by various NLC scenarios, effective feature learning methods and accurate knowledgeworld mapping models are required. Developing these methods could be a future direction of NLC research. In the specification trend, a robot is endowed with delicate knowledge to support the specific types of HRC [206][207]. The delicate knowledge provides execution details for practical robot execution. The objective is to endow a robot with high professionality on specific tasks in specific environmental conditions with specific users. It is critical for a robot to have a broad and delicate knowledge for intuitive NLC.\nBoth trends are suffering from challenges. For the generalization trend, NLC emphasizes general situation adaptation by extracting the common-sharing knowledge in various scenarios and ignoring the unique knowledge in individual scenarios. Caused by the unique-knowledge ignorance, general NLC methods are relatively simple, being challenged by cold-start phenomenon, which refers the knowledge not being learned in the training stage and causing execution failures in the implementation stage [208]. For the specification trend, limited by the knowledge coverage range, the robot-capable task types are within a narrow range. A robot, which is expected to execute a specific type of task with high professionality, is incapable of executing a wide range of tasks due to intrinsic mental/physical conflictions between being specific and being general [209].\nNL is informative, containing the general execution knowledge such as a typical execution method, and the specific knowledge such as a user’s emotions, preferences and personalities. HRC using NL is a useful way to balance the generation and specification trends in robotic research. During NLC, NL transfers the general knowledge and emphasizes the specific knowledge in the cooperation process. A future trend of NLC could be using advanced NLP techniques to realize the mutual compensation between the robot generalization and specification.\n8.3. Robot knowledge scalability\nScaling up robot knowledge for supporting robot decision making is a critical issue. On one hand, to understand human NL instructions, represent cooperation tasks, or fill up the knowledge gaps, the effective knowledge scalingup capability is needed to accurately learn a large amount of knowledge. On the other hand, the time/labor cost are expected to be reduced. Currently, the knowledge-scaling-up research goes in two directions: existed-knowledge exploitation, and new-knowledge exploration. In existed-knowledge exploitation, the abstract meanings of existed knowledge are summarized at a high level to increase the knowledge interchangeability, making one type of knowledge useful in other similar scenarios. The new-knowledge exploration includes: human-based methods, which query humans for new knowledge, and the big-data-based method, which is an automatic and low-cost information retrieval method that extracts knowledge from information sources such as the World Wide Web [205], books [210], machine operation log files [211], and videos [212]. Given the new techniques in computer science and neuro science, there is still a need to develop efficient, low-cost and large-scale knowledge scaling-up methods.\n8.4. Enhancing robot adaptability\nWeak robot adaptability is typically caused by failures in execution importance modeling, based on which the execution priority is made, and execution interchangeability modeling, based on which the execution flexibility is made. To increase robot adaptability, new research was launched to model the human cognition process [213][214], which aims to explore humans’ decision-making mechanism for modeling robot execution priority and flexibility.\nsubmitted to Knowledge-based Systems, January 2017\nFor execution priority, not all the executions are essential for the execution success. For example, in the task “assembly”, the procedure “clean the place” is much less important than the procedure “install the screw”. For interchangeability, a tool request “deliver me a brush” does not necessarily mean the involvement of a specific tool “brush”, but instead means a practical purpose “cleaning the surface” [15]. By knowing these meanings, the cooperation plans are flexibly changed by ignoring the trivial execution procedures and focusing on the important procedures, and replacing the unavailable tools with the other available similar-function tools. Current methods focus on exploring object affordances (object-action correlation) [215], and lacking the in-depth interpretations of task cooperation. In the future, NLC research could be methods that interpret robot executions from a human perspective, improving robot adaptability in unstructured environments and unfamiliar human users.\n8.5. Learning from failures\nExecution failure causes unnatural task execution or even task failures. The learning-from-failure mechanism has been implemented in computer science for algorithm efficiency improvement [216] and in material science for new material discovery [217]. By exploring information in failure experiences, robots’ performance in task execution is improved by avoiding similar failures in the future. In NLC, learning from failure is involved in a definition-based manner [40], in which the failure is analyzed by comparing the available knowledge with the defined knowledge, lacking the analysis of failure causes and recovery mechanism. Therefore, in NLC, learning from failure is also a promising research direction. From our perspective, the potential research problems could be in-depth failure cause analysis, concise NL failure explaining to a human, proactive knowledge updating methods for recovering from the failures, etc.\n9. Conclusion\nThis review summarized the state-of-the-art robotic systems for using natural language (NL) to facilitate humanrobot cooperation (HRC), thereby providing a summary and comparisons of the natural-language-facilitated humanrobot cooperation (NLC) systems. Regarding the robot-cognition levels, NLC systems were categorized into four types: NL-based control, NL-based robot training, NL-based task execution, and NL-based social companion. Based on our perspective and comprehensive paper review, the current emerging trends of NLC research were discussed, providing helpful information for the future of NLC research.\n10. Acknowledgements\nWe would like to thank Mr. Xu Zhou, Ms. Natalie Kalin and Mr. Ian Coberly for helping us with work finalization."
    } ],
    "references" : [ {
      "title" : "Initiative in robot assistance during collaborative task execution",
      "author" : [ "J. Baraglia", "M. Cakmak", "Y. Nagai", "R. Rao", "M. Asada" ],
      "venue" : "2016 11th ACM/IEEE International Conference on Human-Robot Interaction (HRI), pp. 67-74, 2016.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Teaching robots parametrized executable plans through spoken interaction",
      "author" : [ "G. Gemignani", "E. Bastianelli", "D. Nardi" ],
      "venue" : "International Conference on Autonomous Agents and Multiagent Systems, pp. 851-859, 2015.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Make it so: continuous, flexible natural language interaction with an autonomous robot",
      "author" : [ "J. Brooks", "C. Lignos", "C. Finucane", "M.S. Medvedev", "I. Perera", "V. Raman", "H. KressGazit", "M. Marcus", "H.A. Yanco" ],
      "venue" : "Workshops at the 26th AAAI Conference on Artificial Intelligence, 2012.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Collaboration, dialogue, and human-robot interaction",
      "author" : [ "T. Fong", "C. Thorpe", "C. Baur" ],
      "venue" : "Proceedings of International Symposium of Robotics Research, 2001.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Natural-language-instructed industrial task execution",
      "author" : [ "R. Liu", "J. Webb", "X. Zhang" ],
      "venue" : "ASME International Design Engineering Technical Conferences and Computers and Information in Engineering Conference, pp. V01BT02A043-V01BT02A04, 2016.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Understanding natural language commands for robotic navigation and mobile manipulation",
      "author" : [ "S.A. Tellex", "T.F. Kollar", "S.R. Dickerson", "M.R. Walter", "A. Banerjee", "S. Teller", "N. Roy" ],
      "venue" : "AAAI Conference on Artificial Intelligence, 2011.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Human-robot-contact-state identification based on tactile recognition",
      "author" : [ "H. Iwata", "S. Sugano" ],
      "venue" : "IEEE Transactions on Industrial Electronics, vol. 52, no. 6, pp. 1468-1477, 2005.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Hand force adjustment: robust control of force-coupled human–robot-interaction in assembly processes",
      "author" : [ "J. Kruger", "D. Surdilovic" ],
      "venue" : "CIRP Annals - Manufacturing Technology, vol. 57, no. 1, pp. 41-44, 2008.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "An analysis of contact instability in terms of passive physical equivalents",
      "author" : [ "E. Colgate", "N. Hogan" ],
      "venue" : "IEEE International Conference on Robotics and Automation, pp. 404-409, 1989.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 1989
    }, {
      "title" : "Visual object-action recognition: Inferring object affordances from human demonstration",
      "author" : [ "H. Kjellström", "J. Romero", "D. Kragić" ],
      "venue" : "Computer Vision and Image Understanding, vol. 115, no. 1, pp. 81-90, 2011.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Intention estimation and recommendation system based on attention sharing",
      "author" : [ "S. Kim", "J. Jung", "S. Kavuri", "M. Lee" ],
      "venue" : "International Conference on Neural Information Processing (ICONIP), pp. 395-402, 2013.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Latent hierarchical model for activity recognition",
      "author" : [ "N. Hu", "G. Englebienne", "Z. Lou", "B. Krose" ],
      "venue" : "arxiv: 1503.01820v1, 2015.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 1820
    }, {
      "title" : "A proposed gesture set for the control of industrial collaborative robots",
      "author" : [ "P. Barattini", "C. Morand", "N. Robertson" ],
      "venue" : "IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN), pp. 132-137, 2012",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Learning preferences for manipulation tasks from online coactive feedback",
      "author" : [ "A. Jain", "S. Sharma", "T. Joachims", "A. Saxena" ],
      "venue" : "International Journal of Robotics Research, vol. 34, no. 10, pp. 1296-1313, 2015.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Understanding human behaviors with an object functional role perspective for robotics",
      "author" : [ "R. Liu", "X. Zhang" ],
      "venue" : "IEEE Transactions on Cognitive and Developmental Systems, vol. 8, no. 2, pp. 115-127, 2016.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Transferring skills to humanoid robots by extracting semantic representations from observations of human activities",
      "author" : [ "K. Ramirez-Amaro", "M. Beetz", "G. Cheng" ],
      "venue" : "Artificial Intelligence, DOI: http://dx.doi.org/10.1016/j.artint.2015.08.009, 2015.  submitted to Knowledge-based Systems, January 2017",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Learning the spatial semantics of manipulation actions through preposition grounding",
      "author" : [ "K. Zampogiannis", "Y. Yang", "C. Fermuller", "Y. Aloimonos" ],
      "venue" : "IEEE International Conference on Robotics and Automation (ICRA), pp. 1389-1396, 2015.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Action database for categorizing and inferring human poses from video sequence",
      "author" : [ "W. Takano", "Y. Nakamura" ],
      "venue" : "Robotics and Autonomous Systems, vol. 70, pp. 116-125, 2015.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Deep visual-semantic alignments for generating image descriptions",
      "author" : [ "K. Andrej", "F. Li" ],
      "venue" : "Computer Vision and Pattern Recognition (CVPR), pp. 3128-3137, 2015.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Sorry Dave, I’m afraid I can’t do that: explaining unachievable robot tasks using natural language",
      "author" : [ "V. Raman", "C. Lignos", "C. Finucane", "K.C.T. Lee", "M. Marcus", "H. Kress-Gazit" ],
      "venue" : "Robotics: Science and Systems, vol. 2, no. 1, pp. 2, 2013.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Learning semantic maps from natural language descriptions",
      "author" : [ "M.R. Walter", "S.M. Hemachandra", "B.S. Homberg", "S. Tellex", "S. Teller" ],
      "venue" : "Robotics: Science and Systems, 2013.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Inferring maps and behaviors from natural language instructions",
      "author" : [ "F. Duvallet", "M.R. Walter", "T. Howard", "S. Hemachandra", "J. Oh", "S. Teller", "N. Roy", "A. Stentz" ],
      "venue" : "International Symposium on Experimental Robotics, pp. 373-388, 2014.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Learning to parse natural language commands to a robot control system",
      "author" : [ "C. Matuszek", "E. Herbst", "L. Zettlemoyer", "D. Fox" ],
      "venue" : "13th International Symposium on Experimental Robotics, pp. 403-415, 2013.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Motion capture based human motion recognition and imitation by direct marker control",
      "author" : [ "C. Ott", "D. Lee", "Y. Nakamura" ],
      "venue" : "IEEE-RAS International Conference on Humanoid Robots, pp. 399-405, 2008.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "A gesture based interface for human-robot interaction",
      "author" : [ "S. Waldherr", "R. Romero", "S. Thrun" ],
      "venue" : "Autonomous Robots, vol. 9, no. 2, pp. 151-173, 2000.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Teaching and learning of robot tasks via observation of human performance",
      "author" : [ "R. Dillmann" ],
      "venue" : "Robotics and Autonomous Systems, vol. 47, no. 2-3, pp. 109-116, 2004.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Flexible cooperation between human and robot by interpreting human intention from gaze information",
      "author" : [ "K. Sakita", "K. Ogawara", "S. Murakami", "K. Kawamura", "K. Ikeuchi" ],
      "venue" : "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 846-851, 2004.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Robot learning manipulation action plans by ‘watching’ unconstrained videos from the world wide web",
      "author" : [ "Y. Yang", "Y. Li", "C. Fermuller", "Y. Aloimonos" ],
      "venue" : "Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence, pp. 3686-3692, 2015.",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Towards interactive physical robotic assistance: parameterizing motion primitives through natural language",
      "author" : [ "J.R. Medina", "M. Shelley", "D. Lee", "W. Takano", "S. Hirche" ],
      "venue" : "IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN), pp. 1097-1102, 2012.",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Information-theoretic dialog to improve spatial-semantic representations",
      "author" : [ "S. Hemachandra", "M.R. Walter" ],
      "venue" : "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2015
    }, {
      "title" : "Pattern grammar: A corpus-driven approach to the lexical grammar of English",
      "author" : [ "S. Hunston", "G. Francis" ],
      "venue" : "Computational Linguistics, vol. 27, no. 2, pp. 318- 320, 2000.",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Modelling and analysis of natural language controlled robotic systems",
      "author" : [ "Y. Cheng", "Y. Jia", "R. Fang", "L. She", "N. Xi", "J. Chai" ],
      "venue" : "IFAC Proceedings Volumes, vol. 47, no. 3, pp. 11767-11772, 2014.",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Understanding and executing instructions for everyday manipulation tasks from the world wide web",
      "author" : [ "M. Tenorth", "D. Nyga", "M. Beetz" ],
      "venue" : "IEEE International Conference on Robotics and Automation (ICRA), pp. 1486-1491, 2010.",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Hierarchical semantic labeling for task-relevant rgb-d perception",
      "author" : [ "C. Wu", "I. Lenz", "A. Saxena" ],
      "venue" : "Robotics: Science and Systems, 2014.",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Learning models for following natural language directions in unknown environments",
      "author" : [ "S. Hemachandra", "F. Duvallet", "T.M. Howard", "N. Roy", "A. Stentz", "M.R. Walter" ],
      "venue" : "IEEE International Conference on Robotics and Automation (ICRA), pp. 5608-5615, 2015.",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "The RoboEarth language: representing and exchanging knowledge about actions, objects, and environments",
      "author" : [ "M. Tenorth", "A. Perzylo", "R. Lafrenz", "M. Beetz" ],
      "venue" : "IEEE International Conference on Robotics and Automation (ICRA), pp. 1284-1289, 2012.",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "On the feasibility of using a standardized test for evaluating a speech-controlled smart wheelchair",
      "author" : [ "J. Pineau", "R. West", "A. Atrash", "J. Villemure", "F. Routhier" ],
      "venue" : "International Journal of Intelligent Control and System, vol. 16, no. 2, pp. 124-131, 2011.",
      "citeRegEx" : "38",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Voice and graphical -based interfaces for interaction with a robot dedicated to elderly and people with cognitive disorders",
      "author" : [ "C. Granata", "M. Chetouani", "A. Tapus", "P. Bidaud" ],
      "venue" : "pp. 785-790, 2010.",
      "citeRegEx" : "39",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Recovering from failure by asking for help",
      "author" : [ "R.A. Knepper", "S. Tellex", "A. Li", "N. Roy", "D. Rus" ],
      "venue" : "Autonomous Robots, vol. 39, no. 3, pp. 347-362, 2015.",
      "citeRegEx" : "40",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "A helping hand: industrial robotics, knowledge and user-oriented services",
      "author" : [ "M. Stenmark", "J. Malec" ],
      "venue" : "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2013",
      "citeRegEx" : "41",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Robot navigation using human cues: a robot navigation system for symbolic goal-directed exploration",
      "author" : [ "R. Schulz", "B. Talbot", "O. Lam", "F. Dayoub", "P. Corke", "B. Upcroft", "G. Wyeth" ],
      "venue" : "IEEE International Conference on Robotics and Automation (ICRA), pp. 1100-1105, 2015.",
      "citeRegEx" : "42",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Grounding spatial relations for outdoor robot navigation",
      "author" : [ "A. Boularias", "F. Duvallet", "J. Oh", "A. Stentz" ],
      "venue" : "IEEE International Conference on Robotics and Automation (ICRA), pp. 1976-1982, 2015.",
      "citeRegEx" : "43",
      "shortCiteRegEx" : null,
      "year" : 1976
    }, {
      "title" : "Storytelling with robots: Learning companions for preschool children’s language development",
      "author" : [ "J. Kory", "C. Breazeal" ],
      "venue" : "IEEE International Symposium on Robot and Human Interactive Communication, pp. 643-648, 2014.",
      "citeRegEx" : "44",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "An emotion recognition comparative study of autistic and typically-developing children using the zeno robot",
      "author" : [ "M.J. Salvador", "S. Silver", "M.H. Mahoor" ],
      "venue" : "IEEE International Conference on Robotics and Automation (ICRA), pp. 6128-6133, 2015.",
      "citeRegEx" : "45",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Social interactions in HRI: The robot view",
      "author" : [ "C. Breazeal" ],
      "venue" : "IEEE Transactions on Systems, Man, and Cybernetics, vol. 34, no. 2, pp. 181-186, 2004.",
      "citeRegEx" : "46",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Supervised machine learning: A review of classification techniques",
      "author" : [ "S.B. Kotsiantis", "I. Zaharakis", "P. Pintelas" ],
      "venue" : "Informatica, vol. 31, no. 3, pp. 249-268, 2007.",
      "citeRegEx" : "47",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "A survey of clustering data mining techniques",
      "author" : [ "P. Berkhin" ],
      "venue" : "Grouping multidimensional data, vol. 43, no. 1, pp. 25-71, 2006.",
      "citeRegEx" : "48",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "A survey on feature extraction for pattern recognition",
      "author" : [ "S. Ding", "H. Zhu", "W. Jia", "C. Su" ],
      "venue" : "Artificial Intelligence Review, vol. 37, no. 3, pp. 169-180, 2012.",
      "citeRegEx" : "49",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "A Survey on Techniques in NLP",
      "author" : [ "N. Ranjan", "K. Mundada", "K. Phaltane" ],
      "venue" : "International Journal of Computer Applications, vol. 134, no. 8, pp.6-9, 2016.",
      "citeRegEx" : "50",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Jumping NLP curves: A review of natural language processing research",
      "author" : [ "E. Cambria", "B. White" ],
      "venue" : "IEEE Computational Intelligence Magazine, vol. 9, no. 2, pp. 48-57, 2014.",
      "citeRegEx" : "51",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Progress in the application of natural language processing to information retrieval tasks",
      "author" : [ "A.F. Smeaton" ],
      "venue" : "Computer journal, vol. 35, no. 3, pp. 268-278, 1992.",
      "citeRegEx" : "52",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "The cognitive structure of emotions",
      "author" : [ "A. Ortony", "G. Clore", "A. Collins" ],
      "venue" : null,
      "citeRegEx" : "53",
      "shortCiteRegEx" : "53",
      "year" : 1988
    }, {
      "title" : "The predictive value of transitional probability for word-boundary palatalization in English",
      "author" : [ "N. Bush" ],
      "venue" : "M.S. thesis, Univ. New Mexico, Albuquerque, NM, 1999.",
      "citeRegEx" : "54",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "AbstFinder, a prototype natural language text abstraction finder for use in requirements elicitation",
      "author" : [ "L. Goldin", "D.M. Berry" ],
      "venue" : "Automated Software Engineering, vol. 4, no. 4, pp. 375-412, 1997.",
      "citeRegEx" : "55",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "A statistical natural language processor for medical reports",
      "author" : [ "R.K. Taira", "S.G. Soderland" ],
      "venue" : "Proceedings of the AMIA Symposium, 1999.",
      "citeRegEx" : "56",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Semantic relevance and aspect dependency in a given subject domain: contents-driven algorithmic processing of fuzzy word meanings to form dynamic stereotype representations",
      "author" : [ "B.B. Rieger" ],
      "venue" : "International Conference on Computational Linguistics, pp. 298-301, 1984.",
      "citeRegEx" : "57",
      "shortCiteRegEx" : null,
      "year" : 1984
    }, {
      "title" : "Bridging the gap between discrete symbolic planning and optimization-based robot control",
      "author" : [ "E. Scioni", "G. Borghesan", "H. Bruyninckx", "M. Bonfe" ],
      "venue" : "IEEE International Conference on Robotics and Automation (ICRA), pp. 5075-5081, 2015.",
      "citeRegEx" : "58",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "The motion grammar: Analysis of a linguistic method for robot control",
      "author" : [ "N. Dantam", "M. Stillman" ],
      "venue" : "IEEE Transactions on Robotics, vol. 29, no. 3, pp. 704- 718, 2013.",
      "citeRegEx" : "59",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "HMM-based passage models for document classification and ranking",
      "author" : [ "L. Denoyer", "H. Zaragoza", "P. Gallinari" ],
      "venue" : "European Colloquium on Information Retrieval Research, Darmstadt, pp. 126-135, 2001.",
      "citeRegEx" : "60",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Unsupervised learning by probabilistic latent semantic analysis",
      "author" : [ "T. Hofmann" ],
      "venue" : "Machine Learning, vol. 42, nos.1–2, pp. 177–196, 2001.",
      "citeRegEx" : "61",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Ontology-based parser for natural language processing",
      "author" : [ "J.E. Busch", "A.D. Lin", "P.J. Graydon", "M. Caudill" ],
      "venue" : "U.S. Patent No. 7,027,974, 2006.  submitted to Knowledge-based Systems, January 2017",
      "citeRegEx" : "62",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Automatic ontology-based knowledge extraction from web documents",
      "author" : [ "H. Alani", "S. Kim", "D.E. Millard", "M.J. Weal", "W. Hall", "P.H. Lewis", "N.R. Shadbolt" ],
      "venue" : "IEEE Intelligent Systems, vol. 18, no. 1, pp. 14-21, 2003.",
      "citeRegEx" : "63",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Sentic computing: Techniques, tools, and applications",
      "author" : [ "E. Cambria", "A. Hussain" ],
      "venue" : null,
      "citeRegEx" : "64",
      "shortCiteRegEx" : "64",
      "year" : 2012
    }, {
      "title" : "COGVIEW & INTELNET: Nuanced energy-based knowledge representation and integrated cognitive-conceptual framework for realistic culture, values, and concept-affected systems simulation",
      "author" : [ "D. Olsher" ],
      "venue" : "IEEE Symposium on Computational Intelligence for Human-like Intelligence (CIHLI), pp. 82–91, 2013.",
      "citeRegEx" : "65",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Multi-purpose natural language understanding linked to sensorimotor experience in humanoid robots",
      "author" : [ "E. Ovchinnikova", "M. Wachter", "V. Wittenbeck", "T. Asfour" ],
      "venue" : "IEEE-RAS 15th International Conference on Humanoid Robots (Humanoids), pp. 365-372, 2015.",
      "citeRegEx" : "66",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Child-robot interaction: Perspectives and challenges",
      "author" : [ "T. Belpaeme", "P. Baxter", "J.D. Greeff", "J. Kennedy", "R. Read", "R. Looije", "M. Neerincx", "I. Baroni", "M.C. Zelati" ],
      "venue" : "International Conference on Social Robotics (ICSR), pp. 452-459, 2013.",
      "citeRegEx" : "67",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Story and discourse: A bipartite model of narrative generation in virtual worlds",
      "author" : [ "R. Young" ],
      "venue" : "Interaction Studies, vol. 8, no. 2, pp. 177–208, 2007.",
      "citeRegEx" : "68",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Modeling space and time in narratives about restaurants",
      "author" : [ "E. Mueller" ],
      "venue" : "Literary Linguistic Computing, vol. 22, no.1, pp. 67–84, 2007.",
      "citeRegEx" : "69",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Formalizing argumentative story-based analysis of evidence",
      "author" : [ "F. Bex", "H. Prakken", "B. Verheij" ],
      "venue" : "International Conference on Artificial Intelligence and Law, pp. 1-10, 2007.",
      "citeRegEx" : "70",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Narrative is a key cognitive competency",
      "author" : [ "M.A. Finlayson", "P.H. Winston" ],
      "venue" : "Annual Meeting Biologically Inspired Cognitive Architectures (BICA), pp. 110, 2011.",
      "citeRegEx" : "71",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "The new robotics—towards human-centered machines",
      "author" : [ "S. Schaal" ],
      "venue" : "HFSP journal, vol. 1, no. 2, pp. 115-126, 2007.",
      "citeRegEx" : "72",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "A survey of socially interactive robots",
      "author" : [ "T.W. Fong", "I. Nourbakhsh", "K. Dautenhahn" ],
      "venue" : "Robotics and Autonomous Systems, vol. 42, no. 3-4, pp. 143-166, 2002.",
      "citeRegEx" : "73",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Introduction to this special issue on human-robot interaction",
      "author" : [ "S. Kiesler", "P. Hinds" ],
      "venue" : "Human-Computer Interaction, vol. 19, nos. 1-2, pp. 1-8, 2004.",
      "citeRegEx" : "74",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Classifying human-robot interaction: an updated taxonomy",
      "author" : [ "H.A. Yanco", "J.L. Drury" ],
      "venue" : "IEEE International Conference on Systems, Man & Cybernetics (SMC), pp. 2841-2846, 2004.",
      "citeRegEx" : "75",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Human-robot collaboration: A survey",
      "author" : [ "A. Bauer", "D. Wollherr", "M. Buss" ],
      "venue" : "International Journal of Humanoid Robotics, vol. 5, no. 1, pp. 47-66, 2008.",
      "citeRegEx" : "76",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Cognitive developmental robotics: A survey",
      "author" : [ "M. Asada", "K. Hosoda", "Y. Kuniyoshi", "H. Ishiguro", "T. Inui", "Y. Yoshikawa", "M. Ogino", "C. Yoshida" ],
      "venue" : "IEEE Transactions on Autonomous Mental Development, vol. 1, no. 1, pp. 12-34, 2009.",
      "citeRegEx" : "77",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "A novel approach to proactive human-robot cooperation",
      "author" : [ "O.C. Schrempf", "U.D. Hanebeck", "A.J. Schmid", "H. Worn" ],
      "venue" : "IEEE International Workshop on Robot and Human Interactive Communication, pp. 555-560, 2005.",
      "citeRegEx" : "78",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Effects of anticipatory action on human-robot teamwork efficiency, fluency, and perception of team",
      "author" : [ "G. Hoffman", "C. Breazeal" ],
      "venue" : "Proceedings of the ACM/IEEE international conference on Human-robot interaction, pp. 1-8, 2007.",
      "citeRegEx" : "79",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "The shadow robot mimics human actions",
      "author" : [ "P. Tuffield", "H. Elias" ],
      "venue" : "Industrial Robot: An International Journal, vol. 30, no. 1, pp. 56-60, 2003.",
      "citeRegEx" : "80",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "When humanoid robots become human-like interaction partners: corepresentation of robotic actions",
      "author" : [ "A. Stenzel", "E. Chinellato", "M.A.T. Bou", "A.P. del Pobil", "M. Lappe", "R. Liepelt" ],
      "venue" : "Journal of Experimental Psychology: Human Perception and Performance, vol. 38, no. 5, pp. 1073, 2012.",
      "citeRegEx" : "81",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "An atlas of physical human–robot interaction",
      "author" : [ "A. De Santis", "B. Siciliano", "A. De Luca", "A. Bicchi" ],
      "venue" : "Mechanism and Machine Theory, vol. 43, no. 3, pp. 253- 270, 2008.",
      "citeRegEx" : "82",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Safe planning for human‐robot interaction",
      "author" : [ "D. Kulic", "E.A. Croft" ],
      "venue" : "Journal of Robotic Systems, vol. 22, no. 7, pp. 383-396, 2005.",
      "citeRegEx" : "83",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Adapting robot behavior for human--robot interaction",
      "author" : [ "N. Mitsunaga", "C. Smith", "T. Kanda", "H. Ishiguro", "N. Hagita" ],
      "venue" : "IEEE Transactions on Robotics, vol. 24, no. 4, pp. 911-916, 2008.",
      "citeRegEx" : "84",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Is imitation learning the route to humanoid robots?",
      "author" : [ "S. Schaal" ],
      "venue" : "Trends in Cognitive Science,",
      "citeRegEx" : "85",
      "shortCiteRegEx" : "85",
      "year" : 1999
    }, {
      "title" : "Getting humanoids to move and imitate",
      "author" : [ "M. Mataric" ],
      "venue" : "IEEE Intelligent Systems, vol. 15, no. 4, pp. 18-24, 2000.",
      "citeRegEx" : "86",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "The neuroscience of social interaction: Decoding, imitating, and influencing the actions of others",
      "author" : [ "C.D. Frith", "D.M. Wolpert" ],
      "venue" : "Journal of Consciousness Studies, vol. 119, no. 4, pp. 664-668, 2004.",
      "citeRegEx" : "87",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Natural human-robot interaction using speech, head pose and gestures",
      "author" : [ "R. Stiefelhagen", "C. Fugen", "R. Gieselmann", "H. Holzapfel", "K. Nickel", "A. Waibel" ],
      "venue" : "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2004.",
      "citeRegEx" : "88",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Physiological signals based human emotion recognition: a review",
      "author" : [ "S. Jerritta", "M. Murugappan", "R. Nagarajan", "K. Wan" ],
      "venue" : "IEEE International Colloquium on Signal Processing and its Applications (CSPA), 2011.",
      "citeRegEx" : "89",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Stochastic optimal control and estimation methods adapted to the noise characteristics of the sensorimotor system",
      "author" : [ "E. Todorov" ],
      "venue" : "Neural Computation, vol. 17, no. 5, pp. 1084–1108, 2005.",
      "citeRegEx" : "90",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Context-specific intention awareness through web query in robotic caregiving",
      "author" : [ "R. Liu", "X. Zhang", "J. Webb", "S. Li" ],
      "venue" : "IEEE International Conference on Robotics and Automation (ICRA), pp. 1962-1967, 2015.",
      "citeRegEx" : "91",
      "shortCiteRegEx" : null,
      "year" : 1962
    }, {
      "title" : "Use context to understand user's implicit intentions in activities of daily living",
      "author" : [ "R. Liu", "X. Zhang", "S. Li" ],
      "venue" : "IEEE International Conference on Mechatronics and Automation (ICMA), pp. 1214-1219, 2014.",
      "citeRegEx" : "92",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Fuzzy-context-specific intention inference for robotic caregiving",
      "author" : [ "R. Liu", "X. Zhang" ],
      "venue" : "International Journal of Advanced Robotic Systems, vol. 99, no. 99, pp. 99, 2016.",
      "citeRegEx" : "93",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Context-specific grounding of web natural descriptions to human-centered situations",
      "author" : [ "R. Liu", "X. Zhang" ],
      "venue" : "Knowledge-Based Systems, vol. 111, pp. 1-16, 2016.",
      "citeRegEx" : "94",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "The role of expressiveness and attention in human-robot interaction",
      "author" : [ "A. Burce", "I. Nourbakhsh", "R. Simmons" ],
      "venue" : "IEEE International Conference on Robotics and Automation (ICRA), vol. 4, pp. 4138-4142, 2002.",
      "citeRegEx" : "95",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Investigating joint attention mechanisms through spoken human–robot interaction",
      "author" : [ "M. Staudte", "M.W. Crocker" ],
      "venue" : "Cognition, vol. 120, no. 2, pp. 268-291, 2011.",
      "citeRegEx" : "96",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "The influence of subjects' personality traits on personal spatial zones in a human-robot interaction experiment",
      "author" : [ "M.L. Walters", "K. Dautenhahn", "R. Te Boekhorst", "K.L. Koay", "C. Kaouri", "S. Woods", "C. Nehaniv", "D. Lee", "I. Werry" ],
      "venue" : "IEEE International Workshop on Robot and Human Interactive Communication, pp. 347-352, 2005.",
      "citeRegEx" : "97",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Influences on proxemic behaviors in human-robot interaction",
      "author" : [ "L. Takayama", "C. Pantofaru" ],
      "venue" : "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 5495-5502, 2009.",
      "citeRegEx" : "98",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "An ethological and emotional basis for human–robot interaction",
      "author" : [ "R.C. Arkin", "M. Fujita", "T. Takagi", "R. Hasegawa" ],
      "venue" : "Robotics and Autonomous Systems, vol. 42, no. 3, pp. 191-201, 2003.",
      "citeRegEx" : "99",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Human-inspired robots",
      "author" : [ "S. Coradeschi", "H. Ishiguro", "M. Asada", "S.C. Shapiro", "M. Thielscher", "C. Breazeal", "M.J. Mataric", "H. Ishida" ],
      "venue" : "IEEE Intelligent Systems, vol. 21, no. 4, pp. 74–85, 2006.",
      "citeRegEx" : "100",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Design and control of a variable stiffness actuator for safe and fast physical human/robot interaction",
      "author" : [ "G. Tonietti", "R. Schiavi", "A. Bicchi" ],
      "venue" : "IEEE International Conference on Robotics and Automation (ICRA), pp. 526-531, 2015.",
      "citeRegEx" : "101",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "NRI: Collaborative Research: Jointly Learning Language and Affordances",
      "author" : [ "B. Selman" ],
      "venue" : null,
      "citeRegEx" : "102",
      "shortCiteRegEx" : "102",
      "year" : 2014
    }, {
      "title" : "NRI: Collaborative Research: Modeling and Verification of Language-based Interaction",
      "author" : [ "N. Roy" ],
      "venue" : null,
      "citeRegEx" : "104",
      "shortCiteRegEx" : "104",
      "year" : 2014
    }, {
      "title" : "The international journal of robotics research",
      "author" : [ "J. Hollerbach" ],
      "venue" : "SAGE. [Online]. Available: http://journals.sagepub.com/home/ijr. Accessed: Jan. 27, 2017.",
      "citeRegEx" : "107",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "IEEE Xplore: IEEE transactions on robotics",
      "author" : [ "F. Park" ],
      "venue" : "IEEE transactions on robotics. [Online]. Available: http://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=8860. Accessed: Jan. 27, 2017.",
      "citeRegEx" : "108",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "A survey of robot learning from demonstration",
      "author" : [ "B.D. Argall", "S. Chernova", "M. Veloso", "B. Browning" ],
      "venue" : "Robotics and autonomous systems, vol. 57, no. 5, pp. 469-483, 2009.",
      "citeRegEx" : "114",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Vision based hand gesture recognition for human computer interaction: a survey",
      "author" : [ "S.S. Rautaray", "A. Agrawal" ],
      "venue" : "Artificial Intelligence Review, vol. 43, no. 1, pp. 1-54, 2015.",
      "citeRegEx" : "115",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Survey of psychophysiology measurements applied to human-robot interaction",
      "author" : [ "C.L. Bethel", "K. Salomon", "R.R. Murphy", "J.L. Burke" ],
      "venue" : "IEEE International Symposium on Robot and Human Interactive Communication, pp. 732-737, 2007.",
      "citeRegEx" : "116",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "A survey of tactile human–robot interactions",
      "author" : [ "B.D. Argall", "A.G. Billard" ],
      "venue" : "Robotics and Autonomous Systems, vol. 58, no. 10, pp. 1159-1176, 2010.",
      "citeRegEx" : "117",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Robust speech understanding for robot telecontrol",
      "author" : [ "G. Antoniol", "R. Cattoni", "M. Cettolo", "M. Federico" ],
      "venue" : "International Conference on Advanced robotics, pp. 205-209,1993.",
      "citeRegEx" : "118",
      "shortCiteRegEx" : null,
      "year" : 1993
    }, {
      "title" : "Gesture-speech based HMI for a rehabilitation robot",
      "author" : [ "S. Chen", "Z. Kazi", "M. Beitler", "M. Salganicoff", "D. Chester", "R. Foulds" ],
      "venue" : "Proceedings of Bringing Together Education, Science and Technology, pp.29-36, 1996.",
      "citeRegEx" : "119",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "Integrating vision, touch and natural language in the control of a situation-oriented behavior-based humanoid robot",
      "author" : [ "R. Bischoff", "V. Graefe" ],
      "venue" : "IEEE International Conference on Systems, Man, and Cybernetics (SMC), vol.2, pp. 999-1004, 1999.",
      "citeRegEx" : "120",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Voice command generation for teleoperated robot systems",
      "author" : [ "M. Ferre", "J. Macias-Guarasa", "R. Aracil", "A. Barrientos" ],
      "venue" : "IEEE International Workshop on Robot and Human Communication, pp. 679-685, 1998.",
      "citeRegEx" : "122",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Control of a mobile robot using spoken commands",
      "author" : [ "J. Savage", "E. Hernandez", "G. Vazquez", "A. Hernandez", "A.L. Ronzhin" ],
      "venue" : "Conference Speech and Computer, pp. 333-338, 2004.",
      "citeRegEx" : "123",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Posture control of robot manipulators with fuzzy voice commands using a fuzzy coach–player system",
      "author" : [ "C. Jayawardena", "K. Watanabe", "K. Izumi" ],
      "venue" : "Advanced Robotics, vol. 21, nos. 3-4, pp. 293-328, 2007.",
      "citeRegEx" : "124",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Modeling of movement control architectures based on motion primitives using domain-specific language",
      "author" : [ "A. Nordmann", "S. Wrede", "J. Steil" ],
      "venue" : "IEEE International Conference on Robotics and Automation (ICRA), pp. 5032-5039, 2015.",
      "citeRegEx" : "125",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "ROBOSHERLOCK: Unstructured information processing for robot perception",
      "author" : [ "M. Beetz", "F. Balint-Benczedi", "N. Blodow", "D. Nyga", "T. Wiedemeyer", "Z.C. Marton" ],
      "venue" : "IEEE International Conference on Robotics and Automation (ICRA), pp. 1549-1556, 2015.",
      "citeRegEx" : "126",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Natural language service for controlling robots and other agents",
      "author" : [ "J. Allen", "Q. Duong", "C. Thompson" ],
      "venue" : "IEEE Integration of Knowledge Intensive Multi- Agent Systems (KIMAS-05), pp. 592-595, 2005.",
      "citeRegEx" : "127",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "The VoiceBot: A Voice Controlled Robot Arm",
      "author" : [ "B. House", "J. Malkin", "J. Bilmes" ],
      "venue" : "Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, pp. 183-192, 2009.",
      "citeRegEx" : "128",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Natural Language Programming of Industrial Robots",
      "author" : [ "M. Stenmark", "P. Nugues" ],
      "venue" : "International Symposium on Robotics, pp. 1-5, 2013.",
      "citeRegEx" : "129",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Equipping robot control programs with first-order probabilistic reasoning capabilities",
      "author" : [ "D. Jain", "L. Mosenlechner", "M. Beetz" ],
      "venue" : "IEEE International Conference on Robotics and Automation (ICRA), pp. 3626-3631, 2009.",
      "citeRegEx" : "130",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Temporal logic motion planning for mobile robots",
      "author" : [ "G.E. Fainekos", "H. Kress-Gazit", "G.J. Pappas" ],
      "venue" : "IEEE International Conference on Robotics and Automation (ICRA), pp. 2020-2025, 2005.",
      "citeRegEx" : "131",
      "shortCiteRegEx" : null,
      "year" : 2020
    }, {
      "title" : "Using syntax to learn semantics: An experiment in language acquisition with a mobile robot",
      "author" : [ "T. Oates", "Z. Eyler-Walker", "P.R. Cohen" ],
      "venue" : "Walker, pp. 1-6, 1999.",
      "citeRegEx" : "132",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "On distributed knowledge bases for robotized small-batch assembly",
      "author" : [ "M. Stenmark", "J. Malec", "K. Nilsson", "A. Robertsson" ],
      "venue" : "IEEE Transactions on Automation Science and Engineering, vol. 12, no. 2, pp. 1-10, 2015.",
      "citeRegEx" : "133",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "A generic natural language interface for task planning application to a mobile robot",
      "author" : [ "J.M.G. Romano", "E.F. Camacho", "J.G. Ortega", "M.T. Bonilla" ],
      "venue" : "Control Engineering Practice, vol. 8, no. 10, pp. 1119-1133, 2000.",
      "citeRegEx" : "134",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "LTLMoP: Experimenting with language, temporal logic and robot control",
      "author" : [ "C. Finucane", "G. Jing", "H. Kress-Gazit" ],
      "venue" : "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 1988-1993, 2010.",
      "citeRegEx" : "135",
      "shortCiteRegEx" : null,
      "year" : 1988
    }, {
      "title" : "Interpreting and executing recipes with a cooking robot",
      "author" : [ "M. Bollini", "S. Tellex", "T. Thompson", "N. Roy", "D. Rus" ],
      "venue" : "13th International Symposium on Experimental Robotics, pp. 481-495, 2013.",
      "citeRegEx" : "136",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Robot learning from verbal interaction: A brief survey",
      "author" : [ "H. Cuayahuitl" ],
      "venue" : "International Symposium on New Frontiers in Human-Robot Interaction, 2015.",
      "citeRegEx" : "137",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "On the integration of grounding language and learning objects",
      "author" : [ "Y. Chen", "D.H. Ballard" ],
      "venue" : "National Conference on Artificial Intelligence, vol. 4, pp. 2, 2004.",
      "citeRegEx" : "138",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Task learning through imitation and human-robot interaction",
      "author" : [ "M.N. Nicolescu", "M.J. Matari" ],
      "venue" : "Models and Mechanisms of Imitation and Social Learning in Robots, Humans and Animals: Behavioural, Social and Communicative Dimension, pp. 407-424, 2005.",
      "citeRegEx" : "139",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Learning visually grounded words and syntax of natural spoken language",
      "author" : [ "D. Roy" ],
      "venue" : "Evolution of Communication, vol. 4, no. 1, pp. 33-56, 2002.",
      "citeRegEx" : "140",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Personal Robot Training via Natural-Language Instructions",
      "author" : [ "S. Lauria", "G. Bugmann", "T. Kyriacou", "J. Bos" ],
      "venue" : "IEEE Intelligent Systems, vol. 16, no.5, pp. 38- 45, 2001.",
      "citeRegEx" : "141",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Natural methods for robot task learning: instructive demonstrations, generalization and practice",
      "author" : [ "M.N. Nicolescu", "M.J. Mataric" ],
      "venue" : "Proceedings of the second international joint conference on Autonomous agents and multiagent systems, pp. 241-248, 2003.",
      "citeRegEx" : "142",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Structural descriptions in human-assisted robot visual learning",
      "author" : [ "G.M. Kruijff", "J.D. Kelleher", "G. Berginc", "A. Leonardis" ],
      "venue" : "ACM SIGCHI/SIGART conference on Human-robot interaction (HRI), pp. 343-344, 2006.",
      "citeRegEx" : "143",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Learning object-manipulation verbs for human-robot communication",
      "author" : [ "K. Sugiura", "N. Iwahashi" ],
      "venue" : "Proceedings of the 2007 workshop on Multimodal interfaces in semantic interaction (WMISI), pp. 32-38, 2007.",
      "citeRegEx" : "144",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Learning to interpret spatial natural language in terms of qualitative spatial relations",
      "author" : [ "P. Kordjamshidi", "J. Hois", "M.V. Otterlo", "M. Moens" ],
      "venue" : "Representing space in cognition: interrelations of behavior, language, and formal models, 2012.",
      "citeRegEx" : "145",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Robots that learn language: A developmental approach to situated human-robot conversations",
      "author" : [ "N. Iwahashi" ],
      "venue" : "Human Robot Interaction, Nilanjan Sarkar (Ed.), InTech, 2007.",
      "citeRegEx" : "146",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Learning social affordances and using them for planning",
      "author" : [ "K.F. Uyanik", "Y. Caliskan", "A.K. Bozcuoglu", "O. Yuruten", "S. Kalkan", "E. Sahin" ],
      "venue" : "35th Annual Meeting of the Cognitive Science Society (CogSci), 2013.",
      "citeRegEx" : "147",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Using the behavior markup language for human-robot interaction",
      "author" : [ "A. Holroyd", "C. Rich" ],
      "venue" : "Proceedings of the seventh annual ACM/IEEE international conference on Human-Robot Interaction, pp. 147-148, 2012.",
      "citeRegEx" : "148",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Modeling affordances using Bayesian networks",
      "author" : [ "L. Montesano", "M. Lopes", "A. Bernardino", "J. Santos-Victor" ],
      "venue" : "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 4102-4107, 2007.",
      "citeRegEx" : "149",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Towards an integrated model of speech and gesture production for multi-modal robot behavior",
      "author" : [ "M. Salem", "S. Kopp", "I. Wachsmuth", "F. Joublin" ],
      "venue" : "IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN), pp. 614-619, 2010.",
      "citeRegEx" : "150",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Grounding antonym adjective pairs through interaction",
      "author" : [ "M. Forbes", "M. Chung", "M. Cakmak", "L. Zettlemoyer" ],
      "venue" : "HRI, 2014.",
      "citeRegEx" : "151",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Learning to recognize novel objects in one shot through human-robot interactions in natural language dialogues",
      "author" : [ "E. Krause", "M. Zillich", "T. Williams", "M. Scheutz" ],
      "venue" : "Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence, pp. 2796-2802, 2014.",
      "citeRegEx" : "152",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "A probabilistic approach to learning a visually grounded language model through human-robot interaction",
      "author" : [ "H. Dindo", "D. Zambuto" ],
      "venue" : "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 790-796, 2010.  submitted to Knowledge-based Systems, January 2017",
      "citeRegEx" : "153",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Going beyond literal command-based instructions: Extending robotic natural language interaction capabilities",
      "author" : [ "T. Williams", "G. Briggs", "B. Oosterveld", "M. Scheutz" ],
      "venue" : "AAAI Conference on Artificial Intelligence, pp. 1387-1393, 2015.",
      "citeRegEx" : "154",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "A multimoda l human-robot-dialog applying emotional feedbacks",
      "author" : [ "A. Bannat", "J. Blume", "J.T. Geiger", "T. Rehrl", "F. Wallhoff", "C. Mayer", "B. Radig", "S. Sosnowski", "K. Kuhnlenz" ],
      "venue" : "Proceedings of Second International Conference on Social Robotics (ICSR), 2010.",
      "citeRegEx" : "155",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Teachable robots: Understanding human teaching behavior to build more effective robot learners",
      "author" : [ "A.L. Thomaz", "C. Breazeal" ],
      "venue" : "Artificial Intelligence, vol. 172, nos. 6-7, pp. 716-737, 2008.",
      "citeRegEx" : "156",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "An experience-driven robotic assistant acquiring human knowledge to improve haptic cooperation",
      "author" : [ "J.R. Medina", "M. Lawitzky", "A. Mortl", "D. Lee" ],
      "venue" : "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2011.",
      "citeRegEx" : "157",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Situated spoken dialogue with robots using active learning",
      "author" : [ "K. Sugiura", "N. Iwahashi", "H. Kawai", "S. Nakamura" ],
      "venue" : "Advanced Robotics, vol. 25, no. 17, pp. 2207-2232, 2011.",
      "citeRegEx" : "158",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Tell me Dave: Context-sensitive grounding of natural language to manipulation instructions",
      "author" : [ "D.K. Misra", "J. Sung", "K. Lee", "A. Saxena" ],
      "venue" : "Robotics: Science and Systems, vol.35, no. 1, pp. 281-300, 2014.",
      "citeRegEx" : "159",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Integrating verbal and nonverbal communication in a dynamic neural field architecture for human–robot interaction",
      "author" : [ "E. Bicho", "L. Louro", "W. Erlhagen" ],
      "venue" : "Frontiers in neurorobotics, vol. 4, no. 5, 2010.",
      "citeRegEx" : "160",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Towards Real-Time Natural Language Corrections for Assistive Robots",
      "author" : [ "A. Broad", "J. Arkin", "N. Ratliff", "T. Howard", "B. Argall" ],
      "venue" : "RSS Workshop on Model Learning for Human-Robot Communication, 2016.",
      "citeRegEx" : "161",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Progress in programming the hrp-2 humanoid using spoken language",
      "author" : [ "P.F. Dominey", "A. Mallet", "E. Yoshida" ],
      "venue" : "IEEE International Conference on Robotics and Automation (ICRA), pp. 2169-2174, 2007.",
      "citeRegEx" : "162",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Analysis and semantic modeling of modality preferences in industrial human-robot interaction",
      "author" : [ "S. Profanter", "A. Perzylo", "N. Somani", "M. Rickert", "A. Knoll" ],
      "venue" : "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 1812-1818, 2015.",
      "citeRegEx" : "163",
      "shortCiteRegEx" : null,
      "year" : 1812
    }, {
      "title" : "Understanding User Instructions by Utilizing Open Knowledge for Service Robots",
      "author" : [ "D. Lu", "F. Wu", "X. Chen" ],
      "venue" : "arXiv preprint arXiv:1606.02877, 2016.",
      "citeRegEx" : "164",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "RoboFrameNet: Verb-Centric Semantics for Actions in Robot Middleware",
      "author" : [ "B. Thomas", "O. Jenkins" ],
      "venue" : "IEEE International Conference on Robotics and Automation, pp. 4750-4755, 2012.",
      "citeRegEx" : "165",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Two-handed gesture recognition and fusion with speech to command a robot",
      "author" : [ "B. Burger", "I. Ferrane", "F. Lerasle", "G. Infantes" ],
      "venue" : "Autonomous Robots, vol. 32, no. 2, pp. 129-147, 2012.",
      "citeRegEx" : "166",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "The Peer-to-Peer Human-Robot Interaction Project",
      "author" : [ "T. Fong", "I. Nourbakhsh", "C Kunz", "L. Fluckiger", "J. Schreiner" ],
      "venue" : "Space 2005, pp. 6750, 2005.",
      "citeRegEx" : "167",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Clarifying commands with information-theoretic human-robot dialog",
      "author" : [ "R. Deits", "S. Tellex", "P. Thaker", "D. Simeonov", "T. Kolloar", "N. Roy" ],
      "venue" : "Journal of Human- Robot Interaction, vol. 1, no. 1, pp. 78-95, 2012.",
      "citeRegEx" : "168",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Using dialog and human observations to dictate tasks to a learning robot assistant",
      "author" : [ "P. Rybski", "J. Stolarz", "K. Yoon", "M. Veloso" ],
      "venue" : "Intelligent Service Robotics, vol. 1, no. 2, pp. 159-167, 2008.",
      "citeRegEx" : "169",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Dependable multimodal communication and interaction with robotic assistants",
      "author" : [ "R. Bischoff", "V. Graefe" ],
      "venue" : "IEEE International Workshop on Robot and Human Interactive Communication (RO-MAN), pp. 300-305, 2002.",
      "citeRegEx" : "170",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "A study of interaction between dialog and decision for human-robot collaborative task achievement",
      "author" : [ "A. Clodic", "R. Alami", "V. Montreuil", "S. Li" ],
      "venue" : "International Symposium on Robot and Human interactive Communication (RO-MAN), pp. 913-918, 2007.",
      "citeRegEx" : "171",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Multi-Modal human robot interaction for map generation",
      "author" : [ "S.S. Ghidary", "Y. Nakata", "H. Saito", "M. Hattori", "T. Takamori" ],
      "venue" : "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), vol. 4, pp. 2246-2251, 2001.",
      "citeRegEx" : "172",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Grounding verbs of motion in natural language commands to robots",
      "author" : [ "T. Kollar", "S. Tellex", "D. Roy", "N. Roy" ],
      "venue" : "12th International Symposium on Experimental Robotics, pp. 31-47, 2014.",
      "citeRegEx" : "173",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Applying automated deduction to natural language understanding",
      "author" : [ "J. Bos" ],
      "venue" : "Journal of Applied Logic, vol. 7, no. 1, pp. 100-112, 2009.",
      "citeRegEx" : "174",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Robot self-initiative and personalization by learning through repeated interactions",
      "author" : [ "M. Mason", "M. Lopes" ],
      "venue" : "Proceedings of the 6th international conference on Human-robot interaction, pp. 433-440, 2011.",
      "citeRegEx" : "175",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Gender, more so than Age, Modulates Positive Perceptions of Language-Based Human-Robot Interactions",
      "author" : [ "M. Strait", "P. Briggs", "M. Scheutz" ],
      "venue" : "International Symposium on New Frontiers in Human-Robot Interaction, 2015.",
      "citeRegEx" : "176",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Natural programming of a social robot by dialogs",
      "author" : [ "J.F. Gorostiza", "M.A. Salichs" ],
      "venue" : "AAAI Fall Symposium, 2010.",
      "citeRegEx" : "177",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "A Storytelling Robot: Modeling and Evaluation of Human-like Gaze Behavior",
      "author" : [ "B. Mutlu", "J. Forlizzi", "J. Hodgins" ],
      "venue" : "IEEE-RAS International conference on humanoid robotics, pp. 518-523, 2006.",
      "citeRegEx" : "178",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Natural emotion elicitation for emotion modeling in child-robot interactions",
      "author" : [ "W. Wang", "G. Athanasopoulos", "S. Yilmazyildiz", "G. Patsis", "V. Enescu", "H. Sahli", "W. Verhelst", "A. Hiolle", "M. Lewis", "L. Canamero" ],
      "venue" : "4th Workshop on Child Computer Interaction (WOCCI), pp. 51-56, 2014.",
      "citeRegEx" : "179",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Recognition of affective communicative intent in robot-directed speech",
      "author" : [ "C. Breazeal", "L. Aryananda" ],
      "venue" : "Autonomous Robots, vol. 12, no. 1, pp. 83-104, 2002.",
      "citeRegEx" : "180",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Tutelage and socially guided robot learning",
      "author" : [ "A. Lockerd", "C. Breazeal" ],
      "venue" : "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), vol. 4, pp. 3475-3480, 2004.",
      "citeRegEx" : "181",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Toward sociable robots",
      "author" : [ "C. Breazeal" ],
      "venue" : "Robotics and Autonomous Systems, vol. 42, no. 3, pp. 167-175, 2003.",
      "citeRegEx" : "182",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Social and collaborative aspects of interaction with a service robot",
      "author" : [ "K. Severinson-Eklundh", "A. Green", "H. Huttenrauch" ],
      "venue" : "Robotics and Autonomous Systems, vol. 42, nos. 3-4, pp. 223-234, 2003.",
      "citeRegEx" : "183",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Fuzzy emotion recognition in natural speech dialogue",
      "author" : [ "A. Austermann", "N. Esau", "L. Kleinjohann", "B. Kleinjohann" ],
      "venue" : "International Workshop on Robot and Human interactive Communication (RO-MAN), pp. 317-332, 2005.",
      "citeRegEx" : "184",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "You, robot: on the linguistic construction of artificial others",
      "author" : [ "M. Coeckelbergh" ],
      "venue" : "AI & Society, vol. 26, no. 1, pp. 61-69, 2011.",
      "citeRegEx" : "185",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "How to use non-linguistic utterances to convey emotion in child-robot interaction",
      "author" : [ "R. Read", "T. Belpaeme" ],
      "venue" : "7th ACM/IEEE International Conference on Human-Robot Interaction (HRI), pp. 219-220, 2012.",
      "citeRegEx" : "186",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Children’s Turn-Taking Behavior Adaptation in Multi-Session Interactions with a Humanoid Robot",
      "author" : [ "I. Kruijff-Korbayova", "I. Baroni", "M. Nalin" ],
      "venue" : "IEEE workshop on timing in human-robot interaction, 2014.",
      "citeRegEx" : "187",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Robots in the wild: Observing human-robot social interaction outside the lab",
      "author" : [ "S. Sabanovic", "M.P. Michalowski", "R. Simmons" ],
      "venue" : "9th IEEE International Workshop on Advanced Motion Control, pp. 596-601, 2006.",
      "citeRegEx" : "188",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Social interaction of humanoid robot based on audio-visual tracking",
      "author" : [ "H.G. Okuno", "K. Nakadai", "H. Kitano" ],
      "venue" : "International Conference on Industrial and Engineering Applications of Artificial Intelligence and Expert Systems, pp. 725-735, 2002.",
      "citeRegEx" : "189",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "An emotional storyteller robot",
      "author" : [ "A. Chella", "R.E. Barone", "G. Pilato", "R. Sorbello" ],
      "venue" : "AAAI Spring Symposium on Emotion, Personality, and Social Behavior, pp. 17-22, 2008",
      "citeRegEx" : "190",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Generalized grounding graphs: A probabilistic framework for understanding grounded language",
      "author" : [ "T. Kollar", "S. Tellex", "M.R. Walter", "A. Huang", "A. Bachrach", "S. Hemachandra", "E. Brunskill", "A. Banerjee", "D. Roy", "S. Teller", "N. Roy" ],
      "venue" : "Journal of Artificial Intelligence Research, pp. 1-35, 2013.",
      "citeRegEx" : "191",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Robot programming by demonstration with situated spatial language understanding",
      "author" : [ "M. Forbes", "R.P.N. Rao", "L. Zettlemoyer", "M. Cakmak" ],
      "venue" : "IEEE International Conference on Robotics and Automation (ICRA), pp. 2014-2020, 2015.",
      "citeRegEx" : "192",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Language bootstrapping: Learning word meanings from perception–action association",
      "author" : [ "G. Salvi", "L. Montesano", "A. Bernardino", "J. Santos-Victor" ],
      "venue" : "IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), vol. 42, no. 3, pp. 660-671, 2011.",
      "citeRegEx" : "193",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Anticipatory robot control for efficient human-robot collaboration",
      "author" : [ "C.M. Huang", "B. Mutlu" ],
      "venue" : "11th ACM/IEEE International Conference on Human-Robot Interaction (HRI), pp. 83-90, 2016.",
      "citeRegEx" : "194",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Enhanced visual scene understanding through human-robot dialog",
      "author" : [ "M. Johnson-Roberson", "J. Bohg", "G. Skantze", "J. Gustafson", "R. Carlson", "B. Rasolzadeh", "D. Kragic" ],
      "venue" : "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 3342-3348, 2011.",
      "citeRegEx" : "195",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Spoken language interaction with model uncertainty: an adaptive human–robot interaction system",
      "author" : [ "F. Doshi", "N. Roy" ],
      "venue" : "Connection Science, vol. 20, no. 4, pp. 299-318, 2008.  submitted to Knowledge-based Systems, January 2017",
      "citeRegEx" : "196",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Mobile robot programming using natural language",
      "author" : [ "S. Lauria", "G. Bugmann", "T. Kyriacou", "E. Klein" ],
      "venue" : "Robotics and Autonomous Systems, vol. 38, no. 3-4, pp. 171-181, 2002.",
      "citeRegEx" : "197",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Learning spatial-semantic representations from natural language descriptions and scene classifications",
      "author" : [ "S. Hemachandra", "M.R. Walter", "S. Tellex", "S. Teller" ],
      "venue" : "IEEE International Conference on Robotics and Automation (ICRA), pp. 2623-2630, 2015.",
      "citeRegEx" : "198",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Robot task planning and explanation in open and uncertain worlds",
      "author" : [ "M. Hanheide", "M. Gobelbecker", "G.S. Horn", "A. Pronobis", "K. Sjoo", "A. Aydemir", "P. Jensfelt", "C. Gretton", "R. Dearden", "M. Janicek", "H. Zender", "G. Kruijff", "N. Hawes", "J.L. Wyatt" ],
      "venue" : "Artificial Intelligence, 2015.",
      "citeRegEx" : "199",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Toward open knowledge enabling for Human-Robot interaction",
      "author" : [ "X. Chen", "J. Xie", "J. Ji", "Z. Sui" ],
      "venue" : "Journal of Human-Robot Interaction, vol. 1, no. 2, pp. 100- 117, 2012.",
      "citeRegEx" : "200",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Deep learning via semi-supervised embedding",
      "author" : [ "J. Weston", "F. Ratle", "R. Collobert" ],
      "venue" : "Proceedings of the 25th international conference on Machine learning (ICML), pp. 639-655, 2008.",
      "citeRegEx" : "201",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "I. Sutskever", "O. Vinyals", "Q.V. Le" ],
      "venue" : "Advances in neural information processing systems, vol. 4, pp. 3104-3112, 2014.",
      "citeRegEx" : "202",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "D. Bahdanau", "K. Cho", "Y. Bengio" ],
      "venue" : "International Conference on Learning Representation (ICLR), 2015.",
      "citeRegEx" : "203",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Domain adaptation for large-scale sentiment classification: A deep learning approach",
      "author" : [ "X. Glorot", "A. Bordes", "Y. Bengio" ],
      "venue" : "International Conference on Machine learning (ICML), pp. 513-520, 2011.",
      "citeRegEx" : "204",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Using the web to interactively learn to find objects",
      "author" : [ "M. Samadi", "T. Kollar", "M. Veloso" ],
      "venue" : "Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence, 2012.",
      "citeRegEx" : "205",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Affective personalization of a social robot tutor for children's second language skills",
      "author" : [ "G. Gordon", "S. Spaulding", "J.K. Westlund", "J.L. Jin", "L. Plummer", "M. Martinez", "M. Das", "C. Breazeal" ],
      "venue" : "Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, pp. 3951-3957, 2016.",
      "citeRegEx" : "206",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Teach me -- show me” -- end-user personalization of a smart home and companion robot",
      "author" : [ "J. Saunders", "D.S. Syrdal", "K.L. Koay", "N. Burke", "K. Dautenhahn" ],
      "venue" : "IEEE Transactions on Human-Machine Systems, vol. 46, no. 1, pp. 27-40, 2016.",
      "citeRegEx" : "207",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Assessment of the positioning performance of an industrial robot",
      "author" : [ "M. Slamani", "A. Nubiola", "I. Bonev" ],
      "venue" : "Industrial Robot, vol. 39, no. 1, pp. 57-68, 2012.",
      "citeRegEx" : "208",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "From human instructions to robot actions: Formulation of goals, affordances and probabilistic planning",
      "author" : [ "A. Antunes", "L. Jamone", "G. Saponaro", "A. Bernardino", "R. Ventura" ],
      "venue" : "IEEE International Conference on Robotics and Automation (ICRA), pp. 5449-5454, 2016.",
      "citeRegEx" : "209",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Bayesian active learning-based robot tutor for children’s word-reading skills",
      "author" : [ "G. Gordon", "C. Breazeal" ],
      "venue" : "Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence, pp. 1343-1349, 2015.",
      "citeRegEx" : "210",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Cross-organizational collaborative workflow mining from a multi-source log",
      "author" : [ "Q. Zeng", "S. Sun", "H. Duan", "C. Liu", "H. Wang" ],
      "venue" : "Decision Support Systems vol. 54, no. 3, pp. 1280-1301, 2013.",
      "citeRegEx" : "211",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Web-video-mining-supported workflow modeling for laparoscopic surgeries",
      "author" : [ "R. Liu", "X. Zhang", "H. Zhang" ],
      "venue" : "Artificial Intelligence in Medicine, vol. 74, pp. 9-20, 2016.",
      "citeRegEx" : "212",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Generating machine-executable plans from end-user’s natural-language-instructions",
      "author" : [ "R. Liu", "X. Zhang" ],
      "venue" : "arXiv preprint arXiv:1611.06468, 2016.",
      "citeRegEx" : "213",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "ACT-R/E: An embodied cognitive architecture for human-robot interaction",
      "author" : [ "N. Legany", "G. Toldi", "N. Megyes", "C. Orban", "L. Kovacs", "A. Balog" ],
      "venue" : "Journal of Human-Robot Interaction, vol. 2, no. 1, pp. 30-55, 2013.",
      "citeRegEx" : "214",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Anticipating human activities using object affordances for reactive robotic response",
      "author" : [ "H. Koppula", "A. Saxena" ],
      "venue" : "Robotics: Science and Systems, vol. 38, no. 1, pp. 14-29, 2013.",
      "citeRegEx" : "215",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Reinforcement learning improves behaviour from evaluative feedback",
      "author" : [ "M.L. Littman" ],
      "venue" : "Nature, vol. 521, no. 7553, pp. 445-451, 2015.",
      "citeRegEx" : "216",
      "shortCiteRegEx" : null,
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "1 Advantages Natural-language-facilitated human-robot cooperation (NLC), in which a human uses either spoken or written instructions to communicate with a robot for task cooperation [1][2][3][4], has received increasing attention in humaninvolved robotics research.",
      "startOffset" : 182,
      "endOffset" : 185
    }, {
      "referenceID" : 1,
      "context" : "1 Advantages Natural-language-facilitated human-robot cooperation (NLC), in which a human uses either spoken or written instructions to communicate with a robot for task cooperation [1][2][3][4], has received increasing attention in humaninvolved robotics research.",
      "startOffset" : 185,
      "endOffset" : 188
    }, {
      "referenceID" : 2,
      "context" : "1 Advantages Natural-language-facilitated human-robot cooperation (NLC), in which a human uses either spoken or written instructions to communicate with a robot for task cooperation [1][2][3][4], has received increasing attention in humaninvolved robotics research.",
      "startOffset" : 188,
      "endOffset" : 191
    }, {
      "referenceID" : 3,
      "context" : "1 Advantages Natural-language-facilitated human-robot cooperation (NLC), in which a human uses either spoken or written instructions to communicate with a robot for task cooperation [1][2][3][4], has received increasing attention in humaninvolved robotics research.",
      "startOffset" : 191,
      "endOffset" : 194
    }, {
      "referenceID" : 4,
      "context" : "By using natural language (NL), human intelligence at high-level mental planning and robot capability at low-level physical execution are combined to perform an intuitive cooperation [5][6].",
      "startOffset" : 183,
      "endOffset" : 186
    }, {
      "referenceID" : 5,
      "context" : "By using natural language (NL), human intelligence at high-level mental planning and robot capability at low-level physical execution are combined to perform an intuitive cooperation [5][6].",
      "startOffset" : 186,
      "endOffset" : 189
    }, {
      "referenceID" : 6,
      "context" : "Compared with human-robot cooperation (HRC) using tactile indications such as contact location [7] and force",
      "startOffset" : 95,
      "endOffset" : 98
    }, {
      "referenceID" : 7,
      "context" : "strength [8][9][10], and visual indications such as body pose [11][12][13] and motion [14][15][16], HRC using NL indications has several advantages.",
      "startOffset" : 9,
      "endOffset" : 12
    }, {
      "referenceID" : 8,
      "context" : "strength [8][9][10], and visual indications such as body pose [11][12][13] and motion [14][15][16], HRC using NL indications has several advantages.",
      "startOffset" : 12,
      "endOffset" : 15
    }, {
      "referenceID" : 9,
      "context" : "strength [8][9][10], and visual indications such as body pose [11][12][13] and motion [14][15][16], HRC using NL indications has several advantages.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 10,
      "context" : "strength [8][9][10], and visual indications such as body pose [11][12][13] and motion [14][15][16], HRC using NL indications has several advantages.",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 11,
      "context" : "strength [8][9][10], and visual indications such as body pose [11][12][13] and motion [14][15][16], HRC using NL indications has several advantages.",
      "startOffset" : 66,
      "endOffset" : 70
    }, {
      "referenceID" : 12,
      "context" : "strength [8][9][10], and visual indications such as body pose [11][12][13] and motion [14][15][16], HRC using NL indications has several advantages.",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 13,
      "context" : "strength [8][9][10], and visual indications such as body pose [11][12][13] and motion [14][15][16], HRC using NL indications has several advantages.",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 14,
      "context" : "strength [8][9][10], and visual indications such as body pose [11][12][13] and motion [14][15][16], HRC using NL indications has several advantages.",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 15,
      "context" : "strength [8][9][10], and visual indications such as body pose [11][12][13] and motion [14][15][16], HRC using NL indications has several advantages.",
      "startOffset" : 94,
      "endOffset" : 98
    }, {
      "referenceID" : 16,
      "context" : "the human involved in HRC needs to be trained to use certain actions/poses for making themselves understandable [17][18][19][20][21] .",
      "startOffset" : 112,
      "endOffset" : 116
    }, {
      "referenceID" : 17,
      "context" : "the human involved in HRC needs to be trained to use certain actions/poses for making themselves understandable [17][18][19][20][21] .",
      "startOffset" : 116,
      "endOffset" : 120
    }, {
      "referenceID" : 18,
      "context" : "the human involved in HRC needs to be trained to use certain actions/poses for making themselves understandable [17][18][19][20][21] .",
      "startOffset" : 120,
      "endOffset" : 124
    }, {
      "referenceID" : 19,
      "context" : "the human involved in HRC needs to be trained to use certain actions/poses for making themselves understandable [17][18][19][20][21] .",
      "startOffset" : 124,
      "endOffset" : 128
    }, {
      "referenceID" : 20,
      "context" : "the human involved in HRC needs to be trained to use certain actions/poses for making themselves understandable [17][18][19][20][21] .",
      "startOffset" : 128,
      "endOffset" : 132
    }, {
      "referenceID" : 2,
      "context" : "While in NLC, even non-expert users without prior training can cooperate with a robot by using human-like communication [3][22][23] .",
      "startOffset" : 120,
      "endOffset" : 123
    }, {
      "referenceID" : 21,
      "context" : "While in NLC, even non-expert users without prior training can cooperate with a robot by using human-like communication [3][22][23] .",
      "startOffset" : 123,
      "endOffset" : 127
    }, {
      "referenceID" : 22,
      "context" : "While in NLC, even non-expert users without prior training can cooperate with a robot by using human-like communication [3][22][23] .",
      "startOffset" : 127,
      "endOffset" : 131
    }, {
      "referenceID" : 23,
      "context" : "The traditional methods using actions/poses only provide limited patterns to roughly describe cooperation requests due to the information loss in the action/pose simplification (such as using markers to simplify the actions) [24][25][26][27].",
      "startOffset" : 225,
      "endOffset" : 229
    }, {
      "referenceID" : 24,
      "context" : "The traditional methods using actions/poses only provide limited patterns to roughly describe cooperation requests due to the information loss in the action/pose simplification (such as using markers to simplify the actions) [24][25][26][27].",
      "startOffset" : 229,
      "endOffset" : 233
    }, {
      "referenceID" : 25,
      "context" : "The traditional methods using actions/poses only provide limited patterns to roughly describe cooperation requests due to the information loss in the action/pose simplification (such as using markers to simplify the actions) [24][25][26][27].",
      "startOffset" : 233,
      "endOffset" : 237
    }, {
      "referenceID" : 26,
      "context" : "The traditional methods using actions/poses only provide limited patterns to roughly describe cooperation requests due to the information loss in the action/pose simplification (such as using markers to simplify the actions) [24][25][26][27].",
      "startOffset" : 237,
      "endOffset" : 241
    }, {
      "referenceID" : 4,
      "context" : "While in NLC, cooperation requests related to action, speed, tool and location are already defined in NL expressions [5][28][29][30].",
      "startOffset" : 117,
      "endOffset" : 120
    }, {
      "referenceID" : 27,
      "context" : "While in NLC, cooperation requests related to action, speed, tool and location are already defined in NL expressions [5][28][29][30].",
      "startOffset" : 120,
      "endOffset" : 124
    }, {
      "referenceID" : 28,
      "context" : "While in NLC, cooperation requests related to action, speed, tool and location are already defined in NL expressions [5][28][29][30].",
      "startOffset" : 124,
      "endOffset" : 128
    }, {
      "referenceID" : 29,
      "context" : "While in NLC, cooperation requests related to action, speed, tool and location are already defined in NL expressions [5][28][29][30].",
      "startOffset" : 128,
      "endOffset" : 132
    }, {
      "referenceID" : 23,
      "context" : "the design of informative patterns for different cooperation requests [24][25][26][27].",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 24,
      "context" : "the design of informative patterns for different cooperation requests [24][25][26][27].",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 25,
      "context" : "the design of informative patterns for different cooperation requests [24][25][26][27].",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 26,
      "context" : "the design of informative patterns for different cooperation requests [24][25][26][27].",
      "startOffset" : 82,
      "endOffset" : 86
    }, {
      "referenceID" : 30,
      "context" : "expressions to serve as patterns [31][32].",
      "startOffset" : 33,
      "endOffset" : 37
    }, {
      "referenceID" : 31,
      "context" : "Lastly, since the instructions are delivered orally instead of being physically involved, human hands are set free to perform more important executions [33].",
      "startOffset" : 152,
      "endOffset" : 156
    }, {
      "referenceID" : 0,
      "context" : "advantages, NLC has been widely explored in areas, including daily assistances [1][33][35], medical caregiving [36][37][38][39], manufacturing [5][40][41], indoor/outdoor navigation [2][42][43], social accompany [44][45][46]",
      "startOffset" : 79,
      "endOffset" : 82
    }, {
      "referenceID" : 31,
      "context" : "advantages, NLC has been widely explored in areas, including daily assistances [1][33][35], medical caregiving [36][37][38][39], manufacturing [5][40][41], indoor/outdoor navigation [2][42][43], social accompany [44][45][46]",
      "startOffset" : 82,
      "endOffset" : 86
    }, {
      "referenceID" : 33,
      "context" : "advantages, NLC has been widely explored in areas, including daily assistances [1][33][35], medical caregiving [36][37][38][39], manufacturing [5][40][41], indoor/outdoor navigation [2][42][43], social accompany [44][45][46]",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 34,
      "context" : "advantages, NLC has been widely explored in areas, including daily assistances [1][33][35], medical caregiving [36][37][38][39], manufacturing [5][40][41], indoor/outdoor navigation [2][42][43], social accompany [44][45][46]",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 35,
      "context" : "advantages, NLC has been widely explored in areas, including daily assistances [1][33][35], medical caregiving [36][37][38][39], manufacturing [5][40][41], indoor/outdoor navigation [2][42][43], social accompany [44][45][46]",
      "startOffset" : 115,
      "endOffset" : 119
    }, {
      "referenceID" : 36,
      "context" : "advantages, NLC has been widely explored in areas, including daily assistances [1][33][35], medical caregiving [36][37][38][39], manufacturing [5][40][41], indoor/outdoor navigation [2][42][43], social accompany [44][45][46]",
      "startOffset" : 119,
      "endOffset" : 123
    }, {
      "referenceID" : 37,
      "context" : "advantages, NLC has been widely explored in areas, including daily assistances [1][33][35], medical caregiving [36][37][38][39], manufacturing [5][40][41], indoor/outdoor navigation [2][42][43], social accompany [44][45][46]",
      "startOffset" : 123,
      "endOffset" : 127
    }, {
      "referenceID" : 4,
      "context" : "advantages, NLC has been widely explored in areas, including daily assistances [1][33][35], medical caregiving [36][37][38][39], manufacturing [5][40][41], indoor/outdoor navigation [2][42][43], social accompany [44][45][46]",
      "startOffset" : 143,
      "endOffset" : 146
    }, {
      "referenceID" : 38,
      "context" : "advantages, NLC has been widely explored in areas, including daily assistances [1][33][35], medical caregiving [36][37][38][39], manufacturing [5][40][41], indoor/outdoor navigation [2][42][43], social accompany [44][45][46]",
      "startOffset" : 146,
      "endOffset" : 150
    }, {
      "referenceID" : 39,
      "context" : "advantages, NLC has been widely explored in areas, including daily assistances [1][33][35], medical caregiving [36][37][38][39], manufacturing [5][40][41], indoor/outdoor navigation [2][42][43], social accompany [44][45][46]",
      "startOffset" : 150,
      "endOffset" : 154
    }, {
      "referenceID" : 1,
      "context" : "advantages, NLC has been widely explored in areas, including daily assistances [1][33][35], medical caregiving [36][37][38][39], manufacturing [5][40][41], indoor/outdoor navigation [2][42][43], social accompany [44][45][46]",
      "startOffset" : 182,
      "endOffset" : 185
    }, {
      "referenceID" : 40,
      "context" : "advantages, NLC has been widely explored in areas, including daily assistances [1][33][35], medical caregiving [36][37][38][39], manufacturing [5][40][41], indoor/outdoor navigation [2][42][43], social accompany [44][45][46]",
      "startOffset" : 185,
      "endOffset" : 189
    }, {
      "referenceID" : 41,
      "context" : "advantages, NLC has been widely explored in areas, including daily assistances [1][33][35], medical caregiving [36][37][38][39], manufacturing [5][40][41], indoor/outdoor navigation [2][42][43], social accompany [44][45][46]",
      "startOffset" : 189,
      "endOffset" : 193
    }, {
      "referenceID" : 42,
      "context" : "advantages, NLC has been widely explored in areas, including daily assistances [1][33][35], medical caregiving [36][37][38][39], manufacturing [5][40][41], indoor/outdoor navigation [2][42][43], social accompany [44][45][46]",
      "startOffset" : 212,
      "endOffset" : 216
    }, {
      "referenceID" : 43,
      "context" : "advantages, NLC has been widely explored in areas, including daily assistances [1][33][35], medical caregiving [36][37][38][39], manufacturing [5][40][41], indoor/outdoor navigation [2][42][43], social accompany [44][45][46]",
      "startOffset" : 216,
      "endOffset" : 220
    }, {
      "referenceID" : 44,
      "context" : "advantages, NLC has been widely explored in areas, including daily assistances [1][33][35], medical caregiving [36][37][38][39], manufacturing [5][40][41], indoor/outdoor navigation [2][42][43], social accompany [44][45][46]",
      "startOffset" : 220,
      "endOffset" : 224
    }, {
      "referenceID" : 45,
      "context" : "Pushing force one: development of NLP Recently, supported by machine learning technics in classification [47], clustering [48] and feature extraction",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 46,
      "context" : "Pushing force one: development of NLP Recently, supported by machine learning technics in classification [47], clustering [48] and feature extraction",
      "startOffset" : 122,
      "endOffset" : 126
    }, {
      "referenceID" : 47,
      "context" : "[49], NLP has been developed from simply syntax-driven processing, which builds syntax representations of sentence",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 48,
      "context" : "submitted to Knowledge-based Systems, January 2017 structures, to semantically-driven processing, which builds semantic networks for sentence meanings [50].",
      "startOffset" : 151,
      "endOffset" : 155
    }, {
      "referenceID" : 49,
      "context" : "At the early stage (1950-2000) of NLP [51], a word-based understanding method was developed to enable a naïve word-symbol understanding by identifying single/multiple keywords [52][53], lexical affinities [54][55], and word/affinity occurrences [56][57].",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 50,
      "context" : "At the early stage (1950-2000) of NLP [51], a word-based understanding method was developed to enable a naïve word-symbol understanding by identifying single/multiple keywords [52][53], lexical affinities [54][55], and word/affinity occurrences [56][57].",
      "startOffset" : 176,
      "endOffset" : 180
    }, {
      "referenceID" : 51,
      "context" : "At the early stage (1950-2000) of NLP [51], a word-based understanding method was developed to enable a naïve word-symbol understanding by identifying single/multiple keywords [52][53], lexical affinities [54][55], and word/affinity occurrences [56][57].",
      "startOffset" : 180,
      "endOffset" : 184
    }, {
      "referenceID" : 52,
      "context" : "At the early stage (1950-2000) of NLP [51], a word-based understanding method was developed to enable a naïve word-symbol understanding by identifying single/multiple keywords [52][53], lexical affinities [54][55], and word/affinity occurrences [56][57].",
      "startOffset" : 205,
      "endOffset" : 209
    }, {
      "referenceID" : 53,
      "context" : "At the early stage (1950-2000) of NLP [51], a word-based understanding method was developed to enable a naïve word-symbol understanding by identifying single/multiple keywords [52][53], lexical affinities [54][55], and word/affinity occurrences [56][57].",
      "startOffset" : 209,
      "endOffset" : 213
    }, {
      "referenceID" : 54,
      "context" : "At the early stage (1950-2000) of NLP [51], a word-based understanding method was developed to enable a naïve word-symbol understanding by identifying single/multiple keywords [52][53], lexical affinities [54][55], and word/affinity occurrences [56][57].",
      "startOffset" : 245,
      "endOffset" : 249
    }, {
      "referenceID" : 55,
      "context" : "At the early stage (1950-2000) of NLP [51], a word-based understanding method was developed to enable a naïve word-symbol understanding by identifying single/multiple keywords [52][53], lexical affinities [54][55], and word/affinity occurrences [56][57].",
      "startOffset" : 249,
      "endOffset" : 253
    }, {
      "referenceID" : 14,
      "context" : "For example, with the word-based method, a robot understood when a human mentioned the word “cup”, but it did not understand the related requests such as “I need a cup of water”[15].",
      "startOffset" : 177,
      "endOffset" : 181
    }, {
      "referenceID" : 49,
      "context" : "If the available training samples are limited, thereby leading to the ignorance of some keywords, the meaning understanding will be poor [51].",
      "startOffset" : 137,
      "endOffset" : 141
    }, {
      "referenceID" : 56,
      "context" : "These two drawbacks limited robots’ understanding to a shallow level where only symbolic NL expressions were analyzed [58][59].",
      "startOffset" : 118,
      "endOffset" : 122
    }, {
      "referenceID" : 57,
      "context" : "These two drawbacks limited robots’ understanding to a shallow level where only symbolic NL expressions were analyzed [58][59].",
      "startOffset" : 122,
      "endOffset" : 126
    }, {
      "referenceID" : 58,
      "context" : "meanings for sentences by exploring the embedded concepts, which mainly included implicit NL indications [60][61], hierarchical ontologies [62][63], and semantic correlations [64][65].",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 59,
      "context" : "meanings for sentences by exploring the embedded concepts, which mainly included implicit NL indications [60][61], hierarchical ontologies [62][63], and semantic correlations [64][65].",
      "startOffset" : 109,
      "endOffset" : 113
    }, {
      "referenceID" : 60,
      "context" : "meanings for sentences by exploring the embedded concepts, which mainly included implicit NL indications [60][61], hierarchical ontologies [62][63], and semantic correlations [64][65].",
      "startOffset" : 139,
      "endOffset" : 143
    }, {
      "referenceID" : 61,
      "context" : "meanings for sentences by exploring the embedded concepts, which mainly included implicit NL indications [60][61], hierarchical ontologies [62][63], and semantic correlations [64][65].",
      "startOffset" : 143,
      "endOffset" : 147
    }, {
      "referenceID" : 62,
      "context" : "meanings for sentences by exploring the embedded concepts, which mainly included implicit NL indications [60][61], hierarchical ontologies [62][63], and semantic correlations [64][65].",
      "startOffset" : 175,
      "endOffset" : 179
    }, {
      "referenceID" : 63,
      "context" : "meanings for sentences by exploring the embedded concepts, which mainly included implicit NL indications [60][61], hierarchical ontologies [62][63], and semantic correlations [64][65].",
      "startOffset" : 179,
      "endOffset" : 183
    }, {
      "referenceID" : 4,
      "context" : "The concept-based method was widely used in research such as [5][6][23] for complicated NL expression analysis.",
      "startOffset" : 61,
      "endOffset" : 64
    }, {
      "referenceID" : 5,
      "context" : "The concept-based method was widely used in research such as [5][6][23] for complicated NL expression analysis.",
      "startOffset" : 64,
      "endOffset" : 67
    }, {
      "referenceID" : 22,
      "context" : "The concept-based method was widely used in research such as [5][6][23] for complicated NL expression analysis.",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 4,
      "context" : "expressions, the knowledge represented by the concept-based method cannot model a structural knowledge [5][17][40] This drawback limited robots’ understanding towards task procedures and task-world correlations and further limited NLC in practical situations in the real world.",
      "startOffset" : 103,
      "endOffset" : 106
    }, {
      "referenceID" : 16,
      "context" : "expressions, the knowledge represented by the concept-based method cannot model a structural knowledge [5][17][40] This drawback limited robots’ understanding towards task procedures and task-world correlations and further limited NLC in practical situations in the real world.",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 38,
      "context" : "expressions, the knowledge represented by the concept-based method cannot model a structural knowledge [5][17][40] This drawback limited robots’ understanding towards task procedures and task-world correlations and further limited NLC in practical situations in the real world.",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 66,
      "context" : "To support a practical implementation of NL understanding, a narrative-based method was developed to create a more sophisticated knowledge representation in a decision-making-focused [68], real-world-aware [69] and human-",
      "startOffset" : 183,
      "endOffset" : 187
    }, {
      "referenceID" : 67,
      "context" : "To support a practical implementation of NL understanding, a narrative-based method was developed to create a more sophisticated knowledge representation in a decision-making-focused [68], real-world-aware [69] and human-",
      "startOffset" : 206,
      "endOffset" : 210
    }, {
      "referenceID" : 68,
      "context" : "cognition-imitated manner [70][71].",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 69,
      "context" : "cognition-imitated manner [70][71].",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 70,
      "context" : "In an early period (about 1940s’[72]), humans started to cooperate with robots by using remote controllers, developing an initial HRC, in which the cooperation requirements for action mapping, task goal mapping, and",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 3,
      "context" : "Compared with HRI, which focuses on general interactions (detailed HRI reviews are in [4][73][74][75]) for physical/mental assistances without task-goal constrains, HRC focuses on specific cooperation for task fulfillment with task-goal constrains such as task planning and adjusting (detailed HRC reviews",
      "startOffset" : 86,
      "endOffset" : 89
    }, {
      "referenceID" : 71,
      "context" : "Compared with HRI, which focuses on general interactions (detailed HRI reviews are in [4][73][74][75]) for physical/mental assistances without task-goal constrains, HRC focuses on specific cooperation for task fulfillment with task-goal constrains such as task planning and adjusting (detailed HRC reviews",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 72,
      "context" : "Compared with HRI, which focuses on general interactions (detailed HRI reviews are in [4][73][74][75]) for physical/mental assistances without task-goal constrains, HRC focuses on specific cooperation for task fulfillment with task-goal constrains such as task planning and adjusting (detailed HRC reviews",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 73,
      "context" : "Compared with HRI, which focuses on general interactions (detailed HRI reviews are in [4][73][74][75]) for physical/mental assistances without task-goal constrains, HRC focuses on specific cooperation for task fulfillment with task-goal constrains such as task planning and adjusting (detailed HRC reviews",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 70,
      "context" : "are [72][76][77]).",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 74,
      "context" : "are [72][76][77]).",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 75,
      "context" : "are [72][76][77]).",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 64,
      "context" : "a[66] and 1.",
      "startOffset" : 1,
      "endOffset" : 5
    }, {
      "referenceID" : 0,
      "context" : "b[1] are NL-based robot daily assistance.",
      "startOffset" : 1,
      "endOffset" : 4
    }, {
      "referenceID" : 34,
      "context" : "a [36] and 2.",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 35,
      "context" : "b [37] are NL-based healthcare.",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 39,
      "context" : "a [41] and 3.",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 4,
      "context" : "b [5] are NL-based intelligence manufacturing.",
      "startOffset" : 2,
      "endOffset" : 5
    }, {
      "referenceID" : 40,
      "context" : "a [42] and 4.",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 41,
      "context" : "b [43] are NL-based indoor/outdoor navigation.",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 65,
      "context" : "a [67] and 5.",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 43,
      "context" : "b [45] are NL-based accompanying.",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 76,
      "context" : "For action-based HRC, which started from motor-control-based action design, a robot followed simple human instructions to adjust its actions [78][79].",
      "startOffset" : 141,
      "endOffset" : 145
    }, {
      "referenceID" : 77,
      "context" : "For action-based HRC, which started from motor-control-based action design, a robot followed simple human instructions to adjust its actions [78][79].",
      "startOffset" : 145,
      "endOffset" : 149
    }, {
      "referenceID" : 78,
      "context" : "To make the robot actions natural, human-like motion style was then adopted in robot action design [80][81].",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 79,
      "context" : "To make the robot actions natural, human-like motion style was then adopted in robot action design [80][81].",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 80,
      "context" : "Though robots’ motion behavior was similar to that of a human, robots’ cooperation performances were still poor due to the limited action-understanding being insufficient to support its adaptations towards users/environments [82][83] .",
      "startOffset" : 225,
      "endOffset" : 229
    }, {
      "referenceID" : 81,
      "context" : "Though robots’ motion behavior was similar to that of a human, robots’ cooperation performances were still poor due to the limited action-understanding being insufficient to support its adaptations towards users/environments [82][83] .",
      "startOffset" : 229,
      "endOffset" : 233
    }, {
      "referenceID" : 38,
      "context" : "For example, ‘cup manipulation’ in ‘drinking’ activity meant ‘containing liquid’”[40].",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 82,
      "context" : "Though the understanding was still limited into action level, robots’ understanding towards human behaviors in HRC was improved [84].",
      "startOffset" : 128,
      "endOffset" : 132
    }, {
      "referenceID" : 83,
      "context" : "For interaction-based HRC, which started from action-understanding-based movement imitation [85][86], a robot was required to learn from human demonstrations, understand human movements and develop its own movements.",
      "startOffset" : 92,
      "endOffset" : 96
    }, {
      "referenceID" : 84,
      "context" : "For interaction-based HRC, which started from action-understanding-based movement imitation [85][86], a robot was required to learn from human demonstrations, understand human movements and develop its own movements.",
      "startOffset" : 96,
      "endOffset" : 100
    }, {
      "referenceID" : 85,
      "context" : "To improve the understanding of human movements, robots were provided with various informative motion data such as human action trajectories [87], hand/body poses [88], and bio-signals [89].",
      "startOffset" : 141,
      "endOffset" : 145
    }, {
      "referenceID" : 86,
      "context" : "To improve the understanding of human movements, robots were provided with various informative motion data such as human action trajectories [87], hand/body poses [88], and bio-signals [89].",
      "startOffset" : 163,
      "endOffset" : 167
    }, {
      "referenceID" : 87,
      "context" : "To improve the understanding of human movements, robots were provided with various informative motion data such as human action trajectories [87], hand/body poses [88], and bio-signals [89].",
      "startOffset" : 185,
      "endOffset" : 189
    }, {
      "referenceID" : 88,
      "context" : "environment [90][91][92][93][94].",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 89,
      "context" : "environment [90][91][92][93][94].",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 90,
      "context" : "environment [90][91][92][93][94].",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 91,
      "context" : "environment [90][91][92][93][94].",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 92,
      "context" : "environment [90][91][92][93][94].",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 93,
      "context" : "For engagement-based HRC, individual-level factors, such as individual attentions [95][96], personalities [97][98], emotions [99][100] and safety factors [83][101] , were considered by robots in the cooperation.",
      "startOffset" : 82,
      "endOffset" : 86
    }, {
      "referenceID" : 94,
      "context" : "For engagement-based HRC, individual-level factors, such as individual attentions [95][96], personalities [97][98], emotions [99][100] and safety factors [83][101] , were considered by robots in the cooperation.",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 95,
      "context" : "For engagement-based HRC, individual-level factors, such as individual attentions [95][96], personalities [97][98], emotions [99][100] and safety factors [83][101] , were considered by robots in the cooperation.",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 96,
      "context" : "For engagement-based HRC, individual-level factors, such as individual attentions [95][96], personalities [97][98], emotions [99][100] and safety factors [83][101] , were considered by robots in the cooperation.",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 97,
      "context" : "For engagement-based HRC, individual-level factors, such as individual attentions [95][96], personalities [97][98], emotions [99][100] and safety factors [83][101] , were considered by robots in the cooperation.",
      "startOffset" : 125,
      "endOffset" : 129
    }, {
      "referenceID" : 98,
      "context" : "For engagement-based HRC, individual-level factors, such as individual attentions [95][96], personalities [97][98], emotions [99][100] and safety factors [83][101] , were considered by robots in the cooperation.",
      "startOffset" : 129,
      "endOffset" : 134
    }, {
      "referenceID" : 81,
      "context" : "For engagement-based HRC, individual-level factors, such as individual attentions [95][96], personalities [97][98], emotions [99][100] and safety factors [83][101] , were considered by robots in the cooperation.",
      "startOffset" : 154,
      "endOffset" : 158
    }, {
      "referenceID" : 99,
      "context" : "For engagement-based HRC, individual-level factors, such as individual attentions [95][96], personalities [97][98], emotions [99][100] and safety factors [83][101] , were considered by robots in the cooperation.",
      "startOffset" : 158,
      "endOffset" : 163
    }, {
      "referenceID" : 49,
      "context" : "The development of natural language processing (NLP) techniques [51].",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 100,
      "context" : "submitted to Knowledge-based Systems, January 2017 jointly learning language and affordances” in Cornell University [102], “robots that learn to communicate with humans through natural dialog” in the University of Texas at Austin [103], “collaborative research: modeling and verification of language-based interaction” in MIT [104], “language grounding in robotics” in University of Washington [105],",
      "startOffset" : 116,
      "endOffset" : 121
    }, {
      "referenceID" : 101,
      "context" : "submitted to Knowledge-based Systems, January 2017 jointly learning language and affordances” in Cornell University [102], “robots that learn to communicate with humans through natural dialog” in the University of Texas at Austin [103], “collaborative research: modeling and verification of language-based interaction” in MIT [104], “language grounding in robotics” in University of Washington [105],",
      "startOffset" : 326,
      "endOffset" : 331
    }, {
      "referenceID" : 102,
      "context" : "NLC research is regularly published in international journals such as IJRR [107], TRO [108], AI [109] and KBS [110], and international conferences such as ICRA [111], IROS [112]",
      "startOffset" : 75,
      "endOffset" : 80
    }, {
      "referenceID" : 103,
      "context" : "NLC research is regularly published in international journals such as IJRR [107], TRO [108], AI [109] and KBS [110], and international conferences such as ICRA [111], IROS [112]",
      "startOffset" : 86,
      "endOffset" : 91
    }, {
      "referenceID" : 104,
      "context" : "Compared with the existing review papers about HRC using gesture/pose [114][115], action/motion [116], and",
      "startOffset" : 70,
      "endOffset" : 75
    }, {
      "referenceID" : 105,
      "context" : "Compared with the existing review papers about HRC using gesture/pose [114][115], action/motion [116], and",
      "startOffset" : 75,
      "endOffset" : 80
    }, {
      "referenceID" : 106,
      "context" : "Compared with the existing review papers about HRC using gesture/pose [114][115], action/motion [116], and",
      "startOffset" : 96,
      "endOffset" : 101
    }, {
      "referenceID" : 107,
      "context" : "tactile [117], a review paper about NLC is lacking.",
      "startOffset" : 8,
      "endOffset" : 13
    }, {
      "referenceID" : 70,
      "context" : "The human is increasingly involved in HRC [72].",
      "startOffset" : 42,
      "endOffset" : 46
    }, {
      "referenceID" : 31,
      "context" : "To set human hands free for other tasks and reduce human’s physical burden, NL was initially used to replace physical control means, such as joysticks and remote controllers, which required the use of human hands [33].",
      "startOffset" : 213,
      "endOffset" : 217
    }, {
      "referenceID" : 108,
      "context" : "NL was initially used in robot teleoperation [118], in which the correct robot actions were selected by a human to guide it to a desired destination.",
      "startOffset" : 45,
      "endOffset" : 50
    }, {
      "referenceID" : 109,
      "context" : "The system in paper [119] used NL instructions to plan task-execution procedures for a robot.",
      "startOffset" : 20,
      "endOffset" : 25
    }, {
      "referenceID" : 109,
      "context" : "Typical NLC systems using symbolic word control include manipulation control [119][120] , motion trajectory control [121][122], navigation location & behavior control [88][123]etc.",
      "startOffset" : 77,
      "endOffset" : 82
    }, {
      "referenceID" : 110,
      "context" : "Typical NLC systems using symbolic word control include manipulation control [119][120] , motion trajectory control [121][122], navigation location & behavior control [88][123]etc.",
      "startOffset" : 82,
      "endOffset" : 87
    }, {
      "referenceID" : 111,
      "context" : "Typical NLC systems using symbolic word control include manipulation control [119][120] , motion trajectory control [121][122], navigation location & behavior control [88][123]etc.",
      "startOffset" : 121,
      "endOffset" : 126
    }, {
      "referenceID" : 86,
      "context" : "Typical NLC systems using symbolic word control include manipulation control [119][120] , motion trajectory control [121][122], navigation location & behavior control [88][123]etc.",
      "startOffset" : 167,
      "endOffset" : 171
    }, {
      "referenceID" : 112,
      "context" : "Typical NLC systems using symbolic word control include manipulation control [119][120] , motion trajectory control [121][122], navigation location & behavior control [88][123]etc.",
      "startOffset" : 171,
      "endOffset" : 176
    }, {
      "referenceID" : 113,
      "context" : "During the development of NL-based control, the mapping rules such as fuzzy [124]/strict [118] mappings were designed.",
      "startOffset" : 76,
      "endOffset" : 81
    }, {
      "referenceID" : 108,
      "context" : "During the development of NL-based control, the mapping rules such as fuzzy [124]/strict [118] mappings were designed.",
      "startOffset" : 89,
      "endOffset" : 94
    }, {
      "referenceID" : 114,
      "context" : "By exploring the semantic correlations, such as “grasp-cup” and “go-To-left” among these symbols [125], human commonsense was initially",
      "startOffset" : 97,
      "endOffset" : 102
    }, {
      "referenceID" : 109,
      "context" : "Moreover, the execution flexibility of a robot was also improved by extracting the general cooperation patterns such as “grasp(object)” [119][126] and “goTo(Location)” [2][127], improving robots’",
      "startOffset" : 136,
      "endOffset" : 141
    }, {
      "referenceID" : 115,
      "context" : "Moreover, the execution flexibility of a robot was also improved by extracting the general cooperation patterns such as “grasp(object)” [119][126] and “goTo(Location)” [2][127], improving robots’",
      "startOffset" : 141,
      "endOffset" : 146
    }, {
      "referenceID" : 1,
      "context" : "Moreover, the execution flexibility of a robot was also improved by extracting the general cooperation patterns such as “grasp(object)” [119][126] and “goTo(Location)” [2][127], improving robots’",
      "startOffset" : 168,
      "endOffset" : 171
    }, {
      "referenceID" : 116,
      "context" : "Moreover, the execution flexibility of a robot was also improved by extracting the general cooperation patterns such as “grasp(object)” [119][126] and “goTo(Location)” [2][127], improving robots’",
      "startOffset" : 171,
      "endOffset" : 176
    }, {
      "referenceID" : 117,
      "context" : "The joints’ motion directions of a robot arm were controlled by mapping the symbolic vowels from human’s oral instructions [128].",
      "startOffset" : 123,
      "endOffset" : 128
    }, {
      "referenceID" : 118,
      "context" : "By mapping the semantic correlations of the parts from human’s oral instructions, a robot was orally controlled by a human to perform assembling works [129].",
      "startOffset" : 151,
      "endOffset" : 156
    }, {
      "referenceID" : 119,
      "context" : "By mapping these logics from human oral instructions, robot performed kitchen tasks [130].",
      "startOffset" : 84,
      "endOffset" : 89
    }, {
      "referenceID" : 31,
      "context" : "During the task execution, a robot considered the practical environmental conditions such as “object availability, objects’ relative sizes” to perform the tasks such as “grab the small brown box” [33].",
      "startOffset" : 196,
      "endOffset" : 200
    }, {
      "referenceID" : 116,
      "context" : "trajectory in different situations [127][131], designing robot manipulation post according to fuzzy action type/speed requirements [88][124], serving meals by considering “foodType–vesselShape” relations [130][132], and assembling",
      "startOffset" : 35,
      "endOffset" : 40
    }, {
      "referenceID" : 120,
      "context" : "trajectory in different situations [127][131], designing robot manipulation post according to fuzzy action type/speed requirements [88][124], serving meals by considering “foodType–vesselShape” relations [130][132], and assembling",
      "startOffset" : 40,
      "endOffset" : 45
    }, {
      "referenceID" : 86,
      "context" : "trajectory in different situations [127][131], designing robot manipulation post according to fuzzy action type/speed requirements [88][124], serving meals by considering “foodType–vesselShape” relations [130][132], and assembling",
      "startOffset" : 131,
      "endOffset" : 135
    }, {
      "referenceID" : 113,
      "context" : "trajectory in different situations [127][131], designing robot manipulation post according to fuzzy action type/speed requirements [88][124], serving meals by considering “foodType–vesselShape” relations [130][132], and assembling",
      "startOffset" : 135,
      "endOffset" : 140
    }, {
      "referenceID" : 119,
      "context" : "trajectory in different situations [127][131], designing robot manipulation post according to fuzzy action type/speed requirements [88][124], serving meals by considering “foodType–vesselShape” relations [130][132], and assembling",
      "startOffset" : 204,
      "endOffset" : 209
    }, {
      "referenceID" : 121,
      "context" : "trajectory in different situations [127][131], designing robot manipulation post according to fuzzy action type/speed requirements [88][124], serving meals by considering “foodType–vesselShape” relations [130][132], and assembling",
      "startOffset" : 209,
      "endOffset" : 214
    }, {
      "referenceID" : 118,
      "context" : "industrial parts by considering the spatial matching relations [129][133] etc.",
      "startOffset" : 63,
      "endOffset" : 68
    }, {
      "referenceID" : 122,
      "context" : "industrial parts by considering the spatial matching relations [129][133] etc.",
      "startOffset" : 68,
      "endOffset" : 73
    }, {
      "referenceID" : 56,
      "context" : "With the NLC systems, typical applications that consider practical environment conditions include grasping with considering constraints in safety, temporal relations and human preferences [58][134], navigating by considering location/building matching [20][135], serving food with consideration of user locations [136], food vessel shapes and path conditions [119] etc.",
      "startOffset" : 188,
      "endOffset" : 192
    }, {
      "referenceID" : 123,
      "context" : "With the NLC systems, typical applications that consider practical environment conditions include grasping with considering constraints in safety, temporal relations and human preferences [58][134], navigating by considering location/building matching [20][135], serving food with consideration of user locations [136], food vessel shapes and path conditions [119] etc.",
      "startOffset" : 192,
      "endOffset" : 197
    }, {
      "referenceID" : 19,
      "context" : "With the NLC systems, typical applications that consider practical environment conditions include grasping with considering constraints in safety, temporal relations and human preferences [58][134], navigating by considering location/building matching [20][135], serving food with consideration of user locations [136], food vessel shapes and path conditions [119] etc.",
      "startOffset" : 252,
      "endOffset" : 256
    }, {
      "referenceID" : 124,
      "context" : "With the NLC systems, typical applications that consider practical environment conditions include grasping with considering constraints in safety, temporal relations and human preferences [58][134], navigating by considering location/building matching [20][135], serving food with consideration of user locations [136], food vessel shapes and path conditions [119] etc.",
      "startOffset" : 256,
      "endOffset" : 261
    }, {
      "referenceID" : 125,
      "context" : "With the NLC systems, typical applications that consider practical environment conditions include grasping with considering constraints in safety, temporal relations and human preferences [58][134], navigating by considering location/building matching [20][135], serving food with consideration of user locations [136], food vessel shapes and path conditions [119] etc.",
      "startOffset" : 313,
      "endOffset" : 318
    }, {
      "referenceID" : 109,
      "context" : "With the NLC systems, typical applications that consider practical environment conditions include grasping with considering constraints in safety, temporal relations and human preferences [58][134], navigating by considering location/building matching [20][135], serving food with consideration of user locations [136], food vessel shapes and path conditions [119] etc.",
      "startOffset" : 359,
      "endOffset" : 364
    }, {
      "referenceID" : 110,
      "context" : "Disadvantages inflexible, unnatural, limited adaptability unnatural, limited adaptability on dynamic situations implicit, unsafe/risky by ignoring real-world conditions not natural and intelligent for lacking a meaningful interpretation Typical references [120][121][122] [125][126][127] [129][130][131] [134][135][136]",
      "startOffset" : 256,
      "endOffset" : 261
    }, {
      "referenceID" : 111,
      "context" : "Disadvantages inflexible, unnatural, limited adaptability unnatural, limited adaptability on dynamic situations implicit, unsafe/risky by ignoring real-world conditions not natural and intelligent for lacking a meaningful interpretation Typical references [120][121][122] [125][126][127] [129][130][131] [134][135][136]",
      "startOffset" : 266,
      "endOffset" : 271
    }, {
      "referenceID" : 114,
      "context" : "Disadvantages inflexible, unnatural, limited adaptability unnatural, limited adaptability on dynamic situations implicit, unsafe/risky by ignoring real-world conditions not natural and intelligent for lacking a meaningful interpretation Typical references [120][121][122] [125][126][127] [129][130][131] [134][135][136]",
      "startOffset" : 272,
      "endOffset" : 277
    }, {
      "referenceID" : 115,
      "context" : "Disadvantages inflexible, unnatural, limited adaptability unnatural, limited adaptability on dynamic situations implicit, unsafe/risky by ignoring real-world conditions not natural and intelligent for lacking a meaningful interpretation Typical references [120][121][122] [125][126][127] [129][130][131] [134][135][136]",
      "startOffset" : 277,
      "endOffset" : 282
    }, {
      "referenceID" : 116,
      "context" : "Disadvantages inflexible, unnatural, limited adaptability unnatural, limited adaptability on dynamic situations implicit, unsafe/risky by ignoring real-world conditions not natural and intelligent for lacking a meaningful interpretation Typical references [120][121][122] [125][126][127] [129][130][131] [134][135][136]",
      "startOffset" : 282,
      "endOffset" : 287
    }, {
      "referenceID" : 118,
      "context" : "Disadvantages inflexible, unnatural, limited adaptability unnatural, limited adaptability on dynamic situations implicit, unsafe/risky by ignoring real-world conditions not natural and intelligent for lacking a meaningful interpretation Typical references [120][121][122] [125][126][127] [129][130][131] [134][135][136]",
      "startOffset" : 288,
      "endOffset" : 293
    }, {
      "referenceID" : 119,
      "context" : "Disadvantages inflexible, unnatural, limited adaptability unnatural, limited adaptability on dynamic situations implicit, unsafe/risky by ignoring real-world conditions not natural and intelligent for lacking a meaningful interpretation Typical references [120][121][122] [125][126][127] [129][130][131] [134][135][136]",
      "startOffset" : 293,
      "endOffset" : 298
    }, {
      "referenceID" : 120,
      "context" : "Disadvantages inflexible, unnatural, limited adaptability unnatural, limited adaptability on dynamic situations implicit, unsafe/risky by ignoring real-world conditions not natural and intelligent for lacking a meaningful interpretation Typical references [120][121][122] [125][126][127] [129][130][131] [134][135][136]",
      "startOffset" : 298,
      "endOffset" : 303
    }, {
      "referenceID" : 123,
      "context" : "Disadvantages inflexible, unnatural, limited adaptability unnatural, limited adaptability on dynamic situations implicit, unsafe/risky by ignoring real-world conditions not natural and intelligent for lacking a meaningful interpretation Typical references [120][121][122] [125][126][127] [129][130][131] [134][135][136]",
      "startOffset" : 304,
      "endOffset" : 309
    }, {
      "referenceID" : 124,
      "context" : "Disadvantages inflexible, unnatural, limited adaptability unnatural, limited adaptability on dynamic situations implicit, unsafe/risky by ignoring real-world conditions not natural and intelligent for lacking a meaningful interpretation Typical references [120][121][122] [125][126][127] [129][130][131] [134][135][136]",
      "startOffset" : 309,
      "endOffset" : 314
    }, {
      "referenceID" : 125,
      "context" : "Disadvantages inflexible, unnatural, limited adaptability unnatural, limited adaptability on dynamic situations implicit, unsafe/risky by ignoring real-world conditions not natural and intelligent for lacking a meaningful interpretation Typical references [120][121][122] [125][126][127] [129][130][131] [134][135][136]",
      "startOffset" : 314,
      "endOffset" : 319
    }, {
      "referenceID" : 126,
      "context" : "Given that human NL instructions are informative and natural, robot training initially started with using NL instructions to define task execution methods [137].",
      "startOffset" : 155,
      "endOffset" : 160
    }, {
      "referenceID" : 121,
      "context" : "applications include using the NL instructions to identify an object [132][138], object physical properties [139][140], and action associations [141][142].",
      "startOffset" : 69,
      "endOffset" : 74
    }, {
      "referenceID" : 127,
      "context" : "applications include using the NL instructions to identify an object [132][138], object physical properties [139][140], and action associations [141][142].",
      "startOffset" : 74,
      "endOffset" : 79
    }, {
      "referenceID" : 128,
      "context" : "applications include using the NL instructions to identify an object [132][138], object physical properties [139][140], and action associations [141][142].",
      "startOffset" : 108,
      "endOffset" : 113
    }, {
      "referenceID" : 129,
      "context" : "applications include using the NL instructions to identify an object [132][138], object physical properties [139][140], and action associations [141][142].",
      "startOffset" : 113,
      "endOffset" : 118
    }, {
      "referenceID" : 130,
      "context" : "applications include using the NL instructions to identify an object [132][138], object physical properties [139][140], and action associations [141][142].",
      "startOffset" : 144,
      "endOffset" : 149
    }, {
      "referenceID" : 131,
      "context" : "applications include using the NL instructions to identify an object [132][138], object physical properties [139][140], and action associations [141][142].",
      "startOffset" : 149,
      "endOffset" : 154
    }, {
      "referenceID" : 132,
      "context" : "manipulation [143][144], to describe spatial/temporal correlations for intuitive navigations [145][146], and to model human cognition for daily activity performing [147][148] .",
      "startOffset" : 13,
      "endOffset" : 18
    }, {
      "referenceID" : 133,
      "context" : "manipulation [143][144], to describe spatial/temporal correlations for intuitive navigations [145][146], and to model human cognition for daily activity performing [147][148] .",
      "startOffset" : 18,
      "endOffset" : 23
    }, {
      "referenceID" : 134,
      "context" : "manipulation [143][144], to describe spatial/temporal correlations for intuitive navigations [145][146], and to model human cognition for daily activity performing [147][148] .",
      "startOffset" : 93,
      "endOffset" : 98
    }, {
      "referenceID" : 135,
      "context" : "manipulation [143][144], to describe spatial/temporal correlations for intuitive navigations [145][146], and to model human cognition for daily activity performing [147][148] .",
      "startOffset" : 98,
      "endOffset" : 103
    }, {
      "referenceID" : 136,
      "context" : "manipulation [143][144], to describe spatial/temporal correlations for intuitive navigations [145][146], and to model human cognition for daily activity performing [147][148] .",
      "startOffset" : 164,
      "endOffset" : 169
    }, {
      "referenceID" : 137,
      "context" : "manipulation [143][144], to describe spatial/temporal correlations for intuitive navigations [145][146], and to model human cognition for daily activity performing [147][148] .",
      "startOffset" : 169,
      "endOffset" : 174
    }, {
      "referenceID" : 132,
      "context" : "By describing the physical properties of the objects, the robot-related knowledge was transferred from a human to a robot, enabling the robot to recognize objects in the future [143].",
      "startOffset" : 177,
      "endOffset" : 182
    }, {
      "referenceID" : 139,
      "context" : "[150].",
      "startOffset" : 0,
      "endOffset" : 5
    }, {
      "referenceID" : 38,
      "context" : "A robot proactively detected its missing knowledge and proactively ask human for knowledge support [40].",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 142,
      "context" : "During the execution process, a human proactively interfered with the execution and gives the timely feedbacks to improve a robot’s performances [153].",
      "startOffset" : 145,
      "endOffset" : 150
    }, {
      "referenceID" : 133,
      "context" : "With the NLC systems, typical applications using the demonstration-based robot training method include: learning object-manipulation methods by associating human NL expressions with sensor data such as touching force values, object color/shape/size and visual trajectory [144][149]; learning human-like gestures by associating human hand gesture with speech context [150][151]; learning object functional usages by simultaneously considering human voice behaviors, motion behaviors and environment conditions [147]; learning abstract interpretations of environmental conditions by combining human operations, human NL explanations, and the corresponding sensor data patterns [21][151]; adapting new situations by replacing NL-instructed knowledge with real-world-available knowledge [36][152] etc.",
      "startOffset" : 271,
      "endOffset" : 276
    }, {
      "referenceID" : 138,
      "context" : "With the NLC systems, typical applications using the demonstration-based robot training method include: learning object-manipulation methods by associating human NL expressions with sensor data such as touching force values, object color/shape/size and visual trajectory [144][149]; learning human-like gestures by associating human hand gesture with speech context [150][151]; learning object functional usages by simultaneously considering human voice behaviors, motion behaviors and environment conditions [147]; learning abstract interpretations of environmental conditions by combining human operations, human NL explanations, and the corresponding sensor data patterns [21][151]; adapting new situations by replacing NL-instructed knowledge with real-world-available knowledge [36][152] etc.",
      "startOffset" : 276,
      "endOffset" : 281
    }, {
      "referenceID" : 139,
      "context" : "With the NLC systems, typical applications using the demonstration-based robot training method include: learning object-manipulation methods by associating human NL expressions with sensor data such as touching force values, object color/shape/size and visual trajectory [144][149]; learning human-like gestures by associating human hand gesture with speech context [150][151]; learning object functional usages by simultaneously considering human voice behaviors, motion behaviors and environment conditions [147]; learning abstract interpretations of environmental conditions by combining human operations, human NL explanations, and the corresponding sensor data patterns [21][151]; adapting new situations by replacing NL-instructed knowledge with real-world-available knowledge [36][152] etc.",
      "startOffset" : 366,
      "endOffset" : 371
    }, {
      "referenceID" : 140,
      "context" : "With the NLC systems, typical applications using the demonstration-based robot training method include: learning object-manipulation methods by associating human NL expressions with sensor data such as touching force values, object color/shape/size and visual trajectory [144][149]; learning human-like gestures by associating human hand gesture with speech context [150][151]; learning object functional usages by simultaneously considering human voice behaviors, motion behaviors and environment conditions [147]; learning abstract interpretations of environmental conditions by combining human operations, human NL explanations, and the corresponding sensor data patterns [21][151]; adapting new situations by replacing NL-instructed knowledge with real-world-available knowledge [36][152] etc.",
      "startOffset" : 371,
      "endOffset" : 376
    }, {
      "referenceID" : 136,
      "context" : "With the NLC systems, typical applications using the demonstration-based robot training method include: learning object-manipulation methods by associating human NL expressions with sensor data such as touching force values, object color/shape/size and visual trajectory [144][149]; learning human-like gestures by associating human hand gesture with speech context [150][151]; learning object functional usages by simultaneously considering human voice behaviors, motion behaviors and environment conditions [147]; learning abstract interpretations of environmental conditions by combining human operations, human NL explanations, and the corresponding sensor data patterns [21][151]; adapting new situations by replacing NL-instructed knowledge with real-world-available knowledge [36][152] etc.",
      "startOffset" : 509,
      "endOffset" : 514
    }, {
      "referenceID" : 20,
      "context" : "With the NLC systems, typical applications using the demonstration-based robot training method include: learning object-manipulation methods by associating human NL expressions with sensor data such as touching force values, object color/shape/size and visual trajectory [144][149]; learning human-like gestures by associating human hand gesture with speech context [150][151]; learning object functional usages by simultaneously considering human voice behaviors, motion behaviors and environment conditions [147]; learning abstract interpretations of environmental conditions by combining human operations, human NL explanations, and the corresponding sensor data patterns [21][151]; adapting new situations by replacing NL-instructed knowledge with real-world-available knowledge [36][152] etc.",
      "startOffset" : 675,
      "endOffset" : 679
    }, {
      "referenceID" : 140,
      "context" : "With the NLC systems, typical applications using the demonstration-based robot training method include: learning object-manipulation methods by associating human NL expressions with sensor data such as touching force values, object color/shape/size and visual trajectory [144][149]; learning human-like gestures by associating human hand gesture with speech context [150][151]; learning object functional usages by simultaneously considering human voice behaviors, motion behaviors and environment conditions [147]; learning abstract interpretations of environmental conditions by combining human operations, human NL explanations, and the corresponding sensor data patterns [21][151]; adapting new situations by replacing NL-instructed knowledge with real-world-available knowledge [36][152] etc.",
      "startOffset" : 679,
      "endOffset" : 684
    }, {
      "referenceID" : 34,
      "context" : "With the NLC systems, typical applications using the demonstration-based robot training method include: learning object-manipulation methods by associating human NL expressions with sensor data such as touching force values, object color/shape/size and visual trajectory [144][149]; learning human-like gestures by associating human hand gesture with speech context [150][151]; learning object functional usages by simultaneously considering human voice behaviors, motion behaviors and environment conditions [147]; learning abstract interpretations of environmental conditions by combining human operations, human NL explanations, and the corresponding sensor data patterns [21][151]; adapting new situations by replacing NL-instructed knowledge with real-world-available knowledge [36][152] etc.",
      "startOffset" : 783,
      "endOffset" : 787
    }, {
      "referenceID" : 141,
      "context" : "With the NLC systems, typical applications using the demonstration-based robot training method include: learning object-manipulation methods by associating human NL expressions with sensor data such as touching force values, object color/shape/size and visual trajectory [144][149]; learning human-like gestures by associating human hand gesture with speech context [150][151]; learning object functional usages by simultaneously considering human voice behaviors, motion behaviors and environment conditions [147]; learning abstract interpretations of environmental conditions by combining human operations, human NL explanations, and the corresponding sensor data patterns [21][151]; adapting new situations by replacing NL-instructed knowledge with real-world-available knowledge [36][152] etc.",
      "startOffset" : 787,
      "endOffset" : 792
    }, {
      "referenceID" : 38,
      "context" : "With human NL feedback, robot behaviors in cooperation were logically modified by adding/removing some operation steps [40][96][153] or subjectively emphasizing on executions [154][155][156].",
      "startOffset" : 119,
      "endOffset" : 123
    }, {
      "referenceID" : 94,
      "context" : "With human NL feedback, robot behaviors in cooperation were logically modified by adding/removing some operation steps [40][96][153] or subjectively emphasizing on executions [154][155][156].",
      "startOffset" : 123,
      "endOffset" : 127
    }, {
      "referenceID" : 142,
      "context" : "With human NL feedback, robot behaviors in cooperation were logically modified by adding/removing some operation steps [40][96][153] or subjectively emphasizing on executions [154][155][156].",
      "startOffset" : 127,
      "endOffset" : 132
    }, {
      "referenceID" : 143,
      "context" : "With human NL feedback, robot behaviors in cooperation were logically modified by adding/removing some operation steps [40][96][153] or subjectively emphasizing on executions [154][155][156].",
      "startOffset" : 175,
      "endOffset" : 180
    }, {
      "referenceID" : 144,
      "context" : "With human NL feedback, robot behaviors in cooperation were logically modified by adding/removing some operation steps [40][96][153] or subjectively emphasizing on executions [154][155][156].",
      "startOffset" : 180,
      "endOffset" : 185
    }, {
      "referenceID" : 145,
      "context" : "With human NL feedback, robot behaviors in cooperation were logically modified by adding/removing some operation steps [40][96][153] or subjectively emphasizing on executions [154][155][156].",
      "startOffset" : 185,
      "endOffset" : 190
    }, {
      "referenceID" : 142,
      "context" : "With the NLC systems, typical applications include: indicating the human-desired locations/objects with NL instructions during HRC [153]; assessing robot execution performances and correcting robots’ undesired manipulation behaviors such as hand poses and object selections by using real-time NL instructions [96]; analyzing robots’ execution failures and helping a robot to learn from failures with NL conversations",
      "startOffset" : 131,
      "endOffset" : 136
    }, {
      "referenceID" : 94,
      "context" : "With the NLC systems, typical applications include: indicating the human-desired locations/objects with NL instructions during HRC [153]; assessing robot execution performances and correcting robots’ undesired manipulation behaviors such as hand poses and object selections by using real-time NL instructions [96]; analyzing robots’ execution failures and helping a robot to learn from failures with NL conversations",
      "startOffset" : 309,
      "endOffset" : 313
    }, {
      "referenceID" : 38,
      "context" : "[40]; and emphasizing/ignoring robot behaviors in complex task execution by subjective NL rewards and punishments such as “joy, anger” [155].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 144,
      "context" : "[40]; and emphasizing/ignoring robot behaviors in complex task execution by subjective NL rewards and punishments such as “joy, anger” [155].",
      "startOffset" : 135,
      "endOffset" : 140
    }, {
      "referenceID" : 146,
      "context" : "trajectory/action/pose selections in tasks such as “human-robot jointly carrying a bulky bumper” [157]; asking for knowledge disambiguation of human commands such as confirming the human-attended object “the blue cup” [158];",
      "startOffset" : 97,
      "endOffset" : 102
    }, {
      "referenceID" : 147,
      "context" : "trajectory/action/pose selections in tasks such as “human-robot jointly carrying a bulky bumper” [157]; asking for knowledge disambiguation of human commands such as confirming the human-attended object “the blue cup” [158];",
      "startOffset" : 218,
      "endOffset" : 223
    }, {
      "referenceID" : 38,
      "context" : "“deliver a table leg for a robot” [40]; asking for additional information such as “the object is yellow and rectangle” from a human to assisting robots’ perceiving [153][159] etc.",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 142,
      "context" : "“deliver a table leg for a robot” [40]; asking for additional information such as “the object is yellow and rectangle” from a human to assisting robots’ perceiving [153][159] etc.",
      "startOffset" : 164,
      "endOffset" : 169
    }, {
      "referenceID" : 148,
      "context" : "“deliver a table leg for a robot” [40]; asking for additional information such as “the object is yellow and rectangle” from a human to assisting robots’ perceiving [153][159] etc.",
      "startOffset" : 169,
      "endOffset" : 174
    }, {
      "referenceID" : 89,
      "context" : "submitted to Knowledge-based Systems, January 2017 continuous knowledge acquiring and refining [91].",
      "startOffset" : 95,
      "endOffset" : 99
    }, {
      "referenceID" : 128,
      "context" : "Typical references [139][140][141] [36][151][152] [154][155][156] [157][158][159]",
      "startOffset" : 19,
      "endOffset" : 24
    }, {
      "referenceID" : 129,
      "context" : "Typical references [139][140][141] [36][151][152] [154][155][156] [157][158][159]",
      "startOffset" : 24,
      "endOffset" : 29
    }, {
      "referenceID" : 130,
      "context" : "Typical references [139][140][141] [36][151][152] [154][155][156] [157][158][159]",
      "startOffset" : 29,
      "endOffset" : 34
    }, {
      "referenceID" : 34,
      "context" : "Typical references [139][140][141] [36][151][152] [154][155][156] [157][158][159]",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 140,
      "context" : "Typical references [139][140][141] [36][151][152] [154][155][156] [157][158][159]",
      "startOffset" : 39,
      "endOffset" : 44
    }, {
      "referenceID" : 141,
      "context" : "Typical references [139][140][141] [36][151][152] [154][155][156] [157][158][159]",
      "startOffset" : 44,
      "endOffset" : 49
    }, {
      "referenceID" : 143,
      "context" : "Typical references [139][140][141] [36][151][152] [154][155][156] [157][158][159]",
      "startOffset" : 50,
      "endOffset" : 55
    }, {
      "referenceID" : 144,
      "context" : "Typical references [139][140][141] [36][151][152] [154][155][156] [157][158][159]",
      "startOffset" : 55,
      "endOffset" : 60
    }, {
      "referenceID" : 145,
      "context" : "Typical references [139][140][141] [36][151][152] [154][155][156] [157][158][159]",
      "startOffset" : 60,
      "endOffset" : 65
    }, {
      "referenceID" : 146,
      "context" : "Typical references [139][140][141] [36][151][152] [154][155][156] [157][158][159]",
      "startOffset" : 66,
      "endOffset" : 71
    }, {
      "referenceID" : 147,
      "context" : "Typical references [139][140][141] [36][151][152] [154][155][156] [157][158][159]",
      "startOffset" : 71,
      "endOffset" : 76
    }, {
      "referenceID" : 148,
      "context" : "Typical references [139][140][141] [36][151][152] [154][155][156] [157][158][159]",
      "startOffset" : 76,
      "endOffset" : 81
    }, {
      "referenceID" : 151,
      "context" : "With NLC systems, typical applications using human-centered executions include performing tasks such as “table assembly”, during which the human sets up task goal (assembly a specific part), makes plans (action steps, pose and tool usages) and partially executes tasks (assemble the parts together), and the robot provides humandesired assistances (tool delivery, part delivery, part holding) [162][163].",
      "startOffset" : 393,
      "endOffset" : 398
    }, {
      "referenceID" : 152,
      "context" : "With NLC systems, typical applications using human-centered executions include performing tasks such as “table assembly”, during which the human sets up task goal (assembly a specific part), makes plans (action steps, pose and tool usages) and partially executes tasks (assemble the parts together), and the robot provides humandesired assistances (tool delivery, part delivery, part holding) [162][163].",
      "startOffset" : 398,
      "endOffset" : 403
    }, {
      "referenceID" : 153,
      "context" : "centered execution was developed so that a human was only burdened with cognitive responsibilities such as explaining the navigation routine [164][165], describing the needed objects and location/pose [66][166] and guiding the fine/rough processing [5][167].",
      "startOffset" : 141,
      "endOffset" : 146
    }, {
      "referenceID" : 154,
      "context" : "centered execution was developed so that a human was only burdened with cognitive responsibilities such as explaining the navigation routine [164][165], describing the needed objects and location/pose [66][166] and guiding the fine/rough processing [5][167].",
      "startOffset" : 146,
      "endOffset" : 151
    }, {
      "referenceID" : 64,
      "context" : "centered execution was developed so that a human was only burdened with cognitive responsibilities such as explaining the navigation routine [164][165], describing the needed objects and location/pose [66][166] and guiding the fine/rough processing [5][167].",
      "startOffset" : 201,
      "endOffset" : 205
    }, {
      "referenceID" : 155,
      "context" : "centered execution was developed so that a human was only burdened with cognitive responsibilities such as explaining the navigation routine [164][165], describing the needed objects and location/pose [66][166] and guiding the fine/rough processing [5][167].",
      "startOffset" : 205,
      "endOffset" : 210
    }, {
      "referenceID" : 4,
      "context" : "centered execution was developed so that a human was only burdened with cognitive responsibilities such as explaining the navigation routine [164][165], describing the needed objects and location/pose [66][166] and guiding the fine/rough processing [5][167].",
      "startOffset" : 249,
      "endOffset" : 252
    }, {
      "referenceID" : 156,
      "context" : "centered execution was developed so that a human was only burdened with cognitive responsibilities such as explaining the navigation routine [164][165], describing the needed objects and location/pose [66][166] and guiding the fine/rough processing [5][167].",
      "startOffset" : 252,
      "endOffset" : 257
    }, {
      "referenceID" : 157,
      "context" : "Correspondingly, a robot took on only physical responsibilities such as grasping/transferring the fragile/heavy objects [168][169].",
      "startOffset" : 120,
      "endOffset" : 125
    }, {
      "referenceID" : 158,
      "context" : "Correspondingly, a robot took on only physical responsibilities such as grasping/transferring the fragile/heavy objects [168][169].",
      "startOffset" : 125,
      "endOffset" : 130
    }, {
      "referenceID" : 159,
      "context" : "robot physical capability by providing robot with physical assistances such as grasping [170] and fetching [171]; robot executed tasks such as heavy object moving and elderly navigation in unstructured outdoor environments, in which a",
      "startOffset" : 88,
      "endOffset" : 93
    }, {
      "referenceID" : 160,
      "context" : "robot physical capability by providing robot with physical assistances such as grasping [170] and fetching [171]; robot executed tasks such as heavy object moving and elderly navigation in unstructured outdoor environments, in which a",
      "startOffset" : 107,
      "endOffset" : 112
    }, {
      "referenceID" : 21,
      "context" : "human analyzed and conquered the environment limitations such as objects/space availability [22][172][173][174].",
      "startOffset" : 92,
      "endOffset" : 96
    }, {
      "referenceID" : 161,
      "context" : "human analyzed and conquered the environment limitations such as objects/space availability [22][172][173][174].",
      "startOffset" : 96,
      "endOffset" : 101
    }, {
      "referenceID" : 162,
      "context" : "human analyzed and conquered the environment limitations such as objects/space availability [22][172][173][174].",
      "startOffset" : 101,
      "endOffset" : 106
    }, {
      "referenceID" : 163,
      "context" : "human analyzed and conquered the environment limitations such as objects/space availability [22][172][173][174].",
      "startOffset" : 106,
      "endOffset" : 111
    }, {
      "referenceID" : 149,
      "context" : "A robot was expected to infer the human’s ongoing activities, detect human needs timely and proactively provide the appropriate help such as “a toy part” [160].",
      "startOffset" : 154,
      "endOffset" : 159
    }, {
      "referenceID" : 150,
      "context" : "If abnormal executions or execution failures occurred, the human provided the timely verbal corrections such as “stop, grasp the top” or physical assistances such as “delivering the robot-needed object” [161].",
      "startOffset" : 203,
      "endOffset" : 208
    }, {
      "referenceID" : 164,
      "context" : "While these robot capabilities are still immature [175].",
      "startOffset" : 50,
      "endOffset" : 55
    }, {
      "referenceID" : 156,
      "context" : "Typical references [167][168][169] [172][173][174]",
      "startOffset" : 19,
      "endOffset" : 24
    }, {
      "referenceID" : 157,
      "context" : "Typical references [167][168][169] [172][173][174]",
      "startOffset" : 24,
      "endOffset" : 29
    }, {
      "referenceID" : 158,
      "context" : "Typical references [167][168][169] [172][173][174]",
      "startOffset" : 29,
      "endOffset" : 34
    }, {
      "referenceID" : 161,
      "context" : "Typical references [167][168][169] [172][173][174]",
      "startOffset" : 35,
      "endOffset" : 40
    }, {
      "referenceID" : 162,
      "context" : "Typical references [167][168][169] [172][173][174]",
      "startOffset" : 40,
      "endOffset" : 45
    }, {
      "referenceID" : 163,
      "context" : "Typical references [167][168][169] [172][173][174]",
      "startOffset" : 45,
      "endOffset" : 50
    }, {
      "referenceID" : 169,
      "context" : "systems, typical applications using NL-based social communication include: a receptionist robot increased its social acceptance in conference arrangements by using social dialogs with pleasant prosodic contours [180]; cooperative",
      "startOffset" : 211,
      "endOffset" : 216
    }, {
      "referenceID" : 170,
      "context" : "machine operations used social descriptions considering human action preferences [181][182]; health-caregiving robots searched and delivered objects by considering user speech confidences, user safety and user roles such as",
      "startOffset" : 81,
      "endOffset" : 86
    }, {
      "referenceID" : 171,
      "context" : "machine operations used social descriptions considering human action preferences [181][182]; health-caregiving robots searched and delivered objects by considering user speech confidences, user safety and user roles such as",
      "startOffset" : 86,
      "endOffset" : 91
    }, {
      "referenceID" : 172,
      "context" : "“primary user, bystander” [183]; adapted unfamiliar users by using NL expressions with fuzzy emotion statuses such",
      "startOffset" : 26,
      "endOffset" : 31
    }, {
      "referenceID" : 173,
      "context" : "as “fuzzy happiness/sadness/anger” [184]; modeled social NL communications in NLC by defining human-robot relations such as “love”, “friendship” and “marriage” [185]; designed robotic companion by using friendly NL",
      "startOffset" : 35,
      "endOffset" : 40
    }, {
      "referenceID" : 174,
      "context" : "as “fuzzy happiness/sadness/anger” [184]; modeled social NL communications in NLC by defining human-robot relations such as “love”, “friendship” and “marriage” [185]; designed robotic companion by using friendly NL",
      "startOffset" : 160,
      "endOffset" : 165
    }, {
      "referenceID" : 65,
      "context" : "conversation [67][186][187] etc.",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 175,
      "context" : "conversation [67][186][187] etc.",
      "startOffset" : 17,
      "endOffset" : 22
    }, {
      "referenceID" : 176,
      "context" : "conversation [67][186][187] etc.",
      "startOffset" : 22,
      "endOffset" : 27
    }, {
      "referenceID" : 177,
      "context" : "submitted to Knowledge-based Systems, January 2017 typical applications using NL-based social executions include: a navigation robot autonomously modified its motion behaviors (stop, slower, faster) by considering human density (crowded, dense) with the reminding of human NL instructions (“go ahead to move”, “stop”) [188]; a companion robot moved its head towards the human speaker according to human’s NL tunes [189]; a storytelling robot depictd stories by mapping NL expressions with human’s",
      "startOffset" : 318,
      "endOffset" : 323
    }, {
      "referenceID" : 178,
      "context" : "submitted to Knowledge-based Systems, January 2017 typical applications using NL-based social executions include: a navigation robot autonomously modified its motion behaviors (stop, slower, faster) by considering human density (crowded, dense) with the reminding of human NL instructions (“go ahead to move”, “stop”) [188]; a companion robot moved its head towards the human speaker according to human’s NL tunes [189]; a storytelling robot depictd stories by mapping NL expressions with human’s",
      "startOffset" : 414,
      "endOffset" : 419
    }, {
      "referenceID" : 167,
      "context" : "body motion behaviors to catch humans’ attention [178][190].",
      "startOffset" : 49,
      "endOffset" : 54
    }, {
      "referenceID" : 179,
      "context" : "body motion behaviors to catch humans’ attention [178][190].",
      "startOffset" : 54,
      "endOffset" : 59
    }, {
      "referenceID" : 165,
      "context" : "A robot learned to nicely response a human’s request such as “drawing a picture on the paper” [176] and “stop until I touch you” [177].",
      "startOffset" : 94,
      "endOffset" : 99
    }, {
      "referenceID" : 166,
      "context" : "A robot learned to nicely response a human’s request such as “drawing a picture on the paper” [176] and “stop until I touch you” [177].",
      "startOffset" : 129,
      "endOffset" : 134
    }, {
      "referenceID" : 167,
      "context" : "A robot learned to do appropriate body languages during its speaking such as storytelling [178][179].",
      "startOffset" : 90,
      "endOffset" : 95
    }, {
      "referenceID" : 168,
      "context" : "A robot learned to do appropriate body languages during its speaking such as storytelling [178][179].",
      "startOffset" : 95,
      "endOffset" : 100
    }, {
      "referenceID" : 170,
      "context" : "Typical references [181][182][183] [188][189][190]",
      "startOffset" : 19,
      "endOffset" : 24
    }, {
      "referenceID" : 171,
      "context" : "Typical references [181][182][183] [188][189][190]",
      "startOffset" : 24,
      "endOffset" : 29
    }, {
      "referenceID" : 172,
      "context" : "Typical references [181][182][183] [188][189][190]",
      "startOffset" : 29,
      "endOffset" : 34
    }, {
      "referenceID" : 177,
      "context" : "Typical references [181][182][183] [188][189][190]",
      "startOffset" : 35,
      "endOffset" : 40
    }, {
      "referenceID" : 178,
      "context" : "Typical references [181][182][183] [188][189][190]",
      "startOffset" : 40,
      "endOffset" : 45
    }, {
      "referenceID" : 179,
      "context" : "Typical references [181][182][183] [188][189][190]",
      "startOffset" : 45,
      "endOffset" : 50
    }, {
      "referenceID" : 4,
      "context" : "With the NL understanding research, human verbal instructions were processed and the task-related knowledge, such as task goal, execution steps, and the execution parameters “speed, tools, locations, human requirements” were extracted to perform a comprehensive semantic analysis [5][172][192] .",
      "startOffset" : 280,
      "endOffset" : 283
    }, {
      "referenceID" : 161,
      "context" : "With the NL understanding research, human verbal instructions were processed and the task-related knowledge, such as task goal, execution steps, and the execution parameters “speed, tools, locations, human requirements” were extracted to perform a comprehensive semantic analysis [5][172][192] .",
      "startOffset" : 283,
      "endOffset" : 288
    }, {
      "referenceID" : 181,
      "context" : "With the NL understanding research, human verbal instructions were processed and the task-related knowledge, such as task goal, execution steps, and the execution parameters “speed, tools, locations, human requirements” were extracted to perform a comprehensive semantic analysis [5][172][192] .",
      "startOffset" : 288,
      "endOffset" : 293
    }, {
      "referenceID" : 89,
      "context" : "With the knowledge representation research, task knowledge was constructed by algorithms such as Bayesian Network [91][193], Support Vector Machine (SVM) [194][195], and Hidden Markov Model (HMM) [118][196].",
      "startOffset" : 114,
      "endOffset" : 118
    }, {
      "referenceID" : 182,
      "context" : "With the knowledge representation research, task knowledge was constructed by algorithms such as Bayesian Network [91][193], Support Vector Machine (SVM) [194][195], and Hidden Markov Model (HMM) [118][196].",
      "startOffset" : 118,
      "endOffset" : 123
    }, {
      "referenceID" : 183,
      "context" : "With the knowledge representation research, task knowledge was constructed by algorithms such as Bayesian Network [91][193], Support Vector Machine (SVM) [194][195], and Hidden Markov Model (HMM) [118][196].",
      "startOffset" : 154,
      "endOffset" : 159
    }, {
      "referenceID" : 184,
      "context" : "With the knowledge representation research, task knowledge was constructed by algorithms such as Bayesian Network [91][193], Support Vector Machine (SVM) [194][195], and Hidden Markov Model (HMM) [118][196].",
      "startOffset" : 159,
      "endOffset" : 164
    }, {
      "referenceID" : 108,
      "context" : "With the knowledge representation research, task knowledge was constructed by algorithms such as Bayesian Network [91][193], Support Vector Machine (SVM) [194][195], and Hidden Markov Model (HMM) [118][196].",
      "startOffset" : 196,
      "endOffset" : 201
    }, {
      "referenceID" : 185,
      "context" : "With the knowledge representation research, task knowledge was constructed by algorithms such as Bayesian Network [91][193], Support Vector Machine (SVM) [194][195], and Hidden Markov Model (HMM) [118][196].",
      "startOffset" : 201,
      "endOffset" : 206
    }, {
      "referenceID" : 186,
      "context" : "temporal/spatial/visual/physical features [197][198] .",
      "startOffset" : 42,
      "endOffset" : 47
    }, {
      "referenceID" : 187,
      "context" : "temporal/spatial/visual/physical features [197][198] .",
      "startOffset" : 47,
      "endOffset" : 52
    }, {
      "referenceID" : 56,
      "context" : "knowledge, which was needed in real-world situations but ignored in theoretical knowledge representations, and the inconsistent knowledge, which was instructed by a human but was not available in the real world [58][199] [200].",
      "startOffset" : 211,
      "endOffset" : 215
    }, {
      "referenceID" : 188,
      "context" : "knowledge, which was needed in real-world situations but ignored in theoretical knowledge representations, and the inconsistent knowledge, which was instructed by a human but was not available in the real world [58][199] [200].",
      "startOffset" : 215,
      "endOffset" : 220
    }, {
      "referenceID" : 189,
      "context" : "knowledge, which was needed in real-world situations but ignored in theoretical knowledge representations, and the inconsistent knowledge, which was instructed by a human but was not available in the real world [58][199] [200].",
      "startOffset" : 221,
      "endOffset" : 226
    }, {
      "referenceID" : 180,
      "context" : "The input for a NLC method is the human NL request, the output is the HRC [191].",
      "startOffset" : 74,
      "endOffset" : 79
    }, {
      "referenceID" : 190,
      "context" : "submitted to Knowledge-based Systems, January 2017 meanings such as “cats and dogs are animals” in data preparation stages [201], sequence-to-sequence language",
      "startOffset" : 123,
      "endOffset" : 128
    }, {
      "referenceID" : 191,
      "context" : "understanding/translation by sequentially outputting the meaning-modeling results based on the previous semantic context [202], attention-based NL understanding in which the relatively important words/expressions are valued by",
      "startOffset" : 121,
      "endOffset" : 126
    }, {
      "referenceID" : 192,
      "context" : "increasing the weights of the important expressions and decreasing the weights of the unimportant expressions [203], unsupervised long-term meaning modeling [204] etc.",
      "startOffset" : 110,
      "endOffset" : 115
    }, {
      "referenceID" : 193,
      "context" : "increasing the weights of the important expressions and decreasing the weights of the unimportant expressions [203], unsupervised long-term meaning modeling [204] etc.",
      "startOffset" : 157,
      "endOffset" : 162
    }, {
      "referenceID" : 31,
      "context" : "In the generalization trend, a robot is endowed with broad commonsense knowledge to support the general NLC under various situations [33][94][205].",
      "startOffset" : 133,
      "endOffset" : 137
    }, {
      "referenceID" : 92,
      "context" : "In the generalization trend, a robot is endowed with broad commonsense knowledge to support the general NLC under various situations [33][94][205].",
      "startOffset" : 137,
      "endOffset" : 141
    }, {
      "referenceID" : 194,
      "context" : "In the generalization trend, a robot is endowed with broad commonsense knowledge to support the general NLC under various situations [33][94][205].",
      "startOffset" : 141,
      "endOffset" : 146
    }, {
      "referenceID" : 195,
      "context" : "In the specification trend, a robot is endowed with delicate knowledge to support the specific types of HRC [206][207].",
      "startOffset" : 108,
      "endOffset" : 113
    }, {
      "referenceID" : 196,
      "context" : "In the specification trend, a robot is endowed with delicate knowledge to support the specific types of HRC [206][207].",
      "startOffset" : 113,
      "endOffset" : 118
    }, {
      "referenceID" : 197,
      "context" : "Caused by the unique-knowledge ignorance, general NLC methods are relatively simple, being challenged by cold-start phenomenon, which refers the knowledge not being learned in the training stage and causing execution failures in the implementation stage [208].",
      "startOffset" : 254,
      "endOffset" : 259
    }, {
      "referenceID" : 198,
      "context" : "conflictions between being specific and being general [209].",
      "startOffset" : 54,
      "endOffset" : 59
    }, {
      "referenceID" : 194,
      "context" : "query humans for new knowledge, and the big-data-based method, which is an automatic and low-cost information retrieval method that extracts knowledge from information sources such as the World Wide Web [205], books [210],",
      "startOffset" : 203,
      "endOffset" : 208
    }, {
      "referenceID" : 199,
      "context" : "query humans for new knowledge, and the big-data-based method, which is an automatic and low-cost information retrieval method that extracts knowledge from information sources such as the World Wide Web [205], books [210],",
      "startOffset" : 216,
      "endOffset" : 221
    }, {
      "referenceID" : 200,
      "context" : "machine operation log files [211], and videos [212].",
      "startOffset" : 28,
      "endOffset" : 33
    }, {
      "referenceID" : 201,
      "context" : "machine operation log files [211], and videos [212].",
      "startOffset" : 46,
      "endOffset" : 51
    }, {
      "referenceID" : 202,
      "context" : "To increase robot adaptability, new research was launched to model the human cognition process [213][214], which aims to explore humans’ decision-making mechanism for modeling robot execution priority and flexibility.",
      "startOffset" : 95,
      "endOffset" : 100
    }, {
      "referenceID" : 203,
      "context" : "To increase robot adaptability, new research was launched to model the human cognition process [213][214], which aims to explore humans’ decision-making mechanism for modeling robot execution priority and flexibility.",
      "startOffset" : 100,
      "endOffset" : 105
    }, {
      "referenceID" : 14,
      "context" : "“brush”, but instead means a practical purpose “cleaning the surface” [15].",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 204,
      "context" : "Current methods focus on exploring object affordances (object-action correlation) [215], and lacking the in-depth interpretations of task cooperation.",
      "startOffset" : 82,
      "endOffset" : 87
    }, {
      "referenceID" : 205,
      "context" : "The learning-from-failure mechanism has been implemented in computer science for algorithm efficiency improvement [216] and in material science for new material discovery [217].",
      "startOffset" : 114,
      "endOffset" : 119
    }, {
      "referenceID" : 38,
      "context" : "manner [40], in which the failure is analyzed by comparing the available knowledge with the defined knowledge, lacking the analysis of failure causes and recovery mechanism.",
      "startOffset" : 7,
      "endOffset" : 11
    } ],
    "year" : 2017,
    "abstractText" : "Natural-language-facilitated human-robot cooperation (NLC), in which natural language (NL) is used to share knowledge between a human and a robot for conducting intuitive human-robot cooperation (HRC), is continuously developing in the recent decade. Currently, NLC is used in several robotic domains such as manufacturing, daily assistance and health caregiving. It is necessary to summarize current NLC-based robotic systems and discuss the future developing trends, providing helpful information for future NLC research. In this review, we first analyzed the driving forces behind the NLC research. Regarding to a robot’s cognition level during the cooperation, the NLC implementations then were categorized into four types “NL-based control, NL-based robot training, NL-based task execution, NL-based social companion” for comparison and discussion. Last based on our perspective and comprehensive paper review, the future research trends were discussed.",
    "creator" : "Microsoft® Word 2013"
  }
}