{
  "name" : "1702.05962.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Latent Variable Dialogue Models and their Diversity",
    "authors" : [ "Kris Cao", "Stephen Clark" ],
    "emails" : [ "sc609}@cam.ac.uk" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "The task of open-domain dialogue generation is an area of active development, with neural sequenceto-sequence models dominating the recently published literature (Shang et al., 2015; Vinyals and Le, 2015; Li et al., 2016b,a; Serban et al., 2016). Most previously published models train to minimise the negative log-likelihood of the training data, and then at generation time either perform beam search to find the output Y which maximises P (Y |input) (Shang et al., 2015; Vinyals and Le, 2015; Serban et al., 2016) (ML decoding), or sample from the resulting distribution (Serban et al., 2016).\nA notorious issue with ML decoding is that this tends to generate short, boring responses to a wide range of inputs, such as “I don’t know”. These responses are common in the training data, and can be replies to a wide range of inputs (Li et al., 2016a; Serban et al., 2016). In addition, shorter responses typically have higher likelihoods, and so wide beam sizes often result in very short responses (Tu et al., 2017; Belz, 2007). To resolve this problem, Li et al. (2016a) propose instead using maximum mutual information with a length boost as a decoding objective, and report more interesting generated responses.\nFurther, natural dialogue is not deterministic; for example, the replies to “What’s your name and where do you come from?” will vary from person to person. Li et al. (2016b) have proposed learning representations of personas to account for interperson variation, but there can be variation even among a single person’s responses to certain questions.\nRecently, Serban et al. (2017) have introduced latent variables to the dialogue modelling framework, to model the underlying distribution over possible responses directly. These models have the benefit that, at generation time, we can sample a response from the distribution by first sampling an assignment of the latent variables, and then decoding deterministically. In this way, we introduce stochasticity without resorting to sampling from the decoder, which can lead to incoherent output – see Table 1 for examples.\nIn this paper, we present a latent variable model for one-shot dialogue response, and investigate what kinds of diversity the latent variables capture. Our experiments show that our model has higher lexical as well as sentential diversity than baseline models. We also show that our model generates more acceptable diverse output than sampling from a deterministic decoder. We end by noting that all three methods proposed above to combat the ‘maximum likelihood response’ are ways of decreasing the probability of the generated output, and report some preliminary results for how response probability interacts with grammaticality and interestingness."
    }, {
      "heading" : "2 A Latent Variable Dialogue Model",
      "text" : ""
    }, {
      "heading" : "2.1 Model Description",
      "text" : "Our task is to model the true probability of a response Y given an input X . We denote our model distribution by P (Y |X). We introduce a latent\nar X\niv :1\n70 2.\n05 96\n2v 1\n[ cs\n.C L\n] 2\n0 Fe\nb 20\n17\nvariable z with a standard Gaussian prior – i.e. P (z) = N (0, In) – and factor P (Y |X) as:\nP (Y |X) = ∫ z P (Y |z,X)P (z)dz (1)\nTo motivate this model, we point out that existing encoder-decoder models encode an input X as a single fixed representation. Hence, all of the possible replies toX must be stored within the decoder’s probability distribution P (Y |X), and during decoding it is hard to disentangle these possible replies.\nHowever, our model contains a stochastic component z in the decoder P (Y |z,X), and so by sampling different z and then performing ML decoding on P (Y |z,X), we hope to tease apart the replies stored in the probability distribution P (Y |X), without resorting to sampling from the decoder. This has the benefit that we use the decoder at generation time in a similar way to how we train it, making it more likely that the output of our model is grammatical and coherent. Further, as we do not marginalize out z when decoding, we no longer perform exact maximum likelihood search for a reply Y , and so we hope to avoid the boring reply problem.\nAt training time, we follow the variational autoencoder framework (Kingma and Welling, 2014; Kingma et al., 2014; Sohn et al., 2015; Miao et al., 2016) , and approximate the posterior P (z|X,Y ) with a proposal distribution Q(z|X,Y ), which in our case is a diagonal Gaussian whose parameters depend on X and Y . We thus have the following evidence lower bound (ELBO) for the loglikelihood of the data:\nlogP (Y |X) ≥ −KL(Q(z|X,Y )||P (z)) + Ez∼Q logP (Y |z,X) (2)\nNote that this loss decomposes into two parts: the KL divergence between the approximate pos-\nterior and the prior, and the cross-entropy loss between the model distribution and the data distribution. If the model can encode useful information into z, then the KL divergence term will be non-zero (Bowman et al., 2016). As our model decoder is given a deterministic representation of X already, z will then encode information about the variation in replies to X ."
    }, {
      "heading" : "2.2 Model Implementation",
      "text" : "Given an input sentence X and a response Y , we run two separate bidirectional RNNs over their word embeddings xi and yi. We concatenate the final states of each and pass them through a single nonlinear layer to obtain our representations hx and hy of X and Y . We use GRUs (Cho et al., 2014) as our RNN cell as a compromise between expressive power and computational cost.\nWe calculate the mean and variance of Q as:\nµ = Wµ[hx hy] + bµ\nlog(Σ) = diag(WΣ[hx hy] + bΣ) (3)\nwhere [a b] denotes the concatenation of a and b, and diag denotes inserting along the diagonal of a matrix.\nWe take a single sample z from Q using the reparametrization trick (Kingma and Welling, 2014), concatenate hx and z, and initialize the hidden state of the decoder GRU with [hx z]. We then train the decoder GRU to minimize the negative log-likelihood of the response Y .\nWhile training this model, we noted the same difficulties as Bowman et al. (2016) – as RNNs are powerful density estimators, the model will prefer to ignore the latent variables and instead optimize the data reconstruction term of the ELBO, while forcing the KL term to 0. We overcome this using similar techniques by gradually annealing the KL term weight over the course of model training and using word dropout in the decoder with a drop rate of 0.5."
    }, {
      "heading" : "3 Experiments",
      "text" : "We compare our model, DIAL-LV, to three baselines. The first is an encoder-decoder dialogue model with ML decoding (DIAL-MLE). The second baseline model implements the anti-LM decoder of Li et al. (2016a) (DIAL-MMI) on top of the encoder-decoder, with no length normalization. For these models, we use beam search with a width of 2 to find the sentence Y which maximises the decoding objective (either ML or MMI).\nThe final baseline uses the encoder-decoder model, but instead samples from the decoder to find Y (DIAL-SAMP). We found that naively sampling from the decoder resulted in meaningless jumbles of words. To solve this, we introduced a temperature parameter τ ∈ (0, 1], which scales the probability of each word of the decoder as pw 7→ p1/τw . This parameter serves to sharpen the word distribution of the decoder. We found τ = 0.35 to be a reasonable balance between preserving stochasticity while also improving the coherence of the generated output.\nWe used the OpenSubtitles dataset of movie subtitles to train our models (Tiedemann, 2012). We took a random sample of 100K files from the full dataset to train our models on, and then pruned this of repeated files to leave roughly 95K files and capped sentence length to 50. The total size of the resulting corpus was around 731M tokens. Please see the supplementary material for model hyperparameters and training details.\nAs seeds for our replies, we used a list of 50 prompts: 150 lines from the OpenSubtitles dataset outside of our training set which we judged to make sense as independent sentences and 50 questions chosen from a list of suggested conversation starters1."
    }, {
      "heading" : "3.1 Reply statistics",
      "text" : "Previous work (e.g. Li et al. (2016a)) used typetoken ratio (TTR) to measure the diversity of the generated output. However, as language follows a Zipf distribution, TTR is affected by the length of the generated replies (Mitchell, 2015). Hence, we use the estimated parameter of a Zipf distribution fitted to our replies as a proxy for the lexical diversity of generated output, with more diverse output having smaller scores. As ML decoding is known to give the same few replies repeatedly, we also report the percentage of unique replies, as a coarser measure of sentential diversity compared to lexical diversity. Further, we give the negative loglikelihood (NLL) as predicted by the deterministic encoder-decoder model, to see what regions of the probability space the replies occupy. We present these statistics in Table 2.\nWe note that DIAL-LV generates more diverse replies than the other deterministic models, measured in terms of percentage of unique responses. Interestingly, the lexical diversity of DIAL-LV is almost identical to DIAL-MLE, suggesting that the latent variables help DIAL-LV avoid the boring output problem and generate more diverse outputs. We note that DIAL-LV even rivals DIALSAMP in terms of sentential diversity, and beats DIAL-SAMP in terms of lexical diversity. This could be because DIAL-SAMP chooses words greedily, and so is biased towards choosing highprobability words at each timestep. This suggests that maintaining a beam of hypotheses while sampling could help sampling-based methods escape\n1Obtained from http://conversationstartersworld.com/250conversation-starters/\nthe trap of having to make near-greedy local decisions."
    }, {
      "heading" : "3.2 Human acceptability judgments",
      "text" : "We also tested whether DIAL-LV could generate a greater number of acceptable replies to a prompt than DIAL-SAMP. We randomly selected 50 prompts from our list of 200, and generated 5 replies at random to each one using both models. We then asked human annotators2 to judge how many replies were appropriate replies, taking into account grammaticality, coherence and relevance. The results are shown in Table 3.\nInterestingly, even though DIAL-LV has a lower NLL score, both models generate roughly the same number of acceptable replies. DIAL-LV also has less variance in the number of acceptable replies, suggesting that the outputs it generates are more consistent than responses from DIAL-SAMP. Finally, we note that DIAL-LV generates more diverse output than DIAL-SAMP in this scenario, even thought its replies are judged equally acceptable, suggesting that it is managing to produce a wide range of coherent, fluent and appropriate output."
    }, {
      "heading" : "3.3 Sampling from the latent variable space",
      "text" : "We next explored the effect of sampling from different regions of the latent space. For each prompt in the test set, we took 5 uniform samples from shells of radius 0 (which collapses to determinis-\n2We used 50 in total, 25 for each model\ntic decoding), 4, 8, 12 and 16 in the latent space3 by sampling from P (z) = N (0, I) and then scaling the sample z by the appropriate amount. We then generated a response to the prompt using each value of z, and measured some statistics of the replies. The results are shown in Table 4.\nAs expected, samples with small radius show less diversity in terms of unique outputs. Further, we see a consistent trend that samples with greater radius have a higher NLL score, showing the influence of the prior in Eqn. 1. However, at the highest radius, we observe the highest NLLs, but also the lowest lexical diversities, suggesting that it manages to combine the words it produces in many different ways."
    }, {
      "heading" : "4 Discussion",
      "text" : "Taken together, our experiments show that ML decoding does not seem to be the best objective for generating diverse dialogue, and so corroborates the inadequacy of perplexity as an evaluation metric for dialogue models (Liu et al., 2016). Indeed, all three models which show a diversity gain over the vanilla encoder-decoder with MLE decoding try to instead sample responses from a lowerprobability region of the response space. However, if the response probability is too low, it runs the risk of being nonsensical. Hence, there appears to be a ‘Goldilocks’ region of the probability space, where the responses are interesting and coherent. Finding ways of concentrating model samples to this region is thus a potentially promising area of research for open-domain dialogue agents.\nWe also note that our proposed model can be combined with MMI decoding or temperaturebased sampling to get the benefits of both worlds. While we did not do this in our experiments in order to isolate the impact of our model, doing so improves the diversity of our generated output even more."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this paper, we present a latent variable model to generate responses to input utterances. We investigate the diversity of output generated from this model, and show that it improves both lexical and sentential diversity. It also generates more consistently acceptable output as judged by humans compared to sampling from a decoder.\n3For a d-dim standard Gaussian, E(‖X‖) ≈ √ d, and\nV ar(‖X‖)→ 0 as d→∞. Here d = 64."
    }, {
      "heading" : "Acknowledgements",
      "text" : "KC is supported by an EPSRC doctoral award. SC is supported by ERC Starting Grant DisCoTex (306920) and ERC Proof of Concept Grant GroundForce (693579). The authors would like to thank everyone who helped prototype the human evaluation experiments. The authors would also like to thank the anonymous reviewers for all their insightful comments."
    }, {
      "heading" : "A Model training information",
      "text" : "We implemented all of our models using Keras (Chollet, 2015) running on Theano (Theano Development Team, 2016). As vocabulary, we took all words appearing at least 1000 times in the whole corpus. As this amounted to ∼30K words, we used a 2-level hierarchical approximation to the full softmax to speed up model training (Morin and Bengio, 2005), with random clustering. We trained all our models for 3 epochs using the\nAdadelta optimizer (Zeiler, 2012), with default values for the optimizer parameters.\nWe used 512 dimensional word embeddings and encoder hidden state sizes across all of our models. We used 64 latent dimensional latent variables, and so the decoder RNN for the DIAL-LV model had hidden state size 576. The decoder RNN for the DIAL-MLE model also had hidden state size 576, to keep the capacity of the decoder comparable across the two models. We used tanh non-linearities throughout our model. For training the vanilla encoder-decoder, we also used word dropout on the decoder input with a drop rate of 0.5 to prevent overfitting. Each epoch took roughly 4 days on a Titan Black.\nFor the MMI decoding, we used a LM penalty weight of 0.45 and applied this for the first 6 words."
    } ],
    "references" : [ {
      "title" : "Probabilistic generation of weather forecast texts. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics",
      "author" : [ "Anja Belz" ],
      "venue" : null,
      "citeRegEx" : "Belz.,? \\Q2007\\E",
      "shortCiteRegEx" : "Belz.",
      "year" : 2007
    }, {
      "title" : "Autoencoding variational Bayes",
      "author" : [ "Diederik P. Kingma", "Max Welling" ],
      "venue" : "ICLR,",
      "citeRegEx" : "Kingma and Welling.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kingma and Welling.",
      "year" : 2014
    }, {
      "title" : "Semisupervised learning with deep generative models",
      "author" : [ "Diederik P. Kingma", "Danilo Jimenez Rezende", "Shakir Mohamed", "Max Welling" ],
      "venue" : null,
      "citeRegEx" : "Kingma et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kingma et al\\.",
      "year" : 2014
    }, {
      "title" : "A diversity-promoting objective function for neural conversation models",
      "author" : [ "Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan" ],
      "venue" : "In Proceedings of the 2016 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Li et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2016
    }, {
      "title" : "Neural variational inference for text processing",
      "author" : [ "Yishu Miao", "Lei Yu", "Phil Blunsom" ],
      "venue" : null,
      "citeRegEx" : "Miao et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Miao et al\\.",
      "year" : 2016
    }, {
      "title" : "Type-token models: a comparative study",
      "author" : [ "David Mitchell" ],
      "venue" : "Journal of Quantitative Linguistics,",
      "citeRegEx" : "Mitchell.,? \\Q2015\\E",
      "shortCiteRegEx" : "Mitchell.",
      "year" : 2015
    }, {
      "title" : "Hierarchical probabilistic neural network language model",
      "author" : [ "Frederic Morin", "Yoshua Bengio" ],
      "venue" : "Tenth International Workshop on Artificial Intelligence and Statistics,",
      "citeRegEx" : "Morin and Bengio.,? \\Q2005\\E",
      "shortCiteRegEx" : "Morin and Bengio.",
      "year" : 2005
    }, {
      "title" : "Building end-to-end dialogue systems using generative hierarchical neural network models",
      "author" : [ "Iulian V. Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau" ],
      "venue" : null,
      "citeRegEx" : "Serban et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Serban et al\\.",
      "year" : 2016
    }, {
      "title" : "A hierarchical latent variable encoder-decoder model for generating dialogues",
      "author" : [ "Iulian Vlad Serban", "Alessandro Sordoni", "Ryan Lowe", "Laurent Charlin", "Joelle Pineau", "Aaron Courville", "Yoshua Bengio" ],
      "venue" : "In AAAI,",
      "citeRegEx" : "Serban et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Serban et al\\.",
      "year" : 2017
    }, {
      "title" : "Learning structured output representation using deep conditional generative models",
      "author" : [ "Kihyuk Sohn", "Honglak Lee", "Xinchen Yan" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Sohn et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Sohn et al\\.",
      "year" : 2015
    }, {
      "title" : "Neural machine translation with reconstruction",
      "author" : [ "Zhaopeng Tu", "Yang Liu", "Lifeng Shang", "Xiaohua Liu" ],
      "venue" : "In AAAI,",
      "citeRegEx" : "Tu et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Tu et al\\.",
      "year" : 2017
    }, {
      "title" : "A neural conversation model",
      "author" : [ "Orial Vinyals", "Quoc V. Le" ],
      "venue" : null,
      "citeRegEx" : "Vinyals and Le.,? \\Q2015\\E",
      "shortCiteRegEx" : "Vinyals and Le.",
      "year" : 2015
    }, {
      "title" : "ADADELTA: an adaptive learning rate method",
      "author" : [ "Matthew D. Zeiler" ],
      "venue" : "CoRR, abs/1212.5701,",
      "citeRegEx" : "Zeiler.,? \\Q2012\\E",
      "shortCiteRegEx" : "Zeiler.",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 11,
      "context" : "The task of open-domain dialogue generation is an area of active development, with neural sequenceto-sequence models dominating the recently published literature (Shang et al., 2015; Vinyals and Le, 2015; Li et al., 2016b,a; Serban et al., 2016).",
      "startOffset" : 162,
      "endOffset" : 245
    }, {
      "referenceID" : 7,
      "context" : "The task of open-domain dialogue generation is an area of active development, with neural sequenceto-sequence models dominating the recently published literature (Shang et al., 2015; Vinyals and Le, 2015; Li et al., 2016b,a; Serban et al., 2016).",
      "startOffset" : 162,
      "endOffset" : 245
    }, {
      "referenceID" : 11,
      "context" : "Most previously published models train to minimise the negative log-likelihood of the training data, and then at generation time either perform beam search to find the output Y which maximises P (Y |input) (Shang et al., 2015; Vinyals and Le, 2015; Serban et al., 2016) (ML decoding), or sample from the resulting distribution (Serban et al.",
      "startOffset" : 206,
      "endOffset" : 269
    }, {
      "referenceID" : 7,
      "context" : "Most previously published models train to minimise the negative log-likelihood of the training data, and then at generation time either perform beam search to find the output Y which maximises P (Y |input) (Shang et al., 2015; Vinyals and Le, 2015; Serban et al., 2016) (ML decoding), or sample from the resulting distribution (Serban et al.",
      "startOffset" : 206,
      "endOffset" : 269
    }, {
      "referenceID" : 7,
      "context" : ", 2016) (ML decoding), or sample from the resulting distribution (Serban et al., 2016).",
      "startOffset" : 65,
      "endOffset" : 86
    }, {
      "referenceID" : 7,
      "context" : "These responses are common in the training data, and can be replies to a wide range of inputs (Li et al., 2016a; Serban et al., 2016).",
      "startOffset" : 94,
      "endOffset" : 133
    }, {
      "referenceID" : 10,
      "context" : "In addition, shorter responses typically have higher likelihoods, and so wide beam sizes often result in very short responses (Tu et al., 2017; Belz, 2007).",
      "startOffset" : 126,
      "endOffset" : 155
    }, {
      "referenceID" : 0,
      "context" : "In addition, shorter responses typically have higher likelihoods, and so wide beam sizes often result in very short responses (Tu et al., 2017; Belz, 2007).",
      "startOffset" : 126,
      "endOffset" : 155
    }, {
      "referenceID" : 0,
      "context" : ", 2017; Belz, 2007). To resolve this problem, Li et al. (2016a) propose instead using maximum mutual information with a length boost as a decoding objective, and report more interesting generated responses.",
      "startOffset" : 8,
      "endOffset" : 64
    }, {
      "referenceID" : 0,
      "context" : ", 2017; Belz, 2007). To resolve this problem, Li et al. (2016a) propose instead using maximum mutual information with a length boost as a decoding objective, and report more interesting generated responses. Further, natural dialogue is not deterministic; for example, the replies to “What’s your name and where do you come from?” will vary from person to person. Li et al. (2016b) have proposed learning representations of personas to account for interperson variation, but there can be variation even among a single person’s responses to certain questions.",
      "startOffset" : 8,
      "endOffset" : 381
    }, {
      "referenceID" : 7,
      "context" : "Recently, Serban et al. (2017) have introduced latent variables to the dialogue modelling framework, to model the underlying distribution over possible responses directly.",
      "startOffset" : 10,
      "endOffset" : 31
    }, {
      "referenceID" : 1,
      "context" : "At training time, we follow the variational autoencoder framework (Kingma and Welling, 2014; Kingma et al., 2014; Sohn et al., 2015; Miao et al., 2016) , and approximate the posterior P (z|X,Y ) with a proposal distribution Q(z|X,Y ), which in our case is a diagonal Gaussian whose parameters depend on X and Y .",
      "startOffset" : 66,
      "endOffset" : 151
    }, {
      "referenceID" : 2,
      "context" : "At training time, we follow the variational autoencoder framework (Kingma and Welling, 2014; Kingma et al., 2014; Sohn et al., 2015; Miao et al., 2016) , and approximate the posterior P (z|X,Y ) with a proposal distribution Q(z|X,Y ), which in our case is a diagonal Gaussian whose parameters depend on X and Y .",
      "startOffset" : 66,
      "endOffset" : 151
    }, {
      "referenceID" : 9,
      "context" : "At training time, we follow the variational autoencoder framework (Kingma and Welling, 2014; Kingma et al., 2014; Sohn et al., 2015; Miao et al., 2016) , and approximate the posterior P (z|X,Y ) with a proposal distribution Q(z|X,Y ), which in our case is a diagonal Gaussian whose parameters depend on X and Y .",
      "startOffset" : 66,
      "endOffset" : 151
    }, {
      "referenceID" : 4,
      "context" : "At training time, we follow the variational autoencoder framework (Kingma and Welling, 2014; Kingma et al., 2014; Sohn et al., 2015; Miao et al., 2016) , and approximate the posterior P (z|X,Y ) with a proposal distribution Q(z|X,Y ), which in our case is a diagonal Gaussian whose parameters depend on X and Y .",
      "startOffset" : 66,
      "endOffset" : 151
    }, {
      "referenceID" : 1,
      "context" : "We take a single sample z from Q using the reparametrization trick (Kingma and Welling, 2014), concatenate hx and z, and initialize the hidden state of the decoder GRU with [hx z].",
      "startOffset" : 67,
      "endOffset" : 93
    }, {
      "referenceID" : 3,
      "context" : "The second baseline model implements the anti-LM decoder of Li et al. (2016a) (DIAL-MMI) on top of the encoder-decoder, with no length normalization.",
      "startOffset" : 60,
      "endOffset" : 78
    }, {
      "referenceID" : 5,
      "context" : "However, as language follows a Zipf distribution, TTR is affected by the length of the generated replies (Mitchell, 2015).",
      "startOffset" : 105,
      "endOffset" : 121
    }, {
      "referenceID" : 3,
      "context" : "Li et al. (2016a)) used typetoken ratio (TTR) to measure the diversity of the generated output.",
      "startOffset" : 0,
      "endOffset" : 18
    } ],
    "year" : 2017,
    "abstractText" : "We present a dialogue generation model that directly captures the variability in possible responses to a given input, which reduces the ‘boring output’ issue of deterministic dialogue models. Experiments show that our model generates more diverse outputs than baseline models, and also generates more consistently acceptable output than sampling from a deterministic encoder-decoder model.",
    "creator" : "LaTeX with hyperref package"
  }
}