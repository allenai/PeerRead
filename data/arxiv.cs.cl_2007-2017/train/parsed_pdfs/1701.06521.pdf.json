{
  "name" : "1701.06521.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Incorporating Global Visual Features into Attention-Based Neural Machine Translation",
    "authors" : [ "Iacer Calixto", "Qun Liu" ],
    "emails" : [ "iacer.calixto@adaptcentre.ie" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Neural Machine Translation (NMT) has recently been proposed as an instantiation of the sequence to sequence (seq2seq) learning problem (Kalchbrenner and Blunsom, 2013; Cho et al., 2014b; Sutskever et al., 2014). In this problem, each training example consists of one source and one target variable-length sequence, with no prior information regarding the alignments between the two. A model is trained to translate sequences in the\nsource language into corresponding sequences in the target. This framework has been successfully used in many different tasks, such as handwritten text generation (Graves, 2013), image description generation (Hodosh et al., 2013; Kiros et al., 2014; Mao et al., 2014; Elliott et al., 2015; Karpathy and Fei-Fei, 2015; Vinyals et al., 2015), machine translation (Cho et al., 2014b; Sutskever et al., 2014) and video description generation (Donahue et al., 2015; Venugopalan et al., 2015).\nRecently, there has been an increase in the number of natural language generation models that explicitly use attention-based decoders, i.e. decoders that model an intra-sequential mapping between source and target representations. For instance, Xu et al. (2015) proposed an attentionbased model for the task of image description generation where the model learns to attend to specific parts of an image (the source) as it generates its description (the target). In MT, one can intuitively interpret this attention mechanism as inducing an alignment between source and target sentences, as first proposed by Bahdanau et al. (2015). The common idea is to explicitly frame a learning task in which the decoder learns to attend to the relevant parts of the source sequence when generating each part of the target sequence.\nWe are inspired by recent successes in using attention-based models in both image description generation and NMT. Our main goal in this work is to propose end-to-end multi-modal NMT models which effectively incorporate visual features in different parts of the attention-based NMT framework. The main contributions of our work are:\n• We propose novel attention-based multimodal NMT models which incorporate visual features into the encoder and the decoder.\n• We discuss the impact that adding synthetic multi-modal and multilingual data brings to\nar X\niv :1\n70 1.\n06 52\n1v 1\n[ cs\n.C L\n] 2\n3 Ja\nn 20\n17\nmulti-modal NMT.\n• We show that images bring useful information to an NMT model and report state-ofthe-art results.\nOne additional contribution of our work is that we corroborate previous findings by Vinyals et al. (2015) that suggested that using image features directly as additional context to update the hidden state of the decoder (at each time step) leads to overfitting, ultimately preventing learning.\nThe remainder of this paper is structured as follows. In §1.1 we briefly discuss relevant previous related work. We then revise the attention-based NMT framework and further expand it into different multi-modal NMT models (§2). In §3 we introduce the data sets we use in our experiments. In §4 we detail the hyperparameters, parameter initialisation and other relevant details of our models. Finally, in §5 we draw conclusions and provide some avenues for future work."
    }, {
      "heading" : "1.1 Related work",
      "text" : "Attention-based encoder-decoder models for MT have been actively investigated in recent years. Some researchers have studied how to improve attention mechanisms (Luong et al., 2015; Tu et al., 2016) and how to train attention-based models to translate between many languages (Dong et al., 2015; Firat et al., 2016).\nThere has been some previous related work on using images in tasks involving multilingual and multi-modal natural language generation. Calixto et al. (2012) studied how the visual context of a textual description can be helpful in the disambiguation of Statistical MT (SMT) systems. Hitschler et al. (2016) used image features for re-ranking translations of image descriptions generated by an SMT model and reported significant improvements. Elliott et al. (2015) generated multilingual descriptions of images by learning and transferring features between two independent, non-attentive neural image description models. Luong et al. (2016) proposed a multi-task learning approach and incorporated neural image description as an auxiliary task to sequence-tosequence NMT and improved translations in the main translation task.\nMulti-modal MT has recently been addressed by the MT community in the form of a shared task (Specia et al., 2016). We note that in the of-\nficial results of this first shared task no submissions based on a purely neural architecture could improve on the phrase-based SMT (PBSMT) baseline. Nevertheless, researchers have proposed to include global visual features in re-ranking nbest lists generated by a PBSMT system or directly in a purely NMT framework with some success (Caglayan et al., 2016; Calixto et al., 2016; Libovický et al., 2016; Shah et al., 2016). The best results achieved by a purely NMT model in this shared task are those of Huang et al. (2016), who proposed to use global and regional image features extracted with the VGG19 network.\nSimilarly to one model we propose,1 they extract global features for an image, project these features into the vector space of the source words and then add it as a word in the input sequence. Their best model improves over a strong NMT baseline and is comparable to results obtained with a PBSMT model trained on the same data. For that reason, their models are used as baselines in our experiments. Next, we point out some key differences between their models and ours.\nArchitecture Their implementation is based on the attention-based model of Luong et al. (2015), which has some differences to that of Bahdanau et al. (2015), used in our work (§2.1). Their encoder is a single-layer unidirectional LSTM and they use the last hidden state of the encoder to initialise the decoder’s hidden state, therefore indirectly using the image features to do so. We use a bi-directional recurrent neural network (RNN) with GRU (Cho et al., 2014a) as our encoder, better encoding the semantics of the source sentence.\nImage features We include image features separately either as a word in the source sentence (§2.2.1) or directly for encoder (§2.2.2) or decoder initialisation (§2.2.3), whereas Huang et al. (2016) only use it as a word. We also show it is better to include an image exclusively for the encoder or the decoder initialisation (Tables 1 and 2).\nData Huang et al. (2016) use object detections obtained with the RCNN of Girshick et al. (2014) as additional data, whereas we study the impact that additional back-translated data brings.\nPerformance All our models outperform Huang et al. (2016)’s according to all metrics evaluated,\n1This idea has been developed independently by both research groups.\neven when they use additional object detections. If we use additional back-translated data, the difference becomes even larger."
    }, {
      "heading" : "2 Attention-based NMT",
      "text" : "In this section, we briefly revise the attentionbased NMT framework (§2.1) and expand it into a multi-modal NMT framework (§2.2)."
    }, {
      "heading" : "2.1 Text-only attention-based NMT",
      "text" : "We follow the notation of Bahdanau et al. (2015) and Firat et al. (2016) throughout this section. Given a source sequence X = (x1, x2, · · · , xN ) and its translation Y = (y1, y2, · · · , yM ), an NMT model aims at building a single neural network that translates X into Y by directly learning to model p(Y |X). Each xi is a row index in a source lookup matrix Wx ∈ R|Vx|×dx (the source word embeddings matrix) and each yj is an index in a target lookup matrix Wy ∈ R|Vy |×dy (the target word embeddings matrix). Vx and Vy are source and target vocabularies and dx and dy are source and target word embeddings dimensionalities, respectively.\nA bidirectional RNN with GRU is used as the encoder. A forward RNN −→ Φ enc reads X word by word, from left to right, and generates a sequence of forward annotation vectors ( −→ h 1, −→ h 2, · · · , −→ hN ) at each encoder time step i ∈ [1, N ]. Similarly, a backward RNN ←− Φ enc reads X from right to left, word by word, and generates a sequence of backward annotation vectors ( ←− h 1, ←− h 2, · · · ,\n←− hN ), as in (1):−→\nhi = −→ Φ enc ( Wx[xi], −→ h i−1 ) , ←− hi = ←− Φ enc ( Wx[xi], ←− h i+1 ) . (1)\nThe final annotation vector for a given time step i is the concatenation of forward and backward vectors hi = [−→ hi; ←− hi ] .\nIn other words, each source sequence X is encoded into a sequence of annotation vectors h = (h1,h2, · · · ,hN ), which are in turn used by the decoder: essentially a neural language model (LM) (Bengio et al., 2003) conditioned on the previously emitted words and the source sentence via an attention mechanism.\nAt each time step t of the decoder, we compute a time-dependent context vector ct based on the annotation vectors h, the decoder’s previous hidden state st−1 and the target word ỹt−1 emitted by\nthe decoder in the previous time step.2\nWe follow Bahdanau et al. (2015) and use a single-layer feed-forward network to compute an expected alignment et,i between each source annotation vector hi and the target word to be emitted at the current time step t, as in (2):\net,i = va T tanh(Uast−1 +Wahi). (2) In Equation (3), these expected alignments are further normalised and converted into probabilities:\nαt,i = exp (et,i)∑N j=1 exp (et,j) , (3)\nwhere αt,i are called the model’s attention weights, which are in turn used in computing the time-dependent context vector ct = ∑N i=1αt,ihi. Finally, the context vector ct is used in computing the decoder’s hidden state st for the current time step t, as shown in Equation (4):\nst = Φdec(st−1,Wy[ỹt−1], ct), (4) where st−1 is the decoder’s previous hidden state, Wy[ỹt−1] is the embedding of the word emitted in the previous time step, and ct is the updated timedependent context vector. In Figure 1 we illustrate the computation of the decoder’s hidden state st.\nWe use a single-layer feed-forward neural network to initialise the decoder’s hidden state s0 at time step t = 0 and feed it the concatenation of the last hidden states of the encoder’s forward RNN\n2At training time, the correct previous target word yt−1 is known and therefore used instead of ỹt−1. At test or inference time, yt−1 is not known and ỹt−1 is used instead. Bengio et al. (2015) discussed problems that may arise from this difference between training and inference distributions.\n( −→ Φ enc) and backward RNN ( ←− Φ enc), as in (5): s0 = tanh ( Wdi[ ←− h 1; −→ hN ] + bdi ) , (5) where Wdi and bdi are model parameters. Since RNNs normally better store information about recent inputs in comparison to more distant ones (Hochreiter and Schmidhuber, 1997; Bahdanau et al., 2015), we expect to initialise the decoder’s hidden state with a strong source sentence representation, i.e. a representation with a strong focus on both the first and the last tokens in the source sentence."
    }, {
      "heading" : "2.2 Multi-modal NMT (MNMT)",
      "text" : "Our models can be seen as expansions of the attention-based NMT framework described in §2 with the addition of a visual component to incorporate image features.\nSimonyan and Zisserman (2014) trained and evaluated an extensive set of deep convolutional neural network (CNN) models for classifying images into one out of the 1000 classes in ImageNet (Russakovsky et al., 2015). We use their 19-layer VGG network (VGG19) to extract image feature vectors for all images in our dataset. We feed an image to the pre-trained VGG19 network and use the 4096D activations of the penultimate fully-connected layer FC73 as our image feature vector, henceforth referred to as q.\nWe propose three different methods to incorporate images into the attentive NMT framework: using an image as words in the source sentence (§2.2.1), using an image to initialise the source language encoder (§2.2.2) and the target language decoder (§2.2.3).\nWe also evaluated a fourth mechanism to incorporate images into NMT, namely to use an image as one of the different contexts available to the decoder at each time step of the decoding process. We add the image features directly as an additional context, in addition to Wy[ỹt−1], st−1 and ct, to compute the hidden state st of the decoder at a given time step t. We corroborate previous findings by Vinyals et al. (2015) in that adding the image features as such causes the model to overfit, ultimately preventing learning.4\n3We use the activations of the FC7 layer, which encode information about the entire image, of the VGG19 network (configuration E) in Simonyan and Zisserman (2014)’s paper.\n4For comparison, translations for the translated Multi30k test set (described in §3) achieve just 3.8 BLEU (Papineni et al., 2002), 15.5 METEOR (Denkowski and Lavie, 2014) and 93.0 TER (Snover et al., 2006)."
    }, {
      "heading" : "2.2.1 Images as source words: IMGW",
      "text" : "One way we propose to incorporate images into the encoder is to project an image feature vector into the space of the words of the source sentence. We use the projected image as the first and/or last word of the source sentence and let the attention model learn when to attend to the image representation. Specifically, given the global image feature vector q ∈ R4096, we compute (6): d = W 2I · (W 1I · q + b1I) + b2I , (6) whereW 1I ∈ R4096×4096 andW 2I ∈ R4096×dx are image transformation matrices, b1I ∈ R4096 and b2I ∈ Rdx are bias vectors, and dx is the source words vector space dimensionality, all trained with the model. We then directly use d as words in the source words vector space: as the first word only (model IMG1W), and as the first and last words of the source sentence (model IMG2W).\nAn illustration of this idea is given in Figure 2, where a source sentence that originally contained N tokens, after including the image as source words will contain N + 1 tokens (model IMG1W) or N + 2 tokens (model IMG2W). In model IMG1W, the image is projected as the first source word only (solid line in Figure 2); in model IMG2W, it is projected into the source words space as both first and last words (both solid and dashed lines in Figure 2).\nGiven a source sequence X = (x1, x2, · · · , xN ), we concatenate the transformed image vector d to Wx[X] and apply the forward and backward encoder RNN passes, generating hidden vectors as in Figure 2. When computing the context vector ct (Equations (2) and (3)), we effectively make use of the transformed image vector, i.e. the αt,i attention weight parameters will use this information to attend or not to the image features.\nBy including images into the encoder in models IMG1W and IMG2W, our intuition is that (i) by including the image as the first word, we propagate image features into the source sentence vector representations when applying the forward RNN −→ Φ enc (vectors −→ hi), and (ii) by including the image as the last word, we propagate image features into the source sentence vector representations when applying the backward RNN ←− Φ enc (vectors ←− hi)."
    }, {
      "heading" : "2.2.2 Images for encoder initialisation: IMGE",
      "text" : "In the original attention-based NMT model described in §2, the hidden state of the encoder is initialised with the zero vector #» 0 . Instead, we propose to use two new single-layer feed-forward neural networks to compute the initial states of the forward RNN −→ Φ enc and the backward RNN ←− Φ enc, respectively, as illustrated in Figure 3. Similarly to §2.2.1, given a global image feature vector q ∈ R4096, we compute a vector d using Equation (6), only this time the parameters W 2I and b2I project the image features into the same dimensionality as the textual encoder hidden states.\nThe feed-forward networks used to initialise the encoder hidden state are computed as in (7):\n←− h init = tanh ( Wfd+ bf ) ,\n−→ h init = tanh ( Wbd+ bb ) , (7)\nwhere Wf and Wb are multi-modal projection matrices that project the image features d into the encoder forward and backward hidden states dimensionality, respectively, and bf and bb are bias vectors."
    }, {
      "heading" : "2.2.3 Images for decoder initialisation: IMGD",
      "text" : "To incorporate an image into the decoder, we introduce a new single-layer feed-forward neural network to be used instead of the one described in Equation 5. Originally, the decoder’s initial hidden state was computed using the concatenation of the last hidden states of the encoder forward\nRNN ( −→ Φ enc) and backward RNN ( ←− Φ enc), respectively −→ hN and ←− h 1.\nOur proposal is that we include the image features as additional input to initialise the decoder hidden state at time step t = 0, as in (8): s0 = tanh ( Wdi[ ←− h 1; −→ hN ] +Wmd+ bdi ) , (8) whereWm is a multi-modal projection matrix that projects the image features d into the decoder hidden state dimensionality and Wdi and bdi are the same as in Equation (5).\nOnce again we compute d by applying Equa-\ntion (6) onto a global image feature vector q ∈ R4096, only this time the parameters W 2I and b2I project the image features into the same dimensionality as the decoder hidden states. We illustrate this idea in Figure 4."
    }, {
      "heading" : "3 Data set",
      "text" : "Our multi-modal NMT models need bilingual sentences accompanied by one or more images as training data. The original Flickr30k data set contains 30k images and 5 English sentence descriptions for each image (Young et al., 2014). We use the translated and the comparable Multi30k datasets (Elliott et al., 2016), henceforth referred to as M30kT and M30kC, respectively, which are multilingual expansions of the original Flickr30k.\nFor each of the 30k images in the Flickr30k, the M30kT has one of its English descriptions manually translated into German by a professional translator. Training, validation and test sets contain 29k, 1014 and 1k images, respectively, each accompanied by one sentence pair (the original English sentence and its German translation). For each of the 30k images in the Flickr30k, the M30kC has five descriptions in German collected independently of the English descriptions. Training, validation and test sets contain 29k, 1014 and 1k images, respectively, each accompanied by five sentences in English and five sentences in German.\nWe use the scripts in the Moses SMT Toolkit (Koehn et al., 2007) to normalise, truecase and tokenize English and German descriptions and we also convert space-separated tokens into subwords (Sennrich et al., 2016b). All models use a common vocabulary of 83,093 English and 91,141 German subword tokens. If sentences in English or German are longer than 80 tokens, they are discarded.\nWe use the entire M30kT training set for training, its validation set for model selection with BLEU, and its test set to evaluate our models. In order to study the impact that additional training data brings to the models, we use the baseline model described in §2 trained on the textual part of the M30kT data set (German→English) without the images to build a back-translation model (Sennrich et al., 2016a). We back-translate the 145k German descriptions in the M30kC into English and include the triples (synthetic English description, German description, image) as additional training data.\nWe train models to translate from English into German and report evaluation of cased, tokenized sentences with punctuation."
    }, {
      "heading" : "4 Experimental setup",
      "text" : "Our encoder is a bidirectional RNN with GRU (one 1024D single-layer forward RNN and one 1024D single-layer backward RNN). Source and target word embeddings are 620D each and both are trained jointly with our model. All nonrecurrent matrices are initialised by sampling from a Gaussian distribution (µ = 0, σ = 0.01), recurrent matrices are orthogonal and bias vectors are all initialised to zero. Our decoder RNN also uses GRU and is a neural LM (Bengio et al., 2003) conditioned on its previous emissions and the source sentence by means of the source attention mechanism.\nImage features are obtained by feeding images to the pre-trained VGG19 network of Simonyan and Zisserman (2014) and using the activations of the penultimate fully-connected layer FC7. We apply dropout with a probability of 0.2 in both source and target word embeddings and with a probability of 0.5 in the image features (in all MNMT models), in the encoder and decoder RNNs inputs and recurrent connections, and before the readout operation in the decoder RNN. We follow Gal and Ghahramani (2016) and apply dropout to the encoder bidirectional RNN and decoder RNN using the same mask in all time steps.\nOur models are trained using stochastic gradient descent with Adadelta (Zeiler, 2012) and minibatches of size 40, where each training instance consists of one English sentence, one German sentence and one image. We apply early stopping for model selection based on BLEU scores, so that if a model does not improve on BLEU in the validation set for more than 20 epochs, training is halted.\nWe evaluate our models’ translation quality quantitatively in terms of BLEU4 (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014), TER (Snover et al., 2006), and chrF3 scores5 (Popović, 2015) and we report statistical significance for the three first metrics using approximate randomisation computed with MultEval (Clark et al., 2011).\nAs our main baseline we train an attentionbased NMT model (§2) in which only the textual part of M30kT is used for training. We also train a\n5We specifically compute character 6-gram F3 scores.\nPBSMT model built with Moses on the same data. The LM is a 5–gram LM with modified KneserNey smoothing (Kneser and Ney, 1995) trained on the German side of the M30kT dataset. We use minimum error rate training (Och, 2003) for tuning the model parameters for BLEU scores. Our third baseline is the best comparable multi-modal model by Huang et al. (2016) and also their best model with additional object detections: respectively models m1 (image at head) and m3 in the authors’ paper."
    }, {
      "heading" : "4.1 Results",
      "text" : "The Multi30K dataset contains images and bilingual descriptions. Overall, it is a small dataset with a small vocabulary whose sentences have simple syntactic structures and not much ambiguity (Elliott et al., 2016). This is reflected in the fact that even the simplest baselines perform fairly well on it, i.e. the smallest BLEU score of 32.9 is that of the PBSMT model, which is still good for translating into German.\nFrom Table 1 we see that our multi-modal models perform well, with models IMGE and IMGD improving on both baselines according to all metrics analysed. We also note that all models but\nIMG2W+D perform consistently better than the strong multi-modal NMT baseline of Huang et al. (2016), even when this model has access to more data (+RCNN features).6 Combining image features in the encoder and the decoder at the same time (last two entries in Table 1) does not seem to improve results compared to using the image features in only the encoder or the decoder. To the best of our knowledge, it is the first time a purely neural model significantly improves over a PBSMT model in all metrics on this data set.\nArguably, the main downside of applying multimodal NMT in a real-world scenario is the small amount of publicly available training data (∼30k), which restricts its applicability. For that reason, we back-translated the German sentences in the M30kC and created additional 145k synthetic triples (synthetic English sentence, original German sentence and image).\nIn Table 2, we present results for some of the models evaluated in Table 1 but when also trained on the additional data. In order to add more data to the PBSMT baseline, we simply added the German sentences in the M30kC as additional data to train the LM.7 Both our models IMGE and IMGD that use global image features to initialise the encoder and the decoder, respectively, improve significantly according to BLEU, METEOR and TER with the additional back-translated data, and also achieved better chrF3 scores. Model IMG2W, that uses images as words in the source sentence, does not significantly differ in BLEU, METEOR or TER (p = 0.05), but achieves a lower chrF3 score than the comparable PBSMT model. Although model IMG2W trained on only the original data has the best TER score (= 41.9), both models IMGE and IMGD perform comparably with the additional back-translated data (= 41.4 and 41.6, respectively), though the difference between the latter and the former is still not statistically significant (p = 0.05).\nWe see in Tables 1 and 2 that our models that use images directly to initialise either the encoder or the decoder are the only ones to consistently outperform the PBSMT baseline according to the chrF3 metric, a character-based metric that in-\n6In fact, model IMG2W+D still improves on the multimodal baseline of Huang et al. (2016) when trained on the same data.\n7Adding the synthetic sentence pairs to train the baseline PBSMT model, as we did with all neural MT models, deteriorated the results.\ncludes both precision and recall, and has a recall bias. That is also a noteworthy finding, since chrF3 is the only character-level metric we use, and it has shown a high correlation with human judgements (Stanojević et al., 2015).\nIn Table 3 we see translations for two entries in the test M30k set. In the first entry, although the reference translation is incorrect—there is just one dog in the image—, the multi-modal models translated it correctly. In the second entry, the last three multi-modal models extrapolate the reference+image and describe “ceremony” as a “wedding ceremony” (IMG2W) and as an “Olympics ceremony” (IMGE and IMGD). This could be due to the fact that the training set is small, depicts a small variation of different scenes and contains different forms of biasses (van Miltenburg, 2015).\nWe note that the idea of using images as words in the source sentence, also entertained by Huang et al. (2016), does not perform as well as directly using the images in the encoder or decoder initialisation. The fact that multi-modal NMT models can benefit from back-translated data is also an interesting finding."
    }, {
      "heading" : "5 Conclusions",
      "text" : "We have introduced different ideas to incorporate images into state-of-the-art attention-based NMT, by using images as words in the source sentence, to initialise the encoder’s hidden state and as additional data in the initialisation of the decoder’s hidden state. We corroborate previous findings in that using image features directly at each time step of the decoder causes the model to overfit and prevents learning. The intuition behind our effort is to use global image feature vectors to visually ground translations and consequently increase translation quality. Extensive experiments show that adding global image features into attentionbased NMT is useful and improves over NMT and PBSMT as well as a strong multi-modal NMT baseline, according to all metrics evaluated.\nIn future work we will conduct a more systematic study on the impact that synthetic backtranslated data can have on multi-modal NMT, and also investigate how to incorporate local, spatialpreserving image features."
    } ],
    "references" : [ {
      "title" : "Neural Machine Translation by Jointly Learning to Align and Translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "International Conference on Learning Representations, ICLR 2015. San Diego, California.",
      "citeRegEx" : "Bahdanau et al\\.,? 2015",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2015
    }, {
      "title" : "Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks",
      "author" : [ "Samy Bengio", "Oriol Vinyals", "Navdeep Jaitly", "Noam M. Shazeer." ],
      "venue" : "Advances in Neural Information Processing Systems, NIPS. http://arxiv.org/abs/1506.03099.",
      "citeRegEx" : "Bengio et al\\.,? 2015",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2015
    }, {
      "title" : "A Neural Probabilistic Language Model",
      "author" : [ "Yoshua Bengio", "Réjean Ducharme", "Pascal Vincent", "Christian Janvin." ],
      "venue" : "J. Mach. Learn. Res. 3:1137–1155. http://dl.acm.org/citation.cfm?id=944919.944966.",
      "citeRegEx" : "Bengio et al\\.,? 2003",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2003
    }, {
      "title" : "Does Multimodality Help Human and Machine for Translation and Image Captioning",
      "author" : [ "Ozan Caglayan", "Walid Aransa", "Yaxing Wang", "Marc Masana", "Mercedes Garcı́a-Martı́nez", "Fethi Bougares", "Loı̈c Barrault", "Joost van de Weijer" ],
      "venue" : null,
      "citeRegEx" : "Caglayan et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Caglayan et al\\.",
      "year" : 2016
    }, {
      "title" : "Images as context in Statistical Machine Translation",
      "author" : [ "Iacer Calixto", "Teofilo de Campos", "Lucia Specia." ],
      "venue" : "Proceedings of the Workshop on Vision and Language, VL 2012. Sheffield, England.",
      "citeRegEx" : "Calixto et al\\.,? 2012",
      "shortCiteRegEx" : "Calixto et al\\.",
      "year" : 2012
    }, {
      "title" : "DCU-UvA Multimodal MT System Report",
      "author" : [ "Iacer Calixto", "Desmond Elliott", "Stella Frank." ],
      "venue" : "Proceedings of the First Conference on Machine Translation. Berlin, Germany, pages 634–638. http://www.aclweb.org/anthology/W/W16/W16-",
      "citeRegEx" : "Calixto et al\\.,? 2016",
      "shortCiteRegEx" : "Calixto et al\\.",
      "year" : 2016
    }, {
      "title" : "On the properties of neural machine translation: Encoder–decoder approaches",
      "author" : [ "Kyunghyun Cho", "Bart van Merriënboer", "Dzmitry Bahdanau", "Yoshua Bengio." ],
      "venue" : "Syntax, Semantics and Structure in Statistical Translation page 103.",
      "citeRegEx" : "Cho et al\\.,? 2014a",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2014
    }, {
      "title" : "Learning Phrase Representations using RNN Encoder– Decoder for Statistical Machine Translation",
      "author" : [ "Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio." ],
      "venue" : "In",
      "citeRegEx" : "Cho et al\\.,? 2014b",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2014
    }, {
      "title" : "Better Hypothesis Testing for Statistical Machine Translation: Controlling for Optimizer Instability",
      "author" : [ "Jonathan H. Clark", "Chris Dyer", "Alon Lavie", "Noah A. Smith." ],
      "venue" : "Proceedings of the 49th Annual Meeting of the Associa-",
      "citeRegEx" : "Clark et al\\.,? 2011",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2011
    }, {
      "title" : "Meteor Universal: Language Specific Translation Evaluation for Any Target Language",
      "author" : [ "Michael Denkowski", "Alon Lavie." ],
      "venue" : "Proceedings of the EACL 2014 Workshop on Statistical Machine Translation.",
      "citeRegEx" : "Denkowski and Lavie.,? 2014",
      "shortCiteRegEx" : "Denkowski and Lavie.",
      "year" : 2014
    }, {
      "title" : "Longterm Recurrent Convolutional Networks for Visual Recognition and Description",
      "author" : [ "Jeff Donahue", "Lisa Anne Hendricks", "Sergio Guadarrama", "Marcus Rohrbach", "Subhashini Venugopalan", "Trevor Darrell", "Kate Saenko." ],
      "venue" : "Computer Vision",
      "citeRegEx" : "Donahue et al\\.,? 2015",
      "shortCiteRegEx" : "Donahue et al\\.",
      "year" : 2015
    }, {
      "title" : "Multi-Task Learning for Multiple Language Translation",
      "author" : [ "Daxiang Dong", "Hua Wu", "Wei He", "Dianhai Yu", "Haifeng Wang." ],
      "venue" : "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint",
      "citeRegEx" : "Dong et al\\.,? 2015",
      "shortCiteRegEx" : "Dong et al\\.",
      "year" : 2015
    }, {
      "title" : "Multi-language image description with neural sequence models",
      "author" : [ "Desmond Elliott", "Stella Frank", "Eva Hasler." ],
      "venue" : "CoRR abs/1510.04709. http://arxiv.org/abs/1510.04709.",
      "citeRegEx" : "Elliott et al\\.,? 2015",
      "shortCiteRegEx" : "Elliott et al\\.",
      "year" : 2015
    }, {
      "title" : "Multi30K: Multilingual English-German Image Descriptions",
      "author" : [ "Desmond Elliott", "Stella Frank", "Khalil Sima’an", "Lucia Specia" ],
      "venue" : "In Proceedings of the 5th Workshop on Vision and Language,",
      "citeRegEx" : "Elliott et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Elliott et al\\.",
      "year" : 2016
    }, {
      "title" : "Multi-Way, Multilingual Neural Machine Translation with a Shared Attention Mechanism",
      "author" : [ "Orhan Firat", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "Proceedings of the 2016 Conference of the North American Chapter of the Association for Com-",
      "citeRegEx" : "Firat et al\\.,? 2016",
      "shortCiteRegEx" : "Firat et al\\.",
      "year" : 2016
    }, {
      "title" : "A Theoretically Grounded Application of Dropout in Recurrent Neural Networks",
      "author" : [ "Yarin Gal", "Zoubin Ghahramani." ],
      "venue" : "Advances in Neural Information Processing Systems, NIPS, Barcelona, Spain, pages 1019–1027. http://papers.nips.cc/paper/6241-",
      "citeRegEx" : "Gal and Ghahramani.,? 2016",
      "shortCiteRegEx" : "Gal and Ghahramani.",
      "year" : 2016
    }, {
      "title" : "Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation",
      "author" : [ "Ross Girshick", "Jeff Donahue", "Trevor Darrell", "Jitendra Malik." ],
      "venue" : "Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition.",
      "citeRegEx" : "Girshick et al\\.,? 2014",
      "shortCiteRegEx" : "Girshick et al\\.",
      "year" : 2014
    }, {
      "title" : "Generating Sequences With Recurrent Neural Networks",
      "author" : [ "Alex Graves." ],
      "venue" : "CoRR abs/1308.0850. http://arxiv.org/abs/1308.0850.",
      "citeRegEx" : "Graves.,? 2013",
      "shortCiteRegEx" : "Graves.",
      "year" : 2013
    }, {
      "title" : "Multimodal Pivots for Image Caption Translation",
      "author" : [ "Julian Hitschler", "Shigehiko Schamoni", "Stefan Riezler." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long",
      "citeRegEx" : "Hitschler et al\\.,? 2016",
      "shortCiteRegEx" : "Hitschler et al\\.",
      "year" : 2016
    }, {
      "title" : "Long Short-Term Memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural Comput. 9(8):1735– 1780. https://doi.org/10.1162/neco.1997.9.8.1735.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Framing Image Description As a Ranking Task: Data, Models and Evaluation Metrics",
      "author" : [ "Micah Hodosh", "Peter Young", "Julia Hockenmaier." ],
      "venue" : "J. Artif. Int. Res. 47(1):853–899. http://dl.acm.org/citation.cfm?id=2566972.2566993.",
      "citeRegEx" : "Hodosh et al\\.,? 2013",
      "shortCiteRegEx" : "Hodosh et al\\.",
      "year" : 2013
    }, {
      "title" : "Attention-based Multimodal Neural Machine Translation",
      "author" : [ "Po-Yao Huang", "Frederick Liu", "Sz-Rung Shiang", "Jean Oh", "Chris Dyer." ],
      "venue" : "Proceedings of the First Conference on Machine Translation. Berlin, Germany, pages 639–645.",
      "citeRegEx" : "Huang et al\\.,? 2016",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2016
    }, {
      "title" : "Recurrent Continuous Translation Models",
      "author" : [ "Nal Kalchbrenner", "Phil Blunsom." ],
      "venue" : "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, EMNLP 2013. Seattle, pages 1700–1709.",
      "citeRegEx" : "Kalchbrenner and Blunsom.,? 2013",
      "shortCiteRegEx" : "Kalchbrenner and Blunsom.",
      "year" : 2013
    }, {
      "title" : "Deep visualsemantic alignments for generating image descriptions",
      "author" : [ "Andrej Karpathy", "Li Fei-Fei." ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015. Boston, Massachusetts, pages 3128–3137.",
      "citeRegEx" : "Karpathy and Fei.Fei.,? 2015",
      "shortCiteRegEx" : "Karpathy and Fei.Fei.",
      "year" : 2015
    }, {
      "title" : "Unifying visual-semantic embeddings with multimodal neural language models",
      "author" : [ "Ryan Kiros", "Ruslan Salakhutdinov", "Richard S. Zemel." ],
      "venue" : "CoRR abs/1411.2539. http://arxiv.org/abs/1411.2539.",
      "citeRegEx" : "Kiros et al\\.,? 2014",
      "shortCiteRegEx" : "Kiros et al\\.",
      "year" : 2014
    }, {
      "title" : "Improved backing-off for m-gram language modeling",
      "author" : [ "Reinhard Kneser", "Hermann Ney." ],
      "venue" : "In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing. Detroit, Michigan, volume I, pages 181–184.",
      "citeRegEx" : "Kneser and Ney.,? 1995",
      "shortCiteRegEx" : "Kneser and Ney.",
      "year" : 1995
    }, {
      "title" : "CUNI System for WMT16 Automatic Post-Editing and Multimodal Translation Tasks",
      "author" : [ "Jindřich Libovický", "Jindřich Helcl", "Marek Tlustý", "Ondřej Bojar", "Pavel Pecina." ],
      "venue" : "Proceedings of the First Conference on Machine",
      "citeRegEx" : "Libovický et al\\.,? 2016",
      "shortCiteRegEx" : "Libovický et al\\.",
      "year" : 2016
    }, {
      "title" : "BLEU: A Method",
      "author" : [ "Wei-Jing Zhu" ],
      "venue" : null,
      "citeRegEx" : "Zhu.,? \\Q2002\\E",
      "shortCiteRegEx" : "Zhu.",
      "year" : 2002
    }, {
      "title" : "chrf: character n-gram f",
      "author" : [ ],
      "venue" : null,
      "citeRegEx" : "Popović.,? \\Q2015\\E",
      "shortCiteRegEx" : "Popović.",
      "year" : 2015
    }, {
      "title" : "SHEF-Multimodal: Grounding Machine Translation on Images",
      "author" : [ "Kashif Shah", "Josiah Wang", "Lucia Specia." ],
      "venue" : "Proceedings of the First Conference on Machine Translation. Berlin, Germany, pages 660–665.",
      "citeRegEx" : "Shah et al\\.,? 2016",
      "shortCiteRegEx" : "Shah et al\\.",
      "year" : 2016
    }, {
      "title" : "Very Deep Convolutional Networks for Large-Scale Image Recognition",
      "author" : [ "K. Simonyan", "A. Zisserman." ],
      "venue" : "CoRR abs/1409.1556.",
      "citeRegEx" : "Simonyan and Zisserman.,? 2014",
      "shortCiteRegEx" : "Simonyan and Zisserman.",
      "year" : 2014
    }, {
      "title" : "A study of translation edit rate with targeted human annotation",
      "author" : [ "Matthew Snover", "Bonnie Dorr", "Richard Schwartz", "Linnea Micciulla", "John Makhoul." ],
      "venue" : "In Proceedings of Association for Machine Translation in the Americas. Cambridge, MA, pages",
      "citeRegEx" : "Snover et al\\.,? 2006",
      "shortCiteRegEx" : "Snover et al\\.",
      "year" : 2006
    }, {
      "title" : "A Shared Task on Multimodal Machine Translation and Crosslingual Image Description",
      "author" : [ "Lucia Specia", "Stella Frank", "Khalil Sima’an", "Desmond Elliott" ],
      "venue" : "In Proceedings of the First Conference on Machine Translation,",
      "citeRegEx" : "Specia et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Specia et al\\.",
      "year" : 2016
    }, {
      "title" : "Berlin, Germany, pages 543– 553",
      "author" : [ "WMT" ],
      "venue" : "http://aclweb.org/anthology/W/W16/W162346.pdf.",
      "citeRegEx" : "WMT,? 2016",
      "shortCiteRegEx" : "WMT",
      "year" : 2016
    }, {
      "title" : "Results of the WMT15 Metrics Shared Task",
      "author" : [ "Miloš Stanojević", "Amir Kamran", "Philipp Koehn", "Ondřej Bojar." ],
      "venue" : "Proceedings of the Tenth Workshop on Statistical Machine Translation. Lisbon, Portugal, pages 256–273.",
      "citeRegEx" : "Stanojević et al\\.,? 2015",
      "shortCiteRegEx" : "Stanojević et al\\.",
      "year" : 2015
    }, {
      "title" : "Sequence to Sequence Learning with Neural Networks",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc V Le." ],
      "venue" : "Advances in Neural Information Processing Systems. Montréal, Canada, pages 3104–3112.",
      "citeRegEx" : "Sutskever et al\\.,? 2014",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "Modeling Coverage for Neural Machine Translation",
      "author" : [ "Zhaopeng Tu", "Zhengdong Lu", "Yang Liu", "Xiaohua Liu", "Hang Li." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume",
      "citeRegEx" : "Tu et al\\.,? 2016",
      "shortCiteRegEx" : "Tu et al\\.",
      "year" : 2016
    }, {
      "title" : "Stereotyping and bias in the flickr30k dataset",
      "author" : [ "Emiel van Miltenburg." ],
      "venue" : "Proceedings of the Workshop on Multimodal Corpora, MMC-2016. Portorož, Slovenia, pages 1–4.",
      "citeRegEx" : "Miltenburg.,? 2015",
      "shortCiteRegEx" : "Miltenburg.",
      "year" : 2015
    }, {
      "title" : "Sequence to Sequence - Video to Text",
      "author" : [ "Subhashini Venugopalan", "Marcus Rohrbach", "Jeffrey Donahue", "Raymond Mooney", "Trevor Darrell", "Kate Saenko." ],
      "venue" : "Proceedings of the IEEE International Conference on Computer Vision. Santiago, Chile,",
      "citeRegEx" : "Venugopalan et al\\.,? 2015",
      "shortCiteRegEx" : "Venugopalan et al\\.",
      "year" : 2015
    }, {
      "title" : "Show and tell: A neural image caption generator",
      "author" : [ "Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan." ],
      "venue" : "IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015. Boston, Massachusetts, pages 3156–3164.",
      "citeRegEx" : "Vinyals et al\\.,? 2015",
      "shortCiteRegEx" : "Vinyals et al\\.",
      "year" : 2015
    }, {
      "title" : "Show, attend and tell: Neural image caption generation with visual attention",
      "author" : [ "Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhudinov", "Rich Zemel", "Yoshua Bengio." ],
      "venue" : "Proceedings of",
      "citeRegEx" : "Xu et al\\.,? 2015",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2015
    }, {
      "title" : "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions",
      "author" : [ "Peter Young", "Alice Lai", "Micah Hodosh", "Julia Hockenmaier." ],
      "venue" : "Transactions of the Association for Computational Linguis-",
      "citeRegEx" : "Young et al\\.,? 2014",
      "shortCiteRegEx" : "Young et al\\.",
      "year" : 2014
    }, {
      "title" : "ADADELTA: An Adaptive Learning Rate Method",
      "author" : [ "Matthew D. Zeiler." ],
      "venue" : "CoRR abs/1212.5701. http://arxiv.org/abs/1212.5701.",
      "citeRegEx" : "Zeiler.,? 2012",
      "shortCiteRegEx" : "Zeiler.",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 22,
      "context" : "to sequence (seq2seq) learning problem (Kalchbrenner and Blunsom, 2013; Cho et al., 2014b; Sutskever et al., 2014).",
      "startOffset" : 39,
      "endOffset" : 114
    }, {
      "referenceID" : 7,
      "context" : "to sequence (seq2seq) learning problem (Kalchbrenner and Blunsom, 2013; Cho et al., 2014b; Sutskever et al., 2014).",
      "startOffset" : 39,
      "endOffset" : 114
    }, {
      "referenceID" : 35,
      "context" : "to sequence (seq2seq) learning problem (Kalchbrenner and Blunsom, 2013; Cho et al., 2014b; Sutskever et al., 2014).",
      "startOffset" : 39,
      "endOffset" : 114
    }, {
      "referenceID" : 17,
      "context" : "This framework has been successfully used in many different tasks, such as handwritten text generation (Graves, 2013), image description generation (Hodosh et al.",
      "startOffset" : 103,
      "endOffset" : 117
    }, {
      "referenceID" : 7,
      "context" : ", 2015), machine translation (Cho et al., 2014b; Sutskever et al., 2014) and video description generation (Donahue et al.",
      "startOffset" : 29,
      "endOffset" : 72
    }, {
      "referenceID" : 35,
      "context" : ", 2015), machine translation (Cho et al., 2014b; Sutskever et al., 2014) and video description generation (Donahue et al.",
      "startOffset" : 29,
      "endOffset" : 72
    }, {
      "referenceID" : 10,
      "context" : ", 2014) and video description generation (Donahue et al., 2015; Venugopalan et al., 2015).",
      "startOffset" : 41,
      "endOffset" : 89
    }, {
      "referenceID" : 38,
      "context" : ", 2014) and video description generation (Donahue et al., 2015; Venugopalan et al., 2015).",
      "startOffset" : 41,
      "endOffset" : 89
    }, {
      "referenceID" : 40,
      "context" : "For instance, Xu et al. (2015) proposed an attention-",
      "startOffset" : 14,
      "endOffset" : 31
    }, {
      "referenceID" : 0,
      "context" : "ing an alignment between source and target sentences, as first proposed by Bahdanau et al. (2015). The common idea is to explicitly frame a learning task in which the decoder learns to attend to the relevant parts of the source sequence when gener-",
      "startOffset" : 75,
      "endOffset" : 98
    }, {
      "referenceID" : 39,
      "context" : "One additional contribution of our work is that we corroborate previous findings by Vinyals et al. (2015) that suggested that using image features di-",
      "startOffset" : 84,
      "endOffset" : 106
    }, {
      "referenceID" : 36,
      "context" : "Some researchers have studied how to improve attention mechanisms (Luong et al., 2015; Tu et al., 2016) and how to train attention-based models to",
      "startOffset" : 66,
      "endOffset" : 103
    }, {
      "referenceID" : 11,
      "context" : "translate between many languages (Dong et al., 2015; Firat et al., 2016).",
      "startOffset" : 33,
      "endOffset" : 72
    }, {
      "referenceID" : 14,
      "context" : "translate between many languages (Dong et al., 2015; Firat et al., 2016).",
      "startOffset" : 33,
      "endOffset" : 72
    }, {
      "referenceID" : 4,
      "context" : "Calixto et al. (2012) studied how the visual con-",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 18,
      "context" : "Hitschler et al. (2016) used image features for re-ranking translations of image descriptions generated by an SMT model and reported signif-",
      "startOffset" : 0,
      "endOffset" : 24
    }, {
      "referenceID" : 12,
      "context" : "Elliott et al. (2015) generated multilingual descriptions of images by learning and transferring features between two independent, non-attentive neural image description models.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 12,
      "context" : "Elliott et al. (2015) generated multilingual descriptions of images by learning and transferring features between two independent, non-attentive neural image description models. Luong et al. (2016) proposed a multi-task",
      "startOffset" : 0,
      "endOffset" : 198
    }, {
      "referenceID" : 32,
      "context" : "task (Specia et al., 2016).",
      "startOffset" : 5,
      "endOffset" : 26
    }, {
      "referenceID" : 21,
      "context" : "The best results achieved by a purely NMT model in this shared task are those of Huang et al. (2016), who proposed to use global and regional image features extracted with the VGG19 network.",
      "startOffset" : 81,
      "endOffset" : 101
    }, {
      "referenceID" : 0,
      "context" : "(2015), which has some differences to that of Bahdanau et al. (2015), used in our work (§2.",
      "startOffset" : 46,
      "endOffset" : 69
    }, {
      "referenceID" : 6,
      "context" : "We use a bi-directional recurrent neural network (RNN) with GRU (Cho et al., 2014a) as our encoder, better encoding the semantics of the source sentence.",
      "startOffset" : 64,
      "endOffset" : 83
    }, {
      "referenceID" : 21,
      "context" : "3), whereas Huang et al. (2016) only use it as a word.",
      "startOffset" : 12,
      "endOffset" : 32
    }, {
      "referenceID" : 20,
      "context" : "Data Huang et al. (2016) use object detections obtained with the RCNN of Girshick et al.",
      "startOffset" : 5,
      "endOffset" : 25
    }, {
      "referenceID" : 16,
      "context" : "(2016) use object detections obtained with the RCNN of Girshick et al. (2014) as additional data, whereas we study the impact that additional back-translated data brings.",
      "startOffset" : 55,
      "endOffset" : 78
    }, {
      "referenceID" : 21,
      "context" : "Performance All our models outperform Huang et al. (2016)’s according to all metrics evaluated,",
      "startOffset" : 38,
      "endOffset" : 58
    }, {
      "referenceID" : 0,
      "context" : "We follow the notation of Bahdanau et al. (2015) and Firat et al.",
      "startOffset" : 26,
      "endOffset" : 49
    }, {
      "referenceID" : 0,
      "context" : "We follow the notation of Bahdanau et al. (2015) and Firat et al. (2016) throughout this section.",
      "startOffset" : 26,
      "endOffset" : 73
    }, {
      "referenceID" : 2,
      "context" : "In other words, each source sequence X is encoded into a sequence of annotation vectors h = (h1,h2, · · · ,hN ), which are in turn used by the decoder: essentially a neural language model (LM) (Bengio et al., 2003) conditioned on the previously emitted words and the source sentence via an attention mechanism.",
      "startOffset" : 193,
      "endOffset" : 214
    }, {
      "referenceID" : 0,
      "context" : "We follow Bahdanau et al. (2015) and use a single-layer feed-forward network to compute an expected alignment et,i between each source annotation vector hi and the target word to be emitted at the current time step t, as in (2): et,i = va T tanh(Uast−1 +Wahi).",
      "startOffset" : 10,
      "endOffset" : 33
    }, {
      "referenceID" : 1,
      "context" : "Bengio et al. (2015) discussed problems that may arise from this difference between training and inference distributions.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 19,
      "context" : "recent inputs in comparison to more distant ones (Hochreiter and Schmidhuber, 1997; Bahdanau et al., 2015), we expect to initialise the decoder’s hidden state with a strong source sentence representation, i.",
      "startOffset" : 49,
      "endOffset" : 106
    }, {
      "referenceID" : 0,
      "context" : "recent inputs in comparison to more distant ones (Hochreiter and Schmidhuber, 1997; Bahdanau et al., 2015), we expect to initialise the decoder’s hidden state with a strong source sentence representation, i.",
      "startOffset" : 49,
      "endOffset" : 106
    }, {
      "referenceID" : 39,
      "context" : "We corroborate previous findings by Vinyals et al. (2015) in that adding the image features as such causes the model to overfit, ultimately preventing learning.",
      "startOffset" : 36,
      "endOffset" : 58
    }, {
      "referenceID" : 9,
      "context" : "5 METEOR (Denkowski and Lavie, 2014) and 93.",
      "startOffset" : 9,
      "endOffset" : 36
    }, {
      "referenceID" : 31,
      "context" : "0 TER (Snover et al., 2006).",
      "startOffset" : 6,
      "endOffset" : 27
    }, {
      "referenceID" : 29,
      "context" : "We use the activations of the FC7 layer, which encode information about the entire image, of the VGG19 network (configuration E) in Simonyan and Zisserman (2014)’s paper.",
      "startOffset" : 132,
      "endOffset" : 162
    }, {
      "referenceID" : 41,
      "context" : "The original Flickr30k data set contains 30k images and 5 English sentence descriptions for each image (Young et al., 2014).",
      "startOffset" : 103,
      "endOffset" : 123
    }, {
      "referenceID" : 13,
      "context" : "use the translated and the comparable Multi30k datasets (Elliott et al., 2016), henceforth referred to as M30kT and M30kC, respectively, which are multilingual expansions of the original Flickr30k.",
      "startOffset" : 56,
      "endOffset" : 78
    }, {
      "referenceID" : 2,
      "context" : "Our decoder RNN also uses GRU and is a neural LM (Bengio et al., 2003) conditioned on its previous emissions and the source sentence by means of the source attention mechanism.",
      "startOffset" : 49,
      "endOffset" : 70
    }, {
      "referenceID" : 30,
      "context" : "Image features are obtained by feeding images to the pre-trained VGG19 network of Simonyan and Zisserman (2014) and using the ac-",
      "startOffset" : 82,
      "endOffset" : 112
    }, {
      "referenceID" : 15,
      "context" : "We follow Gal and Ghahramani (2016) and apply dropout to the encoder bidirectional RNN and decoder RNN using the same mask in all time steps.",
      "startOffset" : 10,
      "endOffset" : 36
    }, {
      "referenceID" : 42,
      "context" : "Our models are trained using stochastic gradient descent with Adadelta (Zeiler, 2012) and minibatches of size 40, where each training instance",
      "startOffset" : 71,
      "endOffset" : 85
    }, {
      "referenceID" : 9,
      "context" : ", 2002), METEOR (Denkowski and Lavie, 2014), TER (Snover et al.",
      "startOffset" : 16,
      "endOffset" : 43
    }, {
      "referenceID" : 31,
      "context" : ", 2002), METEOR (Denkowski and Lavie, 2014), TER (Snover et al., 2006), and chrF3 scores5 (Popović, 2015) and we report statistical significance for the three first metrics using approximate randomisation computed with MultEval (Clark et al.",
      "startOffset" : 49,
      "endOffset" : 70
    }, {
      "referenceID" : 28,
      "context" : ", 2006), and chrF3 scores5 (Popović, 2015) and we report statistical significance for the three first metrics using approximate randomisation computed with MultEval (Clark et al.",
      "startOffset" : 27,
      "endOffset" : 42
    }, {
      "referenceID" : 8,
      "context" : ", 2006), and chrF3 scores5 (Popović, 2015) and we report statistical significance for the three first metrics using approximate randomisation computed with MultEval (Clark et al., 2011).",
      "startOffset" : 165,
      "endOffset" : 185
    }, {
      "referenceID" : 21,
      "context" : "better) and TER scores (lower is better) on the M30kT test set for the two text-only baselines PBSMT and NMT, the two multi-modal NMT models by Huang et al. (2016) and our MNMT models that: (i) use images as words in the source sentence",
      "startOffset" : 144,
      "endOffset" : 164
    }, {
      "referenceID" : 25,
      "context" : "The LM is a 5–gram LM with modified KneserNey smoothing (Kneser and Ney, 1995) trained on",
      "startOffset" : 56,
      "endOffset" : 78
    }, {
      "referenceID" : 21,
      "context" : "Our third baseline is the best comparable multi-modal model by Huang et al. (2016) and also their best",
      "startOffset" : 63,
      "endOffset" : 83
    }, {
      "referenceID" : 13,
      "context" : "Overall, it is a small dataset with a small vocabulary whose sentences have simple syntactic structures and not much ambiguity (Elliott et al., 2016).",
      "startOffset" : 127,
      "endOffset" : 149
    }, {
      "referenceID" : 21,
      "context" : "In fact, model IMG2W+D still improves on the multimodal baseline of Huang et al. (2016) when trained on the same data.",
      "startOffset" : 68,
      "endOffset" : 88
    }, {
      "referenceID" : 34,
      "context" : "That is also a noteworthy finding, since chrF3 is the only character-level metric we use, and it has shown a high correlation with human judgements (Stanojević et al., 2015).",
      "startOffset" : 148,
      "endOffset" : 173
    }, {
      "referenceID" : 21,
      "context" : "We note that the idea of using images as words in the source sentence, also entertained by Huang et al. (2016), does not perform as well as directly using the images in the encoder or decoder initialisation.",
      "startOffset" : 91,
      "endOffset" : 111
    } ],
    "year" : 2017,
    "abstractText" : "We introduce multi-modal, attentionbased neural machine translation (NMT) models which incorporate visual features into different parts of both the encoder and the decoder. We utilise global image features extracted using a pre-trained convolutional neural network and incorporate them (i) as words in the source sentence, (ii) to initialise the encoder hidden state, and (iii) as additional data to initialise the decoder hidden state. In our experiments, we evaluate how these different strategies to incorporate global image features compare and which ones perform best. We also study the impact that adding synthetic multi-modal, multilingual data brings and find that the additional data have a positive impact on multi-modal models. We report new state-of-the-art results and our best models also significantly improve on a comparable phrase-based Statistical MT (PBSMT) model trained on the Multi30k data set according to all metrics evaluated. To the best of our knowledge, it is the first time a purely neural model significantly improves over a PBSMT model on all metrics evaluated on this data set.",
    "creator" : "LaTeX with hyperref package"
  }
}