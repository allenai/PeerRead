{
  "name" : "1202.3759.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Compressed Inference for Probabilistic Sequential Models",
    "authors" : [ "Gungor Polatkan", "Oncel Tuzel" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Hidden Markov models (HMMs) and conditional random fields (CRFs) are two popular techniques for modeling sequential data. Inference algorithms designed over CRFs and HMMs allow estimation of the state sequence given the observations. In several applications, estimation of the state sequence is not the end goal; instead the goal is to compute some function of it. In such scenarios, estimating the state sequence by conventional inference techniques, followed by computing the functional mapping from the estimate is not necessarily optimal. A more formal approach is to directly infer the final outcome from the observations. In particular, we consider the specific instantiation of the problem where the goal is to find the state trajectories without exact transition points and derive a novel polynomial time inference algorithm that outperforms vanilla inference techniques. We show that this particular problem arises commonly in many disparate applications and present experiments on three of them: (1) Toy robot tracking; (2) Single stroke character recognition; (3) Handwritten word recognition."
    }, {
      "heading" : "1 Introduction",
      "text" : "Assigning labels to sequential data is a common problem extensively studied in several application domains such as computer vision and computational linguistics (Rabiner (1989), Lafferty et al. (2001), Quattoni et al. (2004)). For instance, in part-of-speech tagging, the problem is to tag parts of speech by considering the grammatical structure of the language, e.g., (verb verb noun noun verb adjective) is a very unlikely sequence in English. Likewise, one can assign letters to a sequence of images of hand-written characters by again exploiting the structure enforced by the grammar of that language. In these examples, sequential\npatterns are important and they can be used to extract information from massive data sets.\nTwo common models for solving such problems are hidden Markov models (HMMs), and conditional random fields (CRFs, often as a linear-chain). These models have been extended in various forms to adapt to different types of problems. For example, semi-Markovian CRFs are introduced as a solution to segmentation problem allowing non-Markovian transitions in segments and assigning direct labels not to individual samples but to overall segments (Sarawagi & Cohen (2004)). Fox et al. (2008) proposes a non-parametric prior for systems with state persistence to prevent unrealistically many transitions. This method not only provides state persistence, but also allows learning the transition probabilities in an infinite state space.\nIn these examples, the inference algorithm estimates the state sequence. But in several applications, this is not the end goal; instead the goal is to compute some function of the state sequence. In particular, we consider a frequently occurring form of this problem, compressed inference, where the function compress just keeps track of the state transitions without keeping track of the dwell times at each state and exact transition points.\nA simple example is the detection of actions in the movement of a human subject where the exact transition points between states like “sitting(s)”, “jumping(j)”, “walking(w)”, and “running(r)” are ambiguous and not important, but the detection of unique actions and the order of appearance is significantly important. For example, let the observation sequence be x = {x1, x2, ....x9} and the corresponding true state sequence be y = {s, s, j, j, j, w, w, r, r}. The end goal is the accurate prediction of the output of the function compress(y) given the observation sequence x, where compress(y) = {s, j, w, r} in this case. When a prediction y is {s, s, j, j, j, j, w, r, r} (which is acquired by converting a state ‘w’ to ‘j’ exactly at the transition from j to w), it is an error for conventional applications, but it is not an error for this application, since compress(y) = compress(y). Inversely, when a prediction y is {s, s, j, j, w, j, w, r, r},\nit is a fatal error for this application, even though it only differs from the ground truth sequence y by one state.\nIn contrast to standard sequence labeling problem, the length of the compressed output is also unknown, e.g., it is unclear how many unique actions occurred in the order of appearance during the movement of a human subject. More precisely, although y has the same length with x, the length of compress(y) is not known before hand, which might take values from 1 (means there are no state transitions) to the length of the sequence x. Therefore the inference algorithm should estimate the length of the compressed sequence in conjunction with the states.\nTo the best of our knowledge, this is a problem largely unaddressed in machine learning. In this paper, we present a polynomial time algorithm to directly infer the length and the states of the compressed sequence using marginalization over all state sequences. The experiments show that the proposed inference algorithm consistently outperforms standard inference techniques using the same training model.\nOne important application domain that benefits from compressed inference is the labeling of sequence data. The conventional approach for sequence classification applications such as action recognition, gesture recognition, etc. requires training a separate sequence model for each action/event class. The classification task then assigns the new observations to the action/event classes according to the observation likelihoods among the trained models. This approach becomes unpractical or even infeasible when the number of action/event classes is very large such as recognizing all the words in a language. In such cases training a single sequence model and classifying the sequence data according to the unique states decoded is a feasible approach which can be obtained via compressed labeling."
    }, {
      "heading" : "2 Background: Conditional Random Fields and Inference Techniques",
      "text" : "The sequence labeling problem can be formulated as finding the best function f that can predict y = f(x), given N training sequences {(xi, yi)}Ni=1, where xi = xi,1, xi,2, ....., xi,Ti is the observation sequence and yi = yi,1, yi,2, ....., yi,Ti is the label sequence. Linear-chain CRFs and HMMs are two probabilistic models targeting this problem. Linear-chain CRFs can be thought as conditional HMMs, or HMMs can be thought as a special case of CRFs with a particular choice of feature function. While we present algorithms and results for CRFs, they are equally applicable to HMMs without loss of generality. We use linear-chain CRFs as our base learners throughout this paper. Next, we review CRFs and conventional inference techniques."
    }, {
      "heading" : "2.1 Conditional Random Fields(CRFs)",
      "text" : "In a linear chain conditional random field of Lafferty et al. (2001), the conditional distribution is modeled as\np(y|x) = 1 Z(x)\nT\nt=1\nΨ(yt, yt−1, xt), (1)\nΨ(yt, yt−1, xt) = exp\n  \nj\nλjgj(yt−1, yt, xt) (2)\n+ \nk\nµkuk(yt, xt)  .\nwhere Ψ(yt, yt−1, xt) is called the potential function; gj(yt−1, yt, xt) is called the transition feature function from state yt−1 to yt; uk(yt, xt) is called the state feature function at state yt; λj and µk are the parameters estimated at the learning process, and Z(x) is the normalization factor as a function of the observation sequence. In this paper, we assume that the trained model is given, and refer readers to Sutton & McCallum (2006) and Altun et al. (2003) for detailed discussions on learning model parameters."
    }, {
      "heading" : "2.2 Vanilla Inference Techniques",
      "text" : "Here, we present a brief overview of conventional inference algorithms on probabilistic sequential models. One way of labeling a test sequence is the most likely labeling using the joint density y∗ = arg maxy p(y|x). The solution can be efficiently computed via Viterbi algorithm using recursion δt(j) = maxiΨ(j, i, xt)δt−1(i), which propagates the most likely path based on the max product rule (Sutton & McCallum (2006)). However, in many applications, accurately predicting the whole label sequence is very difficult so that individual predictions are used. This is achieved via predicting yt from the marginal distribution p(yt|x) using a similar dynamic programming procedure, forward-backward. The forward recursion is given by αt(j) =  iΨ(j, i, xt)αt−1(i), where αt(j) are the forward variables, and the backward recursion is given by βt(i) =  j Ψ(j, i, xt+1)βt+1(j), where βt(i) are the backward variables. The marginal probabilities can be computed by using these variables as given in Sutton & McCallum (2006).\nIn Culotta & McCallum (2004) and Kristjansson et al. (2004), a constrained forward algorithm is used to compute the confidence of a particular state sequence. The approach is to restrain the forward recursion to the constrained state sequence. In other words, given a set of constraints Y  = {yq...yr}, a modified forward algorithm is used to compute the probability of any sequence satisfying Y . The modified forward recursion is given as\nαt(j) =  \niΨ(j, i, xt)αt−1(i) for j  yt+1 0 otherwise (3)\nfor all yt+1 ∈ Y , where the operator j  yt+1 is defined in Culotta & McCallum (2004) as “j conforms to constraint yt+1”. At time T , the confidence of a specific constraint is given as Z /Z where the constrained lattice factor Z  =  i αT (i) and the unconstrained lattice fac-\ntor Z = \ni αT (i) are computed by using constrained forward variables and unconstrained forward variables, respectively. When only a single constraint yt is included in the constraint set Y , the method outputs the marginal distribution p(yt|x). Note that our algorithm for compressed inference is based on a similar idea."
    }, {
      "heading" : "3 Compressed Inference",
      "text" : "In this section, we present the core of our inference algorithm designed to solve the compressed labeling problem. We first define a new sequence s = compress(y), e.g., if y = {s, s, j, j, j, w, w, r, r}, then s = compress(y) = {s, j, w, r}. From now on, we use the symbol C to represent the function compress. The goal of compressed inference is to predict s given the observation x. Throughout this paper, we use a traditionally trained Markov model (e.g. a linear chain CRF).\nNext, we construct the mathematical framework for computing p(s|x). The overall joint density p(s|x) can be computed from p(y|x) by using the fundamental rules of probability theory.\nProposition 3.1. Let y be a random variable taking values in E and s be a variable taking values in F. Then, the function C : E −→ F is measurable and s is a random variable. Moreover, the conditional probability of s can be computed by the conditional probability of y as follows:\np(s = s0|x) = \n∀y:C (y)=s0\np(y|x). (4)\nProof. A measure theoretic proof is given in the appendix.\nIn a more verbal way, if one would like to compute p(s = s0|x), brute force approach is to find the set Y  = C−1(s0), whose elements are all y sequences with compressed values s0, and then to compute the cumulative probability of Y . Since we are working on discrete states, we just use summations in order to compute this probability.\nThis operation is a marginalization over all segmentations, y, whose compressed values are s0. Unfortunately, given a model, though the computation of p(y|x) by using the methods in Section 2.2 is efficient, the final summation in equation (4) includes exponentially many operations (MT , where M is the number of states and T is the length of y), which is intractable. Next, we propose a novel polynomial time algorithm using dynamic programming."
    }, {
      "heading" : "3.1 Compressed Inference Algorithm",
      "text" : "In this section, we first present solutions to three subproblems of compressed inference and then we perform compressed labeling using these techniques. Finally, we analyze the complexity of the proposed algorithm.\nThe first subproblem is computing the probability of a given compressed signal s0. In the second subproblem, we generalize this result and derive a dynamic programming algorithm to compute the probability distribution of the length of the compressed sequence. The third subproblem is computing the marginal probabilities of the compressed states. Finally, we perform labeling in the compressed domain by first finding the length of the compressed sequence from the distribution found in second subproblem, and then finding the compressed states from the marginal probabilities found in the third subproblem.\nSubproblem 1: Computing the probability of a compressed sequence s0, p(s = s0|x). Here, we present a dynamic programming technique to compute this probability. This construction will help us in deriving the algorithm for the unknown s case which are explored by second and third subproblems.\nLet c = |s0| be the length of s0. For ease of notation, we refer to individual terms of s0 with si, i = 1 . . . c. We define new forward variables αt(i), i = 1 . . . c, which keeps track of making exactly i− 1 transitions on sequence s0 up\nto time t. From time instance t − 1 to t, the forward variables are updated based on: (1) Staying at the same state on observation xt, which is shown with blue arrows in Figure 1(a); (2) Making a transition from si to si+1, which is shown with orange arrows in Figure 1(a). The recursion is as follows:\nProposition 3.2. The probability of a sequence s0, p(s = s0|x), is given by\np(s = s0|x) = \n∀y: C (y)=s0\np(y|x) ∝ \n∀y: C (y)=s0\nT\nt=1\nΨ(yt, yt−1, xt)\n(5)\nwhich can be computed by the recursion\nαt(i) = Ψ(si, si−1, xt)αt−1(i− 1) +Ψ(si, si, xt)αt−1(i), (6)\nwhere i = 1 . . . c. At time T , we attain\nαT (c) = \n∀y:C (y)=s0\nT\nt=1\nΨ(yt, yt−1, xt). (7)\nProof. See the appendix.\nBy this recursion we compute the lattice factor Z(s0) = αT (c). We will explain the way to compute the overall normalization factor Z later in this section which will convert Z(s0) to a probability by p(s = s0|x) = Z(s0)/Z.\nSubproblem 2: Computing the probability distribution of the length of the compressed sequence, p(c|x). Given observations x, the first step of finding the compressed labels s is to find the length c of s, where c can take values from 1 (which means there is no state transition) up to the sequence length T (which means there is a transition at every single time step). Note that, for all c0 > T , p(c = c0|x) is trivially zero.\nLet Si be the set of all compressed state sequences of length i, i.e., Si = {s : |s| = i}, i = 1...T . It is obvious that Si \nSj = ∅ for i = j. Then, the probability p(c = c0|x) can be written as\np(c = c0|x) = p(s ∈ Sc0 |x) = \n∀s: |s|=c0\np(s = s|x)\n∝\n\n∀s:|s|=c0\n\n∀y:C (y)=s\nT\nt=1\nΨ(yt, yt−1, xt). (8)\nWe first note that p(s = s0|x) gives the probability of one possible s0 of length c. Suppose we have two such signals: s1 and s2 as shown in Figure 1(b). Then, p(s = s1|x) + p(s = s2|x) ∝ αT (c)s1 + αT (c)s2 , where αT (c)si\nmeans forward recursion was run for si. However, these two signals are different at only one point in compressed domain. To be able to represent them on the same lattice and avoid multiple calculations, we extend the vector representation of αt into a table, as shown in Figure 1(b). We note that conventional forward variable αt was M dimensional, previous αt was c dimensional, and new α̂t is c×M dimensional.\nThis new representation requires the signal to traverse the table through certain cells. From now on we call these cells “constraints”. Let the set of all constraints on the lattice be Q = {..., ql−1, ql, ql+1...}, where each constraint ql is a tuple of the coordinates of the nonzero entries on the table. For example, for a particular compressed sequence s0, it corresponds to {(1, s1), (2, s2) . . . (c, sc)}, and for a particular set Si, it corresponds to all coordinates of the table with height i which is denoted by QSi . The recursion for a given constraint set Q is as follows:\nα̂t(i, j)Q =\n \n\n  Ψ(j, j, xt)α̂t−1(i, j)Q+ ∀k:k =j  Ψ(j, k, xt)\n×α̂t−1(i− 1, k)Q) \n\n if(i, j) ∈ Q\n0 otherwise. (9)\nThis recursion propagates through all colored nodes (which correspond to nonzero entries) on the table in Figure 1(b) and ignores all empty nodes since they are not included in Q. For simplicity, self loops are removed from Figure 1(b) . Moreover, the recursion for which the constraints included all the locations of the table (all entries on the table can be visited) is explained schematically in Figure 1(c) from t−1 to t and from t to t + 1.\nThe recursion given in equation (9) computes the probability of all compressed sequences which are defined by the set Q via\np(Q|x) ∝ Z(Q) = \nj\nα̂T (c0, j)Q.1 (10)\nUsing constraint set notation QSc0 , the probability of p(c = c0|x) can be written as\np(c = c0|x) ∝ \n∀s: |s|=c0\n\n∀y: C (y)=s\nT\nt=1\nΨ(yt, yt−1, xt)\n= Z(QSc0 ) = \nj\nα̂T (c0, j)QSc0 . (11)\nThis corresponds to running the recursion in equation (9) with constraint set QSc0 and summing the entries at row c0. As we discussed before, p(c = c0|x) = 0 when c0 > T or\n1The proof of this proportionality is just a generalization of the proof of Proposition (3.2) as given in the supplementary material.\nc0 < 1. If we employ this procedure for the constraint set QST , the row sums of the table α̂T (i, j)QST produces all the lattice factors Z(QSi), i = 1 . . . T simultaneously. The overall summation of this table is equal to the normalizing factor Z which is necessary for computing p(s = s0|x) = Z(s0)/Z and p(c = c0|x) = Z(QSc0 )/Z. This identity follows from the fact that Z is equal to the summation of the lattice factors for all possible lengths and combinations of s.\nSubproblem 3: Computing the marginal probabilities of the compressed state sequences, p(si = j|x, c). To compute the marginal distribution p(si = j|x, c), we construct the constraint set Qi,j by including all the entries of the table with height c except the ones at row i. Then we add (i, j) to this set. This particular constraint set configuration includes all the possible compressed sequence configurations with length c and si = j. Then, the marginal probability is computed by p(si = j|x, c) = Z(Qi,j)/  j Z(Qi,j).\nCompressed labeling. The compressed labeling is a simple application of the aforementioned methods: (1) Estimate c by ĉ = arg maxc0 p(c = c0|x); (2) Estimate si by si = arg maxj p(si = j|x, ĉ).\nComputational Complexity. The time complexity of the Viterbi algorithm is O(TM2) and space complexity is O(TM) where T is the length of the sequence and M is the number of states. The new recursion propagates on a two dimensional table which requires O(cmaxTM2) time and O(cmaxTM) space where cmax is the maximum feasible length of the compressed sequence. Although the maximum possible value of cmax is equal to T , in general cmax  T ."
    }, {
      "heading" : "4 Experiments",
      "text" : "Experiments are performed on three different applications: toy robot tracking, single stroke character recognition and handwritten word recognition. In all experiments, we use both marginal (using forward-backward) and joint posterior (using Viterbi) estimates as input to the compress function C to obtain smarginal and sjoint as final decisions for the baseline predictions of s. We compare these baseline predictions with the proposed compressed inference algorithm described in Section 3.1. We used the hCRF package (Morency (2007)) with the BFGS optimizer to learn the CRF model parameters. In addition, we present comparison with the semi-Markov CRF model in the first two problems where the training and inference are performed using the semi-Markov CRF package (Sarawagi (2009)). The semi-Markov CRF model produces segmentation of the input sequence where we discard the transition points and the labels of the segments give the compressed sequence.\nMetrics. We consider two evaluation metrics which we call Exact score and Edit Distance Score (EDS). Exact score is used in order to check the perfect match of the compressed predictions to the truth. If we miss even one state in s or C (y), this is a fatal error and we consider this sequence as missed during the evaluation. Hence, both the compressed length of the prediction and exact compressed state sequence should perfectly match with the truth. In EDS, we measure how well we are performing state by state. We use edit distance between two strings of characters which is defined as the minimum number of operations required to convert one to the other. While computing EDS, in order to prevent the effect of the length of the sequences, we normalize EditDistance(prediction, truth) of each sequence by maxLength{prediction, truth}. Then, we sum the normalized distances of all sequences and finally we find EDS = 100− 100 ∗ sum/(# of sequences).\nToy Robot Tracking. Sequential models are frequently used in robot tracking applications, where a robot is moving in a small grid-based environment to discover the world (Baltzakis & Trahanias (2002)). If one is interested in finding the visited locations only, the unique grids that the robot goes through is important but the self-loops and the exact transition times/boundaries are not important. For instance, our robot moves in a small world as seen in Figure 2(a), where transition possible grids are {blue, green, yellow, red} colored and obstacles, which prevent motion are {black}. At every step, the robot attempts to move {up, down, left, right} based on a random choice of direction. If there is a block towards the intended direction, it tries again. In this problem, state refers to the location of the robot in (x:y) coordinates, and observation refers to an observed color which is the output of a sensor (a camera for instance) with a color detection accuracy of P%.\nWe generated 400 training and 400 test sequences at each of 6 different P values ({100, 90, 80, 70, 60, 50}) by simulating the robot in the small world as defined in Figure 2(a). The sequence lengths are in the range of 100 - 300 and the compressed lengths are in the range of 5 - 15. In terms of exact score, our algorithm introduces an improvement when noise level is high or moderate (Figure 2(c)). But when noise level is very low, we observe that joint estimate performs better. We note that for very long state sequences (as used in this experiment), the exact score is not very reliable since a single state error causes a sequence to be labeled incorrect which drastically changes final outcome. In terms of EDS, as we can see in Figure 2(d), our state inference algorithm performs better than vanilla inference algorithms at high and moderate noise and at very low noise performance is similar.\nThe predictions of the semi-Markov CRF model are significantly inferior compared to the proposed compressed inference algorithm: the EDS based scores are %{45.4,\n40.7, 32.2, 15.5, 5, 4.5 } where as the Exact scores are %{10.5, 3.25, 2, 1, 0, 0 } for P values of {100, 90, 80, 70, 60, 50} respectively. We attribute the poor performance of this model to the large number of maximum segmentation lengths. Likewise, the training and the inference of semiMarkov CRF models are extremely slow compared to the original CRF model (and compressed inference). The training takes 100 hours compared to 1-2 hours for the original CRF model. Please see Section 5 for a more detailed discussion.\nCharacter Recognition. In this experiment, we apply compressed inference to single stroke character recognition application (Ozun et al. (2001)). The problem is to recognize the shape drawn on a touch screen. It is assumed that the drawing operation is performed by a single stroke. One well known alphabet which has this property is the GraffitiTM.\nIn this application, state refers to directions {up,right,down,left} and observation refers to quantized angles between successive points acquired from the user interface as shown in Figure 3(a) and 3(b). Existing systems use stochastic finite state machines (FSM) or HMMs for this purpose (Ozun et al. (2001)). Usually, an HMM or FSM is trained for each single character, then, one class is chosen by a likelihood test. One drawback of this method is its limited capability of handling arbitrary shapes. One can train a single model for all characters and decode the states by using this single model as well. Nevertheless, a single state error can spoil the whole prediction in such a setting. Hence, we need a strong inference scheme which is robust to noise. Moreover, the ambiguity in state transitions is an important obstacle as well, since passing from one state to another is generally ambiguous. Our approach is robust to all these issues since it does not spend effort to estimate the exact transition locations but only produces transition sequences which is sufficient for this task.\nWe generated a data set of 20 training and 20 test sam-\nples for each character from the set {0,1,2,3,4,5,6,7,8,9} by using a computer user interface as shown in Figure 3(a). As shown in Table 3(d), compressed inference outperforms conventional inference. It especially performs better on characters like 0 and 3 which are the ones with the most ambiguous state transitions.\nThe results of the semi-CRFs match that of the joint estimate, %60 for the exact and %87 for the EDS based score, whereas the compressed inference algorithm significantly outperforms both, %95.5 and %98.97 respectively. Particularly, this problem is a good example where the transition boundaries are ambiguous. The semi-Markov CRFs force a segmentation over ambiguous boundaries whereas our model benefits from marginalization over all possible segmentations.\nHandwritten Word Recognition. In this application, we use the data set in Taskar et al. (2003), which includes 16 × 8 size characters from the English alphabet. In the literature, handwritten word recognition is generally performed by first segmenting the characters and then recognizing them by multi class classification such as SVMs. In many studies, the structure of language is used as well (Taskar et al. (2003)). However, in all these works, the characters are already segmented during pre-processing step. Our setup is more challenging compared to these studies, i.e., the characters are not segmented and as a result the lengths of the sequences are not known as well.\nFor experimental purposes, we specified 20 names used in English {jake, conor, taner, wyat, cody, dustin, luke, jack, scot, logan, deshawn, deandre, marquis, darnel, terel, malik, reginald, tyrone, wilie, dominique} where none of them including any successive letters such as ‘nn’, etc. (Application can be generalized to arbitrary number of words and currently we do not consider words with successive letters which might be handled specially). Each word is written by different combinations of styles, 20 times for training set and 20 times for test set. Moreover, each time step corresponds to one vertical column of the image of handwriting,\nstates refer to the corresponding character of the time step, and observations correspond to the shape context features of Belongie et al. (2001).\nFor feature extraction, we first take overlapping 16 × 7 patches from the word image by sliding window technique. Next, we apply shape context descriptor of Belongie et al. (2001) for each data point on the corresponding patch as shown in Figure 4(d). Then, we apply K-means clustering to learn a dictionary. In our experiments, K = 50 produced the best performance. Next, we generate histograms via vector quantization, which completes the feature extraction.\nIn the experimental results, we observe improvements in both scores compared to vanilla inference techniques, as can be seen in Figure 4(c) and Table 4(e)."
    }, {
      "heading" : "5 Related Work",
      "text" : "Compressed labeling of sequences is mentioned as a videointerpretation application in Fern & Givan (2004), where we get the function name “compress” from. Their main focus was to model a sequence problem that had an enormous number of states which was not known before hand. Due to the unknown number of states the conventional probabilistic models can not be used, hence the approach is applicable to a limited domain. In contrast our inference scheme uses a standard CRF or HMM and we do not make any assumption other than usual sequential modeling assumptions. Therefore the presented algorithm is applicable to any problem where distinct states are important and state transitions are ambiguous.\nA simple transition-cost model is proposed in Fern (2005) for video interpretation where self transition is assumed to have no cost whereas all other possible transitions are assumed to have the same cost K. This is similar to training a probabilistic sequential model which has zero weights for all self transition parameters and same K as the weight for all other transitions. However, this ad-hoc assumption is unrealistic for many applications.\nSegmentation is the process of identifying the boundaries between segments (e.g., words in natural language processing, set of pixels in image processing). The output of segmentation is a set of segments with exact boundary locations (e.g. super-pixels with contours in images or state trajectories with exact state transition points in sequential models). The Semi-Markovian approach of Sarawagi & Cohen (2004) proposes a solution to the segmentation problem. Semi-Markov models explicitly model duration in a state with different distributions (which violates the Markovian assumption) and are widely used when one is interested in exact transition points and segmentation boundaries. In contrast to Sarawagi & Cohen (2004), our inference algorithm is designed for Markov models and benefits from ambiguities in segmentation boundaries via marginalizing over all boundary locations.\nThe problems where semi-Markov CRF models were shown to be successful, such as name entity recognition (Sarawagi & Cohen (2004)), have relatively short maximum segmentation lengths (around 3-4). In addition, in such problems the segmentation boundaries are very well defined. The experiments in this paper show that when the maximum segmentation length is large (in most of our applications it ranges from 50 to 100) or the transition boundaries are ambiguous, the compressed inference algorithm significantly outperforms semi-Markov CRFs. Moreover, the training time of the semi-Markov model is also prohibitive when the maximum segmentation length is large, which is another motivation for using a Markov model."
    }, {
      "heading" : "6 Discussion",
      "text" : "Maximum likelihood or marginal estimate of a full state sequence is the standard approach for inference on Markov models. In this paper, we have shown that when the problem is finding the state trajectories only, without exact transition points, inference can be done in a more accurate way. To directly infer the unique states, we have proposed marginalization over possible transitions and derived a polynomial time algorithm. In three different applica-\ntion domains, we have shown that the proposed compressed labeling algorithm outperforms vanilla techniques particularly when there is state transition ambiguity.\nThe proposed construction also offers significant potential for future research. Here, we have proposed polynomial time inference using marginal probabilities given the prediction for the length of the signal. Inference of the most likely joint compressed sequence is still an open problem which is unlikely to be polynomial time computable."
    }, {
      "heading" : "A Appendix",
      "text" : "Proof of Proposition 3.1\nProof. Let (Ω,H, P) be a probability space where Ω is a set, H ( whose elements are called the events) is a σ−algebra on Ω, and P is a probability measure on (Ω,H). Let (E,E) and (F,F) be measurable spaces, where E and F are two sets, and E and F are σ−algebras on E and F, respectively. Let y be a random variable taking values in (E,E). Next, we analyse the measurability of C , which will be useful while showing that C (y) is a random variable. Next, we define the inverse of a mapping: 1\nDefinition A function h from E to F, i.e. h : E −→ F, is a mapping which takes each y in E and assigns to an element h(y) in F. For any subset A in F, the inverse image of A under h is defined as\nh −1 A = {y ∈ E : h(y) ∈ A} (12)\nProposition A.1. The function C : E −→ F is measurable relative to E and F.\nProof. For every A ∈ F, C−1A is the set of all y whose compressed values are in A, where A =  i{s = si}. That is to say,\nC−1A = C−1 \ni\n{s = si} = \ni\nC−1{s = si}\n= \ni\n{y ∈ E : C (y) = si}.\nSince {y ∈ E : C (y) = si} ∈ E for all i, it is obvious that i{y ∈ E : C (y) = si} ∈ E, because a σ-algebra is closed under union. As a result, C−1A ∈ E for every A in F which completes the proof by definition of measurability.\nProposition A.2. The variable defined by s = C (y), that is to say,\ns(ω) = C ◦ y(ω) = C (y(w)), where ω ∈ Ω, (13)\nis a random variable taking values in (F,F).\nProof. By Proposition (A.1), C is measurable. Since measurable functions of measurable functions (y is a random variable) are measurable, we are done.\nNow, we know that y and s are random variables. Next question is how to find the distribution of s. We observe that if φ is the distribution of y, then the distribution κ of s is κ = φ ◦ C−1, in other words,\nκ(A) = P{s ∈ A} = P{y ∈ C−1A} = φ(C−1A), A ∈ F, 1The proofs are given for one dimensional random variable y, however, it is trivial to extend them to T-tuple variable y.\nwhere φ(C−1A) corresponds to taking the integral of the set C−1A with respect to the measure φ. In a more verbal way, if one would like to compute p(s = s0|x), where s0 is in A, brute force approach is to find the set Y  = C−1(s0), whose elements are all y sequences with compressed values s0, and then to take the integral with respect to φ. Since we are working on discrete states, integrals are converted to summations:\np(s = s0|x) = \n∀y:C (y)=s0\np(y|x). (14)\nProof of Proposition 3.2\nProof. Without loss of generality, let s0,1 = 1, s0,2 = 2, . . . s0,c = c and let t1, t2 through tc−1 are the state transition times, i.e., t1 is the transition from s0,1 = 1 to s0,2 = 2.\np(s = s0|x) (15)\n= \n∀y:C (y)=s0\np(y|x) (16)\n∝\n\n∀y:C (y)=s0\nT\nt=1\nΨ(yt, yt−1, xt) (17)\n= \n0<t1<t2<...tc−1≤T\n t1−1\nt=1\nΨ(1, 1, xt)  Ψ(2, 1, xt1)\n(18)  t2−1\nt=t1+1\nΨ(2, 2, xt)  Ψ(3, 2, xt2)\n t3−1\nt=t2+1\nΨ(3, 3, xt)  Ψ(4, 3, xt3)...\nΨ(c, c− 1, xtc−1)  T\nt=tc−1+1\nΨ(c, c, xt) \n= Ψ(c, c− 1, xT )\n \n0<t1<t2<...tc−2≤T−1\n t1−1\nt=1\nΨ(1, 1, xt) \n(19)\nΨ(2, 1, xt1)  t2−1\nt=t1+1\nΨ(2, 2, xt)  Ψ(3, 2, xt2) · · ·\nΨ(c− 1, c− 2, xtc−2)  T−1\nt=tc−2+1\nΨ(c− 1, c− 1, xt) \n+Ψ(c, c, xT )\n \n0<t1<t2<...tc−1≤T−1\n t1−1\nt=1\nΨ(1, 1, xt) \n(20)\nΨ(2, 1, xt1)  t2−1\nt=t1+1\nΨ(2, 2, xt)  Ψ(3, 2, xt2) · · ·\nΨ(c, c− 1, xtc−1)  T−1\nt=tc−1+1\nΨ(c, c, xt) \n(21)\nIn equation 18 we rewrite equation 17 by using the distributive law. In equation 19, we divide the summation into two cases by only factoring out time T : (1) First part considers the case in which there is a transition from c − 1 to c at time T ; (2) Second part considers no transition at time T , so transition from c−1 to c was before T and at time T the previous state c is repeated. Next, let’s define the forward variable for the s domain as αT (c) as:\nαT (c) = \n0<t1<t2<...tc−1≤T\n t1−1\nt=1\nΨ(1, 1, xt)  Ψ(2, 1, xt1)\n t2−1\nt=t1+1\nΨ(2, 2, xt)  Ψ(3, 2, xt2)\n t3−1\nt=t2+1\nΨ(3, 3, xt)  Ψ(4, 3, xt3) · · ·\nΨ(c, c− 1, xtc−1)  T\nt=tc−1+1\nΨ(c, c, xt) \n(22)\nThen it is obvious that two summations in equation 19 can be written in terms of these forward variables:\nαT−1(c− 1) =\n \n0<t1<t2<...tc−2≤T−1\n t1−1\nt=1\nΨ(1, 1, xt) \nΨ(2, 1, xt1)  t2−1\nt=t1+1\nΨ(2, 2, xt) \nΨ(3, 2, xt2) · · ·Ψ(c− 1, c− 2, xtc−2)  T−1\nt=tc−2+1\nΨ(c− 1, c− 1, xt) \n(23)\nαT−1(c) =\n \n0<t1<t2<...tc−1≤T−1\n t1−1\nt=1\nΨ(1, 1, xt) \nΨ(2, 1, xt1)  t2−1\nt=t1+1\nΨ(2, 2, xt)  Ψ(3, 2, xt2)...\nΨ(c, c− 1, xtc−1)  T−1\nt=tc−1+1\nΨ(c, c, xt) \n(24)\nFinally, we reach to the recursion formula:\nαT (c) = Ψ(c, c−1, xT )αT−1(c−1)+Ψ(c, c, xT )αT−1(c)\nThis proof is valid for all lengths c. In other words, we can think of the signal from 1 to c − 1 as our signal of interest and, at any such input, the recursion at time T can be written as\nαT (i) = Ψ(i, i− 1, xT )αT−1(i− 1)+Ψ(i, i, xT )αT−1(i)\nMoreover, this can be generalized to arbitrary t = 1.....T as well by recursing back in T. Thus, the final form is\nαt(i) = Ψ(i, i− 1, xt)αt−1(i− 1) +Ψ(i, i, xt)αt−1(i)"
    } ],
    "references" : [ {
      "title" : "Investigating Loss Functions and Optimization Methods for Discriminative Learning of Label Sequences, in ‘Proc. EMNLP",
      "author" : [ "Y. Altun", "M. Johnson", "T. Hofmann" ],
      "venue" : null,
      "citeRegEx" : "Altun et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Altun et al\\.",
      "year" : 2003
    }, {
      "title" : "Hybrid mobile robot localization using switching state-space models, in ‘ICRA",
      "author" : [ "H. Baltzakis", "P. Trahanias" ],
      "venue" : null,
      "citeRegEx" : "Baltzakis and Trahanias,? \\Q2002\\E",
      "shortCiteRegEx" : "Baltzakis and Trahanias",
      "year" : 2002
    }, {
      "title" : "Shape matching and object recognition using shape contexts",
      "author" : [ "S. Belongie", "J. Malik", "J. Puzicha" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "citeRegEx" : "Belongie et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Belongie et al\\.",
      "year" : 2001
    }, {
      "title" : "Confidence estimation for information extraction, in ‘Human Language Technology Conference and North American Chapter of the Association for Computational Linguistics (HLTNAACL)",
      "author" : [ "A. Culotta", "A. McCallum" ],
      "venue" : null,
      "citeRegEx" : "Culotta and McCallum,? \\Q2004\\E",
      "shortCiteRegEx" : "Culotta and McCallum",
      "year" : 2004
    }, {
      "title" : "A simple-transition model for relational sequences, in ‘IJCAI",
      "author" : [ "A. Fern" ],
      "venue" : null,
      "citeRegEx" : "Fern,? \\Q2005\\E",
      "shortCiteRegEx" : "Fern",
      "year" : 2005
    }, {
      "title" : "Relational sequential inference with reliable observations, in ‘Proc. of the International Conference on Machine Learning",
      "author" : [ "A. Fern", "R. Givan" ],
      "venue" : null,
      "citeRegEx" : "Fern and Givan,? \\Q2004\\E",
      "shortCiteRegEx" : "Fern and Givan",
      "year" : 2004
    }, {
      "title" : "An hdp-hmm for systems with state persistence, in ‘Advances in Neural Information Processing Systems",
      "author" : [ "E.B. Fox", "E.B. Sudderth", "M.I. Jordan", "A.S. Willsky" ],
      "venue" : null,
      "citeRegEx" : "Fox et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Fox et al\\.",
      "year" : 2008
    }, {
      "title" : "Interactive information extraction with constrained conditional random fields, in ‘AAAI",
      "author" : [ "T. Kristjansson", "A. Culotta", "P. Viola" ],
      "venue" : null,
      "citeRegEx" : "Kristjansson et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Kristjansson et al\\.",
      "year" : 2004
    }, {
      "title" : "Conditional random fields: Probabilistic models for segmenting and labeling sequence data, in ‘International Conference on Machine Learning",
      "author" : [ "J. Lafferty", "A. McCallum", "F. Pereira" ],
      "venue" : null,
      "citeRegEx" : "Lafferty et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Lafferty et al\\.",
      "year" : 2001
    }, {
      "title" : "HCRF library: Discriminative models for sequence labeling",
      "author" : [ "Morency", "L.-P" ],
      "venue" : null,
      "citeRegEx" : "Morency and L..P.,? \\Q2007\\E",
      "shortCiteRegEx" : "Morency and L..P.",
      "year" : 2007
    }, {
      "title" : "Vision based single stroke character recognition for wearable computing",
      "author" : [ "O. Ozun", "O.F. Ozer", "C.O. Tuzel", "V. Atalay", "A.E. Cetin" ],
      "venue" : "IEEE Intelligent Systems and Applications",
      "citeRegEx" : "Ozun et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Ozun et al\\.",
      "year" : 2001
    }, {
      "title" : "Conditional random fields for object recognition, in ‘Advances in Neural Information Processing Systems",
      "author" : [ "A. Quattoni", "M. Collins", "T. Darrell" ],
      "venue" : null,
      "citeRegEx" : "Quattoni et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Quattoni et al\\.",
      "year" : 2004
    }, {
      "title" : "A tutorial on hidden markov models and selected applications in speech recognition, in ‘Proceedings of the IEEE",
      "author" : [ "L.R. Rabiner" ],
      "venue" : null,
      "citeRegEx" : "Rabiner,? \\Q1989\\E",
      "shortCiteRegEx" : "Rabiner",
      "year" : 1989
    }, {
      "title" : "semi-CRF :a java implementation of conditional random fields for sequential labeling",
      "author" : [ "S. Sarawagi" ],
      "venue" : null,
      "citeRegEx" : "Sarawagi,? \\Q2009\\E",
      "shortCiteRegEx" : "Sarawagi",
      "year" : 2009
    }, {
      "title" : "Semi-markov conditional random fields for information extraction, in ‘Advances in Neural Information",
      "author" : [ "S. Sarawagi", "W.W. Cohen" ],
      "venue" : "Processing Systems’,",
      "citeRegEx" : "Sarawagi and Cohen,? \\Q2004\\E",
      "shortCiteRegEx" : "Sarawagi and Cohen",
      "year" : 2004
    }, {
      "title" : "Introduction to Conditional Random Fields for Relational Learning",
      "author" : [ "C. Sutton", "A. McCallum" ],
      "venue" : null,
      "citeRegEx" : "Sutton and McCallum,? \\Q2006\\E",
      "shortCiteRegEx" : "Sutton and McCallum",
      "year" : 2006
    }, {
      "title" : "Max-margin markov networks, in ‘Neural Information Processing Systems Conference",
      "author" : [ "B. Taskar", "C. Guestrin", "D. Koller" ],
      "venue" : null,
      "citeRegEx" : "Taskar et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Taskar et al\\.",
      "year" : 2003
    } ],
    "referenceMentions" : [ {
      "referenceID" : 9,
      "context" : "Assigning labels to sequential data is a common problem extensively studied in several application domains such as computer vision and computational linguistics (Rabiner (1989), Lafferty et al.",
      "startOffset" : 162,
      "endOffset" : 177
    }, {
      "referenceID" : 7,
      "context" : "Assigning labels to sequential data is a common problem extensively studied in several application domains such as computer vision and computational linguistics (Rabiner (1989), Lafferty et al. (2001), Quattoni et al.",
      "startOffset" : 178,
      "endOffset" : 201
    }, {
      "referenceID" : 7,
      "context" : "Assigning labels to sequential data is a common problem extensively studied in several application domains such as computer vision and computational linguistics (Rabiner (1989), Lafferty et al. (2001), Quattoni et al. (2004)).",
      "startOffset" : 178,
      "endOffset" : 225
    }, {
      "referenceID" : 7,
      "context" : "Assigning labels to sequential data is a common problem extensively studied in several application domains such as computer vision and computational linguistics (Rabiner (1989), Lafferty et al. (2001), Quattoni et al. (2004)). For instance, in part-of-speech tagging, the problem is to tag parts of speech by considering the grammatical structure of the language, e.g., (verb verb noun noun verb adjective) is a very unlikely sequence in English. Likewise, one can assign letters to a sequence of images of hand-written characters by again exploiting the structure enforced by the grammar of that language. In these examples, sequential patterns are important and they can be used to extract information from massive data sets. Two common models for solving such problems are hidden Markov models (HMMs), and conditional random fields (CRFs, often as a linear-chain). These models have been extended in various forms to adapt to different types of problems. For example, semi-Markovian CRFs are introduced as a solution to segmentation problem allowing non-Markovian transitions in segments and assigning direct labels not to individual samples but to overall segments (Sarawagi & Cohen (2004)).",
      "startOffset" : 178,
      "endOffset" : 1194
    }, {
      "referenceID" : 6,
      "context" : "Fox et al. (2008) proposes a non-parametric prior for systems with state persistence to prevent unrealistically many transitions.",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 8,
      "context" : "In a linear chain conditional random field of Lafferty et al. (2001), the conditional distribution is modeled as",
      "startOffset" : 46,
      "endOffset" : 69
    }, {
      "referenceID" : 0,
      "context" : "In this paper, we assume that the trained model is given, and refer readers to Sutton & McCallum (2006) and Altun et al. (2003) for detailed discussions on learning model parameters.",
      "startOffset" : 108,
      "endOffset" : 128
    }, {
      "referenceID" : 7,
      "context" : "In Culotta & McCallum (2004) and Kristjansson et al. (2004), a constrained forward algorithm is used to compute the confidence of a particular state sequence.",
      "startOffset" : 33,
      "endOffset" : 60
    }, {
      "referenceID" : 13,
      "context" : "In addition, we present comparison with the semi-Markov CRF model in the first two problems where the training and inference are performed using the semi-Markov CRF package (Sarawagi (2009)).",
      "startOffset" : 174,
      "endOffset" : 190
    }, {
      "referenceID" : 10,
      "context" : "In this experiment, we apply compressed inference to single stroke character recognition application (Ozun et al. (2001)).",
      "startOffset" : 102,
      "endOffset" : 121
    }, {
      "referenceID" : 10,
      "context" : "Existing systems use stochastic finite state machines (FSM) or HMMs for this purpose (Ozun et al. (2001)).",
      "startOffset" : 86,
      "endOffset" : 105
    }, {
      "referenceID" : 16,
      "context" : "In this application, we use the data set in Taskar et al. (2003), which includes 16 × 8 size characters from the English alphabet.",
      "startOffset" : 44,
      "endOffset" : 65
    }, {
      "referenceID" : 16,
      "context" : "In this application, we use the data set in Taskar et al. (2003), which includes 16 × 8 size characters from the English alphabet. In the literature, handwritten word recognition is generally performed by first segmenting the characters and then recognizing them by multi class classification such as SVMs. In many studies, the structure of language is used as well (Taskar et al. (2003)).",
      "startOffset" : 44,
      "endOffset" : 388
    }, {
      "referenceID" : 2,
      "context" : "states refer to the corresponding character of the time step, and observations correspond to the shape context features of Belongie et al. (2001). For feature extraction, we first take overlapping 16 × 7 patches from the word image by sliding window technique.",
      "startOffset" : 123,
      "endOffset" : 146
    }, {
      "referenceID" : 2,
      "context" : "states refer to the corresponding character of the time step, and observations correspond to the shape context features of Belongie et al. (2001). For feature extraction, we first take overlapping 16 × 7 patches from the word image by sliding window technique. Next, we apply shape context descriptor of Belongie et al. (2001) for each data point on the corresponding patch as shown in Figure 4(d).",
      "startOffset" : 123,
      "endOffset" : 327
    }, {
      "referenceID" : 4,
      "context" : "Compressed labeling of sequences is mentioned as a videointerpretation application in Fern & Givan (2004), where we get the function name “compress” from.",
      "startOffset" : 86,
      "endOffset" : 106
    }, {
      "referenceID" : 4,
      "context" : "Compressed labeling of sequences is mentioned as a videointerpretation application in Fern & Givan (2004), where we get the function name “compress” from. Their main focus was to model a sequence problem that had an enormous number of states which was not known before hand. Due to the unknown number of states the conventional probabilistic models can not be used, hence the approach is applicable to a limited domain. In contrast our inference scheme uses a standard CRF or HMM and we do not make any assumption other than usual sequential modeling assumptions. Therefore the presented algorithm is applicable to any problem where distinct states are important and state transitions are ambiguous. A simple transition-cost model is proposed in Fern (2005) for video interpretation where self transition is assumed to have no cost whereas all other possible transitions are assumed to have the same cost K.",
      "startOffset" : 86,
      "endOffset" : 758
    }, {
      "referenceID" : 4,
      "context" : "Compressed labeling of sequences is mentioned as a videointerpretation application in Fern & Givan (2004), where we get the function name “compress” from. Their main focus was to model a sequence problem that had an enormous number of states which was not known before hand. Due to the unknown number of states the conventional probabilistic models can not be used, hence the approach is applicable to a limited domain. In contrast our inference scheme uses a standard CRF or HMM and we do not make any assumption other than usual sequential modeling assumptions. Therefore the presented algorithm is applicable to any problem where distinct states are important and state transitions are ambiguous. A simple transition-cost model is proposed in Fern (2005) for video interpretation where self transition is assumed to have no cost whereas all other possible transitions are assumed to have the same cost K. This is similar to training a probabilistic sequential model which has zero weights for all self transition parameters and same K as the weight for all other transitions. However, this ad-hoc assumption is unrealistic for many applications. Segmentation is the process of identifying the boundaries between segments (e.g., words in natural language processing, set of pixels in image processing). The output of segmentation is a set of segments with exact boundary locations (e.g. super-pixels with contours in images or state trajectories with exact state transition points in sequential models). The Semi-Markovian approach of Sarawagi & Cohen (2004) proposes a solution to the segmentation problem.",
      "startOffset" : 86,
      "endOffset" : 1561
    }, {
      "referenceID" : 4,
      "context" : "Compressed labeling of sequences is mentioned as a videointerpretation application in Fern & Givan (2004), where we get the function name “compress” from. Their main focus was to model a sequence problem that had an enormous number of states which was not known before hand. Due to the unknown number of states the conventional probabilistic models can not be used, hence the approach is applicable to a limited domain. In contrast our inference scheme uses a standard CRF or HMM and we do not make any assumption other than usual sequential modeling assumptions. Therefore the presented algorithm is applicable to any problem where distinct states are important and state transitions are ambiguous. A simple transition-cost model is proposed in Fern (2005) for video interpretation where self transition is assumed to have no cost whereas all other possible transitions are assumed to have the same cost K. This is similar to training a probabilistic sequential model which has zero weights for all self transition parameters and same K as the weight for all other transitions. However, this ad-hoc assumption is unrealistic for many applications. Segmentation is the process of identifying the boundaries between segments (e.g., words in natural language processing, set of pixels in image processing). The output of segmentation is a set of segments with exact boundary locations (e.g. super-pixels with contours in images or state trajectories with exact state transition points in sequential models). The Semi-Markovian approach of Sarawagi & Cohen (2004) proposes a solution to the segmentation problem. Semi-Markov models explicitly model duration in a state with different distributions (which violates the Markovian assumption) and are widely used when one is interested in exact transition points and segmentation boundaries. In contrast to Sarawagi & Cohen (2004), our inference algorithm is designed for Markov models and benefits from ambiguities in segmentation boundaries via marginalizing over all boundary locations.",
      "startOffset" : 86,
      "endOffset" : 1875
    }, {
      "referenceID" : 13,
      "context" : "The problems where semi-Markov CRF models were shown to be successful, such as name entity recognition (Sarawagi & Cohen (2004)), have relatively short maximum segmentation lengths (around 3-4).",
      "startOffset" : 104,
      "endOffset" : 128
    }, {
      "referenceID" : 2,
      "context" : "Figure 4: 3 instances of a) jake, b) conor; c) Results; d) Shape context feature extraction of Belongie et al. (2001); e) Results in terms of EDS and Exact scores.",
      "startOffset" : 95,
      "endOffset" : 118
    } ],
    "year" : 2011,
    "abstractText" : "Hidden Markov models (HMMs) and conditional random fields (CRFs) are two popular techniques for modeling sequential data. Inference algorithms designed over CRFs and HMMs allow estimation of the state sequence given the observations. In several applications, estimation of the state sequence is not the end goal; instead the goal is to compute some function of it. In such scenarios, estimating the state sequence by conventional inference techniques, followed by computing the functional mapping from the estimate is not necessarily optimal. A more formal approach is to directly infer the final outcome from the observations. In particular, we consider the specific instantiation of the problem where the goal is to find the state trajectories without exact transition points and derive a novel polynomial time inference algorithm that outperforms vanilla inference techniques. We show that this particular problem arises commonly in many disparate applications and present experiments on three of them: (1) Toy robot tracking; (2) Single stroke character recognition; (3) Handwritten word recognition.",
    "creator" : "TeX"
  }
}