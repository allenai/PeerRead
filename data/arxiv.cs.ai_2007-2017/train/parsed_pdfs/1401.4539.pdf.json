{
  "name" : "1401.4539.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A MAX -MIN Ant Colony System for Minimum Common String Partition Problem",
    "authors" : [ "S. M. Ferdous", "M. Sohel Rahman" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "In this paper, we consider the problem of finding a minimum common partition of two strings (MCSP). The problem has its application in genome comparison. As it is an NP-hard, discrete combinatorial optimization problem, we employ a metaheuristic technique, namely, MAX-MIN ant system to solve this. The experimental results are found to be promising.\nKeywords: Ant Colony Optimization, Stringology, Genome sequencing, Combinatorial Optimization, Swarm Intelligence, String partitioning"
    }, {
      "heading" : "1. Introduction",
      "text" : "String comparison is one of the important problems in Computer Science with diverse applications in different areas including Genome Sequencing, text processing and compressions. In this paper, we address the problem of finding a minimum common partition (MCSP) of two strings. MCSP is closely related to genome arrangement which is an important topic in computational biology. Given two DNA sequences, the MCSP asks for the least-sized set of the common building blocks of the sequences.\nIn the MCSP problem, we are given two related strings (X, Y ). Two strings are related if every letter appears the same number of times in each of them. Clearly, two strings have a common partition if and only if they are related. So, the length of the two strings are also the same (say, n). Our goal is to partition each string into c segments called blocks, so that the blocks in the partition of X and that of Y constitute the same multiset of substrings. Cardinality of the partition set, i.e., c is to be minimized. A partition of a string X is a sequence P = (B1, B2, · · ·, Bm) of strings whose concatenation is equal to X, that is P1P2 · · · Pm = X. The strings Bi are called the blocks\nPreprint submitted to Elsevier October 3, 2017\nar X\niv :1\n40 1.\n45 39\nv1 [\ncs .A\nI] 1\n8 Ja\nn 20\nof P . Given a partition P of a string X and a partition Q of a string Y , we say that the pair π =< P,Q > is a common partition of X and Y if Q is a permutation of P . The minimum common string partition problem is to find a common partition of X, Y with the minimum number of blocks. For example, if (X, Y ) = {“ababcab”,“abcabab”}, then one of the minimum common partition sets is π ={“ab”,“abc”,“ab”} and the minimum common partition size is 3. The restricted version of MCSP where each letter occurs at most k times in each input string, is denoted by k-MCSP.\nMCSP has its vast application rooted in Comparative Genomics. Given two DNA strings, MCSP answers the possibilities of re-arrangement of one DNA string to another [5]. MCSP is also important in ortholog assignment. In[3], the authors present a new approach to ortholog assignment that takes into account both sequence similarity and evolutionary events at a genomic level. In that approach, first, the problem is formulated as that of computing the signed reversal distance with duplicates between the two genomes of interest. Then, the problem is decomposed into two optimization problems, namely minimum common partition and maximum cycle decomposition problem. Thus MCSP plays an integral part in computing ortholog assignment of genes."
    }, {
      "heading" : "1.1. Our Contribution",
      "text" : "In this paper, we consider metaheuristic approaches to solve the problem. To the best of our knowledge, there exists no attempt to solve the problem with metaheuristic approaches. Only theoretical works are present in literature. Particularly we are interested in nature inspired algorithms. As the problem is discrete combinatorial optimization problems, the natural choice is Ant Colony Optimization (ACO). Before applying ACO, it is necessary to map the problem into a graph. We have developed this mapping. In this paper, we implement a variant of ACO algorithm namely MAX-MIN Ant System (MMAS) to solve the MCSP problem. We conduct experiments on both random and real data to compare our algorithm with the state of the art algorithm in the literature and achieve promising results."
    }, {
      "heading" : "2. Literature Review",
      "text" : "MCSP is essentially the breakpoint distance problem [22] between two permutations which is to count the number of ordered pairs of symbols that\nare adjacent in the first string but not in the other; this problem is obviously solvable in polynomial time [15]. The 2-MCSP is proved to be NPhard and moreover APX-hard in [15]. The authors in [15] also presented several approximation algorithms. Chen et al. [3] studied the problem, Signed Reversal Distance with Duplicates (SRDD), which is a generalization of MCSP. They gave a 1.5-approximation algorithm for 2-MCSP. In [5], the author analyzed the fixed-parameter tractability of MCSP considering different parametrs. In [16], the authors investigated k-MCSP along with two other variants: MCSP c, where the alphabet size is at most c; and xbalanced MCSP, which requires that the length of the blocks must be witnin the range (n/d− x, n/d+ x), where d is the number of blocks in the optimal common partition and x is a constant integer. They showed that MCSP c is NP-hard when c ≥ 2. As for k-MCSP, they presented an FPT algorithm which runs in O∗((d!)2k) time.\nChrobak et al. [4] analyzed a natural greedy heuristic for MCSP: iteratively, at each step, it extracts a longest common substring from the input strings. They showed that for 2-MCSP, the approximation ratio (for the greedy heuristic) is exactly 3. They also proved that for 4-MCSP the ratio would be log n and for the general MCSP, between Ω(n0.43) and O(n0.67).\nAnt colony optimization (ACO) [8, 9, 12] was introduced by M. Dorigo and colleagues as a novel nature-inspired metaheuristic for the solution of hard combinatorial optimization (CO) problems. The inspiring source of ACO is the pheromone trail laying and following behavior of real ants which use pheromones as a communication medium. In analogy to the biological example, ACO is based on the indirect communication of a colony of simple agents, called (artificial) ants, mediated by (artificial) pheromone trails. The pheromone trails in ACO serve as a distributed, numerical information which the ants use to probabilistically construct solutions to the problem being solved and which the ants adapt during the algorithms execution to reflect their search experience.\nDifferent ACO algorithms have been proposed in the literature. The original algorithm is known as the Ant System(AS) [7, 6, 10]. The other variants are, Elitist AS [6, 10], ANT-Q [14], Ant Colony System (ACS) [9], MAX-MIN AS [18, 19, 20] etc.\nRecently growing interest has been noticed towards ACO in the scientific community. There are now available several successful implementations of the ACO metaheuristic applied to a number of different discrete combinatorial optimization problems. In [8] the authors distinguished among two\nclasses of applications of ACO: those to static combinatorial optimization problems, and those to the dynamic ones. When the problem is defined and does not change while the problem is being solved is termed as static combinatorial optimization problems. The authors list some static combinatorial optimization problems those are successfully solved by different variants of ACO. Some of the problems are, travelling salesperson, Quadratic Assignment, job-shop scheduling, vehicle routing, sequential ordering, graph coloring etc. Dynamic problems are defined as a function of some quantities whose values are set by the dynamics of an underlying system. The problem changes therefore at run time and the optimization algorithm must be capable of adapting online to the changing environment. The authors listed connection-oriented network routing and connectionless network routing as the examples of dynamic problems those are successfully solved by ACO.\nIn 2010 a non-exhaustive list of applications of ACO algorithms grouped by problem types is presented in [11]. The authors categorized the problems into different types namely routing, assignment, scheduling, subset machine learning and bioinformatics. In each type they listed the problems those are successfully solved by some variants of ACO.\nThere are not too many string related problems solved by ACO in the literature. In [2], the authors addressed the reconstruction of DNA sequences from DNA fragments by ACO. Several ACO algorithms have been proposed for the longest common subsequence (LCS) problem in [17, 1]. Recently minimum string cover problem is solved by ACO in [13]. Finally, we note that a preliminary version of this work was presented at [? ]."
    }, {
      "heading" : "3. Preliminaries",
      "text" : "In this section, we present some definitions and notations that are used throughout the paper. Two strings (X, Y ), each of length n, over an alphabet∑\nare called related if every letter appears the same number of times in each of them. A block B = ([id, i, j]), 0 ≤ i ≤ j < n, of a string S is a data structure having three fields: id is an identifier of S and the starting and ending positions of the block in S are represented by i and j, respectively. Naturally, the length of a block [id, i, j] is (j − i + 1). We use substring([id, i, j]) to denote the substring of S induced by the block [id, i, j]. Throughout the paper we will use 0 and 1 as the identifiers of X(i.e., id(X)) and Y (i.e., id(Y )) respectively. We use [] to denote an empty block.\nFor example, if we have two strings (X, Y ) = {“abcdab”,“bcdaba”}, then [0, 0, 1] and [0, 4, 5] both represent the substring “ab” of X. In other words, substring([0, 0, 1]) = substring([0, 4, 5]) = “ab”.\nTwo blocks can be intersected or unioned. The intersection of two blocks is a block that contains the common portion of the two. Formally, the intersection operation of B1=[id, i, j] and B2=[id, i ′, j′] is defined as follows:\nB1 ∩B2 =  [] if i′ > j or i > j′\n[id, i′, j] if i′ ≤ j [id, i, j′] else\n(1)\nUnion of two blocks is either another block or an ordered (based on the starting position) set of blocks. Without the loss of generality we suppose that, i ≤ i′ for B1=[id, i, j] and B2=[id, i′, j′]. Then, formally the union operation of B1 and B2 is defined as follows:\nB1 ∪B2 =  [id, i, j] if j′ <= j [id, i, j′] if j′ > j or i′ == j + 1 {B1, B2} else\n(2)\nThe union rule with an ordered set of blocks, Blst and a block, B ′ can be defined as follows. We have to find the position where B′ can be placed in Blst, i.e., we have to find Bk ∈ Blst after which B′ can be placed. Then, we have to replace the ordered subset {Bk, Bk+1} with Bk ∪ B′ ∪ Bk+1. As an example, suppose we have three blocks, namely, B1 = [0, 5, 7],B2 = [0, 11, 12] and B3 = [0, 8, 10]. Then B1 ∪ B2 = B′lst = {[0, 5, 7], [0, 11, 12]}. On the other hand, B′lst∪B3 = [0, 5, 12], which is basically identical to B1∪B2∪B3.\nTwo blocks B1 and B2 (in the same string or in two different strings) matches if substring(B1) = substring(B2). If the two matched blocks are in two different strings then the matched substring is called a common substring of the two strings denoted by cstring(B1, B2).\nGiven a list of blocks with same id, the span of a block, B = [id, i, j] in the list denoted by, span(B) is the length of the block (also in the list) that contains B and whose length is maximum over all such blocks in the list. Note that a block is assumed to contain itself. More formally, given a list of blocks, listb, span(B ∈ listb) = max{` | ` = length(B′), B ⊆ B′,∀B′ ∈ listb}. For example, if listb = {[0, 0, 0], [0, 0, 1], [0, 0, 2], [0, 4, 5]} then span([0, 0, 0]) = span([0, 0, 1]) = span([0, 0, 2]) = 3 where as, span([0, 4, 5]) = 2. In other words, span of a block is the maximum length of the super string than contains the substring induced by the block.\nA partition of a string X is a list of blocks all with id(X) having the following two properties:\n(a) Non Overlapping: The blocks must be be disjoint, i.e., no block should overlap with another block. So the intersection of any two block must be empty.\n(b) Cover: The blocks must cover the whole string."
    }, {
      "heading" : "3.1. Basics of ACO",
      "text" : "In general, the ACO approach attempts to solve a combinatorial optimization (CO) problem by iterating the following two steps. At first, solutions are constructed using a pheromone model, i.e., a parameterized probability distribution over the solution space. Then, the solutions that were constructed in earlier iterations are used to modify the pheromone values in a way that is deemed to bias the search towards the high quality solutions."
    }, {
      "heading" : "3.2. Ant Based Solutions Construction",
      "text" : "As mentioned above, the basic ingredient of an ACO algorithm is a constructive heuristic to probabilistically construct solutions. A constructive heuristic assembles solutions as sequences of solution components taken from a finite set of solution components C = {c1, c2, ...cn}. A solution construction starts with an empty partial solution sp = ∅. Then at each construction step the current partial solution sp is extended by adding a feasible solution component from the solution space C. The process of constructing solutions can be regarded as a walk (or a path) on the so-called construction graph Gc = (C,E) whose vertices are the solution components C and the set E are the connections (i.e., edges)."
    }, {
      "heading" : "3.3. Heuristic Information",
      "text" : "In most ACO algorithms the transition probabilities, i.e., the probabilities for choosing the next solution component, are defined as follows:\np(ci|sp) = τi α · η(ci)β∑\ncj∈N(sp) τj α · η(cj)β\n,∀ci ∈ N(sp) (3)\nHere, η is a weight function that contains heuristic information and α, β are positive parameters whose values determine the relation between the pheromone information and the heuristic information. The pheromones deployed by the ants are denoted by τ ."
    }, {
      "heading" : "3.4. Pheromone Update",
      "text" : "There are different types of pheromone updates. We employ a pheromone update process that is used by almost every ACO algorithm. This pheromone update consists of two parts. First, a pheromone evaporation, which uniformly decreases all the pheromone values, is performed. From a practical point of view, pheromone evaporation is needed to avoid a too rapid convergence of the algorithm toward a sub-optimal region. It helps to forget the local optimal solutions and thus favors the exploration of new areas in the search space. Then, one or more solutions from the current or from earlier iterations are used to increase the values of pheromone trail parameters on solution components that are part of these solutions. As a prominent example, we describe the following pheromone update rule that was used in Ant System (AS) [8], which was the first ACO algorithm proposed.\nτi ← (1− ε)× τi + τi × ∑\ns∈Giter|ci∈s\nF (s)× ε, i = 1, 2, ..., n (4)\nLet W (.) is the cost function. Here, Giter is the set of solutions found in the current iteration, ε ∈ (0, 1] is a parameter called the evaporation rate, and F : G → R+ is a function such that W (s) < W (ś) ⇒ F (s) ≥ F (ś), s 6= ś,∀s ∈ G. The function F (.) is commonly called the Fitness Function.\nIn general, different versions of ACO algorithms differ in the way they update the pheromone values. This also holds for the two currently bestperforming ACO variants in practice, namely, the Ant Colony System (ACS) [9] and the MAX-MIN Ant System (MMAS) [20]. Since in our algorithm we hybridize ACS with MMAS, below we give a brief description of MMAS."
    }, {
      "heading" : "3.5. MAX-MIN Ant System (MMAS)",
      "text" : "MMAS algorithms are characterized as follows. First, the pheromone values are limited to an interval [τMIN , τMAX ] with 0 < τMIN < τMAX . Pheromone trails are initialized to τmax to favor the diversification during the early iterations so that premature convergence is prevented. Explicit limits on the pheromone values ensure that the chance of finding a global optimum never becomes zero. Second, in case the algorithm detects that the search is too much confined to a certain area in the search space, a restart is performed. This is done by initializing all the pheromone values again. Third, the pheromone update is always performed with either the iterationbest solution, the restart-best solution (i.e., the best solution found since the last restart was performed), or the best-so-far solution."
    }, {
      "heading" : "4. Our Approach: MAX-MIN Ant System on the Common Substring Graph",
      "text" : "4.1. Formulation of Common Substring Graph\nWe define a common substring graph, Gcs(V,E, id(X)) of a string X with respect to Y as follows. Here V is the vertex set of the graph and E is the edge set. Vertices are the positions of string X, i.e., for each v ∈ V , v ∈ [0, |X|−1]. Two vertices vi ≤ vj are connected with and edge, i.e, (vi, vj) ∈ E, if the substring induced by the block [id(X), vi, vj] matches some substring of Y . More formally, we have:\n(vi, vj) ∈ E ⇔ cstring([id(X), vi, vj], B′) is not empty ∃B′ ∈ Y\nIn other words, each edge in the edge set corresponds to a block satisfying the above condition. For convenience, we will denote the edges as edge blocks and use the list of edge blocks (instead of edges) to define the edgeset E. Notably, each edge block on the edge set of Gcs(V,E, id(X)) of string (X, Y ) may match with more than one blocks of Y . For each edge block B a list is maintained containing all the matched blocks of string Y to that edge block. This list is called the matchList(B).\nFor example, suppose (X, Y ) = {“abcdba”,“abcdab”}. Now consider the corresponding common substring graph. Then, we have V = {0, 1, 2, 3, 4, 5} and E = {[0, 0, 0], [0, 0, 1], [0, 1, 1], [0, 2, 2], [0, 2, 3], [0, 3, 3][0, 4, 4], [0, 5, 5]}. The matchList of the second edge block, i.e., matchList([0, 0, 1]) = {[1, 0, 1], [1, 4, 5]}.\nTo find a common partition of two strings (X, Y ) we first construct the common substring graph of (X, Y ). Then from a vertex vi on the graph we take an edge block [id(X), vi, vj]. Suppose Mi is the matchList of this block. We take a block B′i from Mi. Then we advance to the next vertex that is (vj + 1) MOD |X| and choose another corresponding edge block as before. We continue this until we come back to the starting vertex. Let partitionList and mappedList are two lists, each of length c, containing the traversed edge blocks and the corresponding matched blocks. Now we have the following lemma.\nLemma 1. partitionList is a common partition of length c iff,\nBi ∩Bj = [] ∀Bi, Bj ∈ mappedList, i 6= j (5)\nand B1 ∪B2 ∪ · · · ∪Bc = [id(Y ), 0, |Y | − 1] (6)\nProof. By construction, partitionList is a partition of X. We need to prove that mappedList is a partition of Y and with the one to one correspondence between partitionList and mappedList it is obvious that partitionList would be the common partition of (X, Y ). Equation 5 asserts the non overlapping property of mappedList and Equation 6 assures the cover property. So, mappedList will be a partition of Y if Equation 5 and 6 are satisfied.\nOn the other hand let partitionList along with mappedList is a common partition of (X, Y ). According to construction, partitionList satisfies the two properties of a partition. Let, mappedList is a partition of Y . We assume mappedList does not follow the Equation 5 or 6. So, there might be overlapping between the blocks or the blocks do not cover the string Y , a contradiction. This completes the proof."
    }, {
      "heading" : "4.2. Heuristics",
      "text" : "Heuristics (η) contain the problem specific information. We propose two different (types of) heuristics for MCSP. Firstly, we propose a static heuristic that does not change during the runs of algorithm. The other heuristic we propose is dynamic in the sense that it changes between the runs."
    }, {
      "heading" : "4.2.1. The Static Heuristic for MCSP",
      "text" : "We employ an intuitive idea. It is obvious that the larger is the size of the blocks the smaller is the partition set. To capture this phenomenon, we assign on each edge of the common substring graph a numerical value that is proportional to the length of the substring corresponding to the edge block. Formally, the static heuristic (ηs) of an edge block [id, i, j] is defined as follows:\nηs([id, i, j]) ∝ length([id, i, j]) (7)"
    }, {
      "heading" : "4.2.2. The Dynamic Heuristic for MCSP",
      "text" : "We observe that the static heuristic can sometimes lead us to very bad solutions. For example if (X, Y ) = {“bceabcd”,“abcdbec”} then according to the static heuristic much higher value will be assigned to edge block [0, 0, 1] than to [0, 0, 0]. But if we take [0, 0, 1], we must match it to the block [1, 1, 2] and we further miss the opportunity to take [0, 3, 6] later. The resultant partition will be {“bc”,“e”,“a”,“b”,“c”,“d”} but if we would take [0, 0, 0] at the first step, then one of the resultant partitions would be {“b”,“c”,“e”,“abcd”}. To overcome this shortcoming of the static heuristic we define a dynamic heuristic as follows. The dynamic heuristic (ηd) of an\nedge block (B = [id, i, j]) is inversely proportional to the difference between the length of the block and the minimum span of its corresponding blocks in its matchList. More formally, ηd(B) is defined as follows:\nηd(B) ∝ 1\n|length(B)−minSpan(B)|+ 1 , (8)\nwhere minSpan(B) = min{span(B′) | B′ ∈ matchList(B)} (9)\nIn the example, minSpan([0, 0, 0]) is 1 as follows: matchList([0, 0, 0]) = {[1, 1, 1], [1, 4, 4]}. span([1, 1, 1]) = 4 and span([1, 4, 4] = 1). On the other hand, minSpan([0, 0, 1]) is 4. So, according to the dynamic heuristic much higher numeral will be assigned to block [0, 0, 0] rather than to block [0, 0, 1].\nWe define the total heuristic (η) to the linear combination of the static heuristic (ηs) and the dynamic heuristic (ηd). Formally, the total heuristic of an edge block B is,\nη(B) = a · ηs(B) + b · ηd(B) (10)\nwhere a, b are any real valued constant."
    }, {
      "heading" : "4.3. Initialization and Configuration",
      "text" : "Given two strings (X, Y ), we first construct the common substring graph Gcs = (V,E, id(X)). We use the following notations. Local best solution (LLB) is the best solution found in each iteration. Global best solution (LGB) is the best solution found so far among all iterations. The pheromone of the edge block is bounded between τmax and τmin. Like [20], we use the following values for τmax and τmin: τmax = 1\nε·cost(LGB) , and τmin =\nτmax(1− n √ pbest)\n(avg−1) n√pbest .\nHere, avg is the average number of choices an ant has in the construction phase; n is the length of the string; pbest is the probability of finding the best solution when the system converges and ε is the evaporation rate. Initially, the pheromone values of all edge blocks (substring) are initialized to initPheromone which is a large value to favor the exploration at the first iteration [20]."
    }, {
      "heading" : "4.4. Construction of a Solution",
      "text" : "Let, nAnts denotes the total number of ants in the colony. Each ant is deployed randomly to a vertex vs of Gcs. A solution for an ant starting at a vertex vs is constructed by the following steps:\nstep 1 : Let vi = vs. Choose an available edge block starting from vi by the discrete probability distribution defined below. An edge block is available if its MatchList is not empty and inclusion of it to the partitionList and mappedList obeys Equation 11. The probability for choosing edge block [0, vi, vj] is:\np([0, vi, vj]) = τ([0, vi, vj]) α · η([0, vi, vj])β∑ ` τ([0, vi, v`]) α · η([0, vi, v`])β ,∀` such that[0, vi, vl] is an available block.\n(11) step 2 : Suppose, [0, vi, vk] is chosen according to Equation 11 above. We choose a match block Bm from the matchList of [0, vi, vk] and delete Bm from the matchList. We also delete every block from every matchList of every edge block that overlaps with Bm. Formally we delete a block B if\nBm ∩B 6= [] ∀Bi ∈ E,B ∈ matchList(Bi).\nWe add [0, vi, vk] to the partitionList and Bm to the mappedList. step 3 : If (vk + 1) MOD |X| = vs and the mappedList obeys Equation 6, then we have found a common partition of X and Y . The size of the partition is the length of the partitionList. Otherwise, we jump to the step 1."
    }, {
      "heading" : "4.5. Intelligent Positioning",
      "text" : "For every edge block of Gcs in X, we have a matchList that contains the matched block of string Y . In construction (step 1), when an edge block is chosen by the probability distribution, we take a block from the matchList of the chosen edge block. We can choose the matched block randomly. But we observe that random choosing may lead to a very bad partition. For example, if (X, Y ) = {“ababc”,“abcab”} then the matchList([0, 0, 1]) = {[1, 0, 1], [1, 3, 4]}. If we choose the first match block then eventually we will get the partition as {“ab”,“ab”,“c”} but a smaller partition exists and that is {“ab”,“abc”}.\nTo overcome this problem, we have imposed a rule for choosing the matched block. We will select a block from the matchList having the lowest possible span. Formally, for the edge block, Bi, a block B\n′ ∈ matchList(Bi) will be selected such that span(B′) is the minimum.\nIn our example span([1, 0, 1]) = 3 where as span([1, 3, 4]) = 2. So it is better to select the second block so that we do not miss the opportunity to match a larger block."
    }, {
      "heading" : "4.6. Pheromone Update",
      "text" : "When each of the ants in the colony construct a solution (i.e., a common partition), an iteration completes. We set the local best solution as the best partition that is the minimum length partition in an iteration. The global best solution for n iterations is defined as the minimum length common partition over all the n iteration.\nWe define the fitness F (L) of a solution L as the reciprocal of the length of L. The pheromone of each interval of each target string is computed according to Equation 4 after each iteration. The pheromone values are bounded within the range τMIN and τMAX . We update the pheromone values according to LLB or LGB. Initially for the first 50 iterations we update pheromone by only LLB to favor the search exploration. After that we develop a scheduling where the frequency of updating with LLB decreases and LGB increases to facilitate exploitation."
    }, {
      "heading" : "4.7. The Pseudocode",
      "text" : "The pseudocode of our approach for solving MCSP is given in Algorithm 1.\nAlgorithm 1 MMAS for MCSP\nCalculate heuristic information() for run = 1→MAXRUN do\nInitialize pheromone Initialize global best repeat\nInitialize local best for i = 1→ nAnts do\nConstruction for anti update local best\nend for update global best update pheromone . either by local best or global best until time reaches maxAllowedT ime or No update found for maxAllowedIteration end for"
    }, {
      "heading" : "5. Experiments",
      "text" : "We have conducted our experiments in a computer with Intel Core 2 Quad CPU 2.33 GHz. The available RAM was 4.00 GB. The operating system was Windows 7. The programming environment was java. jre version is“1.7.0 15”. We have used JCreator as the Integrated Development Environment. The maximum allowed time for each instance was 120 minutes."
    }, {
      "heading" : "5.1. Datasets",
      "text" : "We have conducted our experiments on two types of data: randomly generated DNA sequences and real gene sequences."
    }, {
      "heading" : "5.1.1. Random DNA sequences:",
      "text" : "We have generated 30 random DNA sequences each of length at most 600 using [? ]. The fraction of bases A, T , G and C is assumed to be 0.25 each. For each DNA sequence we shuffle it to create a new DNA sequence. The shuffling is done using the online toolbox [21]. The original random DNA sequence and its shuffled pair constitute a single input (X, Y ) in our experiment. This dataset is divided into 3 classes. The first 10 have lengths less than or equal to 200 bps (base-pairs), the next 10 have lengths within [201, 400] and the rest 10 have lengths within [401, 600] bps."
    }, {
      "heading" : "5.1.2. Real Gene Sequences:",
      "text" : "We have collected the real gene sequence data from the NCBI GenBank1. For simulation, we have chosen Bacterial Sequencing (part 14). We have taken the first 15 gene sequences whose lengths are within [200, 600]."
    }, {
      "heading" : "5.2. Parameters",
      "text" : "There are several parameters which have to be carefully set to obtain good results. The settings of parameters for which we achieved the results are described in Table 1."
    }, {
      "heading" : "5.3. Results and Analysis",
      "text" : "We have compared our approach with the greedy algorithm of [4] because none of the other algorithms in the literature are for general MCSP: each of the other approximation algorithms put some restrictions on the parameters.\n1http://www.ncbi.nlm.nih.gov"
    }, {
      "heading" : "5.3.1. Random DNA sequence:",
      "text" : "Table 2, Table 3 and Table 4 present the comparison between our approach and the greedy approach [4] for the random DNA sequences. For a particular DNA sequence, the experiment was run 4 times and the average result is reported. The first column under any group reports the partition size computed by the greedy approach, the second column is the average partition size found by MMAS, the third column represents the difference between the two approaches, the fourth column reports the standard deviation of 4 runs and the fifth column is the average time in second by which the reported partition size is achieved. A positive (negative) difference indicates that the greedy result is better (worse) than the MMAS result by that amount. From the table, we can see that out of 30 instances our approach gets better partition size for 28 cases.\nTable 7 shows the result of student’s t-test of the difference values of each group. The 95% confidence interval of each group suggests that the actual mean of the difference is negative and lies on the interval. Here the null hypothesis is that, the actual mean of the difference is zero. For all the group, the null hypothesis is rejected. That proves the significant improvement of MMAS over greedy approach."
    }, {
      "heading" : "5.3.2. Effects of Dynamic Heuristics:",
      "text" : "In Section 4.2.2, we discussed the dynamic heuristic we employ in our algorithm. We conducted experiments to check and verify the effect of this dynamic heuristic. We conducted experiments with two versions of our algorithm- with and without applying the dynamic heuristic. The effect\nis presented in Table 5, where for each group the average partition size with dynamic heuristic and without dynamic heuristic is reported. The positive difference depicts the improvement using dynamic heuristic. Out of 30 cases we found positive differences on 27 cases. This clearly shows the significant improvement using dynamic heuristics. It can also be observed that with the increase in length, the positive differences are increased. Figures 1, 2, and 3 show the case by case results. The blue bars represent the partition size using dynamic heuristic and the red bars represent the partition size without the dynamic heuristic."
    }, {
      "heading" : "5.3.3. Real Gene Sequence:",
      "text" : "Table 6 shows the minimum common partition size found by our approach and the greedy approach for the real gene sequences. Out of the 15 instances we get better results on 11 instances.The result of t-test for the real gene sequence is shown on Table 7. The 95% confidence interval suggests that the actual mean of the difference is negative and lies on the interval. Here the null hypothesis is that, the actual mean of the difference is zero. The null hypothesis is rejected (Table 7), which proves the significant improvement over the greedy approach."
    }, {
      "heading" : "6. Conclusion",
      "text" : "Minimum Common String Partition problem has important applications in computational biology. In this paper, we have described a metaheuristic approach to solve the problem. We have used static and dynamic heuristic information in this approach with intelligent positioning. The simulation is conducted on random DNA sequences and real gene sequences. The results are significantly better than the previous results. The t-test result also shows significant improvement. As a future work different other metaheuristic tech-\nniques may be applied to present better solutions to the problem."
    } ],
    "references" : [ {
      "title" : "Beam-aco for the longest common subsequence problem",
      "author" : [ "C. Blum" ],
      "venue" : "in: IEEE Congress on Evolutionary Computation,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2010
    }, {
      "title" : "An ant colony optimization algorithm for dna sequencing by hybridization",
      "author" : [ "C. Blum", "M.Y. Vallès", "M.J. Blesa" ],
      "venue" : "Comput. Oper. Res",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2008
    }, {
      "title" : "Assignment of orthologous genes via genome rearrangement",
      "author" : [ "X. Chen", "J. Zheng", "Z. Fu", "P. Nan", "Y. Zhong", "S. Lonardi", "T. Jiang" ],
      "venue" : "IEEE/ACM Trans. Comput. Biol. Bioinformatics",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2005
    }, {
      "title" : "The greedy algorithm for the minimum common string partition problem",
      "author" : [ "M. Chrobak", "P. Kolman", "J. Sgall" ],
      "venue" : "ACM Trans. Algorithms",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2005
    }, {
      "title" : "Minimum common string partition parameterized",
      "author" : [ "P. Damaschke" ],
      "venue" : "Algorithms in Bioinformatics. Springer Berlin Heidelberg. volume 5251 of Lecture Notes in Computer Science,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2008
    }, {
      "title" : "Optimization, Learning and Natural Algorithms",
      "author" : [ "M. Dorigo" ],
      "venue" : "Ph.D. thesis. Politecnico di Milano, Italy",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1992
    }, {
      "title" : "Positive feedback as a search strategy",
      "author" : [ "M. Dorigo", "A. Colorni", "V. Maniezzo" ],
      "venue" : "Technical Report 91-016. Dipartimento di Elettronica, Politecnico di Milano. Milan, Italy. URL: http://citeseerx.ist.psu. edu/viewdoc/summary?doi=10.1.1.52.6342",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1991
    }, {
      "title" : "Ant algorithms for discrete optimization",
      "author" : [ "M. Dorigo", "G. Di Caro", "L.M. Gambardella" ],
      "venue" : "Artif. Life",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1999
    }, {
      "title" : "Ant colony system: A cooperative learning approach to the traveling salesman problem",
      "author" : [ "M. Dorigo", "L.M. Gambardella" ],
      "venue" : "Trans. Evol. Comp",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1997
    }, {
      "title" : "The ant system: Optimization by a colony of cooperating agents",
      "author" : [ "M. Dorigo", "V. Maniezzo", "A. Colorni" ],
      "venue" : "IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS-PART B",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1996
    }, {
      "title" : "Ant colony optimization: Overview and recent advances",
      "author" : [ "M. Dorigo", "T. Sttzle" ],
      "venue" : "Handbook of Metaheuristics. Springer US. volume 146 of International Series in Operations Research & Management Science,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2010
    }, {
      "title" : "Ant Colony Optimization",
      "author" : [ "M. Dorigo", "T. Stützle" ],
      "venue" : null,
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2004
    }, {
      "title" : "Ant colony optimization approach to solve the minimum string cover problem",
      "author" : [ "S. Ferdous", "A. Das", "R.M.S", "R.M.M" ],
      "venue" : "in: International Conference on Informatics, Electronics & Vision (ICIEV),",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2012
    }, {
      "title" : "Ant-q: A reinforcement learning approach to the traveling salesman",
      "author" : [ "L. Gambardella", "M. Dorigo" ],
      "venue" : null,
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1995
    }, {
      "title" : "Minimum common string partitioning problem: Hardness and approximations",
      "author" : [ "A. Goldstein", "P. Kolman", "J. Zheng" ],
      "venue" : "The Electronic Journal of Combinatorics",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2005
    }, {
      "title" : "Minimum common string partition revisited",
      "author" : [ "H. Jiang", "B. Zhu", "D. Zhu", "H. Zhu" ],
      "venue" : "in: Proceedings of the 4th International Conference on Frontiers in Algorithmics,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2010
    }, {
      "title" : "Finding the longest common subsequence for multiple biological sequences by ant colony optimization",
      "author" : [ "S.J. Shyu", "C.Y. Tsai" ],
      "venue" : "Comput. Oper. Res",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2009
    }, {
      "title" : "Improving the Ant System: A Detailed Report on the MAX-MIN Ant System",
      "author" : [ "T. Stützle", "H. Hoos" ],
      "venue" : null,
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 1996
    }, {
      "title" : "Max-min ant system and local search for the traveling salesman problem, in: IEEE INTERNATIONAL CON- FERENCE ON EVOLUTIONARY COMPUTATION (ICEC’97)",
      "author" : [ "T. Stützle", "H. Hoos" ],
      "venue" : null,
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 1997
    }, {
      "title" : "Max-min ant system",
      "author" : [ "T. Stützle", "H.H. Hoos" ],
      "venue" : "Future Gener. Comput. Syst",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2000
    }, {
      "title" : "Fabox: An online fasta sequence toolbox. URL: http://www.birc.au.dk/software/fabox",
      "author" : [ "P. Villesen" ],
      "venue" : null,
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2007
    }, {
      "title" : "The chromosome inversion problem",
      "author" : [ "G. Watterson", "W. Ewens", "T. Hall", "A. Morgan" ],
      "venue" : "Journal of Theoretical Biology",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 1982
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "Given two DNA strings, MCSP answers the possibilities of re-arrangement of one DNA string to another [5].",
      "startOffset" : 101,
      "endOffset" : 104
    }, {
      "referenceID" : 2,
      "context" : "In[3], the authors present a new approach to ortholog assignment that takes into account both sequence similarity and evolutionary events at a genomic level.",
      "startOffset" : 2,
      "endOffset" : 5
    }, {
      "referenceID" : 21,
      "context" : "MCSP is essentially the breakpoint distance problem [22] between two permutations which is to count the number of ordered pairs of symbols that",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 14,
      "context" : "are adjacent in the first string but not in the other; this problem is obviously solvable in polynomial time [15].",
      "startOffset" : 109,
      "endOffset" : 113
    }, {
      "referenceID" : 14,
      "context" : "The 2-MCSP is proved to be NPhard and moreover APX-hard in [15].",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 14,
      "context" : "The authors in [15] also presented several approximation algorithms.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 2,
      "context" : "[3] studied the problem, Signed Reversal Distance with Duplicates (SRDD), which is a generalization of MCSP.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 4,
      "context" : "In [5], the author analyzed the fixed-parameter tractability of MCSP considering different parametrs.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 15,
      "context" : "In [16], the authors investigated k-MCSP along with two other variants: MCSP , where the alphabet size is at most c; and xbalanced MCSP, which requires that the length of the blocks must be witnin the range (n/d− x, n/d+ x), where d is the number of blocks in the optimal common partition and x is a constant integer.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 3,
      "context" : "[4] analyzed a natural greedy heuristic for MCSP: iteratively, at each step, it extracts a longest common substring from the input strings.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 7,
      "context" : "Ant colony optimization (ACO) [8, 9, 12] was introduced by M.",
      "startOffset" : 30,
      "endOffset" : 40
    }, {
      "referenceID" : 8,
      "context" : "Ant colony optimization (ACO) [8, 9, 12] was introduced by M.",
      "startOffset" : 30,
      "endOffset" : 40
    }, {
      "referenceID" : 11,
      "context" : "Ant colony optimization (ACO) [8, 9, 12] was introduced by M.",
      "startOffset" : 30,
      "endOffset" : 40
    }, {
      "referenceID" : 6,
      "context" : "The original algorithm is known as the Ant System(AS) [7, 6, 10].",
      "startOffset" : 54,
      "endOffset" : 64
    }, {
      "referenceID" : 5,
      "context" : "The original algorithm is known as the Ant System(AS) [7, 6, 10].",
      "startOffset" : 54,
      "endOffset" : 64
    }, {
      "referenceID" : 9,
      "context" : "The original algorithm is known as the Ant System(AS) [7, 6, 10].",
      "startOffset" : 54,
      "endOffset" : 64
    }, {
      "referenceID" : 5,
      "context" : "The other variants are, Elitist AS [6, 10], ANT-Q [14], Ant Colony System (ACS) [9], MAX-MIN AS [18, 19, 20] etc.",
      "startOffset" : 35,
      "endOffset" : 42
    }, {
      "referenceID" : 9,
      "context" : "The other variants are, Elitist AS [6, 10], ANT-Q [14], Ant Colony System (ACS) [9], MAX-MIN AS [18, 19, 20] etc.",
      "startOffset" : 35,
      "endOffset" : 42
    }, {
      "referenceID" : 13,
      "context" : "The other variants are, Elitist AS [6, 10], ANT-Q [14], Ant Colony System (ACS) [9], MAX-MIN AS [18, 19, 20] etc.",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 8,
      "context" : "The other variants are, Elitist AS [6, 10], ANT-Q [14], Ant Colony System (ACS) [9], MAX-MIN AS [18, 19, 20] etc.",
      "startOffset" : 80,
      "endOffset" : 83
    }, {
      "referenceID" : 17,
      "context" : "The other variants are, Elitist AS [6, 10], ANT-Q [14], Ant Colony System (ACS) [9], MAX-MIN AS [18, 19, 20] etc.",
      "startOffset" : 96,
      "endOffset" : 108
    }, {
      "referenceID" : 18,
      "context" : "The other variants are, Elitist AS [6, 10], ANT-Q [14], Ant Colony System (ACS) [9], MAX-MIN AS [18, 19, 20] etc.",
      "startOffset" : 96,
      "endOffset" : 108
    }, {
      "referenceID" : 19,
      "context" : "The other variants are, Elitist AS [6, 10], ANT-Q [14], Ant Colony System (ACS) [9], MAX-MIN AS [18, 19, 20] etc.",
      "startOffset" : 96,
      "endOffset" : 108
    }, {
      "referenceID" : 7,
      "context" : "In [8] the authors distinguished among two",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 10,
      "context" : "In 2010 a non-exhaustive list of applications of ACO algorithms grouped by problem types is presented in [11].",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 1,
      "context" : "In [2], the authors addressed the reconstruction of DNA sequences from DNA fragments by ACO.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 16,
      "context" : "Several ACO algorithms have been proposed for the longest common subsequence (LCS) problem in [17, 1].",
      "startOffset" : 94,
      "endOffset" : 101
    }, {
      "referenceID" : 0,
      "context" : "Several ACO algorithms have been proposed for the longest common subsequence (LCS) problem in [17, 1].",
      "startOffset" : 94,
      "endOffset" : 101
    }, {
      "referenceID" : 12,
      "context" : "Recently minimum string cover problem is solved by ACO in [13].",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 0,
      "context" : "For example, if we have two strings (X, Y ) = {“abcdab”,“bcdaba”}, then [0, 0, 1] and [0, 4, 5] both represent the substring “ab” of X.",
      "startOffset" : 72,
      "endOffset" : 81
    }, {
      "referenceID" : 3,
      "context" : "For example, if we have two strings (X, Y ) = {“abcdab”,“bcdaba”}, then [0, 0, 1] and [0, 4, 5] both represent the substring “ab” of X.",
      "startOffset" : 86,
      "endOffset" : 95
    }, {
      "referenceID" : 4,
      "context" : "For example, if we have two strings (X, Y ) = {“abcdab”,“bcdaba”}, then [0, 0, 1] and [0, 4, 5] both represent the substring “ab” of X.",
      "startOffset" : 86,
      "endOffset" : 95
    }, {
      "referenceID" : 0,
      "context" : "In other words, substring([0, 0, 1]) = substring([0, 4, 5]) = “ab”.",
      "startOffset" : 26,
      "endOffset" : 35
    }, {
      "referenceID" : 3,
      "context" : "In other words, substring([0, 0, 1]) = substring([0, 4, 5]) = “ab”.",
      "startOffset" : 49,
      "endOffset" : 58
    }, {
      "referenceID" : 4,
      "context" : "In other words, substring([0, 0, 1]) = substring([0, 4, 5]) = “ab”.",
      "startOffset" : 49,
      "endOffset" : 58
    }, {
      "referenceID" : 4,
      "context" : "As an example, suppose we have three blocks, namely, B1 = [0, 5, 7],B2 = [0, 11, 12] and B3 = [0, 8, 10].",
      "startOffset" : 58,
      "endOffset" : 67
    }, {
      "referenceID" : 6,
      "context" : "As an example, suppose we have three blocks, namely, B1 = [0, 5, 7],B2 = [0, 11, 12] and B3 = [0, 8, 10].",
      "startOffset" : 58,
      "endOffset" : 67
    }, {
      "referenceID" : 10,
      "context" : "As an example, suppose we have three blocks, namely, B1 = [0, 5, 7],B2 = [0, 11, 12] and B3 = [0, 8, 10].",
      "startOffset" : 73,
      "endOffset" : 84
    }, {
      "referenceID" : 11,
      "context" : "As an example, suppose we have three blocks, namely, B1 = [0, 5, 7],B2 = [0, 11, 12] and B3 = [0, 8, 10].",
      "startOffset" : 73,
      "endOffset" : 84
    }, {
      "referenceID" : 7,
      "context" : "As an example, suppose we have three blocks, namely, B1 = [0, 5, 7],B2 = [0, 11, 12] and B3 = [0, 8, 10].",
      "startOffset" : 94,
      "endOffset" : 104
    }, {
      "referenceID" : 9,
      "context" : "As an example, suppose we have three blocks, namely, B1 = [0, 5, 7],B2 = [0, 11, 12] and B3 = [0, 8, 10].",
      "startOffset" : 94,
      "endOffset" : 104
    }, {
      "referenceID" : 4,
      "context" : "Then B1 ∪ B2 = B′ lst = {[0, 5, 7], [0, 11, 12]}.",
      "startOffset" : 25,
      "endOffset" : 34
    }, {
      "referenceID" : 6,
      "context" : "Then B1 ∪ B2 = B′ lst = {[0, 5, 7], [0, 11, 12]}.",
      "startOffset" : 25,
      "endOffset" : 34
    }, {
      "referenceID" : 10,
      "context" : "Then B1 ∪ B2 = B′ lst = {[0, 5, 7], [0, 11, 12]}.",
      "startOffset" : 36,
      "endOffset" : 47
    }, {
      "referenceID" : 11,
      "context" : "Then B1 ∪ B2 = B′ lst = {[0, 5, 7], [0, 11, 12]}.",
      "startOffset" : 36,
      "endOffset" : 47
    }, {
      "referenceID" : 4,
      "context" : "On the other hand, B′ lst∪B3 = [0, 5, 12], which is basically identical to B1∪B2∪B3.",
      "startOffset" : 31,
      "endOffset" : 41
    }, {
      "referenceID" : 11,
      "context" : "On the other hand, B′ lst∪B3 = [0, 5, 12], which is basically identical to B1∪B2∪B3.",
      "startOffset" : 31,
      "endOffset" : 41
    }, {
      "referenceID" : 0,
      "context" : "For example, if listb = {[0, 0, 0], [0, 0, 1], [0, 0, 2], [0, 4, 5]} then span([0, 0, 0]) = span([0, 0, 1]) = span([0, 0, 2]) = 3 where as, span([0, 4, 5]) = 2.",
      "startOffset" : 36,
      "endOffset" : 45
    }, {
      "referenceID" : 1,
      "context" : "For example, if listb = {[0, 0, 0], [0, 0, 1], [0, 0, 2], [0, 4, 5]} then span([0, 0, 0]) = span([0, 0, 1]) = span([0, 0, 2]) = 3 where as, span([0, 4, 5]) = 2.",
      "startOffset" : 47,
      "endOffset" : 56
    }, {
      "referenceID" : 3,
      "context" : "For example, if listb = {[0, 0, 0], [0, 0, 1], [0, 0, 2], [0, 4, 5]} then span([0, 0, 0]) = span([0, 0, 1]) = span([0, 0, 2]) = 3 where as, span([0, 4, 5]) = 2.",
      "startOffset" : 58,
      "endOffset" : 67
    }, {
      "referenceID" : 4,
      "context" : "For example, if listb = {[0, 0, 0], [0, 0, 1], [0, 0, 2], [0, 4, 5]} then span([0, 0, 0]) = span([0, 0, 1]) = span([0, 0, 2]) = 3 where as, span([0, 4, 5]) = 2.",
      "startOffset" : 58,
      "endOffset" : 67
    }, {
      "referenceID" : 0,
      "context" : "For example, if listb = {[0, 0, 0], [0, 0, 1], [0, 0, 2], [0, 4, 5]} then span([0, 0, 0]) = span([0, 0, 1]) = span([0, 0, 2]) = 3 where as, span([0, 4, 5]) = 2.",
      "startOffset" : 97,
      "endOffset" : 106
    }, {
      "referenceID" : 1,
      "context" : "For example, if listb = {[0, 0, 0], [0, 0, 1], [0, 0, 2], [0, 4, 5]} then span([0, 0, 0]) = span([0, 0, 1]) = span([0, 0, 2]) = 3 where as, span([0, 4, 5]) = 2.",
      "startOffset" : 115,
      "endOffset" : 124
    }, {
      "referenceID" : 3,
      "context" : "For example, if listb = {[0, 0, 0], [0, 0, 1], [0, 0, 2], [0, 4, 5]} then span([0, 0, 0]) = span([0, 0, 1]) = span([0, 0, 2]) = 3 where as, span([0, 4, 5]) = 2.",
      "startOffset" : 145,
      "endOffset" : 154
    }, {
      "referenceID" : 4,
      "context" : "For example, if listb = {[0, 0, 0], [0, 0, 1], [0, 0, 2], [0, 4, 5]} then span([0, 0, 0]) = span([0, 0, 1]) = span([0, 0, 2]) = 3 where as, span([0, 4, 5]) = 2.",
      "startOffset" : 145,
      "endOffset" : 154
    }, {
      "referenceID" : 7,
      "context" : "As a prominent example, we describe the following pheromone update rule that was used in Ant System (AS) [8], which was the first ACO algorithm proposed.",
      "startOffset" : 105,
      "endOffset" : 108
    }, {
      "referenceID" : 8,
      "context" : "This also holds for the two currently bestperforming ACO variants in practice, namely, the Ant Colony System (ACS) [9] and the MAX-MIN Ant System (MMAS) [20].",
      "startOffset" : 115,
      "endOffset" : 118
    }, {
      "referenceID" : 19,
      "context" : "This also holds for the two currently bestperforming ACO variants in practice, namely, the Ant Colony System (ACS) [9] and the MAX-MIN Ant System (MMAS) [20].",
      "startOffset" : 153,
      "endOffset" : 157
    }, {
      "referenceID" : 0,
      "context" : "Then, we have V = {0, 1, 2, 3, 4, 5} and E = {[0, 0, 0], [0, 0, 1], [0, 1, 1], [0, 2, 2], [0, 2, 3], [0, 3, 3][0, 4, 4], [0, 5, 5]}.",
      "startOffset" : 57,
      "endOffset" : 66
    }, {
      "referenceID" : 0,
      "context" : "Then, we have V = {0, 1, 2, 3, 4, 5} and E = {[0, 0, 0], [0, 0, 1], [0, 1, 1], [0, 2, 2], [0, 2, 3], [0, 3, 3][0, 4, 4], [0, 5, 5]}.",
      "startOffset" : 68,
      "endOffset" : 77
    }, {
      "referenceID" : 0,
      "context" : "Then, we have V = {0, 1, 2, 3, 4, 5} and E = {[0, 0, 0], [0, 0, 1], [0, 1, 1], [0, 2, 2], [0, 2, 3], [0, 3, 3][0, 4, 4], [0, 5, 5]}.",
      "startOffset" : 68,
      "endOffset" : 77
    }, {
      "referenceID" : 1,
      "context" : "Then, we have V = {0, 1, 2, 3, 4, 5} and E = {[0, 0, 0], [0, 0, 1], [0, 1, 1], [0, 2, 2], [0, 2, 3], [0, 3, 3][0, 4, 4], [0, 5, 5]}.",
      "startOffset" : 79,
      "endOffset" : 88
    }, {
      "referenceID" : 1,
      "context" : "Then, we have V = {0, 1, 2, 3, 4, 5} and E = {[0, 0, 0], [0, 0, 1], [0, 1, 1], [0, 2, 2], [0, 2, 3], [0, 3, 3][0, 4, 4], [0, 5, 5]}.",
      "startOffset" : 79,
      "endOffset" : 88
    }, {
      "referenceID" : 1,
      "context" : "Then, we have V = {0, 1, 2, 3, 4, 5} and E = {[0, 0, 0], [0, 0, 1], [0, 1, 1], [0, 2, 2], [0, 2, 3], [0, 3, 3][0, 4, 4], [0, 5, 5]}.",
      "startOffset" : 90,
      "endOffset" : 99
    }, {
      "referenceID" : 2,
      "context" : "Then, we have V = {0, 1, 2, 3, 4, 5} and E = {[0, 0, 0], [0, 0, 1], [0, 1, 1], [0, 2, 2], [0, 2, 3], [0, 3, 3][0, 4, 4], [0, 5, 5]}.",
      "startOffset" : 90,
      "endOffset" : 99
    }, {
      "referenceID" : 2,
      "context" : "Then, we have V = {0, 1, 2, 3, 4, 5} and E = {[0, 0, 0], [0, 0, 1], [0, 1, 1], [0, 2, 2], [0, 2, 3], [0, 3, 3][0, 4, 4], [0, 5, 5]}.",
      "startOffset" : 101,
      "endOffset" : 110
    }, {
      "referenceID" : 2,
      "context" : "Then, we have V = {0, 1, 2, 3, 4, 5} and E = {[0, 0, 0], [0, 0, 1], [0, 1, 1], [0, 2, 2], [0, 2, 3], [0, 3, 3][0, 4, 4], [0, 5, 5]}.",
      "startOffset" : 101,
      "endOffset" : 110
    }, {
      "referenceID" : 3,
      "context" : "Then, we have V = {0, 1, 2, 3, 4, 5} and E = {[0, 0, 0], [0, 0, 1], [0, 1, 1], [0, 2, 2], [0, 2, 3], [0, 3, 3][0, 4, 4], [0, 5, 5]}.",
      "startOffset" : 110,
      "endOffset" : 119
    }, {
      "referenceID" : 3,
      "context" : "Then, we have V = {0, 1, 2, 3, 4, 5} and E = {[0, 0, 0], [0, 0, 1], [0, 1, 1], [0, 2, 2], [0, 2, 3], [0, 3, 3][0, 4, 4], [0, 5, 5]}.",
      "startOffset" : 110,
      "endOffset" : 119
    }, {
      "referenceID" : 4,
      "context" : "Then, we have V = {0, 1, 2, 3, 4, 5} and E = {[0, 0, 0], [0, 0, 1], [0, 1, 1], [0, 2, 2], [0, 2, 3], [0, 3, 3][0, 4, 4], [0, 5, 5]}.",
      "startOffset" : 121,
      "endOffset" : 130
    }, {
      "referenceID" : 4,
      "context" : "Then, we have V = {0, 1, 2, 3, 4, 5} and E = {[0, 0, 0], [0, 0, 1], [0, 1, 1], [0, 2, 2], [0, 2, 3], [0, 3, 3][0, 4, 4], [0, 5, 5]}.",
      "startOffset" : 121,
      "endOffset" : 130
    }, {
      "referenceID" : 0,
      "context" : ", matchList([0, 0, 1]) = {[1, 0, 1], [1, 4, 5]}.",
      "startOffset" : 12,
      "endOffset" : 21
    }, {
      "referenceID" : 0,
      "context" : ", matchList([0, 0, 1]) = {[1, 0, 1], [1, 4, 5]}.",
      "startOffset" : 26,
      "endOffset" : 35
    }, {
      "referenceID" : 0,
      "context" : ", matchList([0, 0, 1]) = {[1, 0, 1], [1, 4, 5]}.",
      "startOffset" : 26,
      "endOffset" : 35
    }, {
      "referenceID" : 0,
      "context" : ", matchList([0, 0, 1]) = {[1, 0, 1], [1, 4, 5]}.",
      "startOffset" : 37,
      "endOffset" : 46
    }, {
      "referenceID" : 3,
      "context" : ", matchList([0, 0, 1]) = {[1, 0, 1], [1, 4, 5]}.",
      "startOffset" : 37,
      "endOffset" : 46
    }, {
      "referenceID" : 4,
      "context" : ", matchList([0, 0, 1]) = {[1, 0, 1], [1, 4, 5]}.",
      "startOffset" : 37,
      "endOffset" : 46
    }, {
      "referenceID" : 0,
      "context" : "For example if (X, Y ) = {“bceabcd”,“abcdbec”} then according to the static heuristic much higher value will be assigned to edge block [0, 0, 1] than to [0, 0, 0].",
      "startOffset" : 135,
      "endOffset" : 144
    }, {
      "referenceID" : 0,
      "context" : "But if we take [0, 0, 1], we must match it to the block [1, 1, 2] and we further miss the opportunity to take [0, 3, 6] later.",
      "startOffset" : 15,
      "endOffset" : 24
    }, {
      "referenceID" : 0,
      "context" : "But if we take [0, 0, 1], we must match it to the block [1, 1, 2] and we further miss the opportunity to take [0, 3, 6] later.",
      "startOffset" : 56,
      "endOffset" : 65
    }, {
      "referenceID" : 0,
      "context" : "But if we take [0, 0, 1], we must match it to the block [1, 1, 2] and we further miss the opportunity to take [0, 3, 6] later.",
      "startOffset" : 56,
      "endOffset" : 65
    }, {
      "referenceID" : 1,
      "context" : "But if we take [0, 0, 1], we must match it to the block [1, 1, 2] and we further miss the opportunity to take [0, 3, 6] later.",
      "startOffset" : 56,
      "endOffset" : 65
    }, {
      "referenceID" : 2,
      "context" : "But if we take [0, 0, 1], we must match it to the block [1, 1, 2] and we further miss the opportunity to take [0, 3, 6] later.",
      "startOffset" : 110,
      "endOffset" : 119
    }, {
      "referenceID" : 5,
      "context" : "But if we take [0, 0, 1], we must match it to the block [1, 1, 2] and we further miss the opportunity to take [0, 3, 6] later.",
      "startOffset" : 110,
      "endOffset" : 119
    }, {
      "referenceID" : 0,
      "context" : "In the example, minSpan([0, 0, 0]) is 1 as follows: matchList([0, 0, 0]) = {[1, 1, 1], [1, 4, 4]}.",
      "startOffset" : 76,
      "endOffset" : 85
    }, {
      "referenceID" : 0,
      "context" : "In the example, minSpan([0, 0, 0]) is 1 as follows: matchList([0, 0, 0]) = {[1, 1, 1], [1, 4, 4]}.",
      "startOffset" : 76,
      "endOffset" : 85
    }, {
      "referenceID" : 0,
      "context" : "In the example, minSpan([0, 0, 0]) is 1 as follows: matchList([0, 0, 0]) = {[1, 1, 1], [1, 4, 4]}.",
      "startOffset" : 76,
      "endOffset" : 85
    }, {
      "referenceID" : 0,
      "context" : "In the example, minSpan([0, 0, 0]) is 1 as follows: matchList([0, 0, 0]) = {[1, 1, 1], [1, 4, 4]}.",
      "startOffset" : 87,
      "endOffset" : 96
    }, {
      "referenceID" : 3,
      "context" : "In the example, minSpan([0, 0, 0]) is 1 as follows: matchList([0, 0, 0]) = {[1, 1, 1], [1, 4, 4]}.",
      "startOffset" : 87,
      "endOffset" : 96
    }, {
      "referenceID" : 3,
      "context" : "In the example, minSpan([0, 0, 0]) is 1 as follows: matchList([0, 0, 0]) = {[1, 1, 1], [1, 4, 4]}.",
      "startOffset" : 87,
      "endOffset" : 96
    }, {
      "referenceID" : 0,
      "context" : "span([1, 1, 1]) = 4 and span([1, 4, 4] = 1).",
      "startOffset" : 5,
      "endOffset" : 14
    }, {
      "referenceID" : 0,
      "context" : "span([1, 1, 1]) = 4 and span([1, 4, 4] = 1).",
      "startOffset" : 5,
      "endOffset" : 14
    }, {
      "referenceID" : 0,
      "context" : "span([1, 1, 1]) = 4 and span([1, 4, 4] = 1).",
      "startOffset" : 5,
      "endOffset" : 14
    }, {
      "referenceID" : 0,
      "context" : "span([1, 1, 1]) = 4 and span([1, 4, 4] = 1).",
      "startOffset" : 29,
      "endOffset" : 38
    }, {
      "referenceID" : 3,
      "context" : "span([1, 1, 1]) = 4 and span([1, 4, 4] = 1).",
      "startOffset" : 29,
      "endOffset" : 38
    }, {
      "referenceID" : 3,
      "context" : "span([1, 1, 1]) = 4 and span([1, 4, 4] = 1).",
      "startOffset" : 29,
      "endOffset" : 38
    }, {
      "referenceID" : 0,
      "context" : "On the other hand, minSpan([0, 0, 1]) is 4.",
      "startOffset" : 27,
      "endOffset" : 36
    }, {
      "referenceID" : 0,
      "context" : "So, according to the dynamic heuristic much higher numeral will be assigned to block [0, 0, 0] rather than to block [0, 0, 1].",
      "startOffset" : 116,
      "endOffset" : 125
    }, {
      "referenceID" : 19,
      "context" : "Like [20], we use the following values for τmax and τmin: τmax = 1 ε·cost(LGB) , and τmin = τmax(1− n √ pbest) (avg−1) n pbest .",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 19,
      "context" : "Initially, the pheromone values of all edge blocks (substring) are initialized to initPheromone which is a large value to favor the exploration at the first iteration [20].",
      "startOffset" : 167,
      "endOffset" : 171
    }, {
      "referenceID" : 0,
      "context" : "For example, if (X, Y ) = {“ababc”,“abcab”} then the matchList([0, 0, 1]) = {[1, 0, 1], [1, 3, 4]}.",
      "startOffset" : 63,
      "endOffset" : 72
    }, {
      "referenceID" : 0,
      "context" : "For example, if (X, Y ) = {“ababc”,“abcab”} then the matchList([0, 0, 1]) = {[1, 0, 1], [1, 3, 4]}.",
      "startOffset" : 77,
      "endOffset" : 86
    }, {
      "referenceID" : 0,
      "context" : "For example, if (X, Y ) = {“ababc”,“abcab”} then the matchList([0, 0, 1]) = {[1, 0, 1], [1, 3, 4]}.",
      "startOffset" : 77,
      "endOffset" : 86
    }, {
      "referenceID" : 0,
      "context" : "For example, if (X, Y ) = {“ababc”,“abcab”} then the matchList([0, 0, 1]) = {[1, 0, 1], [1, 3, 4]}.",
      "startOffset" : 88,
      "endOffset" : 97
    }, {
      "referenceID" : 2,
      "context" : "For example, if (X, Y ) = {“ababc”,“abcab”} then the matchList([0, 0, 1]) = {[1, 0, 1], [1, 3, 4]}.",
      "startOffset" : 88,
      "endOffset" : 97
    }, {
      "referenceID" : 3,
      "context" : "For example, if (X, Y ) = {“ababc”,“abcab”} then the matchList([0, 0, 1]) = {[1, 0, 1], [1, 3, 4]}.",
      "startOffset" : 88,
      "endOffset" : 97
    }, {
      "referenceID" : 0,
      "context" : "In our example span([1, 0, 1]) = 3 where as span([1, 3, 4]) = 2.",
      "startOffset" : 20,
      "endOffset" : 29
    }, {
      "referenceID" : 0,
      "context" : "In our example span([1, 0, 1]) = 3 where as span([1, 3, 4]) = 2.",
      "startOffset" : 20,
      "endOffset" : 29
    }, {
      "referenceID" : 0,
      "context" : "In our example span([1, 0, 1]) = 3 where as span([1, 3, 4]) = 2.",
      "startOffset" : 49,
      "endOffset" : 58
    }, {
      "referenceID" : 2,
      "context" : "In our example span([1, 0, 1]) = 3 where as span([1, 3, 4]) = 2.",
      "startOffset" : 49,
      "endOffset" : 58
    }, {
      "referenceID" : 3,
      "context" : "In our example span([1, 0, 1]) = 3 where as span([1, 3, 4]) = 2.",
      "startOffset" : 49,
      "endOffset" : 58
    }, {
      "referenceID" : 20,
      "context" : "The shuffling is done using the online toolbox [21].",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 3,
      "context" : "We have compared our approach with the greedy algorithm of [4] because none of the other algorithms in the literature are for general MCSP: each of the other approximation algorithms put some restrictions on the parameters.",
      "startOffset" : 59,
      "endOffset" : 62
    }, {
      "referenceID" : 3,
      "context" : "Table 2, Table 3 and Table 4 present the comparison between our approach and the greedy approach [4] for the random DNA sequences.",
      "startOffset" : 97,
      "endOffset" : 100
    }, {
      "referenceID" : 3,
      "context" : "Table 2: Comparison between Greedy approach [4] and MMAS on random DNA sequences (Group 1, 200 bps).",
      "startOffset" : 44,
      "endOffset" : 47
    }, {
      "referenceID" : 3,
      "context" : "Table 3: Comparison between Greedy approach [4] and MAX-MIN on random DNA sequences (Group 2, 400 bps).",
      "startOffset" : 44,
      "endOffset" : 47
    }, {
      "referenceID" : 3,
      "context" : "Table 4: Comparison between Greedy approach [4] and MAX-MIN on random DNA sequences (Group 3, 600 bps).",
      "startOffset" : 44,
      "endOffset" : 47
    }, {
      "referenceID" : 3,
      "context" : "Table 6: Comparison between Greedy approach [4] and MMAS on real gene sequence.",
      "startOffset" : 44,
      "endOffset" : 47
    } ],
    "year" : 2017,
    "abstractText" : "In this paper, we consider the problem of finding a minimum common partition of two strings (MCSP). The problem has its application in genome comparison. As it is an NP-hard, discrete combinatorial optimization problem, we employ a metaheuristic technique, namely, MAX-MIN ant system to solve this. The experimental results are found to be promising.",
    "creator" : "LaTeX with hyperref package"
  }
}