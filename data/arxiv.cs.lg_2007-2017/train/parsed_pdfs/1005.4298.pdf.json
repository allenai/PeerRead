{
  "name" : "1005.4298.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Distantly Labeling Data for Large Scale Cross-Document Coreference",
    "authors" : [ "Sameer Singh", "Michael Wick", "Andrew McCallum" ],
    "emails" : [ "sameer@cs.umass.edu", "mwick@cs.umass.edu", "mccallum@cs.umass.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Keywords: information extraction, coreference, weak supervision, structured prediction"
    }, {
      "heading" : "1 Introduction",
      "text" : "Given a collection of mentions of people or other entities extracted from a body of text, coreference or entity resolution consists of partitioning (or clustering) the mentions such that all mentions within a partition refer to the same underlying entity. Coreference is vital for many down-stream semantic analysis and knowledge discovery tasks [6]. Yet despite being extensively studied, considerable challenges remain. In particular, while significant progress has been made in within-document coreference [22,4,9,15] (resolving mentions from inside a single document), the larger problem of cross-document coreference (resolving mentions from across a collection of many documents) has received significantly less attention. We hypothesize two reasons for this discrepancy. First, there is a scarcity of substantial datasets labeled for cross-document coreference. Second, both the data and the hypothesis space are as a consequence larger and more difficult to manage.\nOne approach to coping with the lack of training data is to employ unsupervised methods, where weights, thresholds, or priors are set by hand. Some existing methods\nar X\niv :1\n00 5.\n42 98\nv1 [\ncs .A\nI] 2\n4 M\nay 2\n01 0\ncombine a clustering procedure with thresholded distance function over entities [1,14]. More recently, generative and non-parametric Bayesian clustering techniques have been proposed as a way to circumvent the need for labeled data [15]. Unfortunately, these unsupervised methods tend to perform considerably worse than supervised methods, making labeled data essential for achieving state-of-the art performance [4].\nOf course another alternative is to manually label a corpus. However, this presents more difficulties for cross-document coreference than for within-document coreference. For cross-document coreference, the number of mentions and entities is typically large, and thus the space of partitions immense—making the labeling process cumbersome, with high uncertainty about the true number of entities. Faced with these difficulties cross-document labelers often use tools that allow them to query and label pairs of mentions at a time; however, this process scales poorly, has high cognitive load, and results in transitivity violations that must be resolved in a subsequent step. Furthermore, information from the immediate context of the mention is often not sufficient to resolve them, requiring additional search or investigation.\nWe present an alternative approach that supports supervised learning, while avoiding the need for human labeling effort. In addition to the unlabeled corpus of mentions, we leverage readily-available supplementary data—data which does not directly provide the required labels, but which is distantly related to those labels needed for the coreference task at hand. We process the original data together with this distantlylabeled data [31] to automatically label the original relevant corpus. This is a type of indirect supervision by alignment [3]. In this paper we develop and demonstrate distantlabeling for coreference of over a million mentions from 3.5 years of New York Times newspaper articles by using Wikipedia as distantly-labeled data. We present a generative probabilistic model that performs the alignment with 92% accuracy.\nWe also present a sophisticated conditional random field model of coreference that includes factors over entities, as well as traditional mention-mention pairs. The model is trained on the distantly-labeled data, and evaluated on unseen mentions and entities. We address the challenge of scaling up this model to our massive data set by using a family of Metropolis-Hastings proposal distributions that use canopies [20] to efficiently explore the hypothesis space. Our experiments show that both learning and inference can be performed in less than 10 hours on a single CPU, even though the coreference hypothesis space is exponential in the millions of cross-document mentions present in the corpus.\nThe rest of the paper is organized as follows: Section 2 defines and motivates the problem of cross-document coreference. In Section 3 we describe and evaluate our proposed approach for generating training data using Wikipedia. The cross-document coreference model trained on this data is described in Section 4. In Section 5 we explore related work. We conclude and lay out a number of ideas for future work in Section 6."
    }, {
      "heading" : "2 Problem Definition",
      "text" : "The problem of cross-document coreference is to identify the sets of mention strings that refer to the same underlying entity, where the number of entities is not known. The source of the mentions may be a single document, in which case the task is within-\ndocument coreference. The number of mentions (and underlying entities) in each document is usually in the hundreds. The difficulty of the task arises from large hypothesis space (exponential in the number of mentions) and from challenges in resolving nominal and pronominal mentions to the correct named mentions. Usually, named mentions are not ambiguous in within-document coreference.\nThe problem we study in this paper is that of cross-document coreference, where the source of mentions is a large collection of documents. The same sources of ambiguity as within-doc coreference exist in this scenario also. Furthermore, the number of mentions and entities are typically much larger, and for some corpora it can be in the millions, making the hypothesis space of all possible clustering exponentially larger than that of within-doc. Additionally, often there exists ambiguity even in the named entities since the same string can refer to multiple entities in different document, and multiple distinct strings may refer to the same entity in different documents.\nWe show an example of some ambiguities in Fig 1. The most common problem in cross-document coreference is resolving people that have the exact same name. The example above contains an example of such entities that are in the same general category (football), making the problem more difficult. Another common ambiguity is that of alternate names, where the same entity is referred to variations on the same name (such as “Richard/Dick”). The figure also shows an example of renaming ambiguity (“Lovebug Starski” may be mentioned as “Kevin Smith”), which is an extreme case of alternate names. Rare singleton entities (like the firefighter), who may appear only once in the whole corpus, are also often difficult to isolate. Our approach aims to address all these various challenges."
    }, {
      "heading" : "3 Distant-Labels for Coreference",
      "text" : "Wikipedia1 pages contain historical and biographical descriptions of a large number of entities, which we use as the external source of “distant labels”. These pages are used\n1 http://en.wikipedia.org/\nto identify the entities that mention strings from a different corpus refer to, resulting in a clustering over these mentions.\nOur approach to distant-labeling for coreference consists of three steps. Given the set of mention strings, the first step performs preliminary within-document coreference to resolve entities for mentions that appear in the same document. Second, a set of candidate entities for each mention string is identified from Wikipedia using the redirect and disambiguation pages. This set of candidates is reduced using a relevance scoring function based on a generative model over the article and Wikipedia pages. The steps are described in detail below in the context of person-name disambiguation in newswire articles, however the method can be applied to other forms of cross-doc coreference."
    }, {
      "heading" : "3.1 Within-Document Coreference",
      "text" : "To reduce the number of mentions available for cross-document, within-document coreference is usually applied as a preprocessing step to resolve the entities within the document [1,6,29]. The task of resolving proper nouns within a document is usually straightforward; however, incorporating pronouns and nominal nouns (like “he”, “she”, “the president”, etc.) make within-doc coreference considerably more challenging.\nSince our method of within-doc coreference is applied only to mention strings that are proper nouns, it is similar to that used in [6,27]. A distance function is defined between a pair of mention sets (clusters) that uses hand-tuned weights over features that look at various string matches (such as whether the first name/last name is same) and gender matches (a mention contains “Mr.” and the other contains “Mrs.”). We apply standard greedy agglomerative clustering using this distance function, and use a low threshold to obtain high precision. Although this may result in multiple within-doc entities that refer to the same entity, we rely on the cross-doc disambiguation to resolve these. For the rest of the paper we refer to the within-doc coreference entities as mentions, and the strings as as sub-mentions. For each mention, the longest sub-mention is selected as its canonical string representation.\nThis within-doc coreference system is simple and domain-specific. The rest of the approach does not rely on a particular choice of within-doc coreference and instead a machine-learned coreference model that also considers pronouns can be used, such as [9]. However, since it is not directly relevant for cross-document coreference, for the purposes of illustration in this paper we used the domain-specific approach."
    }, {
      "heading" : "3.2 Identifying Candidate Entities",
      "text" : "For each canonical mention string, a set of potential Wikipedia entities that the mention may refer to need to be identified. To discover these candidates, we utilize the redirect and disambiguation pages available in Wikipedia.\nRedirect pages are used to forward the user to the page with the correct title, given that the user query is either a common misspelling (“Barak Obama”), an alternate spelling (“Dick Nixon”), or refers to multiple entities of which one is prominent (“Obama”). Redirect pages also exist for less common but difficult to disambiguate cases of renames (Sean Combs/Puff Daddy), typographical issues (Pointcare/Pointcaré), spacing and casing (vangogh/Vincent van Gogh), etc. Note that the destination of a redirect link\nis not always a content page, it may be another redirect page. These redirections offer a reliable signal for identifying a candidate entity for a mention, since many different variations of the entity are represented.\nAlthough the redirect pages provide alternate mentions for a single entity, often the same string can refer to multiple entities, for example, “Hillary” may refer to Hillary Clinton or to Edmund Hillary. To incorporate this information, Wikipedia includes disambiguation pages that list, for a given string, all the entities on Wikipedia that the string may refer to. These pages can range from 2 or 3 entities (“William Clinton”) to more than 50 entities (“John Smith”). We use these disambiguation pages to expand the set of candidate entities for a given string.\nThe set of candidate Wikipedia pages for a given mentionm is computed as follows. We start by initializing a set S = {m}. Within a loop, we go through every element of s ∈ S, and do the following:\n1. if there is a redirect page s→ s′, we remove s and insert s′ into S. 2. if there is a disambiguation page for s containing S′ as the set of possible pages,\nwe remove s and update S ← S ∪ S′. 3. if s is a content page in Wikipedia, do nothing 4. if s is not present in Wikipedia, remove it from S.\nThe above process is repeated until S stops changing, resulting in a final set of Wikipedia page candidates for m. This algorithm is analogous to taking a transitive closure of a graph where the mentions and content pages are the nodes, and the redirect and disambiguation pages represent the edges.\nNote that this approach may result in many candidates that are trivially inapplicable to the mention string; however, the objective is to obtain a super-set of candidates. Furthermore, we may discover only one candidate for a mention on Wikipedia, but it may be the incorrect one (i.e. the mention’s correct entity does not exist on Wikipedia). The next section describes how this set of candidates is pruned to a single (or no) match."
    }, {
      "heading" : "3.3 Selection Using Multinomials",
      "text" : "Once the candidates for each mention are identified, the method selects which candidate the mention refers to, or whether it refers to none of the Wikipedia entities. To make this decision, we use the contents of the Wikipedia page of the candidate and the news article that the mention belongs to. A score is calculated between the article and the Wikipedia page, and the candidate with the highest score is picked (or none of the candidates are picked if this score is below a threshold). It is possible to use an alternative approach that only considers the local contexts around the sub-mentions instead of the complete article text, however we are interested in thematic match between the candidate page and the mention, and the local contexts are a noisy signal of this match.\nThe task of identifying the best candidate for a mention is represented as a retrieval problem. In information retrieval, documents are ranked for a given query according to the probability that the query is generated from the document. The document is represented as a unigram language model (multinomial over the words) and the probability of generating the query is the product of the probabilities of generating individual tokens\nof the query. Smoothing is often introduced as a back-off distribution so that tokens that do not appear in the document have a small positive probability of being generated [25,32].\nSince the candidates refer to a single Wikipedia entity, and an article may refer to multiple entities, we rank the candidate pages (queries) according to the probability that they were generated from the news article (document). Tokens are weighted according to the standard inverse-document frequency2 so that the rare tokens contribute more to the generation probability. To account for the back-off distribution, we use uniform multinomial for the global language model (P (t|Mg) = α), and use linear interpolation (Jelinek-Mercer smoothing) using λ ([17]).\nThe model is described below, where c is the candidate page, a is the article, Ma is the language model for article a, Mg is the global language model, nt,i is the count of word t in page/article i, and idfD(t) is the inverse document frequency for word t in corpus D.\nP (c|a) ∝ ∏ t∈c P (t|a) P (t|a) = (λP (t|Ma) + (1− λ)P (t|Mg))w t c\n= ( λwta + (1− λ)α )wtc where wtc =\nnt,c × idfC(t)∑ t∈c nt,c × idfC(t) ,\nand wta = nt,a × idfA(t)∑\nt∈a nt,a × idfA(t)\n.\nWe rank the candidates according to P (c|a) based on the article that contains the mention. To account for the case where the mention does not refer to any of the candidates, a threshold β is used on the probability. Therefore we apply our candidate selection method even when mentions have a single candidate."
    }, {
      "heading" : "3.4 Evaluation",
      "text" : "To evaluate the quality of the labels, we apply our method to the New York Times corpus [30] which contains 20 years of New York Times articles. Our approach is applied to a subset (Jan 1, 2004–June 19, 2007) that consists of 308k articles. The Stanford NER tagger [12] is used to extract person-name string from these articles, resulting in 5 million sub-mentions. High-precision within-doc coreference identifies 2.5 million mentions that are used in cross-document disambiguation. We use a recent snapshot of Wikipedia3 consisting of 6.2 million pages, out of which 2.5 million are redirects and 121k are disambiguation pages.\n2 Since the distribution of the words in Wikipedia and our corpus may differ considerably, we compute separate idf s. 3 enwiki-20080103-pages-articles.xml\nOut of the 2.5 million mentions, our method of candidate set selection does not find any candidates for ∼ 1.4 million mentions, due to occurrence of rare entities, absence of sufficient context and string variations that are not captured by the redirect pages. Since we rely on Wikipedia for our distant-labels, these mentions cannot be resolved and are hence ignored.\nSince we want as little smoothing as possible, we set α = 10−4 and λ = 1 − α. This results in higher weights to rare word matches (and less weight to common word matches). The threshold for rejecting candidates β is set to e−18. Running our method of entity selection on ∼ 1.1 million mentions (172k unique strings) results in matches to 125k entities (Wikipedia pages). We show the distribution of the resulting entity sizes in Table 1.\nThe creation of the dataset took a total of 23 hours, out of which 15 hours were for the NER tagging. Since there is no computation on pairs of mentions, our approach is linear in the size of the document corpus, and will scale to larger data, for e.g. all 20 years of the New York Times corpus. Furthermore, each decision is made independently, leading to parallelization across machines that can considerably reduce the running time.\nWe evaluate the accuracy of the resulting data set by manually examining the Wikipedia page selection decision made for individual mentions. 1, 107 total mentions are sampled randomly from the training data and their entity selection is labeled as correct or incorrect. The accuracy reported in Table 2 includes subsets of the labeled mentions that have only a single candidate (decision on whether to pick it or not) and ones that have multiple candidates.\nThe overall accuracy of 92.6% for creating training data for coreference containing more than a million mentions without any manual intervention makes our approach useful for real-world applications. Since resolving disambiguation is a difficult task even by humans (some mentions had more than 50 candidates), we are impressed that our language model based method achieves an accuracy of 69.9% for mentions with mul-\ntiple candidates. Furthermore, much of the dataset does not contain these disambiguations, resulting in the redirect link resolving a number of correct entities, achieving an accuracy of 98.2% when there are no disambiguations."
    }, {
      "heading" : "3.5 Dataset Ambiguity",
      "text" : "In this section we give some examples of cross-document coreference decisions that are difficult to achieve without our distant-labeling approach.\n– Renaming: Consider the changing aliases of celebrities, for example mentions “Sean Combs” and “Puff Daddy”. Since the contexts around these mentions vary significantly due to temporal differences, unsupervised methods that rely on the mention string or the contexts may not merge these mentions. Furthermore, these mentions are problematic for human labelers who lack the domain-specific knowledge required to disambiguate them. However, Wikipedia provides a redirect link, “Sean Combs→Puff Daddy”. Similarly, for the example in Fig 1, “Lovebug Starski” appears on the disambiguation page for “Kevin Smith”. – Alternate Names: There are many examples of cases where the same first name is often referred to by alternate strings, such as “Richard/Dick”, “Chuck/Charles”, “Dan/Daniel”, “Robert/Bob/Bobby”, etc. Many approaches use a pre-built gazetteers for such renames, however they require domain expertise to build, may be incomplete, and may not always be applicable (for e.g. “Edward” is sometimes “Ted”, as in Kennedy, but this is uncommon). Wikipedia redirect and disambiguation links provide alternatives only if the entity is sometimes referred to by the alternate name. – Same Names: This is a major concern for cross-document coreference wherein same mention strings refer to different entities, e.g. “John Smith”. Unsupervised techniques that rely on contexts can create more/less entities than truth. This problem is magnified when the different entities belong to similar fields (“Michael Moore”, filmmaker, and “Michael D. Moore”, film actor and director). From Wikipedia we know which entities to disambiguate between, and our relevance model uses the article text to select a candidate, resulting in an accuracy of 69.9%. – Singletons: Many mention strings in news articles often refer to entities that do not appear in the corpus again (local opinion pieces, death notices, one-off criminal stories, etc.). Pruning these involves considerable labor, as not all articles in a certain category only refer to singleton entities. Frequency based analysis is also not enough, since many of these mentions share the same string with mentions from other articles. Our approach eliminates these from Wikipedia candidates based on the threshold on the article-page scoring function. However, as a side effect, this method does not resolve isolated references to prominent entities, e.g. reference to “George Bush” in a sports article.\nWe investigate the level of ambiguity in our generated dataset by analyzing precision and recall scores of pairwise coreference decisions for some simple baseline procedures evaluated using our generated labeled data as ground truth (see Table 3). Precision, in context of clustering, refers to the ratio of the number of pairs of mention that were correctly predicted to be coreferent (true positives) to the total pairs of mentions that were\npredicted to be coreferent (true positives + false positives). Precision thus encourages smaller clusters that are of high quality. Recall, on the other hand, refers to the number of true positives to the number of pairs of mentions that are coreferent as per the truth (true positives + false negatives), and encourages bigger clusters that are super-sets of true clusters. Precision and recall scores of some simple baseline clustering techniques (by treating our generated data as truth) gives us insight into the ambiguity in our data.\nFirst we cluster the mentions according to their unique canonical strings, that is, each cluster contains mentions whose canonical names are string-identical. There are two important observations here. First, the precision is below 100% revealing that unique name clustering is merging names that refer to different entities, that is, demonstrating “same names” ambiguity. Second, the recall is considerably lower indicating that entities are being referred to by multiple strings, demonstrating “renaming” and “alternate names” ambiguity.\nNext, we relax the notion of canonical name clustering to a last name clustering, where mentions are in the same cluster if their last names are string-identical. If there were no “rename” ambiguity, then we would expect perfect recall (100%) for the last name clustering. However, this is not the case, implying that there is a substantial amount of “rename” ambiguity in our dataset (although considerably less than the “alternate name” ambiguity).\nThis pairwise analysis, along with the manual evaluation in Table 2, can be used to estimate the quality and frequency of the various types of disambiguations in the dataset, and therefore its utility as a cross-document coreference corpus."
    }, {
      "heading" : "4 Cross-Document Coreference",
      "text" : "Now that we have described our distantly labeled coreference corpus, we demonstrate its utility for training a sophisticated discriminative model of coreference. Since this conditional random field based model only consists of features based on the raw text surrounding the mentions, and does not use any supervision from Wikipedia, it can be applied to any source of text. In particular we reveal that the model is capable of generalizing to a held-out evaluation set that contains mentions and entities that are not encountered during training."
    }, {
      "heading" : "4.1 Model",
      "text" : "We adapt the state-of-the-art within document coreference model of [9] to our problem setting. In particular, we use a conditional random field with features defined over\n10 Sameer Singh, Michael Wick, and Andrew McCallum\nd( ) ={a,b,c,..z}\nCn\nc mc\nd( ) ={yes, no} X1 X2 X3 X4\nY1 Y2 Y3 Y4\nmi\nmci\nCn\nc B\nW\nmj\nmi\nW\nE\nmj\nmi\nB\nmjei Ei\n!w !b\n!\nm1\nm3\nm1\ne1 e2\n!+ !-\n!-\n! !\nm1\nm3\nm2\ne1 e2\n!+ !-\n!-\n! !\nFig. 2. The Factor graph for coreference with two entities and three mentions. The shaded areas represent the clustering.\nmention-pairs and cross-document entities. There are two types of hidden variables: mentions (denoted mi) and entities (denoted ei). The mention variable has a domain that ranges over possible entities, and the entity is a set-valued variable whose domain ranges over all possible subsets of mentions. Further, there are variables that encapsulate all the observed properties of mentions, which are implicitly part of the hidden mention variable. These properties include the mention text, the canonical representation for the mention, and the bags of words extracted from context windows as well as the sub-mention texts. These factors model the dependencies between mention pairs in the same cluster (ψ+ modeling attraction), and mention pairs in different clusters (ψ− modeling repulsion). Additionally, we have a factor over each entity variable (ψ) that can examine the cohesiveness of a coreference cluster. Figure 2 shows our model instantiated with three mentions over a two-entity hypothesis.\nLet M be the set of hidden mention variables, E be the set of hidden entity variables, X be the set of observed mention properties, and Y = M ∪ E be the set of all hidden variables. Further let the notation mi.e denote the entity that mi references, let P+ = {〈mi,mj〉 ∈ M2i<j |mi.e = mj .e} and similarly P− = {〈mi,mj〉 ∈ M2i<j |mi.e 6= mj .e}. Then, the probability distribution over the coreference hypothesis space (clusterings) is given as:\nπ(y|x) ∝ ∏ e∈E ψ(e) × ∏ P+ ψ+(mi,mj) (1)\n× ∏ P− ψ−(mi,mj)\nwhere ψ±(mi,mj) = exp ( θ± · φ(mi,mj) ) is a log-linear combination of parameters θ and the feature function φ. Note that while the underlying feature-functions for these two types of factors are equivalent (φ), the parameters between them are not tied, allowing separate sets of weights to be learned. Our model is implemented as an imperativelydefined factor graph using the FACTORIE probabilistic programming library [21]."
    }, {
      "heading" : "4.2 Features",
      "text" : "Since we want to apply the model to arbitrary sources of mentions, we rely only on the text, stored as observed properties of the mentions (X). As described above, these properties include the mention text, the canonical representation for the mention, and the bags of words extracted from context windows as well as the sub-mention texts.\nGiven a mention pair 〈mi,mj〉 we define the following binary pairwise features φ(mi,mj):\n– canonical match true iff mi’s canonical representation is lower-case string identical to mj’s. Similarly, we include a canonical mismatch feature. – last name match true iff mi’s canonical last name is lower-case string identical to mj’s Similarly, we include a last name mismatch feature. – cosine distance features for each type of bag-of-words, the cosine distance is measured and quantized into ten bins.\nFor an entity e, we implement the following binary features\n– cluster size features: thresholded cluster sizes (== 1, > 1, > 2, > 4)"
    }, {
      "heading" : "4.3 Inference",
      "text" : "In this section we describe a method for scaling inference to large datasets. In particular, we use a local search method based on the Metropolis-Hastings (MH) algorithm with a canopy-based proposer. We briefly describe MH and canopies, then present our jump function.\nMH is a Markov-chain Monte Carlo method that stochastically performs local changes by probabilistically accepting jumps from a proposal distribution Q conditioned on the current state y producing a new configuration y′. The proposed configuration y′ is accepted with probability α = π(y′|x)/π(y|x)× q(y|y′)/q(y′|y), where π is the distribution encoded by the model (see Eq 1), and q is the probability of proposing the jump to y′. Since we are performing maximum a posteriori (MAP) with no latent variables, we can safely ignore the ratio containing q [21].\nIn order to avoid inefficiencies arising from unnecessary exploration, we inject the following domain-specific knowledge into the proposal distribution. First, the proposal distribution is designed so that inference explores only the space of valid configurations (settings to the hidden variables that result in an invalid clustering are not considered). Second, we use the idea of canopies to propose jumps that are more likely to be accepted by the model, and that lead to high-scoring configurations.\nA canopy is a relaxation of a clustering where mentions can refer to more that one entity (in other words the transitivity assumption is not enforced and mentions can be in more than one cluster) [20]. Formally, we define a canopy C as a set of mention sets. Typically, canopies are constructed so that mentions occurring in the same set are highly likely to be coreference, for example, they can be constructed such that all mentions in the same set are within a certain cosine threshold of each other.\nLet Γ = {Ci} be a set of canopies. Also, let the notation t ∼ρ T mean to draw an element t from a set T with probability distribution ρ : T → [0, 1] s.t. ∑ t∈T ρ(s) = 1. The class of proposal distributions based on canopies is defined as follows:\n1: Input: current configuration y 2: Output: proposed configuration y′ 3: C ∼ρΓ //pick a random canopy 4: S ∼ρC //pick a set of mentions from canopy 5: //pick a new entity for mention ma ∼ρ S mb ∼ρ S ma.e← mb.e // move ma to mb’s entity 6: return y′\nIn practice we take ρ to be uniform in distribution.In our implementation we use the last name and canonical name clusterings from Section 3.4 as the set of canopies Γ . We further enrich the above proposal distribution to create new entities and explore random configurations. Specifically, with 20% probability, we pick a random mention and move it to a random entity (this entity may be an empty one), and with 80% probability we run the canopy proposer.\nNote that for each step, the jump function moves a single mention from one entity to another; each jump changes the settings of only three hidden variables. Therefore, even with factors over entire entities, evaluating the MH acceptance score requires computing as many factors as the number of total mentions in the two entities."
    }, {
      "heading" : "4.4 Learning",
      "text" : "Parameter estimation is performed with SampleRank [34], an extremely efficient stochastic gradient-ascent based method that solves a ranking-objective function. SampleRank employs the same proposal distribution as used during inference and learns a model whose probabilities correspond with a user-specified ranking function over coreference configurations. In particular, we learn a model that ranks configurations according to pairwise accuracy."
    }, {
      "heading" : "4.5 Experiments",
      "text" : "We present preliminary experiments demonstrating that the within document coreference model can be scaled to perform inference on a large dataset. In particular, we use our distantly-labeled NYT data consisting of over a million mentions, as described in Section 3.4. This is evenly divided into training and testing sets, each contains ∼550, 000 mentions and ∼90, 000 entities.\nWe perform ten iterations of SampleRank on the training set, where an iteration consists of 100, 000 Metropolis-Hastings steps; each iteration takes on average only 19.6 minutes and training takes under five hours total. We perform inference using five million Metropolis-Hastings steps on the held out test data. Inference takes 9.5 hours and achieves a Pairwise F1 score of 89.83%. This high score is encouraging for a model that is trained on our distantly-labeled data. Note that since most of the entities that are present in the training data are absent from the evaluation, our model is resolving mentions to unseen entities. This provides evidence that our model may generalize to entities that are not present in Wikipedia, however we could not evaluate this since our evaluation data only consists of entities that appear in Wikipedia."
    }, {
      "heading" : "5 Related Work",
      "text" : "Even though the cross-document coreference problem is challenging and lacks large labeled datasets, its ubiquitous role as a key component to many knowledge discovery tasks has inspired several efforts.\nThere are a number of unsupervised approaches to the problem, many of which rely on a scoring function for pairs of contexts that is used for clustering. One of the first approaches to cross-document coreference [1] uses a pre-trained within-document step, followed by an idf based scoring function for pairs of contexts for clustering. Ravin et al. [29] extend this work to be more scalable by comparing pairs of context only if the mentions are deemed ambiguous enough using a heuristic. Others have explored multiple methods of context similarity, and concluded that agglomerative clustering provides effective means of performing inference [14]. Pedersen et al. [24] and Purandare & Pedersen [28] integrate second-order co-occurrence of words into the similarity function. A number of other approaches include various forms of hand-tuned weights, dictionaries, and heuristics to define similarity for name disambiguation for clustering [6,2,27].\nSince the unsupervised techniques make strong assumptions about the data and/or contain domain specific heuristics, techniques that rely on minimal supervision have been proposed. Mann & Yarowsky [18] extract biographical facts from the Web, such as birthdate, which are used as features for clustering. Niu et al. [23] incorporate information extraction into the context similarity model, and construct small annotated datasets to learn some of the parameters. There has been little work in completely supervised cross-document coreference. The only work of which we are aware is Finin et al. [11]. They incorporate features from Wikitology to train a pairwise classifier. Similarly, Mayfield et al. [19] use additional features that are information extraction based. Both these systems are trained on a small subset of the ACE 2008 data.\nA number of techniques above also create datasets for evaluation. Bagga & Baldwin [1] generate a small, highly-ambiguous “john smith” dataset. Gooi & Allan [14] create the ambiguous “Person-X dataset” that replaces names of different entities to be the same Person-X. Although challenging and helpful for academic evaluation, these artificial datasets offer little realism. Niu et al.[23] generate approximate data sets for partial supervision, however the datasets are too small and noisy to be useful for training. A number of tools have been proposed to help annotators and the resulting datasets have been released. Bentivogli et al. [5] introduces a small dataset containing high ambiguity (209 names over 709 entities), but the data set is not big enough for large-scale models. Day et al.[10] also introduce a tool and a corpus, however the corpus offers little ambiguity.\nVarious aspects of Wikipedia have been used as supervision and features for a number of information extraction tasks. Features based on Wikipedia, including categorical and structure, have been used to train supervised models of within-document [26] and cross-document [11,19] coreference. Similar to our generative model, wikipedia has been used to score similarity between documents [13,16,33]. By making a “one person per document” assumption, Han & Zhao [16] treat document clustering as an approach to unsupervised coreference. Strube & Ponzetto [33] uses the semantic relatedness of articles as a feature for a supervised coreference model.\nOur work is most similar to Bunescu & Pasca [7] and Cucerzan [8] which also use Wikipedia to disambiguate entities in an unsupervised manner. Bunescu & Pasca [7] use content and categorical information to create features for a scoring model that can disambiguate mentions that appear in wikipedia articles. This scoring model is trained on the existing links in Wikipedia. It unclear whether this method can generalize to mentions from other sources such as newswire. Cucerzan [8] disambiguates mentions in arbitrary data sources, and is evaluated on news articles. The vector representation of each document is created using features from wikipedia, and the dot product is used to denote document similarity. This disambiguation approach was applied to a small dataset. Additionally, none of these approaches train a cross-document coreference model that can run on a large number of mentions and resolve entities that do not appear in Wikipedia, restricting their utility."
    }, {
      "heading" : "6 Conclusions",
      "text" : "Motivated by the difficulty of labeling data for large-scale cross-document coreference, we propose a distantly-labeling approach to automatically produce large datasets using Wikipedia. We applied the method to the New York Times corpus, and the noise and ambiguity in the generated dataset were analyzed. To enable cross-document coreference on this large dataset, a canopy-based sampling approach for training and inference was introduced. The model that we trained on this data has multiple uses in downstream applications, such as search, reputation analysis, trend analysis, etc. Furthermore, the predictions of the model can be used to suggest additional disambiguations and redirects for Wikipedia.\nThere are a number of avenues for future work. We intend to release the dataset so that the community of cross-document coreference can benefit from a large labeled corpus. Even though the current level of noise is acceptable, our method can be improved to create less noisy datasets, using more complicated models than the current. More ambiguity can be artificially introduced into the dataset; although this is not realistic, the resulting dataset may be more useful for evaluation of cross-document coreference methods."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This work was supported in part by the Center for Intelligent Information Retrieval, in part by SRI International subcontract #27-001338 and ARFL prime contract #FA875009-C-0181, and in part by UPenn NSF medium IIS-0803847. Any opinions, findings and conclusions or recommendations expressed in this material are the authors’ and do not necessarily reflect those of the sponsor."
    } ],
    "references" : [ {
      "title" : "Entity-based cross-document coreferencing using the vector space model",
      "author" : [ "A. Bagga", "B. Baldwin" ],
      "venue" : "International Conference on Computational Linguistics. pp. 79–85. Association for Computational Linguistics, Morristown, NJ, USA",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Who is who and what is what: experiments in cross-document coreference",
      "author" : [ "A. Baron", "M. Freedman" ],
      "venue" : "Empirical Methods in Natural Language Processing (EMNLP). pp. 274–283. Association for Computational Linguistics",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Generalized expectation criteria for bootstrapping extractors using record-text alignment",
      "author" : [ "K. Bellare", "A. McCallum" ],
      "venue" : "Conference on Empirical Methods on Natural Language Processing (EMNLP). pp. 131–140. Association for Computational Linguistics, Singapore",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Understanding the value of features for coreference resolution",
      "author" : [ "E. Bengston", "D. Roth" ],
      "venue" : "Empirical Methods in Natural Language Processing (EMNLP)",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Creating a gold standard for person cross-document coreference resolution in italian news",
      "author" : [ "L. Bentivogli", "C. Girardi", "E. Pianta" ],
      "venue" : "LREC Workshop on Resources and Evaluation for Identity Matching, Entity Resolution and Entity Management",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Automatic entity disambiguation: Benefits to NER, relation extraction, link analysis, and inference",
      "author" : [ "M. Blume" ],
      "venue" : "International Conference on Intelligence Analysis (ICIA)",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Using encyclopedic knowledge for named entity disambiguation",
      "author" : [ "R.C. Bunescu", "M. Pasca" ],
      "venue" : "European Chapter of the Association for Computational Linguistics (EACL)",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Large-scale named entity disambiguation based on Wikipedia data",
      "author" : [ "S. Cucerzan" ],
      "venue" : "Empirical Methods in Natural Language Processing (EMNLP). pp. 708–716. Association for Computational Linguistics, Prague, Czech Republic",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "First-order probabilistic models for coreference resolution",
      "author" : [ "A. Culotta", "M. Wick", "A. McCallum" ],
      "venue" : "North American Chapter of the Association for Computational Linguistics - Human Language Technologies (NAACL HLT)",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "A corpus for cross-document co-reference",
      "author" : [ "D. Day", "J. Hitzeman", "M. Wick", "K. Crouch", "M. Poesio" ],
      "venue" : "International Conference on Language Resources and Evaluation (LREC)",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Using Wikitology for crossdocument entity coreference resolution",
      "author" : [ "T. Finin", "Z. Syed", "J. Mayfield", "P. McNamee", "C. Piatko" ],
      "venue" : "AAAI Spring Symposium on Learning by Reading and Learning to Read. AAAI Press",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Incorporating non-local information into information extraction systems by gibbs sampling",
      "author" : [ "J.R. Finkel", "T. Grenager", "C. Manning" ],
      "venue" : "Annual Meeting of the Association for Computational Linguistics (ACL). pp. 363–370. Association for Computational Linguistics, Ann Arbor, Michigan",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Computing semantic relatedness using wikipedia-based explicit semantic analysis",
      "author" : [ "E. Gabrilovich", "S. Markovitch" ],
      "venue" : "International Joint Conference on Artificial Intelligence (IJCAI). pp. 1606–1611. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Cross-document coreference on a large scale corpus",
      "author" : [ "C.H. Gooi", "J. Allan" ],
      "venue" : "Human Language Technologies: Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL HLT). pp. 9–16. Association for Computational Linguistics, Boston, Massachusetts, USA",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Unsupervised coreference resolution in a nonparametric bayesian model",
      "author" : [ "A. Haghighi", "D. Klein" ],
      "venue" : "Annual Meeting of the Association for Computational Linguistics (ACL). pp. 848–855. Association for Computational Linguistics, Prague, Czech Republic",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Named entity disambiguation by leveraging Wikipedia semantic knowledge",
      "author" : [ "X. Han", "J. Zhao" ],
      "venue" : "Conference on Information and Knowledge Management (CIKM). pp. 215–224. ACM, New York, NY, USA",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Interpolated estimation of markov source parameters from sparse data",
      "author" : [ "F. Jelinek", "R.L. Mercer" ],
      "venue" : "Pattern Recognition in Practice pp. 381–397",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 1980
    }, {
      "title" : "Unsupervised personal name disambiguation",
      "author" : [ "G.S. Mann", "D. Yarowsky" ],
      "venue" : "Human Language Technologies: Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL HLT). pp. 33–40",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Cross-document coreference resolution: A key technology for learning by reading",
      "author" : [ "J. Mayfield", "D. Alexander", "B. Dorr", "J. Eisner", "T. Elsayed", "T. Finin", "C. Fink", "M. Freedman", "N. Garera", "P McNamee" ],
      "venue" : "AAAI 2009 Spring Symposium on Learning by Reading and Learning to Read",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Efficient clustering of high-dimensional data sets with application to reference matching",
      "author" : [ "A. McCallum", "K. Nigam", "L. Ungar" ],
      "venue" : "Knowledge Discovery and Data Mining (KDD). pp. 169–178",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "FACTORIE: Probabilistic programming via imperatively defined factor graphs",
      "author" : [ "A. McCallum", "K. Schultz", "S. Singh" ],
      "venue" : "Neural Information Processing Systems (NIPS)",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Machine learning for coreference resolution: From local classification to global ranking",
      "author" : [ "V. Ng" ],
      "venue" : "Annual Meeting of the Association for Computational Linguistics (ACL)",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Weakly supervised learning for cross-document person name disambiguation supported by information extraction",
      "author" : [ "C. Niu", "W. Li", "R.K. Srihari" ],
      "venue" : "Annual Meeting of the Association for Computational Linguistics (ACL). p. 597. Association for Computational Linguistics, Morristown, NJ, USA",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "An unsupervised language independent method of name discrimination using second order co-occurrence features",
      "author" : [ "T. Pedersen", "A. Kulkarni", "R. Angheluta", "Z. Kozareva", "T. Solorio" ],
      "venue" : "International Conference on Intelligent Text Processing and Computational Linguistics (CICLing). pp. 208–222",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "A language modeling approach to information retrieval",
      "author" : [ "J.M. Ponte", "W.B. Croft" ],
      "venue" : "Annual International ACM SIGIR Conference on Research and development in Information Retrieval. pp. 275–281. ACM, New York, NY, USA",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Exploiting semantic role labeling, WordNet and Wikipedia for coreference resolution",
      "author" : [ "S.P. Ponzetto", "M. Strube" ],
      "venue" : "Human Language Technologies: Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL HLT). pp. 192–199. Association for Computational Linguistics, Morristown, NJ, USA",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Improving cross-document coreference",
      "author" : [ "O. Popescu", "C. Girardi", "E. Pianta", "B. Magnini" ],
      "venue" : "Journées Internationales d’Analyse statistique des Données Textuelles 9, 961–969",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Word sense discrimination by clustering contexts in vector and similarity spaces",
      "author" : [ "A. Purandare", "T. Pedersen" ],
      "venue" : "Conference on Computational Natural Language Learning (CoNLL). pp. 41–48. Boston",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Is Hillary Rodham Clinton the president? disambiguating names across documents",
      "author" : [ "Y. Ravin", "Z. Kazi" ],
      "venue" : "Annual Meeting of the Association for Computational Linguistics (ACL). pp. 9–16",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "The New York Times annotated corpus",
      "author" : [ "E. Sandhaus" ],
      "venue" : "Linguistic Data Consortium",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Learning hidden markov model structure for information extraction",
      "author" : [ "K. Seymore", "A. McCallum", "R. Rosenfeld" ],
      "venue" : "AAAI Workshop on Machine Learning for Information Extraction. pp. 37–42",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "A general language model for information retrieval",
      "author" : [ "F. Song", "W.B. Croft" ],
      "venue" : "International Conference on Information and Knowledge Management (CIKM). pp. 316–321. ACM, New York, NY, USA",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "WikiRelate! computing semantic relatedness using wikipedia",
      "author" : [ "M. Strube", "S.P. Ponzetto" ],
      "venue" : "AAAI Conference on Artificial Intelligence. pp. 1419–1424. AAAI Press",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Samplerank: Learning preferences from atomic gradients",
      "author" : [ "M. Wick", "K. Rohanimanesh", "A. Culotta", "A. McCallum" ],
      "venue" : "Neural Information Processing Systems (NIPS), Workshop on Advances in Ranking",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : "Coreference is vital for many down-stream semantic analysis and knowledge discovery tasks [6].",
      "startOffset" : 90,
      "endOffset" : 93
    }, {
      "referenceID" : 21,
      "context" : "In particular, while significant progress has been made in within-document coreference [22,4,9,15] (resolving mentions from inside a single document), the larger problem of cross-document coreference (resolving mentions from across a collection of many documents) has received significantly less attention.",
      "startOffset" : 87,
      "endOffset" : 98
    }, {
      "referenceID" : 3,
      "context" : "In particular, while significant progress has been made in within-document coreference [22,4,9,15] (resolving mentions from inside a single document), the larger problem of cross-document coreference (resolving mentions from across a collection of many documents) has received significantly less attention.",
      "startOffset" : 87,
      "endOffset" : 98
    }, {
      "referenceID" : 8,
      "context" : "In particular, while significant progress has been made in within-document coreference [22,4,9,15] (resolving mentions from inside a single document), the larger problem of cross-document coreference (resolving mentions from across a collection of many documents) has received significantly less attention.",
      "startOffset" : 87,
      "endOffset" : 98
    }, {
      "referenceID" : 14,
      "context" : "In particular, while significant progress has been made in within-document coreference [22,4,9,15] (resolving mentions from inside a single document), the larger problem of cross-document coreference (resolving mentions from across a collection of many documents) has received significantly less attention.",
      "startOffset" : 87,
      "endOffset" : 98
    }, {
      "referenceID" : 0,
      "context" : "combine a clustering procedure with thresholded distance function over entities [1,14].",
      "startOffset" : 80,
      "endOffset" : 86
    }, {
      "referenceID" : 13,
      "context" : "combine a clustering procedure with thresholded distance function over entities [1,14].",
      "startOffset" : 80,
      "endOffset" : 86
    }, {
      "referenceID" : 14,
      "context" : "More recently, generative and non-parametric Bayesian clustering techniques have been proposed as a way to circumvent the need for labeled data [15].",
      "startOffset" : 144,
      "endOffset" : 148
    }, {
      "referenceID" : 3,
      "context" : "Unfortunately, these unsupervised methods tend to perform considerably worse than supervised methods, making labeled data essential for achieving state-of-the art performance [4].",
      "startOffset" : 175,
      "endOffset" : 178
    }, {
      "referenceID" : 30,
      "context" : "We process the original data together with this distantlylabeled data [31] to automatically label the original relevant corpus.",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 2,
      "context" : "This is a type of indirect supervision by alignment [3].",
      "startOffset" : 52,
      "endOffset" : 55
    }, {
      "referenceID" : 19,
      "context" : "We address the challenge of scaling up this model to our massive data set by using a family of Metropolis-Hastings proposal distributions that use canopies [20] to efficiently explore the hypothesis space.",
      "startOffset" : 156,
      "endOffset" : 160
    }, {
      "referenceID" : 0,
      "context" : "To reduce the number of mentions available for cross-document, within-document coreference is usually applied as a preprocessing step to resolve the entities within the document [1,6,29].",
      "startOffset" : 178,
      "endOffset" : 186
    }, {
      "referenceID" : 5,
      "context" : "To reduce the number of mentions available for cross-document, within-document coreference is usually applied as a preprocessing step to resolve the entities within the document [1,6,29].",
      "startOffset" : 178,
      "endOffset" : 186
    }, {
      "referenceID" : 28,
      "context" : "To reduce the number of mentions available for cross-document, within-document coreference is usually applied as a preprocessing step to resolve the entities within the document [1,6,29].",
      "startOffset" : 178,
      "endOffset" : 186
    }, {
      "referenceID" : 5,
      "context" : "Since our method of within-doc coreference is applied only to mention strings that are proper nouns, it is similar to that used in [6,27].",
      "startOffset" : 131,
      "endOffset" : 137
    }, {
      "referenceID" : 26,
      "context" : "Since our method of within-doc coreference is applied only to mention strings that are proper nouns, it is similar to that used in [6,27].",
      "startOffset" : 131,
      "endOffset" : 137
    }, {
      "referenceID" : 8,
      "context" : "The rest of the approach does not rely on a particular choice of within-doc coreference and instead a machine-learned coreference model that also considers pronouns can be used, such as [9].",
      "startOffset" : 186,
      "endOffset" : 189
    }, {
      "referenceID" : 24,
      "context" : "Smoothing is often introduced as a back-off distribution so that tokens that do not appear in the document have a small positive probability of being generated [25,32].",
      "startOffset" : 160,
      "endOffset" : 167
    }, {
      "referenceID" : 31,
      "context" : "Smoothing is often introduced as a back-off distribution so that tokens that do not appear in the document have a small positive probability of being generated [25,32].",
      "startOffset" : 160,
      "endOffset" : 167
    }, {
      "referenceID" : 16,
      "context" : "To account for the back-off distribution, we use uniform multinomial for the global language model (P (t|Mg) = α), and use linear interpolation (Jelinek-Mercer smoothing) using λ ([17]).",
      "startOffset" : 180,
      "endOffset" : 184
    }, {
      "referenceID" : 29,
      "context" : "To evaluate the quality of the labels, we apply our method to the New York Times corpus [30] which contains 20 years of New York Times articles.",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 11,
      "context" : "The Stanford NER tagger [12] is used to extract person-name string from these articles, resulting in 5 million sub-mentions.",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 8,
      "context" : "We adapt the state-of-the-art within document coreference model of [9] to our problem setting.",
      "startOffset" : 67,
      "endOffset" : 70
    }, {
      "referenceID" : 20,
      "context" : "Our model is implemented as an imperativelydefined factor graph using the FACTORIE probabilistic programming library [21].",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 20,
      "context" : "Since we are performing maximum a posteriori (MAP) with no latent variables, we can safely ignore the ratio containing q [21].",
      "startOffset" : 121,
      "endOffset" : 125
    }, {
      "referenceID" : 19,
      "context" : "A canopy is a relaxation of a clustering where mentions can refer to more that one entity (in other words the transitivity assumption is not enforced and mentions can be in more than one cluster) [20].",
      "startOffset" : 196,
      "endOffset" : 200
    }, {
      "referenceID" : 0,
      "context" : "Also, let the notation t ∼ρ T mean to draw an element t from a set T with probability distribution ρ : T → [0, 1] s.",
      "startOffset" : 107,
      "endOffset" : 113
    }, {
      "referenceID" : 33,
      "context" : "Parameter estimation is performed with SampleRank [34], an extremely efficient stochastic gradient-ascent based method that solves a ranking-objective function.",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 0,
      "context" : "One of the first approaches to cross-document coreference [1] uses a pre-trained within-document step, followed by an idf based scoring function for pairs of contexts for clustering.",
      "startOffset" : 58,
      "endOffset" : 61
    }, {
      "referenceID" : 28,
      "context" : "[29] extend this work to be more scalable by comparing pairs of context only if the mentions are deemed ambiguous enough using a heuristic.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 13,
      "context" : "Others have explored multiple methods of context similarity, and concluded that agglomerative clustering provides effective means of performing inference [14].",
      "startOffset" : 154,
      "endOffset" : 158
    }, {
      "referenceID" : 23,
      "context" : "[24] and Purandare & Pedersen [28] integrate second-order co-occurrence of words into the similarity function.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 27,
      "context" : "[24] and Purandare & Pedersen [28] integrate second-order co-occurrence of words into the similarity function.",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 5,
      "context" : "A number of other approaches include various forms of hand-tuned weights, dictionaries, and heuristics to define similarity for name disambiguation for clustering [6,2,27].",
      "startOffset" : 163,
      "endOffset" : 171
    }, {
      "referenceID" : 1,
      "context" : "A number of other approaches include various forms of hand-tuned weights, dictionaries, and heuristics to define similarity for name disambiguation for clustering [6,2,27].",
      "startOffset" : 163,
      "endOffset" : 171
    }, {
      "referenceID" : 26,
      "context" : "A number of other approaches include various forms of hand-tuned weights, dictionaries, and heuristics to define similarity for name disambiguation for clustering [6,2,27].",
      "startOffset" : 163,
      "endOffset" : 171
    }, {
      "referenceID" : 17,
      "context" : "Mann & Yarowsky [18] extract biographical facts from the Web, such as birthdate, which are used as features for clustering.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 22,
      "context" : "[23] incorporate information extraction into the context similarity model, and construct small annotated datasets to learn some of the parameters.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 10,
      "context" : "[11].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 18,
      "context" : "[19] use additional features that are information extraction based.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 0,
      "context" : "Bagga & Baldwin [1] generate a small, highly-ambiguous “john smith” dataset.",
      "startOffset" : 16,
      "endOffset" : 19
    }, {
      "referenceID" : 13,
      "context" : "Gooi & Allan [14] create the ambiguous “Person-X dataset” that replaces names of different entities to be the same Person-X.",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 22,
      "context" : "[23] generate approximate data sets for partial supervision, however the datasets are too small and noisy to be useful for training.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 4,
      "context" : "[5] introduces a small dataset containing high ambiguity (209 names over 709 entities), but the data set is not big enough for large-scale models.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 9,
      "context" : "[10] also introduce a tool and a corpus, however the corpus offers little ambiguity.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 25,
      "context" : "Features based on Wikipedia, including categorical and structure, have been used to train supervised models of within-document [26] and cross-document [11,19] coreference.",
      "startOffset" : 127,
      "endOffset" : 131
    }, {
      "referenceID" : 10,
      "context" : "Features based on Wikipedia, including categorical and structure, have been used to train supervised models of within-document [26] and cross-document [11,19] coreference.",
      "startOffset" : 151,
      "endOffset" : 158
    }, {
      "referenceID" : 18,
      "context" : "Features based on Wikipedia, including categorical and structure, have been used to train supervised models of within-document [26] and cross-document [11,19] coreference.",
      "startOffset" : 151,
      "endOffset" : 158
    }, {
      "referenceID" : 12,
      "context" : "Similar to our generative model, wikipedia has been used to score similarity between documents [13,16,33].",
      "startOffset" : 95,
      "endOffset" : 105
    }, {
      "referenceID" : 15,
      "context" : "Similar to our generative model, wikipedia has been used to score similarity between documents [13,16,33].",
      "startOffset" : 95,
      "endOffset" : 105
    }, {
      "referenceID" : 32,
      "context" : "Similar to our generative model, wikipedia has been used to score similarity between documents [13,16,33].",
      "startOffset" : 95,
      "endOffset" : 105
    }, {
      "referenceID" : 15,
      "context" : "By making a “one person per document” assumption, Han & Zhao [16] treat document clustering as an approach to unsupervised coreference.",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 32,
      "context" : "Strube & Ponzetto [33] uses the semantic relatedness of articles as a feature for a supervised coreference model.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 6,
      "context" : "Our work is most similar to Bunescu & Pasca [7] and Cucerzan [8] which also use Wikipedia to disambiguate entities in an unsupervised manner.",
      "startOffset" : 44,
      "endOffset" : 47
    }, {
      "referenceID" : 7,
      "context" : "Our work is most similar to Bunescu & Pasca [7] and Cucerzan [8] which also use Wikipedia to disambiguate entities in an unsupervised manner.",
      "startOffset" : 61,
      "endOffset" : 64
    }, {
      "referenceID" : 6,
      "context" : "Bunescu & Pasca [7] use content and categorical information to create features for a scoring model that can disambiguate mentions that appear in wikipedia articles.",
      "startOffset" : 16,
      "endOffset" : 19
    }, {
      "referenceID" : 7,
      "context" : "Cucerzan [8] disambiguates mentions in arbitrary data sources, and is evaluated on news articles.",
      "startOffset" : 9,
      "endOffset" : 12
    } ],
    "year" : 2010,
    "abstractText" : "Cross-document coreference, the problem of resolving entity mentions across multi-document collections, is crucial to automated knowledge base construction and data mining tasks. However, the scarcity of large labeled data sets has hindered supervised machine learning research for this task. In this paper we develop and demonstrate an approach based on “distantly-labeling” a data set from which we can train a discriminative cross-document coreference model. In particular we build a dataset of more than a million people mentions extracted from 3.5 years of New York Times articles, leverage Wikipedia for distant labeling with a generative model (and measure the reliability of such labeling); then we train and evaluate a conditional random field coreference model that has factors on cross-document entities as well as mention-pairs. This coreference model obtains high accuracy in resolving mentions and entities that are not present in the training data, indicating applicability to non-Wikipedia data. Given the large amount of data, our work is also an exercise demonstrating the scalability of our approach.",
    "creator" : "LaTeX with hyperref package"
  }
}