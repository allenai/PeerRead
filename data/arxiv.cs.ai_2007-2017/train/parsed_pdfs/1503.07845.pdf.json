{
  "name" : "1503.07845.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Averaged Hausdorff Approximations of Pareto Fronts based on Multiobjective Estimation of Distribution Algorithms 2015",
    "authors" : [ "Luis Mart́ı", "Christian Grimme" ],
    "emails" : [ "lmarti@ele.puc-rio.br", "christian.grimme@wi.uni-muenster.de", "kerschke@uni-muenster.de", "trautmann@wi.uni-muenster.de", "guenter.rudolph@tu-dortmund.de" ],
    "sections" : [ {
      "heading" : null,
      "text" : "by a finite set of solutions in the objective space. The quality of the approximation can be measured by different indicators that take into account the approximation’s closeness to the Pareto front and its distribution along the Pareto front. In particular, the averaged Hausdorff indicator prefers an almost uniform distribution. An observed drawback of multiobjective estimation of distribution algorithms (MEDAs) is that - as common for randomized metaheuristics - the final population usually is not uniformly distributed along the Pareto front. Therefore, we propose a postprocessing strategy which consists of applying the averaged Hausdorff indicator to the complete archive of generated solutions after optimization in order to select a uniformly distributed subset of nondominated solutions from the archive. In this paper, we put forward a strategy for extracting the above described subset. The effectiveness of the proposal is contrasted in a series of experiments that involve different MEDAs and filtering techniques.\nar X"
    }, {
      "heading" : "1 Introduction",
      "text" : "Many real-world optimization problems involve more than one goal to be optimized and are known as multiobjective optimization problems (MOPs), i.e. a set of objective functions f1(x), . . . , fM (x) should be jointly optimized; formally,\nmin F (x) = ( f1(x), . . . , fM (x) ) ; x ∈ S ; (1)\nwhere S ⊆ Rn is known as the feasible set and could be expressed as a set of restrictions over the decision or search space Rn. The image set O ⊆ RM of S produced by the vector-valued function F (·) is called feasible objective set or criterion set. The solution to this type of problem is a set of trade-off points. The optimality of a solution can be expressed in terms of the Pareto dominance relation.\nDefinition 1 (Pareto dominance) In the optimization problem (1) and having x,y ∈ S, x is said to dominate y (expressed as x ≺ y) if ∀j = 1, . . .M : fj(x) ≤ fj(y) and ∃i ∈ {1, . . . ,M}: fi(x) < fi(y). The non-dominated subset A∗ of set A ⊆ S is defined as\nA∗ = {x ∈ A |6 ∃x′ ∈ A : x′ ≺ x} .\nThe solution of (1) is S∗, the non-dominated subset of S. S∗ is known as the efficient set or Pareto-optimal set [1]. Its image in objective space is known as the Pareto-optimal front, O∗. As finding the explicit formulation of S∗ is often impossible, generally, an algorithm solving (1) yields a discrete non-dominated set, P∗, that approximates S∗. The image of P∗ in objective set, PF∗, is known as the non-dominated front.\nA broad range of heuristic and metaheuristic approaches has been used to address MOPs [1]. Of these, evolutionary multiobjective optimization algorithms (EMOAs) [2] have been found to be a competent approach in a wide variety of application domains. Alternatively, multiobjective estimation of distribution algorithms (MEDAs) were introduced which aim at learning the problem structure and characteristics along the run. In this paper, we will experimentally investigate how representative MEDAs perform compared to classical EMOAs and how especially MEDAs can be improved after the run by means of a postprocessing approach in order to get more equally spaced solutions on the final Pareto front approximation.\nThe crucial task is how to measure the performance in the multiobjective setting, i.e. how to asses the relation of PF∗ to O∗. Several performance indicators have been proposed including the hypervolume indicator or the R indicators, see [3, 4] for an overview. Each indicator concentrates on special desired characteristics of the front approximation while one frequently discussed aim is that elements of PF∗ should be evenly spread along the true Pareto front in order to present an unbiased solution set to the decision maker (e.g. [5]). On the application side, specifically optimization systems or online control could profit from such approximations such that changes from one solution to the other are quite moderate and of equal extent. In [6] an approach for evolving Pareto front approximations with uniform gap is presented. However, most indicators do not address this issue.\nThe ∆p indicator introduced in [7] specifically favors approximations with the desired characteristic based on averaged Hausdorff distances between PF∗ and O∗. Several algorithmic approaches have been introduced so far (e.g. [8, 9, 10] to generate quite equally spaced Pareto front approximations but all of them are focusing on integrating the ∆p based subset selection into the EMOA. In this paper, we present a strategy to be applied posterior to an approximation run to extract a most uniformly distributed subset from the archive of all nondominated solutions generated within the course of the used EMOA.\nThe remainder of this paper is organized as follows: In Section 2 the methodological background is provided regarding multiobjective estimation of distribution algorithms and the postprocessing strategy. Experimental results are presented and discussed in Section 3. Conclusions are drawn in Section 4 supplemented by an outlook on further research directions."
    }, {
      "heading" : "2 Methodology",
      "text" : ""
    }, {
      "heading" : "2.1 Multiobjective Estimation of Distribution Algorithms",
      "text" : "The inclusion of learning as part of the search process can improve the performance of evolutionary multiobjective optimization algorithms [11]. It can be argued that learning would allow to grasp the characteristics of the problem being solved and, hence, explore the search space in a more efficient manner. There are some approaches that perform this task by providing hybrid evolutionary/machine learning methods, like, for example, the learnable evolution model (LEM) [12]. Other approaches attempt to infer the fitness landscape of the problem in order to find promising search directions [13, 14, 15]. Another alternative for carrying out this task is to resort to what has been denominated as estimation of distribution algorithms (EDAs) [16]. This is because of EDAs capacity of learning the problem structure. EDAs replace the application of evolutionary operators in the offspring generation process with the creation of a statistical model of the fittest elements of the population in a process known as model-building. This model is then sampled to produce new elements. Nevertheless, the so called multiobjective EDAs (MEDAs) [17] have not lived up to their a priori expectations. This can be attributed to the fact that most MEDAs have limited themselves to transforming single-objective EDAs into a multiobjective formulation by including an existing multiobjective fitness assignment function.\nBosman and Thierens [18] proposed the successful multiobjective mixture-based iterated density estimation algorithms (MIDEAs). They also proposed a novel Pareto-based and diversitypreserving fitness assignment function. MIDEA considered several types of probabilistic models for both discrete and continuous problems. A mixture of univariate distributions and a mixture of tree distributions were used for discrete variables. A mixture of univariate Gaussian models and a mixture of multivariate Gaussian factorizations were applied for continuous variables. Different variants of MIDEAs implement this in a particular form. For example, naive MIDEA the naive Bayes algorithm.\nIt can also be argued that the covariance matrix adaptation evolution strategies (CMA-ES) [19] are also EDAs, as they construct an abstract model of the population and then sample it to produce new individuals. CMA-ES provides a method for updating the covariance matrix of the multivariate normal mutation distribution used in an evolution strategy [20]. The covariance matrix describes the pairwise dependencies between the variables in the distribution. Adaptation of the covariance matrix is equivalent to learning a second-order model of the underlying objective function. The multi-objective CMA-ES (MO-CMA-ES) [21] is the extrapolation to the multiobjective domain.\nIt has been pointed out that the current model-building algorithms of MEDAs have a set of drawbacks that would prevent those algorithms from yielding substantially better results. In particular, the tendency of MEDAs loosing population diversity has repeatedly been reported [22, 23]. This situation, although already described in single-objective EDAs [24], is particularly dramatic in the multiobjective case, as diversity and homogeneity are among the desired features of the final non-dominated set. An analysis of the results yielded by current MEDAs and their scalability with regard to the number of objectives leads to hypotheses regarding the issues that might be hampering the obtention of substantially better results with regard to other evolutionary approaches [25].\nThe incorrect handling of data outliers is a paradigmatic example of insufficient comprehension of the nature of the model building problem. In ‘standard’ machine learning practice, outliers are treated as noisy, inconsistent or irrelevant data. Therefore, this type of data is expected to have little influence on the model or just to be disregarded. However, that behavior is not adequate for model building. In this case, it is known beforehand that all elements in the data set should be taken into account as they represent newly discovered or candidate regions of the search space and therefore must be explored. Therefore, these instances should be at least equally represented by the model and perhaps even reinforced. This situation is further aggravated by the mating selection scheme employed in most MEDAs. The continuous selection of the best part of the\npopulation might lead to a premature homogenization of the population and, therefore, to the stagnation of the search process. In the second case, the loss of diversity can be traced back to the above-described outliers issue of model-building algorithms and also to the incorrect estimation or sampling of the model. This fact leads us back to the statement referring that model building has not been correctly acknowledged as a different problem with particular requirements.\nThere have been some works that propose to improve this situation by introducing model building algorithms better suited for the task. That is the case of the MIDEA algorithm family, with the introduction of the adaptive variance scaling (AVS) and the standard deviation ratio (SDR) [26]. The AVS and SDR combination helps fight the early reduction of the mixture densities variances and therefore the premature convergence and diversity loss. Another important milestone has been the introduction of the anticipated mean shift (AMS) that takes into account the previous values of the means of the distribution to “push” solutions towards the Pareto-optimal front. AMS has been conjointly used with AVS in the multiobjective adapted maximum-likelihood Gaussian mixture model (MAMaLGaM-X) [27]. Similarly, the multiobjective optimization neural EDA (MONEDA) [28] embeds a custom-made model building algorithm [29] that is able to maintain diversity by correctly handling the outlier elements. This approach has been improved by the introduction of the match-based learning paradigm of adaptive resonance theory (ART) [30] leading to the multiobjective ART EDA (MARTEDA) [31].\nOther approaches propose modifications to currently existing methods. For example, in [23] a method that avoids too sudden drops (to zero) of the variances of a multivariate Gaussian model is proposed. Similarly, in [32] a permutation sampling is introduced that eliminates the sampling errors of UMDA [33]. Other approaches [34, 35] have tried strategies to re-inject “fresh” individuals that are kept on a evolutionary algorithm that is run in parallel. In spite of these efforts, the issue of population diversity and lack of homogeneity is still an open topic that must be addressed."
    }, {
      "heading" : "2.2 Postprocessing of MEDA results",
      "text" : "Suppose we are particularly interested in a finite size approximation of O∗ = F (S∗) that exhibits a sufficiently good spread and a small distance to O∗. Previous work in the context of EMOA (see e.g. [9]) has shown that the averaged Hausdorff distance [7] can be used to achieve this goal.\nDefinition 2 Let A,B ⊂ RM be non-empty finite sets. The value\n∆p(A,B) = max(GDp(A,B), IGDp(A,B)) with\nGDp(A,B) =\n( 1\n|A| ∑ a∈A d(a,B)p\n)1/p and\nIGDp(A,B) =\n( 1\n|B| ∑ b∈B d(b, A)p )1/p for p > 0 is termed the averaged Hausdorff distance between sets A and B where d(u,A) := inf{‖u− v‖ : v ∈ A} for u, v ∈ RM and some vector norm ‖ · ‖. Suppose that some MEDA has generated an approximation of the Pareto front for some MOP. Typically, this approximation does not yield a finite point set in objective space that is evenly distributed. Therefore, we propose the following postprocessing approach:\n1. Run your favorite MEDA that is equipped with a tiny add-on: store a copy of each offspring that is generated and evaluated in a file.\n2. After termination of your favorite MEDA: construct an evenly spaced reference front from a given approximation of the Pareto front (e.g. from last population of favorite MEDA); then feed each stored offspring into the ∆p archive updater (see alg. 1) sequentially; the final content of the archive A is the desired approximation.\nFor bi-objective problems the reference front R can be constructed as follows: calculate a linear interpolation from a given approximation of the Pareto front. Since the length L of the resulting polygonal line can be derived easily, division by the number of desired archive points m yields the size of the equal spacing δ = L/m which is used to place m points along the polygonal line with distance δ, where the extreme points of the discretization are moved half the length inwards the polygonal line. After the reference front R has been constructed it is used in the ∆p archive updater to decide which point should be added to or deleted from the archive.\nAn update operation can be realized as sketched in alg.1. This naive approach takes Θ(|A| · (|A| · |R| ·M)) time units, but there exists a quick update version [9] that only needs Θ(|A| · |R| ·M) time units.\nAlgorithm 1 Naive ∆1-update [9]\nRequire: archive set A, reference set R, new element x 1: A = NDf (A ∪ {x}, ) 2: if |A| > NR := |R| then 3: for all a ∈ A do 4: h(a) = ∆1(A \\ {a}, R) 5: end for 6: A∗ = {a∗ ∈ A : a∗ = argmin{h(a) : a ∈ A}} 7: if |A∗| > 1 then 8: a∗ = argmin{GDp(A \\ {a}, R) : a ∈ A∗} 9: end if 10: A = A \\ {a∗} 11: end if\nThe most obvious order of feeding the stored pairs (x, F (x)) into the archive updater is the order of their generation. We call this the ‘forward update.’ In this manner, many individuals will pass the initial dominance check, so that subsequent ∆p calculations are necessary. Some time saving may be achieved by feeding the stored pairs into the archive update in inverted order. We call this the ‘backward update.’ Since points that have been generated in later iterations of the MEDA are more likely to dominate previous points, most points from the rear of the inverted sequence will probably not pass the initial dominance check, so that subsequent ∆p calculations can be avoided. Since the order of the points presented to the archive clearly affects the final outcome of the archive, we shall compare both approaches experimentally in the next section."
    }, {
      "heading" : "3 Experiments",
      "text" : ""
    }, {
      "heading" : "3.1 Experimental Setup",
      "text" : "In order to experimentally evaluate the ∆p archive approximation for multiple multiobjective EDAs, we implemented two offline strategies in the JAVATM jMetal framework [36]: a forward ∆p-approach (+fDP ), starting with the initial population and iteratively updating with previously traced generations, whereas the backward-strategy (+bDP ) starts with the final population. Within the ∆p-update mechanisms, a linear interpolation and a PSA-based strategy, both using the aggregated solutions of the whole run, were used for deriving a set of evenly spaced reference points. Based on that set, a solution was eventually added to the archive.\nWe then used well known MOPs for benchmarking [8]: an instance of the sphere problem with X ⊆R2 and a connected convex Pareto front, the DENT problem with X ⊆R2 and a connected convex-concave-convex front, ZDT3 with X ⊆ R20 and a disconnected convex/concave front, as well as WFG1 with X⊆R2+4, also with a convex-concave front. Then, reference fronts covered by 1,000 uniformly spaced points were created in order to compare our ∆p approximation quality for all algorithms. With the exception of WFG1, we were able to\nfind a parametric form for exactly calculating the optimal fronts’ length L (by rectification), for all benchmark problems. Given that form 1,000 points were placed uniformly on the PF using distance ∆L = L/1,000 between points and starting with the extreme points moved inwards by ∆L/ 2. In case of WFG1 [37], the neighborhood of the known Pareto set was explored using a very fine grid (per dimension of the decision space), in order to find a PF. Then 1,000 welldistributed solutions among the non-dominated solutions were kept. As the number of reference solutions is by far larger than the approximated set’s size, a not perfectly uniform distribution of the Pareto-optimal solutions is acceptable.\nOur evaluation is based on four state-of-the-art general purpose EMOAs and four MEDAs. From the former group, we employed the NSGA2 [5] and SMS-EMOA [38] with standard parameter settings – SBX crossover with px = 0.9 and polynomial mutation with pmut = 1/n – as well as PSEMOA as special purpose EMOA with inherent internal clustering for generating evenly distributed solutions. For the same reason we also used a modified version of NSGA2 [39] – the sequential crowding distance NSGA2 (SCD-NSGA2) – which successively alternates between removing the individuals with the smallest crowding distance and recomputing the crowding distance values, until only µ solutions are left.\nConsidering the MEDAs we deal with two well-known approaches: the naive MIDEA [18] and the multiobjective CMA-ES (MO-CMA-ES) [21]. We also include MONEDA [28] and MARTEDA [31] as they are supposed to have a better handling of diversity. The parameters of the MEDAs are summarized in Table 1.\nDuring our experiments, all test problems were optimized 20 times by all algorithms, given a budget of 50,000 function evaluations each and population sizes µ ∈ {10, 20, 100}. The data, which were generated from the optimizers’ runs, have been submitted to the offline ∆p archivers. Both update strategies were parameterized with p ∈ {1, 2}."
    }, {
      "heading" : "3.2 Results",
      "text" : "At first, overall algorithm behavior is investigated by comparing the MEDAs with the classical EMOA approaches in terms of the Dominated Hypervolume indicator (HV, [4]) as well as ∆p (Figures 1 and 2) based on the generated reference fronts as described in Section 3.1. It becomes obvious that the MEDAs lead to very stable results over the repeated runs which is reflected by both indicators while no MEDA is superior to the others. Moreover, for all test problems the respective performance is comparable to the best performing algorithm out of the classical EMOA set. While over all test problems but WFG1 PSEMOA and SCD-NSGA2 can be considered as best regarding ∆p, the SMS-EMOA unsurprisingly outperforms the other classical EMOA in terms of HV as it internally optimizes the HV indicator. The test problem WFG1 results in different algorithm rankings within the classical EMOA, i.e. NSGA2 clearly is best regarding both indicators. The stability of MEDA results is very noticeable here as the variability of PSEMOA, SMS-EMOA as well as SCD-NSGA2 is large. Concluding, the MEDAs are at the least competitive with the classical EMOAs on the considered test problems.\nIn Figures 3 - 5 results of the postprocessing strategies applied to the MEDA variants are visualized in terms of ∆p regarding the generated reference fronts for population sizes µ ∈ {10, 20, 100}. In general, experimental results become more distinguishable with increasing population size but the basic findings are reflected for the smallest population size as well. However, varying p does not have a noticeable effect in this setting, thus the analysis concentrates on p = 1 for illustration purposes.\nClearly, the postprocessing improves the final approximation quality of the MEDA’s run as the primary algorithm in terms of ∆p for all population sizes. As the Pareto front of ZDT3 is disconnected, the performance of the considered postprocessing differs from the remaining test problems. Here, the two PSA-based strategies (+(fPSA) and +(bPSA)) outperform the archive strategy using linear interpolations of the set of all nondominated points generated in the course of the algorithm run (+(fDP ) and +(bDP )). This is intuitive as the latter approach ignores the discontinuity in the interpolation and tends to place reference points also in unattainable regions [40]. Thus, this effect increases with the population size as more points have to placed on the linear interpolation of the front. For µ = 10 the strategies +(fDP ) and +(bDP ) are comparable or even slightly better than the PSA-based ones. For all other test problems +(fDP ) and +(bDP ) generate the largest improvement in terms of ∆p. Differences are statistically significant in most settings reflected by a Wilcoxon rank sum test (p ≤ 0.05) and annotated with a ? in Figures 3 - 5.\nInterestingly, for µ = 100 and WFG1 differences between the primary MEDA and all postprocessing variants are very small and not statistically significant. This is due to the special challenges the test problem poses onto the algorithms. Thorough analysis of the final approximation sets of the MEDAs revealed that those runs do not allow for substantial improvements of the final solution sets as the algorithm performance is not good enough and the archive of solutions does not provide the means for selecting a better subset regarding ∆p. However, substantially increasing the budget of the MEDAs will lead to results showing the same tendencies as described above for smaller population sizes. As expected, the direction of the archive update strategy has no influence on the final approximation quality. However, our experiments revealed that the backward strategy which starts from the final population of the primary MEDA requires much less updating iterations until the selected subset becomes stable than the forward strategy. Therefore, we recommend to favor this approach over the forward update in general such that the postprocessing will be much more computationally efficient."
    }, {
      "heading" : "4 Conclusions and Outlook",
      "text" : "Within this work, we first compared the quality of the solutions of four evolutionary multiobjective optimization algorithms (NSGA2, PSEMOA, SCD-NSGA2, SMS-EMOA) and four multiobjective estimation of distribution algorithms (MARTEDA, MO-CMA-ES, MONEDA, Naive MIDEA) on a set of well-known multiobjective test problems. Based on two considered indicators – HV and ∆p – it could be shown that the performance of the MEDAs is at least competitive to classical EMOAs. Also, the MEDAs itself performed quite similar to each other.\nDue to those competitive and thus promising results, we analyzed whether the performance of the latter ones regarding ∆p could even be improved by a subsequent application of either forward or backward ∆p-update strategies which will lead to a more uniformly distributed solutions along the approximated Pareto front. Experiments confirmed this hypothesis and apparently the influence of using either one of those update directions is negligible. On the other hand, the ∆p-based techniques outperform the PSA-based ones besides for problems with disconnected Pareto fronts.\nIn future work, one might want to improve the MEDAs performance either based on the PSA- or ∆p-update strategy by integrating that approach into the algorithm’s mechanics. This hopefully leads to algorithms, which generate comparable or even better solutions within a single run and therefore without employing any additional postprocessing."
    } ],
    "references" : [ {
      "title" : "Towards a New Evolutionary Computation: Advances on Estimation of Distribution Algorithms",
      "author" : [ "J.A. Lozano", "P. Larrañaga", "I. Inza", "E. Bengoetxea", "editors" ],
      "venue" : null,
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2006
    }, {
      "title" : "Multiobjective estimation of distribution algorithms",
      "author" : [ "Martin Pelikan", "Kumara Sastry", "David E. Goldberg" ],
      "venue" : null,
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2006
    }, {
      "title" : "The naive midea: A baseline multi-objective ea",
      "author" : [ "Peter AN Bosman", "Dirk Thierens" ],
      "venue" : "In Evolutionary Multi-Criterion Optimization,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2005
    }, {
      "title" : "Reducing the time complexity of the derandomized evolution strategy with covariance matrix adaptation (CMA–ES)",
      "author" : [ "N. Hansen", "S.D. Muller", "P. Koumoutsakos" ],
      "venue" : "Evolutionary Computation,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2003
    }, {
      "title" : "Evolution strategies — A comprehensive introduction",
      "author" : [ "Hans-Georg Beyer", "Hans-Paul Schwefel" ],
      "venue" : "Natural Computing: an international journal,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2002
    }, {
      "title" : "Covariance matrix adaptation for multiobjective optimization",
      "author" : [ "Christian Igel", "Nikolaus Hansen", "Stefan Roth" ],
      "venue" : "Evolutionary computation,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2007
    }, {
      "title" : "Multiobjective real-coded Bayesian optimization algorithm revisited: Diversity preservation",
      "author" : [ "Chang Wook Ahn", "R.S. Ramakrishna" ],
      "venue" : "Proceedings of the 9th annual conference on Genetic and evolutionary computation,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2007
    }, {
      "title" : "On the importance of diversity maintenance in estimation of distribution algorithms",
      "author" : [ "Bo Yuan", "Marcus Gallagher" ],
      "venue" : "Proceedings of the 2005 conference on Genetic and evolutionary computation,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2005
    }, {
      "title" : "Diversity loss in general estimation of distribution algorithms. In Parallel Problem Solving from Nature ",
      "author" : [ "Jonathan Shapiro" ],
      "venue" : "PPSN IX,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2006
    }, {
      "title" : "Scalable Multi-Objective Optimization",
      "author" : [ "Luis Mart́ı" ],
      "venue" : "PhD thesis, Departmento de Informática, Universidad Carlos III de Madrid, Colmenarejo,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2011
    }, {
      "title" : "Adaptive variance scaling in continuous multiobjective estimation-of-distribution algorithms",
      "author" : [ "Peter A.N. Bosman", "Dirk Thierens" ],
      "venue" : "In Proceedings of the 9th annual conference on Genetic and evolutionary computation - GECCO",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2007
    }, {
      "title" : "The anticipated mean shift and cluster registration in mixture-based EDAs for multi-objective optimization",
      "author" : [ "Peter A.N. Bosman" ],
      "venue" : "In Proceedings of the 12th annual conference on Genetic and evolutionary computation - GECCO",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2010
    }, {
      "title" : "Introducing moneda: Scalable multiobjective optimization with a neural estimation of distribution algorithm",
      "author" : [ "Luis Mart́ı", "Jesús Garćıa", "Antonio Berlanga", "José Manuel Molina" ],
      "venue" : "In Proceedings of the 10th Annual Conference on Genetic and Evolutionary Computation,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2008
    }, {
      "title" : "MB-GNG: Addressing drawbacks in multi-objective optimization estimation of distribution algorithms",
      "author" : [ "Luis Mart́ı", "Jesús Garćıa", "Antonio Berlanga", "Carlos A. Coello Coello", "José Manuel Molina" ],
      "venue" : "Operations Research Letters,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2011
    }, {
      "title" : "Studies of Mind and Brain: Neural Principles of Learning, Perception, Development, Cognition, and Motor Control",
      "author" : [ "Stephen Grossberg" ],
      "venue" : null,
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 1982
    }, {
      "title" : "Multi-objective optimization with an adaptive resonance theory-based estimation of distribution algorithm: a comparative study",
      "author" : [ "Luis Mart́ı", "Jesús Garćıa", "Antonio Berlanga", "José M Molina" ],
      "venue" : "In Learning and Intelligent Optimization,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2011
    }, {
      "title" : "Addressing sampling errors and diversity loss in umda",
      "author" : [ "Jüergen Branke", "Clemens Lode", "Jonathan L. Shapiro" ],
      "venue" : "In Proceedings of the 9th annual conference on Genetic and evolutionary computation — GECCO",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2007
    }, {
      "title" : "From recombination of genes to the estimation of distributions I",
      "author" : [ "H. Mühlenbein", "G. Paaß" ],
      "venue" : "Binary parameters",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 1996
    }, {
      "title" : "GAEDA: Hybrid evolutionary algorithm using genetic and estimation of distribution algorithms",
      "author" : [ "J.M. Pena", "Victor Robles", "Pedro Larrañaga", "V. Herves", "F. Rosales", "M.S. Pérez" ],
      "venue" : "In Innovations in Applied Artificial Intelligence,",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2004
    }, {
      "title" : "An evolutionary algorithm with guided mutation for the maximum clique problem",
      "author" : [ "Q. Zhang", "Jianyong Sun", "Edward Tsang" ],
      "venue" : "IEEE Transactions on Evolutionary Computation,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 2005
    }, {
      "title" : "jmetal: A java framework for multi-objective optimization",
      "author" : [ "Juan J Durillo", "Antonio J Nebro" ],
      "venue" : "Advances in Engineering Software,",
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 2011
    }, {
      "title" : "A review of multiobjective test problems and a scalable test problem toolkit",
      "author" : [ "Simon Huband", "Philip Hingston", "Luigi Barone", "Lyndon While" ],
      "venue" : "Evolutionary Computation, IEEE Transactions on,",
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 2006
    }, {
      "title" : "Sms-emoa: Multiobjective selection based on dominated hypervolume",
      "author" : [ "Nicola Beume", "Boris Naujoks", "Michael Emmerich" ],
      "venue" : "European Journal of Operational Research,",
      "citeRegEx" : "38",
      "shortCiteRegEx" : "38",
      "year" : 2007
    }, {
      "title" : "Improved pruning of non-dominated solutions based on crowding distance for bi-objective optimization problems",
      "author" : [ "Saku Kukkonen", "Kalyanmoy Deb" ],
      "venue" : "In Proceedings of the World Congress on Computational Intelligence",
      "citeRegEx" : "39",
      "shortCiteRegEx" : "39",
      "year" : 2006
    }, {
      "title" : "Homogene Approximation der Paretofront bei mehrkriteriellen Kontrollproblemen",
      "author" : [ "G. Rudolph", "H. Trautmann", "O. Schütze" ],
      "venue" : "Automatisierungstechnik (at),",
      "citeRegEx" : "40",
      "shortCiteRegEx" : "40",
      "year" : 2012
    } ],
    "referenceMentions" : [ ],
    "year" : 2015,
    "abstractText" : "In the a posteriori approach of multiobjective optimization the Pareto front is approximated by a finite set of solutions in the objective space. The quality of the approximation can be measured by different indicators that take into account the approximation’s closeness to the Pareto front and its distribution along the Pareto front. In particular, the averaged Hausdorff indicator prefers an almost uniform distribution. An observed drawback of multiobjective estimation of distribution algorithms (MEDAs) is that as common for randomized metaheuristics the final population usually is not uniformly distributed along the Pareto front. Therefore, we propose a postprocessing strategy which consists of applying the averaged Hausdorff indicator to the complete archive of generated solutions after optimization in order to select a uniformly distributed subset of nondominated solutions from the archive. In this paper, we put forward a strategy for extracting the above described subset. The effectiveness of the proposal is contrasted in a series of experiments that involve different MEDAs and filtering techniques. ar X iv :1 50 3. 07 84 5v 1 [ cs .A I] 2 6 M ar 2 01 5",
    "creator" : "LaTeX with hyperref package"
  }
}