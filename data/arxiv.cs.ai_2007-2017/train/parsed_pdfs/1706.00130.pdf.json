{
  "name" : "1706.00130.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Teaching Machines to Describe Images via Natural Language Feedback",
    "authors" : [ "Huan Ling", "Sanja Fidler" ],
    "emails" : [ "linghuan@cs.toronto.edu", "fidler@cs.toronto.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "In the era where A.I. is slowly finding its way into everyone’s lives, be in the form of social bots [35, 2], personal assistants [24, 13, 31], or household robots [1], it becomes critical to allow non-expert users to teach and guide their robots [36, 18]. For example, if a household robot keeps bringing food served on an ashtray thinking it’s a plate, one should ideally be able to educate the robot about its mistakes, possibly without needing to dig into the underlying software.\nReinforcement learning has become a standard way of training artificial agents that interact with an environment. There have been significant advances in a variety of domains such as games [30, 25], robotics [17], and even fields like vision and NLP [29, 19]. RL agents optimize their action policies so as to maximize the expected reward received from the environment. Training typically requires a large number of episodes, particularly in environments with large action spaces and sparse rewards.\nSeveral works explored the idea of incorporating humans in the learning process, in order to help the reinforcement learning agent to learn faster [34, 12, 11, 6, 5]. In most cases, a human teacher observes the agent act in an environment, and is allowed to give additional guidance to the learner. This feedback typically comes in the form of a simple numerical (or “good”/“bad”) reward which is used to either shape the MDP reward [34] or directly shape the policy of the learner [5].\nIn this paper, we aim to exploit natural language as a way to guide an RL agent. We argue that a sentence provides a much stronger learning signal than a numeric reward in that it can easily point to where the mistakes occur and suggests how to correct them. Such descriptive feedback can thus naturally facilitate solving the credit assignment problem as well as to help guide exploration. Despite its clear benefits, very few approaches aimed at incorporating language in Reinforcement Learning. In pioneering work, [22] translated natural language advice into a short program which was used to bias action selection. While this is possible in limited domains such as in navigating a maze [22] or learning to play a soccer game [15], it can hardly scale to the real scenarios with large action spaces requiring versatile language feedback.\nar X\niv :1\n70 6.\n00 13\n0v 2\n[ cs\n.C L\n] 5\nJ un\n2 01\nHere our goal is to allow a non-expert human teacher to give feedback to an RL agent in the form of natural language, just as one would to a learning child. We focus on the problem of image captioning in which the quality of the output can easily be judged by non-experts.\nTowards this goal, we make several contributions. We propose a hierarchical phrase-based RNN as our image captioning model, as it can be naturally integrated with human feedback. We design a web interface which allows us to collect natural language feedback from human “teachers” for a snapshot of our model, as in Fig. 1. We show how to incorporate this information in Policy Gradient RL [29], and show that we can improve over RL that has access to the same amount of ground-truth captions. Our code and data will be released (http://www.cs.toronto.edu/~linghuan/feedbackImageCaption/) to facilitate more human-like training of captioning models."
    }, {
      "heading" : "2 Related Work",
      "text" : "Several works incorporate human feedback to help an RL agent learn faster. [34] exploits humans in the loop to teach an agent to cook in a virtual kitchen. The users watch the agent learn and may intervene at any time to give a scalar reward. Reward shaping [26] is used to incorporate this information in the MDP. [6] iterates between “practice”, during which the agent interacts with the real environment, and a critique session where a human labels any subset of the chosen actions as good or bad. In [12], the authors compare different ways of incorporating human feedback, including reward shaping, Q augmentation, action biasing, and control sharing. The same authors implement their TAMER framework on a real robotic platform [11]. [5] proposes policy shaping which incorporates right/wrong feedback by utilizing it as direct policy labels. These approaches mostly assume that humans provide a numeric reward, unlike in our work where the feedback is given in natural language.\nA few attempts have been made to advise an RL agent using language. [22]’s pioneering work translated advice to a short program which was then implemented as a neural network. The units in this network represent Boolean concepts, which recognize whether the observed state satisfies the constraints given by the program. In such a case, the advice network will encourage the policy to take the suggested action. [15] incorporated natural language advice for a RoboCup simulated soccer task. They too translate the advice in a formal language which is then used to bias action selection. Parallel to our work, [7] exploits textual advice to improve training time of the A3C algorithm in playing an Atari game. Recently, [36, 18] incorporates human feedback to improve a text-based QA agent. Our work shares similar ideas, but applies them to the problem of image captioning.\nCaptioning represents a natural way of showing that our algorithm understands a photograph to a non-expert observer. This domain has received significant attention [8, 38, 10], achieving impressive performance on standard benchmarks. Our phrase model shares the most similarity with [16], but differs in that exploits attention [38], linguistic information, and RL to train. Several recent approaches trained the captioning model with policy gradients in order to directly optimize for the desired performance metrics [21, 29, 3]. We follow this line of work. However, to the best of our knowledge, our work is the first to incorporate natural language feedback into a captioning model. Related to our efforts is also work on dialogue based visual representation learning [39, 40], however this work tackles a simpler scenario, and employs a slightly more engineered approach.\nWe stress that our work differs from the recent efforts in conversation modeling [19] or visual dialog [4] using Reinforcement Learning. These models aim to mimic human-to-human conversations while in our work the human converses with and guides an artificial learning agent."
    }, {
      "heading" : "3 Our Approach",
      "text" : "Our framework consists of a new phrase-based captioning model trained with Policy Gradients that incorporates natural language feedback provided by a human teacher. While a number of captioning methods exist, we design our own which is phrase-based, allowing for natural guidance by a nonexpert. In particular, we argue that the strongest learning signal is provided when the feedback describes one mistake at a time, e.g. a single wrong word or a phrase in a caption. An example can be seen in Fig. 1. This is also how one most effectively teaches a learning child. To avoid parsing the generated sentences at test time, we aim to predict phrases directly with our captioning model. We first describe our phrase-based captioner, then describe our feedback collection process, and finally propose how to exploit feedback as a guiding signal in policy gradient optimization."
    }, {
      "heading" : "3.1 Phrase-based Image Captioning",
      "text" : "Our captioning model, forming the base of our approach, uses a hierarchical Recurrent Neural Network, similar to [33, 14]. In [14], the authors use a two-level LSTM to generate paragraphs, while [33] uses it to generate sentences as a sequence of phrases. The latter model shares a similar overall structure as ours, however, our model additionally reasons about the type of phrases and exploits the attention mechanism over the image.\nThe structure of our model is best explained through Fig. 2. The model receives an image as input and outputs a caption. It is composed of a phrase RNN at the top level, and a word RNN that generates a sequence of words for each phrase. One can think of the phrase RNN as providing a “topic” at each time step, which instructs the word RNN what to talk about.\nFollowing [38], we use a convolutional neural network in order to extract a set of feature vectors a = (a1, . . . ,an), with aj a feature in location j in the input image. We denote the hidden state of the phrase RNN at time step t with ht, and ht,i to denote the i-th hidden state of the word RNN for the t-th phrase. Computation in our model can be expressed with the following equations:\nph ra\nse -R\nN N ︸ ︷︷\n︸\nw or\ndR\nN N ︸︷︷︸\nht = fphrase(ht−1, lt−1, ct−1, et−1)\nlt = softmax(fphrase−label(ht))\nct = fatt(ht, lt, a)\nht,0 = fphrase−word(ht, lt, ct)\nht,i = fword(ht,i−1, ct, wt,i)\nwt,i = fout(ht,i, ct, wt,i−1)\net = fword−phrase(wt,1, . . . , wt,end)\nfphrase LSTM, dim 256\nfphrase−label 3-layer MLP\nfatt 2-layer MLP with ReLu fphrase−word 3-layer MLP with ReLu\nfword LSTM, dim 256 fout deep decoder [27] fword−phrase mean+3-lay. MLP with ReLu\nAs in [38], ct denotes a context vector obtained by applying the attention mechanism to the image. This context vector essentially represents the image area that the model “looks at” in order to generate the t-th phrase. This information is passed to both the word-RNN as well as to the next hidden state\n500\n1000\n1500\n2000\n2500\nAbove, wt,i denotes the i-th word output of the word-RNN in the t-th phrase, encoded with a one-hot vector. Note that we use an additional <EOP> token in word-RNN’s vocabulary, which signals the end-of-phrase. Further, et encodes the generated phrase via simple mean-pooling over the words, which provides additional word-level context to the next phrase. Details about the choices of the functions are given in the table. Following [38], we use a deep output layer [27] in the LSTM and double stochastic attention.\nImplementation details. To train our hierarchical model, we first process MS-COCO image caption data [20] using the Stanford Core NLP toolkit [23]. We flatten each parse tree, separate a sentence into parts, and label each part with a phrase label (<NP>, <PP>, <CP>, <VP>). To simplify the phrase structure, we merge some NPs to its previous phrase label if it is not another NP.\nPre-training. We pre-train our model using the standard cross-entropy loss. We use the ADAM optimizer [9] with learning rate 0.001. We discuss Policy Gradient optimization in Subsec. 3.4."
    }, {
      "heading" : "3.2 Crowd-sourcing Human Feedback",
      "text" : "We aim to bring a human in the loop when training the captioning model. Towards this, we create a web interface that allows us to collect feedback information on a larger scale via AMT. Our interface is akin to that depicted in Fig. 1, and we provide further visualizations in the Appendix. We also provide it online on our project page. In particular, we take a snapshot of our model and generate\ncaptions for a subset of MS-COCO images [20] using greedy decoding. In our experiments, we take the model trained with the MLE objective.\nWe do two rounds of annotation. In the first round, the annotator is shown a captioned image and is asked to assess the quality of the caption, by choosing between: perfect, acceptable, grammar mistakes only, minor or major errors. We asked the annotators to choose minor and major error if the caption contained errors in semantics, i.e., indicating that the “robot” is not understanding the photo correctly. We advised them to choose minor for small errors such as wrong or missing attributes or awkward prepositions, and go with major errors whenever any object or action naming is wrong.\nFor the next (more detailed, and thus more costly) round of annotation, we only select captions which are not marked as either perfect or acceptable in the first round. Since these captions contain errors, the new annotator is required to provide detailed feedback about the mistakes. We found that some of the annotators did not find errors in some of these captions, pointing to the annotator noise in the process. The annotator is shown the generated caption, delineating different phrases with the “(” and “)” tokens. We ask the annotator to 1) choose the type of required correction, 2) write feedback in natural language, 3) mark the type of mistake, 4) highlight the word/phrase that contains the mistake, 5) correct the chosen word/phrase, 6) evaluate the quality of the caption after correction. We allow the annotator to submit the HIT after one correction even if her/his evaluation still points to errors. However, we plea to the good will of the annotators to continue in providing feedback. In the latter case, we reset the webpage, and replace the generated caption with their current correction.\nThe annotator first chooses the type of error, i.e., something “ should be replaced”, “is missing”, or “should be deleted”. (S)he then writes a sentence providing feedback about the mistake and how it should be corrected. We require that the feedback is provided sequentially, describing a single mistake at a time. We do this by restricting the annotator to only select mistaken words within a single phrase (in step 4). In 3), the annotator marks further details about the mistake, indicating whether it corresponds to an error in object, action, attribute, preposition, counting, or grammar. For 4) and 5) we let the annotator highlight the area of mistake in the caption, and replace it with a correction.\nThe statistics of the data is provided in Table 2, with examples shown in Table 8. An interesting fact is that the feedback sentences in most cases mention both the wrong word from the caption, as well as the correction word. Fig. 3 (left) shows evaluation of the caption quality of the reference (MLE) model. Out of 9000 captions, 5150 are marked as containing errors (either semantic or grammar), and we randomly choose 4174 for the second round of annotation (detailed feedback). Fig. 3 (left) shows the quality of all the captions after correction, i.e. good reference captions as well as 4174 corrected captions as submitted by the annotators. Note that we only paid for one round of feedback, thus some of the captions still contained errors even after correction. Interestingly, on average the annotators still did 2.2 rounds of feedback per image (Table 2)."
    }, {
      "heading" : "3.3 Feedback Network",
      "text" : "Our goal is to incorporate natural language feedback into the learning process. The collected feedback contains rich information of how the caption can be improved: it conveys the location of the mistake and typically suggests how to correct it, as seen in Table 2. This provides strong supervisory signal which we want to exploit in our RL framework. In particular, we design a neural network which will provide additional reward based on the feedback sentence. We refer to it as the feedback network (FBN). We first explain our feedback network, and show how to integrate its output in RL.\nNote that RL training will require us to generate samples (captions) from the model. Thus, during training, the sampled captions for each training image will change (will differ from the reference MLE caption for which we obtained feedback for). The goal of the feedback network is to read a newly sampled caption, and judge the correctness of each phrase conditioned on the feedback. We make our FBN to only depend on text (and not on the image), making its learning task easier. In particular, our FBN performs the following computation:\nhcaptiont = fsent(h caption t−1 , w c t ) (1)\nhfeedbackt = fsent(h feedback t−1 , w f t ) (2)\nqi = fphrase(w c i,1, . . . , w c i,N ) (3)\noi = ffbn(h c T , h f T ′ , qi,m) (4)\nfsent LSTM, dim 256 fphrase linear+mean pool ffbn 3-layer MLP with dropout\n+3-way softmax\nHere, wct and w f t denote the one-hot encoding of words in the sampled caption and feedback sentence, respectively. By wci,· we denote words in the i-th phrase of the sampled caption. FBN thus encodes both the caption and feedback using an LSTM (with shared parameters), performs mean pooling over the words in a phrase to represent the phrase i, and passes this information through a 3-layer MLP. The MLP additionally accepts information about the mistake type (e.g., wrong object/action) encoded as a one hot vector m (denoted as “extra information” in Fig. 4). The output layer of the MLP is a 3-way classification layer that predicts whether the phrase i is correct, wrong, or not relevant (wrt feedback sentence). An example output from FBN is shown in Table 3.\nImplementation details. We train our FBN with the ground-truth data that we collected. In particular, we use (reference, feedback, marked phrase in reference caption) as an example of a wrong phrase, (corrected sentence, feedback, marked phrase in corrected caption) as an example of the correct phrase, and treat the rest as the not relevant label. Reference here means the generated caption that we collected feedback for, and marked phrase means the phrase that the annotator highlighted in either the reference or the corrected caption. We use the standard cross-entropy loss to train our model. We use ADAM [9] with learning rate 0.001, and a batch size of 256. When a reference caption has several feedback sentences, we treat each one as independent training data."
    }, {
      "heading" : "3.4 Policy Gradient Optimization using Natural Language Feedback",
      "text" : "We follow [29, 28] to directly optimize for the desired image captioning metrics using the Policy Gradient technique. For completeness, we briefly summarize it here [29].\nOne can think of an caption decoder as an agent following a parameterized policy pθ that selects an action at each time step. An “action” in our case requires choosing a word from the vocabulary (for the word RNN), or a phrase label (for the phrase RNN). An “agent” (our captioning model) then receives the reward after generating the full caption, i.e., the reward can be any of the automatic metrics, their weighted sum [29, 21], or in our case will also include the reward from feedback.\nThe objective for learning the parameters of the model is the expected reward received when completing the caption ws = (ws1, . . . , w s T ) (w s t is the word sampled from the model at time step t):\nL(θ) = −Ews∼pθ [r(ws)] (5) To optimize this objective, we follow the reinforce algorithm [37], as also used in [29, 28]. The gradient of (5) can be computed as\n∇θL(θ) = −Ews∼pθ [r(ws)∇θ log pθ(ws)], (6) which is typically estimated by using a single Monte-Carlo sample:\n∇θL(θ) ≈ −r(ws)∇θ log pθ(ws) (7)\nWe follow [29] to define the baseline b as the reward obtained by performing greedy decoding: b = r(ŵ), ŵt = argmax p(wt|ht) ∇θL(θ) ≈ −(r(ws)− r(ŵ))∇θ log pθ(ws) (8)\nNote that the baseline does not change the expected gradient but can drastically reduce its variance.\nReward. We define two different rewards, one at the sentence level (optimizing for a performance metrics), and one at the phrase level. We use human feedback information in both. We first define the sentence reward wrt to a reference caption as a weighted sum of the BLEU scores:\nr(ws) = β ∑ i λi ·BLEUi(ws, ref) (9)\nIn particular, we choose λ1 = λ2 = 0.5, λ3 = λ4 = 1, λ5 = 0.3. As reference captions to compute the reward, we either use the reference captions generated by a snapshot of our model which were evaluated as not having minor and major errors, or ground-truth captions. The details are given in the experimental section. We weigh the reward by the caption quality as provided by the annotators. In particular, β = 1 for perfect (or GT), 0.8 for acceptable, and 0.6 for grammar/fluency issues only.\nWe further incorporate the reward provided by the feedback network. In particular, our FBN allows us to define the reward at the phrase level (thus helping with the credit assignment problem). Since our generated sentence is segmented into phrases, i.e., ws = wp1w p 2 . . . w p P , where w p t denotes the (sequence of words in the) t-th phrase, we define the combined phrase reward as: r(wpt ) = r(w s) + λfffbn(w s, feedback, wpt ) (10) Note that FBN produces a classification of each phrase. We convert this into reward, by assigning correct to 1, wrong to −1, and 0 to not relevant. We do not weigh the reward by the confidence of the network, which might be worth exploring in the future. Our final gradient takes the following form:\n∇θL(θ) = − P∑ p=1 (r(wp)− r(ŵp))∇θ log pθ(wp) (11)\nImplementation details. We use Adam with learning rate 1e−6 and batch size 50. As in [28], we follow an annealing schedule. We first optimize the cross entropy loss for the first K epochs, then for the following t = 1, . . . , T epochs, we use cross entropy loss for the first (P − floor(t/m)) phrases (where P denotes the number of phrases), and the policy gradient algorithm for the remaining floor(t/m) phrases. We choose m = 5. When a caption has multiple feedback sentences, we take the sum of the FBN’s outputs (converted to rewards) as the reward for each phrase. When a sentence does not have any feedback, we assign it a zero reward."
    }, {
      "heading" : "4 Experimental Results",
      "text" : "To validate our approach we use the MS-COCO dataset [20]. We use 82K images for training, 2K for validation, and 4K for testing. In particular, we randomly chose 2K val and 4K test images from the official validation split. To collect feedback, we randomly chose 7K images from the training set, as well as all 2K images from our validation. In all experiments, we report the performance on our (held out) test set. For all the models (including baselines) we used a pre-trained VGG [32] network to extract image features. We use a word vocabulary size of 23,115.\nPhrase-based captioning model. We analyze different instantiations of our phrase-based captioning in Table 4, showing the importance of predicting phrase labels. To sanity check our model we compare it to a flat approach (word-RNN only) [38]. Overall, our model performs slightly worse than [38] (0.66 points). However, the main strength of our model is that it allows a more natural integration with feedback. Note that these results are reported for the models trained with MLE.\nFeedback network. As reported in Table 2, our dataset which contains detailed feedback (descriptions) contains 4173 images. We randomly select 9/10 of them to serve as a training set for our feedback network, and use 1/10 of them to be our test set. The classification performance of our FBN is reported in Table 5. We tried exploiting additional information in the network. The second line reports the result for FBN which also exploits the reference caption (for which the feedback was written) as input, represented with a LSTM. The model in the third line uses the type of error, i.e. the phrase is “missing”, “wrong”, or “redundant”. We found that by using information about what kind of mistake the reference caption had (e.g., corresponding to misnaming an object, action, etc) achieves the best performance. We use this model as our FBN used in the following experiments.\nRL with Natural Language Feedback. In Table 6 we report the performance for several instantiations of the RL models. All models have been pre-trained using cross-entropy loss (MLE) on the full MS-COCO training set. For the next rounds of training, all the models are trained only on the 9K images that comprise our full evaluation+feedback dataset from Table 2. In particular, we separate two cases. In the first, standard case, the “agent” has access to 5 captions for each image. We experiment with different types of captions, e.g. ground-truth captions (provided by MS-COCO), as well as feedback data. For a fair comparison, we ensure that each model has access to (roughly) the same amount of data. This means that we count a feedback sentence as one source of information, and a human-corrected reference caption as yet another source. We also exploit reference (MLE) captions which were evaluated as correct, as well as corrected captions obtained from the annotators. In particular, we tried two types of experiments. We define “C” captions as all captions that were corrected by the annotators and were not evaluated as containing minor or major error, and ground-truth captions for the rest of the images. For “A”, we use all captions (including reference MLE captions) that did not have minor or major errors, and GT for the rest. A detailed break-down of these captions is reported in Table 7.\nWe first test a model using the standard cross-entropy loss, but which now also has access to the corrected captions in addition to the 5GT captions. This model (MLEC) is able to improve over the original MLE model by 1.4 points. We then test the RL model by optimizing the metric wrt the 5GT captions (as in [29]). This brings an additional point, achieving 2.4 over the MLE model. Our RL agent with feedback is given access to 3GT captions, the “C\" captions and feedback sentences. We show that this model outperforms the no-feedback baseline by 0.5 points. Interestingly, with “A” captions we get an additional 0.3 boost. If our RL agent has access to 4GT captions and feedback descriptions, we achieve a total of 1.1 points over the baseline RL model and 3.5 over the MLE model. Examples of generated captions are shown in Fig. 5.\nWe also test a more realistic scenario, in which the models have access to either a single GT caption, or in our case “C\" (or “A”) and feedback. This mimics a scenario in which the human teacher observes the agent and either gives feedback about the agent’s mistakes, or, if the agent’s caption is completely wrong, the teacher writes a new caption. Interestingly, RL when provided with the corrected captions performs better than when given GT captions. Overall, our model outperforms the base RL (no feedback) by 1.2 points. We note that our RL agents are trained (not counting pre-training) only on a small (9K) subset of the full MS-COCO training set. Further improvements are thus possible.\nDiscussion. These experiments make an important point. Instead of giving the RL agent a completely new target (caption), a better strategy is to “teach” the agent about the mistakes it is doing and suggest a correction. Natural language thus offers itself as a rich modality for providing such guidance not only to humans but also to artificial agents."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this paper, we enable a human teacher to provide feedback to the learning agent in the form of natural language. We focused on the problem of image captioning. We proposed a hierarchical phrase-based RNN as our captioning model, which allowed natural integration with human feedback. We crowd-sourced feedback for a snapshot of our model, and showed how to incorporate it in Policy Gradient optimization. We showed that by exploiting descriptive feedback our model learns to perform better than when given independently written captions.\nTable 6: Comparison of our RL with feedback information to baseline RL and MLE models.\nBLEU-1 BLEU-2 BLEU-3 BLEU-4 ROUGE-L Weighted metric MLE (5 GT) 65.37 44.02 29.51 19.91 50.90 104.12\nMLEC (5 GT + C) 66.85 45.19 29.89 19.79 51.20 105.58 MLEC (5 GT + A) 66.14 44.87 30.17 20.27 51.32 105.47\nRLB (5 GT) 66.90 45.10 30.10 20.30 51.10 106.55 RLF (3GT+FB+C) 66.52 45.23 30.48 20.66 51.41 107.02 RLF (3GT+FB+A) 66.98 45.54 30.52 20.53 51.54 107.315 se nt .\nRLF (4GT + FB) 67.10 45.50 30.60 20.30 51.30 107.67 RLB (1 GT) 65.68 44.58 29.81 19.97 51.07 104.93\nRLB (C) 65.84 44.64 30.01 20.23 51.06 105.50 RLB (A) 65.81 44.58 29.87 20.24 51.28 105.31 RLF (C + FB) 65.76 44.65 30.20 20.62 51.35 106.03\n1 se\nnt .\nRLF (A + FB) 66.23 45.00 30.15 20.34 51.58 106.12 GT: ground truth captions; FB: feedback; MLE(A)(C): MLE model using five GT sentences + either C or A captions (see text and Table 7); RLB: baseline RL (no feedback network); RLF: RL with feedback (here we\nalso use C or A captions as well as FBN);\nground-truth perfect acceptable grammar error only A 3107 2661 2790 442 C 6326 1502 1502 234\nTable 7: Detailed break-down of what captions were used as “A” or “C” in Table 6 for computing additional rewards in RL.\nMLE: ( a man ) ( walking ) ( in front of a building ) ( with a cell phone . ) RLB: ( a man ) ( is standing ) ( on a sidewalk ) ( with a cell phone . ) RLF: ( a man ) ( wearing a black suit ) ( and tie ) ( on a sidewalk . )\nMLE: ( two giraffes ) ( are standing ) ( in a field ) ( in a field . ) RLB: ( a giraffe ) ( is standing ) ( in front of a large building . ) RLF: ( a giraffe ) ( is ) ( in a green field ) ( in a zoo . )\nMLE: ( a clock tower ) ( with a clock ) ( on top . ) RLB: ( a clock tower ) ( with a clock ) ( on top of it . ) RLF: ( a clock tower ) ( with a clock ) ( on the front . )\nMLE: ( two birds ) ( are standing ) ( on the beach ) ( on a beach . ) RLB: ( a group ) ( of birds ) ( are ) ( on the beach . ) RLF: ( two birds ) ( are standing ) ( on a beach ) ( in front of water . )\nFigure 5: Qualitative examples of captions from the MLE and RLB models (baselines), and our RBF model."
    }, {
      "heading" : "Acknowledgment",
      "text" : "We gratefully acknowledge the support from NVIDIA for their donation of the GPUs used for this research. This work was partially supported by NSERC. We also thank Relu Patrascu for infrastructure support."
    }, {
      "heading" : "A Qualitative Examples: Phrase-based Captioning",
      "text" : "We provide qualitative results from our phrase-based model in Fig. 6. The figure shows the attention maps, the generated phrase under each map, and the predicted phrase label."
    }, {
      "heading" : "B Feedback Crowd-Sourcing Interface",
      "text" : ""
    }, {
      "heading" : "C Examples of Collected Feedback",
      "text" : "In Table 8 we provide examples of collected feedback for our reference (MLE) model."
    }, {
      "heading" : "D Qualitative Examples: RL that Incorporates Feedback",
      "text" : ""
    } ],
    "references" : [ {
      "title" : "Towards diverse and natural image descriptions via a conditional gan",
      "author" : [ "Bo Dai", "Dahua Lin", "Raquel Urtasun", "Sanja Fidler" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2017
    }, {
      "title" : "Visual dialog",
      "author" : [ "A. Das", "S. Kottur", "K. Gupta", "A. Singh", "D. Yadav", "J.M. Moura", "D. Parikh", "D. Batra" ],
      "venue" : "arXiv:1611.08669",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Policy shaping: Integrating human feedback with reinforcement learning",
      "author" : [ "Shane Griffith", "Kaushik Subramanian", "Jonathan Scholz", "Charles L. Isbell", "Andrea Lockerd Thomaz" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2013
    }, {
      "title" : "Reinforcement learning via practice and critique advice",
      "author" : [ "K. Judah", "S. Roy", "A. Fern", "T. Dietterich" ],
      "venue" : "AAAI",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Beating atari with natural language guided reinforcement learning",
      "author" : [ "Russell Kaplan", "Christopher Sauer", "Alexander Sosa" ],
      "venue" : "In arXiv:1704.05539,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2017
    }, {
      "title" : "Deep visual-semantic alignments for generating image descriptions",
      "author" : [ "A. Karpathy", "L. Fei-Fei" ],
      "venue" : "CVPR",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik Kingma", "Jimmy Ba" ],
      "venue" : "arXiv preprint arXiv:1412.6980,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2014
    }, {
      "title" : "Unifying visual-semantic embeddings with multimodal neural language models",
      "author" : [ "Ryan Kiros", "Ruslan Salakhutdinov", "Richard S. Zemel" ],
      "venue" : "CoRR, abs/1411.2539,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2014
    }, {
      "title" : "Cynthia Breazeal",
      "author" : [ "W. Bradley Knox" ],
      "venue" : ", and Peter Stone. Training a robot via human feedback: A case study. In International Conference on Social Robotics",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Reinforcement learning from simultaneous human and mdp reward",
      "author" : [ "W. Bradley Knox", "Peter Stone" ],
      "venue" : "In Intl. Conf. on Autonomous Agents and Multiagent Systems,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2012
    }, {
      "title" : "Tega: A social robot",
      "author" : [ "Jacqueline Kory Westlund", "Jin Joo Lee", "Luke Plummer", "Fardad Faridi", "Jesse Gray", "Matt Berlin", "Harald Quintus-Bosz", "Robert Hartmann", "Mike Hess", "Stacy Dyer", "Kristopher dos Santos", "Sigurdhur Örn Adhalgeirsson", "Goren Gordon", "Samuel Spaulding", "Marayna Martinez", "Madhurima Das", "Maryam Archie", "Sooyeon Jeong", "Cynthia Breazeal" ],
      "venue" : "In International Conference on Human-Robot Interaction,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2016
    }, {
      "title" : "A hierarchical approach for generating descriptive image paragraphs",
      "author" : [ "Jonathan Krause", "Justin Johnson", "Ranjay Krishna", "Li Fei-Fei" ],
      "venue" : null,
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2017
    }, {
      "title" : "Guiding a reinforcement learner with natural language advice: Initial results in robocup soccer",
      "author" : [ "G. Kuhlmann", "P. Stone", "R. Mooney", "J. Shavlik" ],
      "venue" : "AAAI Workshop on Supervisory Control of Learning and Adaptive Systems",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Phrase-based image captioning",
      "author" : [ "Remi Lebret", "Pedro O. Pinheiro", "Ronan Collobert" ],
      "venue" : "In arXiv:1502.03671,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2015
    }, {
      "title" : "End-to-end training of deep visuomotor policies",
      "author" : [ "Sergey Levine", "Chelsea Finn", "Trevor Darrell", "Pieter Abbeel" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2016
    }, {
      "title" : "Dialogue learning with human-in-the-loop",
      "author" : [ "Jiwei Li", "Alexander H. Miller", "Sumit Chopra", "Marc’Aurelio Ranzato", "Jason Weston" ],
      "venue" : "In arXiv:1611.09823,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2016
    }, {
      "title" : "Deep reinforcement learning for dialogue generation",
      "author" : [ "Jiwei Li", "Will Monroe", "Alan Ritter", "Michel Galley", "Jianfeng Gao", "Dan Jurafsky" ],
      "venue" : null,
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2016
    }, {
      "title" : "Microsoft coco: Common objects in context",
      "author" : [ "Tsung-Yi Lin", "Michael Maire", "Serge Belongie", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Dollár", "C Lawrence Zitnick" ],
      "venue" : "In ECCV,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2014
    }, {
      "title" : "Improved image captioning via policy gradient optimization of spider",
      "author" : [ "Siqi Liu", "Zhenhai Zhu", "Ning Ye", "Sergio Guadarrama", "Kevin Murphy" ],
      "venue" : null,
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2016
    }, {
      "title" : "Incorporating advice into agents that learn from reinforcements",
      "author" : [ "Richard Maclin", "Jude W. Shavlik" ],
      "venue" : "In National Conference on Artificial Intelligence,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 1994
    }, {
      "title" : "The Stanford CoreNLP natural language processing toolkit",
      "author" : [ "Christopher D. Manning", "Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven J. Bethard", "David McClosky" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2014
    }, {
      "title" : "Socially assistive robotics: Human augmentation vs. automation",
      "author" : [ "Maja J. Matarič" ],
      "venue" : "Science Robotics,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2017
    }, {
      "title" : "Human-level control through deep reinforcement learning",
      "author" : [ "Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A. Rusu", "Joel Veness", "Marc G. Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K. Fidjeland", "Georg Ostrovski", "Stig Petersen", "Charles Beattie", "Amir Sadik", "Ioannis Antonoglou", "Helen King", "Dharshan Kumaran", "Daan Wierstra", "Shane Legg", "Demis Hassabis" ],
      "venue" : "Nature, 518(7540):529–533,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2015
    }, {
      "title" : "Policy invariance under reward transformations: Theory and application to reward shaping",
      "author" : [ "Andrew Y. Ng", "Daishi Harada", "Stuart J. Russell" ],
      "venue" : "In ICML,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 1999
    }, {
      "title" : "How to construct deep recurrent neural networks. In Association for Computational Linguistics (ACL) System Demonstrations",
      "author" : [ "Razvan Pascanu", "Caglar Gulcehre", "Kyunghyun Cho", "Yoshua Bengio" ],
      "venue" : null,
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2014
    }, {
      "title" : "Sequence level training with recurrent neural networks",
      "author" : [ "Marc’Aurelio Ranzato", "Sumit Chopra", "Michael Auli", "Wojciech Zaremba" ],
      "venue" : "In arXiv:1511.06732,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2015
    }, {
      "title" : "Selfcritical sequence training for image captioning",
      "author" : [ "Steven J. Rennie", "Etienne Marcheret", "Youssef Mroueh", "Jarret Ross", "Vaibhava Goel" ],
      "venue" : "In arXiv:1612.00563,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2016
    }, {
      "title" : "Mastering the game of Go with deep neural networks and tree",
      "author" : [ "David Silver", "Aja Huang", "Chris J. Maddison", "Arthur Guez", "Laurent Sifre", "George van den Driessche", "Julian Schrittwieser", "Ioannis Antonoglou", "Veda Panneershelvam", "Marc Lanctot", "Sander Dieleman", "Dominik Grewe", "John Nham", "Nal Kalchbrenner", "Ilya Sutskever", "Timothy Lillicrap", "Madeleine Leach", "Koray Kavukcuoglu", "Thore Graepel", "Demis Hassabis" ],
      "venue" : "search. Nature,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2016
    }, {
      "title" : "Neuroaesthetics in fashion: Modeling the perception of beauty",
      "author" : [ "Edgar Simo-Serra", "Sanja Fidler", "Francesc Moreno-Noguer", "Raquel Urtasun" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2015
    }, {
      "title" : "Very deep convolutional networks for large-scale image recognition",
      "author" : [ "Karen Simonyan", "Andrew Zisserman" ],
      "venue" : null,
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2015
    }, {
      "title" : "phi-lstm: A phrase-based hierarchical lstm model for image captioning",
      "author" : [ "Ying Hua Tan", "Chee Seng Chan" ],
      "venue" : "In ACCV,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2016
    }, {
      "title" : "Reinforcement learning with human teachers: Evidence of feedback and guidance",
      "author" : [ "A. Thomaz", "C. Breazeal" ],
      "venue" : "AAAI",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "A neural conversational model",
      "author" : [ "Oriol Vinyals", "Quoc Le" ],
      "venue" : "In arXiv:1506.05869,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 2015
    }, {
      "title" : "Dialog-based language learning",
      "author" : [ "Jason Weston" ],
      "venue" : "In arXiv:1604.06045,",
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 2016
    }, {
      "title" : "Simple statistical gradient-following algorithms for connectionist reinforcement learning",
      "author" : [ "Ronald J. Williams" ],
      "venue" : "In Machine Learning,",
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 1992
    }, {
      "title" : "Show, attend and tell: Neural image caption generation with visual attention",
      "author" : [ "Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhutdinov", "Richard Zemel", "Yoshua Bengio" ],
      "venue" : "In ICML,",
      "citeRegEx" : "38",
      "shortCiteRegEx" : "38",
      "year" : 2015
    }, {
      "title" : "Training an adaptive dialogue policy for interactive learning of visually grounded word meanings",
      "author" : [ "Yanchao Yu", "Arash Eshghi", "Oliver Lemon" ],
      "venue" : "In Proc. of SIGDIAL,",
      "citeRegEx" : "39",
      "shortCiteRegEx" : "39",
      "year" : 2016
    }, {
      "title" : "The burchak corpus: a challenge data set for interactive learning of visually grounded word meanings",
      "author" : [ "Yanchao Yu", "Arash Eshghi", "Gregory Mills", "Oliver Lemon" ],
      "venue" : "In Workshop on Vision and Language,",
      "citeRegEx" : "40",
      "shortCiteRegEx" : "40",
      "year" : 2017
    } ],
    "referenceMentions" : [ {
      "referenceID" : 32,
      "context" : "is slowly finding its way into everyone’s lives, be in the form of social bots [35, 2], personal assistants [24, 13, 31], or household robots [1], it becomes critical to allow non-expert users to teach and guide their robots [36, 18].",
      "startOffset" : 79,
      "endOffset" : 86
    }, {
      "referenceID" : 21,
      "context" : "is slowly finding its way into everyone’s lives, be in the form of social bots [35, 2], personal assistants [24, 13, 31], or household robots [1], it becomes critical to allow non-expert users to teach and guide their robots [36, 18].",
      "startOffset" : 108,
      "endOffset" : 120
    }, {
      "referenceID" : 10,
      "context" : "is slowly finding its way into everyone’s lives, be in the form of social bots [35, 2], personal assistants [24, 13, 31], or household robots [1], it becomes critical to allow non-expert users to teach and guide their robots [36, 18].",
      "startOffset" : 108,
      "endOffset" : 120
    }, {
      "referenceID" : 28,
      "context" : "is slowly finding its way into everyone’s lives, be in the form of social bots [35, 2], personal assistants [24, 13, 31], or household robots [1], it becomes critical to allow non-expert users to teach and guide their robots [36, 18].",
      "startOffset" : 108,
      "endOffset" : 120
    }, {
      "referenceID" : 33,
      "context" : "is slowly finding its way into everyone’s lives, be in the form of social bots [35, 2], personal assistants [24, 13, 31], or household robots [1], it becomes critical to allow non-expert users to teach and guide their robots [36, 18].",
      "startOffset" : 225,
      "endOffset" : 233
    }, {
      "referenceID" : 15,
      "context" : "is slowly finding its way into everyone’s lives, be in the form of social bots [35, 2], personal assistants [24, 13, 31], or household robots [1], it becomes critical to allow non-expert users to teach and guide their robots [36, 18].",
      "startOffset" : 225,
      "endOffset" : 233
    }, {
      "referenceID" : 27,
      "context" : "There have been significant advances in a variety of domains such as games [30, 25], robotics [17], and even fields like vision and NLP [29, 19].",
      "startOffset" : 75,
      "endOffset" : 83
    }, {
      "referenceID" : 22,
      "context" : "There have been significant advances in a variety of domains such as games [30, 25], robotics [17], and even fields like vision and NLP [29, 19].",
      "startOffset" : 75,
      "endOffset" : 83
    }, {
      "referenceID" : 14,
      "context" : "There have been significant advances in a variety of domains such as games [30, 25], robotics [17], and even fields like vision and NLP [29, 19].",
      "startOffset" : 94,
      "endOffset" : 98
    }, {
      "referenceID" : 26,
      "context" : "There have been significant advances in a variety of domains such as games [30, 25], robotics [17], and even fields like vision and NLP [29, 19].",
      "startOffset" : 136,
      "endOffset" : 144
    }, {
      "referenceID" : 16,
      "context" : "There have been significant advances in a variety of domains such as games [30, 25], robotics [17], and even fields like vision and NLP [29, 19].",
      "startOffset" : 136,
      "endOffset" : 144
    }, {
      "referenceID" : 31,
      "context" : "Several works explored the idea of incorporating humans in the learning process, in order to help the reinforcement learning agent to learn faster [34, 12, 11, 6, 5].",
      "startOffset" : 147,
      "endOffset" : 165
    }, {
      "referenceID" : 9,
      "context" : "Several works explored the idea of incorporating humans in the learning process, in order to help the reinforcement learning agent to learn faster [34, 12, 11, 6, 5].",
      "startOffset" : 147,
      "endOffset" : 165
    }, {
      "referenceID" : 8,
      "context" : "Several works explored the idea of incorporating humans in the learning process, in order to help the reinforcement learning agent to learn faster [34, 12, 11, 6, 5].",
      "startOffset" : 147,
      "endOffset" : 165
    }, {
      "referenceID" : 3,
      "context" : "Several works explored the idea of incorporating humans in the learning process, in order to help the reinforcement learning agent to learn faster [34, 12, 11, 6, 5].",
      "startOffset" : 147,
      "endOffset" : 165
    }, {
      "referenceID" : 2,
      "context" : "Several works explored the idea of incorporating humans in the learning process, in order to help the reinforcement learning agent to learn faster [34, 12, 11, 6, 5].",
      "startOffset" : 147,
      "endOffset" : 165
    }, {
      "referenceID" : 31,
      "context" : "This feedback typically comes in the form of a simple numerical (or “good”/“bad”) reward which is used to either shape the MDP reward [34] or directly shape the policy of the learner [5].",
      "startOffset" : 134,
      "endOffset" : 138
    }, {
      "referenceID" : 2,
      "context" : "This feedback typically comes in the form of a simple numerical (or “good”/“bad”) reward which is used to either shape the MDP reward [34] or directly shape the policy of the learner [5].",
      "startOffset" : 183,
      "endOffset" : 186
    }, {
      "referenceID" : 19,
      "context" : "In pioneering work, [22] translated natural language advice into a short program which was used to bias action selection.",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 19,
      "context" : "While this is possible in limited domains such as in navigating a maze [22] or learning to play a soccer game [15], it can hardly scale to the real scenarios with large action spaces requiring versatile language feedback.",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 12,
      "context" : "While this is possible in limited domains such as in navigating a maze [22] or learning to play a soccer game [15], it can hardly scale to the real scenarios with large action spaces requiring versatile language feedback.",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 26,
      "context" : "We show how to incorporate this information in Policy Gradient RL [29], and show that we can improve over RL that has access to the same amount of ground-truth captions.",
      "startOffset" : 66,
      "endOffset" : 70
    }, {
      "referenceID" : 31,
      "context" : "[34] exploits humans in the loop to teach an agent to cook in a virtual kitchen.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 23,
      "context" : "Reward shaping [26] is used to incorporate this information in the MDP.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 3,
      "context" : "[6] iterates between “practice”, during which the agent interacts with the real environment, and a critique session where a human labels any subset of the chosen actions as good or bad.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 9,
      "context" : "In [12], the authors compare different ways of incorporating human feedback, including reward shaping, Q augmentation, action biasing, and control sharing.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 8,
      "context" : "The same authors implement their TAMER framework on a real robotic platform [11].",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 2,
      "context" : "[5] proposes policy shaping which incorporates right/wrong feedback by utilizing it as direct policy labels.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 19,
      "context" : "[22]’s pioneering work translated advice to a short program which was then implemented as a neural network.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 12,
      "context" : "[15] incorporated natural language advice for a RoboCup simulated soccer task.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 4,
      "context" : "Parallel to our work, [7] exploits textual advice to improve training time of the A3C algorithm in playing an Atari game.",
      "startOffset" : 22,
      "endOffset" : 25
    }, {
      "referenceID" : 33,
      "context" : "Recently, [36, 18] incorporates human feedback to improve a text-based QA agent.",
      "startOffset" : 10,
      "endOffset" : 18
    }, {
      "referenceID" : 15,
      "context" : "Recently, [36, 18] incorporates human feedback to improve a text-based QA agent.",
      "startOffset" : 10,
      "endOffset" : 18
    }, {
      "referenceID" : 5,
      "context" : "This domain has received significant attention [8, 38, 10], achieving impressive performance on standard benchmarks.",
      "startOffset" : 47,
      "endOffset" : 58
    }, {
      "referenceID" : 35,
      "context" : "This domain has received significant attention [8, 38, 10], achieving impressive performance on standard benchmarks.",
      "startOffset" : 47,
      "endOffset" : 58
    }, {
      "referenceID" : 7,
      "context" : "This domain has received significant attention [8, 38, 10], achieving impressive performance on standard benchmarks.",
      "startOffset" : 47,
      "endOffset" : 58
    }, {
      "referenceID" : 13,
      "context" : "Our phrase model shares the most similarity with [16], but differs in that exploits attention [38], linguistic information, and RL to train.",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 35,
      "context" : "Our phrase model shares the most similarity with [16], but differs in that exploits attention [38], linguistic information, and RL to train.",
      "startOffset" : 94,
      "endOffset" : 98
    }, {
      "referenceID" : 18,
      "context" : "Several recent approaches trained the captioning model with policy gradients in order to directly optimize for the desired performance metrics [21, 29, 3].",
      "startOffset" : 143,
      "endOffset" : 154
    }, {
      "referenceID" : 26,
      "context" : "Several recent approaches trained the captioning model with policy gradients in order to directly optimize for the desired performance metrics [21, 29, 3].",
      "startOffset" : 143,
      "endOffset" : 154
    }, {
      "referenceID" : 0,
      "context" : "Several recent approaches trained the captioning model with policy gradients in order to directly optimize for the desired performance metrics [21, 29, 3].",
      "startOffset" : 143,
      "endOffset" : 154
    }, {
      "referenceID" : 36,
      "context" : "Related to our efforts is also work on dialogue based visual representation learning [39, 40], however this work tackles a simpler scenario, and employs a slightly more engineered approach.",
      "startOffset" : 85,
      "endOffset" : 93
    }, {
      "referenceID" : 37,
      "context" : "Related to our efforts is also work on dialogue based visual representation learning [39, 40], however this work tackles a simpler scenario, and employs a slightly more engineered approach.",
      "startOffset" : 85,
      "endOffset" : 93
    }, {
      "referenceID" : 16,
      "context" : "We stress that our work differs from the recent efforts in conversation modeling [19] or visual dialog [4] using Reinforcement Learning.",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 1,
      "context" : "We stress that our work differs from the recent efforts in conversation modeling [19] or visual dialog [4] using Reinforcement Learning.",
      "startOffset" : 103,
      "endOffset" : 106
    }, {
      "referenceID" : 30,
      "context" : "Our captioning model, forming the base of our approach, uses a hierarchical Recurrent Neural Network, similar to [33, 14].",
      "startOffset" : 113,
      "endOffset" : 121
    }, {
      "referenceID" : 11,
      "context" : "Our captioning model, forming the base of our approach, uses a hierarchical Recurrent Neural Network, similar to [33, 14].",
      "startOffset" : 113,
      "endOffset" : 121
    }, {
      "referenceID" : 11,
      "context" : "In [14], the authors use a two-level LSTM to generate paragraphs, while [33] uses it to generate sentences as a sequence of phrases.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 30,
      "context" : "In [14], the authors use a two-level LSTM to generate paragraphs, while [33] uses it to generate sentences as a sequence of phrases.",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 35,
      "context" : "Following [38], we use a convolutional neural network in order to extract a set of feature vectors a = (a1, .",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 24,
      "context" : "fword LSTM, dim 256 fout deep decoder [27] fword−phrase mean+3-lay.",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 35,
      "context" : "As in [38], ct denotes a context vector obtained by applying the attention mechanism to the image.",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 35,
      "context" : "Following [38], we use a deep output layer [27] in the LSTM and double stochastic attention.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 24,
      "context" : "Following [38], we use a deep output layer [27] in the LSTM and double stochastic attention.",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 17,
      "context" : "To train our hierarchical model, we first process MS-COCO image caption data [20] using the Stanford Core NLP toolkit [23].",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 20,
      "context" : "To train our hierarchical model, we first process MS-COCO image caption data [20] using the Stanford Core NLP toolkit [23].",
      "startOffset" : 118,
      "endOffset" : 122
    }, {
      "referenceID" : 6,
      "context" : "We use the ADAM optimizer [9] with learning rate 0.",
      "startOffset" : 26,
      "endOffset" : 29
    }, {
      "referenceID" : 17,
      "context" : "captions for a subset of MS-COCO images [20] using greedy decoding.",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 6,
      "context" : "We use ADAM [9] with learning rate 0.",
      "startOffset" : 12,
      "endOffset" : 15
    }, {
      "referenceID" : 26,
      "context" : "We follow [29, 28] to directly optimize for the desired image captioning metrics using the Policy Gradient technique.",
      "startOffset" : 10,
      "endOffset" : 18
    }, {
      "referenceID" : 25,
      "context" : "We follow [29, 28] to directly optimize for the desired image captioning metrics using the Policy Gradient technique.",
      "startOffset" : 10,
      "endOffset" : 18
    }, {
      "referenceID" : 26,
      "context" : "For completeness, we briefly summarize it here [29].",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 26,
      "context" : ", the reward can be any of the automatic metrics, their weighted sum [29, 21], or in our case will also include the reward from feedback.",
      "startOffset" : 69,
      "endOffset" : 77
    }, {
      "referenceID" : 18,
      "context" : ", the reward can be any of the automatic metrics, their weighted sum [29, 21], or in our case will also include the reward from feedback.",
      "startOffset" : 69,
      "endOffset" : 77
    }, {
      "referenceID" : 34,
      "context" : "L(θ) = −Ews∼pθ [r(w)] (5) To optimize this objective, we follow the reinforce algorithm [37], as also used in [29, 28].",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 26,
      "context" : "L(θ) = −Ews∼pθ [r(w)] (5) To optimize this objective, we follow the reinforce algorithm [37], as also used in [29, 28].",
      "startOffset" : 110,
      "endOffset" : 118
    }, {
      "referenceID" : 25,
      "context" : "L(θ) = −Ews∼pθ [r(w)] (5) To optimize this objective, we follow the reinforce algorithm [37], as also used in [29, 28].",
      "startOffset" : 110,
      "endOffset" : 118
    }, {
      "referenceID" : 26,
      "context" : "We follow [29] to define the baseline b as the reward obtained by performing greedy decoding: b = r(ŵ), ŵt = argmax p(wt|ht) ∇θL(θ) ≈ −(r(w)− r(ŵ))∇θ log pθ(w) (8)",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 25,
      "context" : "As in [28], we follow an annealing schedule.",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 17,
      "context" : "To validate our approach we use the MS-COCO dataset [20].",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 29,
      "context" : "For all the models (including baselines) we used a pre-trained VGG [32] network to extract image features.",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 35,
      "context" : "To sanity check our model we compare it to a flat approach (word-RNN only) [38].",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 35,
      "context" : "Overall, our model performs slightly worse than [38] (0.",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 35,
      "context" : "12 Table 4: Comparing performance of the flat captioning model [38], and different instantiations of our phrasebased captioning model.",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 26,
      "context" : "We then test the RL model by optimizing the metric wrt the 5GT captions (as in [29]).",
      "startOffset" : 79,
      "endOffset" : 83
    } ],
    "year" : 2017,
    "abstractText" : "Robots will eventually be part of every household. It is thus critical to enable algorithms to learn from and be guided by non-expert users. In this paper, we bring a human in the loop, and enable a human teacher to give feedback to a learning agent in the form of natural language. We argue that a descriptive sentence can provide a much stronger learning signal than a numeric reward in that it can easily point to where the mistakes are and how to correct them. We focus on the problem of image captioning in which the quality of the output can easily be judged by non-experts. We propose a hierarchical phrase-based captioning model trained with policy gradients, and design a feedback network that provides reward to the learner by conditioning on the human-provided feedback. We show that by exploiting descriptive feedback our model learns to perform better than when given independently written human captions.",
    "creator" : "LaTeX with hyperref package"
  }
}