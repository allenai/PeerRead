{
  "name" : "1409.8498.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Non-Myopic Learning in Repeated Stochastic Games",
    "authors" : [ "Jacob W. Crandall" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Many scenarios require computers, robots, and other intelligent devices to repeatedly interact with people and other machines. In these interactions, associates’ behaviors are often not known a priori. For example, in the context of Ad Hoc team coordination (Stone et al. 2010), a machine must be able to effectively interact with arbitrary teammates without prior coordination. These teammates are created by other stakeholders, and may not share all the same objectives or run the same algorithms. Such interactions occur in new-age power systems, electronic commerce, and other domains.\nMany repeated interactions can be formulated as repeated stochastic games (RSGs). Because such interactions are finite, a successful player must learn effective strategies within short time scales. Unfortunately, this is extremely challenging. Existing learning algorithms for this domain either require thousands of interactions to learn (Claus and Boutilier 1998; Crandall and Goodrich 2011) or make narrow assumptions about associates’ behaviors (and, thus, learn ineffective strategies when these assumptions fail).\nThese deficiencies are due to a number of characteristics of the domain. RSGs are punctuated by large strategy spaces, even for games with relatively few states. Furthermore, the strategies of associates are unknown a priori, and can change over time (when they also learn). Since\nthese games typically have multiple (often infinite) equilibria, associates’ future behaviors can be very difficult to infer or learn – reliance on so-called rationality and equilibrium computation is insufficient.\nGame abstraction has emerged in recent years as an effective methodology for dealing with games having large state and action spaces (Gilpin and Sandholm 2006; Schnizlein, Bowling, and Szafron 2009; Ganzfried, Sandholm, and Waugh 2012; Sandholm and Singh 2012). In most approaches, the game is abstracted to a smaller game. An equilibrium strategy is then computed for this smaller game, but is then executed in the original game. While this abstraction methodology is effective in large zero-sum, extensive-form games, it does little to help an algorithm deal with multiple equilibria and adaptive associates.\nWe introduce a new game-abstraction method called game abstraction by experts (Gabe). Gabe reduces an RSG to a multi-armed bandit problem, which can then be solved using an expert algorithm such as Exp3 (Auer et al. 1995), UCB (Auer, Cesa-Bianchi, and Fischer 2002), or EEE (de Farias and Megiddo 2004). It does this by computing a finite set of expert strategies or learning rules, which are followed based on the selections made by the expert algorithm. The effectiveness of the resulting algorithm depends on both the set of expert strategies and the ability of the expert algorithm to select the best experts for the given scenario.\nIn this paper, we describe a general set of experts for twoplayer general-sum RSGs. While this set of experts effectively reduces the set of strategies that a player can play, it maintains important characteristics of the original game, such as its security level and various Pareto optimal Nash equilibria (NEs). We demonstrate that, when combined with an effective expert algorithm, Gabe outperforms many other algorithms in three two-player RSGs played against a variety of both non-learning and learning associates. In short, Gabe provides a mechanism for agents to quickly learn profitable non-myopic strategies that are otherwise difficult to learn."
    }, {
      "heading" : "2 Notation",
      "text" : "We consider two-player RSGs played by players i and −i. An RSG consists of a set of stage games (or states) S. In each state s ∈ S, both players choose an action from a finite set. Let A(s) = Ai(s)×A−i(s) be the set of joint actions available in s, where Ai(s) and A−i(s) are the action sets of players i\nar X\niv :1\n40 9.\n84 98\nv1 [\ncs .G\nT ]\n3 0\nSe p\n20 14\nand −i, respectively. Each episode (or round) of an RSG begins in the start state ŝ∈ S and terminates when some goal state sg ∈G⊆ S is reached. Once a goal state is encountered, a new episode begins in state ŝ.\nWhen joint action a = (ai,a−i) is played in state s, each players receive the finite rewards ri(s,a) and r−i(s,a), respectively. The world also transitions to some new state s′ with probability defined by PM(s,a,s′). We assume that the transition model PM and the reward functions ri(s,a) and r−i(s,a) are known to both players before the game begins, and that the players can observe each other’s actions."
    }, {
      "heading" : "3 Example RSG",
      "text" : "For illustrative purposes, we consider a Microgrid Scenario (Figure 1) involving two players. These players interact for an unspecified number of days, each seeking to maximize their own utility over this time. To gain utility, a player executes its tasks within the specified time windows (Figure 1ab). These tasks require the specified electricity loads. A task can be executed no more than once a day, and must be executed within a single time period.\nThe players share a limited electricity supply, which has the per-hour generation characteristics shown in Figure 1(c). We assume that generation occurs at the beginning of each hour. Additionally, the system automatically stores up to five units of unused electricity for later use. For simplicity, we assume that all storage is lost at the end of each day.\nIf the players attempt to consume more electricity in an hour than is available, a blackout occurs. In a blackout, electricity storage empties, and the tasks that the players attempted to execute are left unfinished (as if they had not been initiated). A cost of two utility units is incurred to bring the microgrid back online, which is evenly distributed among the players that attempted to execute tasks in that hour.\nIn this RSG, the state of the world is defined by (1) the current time (hour), (2) the amount of stored electricity, and (3) the set of currently active tasks (i.e., unexecuted tasks whose time windows correspond to the current hour). In total, this game has 2,033 unique states. The beginning state is the state of the system in hour t = 0. The goal states consist of all states with time t = 24. Each player’s action set for each state is the set of all subsets of its active tasks.\nThere is not sufficient electricity generated in a day for all tasks to be executed. Furthermore, some high-value tasks can only be executed when the other player complies. For example, there is only sufficient electricity for Player 2 to execute Task 12 if Player 1 refrains from executing Task 1 prior to time t = 6. Similarly, Player 1 can only successfully execute Task 10 if Player 2 refrains from Task 20. The players must coordinate and compromise to be successful.\nOur observations are that existing learning algorithms are unable to learn effective strategies in this RSG, especially when they are paired with each other. For example, model-free reinforcement learning (RL; e.g., Q-learning (Watkins and Dayan 1992)) is unable to learn effective strategies within a reasonable number of days due to the game’s large state-joint action space. Existing algorithms that learn at faster time scales still fail to learn collaborative\nTask Time Load UtilityID window (units) 1 [0,8) 2.0 7.0 2 [5,8) 2.0 1.5 3 [8,12) 3.6 0.8 4 [10,11) 2.4 1.6 5 [11,13) 3.9 2.7 6 [14,17) 3.8 1.4 7 [17,18) 3.6 2.9 8 [18,21) 1.2 1.5 9 [18,23) 1.5 2.4 10 [23,24) 5.0 20.2\n(a) Player 1’s tasks\nTask Time Load UtilityID window (units) 11 [0,3) 1.5 2.0 12 [4,6) 5.0 22.2 13 [7,8) 1.5 0.9 14 [9,13) 1.3 1.4 15 [11,15) 0.7 2.4 16 [13,17) 4.5 2.6 17 [15,18) 2.7 1.7 18 [17,18) 5.0 1.6 19 [18,22) 2.8 1.5 20 [22,23) 4.0 5.7\n(b) Player 2’s tasks\nsolutions. For example, when both players utilize modelbased RL (MBRL) or counterfactual regret (CFR, an effective algorithm for the poker domain (Zinkevich et al. 2007; Johanson et al. 2012)), they fail to learn a collaborative solution in self play. As a result, as shown in Figure 1(d), their average daily utilities are far below those obtained by FolkEgal (de Cote and Littman 2008). On the other hand, FolkEgal does not perform effectively when associating with algorithms that do not meet its assumptions (see Table 1(a)).\nBecause RSGs are punctuated with large state-action spaces, multiple (often infinite) equilibria, and associates with unknown (potentially changing) strategies, it is infeasible to learn non-myopic solutions using algorithms that use locally optimal computations and assumptions. While such methods might be sufficient if all consequences of the repeated interactions could be fully represented, the stateaction spaces of such representations are prohibitively large. Novel game-abstraction methods are needed. In this paper, we study game abstraction by experts (Gabe)."
    }, {
      "heading" : "4 Game Abstraction By Experts (Gabe)",
      "text" : "Gabe (summarized in Algorithm 1) reduces the strategy space of an RSG to a finite set Φ of strategies or algorithms. Each φ ∈Φ defines a policy for all states s ∈ S. In this way, Gabe converts an RSG into a multi-armed bandit problem, where each arm is an expert φ ∈Φ. Thus, rather than learning a separate policy for each state s∈ S, the agent must learn which high-level strategies or algorithms φ ∈Φ are the most profitable, and then follow these strategies.\nGabe has similarities to the work of Elidrisi et al. (2014), in which the RSG is reduced to a normal-form game by identifying common sequences of actions (or paths) for each player. These paths are then used as the actions of a normalform game. Identifying these actions relies on effective exploration strategies and clustering and thresholding algorithms. Gabe instead defines Φ to reduce the RSG to a multiarmed bandit problem. In so doing, it maintains important\nAlgorithm 1 Game abstraction by experts (Gabe). Input: An expert algorithm A Initialize: Compute a set Φ of experts Run: A on the set Φ\nIn each round t - Select select an expert φt ∈Φ using A - Follow the strategy prescribed by φt throughout round t - Update each φ ∈Φ and A as specified\nattributes of the original RSG. Algorithm 1 requires that we solve two technical problems. First, we must identify an expert algorithm A that learns effectively in repeated interactions with unknown associates. A common goal for such algorithms is to learn to play nearly as well as the best expert would have performed had it always been followed. This problem has been well-studied in the literature (Auer et al. 1995; Bowling 2004; Arora, Dekel, and Tewari 2012; Crandall 2014; Cesa-Bianchi, Dekel, and Shamir 2013).\nSecond, we must define the set of experts Φ. Ideally, in any scenario (i.e., RSG and associate) that a player is likely to encounter, at least one φ ∈Φ should play near-optimally. However, no single expert need be effective in all situations.\nGiven that expert algorithms have been well-studied, we focus in the next section on identifying and computing a general set of experts Φ for two-player general-sum RSGs."
    }, {
      "heading" : "5 Experts",
      "text" : "Our set Φ for two-player RSGs consists of three genres of algorithms. The first two genres, leader and follower strategies, were defined by Littman and Stone (2001). We introduce the third genre, call preventative strategies, in this paper. These three genres define appropriate responses to many algorithms that associates are likely to use.\nLeader Strategies A leader strategy tries to encourage its associate to follow a target solution (Littman and Stone 2005). The leader plays its own portion of the target solution as long as its associate plays its part. However, when the associate deviates from this solution, the leader retaliates in subsequent moves to ensure that the associate does not profit from the deviation.\nTo derive an effective set of leader strategies, we must do three things. First, we must efficiently compute desirable target solutions. Second, we must select a set of target solutions, one solution corresponding to each leader strategy we place in Φ. Third, we must determine how to punish deviations from the target solution.\nComputing Target Solutions Possible target solutions of an RSG can be computed by solving a Markov decision process (MDP). This MDP is defined by PM , the RSG’s jointaction space, and a payoff function that is a convex combination of the players’ rewards (de Cote and Littman 2008). That is, for ω ∈ [0,1], the payoff function is given by\nσω(s,a) = ωri(s,a)+(1−ω)r−i(s,a). (1)\nThen, the value for taking joint-action a in state s is\nQω(s,a) = σω(s,a)+ ∑ s′∈S PM(s,a,s′)V ω(s′), (2)\nwhere V ω(s′) = maxa∈A(s) Qω(s′,a), which can be solved in polynomial time using linear programming (Papadimitriou and Tsitsiklis 1987; Littman, Dean, and Kaelbling 1995).\nLet MDP(ω) denote the joint strategy produced by solving the MDP for a particular ω . Also, let V ωi (s) be player i’s expected future payoff from state s when MDP(ω) is followed. Then, the ordered pair ( V ωi (ŝ),V ω −i(ŝ) ) is the joint payoff vector for the target solution defined by MDP(ω). This payoff vector is Pareto optimal.\nBy varying ω , we can compute a variety of possible target solutions. For example, Table 1 gives the solutions computed for the Microgrid Scenario using this method. We call these solutions pure solutions. Additional possible target solutions can be obtained by alternating between solutions produced using different ω . We call these solutions alternating solutions. For example, the players could play MDP(0.1) in odd-numbered rounds, and MDP(0.3) in even-numbered rounds. This would produce the average joint payoff vector of (24.05,36.35) in the Microgrid Scenario. In this paper, we only include alternating cycles of length 2, since longer cycles are likely to be difficult to reinforce using a leader strategy, and hence will likely be difficult to agree upon.\nWhich Target Solutions? Figure 2 shows the joint payoff vectors of possible pure and alternating target solutions in the Microgrid Scenario. As with many other RSGs, the oneshot NEs (one shown) are Pareto dominated by several possible target solutions. Furthermore, the possible target solutions in which each player’s payoff exceeds its maximin strategy can be sustained as NEs of the repeated game (Gintis 2000). As such, these possible target solutions offer a variety of potentially desirable equilibrium solutions, each differing in the value provided to each player.\nIn practice, more experts makes the task of finding the best expert more difficult for the expert algorithm. Thus, we form leader experts for only up to five of the possible target solutions we have identified. First, we select the solution that maximizes the minimum payoff between the players (i.e., the egalitarian solution). Next, we select the two points that give each player the highest payoff while providing the other player with at least its security level. The last two points are chosen to maximize the Euclidean distance from the other selected payoff vectors. For the Microgrid Scenario, our algorithm selects the target solutions shown in red in Figure 2.\nAdding punishment When the associate deviates from the target solution, the leader plays an attack strategy (usually its minimax strategy (Littman and Stone 2005)) in subsequent moves of the game. In practice, we deviate slightly from this protocol: player i only punishes deviations by player−i that substantially lower i’s payoffs. Formally, let sτ denote the state of the τth move of the round. Then, player i begins punishing −i when its action deviates from the target solution and the following two conditions hold:\n(1) V ωi (sτ)+ r t i(τ−1)<V ωi (sτ−1), (3) (2) τ\n∑ j=1 rti( j)+V ω i (sτ)< α t i , (4)\nwhere rti( j) is i’s payoff after the jth move of round t. Also, α ti = λα t−1 i +(1−λ )Rti , λ ∈ (0,1), and Rti is i’s total payoff in round t. α0i is set to i’s payoff in the egalitarian solution. When player−i deviates and conditions (1-2) hold, i punishes −i for the remainder of round t, and continues to do so in subsequent rounds until −i’s payoffs are at least δ less than they would have been had −i not deviated. We chose this punishment mechanism because RSGs are inherently noisy in practice. The associate’s payoffs are often uncertain, and the reward and transition functions of RSGs can be non-deterministic. In such situations, being more lenient helps avoid cycles of unnecessary punishment.\nSince the attack strategy can also be computed in polynomial time, a leader strategy can be built in polynomial time.\nFollower Strategies We also include follower strategies in Φ. Rather than attempt to drive play toward a particular solution, followers seek to maximize their payoffs against the strategy they attribute to their associate. We use followers that estimate their associate’s strategy in three different ways. The first set of followers assume their associate is playing a leader strategy. Against such associates, a player maximizes its payoffs by\nplaying its part of the corresponding target solution. Thus, we form a separate follower strategy for each of the selected target solutions mentioned earlier, each of which plays its part of its target solution unconditionally.\nThe set Φ also includes two other follower strategies: its maximin strategy, which is a best response to an associate seeking to minimize its payoffs, and MBRL, which plays a best response to stationary associates it can model. MBRL estimates its associate’s strategy using the Fictitious-play assessment (Fudenberg and Levine 1998). That is, in state s, i assumes that −i plays action a−i proportionally to the frequency it has played it in past visits to s. We denote this probability as γ−i(s,a−i).\nPreventative Strategies Due to their representations of the world, some learning associates have difficulty perceiving the punishment signals communicated by a leader strategy. As such, they do not learn to play the target solution intended by the leader. In such cases, preventative strategies can sometimes be more effective. Rather than punishing past deviations, preventative strategies seek to make deviations unprofitable in first place. They do this by anticipating profitable deviations the associate might make, and then acting in advance so that such deviations are unprofitable to the associate.\nWe include one preventative algorithm in Φ, which we call Bouncer. Bouncer seeks to minimize the difference between the players’ payoffs, without further regard for its own payoffs. Formally, Bouncer computes both Qi(s,a) and Q−i(s,a) using SARSA (Rummery and Niranjan 1994), where the estimated strategies for the players are given by the Fictitious-play assessments γi(s,ai) and γ−i(s,a−i), respectively. It then selects actions as follows:\na∗i (s) = min ai∈Ai(s) ∑ a−i∈A−i(s) γ−i(s,a−i)U(s,(ai,a−i)), (5)\nwhere U(s,a) = |Qi(s,a)−Q−i(s,a)|."
    }, {
      "heading" : "6 Properties of the Abstraction",
      "text" : "The set Φ as defined in Section 5 is an abstraction of the strategy space of the original RSG. This abstraction limits the set of strategies available to a player. However, it preserves the follow three attributes of the original RSG. Property 1 (ε-Pareto optimal NEs) Φ contains leader strategies that seek to play ε-Pareto optimal NEs of the repeated game. These same NEs exist in the original RSG. Property 2 (Security) Because Φ contains the maximin strategy, it maintains the original RSG’s security level. Property 3 (Best response) Φ contains MBRL, which learns a best response (in the original game) to the play of stationary associates having the same state representation.\nThus, Φ preserves important attribute of the original RSG, attributes that correspond to previously stated goals for learning in games (Powers and Shoham 2005). In the next section, we show that this simplified strategy space allows an expert algorithm to learn non-myopic strategies in many scenarios, which helps it to outperform existing algorithms."
    }, {
      "heading" : "7 Empirical Performance",
      "text" : "We combined the expert algorithms Exp3 (Auer et al. 1995) and S++ (Crandall 2014) with the set Φ to form two new algorithms: Gabe-Exp3 and Gabe-S++. Exp3 is a commonly used expert algorithm, while S++ is a recently developed algorithm that has demonstrated high empirical performance in repeated normal-form games. We evaluated these algorithms against ten associates in three separate games.\nAssociates We evaluated Gabe-Exp3 and Gabe-S++ when paired with both non-learning and learning algorithms. The non-learning algorithms were Coop, Bully, FolkEgal (de Cote and Littman 2008), Maximin, Bouncer, and CFRNE. Coop plays the strategy that maximizes its associate’s payoffs. Bully is identical to FolkEgal except that it seeks to enforce the target solution that maximizes its own payoffs subject to the other player receiving at least its security level. CFR-NE computes an ε-NE prior to the game using counterfactual regret in simulated self play (Zinkevich et al. 2007; Johanson et al. 2012). The learning algorithms were MBRL, CFR, Gabe-Exp3, and Gabe-S++. CFR is identical to CFRNE except that it also updates its strategy after each round based on its associate’s observed actions.\nGames We evaluated the algorithms in three RSGs: the Microgrid Scenario, the SGPD (Figure 3(a)), and a block game. These games require the players to make and accept different kinds of compromises to be successful. The SGPD (fully explained by Goodrich, Crandall, and Stimpson, 2003) is a prisoners’ dilemma in maze form with 26,896 states. A player receives 30 points when it moves to the other player’s start position, and loses one point every time it attempts a move. A successful player in this RSG must effectively balance cooperation and defection.\nThe Block Game is a turn-taking game in which the two players share a set of blocks (Figure 3(b)). In each round, the players take turns selecting blocks (the oldest player always goes first). A round ends once both players have three blocks. If a player’s blocks form a valid set (i.e., all blocks of the same color, all blocks of the same shape, or none of the blocks have the same color or shape), her payoff is the sum of the numbers on her blocks. Otherwise, her payoff is the sum of the numbers divided by 4. The sub-game perfect one-shot NEs of this game give each player 18 points. However, these solutions are Pareto dominated by the solution in which the players alternate between taking all of the squares and all of the triangles (which gives each player an average\npayoff of 25). Even better, a player could potentially bully the other by always insisting on taking all of the squares.\nResults Figure 4 provides a summary of the average empirical performance of the four learning algorithms in each game over the first 500 rounds. Table 2 shows the results of each pairing in each game in 365-round games. Averaged over all pairings, Gabe-S++ was the highest performer in each game. The order of the other players varied from game to game. The consistently high performance of Gabe-S++ can be traced to the two components of Gabe: the strategy abstraction Φ and the quality of the expert algorithm A .\nFirst, the strategy abstraction Φ allows Gabe-S++ to (1) make profitable compromises with associates that are apt to compromise, (2) exploit naive associates, and (3) avoid being exploited by antagonistic associates. For example, FolkEgal, Gabe-S++, and Bouncer (in the SGPD only) are apt to play the egalitarian solution in each game. Gabe-S++ is able to learn to play these solutions against these associates (and, hence, performs well) because at least one of the leader and follower strategies in Φ play these compromises. Φ also contains Bouncer, which allows it to learn mutual cooperation against MBRL in the SGPD in some trials. Meanwhile, the follower strategies in Φ permit Gabe-S++ to exploit Coop and to avoid being perpetually exploited by Bully, CFR, and CFR-NE in each game. Thus, Φ contains a set of strategies that allow it to perform effectively against each of these ten associates in each game.\nMBRL and CFR, on the other hand, cannot effectively represent the egalitarian solution, or other effective nonmyopic compromises. Thus, while they can exploit naive algorithms (such as Coop) and also avoid being exploited, they do not typically perform well against associates that are apt to cooperate in these games. This is because they cannot represent non-myopic strategies in general-sum RSGs.\nSecond, Gabe-S++ performs well because of its expert algorithm. While Gabe-Exp3 used the same set of expert strategies Φ as Gabe-S++, it did not perform nearly as well in any of these three games. Despite sometimes sharing theoretical properties (such as no-regret), both the short- and long-term empirical performance of expert algorithms can vary substantially in repeated interactions (Crandall 2014). Thus, both the selection of the expert algorithm and the strategy set Φ are important components of Gabe."
    }, {
      "heading" : "8 Conclusion and Discussion",
      "text" : "In this paper, we have proposed game abstractions by experts (Gabe) for two player, repeated stochastic games (RSGs). Gabe reduces an RSG to a multi-armed bandit problem, which can then be solved using an expert algorithm. Gabe maintains many important characteristics of the original game, including security, best response, and Pareto optimality. Furthermore, we demonstrated empirically that, given an effective expert algorithm, Gabe outperforms existing learning algorithms in a number of general-sum RSGs.\nGabe differs from previous game-abstraction methods (Gilpin and Sandholm 2006; Schnizlein, Bowling, and Szafron 2009; Ganzfried, Sandholm, and Waugh 2012; Sandholm and Singh 2012). Whereas previous methods seek\nto reduce the number of states and actions in the game (to make equilibrium computation feasible), Gabe abstracts the high-level strategies of the game. This allows it to represent non-myopic strategies. Since these abstraction methods have different purposes, Gabe can be used simultaneously with previous game-abstraction methods where necessary."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2014,
    "abstractText" : "This paper addresses learning in repeated stochastic games (RSGs) played against unknown associates. Learning in RSGs is extremely challenging due to their inherently large strategy spaces. Furthermore, these games typically have multiple (often infinite) equilibria, making attempts to solve them via equilibrium analysis and rationality assumptions wholly insufficient. As such, previous learning algorithms for RSGs either learn very slowly or make extremely limiting assumptions about the game structure or associates’ behaviors. In this paper, we propose and evaluate the notion of game abstraction by experts (Gabe) for two-player general-sum RSGs. Gabe reduces an RSG to a multiarmed bandit problem, which can then be solved using an expert algorithm. Gabe maintains many aspects of the original game, including security and Pareto optimal Nash equilibria. We demonstrate that Gabe substantially outperforms existing algorithms in many scenarios.",
    "creator" : "LaTeX with hyperref package"
  }
}