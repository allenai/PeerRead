{
  "name" : "1606.05250.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "SQuAD: 100,000+ Questions for Machine Comprehension of Text",
    "authors" : [ "Pranav Rajpurkar" ],
    "emails" : [ "pranavsr@cs.stanford.edu", "zjian@cs.stanford.edu", "klopyrev@cs.stanford.edu", "pliang@cs.stanford.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Reading Comprehension (RC), or the ability to read text and then answer questions about it, is a challenging task for machines, requiring both understanding of natural language and knowledge about the world. Consider the question “what causes precipitation to fall?” posed on the passage in Figure 1. In order to answer the question, one might first locate the relevant part of the passage “precipitation ... falls under gravity”, then reason that “under” refers to a cause (not location), and thus determine the correct answer: “gravity”.\nHow can we get a machine to make progress on the challenging task of reading comprehension?\nSQuAD is freely available at https://stanford-qa.com.\nHistorically, large, realistic datasets have played a critical role for driving fields forward—famous examples include ImageNet for object recognition (Deng et al., 2009) and the Penn Treebank for syntactic parsing (Marcus et al., 1993). Existing datasets for RC have one of two shortcomings: (i) those that are high in quality (Richardson et al., 2013; Berant et al., 2014) are too small for training modern data-intensive models, while (ii) those that are large (Hermann et al., 2015; Hill et al., 2015) are semi-synthetic and do not share the same characteristics as explicit reading comprehension questions.\nTo address the need for a large and high-quality\nar X\niv :1\n60 6.\n05 25\n0v 1\n[ cs\n.C L\n] 1\n6 Ju\nn 20\nreading comprehension dataset, we present the Stanford Question Answering Dataset (SQuAD), consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage. SQuAD contains 107,785 question-answer pairs on 536 articles, and is almost two orders of magnitude larger than previous manually labeled RC datasets such as MCTest (Richardson et al., 2013).\nIn contrast to prior datasets, SQuAD does not provide a list of answer choices for each question. Rather, systems must select the answer from all possible spans in the passage. Thus, systems need to cope with a larger number of candidates, some of which are very similar. While this setup potentially excludes the possibility of asking highly interpretative questions found in standardized tests, in practice we find a rich diversity of answer types in SQuAD, with the additional benefit that answers as spans are easier to evaluate than as free-form text. We develop automatic techniques to quantify this diversity and stratify the questions by difficulty.\nTo assess the difficulty of SQuAD, we implemented a logistic regression model with a range of features. We find that lexicalized and dependency tree path features are important to the performance of the model. We also find that the model performance worsens with increasing complexity of (i) answer types and (ii) syntactic divergence between the question and the sentence containing the answer; interestingly, there is no such degradation for humans. Our best model achieves an F1 score of 51.0%, which is much better than the sliding window baseline (20%). However, it is still far from the human F1 of 86.8%, suggesting that there is plenty of room for the development of more sophisticated models."
    }, {
      "heading" : "2 Existing Datasets",
      "text" : "We begin with a survey of existing reading comprehension and question answering (QA) datasets, highlighting a variety of task formulation and creation strategies (see Table 1 for an overview).\nReading comprehension. A data-driven approach to reading comprehension goes back to Hirschman et al. (1999), who curated a dataset of 600 real 3rd–\n6th grade reading comprehension questions. Their pattern matching baseline was subsequently improved by a rule-based system (Riloff and Thelen, 2000) and a logistic regression model (Ng et al., 2000). More recently, Richardson et al. (2013) curated MCTest, which contains 660 stories created by crowdworkers, with 4 questions per story and 4 answer choices per question. Because many of the questions require commonsense reasoning and reasoning across multiple sentences, the dataset remains quite challenging, though there has been noticeable progress (Narasimhan and Barzilay, 2015; Sachan et al., 2015; Wang et al., 2015). Both curated datasets, although real and difficult, are too small to support very expressive statistical models.\nSome datasets focus on deeper reasoning abilities. Algebra word problems require understanding a story well enough to turn it into a system of equations, which can be easily solved to produce the answer (Kushman et al., 2014; Hosseini et al., 2014). BAbI (Weston et al., 2015), a fully synthetic RC dataset, is stratified by different types of reasoning required to solve each task. Clark and Etzioni (2016) describe the task of solving 4th grade science exams, and stress the need to reason with world knowledge.\nOpen-domain question answering. The goal of open-domain QA is to answer a question from a\nlarge collection of documents. The annual evaluations at the Text REtreival Conference (TREC) (Voorhees and Tice, 2000) led to many advances in open-domain QA, many of which were used in IBM Watson for Jeopardy! (Ferrucci et al., 2013). Recently, Yang et al. (2015) created the WikiQA dataset, which, like SQuAD, use Wikipedia passages as a source of answers, but their task is sentence selection, while ours requires selecting a specific span in the sentence.\nSelecting the span of text that answers a question is similar to answer extraction, the final step in the open-domain QA pipeline, methods for which include bootstrapping surface patterns (Ravichandran and Hovy, 2002), using dependency trees (Shen and Klakow, 2006), and using a factor graph over multiple sentences (Sun et al., 2013). One key difference between our RC setting and answer extraction is that answer extraction typically exploits the fact that the answer occurs in multiple documents (Brill et al., 2002), which is more lenient than in our setting, where a system only has access to a single reading passage.\nCloze datasets. Recently, researchers have constructed cloze datasets, in which the goal is to predict the missing word (often a named entity) in a passage. Since these datasets can be automatically generated from naturally occurring data, they can be extremely large. The Children’s Book Test (CBT) (Hill et al., 2015), for example, involves predicting a blanked-out word of a sentence given the 20 previous sentences. Hermann et al. (2015) constructed a corpus of cloze style questions by blanking out entities in abstractive summaries of CNN / Daily News articles; the goal is to fill in the entity based on the original article. While the size of this dataset is impressive, Chen et al. (2016) showed that the dataset requires less reasoning than previously thought, and concluded that performance is almost saturated.\nOne difference between SQuAD questions and cloze-style queries is that answers to cloze queries are single words or entities, while answers in SQuAD often include non-entities and can be much longer phrases. Another difference is that SQuAD focuses on questions whose answers are entailed by the passage, whereas the answers to cloze-style queries are merely suggested by the passage."
    }, {
      "heading" : "3 Dataset Collection",
      "text" : "We collect our dataset in three stages: curating passages, crowdsourcing question-answers on those passages, and obtaining additional answers.\nPassage curation. To retrieve high-quality articles, we used Project Nayuki’s Wikipedia’s internal PageRanks to obtain the top 10000 articles of English Wikipedia, from which we sampled 536 articles uniformly at random. From each of these articles, we extracted individual paragraphs, stripping away images, figures, tables, and discarding paragraphs shorter than 500 characters. The result was 23,215 paragraphs for the 536 articles covering a wide range of topics, from musical celebrities to abstract concepts. We partitioned the articles randomly into a training set (80%), a development set (10%), and a test set (10%).\nQuestion-answer collection. Next, we employed crowdworkers to create questions. We used the Daemo platform (Gaikwad et al., 2015), with Amazon Mechanical Turk at its backend. Crowdworkers were required to have a 97% HIT acceptance rate, a minimum of 1000 HITs, and be located in the United States or Canada. Workers were asked to spend 4 minutes on every paragraph, and paid $9 per hour for the number of hours required to complete the article.\nThe task was reviewed favorably by crowdworkers, receiving positive comments on Turkopticon.\nFigure 2 shows the interface. On each paragraph, crowdworkers were tasked with asking and answering up to 5 questions on the content of that paragraph. The questions had to be entered in a text field, and the answers had to be highlighted in the paragraph. To guide the workers, tasks contained a sample paragraph, and examples of good and bad questions and answers on that paragraph along with the reasons they were categorized as such. Additionally, crowdworkers were encouraged to ask questions in their own words, without copying word phrases from the paragraph. On the interface, this was reinforced by a reminder prompt at the beginning of every paragraph, and by disabling copypaste functionality on the paragraph text.\nAdditional answers collection. To get an indication of human performance on SQuAD and to make our evaluation more robust, we obtained 2 additional answers for each question in the development and test sets. In the secondary answer generation task, each crowdworker was shown only the questions along with the paragraphs of an article, and asked to select the shortest span in the paragraph that answered the question. If a question was not answerable by a span in the paragraph, workers were asked to submit the question without marking an answer. Workers were recommended a speed of 5 questions for 2 minutes, and paid at the same rate of $9 per hour for the number of hours required for the entire article."
    }, {
      "heading" : "4 Dataset Analysis",
      "text" : "To understand the properties of SQuAD, we analyze the questions and answers in SQuAD’s development set. Specifically, we explore the (i) diversity of answer types, (ii) the difficulty of questions in terms of type of reasoning required to answer them, and (iii) the degree of syntactic divergence between the question and answer sentences.\nDiversity in answers. We automatically categorize the answers as follows: We first separate the numerical and non-numerical answers. The non-numerical answers are categorized using constituency parses and POS tags generated by Stan-\nQ: What department store is thought to be the first in the world? S: Bainbridge’s is often cited as the world’s first department store.\nford CoreNLP. The proper noun phrases are further split into person, location and other entities using NER tags. In Table 2, we can see dates and other numbers make up 19.8% of the data; 32.6% of the answers are proper nouns of three different types; 31.8% are common noun phrases answers; and the remaining 15.8% are made up of adjective phrases, verb phrases, clauses and other types.\nReasoning required to answer questions. To get a better understanding of the reasoning required to answer the questions, we sampled 4 questions from each of the 48 articles in the development set, and then manually labeled the examples with the categories shown in Table 3. The results show that all examples have some sort of lexical or syntactic divergence between the question and the answer in the passage. Note that some examples fall into more than one category.\nStratification by syntactic divergence. We also develop an automatic method to quantify the syntactic divergence between a question and the sentence containing the answer. This provides another way\nto measure the difficulty of a question and to stratify the dataset, which we return to in Section 6.3. We illustrate how we measure the divergence with the example in Figure 3. We first detect anchors (word-lemma pairs common to both the question and answer sentences); in the example, the anchor is “first”. The unlexicalized paths from the anchor “first” to the question word “what” and to the answer span “Bainbridge’s” are then extracted from the dependency parse trees. We measure the edit distance between two paths with a cost of 1 for deletion and insertion, and 2 for substitution. The syntactic divergence is the minimum edit distance over all possible anchors. The histogram in Figure 4a show that there is a wide range of syntactic divergence in our dataset. We also show a concrete example where the edit distance is 0 and another where it is 6. Note that our syntactic divergence ignores lexical variation. Small divergence does not mean that a question is easy since there could be other candidates with similarly small divergence."
    }, {
      "heading" : "5 Methods",
      "text" : "We developed a logistic regression model and compare its accuracy with that of three baseline methods. Candidate answer generation. For all of the methods, we first prune down the list of spans using constituency trees. Assuming there are M tokens in a passage, there are O(M2) possible answers to each question. To prune the large space of possible answers, we consider those that are constituents in parses generated by Stanford CoreNLP as candidate answers for all our models. After stripping punctuation and articles, we find that 77.3% of the correct answers in the development set are constituents."
    }, {
      "heading" : "5.1 Sliding Window Baseline",
      "text" : "For each candidate answer, we compute the unigram/bigram overlap between the sentence containing it (excluding the candidate itself) and the question. We keep all the candidates that have the maximal overlap. Among these, we select the best one using the sliding-window approach proposed in Richardson et al. (2013).\nIn addition to the basic sliding window approach, we also implemented the distance-based extension (Richardson et al., 2013). Whereas Richardson et al. (2013) used the entire passage as the context of an answer, we used only the sentence containing the candidate answer for efficiency."
    }, {
      "heading" : "5.2 Logistic Regression",
      "text" : "In our logistic regression model, we extract several types of features for each candidate answer. We discretize each continuous feature into 10 equallysized buckets, building a total of 180 million features, most of which are lexicalized features or dependency tree path features. The descriptions and examples of the features are summarized in Table 4.\nThe matching word and bigram frequencies as well as the root match features help the model pick the correct sentences. Length features bias the model to picking common lengths and positions for answer spans, while span word frequencies bias the model against uninformative words. Constituent label and span POS tag features guide the model towards the correct answer types. In addition to these basic features, we resolve lexical variation using lexicalized features, and syntactic variation using dependency tree path features.\nThe multiclass log-likelihood loss is optimized using AdaGrad with an initial learning rate of 0.1.\nEach update is performed on the batch of all questions in a paragraph for efficiency, since they share the same candidates. L2 regularization is used, with a coefficient of 0.1 divided by the number of batches. The model is trained with three passes over the training data."
    }, {
      "heading" : "6 Experiments",
      "text" : ""
    }, {
      "heading" : "6.1 Model Evaluation",
      "text" : "We use two different metrics to evaluate model performance. Both metrics ignore string differences in punctuation and inclusion/exclusion of definite and indefinite articles.\nExact match. This metric measures the percentage of predictions that match one of the ground truth answers exactly.\n(Macro-averaged) F1 score. This metric loosely measures the average overlap between the prediction and ground truth answer. We treat the prediction and ground truth as bags of tokens, and compute their F1. We take the maximum F1 over all of the ground truth answers for a given question, and then average over all of the questions."
    }, {
      "heading" : "6.2 Human Performance",
      "text" : "We assess human performance on SQuAD’s development and test sets. Recall that each of the questions in these sets has at least three answers. To evaluate human performance, we treat the second answer to each question as the human prediction, and keep the other answers as ground truth answers. The resulting human performance score on the test set is 77.0% for the exact match metric, and 86.8% for F1. Mismatch occurs mostly to inclusion of optional phrases (e.g., monsoon trough versus movement of the monsoon trough) rather than disagreements about the answer."
    }, {
      "heading" : "6.3 Model Performance",
      "text" : "Table 5 shows the performance of our models alongside human performance on the development and test sets. The logistic regression model significantly outperforms the baselines, but underperforms hu-\nmans. We note that the model is able to select the sentence containing the answer correctly with 79.3% accuracy; the real difficulty lies in finding the exact span within the sentence.\nFeature ablations. In order to understand the features that are responsible for the performance of the logistic regression model, we perform a feature ablation where we remove one group of features from\nour model at a time. The results, shown in Table 6, indicate that lexicalized and dependency tree path features are most important. Comparing our analysis to the one in Chen et al. (2016), we note that the dependency tree path features play a much bigger role in our dataset. Additionally, we note that with lexicalized features, the model significantly overfits the training set; however, with increased L2 regularization, development performance would degrade.\nPerformance stratified by answer type. To gain more insight into the performance of our logistic regression model, we report its performance across the answer types explored in Table 2. The results (shown in Table 7) show that the model performs best on dates and other numbers, categories for which there are usually few plausible candidates and most answers are single tokens. The model is\nchallenged more on other named entities (i.e. location, person and other entity) because there are many more plausible candidates. However, named entities are still relatively easy to identify by their POS tag features. The model performs worst on other answer types, which together form 47.6% of the dataset. Humans have exceptional performance on dates, numbers and all named entities. Their performance on other answer types degrades only slightly.\nPerformance stratified by syntactic divergence. Another aspect of the dataset that makes it difficult is the syntactic divergence between the question and answer sentence, as discussed in Section 4. As shown in Figure 5, the more divergence there is, the lower the performance of the logistic regression model. Interestingly, humans do not seem to be sensitive to syntactic divergence."
    }, {
      "heading" : "7 Conclusion",
      "text" : "We view datasets as a way to guide progress towards the end goal of natural language understanding. To that end, we introduce SQuAD, a large reading comprehension dataset on Wikipedia articles with crowdsourced question-answer pairs. SQuAD features a diverse range of question and answer types. The performance of our logistic regression model, with 51.0% F1, against the human F1 of 86.8% suggests ample opportunity for improvement. We make our dataset freely available to encourage exploration of more expressive models."
    } ],
    "references" : [ {
      "title" : "Modeling biological processes for reading comprehension",
      "author" : [ "Berant et al.2014] J. Berant", "V. Srikumar", "P. Chen", "A.V. Linden", "B. Harding", "B. Huang", "P. Clark", "C.D. Manning" ],
      "venue" : "In Empirical Methods in Natural Language Processing (EMNLP)",
      "citeRegEx" : "Berant et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Berant et al\\.",
      "year" : 2014
    }, {
      "title" : "An analysis of the AskMSR question-answering system",
      "author" : [ "Brill et al.2002] E. Brill", "S. Dumais", "M. Banko" ],
      "venue" : "In Association for Computational Linguistics (ACL),",
      "citeRegEx" : "Brill et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Brill et al\\.",
      "year" : 2002
    }, {
      "title" : "A thorough examination of the CNN / Daily Mail reading comprehension",
      "author" : [ "Chen et al.2016] D. Chen", "J. Bolton", "C.D. Manning" ],
      "venue" : null,
      "citeRegEx" : "Chen et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2016
    }, {
      "title" : "My computer is an honor student but how intelligent is it? standardized tests as a measure of AI",
      "author" : [ "Clark", "Etzioni2016] P. Clark", "O. Etzioni" ],
      "venue" : null,
      "citeRegEx" : "Clark et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2016
    }, {
      "title" : "ImageNet: A large-scale hierarchical image database",
      "author" : [ "Deng et al.2009] J. Deng", "W. Dong", "R. Socher", "L. Li", "K. Li", "L. Fei-Fei" ],
      "venue" : "In Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "Deng et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Deng et al\\.",
      "year" : 2009
    }, {
      "title" : "Building Watson: An overview of the DeepQA project",
      "author" : [ "Ferrucci et al.2013] D. Ferrucci", "E. Brown", "J. ChuCarroll", "J. Fan", "D. Gondek", "A.A. Kalyanpur", "A. Lally", "J.W. Murdock", "E. Nyberg", "J. Prager", "N. Schlaefer", "C. Welty" ],
      "venue" : null,
      "citeRegEx" : "Ferrucci et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Ferrucci et al\\.",
      "year" : 2013
    }, {
      "title" : "Daemo: A self-governed crowdsourcing marketplace",
      "author" : [ "D. Morina", "R. Nistala", "M. Agarwal", "A. Cossette", "R. Bhanu", "S. Savage", "V. Narwal", "K. Rajpal", "J. Regino" ],
      "venue" : "In Proceedings of the 28th Annual ACM",
      "citeRegEx" : "Gaikwad et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Gaikwad et al\\.",
      "year" : 2015
    }, {
      "title" : "Teaching machines to read and comprehend",
      "author" : [ "T. Kočiský", "E. Grefenstette", "L. Espeholt", "W. Kay", "M. Suleyman", "P. Blunsom" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS)",
      "citeRegEx" : "Hermann et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Hermann et al\\.",
      "year" : 2015
    }, {
      "title" : "The goldilocks principle: Reading children’s books with explicit memory representations",
      "author" : [ "Hill et al.2015] F. Hill", "A. Bordes", "S. Chopra", "J. Weston" ],
      "venue" : "In International Conference on Learning Representations (ICLR)",
      "citeRegEx" : "Hill et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Hill et al\\.",
      "year" : 2015
    }, {
      "title" : "Deep read: A reading comprehension system",
      "author" : [ "M. Light", "E. Breck", "J.D. Burger" ],
      "venue" : "In Association for Computational Linguistics (ACL),",
      "citeRegEx" : "Hirschman et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Hirschman et al\\.",
      "year" : 1999
    }, {
      "title" : "Learning to solve arithmetic word problems with verb categorization",
      "author" : [ "H. Hajishirzi", "O. Etzioni", "N. Kushman" ],
      "venue" : "In Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Hosseini et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Hosseini et al\\.",
      "year" : 2014
    }, {
      "title" : "Learning to automatically solve algebra word problems",
      "author" : [ "Kushman et al.2014] N. Kushman", "Y. Artzi", "L. Zettlemoyer", "R. Barzilay" ],
      "venue" : null,
      "citeRegEx" : "Kushman et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kushman et al\\.",
      "year" : 2014
    }, {
      "title" : "Building a large annotated corpus of English: the Penn Treebank",
      "author" : [ "M.P. Marcus", "M.A. Marcinkiewicz", "B. Santorini" ],
      "venue" : null,
      "citeRegEx" : "Marcus et al\\.,? \\Q1993\\E",
      "shortCiteRegEx" : "Marcus et al\\.",
      "year" : 1993
    }, {
      "title" : "Machine comprehension with discourse relations. In Association for Computational Linguistics (ACL)",
      "author" : [ "Narasimhan", "Barzilay2015] K. Narasimhan", "R. Barzilay" ],
      "venue" : null,
      "citeRegEx" : "Narasimhan et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Narasimhan et al\\.",
      "year" : 2015
    }, {
      "title" : "A machine learning approach to answering questions for reading comprehension tests. In Joint SIGDAT conference on empirical methods in natural language processing and very large corpora - Volume",
      "author" : [ "H.T. Ng", "L.H. Teo", "J.L.P. Kwan" ],
      "venue" : null,
      "citeRegEx" : "Ng et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Ng et al\\.",
      "year" : 2000
    }, {
      "title" : "Learning surface text patterns for a question answering system",
      "author" : [ "Ravichandran", "Hovy2002] D. Ravichandran", "E. Hovy" ],
      "venue" : "In Association for Computational Linguistics (ACL),",
      "citeRegEx" : "Ravichandran et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Ravichandran et al\\.",
      "year" : 2002
    }, {
      "title" : "Mctest: A challenge dataset for the open-domain machine comprehension of text",
      "author" : [ "C.J. Burges", "E. Renshaw" ],
      "venue" : "In Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Richardson et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Richardson et al\\.",
      "year" : 2013
    }, {
      "title" : "A rule-based question answering system for reading comprehension tests. In ANLP/NAACL Workshop on reading comprehension tests as evaluation for computer-based language understanding sytems",
      "author" : [ "Riloff", "Thelen2000] E. Riloff", "M. Thelen" ],
      "venue" : null,
      "citeRegEx" : "Riloff et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Riloff et al\\.",
      "year" : 2000
    }, {
      "title" : "Learning answer-entailing structures for machine comprehension",
      "author" : [ "Sachan et al.2015] M. Sachan", "A. Dubey", "E.P. Xing", "M. Richardson" ],
      "venue" : null,
      "citeRegEx" : "Sachan et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Sachan et al\\.",
      "year" : 2015
    }, {
      "title" : "Exploring correlation of dependency relation paths for answer extraction",
      "author" : [ "Shen", "Klakow2006] D. Shen", "D. Klakow" ],
      "venue" : "In International Conference on Computational Linguistics and Association for Computational Linguistics (COLING/ACL),",
      "citeRegEx" : "Shen et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2006
    }, {
      "title" : "N-gram idf: A global term weighting scheme based on information distance",
      "author" : [ "T. Hara", "S. Nishio" ],
      "venue" : "In World Wide Web (WWW),",
      "citeRegEx" : "Shirakawa et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Shirakawa et al\\.",
      "year" : 2015
    }, {
      "title" : "Answer extraction from passage graph for question answering",
      "author" : [ "H. Sun", "N. Duan", "Y. Duan", "M. Zhou" ],
      "venue" : "In International Joint Conference on Artificial Intelligence (IJCAI)",
      "citeRegEx" : "Sun et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2013
    }, {
      "title" : "Building a question answering test collection",
      "author" : [ "Voorhees", "Tice2000] E.M. Voorhees", "D.M. Tice" ],
      "venue" : null,
      "citeRegEx" : "Voorhees et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Voorhees et al\\.",
      "year" : 2000
    }, {
      "title" : "Machine comprehension with syntax, frames, and semantics. In Association for Computational Linguistics (ACL)",
      "author" : [ "H. Wang", "M. Bansal", "K. Gimpel", "D. McAllester" ],
      "venue" : null,
      "citeRegEx" : "Wang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2015
    }, {
      "title" : "Towards AI-complete question answering: A set of prerequisite toy tasks. arXiv",
      "author" : [ "Weston et al.2015] J. Weston", "A. Bordes", "S. Chopra", "T. Mikolov" ],
      "venue" : null,
      "citeRegEx" : "Weston et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Weston et al\\.",
      "year" : 2015
    }, {
      "title" : "WikiQA: A challenge dataset for open-domain question answering",
      "author" : [ "Y. Yang", "W. Yih", "C. Meek" ],
      "venue" : "In Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Yang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "Historically, large, realistic datasets have played a critical role for driving fields forward—famous examples include ImageNet for object recognition (Deng et al., 2009) and the Penn Treebank for syntactic parsing (Marcus et al.",
      "startOffset" : 151,
      "endOffset" : 170
    }, {
      "referenceID" : 12,
      "context" : ", 2009) and the Penn Treebank for syntactic parsing (Marcus et al., 1993).",
      "startOffset" : 52,
      "endOffset" : 73
    }, {
      "referenceID" : 16,
      "context" : "Existing datasets for RC have one of two shortcomings: (i) those that are high in quality (Richardson et al., 2013; Berant et al., 2014) are too small for training modern data-intensive models, while (ii) those that are large (Hermann et al.",
      "startOffset" : 90,
      "endOffset" : 136
    }, {
      "referenceID" : 0,
      "context" : "Existing datasets for RC have one of two shortcomings: (i) those that are high in quality (Richardson et al., 2013; Berant et al., 2014) are too small for training modern data-intensive models, while (ii) those that are large (Hermann et al.",
      "startOffset" : 90,
      "endOffset" : 136
    }, {
      "referenceID" : 7,
      "context" : ", 2014) are too small for training modern data-intensive models, while (ii) those that are large (Hermann et al., 2015; Hill et al., 2015) are semi-synthetic and do not share the same characteristics as explicit reading comprehension questions.",
      "startOffset" : 97,
      "endOffset" : 138
    }, {
      "referenceID" : 8,
      "context" : ", 2014) are too small for training modern data-intensive models, while (ii) those that are large (Hermann et al., 2015; Hill et al., 2015) are semi-synthetic and do not share the same characteristics as explicit reading comprehension questions.",
      "startOffset" : 97,
      "endOffset" : 138
    }, {
      "referenceID" : 16,
      "context" : "SQuAD contains 107,785 question-answer pairs on 536 articles, and is almost two orders of magnitude larger than previous manually labeled RC datasets such as MCTest (Richardson et al., 2013).",
      "startOffset" : 165,
      "endOffset" : 190
    }, {
      "referenceID" : 9,
      "context" : "A data-driven approach to reading comprehension goes back to Hirschman et al. (1999), who curated a dataset of 600 real 3rd– Dataset Question source Formulation Size",
      "startOffset" : 61,
      "endOffset" : 85
    }, {
      "referenceID" : 16,
      "context" : "MCTest (Richardson et al., 2013) crowdsourced RC, multiple choice 2640",
      "startOffset" : 7,
      "endOffset" : 32
    }, {
      "referenceID" : 11,
      "context" : "Algebra (Kushman et al., 2014) standardized tests computation 514",
      "startOffset" : 8,
      "endOffset" : 30
    }, {
      "referenceID" : 25,
      "context" : "WikiQA (Yang et al., 2015) query logs IR, sentence selection 3047",
      "startOffset" : 7,
      "endOffset" : 26
    }, {
      "referenceID" : 7,
      "context" : "CNN/Daily Mail (Hermann et al., 2015) summary + cloze RC, fill in single entity 1.",
      "startOffset" : 15,
      "endOffset" : 37
    }, {
      "referenceID" : 8,
      "context" : "CBT (Hill et al., 2015) cloze RC, fill in single word 688K",
      "startOffset" : 4,
      "endOffset" : 23
    }, {
      "referenceID" : 14,
      "context" : "Their pattern matching baseline was subsequently improved by a rule-based system (Riloff and Thelen, 2000) and a logistic regression model (Ng et al., 2000).",
      "startOffset" : 139,
      "endOffset" : 156
    }, {
      "referenceID" : 18,
      "context" : "Because many of the questions require commonsense reasoning and reasoning across multiple sentences, the dataset remains quite challenging, though there has been noticeable progress (Narasimhan and Barzilay, 2015; Sachan et al., 2015; Wang et al., 2015).",
      "startOffset" : 182,
      "endOffset" : 253
    }, {
      "referenceID" : 23,
      "context" : "Because many of the questions require commonsense reasoning and reasoning across multiple sentences, the dataset remains quite challenging, though there has been noticeable progress (Narasimhan and Barzilay, 2015; Sachan et al., 2015; Wang et al., 2015).",
      "startOffset" : 182,
      "endOffset" : 253
    }, {
      "referenceID" : 14,
      "context" : "Their pattern matching baseline was subsequently improved by a rule-based system (Riloff and Thelen, 2000) and a logistic regression model (Ng et al., 2000). More recently, Richardson et al. (2013) curated MCTest, which contains 660 stories created by crowdworkers, with 4 questions per story and 4 answer choices per question.",
      "startOffset" : 140,
      "endOffset" : 198
    }, {
      "referenceID" : 11,
      "context" : "Algebra word problems require understanding a story well enough to turn it into a system of equations, which can be easily solved to produce the answer (Kushman et al., 2014; Hosseini et al., 2014).",
      "startOffset" : 152,
      "endOffset" : 197
    }, {
      "referenceID" : 10,
      "context" : "Algebra word problems require understanding a story well enough to turn it into a system of equations, which can be easily solved to produce the answer (Kushman et al., 2014; Hosseini et al., 2014).",
      "startOffset" : 152,
      "endOffset" : 197
    }, {
      "referenceID" : 24,
      "context" : "BAbI (Weston et al., 2015), a fully synthetic RC dataset, is stratified by different types of reasoning required to solve each task.",
      "startOffset" : 5,
      "endOffset" : 26
    }, {
      "referenceID" : 10,
      "context" : ", 2014; Hosseini et al., 2014). BAbI (Weston et al., 2015), a fully synthetic RC dataset, is stratified by different types of reasoning required to solve each task. Clark and Etzioni (2016) describe the task of solving 4th grade science exams, and stress the need to reason with world knowledge.",
      "startOffset" : 8,
      "endOffset" : 190
    }, {
      "referenceID" : 5,
      "context" : "The annual evaluations at the Text REtreival Conference (TREC) (Voorhees and Tice, 2000) led to many advances in open-domain QA, many of which were used in IBM Watson for Jeopardy! (Ferrucci et al., 2013).",
      "startOffset" : 181,
      "endOffset" : 204
    }, {
      "referenceID" : 5,
      "context" : "The annual evaluations at the Text REtreival Conference (TREC) (Voorhees and Tice, 2000) led to many advances in open-domain QA, many of which were used in IBM Watson for Jeopardy! (Ferrucci et al., 2013). Recently, Yang et al. (2015) created the WikiQA dataset, which, like SQuAD, use Wikipedia passages as a source of answers, but their task is sentence selection, while ours requires selecting a specific span in the sentence.",
      "startOffset" : 182,
      "endOffset" : 235
    }, {
      "referenceID" : 21,
      "context" : "Selecting the span of text that answers a question is similar to answer extraction, the final step in the open-domain QA pipeline, methods for which include bootstrapping surface patterns (Ravichandran and Hovy, 2002), using dependency trees (Shen and Klakow, 2006), and using a factor graph over multiple sentences (Sun et al., 2013).",
      "startOffset" : 316,
      "endOffset" : 334
    }, {
      "referenceID" : 1,
      "context" : "One key difference between our RC setting and answer extraction is that answer extraction typically exploits the fact that the answer occurs in multiple documents (Brill et al., 2002), which is more lenient than in our setting, where a system only has access to a single reading passage.",
      "startOffset" : 163,
      "endOffset" : 183
    }, {
      "referenceID" : 8,
      "context" : "The Children’s Book Test (CBT) (Hill et al., 2015), for example, involves predicting a blanked-out word of a sentence given the 20 previous sentences.",
      "startOffset" : 31,
      "endOffset" : 50
    }, {
      "referenceID" : 6,
      "context" : "Hermann et al. (2015) constructed a corpus of cloze style questions by blanking out entities in abstractive summaries of CNN / Daily News articles; the goal is to fill in the entity based on the original article.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 2,
      "context" : "While the size of this dataset is impressive, Chen et al. (2016) showed that the dataset requires less reasoning than previously thought, and concluded that performance is almost saturated.",
      "startOffset" : 46,
      "endOffset" : 65
    }, {
      "referenceID" : 6,
      "context" : "We used the Daemo platform (Gaikwad et al., 2015), with Amazon Mechanical Turk at its backend.",
      "startOffset" : 27,
      "endOffset" : 49
    }, {
      "referenceID" : 16,
      "context" : "Among these, we select the best one using the sliding-window approach proposed in Richardson et al. (2013).",
      "startOffset" : 82,
      "endOffset" : 107
    }, {
      "referenceID" : 16,
      "context" : "In addition to the basic sliding window approach, we also implemented the distance-based extension (Richardson et al., 2013).",
      "startOffset" : 99,
      "endOffset" : 124
    }, {
      "referenceID" : 16,
      "context" : "In addition to the basic sliding window approach, we also implemented the distance-based extension (Richardson et al., 2013). Whereas Richardson et al. (2013) used the entire passage as the context of an answer, we used only the sentence containing the candidate answer for efficiency.",
      "startOffset" : 100,
      "endOffset" : 159
    }, {
      "referenceID" : 20,
      "context" : "We use the generalization of the TF-IDF described in Shirakawa et al. (2015). Span: [0 ≤ sum < 2.",
      "startOffset" : 53,
      "endOffset" : 77
    }, {
      "referenceID" : 2,
      "context" : "Comparing our analysis to the one in Chen et al. (2016), we note that the dependency tree path features play a much bigger role in our dataset.",
      "startOffset" : 37,
      "endOffset" : 56
    } ],
    "year" : 2016,
    "abstractText" : "We present a new reading comprehension dataset, SQuAD, consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage. We analyze the dataset in both manual and automatic ways to understand the types of reasoning required to answer the questions, leaning heavily on dependency and constituency trees. We built a strong logistic regression model, which achieves an F1 score of 51.0%, a significant improvement over a simple baseline (20%). However, human performance (86.8%) is much higher, indicating that the dataset presents a good challenge problem for future research.",
    "creator" : "LaTeX with hyperref package"
  }
}