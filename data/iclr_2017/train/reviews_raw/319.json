{"conference": "ICLR 2017 conference submission", "title": "Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer", "abstract": "Attention plays a critical role in human visual experience. Furthermore, it has recently been demonstrated that attention can also play an important role in the context of applying artificial neural networks to a variety of tasks from fields such as computer vision and NLP. In this work we show that, by properly defining attention for convolutional neural networks, we can actually use this type of information in order to significantly improve the performance of a student CNN network by forcing it to mimic the attention maps of a powerful teacher network. To that end, we propose several novel methods of transferring attention, showing consistent improvement across a variety of datasets and convolutional neural network architectures.", "reviews": [{"TITLE": "ICLR committee final decision", "OTHER_KEYS": "ICLR 2017 pcs", "comments": "Important task (attention models), interesting distillation application, well-written paper. The authors have been responsive in updating the paper, adding new experiments, and being balanced in presenting their findings. I support accepting this paper.", "IS_META_REVIEW": false, "DATE": "06 Feb 2017", "is_meta_review": false}, {"TITLE": "Answer to reviewers", "OTHER_KEYS": "Sergey Zagoruyko", "comments": "We would like to thank the reviewers for their comments and for providing us with valuable feedback. We already updated the paper with new results on ImageNet, and are also going to further update the paper based on the reviewer comments as explained in our responses below.\nWe also plan to release the code for our experiments next week.", "IS_META_REVIEW": false, "DATE": "29 Dec 2016", "is_meta_review": false}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "official review", "is_meta_review": false, "comments": "The paper proposes a new way of transferring knowledge.\nI like the idea of transferring attention maps instead of activations.\nHowever, the experiments don\u2019t show a big improvement compared with knowledge distillation alone and I think more experiments are required in IMAGENET section.\nI would consider updating the score if the authors extend the last section 4.2.2.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "21 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "Review", "is_meta_review": false, "comments": "The paper presented a modified knowledge distillation framework that minimizes the difference of the sum of statistics across the a feature map between the teacher and the student network. The authors empirically demonstrated the proposed methods outperform the fitnet style distillation baseline. \n\nPros:\n+ The author evaluated the proposed methods on various computer vision dataset \n+ The paper is in general well-written\n\nCons:  \n- The method seems to be limited to the convolutional architecture\n- The attention terminology is misleading in the paper. The proposed method really just try to distill the summed squared(or other statistics e.g. summed lp norm) of  activations in a hidden feature map.\n- The gradient-based attention transfer seems out-of-place. The proposed gradient-based methods are never compared directly to nor are used jointly with the \"attention-based\" transfer. It seems like a parallel idea added to the paper that does not seem to add much value.\n- It is also not clear how the induced 2-norms in eq.(2) is computed. Q is a matrix \\in \\mathbb{R}^{H \\times W}  whose induced 2-norm is its largest singular value. It seems computationally expensive to compute such cost function. Is it possible the authors really mean the Frobenius norm?\n\nOverall, the proposed distillation method works well in practice but the paper has some organization issues and unclear notation.  \n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "20 Dec 2016 (modified: 24 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Some nice results, but it is not clear what are the advantages/drawbacks of the different attention maps", "is_meta_review": false, "comments": "This paper proposes to investigate attention transfers between a teacher and a student network. \n\nAttention transfer is performed by minimising the l2 distance between the teacher/student attention maps at different layers, in addition to minimising the classification loss and optionally a knowledge distillation term.\nAuthors define several activation based attentions (sum of absolute feature values raise at the power p or max of values raised at the power p). They also propose a gradient based attention (derivative of the Loss w.r.t. inputs). \n\nThey evaluate their approaches on several datasets (CIFAR, Cub/Scene, Imagenet) showing that attention transfers  does help improving the student network test performance.  However, the student networks performs worst than the teacher, even with attention.\n\nFew remarks/questions:\n- in section 3 authors  claim that networks with higher accuracy have a higher spatial correlation between the object and the attention map. While Figure 4 is compelling, it would be nice to have quantitative results showing that as well.\n- how did you choose the hyperparameter values, it would be nice to see what is the impact of $\\beta$.\n- it would be nice to report teacher train and validation loss in Figure 7 b)\n- from the experiments, it is not clear what at the pros/cons of the different attention maps\n- AT does not lead to better result than the teacher. However, the student networks have less parameters. It would be interesting to characterise the corresponding speed-up. If you keep the same architecture between the student and the teacher, is there any benefit to the attention transfer?\n\nIn summary:\nPros:\n- Clearly written and well motivated.\n- Consistent improvement of the student with attention compared to the student alone.\nCons:\n- Students have worst performances than the teacher models.\n- It is not clear which attention to use in which case?\n- Somewhat incremental novelty relatively to Fitnet\n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "the definition of activation tensor", "OTHER_KEYS": "Zehao Huang", "comments": "Hi, Sergey.\n\nI am confused about the definition of activation tensor in section 3.1.\n\nIs it obtained before or after ReLU activation function?\n\nIf it's got after ReLu, there is no need adding absolute function.\n\nThanks!", "IS_META_REVIEW": false, "DATE": "12 Dec 2016", "is_meta_review": false}, {"TITLE": "Notation question", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "", "IS_META_REVIEW": false, "DATE": "03 Dec 2016", "is_meta_review": false}, {"TITLE": "Pre review questions", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "", "IS_META_REVIEW": false, "DATE": "01 Dec 2016", "is_meta_review": false}], "authors": "Sergey Zagoruyko, Nikos Komodakis", "accepted": true, "id": ""}
