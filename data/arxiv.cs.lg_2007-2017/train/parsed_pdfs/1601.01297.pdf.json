{
  "name" : "1601.01297.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Angrier Birds: Bayesian reinforcement learning",
    "authors" : [ "Imanol Arrieta Ibarra", "Bernardo Ramos", "Lars Roemheld" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Keywords Reinforcement Learning — Q-Learning — RLSVI — Exploration\n1Department of Management Science and Engineering, Stanford University, California, United States\nContents\nIntroduction 1"
    }, {
      "heading" : "1 Model 1",
      "text" : ""
    }, {
      "heading" : "2 Algorithms 2",
      "text" : ""
    }, {
      "heading" : "2.1 Q-Learning (linear function approximation) . . . . 2",
      "text" : ""
    }, {
      "heading" : "2.2 Randomized Least Squares Value Iteration . . . . 3",
      "text" : ""
    }, {
      "heading" : "2.3 Feature extractors . . . . . . . . . . . . . . . . . . . . . . 3",
      "text" : "Pig Position Values • Pig Position Indicator (PP) • Nested Pig Position Counter (NPP) • Nested Pig Positions: Shifted Counters (NPPS) • Nested Pig Positions Counters with Obstacles (NPPO)"
    }, {
      "heading" : "3 Results and Discussion 4",
      "text" : ""
    }, {
      "heading" : "3.1 Comparison of feature extractors . . . . . . . . . . . 4",
      "text" : ""
    }, {
      "heading" : "3.2 RLSVI vs. regular Q-Learning . . . . . . . . . . . . . . 4",
      "text" : ""
    }, {
      "heading" : "3.3 Learning success . . . . . . . . . . . . . . . . . . . . . . . 4",
      "text" : ""
    }, {
      "heading" : "4 Conclusions and Future Work 5",
      "text" : "References 6\nIntroduction Angry Birds has been a wildly successful internet franchise centered around an original mobile game in which players shoot birds out of a slingshot to hit targets for points. Angry Birds is largely a deterministic game: 1 a complex physics engine governs flight, collision and impact to appear almost realistically. As such, optimal gameplay could be achieved by optimizing a highly complex function that describes trajectory planning – instead, we train several reinforcement learning algorithms to play the game, and compare their final performance and learning speeds with that of a human-play oracle.\n1In our simulation, the physics engine actually seemed to produce semirandom results: the same sequence of moves was not guaranteed to yield the exact same outcomes.\nTo simulate gameplay, we adapted an open-source project to recreate Angry Birds in Python [1] using the Pygame framework. After adapting the base code for our needs, we designed an API which exposed the game as a Markov Decision Process (MDP), largely following the conventions of our class framework.\nThe Python port we used is a simplified version of the original Angry Birds game: it includes 11 levels, only one (standard) type of bird, only one type of target, and only one type of beam. A level ends whenever there are either no targets or no more birds on the screen. If all targets were destroyed then the agent advances to the next level, otherwise, the agent loses and goes back to the first level. Each level’s score is calculated according to three parameters: number of birds left, shattered beams and columns and destroyed targets. If the agent loses, it incurs an arbitrary penalty.\nThis somewhat reduced state-action space allowed a relatively straightforward proof-of-concept – we believe that very similar algorithms and models would also work for more complex versions of the game."
    }, {
      "heading" : "1. Model",
      "text" : "The game state exposed by our API is composed of the following information. This state representation fully describes the relevant game situation from the player’s perspective and therefore suffices to interface between the learner and the game. We note that all parts of the game state could be obtained from computer vision algorithms, so that in principle no direct interaction with the game mechanics would be necessary.\n• Number of birds left (“ammunition”)\n• Number of targets (“pigs”), and their absolute positions on the game map\nar X\niv :1\n60 1.\n01 29\n7v 2\n[ cs\n.A I]\n7 J\nan 2\n01 6\n• Number of obstacles (“beams”), and their absolute positions on the game map\n• The game score and level achieved by the player so far.\nWe further simplified game mechanics by making the game fully turn-based: the learner always waits until all objects are motionless before taking the next action. This assumption allowed us to work on absolute positions only.\nThe set of possible actions is a pair (angle, distance), describing the extension and aiming of the slingshot and thereby the bird’s launching momentum and direction. To somewhat accelerate learning, we allowed only shooting forwards. Within that constraint, we allowed all possible extensions and angles – we compare discretizations of the action space in varying levels of granularity. Our base discretization used 32 different (evenly spaced) actions."
    }, {
      "heading" : "2. Algorithms",
      "text" : "Reinforcement learning is a sub-field of artificial intelligence that describes ways for a learning algorithm to act in unknown and unspecified decision processes, guided merely by rewards and punishments for desirable and unwanted outcomes, respectively. As such, reinforcement learning closely mirrors human intuition about learning through conditioning, and allows algorithms to master systems that would otherwise be hard or impossible to specify.\nA standard algorithm in reinforcement learning is ε-greedy Q-learning. While very effective, this algorithm relies on an\nerratic exploration of possible actions. We implemented a variation which updates a belief distribution on optimal policies, and then samples policies from this distribution [2]. This implies more systematic exploration, since policies that seem unattractive with high confidence will not be re-tried.\nAfter introducing the two algorithms, we discuss various iterations to find optimal feature extractors."
    }, {
      "heading" : "2.1 Q-Learning (linear function approximation)",
      "text" : "As exposed in [3], Q-Learning is a model-free algorithm that approximates a state-action value function Qopt(s,a) in order to derive an optimal policy for an unknown decision process. In order to learn optimal behavior in an unknown game, a Q-Learner needs to both explore the system by choosing previously unseen actions, and to exploit the knowledge thus gathered by choosing the action that promises best outcomes, given previous experience.\nBalancing efficient exploration and exploitation is a main challenge in reinforcement learning: the most popular method in literature is the use of ε-greedy methods: with probability ε , the algorithm would ignore its previous experience and pick an action at random. This method tends to work well in practice whenever the actions needed to increase a payoff are not too complicated. However, when complex sets of actions need to be taken in order to get a higher reward, ε-greedy tends to take exponential time to find these rewards (see [2], [4], [5].)\nTo aid generalization over the Angry Birds state space, we used linear function approximation: we defined Qw(s,a) =\nwT φ(s,a), where w ∈ R f , and φ : S×A→ R f is a feature extractor that calculates f features from the given state-action pair. This allows the exploitation step to be an update only on the weights vector w, which we performed in a fashion similar to stochastic gradient descent (following the standard algorithm from [3])."
    }, {
      "heading" : "2.2 Randomized Least Squares Value Iteration",
      "text" : "Osband et al. propose Randomized Least Squares Value Iteration (RLSVI) [2], an algorithm that differs from our implementation of Q-Learning in two points:\n1. Instead of using gradient descent to learn from experience in an online fashion, RLSVI stores a full history of (state, action, reward, new state) tuples to derive an exact ”optimal” policy, given the data.\n2. Given hyperparameters about the quality of the linear approximation, Bayesian least squares is employed to estimate a distribution around the ”optimal” policy derived in 1. The learner then samples a policy from this distribution – this replaces ε-greedy exploration.\nSpecifically, RLSVI models the state-action value function as follows:\nQw(st ,at) = Reward(st ,at)+ γ max at+1 Qw(Successor(st ,at),at+1)\n≈ wTt φ(st ,at)+ν\nwhere γ is an arbitrary discount factor, ν ∼ N(0,σ2), and σ is a hyperparameter.\nGiven a memory of (state, action, reward, new state) tuples, we can use Bayesian least squares to derive w̄t , the expected value of the optimal policy and Σt = Cov(wt), the covariance of the optimal policy (the details of which are fairly straight-forward, and can be found in [2]). Note that given the assumption of linear approximation, a weights vector wt fully specifies a Q-function, and thereby a policy.\nThe learner then finally picks a policy by sampling\nŵt ∼ N(w̄t ,Σt)\nand taking the action such that Qŵt predicts the highest reward. Instead of taking entirely random actions with probability ε , the algorithm thus always samples random policies, but converges to optimal means as more data is accumulated and variances shrink. Therefore, less time may be expected to be ”wasted” on policies that are highly unlikely to be successful.\nTo keep this algorithm from erratic policy changes when only few observations have been made yet, we initialize with an uninformed prior. Furthermore, inspection showed that Σ was almost diagonal. To save computation, we therefore decided to sample using only variances of the individual weights (ignoring covariance between weights).\nOsband et al. propose RLSVI in the context of episodic learning, i.e. they suggest updating the policy only at the end of episodes of a fixed number of actions. This helps\nsteady the algorithm – unfortunately, our game simulation was computationally expensive, and the relatively small number of actions to be simulated forced us to learn in a continuous context."
    }, {
      "heading" : "2.3 Feature extractors",
      "text" : "In designing our feature extractors φ(s,a), we followed two premises: first, given that our value function approximation was linear, it was clear that interaction between features would have to be linear; in particular, separating the x and y components of locations would not work. Second, we wanted to give away as little domain knowledge about the game as possible – the task for the algorithm was to understand how to play Angry Birds without previously having a concept of targets or obstacles.\nThese two premises effectively impose a constraint on strategies, as there is no reliable way for the algorithm to detect -for example- whether an obstacle is just in front or just behind a target. Regardless, our learner developed impressive performance, as will be seen later. One reason for this may be that the levels in our simulator were relatively simple.\nWe iterated over the following five ideas to form meaningful features. Due to the relatively complex state-action space, our feature space was fairly big, quickly spanning several thousand features. This turned out to be a problem for RLSVI, as memory and computational requirements grew to be intractable. Rewriting the algorithm to run on sparse matrices offered some help, but ultimately we found reasonable performance with only the best-functioning features."
    }, {
      "heading" : "2.3.1 Pig Position Values",
      "text" : "In a first attempt, we used rounded target positions as feature values: for every pig i in the given state S, we would have feature values pi = (x,y,xy)T . The features were repeated for every action a, and would be non-zero only for the action that was taken. This allows different weights for different positionaction combinations, which ultimately implies learning the best position-action combinations.\nBy including the interaction term xy, we had hoped to capture the relative distance of the target to the slingshot (which x and y couldn’t establish by themselves in a linear function): this would have allowed fast generalization of target positions. Unfortunately, if in hindsight not surprisingly, this approach failed very quickly and produced practically no learning."
    }, {
      "heading" : "2.3.2 Pig Position Indicator (PP)",
      "text" : "The next approach was an indicator variable for a fine grid of pig positions: we created a separate feature for every possible pig position and again included the action taken. This created an impractically large feature space, yet worked relatively well. It did, however, clearly ”over-fit” – once a successful series of actions was found, the algorithm would reliably complete a level over and over again. If a target had moved only a few pixels, however, the algorithm would have to be retrained completely."
    }, {
      "heading" : "2.3.3 Nested Pig Position Counter (NPP)",
      "text" : "In order to solve the over-fitting problem we developed a nested mechanism that would generalize to unobserved states. To achieve this, we defined 3 nested grids over the game screen as exemplified in Figure 2. The three grids are progressively smaller, and each square of each grid is a count of the number of targets contained in it.\nThis solved the generalization issue while maintaining some of the nice properties of the previous feature extractor. While the larger grids helped to generalize, the finer ones provided a way to remember when a level had already been observed."
    }, {
      "heading" : "2.3.4 Nested Pig Positions: Shifted Counters (NPPS)",
      "text" : "One issue with NPP is that especially for the larger squares, targets that are close to a square boundary are not captured adequately. To improve on this, we created a copy of each grid, shifted diagonally by half a square size. Now every target lay in exactly two squares, allowing to judge whether a target is further to the left or the right within a square.\nNPPS was therefore a feature set of two sets of three overlapping square grids each. To our surprise, NPPS performed worse than the simpler NPP. We assume that this is due to the much bigger feature space, and the fact that we could not learn the algorithms until full convergence due to computational limits."
    }, {
      "heading" : "2.3.5 Nested Pig Positions Counters with Obstacles (NPPO)",
      "text" : "Finally, we tried to address the issue of obstacles: as described, we did not want to give away gameplay-related information by representing a fixed relationship between targets and obstacles. We therefore added counters for obstacles in the same way\nthat we used them for targets, hoping that the algorithm would learn to prefer areas with a high targetsobstacles ratio.\nJust as in the case of NPPS, we were surprised that adding information about obstacles was detrimental to learning success – indeed, adding obstacle information stopped learning altogether. As in NPPS, we suspect that NPPO may work well if given more time to converge in the bloated feature space."
    }, {
      "heading" : "3. Results and Discussion",
      "text" : "The somewhat crude feature extractor NPP provided best results, and RSLVI did indeed outperform the regular ε-greedy Q-Learner. RLSVI was particularly impressive for its ability to clear levels at almost the same speed as an (untrained) human player.\nWe could not simulate the gameplay until convergence as a result of computational limitations: the game engine we used was not intended for big simulations, and did not even speed up significantly when graphics were disabled."
    }, {
      "heading" : "3.1 Comparison of feature extractors",
      "text" : "We compared our five different feature extractors on the QLearning baseline algorithm, as displayed in figure 3.\nIt is noteworthy that PP starts on a high baseline score, but does not improve much from there: this is due to the fact that the learner very quickly masters a level, but then does not generalize to following levels. As a consequence, PP tends to get stuck and achieve relatively constant game scores in the first few levels.\nIn contrast, NPP learns to master easy levels equally fast, but then carries over to harder levels better, yielding higher total scores per attempt.\nNPPS shows a very similar slope to NPP, supporting the explanation that the sheer number of features slows the learning process down. However, NPPS is clearly outperformed by NPP within our simulation time frame. We are somewhat surprised at the complete failure of NPPO to improve over time."
    }, {
      "heading" : "3.2 RLSVI vs. regular Q-Learning",
      "text" : "Exploring the state-action space according to posterior beliefs did indeed produce better results than the ε-greedy Q-Learner. We trained both algorithms on the same feature extractor (NPP) and compared a moving average game score per attempt. RLSVI achieved higher scores overall in the simulated time frame, and learned more quickly. A comparison of the two algorithms is given in figure 4.\nIt should be noted that in our implementation, RLSVI required significantly more memory and computation than the regular Q-Learner, since it required keeping a complete history of observed features, and matrix operations on large data blocks."
    }, {
      "heading" : "3.3 Learning success",
      "text" : "Neither a human player -our oracle for score comparison- nor any of the algorithms managed to go through all 11 levels\nprovided by the game engine we used in one attempt. Both the regular Q-Learner and RLSVI however outperformed the human player (who shall remain unnamed, given that he was shamefully beat by crude algorithms) in terms of maximum points achieved in a single attempt, and in terms of the highest level reached. Table 1 summarizes the maximum score attained and the highest level reached for different players.\nComparing the number of attempts required to pass a given level provides interesting insights: especially in later levels, RLSVI’s exploration proves very efficient, as it passes the levels at almost the same number of attempts as required by the human player. With the same features, regular Q-Learning required 3 times as many attempts, as can be seen in table 2.\nModel Number of trials to finish:\nAlgorithm Features Level 0 Level 5 Level 7\nHuman —- 1 2 4 Q-Learning PP 2 5 10 Q-Learning NPP 1 4 15 Q-Learning NPPO 4 20 40 RLSVI NPP 1 2 5"
    }, {
      "heading" : "4. Conclusions and Future Work",
      "text" : "RLSVI’s Bayesian approach to state-action space exploration seems like a promising and rather intuitive way of navigating unknown systems. Previous work has shown Bayesian belief updating and sampling for exploration to be highly efficient [4], which our findings confirm. RLSVI beat both the baseline Q-Learning algorithm and, somewhat embarrassingly, our human oracle.\nA main constraint on our work was simulation speed, limiting the algorithm convergence we could achieve. We would have liked to further explore possible features, especially after more computation.\nIn the same vein, it would be interesting to train a deep neural network for the Q(s,a) function, to allow learning more complex interaction effects, in particular between targets and obstacles. Promising results combining Bayesian exploration with deep reinforcement learning have been shown in [5].\nFinally, it may be interesting to explore different distri-\nbution assumptions within RLSVI; assuming least-squares noise to be normally distributed, and therefore sampling from a normal distribution around expected optimal policies may be particularly prone to get stuck in local minima. Using bootstrapping methods in lieu of closed-form Bayesian updates may prove to be a powerful improvement on the RLSVI algorithm explored here.\nAppendix\nOur work can be found at github.com/imanolarrieta/angrybirds. Original codes for Angry Birds in Python can be found at https: //github.com/estevaofon/angry-birds-python."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We would like to thank the CS221 teaching team for an inspiring quarter."
    } ],
    "references" : [ {
      "title" : "Generalization and exploration via randomized value functions",
      "author" : [ "Ian Osband", "Benjamin Van Roy", "Zheng Wen" ],
      "venue" : "arXiv preprint arXiv:1402.0635,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2014
    }, {
      "title" : "Reinforcement learning: An introduction, volume 1",
      "author" : [ "Richard S Sutton", "Andrew G Barto" ],
      "venue" : "MIT press Cambridge,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1998
    }, {
      "title" : "more) efficient reinforcement learning via posterior sampling",
      "author" : [ "Ian Osband", "Dan Russo", "Benjamin Van Roy" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2013
    }, {
      "title" : "Bootstrapped thompson sampling and deep exploration",
      "author" : [ "Ian Osband", "Benjamin Van Roy" ],
      "venue" : "arXiv preprint arXiv:1507.00300,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2015
    }, {
      "title" : "Bayesian q-learning",
      "author" : [ "Richard Dearden", "Nir Friedman", "Stuart Russell" ],
      "venue" : "In AAAI/IAAI,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1998
    }, {
      "title" : "Near-optimal reinforcement learning in polynomial time",
      "author" : [ "Michael Kearns", "Satinder Singh" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2002
    }, {
      "title" : "Reinforcement learning: A survey",
      "author" : [ "Leslie Pack Kaelbling", "Michael L Littman", "Andrew W Moore" ],
      "venue" : "Journal of artificial intelligence research,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1996
    }, {
      "title" : "The elements of statistical learning, volume 1. Springer series in statistics",
      "author" : [ "Jerome Friedman", "Trevor Hastie", "Robert Tibshirani" ],
      "venue" : null,
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2001
    }, {
      "title" : "Playing atari with deep reinforcement learning",
      "author" : [ "Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Alex Graves", "Ioannis Antonoglou", "Daan Wierstra", "Martin Riedmiller" ],
      "venue" : "arXiv preprint arXiv:1312.5602,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2013
    }, {
      "title" : "Reinforcement learning-based multi-agent system for network traffic signal control",
      "author" : [ "Itamar Arel", "Cong Liu", "T Urbanik", "AG Kohls" ],
      "venue" : "Intelligent Transport Systems, IET,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "We implemented a variation which updates a belief distribution on optimal policies, and then samples policies from this distribution [2].",
      "startOffset" : 133,
      "endOffset" : 136
    }, {
      "referenceID" : 1,
      "context" : "As exposed in [3], Q-Learning is a model-free algorithm that approximates a state-action value function Qopt(s,a) in order to derive an optimal policy for an unknown decision process.",
      "startOffset" : 14,
      "endOffset" : 17
    }, {
      "referenceID" : 0,
      "context" : "However, when complex sets of actions need to be taken in order to get a higher reward, ε-greedy tends to take exponential time to find these rewards (see [2], [4], [5].",
      "startOffset" : 155,
      "endOffset" : 158
    }, {
      "referenceID" : 2,
      "context" : "However, when complex sets of actions need to be taken in order to get a higher reward, ε-greedy tends to take exponential time to find these rewards (see [2], [4], [5].",
      "startOffset" : 160,
      "endOffset" : 163
    }, {
      "referenceID" : 3,
      "context" : "However, when complex sets of actions need to be taken in order to get a higher reward, ε-greedy tends to take exponential time to find these rewards (see [2], [4], [5].",
      "startOffset" : 165,
      "endOffset" : 168
    }, {
      "referenceID" : 1,
      "context" : "This allows the exploitation step to be an update only on the weights vector w, which we performed in a fashion similar to stochastic gradient descent (following the standard algorithm from [3]).",
      "startOffset" : 190,
      "endOffset" : 193
    }, {
      "referenceID" : 0,
      "context" : "propose Randomized Least Squares Value Iteration (RLSVI) [2], an algorithm that differs from our implementation of Q-Learning in two points:",
      "startOffset" : 57,
      "endOffset" : 60
    }, {
      "referenceID" : 0,
      "context" : "Given a memory of (state, action, reward, new state) tuples, we can use Bayesian least squares to derive w̄t , the expected value of the optimal policy and Σt = Cov(wt), the covariance of the optimal policy (the details of which are fairly straight-forward, and can be found in [2]).",
      "startOffset" : 278,
      "endOffset" : 281
    }, {
      "referenceID" : 2,
      "context" : "Previous work has shown Bayesian belief updating and sampling for exploration to be highly efficient [4], which our findings confirm.",
      "startOffset" : 101,
      "endOffset" : 104
    }, {
      "referenceID" : 3,
      "context" : "Promising results combining Bayesian exploration with deep reinforcement learning have been shown in [5].",
      "startOffset" : 101,
      "endOffset" : 104
    } ],
    "year" : 2016,
    "abstractText" : "We train a reinforcement learner to play a simplified version of the game Angry Birds. The learner is provided with a game state in a manner similar to the output that could be produced by computer vision algorithms. We improve on the efficiency of regular ε-greedy Q-Learning with linear function approximation through more systematic exploration in Randomized Least Squares Value Iteration (RLSVI), an algorithm that samples its policy from a posterior distribution on optimal policies. With larger state-action spaces, efficient exploration becomes increasingly important, as evidenced by the faster learning in RLSVI.",
    "creator" : "LaTeX with hyperref package"
  }
}