{
  "name" : "1303.5750.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Fusion Algorithm for Solving Bayesian Decision Problems",
    "authors" : [ "Prakash P. Shenoy" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 INTRODUCTION The main goal of this paper is to describe a new method for solving Bayesian decision problems. The method con sists of representing a Bayesian decision problem as a val uation-based system and applying a fusion algorithm for solving it.\nValuation-based systems are described in Shenoy [1989, 1991c]. In valuation-based system representations of deci sion problems, we encode utility functions and probability distributions by functions called valuations. We solve valuation-based systems using two operations called com bination and marginalization. Solving can be described simply as marginalizing all variables out of the joint val uation. The joint valuation is the result of combining all valuations. The framework of valuation-based systems is powerful enough to include also probability theory [Shenoy, 1991c], Dempster- Sha.fer theory of belief func tions [Shenoy, 199lc], Spohn's theory of epistemic be liefs [Shenoy, 199la,c], possibility theory [Dubois and Prade, 1990], discrete optimization [Shenoy, 1991 b], propositional logic [Shenoy, l990a], and constraint satis faction problems [Shenoy and Shafer, 1988].\nThe fusion algorithm for solving valuation-based represen tations of decision problems is a hybrid of local computa tional methods for computation of marginals of joint probability distributions and local computational methods for discrete optimization. Local computational methods for computation of marginals of joint probability distribu tions have been proposed by, e.g., Pearl [1988], Lauritzen and Spiegelhalter [1988], Shafer and Shenoy [1988], and Jensen et al. [1990]. Local computational methods for discrete optimization are also called non-serial dynamic programming [Bertele and Brioschi, 1972]. Viewed ab-\nstractly using the framework of valuation-based systems, these two local computational methods are actually simi lar. Shenoy and Shafer [1990] and Shenoy [199lb] show that the same three axioms justify the use of local compu tation in both these cases.\nOur method for representing and solving decision prob lems has many similarities to influence diagram method ology [Howard and Matheson, 1984; Olmsted, 1983; Shachter, 1986; Ezawa, 1986; Tatman, 1986]. But there are also many differences both in representation and solu tion. A comparison of these two methods is given in [Shenoy, 199Gb).\nWe describe our new method using a diabetes diagnosis problem. Section 2 gives a statement of this problem. Section 3 describes a valuation-based representation of a decision problem. Section 4 describes the method for solving valuation-based systems. Section 5 describes a fusion algorithm for solving valuation-based systems us ing local computation. Finally, section 6 summarizes the paper.\n2 A DIABETES DIAGNOSIS PROBLEM\nA medical intern is trying to decide on a policy for treat ing patients suspected of suffering from diabetes. The in tern first observes whether a patient exhibits two symp toms of diabetes-blue toe and glucose in urine. After she observes the presence or absence of these symptoms, she then either prescribes a treatment for diabetes or doesn't.\n362 Shenoy\nLet Xo denote the set of all decision vari ables, let X R denote the set of all random variables, and let X= XouXR denote the set of all variables. We will often deal with non-empty subsets of variables in%. Given a non-empty subset h of X, let 'W h denote the Cartesian product of 'WxforX in h, i.e., 'Wh =x{'WxiXEh}. We can think of the set 'W h as the set of possible values of the joint variable h. Accordingly, we call 'W h the frame for h. Also, we refer to elements of 'W has con figurations of h. We use lower-case, bold-faced letters such as x, y, etc. to de note configurations. Also, if x is a con figuration of g and y is a configuration of h and gnh = 0, then (x,y) denotes a con-\nFi ure 1: A Valuation Network for the Diabetes Dia nosis Problem figuration of guh. L-������������������\nblue toe, and 1% exhibit glucose in urine. We assume that blue toe and glucose in urine are conditionally inde pendent given diabetes.\n3 VALUATION-BASED SYSTEM REPRESENTATION\nIn this section, we describe a valuation-based system (VBS) representation of a decision problem. A VBS rep resentation consists of decision variables, random vari ables, frames, a utility valuation, potentials, and prece dence constraints. A graphical depiction of a VBS is called a valuation network. Figure 1 shows a valuation network for the diabetes diagnosis problem.\nVariables, Frames and Configurations. A deci sion node is represented as a variable. The possible values of a decision variable represent the acts available at that point. We use the symbol 'Wo for the set of possible values of decision variable D. We assume that the deci sion-maker has to pick one and only one of the elements of 'WD as their decision. We call 'Wo the frame for D. Decision variables are represented in valuation networks by rectangular nodes.\nIn the diabetes diagnosis problem, there is one decision node T. The frame forT has two elements: Treat the pa tient for diabetes (t), and not treat ( -!). If R is a random variable, we use the symbol 'W R to de note its possible values. We assume that one and only one of the elements of 'W R can be the true value of R. We call 'WR the frame for R. Random variables are repre sented in valuation networks by circular nodes.\nIn the diabetes diagnosis problem, there are three random variables: Blue toe (B), Glucose in urine (G), and Diabetes (D). Each variable has a frame consisting of two elements.\nD\nd\n.-{}\np .I\n.9\nIt is convenient to extend this terminol ogy to the case where the set of variables h is empty. We adopt the convention that the frame for the empty set 0 consists of a single configuration, and we use the symbol + to name that configuration; 'W 0 = { +}. To be consis tent with our notation above, we adopt the convention that if x is a configuration for g, then (x. +) = x. Valuations. Suppose he;;;%. A utility valuation nfor h is a function from 'W h to lR, where lR denotes the set of real numbers. The values of utility valuations are utili ties. If h = dur where de;;;% D and rc;;;% R· xE 'W d· and yE 'W r• then n(x,y) denotes the utility to the decision maker if the decision maker chooses configuration x and the true configuration of r is y. If 1t is a utility valuation for h, and XE h, then we say that 11: bears on X. In a valuation network, a utility valuation is represented by a diamond-shaped node. To permit the identification of all valuations that bear on a variable, we draw undirected edges between the utility valuation node and all the vari able nodes it bears on. In the diabetes diagnosis problem, there is one utility valuation 1t as shown in Figure I.\nIn a valuation network, a potential is represented by a tri angular node. Again, to identify the variables related by a potential, we draw undirected edges between the potential node and all the variable nodes it bears on.\nIn the diabetes diagnosis problem, there are three paten-\nTable 2: Potentials p, �- and v\nB G\n� b -b v b -b d .014 .986 d .90 .10 D D .-{} .006 .994 -d .01 .99\nA Fusion Algorithm for Solving Bayesian Decision Problems 363\ntials jl , v , and p as shown in Figure I. Table 2 shows the details of these potentials. Note that ll is a potential for IB, D), vis a potential for IG, D }, and p is a potential for I D }.\nTable 3: Computation of jl®V®p and the Joint Valuation 7t®jl®v®p\nPrecedence Constraints. Besides acts, states, probabilities and utilities, an important ingredient of problems in decision analysis is in formation constraints. Some deci sions have to be made before the ob servation of some uncertain states, and some decisions can be postponed until after some states are observed. In the diabetes diagnosis problem, for example, the medical intern doesn't know whether the patient has dia betes or not. And the decision whether to treat the patient for dia betes or not may be postponed until after the observation of blue toe and glucose in urine.\nIf a decision-maker expects to be in formed of the true value of random variable R before they make a decision D, then we represent this situa-\n'UfiBGTDI b g t d\nb g t -d b g -t d\nb g -t -d b -g t d\nb -g t -d b -g -t d\nb -g -t -d -b g t d\n-b g t -d -b g -t d\n-b g -t -d -b -g t d\n-b -g t -d -b -g -t d\n-b -1! -t -d\ntion by the binary relation R�D (read as R precedes D). On the other hand, if a random variable R is only revealed after a decision D is made or perhaps never revealed, then we represent this situation by the binary relation D�R.\nIn the diabetes diagnosis problem, we have the precedence constraints B�T. G�T. T �D. The decision whether to treat the patient for diabetes or not (T) is only made after observing blue toe (B) and glucose in urine (G). And, di abetes (D) is not known at the time the decision whether to treat the patient for diabetes (T) has to be made.\nSuppose > is a binary relation on X such that it is the transitive closure of �. i.e., X> Y if either X� Y, or there exists a ZE X such that X> Z and Z > Y. First, we assume that > is a partial order on X (otherwise the deci sion problem is ill-defined and not solvable). Second, we require that this partial order > is such that for any DE X D and any RE XR. either D>R or R>D. We refer to this second condition as the peifect recall condition. The rea son for the perfect recall condition is as follows. Given the meaning of the precedence relation �, for any decision variable D and any random variable R, either R is known when decision D has to be made, or not. This translates to either R>D or D>R. (This condition is called \"no-for getting assumption\" in influence diagram literature.)\nNext, we will define two operations called combination and marginalization. We use these operations to solve the valuation-based system representation. First we start with some notation.\nProjection of Configurations. Projection of con figurations simply means dropping extra coordinates; if\n7t ll v p jl®V®p 7t®jl®V®p 10 .014 .90 .10 .00126 0.0126\n5 .006 .01 .90 .000 54 0.00027\n0 .014 .90 .10 .00126 0\n10 .006 .01 .90 .000 54 0.00054\n10 .014 . 10 .10 .00014 0.0014\n5 .006 . 99 .90 .005346 0.02673\n0 .014 .10 .10 .00014 0\n10 .006 .99 .90 .005346 0.05346\n10 .986 .90 .10 .08874 0.8874\n5 .994 .01 .90 .008946 0.04473\n0 .986 .90 .10 .08874 0\n10 .994 .01 .90 .008946 0.08646\n10 .986 .10 .10 .00986 0.0986\n5 .994 .99 .90 .885654 4.42827\n0 .986 .10 .10 .00986 0\n10 .994 .99 .90 .885 654 8.86554\n(w,x,y,z) is a configuration of IW,X,Y,Z}, for example, then the projection of (w,x,y,z) to IW,X} is simply (w,x), which is a configuration of IW.X}.\nIf g and h are sets of variables, h\\::g, and x is a configura tion of g, then we let xth denote the projection of x to h. The projection xth is always a configuration of h. If h=g and xis a configuration of g, then xth = x. If h=0, then xth = +. Combination. The definition of combination depends on the type of valuations being combined.\nSuppose h and g are subsets of X, suppose Pi is a poten tial for h, and suppose Pi is a potential for g. Then the combination of Pi and Pj. denoted by Pi®Pi· is a potential for hug obtained by pointwise multiplication of Pi and Pi· i.e., (pi® Pi)(x) = Pi(xth)Pi(x,l.g) for all xE 'Ufhug· See Table 3 for an example.\nSuppose h and g are subsets of X, suppose 1ti is a utility valuation for h, and suppose Pi is a potential for g. Then the combination of 1t; and Pj. denoted by 7ti®Pi· is a util ity valuation for hug obtained by pointwise multiplication of 1tj and Pi• i.e., (7ti®Pi)(x) = 7ti(xth) Pi(x,l.g) for all xE 'Uf hug. See Table 3 for an example. Note that combination is commutative and associative. Thus, if I a1, ... , ak} is a set of valuations, we write ®la1, ... , ak} to mean the combination of valuations in I a 1 , ... , ak I in some sequence.\n364 Shenoy\nMarginalization. Suppose h is a subset of variables and suppose a is a valuation for h. Marginalization is an opera tion where we reduce valua tion a to a valuation a!(h-{X}) for h-{X}.\na!(h-{X}) is called the marginal of a for h-{X). Unlike combination, the def inition of marginalization does not depend on the nature of a. But the definition of marginalization does depend on whether X is a decision or a random variable.\nIf R is a random variable, a!(h-{ R l) is obtained by summing a over the frame for R, i.e., a!(h-{Rll(c) =\n-b -g -t --<1\n8.86554 I.{ a(c,r)l rE 'WR} for all CE 'Uf h-{R}· Here, a could be either a utility valuation or a potential. See Table 4 . . (\"t denotes the JOint valuatiOn lt®IJ.®V®p) for an example.\nIf D is a decision variable, a!(h-{D}) is obtained by max imizing a over the frame forD, i.e., a!(h-{Dll(c) = MAX{a(c, d)ldE 'Ufo} foral cE 'Ufh-{D}· Here, a must be a utility valuation. See Table 4 for an example.\nWe now state three lemmas regarding the marginalization operation. Lemma 3.1 states that in marginalizing two decision variables out of a valuation, the order in which the variables are eliminated does not affect the result. Lemma 3.2 states a similar result for marginalizing two random variables out of a valuation. Lemma 3.3 states that in marginalizing a decision variable and a random variable out of a valuation, the order in which the two variables are eliminated may make a difference.\nLemma 3.1. Suppose h is a subset of X contain ing decision variables D1 and D2, and suppose a is a utility valuation for h. Then (a!(h-{ D1 ll)!(h-{D1 .D2 ll(c) = (a!(h-{D2ll)!(h-{D1.D2ll(c) for all CE 'Ufh-{01,02}·\nLemma 3.2. Suppose h is a subset of X contain ing random variables R1 and R2. and suppose a is a valuation for h. Then (a!(h-{R1 ll)!(h-{R1,R2ll(c) = (a!(h-{R2ll)!(h-{R1,R2ll(c) for all\nCE 'Uf h-{R1,R2}·\nLemma 3.3. Suppose h is a subset of X contain ing decision variable D and random variable R, and suppose a is a utility valuation for h. Then (a!(h-{D}))!(h-{R,D})(c);:::: (a!(h-{R}))!(h-{R,D})(c) for all CE 'Uf h-{R,D}·\nIt is clear from Lemma 3.3, that in marginalizing more than one variable, the order of elimination of the variables may make a difference. As we will see shortly, we need to marginalize all variables out of the joint valuation. What sequence should we use? This is where the prece dence constraints come into play. We define marginaliza tion such that variable Y is marginalized before X when ever X> Y. Here is a formal definition.\nSuppose h and g are non-empty subsets of X such that g is a proper subset of h, suppose a is a valuation for h, and suppose> is a partial order on X satisfying the per fect recall condition. The marginal of a for g with respect to the partial order >,denoted by a!g, is a valuation for g defined as follows: a!g = (((a!(h-{X 1l ))!(h-{X 1 ,X2l l) ... )!(h-{X 1 , X 2 · · · · ·Xkl) (3.1) where h-g = {X1 , ... , Xkl and X1X2 ... Xk is a sequence of variables in h-g such that with respect to the partial order>, X1 is a minimal element of h-g, X2 is a mini mal element of h-g-{X1}, etc.\nThe marginalization sequence X1X2 ... Xk may not be\nA Fusion Algorithm for Solving Bayesian Decision Problems 365\nunique since > is only a partial order. But, since > satis fies the perfect recall condition, it is clear from Lemmas 3.1 and 3.2, that the definition of a!g in (3.1) is well de fined. Strategy. The main objective in solving a decision problem is to compute an optimal strategy. What consti tutes a strategy? Intuitively, a strategy is a choice of an act for each decision variable D as a function of configura tions of random variables R such that R>D. Let Pr(D) = {RE XR I R>DI. We refer to Pr(D) as the predecessors of\nD. Thus a strategy a is a collection of functions { � oloE x0 where �0: 'Ui'Pr(D) � 'UI' D· Solution for a Variable. Computing an optimal strategy is a matter of bookkeeping. Each time we marginalize a decision variable out of a utility valuation using maximization, we store a table of optimal values of the decision variable where the maximums are achieved. We can think of this table as a function. We call this function \"a solution\" for the decision variable. Suppose h is a subset of variables such that decision variable DE h, and suppose 1t is a utility valuation for h. A function 'Po: 'UI' h-{D} � 'UI' o is called a solution forD (with respect ton) if n!(h-{D})(c) = n(c,'P(c)) for all cE 'UI' h-{D)· See Table 4 for an example.\n4 SOLVING A VBS SupposeD.= {Xo. XR. {'UI'xlxEX• {nj). IPI·····Pnl. -t 1 is a VBS representation of a decision problem consist ing of one utility valuation and n potentials. What do the potentials represent? And how do we solve D.? We will answer these two related questions in terms of a canonical decision problem.\nCanonical Decision Problem. A canonical decision problem � consists of a single decision variable D with a finite frame 'UI' o. a single random variable R with a fi nite frame 'UI'R, a single utility valuation 1t for {D,RI. a single potential p for { R, D I such that I:{p(d,r) IrE 'UI'RI = 1 for all dE 'UI' D• (4.1) and a precedence relation � defined by D�R.\nThe meaning of the canonical decision problem is as fol lows. The elements of 'UI'o are acts, and the elements of 'UI'R are states of nature. The potential pis a family of probability distributions for R, one for each act dE 'UI' D• i.e., I:{ p( d, r) I rE 'UI' R I = 1 for all dE 'UI' D· The utility valuation 1t is a utility function-if the deci sion maker chooses act d, and the state of nature r pre vails, then the utility to the decision maker is n(d,r). The precedence relation � states that the true state of nature is revealed to the decision maker only after the decision maker has chosen an act. Solving a canonical decision problem using the criterion of maximizing expected utility is easy. The expected util ity associated with act d is I:{(7t®p)(d,r) IrE 'UI'RI =\n(n®p)!{Dl(d). The maximum expected utility (associated with an optimal act, say d*) is MAX{ (7t®p)!{Dl(d) I dE 'UI' ol = ((7t®p)J,{D})J,0( +) = (7t®p )!0( + ). Finally, act d* is optimal if and only if (7t®p)J,{D}(d*) = (7t®p)J,0( + ). Consider the decision problem D. = { X o. X R, {'UI'xlxEX• {nj), {pJ, ... , Pnl. �1. We will explain the meaning of D. by reducing it to an equivalent canonical de cision problem � = { { D}. { R}. { 'UI' D• 'UI' R}. I 1t I, { p },� 1. To define�. we need to define 'UI' D• 'UI' R· 1t, and p. Define 'UI' 0 such that for each distinct strategy <J of ll, there is a corresponding act d0 in 'UI' D · Define 'UI' R such that for each distinct configuration y of X R in ll, there is a corresponding configuration r y in 'UI' R. Before we define utility valuation 1t for {D, RI, we need some notation. Suppose o = { � o loE x0 is a strategy, and suppose y is a configuration of X R. Then together o and y determine a unique configuration of Xo. Let a0,y denote this unique configuration of X0. By definition, a !{D) = �0(y!Pr(D)) for all DE x0. Consider the o,y utility valuation 1t 1 in ll. Assume that the domain of this valuation includes all of X0. Typically the domain of this valuation will include also some (or all) random variables. Let p denote the subset of random variables in cluded in the domain of the joint utility valuation, i.e., p<;;;XR such that n1 is a utility valuation for Xoup. Define utility valuation 1t for { D,R I such that n(d0,r yl = n1 ( acr,y.YJ,P), for all strategy <J of ll, and all configura tion yE 'UI' XR· Remember that a0,y is the unique con figuration of X0 determined by o andy.\nConsider the joint potential PI ® ... ®pn. Assume that this potential includes all random variables in its domain. Let q denote the subset of decision variables included in the domain of the joint potential, i.e., q<;;;Xo such that p1 ® ... ®pn is a potential for quXR. Note that q could be empty. Define potential p for { D ,R} such that p( d0,r ) = (p1® ... ®p n)(a0,y'J,q ,y) , for all strategy o.' and all c.onfiguration yE 'UI' XR· �. as defmed above, ts a canomcal decision problem only if p satisfies condition (4.1). This motivates the following definition. ll is a well-defined VBS representation of a decision problem if and only if I:{ (PJ ® ... ®pn)(x,y) I YE 'UI' XR I = I for every xE 'UI' q· In summary, the potentials I PI· ... , Pnl represent the fac tors of a family of probability distributions. It is easy to verify that the VBS representation of the diabetes diagno sis problem is well-defined since p®J.!®V is a joint prob ability distribution for { D, B, G I.\nThe Decision Problem. Suppose D.= {Xo. XR. { 'UI' X l XE x, { 7tJ\\. { p J, ...• Pn}, �I is a well-defined deci-\n366 Shenoy\nsion problem. Let�c= {{D), {R}, {'llfo. 'UfR}, {7tl, {pI, �I, represent an equivalent canonical decision prob lem. In the canonical decision problem �c. the two computations that are of interest are (I) the computation of the maximum expected value (7t®p )J,0( +),and (2) the computation of an optimal act dcr• such that (7t®p)J,{D)(dcr•) = (7t®p)J,0( + ). Since we know the mapping between � and �. we can now formally define the questions posed in a decision problem �. There are two computations of interest.\nFirst, we would like to compute the maximum expected utility. The maximum expected utility is given by (®{7t1, P1· ... , Pnl).j,0( + ). Second, we would like to compute an optimal strategy cr* that gives us the maximum expected value (®(1t1, P1· ... , PnDJ,0( + ). A strat egy cr* of� is optimal if (7t®p)J,IDI(dcr•) = (®(7t1, P1, ... , Pn I )J,0( + ), where 7t, p, and D refer to the equivalent canonical decision problem �C·\nIn the diabetes diagnosis problem, we have four valuations 1t, J..l, v, and p. Also, from the precedence constraints, we have B>T, G>T, T>D. Thus we need to compute either ((((7t®J..l®V®p)J,{B,G,T))J,{B,G I)J,{B I)J,0 or ((((7t®J..l®V®p)J,{B,G,T})J,IB,G I)J,{G I)J,0. In either case, we get the same answer.\nTables 3 and 4 display the former computations. As seen in Table 4, the maximum expected utility is 9.864. Also, from '�'T· the solution for T (shown in Table 4), the optimal act is to treat the patient for diabetes if and only if the patient exhibits glucose in urine.\nNote that no divisions were done in the solution process, only additions and multiplications. But, both decision tree and influence diagram methodologies involve unnec essary divisions, and unnecessary multiplications to com pensate for the unnecessary divisions. It is this feature of valuation-based systems that makes it more efficient than decision trees and influence diagrams.\nIn solving the diabetes diagnosis problem using our method, we do only II additions, 28 multiplications and 4 comparisons, for a total of 43 operations. On the other hand, both decision tree and influence diagram methodolo gies require 1 7 additions, 38 multiplications, 12 divisions, and 4 comparisons for a total of 71 operations. Thus, for this problem, our method results in a savings of 40 per cent over the decision tree and influence diagram method ologies.\n5 A FUSION ALGORITHM In this section, we describe a method for solving a VBS using local computation. The solution for the diabetes di agnosis problem shown in Tables 3 and 4 involves com bination on the space 'Uf%. While this is possible for\nsmall problems, it is computationally not feasible for problems with many variables. Given the structure of the diabetes diagnosis problem, it is not possible to avoid the combination operation on the space of all four variables, B, G, T, and D. But, in some problems, it may be possi ble to avoid such global computations.\nThe basic idea of the method is to successively delete all variables from the VBS. The sequence in which variables are deleted must respect the precedence constraints in the sense that if X> Y, then Y must be deleted before X. Since > is only a partial order, a problem may allow sev eral deletion sequences. Any allowable deletion sequence may be used. All allowable deletion sequences lead to the same answers. But, different deletion sequences may in volve different computational costs. We will comment on good deletion sequences at the end of this section.\nWhen we delete a variable, we have to do a \"fusion\" opera tion on the valuations. Consider a set of k valuations a1, ... , ak. Suppose ai is a valuation for hi. Let Fusx { a1, ... , ak I denote the collection of valuations after fusing the valuations in the set {a 1, ... , ak I with respect to variable X. Then\nJ,(h-{ X ) ) Fusx{a1, ... ,ak}={a lv{aiiX�hj}, (5.1) where a= ® {ai I XE hi}, and h = v{hi I XE hj}. After fusion, the set of valuations is changed as follows. All valuations that bear on X are combined, and the resulting valuation is marginalized such that X is eliminated from its domain. The valuations that do not bear on X remain unchanged.\nWe are ready to state the main theorem.\nTheorem 1. Suppose�= {XD, XR, {'llfxlxEX• { 1t1 I, { p 1, ... , Pn}, �} is a well-defined decision problem. Suppose X 1 Xz ... Xk is a sequence of vari ables in X=XovXR such that with respect to the partial order >, X1 is a minimal element of X , Xz is a minimal element of X-{X 1 }, etc. Then { ( ® { 1t1, P1· ... , PnD 0 1 =\nA Fusion Algorithm for Solving Bayesian Decision Problems 367\nfor h;. and 1tj is a utility valuation for hj, then 1t;®1tj is a utility valuation for h;uhj defined by (1ti®1tj)(x) = n;(x J..hi)nj(x J..hj) for all xE 'W h·uh·· This method does not apply di-I J\nFi ure 2: A Valuation Network for a Medical Dia nosis Problem.\nrectly in problems where the utility valuation decomposes additively. In such problems, we first have to combine all utility valuations be fore we apply the method described in this sec tion. Thus the fusion method described in this section is unable to take computational advan tage of an additive decomposition of the utility valuation. In Shenoy [ 1990b], we describe a modification of the fusion method that is able to take advantage of an additive decomposition of the utility function. The modification involves some divisions.\nproblem. The deletion sequence used is OPTS. The first network in Figure 3 is the same as the one in Figure 2.\nThe second network is the result after deletion of D and the resulting fusion. The combination in the fusion oper ation involves only variables D, P, and T. The third net work is the result after deletion of P. The combination operation in the corresponding fusion operation involves only three variables, P, T, and S. The fourth network is the result after deletion ofT. There is no combination in volved here, only marginalization on the frame of { S, T}.\nThe fifth network is the result after deletion of S. Again, there is no combination involved here, only marginaliza tion on the frame of { S}. The maximum expected utility\nvalue is given by ((n®v®p )J..{T,P}®Jl)J..0( + ) . An opti mal strategy is given by the solution forT with respect to ((n®v®p )J..{T,P}®Jl)J.,{ S,T}, computed during fusion with respect toT. Note that in this problem, the fusion algorithm avoids computation on the frame of all four variables.\nIn solving the medical diagnosis problem using our method, we do only 9 additions, 2 0 multiplications and 2 comparisons, for a total of 31 operations. On the other hand, for this problem, decision tree methodology requires 2 3 additions, 42 multiplications, 12 divisions, and 2 comparisons for a total of 79 operations. Thus, for this problem, our method results in a savings of 61 percent over the decision tree methodology. If we use the influ ence diagram methodology for this problem, we do 13 ad ditions, 2 6 multiplications, 8 divisions, and 2 compar isons, for a total of 49 operations. Thus, for this prob lem, our method results in a savings of 37 percent over the influence diagram methodology.\nThe fusion method described in this section applies when there is one utility valuation in the VBS. This method applies unchanged in problems where the utility valuation factors multiplicatively into several utility valuations. In this case, we define combination of utility valuations as pointwise multiplication, i.e., if 1ti is a utility valuation\nDeletion Sequences. Since > is only a par tial order, in general, we may have many deletion se quences (sequences that satisfy the condition stated in Theorem 1). If so, which deletion sequence should one use? First, we note that all deletion sequences lead to the same final result. This is implied in the statement of the theorem. Second, different deletion sequences may in volve different computational efforts. For example, con sider the VBS shown in Figure 3. In this example, dele tion sequence OPTS involves less computational effort than POTS as the former involves combinations on the frame of three variables only whereas the latter involves combination on the frame of all four variables. Finding an optimal deletion sequence is a secondary optimization problem that has shown to be NP-complete [Am borg et al., 1987]. But, there are several heuristics for finding good deletion sequences [Kong, 1986; Mellouli, 1987; Zhang, 1988].\nOne such heuristic is called one-step-look-ahead [Kong, 1986]. This heuristic tells us which variable to delete next from amongst those that qualify. As per this heuris tic, the variable that should be deleted next is one that leads to combination over the smallest frame. For exam ple, for the VBS in Figure 5, two variables qualify for first deletion, P and D. This heuristic picks D over P since deletion of P involves combination over the frame of { S, D, P, T} whereas deletion of D involves combina tion over the frame of {T, P, D). Thus, this heuristic would choose deletion sequence DPTS.\n6 CONCLUSIONS\nThe main objective of this paper is to propose a new method for solving Bayesian decision problems. The VBS representation and solution described here is a hybrid of valuations-based systems for probability propagation [Shenoy, 1991c] and valuation-based systems for opti mization [Shenoy. 199lb].\nThere are several advantages of the VBS representation and solution methodology. First, like influence diagrams, a\n368 Shenoy\n2.\n3.\n1.\n4.\ns.\nsion problems, how ever, decision trees may be more efficient than VBSs.\nFourth, the VBS rep resentation is more powerful than influ ence diagram repre sentation. Whereas influence diagram rep resentation is only capable of directly representing condi tional probabilities, VBS representation is capable of directly representing arbitrary probabilities. (By di rectly, we mean without any prepro cessing.)\nFifth, the solution method of VBSs in volves minimal divi sions. In compari son, the influence dia gram solution method involves unnecessary divisions (in every arc reversal operation) and additional multi plications to compen sate for the unneces-\nFi ore 3: The Fusion AI orithm for the Medical Dia nosis Problem. sary divisions. These unnecessary divisions\nvaluation network representation is compact when com pared to decision trees. A valuation network graphically depicts the qualitative structure of the decision problem and de-emphasizes the quantitative details of the problem. However, both VBSs and influence diagrams are appropri ate only for symmetric decision problems. For non symmetric decision problems, decision tree representation is more flexible.\nSecond, like influence diagrams, the VBS representation separates the formulation of the problem from its solu tion.\nThird, in symmetric decision problems, the solution pro cedure of VBSs is more efficient than that of decision trees since it involves minimal divisions. This assumes that the computational procedure of decision trees includes the preprocessing of probabilities. The solution procedure of decision trees includes unnecessary divisions and multipli cations. The unnecessary divisions take place during pre processing of probabilities. The unnecessary multiplica tions make up for the unnecessary divisions and take place during the averaging-out process. In non-symmetric deci-\nand multiplications are the same as those in the decision tree solution process. In influence diagrams, these unnecessary operations are performed for semantical considerations. The influence di agram solution process has the property that the diagram resulting from the deletion of a chance node is again an in fluence diagram. This means that the resulting probabili ties in the reduced influence diagram are conditional prob abilities. It is this demand for conditional probabilities at each stage that results in the unnecessary divisions and multiplications.\nSixth, the semantics of VBSs are different from the se mantics of influence diagrams. Whereas influence dia grams are based on the semantics of conditional indepen dence, VBSs are based on the semantics of factorization.\nSeventh, if a decision problem has no random variables, it reduces to an optimization problem. And the solution technique of VBSs reduces to dynamic programming [Shenoy, 199lb].\nEighth, in cases where a decision problem has no decision variables, we may be interested in finding marginals of the\nA Fusion Algorithm for Solving Bayesian Decision Problems 369\njoint distribution for ea�h random. vari�ble .. In such prob lems, the solution techmque descnbed m this paper re duces to the technique for finding marginals [Shenoy, l99 lc]. This technique also can revise marginals in light of new observations. We represent each new observation by a potential and then use the fusion algorithm to com pute the desired marginals.\nAcknowledgements\nThis research was partially supported by NSF grant IRI8902444. I have benefitted from discussions with and comments from Dan Geiger, Steffen Lauritzen, Anthony Neugebauer, Pierre Ndilikilikesha, Geoff Schemmel, Glenn Shafer, Philippe Smets, Leen-Kiat Soh, Po-Lung Yu, and Lianwen Zhang.\nReferences\nArnborg, S., D. G. Comeil, and A. Proskurowski (1987), \"Complexity of finding em beddings in a k-tree,\" SIAM Journal of Algebraic and Discrete MethDds, 8, 277-284.\nBertele, U. and F. Brioschi (1972), Nonserial Dynamic Programming, Academic Press, New York, NY.\nDubois, D. and H. Prade (1990), \"Inference in possibilis tic hypergraphs,\" Proceedings of the Third International Conference on ltiformation Processing and Management of Uncertainty in Knowledge-based Systems (IPMU90 ), Paris, France, 228-230.\nEzawa, K. J. (1986), \"Efficient evaluation of influence diagrams,\" Ph.D. dissertation, Department of . . Engineering-Economic Systems, Stanford Umvers1ty.\nHoward, R. A. and J. E. Matheson (1984), \"Influence di agrams,\" in Howard, R. A. and J. E. Matheson (eds.), The Principles and Applications of Decision Analysis, 2, 719-762, Strategic Decisions Group, Menlo Park, CA.\nJensen, F. V .. K. G. Olesen, and S. K. Andersen (1990), \"An algebra of Bayesian belief universes for knowledge based systems,\" Networks, 20, 637-659.\nKong, A. ( 1986), \"Multivariate belief functions and graphical models,\" Ph.D. dissertation, Department of Statistics, Harvard University, Cambridge, MA.\nLauritzen, S. L. and D. J. Spiegelhalter ( 1988), \"Local computations with probabilities on graphical structures and their application to expert systems (with discus sion),\" Journal of the Royal Statistical Society, series B. 50(2), 157-224.\nMellouli, K. ( 1987), \"On the propagation of beliefs in networks using the Dempster-Shafer theory of evi dence,\" Ph.D. dissertation, School of Business, University of Kansas, Lawrence, KS.\nOlmsted, S.M. (1983), \"On representing and solving de cision problems,\" Ph.D. dissertation, Department of Engineering-Economic Systems, Stanford University.\nPearl, J. (1988), Probabilistic Reasoning in Intelligent Systems, Morgan Kaufmann, San Mateo, CA.\nShachter, R. D. (1986), \"Evaluating influence diagrams,\" Operations Research, 34(6), 871-882.\nShafer, G. and P. P. Shenoy (1988), \"Local computation in hypertrees,\" Working Paper No. 202, School of Business, University of Kansas, Lawrence, KS. To ap pear as a monograph in Lecture Notes in Artificial Intelligence series, Springer-Verlag, 1991.\nShenoy, P. P. (1989), \"A valuation-based language for expert systems,\" International Journal of Approximate Reasoning, 3(5), 383-411.\nShenoy, P. P. (1990a), \"Valuation-based systems for propositional logic,\" in Ras, Z. W., M. Zemankova, and M. L. Emrich (eds.), MethDdologies for Intelligent Systems, 5, 305-312, North-Holland, Amsterdam.\nShenoy, P. P. (1990b), \"Valuation-based systems for Bayesian decision analysis,\" Working Paper No. 220, School of Business, University of Kansas, Lawrence, KS.\nShenoy, P. P. (1990c), \"A new method for representing and solving Bayesian decision problems,\" Working Paper No. 223, School of Business, University of Kansas, Lawrence, KS.\nShenoy, P. P. (199 la), \"On Spohn's rule for revision of beliefs,\" International Journal of Approxinwte Reasoning, 5(2), 149-181.\nShenoy, P. P. (199 lb), \"Valuation-based systems fo� dis crete optimization,\" in Bonissone, P. P., M. Hennon, L. N. Kana!, and J. F. Lemmer (eds.), Uncertainty in Artificial Intelligence, 6, North-Holland, Amsterdam, to appear.\nShenoy, P. P. (199lc), \"Valuation-based systems: A framework for managing uncertainty in expert systems,\" Working Paper No. 226, School of Business, University of Kansas, Lawrence, KS.\nShenoy, P. P. and G. Shafer (1988), \"Constraint propaga tion,\" Working Paper No. 208, School of Business, University of Kansas, Lawrence, KS.\nShenoy, P. P. and G. Shafer (1990), \"Axioms for proba bility and belief-function propagation,\" in R. D. Shachter, T. S. Levitt, J. F. Lemmer and L. N. Kana! (eds.), Uncertainty in Artificial Intelligence, 4, 169- 198, North-Holland, Amsterdam.\nTatman, J. A. (1986), \"Decision processes in influence di agrams: Formulation and analysis,\" Ph.D. dissertation, Department of Engineering-Economic Systems, Stanford University.\nZhang, L. (1988), \"Studies on finding hypertree covers of hypergraphs,\" Working Paper No. 198, School of Business, University of Kansas, Lawrence, KS."
    } ],
    "references" : [ {
      "title" : "Complexity of finding em beddings in a k-tree,",
      "author" : [ "S. Arnborg", "D.G. Comeil", "A. Proskurowski" ],
      "venue" : "SIAM Journal of Algebraic and",
      "citeRegEx" : "Arnborg et al\\.,? \\Q1987\\E",
      "shortCiteRegEx" : "Arnborg et al\\.",
      "year" : 1987
    }, {
      "title" : "Inference in possibilis­ tic hypergraphs,",
      "author" : [ "D. Dubois", "H. Prade" ],
      "venue" : "Proceedings of the Third International Conference on ltiformation Processing and Management of Uncertainty in Knowledge-based Systems (IPMU90 ),",
      "citeRegEx" : "Dubois and Prade,? \\Q1990\\E",
      "shortCiteRegEx" : "Dubois and Prade",
      "year" : 1990
    }, {
      "title" : "Efficient evaluation of influence diagrams,",
      "author" : [ ],
      "venue" : "Ph.D. dissertation,",
      "citeRegEx" : ".1986,? \\Q1986\\E",
      "shortCiteRegEx" : ".1986",
      "year" : 1986
    }, {
      "title" : "Influence di­ agrams,",
      "author" : [ "J.E. Matheson" ],
      "venue" : null,
      "citeRegEx" : "Matheson,? \\Q1984\\E",
      "shortCiteRegEx" : "Matheson",
      "year" : 1984
    }, {
      "title" : "Local computations with probabilities on graphical structures and their application to expert systems (with discus­ sion),",
      "author" : [ "S.L. Lauritzen", "D.J. Spiegelhalter" ],
      "venue" : "Journal of the Royal Statistical Society,",
      "citeRegEx" : "Lauritzen and Spiegelhalter,? \\Q1988\\E",
      "shortCiteRegEx" : "Lauritzen and Spiegelhalter",
      "year" : 1988
    }, {
      "title" : "Evaluating influence diagrams,",
      "author" : [ "Morgan Kaufmann", "San Mateo", "R.D. CA. Shachter" ],
      "venue" : null,
      "citeRegEx" : "Kaufmann et al\\.,? \\Q1986\\E",
      "shortCiteRegEx" : "Kaufmann et al\\.",
      "year" : 1986
    }, {
      "title" : "Local computation in hypertrees,\" Working Paper No. 202, School of Business, University of Kansas, Lawrence, KS",
      "author" : [ "G. Shafer", "P.P. Shenoy" ],
      "venue" : null,
      "citeRegEx" : "Shafer and Shenoy,? \\Q1988\\E",
      "shortCiteRegEx" : "Shafer and Shenoy",
      "year" : 1988
    }, {
      "title" : "Valuation-based systems for propositional logic,\" in Ras",
      "author" : [ "P.P. Shenoy" ],
      "venue" : "MethDdologies for Intelligent Systems,",
      "citeRegEx" : "Shenoy,? \\Q1990\\E",
      "shortCiteRegEx" : "Shenoy",
      "year" : 1990
    }, {
      "title" : "Valuation-based systems for Bayesian decision analysis,",
      "author" : [ "P.P. Shenoy" ],
      "venue" : "Working Paper No. 220, School of Business,",
      "citeRegEx" : "Shenoy,? \\Q1990\\E",
      "shortCiteRegEx" : "Shenoy",
      "year" : 1990
    }, {
      "title" : "A new method for representing and solving Bayesian decision problems,",
      "author" : [ "P.P. Shenoy" ],
      "venue" : "Working Paper No. 223, School of Business,",
      "citeRegEx" : "Shenoy,? \\Q1990\\E",
      "shortCiteRegEx" : "Shenoy",
      "year" : 1990
    }, {
      "title" : "Constraint propaga­ tion,",
      "author" : [ "P.P. Shenoy", "G. Shafer" ],
      "venue" : "Working Paper No. 208, School of Business,",
      "citeRegEx" : "Shenoy and Shafer,? \\Q1988\\E",
      "shortCiteRegEx" : "Shenoy and Shafer",
      "year" : 1988
    }, {
      "title" : "Axioms for proba­ bility and belief-function propagation,",
      "author" : [ "P.P. Shenoy", "G. Shafer" ],
      "venue" : null,
      "citeRegEx" : "Shenoy and Shafer,? \\Q1990\\E",
      "shortCiteRegEx" : "Shenoy and Shafer",
      "year" : 1990
    }, {
      "title" : "Decision processes in influence di­ agrams: Formulation and analysis,",
      "author" : [ "A. J" ],
      "venue" : "Ph.D. dissertation,",
      "citeRegEx" : "J.,? \\Q1986\\E",
      "shortCiteRegEx" : "J.",
      "year" : 1986
    }, {
      "title" : "Studies on finding hypertree covers of hypergraphs,",
      "author" : [ "L. Zhang" ],
      "venue" : "Working Paper No. 198, School of Business,",
      "citeRegEx" : "Zhang,? \\Q1988\\E",
      "shortCiteRegEx" : "Zhang",
      "year" : 1988
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "fer theory of belief func­ tions [Shenoy, 199lc], Spohn's theory of epistemic be­ liefs [Shenoy, 199la,c], possibility theory [Dubois and Prade, 1990], discrete optimization [Shenoy, 1991 b], propositional logic [Shenoy, l990a], and constraint satis­ faction problems [Shenoy and Shafer, 1988].",
      "startOffset" : 126,
      "endOffset" : 150
    }, {
      "referenceID" : 10,
      "context" : "fer theory of belief func­ tions [Shenoy, 199lc], Spohn's theory of epistemic be­ liefs [Shenoy, 199la,c], possibility theory [Dubois and Prade, 1990], discrete optimization [Shenoy, 1991 b], propositional logic [Shenoy, l990a], and constraint satis­ faction problems [Shenoy and Shafer, 1988].",
      "startOffset" : 268,
      "endOffset" : 293
    }, {
      "referenceID" : 4,
      "context" : ", Pearl [1988], Lauritzen and Spiegelhalter [1988], Shafer and Shenoy [1988], and Jensen et al.",
      "startOffset" : 16,
      "endOffset" : 51
    }, {
      "referenceID" : 4,
      "context" : ", Pearl [1988], Lauritzen and Spiegelhalter [1988], Shafer and Shenoy [1988], and Jensen et al.",
      "startOffset" : 16,
      "endOffset" : 77
    }, {
      "referenceID" : 4,
      "context" : ", Pearl [1988], Lauritzen and Spiegelhalter [1988], Shafer and Shenoy [1988], and Jensen et al. [1990]. Local computational methods for discrete optimization are also called non-serial dynamic programming [Bertele and Brioschi, 1972].",
      "startOffset" : 16,
      "endOffset" : 103
    }, {
      "referenceID" : 4,
      "context" : ", Pearl [1988], Lauritzen and Spiegelhalter [1988], Shafer and Shenoy [1988], and Jensen et al. [1990]. Local computational methods for discrete optimization are also called non-serial dynamic programming [Bertele and Brioschi, 1972]. Viewed abstractly using the framework of valuation-based systems, these two local computational methods are actually simi­ lar. Shenoy and Shafer [1990] and Shenoy [199lb] show that the same three axioms justify the use of local compu­ tation in both these cases.",
      "startOffset" : 16,
      "endOffset" : 388
    }, {
      "referenceID" : 13,
      "context" : "But, there are several heuristics for finding good deletion sequences [Kong, 1986; Mellouli, 1987; Zhang, 1988].",
      "startOffset" : 70,
      "endOffset" : 111
    } ],
    "year" : 2011,
    "abstractText" : "This paper proposes a new method for solving Bayesian decision problems. The method con­ sists of representing a Bayesian decision problem as a valuation-based system and applying a fu­ sion algorithm for solving it. The fusion algo­ rithm is a hybrid of local computational methods for computation of marginals of joint probability distributions and the local computational meth­ ods for discrete optimization problems.",
    "creator" : "pdftk 1.41 - www.pdftk.com"
  }
}