{
  "name" : "1301.6683.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Discovering the Hidden Structure of Complex Dynamic Systems",
    "authors" : [ "Xavier Boyen", "Nir Friedman", "Daphne Koller" ],
    "emails" : [ "xb@cs.stanford.", "nir@cs.huji.", "koller@cs." ],
    "sections" : [ {
      "heading" : null,
      "text" : "--;\nDynamic Bayesian networks provide a compact and natural representation for complex dynamic systems. However, in many cases, there is no ex pert available from whom a model can be elicited. Learning provides an alternative approach for constructing models of dynamic systems. In this paper, we address some of the crucial compu tational aspects of learning the structure of dy namic systems, particularly those where some relevant variables are partially observed or even entirely unknown. Our approach is based on the Structural Expectation Maximization (SEM) al gorithm. The main computational cost of the SEM algorithm is the gathering of expected suf ficient statistics. We propose a novel approxima tion scheme that allows these sufficient statistics to be computed efficiently. We also investigate the fundamental problem of discovering the exis tence of hidden variables without exhaustive and expensive search. Our approach is based on the observation that, in dynamic systems, ignoring a hidden variable typically results in a violation of the Markov property. Thus, our algorithm searches for such violations in the data, and in troduces hidden variables to explain them. We provide empirical results showing that the algo rithm is able to learn the dynamics of complex systems in a computationally tractable way.\n1 Introduction\nMany real world phenomena are naturally modeled as dynamic systems: the stock market, measurements of a patient's vital signs in an intensive care unit, vehicles on a freeway, etc. Knowledge of a system's dynamics is essential for many tasks, including prediction, mon itoring, and the detection of anomalies.\nReal world complex systems are typically highly struc tured, consisting of several interacting entities. Dy namic Bayesian networks (DENs) provide a represen tational language for modeling such structured sys tems. By representing a system's state via several variables, and describing the relations between them, a DBN can capture the underlying structure of the system dynamics, e.g., which stocks depend on which other and on what external variables. Compared to less structured representations such as hidden Markov models (HMMs), DBNs support effective approximate inference [Boyen and Koller 1998b] and feature more robust learning due to their reduced number of param eters.\nRecent work has made significant progress on the prob lem of learning Bayesian networks from data (see, for example, [Heckerman 1999]. These ideas have re cently been applied to the problem of learning DBNs in the presence of partially observable data [Friedman et al. 1998], using the Structurol EM (SEM) algo rithm [Friedman 1997]. The basic outline of SEM is as follows: Each iteration starts with the current can didate DBN. In the E�step, the missing data is com pleted using the expected value relative to the current DBN, and the completed data is used to gather ex pected sufficient statistics - the completed \"counts\" corresponding to various events. In the M-step, These statistics are then used to score a variety of new can didate structures, and the best scoring candidate is selected. After one or more structural changes, the completed data and statistics are recomputed.\nThe methods described by Friedman et al., suffer from two major shortcomings. First, their approach has significant computational cost when applied to com plex processes such as stock market data. Second, as this algorithm does not change the variables in the network, hidden variables must be prespecified by the user. In this paper, we outline these limitations and suggest methods that address both issues.\nThe computational cost of the SEM approach is due\n92 Boyen, Friedman, and Koller\nto the complexity of the E-step. The E-step uses in ference to complete the missing data; the inference process propagates messages which are explicit distri butions over the set of state variables, so that their representation is exponential in the size of this set. In particular, if a join tree algorithm (Jensen et a!. 1990; Shenoy and Shafer 1990] is used for inference, the tree essentially reduces to one huge clique for each pair of consecutive time slices. This exponential cost is pro hibitive in all but the simplest DBNs.\nThis problem also appears in the simpler case of learn ing DBN parameters given the structure. For that setting, Boyen and Koller (1998a] propose an approxi mate E-step algorithm. This algorithm propagates ap proximate messages represented as factorized products over independent clusters. This representation allows the propagation of messages from one time slice to the other using a join tree with much smaller cliques than in the exact method. They show that the error in suf ficient statistics resulting from approximation is small, and that the influence on the progress of the learning algorithm is negligible. Even for small networks, order of magnitude speedups can easily be obtained.\nIn this paper, we extend this technique to the prob lem of structure learning. In parametric EM, as used in [Boyen and Koller 1998a], each family in the DBN is guaranteed to be contained in some clique in the clique tree for the two consecutive time slices. Thus, sufficient statistics could easily be accumulated dur ing the execution of the inference algorithm. In SEM, on the other hand, the results of the same inference process must be used for scoring a variety of different candidate structures. Each of these requires sufficient statistics for a different set of events. While infer ence algorithms can in principle be used to compute the probability of any event, this procedure is fairly expensive, especially for a large number of arbitrary events.\nIn Section 4, we propose a new approximation that circumvents this bottleneck. Roughly speaking, this approximation estimates the posterior probability of the event for which we want statistics by a product of small factors. As we show, this estimate is quite good, and can be computed efficiently as a by-product of the inference algorithm.\nOur final contribution addresses a fundamental prob lem in learning models for dynamic systems. In order to learn models that are statistically robust and com putationally tractable, we must often introduce hid den variables into our structure. These variables serve many roles: enabling the Markov property, capturing hidden influences of the observables, etc. It is possi ble, in theory, to discover hidden variables simply by\nintroducing them into the model and hoping that the search algorithm learns their meaning automatically (as in (Friedman et a!. 1998]).\nWe propose a technique that allows for a more targeted search for hidden variables. Our approach is based on the observation that, in DBNs, ignoring a hidden variable typically results in a violation of the Markov property. For example, in a DBN for tracking traf fic, eliminating the velocity variable would result in a non-Markovian dependence of the current location on the location at the two previous time slices. Our al gorithm exploits this property by explicitly searching for violations of the Markov property. For example, a significant correlation between a variable A(t+I) and some other earlier variable B(t-d) (given the Parents of A (t+I)) indicates that A must have additional in fluences not present in our model. In the general case there will be an additional hidden process that influ ences both A and B, and possibly other variables as well. We thus add an extra variable in our model to account for this hidden process.\nWe believe that our algorithmic ideas combine to pro vide a viable solution to the problem of learning com plex dynamic systems from data. We provide some empirical results on real and synthetic data that, while very preliminary, are quite promising in their ability to scale up to larger systems.\n2 Preliminaries: Learning DBNs\n2.1 Dynamic Bayesian Networks\nA dynamic Bayesian network (DEN) is a model of the evolution of a stochastic system over time. We choose to model the system evolution using a sequence of dis crete time points at regular intervals. At each point in time, the instantaneous state of a process is speci fied in terms of a set of attributes X1, . . . , Xn. We use x?) to denote the random variable corresponding to the attribute X; at time t. A DBN represents a distribution over (possibly infi nite) trajectories of the system. In order to allow such a distribution to be represented, we usually make two assumptions. The Markov assumption states that the future is independent of the past given the present. More precisely, for all t, we have the conditional inde pendence I(x<t+I); {X(0), . .. x(t-!)} I x<t)). This as sumption allows us to reduce the representation prob lem to that of specifying P(X(O)) and P(X(t+I) I x<t)) for all t. The second assumption is that the process is stationary (or time-invariant), i.e., that P(X<t+I) I x(t)) is the same for all t. Given these assumptions, we can specify the proba-\nDiscovering the Hidden Structure of Complex Dynamic Systems 93\nbilistic model of a OBN using two Bayesian network (BN) fragments. The first is a prior network Bo that specifies a distribution over initial states X(o). The second is a transition network B_,, which represents the transition probability from states X(t) to states X(t+I). The transition network is a BN fragment over the nodes {X1, . . . ,Xn,Xi, ... ,X�}. A node X; rep resents x?) and x; represents X;(t+I). The nodes X; in the network are forced to be roots (i.e. , have no par ents), and are not associated with conditional proba bility distributions. We denote the parents of X! in the graph by Pa(X;). Each node x; is associated with a conditional probability distribution (CPO), which specifies P (X! I Pa(X;)). The transition probability from one state x to another x' -P (x' I x ) - is then defined to be fl; P (x; I u;) , where u; is the value in x, x' of the variables in Pa(X;).\nA OBN defines a distribution over infinite trajectories of states. In practice, we reason only about a finite time interval 0, . . . , T. To do this reasoning, we can notionally \"unroll\" the OBN structure into a long BN over X(O), .. . , X(T). In time slice 0, the parents of xf0) and its CPO are those specified in the prior network B0; in slice t+ 1, the parents of xJt+I) and its CPO are those specified for X! in B_,. Thus, given a OBN B = (Eo, B_, ), the joint distribution over X(o), ... , X(T) is\nPs(x(o), ... , x(T))\nT-! = Ps0 (x(0)) II Ps� (X'= x(t+I) I X= x(t) )\nt=O\n2.2 Learning DBNs: Complete Data\nWe now consider the task of learning a OBN from data. For simplicity of notation, we assume that our data D is a single trajectory d(0), .. . , d(T) through the system; in this section, we assume that this trajectory is fully observable. Also for simplicity, we will ignore the task of learning the prior network Eo.\nFriedman et al. [1998] provide a detailed description of how BN learning can be applied to OBNs. We briefly highlight the most relevant parts of their anal ysis. Given a training sequence, the learning task is to find the network B_, that \"best matches\" D. The no tion of best match is defined using a scoring function. Several different scoring functions have been proposed in the literature. The most frequently used are the Bayesian Information Criterion {BIG) [Schwarz 1978] and the Bayesian scores, such as the BOe score of [Heckerman et a!. 1995]. Both of these scores combine \"fit to data\" with some penalty relating to the com plexity of the network. For ease of presentation, we will focus on the BIC score.\nIn both these scores, the term that represents \"fit to\ndata\" is the log-likelihood function, defined as l(B : D)= logP(D I B). This function measures the extent to which the data is likely given a candidate model B; it is thus an estimate of how well a given candidate fits the empirical data.\nThe log-likelihood depends on the sufficient statistics that summarize the frequencies of the relevant events in the data. For any event y over X, X', we define\nNy = L t(y(t) I D) t\nwhere t(y(t) I D) is an indicator function which takes on value 1 if the event y over X, X' holds for the in stantiation X = d(t) and X' = d(t+I), and 0 otherwise. The log-likelihood can now be described as:\nl(B_, :D) (1) = L L L Nx:,ulogBx:lu\ni x:evai(X!) uEVai(Pa(X!j)\nThe BIC score is simply the log-likelihood plus a penalty term for network complexity:\nlogT . ScoresJc(B_,) = l(B_,: D)- - 2 -0im(G_,)\nwhere Oim(G_,) is the dimension of G_,, which in the case of complete data is simply the number of param eters.\nOur goal is to find the network that maximizes this score. For a fixed structure, the parameters that max imize the score are exactly the maximum likelihood parameters, which simply mirror the frequencies in the data:\n(2)\nFinding the highest scoring network structure is NP hard [Chickering et al. 1995]. Thus, we usually resort to greedy local search procedures [Buntine 1991; Heck erman et al. 1995] that gradually improve a candidate structure by applying local structural transformation: adding, deleting, or reversing an edge. These transfor mations are usually applied in a greedy fashion, with occasional random steps to deal with local maxima and plateaux. Two crucial properties of the BIC score greatly facilitate this procedure. First, the score of a network can be written as a sum of terms, where each term determines the score of a particular choice of par ents for a particular variable. Thus, a local change to one family x; , Pa(X;), such as the addition or removal of an arc, affects only one of these terms. As a conse quence, the incremental value of any change to another family in the network remains unchanged. Hence, to determine the values of all local changes to the current\n94 Boyen, Friedman, and Koller\nnetwork structure we need only re-evaluate changes to the family of x;.\nSecond, the term that evaluates the family of Xj is a function only of the sufficient statistics for Xj and its parents. Thus, these sufficient statistics are the only aspects of the data that we need to preserve. For each choice of parents for Xj, we need to collect statistics on different events. Evaluation of local changes usually involves computation of new sufficient statistics, and then an evaluation of the score with respect to the statistics of the new model and its dimension.\nThe Bayesian score is somewhat more complex. It in volves taking prior distribution over models and pa rameters in to account. Without going into details, we note that for some choices of priors, such as the BDe priors of [Heckerman et al. 1995], the main fea ture of BIC also hold for the Bayesian score: the score decomposes in to a sum of terms, and the score de pends only on the sufficient statistics collected from data. Although the Bayesian score and the BIC are asymptotically equivalent, for small sample sizes the Bayesian score often performs better.\n2.3 Learning DBNs: Incomplete Data\nThe main difficulty with learning from partial observa tions is that we no longer know the counts in the data. As a consequence, the score no longer decomposes into separate components corresponding to individual fam ilies. The most common solution to the missing data problem is the Expectation-Maximization (EM) algo rithm [Dempster et a!. 1977; Lauritzen 1995). The algorithm is an iterative procedure that searches for a parameter vector 9* which is a local maximum of the likelihood function. It starts with some initial (of ten random) parameter vector 9. It then repeatedly executes a two-phase procedure. In the E-step, the current parameters are used to complete the data by \"filling in\" unobserved values with their expected val ues. In the M-step, the completed data is used as if it was real, in a maximum likelihood estimation step.\nMore precisely, given a current parameter vector 9, the algorithm computes the expected sufficient statistics (ESS) for D relative to 9:\nNy = E [Ny : (G-.,9)) = L E[t(y(t) I D) : (G_,, 9)]\n= LP(y(t)ID,(G-.,9)) (3)\nIt then uses the ESS N. in place of the sufficient statis tics N. in (2) for estimating a new set of parameters 9. The process then repeats. The central theorem\njustifying EM's performance is that each EM cycle is guaranteed to improve the likelihood of the data given the model, until it reaches a local maximum.\nEM has been traditionally viewed as a method for adjusting the parameters of a fixed model structure. Friedman's Structural EM (SEM) algorithm [1997) ex tends it to the structure learning task. The SEM algo rithm has the same E-step as EM, completing the data by computing expected counts based on the current structure and parameters. In addition to re-estimating parameters, the M-step of SEM uses the ESS, com puted according to the current structure, to score other candidate structures. Essentially, the algorithm com pletes the data using the current network structure, then performs a complete-data structure search in the inner loop. After some number of steps, the algorithm stops the structure search, uses the current candidate network to complete the data, and the process repeats. Friedman [1998) shows that, for a large family of scor ing rules, the network resulting from this inner loop must have a higher score than the original. This is true even though the expected counts used in evalu ating the new structure are computed using the old structure.\nMore precisely, Friedman defines a notion of expected score which is the expectation of the score for different completions D+ of the data D, where the probability of a completion D+ is P(D+ I D, B). For a large class of scores, such as BIC or BDe, he shows that if we make a change to the network structure that increases the expected score, then the true score increases by at least as much. The crucial property of the expected score is that, like the score for complete data, it de composes into a sum of local terms. For instance, the expected BIC score is simply the BIC score applied to the expected sufficient statistics. This property rein states both of the important advantages that we had in the case of structure search for complete data: the ability to re-evaluate only a small number of structural changes following a structural change, and the ability to restrict attention to (expected) sufficient statistics.\n3 The E-step\nAs the discussion above clearly shows, the key require ment in applying SEM is the ability to compute the ex pected sufficient statistics. This computation requires that we run probabilistic inference over the entire tra jectory D, as the distribution over any missing value will typically be affected by past and future evidence. Unfortunately, probabilistic inference for DBNs is no toriously expensive, as Friedman et al. [1998) clearly point out in their paper on learning DBNs.\nThe classical solution to DBN inference is based on\nDiscovering the Hidden Structure of Complex Dynamic Systems 95\nthe same ideas as the forward-backward algorithm for HMMs [Rabiner and Juang 1986].\nAt a high level, the algorithm propagates forward mes sages a(tl from the start of the sequence forward, gath ering evidence along the way; it uses a similar process to propagate backward messages f3(t) in the reverse direction. Letting r(ll, ... , r(T) denote the observa tions along the sequence, a(tl represents the distribu tion P(X(t) I r(l), ... , r(tl); f3(t) represents the con ditional distribution P(r(t+l), ... , r(T) I x(tl). The update rules for the messages are:\nX\n(J(t) (x) ex L (3(t+i) (x') PB� (r(t+l) I x') PB� (x' I x) x'\nNow, the posterior distribution ¢Pl over the states at time t is simply a(tl (x) · f3(t) (x) (suitably renor malized). Similarly, the joint posterior ¢(t,t+l)(x,x') over the states at t and t + 1 is proportional to a(tl (x) · PB� (x' I x) · f3(t+!) (x') · PB� (r(tH) I x') . This message passing algorithm can be implemented in a straightforward way for DBNs. Indeed, this algo rithm was essentially the one used by [Friedman et a!. 1998]. Unfortunately, this approach is practical only for very small DBNs: its basic representation - the a and (3 messages - have an entry for every possi ble state of the system, making them exponential in the number of state variables. Even in highly struc tured processes, these messages do not admit a more compact representation, as the variables in the mes sage are almost invariably fully correlated [Ghahra mani and Jordan 1996a; Boyen and Koller 1998b].\nBoyen and Koller [1998b] describe a new approach to approximate inference in dynamic systems, which avoids the problem of explicitly maintaining distribu tions over large spaces. Their algorithm maintains factored representations that approximate these dis tributions. In [Boyen and Koller 1998a], they apply this algorithm to the task of message passing in the forward-backward algorithm.\nSpecifically, they represent messages as products of independent marginals over disjoint clusters. Let zl, ... , ZK be a partition of X. The approxi mate message £i(t) is represented as a product of marginals Tik a(Z�tl). Similarly, i](t) is represented as f1k i](z�tl). To apply forward propagation, a(tl is multiplied by the transition model PB�, and condi tioned on r(t+l). The result is then projected onto each cluster z�t), and the next message a(t+l) is defined as the product of these marginals. Backward propagation works similarly, except that we take the product of\ni](tH) by the transition model P(X(tH) I X(tl), con dition on the observations at time t + 1, and project onto the clusters z�t) to get f3(t). In the case of DBNs, we can accomplish these prop agations quite efficiently. For example, in the for ward propagation case, we first generate a clique tree y(t) [Lauritzen and Spiegelhalter 1988] in which, for every k, some clique contains z�t) and some clique contains z�t+t). A standard clique tree propagation algorithm can then be used to compute the posterior distribution over every clique. Once that is done, the distribution over z�t+l) is easily extracted from the appropriate clique potential. (See [Boyen and Koller 1998b] for details.)\nBoyen and Koller demonstrate the applicability of their approximate message passing algorithm for the task of parameter estimation for a real-life network with ten state variables (the BAT network of [Forbes et a!. 1995]). They show that their algorithm is 10-15 times faster than exact message propagation, and that the degradation in the quality of the learned model is negligible.\n4 Approximate Sufficient Statistics\nThis approximate message passing algorithm allows us to perform inference in much larger dynamic systems, and thereby perhaps to learn them. However, we are still faced with a serious problem in computing ex pected sufficient statistics for structure search. Re call that, to compute ESS, we need to compute joint marginals over sets of variables Y, as in Equation (3). In standard EM for parameter learning, the situation is easy. Here, the only required statistics are the ones over the families of the individual variables in B_,. By construction of the clique tree, each family is contained in some clique of the clique tree Y used in our approx imate inference. Furthermore, we can use Y to com bine a forward message a(tl and a backward message i](tH), and obtain a compact representation of the full joint posterior ¢(t,t+l) as a tree y(t). It is then easy to extract the needed marginals from the appropriate cliques [Boyen and Koller 1998a].\nIn SEM, this easy solution is no longer applicable, since we use the results of our E-step for a given B_, to com pute sufficient statistics for a variety of other candidate structures B'_,. Hence, we will need to compute ESS for events Y which do not correspond to families in the current structure B_,. There is no reason to as sume that the variables in an arbitrary event Y will be contained in a single clique in Y. Note that this issue did not arise in the work of [Friedman et a!. 1998]. In their approach, the corresponding clique tree to Y was\n96 Boyen, Friedman, and Koller\nbasically a single clique containing all of the variables in X(t), x(t+I). Thus, all possible queries Y were con tained in a single clique (the one and only clique). It is our ability to provide a finer grained decomposition of the clique tree that brings up this issue.\nThe straightforward approach to this problem is to compute the necessary posterior P(Y(t) I D,B....,) by performing a special-purpose inference step over y(t), tailored to Y. Unfortunately, this operation can be expensive, especially if Y contains variables which are \"distant\" in the current structure. It also needs to be performed a great many times every time slice - once for each statistic of interest. The problem is even more serious when we seek statistics over several time slices, as in our search for violations of the Markov prop erty. In this case, y( t) contains nodes over several time slices, so that the computation of P(Y(t) I D, B....,) is almost invariably infeasible.\nWe propose a new approximate solution, in the same spirit as the approximate message decomposition of Boyen and Koller [1998b, 1998a] and of the variational approach of [Ghahramani and Jordan 1996a]. Instead of computing the joint distribution P(Y(t)), we ap proximate it as a product of independent marginals over individual variables. We approximate P(Y(t) I D,B....,) as fl; P(Y;'t) I D,B....,), where for each of the individual variables Y;, P(Y;(t) I D, B....,) is computed by marginalizing some clique in y(t) in which the vari able is present. From there, Ny is computed as in (3). This process requires a pass over the clique tree to per form the marginalization, and then a simple computa tion which requires linear time in the number of suffi cient statistics that we are maintaining.1 Furthermore, this approach applies easily to the task of computing ESS for events that span several time slices: we simply extract marginals from more than one clique tree y(t). For example, to approximate the joint marginals over an event X(t+I) X(t-2) we could extract P(X(t+I)) t ' t ' t from y(t) and P(X;(t-2)) from y(t-2). We then multiply the two marginals, and add the result to the ex pected counts for this event, as usual.\nAt first glance one might think that this approxima tion discards all correlations between variables in dif ferent clusters Y;. In general, however, this is not the\n1 We note that we could have used a more refined computation that would have taken advantage of the co occurrence of some subsets of Y within a single clique in order to avoid approximating them as independent. How ever, this extension would require that we marginalize the cliques in a potentially different way for every statistic that we need to compute. Our experiments (see below) suggest that the error introduced by this approximation is probably not large enough to be worth the significant computational overhead.\ncase, since we are examining the posterior distribution Y given different configuration of evidence. Consider, for example, the situation where we have two binary variables A, B E Y, which belong to different cliques. If, in our data, we have that, depending on the evi dence, A and B are either both probably true or both probably false, then at each step the mass of (A, B) in the product distribution P(A(t))P(B(t)) will be large at either (0, 0) or (1, 1), and its accumulation in our sufficient statistics will reveal the correlation.\nOf course, there are cases where this approximation would lose correlations. For example, if we have that A(t) and B(t) are both uniformly distributed yet corre lated, our approximate sufficient statistics would not reveal the correlation. Thus, if the evidence does not give us information about the values of the variables, but only about the correlations, our approximation will lose this correlation. We argue that such models are hard to learn in general (with or without our ap proximation). Indeed, if the evidence does not give us enough information about the value of a hidden vari able, our ability to learn something meaningful about its distribution is very limited.\nWe tested the error introduced by this approximation on the better-understood problem of parameter esti mation. As in [Boyen and Koller 1998a], we gener ated a long sequence of 20,000 synthetic data points from the BAT network [Forbes et a!. 1995]. We then attempted to estimate the parameters back from the data, given the correct structure. We ran two ver sions of this experiment: one with the approximate message passing algorithm (as above) but without the approximation of the ESS, and the other with both. As can be seen on Table 1, the approximation does not degrade the learning accuracy. On the contrary, the approximation even seems to be slightly beneficial, which could be explained as a regularization effect.\n5 Structure Search\n5.1 Efficient Structure Search\nIn the previous section, we suggested methods for effi cient computation of expected sufficient statistics. In SEM search, we need to compute the ESS for each\n-;\nDiscovering the Hidden Structure of Complex Dynamic Systems 97\nfamily we change. How do we choose which ESS we should compute?\nThe first approach is to compute in advance all the expected sufficient statistics the search might need. However, since there are too many of these, this so lution is impractical.2 The second approach, which was used by Friedman et a!. [1998], is to compute suf ficient statistics \"on demand\", i.e., statistics for Y are . computed only when the search needs to evaluate a structure with this family. 3 Unfortunately, that also is typically quite expensive, as it requires a traversal over the entire training sequence.\nThese two solutions are at the extreme ends of a spec trum. Friedman et al. [1999] present an intermediate solution which we also adopt. The search procedure works in stages. At the beginning of each stage the search procedure posts the statistics it will require for that stage. These are selected in an informed way, based on the current state of the search. The requested statistics are then computed in one batch, using a sin gle inference phase for all of them at once. More specif ically, the algorithm finds for each variable x; a set Pot; of potential parents, based on the current net work structure. At each stage, the search is restricted to consider only operations that involve adding edges Y -t x; for Y E Pot; or removing current arcs. The number of ESS required for these operations is fairly small, and can be collected at once. The algorithm uses heuristics to focus the attention of the search procedure on \"plausible\" potential parents. The algo rithm therefore requires relatively few statistics in each stage. After this restricted search is done, the process repeats, using the new network as a basis for finding new potential parents for each variable. This process is iterated until convergence of the scoring function.\n5.2 Discovering Hidden Variables\nAs we mentioned in the introduction, a fundamen tal problem when learning dynamic systems from real data is the discovery of hidden variables. In stock mar ket data, for example, internet stocks are typically cor related. Unless we realize that this correlation is due to a hidden variable - public perception of the future growth of internet revenue - the model we learn is likely to be quite poor.\nThe task of discovering new hidden variables is a nota-\n2In general, there are exponential number of statistics. When we restrict the indegree of each variable, the number is polynomial, but still unrealistically large.\n30f course, we want to avoid unnecessary recomputa tions. The standard solution is to store a cache of com puted statistics, and call the ESS computations on statis tics that are not found in the cache.\nriously difficult one. In temporal sequences, however, we have some cues that can indicate the presence of such variables. In particular, ignoring a hidden vari able will often lead to a non-Markovian correlation, in duced by the loss of information as the hidden variable is \"forgotten\" from step to step. Thus, we can search for non-Markovian correlations, and use them as an in dication that the process needs additional \"memory\" about the past.\nMore precisely, suppose that we discover that we can predict x(t+1l using X(tl and y(t-1). Then we might consider creating a new hidden variable H such that H is a parent of X' and Y is a parent of H'. Thus, we will have that y(t-1) influences H(t) via theY -t H' edge, and that H(t) in turn influences X(t+I) via the H -t X' edge. In other words, H behaves as the \"memory\" of Y with one step of lag.\nIn general, we propose the following algorithm. We start by learning the edges among variables in k time slices (a k-TBN) for some fixed time window k. When some of the variables are unobserved, we use structural EM and our approximation methods to estimate the ESS of the variables in these k consecutive time slices. We note that this process uses structural EM: the suf ficient statistics are computed once, and then used for an extended search phase over structures. That, com bined with our approach to computing ESS for vari ables that are far apart in the network, allows us to estimate the ESS for the k-TBN without ever doing inference on it.\nAfter we learn such a network, we eliminate the non Markovian arcs by creating new hidden variables that \"remember\" those variables that participate as par ents in non-Markovian correlations. Any variable X in time slice t - d which directly influences a variable\nY at time t + 1 requires d new hidden variables: at time t, the ith introduced variable x-i has the same value that X had at time slice t- i. In order to repre sent this \"memory\" model exactly, the CPDs of these newly created variables would have to be determinis tic. However, deterministic models do not easily ac commodate EM-style adaptation. Furthermore, since we want to encourage the search to construct variables that remember global phenomena, we also add \"persis tence\" arcs that allow the hidden variables to depend on longer term past. Therefore, we initialize the pa rameters for these variables to be noisy versions of the appropriate deterministic CPDs, and make the noise biased toward persistence with the previous time slice of the hidden variable.\nHaving constructed a new 2TBN, we are now again in a position where we can run parametric EM to find better parameters for the new hidden variables. Then\n98 Boyen, Friedman, and Koller\nwe repeat the process (structure learning, introduction of variables, etc.).\n6 Experimental Results\nWe tried our algorithm on four different domains: three involve real-world data, while the fourth is sam pled from a given DBN. We originally ran our algo rithm using standard structure search on the observ able variables only, allowing for non-Markovian edges. We tested the accuracy on the test set for the learned network, and then used it to introduce hidden vari ables, as described above. We then repeated this pro cess, running inference to compute a certain predeter mined set of ESS, and then using them for an exten sive structure search. For structure search, we experi mented with both BDe and BIC scores, and with both full and tree-structured CPDs. We report the results for the BDe score with trees; the results for the other cases are somewhat different, but exhibit the same gen eral trends.\nAs points of comparison, we also learned two other types of structure: a standard Markovian DBN over the observable variables (using a standard BN structure learning algorithm), and a Factorial HMM (FHMM) structure [Ghahramani and Jordan 1996b] using parametric EM with the approximate message passing algorithm of [Boyen and Koller 1998a]. An FHMM is best viewed as a DBN with some number e of hidden variables, each of which evolves independently of the others; each observable variable depends on all of the hidden variables within its time slice. FHMMs have been shown to be a good candidate for modeling several interacting processes evolving in parallel (e.g., multiple articulatory processes in speech). In our ex periments, we tried FHMMs with two, four, and six binary hidden variables. (For data sets with only a small number of observables, we tried fewer hidden variables.)\nBach chorales - This data set was proposed as part of the 1991 Santa Fe competition for learning time series [Weigend and Gershenfeld 1990]. It encodes the melodic line of 100 chorales attributed to J .S. Bach. The model has five discrete attributes Key signature, Pitch, Duration, Fermata, and Time signature. The last three all represent temporal aspects of the piece. The training set consisted of 71 chorales, each of which is about 50 notes long, for a total training set size of 3212 transitions; the test set consisted of 29 chorales, chosen at random, for a total test set size of 1379. The first column of Table 2 shows the results for this data set. We can see that all of the instances of the DBN learning algorithms performed significantly bet ter than all instances of the FHMM algorithm. We\nalso see that the introduction of non-Markovian edges and hidden variables helps significantly with respect to standard structure search over the observables. Some what disappointingly, the introduction of hidden vari ables per se does not improve the log-likelihood. We attribute that to numerical overfitting, as the net work structure learned (shown in Figure 1) is actually quite natural. The algorithm detected the correlations between the three tempo attributes. The other two are decoupled from those. All variables have persis tence arcs, except (very naturally) the Fermata vari able, which represents a momentary event correspond ing to the end of a segment. The algorithm intro duced several hidden variables that capture the non Markovian nature of these variables. Most interest ingly, the Duration variable has a time-slice that rep resents a short non-Markovian dependence (two time slices); on the other hand, the hidden variables in troduced for the Time signature variable, which rep resents much longer-term aspects of the piece, corre spond to longer-range dependencies.\nSleep apnea - This data set was also proposed as part of the Santa Fe competition [Weigend and Ger shenfeld 1990]. It was obtained by monitoring 3 med ical parameters on a patient suffering from sleep ap nea. The data contains 34000 points, collected in a single run. This data set is non-stationary, due to the various sleep phases and the condition of the pa tient. Following the suggestion of Dagum and Galper [1993], each variable was discretized into seven buck ets. We partitioned the sequence into four training subsequences, for a total of 19994 transitions, and one test sequence of size 13999. The behavior of the log likelihoods is very similar to the previous data set. Again, the structure is very natural. There is a strong correlation between BloodOxygen and HeartRate and between HeartRate and Chest Volume, but not directly between Blood Oxygen and Chest Volume, a very natu ral assumption. As above, the algorithm discovered interesting non-Markovian correlations; here, the tem poral granularity of BloodOxygen is shorter-term than\n-;\nDiscovering the Hidden Structure of Complex Dynamic Systems 99\nslice t slice t+l evidence at l+ I\nslic:et-1 slice t slice t+l slicct-2 sbr;et-1\n(Yo�) ,_, .\n.\n�\nsliut sli<;et+l\nslice\\ slicct+l\n� I a � �\n: � ' !\n� ! • ..\n9 �·\nw.\n\\f 8 �· :� :1\nI / �- slice t slicet+l\nFigure 1: (clockwise from top left) original BAT network; 2TBNs learned from BAT, Stock, Apnea, and Bach. The shaded nodes are observable variables, denoted by their names; hidden variables are labeled alphabetically. (The Apnea and Bach networks have been unrolled for a few time slices to highlight the long-range dependences.)\nthat of HeartRate, again, a very reasonable model.\nStock market - We constructed this data set our selves from the prices of securities of 20 companies in a handful of industries: internet, hardware, software, chips, and car manufacturers from the US and Japan. Since we are usually more interested in trends and cor relations than absolute prices, only the daily trend of each stock is recorded (up, down, or nfa). The period covered extends from Feb 92 to Feb 99, for a total of 1768 trading days. We extracted one subsequence from the middle of the period and one from the end for use as training data, giving 1 195 transitions for training and 567 for test. The log-likelihood results are shown on Table 2. Here, on the second iteration, the intro duction of the hidden variables shows an improvement over the basic score. The shape of the learned network here is somewhat different, as most of the correlations appear within a time slice. This phenomenon is quite natural, as individual trading days can be quite dif ferent, but within a given day, many stocks tend to move together. However, as our algorithm is geared to detecting correlations that induce temporal depen dencies, it did not discover many hidden variables. To discover hidden variables in this setting, we would need another approach, such as one based on the common intuition of looking for \"almost cliques\" in the net work [Spirtes et al. 1993]. However, we do see that the correlations discovered by our algorithm are quite\nnatural, in that the edges between tend to accumu late between companies in the same industry, or with similar characteristics.\nBAT - We generated this synthetic data set by sampling a long trajectory from the BAT net work [Forbes et al. 1995] for tracking the motion of a car on a freeway. The network has ten state vari ables, ten observable variables, and a few transient variables (see Figure 1). We extracted four training and five test subsequences, for a total of 4992 training data, and 5029 test data. Only the observable vari ables were recorded from the trajectory, and the algo rithm received no prior knowledge whatsoever about the correct structure. For this domain, since the. data is synthetic, we can compute the log-likelihood of the test data according to the correct network, giving us a \"gold standard\". We see that, in fact, our algorithm learns a model whose performance is fairly close to the gold standard, and much better than parametric EM applied to the correct structure (probably due to numerical overfitting). The performance is also sig nificantly better than the FHMM results or the fully observable structure search. In this case, the introduc tion of hidden variables actually helps to a nontrivial extent. In this case, however, the learned structures are not very compelling, particularly when compared to the true network. For clarity, we have chosen to present in Figure 1 the somewhat simpler structure\n100 Boyen, Friedman, and Koller\nlearned after only a single iteration of SEM. We can see that the algorithm does discover a few interest ing correlations, such as one between TumSignal and XdotSens (sensed lateral movement).\n7 Discussion and Conclusions\nIn this paper, we combine two lines of works. The first deals with search techniques for learning in the presence of hidden variables [Friedman 1997; Fried man et al. 1998]. The second deals with fast approxi mate inference in complex networks [Boyen and Koller 1998b; Boyen and Koller 1999]. While approximate DBN inference has been playing a major role in para metric learning [Boyen and Koller 1998a; Ghahramani and Jordan 1996a], this is the first paper to deal with the issues involved in applying it to structure search. In particular, we had to deal with the computation of a large number of different statistics and to introduce methods for discovering hidden variables. Although we based our solution on the Boyen-Koller approxi mation, many of these ideas can be applied to other approximate inference methods, including the varia tional methods of Ghahramani and Jordan [1996a].\nClearly, our work only scratches the surface of the problem of discovering hidden variables. While our algorithm discovers correlations that involve temporal interactions, it is less apt at detecting atemporal cor relations as we saw in the stock market data. On the other extreme, our algorithm does not support the dis covery of truly long-range dependencies and aggregate influences from variables evolving at different speeds. Our current method for discovering hidden influences requires that the time scale of the interaction matches the time scale of the model. If there is a hidden vari able evolving much more slowly than the observables, then our algorithm would not find it. This problem can be addressed by explicitly searching for violations of the Markov property at widely varying time granu larities. Specifically, applying our algorithm on a data sequence subsampled by a factor of k would exhibit interactions with a time constant of the order of k. We believe this issue to be of crucial importance when learning from real-world data. In real systems, ob servable variables are typically influenced by hidden processes with widely differing time scales, which fur thermore are not always related to the sampling rate of the observations.\nReferences\nBoyen, X. and D. Koller (1998a). Approximate learning of dynamic models. In NIPS 11. Boyen, X. and D. Koller (1998b). Tractable inference for complex stochastic processes. In Proc. UAI. Boyen, X. and D. Koller (1999). Exploiting the architec-\nture of dynamic systems. In Proc. AAAI. Buntine, W. (1991). Theory refinement on Bayesian net\nworks. In Proc. UAI, pp. 52-60. Chickering, D., D. Geiger, and D. Heckerman (1995).\nLearning Bayesian networks: search methods and ex perimental results. In Proc. AI & Stats, pp. 112-128. Dagum, P. and A. Galper (1993). Forecasting sleep ap nea with dynamic network models. In Proc. UAI, pp. 64-71. Dempster, A., N. Laird, and D. Rubin (1977). Maximum-likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Soci ety B39, 1-38. Forbes, J., T. Huang, K. Kanazawa, and S. Russell (1995). The BATmobile: Towards a Bayesian auto mated taxi. In Proc. IJCAI. Friedman, N. (1997). Learning belief networks in the presence of missing values and hidden variables. In Proc. ICML. Friedman, N. (1998). The Bayesian structural EM algo rithm. In Proc. UAI, pp. 129-138. Friedman, N., K. Murphy, and S. Russell (1998). Learn ing the structure of dynamic probabilistic networks. In Proc. UAI, pp. 139-147. Friedman, N., D. Peer, and I. Nachman (1999). Learning Bayesian network structure from massive datasets: The \"sparse candidate\" algorithm. In Proc. UAI. Ghahramani, Z. and M. Jordan (1996a). Factorial hid den Markov models. In Proc. NIPS. Ghahramani, Z. and M. Jordan (1996b). Factorial hid den Markov models. In Proc. NIPS. Heckerman, D. (1999). A tutorial on learning with Bayesian networks. In M. I. Jordan (Ed.), Learning in Graphical Models. MIT P ress. Heckerman, D., D. Geiger, and D. M. Chickering (1995). Learning Bayesian networks: The combi nation of knowledge and statistical data. Machine Learning 20, 197-243. Jensen, F., S. Lauritzen, and K. Olesen (1990). Bayesian updating in recursive graphical models by local com putations. Computational Statistical Quarterly 4. Lauritzen, S. and D. Spiegelhalter (1988). Local com putations with probabilities on graphical structures and their application to expert systems. J. Roy. Stat. Soc. B 50. Lauritzen, S. L. (1995). The EM algorithm for graphi cal association models with missing data. Computa tional Statistics and Data Analysis 19, 191-201. Rabiner, L. and B. Juang (1986, January). An intro duction to hidden Markov models. IEEE Acoustics, Speech & Signal Processing. Schwarz, G. (1978). Estimating the dimension of a model. Annals of Statistics 6, 461-464. Shenoy, P. P. and G. R. Shafer (1990). Axioms for proba bility and belief-function propagation. In Proc. UAI, pp. 169-198. Spirtes, H., C. Glymour, and R. Scheines (1993). Cau sation, Prediction, and Search. Springer-Verlag. Weigend, A. and N. Gershenfeld (1990). Time-series competition. In Proc. Nonlinear Modeling and Fore casting, Volume XII. Santa Fe institute: Addison Wesley."
    } ],
    "references" : [ {
      "title" : "Theory refinement on Bayesian net­",
      "author" : [ "W. Buntine" ],
      "venue" : null,
      "citeRegEx" : "Buntine,? \\Q1991\\E",
      "shortCiteRegEx" : "Buntine",
      "year" : 1991
    }, {
      "title" : "Forecasting sleep ap­",
      "author" : [ "P. Dagum", "A. Galper" ],
      "venue" : null,
      "citeRegEx" : "Dagum and Galper,? \\Q1993\\E",
      "shortCiteRegEx" : "Dagum and Galper",
      "year" : 1993
    }, {
      "title" : "Learning belief networks",
      "author" : [ "N. Friedman" ],
      "venue" : null,
      "citeRegEx" : "Friedman,? \\Q1997\\E",
      "shortCiteRegEx" : "Friedman",
      "year" : 1997
    }, {
      "title" : "The Bayesian structural EM algo­",
      "author" : [ "N. Friedman" ],
      "venue" : null,
      "citeRegEx" : "Friedman,? \\Q1998\\E",
      "shortCiteRegEx" : "Friedman",
      "year" : 1998
    }, {
      "title" : "A tutorial on learning with",
      "author" : [ "D. Heckerman" ],
      "venue" : null,
      "citeRegEx" : "Heckerman,? \\Q1999\\E",
      "shortCiteRegEx" : "Heckerman",
      "year" : 1999
    }, {
      "title" : "The EM algorithm for graphi­",
      "author" : [ "S.L. Lauritzen" ],
      "venue" : null,
      "citeRegEx" : "Lauritzen,? \\Q1995\\E",
      "shortCiteRegEx" : "Lauritzen",
      "year" : 1995
    }, {
      "title" : "Estimating the dimension of a model",
      "author" : [ "G. Schwarz" ],
      "venue" : "Annals of Statistics",
      "citeRegEx" : "Schwarz,? \\Q1978\\E",
      "shortCiteRegEx" : "Schwarz",
      "year" : 1978
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "Recent work has made significant progress on the prob­ lem of learning Bayesian networks from data (see, for example, [Heckerman 1999].",
      "startOffset" : 118,
      "endOffset" : 134
    }, {
      "referenceID" : 2,
      "context" : "1998], using the Structurol EM (SEM) algo­ rithm [Friedman 1997].",
      "startOffset" : 49,
      "endOffset" : 64
    }, {
      "referenceID" : 6,
      "context" : "The most frequently used are the Bayesian Information Criterion {BIG) [Schwarz 1978] and the Bayesian scores, such as the BOe score of [Heckerman et a!.",
      "startOffset" : 70,
      "endOffset" : 84
    }, {
      "referenceID" : 0,
      "context" : "Thus, we usually resort to greedy local search procedures [Buntine 1991; Heck­ erman et al. 1995] that gradually improve a candidate structure by applying local structural transformation: adding, deleting, or reversing an edge.",
      "startOffset" : 58,
      "endOffset" : 97
    }, {
      "referenceID" : 5,
      "context" : "The most common solution to the missing data problem is the Expectation-Maximization (EM) algo­ rithm [Dempster et a!. 1977; Lauritzen 1995).",
      "startOffset" : 102,
      "endOffset" : 140
    }, {
      "referenceID" : 2,
      "context" : "Friedman's Structural EM (SEM) algorithm [1997) ex­ tends it to the structure learning task.",
      "startOffset" : 0,
      "endOffset" : 48
    }, {
      "referenceID" : 2,
      "context" : "Friedman [1998) shows that, for a large family of scor­ ing rules, the network resulting from this inner loop must have a higher score than the original.",
      "startOffset" : 0,
      "endOffset" : 16
    }, {
      "referenceID" : 2,
      "context" : "Unfortunately, probabilistic inference for DBNs is no­ toriously expensive, as Friedman et al. [1998) clearly point out in their paper on learning DBNs.",
      "startOffset" : 79,
      "endOffset" : 102
    }, {
      "referenceID" : 2,
      "context" : "2 The second approach, which was used by Friedman et a!. [1998], is to compute suf­ ficient statistics \"on demand\", i.",
      "startOffset" : 41,
      "endOffset" : 64
    }, {
      "referenceID" : 2,
      "context" : "Friedman et al. [1999] present an intermediate solution which we also adopt.",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 1,
      "context" : "Following the suggestion of Dagum and Galper [1993], each variable was discretized into seven buck­ ets.",
      "startOffset" : 28,
      "endOffset" : 52
    }, {
      "referenceID" : 2,
      "context" : "The first deals with search techniques for learning in the presence of hidden variables [Friedman 1997; Fried­ man et al. 1998].",
      "startOffset" : 88,
      "endOffset" : 127
    }, {
      "referenceID" : 2,
      "context" : "The first deals with search techniques for learning in the presence of hidden variables [Friedman 1997; Fried­ man et al. 1998]. The second deals with fast approxi­ mate inference in complex networks [Boyen and Koller 1998b; Boyen and Koller 1999]. While approximate DBN inference has been playing a major role in para­ metric learning [Boyen and Koller 1998a; Ghahramani and Jordan 1996a], this is the first paper to deal with the issues involved in applying it to structure search. In particular, we had to deal with the computation of a large number of different statistics and to introduce methods for discovering hidden variables. Although we based our solution on the Boyen-Koller approxi­ mation, many of these ideas can be applied to other approximate inference methods, including the varia­ tional methods of Ghahramani and Jordan [1996a].",
      "startOffset" : 89,
      "endOffset" : 848
    } ],
    "year" : 2011,
    "abstractText" : "Dynamic Bayesian networks provide a compact and natural representation for complex dynamic systems. However, in many cases, there is no ex­ pert available from whom a model can be elicited. Learning provides an alternative approach for constructing models of dynamic systems. In this paper, we address some of the crucial compu­ tational aspects of learning the structure of dy­ namic systems, particularly those where some relevant variables are partially observed or even entirely unknown. Our approach is based on the Structural Expectation Maximization (SEM) al­ gorithm. The main computational cost of the SEM algorithm is the gathering of expected suf­ ficient statistics. We propose a novel approxima­ tion scheme that allows these sufficient statistics to be computed efficiently. We also investigate the fundamental problem of discovering the exis­ tence of hidden variables without exhaustive and expensive search. Our approach is based on the observation that, in dynamic systems, ignoring a hidden variable typically results in a violation of the Markov property. Thus, our algorithm searches for such violations in the data, and in­ troduces hidden variables to explain them. We provide empirical results showing that the algo­ rithm is able to learn the dynamics of complex systems in a computationally tractable way.",
    "creator" : "pdftk 1.41 - www.pdftk.com"
  }
}