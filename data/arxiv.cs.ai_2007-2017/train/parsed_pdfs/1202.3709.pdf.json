{
  "name" : "1202.3709.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "EDML: A Method for Learning Parameters in Bayesian Networks",
    "authors" : [ "Arthur Choi", "Khaled S. Refaat", "Adnan Darwiche" ],
    "emails" : [ "darwiche}@cs.ucla.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "We propose a method called EDML for learning MAP parameters in binary Bayesian networks under incomplete data. The method assumes Beta priors and can be used to learn maximum likelihood parameters when the priors are uninformative. EDML exhibits interesting behaviors, especially when compared to EM. We introduce EDML, explain its origin, and study some of its properties both analytically and empirically."
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "We consider in this paper the problem of learning Bayesian network parameters given incomplete data, while assuming that all network variables are binary. We propose a specific method, EDML,1 which has a similar structure and complexity to the EM algorithm (Dempster, Laird, & Rubin, 1977; Lauritzen, 1995). EDML assumes Beta priors on network parameters, allowing one to compute MAP parameters. When using uninformative priors, EDML reduces to computing maximum likelihood (ML) parameters.\nEDML originated from applying an approximate inference algorithm (Choi & Darwiche, 2006) to a meta network in which parameters are explicated as variables, and on which data is asserted as evidence. The update equations of EDML resemble the ones for EM, yet EDML appears to have different convergence properties which stem from its being an inference method as opposed to a local search method. For example, we will identify a class of incomplete datasets on which EDML is guaranteed to converge immediately to an\n1EDML stands for Edge-Deletion MAP-Learning or Edge-Deletion Maximum-Likelihood as it is based on an edge-deletion approximate inference algorithm that can compute MAP or maximum likelihood parameters.\noptimal solution, by simply reasoning about the behavior of its underlying inference method.\nEven though EDML originates in a rather involved approximate inference scheme, its update equations can be intuitively justified independently. We therefore present EDML initially in Section 3 before delving into the details of how it was originally derived in Section 5.\nIntuitively, EDML can be thought of as relying on two key concepts. The first concept is that of estimating the parameters of a single random variable given soft observations, i.e., observations that provide soft evidence on the values of a random variable. The second key concept behind EDML is that of interpreting the examples of an incomplete data set as providing soft observations on the random variables of a Bayesian network. As to the first concept, we also show that MAP and ML parameter estimates are unique in this case, therefore, generalizing the fundamental result which says that these estimates are unique for hard observations. This result is interesting and fundamental enough that we treat it separately in Section 4 before we move on and discuss the origin of EDML in Section 5.\nWe discuss some theoretical properties of EDML in Section 6, where we identify situations in which it is guaranteed to converge immediately to optimal estimates. We present some preliminary empirical results in Section 7 that corroborate some of the convergence behaviors predicted. In Section 8, we close with some concluding remarks on related and future work. We note that while we focus on binary variables here, our approach generalizes to multivalued variables as well. We will comment later on this and the reason we restricted our focus here."
    }, {
      "heading" : "2 TECHNICAL PRELIMINARIES",
      "text" : "We use upper case letters (X) to denote variables and lower case letters (x) to denote their values. Variable\nsets are denoted by bold-face upper case letters (X) and their instantiations by bold-face lower case letters (x). Since our focus is on binary variables, we use x (positive) and x̄ (negative) to denote the two values of binary variable X. Generally, we will use X to denote a variable in a Bayesian network and U to denote its parents. A network parameter will therefore have the general form θx|u, representing the probability Pr(X=x|U=u).\nNote that variable X can be thought of as inducing a number of conditional random variables, denoted Xu, where the values of variable Xu are drawn based on the conditional distribution Pr(X|u). In fact, parameter estimation in Bayesian networks can be thought of as a process of estimating the distributions of these conditional random variables. Since we assume binary variables, each of these distributions can be characterized by the single parameter θx|u, since θx̄|u = 1−θx|u.\nWe will use θ to denote the set of all network parameters. Given a network structure G in which all variables are binary, our goal is to learn its parameters from an incomplete dataset, such as:\nexample X Y Z 1 x ȳ ? 2 ? ȳ ? 3 x̄ ? z\nWe use D to denote a dataset, and di to denote an example. The dataset above has three examples, with d3 being the instantiation X= x̄, Z=z.\nA commonly used measure for the quality of parameter estimates θ is their likelihood, defined as:\nL(θ|D) = ∏N i=1 Prθ(di),\nwhere Prθ is the distribution induced by network structure G and parameters θ. In the case of complete data (each example fixes the value of each variable), the ML parameters are unique. Learning ML parameters is harder when the data is incomplete, where EM is typically employed. EM starts with some initial parameters θ0, called a seed, and successively improves on them via iteration. EM uses the update equation:\nθk+1x|u = ∑N i=1 Prθk(xu|di)∑N i=1 Prθk(u|di) ,\nwhich requires inference on a Bayesian network parameterized by θk, in order to compute Prθk(xu|di) and Prθk(u|di). In fact, one run of the jointree algorithm on each distinct example is sufficient to implement an iteration of EM, which is guaranteed to never decrease the likelihood of its estimates across iterations. EM also converges to every local maxima, given that it starts with an appropriate seed. It is common to run\nEM with multiple seeds, keeping the best local maxima it finds. See (Darwiche, 2009; Koller & Friedman, 2009) for recent treatments on parameter learning in Bayesian networks via EM and related methods.\nEM can also be used to find MAP parameters, assuming one has some priors on network parameters. The Beta distribution is commonly used as a prior on the probability of a binary random variable. In particular, the Beta for random variable Xu is specified by two exponents, αXu and βXu , leading to a density ∝ [θx|u]αXu−1[1− θx|u]βXu−1. It is common to assume that exponents are > 1 (the density is then unimodal). For MAP parameters, EM uses the update equation (see, e.g., (Darwiche, 2009)):\nθk+1x|u = αXu − 1 +\n∑N i=1 Prθk(xu|di)\nαXu + βXu − 2 + ∑N i=1 Prθk(u|di) .\nWhen αXu = βXu = 1 (uninformative prior), the equation reduces to the one for computing ML parameters. When computing ML parameters, using αXu = βXu = 2 leads to what is usually known as Laplace smoothing. This is a common technique to deal with the problem of insufficient counts (i.e., instantiations that never appear in the dataset, leading to zero probabilities and division by zero). We will indeed use Laplace smoothing in our experiments.\nOur method for learning MAP and ML parameters makes heavy use of two notions: (1) the odds of an event, which is the probability of the event over the probability of its negation, and (2) the Bayes factor (Good, 1950), which is the relative change in the odds of one event, say, X=x, due to observing some other event, say, η. In this case, we have the odds O(x) and O(x|η), where the Bayes factor is κ = O(x|η)/O(x), which is viewed as quantifying the strength of soft evidence η on X=x. It is known that κ = Pr(η|x)/Pr(η|x̄) and κ ∈ [0,∞]. When κ = 0, the soft evidence reduces to hard evidence asserting X= x̄. When κ = ∞, the soft evidence reduces to hard evidence asserting X=x. When κ = 1, the soft evidence is neutral and bears no information on X=x. A detailed discussion on the use of Bayes factors for soft evidence is given in (Chan & Darwiche, 2005)."
    }, {
      "heading" : "3 AN OVERVIEW OF EDML",
      "text" : "Consider Algorithm 1, which provides pseudocode for EM. EM typically starts with some initial parameters estimates, called a seed, and then iterates to monotonically improve on these estimates. Each iteration consists of two steps. The first step, Line 3, computes marginals over the families of a Bayesian network that is parameterized by the current estimates. The second step, Line 4, uses the computed probabilities to\nAlgorithm 1 EM\ninput: G: A Bayesian network structure D: An incomplete dataset d1, . . . ,dN θ: An initial parameterization of structure G αXu , βXu : Beta prior for each random variable Xu 1: while not converged do 2: Pr← distribution induced by θ and G 3: Compute probabilities:\nPr(xu|di) and Pr(u|di)\nfor each family instantiation xu and example di 4: Update parameters:\nθx|u← αXu − 1 +\n∑N i=1 Pr(xu|di)\nαXu + βXu − 2 + ∑N i=1 Pr(u|di)\n5: return parameterization θ\nAlgorithm 2 EDML\ninput: G: A Bayesian network structure D: An incomplete dataset d1, . . . ,dN θ: An initial parameterization of structure G αXu , βXu : Beta prior for each random variable Xu 1: while not converged do 2: Pr← distribution induced by θ and G 3: Compute Bayes factors:\nκix|u← Pr(xu|di)/Pr(x|u)− Pr(u|di) + 1 Pr(x̄u|di)/Pr(x̄|u)− Pr(u|di) + 1\n(1)\nfor each family instantiation xu and example di 4: Update parameters:\nθx|u← argmax p [p]αXu−1[1− p]βXu−1 N∏ i=1 [κix|u · p− p+ 1]\n(2) 5: return parameterization θ\nupdate the network parameters. The process continues until some convergence criterion is met. The main point here is that the computation on Line 3 can be implemented by a single run of the jointree algorithm, while the update on Line 4 is immediate.\nConsider now Algorithm 2, which provides pseudocode for EDML, to be contrasted with the one for EM. The two algorithms clearly have the same overall structure. That is, EDML also starts with some initial parameters estimates, called a seed, and then iterates to update these estimates. Each iteration consists of two steps. The first step, Line 3, computes Bayes factors using a Bayesian network that is parameterized by the current estimates. The second step, Line 4, uses the computed Bayes factors to update network parameters. The process continues until some convergence criterion is met. Much like EM, the computation on Line 3 can be implemented by a single run of the jointree algorithm. Unlike EM, however, the update on Line 4 is not immediate as it involves solving an optimization problem, albeit a simple one. Aside from this optimization task, EM and EDML have the same computational complexity.\nWe next explain the two concepts underlying EDML and how they lead to the equations of Algorithm 2."
    }, {
      "heading" : "3.1 ESTIMATION FROM SOFT OBSERVATIONS",
      "text" : "Consider a random variableX with values x and x̄, and suppose that we have N > 0 independent observations of X, with Nx as the number of positive observations. It is well known that the ML parameter estimates for\nrandom variable X are unique in this case and characterized by θx = Nx/N . If one further assumes a Beta prior with exponents α and β that are ≥ 1, it is also known that the MAP parameter estimates are unique and characterized by θx = Nx+α−1 N+α+β−2 .\nConsider now a more general problem in which the observations are soft in that they only provide soft evidence on the values of random variable X. That is, each soft observation ηi is associated with a Bayes factor κix = O(x|ηi)/O(x) which quantifies the evidence that ηi provides on having observed the value x of variable X. We will show later that the ML estimates remain unique in this more general case, if at least one of the soft observations is not trivial (i.e., with Bayes factor κix 6= 1). Moreover, we will show that the MAP estimates are also unique assuming a Beta prior with exponents ≥ 1. In particular, we will show that the unique MAP estimates are characterized by Equation 2 of Algorithm 2. Further, we will show that the unique ML estimates are characterized by the same equation while using a Beta prior with exponents = 1. This is the first key concept that underlies our proposed algorithm for estimating ML and MAP parameters in a binary Bayesian network."
    }, {
      "heading" : "3.2 EXAMPLES AS SOFT OBSERVATIONS",
      "text" : "The second key concept underlying EDML is to interpret each example di in a dataset as providing a soft observation on each random variable Xu. As mentioned earlier, soft observations are specified by Bayes factors and, hence, one needs to specify the Bayes factor κix|u that example di induces on random variable\nXu. EDML uses Equation 1 for this purpose, which will be derived in Section 5. We next consider a few special cases of this equation to highlight its behavior.\nConsider first the case in which example di implies parent instantiation u (i.e., the parents U of variable X are instantiated to u in example di). In this case, Equation 1 reduces to κix|u = O(x|u,di) O(x|u) , which is the relative change in the odds of x given u due to conditioning on example di. Note that for root variables X, which have no parents U, Equation 1 further reduces to κix = O(x|di) O(x) .\nThe second case we consider is when example di is inconsistent with parent instantiation u. In this case, Equation 1 reduces to κix|u = 1, which amounts to neutral evidence. Hence, example di is irrelevant to estimating the distribution of variable Xu in this case, and will be ignored by EDML.\nThe last special case of Equation 1 we shall consider is when the example di is complete; that is, it fixes the value of each variable. In this case, one can verify that κix|u ∈ {0, 1,∞} and, hence, the example can be viewed as providing either neutral or hard evidence on each random variable Xu. Thus, an example will provide soft observations on variables only when it is incomplete (i.e., missing some values). Otherwise, it is either irrelevant to, or provides a hard observation on, each variable Xu.\nIn the next section, we prove Equation 2 of Algorithm 2. In Section 5, we discuss the origin of EDML, where we go on and derive Equation 1 of Algorithm 2."
    }, {
      "heading" : "4 ESTIMATION FROM SOFT OBSERVATIONS",
      "text" : "Consider a binary variable X. Figure 1 depicts a network where θx is a parameter representing Pr(X=x) and X1, . . . , XN are independent observations of X. Suppose further that we have a Beta prior on parameter θx with exponents α ≥ 1 and β ≥ 1. A standard estimation problem is to assume that we know the values of these observations and then estimate the parameter θx. We now consider a variant on this problem, in which we only have soft evidence ηi about each obser-\nvation, whose strength is quantified by a Bayes factor κix = O(x|ηi)/O(x). Here, κix represents the change in odds that the i-th observation is positive due to evidence ηi. We will refer to ηi as a soft observation on variable X, and our goal in this section is to compute (and optimize) the posterior density on parameter θx given these soft observations η1, . . . , ηN .\nWe first consider the likelihood: Pr(η1, . . . , ηN |θx) = ∏N i=1 Pr(ηi|θx)\n= ∏N i=1[Pr(ηi|x, θx)Pr(x|θx) + Pr(ηi|x̄, θx)Pr(x̄|θx)]\n= ∏N i=1[Pr(ηi|x)θx + Pr(ηi|x̄)(1− θx)]\n∝ ∏N i=1[κ i x · θx − θx + 1].\nThe last step follows because κix = O(x|ηi)/O(x) = Pr(ηi|x)/Pr(ηi|x̄). The posterior density is then:\nρ(θx|η1, . . . , ηN ) ∝ ρ(θx)Pr(η1, . . . , ηN |θx) ∝ [θx]α−1[1− θx]β−1 ∏N i=1[κ i x · θx − θx + 1].\nThis is exactly Equation 2 of Algorithm 2 assuming we replace the random variable X with the conditional random variable Xu. 2\nThe second derivative of the log posterior is\n−α− 1 [θx]2 − β − 1 [1− θx]2 − ∑ i [ (κix − 1) (κix − 1)θx + 1 ]2 which is strictly negative when κix 6= 1 for at least one i. This remains true when α = β = 1. Hence, both the likelihood function and the posterior density are strictly log-concave and therefore have unique modes. This means that both ML and MAP parameter estimates are unique in the case of soft, independent observations, which generalizes the uniqueness result for hard, independent observations on a variable X."
    }, {
      "heading" : "5 THE ORIGIN OF EDML",
      "text" : "This section reveals the technical origin of EDML, showing how Equation 1 of Algorithm 2 is derived, and providing the basis for the overall structure of EDML as spelled out in Algorithm 2.\nEDML originated from an approximation algorithm for computing MAP parameters in a meta network. Figure 2 depicts an example meta network in which\n2 The case of κix = ∞ needs to be handled carefully in Equation 2. First note that κix = ∞ iff Pr(ηi|x̄) = 0 in the derivation of this equation. In this case, the term Pr(ηi|x)θx+Pr(ηi|x̄)(1−θx) equals c·θx for some constant c ∈ (0, 1]. Since the value of Equation 2 does not depend on constant c, we will assume c = 1. Hence, when κix =∞, the term [κix · θx − θx + 1] evaluates to θx by convention.\nparameters are represented explicitly as nodes (Darwiche, 2009). In particular, for each conditional random variable Xu in the original Bayesian network, called the base network, we have a node θx|u in the meta network which represents a parameter that characterizes the distribution of this random variable. Moreover, the meta network includes enough instances of the base network to allow the assertion of each example di as evidence on one of these instances. Assuming that θ is an instantiation of all parameter variables, and D is a dataset, MAP estimates are then:\nθ? = argmax θ ρ(θ|D),\nwhere ρ is the density induced by the meta network.\nComputing MAP estimates exactly is usually prohibitive due to the structure of the meta network. We therefore use the technique of edge deletion (Choi & Darwiche, 2006), which formulates approximate inference as exact inference on a simplified network that is obtained by deleting edges from the original network. The technique compensates for these deletions by introducing auxiliary parameters whose values must be chosen carefully (and usually iteratively) in order to improve the quality of approximations obtained from the simplified network. EDML is the result of making a few specific choices for deleting edges and for choosing values for the auxiliary parameters introduced, which we explain next."
    }, {
      "heading" : "5.1 INTRODUCING GENERATORS",
      "text" : "Let Xi denote the instance of variable X in the base network corresponding to example di. The first choice\nof EDML is that for each edge θx|u−→Xi in the meta network, we introduce a generator variableXiu, leading to the pair of edges θx|u−→Xiu−→Xi. Figure 3(a) depicts a fragment of the meta network in Figure 2, in which we introduced two generator variables for edges θe|h−→E3 and θe|h̄−→E3, leading to θe|h−→E3h−→E3 and θe|h̄−→E3h̄−→E 3.\nVariable Xiu is meant to generate values of variable X i according to the distribution specified by parameter θx|u. Hence, the conditional distribution of a generator Xiu is such that Pr(x i u|θx|u) = θx|u. Moreover, the CPT of variable Xi is set to ensure that variable Xi copies the value of generator Xiu if and only if the parents of Xi take on the value u. That is, the CPT of variable Xi acts as a selector that chooses a particular generator Xiu to copy from, depending on the values of its parents U. For example, in Figure 3(a), when parent H3 takes on its positive value h, variable E3 copies the value of generator E3h. When parent H 3 takes on its negative value h̄, variable E3 copies the value of generator E3\nh̄ .\nAdding generator variables does not change the meta network as it continues to have the same density over the original variables. Yet, generators are essential to the derivation of EDML as they will be used for interpreting data examples as soft observations."
    }, {
      "heading" : "5.2 DELETING COPY EDGES",
      "text" : "The second choice made by EDML is that we only delete edges of the form Xiu−→Xi from the augmented meta network, which we shall call copy edges. Figure 3(b) depicts an example in which we have deleted\nthe two copy edges from Figure 3(a).\nNote here the addition of another auxiliary variable Xiu:, called a clone, for each generator X i u. The addition of clones is mandated by the edge deletion framework. Moreover, if the CPT of clone Xiu: is chosen carefully, it can compensate for the parent-to-child information lost when deleting edge Xiu−→Xi. We will later see how EDML sets these CPTs. The other aspect of compensating for a deleted edge is to specify soft evidence on each generator Xiu. This is also mandated by the edge deletion framework, and is meant to compensate for the child-to-parent information lost when deleting edge Xiu−→Xi. We will later see how EDML sets this soft evidence as well, which effectively completes the specification of the algorithm. We prelude this specification, however, by making some further observations about the structure of the meta network after edge deletion."
    }, {
      "heading" : "5.3 PARAMETER & EXAMPLE ISLANDS",
      "text" : "Consider the network in Figure 4, which is obtained from the meta network in Figure 2 according to the edge-deletion process indicated earlier.\nThe edge-deleted network contains a set of disconnected structures, called islands. Each island belongs to one of two classes: a parameter island for each network parameter θx|u and an example island for each example di in the dataset. Figure 4 provides the full details for one example island and one parameter island. Note that each parameter island corresponds to a Naive Bayes structure, with parameter θx|u as\nthe root and generators Xiu as children. When soft evidence is asserted on these generators, we get the estimation problem we treated in Section 4.\nEDML can now be fully described by specifying (1) the soft evidence on each generator Xiu in a parameter island, and (2) the CPT of each clone Xiu: in an example island. These specifications are given next."
    }, {
      "heading" : "5.4 CHILD-TO-PARENT COMPENSATION",
      "text" : "The edge deletion approach suggests the following soft evidence on generators Xiu, specified as Bayes factors:\nκix|u = O(xiu:|di) O(xiu:) = Pri(di|xiu:) Pri(di|x̄iu:) , (3)\nwhere Pri is the distribution induced by the island of example di. We will now show that this equation simplifies to Equation 1 of Algorithm 2.\nSuppose that we marginalize all clones Xiu: from the island of example di, leading to a network that induces a distribution Pr . The new network has the following properties. First, it has the same structure as the base network. Second, Pr(x|u) = Pri(xiu:), which means that the CPTs of clones in example islands correspond to parameters in the base network. Finally, if we use u to denote the disjunction of all parent instantiations excluding u, we get:\nκix|u = Pri(di|xiu:) Pri(di|x̄iu:)\n= Pr(di|xu)Pr(u) + Pr(di|u)Pr(u) Pr(di|x̄u)Pr(u) + Pr(di|u)Pr(u) = Pr(xu|di)/Pr(x|u)− Pr(u|di) + 1 Pr(x̄u|di)/Pr(x̄|u)− Pr(u|di) + 1 .\nThis is exactly Equation 1 of Algorithm 2. Hence, we can evaluate Equation 3 by evaluating Equation 1 on the base network, as long as we seed the base network with parameters that correspond to the CPTs of clones in an example island."
    }, {
      "heading" : "5.5 PARENT-TO-CHILD COMPENSATION",
      "text" : "We now complete the derivation of EDML by showing how it specifies the CPTs of clones in example islands, which are needed for computing soft evidence as in the previous section.\nIn a nutshell, EDML assumes an initial value of these CPTs, typically chosen randomly. Given these CPTs, example islands will be fully specified and EDML will compute soft evidence as given by Equation 3. The\ncomputed soft evidence is then injected on the generators of parameter islands, leading to a full specification of these islands. EDML will then estimate parameters by solving an exact optimization problem on each parameter island as shown in Section 4. The estimated parameters are then used as the new values of CPTs for clones in example islands. This process repeats until convergence.\nWe have shown in the previous section that the CPTs of clones are in one-to-one correspondence with the parameters of the base network. We have also shown that soft evidence, as given by Equation 3, can be computed by evaluating Equation 1 of Algorithm 2 (with parameters θ corresponding to the CPTs of clones in an example island). EDML takes advantage of this correspondence, leading to the simplified statement spelled out in Algorithm 2."
    }, {
      "heading" : "6 SOME PROPERTIES OF EDML",
      "text" : "Being an approximate inference method, one can sometimes identify good behaviors of EDML by identifying situations under which the underlying inference algorithm will produce high quality approximations. We provide a result in this section that illustrates this point in the extreme, where EDML is guaranteed to return optimal estimates and in only one iteration. Our result relies on the following observation about parameter estimation via inference on a meta network.\nWhen the parents U of a variable X are observed to u in an example di, all edges θx|u?−→Xi in the meta network become superfluous and can be pruned, except for the one edge that satisfies u? = u. Moreover,\nedges outgoing from observed nodes can also be pruned from a meta network. Suppose now that the parents of each variable are observed in a dataset. After pruning edges as indicated earlier, each parameter variable θx|u will end up being the root of an isolated naive Bayes structure that has some variables Xi as its children (those whose parents are instantiated to u in example di). Figure 5 depicts the result of such pruning in the meta network of Figure 2, given a dataset with H1 = h̄, H2 =h and H3 = h̄.\nThe above observation implies that when the parents of each variable are observed in a dataset, parameters can be estimated independently. This leads to the following well known result.\nProposition 1 When the dataset is complete, the ML estimate for parameter θx|u is unique and given by D#(xu)/D#(u), where D#(xu) is the number of examples containing xu and D#(u) is the number of examples containing u.\nIt is well known that EM returns such estimates and in only one iteration (i.e., independently of its seed). The following more general result is also implied by our earlier observation.\nProposition 2 When only leaf variables have missing values in a dataset, the ML estimate for each parameter θx|u is unique and given by D#(xu)/D+#(u). Here, D+#(u) is the number of examples containing u and in which X is observed.\nWe can now prove the following property of EDML, which is not satisfied by EM, as we show next.\nTheorem 1 When only leaf variables have missing values in a dataset, EDML returns the unique ML estimates given by Proposition 2 and in only one iteration.\nProof Consider an example di that fixes the values of parents U for variable X and consider Equation 1. First, κix|u = 1 iff example di is inconsistent with u or does not set the value of X. Next, κix|u = 0 iff example di contains x̄u. Finally, κ i x|u = ∞ iff example di contains xu. Moreover, these values are independent of the EDML seed so the algorithm converges in one iteration. Given these values of the Bayes factors, Equation 2 leads to the estimate of Proposition 2.\nWe have a number of observations about this result. First, since Proposition 1 is implied by Proposition 2, EDML returns the unique ML estimates in only one iteration when the dataset is complete (just like EM). Next, when only the values of leaf variables are missing in a dataset, Proposition 2 says that there is a unique ML estimate for each network parameter. Moreover,\nTheorem 1 says that EDML returns these unique estimates and in only one iteration. Finally, Theorem 1 does not hold for EM. In particular, one can show that under the conditions of this theorem, an EM iteration will update its current parameter estimates θ and return the following estimates for θx|u:\nD#(xu) +D−#(u)Pr(x|u) D#(u) .\nHere, D−#(u) is the number of examples that contain u and in which the value of X is missing. This next estimate clearly depends on the current parameter estimates. As a result, the behavior of EM will depend on its initial seed, unlike EDML.\nWhen only the values of leaf variables are missing, there is a unique optimal solution as shown by Proposition 2. Since EM is known to converge to a local optimum, it will eventually return the optimal estimates as well, but possibly after some number of iterations. In this case, the difference between EM and EDML is simply in the speed of convergence.\nTheorem 1 clearly suggests better convergence behavior of EDML over EM in some situations. We next present initial experiments supporting this suggestion."
    }, {
      "heading" : "7 MORE ON CONVERGENCE",
      "text" : "We highlight now a few empirical properties of EDML. In particular, we show how EDML can sometimes find higher quality estimates than EM, in fewer iterations and also in less time.\nWe highlight different types of relative convergence behavior in Figure 6, which depicts example runs on a selection of networks: spect, win95pts, emdec6g, and tcc4e. Network spect is a naive Bayes network induced from a dataset in the UCI ML repository, with 1 class variable and 22 attributes. Network win95pts (76 variables) is an expert system for printer troubleshooting in Windows 95. Networks emdec6g (168 variables) and tcc4e (98 variables) are noisy-or networks for diagnosis (courtesy of HRL Laboratories).\nWe simulated datasets of size 2k, using the original CPT parameters of the respective networks, and then used EDML and EM to learn new parameters for a network with the same structure. We assumed that certain variables were hidden (latent); in Figure 6, we randomly chose 14 of the variables to be hidden. Hidden nodes are of particular interest to EM, because it has been observed that local extrema and convergence rates can be problematic for EM here; see, for example (Elidan & Friedman, 2005; Salakhutdinov, Roweis, & Ghahramani, 2003).\nIn Figure 6, each plot represents a simulated data set of size 210, where EDML and EM have been initialized with the same random parameter seeds. Both algorithms were run for a fixed number of iterations, 1024 in this case, and we observed the quality of the parameter estimates found, with respect to the log posterior probability (which has been normalized so that the maximum log probability observed is 0.0). We assumed a Beta prior with exponents 2. EDML damped its parameter updates by a factor of 12 , which is typical for (loopy) belief propagation algorithms.3\n3The simple bisection method suffices for the optimization sub-problem in EDML for binary Bayesian networks. In our current implementation, we used the conjugate gradient method, with a convergence threshold of 10−8.\nIn the left column of Figure 6, we evaluated the quality of estimates over iterations of EDML and EM. In these examples, EDML (represented by a solid red line) tended to have better quality estimates from iteration to iteration (curves that are higher are better), and further managed to find them in fewer iterations (curves to the left are faster).4 This is most dramatic in network spect, where EDML appears to have converged almost immediately, whereas EM spent a significant number of iterations to reach estimates of comparable quality. As most nodes hidden in network spect were leaf nodes, this may be expected due to the considerations from the previous section.\nIn the right column of Figure 6, we evaluated the quality of estimates, now in terms of time. We remark again that procedurally, EDML and EM are very similar, and each algorithm needs only one evaluation of the jointree algorithm per distinct example in the data set (per iteration). EDML solves an optimization problem per distinct example, whereas EM has a closed-form update equation in the corresponding step (Line 4 in Algorithms 1 and 2). Although this optimization problem is a simple one, EDML does require more time per iteration than EM. The right column of Figure 6 suggests that EDML can still find better estimates faster, especially in the cases where EDML has converged in significantly fewer iterations. In network emdec6g, we find that although EDML appeared to converge in fewer iterations, EM was able to find better estimates in less time. We anticipate in larger networks with higher treewidth, the time spent in the simple optimization sub-problem will be dominated by the time to perform jointree propagation.\nWe also performed experiments on networks learned from binary haplotype data (Elidan & Gould, 2008), which are networks with bounded treewidth. Here, we simulated data sets of size 210, where we again randomly selected 14 of the variables to be hidden. We further ran EDML and EM for a fixed number of iterations (512, here). For each of the 74 networks available, we ran EDML and EM with 3 random seeds, for a total of 222 cases. In Figure 7, we highlight a selection of the runs we performed, to illustrate examples of relative convergence behaviors. Again, in the first row, we see a case where EDML identifies better estimates in fewer iterations and less time. In the next two rows, we highlight two cases where EDML appears to converge to a superior fixed point than the one that EM appears to converge to. In the last row, we highlight an instance where EM instead converges to a superior estimate. In Figure 8, we compare the estimates of\n4We omit the results of the first 10 iterations as initial parameter estimates are relatively poor, which make the plots difficult to read.\nEDML and EM at each iteration, computing the percentage of the 74 × 3 = 222 cases considered, where EDML had estimates no worse than those found by EM. In this set of experiments, the estimates identified by EDML are clearly superior (or at least, no worse in most cases), when compared to EM.\nWe remark however, that when both algorithms are given enough iterations to converge, we have observed that the quality of the estimates found by both algorithms are often comparable. This is evident in Figure 6, for example. The analysis from the previous section indicates however that there are (very specialized) situations where EDML would be clearly preferred over EM. One subject of future study is the identification of situations and applications where EDML\nwould be preferred in practice as well."
    }, {
      "heading" : "8 FUTURE AND RELATED WORK",
      "text" : "EM has played a critical role in learning probabilistic graphical models and Bayesian networks (Dempster et al., 1977; Lauritzen, 1995; Heckerman, 1998). However learning (and Bayesian learning in particular) remains challenging in a variety of situations, particularly when there are hidden (latent) variables; see, e.g., (Elidan, Ninio, Friedman, & Shuurmans, 2002; Elidan & Friedman, 2005). Slow convergence of EM has also been recognized, particularly in the presence of hidden variables. A variety of techniques, some incorporating more traditional approaches to optimization, have been proposed in the literature; see, e.g., (Thiesson, Meek, & Heckerman, 2001).\nVariational approaches are an increasingly popular formalism for learning tasks as well, and for topic models in particular, where variational alternatives to EM are used to maximize a lower bound on the log likelihood (Blei, Ng, & Jordan, 2003). Expectation Propagation also provides variations of EM (Minka & Lafferty, 2002) and is closely related to (loopy) belief propagation (Minka, 2001).\nOur empirical results have been restricted to a preliminary investigation of the convergence of EDML, in contrast to EM. A more comprehensive evaluation is called for in relation to both EM and other approaches based on Bayesian inference. We have also focused this paper on binary variables. EDML, however, general-\nizes to multivalued variables since edge deletion does not require a restriction to binary variables and the key result of Section 4 also generalizes to multivalued variables. The resulting formulation is less transparent though when compared to the binary case since Bayes factors no longer apply directly and one must appeal to a more complex method for quantifying soft evidence; see (Chan & Darwiche, 2005). We expect our future work to focus on a more comprehensive empirical evaluation of EDML, in the context of an implementation that uses multivalued variables. Moreover, we seek to identify additional properties of EDML that go beyond convergence."
    } ],
    "references" : [ {
      "title" : "On the revision of probabilistic beliefs using uncertain evidence",
      "author" : [ "H. Chan", "A. Darwiche" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "Chan and Darwiche,? \\Q2005\\E",
      "shortCiteRegEx" : "Chan and Darwiche",
      "year" : 2005
    }, {
      "title" : "An edge deletion semantics for belief propagation and its practical impact on approximation quality",
      "author" : [ "A. Choi", "A. Darwiche" ],
      "venue" : "In AAAI,",
      "citeRegEx" : "Choi and Darwiche,? \\Q2006\\E",
      "shortCiteRegEx" : "Choi and Darwiche",
      "year" : 2006
    }, {
      "title" : "Modeling and Reasoning with Bayesian Networks",
      "author" : [ "A. Darwiche" ],
      "venue" : null,
      "citeRegEx" : "Darwiche,? \\Q2009\\E",
      "shortCiteRegEx" : "Darwiche",
      "year" : 2009
    }, {
      "title" : "Maximum likelihood from incomplete data via the EM algorithm",
      "author" : [ "A. Dempster", "N. Laird", "D. Rubin" ],
      "venue" : "Journal of the Royal Statistical Society B,",
      "citeRegEx" : "Dempster et al\\.,? \\Q1977\\E",
      "shortCiteRegEx" : "Dempster et al\\.",
      "year" : 1977
    }, {
      "title" : "Learning hidden variable networks: The information bottleneck approach",
      "author" : [ "G. Elidan", "N. Friedman" ],
      "venue" : null,
      "citeRegEx" : "Elidan and Friedman,? \\Q2005\\E",
      "shortCiteRegEx" : "Elidan and Friedman",
      "year" : 2005
    }, {
      "title" : "Learning bounded treewidth",
      "author" : [ "G. Elidan", "S. Gould" ],
      "venue" : "Bayesian networks. JMLR,",
      "citeRegEx" : "Elidan and Gould,? \\Q2008\\E",
      "shortCiteRegEx" : "Elidan and Gould",
      "year" : 2008
    }, {
      "title" : "Data perturbation for escaping local maxima in learning",
      "author" : [ "G. Elidan", "M. Ninio", "N. Friedman", "D. Shuurmans" ],
      "venue" : "In AAAI/IAAI,",
      "citeRegEx" : "Elidan et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Elidan et al\\.",
      "year" : 2002
    }, {
      "title" : "Probability and the Weighing of Evidence",
      "author" : [ "I.J. Good" ],
      "venue" : null,
      "citeRegEx" : "Good,? \\Q1950\\E",
      "shortCiteRegEx" : "Good",
      "year" : 1950
    }, {
      "title" : "A tutorial on learning with Bayesian networks",
      "author" : [ "D. Heckerman" ],
      "venue" : "Learning in Graphical Models,",
      "citeRegEx" : "Heckerman,? \\Q1998\\E",
      "shortCiteRegEx" : "Heckerman",
      "year" : 1998
    }, {
      "title" : "Probabilistic Graphical Models: Principles and Techniques",
      "author" : [ "D. Koller", "N. Friedman" ],
      "venue" : null,
      "citeRegEx" : "Koller and Friedman,? \\Q2009\\E",
      "shortCiteRegEx" : "Koller and Friedman",
      "year" : 2009
    }, {
      "title" : "The EM algorithm for graphical association models with missing data",
      "author" : [ "S. Lauritzen" ],
      "venue" : "Computational Statistics and Data Analysis,",
      "citeRegEx" : "Lauritzen,? \\Q1995\\E",
      "shortCiteRegEx" : "Lauritzen",
      "year" : 1995
    }, {
      "title" : "Expectation propagation for approximate Bayesian inference",
      "author" : [ "T.P. Minka" ],
      "venue" : "In UAI,",
      "citeRegEx" : "Minka,? \\Q2001\\E",
      "shortCiteRegEx" : "Minka",
      "year" : 2001
    }, {
      "title" : "Expectationpropogation for the generative aspect model",
      "author" : [ "T.P. Minka", "J.D. Lafferty" ],
      "venue" : "In UAI,",
      "citeRegEx" : "Minka and Lafferty,? \\Q2002\\E",
      "shortCiteRegEx" : "Minka and Lafferty",
      "year" : 2002
    }, {
      "title" : "Optimization with EM and expectation-conjugategradient",
      "author" : [ "R. Salakhutdinov", "S.T. Roweis", "Z. Ghahramani" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Salakhutdinov et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Salakhutdinov et al\\.",
      "year" : 2003
    }, {
      "title" : "Accelerating EM for large databases",
      "author" : [ "B. Thiesson", "C. Meek", "D. Heckerman" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Thiesson et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Thiesson et al\\.",
      "year" : 2001
    } ],
    "referenceMentions" : [ {
      "referenceID" : 10,
      "context" : "We propose a specific method, EDML, which has a similar structure and complexity to the EM algorithm (Dempster, Laird, & Rubin, 1977; Lauritzen, 1995).",
      "startOffset" : 101,
      "endOffset" : 150
    }, {
      "referenceID" : 2,
      "context" : "See (Darwiche, 2009; Koller & Friedman, 2009) for recent treatments on parameter learning in Bayesian networks via EM and related methods.",
      "startOffset" : 4,
      "endOffset" : 45
    }, {
      "referenceID" : 2,
      "context" : ", (Darwiche, 2009)):",
      "startOffset" : 2,
      "endOffset" : 18
    }, {
      "referenceID" : 7,
      "context" : "Our method for learning MAP and ML parameters makes heavy use of two notions: (1) the odds of an event, which is the probability of the event over the probability of its negation, and (2) the Bayes factor (Good, 1950), which is the relative change in the odds of one event, say, X=x, due to observing some other event, say, η.",
      "startOffset" : 205,
      "endOffset" : 217
    }, {
      "referenceID" : 2,
      "context" : "parameters are represented explicitly as nodes (Darwiche, 2009).",
      "startOffset" : 47,
      "endOffset" : 63
    }, {
      "referenceID" : 3,
      "context" : "EM has played a critical role in learning probabilistic graphical models and Bayesian networks (Dempster et al., 1977; Lauritzen, 1995; Heckerman, 1998).",
      "startOffset" : 95,
      "endOffset" : 152
    }, {
      "referenceID" : 10,
      "context" : "EM has played a critical role in learning probabilistic graphical models and Bayesian networks (Dempster et al., 1977; Lauritzen, 1995; Heckerman, 1998).",
      "startOffset" : 95,
      "endOffset" : 152
    }, {
      "referenceID" : 8,
      "context" : "EM has played a critical role in learning probabilistic graphical models and Bayesian networks (Dempster et al., 1977; Lauritzen, 1995; Heckerman, 1998).",
      "startOffset" : 95,
      "endOffset" : 152
    }, {
      "referenceID" : 11,
      "context" : "Expectation Propagation also provides variations of EM (Minka & Lafferty, 2002) and is closely related to (loopy) belief propagation (Minka, 2001).",
      "startOffset" : 133,
      "endOffset" : 146
    } ],
    "year" : 2011,
    "abstractText" : "We propose a method called EDML for learning MAP parameters in binary Bayesian networks under incomplete data. The method assumes Beta priors and can be used to learn maximum likelihood parameters when the priors are uninformative. EDML exhibits interesting behaviors, especially when compared to EM. We introduce EDML, explain its origin, and study some of its properties both analytically and empirically.",
    "creator" : "TeX"
  }
}