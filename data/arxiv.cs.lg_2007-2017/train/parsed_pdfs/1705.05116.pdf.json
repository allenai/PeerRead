{
  "name" : "1705.05116.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Tuning Modular Networks with Weighted Losses for Hand-Eye Coordination",
    "authors" : [ "Fangyi Zhang", "Jürgen Leitner", "Michael Milford", "Peter I. Corke" ],
    "emails" : [ "fangyi.zhang@hdr.qut.edu.au" ],
    "sections" : [ {
      "heading" : null,
      "text" : "to improve hand-eye coordination in modular deep visuomotor policies (modular networks) where each module is trained independently. Benefiting from weighted losses, the fine-tuning method significantly improves the performance of the policies for a robotic planar reaching task."
    }, {
      "heading" : "1. Introduction",
      "text" : "Recent work has demonstrated robotic tasks based directly on real image data using deep learning, for example robotic grasping [2]. However these methods require largescale real-world datasets, which are expensive, slow to acquire and limit the general applicability of the approach.\nTo reduce the cost of real dataset collection, we used simulation to learn robotic planar reaching skills using the DeepMind DQN [3]. The DQN showed impressive results in simulation, but exhibited brittleness when transferred to a real robot and camera [4]. By introducing a bottleneck to separate the DQN into perception and control modules for independent training, the skills learned in simulation (Fig. 1A) were easily adapted to real scenarios (Fig. 1B) by using just 1418 real-world images [5].\nHowever, there is still a performance drop compared to the control module network with ideal perception. To reduce the performance drop, we propose fine-tuning the combined network to improve hand-eye coordination. Preliminary studies show that a naive fine-tuning using Qlearning does not give the desired result [5]. To tackle the problem, we introduce a novel end-to-end fine-tuning method using weighted losses in this work, which significantly improved the performance of the combined network."
    }, {
      "heading" : "2. Methodology",
      "text" : "We consider the planar reaching task, which is defined as controlling a 3 DoF robot arm (Baxter robot’s left arm) so that in operational space its end-effector position x ∈ R2 moves to the position of the target x∗ in a vertical plane (ignoring orientation). The reaching controller adjusts the robot configuration (joint angles q ∈ R3) to minimize the ∗FZ, JL, MM, PIC are with the Australian Centre for Robotic Vision (ACRV), Queensland University of Technology (QUT), Brisbane, Australia. fangyi.zhang@hdr.qut.edu.au †This research was conducted by the Australian Research Council Centre of Excellence for Robotic Vision (project number CE140100016). Additional computational resources and services were provided by the HPC and Research Support Group at QUT.\nerror between the robot’s current and target position, i.e., ‖x− x∗‖. At each time step 1 of 9 possible actions a ∈ a is chosen to change the robot configuration: 3 per joint – increasing or decreasing by a constant amount (0.04 rad) or leaving it unchanged. An agent is required to learn to reach using only raw-pixel visual inputs I from a monocular camera and their accompanying rewards r.\nThe network has the same architecture and training method to [5], but with an additional end-to-end fine-tuning using weighted losses, as shown in Fig. 2. The perception network is first trained to estimate the scene configuration Θ = [x∗ q] ∈ R5 from a raw-pixel image I using the quadratic loss function\nLp = 1\n2m m∑ j=1 ∥∥y(Ij)−Θj∥∥2 ,\nar X\niv :1\n70 5.\n05 11\n6v 1\n[ cs\n.R O\n] 1\n5 M\nay 2\n01 7\nwhere y(Ij) is the prediction of Θj for Ij ; m is the number of samples. The control network is trained using KGPS [5] where network weights are updated using the Bellman equation which is equivalent to the loss function\nLq = 1\n2m m∑ j=1 ∥∥∥∥∥Q(Θjt , ajt )− (rjt + γmaxajt+1 Q(Θjt+1, ajt+1)) ∥∥∥∥∥ 2\nwhere Q(Θjt , a j t ) is the sum of future expected rewards∑∞\nk=0 γ krjt+k when taking action a j t in state Θ j t . γ is a discount factor applied to future rewards. After separate training for perception and control individually, an end-to-end fine-tuning is conducted for the combined network (perception + control) using weighted task (Lq) and perception (Lp) losses. The control network is updated using only Lq , while the perception network is updated using the weighted loss\nL = βLp + (1− β)LBNq ,\nwhere LBNq is a pseudo-loss which reflects the loss of Lq in the bottleneck (BN); β ∈ [0, 1] is a balancing weight. From the backpropagation algorithm [1], we can infer that δL = βδLp + (1 − β)δLBNq , where δL is the gradients resulted by L; δLp and δLBNq are the gradients resulting respectively from Lp and LBNq (equivalent to that resulting from Lq in the perception module)."
    }, {
      "heading" : "3. Experiments and Results",
      "text" : "We evaluated the feasibility of the proposed approach using the metrics of Euclidean distance error d (between the end-effector and target) and average accumulated reward R̄ (a bigger accumulated reward means a faster and closer reaching to a target) in 400 simulated trials. For comparison, we evaluated three networks: Initial, Fine-tuned and CR. Initial is a combined network without end-to-end finetuning, which is labelled as EE2 in [5] (comprising FT75 and CR). FT75 and CR are the selected perception and control modules which have the best performance individually. Fine-tuned is obtained by fine-tuning Initial using the proposed approach. CR works as a baseline indicating performance upper-limit.\nIn fine-tuning, β = 0.8, we used a learning rate between 0.01 and 0.001, a mini-batch size of 64 and 256 for task and perception losses respectively, and an exploration possibility of 0.1 for K-GPS. These parameters were empirically selected. To make sure that the perception module remembers the skills for both simulated and real scenarios, the 1418 real samples were also used to obtain δLp . Similar to FT75, 75% samples in a mini-batch were from real scenarios, i.e., at each weight updating step, 192 extra real samples were used in addition to the 64 simulated samples in the mini-batch for δLq .\nResults are summarized in Fig. 3 and Table 1. dmed and dQ3 are the median and third quartile of d. The error distance in pixels in the 84 × 84 input image is also listed. We can see that Fine-tuned achieved a much better performance (22.4% smaller dmed and 96.2% bigger R̄) than Initial. The fine-tuned performance is even very close to that of the control module (CR) which controls the arm using ground-truth Θ as sensing inputs. We also did the same evaluations in 20 real-world trials on Baxter, and achieved similar results.\nThe experimental results show the feasibility of the proposed fine-tuning approach. Improved hand-eye coordination in modular deep visuo-motor policies is possible due to fine-tuning with weighted losses. The adaptation to real scenarios can still be kept by presenting (a mix of simulated and) real samples to compute the perception loss."
    } ],
    "references" : [ {
      "title" : "A theoretical framework for back-propagation",
      "author" : [ "Y. LeCun" ],
      "venue" : "Proceedings of the 1988 Connectionist Models Summer School,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1988
    }, {
      "title" : "Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection",
      "author" : [ "S. Levine", "P.P. Sampedro", "A. Krizhevsky", "D. Quillen" ],
      "venue" : "In International Symposium on Experimental Robotics (ISER),",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2016
    }, {
      "title" : "Human-level control through deep reinforcement learning",
      "author" : [ "V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski" ],
      "venue" : "Nature, 518(7540):529–533,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2015
    }, {
      "title" : "Towards vision-based deep reinforcement learning for robotic motion control",
      "author" : [ "F. Zhang", "J. Leitner", "M. Milford", "B. Upcroft", "P. Corke" ],
      "venue" : "In Australasian Conference on Robotics and Automation (ACRA),",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2015
    }, {
      "title" : "Transferring visionbased robotic reaching skills from simulation to real world",
      "author" : [ "F. Zhang", "J. Leitner", "B. Upcroft", "P. Corke" ],
      "venue" : "Technical report,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2017
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "Recent work has demonstrated robotic tasks based directly on real image data using deep learning, for example robotic grasping [2].",
      "startOffset" : 127,
      "endOffset" : 130
    }, {
      "referenceID" : 2,
      "context" : "To reduce the cost of real dataset collection, we used simulation to learn robotic planar reaching skills using the DeepMind DQN [3].",
      "startOffset" : 129,
      "endOffset" : 132
    }, {
      "referenceID" : 3,
      "context" : "The DQN showed impressive results in simulation, but exhibited brittleness when transferred to a real robot and camera [4].",
      "startOffset" : 119,
      "endOffset" : 122
    }, {
      "referenceID" : 4,
      "context" : "1B) by using just 1418 real-world images [5].",
      "startOffset" : 41,
      "endOffset" : 44
    }, {
      "referenceID" : 4,
      "context" : "Preliminary studies show that a naive fine-tuning using Qlearning does not give the desired result [5].",
      "startOffset" : 99,
      "endOffset" : 102
    }, {
      "referenceID" : 4,
      "context" : "The architecture is similar to that in [5], but has an additional end-to-end fine-tuning process using weighted perception and task losses.",
      "startOffset" : 39,
      "endOffset" : 42
    }, {
      "referenceID" : 0,
      "context" : "Note that the values in Θ are normalized to the interval [0, 1].",
      "startOffset" : 57,
      "endOffset" : 63
    }, {
      "referenceID" : 4,
      "context" : "The network has the same architecture and training method to [5], but with an additional end-to-end fine-tuning using weighted losses, as shown in Fig.",
      "startOffset" : 61,
      "endOffset" : 64
    }, {
      "referenceID" : 4,
      "context" : "The control network is trained using KGPS [5] where network weights are updated using the Bellman equation which is equivalent to the loss function",
      "startOffset" : 42,
      "endOffset" : 45
    }, {
      "referenceID" : 0,
      "context" : "The control network is updated using only Lq , while the perception network is updated using the weighted loss L = βLp + (1− β)L q , where L q is a pseudo-loss which reflects the loss of Lq in the bottleneck (BN); β ∈ [0, 1] is a balancing weight.",
      "startOffset" : 218,
      "endOffset" : 224
    }, {
      "referenceID" : 0,
      "context" : "From the backpropagation algorithm [1], we can infer that δL = βδLp + (1 − β)δLBN q , where δL is the gradients resulted by L; δLp and δLBN q are the gradients resulting respectively from Lp and L q (equivalent to that resulting from Lq in the perception module).",
      "startOffset" : 35,
      "endOffset" : 38
    }, {
      "referenceID" : 4,
      "context" : "Initial is a combined network without end-to-end finetuning, which is labelled as EE2 in [5] (comprising FT75 and CR).",
      "startOffset" : 89,
      "endOffset" : 92
    } ],
    "year" : 2017,
    "abstractText" : "This paper introduces an end-to-end fine-tuning method to improve hand-eye coordination in modular deep visuomotor policies (modular networks) where each module is trained independently. Benefiting from weighted losses, the fine-tuning method significantly improves the performance of the policies for a robotic planar reaching task.",
    "creator" : "LaTeX with hyperref package"
  }
}