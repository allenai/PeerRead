{
  "name" : "1605.01622.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Improving abcdSAT by At-Least-One Recently Used Clause Management Strategy",
    "authors" : [ "Jingchao Chen" ],
    "emails" : [ "chen-jc@dhu.edu.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n60 5.\n01 62\n2v 1\n[ cs\n.L O\n] 5\nM ay\n2 01\nI. INTRODUCTION\nThe abcdSAT solver submitted to the SAT Competition 2016 is the improved version of abcdSAT 2015 [1], which are built on the top of Glucose 2.3 [6], [7]. Here we provide three versions of abcdSAT: drup, inc and lim, which are submitted to main (agile) track, incremental library track and no-limit track, respectively. The main techniques use by the three versions include at-least-one recently used strategy, learnt clause database approximation reduction, recursive splitting solving, decision variable selection based on blocked clause decomposition [2], [3], bit-encoding phase selection [4], simplification such as lifting, probing, distillation, elimination, hyper binary resolution etc. Of course, all the simplification techniques used here are the existing techniques, so we will omit the description on them."
    }, {
      "heading" : "II. AT-LEAST-ONE RECENTLY USED STRATEGY",
      "text" : "In the search process of CDCL (Conflict Driven, Clause Learning) solvers, the learnt clause database is required to be maintained. Based on our experimental observation, the clause database maintenance is actually similar to cache replacement in CPU cache management or page replacement in a computer operating system. There are many cache (page) replacement algorithms. For example, Least Recently Used (LRU), Most Recently Used (MRU), Pseudo-LRU (PLRU), Least-Frequently Used (LFU), Second Chance FIFO, Random Replacement (RR), Not Recently Used (NRU) [9] etc. Our At-Least-One Recently Used (ALORU) algorithm is similar to NRU page replacement algorithm, but different from the clause freezing mechanism proposed by Audemard et al [8]. ALORU favours keeping learnt clauses in database that have been recently used at least one time. If a learnt clause has not so far involved in any conflict analysis since it was generated, it will be discarded first. Implementing ALORU is very simple. When a conflict clause (called also learnt clause) is generated, its LBD (Literal Block Distance, for its definition, see [7]) is usually set to the number of different decision levels involved\nin it. However, ALORU sets the initial LBD of a conflict clause to +∞, not actual current LBD. In details, in the search procedure, ALORU replaces “setLBD(nblevels)” with “setLBD(0x3fffffff)”. Since any LBD in real instances that can be solved never exceeds 0x3fffffff, we denote +∞ with 0x3fffffff. If a learnt clause involves in a conflict analysis, the procedure for conflict analysis sets its LBD to the actual value."
    }, {
      "heading" : "III. LEARNT CLAUSE DATABASE APPROXIMATION REDUCTION",
      "text" : "The target of learnt clause database reduction is two fold: remove useless clauses and avoid the expansion of database. Almost all the existing reduction algorithms in CDCL solvers are to sort learnt clauses according to the score (e.g. LBD) of clauses, then remove a given number of clauses in the sorted order. This can be viewed as exact reduction. Our approximation reduction is different from the exact reduction. It has no sorting, and replaces sorting with selection. In details, our approximation reduction finds firstly the clause with the k-th smallest (or largest) score, where k is the number of clauses to be removed. Secondly, it removes k clauses with the score less than or equal to the k-th smallest score. Notice, the clauses with the score equal to that of the k-th clause are not often unique. And the clauses with the score less than to the k-th smallest score are not necessarily removed. Therefore, the parameter k is an approximation value or estimate, not exact. Due to this reason, we call reduction implemented by finding the k-th item approximation reduction. Here we choose QUICKSELECT or Hoare’s FIND algorithm [10] to find the k-th item.\nIf all database reductions are done in this approximation way, solving is not the most efficient. Therefore, we apply the approximation reduction when the number of conflicts is larger than 300000 for special CNF instances. In the other cases, we apply still the exact approximation."
    }, {
      "heading" : "IV. DYNAMIC CORE AND LOCAL LEARNT CLAUSE MANAGEMENT",
      "text" : "Like SWDiA5BY [11], glue alt classifies also learnt clauses into two categories: core and local. However, the classification of SWDiA5BY is static, while our classification is dynamic. In SWDiA5BY, the maximum LBD of core learnt clauses is fixed to a constant 5. However, in abcdSAT, the maximum LBD of core learnt clauses is not fixed. AbcdSAT divides\nthe whole search process two stages. When the number of conflicts is less than 2×106, it is considered as the first stage. Otherwise, it is considered as the second stage. In the first stage, the maximum LBD of core learnt clauses is limited to 2. At this stage, core learnt clauses are kept indefinitely, unless eliminated when they are satisfied. In the second stage, the maximum LBD of core learnt clauses is limited to 5. This stage does not ensure that core learnt clauses are kept indefinitely. When local learnt clause database is reduced, we move 5000 core learnt clauses with LBD larger than or equal to 3 to local learnt clause database.\nWhether the first or second stage, the number of local learnt clauses is maintained roughly between 9000 and 24000. That is, once the number of local learnt clauses reaches a upper bound, say 18000, abcdSAT will halve the number of the clauses. And the clauses with the smallest activity scores are removed first. The computation of clause activity scores here is consistent with MiniSat."
    }, {
      "heading" : "V. RECURSIVE SPLITTING SOLVING",
      "text" : "Any CNF formula F can be split into two subproblems F ∪ {x} and F ∪ {¬x}, where x is a variable in F . We can obtain the solution the original problem by solving each subproblem. Solving subproblem in the same way results in a recursive solving algorithm. In general, we limit recursive depth to 10. Here is the pseudo-code of this recursive solving framework.\nAlgorithm SplitSolve(F , level) if level ≥ 10 then return abcdSAT(F , 2× 106) 〈ret,F ′〉 ← abcdSAT(F , 500) if ret = SAT or UNSAT then return ret x ← GetBranchVariable(F ′) SplitSolve(F ′ ∪ {x}, level + 1) SplitSolve(F ′ ∪ {¬x}, level + 1)\nThe 2nd parameter of abcdSAT in the above algorithm denotes the limit of the number of conflicts. abcdSAT(F , 500) means that it searches a solution of F until the number of conflicts reaches 500. Procedure GetBranchVariable selects a branch variable according to the rule given in [12].\nThis solving framework is suitable for small formulas.\nVI. ABCDSAT drug\nBecause each solver participating in the main track is required to provide a DRUP proof in UNSAT case, we add a DRUP patch in the original abcdSAT. In addition to this patch, abcdSAT drug adds learnt clause database approximation reduction, at-least-one recently used strategy, but excludes XOR and cardinality constraint simplification. In particular, XOR simplification is difficult to provide a DRUP proof. The splitting and merging technique used in the original abcdSAT cannot provide a DRUP proof. So abcdSAT drug simplifies it into recursive splitting solving technique given in previous section.\nVII. ABCDSAT inc\nThe solver submitted to the incremental library track is called abcdSAT inc. This version has no DRUP patch. The main difference between abcdSAT inc and abcdSAT drug is that abcdSAT inc adopts dynamic core and local learnt clause management policy, while abcdSAT drug adopts Glucose-style learnt clause management policy.\nVIII. ABCDSAT lim\nThis is the version submitted to the no-limit track. AbcdSAT lim not only includes various techniques given above, but also XOR and cardinality constraint simplification. With respect to learnt clause management, what abcdSAT lim adopts is Glucose-style learnt clause management policy. For a few special instances, abcdSAT lim switches to Lingeling 587f [13] to solve them. When the average LBD score of an instance to be solved is small, say less than 16, this version uses splitting and merging (reconstructing) strategy described in [5], rather than recursive splitting solving strategy mentioned above."
    } ],
    "references" : [ {
      "title" : "MiniSAT BCD and abcdSAT: solvers based on blocked clause decomposition, in Proceedings of the SAT Competition",
      "author" : [ "J.C. Chen" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2015
    }, {
      "title" : "Fast Blocked Clause Decomposition with High Quality, 2015",
      "author" : [ "J.C. Chen" ],
      "venue" : null,
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2015
    }, {
      "title" : "Improving SAT Solvers via Blocked Clause Decomposition, 2016",
      "author" : [ "J.C. Chen" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2016
    }, {
      "title" : "bit-encoding phase selection strategy for satisfiability solvers,in Proceedings of Theory and Applications of Models of Computation (TAMC’14)",
      "author" : [ "J.C. Chen:A" ],
      "venue" : "ser. LNCS 8402,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2014
    }, {
      "title" : "Glue lgl split and GlueSplit clasp with a Split and Merging Strategy, in Proceedings of the SAT Competition",
      "author" : [ "J.C. Chen" ],
      "venue" : null,
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2014
    }, {
      "title" : "Glucose 2.3 in the sat 2013 competition,in",
      "author" : [ "G. Audemard", "L. Simon" ],
      "venue" : "Proceedings of the SAT Competition",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2013
    }, {
      "title" : "Simon:Predicting learnt clauses quality in modern sat solvers",
      "author" : [ "L.G. Audemard" ],
      "venue" : "in proceedings of IJCAI,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2009
    }, {
      "title" : "Saı̈s:On freezing and reactivating learnt clauses",
      "author" : [ "G. Audemard", "J.M. Lagniez", "L.B. Mazure" ],
      "venue" : "Proceedings of SAT 2011, ser. LNCS,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2011
    }, {
      "title" : "Chawan: A comparison of page replacement algorithms",
      "author" : [ "Amit S. Chavan", "Kartik R. Nayak", "Keval D. Vora", "Manish D. Purohit", "Pramila M" ],
      "venue" : "IACSIT, vol.3,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2011
    }, {
      "title" : "Algorithm 63 (PARTITION), Algorithm 64 (QUICKSORT) and Algorithm 65 (FIND)",
      "author" : [ "C. Hoare" ],
      "venue" : "Communications of the ACM",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1961
    }, {
      "title" : "MiniSat HACK 999ED, MiniSat HACK 1430ED, and SWDiA5BY",
      "author" : [ "Oh" ],
      "venue" : "Proceedings of the SAT Competition",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2014
    }, {
      "title" : "Building a hybrid sat solver via conflict-driven, look-ahead and xor reasoning techniques,in",
      "author" : [ "J.C. Chen" ],
      "venue" : "Proceedings of SAT 2009, ser. LNCS,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "The abcdSAT solver submitted to the SAT Competition 2016 is the improved version of abcdSAT 2015 [1], which are built on the top of Glucose 2.",
      "startOffset" : 97,
      "endOffset" : 100
    }, {
      "referenceID" : 5,
      "context" : "3 [6], [7].",
      "startOffset" : 2,
      "endOffset" : 5
    }, {
      "referenceID" : 6,
      "context" : "3 [6], [7].",
      "startOffset" : 7,
      "endOffset" : 10
    }, {
      "referenceID" : 1,
      "context" : "The main techniques use by the three versions include at-least-one recently used strategy, learnt clause database approximation reduction, recursive splitting solving, decision variable selection based on blocked clause decomposition [2], [3], bit-encoding phase selection [4], simplification such as lifting, probing, distillation, elimination, hyper binary resolution etc.",
      "startOffset" : 234,
      "endOffset" : 237
    }, {
      "referenceID" : 2,
      "context" : "The main techniques use by the three versions include at-least-one recently used strategy, learnt clause database approximation reduction, recursive splitting solving, decision variable selection based on blocked clause decomposition [2], [3], bit-encoding phase selection [4], simplification such as lifting, probing, distillation, elimination, hyper binary resolution etc.",
      "startOffset" : 239,
      "endOffset" : 242
    }, {
      "referenceID" : 3,
      "context" : "The main techniques use by the three versions include at-least-one recently used strategy, learnt clause database approximation reduction, recursive splitting solving, decision variable selection based on blocked clause decomposition [2], [3], bit-encoding phase selection [4], simplification such as lifting, probing, distillation, elimination, hyper binary resolution etc.",
      "startOffset" : 273,
      "endOffset" : 276
    }, {
      "referenceID" : 8,
      "context" : "For example, Least Recently Used (LRU), Most Recently Used (MRU), Pseudo-LRU (PLRU), Least-Frequently Used (LFU), Second Chance FIFO, Random Replacement (RR), Not Recently Used (NRU) [9] etc.",
      "startOffset" : 183,
      "endOffset" : 186
    }, {
      "referenceID" : 7,
      "context" : "Our At-Least-One Recently Used (ALORU) algorithm is similar to NRU page replacement algorithm, but different from the clause freezing mechanism proposed by Audemard et al [8].",
      "startOffset" : 171,
      "endOffset" : 174
    }, {
      "referenceID" : 6,
      "context" : "When a conflict clause (called also learnt clause) is generated, its LBD (Literal Block Distance, for its definition, see [7]) is usually set to the number of different decision levels involved in it.",
      "startOffset" : 122,
      "endOffset" : 125
    }, {
      "referenceID" : 9,
      "context" : "Here we choose QUICKSELECT or Hoare’s FIND algorithm [10] to find the k-th item.",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 10,
      "context" : "Like SWDiA5BY [11], glue alt classifies also learnt clauses into two categories: core and local.",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 11,
      "context" : "Procedure GetBranchVariable selects a branch variable according to the rule given in [12].",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 4,
      "context" : "When the average LBD score of an instance to be solved is small, say less than 16, this version uses splitting and merging (reconstructing) strategy described in [5], rather than recursive splitting solving strategy mentioned above.",
      "startOffset" : 162,
      "endOffset" : 165
    } ],
    "year" : 2016,
    "abstractText" : "We improve further the 2015 version of abcdSAT by various heuristics such as at-least-one recently used strategy, learnt clause database approximation reduction etc. Based on the requirement of different tracks at the SAT Competition 2016, we develop three versions of abcdSAT: drup, inc and lim, which participate in the competition of main (agile), incremental library and no-limit track, respectively.",
    "creator" : "LaTeX with hyperref package"
  }
}