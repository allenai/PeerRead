{
  "name" : "1106.4573.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "David V. Pynadath", "pynadath isi.edu", "Milind Tambe" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Towards Adjustable Autonomy for the Real World\nPaul S erri s erri isi.edu David V. Pynadath pynadath isi.edu Milind Tambe tambe us .edu Information S ien es Institute and Computer S ien e Department University of Southern California 4676 Admiralty Way, Marina del Rey, CA 90292 USA\nAbstra t\nAdjustable autonomy refers to entities dynami ally varying their own autonomy, transferring de ision-making ontrol to other entities (typi ally agents transferring ontrol to human users) in key situations. Determining whether and when su h transfers-of- ontrol should o ur is arguably the fundamental resear h problem in adjustable autonomy. Previous work has investigated various approa hes to addressing this problem but has often fo used on individual agent-human intera tions. Unfortunately, domains requiring ollaboration between teams of agents and humans reveal two key short omings of these previous approa hes. First, these approa hes use rigid one-shot transfers of ontrol that an result in una eptable oordination failures in multiagent settings. Se ond, they ignore osts (e.g., in terms of time delays or e e ts on a tions) to an agent's team due to su h transfers-of-\nontrol.\nTo remedy these problems, this arti le presents a novel approa h to adjustable autonomy, based on the notion of a transfer-of- ontrol strategy. A transfer-of- ontrol strategy\nonsists of a onditional sequen e of two types of a tions: (i) a tions to transfer de isionmaking ontrol (e.g., from an agent to a user or vi e versa) and (ii) a tions to hange an agent's pre-spe i ed oordination onstraints with team members, aimed at minimizing mis oordination osts. The goal is for high-quality individual de isions to be made with minimal disruption to the oordination of the team. We present a mathemati al model of transfer-of- ontrol strategies. The model guides and informs the operationalization of the strategies using Markov De ision Pro esses, whi h sele t an optimal strategy, given an un ertain environment and osts to the individuals and teams. The approa h has been\narefully evaluated, in luding via its use in a real-world, deployed multi-agent system that\nassists a resear h group in its daily a tivities."
    }, {
      "heading" : "1. Introdu tion",
      "text" : "Ex iting, emerging appli ation areas ranging from intelligent homes (Lesser et al., 1999), to routine organizational oordination (Pynadath et al., 2000), to ele troni ommer e (Collins et al., 2000a), to long-term spa e missions (Dorais et al., 1998) utilize the de ision-making skills of both agents and humans. These new appli ations have brought forth an in reasing interest in agents' adjustable autonomy (AA), i.e., in entities dynami ally adjusting their own level of autonomy based on the situation (Mulsiner & Pell, 1999). Many of these ex iting appli ations will not be deployed unless reliable AA reasoning is a entral omponent. With AA, an entity need not make all de isions autonomously; rather it an hoose to redu e its own autonomy and transfer de ision-making ontrol to other users or agents, when doing so\n2002 AI A ess Foundation and Morgan Kaufmann Publishers. All rights reserved.\nis expe ted to have some net bene t (Dorais et al., 1998; Barber, Goel, & Martin, 2000a; Hexmoor & Kortenkamp, 2000).\nA entral problem in AA is to determine whether and when transfers of de ision-making ontrol should o ur. A key hallenge is to balan e two potentially on i ting goals. On the one hand, to ensure that the highest-quality de isions are made, an agent an transfer ontrol to a human user (or another agent) whenever that user has superior de ision-making expertise. 1 On the other hand, interrupting a user has high osts and the user may be unable to make and ommuni ate a de ision, thus su h transfers-of- ontrol should be minimized. Previous work has examined several di erent te hniques that attempt to balan e these two on i ting goals and thus address the transfer-of- ontrol problem. For example, one te hnique suggests that de ision-making ontrol should be transferred if the expe ted utility of doing so is higher than the expe ted utility of making an autonomous de ision (Horvitz, Ja obs, & Hovel, 1999). A se ond te hnique uses un ertainty as the sole rationale for de iding who should have ontrol, for ing the agent to relinquish ontrol to the user whenever un ertainty is high (Gunderson & Martin, 1999). Yet other te hniques transfer\nontrol to a user if an erroneous autonomous de ision ould ause signi ant harm (Dorais et al., 1998) or if the agent la ks the apability to make the de ision (Ferguson, Allen, & Miller, 1996).\nUnfortunately, these previous approa hes to transfer-of- ontrol reasoning and indeed most previous work in AA, have fo used on domains involving a single agent and a single user, isolated from intera tions with other entities. When applied to intera ting teams of agents and humans, where intera tion between an agent and a human impa ts the intera tion with other entities, these te hniques an lead to dramati failures. In parti ular, the presen e of other entities as team members introdu es a third goal of maintaining oordination (in addition to the two goals already mentioned above), whi h these previous te hniques fail to address. Failures o ur for two reasons. Firstly, these previous te hniques ignore team related fa tors, su h as osts to the team due to in orre t de isions or due to delays in de isions during su h transfers-of- ontrol. Se ondly (and more importantly), these te hniques use one-shot transfers-of- ontrol, rigidly ommitting to one of two hoi es: (i) transfer ontrol and wait for input ( hoi e H) or (ii) a t autonomously ( hoi e A). However, given intera ting teams of agents and humans, either hoi e an lead to ostly failures if the entity with ontrol fails to make or report a de ision in a way that maintains oordination. For instan e, a human user might be unable to provide the required input due to a temporary ommuni ation failure; this may ause an agent to fail in its part of a joint a tion, as this joint a tion may be dependent on the user's input. On the other hand, for ing a less apable entity to make a de ision simply to avoid mis oordination an lead to poor de isions with signi ant onsequen es. Indeed, as seen in Se tion 2.2, when we applied a rigid transfer-of- ontrol de ision-making to a domain involving teams of agents and users, it failed dramati ally.\nYet, many emerging appli ations do involve multiple agents and multiple humans a ting ooperatively towards joint goals. To address the short omings of previous AA work in su h domains, this arti le introdu es the notion of a transfer-of- ontrol strategy. A transfer-of-\nontrol strategy onsists of a pre-de ned, onditional sequen e of two types of a tions: (i)\n1. While the AA problem in general involves transferring ontrol from one entity to another, in this paper,\nwe will typi ally fo us on intera tions involving autonomous agents and human users.\na tions to transfer de ision-making ontrol (e.g., from an agent to a user or vi e versa); (ii) a tions to hange an agent's pre-spe i ed oordination onstraints with team members, rearranging a tivities as needed (e.g., reordering tasks to buy time to make the de ision). The agent exe utes su h a strategy by performing the a tions in order, transferring ontrol to the spe i ed entity and hanging oordination as required, until some point in time when the entity urrently in ontrol exer ises that ontrol and makes the de ision. Thus, the previous\nhoi es of H or A are just two of many di erent and possibly more omplex transfer-ofontrol strategies. For instan e, an ADAH strategy implies that an agent initially attempts to make an autonomous de ision. If the agent makes the de ision autonomously the strategy exe ution ends there. However, there is a han e that it is unable to make the de ision in a timely manner, perhaps be ause its omputational resour es are busy with higher priority tasks. To avoid mis oordination the agent exe utes a D a tion whi h hanges the\noordination onstraints on the a tivity. For example, a D a tion ould be to inform other agents that the oordinated a tion will be delayed, thus in urring a ost of in onvenien e to others but buying more time to make the de ision. If it still annot make the de ision, it will eventually take a tion H, transferring de ision-making ontrol to the user and waiting for a response. In general, strategies an involve all available entities and ontain many a tions to hange oordination onstraints. While su h strategies may be useful in singleagent single-human settings, they are parti ularly riti al in general multiagent settings, as dis ussed below.\nTransfer-of- ontrol strategies provide a exible approa h to AA in omplex systems with many a tors. By enabling multiple transfers-of- ontrol between two (or more) entities, rather than rigidly ommitting to one entity (i.e., A orH), a strategy attempts to provide the highest quality de ision, while avoiding oordination failures. In parti ular, in a multiagent setting there is often un ertainty about whether an entity will make a de ision and when it will do so, e.g., a user may fail to respond, an agent may not be able to make a de ision as expe ted or a ommuni ation hannel may fail. A strategy addresses su h un ertainty by planning multiple transfers of ontrol to over for su h ontingen ies. For instan e, with the ADH strategy, an agent ultimately transfers ontrol to a human to attempt to ensure that some response will be provided in ase the agent is unable to a t. Furthermore, expli it\noordination- hange a tions, i.e., D a tions, redu e mis oordination e e ts, for a ost, while better de isions are being made. Finally, sin e the utility of transferring ontrol or hanging\noordination is dependent on the a tions taken afterwards, the agent must plan a strategy in advan e to nd the sequen e of a tions that maximizes team bene ts. For example, rea ting to the urrent situation and repeatedly taking and giving ontrol as in the strategy ADHADH : : : may be more ostly than planning ahead, making a bigger oordination\nhange, and using a shorter ADH strategy. We have developed a de ision theoreti model of su h strategies, that allows the expe ted utility of a strategy to be al ulated and, hen e, strategies to be ompared.\nThus, a key AA problem is to sele t the right strategy, i.e., one that provides the bene t of high-quality de isions without risking signi ant osts in interrupting the user and mis oordination with the team. Furthermore, an agent must sele t the right strategy despite signi ant un ertainty. Markov de ision pro esses (MDPs) (Puterman, 1994) are a natural\nhoi e for implementing su h reasoning be ause they expli itly represent osts, bene ts and un ertainty as well as doing lookahead to examine the potential onsequen es of sequen es\nof a tions. In Se tion 4, a general reward fun tion is presented for an MDP that results in an agent arefully balan ing risks of in orre t autonomous de isions, potential mis oordination and osts due to hanging oordination between team members. Detailed experiments were performed on the MDP, the key results of whi h are as follows. As the relative importan e of entral fa tors, su h as the ost of mis oordination, was varied the resulting MDP poli ies varied in a desirable way, i.e., the agent made more de isions autonomously if the ost of transferring ontrol to other entities in reased. Other experiments reveal a phenomenon not reported before in the literature: an agent may a t more autonomously when oordination\nhange osts are either too low or too high, but in a \\middle\" range, the agent tends to a t\nless autonomously.\nOur resear h has been ondu ted in the ontext of a real-world multi-agent system, alled Ele tri Elves (E-Elves) (Chalupsky, Gil, Knoblo k, Lerman, Oh, Pynadath, Russ, & Tambe, 2001; Pynadath et al., 2000), that we have used for over six months at the University of Southern California, Information S ien es Institute. The E-Elves assists a group of resear hers and a proje t assistant in their daily a tivities, providing an ex iting opportunity to test AA ideas in a real environment. Individual user proxy agents alled Friday (from Robinson Crusoe's servant Friday) a t in a team to assist with res heduling meetings, ordering meals, nding presenters and other day-to-day a tivities. Over the ourse of several months, MDP-based AA reasoning was used around the lo k in the E-Elves, making many thousands of autonomy de isions. Despite the unpredi tability of the user's behavior and the agent's limited sensing abilities, the MDP onsistently made sensible AA de isions. Moreover, many times the agent performed several transfers-of- ontrol to\nope with ontingen ies su h as a user not responding. One lesson learned when a tually deploying the system was that sometimes users wished to in uen e the AA reasoning, e.g., to ensure that ontrol was transferred to them in parti ular ir umstan es. To allow users to in uen e the AA reasoning, safety onstraints are introdu ed that allow users to prevent agents from taking parti ular a tions or ensuring that they do take parti ular a tions. These safety onstraints provide guarantees on the behavior of the AA reasoning, making the basi approa h more generally appli able and, in parti ular, making it more appli able to domains where mistakes have serious onsequen es.\nThe rest of this arti le is organized as follows. Se tion 2 gives a detailed des ription of the AA problem and presents the Ele tri Elves as a motivating example appli ation. Se tion 3 presents a formal model of transfer-of- ontrol strategies for AA. (Readers not interested in the mathemati al details may wish to skip over Se tion 3.) The operationalization of the strategies via MDPs is des ribed in Se tion 4. In Se tion 5, the results of detailed experiments are presented. Se tion 6 looks at related work, in luding how earlier AA work\nan be analyzed within the strategies framework. Se tion 7 gives a summary of the arti le. Finally, Se tion 8 outlines areas where the work ould be extended to make it appli able to more appli ations."
    }, {
      "heading" : "2. Adjustable Autonomy { The Problem",
      "text" : "The general AA problem has not been previously formally de ned in the literature, parti - ularly for a multiagent ontext. In the following, a formal de nition of the problem is given so as to learly de ne the task for the AA reasoning. The team, whi h may onsist entirely\nof agents or in lude humans, has some joint a tivity, . Ea h entity in the team works\nooperatively on the joint a tivity. The agent, A, has a role, , in the team. Depending on the spe i task, some or all of the roles will need to be performed su essfully in order for the joint a tivity to su eed. The primary goal of the agent is the su ess of whi h it pursues by performing . Performing requires that one or more non-trivial de isions are made. To make a de ision, d, the agent an draw upon n other entities from a set E = fe\n1\n: : : e\nn\ng, whi h typi ally in ludes the agent itself. Ea h entity in E (e.g., a human\nuser) is apable of making de ision d. The entities in E are not ne essarily part of the team performing . Di erent agents and users will have di ering abilities to make de isions due to available omputational resour es, a ess to relevant information, et . Coordination\nonstraints, , exist between and the roles of other members of the team. For example, various roles might need to be exe uted simultaneously or in a ertain order or with some\nombined quality or total ost. A riti al fa et of the su essful ompletion of the joint task\n, given its jointness, is to ensure that oordination between team members is maintained, i.e., are not violated. Thus, we an des ribe an AA problem instan e with the tuple: hA; ; ; ; d; Ei.\nFrom an AA perspe tive, the agent an take two types of a tions for a de ision, d. First, it an transfer ontrol to an entity in E apable of making that de ision. In general, there are no restri tions on when, how often or for how long de ision-making ontrol an be transferred to a parti ular entity. Typi ally, the agent an also transfer de ision-making\nontrol to itself. In general, we assume that when the agent transfers ontrol, it does not have any guarantee on the exa t time of response or exa t quality of the de ision made by the entity to whi h ontrol is transferred. In fa t, in some ases it will not know whether the entity will be able to make a de ision at all or even whether the entity will know it has de ision-making ontrol, e.g., if ontrol was transferred via email, the agent may not know if the user a tually read the email.\nThe se ond type of a tion that an agent an take is to request hanges in the oordination onstraints, , between team members. A oordination hange gives the agent the possibility of hanging the requirements surrounding the de ision to be made, e.g., the required timing, ost or quality of the de ision, whi h may allow it to better ful ll its responsibilities. A oordination hange might involve reordering or delaying tasks or it may involve hanging roles, or it may be a more dramati hange where the team pursues in a ompletely di erent way. Changing oordination has some ost, but it may be better to in ur this ost than violate oordination onstraints, i.e., in ur mis oordination osts. Mis oordination between team members will o ur for many reasons, e.g., a onstraint that limits the total ost of a joint task might be violated if one team member in urs a higher than expe ted ost and other team members do not redu e their osts. In this arti le, we are primarily on erned with onstraints related to the timing of roles, e.g., ordering onstraints or requirements on simultaneous exe ution. This in turn, usually requires that the agent guards against delayed de isions although it an also require that a de ision is not made too soon.\nThus, the AA problem for the agent, given a problem instan e, hA; ; ; ; d; Ei, is to hoose the transfer-of- ontrol or oordination- hange a tions that maximizes the overall expe ted utility of the team. In the remainder of this se tion we des ribe a on rete, real-\nworld domain for AA (Se tion 2.1) and an initial failed approa h that motivates our solution (Se tion 2.2)."
    }, {
      "heading" : "2.1 The Ele tri Elves",
      "text" : "This resear h was initiated in response to issues that arose in a real appli ation and the resulting approa h was extensively tested in the day-to-day running of that appli ation. The Ele tri Elves (E-Elves) is a proje t at USC/ISI to deploy an agent organization in support of the daily a tivities of a human organization (Pynadath et al., 2000; Chalupsky et al., 2001). We believe this appli ation to be fairly typi al of future generation appli-\nations involving teams of agents and humans. The operation of a human organization requires the performan e of many everyday tasks to ensure oheren e in organizational a tivities, e.g., monitoring the status of a tivities, gathering information and keeping everyone informed of hanges in a tivities. Teams of software agents an aid organizations in a omplishing these tasks, fa ilitating oherent fun tioning and rapid, exible response to rises. A number of underlying AI te hnologies support the E-Elves, e.g., te hnologies devoted to agent-human intera tions, agent oordination, a essing multiple heterogeneous information sour es, dynami assignment of organizational tasks, and deriving information about organization members (Chalupsky et al., 2001). While these te hnologies are useful, AA is fundamental to the e e tive integration of the E-Elves into the day-to-day running of a real organization and, hen e, is the fo us of this paper.\nThe basi design of the E-Elves is shown in Figure 1(a). Ea h agent proxy is alled Friday (after Robinson Crusoes' man-servant Friday) and a ts on behalf of its user in the agent team. The design of the Friday proxies is dis ussed in detail in (Tambe, Pynadath, Chauvat, Das, & Kaminka, 2000) (where they are referred to as TEAMCORE proxies). Currently, Friday an perform several tasks for its user. If a user is delayed to a meeting, Friday an res hedule the meeting, informing other Fridays, who in turn inform their users. If there is a resear h presentation slot open, Friday may respond to the invitation to present on behalf of its user. Friday an also order its user's meals (see Figure 2(a)) and tra k the user's lo ation, posting it on a Web page. Friday ommuni ates with users using wireless devi es, su h as personal digital assistants (PALM VIIs) and WAP-enabled mobile phones, and via user workstations. Figure 1(b) shows a PALM VII onne ted to a Global Positioning Servi e (GPS) devi e, for tra king users' lo ations and enabling wireless ommuni ation between Friday and a user. Ea h Friday's team behavior is based on a teamwork model,\nalled STEAM (Tambe, 1997). STEAM en odes and enfor es the onstraints between roles that are required for the su ess of the joint a tivity, e.g., meeting attendees should arrive at a meeting simultaneously. When a role within the team needs to be lled, STEAM requires that a team member is assigned responsibility for that role. To nd the best suited person, the team au tions o the role, allowing it to onsider a ombination of fa tors and assign the best suited user. Friday an bid on behalf of its user, indi ating whether its user is\napable and/or willing to ll a parti ular role. Figure 2(b) shows a tool that allows users to view au tions in progress and intervene if they so desire. In the au tion in progress, Jay Modi's Friday has bid that Jay is apable of giving the presentation, but is unwilling to do so. Paul S erri's agent has the highest bid and was eventually allo ated the role.\nAA is riti al to the su ess of the E-Elves sin e, despite the range of sensing devi es, Friday has onsiderable un ertainty about the user's intentions and even lo ation; hen e, Friday will not always have the appropriate information to make orre t de isions. On the other hand, while the user has the required information, Friday annot ontinually ask the user for input, sin e su h interruptions are disruptive and time- onsuming. There are four de isions in the E-Elves to whi h AA reasoning is applied: (i) whether a user will attend a meeting on time; (ii) whether to lose an au tion for a role; (iii) whether the user is willing to perform an open team role; and (iv) if and what to order for lun h. In this paper, we fo us on the AA reasoning for two of those de isions: whether a user will attend a meeting on time and whether to lose an au tion for a role. The de ision as to whether a user will attend a meeting on time is the most often used and most diÆ ult of the de isions Friday fa es. We brie y des ribe the de ision to lose an au tion and later show how an insight provided by the model of strategies led to a signi ant redu tion in the amount of ode required to implement the AA reasoning for that de ision. The de ision to volunteer a user for a meeting is similar to the earlier de isions, and omitted for brevity; the de ision to order lun h is urrently implemented in a simpler fashion and is not (at least as yet) illustrative of the full set of omplexities.\nA entral de ision for Friday, whi h we des ribe in terms of our problem formulation, hA; ; ; ; d; Ei, is whether its user will attend a meeting at the urrently s heduled meeting time. In this ase, Friday is the agent, A. The joint a tivity, , is for the meeting attendees to attend the meeting simultaneously. Friday a ts as proxy for its user, hen e its role, , is to ensure that its user arrives at the urrently s heduled meeting time. The\noordination onstraint, , between Friday's role and the roles of other Fridays is that they o ur simultaneously, i.e., the users must attend at the urrently s heduled time. If any attendee arrives late, or not at all, the time of all the attendees is wasted; on the other hand, delaying a meeting is disruptive to users' s hedules. The de ision, d, is whether the user will attend the meeting or not and ould be made by either Friday or the user, i.e., E = fuser;Fridayg. Clearly, the user will be often better pla ed to make this de ision. However, if Friday transfers ontrol to the user for the de ision, it must guard against mis-\noordination, i.e., having the other attendees wait, while waiting for a user response. Some de isions are potentially ostly, e.g., in orre tly telling the other attendees that the user will not attend, and Friday should avoid taking them autonomously. To buy more time for the user to make the de ision or for itself to gather more information, Friday ould hange\noordination onstraints with a D a tion. Friday has several di erent D a tions at its disposal, in luding delaying the meeting by di erent lengths of time, as well as being able to\nan el the meeting entirely. The user an also request a D a tion, e.g., via the dialog box in Figure 5(a), to buy more time to make it to the meeting. If the user de ides a D is required, Friday is the onduit through whi h other Fridays (and hen e their users) are informed. Friday must sele t a sequen e of a tions, either transferring ontrol to the user, delaying or\nan elling the meeting or autonomously announ ing that the user will or will not attend,\nto maximize the utility of the team.\nThe se ond AA de ision that we look at is the de ision to lose an au tion for an open\nrole and assign a user to that role.\n2\nIn this ase, the joint a tivity, , is the group resear h\n2. There are also roles for submitting bids to the au tion but the AA for those de isions is simpler, hen e\nwe do not fo us on them here.\n(a) (b)\nmeeting and the role, , is to be the au tioneer. Users will not always submit bids for the role immediately; in fa t, the bids may be spread out over several days, or some users might not bid at all. The spe i de ision, d, on whi h we fo us is whether to lose the au tion and assign the role or ontinue waiting for in oming bids. On e individual team members provide their bids, the au tioneer agent or human team leader de ides on a presenter based on that input (E = fuser; au tioneer agentg). The team expe ts a willing presenter to do a high-quality resear h presentation, whi h means the presenter will need some time to prepare. Thus, the oordination onstraint, is that the most apable, willing user must be allo ated to the role with enough time to prepare the presentation. Despite individually responsible a tions, the agent team may rea h a highly undesirable de ision, e.g., assigning the same user week after week, hen e there is advantage in getting the human team leader's input. The agent fa es un ertainty (e.g., will better bids ome in?), osts (i.e., the later the assignment, the less time the presenter has to prepare), and needs to onsider the possibility that the human team leader has some spe ial preferen e about who should do a presentation at some parti ular meeting. By transferring ontrol, the agent allows the human team leader to make an assignment. For this de ision, a oordination- hange a tion, D, would res hedule the resear h meeting. However, relative to the ost of an elling the meeting, the ost of res heduling is too high for res heduling to be a useful a tion."
    }, {
      "heading" : "2.2 De ision-Tree Approa h",
      "text" : "One logi al avenue of atta k on the AA problem for the E-Elves was to apply an approa h used in a previously reported, su essful meeting s heduling system, in parti ular CAP (Mit hell, Caruana, Freitag, M Dermott, & Zabowski, 1994). Like CAP, Friday learned user preferen es using C4.5 de ision-tree learning (Quinlan, 1993). Friday re orded values of a dozen arefully sele ted attributes and the user's preferred a tion (identi ed by asking\nthe user) whenever it had to make a de ision. Friday used the data to learn a de ision tree that en oded its autonomous de ision making. For AA, Friday also asked if the user wanted su h de isions taken autonomously in the future. From these responses, Friday used C4.5 to learn a se ond de ision tree whi h en oded its rules for transferring ontrol. Thus, if the se ond de ision tree indi ated that Friday should a t autonomously, it would take the a tion suggested by the rst de ision tree. Initial tests with the C4.5 approa h were promising (Tambe et al., 2000), but a key problem soon be ame apparent. When Friday en ountered a de ision for whi h it had learned to transfer ontrol to the user, it would wait inde nitely for the user to make the de ision, even though this ina tion aused mis oordination with teammates. In parti ular, other team members would arrive at the meeting lo ation, waiting for a response from the user's Friday, but they would end up\nompletely wasting their time as no response arrived. To address this problem, if a user did not respond within a xed time limit ( ve minutes), Friday took an autonomous a tion. Although performan e improved, when the resulting system was deployed 24/7 it led to some dramati failures, in luding:\n1. Example 1: Tambe's (a user) Friday in orre tly an elled a meeting with the division\ndire tor be ause Friday over-generalized from training examples.\n2. Example 2: Pynadath's (another user) Friday in orre tly an elled the group's weekly\nresear h meeting when a time-out for ed the hoi e of an (in orre t) autonomous a tion.\n3. Example 3: A Friday delayed a meeting almost 50 times, ea h time by 5 minutes.\nIt was orre tly applying a learned rule but ignoring the nuisan e to the rest of the meeting parti ipants.\n4. Example 4: Tambe's Friday automati ally volunteered him for a presentation, but he\nwas a tually unwilling. Again Friday over-generalized from a few examples and when a timeout o urred it took an undesirable autonomous a tion.\nClearly, in a team ontext, rigidly transferring ontrol to one agent (user) failed. Furthermore, using a time-out that rigidly transferred ontrol ba k to the agent, when it was not apable of making a high-quality de ision, also failed. In parti ular, the agent needed to better avoid taking risky de isions by expli itly onsidering their osts (example 1), or take lower ost a tions to delay meetings to buy the user more time to respond (example 2 and 4). Furthermore, as example 3 showed, the agent needed to plan ahead, to avoid taking\nostly sequen es of a tions that ould be repla ed by a single less ostly a tion (example 3). In theory, using C4.5 Friday might have eventually been able to learn rules that would su essfully balan e osts and deal with un ertainty and handle all the spe ial ases and so on, but a very large amount of training data would be required."
    }, {
      "heading" : "3. Strategies for Adjustable Autonomy",
      "text" : "To avoid rigid one-shot transfers of ontrol and allow team osts to be onsidered, we introdu e the notion of a transfer-of- ontrol strategy, whi h is de ned as follows:\nDe nition 3.1 A transfer-of- ontrol strategy is a pre-de ned, onditional sequen e of two types of a tions: (i) a tions to transfer de ision-making ontrol (e.g., from an agent to a user or other agents, or vi e versa) and (ii) a tions to hange an agent's pre-spe i ed\noordination onstraints with team members, aimed at minimizing mis oordination osts.\nThe agent exe utes a transfer-of- ontrol strategy by performing the spe i ed a tions in sequen e, transferring ontrol to the spe i ed entity and hanging oordination as required, until some point in time when the entity urrently in ontrol exer ises that ontrol and makes the de ision. Considering multi-step strategies allows an agent to exploit de isionmaking sour es onsidered too risky to exploit without the possibility of retaking ontrol. For example, ontrol ould be transferred to a very apable but not always available de ision maker then taken ba k if the de ision was not made before serious mis oordination o urred. More omplex strategies, potentially involving several oordination hanges, give the agent the option to try several de ision-making sour es or to be more exible in getting input from high-quality de ision makers. As a result, transfer-of- ontrol strategies spe i ally allow an agent to avoid ostly errors, su h as those enumerated in the previous se tion. 3\nGiven an AA problem instan e, hA; ; ; ; d; Ei, agent A an transfer de ision-making\nontrol for a de ision d to any entity e\ni\n2 E, and we denote su h a transfer-of- ontrol\na tion with the symbol representing the entity, i.e., transferring ontrol to e\ni\nis denoted as\ne\ni\n. When the agent transfers de ision-making ontrol, it may stipulate a limit on the time that it will wait for a response from that entity. To apture this additional stipulation, we denote transfer-of- ontrol a tions with this time limit, e.g., e\ni\n(t) represents that e\ni\nhas\nde ision-making ontrol for a maximum time of t. Su h an a tion has two possible out omes: either e\ni\nresponds before time t and makes the de ision, or it does not respond and de ision\nd remains unmade at time t. In addition, the agent has some me hanism by whi h it\nan hange oordination onstraints (denoted D) to hange the expe ted timing of the de ision. The D a tion hanges the oordination onstraints, , between team members. The a tion has an asso iated value, D\nvalue\n, whi h spe i es its magnitude (i.e., how mu h\nthe D has alleviated the temporal pressure), and a ost, D\nost\n, whi h spe i es the pri e paid\nfor making the hange. We an on atenate su h a tions to spe ify a omplete transferof- ontrol strategy. For instan e, the strategy H(5)A would spe ify that the agent rst relinquishes ontrol and asks entity H (denoting the Human user). If the user responds with a de ision within ve minutes, then there is no need to go further. If not, then the agent pro eeds to the next transfer-of- ontrol a tion in the sequen e. In this example, this next a tion, A, spe i es that the agent itself make the de ision and omplete the task. No further transfers of ontrol o ur in this ase. We an de ne the spa e of all possible strategies with the following regular expression:\nS = (E R)((E R) +D) (1)\nwhere (E R) is all possible ombinations of entity and maximum time. For readability, we will frequently omit the time spe i ations from the transfer-ofontrol a tions and instead write just the order in whi h the agent transfers ontrol among\n3. In some domains, it may make sense to attempt to get input from more than one entity at on e, hen e\nrequiring strategies that have a tions that might be exe uted in parallel. However, in this work, as a rst step, we do not onsider su h strategies. Furthermore, they are not relevant for the domains at hand.\nthe entities and exe utes Ds (e.g., we will often write HA instead of H(5)A). If time spe i ations are omitted, we assume the transfers happen at the optimal times, 4 i.e., the times that lead to highest expe ted utility. If we onsider strategies with the same sequen e of a tions but di erent timings to be the same strategy, the agent has O(jEj k ) possible strategies to sele t from, where k is the maximum length of the strategy and jEj is the number of entities. Thus, the agent has a wide range of options, even if pra ti al\nonsiderations lead to a reasonable upper bound on k and jEj. The agent must sele t the\nstrategy that maximizes the overall expe ted utility of .\nIn the rest of this se tion, we present a mathemati al model of transfer-of- ontrol strategies for AA and use that model to guide the sear h for a solution. Moreover, the model provides a tool for predi ting the performan e of various strategies, justifying their use and explaining observed phenomena of their use. Se tion 3.1 presents the model of AA strategies in detail. Se tion 3.2 reveals key properties of omplex strategies, in luding dominan e relationships among strategies. Se tion 3.3 examines the E-Elves appli ation in the light of the model, to make spe i predi tions about some properties that a su essful AA approa h reasoning for that appli ation lass will have. These predi tions shape the operationalization of strategies in Se tion 4."
    }, {
      "heading" : "3.1 A Mathemati al Model of Strategies",
      "text" : "The transfer-of- ontrol model presented in this se tion allows al ulation of the expe ted utility (EU) of individual strategies, thus allowing strategies to be ompared. The al ulation of a strategy's EU onsiders four elements: the likely relative quality of di erent entities' de isions; the probability of getting a response from an entity at a parti ular time; the ost of delaying a de ision; and the osts and bene ts of hanging oordination onstraints. While other parameters might also be modeled in a similar manner, our experien e with the E-Elves and other AA work suggests that these parameters are the riti al ones a ross a wide range of joint a tivities.\nThe rst element of the model is the expe ted quality of an entity's de ision. In general,\nwe apture the quality of an entity's de ision at time t with the fun tions EQ = fEQ\nd e (t) :\nR! Rg. The quality of a de ision re e ts both the probability that the entity will make an \\appropriate\" de ision and the osts in urred if the de ision is wrong. The expe ted quality of a de ision is al ulated in a de ision theoreti way, by multiplying the probability of ea h out ome, i.e., ea h de ision, by the utility of that de ision, i.e., the ost or bene t of that de ision. For example, the higher the probability that the entity will make a mistake, the lower the quality, even lower if the mistakes might be very ostly. The quality of de ision an entity will make an vary over time as the information available to it hanges or as it has more time to \\think\". The se ond element of the model is the probability that an entity will make a de ision if ontrol is transferred to it. The fun tions, P = fP e\n>\n(t) : R! [0; 1℄g,\nrepresent ontinuous probability distributions over the time that the entity e will respond. That is, the probability that e\ni\nwill respond before time t\n0\nis\nR\nt\n0\n0\nP\ne\ni > (t)dt.\nThe third element of the model is a representation of the ost of inappropriate timing of a de ision. In general, not making a de ision until a parti ular point in time in urs some\n4. The best time to transfer ontrol an be found, e.g., by di erentiating the expe ted utility equation in\nSe tion 3.1 and solving for 0.\nost that is a fun tion of both the time, t, and the oordination onstraints, , between team members. As stated earlier, we fo us on ases of onstraint violations due to delays in making de isions. Thus, the ost is due to the violation of the onstraints aused by not making a de ision until that point in time. We an write down a wait- ost fun tion: W = f( ; t) whi h returns the ost of not making a de ision until a parti ular point in time given oordination onstraints, . This mis oordination ost is a fundamental aspe t of our model given our emphasis on multiagent domains. It is alled a \\wait ost\" be ause it models the mis oordination that arises while the team \\waits\" for some entity to make the ultimate de ision. In domains like E-Elves, the team in urs su h wait osts in situations where (for example) other meeting attendees have assembled in a meeting room at the time of the meeting, but are kept waiting without any input or de ision from Friday (potentially be ause it annot provide a high-quality de ision, nor an it get any input from its user). Noti e that di erent roles will lead to di erent wait ost fun tions, sin e delays in the performan e of di erent roles will have di erent e e ts on the team. We assume that there is some point in time, , after whi h no more osts a rue, i.e., if t then f( ; t) = f( ; ). At the deadline, , the maximum ost due to inappropriate timing of a de ision has been in urred. Finally, we assume that, in general, until , the wait ost fun tion is nonde reasing, re e ting the idea that bigger violations of onstraints lead to higher wait osts. The nal element of the model is the oordination- hange a tion, D, whi h moves the agent further away from the deadline and hen e redu es the wait osts that are in urred. We model the e e t of the D by letting W be a fun tion of t D\nvalue\n(rather than t) after\nthe D a tion and as having a xed ost, D\nost\n, in urred immediately upon its exe ution.\nFor example, in the E-Elves domain, suppose at the time of the meeting, Friday delays the meeting by 15 minutes (D a tion). Then, in the following time period, it will in ur the relatively low ost of not making a de ision 15 minutes before the meeting (t D\nvalue\n),\nrather than the relatively high ost of not making the de ision at the time of the meeting. Other, possibly more omplex, models of a D a tion ould also be used.\nWe use these four elements to ompute the EU of an arbitrary strategy, s. The utility derived from a de ision being made at time t by the entity in ontrol is the quality of the entity's de ision minus the osts in urred from waiting until t, i.e., EU d\ne\n(t) = EQ\nd e (t)\nW(t). If a oordination- hange a tion has been taken it will also have an e e t on utility. Until a oordination hange of value D\nvalue\nis taken at some time , the in urred wait ost\nisW( ). Then, between and t, the wait ost in urred isW(t D\nvalue\n) W( D\nvalue\n).\nThus, if a D a tion has been taken at time for ost D\nost\nand with value D\nvalue\n, the\nutility from a de ision at time t (t > ) is: EU\nd e (t) = EQ d e (t) W( ) W( D value ) +\nW(t D\nvalue\n) D\nost\n. To al ulate the EU of an entire strategy, we multiply the response\nprobability mass fun tion's value at ea h instant by the EU of re eiving a response at that instant, and then integrate over the produ ts. Hen e, the EU for a strategy s given a problem instan e, hA; ; ; ; d; Ei, is:\nEU\nhA; ; ; ;d;Ei s =\nZ\n1\n0\nP\n>\n(t)EU\nd e (t) :dt (2)\nIf a strategy involves several a tions, we need to ensure that the probability of response fun tion and the wait- ost al ulation re e t the ontrol situation at that point in the strategy. For example, if the user, H, has ontrol at time t, P\n>\n(t) should re e t H's\nprobability of responding at t, i.e., P\nH > (t 0 ). To this end, we an break the integral from\nEquation 2 into separate terms, with ea h term representing one segment of the strategy, e.g., for a strategy UA there would be one term for when U has ontrol and another for when A has ontrol.\nUsing this basi te hnique for writing down EU al ulations, we an write down the spe i equations for arbitrary transfer-of- ontrol strategies. Equations 3-6 in Table 1 show the EU equations for the strategies A, e, eA and eDeA respe tively. The equations assume that the agent, A, an make the de ision instantaneously (or at least, with no delay signi ant enough to a e t the overall value of the de ision). The equations are reated by writing down the integral for ea h of the segments of the strategy, as des ribed above. T is the time when the agent takes ontrol from e, and is the time at whi h the D o urs. One an write down the equations for more omplex strategies in the same way. Noti e that these equations make no assumptions about the parti ular fun tions.\nGiven that the EU of a strategy an be al ulated, the AA problem for the agent redu es to nding and following the transfer-of- ontrol strategy that will maximize its EU. Formally, the agent's problem is:\nAxiom 3.1 For a problem hA; ; ; ; d; Ei, the agent must sele t s 2 S su h that 8s\n0\n2\nS; s\n0\n6= s;EU\nhA; ; ; ;d;Ei s EU hA; ; ; ;d;Ei\ns\n0"
    }, {
      "heading" : "3.2 Dominan e Relationships among Strategies",
      "text" : "An agent ould potentially nd the strategy with the highest EU by examining ea h and every strategy in S, omputing its EU, and sele ting the strategy with the highest value. For example, onsider the problem for domains with onstant expe ted de ision-making quality, exponentially rising wait osts, and Markovian response probabilities. Figure 3 shows a graph of the EU of two strategies (HDA and H) given this parti ular model instantiation. Noti e that, for di erent response probabilities and rates of wait ost a rual, one strategy outperforms the other, but neither strategy is dominant over the entire parameter spa e. The EU of a strategy is also dependent on the timing of transfers of ontrol, whi h in turn depend on the relative quality of the entities' de ision making. Appendix I provides a more detailed analysis.\nFortunately, we do not have to evaluate and ompare ea h and every andidate in an exhaustive sear h to nd the optimal strategy. We an instead use analyti al methods to draw general on lusions about the relative values of di erent andidate strategies. In parti ular, we present three Lemmas that show the domain-level onditions under whi h parti ular strategy types are superior to others. The Lemmas also lead us to the, perhaps surprising, on lusion that omplex strategies are not ne essarily superior to single-shot strategies, even in a multi-agent ontext; in fa t, no parti ular strategy dominates all other strategies a ross all domains.\nLet us rst onsider the AA subproblem of whether an agent should ever take ba k ontrol from another entity. If we an show that, under ertain onditions, an agent should always eventually take ba k ontrol, then our strategy sele tion pro ess an ignore any strategies where the agent does not do so (i.e., any strategies not ending in A). The agent's goal is to strike the right balan e between not waiting inde nitely for a user response and not\ntaking a risky autonomous a tion. Informally, the agent reasons that it should eventually make a de ision if the expe ted ost of ontinued waiting ex eeds the di eren e between the user's de ision quality and its own. More formally, the agent should eventually take ba k de ision-making ontrol i , for some time t:\nZ\nt\nP\n>\n(t\n0\n)W(t\n0\n):dt\n0\nW(t) > EQ\nd U (t) EQ d A (t) (7)\nwhere the left-hand side al ulates the future expe ted wait osts and the right-hand side\nal ulates the extra utility to be gained by getting a response from the user. This result leads to the following general on lusion about strategies that end with giving ontrol ba k to the agent:\nLemma 1: If s 2 S is a strategy ending with e 2 E, and s\n0\nis sA, then EU\nd s 0 > EU d s i\n8e 2 E;9t < su h that\nR\nt\nP\n>\n(t\n0\n)W(t\n0\n):dt\n0\nW(t) > EQ\nd e (t) EQ d\nA\n(t)\nLemma 1 says that if, at any point in time, the expe ted ost of inde nitely leaving ontrol in the hands of the user ex eeds the di eren e in quality between the agent's and user's de isions, then strategies whi h ultimately give the agent ontrol dominate those whi h do not. Thus, if the rate of wait ost a rual in reases or the di eren e in the relative quality of the de ision-making abilities de reases or the user's probability of response de reases, then strategies where the agent eventually takes ba k ontrol will dominate. A key onsequen e of the Lemma (in the opposite dire tion) is that, if the rate that osts a rue does not a elerate, and if the probability of response stays onstant (i.e., Markovian), then the agent should inde nitely leave ontrol with the user (if the user had originally been given ontrol), sin e the expe ted wait ost will not hange over time. Hen e, even if the agent is fa ed with a situation with potentially high total wait osts, the optimal strategy may be a one-shot strategy of handing over ontrol and waiting inde nitely, be ause the expe ted future wait osts at ea h point in time are relatively low. Thus, Lemma 1 isolates the ondition under whi h we should onsider appending an A transfer-of- ontrol a tion to our strategy.\nWe an perform a similar analysis to identify the onditions under whi h we should in lude a D a tion in our strategy. The agent has in entive in hanging oordination\nonstraints via a D a tion due to the additional time made available for getting a highquality response from an entity. However, the overall value of a D a tion depends on a number of fa tors (e.g., the ost of taking the D a tion and the timing of subsequent transfers of ontrol). We an al ulate the expe ted value of a D by omparing the EU of a strategy with and without a D. The D is useful if and only if the in reased expe ted value of the strategy with it is greater than its ost, D\nost\n.\nLemma 2: if s 2 S has no D and s\n0\nis s with a D in luded at t then EU\nd s 0 > EU d s i\nR\nP\n>\n(t\n0\n)W(t):dt\n0\nR\nP\n>\n(t\n0\n)W(tjD):dt\n0\n> D\nost\nWe an illustrate the onsequen es of Lemma 2 by onsidering the spe i problemmodel\nof Appendix I (i.e., P\n>\n(t) = exp\nt\n,W(t) = ! exp\n!t\n, EQ\nd e (t) = , and andidate strategies\neA and eDA). In this ase, EU\nd eDA > EU d eA i ( !)! exp ( !) (1 exp !D value ) >\nD\nost\n. Figure 4 plots the value of the D a tion as we vary the rate of wait ost a umulation,\nw, and the parameter of the Markovian response probability fun tion, p. The graph shows\nthat the bene t from the D is highest when the probability of response is neither too low nor too high. When the probability of response is low, the user is unlikely to respond, even given the extra time; hen e, the agent will have in urred D\nost\nwith no bene t. A\nD also has little value when the probability of response is high, be ause the user will likely respond shortly after the D, meaning that it has little e e t (the e e t of the D is on the wait osts after the a tion is taken). Overall, a ording to Lemma 2, at those points where the graph goes above D\nost\n, the agent should in lude a D a tion, and, at all other points, it\nshould not. Figure 4 demonstrates the value of a D a tion for a spe i sub lass of problem domains, but we an extend our on lusion to the more general ase as well. For instan e, while the spe i model has exponential wait osts, in models where wait osts grow more slowly, there will be fewer situations where Lemma 2's riterion holds (i.e., where a D will be useful). Thus, Lemma 2 allows us to again eliminate strategies from onsideration, based on the evaluation of its riterion in the parti ular domain of interest.\nGiven Lemma 2's evaluation of adding a single D a tion to a strategy, it is natural to ask whether a se ond, third, et . D a tion would in rease EU even further. In other words, when a omplex strategy is better than a simple one, is an even more omplex strategy even better? The answer is \\not ne essarily\".\nLemma 3: 8K 2 N;9W 2 W; 9P 2 P; 9EQ 2 EQ su h that the optimal strategy\nhas K D a tions.\nInformally, Lemma 3 says that we annot x a single, optimal number of D a tions, be ause for every possible number of D a tions, there is a potential domain (i.e., ombination of a wait- ost, response-probability, and expe ted-quality fun tions) for whi h that number of D a tions is justi ed by being optimal. Consider a situation where the ost of a D was a fun tion of the number of Ds to date (i.e., the ost of the Kth D is f(K)). For example, in the E-Elves' meeting ase, the ost of delaying a meeting for the third time is mu h higher than the ost of the rst delay, sin e ea h delay is su essively more annoying to other meeting parti ipants. Hen e, the test for the usefulness of the Kth D in a strategy,\ngiven the spe i model in Appendix I, is:\nf(K) < !(exp\nD\nvalue\n!\n1) (\nÆ\nexp\nÆT\n!\nÆ\nexp\nÆ\nexp\n! T\n) (8)\nDepending on the nature of f(K), Equation 8 an hold for any number of Ds, so, for any K, there will be some onditions for whi h a strategy with K Ds is optimal. For instan e, in Se tion 5.3, we show that the maximum length of the optimal strategy for a random\non guration of up to 25 entities is usually less than eight a tions.\nEquation 8 illustrates how the value of an additionalD an be limited by hanging D\nost\n,\nbut Lemma 3 also shows us that other fa tors an a e t the value of an additional D. For example, even with a onstant D\nost\n, the value of an additional D depends on how many\nother D a tions the agent performs. Figure 4 shows that the value of the D depends on the rate at whi h wait osts a rue. If the rate of wait ost a rual a elerates over time (e.g., for the exponential model), a D a tion slows that a eleration, rendering a se ond D a tion less useful (sin e the wait osts are now a ruing more slowly). Noti e also that Ds be ome valueless after the deadline, when wait osts stop a ruing.\nTaken together, Lemmas 1-3 show that no parti ular transfer-of- ontrol strategy dominates all others a ross all domains. Moreover, very di erent strategies, from single-shot strategies to arbitrarily omplex strategies, are appropriate for di erent situations, although the range of situations where a parti ular transfer-of- ontrol a tion provides bene t an be quite narrow. Sin e a strategy might have very low EU for some set of parameters, hoosing the wrong strategy an lead to very poor results. On the other hand, on e we understand the parameter on guration of an intended appli ation domain, Lemmas 1-3 provide useful tools for fo using the sear h for an optimal transfer-of- ontrol strategy. The Lemmas an be used o -line to substantially redu e the spa e of strategies that need to be sear hed to\nnd the optimal strategy. However, in general there may be many strategies and nding\nthe optimal strategy may not be possible or feasible."
    }, {
      "heading" : "3.3 Model Predi tions for the E-Elves",
      "text" : "In this se tion, we use the model to predi t properties of a su essful approa h to AA in the E-Elves. Using approximate fun tions for the probability of response, wait ost, and expe ted de ision quality, we an al ulate the EU of various strategies and determine the types of strategies that are going to be useful. Armed with this knowledge, we an predi t some key properties of a su essful implementation.\nA key feature of the E-Elves is that the user is mobile. As she moves around the environment, her probability of responding to requests for de isions hanges drasti ally, e.g., she is most likely to respond when at her workstation. To al ulate the EU of di erent strategies, we need to know P\n>\n(t), whi h means that we need to estimate the response probabilities\nand model how they hange as the user moves around. When Friday ommuni ates via a workstation dialog box, the user will respond, on average, in ve minutes. However, when Friday ommuni ates via a Palm pilot the average user response time is an hour. Users generally take longer to de ide whether they want to present at a resear h meeting, taking approximately two days on average. So, the fun tion P\n>\n(t) should have an average value\nof 5 minutes when the user in her oÆ e, an average of one hour when the user is onta ted via a Palm pilot and an average of two days when the de ision is whether to present at a\nresear h meeting. It is also ne essary to estimate the relative quality of the user, EQ\nd U (t),\nand Friday's de ision making, EQ\nd A (t). We assume that the user's de ision-making EQ d U (t)\nis high with respe t to Friday's, EQ\nd A (t). The un ertainty about user intentions makes it\nvery hard for Friday to onsistently make orre t de isions about the time at whi h the user will arrive at meetings, although its sensors (e.g., GPS devi e) give some indi ation of the user's lo ation. When dealing with more important meetings, the ost of Friday's errors is higher. Thus, in some ases, the de ision-making quality of the user and Friday will be similar, i.e., EQ U\nd\n(t) EQ\nA d (t); while in other ases, there will be an order of magnitude\ndi eren e, i.e., EQ\nU d (t) 10 EQ A d (t). The wait ost fun tion,W(t), will be mu h larger for\nbig meetings than small and in rease rapidly as other attendees wait longer in the meeting room. Finally, the ost of delays, i.e., D\nost\n, an vary by about an order of magnitude. In\nparti ular, the ost of res heduling meetings varies greatly, e.g., the ost of res heduling small informal meetings with olleagues is far less than res heduling a full le ture room at 5 PM Friday.\nThe parameters laid out above show how parameters vary from de ision to de ision. For a spe i de ision, we use Markovian response probabilities (e.g., when the user is in her oÆ e, the average response time is ve minutes), exponentially in reasing wait osts, and\nonstant de ision-making quality (though it hanges from de ision to de ision) to al ulate the EU of interesting strategies. Cal ulating the EU of di erent strategies using the values for di erent parameters shown above allows us to draw the following on lusions (Table 5 in Se tion 5.3 presents a quantitative illustration of these predi tions):\nThe strategy e should not be used, sin e for all ombinations of user lo ation and meeting importan e the EU of this strategy is very low.\nMultiple strategies are required, sin e for di erent user lo ations and meeting importan e di erent strategies are optimal.\nSin e quite di erent strategies are required when the user is in di erent lo ations, the AA reasoning will need to hange strategies when the user hanges lo ation.\nNo strategy has a reasonable EU for all possible parameter instantiations, hen e always using the same strategy will o asionally ause dramati failures.\nFor most de isions, strategies will end with the agent taking a de ision, sin e strategies ending with the user in ontrol generally have very low EU.\nThese predi tions provide important guidan e about a su essful solution for AA in the E-Elves. In parti ular, they make lear that the approa h must exibly hoose between di erent strategies and adjust depending on the meeting type and user lo ation.\nSe tion 2.2 des ribed the unsu essful C4.5 approa h to AA in E-Elves and identi ed several reasons for the mistakes that o urred. In parti ular, rigidly transferring ontrol to one entity and ignoring potential team osts involved in an agent's de ision were highlighted as reasons for the dramati mistakes in Friday's autonomy reasoning. Reviewing the C4.5 approa h in the light of the notion of strategies, we see that Friday learned one strategy and stu k with that strategy. In parti ular, originally, Friday would wait inde nitely for a user response, i.e., it would follow strategy e, if it had learned to transfer ontrol. As shown later\nin Table 5, this strategy has a very low EU. When a xed-length timeout was introdu ed, Friday would follow strategy e(5)A. Su h a strategy has high EU when EQ U\nd\n(t) EQ\nA d (t)\nbut very low EU when EQ\nU d (t) 10 EQ A d (t). Thus, the model explains a phenomenon\nobserved in pra ti e.\nOn the other hand, we an use the model to understand that C4.5's failure in this ase does not mean that it will never be useful for AA. Di erent strategies are only required when ertain parameters (like probability of response or wait ost) hange signi antly. In appli ations where su h parameters do not hange dramati ally from de ision to de ision, one parti ular strategy may always be appropriate. For su h appli ations, C4.5 might learn the right strategy just with a small amount of training data and perform a eptably well."
    }, {
      "heading" : "4. Operationalizing Strategies with MDPs",
      "text" : "We have formalized the problem of AA as the sele tion of the transfer-of- ontrol strategy with the highest EU. We now need an operational me hanism that allows an agent to perform that sele tion. One major on lusion from the previous se tion is that di erent strategies dominate in di erent situations, and that appli ations su h as E-Elves will require me hanism(s) for sele ting strategies in a situation-sensitive fashion. In parti ular, the me hanism must exibly hange strategies as the situation hanges. The required me hanism must also represent the utility fun tion spe i ed by our expe ted de ision qualities, EQ, the osts of violating oordination onstraints, W, and our oordination- hange ost, D\nost\n. Finally, the me hanism must also represent the un ertainty of entity responses and\nthen look ahead over the possible responses (or la k thereof) that may o ur in the future.\nMDPs are a natural means of performing the de ision-theoreti planning required to nd the best transfer-of- ontrol strategy. MDP poli ies provide a mapping between the agent's state and the optimal transfer of ontrol strategy. By en oding the parameters of the model of AA strategies into the MDP, the MDP e e tively be omes a detailed implementation of the model and, hen e, assumes its properties. We an use standard algorithms (Puterman, 1994) to nd the optimal MDP poli y and, hen e, the optimal strategies to follow in ea h state.\nTo simplify exposition, as well as to illustrate the generality of the resulting MDP, this se tion des ribes the mapping from AA strategies to the MDP in four subse tions. In parti ular, Se tion 4.1 provides a dire t mapping of strategies to an abstra t MDP. Se tion 4.2 lls in state features to enable a more on rete realization of the reward fun tion, while still maintaining a domain-independent view. Thus, the se tion ompletely de nes a general MDP for AA is potentially reusable a ross a broad lass of domains. Se tion 4.3 illustrates an implemented instantiation of the MDP in E-Elves. Se tion 4.4 addresses further pra ti al issues in operationalizing su h MDPs in domains su h as E-Elves."
    }, {
      "heading" : "4.1 Abstra t MDP Representation of AA Problem",
      "text" : "Our MDP representation's fundamental state features apture the state of ontrol:\nontrolling-entity is the entity that urrently has de ision-making ontrol.\ne\ni\n-response is any response e\ni\nhas made to the agent's requests for input.\ntime is the urrent time, typi ally dis retized and ranging from 0 to our deadline,\n| i.e., a set ft\n0\n= 0; t\n1\n; t\n2\n; : : : ; t\nn\n= g.\nIf e\ni\n-response is not null or if time = , then the agent is in a terminal state. In the former\nase, the de ision is the value of e\ni\n-response.\nWe an spe ify the set of a tions for this MDP representation as = E[fD;waitg. The set of a tions subsumes the set of entities, E, sin e the agent an transfer de ision-making\nontrol to any one of these entities. The D a tion is the oordination- hange a tion that hanges oordination onstraints, as dis ussed earlier. The \\wait\" a tion puts o transferring ontrol and making any autonomous de ision, without hanging oordination with the team. The agent should reason that \\wait\" is the best a tion when, in time, the situation is likely to hange to put the agent in a position for an improved autonomous de ision or transfer-of- ontrol, without signi ant harm. For example, in the E-Elves domain, at times\nloser to a meeting, users an generally make more a urate determinations about whether they will arrive on time, hen e it is sometimes useful to wait when the meeting is a long time o .\nThe transition probabilities (spe i ed in Table 2) represent the e e ts of the a tions as a distribution over their e e ts (i.e., the ensuing state of the world). If, in a state with time = t\nk\n, the agent hooses an a tion that transfers de ision-making ontrol to an entity,\ne\ni\n, other than the agent itself, the out ome is a state with ontrolling-entity = e\ni\nand\ntime = t\nk+1\n. There are two possible out omes for e\ni\n-response: either the entity responds\nwith a de ision during this transition (produ ing a terminal state), or it does not, and we derive the probability distribution over the two from P. The \\wait\" a tion has a similar bran h, ex ept that the ontrolling-entity remains un hanged. Finally, the D a tion o urs instantaneously, so there is no time for the ontrolling entity to respond, but the resulting state e e tively moves to an earlier time (e.g., from t\nk\nto t\nk\nD\nvalue\n).\nWe an derive the reward fun tion for this MDP in a straightforward fashion from our strategy model. Table 3 presents the omplete spe i ation of this reward fun tion. In transitions that take up time, i.e., transferring ontrol and not re eiving a response (Table 3, row 1) or \\wait\" (Table 3, row 2), the agent in urs the wait ost of that interval. In transitions where the agent performs D, the agent in urs the ost of that a tion (Table 3, row 3). In terminal states with a response from e\ni\n, the agent derives the expe ted quality of\nthat entity's de ision (Table 3, row 4). A poli y that maximizes the reward that an agent expe ts to re eive a ording to this AA MDP model will orrespond exa tly to an optimal\ntransfer-of- ontrol strategy. Note that this reward fun tion is des ribed in an abstra t fashion|for example, it does not spe ify how to ompute the agent's expe ted quality of de ision, EQ A\nd\n(t)."
    }, {
      "heading" : "4.2 MDP Representation of AA Problem within Team Context",
      "text" : "We have now given a high-level des ription of an MDP for implementing the notion of transfer-of- ontrol strategies for AA. The remainder of this se tion provides a more detailed look at the MDP for a broad lass of AA domains (in luding the E-Elves) where the agent a ts on behalf of a user who is lling a role, , within the ontext of a team a tivity, . The reward fun tion ompares the EU of di erent strategies, nding the optimal one for the urrent state. To fa ilitate this al ulation, we need to represent the parameters used in the model. We introdu e the following state features to apture the aspe ts of the AA problem in a team ontext:\nteam-orig-expe t- is what the team originally expe ted of the ful lling of .\nteam-expe t- is the team's urrent expe tations of what ful lling the role implies.\nagent-expe t- is the agent's (probabilisti ) estimation for how will be ful lled.\n\\other attributes\" en apsulate other aspe ts of the joint a tivity that are a e ted by the de ision.\nWhen we add these more spe i features to the generi AA state features already\npresented, the overall state, within the MDP representation of a de ision d, is a tuple:\nh ontrolling-entity; team-orig-expe t- ; team-expe t- ; agent-expe t- ; -status; e\ni\n-response; time; other attributesi\nFor example, for a meeting s enario, team-orig-expe t- ould be \\Meet at 3pm\", teamexpe t- ould be \\Meet at 3:15pm\" after a user requested a delay, and agent-expe t- ould be \\Meet at 3:30pm\" if the agent believes its user will not make the res heduled meeting.\nThe transition probability fun tion for the AA MDP in a team ontext in ludes our underlying AA transition probabilities from Table 3, but it must also in lude probabilities over these new state features. In parti ular, in addition to the temporal e e t of the D a tion des ribed in Se tion 4.1, there is the additional e e t on the oordination of . The D a tion hanges the value of the team-expe t- feature (in a domain-dependent but\ndeterministi way). No other a tions a e t the team's expe tations. The team-orig-expe tfeature does not hange; we in lude it to simplify the de nition of the reward fun tion. The transition probabilities over agent-expe t- and other -spe i features are domain-spe i . We provide an example of su h transition probabilities in Se tion 4.3.\nThe nal part of the MDP representation is the reward fun tion. Our team AA MDP\nframework uses a reward fun tion that breaks down the fun tion from Table 3 as follows:\nR(s; a) = f(team-orig-expe t- (s); team-expe t- (s); agent-expe t- (s);\n-status(s); time(s); a) (9)\n=\nX\ne2EnfAg\nEQ\nd e (time(s)) e-response\n1\nf\n1\n(k team-orig-expe t- (s) team-expe t- (s) k)\n21\nf\n21\n(time(s))\n22\nf\n22\n(k team-expe t- (s) agent-expe t- (s) k)\n+\n3\nf\n3\n( -status(s)) +\n4\nf\n4\n(a)\n(10)\nThe rst omponent of the reward fun tion aptures the value of getting a response from a de ision-making entity other than the agent itself. Noti e that only one entity will a tually respond, so only one e-response will be non-zero. This orresponds to the EQ e\nd\n(t) fun tion\nused in the model and the bottom row of Table 3. The f\n1\nfun tion re e ts the inherent\nvalue of performing a role as the team originally expe ted, hen e deterring the agent from taking ostly oordination hanges unless they an gain some indire t value from doing so. This orresponds to D\nost\nfrom the mathemati al model and the third row of Table 3.\nThe f\n21\norresponds to the se ond row of Table 3, so it represents the wait ost fun tion,\nW(t), from the model. This omponent en ourages the agent to keep other team members informed of the role's status (e.g., by making a de ision or taking an expli it D a tion), rather than ausing them to wait without information. Fun tions f\n22\nand f\n3\nrepresent\nthe quality of the agent's de ision, represented by Q\nA d (t). The standard MDP algorithms\nompute an expe tation over the agent's reward, and an expe tation over this quality will\nprodu e the desired EQ\nA d (t) from the fourth row of Table 3. The rst quality fun tion, f 22 ,\nre e ts the value of keeping the team's understanding of how the role will be performed in a ordan e with how the agent expe ts the user to a tually perform the role. The agent re eives most reward when the role is performed exa tly as the team expe ts, but be ause of the un ertainty in the agent's expe tation, errors are possible. f\n22\nrepresents the osts that\nome with su h errors. The se ond quality omponent, f\n3\n, in uen es overall reward based\non the su essful ompletion of the joint a tivity, whi h en ourages the agent to take a tions that maximize the likelihood that the joint a tivity su eeds. The desire to have the joint task su eed is impli it in the mathemati al model but must be expli itly represented in the MDP. The omponent, f\n4\n, augments the rst row from Table 3 to a ount for additional\nosts of transfer-of- ontrol a tions. In parti ular, f\n4\nan be broken down further as follows:\nf\n4\n(a) =\n(\nq(e) if a 2 E 0 otherwise\n(11)\nThe fun tion q(e) represents the ost of transferring ontrol to a parti ular entity, e.g., the\nost of a WAP phone message to a user. Noti e, that these detailed, domain-spe i osts\ndo not appear dire tly in the model.\nGiven the MDP's state spa e, a tions, transition probabilities, and reward fun tion, an agent an use value iteration to generate a poli y P :S! that spe i es the optimal a tion in ea h state (Puterman, 1994). The agent then exe utes the poli y by taking the a tion that the poli y di tates in ea h and every state in whi h it nds itself. A poli y may in lude several transfers of ontrol and oordination- hange a tions. The parti ular series of a tions depends on the a tivities of the user. We an then interpret this poli y as a ontingent ombination of many transfer-of- ontrol strategies, with the strategy to follow\nhosen depending on the user's status (i.e., agent-expe t- )."
    }, {
      "heading" : "4.3 Example: The E-Elves MDPs",
      "text" : "An example of an AA MDP is the generi delay MDP, whi h an be instantiated for any meeting for whi h Friday may a t on behalf of its user. Re all the de ision, d, is whether to let other meeting attendees wait for a user or to begin their meeting. The joint a tivity,\n, is the meeting in whi h the agent has the role, , of ensuring that its user attends the meeting at the s heduled time. The oordination onstraints, , are that the attendees arrive at the meeting lo ation simultaneously and the e e t of the D a tion is to delay or\nan el the meeting.\nIn the delay MDP's state representation, team-orig-expe t- is originally-s heduledmeeting-time, sin e attendan e at the originally s heduled meeting time is what the team originally expe ts of the user and is the best possible out ome. team-expe t- is timerelative-to-meeting, whi h may in rease if the meeting is delayed. -status be omes statusof-meeting. agent-expe t- is not represented expli itly; instead, user-lo ation is used as an observable heuristi of when the user is likely to attend the meeting. For example, a user who is away from the department shortly before a meeting should begin is unlikely to be attending on time, if at all. With all the state features, the total state spa e ontains 2800 states for ea h individual meeting, with the large number of states arising from a very\nne-grained dis retization of time.\nThe general reward fun tion is mapped to the delay MDP reward fun tion in the fol-\nlowing way.\nf\n1\n=\n(\ng(N; ) if N < 4 1 otherwise\n(12)\nwhere N is the number of times the meeting is res heduled and g is a fun tion that takes into a ount fa tors like the number of meeting attendees, the size of the meeting delay and the time until the originally s heduled meeting time. This fun tion e e tively forbids the agent from ever performing 4 or more D a tions.\nIn the delay MDP, the fun tions, f\n21\nand f\n22\n, both orrespond to the ost of making the\nmeeting attendees wait, so we an merge them into a single fun tion, f\n2\n. We expe t that\nsu h a onsolidation is possible in similar domains where the team's expe tations relate to\nthe temporal aspe t of role performan e.\nf\n2\n=\n(\nh(late; ) if late > 0 0 otherwise\n(13)\nwhere late is the di eren e between the s heduled meeting time and the time the user arrives at the meeting room. late is probabilisti ally al ulated by the MDP based on the user's urrent lo ation and a model of the user's behavior.\nf\n3\n=\n8 > <\n> :\nr + r\nuser\nif the user attends\nr\nif the meeting takes pla e, but the user does not attend\n0 otherwise\n(14)\nThe value, r , models the inherent value of , while the value r\nuser\nmodels the user's\nindividual value to .\nf\n4\nwas given previously in Equation 11. The ost of ommuni ating with the user\ndepends on the medium whi h is used to ommuni ate. For example, there is higher ost to ommuni ating via a WAP phone than via a workstation dialog box.\nWhen the users are asked for input, it is assumed that, if they respond, their response will be \\ orre t\", i.e., if a user says to delay the meeting by 15 minutes, we assume the user will arrive on time for the re-s heduled meeting. If the user is asked while in front of his/her workstation, a dialog like the one shown in Figure 5 is popped up, allowing the user to sele t the a tion to be taken. The expe ted quality of the agent's de ision is al ulated by onsidering the agent's proposed de ision and the possible out omes of that de ision. For example, if the agent proposes delaying the meeting by 15 minutes, the al ulation of the de ision quality in ludes the probability and bene ts that the user will a tually arrive 15 minutes after the originally s heduled meeting time, the probability and osts that the user arrives at the originally s heduled meeting time, et .\n(a) (b)\nThe delay MDP also represents probabilities that a hange in user lo ation (e.g., from oÆ e to meeting lo ation) will o ur in a given time interval. Figure 5(b) shows a portion\nof the state spa e, showing only the user-response, and user lo ation features. A transition labeled \\delay n\" orresponds to the a tion \\delay by n minutes\". The gure also shows multiple transitions due to \\ask\" (i.e., transfer ontrol to the user) and \\wait\" a tions, where the relative probability of ea h out ome is represented by the thi kness of the arrow. Other state transitions orrespond to un ertainty asso iated with a user's response (e.g., when the agent performs the \\ask\" a tion, the user may respond with spe i information or may not respond at all, leaving the agent to e e tively \\wait\"). One possible poli y produ ed by the delay MDP, for a sub lass of meetings, spe i es \\ask\" in state S0 of Figure 5(b) (i.e., the agent gives up some autonomy). If the world rea hes state S3, the poli y spe i es \\wait\". However, if the agent then rea hes state S5, the poli y hooses \\delay 15\", whi h the agent then exe utes autonomously. In terms of strategies, this sequen e of a tions is HD.\nEarlier, we des ribed another AA de ision in the E-Elves, namely whether to lose an au tion for an open team role. Here, we brie y des ribe the key aspe ts of the mapping of that de ision to the MDP. The au tion must be losed in time for the user to prepare for the meeting, but with suÆ ient time given for interested users to submit bids and for the human team leader to hoose a parti ular user. team-orig-expe t- (s) is that a highquality presenter be sele ted with enough time to prepare. There is no D a tion, hen e team-expe t- (s) = team-orig-expe t- (s). agent-expe t- (s) is whether the agent believes it has a high-quality bid or believes su h a bid will arrive in time for that user to be allo ated to the role. The agent's de ision quality, EQ d\nA\n(t), is a fun tion of the number of bids that\nhave been submitted and the quality of those bids, e.g., if all team members have submitted bids and one user's bid stands out, the agent an on dently hoose that user to do the presentation. Thus, -status is primarily the quality of the best bid so far and the di eren e between the quality of that bid and the se ond-best bid. The most riti al omponent of the reward fun tion from Equation 10 is the\n2\nomponent, whi h gives reward if the agent\nful lls the users' expe tation of having a willing presenter do a high-quality presentation."
    }, {
      "heading" : "4.4 User-Spe i ed Constraints",
      "text" : "The standard MDP algorithms provide the agent with optimal poli ies subje t to the en-\noded probabilities and reward fun tion. Thus, if the agent designer has a ess to orre t models of the entities' (e.g., human users in the E-Elves) de ision qualities and probabilities of response, then the agent will sele t the best possible transfer-of- ontrol strategy. However, it is possible that the entities themselves have more a urate information about their own abilities than does the agent designer. To exploit this knowledge, an entity ould\nommuni ate its model of its quality of de ision and probability of response dire tly to the agent designer. Unfortunately, the typi al entity is unlikely to be able to express its knowledge in the form of our MDP reward fun tion and transition probabilities. An agent\nould potentially learn this additional knowledge on its own through its intera tions with the entities in the domain. However, learning may require an arbitrarily large number of su h intera tions, all of whi h will take pla e without the bene t of the entities' inside knowledge.\nAs an alternative, we an provide a language of onstraints that allows the entities to dire tly and immediately ommuni ate their inside information to the agent. Our onstraint\nlanguage provides the entities a simple way to inform the agent of their spe i properties and needs. An entity an use a onstraint to forbid the agent from entering spe i states or performing spe i a tions in spe i states. Su h onstraints an be dire tly ommuni ated by a user via the tool shown in Figure 6. For instan e, in the gure shown the user is forbidding the agent from autonomous a tion ve minutes before the meeting. We de ne su h forbidden-a tion onstraints to be a set, C\nfa\n, where ea h element onstraint is a\nboolean fun tion,\nfa\n:S A!ft; fg. Similarly, we de ne forbidden-state onstraints to\nbe a set, C\nfs\n, with elements,\nfs\n:S!ft; fg. If a onstraint returns t for a parti ular domain\nelement (either state or state-a tion pair, as appropriate), then the onstraint applies to the given element. For example, a forbidden-a tion onstraint,\nfa\n, forbids the a tion a from\nbeing performed in state s if and only if\nfa\n(s; a) = t.\nTo provide probabilisti semanti s, suitable for an MDP ontext, we rst provide some\nnotation. Denote the probability that the agent will ever arrive in state s\nf\nafter following\na poli y, P , from an initial state s\ni\nas Pr(s\ni\n!s\nf\njP ). Then, we de ne the semanti s of\na forbidden-state onstraint\nfs\nas requiring Pr(s\ni\n!s\nf\njP ) = 0. The semanti s given to a\nforbidden-a tion onstraint,\nfa\n, is a bit more omplex, requiring Pr(s\ni\n!s\nf\nP̂ (s\nf\n)=ajP ) = 0\n(i.e.,\nfa\nforbids the agent from entering state s\nf\nand then performing a tion a). In some\nases, an aggregation of onstraints may forbid all a tions in state s\nf\n. In this ase, the\nonjun tion allows the agent to still satisfy all forbidden-a tion onstraints by avoiding s\nf\n(i.e., the state s\nf\nitself be omes forbidden). On e a state, s\nf\n, be omes indire tly forbidden\nin this fashion, any a tion that potentially leads the agent from an an estor state into s\nf\nlikewise be omes forbidden. Hen e, the e e t of forbidding onstraints an propagate ba kward through the state spa e, a e ting state/a tion pairs beyond those whi h ause immediate violations.\nThe forbidding onstraints are powerful enough for the entity to ommuni ate a wide range of knowledge about their de ision quality and probability of response to the agent. For instan e, some E-Elves users have forbidden their agents from res heduling meetings to lun h time. To do so, the users provide a feature spe i ation of the states they want to forbid, su h as meeting-time=12 PM. Su h a spe i ation generates a forbidden-state\nonstraint,\nfs\n, that is true in any state, s, where meeting-time=12 PM in s. This onstraint\ne e tively forbids the agent from performing any D a tion that would result in a state where meeting-time=12PM. Similarly, some users have forbidden autonomous a tions in ertain states by providing a spe i ation of the a tions they want to forbid, e.g., a tion 6=\\ask\". This generates a forbidden-a tion onstraint,\nfa\n, that is true for any state/a tion pair,\n(s; a), with a 6=\\ask\". For example, a user might spe ify su h a onstraint for states where they are in their oÆ e, at the time of a meeting be ause they know that they will always make de isions in that ase. Users an easily reate more ompli ated onstraints by spe ifying values for multiple features, as well as by using omparison fun tions other than = (e.g., 6=, >).\nAnalogous to the forbidding onstraints, we also introdu e required-state and required-\na tion onstraints, de ned as sets, C\nrs\nand C\nra\n, respe tively. The interpretation provided\nto the required-state onstraint is symmetri , but opposite to that of the forbidden-state\nonstraint: Pr(s\ni\n!s\nf\njP ) = 1. Thus, from any state, the agent must eventually rea h a\nrequired state, s\nf\n. Similarly, for the required-a tion onstraint, Pr(s\ni\n!s\nf\n^P (s\nf\n)=ajP ) = 1.\nThe users spe ify su h onstraints as they do for their forbidding ounterparts (i.e., by spe - ifying the values of the relevant state features or a tion, as appropriate). In addition, the requiring onstraints also propagate ba kward. Informally, the forbidden onstraints fo us lo ally on spe i states or a tions, while the required onstraints express global properties over all states.\nThe resulting language allows the agent to exploit synergisti intera tions between its initial model of transfer-of- ontrol strategies and entity-spe i ed onstraints. For example, a forbidden-a tion onstraint that prevents the agent from taking autonomous a tion in a parti ular state is equivalent to the user spe ifying that the agent must transfer ontrol to the user in that state. In AA terms, the user instru ts the agent not to onsider any transferof- ontrol strategies that violate this onstraint. To exploit this pruning of the strategy spa e by the user, we have extended standard value iteration to also onsider onstraint satisfa tion when generating optimal strategies. Appendix II provides a des ription of a novel algorithm that nds optimal poli ies while respe ting user onstraints. The appendix also in ludes a proof of the algorithm's orre tness."
    }, {
      "heading" : "5. Experimental Results",
      "text" : "This se tion presents experimental results aimed at validating the laims made in the previous se tions. In parti ular, the experiments aim to show the utility of omplex transfer-of-\nontrol strategies and the e e tiveness of MDPs as a te hnique for their operationalization. Se tion 5.1 details the use of the E-Elves in daily a tivities and Se tion 5.2 dis usses the pros and ons of living and working with the assistan e of Fridays. Se tion 5.3 shows some\nhara teristi s of strategies in this type of domain (in parti ular, that di erent strategies\nare used in pra ti e). Finally, Se tion 5.4 des ribes detailed experiments that illustrate\nhara teristi s of the AA MDP."
    }, {
      "heading" : "5.1 The E-Elves in Daily Use",
      "text" : "The E-Elves system was heavily used by ten users in a resear h group at ISI, between June 2000 and De ember 2000. 5 The Friday agents ran ontinuously, around the lo k, seven days a week. The exa t number of agents running varied over the period of exe ution, with usually ve to ten Friday agents for individual users, a apability mat her (with proxy), and an interest mat her (with proxy). O asionally, temporary Friday agents operated on behalf of spe ial guests or other short-term visitors.\nDaily Counts of Exchanged Messages\nFigure 7 plots the number of daily messages ex hanged by the Fridays over seven months (June through De ember, 2000). The size of the daily ounts re e ts the large amount of\noordination ne essary to manage various a tivities, while the high variability illustrates the dynami nature of the domain (note the low periods during va ations and nal exams). Figure 8(a) illustrates the number of meetings monitored for ea h user. Over the seven months, nearly 700 meetings where monitored. Some users had fewer than 20 meetings, while others had over 250. Most users had about 50% of their meetings delayed (this in ludes regularly s heduled meetings that were an elled, for instan e due to travel). Figure 8(b) shows that usually 50% or more of delayed meetings were autonomously delayed. In this graph, repeated delays of a single meeting are ounted only on e. The graphs show that the\n5. The user base of the system was greatly redu ed after this period due to personnel relo ations and\nstudent graduations, but it remains in use with a smaller number of users.\n(a) (b)\nagents are a ting autonomously in a large number of instan es, but, equally importantly, humans are also often intervening, indi ating the riti al importan e of adjustable autonomy in Friday agents.\nFor a seven-month period, the presenter for USC/ISI's TEAMCORE resear h group presentations was de ided using au tions. Table 4 shows a summary of the au tion results. Column 1 (\\Date\") shows the dates of the resear h presentations. Column 2 (\\No. of Bids\") shows the total number of bids re eived before a de ision. A key feature is that au tion de isions were made without all 9 users entering bids; in fa t, in one ase, only 4 bids were re eived. Column 3 (\\Best bid\") shows the winning bid. A winner typi ally bid < 1; 1 >, i.e., indi ating that the user it represents is both apable and willing to do the presentation | a high-quality bid. Interestingly, the winner on July 27 made a bid of < 0; 1 >, i.e., not apable but willing. The team was able to settle on a winner despite the bid not being the highest possible, illustrating its exibility. Finally, olumns 4 (\\Winner\") and 5 (\\Method\") show the au tion out ome. An `H' in olumn 5 indi ates the au tion was de ided by a human, an `A' indi ates it was de ided autonomously. In ve of the seven au tions, a user was automati ally sele ted to be presenter. The two manual assignments were due to ex eptional ir umstan es in the group (e.g., a rst-time visitor), again illustrating the need for AA."
    }, {
      "heading" : "5.2 Evaluating the Pros and Cons of E-Elves Use",
      "text" : "The general e e tiveness of the E-Elves is shown by several observations. During the E-Elves' operation, the group members ex hanged very few email messages to announ e meeting delays. Instead, Fridays autonomously informed users of delays, thus redu ing the overhead of waiting for delayed members. Se ond, the overhead of sending emails to re ruit and announ e a presenter for resear h meetings was assumed by agent-run au tions. Third, a web page, where Friday agents post their users' lo ation, was ommonly used to avoid the overhead of trying to tra k users down manually. Fourth, mobile devi es kept users informed remotely of hanges in their s hedules, while also enabling them to remotely delay meetings, volunteer for presentations, order meals, et . Users began relying on Friday so heavily to order lun h that one lo al \\Subway\" restaurant owner even suggested: \\. . .more and more omputers are getting to order food. . . so we might have to think about marketing to them!!\". Noti e that this daily use of the E-Elves by a number of di erent users o urred only after the MDP implementation of AA repla ed the unreliable C4.5 implementation.\nHowever, while the agents ensured that users spent less time on daily oordination (and mis oordination), there was a pri e to be paid. One issue was that users felt they had less priva y when their lo ation was ontinually posted on the web and monitored by their agent. Another issue was the se urity of private information su h as redit ard numbers used for ordering lun h. As users adjusted to having agents monitor their daily a tivities, some users adjusted their own behavior around that of the agent. One example of su h behavior was some users preferring to be a minute or two early for a meeting lest their agent de ide they were late and delay the meeting. In general, sin e the agents never made\natastrophi ally bad de isions most users felt omfortable using their agent and frequently\ntook advantage of its servi es.\nThe most emphati eviden e of the su ess of the MDP approa h is that, sin e repla ing the C4.5 implementation, the agents have never repeated any of the atastrophi mistakes enumerated in Se tion 2.2. In parti ular, Friday avoids errors su h as error 3 from Se tion 2.2 by sele ting a strategy with a single, large D a tion, be ause it has a higher EU than a strategy with many small Ds (e.g., DDDD). Friday avoids error 1, be ause the large ost asso iated with an erroneous an el a tion signi antly penalizes the EU of a an ellation. Friday instead hooses the higher-EU strategy that rst transfers ontrol to a user before taking su h an a tion autonomously. Friday avoids errors su h as errors 2 and 4 by sele ting strategies in a situation-sensitive manner. For instan e, if the agent's de ision-making quality is low (i.e., high risk), then the agent an perform a oordination- hange a tion to allow more time for user response or for the agent itself to get more information. In other words, it exibly uses strategies like eDeA, rather than always using the e(5)A strategy dis ussed in Se tion 2.2. This indi ates that a reasonably appropriate strategy was hosen in ea h situation. Although the urrent agents do o asionally make mistakes, these errors are typi ally on the order of transferring ontrol to the user a few minutes earlier than may be ne essary. Thus, the agents' de isions have been reasonable, though not always optimal. 6\n6. The inherent subje tivity in user feedba k makes a determination of optimality diÆ ult."
    }, {
      "heading" : "5.3 Strategy Evaluation",
      "text" : "The previous se tion looked at the appli ation of the MDP approa h to the E-Elves but did not address strategies in parti ular. In this se tion, we spe i ally examine strategies in the E-Elves. We show that Fridays did indeed follow strategies and that the strategies followed were the ones predi ted by the model. We also show how the model led to an insight that, in turn, led to a dramati simpli ation in one part of the implementation. Finally, we show that the use of strategies is not limited to the E-Elves appli ation by showing empiri ally that, for random on gurations of entities, the optimal strategy will have more than one transfer-of- ontrol a tion in 70% of ases.\nFigure 9 shows a frequen y distribution of the number of a tions taken per meeting (this graph omits \\wait\" a tions). The number of a tions taken for a meeting orresponds to the length of the part of the strategy followed (the strategy may have been longer, but a de ision was made so the a tions were not taken). The graph shows both that the MDP followed omplex strategies in the real world and that it followed di erent strategies at di erent times. The graph bears out the model's predi tions that di erent strategies would be required of a good solution to the AA problem in the E-Elves domain.\nTable 5 shows the EU values omputed by the model and the strategy sele ted by the MDP. Re all that the MDP expli itly models the users' movements between lo ations, while the model assumes that the users do not move. Hen e, in order to do an a urate\nomparison between the model and the MDP's results, we fo us on only those ases when the user's lo ation does not hange (i.e., where the probability of response is onstant). These EU values were al ulated using the parameter values set out in Se tion 3.3. Noti e, that the MDP will often perform Ds before transferring ontrol to buy time to redu e un ertainty. The model is an abstra tion of the domain, so su h D a tions, like hanges in user lo ation, are not aptured. Ex ept for a slight dis repan y in the rst ase the mat h between the MDP's behavior and the model's predi tions is exa t, provided that we ignore the D a tions at the beginning of some MDP strategies. Thus, despite the model being onsiderably abstra ted from the domain there is high orrelation between the MDP poli ies and the model's suggested strategies. Moreover, general properties of the poli ies that were predi ted by the model were borne out exa tly. In parti ular, re all that the model predi ted di erent strategies would be required, that strategy e would not be used, and that generally strategies ending in A would be best | all properties of the MDP poli ies.\nThe model predi ts that if parameters do not vary greatly then it is suÆ ient to nd a single optimal strategy and follow that strategy in ea h situation. The MDP for the de ision to lose an au tion is an instan e of this for the E-Elves. The same pattern of behavior is followed every time an open role needs to be lled by the team. This onsisten y arises be ause the wait ost is the same (sin e the meetings are the same) and be ause the pattern of in oming bids is reasonably onsistent (variations in individuals' behavior\nan el ea h other out when we look at the team as a whole). The model predi ts that when parameters do not hange, we an nd the optimal strategy for those parameters and exe ute that strategy every time. However, sin e the MDP had worked e e tively for the meeting AA, an MDP was also hosen for implementing the au tion AA. When it was realized that the parameters do not vary greatly, we on luded the MDP ould be repla ed with a simple implementation of the optimal strategy. To verify this hypothesis, we repla ed\nthe general MDP ode with three simple lines of ode implementing the eA strategy, whi h we determined to be optimal for the parti ular parameters of the problem. Using log les re orded during the a tual au tions reported in (S erri, Pynadath, & Tambe, 2001), we experimentally veri ed that both the MDP and the eA strategy produ ed the same result. Table 6 shows the per entage of available au tion time remaining (e.g., if the au tion was opened four days before the role should be performed, losing the au tion one day before would orrespond to 25%) when the MDP version and the eA version of the ode losed the au tion. The number of bids is used to estimate the agent's expe ted de ision quality. The timing of the au tion losing is lose, ertainly within just a few hours. The result is not pre isely the same for the MDP and strategy implementations, be ause the MDP implementation was more rea tive to in oming bids than the strategy implementation.\nTo on rm that the need for strategies was not a phenomenon unique to the parti ular settings of the E-Elves, an experiment was run with randomly generated on gurations of entities. The wait ost for ea h on guration in reased exponentially, with the rate of a rual varying from on guration to on guration. The on gurations ontained between 3 and 25 entities, with randomly hosen Markovian response probabilities and randomly\nhosen, but onstant, de ision-making quality. The ost and value of a D a tion was also randomly sele ted. In ea h on guration, there was an agent that ould respond instantly, but with lower de ision quality than any of the other entities. For ea h on guration, the optimal transfer-of- ontrol strategy was found. Figure 10(a) shows the per entage of optimal strategies (z-axis) that were of ea h length (y-axis \\jOpt. Strat.j\"), separated a ording to the rate at whi h wait osts a rued (x-axis, \\Wait Cost Param\"). The gure shows that if the rate at whi h the wait ost a rues is very low, most optimal strategies are of length one, with the agent just handing ontrol to the entity with the highest de ision-making quality. When the rate of wait ost a rual is high, most strategies are of length two, with the agent brie y giving the best de ision maker an opportunity to make the de ision but taking ba k ontrol and a ting before the wait osts be ame too high. For intermediate values of the wait ost parameter, there was onsiderably more variation in the length of the optimal strategy. Figure 10(b) shows the per entage of optimal strategies for ea h length when the wait ost parameter is 0.12 (i.e., a sli e through Figure 10(a)). Hen e, strategies often ontained several transfers of ontrol and several oordination hanges. Thus, this experiment shows that omplex transfer-of- ontrol strategies are useful, not only in E-Elves,\nbut in a range of domains, espe ially those in whi h wait osts are neither negligible nor a ruing too fast.\n(a) (b)\nThus, we have shown that the MDP produ es strategies and that Friday follows these strategies in pra ti e. Moreover, the strategies followed are the ones predi ted by the model. Of pra ti al use, when we followed a predi tion of the model, i.e., that an MDP was not required for au tions, we were able to substantially redu e the omplexity of one part of the system. Finally, we showed that the need for strategies was not spe i ally a phenomenon of the E-Elves domain."
    }, {
      "heading" : "5.4 MDP Experiments",
      "text" : "Experien e using the MDP approa h to AA in the E-Elves indi ates that it is e e tive at making reasonable AA de isions. However, in order to determine whether MDPs are a generally useful tool for AA reasoning, more systemati experiments are required. In this se tion, we present su h systemati experiments to determine important properties of MDPs for AA. The MDP reward fun tion is designed to result in the optimal strategy being followed in ea h state.\nIn ea h of the experiments, we vary one of the parameters that are the weights of the di erent fa tors in Equation 10. The MDP is instantiated with ea h of a range of values for the parameter and a poli y produ ed for ea h value. In ea h ase, the total poli y is de ned over 2800 states. The poli y is analyzed to determine some basi properties of that poli y. In parti ular, we ounted the number of states in whi h the poli y spe i es to ask, to delay, to say the user is attending and to say the user is not attending. The statisti s show broadly how the poli y hanges as the parameters hange, e.g., whether Friday gives up autonomy more or less when the ost of a oordination hange is in reased. The rst aim of the experiments is to simply on rm that poli ies hange in the desired and expe ted way when parameters in the reward fun tion are hanged. For instan e, if Friday's expe ted de ision quality is in reased, there should be more states where it makes an autonomous\nde ision. Se ondly, from a pra ti al perspe tive it is riti al to understand how sensitive the MDP poli ies are to small variations in parameters, be ause su h sensitivity would mean that any small variations in parameter values an signi antly impa t MDP performan e. Finally, the experiments reveal some interesting phenomena.\nThe rst experiment looks at the e e t of the\n1\nparameter from Equation 10, repre-\nsented in the delay MDP implementation by the team repair ost (fun tion g from Equation 12), on the poli ies produ ed by the delay MDP. This parameter determines how averse Friday should be to hanging oordination onstraints. Figure 11 shows how some properties of the poli y hange as the team repair ost value is varied. The x-axis gives the value of the team repair ost, and the y-axis gives the number of times that a tion appears in the poli y. Figure 11(a) shows the number of times Friday will ask the user for input. The number of times it will transfer ontrol exhibits an interesting phenomenon: the number of asks has a maximum at an intermediate value for the parameter. For the low values, Friday an \\ on dently\" (i.e., its de ision quality is high) make de isions autonomously, sin e the ost of errors is low, hen e there is less value to relinquishing autonomy. For very high team repair osts, Friday an \\ on dently\" de ide autonomously not to make a\noordination hange. It is in the intermediate region that Friday is un ertain and needs to all on the user's de ision making more often. Furthermore, as the ost of delaying the meeting in reases, Friday will delay the meeting less (Figure 11(b)) and tell the team the user is not attending more often (Figure 11(d)). By doing so, Friday gives the user less time to arrive at the meeting, hoosing instead to just announ e that the user is not attending. Essentially, Friday's de ision quality has be ome lose enough to the user's de ision quality that asking the user is not worth the risk that they will not respond and the ost of asking for their input. Ex ept for a jump between a value of zero and any non-zero value, the number of times Friday says the user is attending does not hange (Figure 11( )). The delay MDP in use in the E-Elves has the team repair ost parameter set at two. Around this value the poli y hanges little, hen e slight hanges in the parameter do not lead to large hanges in the poli y.\nIn the se ond experiment, we vary the\n2\nparameter from Equation 10, implemented\nin the delay MDP by the variable team wait ost (fun tion h from Equation 13). This is the fa tor that determines how heavily Friday should weigh di eren es between how the team expe ts the user will ful ll the role and how the user will a tually ful ll the role. In parti ular, it determines the ost of having other team members wait in the meeting room for the user. Figure 12 shows the hanges to the poli y when this parameter is varied (again the x-axis shows the value of the parameter and the y-axis shows the number of times the a tion appears in the poli y). The graph of the number of times the agent asks in the poli y (Figure 12(a)), exhibits the same phenomena as when the\n1\nparameter was varied,\ni.e., in reasing and then de reasing as the parameter in reases. The graphs show that, as the ost of teammates' time in reases, Friday a ts autonomously more often (Figure 12(bd)). Friday asks whenever the potential osts of asking are lower than the potential osts of errors it makes { as the ost of time waiting for a user de ision in reases, the balan e tips towards a ting. Noti e that the phenomenon of the number of asks in reasing then de reasing o urs in the same way that it did for the\n1\nparameter; however, it o urs for a\nslightly di erent reason. In this ase, when waiting osts are low, Friday's de ision-making quality is high so it a ts autonomously. When the waiting osts are high, Friday annot\n( ) (d)\na ord the risk that the user will not respond qui kly, so it again a ts autonomously (despite its de ision quality being low). Figure 12(b) shows that the number of delay a tions taken by Friday in reases, but only in states in whi h the meeting has already been delayed twi e. This indi ates that the normally very expensive third delay of the same meeting starts to be ome worthwhile if the ost of having teammates wait in the meeting room is very high. In the delay MDP, a value of 1 is used for\n2\n. The de ision to transfer ontrol (i.e., ask)\nis not parti ularly sensitive to hanges in the parameter around this value|again, slight\nhanges will not have a signi ant impa t.\n( ) (d)\nIn the third experiment, the value of the\n3\n, the weight of the joint task, was varied\n(Figure 13). In the E-Elves, the value of the joint task in ludes the value of the user to the meeting and the value of the meeting without the user. In this experiment, the value of the\nmeeting without the user is varied. Figure 13 shows how the poli y hanges as the value of the meeting without the user hanges (again the x-axis shows the value of the parameter and the y-axis shows the number of times the a tion appears in the poli y). These graphs show signi antly more instability than for the other values. These large hanges are a result of the simultaneous hange in both the utility of taking key a tions and the expe ted quality of Friday's de ision making, e.g., the utility of saying the user is attending is mu h higher if the meeting has very low value without that user. In the urrent delay MDP, this value is set at 0.25, whi h is in a part of the graph that is very insensitive to small hanges of the parameter.\nIn the three experiments above, the spe i E-Elves parameters were in regions of the graph where small hanges in the parameter do not lead to signi ant hanges in the poli y. However, there were regions of the graphs where the poli y did hange dramati ally for small\nhanges in a parameter. This indi ates that in some domains, with parameters di erent to\nthose in E-Elves, the poli ies will be sensitive to small hanges in the parameters.\n( ) (d)\nThe above experiments show three important properties of the MDP approa h to AA. First, hanging the parameters of the reward fun tion generally lead to the hanges in the poli y that are expe ted and desired. Se ond, while the value of the parameters in uen ed the poli y, the e e t on the AA reasoning was often reasonably small, suggesting that small errors in the model should not a e t users too greatly. Finally, the interesting phenomena of the number of asks rea hing a peak at intermediate values of the parameters was revealed.\nThe three previous experiments have examined how the behavior of the MDP hanges as the parameters of the reward fun tion are hanged. In another experiment, a entral domain-level parameter a e ting the behavior of the MDP, i.e., the probability of getting a user response and the ost of getting that response ( orresponding to f\n4\n), is varied. Figure\n14 shows how the number of times Friday hooses to ask (y-axis) varies with both the expe ted time to get a user response (x-axis) and the ost of doing so (ea h line on the graph represents a di erent ost). The MDP performs as expe ted, hoosing to ask more often if the ost of doing so is low and/or it is likely to get a prompt response. Noti e that, if the ost is low enough, Friday will sometimes hoose to ask the user even if there is a long expe ted response time. Conversely, if the expe ted response time is suÆ iently high, Friday will assume omplete autonomy. This graph also shows that there is a distin t\nhange in the number of asks at some point (depending on the ost), but outside this hange point the graphs are relatively at. The key reason for the fairly rapid hange in the number of asks is that often the di eren e between the quality of Friday's and the user's de ision making is in a fairly small range. As the mean response time in reases, the expe ted wait\nosts in rease, eventually be oming high enough for Friday to de ide to a t autonomously\ninstead of asking.\nWe on lude this se tion with a quantitative illustration of the impa t onstraints have on strategy sele tion. In this experiment, we merged user-spe i ed onstraints from all the E-Elves users, resulting in a set of 10 distin t onstraints. We started with an un onstrained\ninstan e of the delay MDP and added these onstraints one at a time, ounting the strategies that satis ed the applied onstraints. We then repeated these experiments on expanded instan es of the delay MDP, where we in reased the initial state spa e by in reasing the frequen y of de isions (i.e., adding values to the time-relative-to-meeting feature). This expansion results in three new delay MDPs, whi h are arti ial, but are in uen ed by the real delay MDP. Figure 15a displays these results (on a logarithmi s ale), where line A\norresponds to the original delay MDP (2760 states), and lines B (3320 states), C (3880 states), and D (4400 states) orrespond to the expanded instan es. Ea h data point is a mean over ve di erent orderings of onstraint addition. For all four MDPs, the onstraints substantially redu e the spa e of possible agent behaviors. For instan e, in the original delay MDP, applying all 10 onstraints eliminated 1180 of the 2760 original states from\nonsideration, and redu ed the mean number of viable a tions per a eptable state from\n3.289 to 2.476. The end result is a 50% redu tion in the size (log\n10\n) of the strategy spa e.\nOn the other hand, onstraints alone did not provide a omplete strategy, sin e all of the plots stay well above 0, even with all 10 onstraints. Sin e none of the individual users were able/willing to provide 10 onstraints, we annot expe t anyone to add enough onstraints to ompletely spe ify an entire strategy. Thus, the MDP representation and asso iated poli y sele tion algorithms are still far from redundant.\nThe onstraints' elimination of behaviors also de reases the time required for strategy sele tion. Figure 15b plots the total time for onstraint propagation and value iteration over the same four MDPs as in Figure 15a (averaged over the same ve onstraint orderings). Ea h data point is also a mean over ve separate iterations, for a total of 25 iterations per data point. The values for the zero- onstraint ase orrespond to standard value iteration without onstraints. The savings in value iteration over the restri ted strategy spa e dramati ally outweigh the ost of pre-propagating the additional onstraints. In addition, the savings in rease with the size of the MDP. For the original delay MDP (A), there is a 28% redu tion in poli y-generation time, while for the largest MDP (D), there is a 53% redu tion. Thus, the introdu tion of onstraints an provide dramati a eleration of the agent's strategy sele tion."
    }, {
      "heading" : "6. Related Work",
      "text" : "We have dis ussed some related work in Se tion 1. This se tion adds to that dis ussion. In Se tion 6.1, we examine two representative AA systems { where detailed experimental results have been presented { and explain those results via our model. This illustrates the potential appli ability of our model to other systems. In Se tion 6.2, we examine other AA systems and other areas of related work, su h as meta-reasoning, onditional planning and anytime algorithms."
    }, {
      "heading" : "6.1 Analyzing Other AA Work Using the Strategy Model",
      "text" : "Goodri h, Olsen, Crandall, and Palmer (2001) report on tele-operated teams of robots, where both the user's high-level reasoning and the robots' low-level skills are required to a hieve some task. Within this domain, they have examined the e e t of user negle t on robot performan e. The idea of user negle t is similar to our idea of entities taking time to make de isions; in this ase, if the user \\negle ts\" the robot, the joint task takes longer to perform. In this domain, the oordination onstraint is that user input must arrive so that the robot an work out the low-level a tions it needs to perform. Four ontrol systems were tested on the robot, ea h giving a di erent amount of autonomy to the robot, and the performan e was measured as user negle t was varied.\nAlthough quite distin t from the E-Elves system, mapping Goodri h's team of robots to our AA problem formulation provides some interesting insights. This system has the interesting feature that the entity the robot an all on for a de ision, i.e., the user, is also part of the team. Changing the autonomy of the robot e e tively hanges the nature of the oordination onstraints between the user and robot. Figure 16 shows the performan e (y-axis) of the four ontrol poli ies as the amount of user negle t was in reased (x-axis). The experiments showed that higher robot autonomy allowed the operator to \\negle t\" the robot more without as serious an impa t on its performan e.\nThe notion of transfer-of- ontrol strategies an be used to qualitatively predi t the same behavior as was observed in pra ti e, even though Goodri h et al. (2001) did not use the notion of strategies. The lowest autonomy ontrol poli y used by Goodri h et al. (2001) was a pure tele-operation one. Sin e the robot annot resort to its own de ision making, we represent this ontrol poli y with a strategy U , i.e., ontrol inde nitely in the hands of the user. The se ond ontrol poli y allows the user to spe ify waypoints and on-board intelligen e works out the details of getting to the waypoints. Sin e the robot has no highlevel de ision-making ability, the strategy is simply to give ontrol to the user. However, sin e the oordination between the robot and user is more abstra t, i.e., the oordination\nonstraints are looser, the wait ost fun tion is less severe. Also the human is giving less detailed guidan e than in the fully tele-operated ase (whi h is not as good a ording to (Goodri h et al., 2001)), hen e we use a lower value for the expe ted quality of the user de ision. We denote this approa h U\nw\np to distinguish it from the fully tele-operated ase.\nThe next ontrol poli y allows the robot to hoose its own waypoints given that the user inputs regions of interest. The robot an also a ept waypoints from the user. The ability for the robot to al ulate waypoints is modeled as a D, sin e it e e tively hanges the\noordination between the entities, by removing the user's need to give waypoints. We model this ontrol poli y as the strategy UDU . The nal ontrol poli y is full autonomy, i.e., A.\n(b)\nRobot de ision making is inferior to that of the user, hen e the robot's de ision quality is less than the user's. The graphs of the four strategies, plotted against the probability of response parameter (getting smaller to the right, to mat h \\negle t\" in the Goodri h et al graph) is shown in Figure 16. Noti e that the shape of the graph theoreti ally derived from our model, shown in Figure 16(b), is qualitatively the same as the shape of the experimentally derived graph, Figure 16(a). Hen e, the theory predi ted qualitatively the same performan e as was found from experimentation.\nA ommon assumption in earlier AA work has been that if any entity is asked for a de ision it will make that de ision promptly, hen e strategies handling the ontingen y\nof a la k of response have not been required. For example, Horvitz's (1999) work using de ision theory is aimed at developing general, theoreti al models for AA reasoning for a user at a workstation. A prototype system, alled LookOut, for helping users manage their\nalendars has been implemented to test these ideas (Horvitz, 1999). Although su h systems are distin tly di erent from E-Elves, mapping them to our problem formulation allows us to analyze the utility of the approa hes a ross a range of domains without having to implement the approa h in those domains.\nA riti al di eren e between Horvitz's work and our work is that LookOut does not address the possibility of not re eiving a (timely) response. Thus, omplex strategies are not required. In the typi al ase for LookOut, the agent has three options: to take some a tion, not to take the a tion, or to engage in dialog. The entral fa tor in uen ing the de ision is whether the user has a parti ular goal that the a tion would aid, i.e., if the user has the goal, then the a tion is useful, but if he/she does not have the goal, the a tion is disruptive. Choosing to a t or not to a t orresponds to pursuing strategy A. 7 Choosing to seek user input orresponds to strategy U . Figure 17(a) shows a graph of the di erent options plotted against the probability the user has the goal ( orresponds to Figure 6 in Horvitz (1999)). The agent's expe ted de ision quality, EQ d\nA\n(t) is derived from Equation\n2 in Horvitz (1999). (In other words, Horvitz's model performs more detailed al ulations of expe ted de ision quality.) Our model then predi ts the same sele tion of strategies as Horvitz does, i.e., hoosing strategy A when EQ d\nA\n(t) is low, U otherwise (assuming that\nonly those two strategies are available). However, our model further predi ts something that Horvitz did not onsider, i.e., that if the rate at whi h wait osts a rue be omes non-negligible then the hoi e is not as simple. Figure 17(b) shows how the EU of the two strategies hanges as the rate of wait osts a ruing is in reased. The fa t that the optimal strategy varies with wait ost suggests that Horvitz's approa h would not immediately be appropriate for a domain where wait osts were non-negligible, e.g., it would need to be modi ed in many multi-agent settings."
    }, {
      "heading" : "6.2 Other Approa hes to AA",
      "text" : "Several di erent approa hes have been taken to the ore problem of whether and when to transfer de ision-making ontrol. For example, Hexmoor examines how mu h time the agent has to do AA reasoning (Hexmoor, 2000). Similarly, in the Dynami Adaptive Autonomy framework, a group of agents allo ates votes amongst themselves, hen e de ning the amount of in uen e ea h agent has over a de ision and thus, by their de nition, the autonomy of the agent with respe t to the de ision (Barber, Martin, & M kay, 2000b). For the related appli ation of meeting s heduling Cesta, Collia, and D'Aloisi (1998) have taken the approa h of providing powerful tools for users to onstrain and monitor the behavior of their proxy agents, but the agents do not expli itly reason about relinquishing ontrol to the user. While at least some of this work is done in a multiagent ontext, the possibility of multiple transfers of ontrol is not onsidered.\nComplementing our work, other resear hers have fo used on issues of ar hite tures for AA. For instan e, an AA interfa e to the 3T ar hite ture (Bonasso, Firby, Gat, Kortenkamp,\n7. We onsider hoosing not to a t an autonomous de ision, hen e ategorize it in the same way as au-\ntonomous a tion\nHorvitz’s EU Calculations with Wait Cost\n(b)\nMiller, & Sla k, 1997) has been implemented to solve human-ma hine intera tion problems experien ed in a number of NASA proje ts (Brann, Thurman, & Mit hell, 1996). The experien es showed that intera tion with the system was required all the way from the deliberative layer through to detailed ontrol of a tuators. The AA ontrols at all layers are en apsulated in what is referred to as the 3T's fourth layer { the intera tion layer\n(S hre kenghost, 1999). A similar area where AA te hnology is required is for safety- riti al intelligent software, su h as for ontrolling nu lear power plants and oil re neries (Musliner & Krebsba h, 1999). That work has resulted in a system alled AEGIS (Abnormal Event Guidan e and Information System) that ombines human and agent apabilities for rapid rea tion to emergen ies in a petro- hemi al re ning plant. AEGIS features a shared task representation that both the users and the intelligent system an work with (Goldman, Guerlain, Miller, & Musliner, 1997). A key hypothesis of the work is that the model needs to have multiple levels of abstra tion so that the user an intera t at the level they see t. Interesting work by Fong, Thorpe, and Baur (2002) has extended the idea of tele-operated roboti s by re-de ning the relationship between the robot and user as a ollaborative one, rather than the traditional master-slave on guration. In parti ular, the robot treats the human as a resour e that an perform per eptual or ognitive fun tions that the robot determines it annot adequately perform. However, as yet the work has not looked at the possibility that the user is not available to provide input when required, whi h would require the robot perform more omplex transfer-of- ontrol reasoning.\nWhile most previous work in AA has ignored omplex strategies for AA, there is work in other resear h elds that is potentially relevant. For example, the resear h issues addressed by elds su h as mixed-initiative de ision-making (Collins, Bilot, Gini, & Mobasher, 2000b), anytime algorithms (Zilberstein, 1996), multi-pro essor s heduling (Stankovi , Ramamritham, & Cheng, 1985), meta-reasoning (Russell & Wefald, 1989), game theory (Fudenberg & Tirole, 1991), and ontingen y plans (Draper, Hanks, & Weld, 1994; Peot & Smith, 1992) all have, at least super ial, similarities with the AA problem. However, it turns out that the ore assumptions and fo us of these other resear h areas are di erent enough that the algorithms developed in these related elds are not dire tly appli able to the AA problem.\nIn mixed-initiative de ision making a human user is assumed to be ontinually available (Collins et al., 2000b; Ferguson & Allen, 1998), negating any need for reasoning about the likelihood of response. Furthermore, there is often little or no time pressure or oordination\nonstraints. Thus, while the basi problem of transferring ontrol between a human and agent is ommon to both mixed-initiative de ision making and AA, the assumptions are quite di erent leading to distin t solutions. Likewise, other related resear h elds make distin tly di erent assumptions whi h lead to distin tly di erent solutions. For instan e,\nontingen y planning (Draper et al., 1994; Peot & Smith, 1992) deals with the problem of reating plans to deal with riti al developments in the environment. Strategies are related to ontingen y planning in that they are plans to deal with the spe i ontingen y of an entity not making a de ision in a manner that maintains oordination. However, in ontingen y planning, the key diÆ ulty is in reating the plans. In ontrast, in AA, reating strategies is straightforward and the key diÆ ulty is hoosing between those strategies. Our\nontribution is in re ognizing the need for strategies in addressing the AA problem, instantiating su h strategies via MDPs, and the development of a general, domain-independent reward fun tion that leads to an MDP hoosing the optimal strategy for a parti ular situation.\nSimilarly, another related resear h area is meta-reasoning (Russell & Wefald, 1989).\nMeta-reasoning work looks at online reasoning about omputation. A type of meta-reasoning, most losely related to AA, hooses between sequen es of omputations with di erent ex-\npe ted quality and running time, subje t to the onstraint that hoosing the highest-quality sequen e of omputations is not possible (be ause it takes too long) (Russell & Wefald, 1989). The idea is to treat omputations as a tions and \\meta-reason\" about the EU of doing ertain ombinations of omputation and (base-level) a tions. The output of metareasoning is a sequen e of omputations that are exe uted in sequen e. AA parallels metareasoning if we onsider reasoning about transferring ontrol to entities as reasoning about sele ting omputations, i.e., we think of entities as omputations. However, in AA, the aim is to have one entity make a high-quality de ision, while in meta-reasoning, the aim is for a sequen e of omputations to have some high quality. Moreover, the meta-reasoning assumption that omputations are guaranteed to return a timely result if exe uted, does not apply in AA. Finally, meta-reasoning looks for a sequen e of omputations that use a\nxed amount of time, while AA reasons about trading o extra time for a better de ision (possibly buying time with a D a tion). Thus, algorithms developed for meta-reasoning are not appli able to AA.\nAnother resear h area with on eptual similarity to AA is the eld of anytime algorithms (Zilberstein, 1996). An anytime algorithm qui kly nds an initial solution and then in rementally tries to improve the solution until stopped. The AA problem is similar when we assume that the agent itself an make an immediate de ision, be ause the problem then has the property that a solution is always available (an important property of an anytime algorithm). However, this will not be the ase in general, i.e., the agent will not always have an answer. Furthermore, anytime algorithms do not generally need to deal with multiple, distributed entities, nor do they have the opportunity to hange oordination (i.e., using a D a tion).\nMulti-pro essor s heduling looks at assigning tasks to nodes in order to meet ertain time onstraints (Stankovi et al., 1985). If entities are thought of as \\nodes\", then AA is also about assigning tasks to nodes. In multipro essor s heduling, the quality of the\nomputation performed on ea h of the nodes is usually assumed to be equal, i.e., the nodes are homogeneous. Thus, reasoning that trades o quality and time is not required, as it is in AA. Moreover, deadlines are externally imposed for multi-pro essor s heduling algorithms, rather than being exibly reasoned about as in AA. Multi-pro essor s heduling algorithms\nan sometimes deal with a node reje ting a task be ause it annot ful ll the time onstraints or network failures. However, while the AA problem fo uses on failure to get a response as a entral issue and load balan ing as an auxiliary issue, multi-pro essor s heduling has the opposite fo us. The di eren e in fo us leads to algorithms being developed in the multipro essor s heduling ommunity that are not well suited to AA (and vi e versa)."
    }, {
      "heading" : "7. Con lusions",
      "text" : "Adjustable autonomy is riti al to the su ess of real-world agent systems be ause it allows an agent to leverage the skills, resour es and de ision-making abilities of other entities, both human and agent. Previous work has addressed AA in the ontext of single-agent and single-human s enarios, but those solutions do not s ale to in reasingly omplex multiagent systems. In parti ular, previous work used rigid, one-shot transfers of ontrol that did not onsider team osts and, more importantly, did not onsider the possibility of ostly\nmis oordination between team members. Indeed, when we applied a rigid transfer-of- ontrol approa h to a multi-agent ontext, it failed dramati ally.\nThis arti le makes three key ontributions to enable the appli ation of AA in more omplex multiagent domains. First, the arti le introdu es the notion of a transfer-of- ontrol strategy. A transfer-of- ontrol strategy onsists of a onditional sequen e of two types of a tions: (i) a tions to transfer de ision-making ontrol and (ii) a tions to hange an agent's pre-spe i ed oordination onstraints with team members, aimed at minimizing mis oordination osts. Su h strategies allow agents to plan sequen es of transfer-of- ontrol a tions. Thus, a strategy allows the agent to transfer ontrol to entities best able to make de isions, buy more time for de isions to be made and still avoid mis oordination | even if the entity to whi h ontrol is transferred fails to make the de ision. Additionally, we introdu ed the idea of hanging oordination onstraints as a me hanism for giving the agent more opportunity to provide high-quality de isions, and we showed that su h hanges\nan, in some ases, be an e e tive way of in reasing the team's expe ted utility.\nThe se ond ontribution of this arti le is a mathemati al model of AA strategies that allows us to al ulate the expe ted utility of su h strategies. The model shows that while\nomplex strategies are indeed better than single-shot strategies in some situations, they are not always superior. In fa t, our analysis showed that no parti ular strategy dominates over the whole spa e of AA de isions; instead, di erent strategies are optimal in di erent situations.\nThe third ontribution of this arti le is the operationalization of the notion of transferof- ontrol strategies via Markov De ision Pro esses and a general reward fun tion that leads the MDP to nd optimal strategies in a multiagent ontext. The general, domainindependent reward fun tion should allow our approa h to potentially be applied to other multi-agent domains. We implemented, applied, and tested our MDP approa h to AA reasoning in a real-world appli ation supporting resear hers in their daily a tivities. Daily use showed the MDP approa h to be e e tive at balan ing the need to avoid risky autonomous de isions and the potential for ostly mis oordination. Furthermore, detailed experiments showed that the poli ies produ ed by the MDPs have desirable properties, su h as transferring ontrol to the user less often when the probability of getting a timely response is low. Finally, pra ti al experien e with the system revealed that users require the ability to manipulate the AA reasoning of the agents. To this end, we introdu ed a onstraint language that allows the user to limit the range of behavior the MDP an exhibit. We presented an algorithm for pro essing su h onstraints, and we showed it to have the desirable property of redu ing the time it takes to nd optimal poli ies."
    }, {
      "heading" : "8. Future Work",
      "text" : "The model of AA presented in this arti le is suÆ iently ri h to model a wide variety of interesting appli ations. However, there are some key fa tors that are not modeled in the\nurrent formulation that are required for some domains. One key issue is to allow an agent to fa tor the AA reasoning of other agents into its own AA reasoning. For instan e, in the Elves domain, if one agent is likely to de ide to delay a meeting, another agent may wait until that de ision and avoid asking its user. Conversely, if an agent about to take ba k ontrol of a de ision knows another agent is going to ontinue waiting for user input,\nit might also ontinue to wait for input. Su h intera tions will substantially in rease the\nomplexity of the reasoning an agent needs to perform. In this arti le, we have assumed that the agent is nding a transfer-of- ontrol strategy for a single, isolated de ision. In general, there will be many de isions to be made at on e and the agent will not be able to ignore the intera tions between those de isions. For example, transferring ontrol of many de isions to a user, redu es the probability of getting a prompt response to any of them. Reasoning about these intera tions will add further omplexity to the required reasoning of the agent.\nAnother fo us of future work will be generalizing the AA de ision making to allow other types of onstraints | not just oordination onstraints | to be taken into a ount. This would in turn require generalization of the on ept of a D a tion to in lude other types of stop-gap a tions and may lead to di erent types of strategies an agent ould pursue. Additionally, transfer-of- ontrol a tions ould be generalized to allow parts of a de ision to be transferred, e.g., to allow input to be re eived from a user without transferring total\nontrol to him/her, or allow a tions that ould be performed ollaboratively. Similarly, if a tions were reversible, the agent ould make the de ision but allow the user to reverse it. We hope that su h generalizations would improve the appli ability of our adjustable autonomy resear h in more omplex domains."
    }, {
      "heading" : "A knowledgments",
      "text" : "This resear h was supported by DARPA award no. F30602-98-2-0108. The e ort is being managed by Air For e Resear h Labs/Rome site. This arti le uni es, generalizes, and signifi antly extends approa hes des ribed in our previous onferen e papers (S erri et al., 2001; S erri, Pynadath, & Tambe, 2002; Pynadath & Tambe, 2001). We thank our olleagues, espe ially, Craig Knoblo k, Yolanda Gil, Hans Chalupsky and Tom Russ for ollaborating on the Ele tri Elves proje t. We would also like to thank the JAIR reviewers for their useful omments."
    }, {
      "heading" : "Appendix A: An Example Instantiation of the Model",
      "text" : "In this Appendix, we present a detailed look at one possible instantiation of the AA model. We use that instantiation to al ulate the EU of ommonly used strategies and show how that EU varies with parameters su h as the rate of wait ost a rual and the time at whi h transfers of ontrol are performed. In this instantiation, the agent, A, has only one entity to\nall on for a de ision (i.e., the user U), hen e E = fA;Ug. For W(t), we use the following\nfun tion:\nW(t) =\n(\n! exp\n!t\nt\n! exp\n!\notherwise\n(15)\nThe exponential wait ost fun tion re e ts the idea that a big delay is mu h worse than a small one. A polynomial or similar fun tion ould have also been used but an exponential was used sin e it makes the mathemati s leaner. For the probability of response we use: P\n>\n(t) = exp\nt\n. A Markovian response probability re e ts an entity that is just as likely\nto respond at the next point in time as they were at the previous point. For users moving around a dynami environment, this turns out to be a reasonable approximation. The entities' de ision-making quality is onstant over time, in parti ular, EQ d\nA\n(t) = and for\nEQ\nd U (t) = . Assuming onstant de ision-making quality will not always be a urate in a dynami environment sin e information available to an entity may hange (hen e in uen ing their ability to make the de ision) however, for de isions involving stati fa ts or preferen es de ision-making quality will be relatively onstant. The fun tions are a oarse approximation of a range of interesting appli ations, in luding the E-Elves. Table 7 shows the resulting instantiated equations for the simple strategies (For onvenien e we let Æ = !). Figures 18(a) and (b) show graphi ally how the EU of the eA strategy varies along di erent axes (w is the parameter to the wait ost fun tion, higher w means faster a ruing wait osts and p is the parameter to the response probability fun tion, higher p means faster response). Noti e how the EU depends on the transfer time (T) as mu h as it does on (the user's de ision quality). Figure 18(d) shows the value of a D (as dis ussed earlier).\nFigure 18( ) ompares the EU of the eDeA and estrategies. The more omplex the transfer-of- ontrol strategy (i.e., the more transfers of ontrol it makes), the atter the EU graph when plotted against wait ost (w) and response probability (p) parameters. In parti ular, the fall-o when the wait osts are high and the probability of response low is not so dramati for the more omplex strategy."
    }, {
      "heading" : "Appendix B: Constraint Propagation Algorithm and its Corre tness",
      "text" : "In Se tion 4.4, we examined the need for user-spe i ed onstraints in onjun tion with our MDP-based approa h to strategies. We must thus extend the standard MDP poli y evaluation algorithms to support the evaluation of strategies while a ounting for both the standard quantitative reward fun tion and these new qualitative onstraints. This appendix provides the novel algorithm that we developed to evaluate strategies while a ounting for\n( ) (d)\nboth. We also present a detailed proof that our algorithm's output is the orre t strategy (i.e., the strategy with the highest expe ted utility, subje t to the user-spe i ed onstraints).\nIn the standard MDP value iteration algorithm, the value of a strategy in a parti ular state is a single number, an expe ted utility U . With the addition of our two types of\nonstraints, this value is now a tuple hF;N;Ui. F represents a strategy's ability to satisfy the forbidding onstraints; therefore, it is a boolean indi ating whether the state is forbidden or not. N represents a strategy's ability to satisfy the ne essary onstraints; therefore, it is the set of requiring onstraints that will be satis ed. As in traditional value iteration, U is the expe ted reward. For instan e, if the value of a state, V (s) = htrue; f\nrs\ng; 0:3i,\nthen exe uting the poli y from state s will a hieve an expe ted value of 0.3 and will satisfy required-state onstraint\nrs\n. However, it is not guaranteed to satisfy any other required-\nstate, nor any required-a tion, onstraints. In addition, s is forbidden, so there is a nonzero probability of violating a forbidden-a tion or forbidden-state onstraint. We do not re ord whi h forbidding onstraints the poli y violates, sin e violating any one of them is equally bad. We do have to re ord whi h requiring onstraints the poli y satis es, sin e satisfying all su h onstraints is preferable to satisfying only some of them. Therefore, the size of the value fun tion grows linearly with the number of requiring onstraints, but is independent of the number of forbidding onstraints.\nFollowing the form of standard value iteration, we initialize the value fun tion over states by onsidering the immediate value of the strategy in the given state, without any lookahead. More pre isely:\nV\n0\n(s)\n*\n_\n2C\nfs\n(s); f 2 C\nrs\nj (s)g ; R\nS\n(s)\n+\n(19)\nThus, the state s is forbidden if any forbidden-state onstraints immediately apply, and it satis es those required-state onstraints that immediately apply. As in standard value iteration, the expe ted utility is the value of the reward fun tion in the state.\nIn value iteration, we must de ne an updated value fun tion V\nt+1\nas a re nement\nof the previous iteration's value fun tion, V\nt\n. States be ome forbidden in V\nt+1\nif they\nviolate any onstraints dire tly or if any of their su essors are forbidden a ording to V\nt\n.\nStates satisfy requirements if they satisfy them dire tly or if all of their su essors satisfy the requirement. To simplify the following expressions, we de ne S 0 to be the set of all su essors: fs 0 2 SjM a\nss\n0\n> 0g. The following expression provides the pre ise de nition of\nthis iterative step:\nV\nt+1\n(s) max\na2A\n*\n_\n2C\nfs\n(s) _\n_\n2C\nfa\n(s; a) _\n_\nV\nt\n(s\n0\n)=hF\n0\n;N\n0\n;U\n0\ni;s\n0\n2S\n0\nF\n0\n;\nf 2 C\nrs\nj (s)g [ f 2 C\nra\nj (s; a)g [\n\\\nV\nt\n(s\n0\n)=hF\n0\n;N\n0\n;U\n0\ni;s\n0\n2S\n0\nN\n0\n;\nR\nS\n(s) +R(s; a) +\nX\nV\nt\n(s\n0\n)=hF\n0\n;N\n0\n;U\n0\ni;s\n0\n2S\n0\nM\na ss 0 U 0\n+\n(20)\nJust as in standard value iteration, this iterative step spe i es a maximization over all possible hoi es of a tion. However, with our two additional omponents to represent the value of the strategy with respe t to the onstraints, we no longer have an obvious omparison fun tion to use when evaluating andidate a tions. Therefore, we perform the maximization using the following preferen e ordering, where x y means that y is preferable to x:\nht;N;Ui\nf;N\n0\n; U\n0\nhF;N;Ui\nF;N\n0\nN;U\n0\nhF;N;Ui\nF;N;U\n0\n> U\nIn other words, satisfying a forbidden onstraint takes highest priority, satisfying more requiring onstraints is se ond, and in reasing expe ted value is last. We de ne the optimal a tion, P (s), as the a tion, a, for whi h the nal V (s) expression above is maximized.\nDespite the various set operations in Equation 20, the time omplexity of this iteration step ex eeds that of standard value iteration by only a linear fa tor, namely the number of onstraints, jC\nfs\nj + jC\nfa\nj + jC\nrs\nj + jC\nra\nj. The eÆ ien y derives from the fa t that the\nonstraints are satis ed/violated independently of ea h other. The determination of whether a single onstraint is satis ed/violated requires no more time than that of standard value iteration, hen e the overall linear in rease in time omplexity.\nBe ause expe ted value has the lowest priority, we an separate the iterative step of Equation 20 into two phases: onstraint propagation and value iteration. During the\nonstraint-propagation phase, we ompute only the rst two omponents of our value fun - tion, hF;N; i. The value-iteration phase omputes the third omponent, h ; ; Ui, as in standard value iteration. However, we an ignore any state/a tion pairs that, a ording to the results of onstraint propagation, violate a forbidding onstraint (ht;N; i) or requiring onstraint (hf;N C\nrs\n[ C\nra\n; i). Be ause of the omponent-wise independen e of\nEquation 20, the two-phase algorithm omputes an identi al value fun tion as the original, single-phase version (over state/a tion pairs that satisfy all onstraints).\nIn the rest of this Appendix we provide a proof of the orre tness of the modi ed value iteration poli y. Given a poli y, P , onstru ted a ording to the above algorithm, we must\nshow that an agent following P will obey the onstraints spe i ed by the user. If the agent begins in some state, s 2 S, we must prove that it will satisfy all of its onstraints if and only if V (s) = hf;C\nra\n[ C\nrs\n; Ui. We prove the results for forbidding and requiring onstraints\nseparately.\nTheorem 1 An agent following poli y, P , with value fun tion, V , generated as in Se - tion 4.4, from any state s 2 S will violate a forbidding onstraint with probability zero if and only if V (s) = hf;N;Ui (for some U and N).\nProof: We prove the theorem by indu tion over subspa es of the states, lassi ed by how \\ lose\" they are to violating a forbidding onstraint. More pre isely, we partition the state spa e, S, into subsets, S\nk\n, de ned to ontain all states that an violate a forbidding\nonstraint after a minimum of k state transitions. In other words, S\n0\nontains those states\nthat violate a forbidding onstraint dire tly; S\n1\nontains those states that do not violate\nany forbidding onstraints themselves, but have a su essor state (following the transition probability fun tion, P ) that does (i.e., a su essor state in S\n0\n); S\n2\nontains those states\nthat do not violate any forbidding onstraints, nor have any su essors that do, but who have at least one su essor state that has a su essor state that does (i.e., a su essor state in S\n1\n); et . There are at most jSj nonempty subsets in this mutually ex lusive sequen e. To\nmake this partition exhaustive, the spe ial subset, S\n1\n, ontains all states from whi h the\nagent will never violate a forbidding onstraint by following P . We rst show, by indu tion over k, that 8s 2 S\nk\n(0 k jSj), V (s) = ht;N;Ui, as required by the theorem.\nBasis step (S\n0\n): By de nition, the agent will violate a forbidding onstraint in s 2 S\n0\n.\nTherefore, either 9 2 C\nfs\nsu h that (s) = t or 9 2 C\nfa\nsu h that (s; P (s)) = t, so we\nknow, from Equation 20, V (s) = ht;N;Ui.\nIndu tive step (S\nk\n; 1 k jSj): Assume, as the indu tion hypothesis, that 8s\n0\n2\nS\nk 1\n, V (s\n0\n) = ht;N\n0\n; U\n0\ni. By the de nition of S\nk\n, ea h state, s 2 S\nk\n, has at least one\nsu essor state, s\n0\n2 S\nk 1\n. Then, a ording to Equation 20, V (s) = ht;N;Ui, be ause the\ndisjun tion over S\n0\nmust in lude s\n0\n, for whi h F\n0\n= t.\nTherefore, by indu tion, we know that for all s 2 S\nk\n(0 k jSj), V (s) = ht;N;Ui.\nWe now show that 8s 2 S\n1\n, V (s) = hf;N;Ui. We prove, by indu tion over t, that, for any\nstate, s 2 S\n1\n, V\nt\n(s) = hf;N;Ui.\nBasis step (V\n0\n): By de nition, if s 2 S\n1\n, there annot exist any 2 C\nfs\nsu h that\n(s) = t. Then, from Equation 19, V\n0\n(s) = f;N\n0\n; U\n0\n.\nIndu tive step (V\nt\n; t > 0): Assume, as the indu tive hypothesis, that, for any s\n0\n2 S\n1\n,\nV\nt 1\n(s\n0\n) = hf;N\n0\n; U\n0\ni. We know that V\nt\n(s) = f;N\nt\n; U\nt\nif and only if all three disjun tions\nin Equation 20 are false. The rst is false, as des ribed in the basis step. The se ond term is similarly false, sin e, by the de nition of S\n1\n, there annot exist any 2 C\nfa\nsu h that\n(s; P (s)) = t. In evaluating the third term, we rst note that S\n0\nS\n1\n. In other words,\nall of the su essor states of s are also in S\n1\n(if su essor s\n0\n2 S\nk\nfor some nite k, then\ns 2 S\nk+1\n). Sin e all of the su essors are in S\n1\n, we know, by the indu tive hypothesis, that\nthe disjun tion over V\nt 1\nin all these su essors is false. Therefore, all three disjun tive\nterms in Equation 20 are false, so V\nt\n(s) = f;N\nt\n; U\nt\n.\nTherefore, by indu tion, we know that for all s 2 S\n1\n, V (s) = hf;N;Ui. By the de nition\nof the state partition, these two results prove the theorem as required. 2\nTheorem 2 An agent following poli y, P , with value fun tion, V , generated as des ribed in Se tion 4.4, from any state s 2 S will satisfy ea h and every requiring onstraint with probability one if and only if V (s) = hF;C\nra\n[C\nrs\n; Ui (for some U and F ).\nProof Sket h: The proof parallels that of Theorem 1, but with a state partition, S\nk\n,\nwhere k orresponds to the maximum number of transitions before satisfying a requiring\nonstraint. However, here, states in S\n1\nare those that violate the onstraint, rather than\nsatisfy it. Some y les in the state spa e an prevent a guarantee of satisfying a requiring\nonstraint within any xed number of transitions, although the probability of satisfa tion in the limit may be 1. In our urrent onstraint semanti s, we have de ided that su h a situation fails to satisfy the onstraint, and our algorithm behaves a ordingly. Su h y les have no e e t on the handling of forbidding onstraints, where, as we saw for Theorem 1, we need onsider only the minimum-length traje tory. 2\nThe proofs of the two theorems operate independently, so the poli y-spe i ed a tion will satisfy all onstraints, if su h an a tion exists. The pre eden e of forbidding onstraints over requiring ones has no e e t on the optimal a tion in su h states. However, if there are on i ting forbidding and requiring onstraints in a state, then the preferen e ordering\nauses the agent to hoose a poli y that satis es the forbidding onstraint and violates a requiring onstraint. The agent an make the opposite hoi e if we simply hange the preferen e ordering from Se tion 4.4. Regardless of the hoi e, from Theorems 1 and 2, the agent an use the value fun tion, V , to identify the existen e of any su h violation and notify the user of the violation and possible onstraint on i t.\nReferen es\nBarber, K., Goel, A., & Martin, C. (2000a). Dynami adaptive autonomy in multi-agent\nsystems. Journal of Experimental and Theoreti al Arti ial Intelligen e, 12 (2), 129{ 148.\nBarber, K. S., Martin, C., & M kay, R. (2000b). A ommuni ation proto ol supporting\ndynami autonomy agreements. In Pro eedings of PRICAI 2000 Workshop on Teams with Adjustable Autonomy, pp. 1{10, Melbourne, Australia.\nBonasso, R., Firby, R., Gat, E., Kortenkamp, D., Miller, D., & Sla k, M. (1997). Expe-\nrien es with an ar hite ture for intelligent rea tive agents. Journal of Experimental and Theore ti al Arti ial Intelligen e, 9 (1), 237{256.\nBrann, D., Thurman, D., & Mit hell, C. (1996). Human intera tion with lights-out automa-\ntion: A eld study. In Pro eedings of the 1996 Symposium on Human Intera tion and Complex Systems, pp. 276{283, Dayton, USA.\nCesta, A., Collia, M., & D'Aloisi, D. (1998). Tailorable intera tive agents for s heduling\nmeetings. In Le ture Notes in AI, Pro eedings of AIMSA'98, No. 1480, pp. 153{166. Springer Verlag.\nChalupsky, H., Gil, Y., Knoblo k, C., Lerman, K., Oh, J., Pynadath, D., Russ, T., & Tambe,\nM. (2001). Ele tri Elves: Applying agent te hnology to support human organizations. In International Conferen e on Innovative Appli ations of AI, pp. 51{58.\nCollins, J., Bilot, C., Gini, M., & Mobasher, B. (2000a). Mixed-initiative de ision-support\nin agent-based automated ontra ting. In Pro eedings of the International Conferen e on Autonomous Agents (Agents'2000).\nCollins, J., Bilot, C., Gini, M., & Mobasher, B. (2000b). Mixed-initiative de ision support\nin agent-based automated ontra ting. In Pro eedings of the International Conferen e on Autonomous Agents (Agents'2000), pp. 247{254.\nDorais, G., Bonasso, R., Kortenkamp, D., Pell, B., & S hre kenghost, D. (1998). Adjustable\nautonomy for human- entered autonomous systems on mars. In Pro eedings of the First International Conferen e of the Mars So iety, pp. 397{420.\nDraper, D., Hanks, S., & Weld, D. (1994). Probabilisti planning with information gathering\nand ontingent exe ution. In Hammond, K. (Ed.), Pro . Se ond International Conferen e on Arti ial Intelligen e Planning Systems, pp. 31{37, University of Chi ago, Illinois. AAAI Press.\nFerguson, G., Allen, J., & Miller, B. (1996). TRAINS-95 : Towards a mixed-initiative\nplanning assistant. In Pro eedings of the Third Conferen e on Arti ial Intelligen e Planning Systems, pp. 70{77.\nFerguson, G., & Allen, J. (1998). TRIPS : An intelligent integrated problem-solving assis-\ntant. In Pro eedings of Fifteenth National Conferen e on Arti ial Intelligen e(AAAI98), pp. 567{573, Madison, WI, USA.\nFong, T., Thorpe, C., & Baur, C. (2002). Robot as partner: Vehi le teleoperation with ol-\nlaborative ontrol. In Workshop on Multi-Robot Systems, Naval Resear h Laboratory, Washington, D.C.\nFudenberg, D., & Tirole, J. (1991). Game Theory. The MIT Press, Cambridge, Mas-\nsa husetts.\nGoldman, R., Guerlain, S., Miller, C., & Musliner, D. (1997). Integrated task representa-\ntion for indire t intera tion. In Working Notes of the AAAI Spring Symposium on Computational Models for Mixed-Initiative Intera tion.\nGoodri h, M., Olsen, D., Crandall, J., & Palmer, T. (2001). Experiments in adjustable\nautonomy. In Hexmoor, H., Castelfran hi, C., Fal one, R., & Cox, M. (Eds.), Pro-\needings of IJCAI Workshop on Autonomy, Delegation and Control: Intera ting with\nIntelligent Agents.\nGunderson, J., & Martin, W. (1999). E e ts of un ertainty on variable autonomy in main-\ntainan e robots. In Agents'99 Workshop on Autonomy Control Software, pp. 26{34.\nHexmoor, H. (2000). A ognitive model of situated autonomy. In Pro eedings of PRICAI-\n2000, Workshop on Teams with Adjustable Autonomy, pp. 11{20, Melbourne, Australia.\nHexmoor, H., & Kortenkamp, D. (2000). Introdu tion to autonomy ontrol software. Journal\nof Experiemental and Theoreti al Arti ial Intelligen e, 12 (2), 123{128.\nHorvitz, E. (1999). Prin iples of mixed-initiative user interfa es. In Pro eedings of ACM\nSIGCHI Conferen e on Human Fa tors in Computing Systems (CHI'99), pp. 159{166, Pittsburgh, PA.\nHorvitz, E., Ja obs, A., & Hovel, D. (1999). Attention-sensitive alerting. In Pro eedings of\nConferen e on Un ertainty and Arti ial Intelligen e (UAI'99), pp. 305{313, Sto kholm, Sweden.\nLesser, V., Atighet hi, M., Benyo, B., Horling, B., Raja, A., Vin ent, R., Wagner, T., Xuan,\nP., & Zhang, S. (1999). The UMASS intelligent home proje t. In Pro eedings of the Third Annual Conferen e on Autonomous Agents, pp. 291{298, Seattle, USA.\nMit hell, T., Caruana, R., Freitag, D., M Dermott, J., & Zabowski, D. (1994). Experien e\nwith a learning personal assistant. Communi ations of the ACM, 37 (7), 81{91.\nMulsiner, D., & Pell, B. (1999). Call for papers: AAAI spring symposium on adjustable\nautonomy. www.aaai.org.\nMusliner, D., & Krebsba h, K. (1999). Adjustable autonomy in pro edural ontrol for\nre neries. In AAAI Spring Symposium on Agents with Adjustable Autonomy, pp. 81{87, Stanford, California.\nPeot, M. A., & Smith, D. E. (1992). Conditional nonlinear planning. In Hendler, J. (Ed.),\nPro . First International Conferen e on Arti ial Intelligen e Planning Systems, pp. 189{197, College Park, Maryland. Morgan Kaufmann.\nPuterman, M. L. (1994). Markov De ision Pro esses. John Wiley & Sons.\nPynadath, D., Tambe, M., Arens, Y., Chalupsky, H., Gil, Y., Knoblo k, C., Lee, H., Lerman,\nK., Oh, J., Kama handran, S., Rosenbloom, P., & Russ, T. (2000). Ele tri -elves: Immersing and agent organization in a human organization. In Pro eedings of the AAAI Fall Symposium on So ially Intelligent Agents { The Human in the Loop.\nPynadath, D., & Tambe, M. (2001). Revisiting Asimov's rst law: A response to the all to\narms. In Intelligent Agents VIII Pro eedings of the International workshop on Agents, Theories, Ar hite tures and Languages (ATAL'01).\nQuinlan, J. R. (1993). C4.5: Programs for ma hine learning. Morgan Kaufmann, San\nMateo, CA.\nRussell, S. J., & Wefald, E. (1989). Prin iples of metareasoning. In Bra hman, R. J.,\nLevesque, H. J., & Reiter, R. (Eds.), KR'89: Prin iples of Knowledge Representation and Reasoning, pp. 400{411. Morgan Kaufmann, San Mateo, California.\nS erri, P., Pynadath, D., & Tambe, M. (2001). Adjustable autonomy in real-world multi-\nagent environments. In Pro eedings of the Fifth International Conferen e on Autonomous Agents (Agents'01), pp. 300{307.\nS erri, P., Pynadath, D., & Tambe, M. (2002). Why the elf a ted autonomously: Towards\na theory of adjustable autonomy. In First International Joint Conferen e on Autonomous Agents and Multi-Agent Systems (AAMAS'02).\nS hre kenghost, D. (1999). Human intera tion with ontrol software supporting adjustable\nautonomy. In Musliner, D., & Pell, B. (Eds.), Agents with Adjustable Autonomy, AAAI 1999 Spring Symposium Series, pp. 116{119.\nStankovi , J., Ramamritham, K., & Cheng, S. (1985). Evaluation of a exible task s hedul-\ning algorithm for distributed hard real-time system. IEEE Transa tions on Computers, 34 (12), 1130{1143.\nTambe, M. (1997). Towards exible teamwork. Journal of Arti ial Intelligen e Resear h\n(JAIR), 7, 83{124.\nTambe, M., Pynadath, D. V., Chauvat, N., Das, A., & Kaminka, G. A. (2000). Adaptive\nagent integration ar hite tures for heterogeneous team members. In Pro eedings of the International Conferen e on MultiAgent Systems, pp. 301{308.\nZilberstein, S. (1996). Using anytime algorithms in intelligent systems. AI Magazine, 17 (3),\n73{83."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "Adjustable autonomy refers to entities dynami ally varying their own autonomy, transferring de ision-making ontrol to other entities (typi ally agents transferring ontrol to human users) in key situations. Determining whether and when su h transfers-ofontrol should o ur is arguably the fundamental resear h problem in adjustable autonomy. Previous work has investigated various approa hes to addressing this problem but has often fo used on individual agent-human intera tions. Unfortunately, domains requiring ollaboration between teams of agents and humans reveal two key short omings of these previous approa hes. First, these approa hes use rigid one-shot transfers of ontrol that an result in una eptable oordination failures in multiagent settings. Se ond, they ignore osts (e.g., in terms of time delays or e e ts on a tions) to an agent's team due to su h transfers-ofontrol. To remedy these problems, this arti le presents a novel approa h to adjustable autonomy, based on the notion of a transfer-ofontrol strategy. A transfer-ofontrol strategy onsists of a onditional sequen e of two types of a tions: (i) a tions to transfer de isionmaking ontrol (e.g., from an agent to a user or vi e versa) and (ii) a tions to hange an agent's pre-spe i ed oordination onstraints with team members, aimed at minimizing mis oordination osts. The goal is for high-quality individual de isions to be made with minimal disruption to the oordination of the team. We present a mathemati al model of transfer-ofontrol strategies. The model guides and informs the operationalization of the strategies using Markov De ision Pro esses, whi h sele t an optimal strategy, given an un ertain environment and osts to the individuals and teams. The approa h has been arefully evaluated, in luding via its use in a real-world, deployed multi-agent system that assists a resear h group in its daily a tivities.",
    "creator" : "dvips(k) 5.86 Copyright 1999 Radical Eye Software"
  }
}