{
  "name" : "1704.02298.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "TransNets: Learning to Transform for Recommendation",
    "authors" : [ "Rose Catherine William Cohen" ],
    "emails" : [ "rosecatherinek@cs.cmu.edu", "wcohen@cs.cmu.edu" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "Using review text for predicting ratings has been shown to greatly improve the performance of recommender systems [4, 22, 24], compared to Collaborative Filtering (CF) techniques that use only past ratings [18, 33]. Recent advances in Deep Learning research have made it possible to use Neural Networks in a multitude of domains including recommender systems, with impressive results. Most neural recommender models [3, 10, 16, 21, 40] have focussed on the content associated with the user and the item, which are used to construct their latent representations. Content associated with a user could include their demographic information, socioeconomic characteristics, their product preferences and the like. Content linked to an item could include their price, appearance, usability and similar attributes in the case of products, food quality, ambience, service and wait times in the case of restaurants, or actors, director, genre, and similar metadata in the case of movies. These representations are then fed into a CF-style architecture or a regression model to make the rating prediction.\nReview text, unlike content, is not a property of only the user or only the item; it is a property associated with their joint interaction. In that sense, it is a context [1] feature. Only a few neural net models [2, 34, 44] have been proposed to date that use review text for predicting the rating. Of these, the most recent model, Deep Cooperative Neural Networks (DeepCoNN) [44] uses neural nets to learn a latent representation for the user from the text of all reviews written by her and a second latent representation for the item from the text of all reviews that were written for it, and then combines these two representations in a regression layer to obtain state-of-the-art performance on rating prediction. However, as we will show, much of the predictive value of review text comes from\nreviews of the target user for the target item, which can be assumed to be available only at training time, and is not available at test time. In this paper, we introduce a way in which this information can be used in training the recommender system, such that when the target user’s review for the target item is not available at the time of prediction, an approximation for it is generated, which is then used for predicting the rating. Our model, called Transformational Neural Networks (TransNets), extends the DeepCoNN model by introducing an additional latent layer representing an approximation of the review corresponding to the target user-target item pair. We then regularize this layer, at training time, to be similar to the latent representation of the actual review written by the target user for the target item. Our experiments illustrate that TransNets and its extensions give substantial improvements in rating prediction.\nThe rest of this paper is organized as follows. The proposed model and architecture are discussed in detail in Section 2. The experiments and results are discussed in Section 3. Section 4 summarizes related work, and we conclude in Section 5."
    }, {
      "heading" : "2 PROPOSED METHOD",
      "text" : ""
    }, {
      "heading" : "2.1 CNNs to process text",
      "text" : "We process text using the same approach as the current state-ofthe-art method for rating prediction, DeepCoNN [44]. The basic building block, referred to as a CNN Text Processor in the rest of this paper, is a Convolutional Neural Network (CNN) [20] that inputs a sequence of words and outputs a n-dimensional vector representation for the input, i.e., theCNN Text Processor is a function Γ : [w1,w2, ...,wT ] → Rn . Figure 1 gives the architecture of the CNN Text Processor. In the rst layer, a word embedding function\nar X\niv :1\n70 4.\n02 29\n8v 1\n[ cs\n.I R\n] 7\nA pr\n2 01\n7\nf : M → Rd maps each word in the review that are also in its Msized vocabulary into ad dimensional vector. The embedding can be any pre-trained embedding like those trained on the GoogleNews corpus using word2vec1[27], or on Wikipedia using GloVe2 [30]. These word vectors are held xed throughout the training process.\nFollowing the embedding layer is the Convolutional Layer, adapted to text processing [8]. It consists ofm neurons each associated with a lterK ∈ Rt×d , where t is a window size, typically 2 – 5. The lter processes t-length windows of d-dimensional vectors to produce features. Let V1:T be the embedded matrix corresponding to the T -length input text. Then, jth neuron produces its features as: zj = α(V1:T ∗ Kj + bj ) where, bj is its bias, ∗ is the convolution operation and α is a nonlinearity like Recti ed Linear Unit (ReLU) [28] or tanh.\nLet z1j , z 2 j , ...z (T−t+1) j be the features produced by the j th neuron on the sliding windows over the embedded text. Then, the nal feature corresponding to this neuron is computed using a maxpooling operation, de ned as:\noj = max{z1j , z 2 j , ...z (T−t+1) j }\nThe max-pooling operation provides location invariance to the neuron, i.e., the neuron is able to detect the features in the text regardless of where it appears.\nThe nal output of the Convolutional Layer is the concatenation of the output from itsm neurons, denoted by:\nO = [o1,o2, ...om ] This output is then passed to a fully connected layer consisting of a weight matrixW ∈ Rm×n and a bias д ∈ Rn , which computes the nal representation of the input text as:\nx = α(W ×O + д)"
    }, {
      "heading" : "2.2 The DeepCoNN model",
      "text" : "To compute the rating rAB that userA would assign to itemB , the DeepCoNN model of [44] uses two CNN Text Processors side by side as shown in Figure 2. The rst one processes the text labeled textA, which consists of a concatenation of all the reviews that userA has written and produces a representation, xA. Similarly, the second processes the text called textB , which consists of a concatenation of all the reviews that have been written about itemB and produces a representation, yB . Both outputs are passed through a dropout layer [36]. Dropout is a function δ : Rn → Rn , that suppresses the 1https://code.google.com/archive/p/word2vec 2https://nlp.stanford.edu/projects/glove\noutput of some of the neurons randomly and is a popular technique for regularizing a network. Let x̄A = δ (xA) and ȳB = δ (yB ), denote the output of the dropout layer applied on xA and yB .\nThe model then concatenates the two representations as z = [x̄A, ȳB ] and passes it through a regression layer consisting of a Factorization Machine (FM) [32]. The FM computes the second order interactions between the elements of the input vector as:\nr̂AB = w0 + |z |∑ i=1 wizi + |z |∑ i=1 |z |∑ j=i+1 〈vi , vj 〉zizj\nwhere w0 ∈ R is the global bias, w ∈ R2n weights each dimension of the input, and V ∈ R2n×k assigns a k dimensional vector to each dimension of the input so that the pair-wise interaction between two dimensions i and j can be weighted using the inner product of the corresponding vectors vi and vj . Note that the FM factorizes the pair-wise interaction, and therefore requires only O(nk) parameters instead of O(n2) parameters which would have been required otherwise, where k is usually chosen such that k n. This has been shown to give better parameter estimates under sparsity [32]. FMs have been used successfully in large scale recommendation services like online news[42].\nFMs can be trained using di erent kinds of loss functions including least squared error (L2), least absolute deviation (L1), hinge loss and logit loss. In our experiments, L1 loss gave a slightly better performance than L2. DeepCoNN [44] also uses L1 loss. Therefore, in this paper, all FMs are trained using L1 loss, de ned as:\nloss = ∑\n(uA,iB,rAB )∈D |rAB − r̂AB |"
    }, {
      "heading" : "2.3 Limitations of DeepCoNN",
      "text" : "DeepCoNN model has achieved impressive MSE values surpassing that of the previous state-of-the-art models that use review texts, like the Hidden Factors as Topics (HFT) model [24], Collaborative Topic Regression (CTR) [39] and Collaborative Deep Learning (CDL) [40], as well as Collaborative Filtering techniques that use only the rating information like Matrix Factorization (MF) [18] and Probabilistic Matrix Factorization (PMF) [33].\nHowever, it was observed in our experiments that DeepCoNN achieves its best performance only when the text of the review written by the target user for the target item is available at test time. In real world recommendation settings, an item is always recommended to a user before they have experienced it. Therefore, it would be unreasonable to assume that the target review would be available at the time of testing.\nLet revAB denote the review written by userA for an itemB . At training time, the text corresponding to userA, denoted as textA, consists of a concatenation of all reviews written by her in the training set. Similarly, the text for itemB , denoted by textB , is a concatenation of all reviews written for that item in the training set. Both textA and textB includes revAB for all (userA, itemB ) pairs in the training set. At test time, there are two options for constructing the test inputs. For a test pair (userP , itemQ ), their pairwise review, revPQ in the test set, could be included in the texts corresponding to the user, textP , and the item, textQ , or could be omitted. In one of our datasets, the MSE obtained by DeepCoNN if revPQ is included in the test inputs is only 1.21. However, if revPQ is omitted, then the performance degrades severely to 1.89. This is lower than Matrix Factorization applied to the same dataset, which has an MSE of 1.86.\nIf we train DeepCoNN in the setting that mimics the test setup, by omitting revAB in the texts of all (userA, itemB ) pairs in the training set, the performance is better at 1.70, but still much higher than when revAB is available in both training and testing.\nIn the setting used in this paper, reviews in the validation and the test set are never accessed at any time, i.e., assumed to be unavailable — both during training and testing — simulating a real world situation."
    }, {
      "heading" : "2.4 TransNets",
      "text" : "As we saw in the case of DeepCoNN, learning using the target review revAB at train time inadvertently makes the model dependent on the presence of such reviews at test time, which is unrealistic. However, as shown by the experiment above, revAB gives an insight into what userA thought about their experience with itemB , and can be an important predictor of the rating rAB . Although unavailable at test time, revAB is available during training.\nTransNet consists of two networks as shown in the architecture diagram of Figure 3, a Target Network that processes the target review revAB and a Source Network that processes the texts of the (userA, itemB ) pair that does not include the joint review, revAB . Given a review text revAB , the Target Network uses a CNN Text Processor, ΓT , and a Factorization Machine, FMT , to predict the rating as:\nxT = ΓT (revAB ) x̄T = δ (xT ) r̂T = FMT (x̄T )\nSince the Target Network uses the actual review, its task is similar to sentiment analysis [19, 35].\nThe Source Network is like the DeepCoNN model with two CNN Text Processors, ΓA for user text, textA − revAB , and ΓB for item text, textB − revAB , and a Factorization Machine, FMS , but with an additional Transform layer. The goal of the Transform layer is to transform the user and the item texts into an approximation of revAB , denoted by ˆrevAB , which is then used for predicting the rating. The Source Network predicts the rating as given below:\nFirst, it converts the input texts into their latent form as: xA = ΓA(textA − revAB ) xB = ΓB (textB − revAB ) z0 = [xAxB ]\nThe last step above is a concatenation of the two latent representations. This is then input to the Transform sub-network, which is a L-layer deep non-linear transformational network. Each layer l in Transform has a weight matrix Gl ∈ Rn×n and bias дl ∈ Rn , and transforms its input zl−1 as: zl = σ (zl−1Gl + дl ) where σ is a non-linear activation function. Since the input to the rst layer, z0, is a concatenation of two vectors each ofn dimensions, the rst layer of Transform uses a weight matrix G1 ∈ R2n×n .\nThe output of the Lth layer of Transform, zL is the approximation constructed by the TransNet for revAB , denoted by ˆrevAB . Note that we do not have to generate the surface form of revAB ; It su ces to approximate ΓT (revAB ), the latent representation of the target review. The Source Network then uses this representation to predict the rating as:\nz̄L = δ (zL) r̂S = FMS (z̄L)\nDuring training, we will force the Source Network’s representation zL to be similar to the encoding of revAB produced by the Target Network, as we discuss below."
    }, {
      "heading" : "2.5 Training TransNets",
      "text" : "TransNet is trained using 3 sub-steps as shown in Algorithm 1. In the rst sub-step, for each training example (or a batch of such examples), the parameters of the Target Network, denoted by θT , which includes those of ΓT and FMT , are updated to minimize a L1 loss computed between the actual rating rAB and the rating r̂T predicted from the actual review text revAB .\nTo teach the Source Network how to generate an approximation of the latent representation of the original review revAB generated by the Target Network, in the second sub-step, its parameters, denoted by θtrans , are updated to minimize a L2 loss computed between the transformed representation, z̄L , of the texts of the user and the item, and the representation xT of the actual review. θtrans includes the parameters of ΓA and ΓB , as well as the weights Wl and biases дl in each of the transformation layers. θtrans does not include the parameters of FMS .\nIn the nal sub-step, the remaining parameters of the Source Network, θS , which consists of the parameters of the FMS are updated to minimize a L1 loss computed between the actual rating rAB and the rating r̂S predicted from the transformed representation, z̄L . Note that each sub-step is repeated for each training example (or a batch of such examples), and not trained to convergence independently. The training method is detailed in Algorithm 1.\nAt test time, TransNet uses only the Source Network to make the prediction as shown in Algorithm 3."
    }, {
      "heading" : "2.6 Design Decisions and Other Architectural Choices",
      "text" : "In this section, we describe some of the choices we have in designing the TransNet architecture and why they did not give good results in our preliminary experiments.\nAlgorithm 1 Training TransNet\n1: procedure Train(Dtrain ) 2: while not converged do 3: for (textA, textB , revAB , rAB ) ∈ Dtrain do 4: #Step 1: Train Target Network on the actual review 5: xT = ΓT (revAB ) 6: r̂T = FMT (δ (xT )) 7: lossT = |rAB − r̂T | 8: update θT to minimize lossT 9: #Step 2: Learn to Transform 10: xA = ΓA(textA) 11: xB = ΓB (textB ) 12: z0 = [xAxB ] 13: zL = Transform(z0) 14: z̄L = δ (zL) 15: losstrans = | |z̄L − xT | |2 16: update θtrans to minimize losstrans 17: #Step 3: Train a predictor on the transformed input 18: r̂S = FMS (z̄L) 19: lossS = |rAB − r̂S | 20: update θS to minimize lossS 21: return θtrans ,θS\nAlgorithm 2 Transform the input\n1: procedure Transform(z0) 2: for layer l ∈ L do 3: zl = σ (zl−1Gl + дl ) 4: return zL\nAlgorithm 3 Testing using TransNet\n1: procedure Test(Dtest ) 2: for (textP , textQ ) ∈ Dtest do 3: #Step 1: Transform the input 4: xP = ΓA(textP ) 5: xQ = ΓB (textQ ) 6: z0 = [xPxQ ] 7: zL = Transform(z0) 8: z̄L = δ (zL) 9: #Step 2: Predict using the transformed input\n10: r̂PQ = FMS (z̄L)\n2.6.1 Training with sub-steps vs. jointly. While training TransNets using Algorithm 1, in each iteration (or batch), we could choose to jointly minimize a total loss, losstotal = lossT + losstrans + lossS . However, doing so will result in parameter updates to the target network, ΓT , resulting from losstrans , in addition to those from lossT , i.e., the Target Network will get penalized for producing a representation that is di erent from that produced by the Source Network. This results in both networks learning to produce sub-optimal representations and converging to a lower performance in our experiments. Therefore, it is important to separate the Target Network’s parameter updates so that it learns to produce the best representation which will enable it to make the most accurate rating predictions from the review text.\n2.6.2 Training Target Network to convergence independently. We could choose to rst train the Target Network to convergence and then train the Source Network to emulate the trained Target Network. However, note that the Target Network’s input is the actual review, which is unavailable for testing its performance, i.e., we do not know when the Target Network has converged with good generalization vs. when it is over tting. The only way to measure the performance at test time is to check the output of the Source Network. Therefore, we let the Source and the Target Networks learn simultaneously and stop when the Source Network’s test performance is good.\n2.6.3 Using the same convolutionalmodel to process text in both the Source and Target networks. We could choose to use the ΓT that was trained in the Target Network to generate features from the user and the item text in the Source Network, instead of learning separate ΓA and ΓB . After all, we are learning to transform the latter’s output into the former. However, in that case, TransNet would be constrained to generate generic features similar to topics. By providing it with separate feature generators, it can possibly learn to transform the occurrence of di erent features in the user text and the item text of the Source Network to another feature in the Target Network. For example, it could learn to transform the occurrence of features corresponding to say, ‘love indian cuisine’ & ‘dislike long wait’ in the user pro le, and ‘lousy service’ & ‘terrible chicken curry’ in the item (restaurant) pro le, to a feature corresponding to say, ‘disappointed’ in the target review, and subsequently predict a lower rating. Having separate feature generators in the Source Network gives TransNets more expressive power and gave a better performance compared to an architecture that reuses the Target Network’s feature generator.\n2.6.4 Training the Transform without the dropout. We could choose to match the output zL of Transform with xT instead of its dropped out version z̄L in Step 2 of Algorithm 1. However, this makes the Transform layer unregularized, leading it to over t thus giving poor performance."
    }, {
      "heading" : "2.7 Extended TransNets",
      "text" : "TransNet uses only the text of the reviews and is user/item identityagnostic, i.e., the user and the item are fully represented using the review texts, and their identities are not used in the model. However, in most real world settings, the identities of the users and items are known to the recommender system. In such a scenario, it is bene cial to learn a latent representation of the users and items, similar to Matrix Factorization methods. The Extended TransNet (TransNet-Ext) model achieves that by extending the architecture\nof TransNet as shown in Figure 4. The Source Network now has two embedding matrices ΩA for users and ΩB for items, which are functions of the form, Ω : id → Rn . These map the string representing the identity of userA and itemB into a n-dimensional representation. These latent representations are then passed through a dropout layer and concatenated with the output of the Transform layer before being passed to the FM regression layer. Therefore, given userA and itemB , TransNet-Ext computes the rating as:\nωA = Ω(userA) ωB = Ω(itemB ) z̄ = [δ (ωA) δ (ωB ) z̄L]\nr̂SE = FMSE (z̄) Computation of the loss in Step 3 of Algorithm 1, lossSE is same as earlier: lossSE = |rAB − r̂SE |. But the parameter θS updated at the end now contains the embedding matrices ΩA and ΩB ."
    }, {
      "heading" : "3 EXPERIMENTS AND RESULTS",
      "text" : ""
    }, {
      "heading" : "3.1 Datasets",
      "text" : "We evaluate the performance of the approach proposed in this paper on four large datasets. The rst one, Yelp17, is from the latest Yelp dataset challenge3, containing about 4M reviews and ratings of businesses by about 1M users. The rest are three of the larger datasets in the latest release of Amazon reviews4 [25, 26] containing reviews and ratings given by users for products purchased on amazon.com, over the period of May 1996 - July 2014. We use the aggressively de-duplicated version of the dataset and also discard entries where the review text is empty. The statistics of the datasets are given in Table 1. The original size of the dataset before discarding empty reviews is given in brackets when applicable."
    }, {
      "heading" : "3.2 Evaluation Procedure and Settings",
      "text" : "Each dataset is split randomly into train, validation and test sets in the ratio 80 : 10 : 10. After training on every 1000 batches of 500 training examples each, MSE is calculated on the validation and the test datasets. We report the MSE obtained on the test dataset when the MSE on the validation dataset was the lowest, similar to [24]. All algorithms, including the competitive baselines, were implemented in Python using TensorFlow5, an open source software library for numerical computation, and were trained/tested on NVIDIA\n3https://www.yelp.com/dataset_challenge 4http://jmcauley.ucsd.edu/data/amazon 5https://www.tensor ow.org\nGeForce GTX TITAN X GPUs. Training TransNet on Yelp17 takes approximately 40 minutes for 1 epoch (∼6600 batches) on 1 GPU, and gives the best performance in about 2–3 epochs.\nBelow are the details of the text processing and the parameter settings used in the experiments:\n3.2.1 Text Pre-Processing and Embedding. All reviews are rst passed through a Stanford Core NLP Tokenizer [23] to obtain the tokens, which are then lowercased. Stopwords (the, and, is etc.) as well as punctuations are considered as separate tokens and are retained. A 64-dimensional word2vec6 [27] embedding using the Skip-gram model is pre-trained on the 50,000 most frequent tokens in each of the training corpora.\n3.2.2 CNN Text Processor. We reuse most of the hyperparameter settings reported by the authors of DeepCoNN [44] since varying them did not give any perceivable improvement. In all of the CNN Text Processors ΓA, ΓB and ΓT , the number of neurons, m, in the convolutional layer is 100, the window size t is 3, and n, the dimension of the output of the CNN Text Processor, is 50. The maximum length of the input text,T , is set to 1000. If there are many reviews, they are randomly sorted and concatenated, and the rst T tokens of the concatenated version are used. In our experiments, the word embedding dimension, d , is 64, and the vocabulary size, |M | is 50,000. Also, the non-linearity, α , is tanh.\n3.2.3 Dropout Layer and Factorization Machines. All dropout layers have a keep probability of 0.5. In all of the factorization machines, FMT , FMS and FMSE , the pair-wise interaction is factorized using a k = 8 dimensional matrix, V . Since FMT processes a n-dimensional input, its parameters are wT ∈ Rn and VT ∈ Rn×k . Similarly, since FMSE processes a 3n-dimensional input, its parameters are wSE ∈ R3n and VSE ∈ R3n×k . All w’s are initialized to 0.001, and all V’s are initialized from a truncated normal distribution with 0.0 mean and 0.001 standard deviation. All FMs are trained to minimize an L1 loss.\n3.2.4 Transform. The default setting for the number of layers, L, is 2. We show the performance for di erent values of L in Section 3.5. All weight matrices Gl are initialized from a truncated normal distribution with 0.0 mean and 0.1 standard deviation, and all biases дl are initialized to 0.1. The non-linearity, σ , is tanh.\n3.2.5 TransNet-Ext. The user (item) embedding matrices, Ω, are initialized from a random uniform distribution (-1.0, 1.0), and map users (items) that appear in the training set to a n = 50 dimensional space. New users (items) in the validation and test sets are mapped to a random vector.\n3.2.6 Training. All optimizations are learned using Adam [17], a stochastic gradient-based optimizer with adaptive estimates, at a learning rate set to 0.002. All gradients are computed by automatic di erentiation in TensorFlow."
    }, {
      "heading" : "3.3 Competitive Baselines",
      "text" : "We compare our method against the current state-of-the-art, DeepCoNN [44]. Since DeepCoNN was extensively evaluated against the previous state-of-the-art models like Hidden Factors as Topics (HFT) model [24], Collaborative Topic Regression (CTR) [39], Collaborative Deep Learning (CDL) [40] and Probabilistic Matrix Factorization (PMF) [33], and shown to surpass their performance by a wide margin, we refrain from repeating those comparisons in this paper. However, we do consider some variations of DeepCoNN. 6https://www.tensor ow.org/tutorials/word2vec#the_skip-gram_model\nOur competitive baselines are: (1) DeepCoNN: The model proposed in [44]. During training,\ntextA and textB corresponding to the userA-itemB pair contains their joint review revAB , along with reviews that userA wrote for other items and what other users wrote for itemB in the training set. During testing, for a userP - itemQ pair, textP and textQ are constructed from only the training set and therefore, does not contain their joint review revPQ . (2) DeepCoNN-revAB: The same DeepCoNN model (1) above, but trained in a setting that mimics the test setup, i.e., during training, textA and textB corresponding to the userAitemB pair does not contain their joint review revAB , but only the reviews thatuserA wrote for other items and what other users wrote for itemB in the training set. Testing procedure is the same as above: for a userP -itemQ pair, textP and textQ are constructed from only the training set and therefore, does not contain their joint review revPQ which is present in the test set. (3) MF: A neural net implementation of Matrix Factorization with n = 50 latent dimensions.\nWe also provide the performance numbers of DeepCoNN in the setting where the test reviews are available at the time of testing. i.e. the same DeepCoNN model (1) above, but with the exception that at test time, for a userP -itemQ pair, textP and textQ are constructed from the training set as well as the test set, and therefore, contains their joint review revPQ from the test set. This is denoted as DeepCoNN + Test Reviews, and its performance is provided for the sole purpose of illustrating how much better the algorithm could perform, had it been given access to the test reviews."
    }, {
      "heading" : "3.4 Evaluation on Rating Prediction",
      "text" : "Like prior work, we use the Mean Square Error (MSE) metric to evaluate the performance of the algorithms. Let N be the total number of datapoints being tested. Then MSE is de ned as:\nMSE = 1 N N∑ i=1 (ri − r̂i )2\nwhere, ri is the ground truth rating and r̂i is the predicted rating for the ith datapoint. Lower MSE indicates better performance.\nThe MSE values of the various competitive baselines are given in Table 2. For each dataset, the best score is highlighted in blue .\nAs can be seen from the Table, it is clear that TransNet and its variant TransNet-Ext perform better at rating prediction compared to the competitive baselines on all the datasets (p-value ≤ 0.05). It can also be seen that learning a user and item embedding using only the ratings in addition to the text helps TransNet-Ext improve the performance over the vanilla TransNet (p-value ≤ 0.1), except in the case of one dataset (AZ-CSJ).\nIt is also interesting to note that training DeepCoNN mimicking the test setup (DeepCoNN-revAB ) gives a large improvement in the case of Yelp, but does not help in the case of the AZ datasets.\n3.5 Picking the number of Transform layers The Transform network uses L fully connected layers. In Figure 5, we plot the MSE of TransNet on the Yelp17 dataset when varying L from 1 to 10. It can be seen from the gure that TransNets are quite robust to the choice of L, uctuating only narrowly in its performance. Using only one layer gives the highest MSE, most\nprobably because it doesn’t have enough parameters to learn how to transform the input. Using 10 layers also gives a high MSE, probably because it over ts or because it has too many parameters to learn. From the gure, using 2 or 5 layers gives the best MSE for this particular setting of TransNets. It is known that a 2 layer non-linear neural network is su cient to represent all the logic gates including the XOR [11]. So, using 2 layers seems like a reasonable choice."
    }, {
      "heading" : "3.6 Finding the most helpful reviews",
      "text" : "Our primary evaluation of TransNet is quantitative, using MSE of predicted ratings. We would also like to investigate whether the learned representation is qualitatively useful—i.e., does it capture interesting high-level properties of the user’s review. One possible use of the learning representation would be to give the user information about her predicted reaction to the item that is more detailed than a rating. In this section, we show how TransNets could be used to nd the most helpful reviews, personalized to each user. For example, the most helpful review for a user who is more concerned about the quality of service and wait times would be di erent from the most helpful review for another user who is sensitive to the price. For a test userP − itemQ pair, we run the Source Network with the text of their reviews from the training set to construct zL , which is an approximation for the representation of their actual joint review. Candidate reviews are all the reviews revCQ in the training set written for itemQ by other users. We pass each of them separately through the Target Network to obtain their latent representation xCQ = ΓT (revCQ ). If revCQ had been most similar to what userP would write for itemQ , then xCQ would be most similar to zL . Therefore, to nd the most helpful review, we simply choose the revCQ whose xCQ is closest to zL in Euclidean distance.\nSome examples of such predicted most helpful reviews on the Yelp17 dataset are listed in Table 3. Here, the column Original Review is the actual review that userP wrote for itemQ , and the column Predicted Review gives the most helpful of the candidate reviews predicted by TransNet. The examples show how the predicted reviews talk about particulars that the original reviews also highlight."
    }, {
      "heading" : "4 RELATEDWORK",
      "text" : ""
    }, {
      "heading" : "4.1 Recommendation Models",
      "text" : "4.1.1 Non-NeuralModels. TheHidden Factors as Topics (HFT) model [24] aims to nd topics in the review text that are correlated\nwith the latent parameters of users. They propose a transformation function which converts user’s latent factors to the topic distribution of the review, and since the former exactly de nes the latter, only one of them is learned. A modi ed version of HFT is the TopicMF model [4], where the goal is to match the latent factors learned for the users and items using MF with the topics learned on their joint reviews using a Non-Negative Matrix Factorization, which is then jointly optimized with the rating prediction. In their\ntransformation function, the proportion of a particular topic in the review is a linear combination of its proportion in the latent factors of the user and the item, which is then converted into a probability distribution over all topics in that review. Unlike these two models, TransNet computes each factor in the transformed review from a non-linear combination of any number of factors from the latent representations of either the user or the item or both. Another extension to HFT is the Rating Meets Reviews (RMR)\nmodel [22] where the rating is sampled from a Gaussian mixture. The Collaborative Topic Regression (CTR) model proposed in [39] is a content based approach, as opposed to a context / review based approach. It uses LDA [5] to model the text of documents (scienti c articles), and a combination of MF and content based model for recommendation. The Rating-boosted Latent Topics (RBLT) model of [37] uses a simple technique of repeating a review r times in the corpus if it was rated r , so that features in higher rated reviews will dominate the topics. Explicit Factor Models (EFM) proposed in [43] aims to generate explainable recommendations by extracting explicit product features (aspects) and users’ sentiments towards these aspects using phrase-level sentiment analysis.\n4.1.2 Neural Net Models. The most recent model to successfully employ neural networks at scale for rating prediction is the Deep Cooperative Neural Networks (DeepCoNN) [44], which was discussed in detail in Section 2. Prior to that work, [2] proposed two models: Bag-of-Words regularized Latent Factor model (BoWLF) and Language Model regularized Latent Factor model (LMLF), where MF was used to learn the latent factors of users and items, and likelihood of the review text, represented either as a bag-of-words or an LSTM embedding [14], was computed using the item factors. [34] proposed a CNN based model identical to DeepCoNN, but with attention mechanism to construct the latent representations, the inner product of which gave the predicted ratings.\nSome of the other past research uses neural networks in a CF setting with content, but not reviews. The Collaborative Deep Learning (CDL) model [40] uses a Stacked De-noising Auto Encoder (SDAE) [38] to learn robust latent representations of items from their content, which is then fed into a CTR model [39] for predicting the ratings. A very similar approach to CDL is the Deep Collaborative Filtering (DCF) method [21] which uses Marginalized De-noising Auto-Encoder (mDA) [7] instead. The Convolutional Matrix Factorization (ConvMF) model [16] uses a CNN to process the description associated with the item and feed the resulting latent vectors into a PMF model for rating prediction. The Multi-View Deep Neural Net (MV-DNN) model [10] uses a deep neural net to map user’s and item’s content into a shared latent space such that their similarity in that space is maximized. [29] proposed to generate the latent factors of items – music in this case— from the content, audio signals. The predicted latent factors of the item were then used in a CF style with the latent factors of the user. [3] also proposed a similar technique but adapted to recommending scienti c-articles. [9] used a deep neural net to learn a latent representation from video content which is then fed into a deep ranking network.\nPrior research has also used deep neural nets for learning latent factors from ratings alone, i.e., without using any content or review. Collaborative De-noising Auto-Encoder model (CDAE) [41] learns to reconstruct user’s feedback from a corrupted version of the same."
    }, {
      "heading" : "4.2 Comparison to Related Architectures",
      "text" : "4.2.1 Student-Teacher Models. Student-Teacher models [6, 13] also have two networks: a Teacher Network, which is large and complex, and typically an ensemble of di erent models, is rst trained to make predictions, and a much simpler Student Network, which learns to emulate the output of the Teacher Network, is trained later. There are substantial di erences between StudentTeacher models and TransNets in how they are structured. Firstly, in Student-Teacher models, the input to both the student and the\nteacher models are the same. For example, in the case of digit recognition, both networks input the same image of the digit. However, in TransNets, the inputs to the two networks are di erent. In the Target, there is only one input – the review by userA for an itemB designated as revAB . But, in the Source, there are two inputs: all the reviews written by userA sans revAB and all the reviews written for itemB sans revAB . Secondly, in Student-Teacher models, the Teacher is considerably complex in terms of width and depth, and the Student is more light-weight, trying to mimic the Teacher. In TransNets, the complexities are reversed. The Target is lean while the Source is heavy-weight, often processing large pieces of text using twice the number of parameters as the Target. Thirdly, in Student-Teacher models, the Teacher is pre-trained whereas in TransNets the Target is trained simultaneously with the Source. A recently proposed Student-Teacher model in [15] does train both the Student and the Teacher simultaneously. Also, in Student-Teacher models, the emphasis is on learning a simpler and easier model that can achieve similar results as a very complex model. But in TransNets, the objective is to learn how to transform a source representation to a target representation.\n4.2.2 Generative Adversarial Networks. TransNets also bear semblance to GANs [12, 31] since both are attempting to generate an output which is similar to realistic data. But the models are fundamentally di erent. Firstly, unlike GAN where the Generative network generates an output from a random starting point, TransNets have a starting point for each example – the reviews written by the user and those written for the item. Secondly, the Adversarial network in GAN tries to classify if the output is real or synthetic. In TransNets, although the objective is to minimize the dissimilarity between the generated representation and that of the real input, there is no adversarial classi er that attempts to separate each out. Thirdly, in GANs, the adversarial network needs to learn a notion of ‘real’ outputs, which is quite generic. In TransNets, there is always a speci c real output to compare to and does not need to learn what a generic real output will look like."
    }, {
      "heading" : "5 CONCLUSIONS",
      "text" : "Using reviews for improving recommender systems is an important task and is gaining a lot of attention in the recent years. A recent neural net model, DeepCoNN, uses the text of the reviews written by the user and for the item to learn their latent representations, which are then fed into a regression layer for rating prediction. However, its performance is dependent on having access to the user-item pairwise review, which is unavailable in real-world settings.\nIn this paper, we propose a new model called TransNets which extends DeepCoNN with an additional Transform layer. This additional layer learns to transform the latent representations of user and item into that of their pair-wise review so that at test time, an approximate representation of the target review can be generated and used for making the predictions. We also showed how TransNets can be extended to learn user and item representations from ratings only which can be used in addition to the generated review representation. Our experiments showed that TransNets and its extended version can improve the state-of-the-art substantially."
    } ],
    "references" : [ {
      "title" : "Context-aware Recommender Systems",
      "author" : [ "Gediminas Adomavicius", "Alexander Tuzhilin" ],
      "venue" : "In Proceedings of the 2008 ACM Conference on Recommender Systems (RecSys",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2008
    }, {
      "title" : "Learning Distributed Representations from Reviews for Collaborative Filtering",
      "author" : [ "Amjad Almahairi", "Kyle Kastner", "Kyunghyun Cho", "Aaron Courville" ],
      "venue" : "In Proceedings of the 9th ACM Conference on Recommender Systems (RecSys",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2015
    }, {
      "title" : "Ask the GRU: Multi-task Learning for Deep Text Recommendations",
      "author" : [ "Trapit Bansal", "David Belanger", "Andrew McCallum" ],
      "venue" : "In Proceedings of the 10th ACM Conference on Recommender Systems (RecSys",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2016
    }, {
      "title" : "TopicMF: Simultaneously Exploiting Ratings and Reviews for Recommendation",
      "author" : [ "Yang Bao", "Hui Fang", "Jie Zhang" ],
      "venue" : "In Proceedings of the Twenty-Eighth AAAI Conference on Arti cial Intelligence",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2014
    }, {
      "title" : "Latent Dirichlet Allocation",
      "author" : [ "David M. Blei", "Andrew Y. Ng", "Michael I. Jordan" ],
      "venue" : "J. Mach. Learn. Res",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2003
    }, {
      "title" : "Marginalized Denoising Autoencoders for Domain Adaptation",
      "author" : [ "Minmin Chen", "Zhixiang Xu", "Kilian Q. Weinberger", "Fei Sha" ],
      "venue" : "In Proceedings of the 29th International Coference on International Conference on Machine Learning",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2012
    }, {
      "title" : "Natural Language Processing (Almost) from Scratch",
      "author" : [ "Ronan Collobert", "Jason Weston", "Léon Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa" ],
      "venue" : "J. Mach. Learn. Res",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2011
    }, {
      "title" : "Deep Neural Networks for YouTube Recommendations",
      "author" : [ "Paul Covington", "Jay Adams", "Emre Sargin" ],
      "venue" : "In Proceedings of the 10th ACM Conference on Recommender Systems (RecSys",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2016
    }, {
      "title" : "A Multi-View Deep Learning Approach for Cross Domain User Modeling in Recommendation Systems",
      "author" : [ "Ali Mamdouh Elkahky", "Yang Song", "Xiaodong He" ],
      "venue" : "In Proceedings of the 24th International Conference on World Wide Web (WWW",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2015
    }, {
      "title" : "Generative Adversarial Nets",
      "author" : [ "Ian J. Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde- Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio" ],
      "venue" : "In Proceedings of the 27th International Conference on Neural Information Processing Systems",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2014
    }, {
      "title" : "Distilling the Knowledge in a Neural Network",
      "author" : [ "Geo rey E. Hinton", "Oriol Vinyals", "Je rey Dean" ],
      "venue" : "In Deep Learning and Representation Learning Workshop",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2014
    }, {
      "title" : "Long Short-Term Memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber" ],
      "venue" : "Neural Computation 9,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1997
    }, {
      "title" : "Harnessing Deep Neural Networks with Logic Rules",
      "author" : [ "Zhiting Hu", "Xuezhe Ma", "Zhengzhong Liu", "Eduard Hovy", "Eric Xing" ],
      "venue" : "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL)",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2016
    }, {
      "title" : "Convolutional Matrix Factorization for Document Context-Aware Recommendation",
      "author" : [ "Donghyun Kim", "Chanyoung Park", "Jinoh Oh", "Sungyoung Lee", "Hwanjo Yu" ],
      "venue" : "In Proceedings of the 10th ACM Conference on Recommender Systems (RecSys",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2016
    }, {
      "title" : "Adam: A Method for Stochastic Optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba" ],
      "venue" : "In Proceedings of the 3rd International Conference on Learning Representations (ICLR)",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2014
    }, {
      "title" : "Factorization Meets the Neighborhood: A Multifaceted Collaborative Filtering Model",
      "author" : [ "Yehuda Koren" ],
      "venue" : "In Proc. KDD",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2008
    }, {
      "title" : "Distributed Representations of Sentences and Documents",
      "author" : [ "Quoc V Le", "Tomas Mikolov" ],
      "venue" : "In ICML,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2014
    }, {
      "title" : "Gradient-based learning applied to document recognition",
      "author" : [ "Y. Lecun", "L. Bottou", "Y. Bengio", "P. Ha ner" ],
      "venue" : "Proc. IEEE 86,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 1998
    }, {
      "title" : "Deep Collaborative Filtering via Marginalized Denoising Auto-encoder",
      "author" : [ "Sheng Li", "Jaya Kawale", "Yun Fu" ],
      "venue" : "In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management (CIKM",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2015
    }, {
      "title" : "Ratings Meet Reviews, a Combined Approach to Recommend",
      "author" : [ "Guang Ling", "Michael R. Lyu", "Irwin King" ],
      "venue" : "In Proceedings of the 8th ACM Conference on Recommender Systems (RecSys",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2014
    }, {
      "title" : "The Stanford CoreNLP Natural Language Processing Toolkit. In Association for Computational Linguistics (ACL) System Demonstrations",
      "author" : [ "Christopher D. Manning", "Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven J. Bethard", "David McClosky" ],
      "venue" : null,
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2014
    }, {
      "title" : "Hidden Factors and Hidden Topics: Understanding Rating Dimensions with Review Text",
      "author" : [ "Julian McAuley", "Jure Leskovec" ],
      "venue" : "In Proceedings of the 7th ACM Conference on Recommender Systems (RecSys",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2013
    }, {
      "title" : "Inferring Networks of Substitutable and Complementary Products",
      "author" : [ "Julian McAuley", "Rahul Pandey", "Jure Leskovec" ],
      "venue" : "In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2015
    }, {
      "title" : "Image-Based Recommendations on Styles and Substitutes",
      "author" : [ "Julian McAuley", "Christopher Targett", "Qinfeng Shi", "Anton van den Hengel" ],
      "venue" : "In Proceedings  of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2015
    }, {
      "title" : "Distributed Representations of Words and Phrases and Their Compositionality",
      "author" : [ "Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg Corrado", "Je rey Dean" ],
      "venue" : "In Proceedings of the 26th International Conference on Neural Information Processing Systems",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2013
    }, {
      "title" : "Recti ed Linear Units Improve Restricted Boltzmann Machines",
      "author" : [ "Vinod Nair", "Geo rey E. Hinton" ],
      "venue" : "In Proceedings of the 27th International Conference on Machine Learning",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2010
    }, {
      "title" : "Deep Content-based Music Recommendation",
      "author" : [ "Aäron van den Oord", "Sander Dieleman", "Benjamin Schrauwen" ],
      "venue" : "In Proceedings of the 26th International Conference on Neural Information Processing Systems",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2013
    }, {
      "title" : "GloVe: Global Vectors for Word Representation",
      "author" : [ "Je rey Pennington", "Richard Socher", "Christopher D. Manning" ],
      "venue" : "In Empirical Methods in Natural Language Processing (EMNLP)",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2014
    }, {
      "title" : "Generative Adversarial Text to Image Synthesis",
      "author" : [ "Scott Reed", "Zeynep Akata", "Xinchen Yan", "Lajanugen Logeswaran", "Bernt Schiele", "Honglak Lee" ],
      "venue" : "In Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2016
    }, {
      "title" : "Factorization Machines",
      "author" : [ "Ste en Rendle" ],
      "venue" : "In Proceedings of the 2010 IEEE International Conference on Data Mining (ICDM",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2010
    }, {
      "title" : "Probabilistic Matrix Factorization",
      "author" : [ "Ruslan Salakhutdinov", "Andriy Mnih" ],
      "venue" : "In Proceedings of the 20th International Conference on Neural Information Processing Systems (NIPS’07). Curran Associates Inc.,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2007
    }, {
      "title" : "Representation Learning of Users and Items for Review Rating Prediction Using Attention-based Convolutional Neural Network. In 3rd International Workshop on Machine Learning Methods for Recommender Systems (MLRec) (SDM ’17)",
      "author" : [ "Sungyong Seo", "Jing Huang", "Hao Yang", "Yan Liu" ],
      "venue" : null,
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2017
    }, {
      "title" : "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank",
      "author" : [ "Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Y. Ng", "Christopher Potts" ],
      "venue" : "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 2013
    }, {
      "title" : "Dropout: A Simple Way to Prevent Neural Networks from Over tting",
      "author" : [ "Nitish Srivastava", "Geo rey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov" ],
      "venue" : "J. Mach. Learn. Res",
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 2014
    }, {
      "title" : "Rating-boosted Latent Topics: Understanding Users and Items with Ratings and Reviews",
      "author" : [ "Yunzhi Tan", "Min Zhang", "Yiqun Liu", "Shaoping Ma" ],
      "venue" : "In Proceedings of the Twenty-Fifth International Joint Conference on Arti cial Intelligence",
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 2016
    }, {
      "title" : "Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion",
      "author" : [ "Pascal Vincent", "Hugo Larochelle", "Isabelle Lajoie", "Yoshua Bengio", "Pierre- Antoine Manzagol" ],
      "venue" : "J. Mach. Learn. Res",
      "citeRegEx" : "38",
      "shortCiteRegEx" : "38",
      "year" : 2010
    }, {
      "title" : "Collaborative Topic Modeling for Recommending Scienti c Articles",
      "author" : [ "Chong Wang", "David M. Blei" ],
      "venue" : "In Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD",
      "citeRegEx" : "39",
      "shortCiteRegEx" : "39",
      "year" : 2011
    }, {
      "title" : "Collaborative Deep Learning for Recommender Systems",
      "author" : [ "Hao Wang", "Naiyan Wang", "Dit-Yan Yeung" ],
      "venue" : "In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD",
      "citeRegEx" : "40",
      "shortCiteRegEx" : "40",
      "year" : 2015
    }, {
      "title" : "Collaborative Denoising Auto-Encoders for Top-N Recommender Systems",
      "author" : [ "Yao Wu", "Christopher DuBois", "Alice X. Zheng", "Martin Ester" ],
      "venue" : "In Proceedings of the Ninth ACM International Conference onWeb Search and DataMining (WSDM",
      "citeRegEx" : "41",
      "shortCiteRegEx" : "41",
      "year" : 2016
    }, {
      "title" : "Beyond Clicks: Dwell Time for Personalization",
      "author" : [ "Xing Yi", "Liangjie Hong", "Erheng Zhong", "Nanthan Nan Liu", "Suju Rajan" ],
      "venue" : "In Proceedings of the 8th ACM Conference on Recommender Systems (RecSys",
      "citeRegEx" : "42",
      "shortCiteRegEx" : "42",
      "year" : 2014
    }, {
      "title" : "Explicit Factor Models for Explainable Recommendation Based on Phrase-level Sentiment Analysis",
      "author" : [ "Yongfeng Zhang", "Guokun Lai", "Min Zhang", "Yi Zhang", "Yiqun Liu", "Shaoping Ma" ],
      "venue" : "In Proceedings of the 37th International ACM SIGIR Conference on Research & Development in Information Retrieval",
      "citeRegEx" : "43",
      "shortCiteRegEx" : "43",
      "year" : 2014
    }, {
      "title" : "Joint Deep Modeling of Users and Items Using Reviews for Recommendation",
      "author" : [ "Lei Zheng", "Vahid Noroozi", "Philip S. Yu" ],
      "venue" : "In Proceedings of the Tenth ACM International Conference on Web Search and Data Mining (WSDM",
      "citeRegEx" : "44",
      "shortCiteRegEx" : "44",
      "year" : 2017
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "Using review text for predicting ratings has been shown to greatly improve the performance of recommender systems [4, 22, 24], compared to Collaborative Filtering (CF) techniques that use only past ratings [18, 33].",
      "startOffset" : 114,
      "endOffset" : 125
    }, {
      "referenceID" : 19,
      "context" : "Using review text for predicting ratings has been shown to greatly improve the performance of recommender systems [4, 22, 24], compared to Collaborative Filtering (CF) techniques that use only past ratings [18, 33].",
      "startOffset" : 114,
      "endOffset" : 125
    }, {
      "referenceID" : 21,
      "context" : "Using review text for predicting ratings has been shown to greatly improve the performance of recommender systems [4, 22, 24], compared to Collaborative Filtering (CF) techniques that use only past ratings [18, 33].",
      "startOffset" : 114,
      "endOffset" : 125
    }, {
      "referenceID" : 15,
      "context" : "Using review text for predicting ratings has been shown to greatly improve the performance of recommender systems [4, 22, 24], compared to Collaborative Filtering (CF) techniques that use only past ratings [18, 33].",
      "startOffset" : 206,
      "endOffset" : 214
    }, {
      "referenceID" : 30,
      "context" : "Using review text for predicting ratings has been shown to greatly improve the performance of recommender systems [4, 22, 24], compared to Collaborative Filtering (CF) techniques that use only past ratings [18, 33].",
      "startOffset" : 206,
      "endOffset" : 214
    }, {
      "referenceID" : 2,
      "context" : "Most neural recommender models [3, 10, 16, 21, 40] have focussed on the content associated with the user and the item, which are used to construct their latent representations.",
      "startOffset" : 31,
      "endOffset" : 50
    }, {
      "referenceID" : 8,
      "context" : "Most neural recommender models [3, 10, 16, 21, 40] have focussed on the content associated with the user and the item, which are used to construct their latent representations.",
      "startOffset" : 31,
      "endOffset" : 50
    }, {
      "referenceID" : 13,
      "context" : "Most neural recommender models [3, 10, 16, 21, 40] have focussed on the content associated with the user and the item, which are used to construct their latent representations.",
      "startOffset" : 31,
      "endOffset" : 50
    }, {
      "referenceID" : 18,
      "context" : "Most neural recommender models [3, 10, 16, 21, 40] have focussed on the content associated with the user and the item, which are used to construct their latent representations.",
      "startOffset" : 31,
      "endOffset" : 50
    }, {
      "referenceID" : 37,
      "context" : "Most neural recommender models [3, 10, 16, 21, 40] have focussed on the content associated with the user and the item, which are used to construct their latent representations.",
      "startOffset" : 31,
      "endOffset" : 50
    }, {
      "referenceID" : 0,
      "context" : "In that sense, it is a context [1] feature.",
      "startOffset" : 31,
      "endOffset" : 34
    }, {
      "referenceID" : 1,
      "context" : "Only a few neural net models [2, 34, 44] have been proposed to date that use review text for predicting the rating.",
      "startOffset" : 29,
      "endOffset" : 40
    }, {
      "referenceID" : 31,
      "context" : "Only a few neural net models [2, 34, 44] have been proposed to date that use review text for predicting the rating.",
      "startOffset" : 29,
      "endOffset" : 40
    }, {
      "referenceID" : 41,
      "context" : "Only a few neural net models [2, 34, 44] have been proposed to date that use review text for predicting the rating.",
      "startOffset" : 29,
      "endOffset" : 40
    }, {
      "referenceID" : 41,
      "context" : "Of these, the most recent model, Deep Cooperative Neural Networks (DeepCoNN) [44] uses neural nets to learn a latent representation for the user from the text of all reviews written by her and a second latent representation for the item from the text of all reviews that were written for it, and then combines these two representations in a regression layer to obtain state-of-the-art performance on rating prediction.",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 41,
      "context" : "We process text using the same approach as the current state-ofthe-art method for rating prediction, DeepCoNN [44].",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 17,
      "context" : "The basic building block, referred to as a CNN Text Processor in the rest of this paper, is a Convolutional Neural Network (CNN) [20] that inputs a sequence of words and outputs a n-dimensional vector representation for the input, i.",
      "startOffset" : 129,
      "endOffset" : 133
    }, {
      "referenceID" : 24,
      "context" : "The embedding can be any pre-trained embedding like those trained on the GoogleNews corpus using word2vec1[27], or on Wikipedia using GloVe2 [30].",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 27,
      "context" : "The embedding can be any pre-trained embedding like those trained on the GoogleNews corpus using word2vec1[27], or on Wikipedia using GloVe2 [30].",
      "startOffset" : 141,
      "endOffset" : 145
    }, {
      "referenceID" : 6,
      "context" : "Following the embedding layer is the Convolutional Layer, adapted to text processing [8].",
      "startOffset" : 85,
      "endOffset" : 88
    }, {
      "referenceID" : 25,
      "context" : "zj = α(V1:T ∗ Kj + bj ) where, bj is its bias, ∗ is the convolution operation and α is a nonlinearity like Recti ed Linear Unit (ReLU) [28] or tanh.",
      "startOffset" : 135,
      "endOffset" : 139
    }, {
      "referenceID" : 41,
      "context" : "To compute the rating rAB that userA would assign to itemB , the DeepCoNN model of [44] uses two CNN Text Processors side by side as shown in Figure 2.",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 33,
      "context" : "Both outputs are passed through a dropout layer [36].",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 29,
      "context" : "The model then concatenates the two representations as z = [x̄A, ȳB ] and passes it through a regression layer consisting of a Factorization Machine (FM) [32].",
      "startOffset" : 154,
      "endOffset" : 158
    }, {
      "referenceID" : 29,
      "context" : "This has been shown to give better parameter estimates under sparsity [32].",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 39,
      "context" : "FMs have been used successfully in large scale recommendation services like online news[42].",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 41,
      "context" : "DeepCoNN [44] also uses L1 loss.",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 21,
      "context" : "DeepCoNN model has achieved impressive MSE values surpassing that of the previous state-of-the-art models that use review texts, like the Hidden Factors as Topics (HFT) model [24], Collaborative Topic Regression (CTR) [39] and Collaborative Deep Learning (CDL) [40], as well as Collaborative Filtering techniques that use only the rating information like Matrix Factorization (MF) [18] and Probabilistic Matrix Factorization (PMF) [33].",
      "startOffset" : 175,
      "endOffset" : 179
    }, {
      "referenceID" : 36,
      "context" : "DeepCoNN model has achieved impressive MSE values surpassing that of the previous state-of-the-art models that use review texts, like the Hidden Factors as Topics (HFT) model [24], Collaborative Topic Regression (CTR) [39] and Collaborative Deep Learning (CDL) [40], as well as Collaborative Filtering techniques that use only the rating information like Matrix Factorization (MF) [18] and Probabilistic Matrix Factorization (PMF) [33].",
      "startOffset" : 218,
      "endOffset" : 222
    }, {
      "referenceID" : 37,
      "context" : "DeepCoNN model has achieved impressive MSE values surpassing that of the previous state-of-the-art models that use review texts, like the Hidden Factors as Topics (HFT) model [24], Collaborative Topic Regression (CTR) [39] and Collaborative Deep Learning (CDL) [40], as well as Collaborative Filtering techniques that use only the rating information like Matrix Factorization (MF) [18] and Probabilistic Matrix Factorization (PMF) [33].",
      "startOffset" : 261,
      "endOffset" : 265
    }, {
      "referenceID" : 15,
      "context" : "DeepCoNN model has achieved impressive MSE values surpassing that of the previous state-of-the-art models that use review texts, like the Hidden Factors as Topics (HFT) model [24], Collaborative Topic Regression (CTR) [39] and Collaborative Deep Learning (CDL) [40], as well as Collaborative Filtering techniques that use only the rating information like Matrix Factorization (MF) [18] and Probabilistic Matrix Factorization (PMF) [33].",
      "startOffset" : 381,
      "endOffset" : 385
    }, {
      "referenceID" : 30,
      "context" : "DeepCoNN model has achieved impressive MSE values surpassing that of the previous state-of-the-art models that use review texts, like the Hidden Factors as Topics (HFT) model [24], Collaborative Topic Regression (CTR) [39] and Collaborative Deep Learning (CDL) [40], as well as Collaborative Filtering techniques that use only the rating information like Matrix Factorization (MF) [18] and Probabilistic Matrix Factorization (PMF) [33].",
      "startOffset" : 431,
      "endOffset" : 435
    }, {
      "referenceID" : 16,
      "context" : "r̂T = FMT (x̄T ) Since the Target Network uses the actual review, its task is similar to sentiment analysis [19, 35].",
      "startOffset" : 108,
      "endOffset" : 116
    }, {
      "referenceID" : 32,
      "context" : "r̂T = FMT (x̄T ) Since the Target Network uses the actual review, its task is similar to sentiment analysis [19, 35].",
      "startOffset" : 108,
      "endOffset" : 116
    }, {
      "referenceID" : 22,
      "context" : "The rest are three of the larger datasets in the latest release of Amazon reviews4 [25, 26] containing reviews and ratings given by users for products purchased on amazon.",
      "startOffset" : 83,
      "endOffset" : 91
    }, {
      "referenceID" : 23,
      "context" : "The rest are three of the larger datasets in the latest release of Amazon reviews4 [25, 26] containing reviews and ratings given by users for products purchased on amazon.",
      "startOffset" : 83,
      "endOffset" : 91
    }, {
      "referenceID" : 21,
      "context" : "We report the MSE obtained on the test dataset when the MSE on the validation dataset was the lowest, similar to [24].",
      "startOffset" : 113,
      "endOffset" : 117
    }, {
      "referenceID" : 20,
      "context" : "All reviews are rst passed through a Stanford Core NLP Tokenizer [23] to obtain the tokens, which are then lowercased.",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 24,
      "context" : "A 64-dimensional word2vec6 [27] embedding using the Skip-gram model is pre-trained on the 50,000 most frequent tokens in each of the training corpora.",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 41,
      "context" : "We reuse most of the hyperparameter settings reported by the authors of DeepCoNN [44] since varying them did not give any perceivable improvement.",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 14,
      "context" : "All optimizations are learned using Adam [17], a stochastic gradient-based optimizer with adaptive estimates, at a learning rate set to 0.",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 41,
      "context" : "We compare our method against the current state-of-the-art, DeepCoNN [44].",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 21,
      "context" : "Since DeepCoNN was extensively evaluated against the previous state-of-the-art models like Hidden Factors as Topics (HFT) model [24], Collaborative Topic Regression (CTR) [39], Collaborative Deep Learning (CDL) [40] and Probabilistic Matrix Factorization (PMF) [33], and shown to surpass their performance by a wide margin, we refrain from repeating those comparisons in this paper.",
      "startOffset" : 128,
      "endOffset" : 132
    }, {
      "referenceID" : 36,
      "context" : "Since DeepCoNN was extensively evaluated against the previous state-of-the-art models like Hidden Factors as Topics (HFT) model [24], Collaborative Topic Regression (CTR) [39], Collaborative Deep Learning (CDL) [40] and Probabilistic Matrix Factorization (PMF) [33], and shown to surpass their performance by a wide margin, we refrain from repeating those comparisons in this paper.",
      "startOffset" : 171,
      "endOffset" : 175
    }, {
      "referenceID" : 37,
      "context" : "Since DeepCoNN was extensively evaluated against the previous state-of-the-art models like Hidden Factors as Topics (HFT) model [24], Collaborative Topic Regression (CTR) [39], Collaborative Deep Learning (CDL) [40] and Probabilistic Matrix Factorization (PMF) [33], and shown to surpass their performance by a wide margin, we refrain from repeating those comparisons in this paper.",
      "startOffset" : 211,
      "endOffset" : 215
    }, {
      "referenceID" : 30,
      "context" : "Since DeepCoNN was extensively evaluated against the previous state-of-the-art models like Hidden Factors as Topics (HFT) model [24], Collaborative Topic Regression (CTR) [39], Collaborative Deep Learning (CDL) [40] and Probabilistic Matrix Factorization (PMF) [33], and shown to surpass their performance by a wide margin, we refrain from repeating those comparisons in this paper.",
      "startOffset" : 261,
      "endOffset" : 265
    }, {
      "referenceID" : 41,
      "context" : "(1) DeepCoNN: The model proposed in [44].",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 21,
      "context" : "TheHidden Factors as Topics (HFT) model [24] aims to nd topics in the review text that are correlated",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 3,
      "context" : "A modi ed version of HFT is the TopicMF model [4], where the goal is to match the latent factors learned for the users and items using MF with the topics learned on their joint reviews using a Non-Negative Matrix Factorization, which is then jointly optimized with the rating prediction.",
      "startOffset" : 46,
      "endOffset" : 49
    }, {
      "referenceID" : 19,
      "context" : "model [22] where the rating is sampled from a Gaussian mixture.",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 36,
      "context" : "The Collaborative Topic Regression (CTR) model proposed in [39] is a content based approach, as opposed to a context / review based approach.",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 4,
      "context" : "It uses LDA [5] to model the text of documents (scienti c articles), and a combination of MF and content based model for recommendation.",
      "startOffset" : 12,
      "endOffset" : 15
    }, {
      "referenceID" : 34,
      "context" : "The Rating-boosted Latent Topics (RBLT) model of [37] uses a simple technique of repeating a review r times in the corpus if it was rated r , so that features in higher rated reviews will dominate the topics.",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 40,
      "context" : "Explicit Factor Models (EFM) proposed in [43] aims to generate explainable recommendations by extracting explicit product features (aspects) and users’ sentiments towards these aspects using phrase-level sentiment analysis.",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 41,
      "context" : "The most recent model to successfully employ neural networks at scale for rating prediction is the Deep Cooperative Neural Networks (DeepCoNN) [44], which was discussed in detail in Section 2.",
      "startOffset" : 143,
      "endOffset" : 147
    }, {
      "referenceID" : 1,
      "context" : "Prior to that work, [2] proposed two models: Bag-of-Words regularized Latent Factor model (BoWLF) and Language Model regularized Latent Factor model (LMLF), where MF was used to learn the latent factors of users and items, and likelihood of the review text, represented either as a bag-of-words or an LSTM embedding [14], was computed using the item factors.",
      "startOffset" : 20,
      "endOffset" : 23
    }, {
      "referenceID" : 11,
      "context" : "Prior to that work, [2] proposed two models: Bag-of-Words regularized Latent Factor model (BoWLF) and Language Model regularized Latent Factor model (LMLF), where MF was used to learn the latent factors of users and items, and likelihood of the review text, represented either as a bag-of-words or an LSTM embedding [14], was computed using the item factors.",
      "startOffset" : 316,
      "endOffset" : 320
    }, {
      "referenceID" : 31,
      "context" : "[34] proposed a CNN based model identical to DeepCoNN, but with attention mechanism to construct the latent representations, the inner product of which gave the predicted ratings.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 37,
      "context" : "The Collaborative Deep Learning (CDL) model [40] uses a Stacked De-noising Auto Encoder (SDAE) [38] to learn robust latent representations of items from their content, which is then fed into a CTR model [39] for predicting the ratings.",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 35,
      "context" : "The Collaborative Deep Learning (CDL) model [40] uses a Stacked De-noising Auto Encoder (SDAE) [38] to learn robust latent representations of items from their content, which is then fed into a CTR model [39] for predicting the ratings.",
      "startOffset" : 95,
      "endOffset" : 99
    }, {
      "referenceID" : 36,
      "context" : "The Collaborative Deep Learning (CDL) model [40] uses a Stacked De-noising Auto Encoder (SDAE) [38] to learn robust latent representations of items from their content, which is then fed into a CTR model [39] for predicting the ratings.",
      "startOffset" : 203,
      "endOffset" : 207
    }, {
      "referenceID" : 18,
      "context" : "A very similar approach to CDL is the Deep Collaborative Filtering (DCF) method [21] which uses Marginalized De-noising Auto-Encoder (mDA) [7] instead.",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 5,
      "context" : "A very similar approach to CDL is the Deep Collaborative Filtering (DCF) method [21] which uses Marginalized De-noising Auto-Encoder (mDA) [7] instead.",
      "startOffset" : 139,
      "endOffset" : 142
    }, {
      "referenceID" : 13,
      "context" : "The Convolutional Matrix Factorization (ConvMF) model [16] uses a CNN to process the description associated with the item and feed the resulting latent vectors into a PMF model for rating prediction.",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 8,
      "context" : "The Multi-View Deep Neural Net (MV-DNN) model [10] uses a deep neural net to map user’s and item’s content into a shared latent space such that their similarity in that space is maximized.",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 26,
      "context" : "[29] proposed to generate the latent factors of items – music in this case— from the content, audio signals.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 2,
      "context" : "[3] also proposed a similar technique but adapted to recommending scienti c-articles.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 7,
      "context" : "[9] used a deep neural net to learn a latent representation from video content which is then fed into a deep ranking network.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 38,
      "context" : "Collaborative De-noising Auto-Encoder model (CDAE) [41] learns to reconstruct user’s feedback from a corrupted version of the same.",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 10,
      "context" : "Student-Teacher models [6, 13] also have two networks: a Teacher Network, which is large and complex, and typically an ensemble of di erent models, is rst trained to make predictions, and a much simpler Student Network, which learns to emulate the output of the Teacher Network, is trained later.",
      "startOffset" : 23,
      "endOffset" : 30
    }, {
      "referenceID" : 12,
      "context" : "A recently proposed Student-Teacher model in [15] does train both the Student and the Teacher simultaneously.",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 9,
      "context" : "TransNets also bear semblance to GANs [12, 31] since both are attempting to generate an output which is similar to realistic data.",
      "startOffset" : 38,
      "endOffset" : 46
    }, {
      "referenceID" : 28,
      "context" : "TransNets also bear semblance to GANs [12, 31] since both are attempting to generate an output which is similar to realistic data.",
      "startOffset" : 38,
      "endOffset" : 46
    } ],
    "year" : 2017,
    "abstractText" : "Recently, deep learning methods have been shown to improve the performance of recommender systems over traditional methods, especially when review text is available. For example, a recent model, DeepCoNN, uses neural nets to learn one latent representation for the text of all reviews written by a target user, and a second latent representation for the text of all reviews for a target item, and then combines these latent representations to obtain state-of-the-art performance on recommendation tasks. We show that (unsurprisingly) much of the predictive value of review text comes from reviews of the target user for the target item. We then introduce a way in which this information can be used in recommendation, even when the target user’s review for the target item is not available. Our model, called TransNets, extends the DeepCoNN model by introducing an additional latent layer representing the target user-target item pair. We then regularize this layer, at training time, to be similar to another latent representation of the target user’s review of the target item. We show that TransNets and extensions of it improve substantially over the previous state-of-the-art.",
    "creator" : "LaTeX with hyperref package"
  }
}