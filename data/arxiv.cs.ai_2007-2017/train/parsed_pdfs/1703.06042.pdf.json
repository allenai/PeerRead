{
  "name" : "1703.06042.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Visual Web Tool to Perform What-If Analysis of Optimization Approaches",
    "authors" : [ "Sascha Van Cauwelaert", "Pierre Schaus" ],
    "emails" : [ "sascha.vancauwelaert@uclouvain.be", "michele.lombardi2@unibo.it", "pierre.schaus@uclouvain.be" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Keywords Operation Research, Evaluation, What-If Analysis, Performance Profiles, Web Tool"
    }, {
      "heading" : "1 Introduction",
      "text" : "In Operation Research (OR), evaluation is of great importance in order to validate a given solution method with respect to existing ones: for example one may be interested in assessing the effect of an improved neighborhood in Local Search, a faster cut for Mixed Integer Programming, or a global constraint in Constraint Programming. When reporting research results, it is critical to\nS. Van Cauwelaert Place Sainte Barbe 2 bte L5.02.01 1348 Louvain-la-Neuve E-mail: sascha.vancauwelaert@uclouvain.be\nM. Lombardi Viale del Risorgimento 2, Bologna (IT) E-mail: michele.lombardi2@unibo.it\nP. Schaus Place Sainte Barbe 2 bte L5.02.01 1348 Louvain-la-Neuve E-mail: pierre.schaus@uclouvain.be\nar X\niv :1\n70 3.\n06 04\n2v 1\n[ cs\n.A I]\n1 6\nM ar\n2 01\nhave the possibility to provide a meaningful analysis and interpretation of tests performed over representative benchmarks. However, some communities (e.g., Constraint Programming) tend to limit the presentation of the results to tables, sometimes with only a few instances. This can drastically reduce the significance of the derived conclusions for the general case, which should instead be the primary target when an evaluation is performed. Finding a meaningful and effective way to aggregate the results is not trivial and it has a direct impact on the conclusions. Some indicators such as the arithmetic average of normalized measures are well known to bias the results [9]. An additional difficulty is the timeout given to the experiments: some methods indeed become better if they are given more time while others are superior at the early stage of the execution.\nA performance profile [8] is a tool that is more and more widely employed to grasp a lot of conclusive information out of evaluated benchmarks. Performance profiles are cumulative distributions for a performance metric that do not suffer from the aforementioned problems. Among other advantages, they directly provide an approximate cumulative (probability) distribution function1 that a certain method can solve an arbitrary instance. Equipped with such a tool, one is clearly able to make better decisions regarding a solver, based on data from quantitative evaluations.\nWhile performance profiles are very useful in practice, there exists no tool to generate and analyze them easily (e.g., via what-if analysis on heterogeneous benchmarks), hampering their broader usage by researchers. The OR community would therefore greatly benefit from an easy-to-use Web tool to build and export such profiles, with an easy and well-defined input format. The current work proposes such a Web-based, free, and public tool.\nIt is well known that premature optimization is the root of all evil [12]. It happens that many OR researchers spend time and energy trying to improve an algorithm that is not the bottleneck of the whole problem. The tool we introduce permits what-if analysis on the performance profiles. We can for instance simulate the effect on the whole computation time of reducing the time complexity of a sub-algorithm (e.g., cut generation, global constraint, local search move).\nWe first describe the tool, and then illustrate how it can be used for the Constraint Programming and the Mixed Integer Linear Programming technologies."
    }, {
      "heading" : "2 The Public Web Tool",
      "text" : "In order to facilitate the use of performance profiles and hopefully to spread their usage among the community as a standard evaluation tool, we have built a Web tool to construct them easily. This Web tool is publicly available at http://performance-profile.info.ucl.ac.be/. It allows generating pro-\n1 Under the assumption the studied benchmarks are representative enough.\nfiles from a simple and well-defined JSON format, and to visualize the approximate effect of improving the solvers considered in an experimentation.\n2.1 Introductory Example\nTo describe how to use the tool, we will use a simple running example. While the tool is used to evaluate the performance of solvers, we will use here an analogy with cars. The evaluation results are reported in Table 1. Let us assume that we desire to compare performances of 3 cars, cA, cB and cC . Those cars will be evaluated according to the time that they need to complete tracks (i.e., from our analogy, the problem instances). The benchmark is made of 6 tracks that have different non-exclusive characteristics that can be used as labels. In this example, some tracks can have parts with roads and woods. Finally, the time required by a car to complete a track can be split in several components. That is, the total time to complete a track for a car is the sum of the times of all components of the car. For example, the car cA has a time associated to its wheels and its motor (the rest is considered as negligible). The sum of both time components will be considered as the total time for the car.\nA performance profile is a cumulative distribution of a performance metric for a solver. Let S be the set of all considered solvers ({cA, cB , cC} in our introductory example) and let I be the set of instances (e.g., the set of tracks). We also refer as metric(s, i) to the metric value for the solver s on the instance i (e.g., the time required to complete a track in our example). The profile of the solver s ∈ S is then given by:\nFs(τ) = 1\n|I| ∣∣∣∣∣∣ i ∈ I : metric(s, i)min\nb∈B (metric(b, i))\n≤ τ  ∣∣∣∣∣∣ (1)\nwhere B ⊆ S is the set of baselines, that is, the solvers against which the ratio is computed. If B contains all the approaches, the definition is the one of the original work introducing performance profiles [8]. If B contains only\none solver, it is the definition used in [18]. Some intermediary settings are also possible, in an attempt to make the tool as generic as possible.\nWhen the data from Table 1 is given to the tool with B = {cA, cB , cC}, the profile given in Figure 1 is generated. In this figure, one can observe that the x-axis is divided in 2 linear parts. The first one goes from 0 to 2, while the second one goes from 2 to 12. The bounds of the first part can be set by the user (see section 2.2), in order to select the region of τ values to be studied. The end of the second part (from 2 to 12 in the figure) cannot be configured and corresponds to the largest metric ratio computed over all the data. This second region provides a shrunk and long-term view of the profiles.\nfile:///Users/saschavancauwelaert/Downloads/performance-profile%20(91).svg 1/1\n2.2 Usage\nIn this section we describe the input and output formats for the Web tool, using our running example. We then describe the different controls that are part of the Graphical User Interface, and their utility.\nInput/Output: The input consists of a JSON file that must be passed to the interface (see Figure 3). It has to be valid according to a JSON schema, available from the Web page. A JSON Schema allows ensuring the JSON file is correct according to the expected format. As an example, Figure 2 shows the JSON file corresponding to the data from Table 1:\n– Line 2 defines the metric name to be used in the plot legend.\nTo export the graph, we offer two possibilities (see Figure 3). First, a button allows downloading the generated profiles as SVG files. SVG is a vector graphics format, similarly to PDF, but that can still be modified if required (using vector graphics tools, or even via a text editor). Moreover, several tools to convert SVG files to PDF exist. A second button provides the possibility to export the plot as a minimal Web page. This page can then be included by the user in another Web page of his choice.\nControls: Several controls are available for configuring the plot (see Figure 4). First, the user can decide what is the non-empty set of solvers to be used as baselines. Moreover, a set of optional labels can be assigned to each instance in order to characterize them (e.g., number of variables, number and type of constraints, authors of the instance, . . . ). Some instances might have no label. Using the tool controls, it is possible to specify which labels should be considered in the profile: removing a label from the set will filter out any instance characterized by this label before generating the profiles. This allows investigating easily how the performance profiles are affected by specific groups of instances in the benchmark.\nFor each component in the input data (to which values for the metric are associated), a slider is generated so as to allow the user to study the impact on the performance of being able to reduce the metric for the component by a given ratio. For example, the user may use the slider to quickly assess the impact on the profiles of making wheels that are 20% more efficient. This feature is very useful for assessing the potential benefits of a certain algorithm improvement, before actually starting to research how the improvement can actually be obtained. Alternatively, this technique allows one to estimate the amount of improvement that would be necessary to achieve a given goal.\nThe minimum and maximum τ values can be specified via an input box, thus allowing the user to focus on a specific τ region, or to have a general viewpoint. The button x scale type can be used to switch between a linear and a logarithmic scale. If the scale is linear, it is split in two parts: the first one goes from τmin to τmax and the second one goes from τmax to the maximum τ value computed out of the data. This allows\nfocusing on the desired τ region (i.e., τmin to τmax ) while also having a “longterm” view.\nFinally, a threshold for the minimum metric value of the baselines can be specified. That is, if one of the approaches considered as a baseline has a value smaller than this threshold for a given instance, this instance is filtered out. This can be used to remove noisy measurements (e.g., imprecisions of time measurements when the solution time is very small) or non-significant measures. A second threshold for the minimum unsolved metric can be specified. That is, if the value of the metric for a solver on a certain instance is larger than this second threshold, the instance will be considered as unsolved. Formally, the metric value for this solver-instance pair is considered infinite.\nThis approach can for example be used to define the time-out value for an experimentation."
    }, {
      "heading" : "3 Use Cases",
      "text" : "The tool we introduce can be useful for the whole OR community. Let us showcase its usage on two successful technologies, Constraint Programming and Mixed Integer Linear Programming. The different performance profiles we present were constructed using the tool we introduce in this work. We use the definition given in [18], that is, exactly one of the approaches is used as a baseline in Equation 1.\n3.1 Constraint Programming\nModeling in Constraint Programming is done by specifying a set of constraints to be enforced on a set of variables. The constraints may represent (in principle) any kind of combinatorial relation. To speed-up the search process, each constraint is associated to one or more algorithms that filter out inconsistent values of the variables domains. Those algorithms are generally called propagators. Because each propagator φ is called very often during search, it is important to make its processing time as small as possible. A non-negligible part of research in Constraint Programming aims at improving the time complexity of propagators. However, it is generally an open question whether reducing the time complexity will actually pay off in practice.\nWe analyzed the Energetic Reasoning propagator for the cumulative constraint [1, 5, 10, 11] on Resource Constrained Project Scheduling Problems (RCPSP) instances. This is one of the strongest propagator for cumulative in terms of performed inference, but it comes with a quite high complexity, O(n3).\nWe compare a baseline model to solve the RCPSP with an enriched model that uses the ER algorithm. Moreover, we focus on investigating the potential benefit of having an ER algorithm running in O(n2) rather than in O(n3).\nThe baseline model M employs the Timetabling algorithm from [15] and the ER Checker [6], which both run in O(n2) [6,7]. We did not use the improvements proposed in [7]. We use a dynamic search strategy, i.e., the classic SetTimes approach from [14]. We consider two benchmarks: the BL instances [5] (20-25 activities) and the PSPLIB (j30 and j90, with 30 and 90 activities) [13]. All the implementation was done in the OscaR solver [16].\nFigure 5 and 6 report profiles respectively for the BL and j90 instances. The real ER propagator provides a more important gain in the case of the BL instances than for the j90 instances. The larger problem size is a likely reason for the performance drop, so it is interesting to analyze the fictional, reducedcost implementations. In the BL benchmark a cost reduction translates to roughly proportional benefits. On j90, an O(n2) ER would lead to dramatic\nperformance improvement, but it would beat the baseline in only ∼ 35% of the cases. More interestingly, if we consider a null processing time for the ER algorithm (see the dark green curve), one can realize there is about a 35% portion of instances where the baseline would win no matter what the efficiency of ER is, i.e., where the additional pruning of ER is sometimes detrimental rather than beneficial. This can be explained by additional calls to the fix point algorithm.\nfile:///Users/saschavancauwelaert/Downloads/performance-profile%20(33).svg 1/1\nFigures 7 and 8 compare profiles for two different search strategies (a lexicographic binary strategy and the Set Times strategy [14]) on the j30 instances. The potential gain of reducing the cost is very different for the two strategies, even if the performance of the real propagator is roughly identical. This illustrates the interest of labeling instances in the format.\n3.2 Mixed Integer Linear Programming\nA well-known framework to solve Mixed Integer Linear Programs is the Branch and Cut method, a sophisticated version of Branch-and-Bound. At each node of the search tree, cutting plane algorithms can be used to find additional linear constraints satisfied by all feasible solutions. Those cuts sometimes allow discarding important parts of the search space, leading to non-negligible improvements. However, their computation time can be large, so it would be of great interest to know the gain provided by a more efficient computation. Yet\nfile:///Users/saschavancauwelaert/Downloads/performance-profile%20(35).svg 1/1\nif the gain is shown to be negligible, research should focus on other aspects of the solving process.\nLet us study the Concorde solver [3], that has been shown to be efficient at solving the Travelling Salesman Problem. In particular, its authors introduced\nfile:///Users/saschavancauwelaert/Downloads/performance-profile%20(36).svg 1/1\nlocal uts [2,4], that were crucial to solve some instances. We are interested in knowing the potential of being able to compute those cuts more efficiently.\nWe considered the TSPLib [17] set of instances, augmented with the VLSI2 instances. For each instance, we set a time limit of 900 seconds, and we measured the total time required to compute the local cuts. Instances that could not be solved to optimality or lasted less than 1 sec. were filtered out. Figure 9 compares the Concorde solver with an hypothetical version of it, where the local cuts are computed instantly. The profile for this hypothetical solver therefore provides an upper bound of the performance that can be obtained by improving computation of local cuts. For instance, one can see that for ∼ 80% of the instances, at the very most ∼ 15% of the time would be gained by reducing local cuts time computation. This is an indicator that working on this time reduction has a low chance to be fruitful in practice."
    }, {
      "heading" : "4 Conclusion",
      "text" : "In this paper, we introduced a tool that allows researchers of the OR community to easily build and export performance profiles from their experimental data. In addition, we proposed a framework to roughly estimate visually improvements brought to specific parts of a solving process. We believe this methodology can help to avoid falling into pitfalls, where improving the efficacy of a given algorithm used in a particular context is not worth experimentally. We showcased this approach for propagators in Constraint Programming and cuts in Mixed Integer Linear Programming.\n2 http://www.math.uwaterloo.ca/tsp/vlsi/index.html\nfile:///Users/saschavancauwelaert/Downloads/performance-profile%20(28).svg 1/1"
    } ],
    "references" : [ {
      "title" : "Extending chip in order to solve complex scheduling and placement problems",
      "author" : [ "A. Aggoun", "N. Beldiceanu" ],
      "venue" : "Mathematical and Computer Modelling 17(7), 57–73",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 1993
    }, {
      "title" : "Tsp cuts which do not conform to the template paradigm",
      "author" : [ "D.L. Applegate", "R.E. Bixby", "V. Chvátal", "W.J. Cook" ],
      "venue" : "Computational Combinatorial Optimization, pp. 261–303. Springer",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "The traveling salesman problem: a computational study",
      "author" : [ "D.L. Applegate", "R.E. Bixby", "V. Chvátal", "W.J. Cook" ],
      "venue" : "Princeton university press",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Constraint propagation and decomposition techniques for highly disjunctive and highly cumulative project scheduling problems",
      "author" : [ "P. Baptiste", "C. Le Pape" ],
      "venue" : "Constraints 5(12), 119–139",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Constraint-based scheduling: applying constraint programming to scheduling problems, vol",
      "author" : [ "P. Baptiste", "C. Le Pape", "W. Nuijten" ],
      "venue" : "39. Springer",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "A new characterization of relevant intervals for energetic reasoning",
      "author" : [ "A. Derrien", "T. Petit" ],
      "venue" : "Principles and Practice of Constraint Programming, pp. 289–297. Springer",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Benchmarking optimization software with performance profiles",
      "author" : [ "E.D. Dolan", "J.J. Moré" ],
      "venue" : "Mathematical programming 91(2), 201–213",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "How not to lie with statistics: the correct way to summarize benchmark results",
      "author" : [ "P.J. Fleming", "J.J. Wallace" ],
      "venue" : "Communications of the ACM 29(3), 218–221",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 1986
    }, {
      "title" : "Simple and scalable time-table filtering for the cumulative constraint",
      "author" : [ "S. Gay", "R. Hartert", "P. Schaus" ],
      "venue" : "International Conference on Principles and Practice of Constraint Programming, pp. 149–157. Springer",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Time-table disjunctive reasoning for the cumulative constraint",
      "author" : [ "S. Gay", "R. Hartert", "P. Schaus" ],
      "venue" : "International Conference on Integration of Artificial Intelligence (AI) and Operations Research (OR) techniques in Constraint Programming, pp. 157–172. Springer",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Structured programming with go to statements",
      "author" : [ "D.E. Knuth" ],
      "venue" : "ACM Comput. Surv. 6(4), 261–301",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 1974
    }, {
      "title" : "Benchmark instances for project scheduling problems",
      "author" : [ "R. Kolisch", "C. Schwindt", "A. Sprecher" ],
      "venue" : "Project Scheduling, pp. 197–212. Springer",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Time-versus-capacity compromises in project scheduling",
      "author" : [ "C. Le Pape", "P. Couronné", "D. Vergamini", "V. Gosselin" ],
      "venue" : null,
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1994
    }, {
      "title" : "A scalable sweep algorithm for the cumulative constraint",
      "author" : [ "A. Letort", "N. Beldiceanu", "M. Carlsson" ],
      "venue" : "International Conference on Principles and Practice of Constraint Programming, pp. 439–454. Springer",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Tsplib—a traveling salesman problem library",
      "author" : [ "G. Reinelt" ],
      "venue" : "ORSA journal on computing 3(4), 376–384",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 1991
    }, {
      "title" : "Integration of AI and OR Techniques in Constraint Programming: 12th International Conference, CPAIOR 2015, Barcelona, Spain, May 18-22, 2015, Proceedings, chap",
      "author" : [ "S. Van Cauwelaert", "M. Lombardi", "P. Schaus" ],
      "venue" : "Understanding the Potential of Propagators, pp. 427–436. Springer International Publishing, Cham",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 15,
      "context" : "In addition, the application relies on a methodology introduced in [18] to estimate the benefit of hypothetical solver improvements.",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 7,
      "context" : "Some indicators such as the arithmetic average of normalized measures are well known to bias the results [9].",
      "startOffset" : 105,
      "endOffset" : 108
    }, {
      "referenceID" : 6,
      "context" : "A performance profile [8] is a tool that is more and more widely employed to grasp a lot of conclusive information out of evaluated benchmarks.",
      "startOffset" : 22,
      "endOffset" : 25
    }, {
      "referenceID" : 10,
      "context" : "It is well known that premature optimization is the root of all evil [12].",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 6,
      "context" : "If B contains all the approaches, the definition is the one of the original work introducing performance profiles [8].",
      "startOffset" : 114,
      "endOffset" : 117
    }, {
      "referenceID" : 15,
      "context" : "one solver, it is the definition used in [18].",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 0,
      "context" : "1 { 2 \"metric\" :\"time\", 3 \"labels\" :[\"Road\",\"Wood\"], 4 \"instances\" :[[0],[0],[0,1],[0,1],[1],[1]], 5 \"data\": { 6 \"Car A\": {",
      "startOffset" : 77,
      "endOffset" : 82
    }, {
      "referenceID" : 0,
      "context" : "1 { 2 \"metric\" :\"time\", 3 \"labels\" :[\"Road\",\"Wood\"], 4 \"instances\" :[[0],[0],[0,1],[0,1],[1],[1]], 5 \"data\": { 6 \"Car A\": {",
      "startOffset" : 83,
      "endOffset" : 88
    }, {
      "referenceID" : 0,
      "context" : "1 { 2 \"metric\" :\"time\", 3 \"labels\" :[\"Road\",\"Wood\"], 4 \"instances\" :[[0],[0],[0,1],[0,1],[1],[1]], 5 \"data\": { 6 \"Car A\": {",
      "startOffset" : 89,
      "endOffset" : 92
    }, {
      "referenceID" : 0,
      "context" : "1 { 2 \"metric\" :\"time\", 3 \"labels\" :[\"Road\",\"Wood\"], 4 \"instances\" :[[0],[0],[0,1],[0,1],[1],[1]], 5 \"data\": { 6 \"Car A\": {",
      "startOffset" : 93,
      "endOffset" : 96
    }, {
      "referenceID" : 8,
      "context" : "7 \"wheels\" :[100,30,40,50,10,20], 8 \"motor\" :[20,3,4,5,1,2]",
      "startOffset" : 12,
      "endOffset" : 32
    }, {
      "referenceID" : 2,
      "context" : "7 \"wheels\" :[100,30,40,50,10,20], 8 \"motor\" :[20,3,4,5,1,2]",
      "startOffset" : 45,
      "endOffset" : 59
    }, {
      "referenceID" : 3,
      "context" : "7 \"wheels\" :[100,30,40,50,10,20], 8 \"motor\" :[20,3,4,5,1,2]",
      "startOffset" : 45,
      "endOffset" : 59
    }, {
      "referenceID" : 0,
      "context" : "7 \"wheels\" :[100,30,40,50,10,20], 8 \"motor\" :[20,3,4,5,1,2]",
      "startOffset" : 45,
      "endOffset" : 59
    }, {
      "referenceID" : 1,
      "context" : "7 \"wheels\" :[100,30,40,50,10,20], 8 \"motor\" :[20,3,4,5,1,2]",
      "startOffset" : 45,
      "endOffset" : 59
    }, {
      "referenceID" : 8,
      "context" : "11 \"wheels\" :[10,7,45,55,30,50] 12 },",
      "startOffset" : 13,
      "endOffset" : 31
    }, {
      "referenceID" : 5,
      "context" : "11 \"wheels\" :[10,7,45,55,30,50] 12 },",
      "startOffset" : 13,
      "endOffset" : 31
    }, {
      "referenceID" : 13,
      "context" : "13 \"Car C\" :{ 14 \"wheels\" :[15,12,35,40,15,25],",
      "startOffset" : 27,
      "endOffset" : 46
    }, {
      "referenceID" : 10,
      "context" : "13 \"Car C\" :{ 14 \"wheels\" :[15,12,35,40,15,25],",
      "startOffset" : 27,
      "endOffset" : 46
    }, {
      "referenceID" : 13,
      "context" : "13 \"Car C\" :{ 14 \"wheels\" :[15,12,35,40,15,25],",
      "startOffset" : 27,
      "endOffset" : 46
    }, {
      "referenceID" : 8,
      "context" : "15 \"motor\" :[10,3,4,5,1,2] 16 } 17 } 18 }",
      "startOffset" : 12,
      "endOffset" : 26
    }, {
      "referenceID" : 2,
      "context" : "15 \"motor\" :[10,3,4,5,1,2] 16 } 17 } 18 }",
      "startOffset" : 12,
      "endOffset" : 26
    }, {
      "referenceID" : 3,
      "context" : "15 \"motor\" :[10,3,4,5,1,2] 16 } 17 } 18 }",
      "startOffset" : 12,
      "endOffset" : 26
    }, {
      "referenceID" : 0,
      "context" : "15 \"motor\" :[10,3,4,5,1,2] 16 } 17 } 18 }",
      "startOffset" : 12,
      "endOffset" : 26
    }, {
      "referenceID" : 1,
      "context" : "15 \"motor\" :[10,3,4,5,1,2] 16 } 17 } 18 }",
      "startOffset" : 12,
      "endOffset" : 26
    }, {
      "referenceID" : 15,
      "context" : "We use the definition given in [18], that is, exactly one of the approaches is used as a baseline in Equation 1.",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 0,
      "context" : "We analyzed the Energetic Reasoning propagator for the cumulative constraint [1, 5, 10, 11] on Resource Constrained Project Scheduling Problems (RCPSP) instances.",
      "startOffset" : 77,
      "endOffset" : 91
    }, {
      "referenceID" : 3,
      "context" : "We analyzed the Energetic Reasoning propagator for the cumulative constraint [1, 5, 10, 11] on Resource Constrained Project Scheduling Problems (RCPSP) instances.",
      "startOffset" : 77,
      "endOffset" : 91
    }, {
      "referenceID" : 8,
      "context" : "We analyzed the Energetic Reasoning propagator for the cumulative constraint [1, 5, 10, 11] on Resource Constrained Project Scheduling Problems (RCPSP) instances.",
      "startOffset" : 77,
      "endOffset" : 91
    }, {
      "referenceID" : 9,
      "context" : "We analyzed the Energetic Reasoning propagator for the cumulative constraint [1, 5, 10, 11] on Resource Constrained Project Scheduling Problems (RCPSP) instances.",
      "startOffset" : 77,
      "endOffset" : 91
    }, {
      "referenceID" : 13,
      "context" : "The baseline model M employs the Timetabling algorithm from [15] and the ER Checker [6], which both run in O(n) [6,7].",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 4,
      "context" : "The baseline model M employs the Timetabling algorithm from [15] and the ER Checker [6], which both run in O(n) [6,7].",
      "startOffset" : 84,
      "endOffset" : 87
    }, {
      "referenceID" : 4,
      "context" : "The baseline model M employs the Timetabling algorithm from [15] and the ER Checker [6], which both run in O(n) [6,7].",
      "startOffset" : 112,
      "endOffset" : 117
    }, {
      "referenceID" : 5,
      "context" : "The baseline model M employs the Timetabling algorithm from [15] and the ER Checker [6], which both run in O(n) [6,7].",
      "startOffset" : 112,
      "endOffset" : 117
    }, {
      "referenceID" : 5,
      "context" : "We did not use the improvements proposed in [7].",
      "startOffset" : 44,
      "endOffset" : 47
    }, {
      "referenceID" : 12,
      "context" : ", the classic SetTimes approach from [14].",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 3,
      "context" : "We consider two benchmarks: the BL instances [5] (20-25 activities) and the PSPLIB (j30 and j90, with 30 and 90 activities) [13].",
      "startOffset" : 45,
      "endOffset" : 48
    }, {
      "referenceID" : 11,
      "context" : "We consider two benchmarks: the BL instances [5] (20-25 activities) and the PSPLIB (j30 and j90, with 30 and 90 activities) [13].",
      "startOffset" : 124,
      "endOffset" : 128
    }, {
      "referenceID" : 12,
      "context" : "Figures 7 and 8 compare profiles for two different search strategies (a lexicographic binary strategy and the Set Times strategy [14]) on the j30 instances.",
      "startOffset" : 129,
      "endOffset" : 133
    }, {
      "referenceID" : 1,
      "context" : "local uts [2,4], that were crucial to solve some instances.",
      "startOffset" : 10,
      "endOffset" : 15
    }, {
      "referenceID" : 2,
      "context" : "local uts [2,4], that were crucial to solve some instances.",
      "startOffset" : 10,
      "endOffset" : 15
    }, {
      "referenceID" : 14,
      "context" : "We considered the TSPLib [17] set of instances, augmented with the VLSI instances.",
      "startOffset" : 25,
      "endOffset" : 29
    } ],
    "year" : 2017,
    "abstractText" : "In Operation Research, practical evaluation is essential to validate the efficacy of optimization approaches. This paper promotes the usage of performance profiles as a standard practice to visualize and analyze experimental results. It introduces a Web tool to construct and export performance profiles as SVG or HTML files. In addition, the application relies on a methodology introduced in [18] to estimate the benefit of hypothetical solver improvements. Therefore, the tool allows one to employ what-if analysis to screen possible research directions, and identify those having the best potential. The approach is showcased on two Operation Research technologies: Constraint Programming and Mixed Integer Linear Programming.",
    "creator" : "LaTeX with hyperref package"
  }
}