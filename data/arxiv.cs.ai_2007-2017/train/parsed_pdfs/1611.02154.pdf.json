{
  "name" : "1611.02154.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "BAYESIAN NON-PARAMETRIC MODEL TO TARGET GAMIFICATION NOTIFICATIONS USING BIG DATA",
    "authors" : [ "Meisam Hejazi Nia", "Brian Ratchford", "Naveen Jindal" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "BAYESIAN NON-PARAMETRIC MODEL TO TARGET GAMIFICATION\nNOTIFICATIONS USING BIG DATA\nMeisam Hejazi Nia Brian Ratchford\nNaveen Jindal School of Management, Department of Marketing Science, SM32\nThe University of Texas at Dallas\n800 W. Campbell Road\nRichardson, TX, 75080-3021\nINTRODUCTION\nUser generated content (UGC) is the cornerstone of social and online marketing. However, the key challenge for online marketers to leverage UGC is to encourage users to generate more quality content. To overcome this burden, practitioners have started to use video game concepts such as badges, leaderboard, and points to encourage users, under the umbrella of an approach called Gamification. Online marketers require a data driven approach to target users based on their response to gamification elements. Knowing the response of individual users to various game elements can help the online marketer to emphasize various content generating tasks in its personal messaging, to maximize the total number of user generated contents. For example, knowing that a user reduces its content contribution after receiving a badge, an online marketer can create a diversified list of content generating tasks for user in a customized message, to make badge earning more difficult. Moreover, knowing that a user increases its content contribution after earning more points, the online marketer can create a targeted list of content generating tasks for users in a customized message, to make badge earning simpler.\nOnline marketers can leverage their massive data sets of users’ content generations to create more customized targeted messages. This big data usually consists of several little data sets for each user, but its key advantage relative to the classic data sets is that it has more information about the tail of the distribution of customer response. This tail is relevant for targeting. Of course, a model can accommodate capturing the behavior on tail, if it allows the number of parameters to grow with the size of the data set. A useful method shall not through away these data by sampling, but it shall be flexible to not to misfit.\nHierarchical Bayesian (HB) approaches are well known for their estimation of individual specific parameters, and for allowing for unobserved heterogeneity, while sharing statistical strength across individual parameters. However, to be flexible, an HB model shall deviate from the normal prior on the consumer response parameters to the mixture normal structure, to capture behavior parameter of users in tail. Furthermore, a suitable method for Big Data shall be not only scalable, but also fast, to allow an online marketer to target its users in timely manner. In summary, a suitable approach shall create a computationally tractable solution for the computationally hard gamified targeting problem for big data.\nThe current proposed model uses hierarchical Bayesian sparse modeling approach for users’ content generating choices to allow for users’ unobserved heterogeneity. It exercises a mixed logit model, with individual specific random effects that control for self-selection. To address scalability and flexibility concerns, I used a version of stochastic optimization approach called mini-batch gradient descend. Unlike the batch approach that uses complete data set to update the parameters, the mini-batch approach iteratively and randomly samples data to create a noisy measure of gradient and hessian of the objective function. Studies show that under regularity conditions the mini-batch approach can converge to the batch optimization approach. However, the advantage of the mini-batch approach is that it uses less memory, and it is computationally faster. In addition, the proposed approach estimates the mixed logit model in two steps. In the first step, it uses the observed data to identify the segment membership of each user. The BIC measure identifies the number of segments. Then, in the second step, conditional on the segment membership the model, it optimizes a-posteriori of the parameters. In summery, the current approach sets the number of segments exogenously, using BIC measure.\nAlthough the mentioned approach is not wrong, a better approach involves endogenizing the number of segments. A realistic approach should not even assume the number of segments, rather it shall assume that the world is infinitely complex, so it shall allow the model to automatically select the finite number of segments observed in the finite data set. This way the approach can be general enough to update the number of segments as firm observes more data. As a result, the learning of an online marketer from its big data is not limited anymore, and the marketer learns more about its users, as it observes more data. In fact a good approach should allow the firm to update its segmentation based on latent information set that it has captured from the streaming data. This segmentation might also evolve across time as users’ latent motivation state changes. Therefore, an online marketer requires a dynamic segmentation technique. This way the online marketers’ posterior belief about parameters evolves, as the marketer updates its belief conditioning on the latest information.\nIn fact, big data makes offline model selection computationally intractable because estimating a non-linear model over a big data for a specific model structure is time consuming. Nonparametric Bayesian provides tools for this computationally hard automatic model structure selection problem. The new approach I have planned to use falls into non-parametric Bayesian approaches category. In particular, to model users’ latent motivation I use infinite Hidden Markov Model (iHMM). In this approach, I assume that users have various latent motivation states that are time varying. These latent time varying motivation states define users’ response parameters to gamification elements, in choosing whether to contribute content or not. In this structure, the transition probability between states is modeled as a Hierarchical Dirichlet Process (HDP), and the emission probability is modeled as ordered logit model of users’ content\ncontribution choice given the latent state motivation and the users’ gamification earnings (i.e. badges, rank on the leaderboard, reputation points).\nTo control for unobserved heterogeneity across users, further I use a Dirichlet Process (DP) on the parameters of the ordered logit emission probability model. As a result the model has two building blocks of iHMM and DP to allow automatic model structure selection over big data, by endogenizing the number of user segments and states. These approaches are scalable, flexible, realistic, and machine learning literature shows that they improve prediction; however, their estimation with MCMC method suffers from slow convergence, and slow mixing problem. Therefore, to allow an online marketer to learn parameter of users responses in timely manner to target them, conditional on the latest information, I use a combination of Particle Learning (PL) and Variational Bayesian (VB). These approaches help to speed up the estimation. To estimate the iHMM model, I will use PL. PL is a Sequential Monte Carlo (SMC) method that uses simulation based on discrete approximation of a random cloud of particle to estimate the targeted posterior density. Its advantage is that it allows belief updating over parameters based on the latest observed information set in a computationally tractable way. I parallelize the PL process to speed up estimation. To speed up estimation of the DP over users specific parameters, I use a Variational Bayesian (VB) approach. This approach maximizes the evidence lower bound for the K-L divergence of parameters to approximate the parameters of the factorized variational distribution of user specific parameters.\nAll in all, I suggest an approach that helps the online marketers to target their gamification elements to users by modifying the order of the list of tasks that they send to users. It is more realistic and flexible as it allows the model to learn more parameters when the online marketers\ncollect more data. The targeting approach is scalable and quick, and it can be used over streaming data.\nKeywords: Bayesian non parametric, infinite hidden Markov model, infinite mixture model, variational Bayesian, particle learning\nMODEL The proposed model has two non-parametric Bayesian building blocks: Infinite Hidden Markov Model and Bayesian infinite Gaussian mixture model. The former is analogous to Chinese Franchise process (CFP) or Hierarchical Dirichlet Process (HDP), and the latter is analogous to Chinese restaurant process (CRP), or Dirichlet process (DP). CRP describes how customers entering a restaurant might select a table with more customers with higher probability and a new empty table with tiny probability. CFP describes a dynamic process in which there are tourists that enter a restaurant in the first night of their trip according to CRP, but the list of restaurants to visit for next night is defined per tables. For the first building block, the Infinite Hidden Markov Model in our case explains the probability of answering to a questions with a vector of gamification assets that user has accumulated until time t (including number of questions answered, number of answered received, total reputation points, reputation points earned last week, last week rank on leaderboard, first order difference rank on leader board, total gold, silver, and bronze badges earned and tags attached to them, and gold, silver, and bronze badges earned a moment ago and tags attached to them), based on a parameter that varies based on unobserved motivation of user i at time t.\nWe call the probability of this choice emission probability, and the probability of user’s transition from one state of motivation to another, the transition probability, consistent with the terminology of Hidden Markov Model. Formally, the transition probability has the following form:\n    \n\n    \n\n\n\n\n\n\n\n\n\n\n\n \n1,1,,1,\n1,,1,\n1\n11\nkkikki\nkkiikktti\nk\nk\nkkt\n \nThe infinite Hidden Markov generative process is formally defined as follows:\n)(~\n,...,1)),log(,(),,(~~\n)(~\n),(~\n),(~\n)1()(~\n),1(~\n22\n00\n1 1\n1\nit\nit\nisit\nililililililil\nisit\niiik\ni\niiiii\ni\nLogity\nlNH\nlMultinomias\nDP\nbaGamma\nSTB\nBeta\n \n\n\n \n\n\n\n \n\n\n \n \n\nwhere\nEarning badges,\nreputation points, rank on\nthe leaderboard,\nquestion answered and\nanswering questions\nMotivation States\nS=k stronger\nS=k’ Weaker\n|\n|\nObserved choice at time t\ni denotes the hyperparameter that controls the number of states, and it has beta distribution with parameter  .\ni denotes the prior distribution on each row of the transition matrix, which has stick breaking\nconstruction, with parameter  . i denotes the concentration (or novelty) parameter for the Dirichlet process rows of transition matrix. It controls how similar rows of the transition matrix are to each other, or how sparse are\nthe rows. This parameter has Gamma distribution with parameters 0a and 0b . ik denotes the k’th rowh of the transition matrix of user k, which has Dirichlet process distribution, with concentration parameter i and baseline distribution i .\nits denotes the motivation state of user i at time t, which is an indicator with multinomial distribution that\nhas a 1itis  state specific 1its parameter corresponding to the row in the transition matrix of state in previous moment 1its il denotes emission (distribution of observed choice) parameter in the logistic link function that connects unobserved motivation state l with observed choice of answering a question. This parameter has normal distribution with mean i and variance 2 i . ))log(,( 2iii   denotes individual specific vector of mean and log variance of emission parameter distribution.\nity denotes the observed choice of user i at time t, indicating whether user answered a question\nor not. The probability of this choice follows logistic distribution with parameter its  .\nThe model allows for accounting for new motivation states that can be lower or higher in the future. The distribution of choice parameters across the user population summarizes the population behavior given each motivation state. State dependent choice model:\nititit\nitititit\nititititititit\nsiitsiitsi\nitsiitsiiwsiiwsi\niwsiiwsiitsiitsi t si i sisi\nctagcbdg\ntagbdgrnkrnk\nrepcreprcvcontU\n|1 10 |1 9 |\n1 8 |1 7 |1 6 |1 5 |\n1 4 |1 3 |1 2 |1 1 ||||\n\n\n\n\n\n\n\n\n\nwhere\nitsi U | is the utility of user i to answer new questions given latent motivation state and gamification assets,\n),..,,,( 10| 1 ||| ititititit sisi t si i sis  is the state-specific parameter of response, while the first two parameters are controlling individual specific and time specific random effects, and\n),,,,,,,,,( 1111111111   ititititiwiwiwiwititit ctagcbdgtagbdgrnkrnkrepcreprcvcontx is a vector of\ntime-varying covariates associated with the gamification assets (i.e. number of questions answered, number of answered received, total reputation points, reputation points earned last week, last week rank on leaderboard, first order difference rank on leader board, total gold, silver, and bronze badges earned and tags attached to them, and gold, silver, and bronze badges earned a moment ago and tags attached to them) of user i at time t, and user choice. Motivation for this utility structure is presented at the end of this paper.\n},..,1{, )exp(1\n)exp( )|(\n|\n| |   s U\nU msSyYP\nit\nit\nit\nsi\nsi siititititi\nIn summary, the iHMM models the observed responding behavior of users as a noisy signal of hidden motivation state. This unobserved motivation state evolves with stochastic process dynamically. The form of the stochastic process is first order Markov. Another interpretation of this process is that from\neconometrician point of view users are segmented based on their unobserved motivation state dynamically. The flexible structure of transition matrix allows iHMM model to capture any type of dynamic that can be assumed for users’ transition between motivation states. The second building block controls for unobserved heterogeneity in users response parameter. This building block consists of Bayesian Dirichlet process prior on the emission parameters of choice given state of motivation. Formally, this generative process’s structure is defined as follows:\n),(~\n),()(),(~)|()(.;~|\n),,...,(~)(~\n1 ~,\n1 ),()(~),,(~\n),(),(\n),/,...,/(~,...,)1()(~\n),1(~\n),(~\n2\n1\n1\n0 0\n0 0\n0\n*\n*\n100\n00000\n001\n1 1\n0\n000\n\n\n \n\n\n \n\n\n\n  \n \n\n\n\n\n\n\n\n\n\n\n\n \n\n \n\n\n\n \n \n \n\n\nN\nDNpDNcpFc\nKDiscretecMultc\nI probwithG\nI\nn probwith\nGGGDPG\nSIWNG\nKKKDirVSTB\nBeta\nbaGamma\nk ccikilcciililcilil\nKilil\ni k\nikik kii\nGGGG\nK\nk\nj jkkk\nk\nililililil\nwhere 0 denotes the distribution of concentration (or novelty) parameter of Dirichlet process, which controls how similar are the partitions (i.e. user segments). k denotes the parameter that controls number of partitions (i.e. number of user segments), and it has beta distribution. k denotes mixture probability/proportion (latent probability measure), and it has stick breaking process construction. 0G denotes the distribution for the prior on each partition’s mean and precision, which has conjugate normal inverse Wishart structure.\ni denotes mean and precision parameter of each partition (i.e. users segment), which has Dirichlet Process distribution denoted by G. ilc denotes the latent indicator of partition membership for user I, which has discrete multinomial distribution, with probability vector of  . il denotes the vector of emission (logistic) parameter distribution mean and log variance for user i that is in partition ilc and in state l.\niD denotes the vector of demographics of user i.  denotes the parameter to explain the effect of demographic on user’s motivation to contribute across population. It has mean  and variance 2  . The Dirichlet process allows for atomic distribution of the emission parameters. Per definition, a finite subset of random measures distributed with Dirichlet Process has Dirichlet distribution. Dirichlet distribution represents the distribution of random probability measures over a simplex, and Dirichlet process represents the distribution of random partition/assignment. The key property of Dirichlet process that allows its closure under marginalization is exchangeability of partitions and assignments. This property allows to the estimation procedure to use De Finnitti theorem to marginalize out the random measure, allowing for close form probability for assignment of given data point given the assignment of all other data points. ESTIMATION\nProbability of transition from one period to another is defined as:\n),()|()),(|),(( 111111 ititititititititiititititititititi sSsSPsSyYPsSyYsSyYP  \nLikelihood of an observed sequence of choices:\n      \n\n  \n \n\n   \nT t itititit\ns s s\nT\nt ititititiiiTiTiii\nsSyYP\nsSsSPsSPyYyYP i i iT\n1\n2 111111\n)|(\n)|()(),...,( 11 12 1 \nConcretely the likelihood is:   )1(|1 |211 )1(),...,( 11 12 1 11 ititititi i iT tt ysiTt ysis s s Tt sisisiTiTiii mmqyYyYP             There are three ways to estimate the model: collapsed Gibbs sampler, Beam sampler, and Particle Filter. The former two methods are not suitable for online streaming data. Collapsed Gibbs sampler iteratively samples latent state by computing the probability of\n),,|(),,,|(),,,|( ,,, iitiititiititiitiit skspHssypHsksp    . The first factor is the\nintegrated likelihood of observation given latent state and prior distribution on the parameter H,\nso it is: ititit sttissitit dHyspsypH   ),,|(),|(: , . Given that emission distribution and prior distribution on its parameter H are conjugate, this probability is easy to compute. Furthermore, the probability of each user i in a given motivation state at time t given all other times can be written as:\n    \n    \n\n\n \n \n \n \n \n \n\n\n\n\n\n\n\n\n\n\n\n\n1\n, 1 )(\n, 1\n1 )(\n,)(\n),,|(\n1\n11\n1\n11\n1\n11\n1\n1,1, .\n, ,\n1,1, .\n, ,\n1, .\n, ,\n,\niiski\ntitiii ik\nsisk kiks\ntitiii ik\nsisk kiks\ntiii ik\nsisk kiks\niitiit\nKkif\nsskKkif n\nn n\nsskKkif n\nn n\nskKkif n\nn n\nsksp\niti\ni\nititi\niiit\ni\nititi\niiit\ni\nititi\niiit\n   \n  \n  \n\nwhere\njin , denotes the number of transitions from state i to state j, excluding time steps t-1 and t\nin. denotes the number of transitions into state i, excluding time steps t-1 and t\n.in denotes the number of transitions out of state i, excluding time steps t-1 and t\ntis , denotes state of user i at each point in time, excluding time steps t-1 and t\niK denotes number of distinct states in tis ,\nThe Gibbs sampler is useful as it is straight forward. However, it suffers from one major drawback: sequential and time series data are likely to be strongly correlated, which slows up mixing and convergence. The Beam sampler does not suffer from this slow mixing behavior, as it samples the whole sequence of states s in one go, through slice sampling and an auxiliary variable, but it is not useful for online streaming data. Particle Learning (PL) however is suitable approach for updating parameters based on the last information observed. Particle learning is an application of Sequential Monte Carlo (SMC) sampler. SMC sequentially updates the\ndistribution once a new observation is collected. PL particle approximation for state tis , and the\nstructural parameters }){,},{,( )()()()( bilt b it b lit b iti    , as follows:\n),(),...,|,( ,),(1 )( 1, , itis\nB b b titiiti swyysp iti     \nwhere\n),( ,),( , itis siti   denotes Kronecker (Dirac) delta function that represents a pulse function (a function that is zero everywhere except at subscript, at which it is one).\nB denotes the number of particles used for approximation.\nPL makes two assumptions:\nFirst, at any time t, the posterior distribution for structural parameter i depends on the states\nand observations through a low dimensional vector of sufficient statistics itr , which can be sequentially updated using recursion R such that ),,( 1,1,1,   titiitti syrRr , so that\n)|(),...,,,...,|( ,,1,,1, tiitiitiii rpyyssp   . This way the system should only keep track of the\nsufficient statistics rather than the history of motivation states and observations.\nSecond, PL requires that the predictive distribution\n   1,1,1,1,1,1,,,,, ),,|,(),|(),,|( titiitttititititiitititi dsdrsrsrpsrypsryp  can be computed in closed\nform. If these two conditions are satisfied, we can treat the sufficient statistics tr as deterministically updated state, and write:\n),..,|,(),|(),..,|,,( ,1,,,,1,1,1,, tiititititititiiititi yysrpsrypyysrp  \nAs a result\ntitiitiiititi\ntiitititiitititiitiiititi\ndsdrdyysrp\nysspyrrprpyysrp\n,, * 1,1 * ,,\n1,,1,1, * ,1,1,1,11,1,\n),..,|,,(\n),,|(),,|()|(),..,|,,(\n\n\n\n   \nwhere ),,|( 1, * ,1,  tiititi yrrp  is a point mass concentrated in ),,( 1,1,  titiit syrR .\nLogistic specification of emission link function does not satisfy the second condition. To satisfy this condition we need a conjugate distribution for the distribution of choice parameters. The data augmentation structure suggests the following form:\n)1(ln~\n)0(\n|||\n|\n\n\nitititit\nit\nsisiitssi\nsiit\nwherexU\nUIy\n\nwhere\nitsi|  is an extreme value distribution of type 1.\n)1( is an exponential of mean one.\n(.)I is an indicator function that is equal to one when the condition is met.\nFrunwirth-Schnatter and Frunwirth-Schnatter (2007) suggest a 10-component mixture of normal distribution to approximate extreme value distribution. This structure satisfies the conjugacy\ncondition. In this structure, conditional on the indicator of the component itsi z | , the model formally becomes:\njsizzsisiitssi\nsiit\nwjzPNwherexU\nUIy\nititsiitsiitititit\nit\n\n\n)(),,(~\n)0(\n||||\n|\n|| \nGiven itsi z | , we have conditional sufficient statistics recursion for its as follows:\n),,,,,( 1 1 | 1 |1 icit t si t sitt cyzUrRr iitit itsits    \nConcretely, the sufficient statistics includes the following vector:\n),,,,( |||| itsiitsiitsiit\nits UUxUxxsiit CCCUxr  \nWhere\nitsi Ux |, are mean of the gamification asset vector, and latent utility given the component\nmembership, and\nitsiitsiitsi UUxUxx CCC ||| ,, are the variances and covariance of the gamification asset vector and latent utility given the component membership.\nTherefore, the recursion for each of the sufficient statistics has the following form:\n2)1( |\n21 |2)(\n| )()1(\n)1( |\n)1( 1\n|1)( | )()(1\n2)1( 2 12)()()1(\n)( | 1 | )( | )1( |\n)( 1 )()1(\n)( 1\n)( ))((\n1\n)() 1 ())(( 1\n)() 1 ())(( 1\n)( 1\n1\n)( 1\n1\n1\n||||\n1\n||\n1\n  \n  \n\n\n \n    \n    \n    \n  \n  \n\n\n\nt si\nt sit\nsi t UU t UU\nt si t i\nt siitt\nsi t i t xU t xU\nt i itt i t ixx t ixx\nt si t si t si t si\nt iit t i t i\nit\nit\nititsiitsiitsiitsi\nit\nit\nititsiitsi\nitititit\nU t\nU UC\nt\nt C\nUx t\nUx UxC\nt\nt C\nx t\nx xC\nt\nt C\nUU t UU\nxx t xx\nIt is important to note that the sufficient statistics is state specific, so for each of the hidden states these statistics should be tracked. In other word, the drawn latent state indicator defines the sufficient statistics of which latent state should be updated for each particle.\nFrunwirth-Schnatter and Frunwirth-Schnatter (2007) furthermore computes the parameters of the 10 component mixture model as follows:\nz 1 2 3 4 5 6 7 8 9 10\nzw 0.00397 0.0396 0.168 0.147 0.125 0.101 0.104 0.116 0.107 0.088 z 5.09 3.29 1.82 1.24 0.764 0.391 0.0431 -0.306 -0.673 -1.06 z 4.50 2.02 1.10 0.422 0.198 0.107 0.0778 0.0766 0.0947 0.146\nThese expressions lead to the following iterative algorithm to update the filtering distribution:\nAlgorithm1 Particle Learning Filtering Sample )(~)(0 i b i p   and )|(~ )(00 )( 0 b ii b i sps   , and initialize )(0 b ir  .\nFor t=0 to T-1 do:\n(step-ahead prediction) Set ),,|( )()()(1 )( b it b it b itit b it rsyp   \n(Re-sample): current particles  Bbbititit rs 1)()~,~,~(  by generating an index )(~)( )(bMultibind  , where\n   B b b t\nb tb\n1'\n)'(\n)( )(  \n(Propagate): Sample\n), ~ ,~,~|(~ˆ )()()()( 1 )( 1 it b it b it b it b it b it yrssps \n(Update) : sufficient statistics\n) ~ ,~,,~( )(| )( 1,1, )()( 1, b si b titi b it b ti it UsyrRr   \n(Sample): draw structural parameters given their posterior distributions, conditioned on\ninformation available up to time t (MCMC adaptation)\n)|(~ )( 1, )( 0 b tii b i rp \n \nEnd for\nOnce filtering algorithm has been run for Tt ,...,1 , the stored particles representation for the\nmarginal distributions  Ttiitititit yysrp 11),...,|,,(  can be used to generate sample paths from the joint distribution ),...,|,,,...,,( 111 iititiiiTiT yysrsrp  by using smoothing algorithm, repeated to generate B’ sample paths, as follows:\nAlgorithm2 Particle Learning Smoothing Sample   B b rsiTiiiTiT b i b iT b iT b iT b iT\nb iTB\nyyrsprs 1 ),,(1\n)'()'()'( )()()( 1 ),...,|,,(~),,(\n  \nFor t=T-1 to 1 do:\nSet ),,|,( )'()'()'()'()'()( bi b it b it b it b it b it rsrspq  \n(Re-sample) Set    B b b it\nb itb\nit q\nq 1'' )''( )( )( and sample   B b rs b it b it b it b iT b iT rs 1'' ),( )''()'()'( )''()''(~),( \nEnd For\nIn order to write the particle learning algorithm for the infinite Hidden Markov Model consistent with Rodriguez (2011) an integrated likelihood for emission probability is required. This approach is less useful when there is uncertainty about the hyperparameters of the emissionprobability’s parameter-prior. In that case we have to draw particles on both hyperparameters and parameter of the emission-probability. I will discuss this aspect later. Formally, the integrated likelihood (which integrates out over the prior on the mean of regression parameter) has the following form for conditional exponential family:\n\n  \n\n\n\n\n\n\n\n  \n\n    \n\nn i cl iTn i ii TTn i il\nn i cil TT i\nn i ii Tn i il n i iiin\nccl TT c\ni iliill i il ii ii\nilii T iilii\niiiiii ii\niiii\nada x xytyh\ndaxaax\nxytyhdpxypxyp\naaahpN\ny yhxyha x xa xy xyt\nxaxytxyhxyp\nxxyyxy xyp\nNxy\n1 2\n2\n2111\n1 21 2\n111:1\n2 02 0 0 21 2 00\n2 2\n2 22\n2\n2\n2\n2\n2\n22\n2\n22\n2\n2\n2\n2\n)}(exp{)}()()),((exp{})(logexp{\n)}()),((exp{)}()(\n),(()(log()|()),|((),|(\n))log((2 1)()},())((exp{)()|(),(~\nlog 2 ))(log()),(log(),()(),( 2\n1 ),(,),(,\n)},(),(exp{),(),|(\n} 2\n)(\n2 exp{\n2\n1 }\n2\n)( exp{\n2\n1 ),,|(\n),0(~,\n  \n\n\n \n\n      \n\n       \n\nwhere the integral is un-normalized posterior distribution of hidden natural parameter, which is equal to the partition function of the posterior. In summary:\n   n i lin aayhxyp 1:1 )}()()(exp{),|( \n\nwhere the posterior parameters have the following forms:\n\n\n\n\n\n\n \n\n \nn i\nii n\nn i ii\nn i ii\nn\nnn\nxx\nxx\nxy\nN\n1 22 0\n2\n1 22 0\n1 22 0\n0\n2\n1 1\n1\n),(~\n\n\n\n \n\n\nAs a result integrated likelihood has the following form:\n))}log(())log(((2 1exp{),,|( 202\n0\n02 21 2 2         n n n n i ii ii xx xyp\nIn fact for the case where we have already observed n data point, for a given state in HMM, the prior has the following form:\n\n\n\n\n  \n \n \nn i l iliin xx\nn xx\nn\nn i l iliin xy\nn xx\nn\nn xy\nn\nnnl\nsxx r\nr\nsxy r\nr\nr\nNs\n1 2\n2 0\n2\n1 2\n2\n2 0\n0\n2\n, 1\n1\n, 1\n),(~|\n \n\n \n \n\n\nwhere ils denotes the indicator variable for data point i that is equal to 1 when the data point is emitted from state l , and 2l denotes the variance of conditional distribution of emission from state l , and\nn xyr and n xxr are sufficient statistics of all n data points that have been observed so far. As a result the posterior parameter after observing the n+1’th data point has the following form:\n\n\n \n\n \n\n\n\n  \n \n \nn i l liin xx\nl\nlnnnn xx\nn\nn i l liin xy\nl\nlnnnn xx\nl\nlnnnn xy\nn\nnnl\nsxx r\nsxx r\nsxy r\nsxx r\nsxy r\nNs\n1 2\n2\n,111\n2 0\n2 1\n1 2\n2\n,111\n2 0\n2\n,111\n2 0\n0\n1\n2 11\n, 1\n1\n, 1\n),(~|\n \n\n \n \n\n\nConsistent Rodriguez (2011), I integrate out the transition probability }{ is  , but to be able to run hierarchical DP model on the emission parameter, I do not integrate out the state-specific emission parameters   is  , so I draw particles for them along with other structural (i.e. non-state) parameters. Once the transition probabilities }{ is  has been integrated out of the model, the transition distribution can be written as:\n1\n1\n1 1\n11\n. 1 . 11 ),,,...,|( \n\n \n\n  \n   it\nit\nitit it it\nit\nititit\nL i t s\niLiL s s i t s\nisi t\nss iiiitit nn\nn sssp       \nwhere\n},...,max{ 1 itiit ssL  denotes the number of distinct states visited by the process up to time t,\nt ss itit n 1 denotes the number of transitions between state its and 1its up to time t, and\nt sit n . denotes the number of transitions out of state its up to time t. Conditional on the component of mixture normal that approximates logit model , the hidden\nmotivation state of user 1its at time t+1, and the component of hierarchical DP prior on the\nemission parameter ic the likelihood of observing data point 1ity weighted by the prior of emission parameter at time t+1 has the following form:\n),,,|(),,,,,,|( 1||||11   ititsissiNormitsisiitii pt sit yrzUprzUscyp itititititit\nwhere ptsit is a particle draw from posterior at time t. As a consequence, the weighted one step ahead prediction distribution reduces to:\n ),,,,,|(\n),,,,,,|(),,,,,,|(\n||11 .\n||111 . ||1\n1\n1\n11\nititit\nit\nit\nititit\nit it\nit\nititit\nititit\nsisiitiisit i t s\niLi\nitsisiitiisit\nL s i t s\nisi t\nss itsisiitiisit\nzUscyp n\nrzUscyp n\nn rzUscyp\n\n\n  \n \n \n\n\n\n \n \nNote that 1its is the only unknown particle that we integrate over. As in our approach we have already taken particles for the emission parameter and its prior hyperparameters, integration over those priors are not required.\nNote that sufficient statistics itr that we defined before is different for different hidden states, so formally it has the following structure:\n),...,(\n),,,,,(\n),...,(\n1\n1\ntiLititit\nik itit ik itit ik itit ikik it ik\ntiLi\nssss\ns UU s xx s xU s it s its s it\ns\nit s itit\nnnn\nCCCxUnr\nrrr\n\n\n\nNote that for the new state 1itL no sufficient statistics exists, so only the prior parameters iiD   and 2 i are relevant. Note that sufficient statistic vector itr is relevant only to compute posterior of structural parameters at time t, which becomes prior at time t+1, but I included it into this equation to emphasize that the information is already conditioned on by including the prior at time t+1. Rodriguez (2011) suggests that consistent with MCMC algorithm series of auxiliary variables can be used to draw structural parameters. To draw ),,...,( 11  LL  , a series of independent auxiliary variables }{ ijm for states },..,1{, Lji  can be sampled so that:\nij m jijij nmmnSmm ,...,0,))(,(...)|Pr(  \nwhere (.,.)S denotes the Stirling number of the first kind. Conditional on these auxiliary variables,\n can be sampled by:\n),,...,(~},{| .1..  Li mmDirm\nwhere   L i ijj mm 1. . Similarly, for shape parameter  , assuming ),(~  baGam a priori, another auxiliary variable  can be introduced, so that ),1(~ ..mBeta  . Under the gamma prior, the full conditional distribution for  given  corresponds to a mixture of two gamma distributions as follows:\n))log(,1()1())log(,(~,,,,|    bLaGambLaGamLba\nwhere ))}log(({\n)1( )1( ..         bm La .\nIn addition, for sampling the shape parameter  , another set of auxiliary variables can be used:\n),1(~ .ii nBetag  and )(~ . . i i i n\nn Berh\n for Li ,...,1 . Conditional on these latent variables,\nand ),(~  baGam a priori, the update of  has the following form:\n)log,(~,,,,| 1.....1...  \nL\ni iL gbhmaGamgmhba \nNote that we have also introduced two latent variables itsi U | and itsiz | to approximate logit emission probability model with mixture normal emission probability model. Frunwirth-\nSchnatter and Frunwirth-Schnatter (2007) also suggest using two extra auxiliary variables id and\nie from uniform distribution to sample latent variable itsiU | as follows:\n             }0{| )exp( log )exp(1 log log it itit it y its i its i si Ix e x d U\nFurthermore, to sample itsi z | , the suggest the following structure:\n\n  \n\n  \n  \n\n  \n  \n2\n|\n||\n|\n|\n| 2\n1 exp),|Pr(\nitsi\nitsiitit\nitsi\nititit\nz\nzitssi\nz\nj ssisi\nxUw Ujz\n\n\n\nAs these auxiliary and latent variables are required to draw other parameters, we can include them with the other structural parameters in a vector that is represented with particles, as follows:\n),,,,,},{},{,},{},{,},{,( )(| )( | )( | )( | )( | )( | )()()()()()()()()( b si b si b si b si b si b si b lit b lit b it b lit b lit b it b lit b it b i itititititit\ncedzUghm   \nAdditional particles that are required to be drawn are sufficient statistics for each state, and state indicator, as follows:\n}){,,( )()()()( blit b it b it b i rLs \nTherefore, for the iHMM the particle learning algorithm for each user I has the following form:\nAlgorithm1 Particle Learning Filtering (Initialize) Sample\n1\n0\n),(~\n),(~\n)( 0\n)( 0\n)( 0\n)( 0\n\n b b i\nb i\nb i\nL\nbaGam\nbaGam\n\n\n\n\n\n\nFor all particles Bb ..1 set:\n1ˆ\n)(\n1\n1\n1\n)( 111\n1 )( 11\n)( 111\n)( 1\n)( 1\n\n\n\n\n\nb\ni b\nb\nb i\nb i\nm\nyrr\nn\ns\nL\n\n\n\nSample )(1 b i  by first sampling ),1(~ )(111 )( 0 )( 1 bb i b i mBeta   and then sampling )(1 b i  from:\n))log(,1()1())log(,(~ )(1 )( 1 )( 1 )( 1 )( 1 )( 1 )( 1 b i b i bb i b i bb i bLaGambLaGam    \nwhere\n)log((\n1\n)1( )(1 )( 1.,.,\n)( 1\n)( 1\n)( 1\nb i b\nb i\nb\nb\nbm\nLa  \n\n \n\n   \nSample )(1ˆ b i by first generating for )( 1 b iLl  \n),1ˆ(~ )( 1,.,, )( 0 )( 1 b li b i b li nBetag\n \nand\n) ˆ (~ )(\n1,.,, )( 0\n)( 1,.,,)( 1,, b li b i b lib li n n Berh \n\n\nand then\n)log,(~ )( 1\n1\n)( 1,, )( 1,., )( 1,.,., )( 1   \nb iL l\nb li b ti b ti b i gbhmaGam\n \n\nSample\n),(~ )(1 )( 111 )( 1 b i bb i mDir  \nSample )( 1 b ilc for 1, )( 1 )( 1  b i b i LLl  from\n1)( 1  b ilc\nSample for 1, )(1 )( 1  b i b i LLl \n),(~ 00 )( 1 BBi b il DN   \n\nSample for 1, )(1 )( 1  b i b i LLl \n),(~ 00 )( 1 )( 1 BBi bb il DN  \nSample ilU by first generating two auxiliary variables 1ild and 1ile , for )( 1 b iLl   :\n),1,0(~),1,0(~ )( 1 )( 1 UnifeUnifd b il b il\nand then\n           }0{ 1 )( 1 )( 1 1 )( 1 )( 1)( 1 1)exp( log )exp(1 log log iy i b il b il i b il b ilb il Ix e x d U\nSample )(bilz , for )( 1 b iLl   by:\n\n  \n\n  \n         \n2\n)( 1\n)( 1 )( 1 )( 1\n)( 1\n)( 1 )( 1 )( 1 2\n1 exp),|Pr(\nb ilj\nb iljit b il b il\nb ilj\njb il b il b il\nxUw Ujz   \n(Update) : sufficient statistics for state )(1 b isl  as:\n2)1( 11 21 11 )1(\n)1( 11 )11(1 111 )1(\n2)1(2 1 )1(\n1 | )1( |\n1 )1(\n)()(\n)(\n)(\nˆ\n|11\n11\n1\niiUU\niiiixU\niiixx\nsisi\nii\nUUC\nUxUxC\nxxC\nUU\nxx\nitsii\ni\niiit\n\n\n\n\n\n\n\n\n\nDraw the emission parameter )(\n1 b il from its posterior:\n))log(,(\n)exp(\n1\n1\n)exp(\n1\n)exp(\n),(~|\n2 11\n2 1,2,\n2)(\n2 1,2,\n2 1,2,\n1,1,\n)( 1\n2)( 1 )( 1 )( 1\n)( 1\n1 )( 1\n)( 1\n1 )( 1\n)( 1\n11 )( 1\niiit\nz\nxU\ni\nb it\nz\nxU\ni\nz\nxU\ni\ni\nb i\nb i b il b il\nb il\ni b til\nb il\ni b\ntil\nb il\ni b til\nr\nr\nr\nNs\n\n\n\n\n\n \n\n \n \n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n(main algorithm) For t=1 to T-1 do:\n(step-ahead prediction) Compute weights    B b b t\nb tb\n1'\n)'(\n)( )(\n\n  , where\n1),,,,,|(),,(\n),,,,,|(),,(\n),,(\n)( 1 )()()( 1 )( 1)()(\n.\n)()(\n1 )()()( 1,\n)( 1 )()()( 1 )( 1)()(\n.\n)()()(\n1 )()()(\n1\n1 1 )()()()(\n111\n1 )(\n111\n11\n)(\n  \n \n \n\n\n\n\n \n\n\n\n\n\nb itit b s b s b itit b sitb\nit b ts\nb iL b it\nit b it b it b\nLi\nb itit b s b s b itit b sitb\nit b ts\nb tis b it b tss\nit b it b it b il\nL l it b it b it b il b it\nLszUsxyp n yq\nLszUsxyp n\nn yq\nyq\nititit\nit\nit b\nit\nititit\nit\nititit\nb it\n \n \n \n \n\n\n\n \n \n\n(Re-sample): Sample Bbbitit 1)()~,~(  from current particles by generating an index )(~)( )(bMultibind  , so that\n),(),..,|,(~ 1 ~ , ~ )( 11 )()( itit B b b itiitit b it b it yyp     \n(Propagate): Propagate particles to generate ),( )()( bit b it   by:\n(a) Sampling )( 1 b its   from ),,|( 1 )()()( 1   it b it b it b it yssp  , where\n)( ),,(\n),,( ),,|( )( 1 1\n1 1\n1 1 )()()(\n1 )()()(\n1 )()()( 1\n)(\n)(\nb itil\nL l L\nc it c it c it c il\nit b it b it b\nil it b it b it b it s\nyq\nyq yssp b it\nb it\n \n \n \n       \n  \n \n(b) Update the number of states by setting\n      otherwsieifL LsifL L b it b it b it b itb it )( )()( 1 )( )( 1 ~\n11 ~ \n(c) If )()( 1 ~ b it b it Ls   set )()( 1 ~ b it b it    . Otherwise, update the transition probability vector\nby setting (STB)\n  \n \n\n\n  \n\n\n1 ~~ )1(\n~~\n~~\n)()(\n,1 ~\n)()(\n,1 ~\n)()(\n)( 1,\n)(\n)(\nb it b\ntLi\nb it b\ntLi\nb it b ilt\nb til\nLlif\nLlif\nLlif\nb it\nb it\n\n   \nwhere ) ~\n,1(~ )(bitBeta \n(Update) : sufficient statistics\n(a) If )()( 1 ~ b it b it Ls   update sufficient statistics for state )( 1 b its   as follows:\n1ˆˆ )( , )( 1, 11    b tsis b tsis itititit nn\notherwise create sufficient statistics for new state 1 ~ )()(\n1  b it b it Ls  by\n1ˆ )( 1,1  b tsis itit n\n(Sample): draw structural parameters and auxiliary variables(MCMC adaptation)\n(a) Sample }ˆ,...,0{ )( 1,,, )( 1,,, b tjli b tjli nm    with\nmb tji b it b tjli b tjli mnSmm )ˆ ~)(,ˆ()Pr( )( 1,, )()( 1,,, )( 1,,,    \n(b) Sample )( 1 b it  by first sampling ),1(~ )( 1,.,., )()( 1 b ti b it b it mBeta     and then sampling\n)( 1 b it  from:\n))log(,1()1())log(,(~ )( 1 )( 1 )( 1 )( 1 )( 1 )( 1 )( 1 b it b it b t b it b it b t b it bLaGambLaGam    \n\nwhere\n)log((\n1\n)1( )( 1 )( 1,.,.,\n)( 1\n)( 1\n)( 1\nb it b ti\nb it\nb t\nb t\nbm\nLa\n\n\n\n\n     \n\n \n\nSample )( 1ˆ b it by first generating for )( 1,..,1 b itLl  \n),1ˆ(~ )( 1,.,, )()( 1 b tli b it b lit nBetag    \nand\n) ˆ (~ )(\n1,.,, )( 0\n)( 1,.,,)(\n1 b tli b i\nb tlib ilt n n Berh\n\n   \n\n\nand then\n)log,(~ )( 1\n1\n)( 1,, )( 1,., )( 1,.,., )( 1    \nb itL l\nb tli b ti b ti b it gbhmaGam\n \n\nRe-sample\n),,..,(~ )( 1 )(\n1,)(,.,\n)( 1,1,., )( 1\n1\nb it b\ntbLi\nb ti b it it mmDir   \n \n(Sample): sample structural emission parameters given their posterior distributions,\nconditioned on information available up to time t for state )( 1 b its  \n(a) Sample 1iltU by first generating two auxiliary variables 1iltd and 1ilte , for\nstate )( 1 b itsl   :\n),1,0(~),1,0(~ )( 1 )( 1 UnifeUnifd b ilt b ilt \nand then\n                 }0{ 1 )( )( 1 1 )( )( 1)( 1 1)exp( log )exp(1 log log ity it b ilt b ilt it b ilt b iltb ilt Ix e x d U\n(b) Draw the component index for the latent utility )(\n1\nb tilz   for state )( 1 b itsl   as\n)),,,,,,10(\n),..,,,,,,,1((~,,| )(\n11010 )( 1110\n)( 111 )( 1111 )()( 1 )( 1\nb tilitzz b tilitN\nb tilitzz b tilitNt b til b til b til\nxUyzpw\nxUyzpwDiryUzz\n\n\n\n\n\n\nor simply:\n\n  \n\n  \n          \n2\n)(\n)( 1 )()(\n1 )( )()( 1 )(\n1 2\n1 exp),|Pr(\nb iljt\nb iljtit b ilt\nb\ntil\nb iljt\njb ilt b til b il\nxUw Ujz   \n(c) (Update) : sufficient statistics for state )( 1 b itsl   as:\nIf )()( 1 ~ b it b it Ls   update sufficient statistics for state )( 1 b its   as follows:\n2)1( |\n21 |2)(\n| )()1(\n)1( |\n)1( 1\n|1)( | )()(1\n2)1( 2 12)()()1(\n)( | 1 | )( | )1( |\n)( 1 )()1(\n)( 1\n)( ))((\n1\n)() 1 ())(( 1\n)() 1 ())(( 1\n)( 1\n1\n)( 1\n1ˆ\n1\n||||\n1\n||\n1\n  \n  \n\n\n \n    \n    \n    \n  \n  \n\n\n\nt si\nt sit\nsi t UU t UU\nt si t i\nt siitt\nsi t i t xU t xU\nt i itt i t ixx t ixx\nt si t si t si t si\nt iit t i t i\nit\nit\nititsiitsiitsiitsi\nit\nit\nititsiitsi\nitititit\nU t\nU UC\nt\nt C\nUx t\nUx UxC\nt\nt C\nx t\nx xC\nt\nt C\nUU t UU\nxx t xx\n\n\n\n\notherwise create sufficient statistics for new state 1 ~ )()(\n1  b it b it Ls  by\n2)1( | 21 | )1(\n)1( | )1(1 |1 1\n2)1(2 1 )1(\n1 | )1( |\n1 )1(\n)()(\n)(\n)(\nˆ\n1||\n1|\n1\n\n  \n  \n\n \n\n\n\n\n\n\n\n\nt si t si t UU\nt si t i t siit t xU\nt iit t ixx\nt si t si\nit t i\nitititsiitsi\nitititsi\nitit\nUUC\nUxUxC\nxxC\nUU\nxx\n\n\n\n\n(d) Draw the emission parameter )(\n1\nb\ntil  from its posterior for )( 1 b itsl   :\n2 ,2,\n2)(\n2 ,2,\n2 ,2,\n,1,\n)(\n2)()()(\n1\n)( 1\n1 )( 1\n)( 1\n1 )( 1\n)( 1\n1 )( 1\n)exp(\n1\n1\n)exp(\n1\n)exp(\n),(~|\nb til\nit b\ntil\nb til\nit b til\nb til\nit b til\nz\nxU\nti\nb it\nz\nxU\nti\nz\nxU\nti\nti\nb it\nb it b itl\nb\ntil\nr\nr\nr\nNs\n\n\n\n\n\n\n \n\n \n \n\n\n\n\n\n\n\n\n\n\n\n\n \n\n(Hierarchical VB)\n(a) Draw )(b same across all users from its posterior:\n2)( ,\n1,\n2)( ,\n2)(\n)( 1\n,\n,\n1,\n1,1, )( 1\n1 1\n1\n),(~\nb c\nii\nt\nt\nb c\nii\nt\nb c\ni b ilt\nt\nt\nt\ntt b t\nilt\nilt\nilt\nDD\nDD\nD\nN\n\n\n \n\n\n \n\n \n \n \n\n\n\n\n\n\n\n\n\n \n(b) Run Variational Bayesian for DP across all users given \n  \n\n    \nB\nDi b t b ilt B b )( 1 )( 1\n\n,\nand recover  )|(),|(),|( ktktcititktkt qcqq   \n(a) Draw prior for the emission parameter distribution based on variational\nposterior as follows\nSample )( 1 b iltc  for 1,...1 )( 1 )( 1   b it b it LLl\n from\n)|()( 1 itilt b ilt cqc  \nSample for 1,...1 )( 1 )( 1   b it b it LLl\n\n),(~ )( 1 )( 1\n)( 1 )( 1 b\nilt b ilt cci b t b ilt DN    \n\n(b) Draw the emission parameter )(\n1\nb til  from its prior for 1 ~ )(  bitLl :\n),(~ )( 2,1 )( 1,1 )( 1 b ilt b ilt b ilt N  \n\nEnd for\nFinally, it is relevant to note that the above PL except the Hierarchical DP VB can be run in parallel to speed up the estimation procedure. Finally, we initialize the procedure with uninformative/ vague prior to get reliable estimates. This procedure gives a time evolving posterior which is approximation to the true/target posterior. Each posterior is updated in the light of recent observations. About the identification we have to note that per exchangeability property the states are subject to label switching.\nVariational Bayesian for Dirichlet Process Prior\nAs Blei and Jordan (2006) suggest, the DP can be used for nonparametric prior in a hierarchical Bayesian model. The process in a general form looks as follows:\n)|(.~\n~\n),(~ 0\nnn\nn\npX\nG\nGDPG   \nwhere  is scaling parameter, and G0 is baseline Dirichlet distribution. As the parameter are drawn from G, the data themselves will partition according to the drawn values from the same parameters. It is a form of infinite mixture model, in which we draw the parameters either from one of the partitions of parameters we have seen before, or from a new partition. This process is sometimes referred to as Polya’s urn or Chinese restaurant process. Another view suggests a stick breaking construction of G, by considering ),1(~ BetaVi and Gi ~ * for ,...}2,1{i . As a result formally we can define G and the proportions i of each of the infinite pieces of stick relative to original unit-length stick with size proportional to number of draws from a distribution as:\n  \n\n \n\n\n1\n*\n1 1\n),()(\n)1(\ni ii\ni\nj iii\nG\nVV\n\n\nAs Blei and Jordan (2006) suggest,  comprises the infinite vector of mixing proportions and\n* :1 are the infinite number of mixture components. We denote nZ as the mixture component\nwith which nX is associated. Therefore the data generating process for DP is as follows:\n1. Draw ,...}2,1{),,1(~ iBetaVi \n2. Draw ,...}2,1{,,~ 0 iGi 3. For each data point n:\na. Draw )(~ MultZn\nb. Draw )(~ nzn FX \nTo estimate this model Blei and Jordan (2006) suggest we truncate this construction at K, by setting 11 KV , which translates into Kkk  ,0 . It has shown that the truncated Dirichlet process (TDP), closely approximates a true Dirichlet process for K chosen large enough relative to the number of data. To estimates this model we use Variational Bayesian (VB) approximate of variational distribution, and its parameters. VB uses optimization of variational distribution (with free parameters) rather than sampling like Gibbs sampler. As we have selected the blocks of our model conjugate to each other, we can use mean field VB (MFVB) rather than fixed form VB (FFVB), which is appropriate for non-conjugate models. MFVB is closely related to Gibbs sampling, but it does not have the problem of mixing and stickiness that Gibbs sampler has.\nThe Jensen’s inequality suggests, a lower bound for log-likelihood as:\n)]([log)],([log\n))(log()()),(log()( )(\n),()( log),(log)(log\nHqEHxpE\ndhhqhqdhhxphqdh hq\nhxphq dhhxpxp\nqq\nhhhh\n\n \nThe above inequality can intuitively be explained by the concavity of the log function, and it should be satisfied with an arbitrary distribution q(h). H denotes the hidden variables (including unknown parameters), and x denotes the observations. The )]([log)],([log HqEHxpE qq  is\ncalled the evidence lower bound (ELBO). The first element of ELBO )],([log HxpEq is called\nthe energy function, and the second one is the entropy of the variational distribution\n)]([log HqEq . Formally, the relationship between K-L divergence and the evidence lower bound\ncan be demonstrated as follows:\n)(log)]([log)],([log\n)]|([log)]([log)]|(||)([\nxpHqEHxpE\nxHpEHqExHpHqKL\nqq\nqq\n\n\nThe key trick behind variational methods is to restrict q(h) to a parametric family such that optimizing the bound is tractable. The solution is usually straight forward by considering the natural parameter and sufficient statistic of specific family of distributions. Penny (2001) computes Kullback-Leibler (KL) divergence or relative entropy of of Normal, Gamma, Dirichlet and Wishart densities. For DP mixture Blei and Jordan (2006) apply mean filed variational approach for the stick-breaking construction. Hidden variables and unknown parameters of the model are V (stick breaking construction parameter that builds mixing distribution) *, (the\nprior on the distribution of mean and variance of each partition), and Z (the index of partition membership for each observation), and coupling them in the likelihood makes it analytically intractable. Thus, we have to introduce a variational distribution )*,,( zvq  , in which all the\nhidden variables are independent, as we factorize this variational distribution. As a result our factorized variational distribution can be written as:\n  N n nn K i ii K i ii zqqvqKzvq 11 * 1 )|()|()|(),*,,( \nwhere  are the Beta parameters for the distributions on iV (stick breaking construction parameter that builds mixing distribution),  are natural parameters for the distributions on *i (the prior on the distribution of mean and variance of each partition), and  are multinomial\nparameters for the distribution on nZ (the index of partition membership for each observation). Therefore the lower bound on the likelihood by K-L divergence criteria can be written as:\ni n\nn ZK\ni i iZ in\nnn\nN n n\nVVEVZpE\nVZqEZxpE\nVZpEpEVpExp\n\n\n \n\n\n\n\n1\n][1\n1\n)1([log()]|([log\n*)],,([log)]|([log(\n)]|([log)]|*([log)]|([log),|(log\n\n\nWe need the following elements to compute the K-L divergence:\n)()()]1[log(\n)()(][log\n)(\n)(\n][log)(]1[log()()]|([log\n2,1,2,\n2,1,1,\n1 ,\n,\n1\niiii\niiii\nK\nij inn\ninn\nin\nk\ni inn\nVE\nVE\nizq\nizq\nVEizqVEizqVZpE\n \n\n\n\n\n\n\n\n\n\n\n\nOptimization of K-L divergence criteria can be done by a coordinate ascent algorithm in the variational parameters. Coordinate ascent for exponential family distributions iteratively sets each natural variational parameter equal to the expectation of the natural conditional parameter given all other variables and observations. The algorithm is derived by equating the first order condition of the K-L divergence (or its corresponding evidence lower bound) with respect to the variational distribution to zero (by including the Largrangian multiplier condition that the variational distribution shall integrate to one). Formally:\n )],|([logexp)( 0\n* iiHii HxHpEHq\nq\nELBO\ni  \n \n\nThe updates of n follow the standard recipe for variational inference with exponential family distribution in a conjugate setting (Ghahramani & Beal, 2001), so for the parameters of the beta\ndistribution iV (stick breaking construction parameter that builds mixing distribution), we have:\n  \n \n\n\n K\nik\nN\nm ini\nN\nm ini\n1 1 ,2,\n1 ,1, 1\n\n\nThe update for the variational multinomial parameter in, of the distribution of the membership\nindex for each observation nZ , with parameter is proportional to:\n)]|)1[log(]|)([]|[]|[logexp( 1  \n K\nij jiiin\nT iiii VEaEXEVE \nFor the Gaussian component portion, we adopted an algorithm suggested by Penny (2002). We refer interested reader to that short instruction. For the model of this paper, I start with defining the prior on the parameters as follows (note that index is time varying as the variational Bayesian procedure is run at each point in time when new information becomes available as a result of running particle learning algorithm):\nThe prior on the mixing distribution, which has tick breaking construction, is defined as:\n\n\n\n \n\n\n \n\n\n1 1 11111\n1 2 1 1 1\n111\n1\n1 00000\n00\n000\n)1()(~\n),(~\n),(~\n)1()(~\n),1(~\n),(~\nk\nj jtktkttkt\nvktvktkt\nttt\nk\nj jkkk\nk\nVSTB\nBeta\nbaGamma\nVSTB\nBeta\nbaGamma\n\n\n\n\n \n\n\n\nThe prior on the distribution of precision parameter of each partition, which has Wishart distribution, is defined as:\n),()(\n),()( 111 000\n     \n\n\n\nktktkt\nk\nBaWp\nBaWp\nThe prior over the mean parameter of the partitions given the precision parameter, which has normal distribution, is defined as:\n),()|(\n),()|( 11111 00000\n         \n\n\n\nktktktktkt\nkkk\nmNp\nmNp\n\n\n\n\nThe joint likelihood of data points and partition membership indicator has the following form:\n),|()|(),( 111      ktktiltktiltiltilt pkcpcp \nThe variational approximation to the posterior of the mixing distribution, which has tick breaking construction, is defined as:\n     1 1 21 )1()(~ ),()( ),()( k j jtktkttkt vktvktkt ttt VSTB Betaq baGammaq    \nwhere\nta and tb denote the variational parameters of gamma posterior distribution of concentration parameter.\n1vkt and 2vkt denote the variational parameters of the beta posterior distribution of the parameter of stick breaking construction of mixture proportion distribution.\nThe variational approximation to the posterior of partition precisions has the following form:\n),()( ktktkt BaWq  \nWhere\nkta and ktB denote the variational parameter of Wishart posterior distribution of the precision parameters of the partitions.\nThe variational approximation to the posterior of the partition means given precisions has the following form:\n),()|( ktktktktkt mNq    \nwhere\nktm  and t denote the variational parameter of multivariate normal posterior distribution of\nthe mean parameters of the partitions.\nThe variational approximation to the posterior of the partition membership indicator for each emission parameter (or assignment probability) of user i at state l has the following form:\n),...,(~)|( 1 ciKttcictilt Multcq \nwhere\nciKttci  ,...,1 denote the variational parameter of multinomial posterior distribution of the partition membership index.\nThe coordinate ascent algorithm to estimate variational distribution as a proxy for the posterior of the parameters follows the iterations of Expectation Maximization (EM) algorithm structure as follows:\nAlgorithm3 Variational Expectation Maximization (VEM) (Initialization-step) To allow the maximum possible partitions (segments) set the number of partitions to the number of data points (emission parameters) LIK * Create random uniform number for partition membership of each data point (emission parameter) )1,0(~ ~\nUnifcilkt\nCreate the probability of partitions membership probability for each data point by normalizing cilkt ~ as follows:\n   K k tcilk cilkt cilkt 1' ' ~\n~\n \nSet the concentration parameter\n1\n1\n\n  vt vt vt b a \nSet the hyper-parameters of the stick breaking construction by\nvtvkt\nvktvkt     \n2\n1 11\nSet the hyper-parameters of the mean of the partitions by\n1   ktkt   1   ktkt mm \nSet the hyper-parameters of the precision of the partitions by\n1\n1\n \n \n\n\nktkt\nktkt\naa\nBB\n\n(E-step) Update the posterior for the partition (segment) membership indicator multinomial distribution ( the probability that k’th partition is responsible for i’th emission parameter in l’th state) by\n   K k tcilk cilkt cilkt 1' ' ~\n~\n \nwhere for computation of probability until the normalization constant we have\n1\n1\n1 212\n2111\n2/1\n2log||log)2/)1(( ~ log\n)()(\n)()(]|)1[log(]|[log) ~ log(\n)dim(\n2 exp)()( 2\n1 exp ~~~\n \n\n\n\n \n\n\n\n\n   \n\n         \n \n\nktktkt\nktkt\nD\ndkt\nK kj vjtvjtvjt\nvktvktvktvktk\nK\nkjvktkkt\nilt\nkt ktiltkt\nT ktiltktktcilkt\nBa\nDBda\nEE\nD\nD mm\n\n\n\n \n\n\n \nand (.) denotes the digamma function. Update posterior for the concentration parameter of DP by\n     1 1 212 1 1 )()( 1 K k vktvktvktvtvt vtvt bb Kaa  \n(M-step) First, we define the following:\nT ktiltktilt\nLI\nli cilkt kt kt\nilt\nLI\nli cilkt kt kt\nLI\nli cilktkt\nLI\nli cilktkt\nmm N\nN\nN\nLI\n))(( 1\n1\n*\n1\n* 1,\n* 1,\n* 1,\n* 1,\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nwhere kt denotes the proportion of data points (emission parameters) in partition k at time t\nktN denotes the number of data points (emission parameters) in partition k at time t ktkt  , denote mean and variance of data points (emission parameters) in partition k at time t\nWe update the hyperparameters as follows: Update hyperparameters of the variational distribution of the Stick Breaking Construction as\nfollows:\n \n\n \n \n\n\nLI li\nK\nkj ciljt vt\nvt vkt\nLI\nli cilktvktvkt\nb\na , 1, 12\n, 1, 1 11\n\n\nUpdate hyperparameters of the variational distribution of mean of partitions as follows:\n1\n1\n11\n \n \n    \n\n\n\n \nktktkt\nktkt\nktktktkt kt\nN\nN\nNm m\n\n\n \n\n \nUpdate the hyperparameters of variational distribution of precision of partitions as follows:\n1\n1\n111 1 ))((\n \n \n     \n\n\n \n \nktktkt\nktkt ktkt\nT ktktktktktkt\nktkt\naNa\nN N\nmmN BB\n\n\n  \n(iterations) Iterated between E-step and M-step until Evidence Lower Bound (ELBO) is maximized (converged), or equivalently the K-L divergence is minimized, as follows:\n  \n  \n  \n \n \n   \n \n  \n \n  \n   \n   \nktkt ilt\niltktktilt ktktkt\nK k\nLI li ilt\nkt ilt\nktilt kt\nK k\nLI li ilt K\nk kt ktkt\nktkt ktkt\nK k kt kt kt kt K k kt kt kt ktvt vt vt vt\ndd cq\ncp qqcq\nd cq\ncp qcqd\nq\np q\nd q\np qd\nq\np qd\nq\np qELBO\n  \n  \n    \n)(\n),,|( log)|()()(\n)(\n)|( log)()(\n)|(\n)|( log)|(\n)(\n)( log)(\n)(\n)( log)(\n)(\n)( log)(\n1\n, 1,\n1\n,\n1,1\n11\nWriting the ELBO in terms of K-L divergence leads to:\n))( ~\nlog2log( 2\n)log() ~ log(\n),;,(\n),;,(),;,(),;,(\n1\n1, 1,\n1 11\n1 1\n1\n11\n1\n1 2 1 121 11\n \n\n\n \n  \n    \n \n \n \n     \n\n \n\nK k kt ktktktkt ktLI li cilktcilktktkt\nK k ktkt kt kt ktkt kt ktN\nK k ktktktktW K k vktvktvktvktBetavtvtvtvtGamma\nD BaTrD N N\na m a mKL\nBaBaKLKLbabaKLELBO\n\n\n \n \n \n\n\nwhere\n1 111\n111\n)log)()(1()log(\n)(log)(loglog)()1(),;,(\n \n\n\n\nvt\nvtvt vtvtvtvtvt\nvtvtvtvtvtvtvtvtvtvtGamma\na\nba abbab\nbbbabbbabaKL\n\n\n)()()()(\n)()( ),(\n),( ln),;,(\n12 1 21 1 12 1 22\n1 1\n11 21\n1 2 1 11\n2 1 121    \n     \n\n  \nvktvktvktvktvktvktvktvkt\nvktvktvkt vktvkt\nvktvkt vktvktvktvktBeta B\nB KL\n\n   \n\n \n\n\n\n\n\n \n\n\n\n    \n\n \n\n   \n    \n      \n       \n                           \n                  \n\n   \nD d\nkt DDa\nkt\nDa\nD d\nkt DDa\nkt\nDa\nkt\nktktkt kt\nD d ktkt\nkt\nD d ktkt ktktktktW\nda B\nda B\nB\nB Tr aDa DB daDa\nDB daDa BaBaKL\nktkt\nktkt\n1 4\n)1(\n22\n1\n1 4 )1( 212\n1 1\n1\n11\n1\n11\n2\n1 ||2\n2\n1 ||2\nlog\n22 2loglog 2\n1\n2\n1\n2loglog 2\n1\n2\n1 ),;,(\n11\n\n\n\n\n\n\n2 )()(5.0\n5.0log5.0),;,(\n1 11\n1 1\n11\n1\n11\n1\n11\n1 1\nD mm\na invmm\na\na Tr\na\na\na m a mKL\nktkt ktkt\nktT ktkt\nktkt\nkt\nktkt\nkt\nktkt\nkt\nktkt\nkt\nktkt\nkt kt\nktkt\nkt ktN\n         \n      \n\n     \n\n\n\n \n\n \n \n  \n \n\n   \n \n\n\n\n\n   \n \n   \n \n \n \n \n \n\n\n\n\n\n\n\n \n \n\n\n\n\n\n\nIt is important to note that anywhere possible, we use log scaled values to prevent underflow. DP also like iHMM is exchangeable, so it is subject to label switching. In other words, switching the labels of partitions does not change the likelihood or posterior.\nMotivation for Gamification Utility Structure\nI start this section with explaining the choices of the gamification platform. In particular the gamification elements that I considered include: fun element, badges, leaderboard, and reputation points. For example, a gamification platform might work on the positive environment of social\ninteraction between content producers and consumers, by putting emphasis on different contents, to make the engagement more fun. It can also manipulate the threshold of earning badges, to make earning badges harder or simpler. In addition, a gamification platform can send empowering messages to users whose rank fall on the leaderboard. To find the effect of each of these policies, the gamification platform should measure the response of the users to the gamification incentives. In the context of this study the choice of users to create content can be in the following forms: to post an answer, to review, or to comment on a question or an answer, so I considered the outcome of the user choice positive if the user makes any of these choices, and negative if the user selects none. Assuming that a contributor has a random state dependent utility, and that the distribution of the random error term is extreme value, a logit function can model the probability of observing a user contribution. As a result, the likelihood of users multiple contributions, follow binomial distribution. Next I explain the rationale behind the variable that might explain the observed state of the users’ utility, in terms of the gamification components. The proposed model includes user and day fixed effect to capture users’ heterogeneous optimal stimulation level and its variation across days, because users require motivation to contribute content (Salcu and Actrinei 2013; Mittelstaedt 1976; Joachimsthaler and Lastovicka 1984; Steenkamp and Baumgartner 1992). To capture the interdependence of users’ stimulation level, the model specification includes the same prior on the fixed effect of users within each segment. Further, the same prior for the fixed effect of days considers that emotional stimulation across days have the same mean and variance. The total cumulative number of contributions acts as proxy for the fun that a user experiences. As a result, a lag cumulative number of contributions might be a state variable to capture the\neffect of the fun elements of the gamification platform. Furthermore, the number of content received (i.e. answer to the posted question) act as the proxy for the social utility of the user. As a result, I included the lagged total number of answered reviewed, and answer accepted by a user, as a proxy for the users’ reciprocity state. Another proxy for the social utility of users to contribute content is the level of reputation points, i.e. the number of up-votes a user has received (Bolton et al. 2013; Bolton et al. 2004; Yoganarasimhan 2013; Lee and Bell 2013; Toubia and Stephen 2013). As the reputation point might have both instant and long term effects, the utility of the user incorporates both the weekly level, and the cumulative level of user reputation (Wei et al. 2015; Li et al. 2015). Another gamification element that is proxy signal for social status of user is the lagged leaderboard absolute rank and rank change. The latter one might be relevant for potential endowment effect. In other words, an individual might be regretful for losing the last week rank or forgone social status. Last but not least, badges might also affect users’ motivations to contribute content, for both intrinsic (empowerment effect), or extrinsic (social status function) motivations (e.g. Antin and Churchill 2011; Wei et al. 2015; Li et al. 2015). Two types of variables capture the effect of badges: badge category (i.e. Gold, Silver, Bronze), and tagged badge category (i.e. knowledge domain tag of the badge). A user might have different preferences for different tag badges. For example, a user might value badge of gold contributor to R programming community tag more than badge of gold contributor to C++ community tag, because he wants to build reputation as a data scientist. I allowed for heterogeneity in the tag badge effects. In addition, a gold badge in any community might have its own value, for creating gold member status. Furthermore, Gold, Bronze, and Silver define different game levels. In summary, I captured the effect of gold,\nbronze, and silver status of the badges in the badge category variable, and the effect of getting either of these tagged badges, as a proxy for progress in different knowledge domains. Like the effect of any marketing policy, short term and long term effect of earning the badges might be different (Liu 2007;Jedidi et al. 1999, Mela et al. 1997; Lewis 2004). As a result, consistent with Wei et al. (2015) and Li et al. (2015), the utility of consumers includes both lagged cumulative and instant number of each of the badges. Figure 3.4 shows box and arrow diagram of the components of the state-dependent utility of users to contribute content.\nLast but not least, to control for the heterogeneity in users’ responses to each of the gamification elements, the model of the users’ state-dependent utility allows for flexible patterns of response, through a random coefficient model. Formally, the users’ state-dependent utility of user i, in segment c, at day t, in week w, has the following form:\nTable 3.1. Model Variables Definition\nVariable Description State Dependent Utility( ictU ) State dependent utility of Consumer i in segment c at time t Individual Specific Fixed Effect )( i Fixed effect, or fixed optimal threshold level of individual i Day Fixed Effect )( t Fixed effect of day t, or average effect of day t on the optimal motivation level Contribution State ( 1itcont ) Total contribution level of individual i, up until the current contribution point in time, demeaned and then normalized by a hundred Reciprocity State ( 1itrcv ) Total number of contribution received by individual i, up until day t, demeaned and normalized by a hundred Reputation State ( 1iwcrep ) Total number of reputation points received by individual i, up until week w Weekly Reputation ( 1iwrep ) Total number of reputation point received by individual i, at the previous week (i.e. week w-1) Leaderboard rank ( 1iwrnk ) Rank of individual i, in the leaderboard at previous week (i.e. week w-1) Leaderboard rank change ( 1 iwrnk ) Change in the individual i’s rank in the leaderboard from the other week to the previous week (i.e. week w-2 to week w-1) Instant Badge category ( 1itbdg ) A vector of number of gold, silver, and bronze badges individual i earned at the previous day (i.e. day t-1) Instant Tag Badges ( 1ittag ) A vector of the number of badges pertained to each of the badge categories that individual i earned until the previous day (i.e. day t-1) Cumulative Badge Category ( 1itcbdg ) A vector of total cumulative number of gold, silver, and bronze badges individual i earned until the previous day (i.e. day t-1) Cumulative Tag Badges ( 1itctag ) A vector of the total cumulative number of badges pertained to each of the badge categories that individual i earned until the previous day (i.e. day t-1) 97654321 ,,,,,,, cccccccc  Segment c specific parameters of state dependent utility of consumer i in segment c ict ,, 108 Segment independent parameters of response to tag badges, and type one extreme value error\nTo capture this heterogeneity, I used a two-step approach. First, based on the users’ cross\nsectional information presented in table 3.3 and 3.4, i.e. ix , I clustered the users, with mixture normal assumption. Second, I considered that users within each cluster c response differently to each of the state variables defined above. Formally, I used the following specification to cluster the users:\n1),,|()( 1    c ccci C c ci xfxf \nWhere f is the normal distribution density function, and ),,( ccc   are the mean, variance, and segment size parameters of each of the segments. Finally, to explain the heterogeneity in the\nparameters across segments, I use a step wise regression of the parameters of segments on the observed information of each segment, to deal with potential multi-collinearity between large numbers of variables in the observed information vector of each segment. It might be relevant to note that for computational tractability over a big data set, and parsimony, I defined the model very simple. According to the machine learning anecdotal evidence data always wins over the complex models1. In other word, although from modeling perspective, it is possible to include latent motivation levels, and forward looking behavior, such modeling choices not only might make strong assumption about the underlying behavior of consumer in an emotionally laden gamification environment, but also might make the estimation of such model over a big data set intractable. References\nBeal, M. J., Ghahramani, Z., & Rasmussen, C. E. (2001). The infinite hidden Markov model. In Advances in neural information processing systems (pp. 577-584). Blei, D. M., & Jordan, M. I. (2006). Variational inference for Dirichlet process mixtures. Bayesian analysis, 1(1), 121-143. Burda, M., Harding, M., & Hausman, J. (2008). A Bayesian mixed logit–probit model for multinomial choice. Journal of Econometrics, 147(2), 232-246. Carvalho, C., Johannes, M. S., Lopes, H. F., & Polson, N. (2010). Particle learning and smoothing. Statistical Science, 25(1), 88-106. Carvalho, C. M., Lopes, H. F., & Polson, N. (2009). Particle learning for generalized dynamic conditionally linear models. Working paper, University of Chicago Booth School of Business. Frühwirth-Schnatter, S., & Frühwirth, R. (2007). Auxiliary mixture sampling with applications to logistic models. Computational Statistics & Data Analysis, 51(7), 3509-3528. Gershman, S. J., & Blei, D. M. (2012). A tutorial on Bayesian nonparametric models. Journal of Mathematical Psychology, 56(1), 1-12. Ghahramani, Z., & Beal, M. J. (2001). Propagation algorithms for variational Bayesian learning. Advances in neural information processing systems, 507-513.\n1 Garrett, W. Why more data and simple algorithms beat complex analytics models. Data informed website. http://data-informed.com/why-more-data-and-simple-algorithms-beat-complex-analytics-models/. August 7, 2013. Accessed June 5, 2015.\nPenny, W. D. (2001). Variational Bayes for d-dimensional Gaussian mixture models. University College London. Penny, W.D. “Kullback-Liebler Divergences of Normal, Gamma, Dirichlet and Wishart Densities.” Technical report, Wellcome Department of Cognitive Neurology, 2001. Netzer, O., Lattin, J. M., & Srinivasan, V. (2008). A hidden Markov model of customer relationship dynamics. Marketing Science, 27(2), 185-204. Rodriguez, A. (2011). On-line learning for the infinite hidden Markov model.Communications in StatisticsSimulation and Computation, 40(6), 879-893. Teh, Y. W., Jordan, M. I., Beal, M. J., & Blei, D. M. (2006). Hierarchical dirichlet processes. Journal of the american statistical association, 101(476). Van Gael, J., Saatci, Y., Teh, Y. W., & Ghahramani, Z. (2008, July). Beam sampling for the infinite hidden Markov model. In Proceedings of the 25th international conference on Machine learning (pp. 1088-1095). ACM. Wood, F., & Black, M. J. (2008). A nonparametric Bayesian alternative to spike sorting. Journal of neuroscience methods, 173(1), 1-12."
    } ],
    "references" : [ {
      "title" : "The infinite hidden Markov model. In Advances in neural information processing systems (pp. 577-584)",
      "author" : [ "M.J. Beal", "Z. Ghahramani", "C.E. Rasmussen" ],
      "venue" : "Bayesian analysis,",
      "citeRegEx" : "Beal et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Beal et al\\.",
      "year" : 2001
    }, {
      "title" : "Particle learning and smoothing",
      "author" : [ "C. Carvalho", "M.S. Johannes", "H.F. Lopes", "N. Polson" ],
      "venue" : "choice. Journal of Econometrics,",
      "citeRegEx" : "Carvalho et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Carvalho et al\\.",
      "year" : 2010
    }, {
      "title" : "Auxiliary mixture sampling with applications to logistic models",
      "author" : [ "S. Frühwirth-Schnatter", "R. Frühwirth" ],
      "venue" : "Computational Statistics & Data Analysis,",
      "citeRegEx" : "Frühwirth.Schnatter and Frühwirth,? \\Q2007\\E",
      "shortCiteRegEx" : "Frühwirth.Schnatter and Frühwirth",
      "year" : 2007
    }, {
      "title" : "Why more data and simple algorithms beat complex analytics models",
      "author" : [ "W. Garrett" ],
      "venue" : "Data informed website. http://data-informed.com/why-more-data-and-simple-algorithms-beat-complex-analytics-models/. August",
      "citeRegEx" : "Garrett,? \\Q2013\\E",
      "shortCiteRegEx" : "Garrett",
      "year" : 2013
    }, {
      "title" : "Variational Bayes for d-dimensional Gaussian mixture models. University College London. Penny, W.D. “Kullback-Liebler Divergences of Normal, Gamma, Dirichlet and Wishart Densities.",
      "author" : [ "W.D. Penny" ],
      "venue" : "Technical report, Wellcome Department of Cognitive Neurology,",
      "citeRegEx" : "Penny,? \\Q2001\\E",
      "shortCiteRegEx" : "Penny",
      "year" : 2001
    }, {
      "title" : "On-line learning for the infinite hidden Markov model.Communications in StatisticsSimulation and Computation",
      "author" : [ "A. Rodriguez" ],
      "venue" : "Journal of the american statistical association,",
      "citeRegEx" : "Rodriguez,? \\Q2011\\E",
      "shortCiteRegEx" : "Rodriguez",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : "In order to write the particle learning algorithm for the infinite Hidden Markov Model consistent with Rodriguez (2011) an integrated likelihood for emission probability is required.",
      "startOffset" : 103,
      "endOffset" : 120
    }, {
      "referenceID" : 5,
      "context" : "Consistent Rodriguez (2011), I integrate out the transition probability } { i s  , but to be able to run hierarchical DP model on the emission parameter, I do not integrate out the state-specific emission parameters   i s  , so I draw particles for them along with other structural (i.",
      "startOffset" : 11,
      "endOffset" : 28
    }, {
      "referenceID" : 5,
      "context" : "Rodriguez (2011) suggests that consistent with MCMC algorithm series of auxiliary variables can be used to draw structural parameters.",
      "startOffset" : 0,
      "endOffset" : 17
    }, {
      "referenceID" : 4,
      "context" : "Penny (2001) computes Kullback-Leibler (KL) divergence or relative entropy of of Normal, Gamma, Dirichlet and Wishart densities.",
      "startOffset" : 0,
      "endOffset" : 13
    }, {
      "referenceID" : 4,
      "context" : "Penny (2001) computes Kullback-Leibler (KL) divergence or relative entropy of of Normal, Gamma, Dirichlet and Wishart densities. For DP mixture Blei and Jordan (2006) apply mean filed variational approach for the stick-breaking construction.",
      "startOffset" : 0,
      "endOffset" : 167
    }, {
      "referenceID" : 4,
      "context" : "For the Gaussian component portion, we adopted an algorithm suggested by Penny (2002). We refer interested reader to that short instruction.",
      "startOffset" : 73,
      "endOffset" : 86
    } ],
    "year" : 2016,
    "abstractText" : null,
    "creator" : null
  }
}