{
  "name" : "1405.0701.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "“Translation can’t change a name”: Using Multilingual Data for Named Entity Recognition",
    "authors" : [ "Manaal Faruqui" ],
    "emails" : [ "mfaruqui@cs.cmu.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Named Entity Recognition is an important preprocessing step for many NLP tasks. As is often the case for NLP tasks, most of the work has been done for English. A major reason for this is the unavailability of manually labeled training data for these languages. In this paper, we address this problem by showing that even in the absence of huge amounts of training data for a given language, unlabeled data from the same language as well as from other languages can be used to improve the existing NER systems. Our approach follows a semi-supervised setting, where, in addition to a small amount of training data, we assume availability of large amounts of unlabeled data from a couple of other languages that are written in a similar or same script as the target language. For example, we hypothesize that data from German & English can be used interchangeably to train NER systems for each other.\nOur hypothesis stems from the fact that NEs behave similarly across languages (Green et al.,\n2011), more so, some of the NEs like the names of the locations and people need not even undergo any orthographic transformation while being used in different languages. For example, Barack Obama is spelled the same across all the languages that use roman script like English, German and French (cf. Table 1). We leverage this repetitive information from different languages and show that it can be used to improve the performance of NER system for a given language.\nIn addition to using manually labeled data for training, we use word clusters obtained from a large monolingual corpus using unsupervised clustering. Word clustering is widely used to reduce the number of parameters in statistical models which leads to improved generalization (Brown et al., 1992; Kneser and Ney, 1993; Koo et al., 2008). Word clusters can effectively capture syntactic, semantic, or distributional regularities among the words belonging to the group (Turian et al., 2010). We acquire such semantic and syntactic similarities from large, unlabeled corpora (§3) that can support the generalization of predictions to new, unseen words in the test set while avoiding overfitting. We show improvements in the NER system performance when informed with these unsupervised clusters for a number of languages (§4.1) as well as from noisy twitter data (§4.2). ar X iv :1\n40 5.\n07 01\nv1 [\ncs .C\nL ]\n4 M\nay 2\n01 4"
    }, {
      "heading" : "2 Methodology",
      "text" : "Our methodology of using secondary language data to train NER for the target language consists of two steps: (1) Training word clusters from unlabeled secondary language data, (2) Use the word clusters as features in training the target language NER along with labeled target language data.\nSince named entities are not a closed word class, it is highly probable that during the test time a named entity is encountered which was not present in the training data. To encounter this sparsity problem, word clusters (trained on a large unlabeled corpus) are used as features in the sequence tagging problems (Clark, 2003; Faruqui and Padó, 2010; Täckström et al., 2012). Thus, an unseen named entity might belong to the same word cluster as some of the seen entities which reinforces the classifier’s belief that it is indeed a named entity, improving the classifier’s performance.\nHowever, the intuition behind using secondary language word clusters as features is that often proper nouns like names of people or locations are spelled the same across orthographically similar languages. Hence, an unseen named entity in the test set might have been present in the word clusters generated from an unlabeled corpus of a secondary language."
    }, {
      "heading" : "3 Experimental Setup",
      "text" : "Tools: We use Stanford Named Entity Recognition system1 which uses a linear-chain Conditional Random Field to predict the most likely sequence of NE labels (Finkel and Manning, 2009). It uses a variety of features, including the word, lemma, and POS tag of the current word and its context, ngram features, and word shape. This system supports inclusion of distributional similarity features such as the ones that we want to use in the form of word clusters.\nFor word clustering, we use the (Clark, 2003) system2 which in addition to the standard distributional similarity features also uses morphological information about a word using a characterbased HMM model for identifying similar words . This gives it the capability to more easily cluster unknown words in morphologically complex lan-\n1http://nlp.stanford.edu/software/ CRF-NER.shtml\n2http://www.cs.rhul.ac.uk/home/alexc/ pos2.tar.gz\nguages like German as compared to only the distributional similarity based approaches (Brown et al., 1992).\nData: We evaluate our approach on four different languages namely: German, English, Spanish & Dutch. The training and test datasets for German and English were obtained from the shared task “Language Independent Named Entity Recognition”3 at CoNLL 2003 (Tjong Kim Sang and De Meulder, 2003). The training and test data for Dutch and Spanish were obtained from a similar shared task4 at CoNLL 2002 (Tjong Kim Sang, 2002). The training data is annotated with four entity types: person (PER), location (LOC), organisation (ORG) and miscellaneous (MISC).\nFor generalization data we use the news commentary corpus released by WMT-2012 5 containing articles from 2011. It contains monolingual corpora for English, German, French, Spanish and Czech. Each of these corpora on an average have approximately 200 million tokens. Although this corpora collection doesn’t include a corpus for Dutch, we do not search for any other source of Dutch, because our aim in the first place is to show cross-language utility of resources. We train clusters of size 400 for each language as this is suitable number for the size of our generalization corpus (Faruqui and Padó, 2010)."
    }, {
      "heading" : "4 Results",
      "text" : ""
    }, {
      "heading" : "4.1 Training on monolingual clusters",
      "text" : "Table 2 shows the F1 score of the NER systems when trained using the word clusters obtained from five different languages (one at a time). The top row in the table shows the baseline NER system performance trained only on the labeled data without any word clusters. The best results are obtained when word clusters of the same language are used and in all the cases these results are highly statistically significant (p < 0.01) according to a McNemar’s test (Dietterich, 1998).\nFor English, using German word clusters helps improve its performance by 1.3 F1 points which is highly statistically significant. We observe a consistent significant improvement (p < 0.05) using\n3http://www.cnts.ua.ac.be/conll2003/ ner/\n4http://www.cnts.ua.ac.be/conll2002/ ner/\n5http://www.statmt.org/wmt12/ translation-task.html\nword clusters from French, Spanish and Czech. For a highly morphologically complex language like German it turns out that any amount of data from other languages can be put to good use, as it can be seen that using clusters from any language gives a highly significant improvement in the F1 score. On an average, an absolute 4 points increase in the score is noted for German.\nFor Spanish, a significant improvement improvement is obtained when using word clusters from all the languages except for German which might be contributed to the fact they are from different language sub-families. As we do not have generalization data for Dutch, we used word clusters from all other languages and observed that the highest improvement is obtained when word clusters from German are used. This is expected as German and Dutch are typographically very similar languages and come from the Germanic language family. It is notable that even a language like Czech which is a slavic language and is significantly different from the romance and germanic languages gives significant improvement for all the languages."
    }, {
      "heading" : "4.2 Training on multilingual clusters",
      "text" : "In events when we have unlabeled data from the given language and many secondary languages, we should ideally be able to use word clusters from all of them. In order to do this, we merge the word clusters from different languages together by: (1) Keeping all the words of the given language intact (2) Importing only those words from the secondary language which are not present\nin the original language (thus improving recall). While importing a foreign word, it is assigned to that word cluster which has maximum number of words in common with its present cluster.\nUsing this technique we merge all the word clusters from different languages (German, English, French, Spanish & Czech) together into one multilingual word clustering. Table 3 shows the performance of multilingual word clusters trained NERs against the baseline NERs. In all the cases, again the NERs trained with multilingual guidance perform significantly better than the baseline NERs and also perform better than the NERs trained with only monolingual word clusters (cf. Table 2)."
    }, {
      "heading" : "4.3 Training on out-of-domain clusters",
      "text" : "The labeled NE training data for all of the languages we have used comes from the newswire domain. Thus the news-commentary data (cf. Sec. 3) that we use for word clustering is indomain data. Since we cannot always expect to obtain in-domain data, we use word clusters6 obtained from a large collection of English tweets containing approx. 850 million tokens clustered into 1000 classes for generalization (Owoputi et al., 2013). Table 4 shows the performance of NER systems trained using the twitter word clusters. For English, German and French we again obtain a highly significant improvement; however the improvements obtained using the out-of-domain data are lesser than that obtained using in-domain data.\n6http://www.ark.cs.cmu.edu/TweetNLP/"
    }, {
      "heading" : "4.4 Analysis",
      "text" : "In order to verify our hypothesis that certain NEs, specially the names of people and locations might not go orthographic changes while transferring to similar languages we look at the category-wise improvement of the NER systems when trained using word clusters of a language (other than self) that gives the best results against the baseline model. For example, for Spanish NER we compare the baseline model with the model trained using French word clusters. Table 5 shows the cateogry-wise improvement in the F1 score of the NER systems.\nFor all the languages, the best improvement is obtained for the LOC or the PER class. On an average, the highest improvement is obtained for PER followed by LOC and least for MISC category. The reason for poor improvement in MISC category is that it mostly contains linguistically inflected forms of proper nouns like Italian, Irish, Palestinians etc. which translate into different lexical forms in different languages.\nWe now analyse the words present in the test set which are out-of-vocabulary (OOV) of the training set. A fraction of these OOV words are present in the word clusters that we obtain from a different language, most of which are names of locations or people as we hypothesised. We list the most frequent such words from the test set in Table 6."
    }, {
      "heading" : "5 Related Work",
      "text" : "Our work is primarily inspired by Faruqui and Padó (2010) which shows that a substantial improvement in the German NER system performance can be obtained by using unsuper-\nvised German word clusters. NER systems have been trained using the same technique for other languages like English (Finkel and Manning, 2009), Croatian (Glavaš et al., 2012) and Slovene (Ljubešic et al., 2012). Other approaches to enhance NER include that of transfer of linguistic structure from one language to another (Täckström et al., 2012; Faruqui and Dyer, 2013) by aligning word clusters across languages. Green et al. (2011) exploits the fact that NEs retain their shape across languages and tries to group NEs across language together.\nIn a broader perspective this can be framed as a problem of resource sharing (Bateman et al., 2005) among different languages. Languages that are closely related like Hindi and Urdu benefit from sharing resources for NLP tasks (Visweswariah et al., 2010). Also, closely related are the problems of multilingual language learning using unsupervised and supervised approaches (Diab, 2003; Guo and Diab, 2010) and cross-lingual annotation projection applied to bootstrapping parsers (Hwa et al., 2005; Smith and Smith, 2007). Multilingual guidance has also been used for training Part-ofSpeech (POS) taggers (Snyder et al., 2008; Snyder et al., 2009; Das and Petrov, 2011).\nOur approach is different from the previous approaches in that we are directly using data from secondary languages for training NER systems for the given language instead of deriving any indirect knowledge from the secondary language data using projection or bilingual clustering techniques. It is simple and significantly effective."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We have shown that a statistically significant improvement in the performance of NER system for a given language can be obtained when the training data is supplemented with word clusters from a secondary language(s) which is written using the same alphabet. The amount of help provided by this secondary language depends on how similar the secondary language is to the given language phylogenetically and also on the domain of the data from which the word clusters are obtained. This performance improvement occurs because many of the NEs, specially, names of persons and locations remain the same when used in a different language and hence the word class information of such an OOV word is helpful in predicting its NE class."
    } ],
    "references" : [ {
      "title" : "Multilingual resource sharing across both related and unrelated languages: An implemented, open-source framework for practical natural language generation",
      "author" : [ "Bateman et al.2005] J. Bateman", "I. Kruijff-Korbayov", "G.-J. Kruijff" ],
      "venue" : "Research on Language",
      "citeRegEx" : "Bateman et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Bateman et al\\.",
      "year" : 2005
    }, {
      "title" : "Classbased n-gram models of natural language",
      "author" : [ "Brown et al.1992] P.F. Brown", "P.V. deSouza", "R.L. Mercer", "V.J.D. Pietra", "J.C. Lai" ],
      "venue" : "Comput. Linguist.,",
      "citeRegEx" : "Brown et al\\.,? \\Q1992\\E",
      "shortCiteRegEx" : "Brown et al\\.",
      "year" : 1992
    }, {
      "title" : "Combining distributional and morphological information for part of speech induction",
      "author" : [ "A. Clark" ],
      "venue" : "In Proceedings of the tenth conference on European chapter of the Association for Computational Linguistics - Volume",
      "citeRegEx" : "Clark.,? \\Q2003\\E",
      "shortCiteRegEx" : "Clark.",
      "year" : 2003
    }, {
      "title" : "Unsupervised part-of-speech tagging with bilingual graph-based projections",
      "author" : [ "Das", "Petrov2011] D. Das", "S. Petrov" ],
      "venue" : "Proceedings of the Association for Computational Linguistics",
      "citeRegEx" : "Das et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Das et al\\.",
      "year" : 2011
    }, {
      "title" : "Word sense disambiguation within a multilingual framework",
      "author" : [ "M.T. Diab" ],
      "venue" : "Ph.D. thesis, University of Maryland at College",
      "citeRegEx" : "Diab.,? \\Q2003\\E",
      "shortCiteRegEx" : "Diab.",
      "year" : 2003
    }, {
      "title" : "Approximate statistical tests for comparing supervised classification learning algorithms",
      "author" : [ "T.G. Dietterich" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Dietterich.,? \\Q1998\\E",
      "shortCiteRegEx" : "Dietterich.",
      "year" : 1998
    }, {
      "title" : "An information theoretic approach to bilingual word clustering",
      "author" : [ "Faruqui", "Dyer2013] M. Faruqui", "C. Dyer" ],
      "venue" : "In Proceedings of ACL",
      "citeRegEx" : "Faruqui et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Faruqui et al\\.",
      "year" : 2013
    }, {
      "title" : "Training and Evaluating a German Named Entity Recognizer with Semantic Generalization",
      "author" : [ "Faruqui", "Padó2010] M. Faruqui", "S. Padó" ],
      "venue" : "In Proceedings of KONVENS",
      "citeRegEx" : "Faruqui et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Faruqui et al\\.",
      "year" : 2010
    }, {
      "title" : "Nested named entity recognition",
      "author" : [ "Finkel", "Manning2009] J.R. Finkel", "C.D. Manning" ],
      "venue" : "In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1 - Volume 1,",
      "citeRegEx" : "Finkel et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Finkel et al\\.",
      "year" : 2009
    }, {
      "title" : "Croner: A state-of-the-art named entity recognition and classification for croatian",
      "author" : [ "Glavaš et al.2012] G. Glavaš", "M. Karan", "F. Šaric", "J. Šnajder", "J. Mijic", "A. Šilic", "B.D. Bašic" ],
      "venue" : null,
      "citeRegEx" : "Glavaš et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Glavaš et al\\.",
      "year" : 2012
    }, {
      "title" : "Entity clustering across languages",
      "author" : [ "Green et al.2011] S. Green", "N. Andrews", "M. Gormley", "M. Dredze", "C. Manning" ],
      "venue" : null,
      "citeRegEx" : "Green et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Green et al\\.",
      "year" : 2011
    }, {
      "title" : "Combining orthogonal monolingual and multilingual sources of evidence for all words wsd",
      "author" : [ "Guo", "Diab2010] W. Guo", "M. Diab" ],
      "venue" : "In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Guo et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2010
    }, {
      "title" : "Bootstrapping parsers via syntactic projection across parallel texts",
      "author" : [ "R. Hwa", "P. Resnik", "A. Weinberg", "C.I. Cabezas", "O. Kolak" ],
      "venue" : "Natural Language Engineering,",
      "citeRegEx" : "Hwa et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Hwa et al\\.",
      "year" : 2005
    }, {
      "title" : "Forming word classes by statistical clustering for statistical language modelling",
      "author" : [ "Kneser", "Ney1993] R. Kneser", "H. Ney" ],
      "venue" : "In R. Khler and B. Rieger, editors, Contributions to Quantitative Linguistics,",
      "citeRegEx" : "Kneser et al\\.,? \\Q1993\\E",
      "shortCiteRegEx" : "Kneser et al\\.",
      "year" : 1993
    }, {
      "title" : "Simple semi-supervised dependency parsing",
      "author" : [ "Koo et al.2008] T. Koo", "X. Carreras", "M. Collins" ],
      "venue" : "In Proc. of ACL",
      "citeRegEx" : "Koo et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Koo et al\\.",
      "year" : 2008
    }, {
      "title" : "Building named entity recognition models for croatian and slovene",
      "author" : [ "Ljubešic et al.2012] N. Ljubešic", "M. Stupar", "T. Juric" ],
      "venue" : null,
      "citeRegEx" : "Ljubešic et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Ljubešic et al\\.",
      "year" : 2012
    }, {
      "title" : "Improved part-of-speech tagging for online conversational text with word clusters",
      "author" : [ "Owoputi et al.2013] O. Owoputi", "B. OConnor", "C. Dyer", "K. Gimpel", "N. Schneider", "N.A. Smith" ],
      "venue" : "In Proceedings of NAACL-HLT,",
      "citeRegEx" : "Owoputi et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Owoputi et al\\.",
      "year" : 2013
    }, {
      "title" : "Probabilistic Models of Nonprojective Dependency Trees",
      "author" : [ "Smith", "Smith2007] D.A. Smith", "N.A. Smith" ],
      "venue" : "In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Lan-",
      "citeRegEx" : "Smith et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Smith et al\\.",
      "year" : 2007
    }, {
      "title" : "Unsupervised multilingual learning for pos tagging",
      "author" : [ "Snyder et al.2008] B. Snyder", "T. Naseem", "J. Eisenstein", "R. Barzilay" ],
      "venue" : "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Snyder et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Snyder et al\\.",
      "year" : 2008
    }, {
      "title" : "Adding more languages improves unsupervised multilingual part-of-speech tagging: a bayesian non-parametric approach",
      "author" : [ "Snyder et al.2009] B. Snyder", "T. Naseem", "J. Eisenstein", "R. Barzilay" ],
      "venue" : "In Proceedings NAACL",
      "citeRegEx" : "Snyder et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Snyder et al\\.",
      "year" : 2009
    }, {
      "title" : "Cross-lingual word clusters for direct transfer of linguistic structure",
      "author" : [ "R. McDonald", "J. Uszkoreit" ],
      "venue" : "In The 2012 Conference of the North American Chapter of the Association for Computational Linguistics:",
      "citeRegEx" : "Täckström et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Täckström et al\\.",
      "year" : 2012
    }, {
      "title" : "Introduction to the conll-2003 shared task: Language-independent named entity recognition",
      "author" : [ "Tjong Kim Sang", "F. De Meulder" ],
      "venue" : "Proceedings of CoNLL-2003,",
      "citeRegEx" : "Sang et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Sang et al\\.",
      "year" : 2003
    }, {
      "title" : "Introduction to the conll-2002 shared task: Language-independent named entity recognition",
      "author" : [ ],
      "venue" : "In Proceedings of CoNLL-2002,",
      "citeRegEx" : "Sang.,? \\Q2002\\E",
      "shortCiteRegEx" : "Sang.",
      "year" : 2002
    }, {
      "title" : "Word representations: a simple and general method for semi-supervised learning",
      "author" : [ "Turian et al.2010] J. Turian", "L. Ratinov", "Y. Bengio" ],
      "venue" : "In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Turian et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Turian et al\\.",
      "year" : 2010
    }, {
      "title" : "Urdu and hindi: Translation and sharing of linguistic resources",
      "author" : [ "V. Chenthamarakshan", "N. Kambhatla" ],
      "venue" : "In Coling 2010: Posters,",
      "citeRegEx" : "Visweswariah et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Visweswariah et al\\.",
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "Word clustering is widely used to reduce the number of parameters in statistical models which leads to improved generalization (Brown et al., 1992; Kneser and Ney, 1993; Koo et al., 2008).",
      "startOffset" : 127,
      "endOffset" : 187
    }, {
      "referenceID" : 14,
      "context" : "Word clustering is widely used to reduce the number of parameters in statistical models which leads to improved generalization (Brown et al., 1992; Kneser and Ney, 1993; Koo et al., 2008).",
      "startOffset" : 127,
      "endOffset" : 187
    }, {
      "referenceID" : 23,
      "context" : "Word clusters can effectively capture syntactic, semantic, or distributional regularities among the words belonging to the group (Turian et al., 2010).",
      "startOffset" : 129,
      "endOffset" : 150
    }, {
      "referenceID" : 2,
      "context" : "To encounter this sparsity problem, word clusters (trained on a large unlabeled corpus) are used as features in the sequence tagging problems (Clark, 2003; Faruqui and Padó, 2010; Täckström et al., 2012).",
      "startOffset" : 142,
      "endOffset" : 203
    }, {
      "referenceID" : 20,
      "context" : "To encounter this sparsity problem, word clusters (trained on a large unlabeled corpus) are used as features in the sequence tagging problems (Clark, 2003; Faruqui and Padó, 2010; Täckström et al., 2012).",
      "startOffset" : 142,
      "endOffset" : 203
    }, {
      "referenceID" : 2,
      "context" : "For word clustering, we use the (Clark, 2003) system2 which in addition to the standard distributional similarity features also uses morphological information about a word using a characterbased HMM model for identifying similar words .",
      "startOffset" : 32,
      "endOffset" : 45
    }, {
      "referenceID" : 1,
      "context" : "tributional similarity based approaches (Brown et al., 1992).",
      "startOffset" : 40,
      "endOffset" : 60
    }, {
      "referenceID" : 5,
      "context" : "McNemar’s test (Dietterich, 1998).",
      "startOffset" : 15,
      "endOffset" : 33
    }, {
      "referenceID" : 16,
      "context" : "850 million tokens clustered into 1000 classes for generalization (Owoputi et al., 2013).",
      "startOffset" : 66,
      "endOffset" : 88
    }, {
      "referenceID" : 9,
      "context" : "other languages like English (Finkel and Manning, 2009), Croatian (Glavaš et al., 2012) and Slovene (Ljubešic et al.",
      "startOffset" : 66,
      "endOffset" : 87
    }, {
      "referenceID" : 15,
      "context" : ", 2012) and Slovene (Ljubešic et al., 2012).",
      "startOffset" : 20,
      "endOffset" : 43
    }, {
      "referenceID" : 20,
      "context" : "Other approaches to enhance NER include that of transfer of linguistic structure from one language to another (Täckström et al., 2012; Faruqui and Dyer, 2013) by aligning word clusters across languages.",
      "startOffset" : 110,
      "endOffset" : 158
    }, {
      "referenceID" : 9,
      "context" : "other languages like English (Finkel and Manning, 2009), Croatian (Glavaš et al., 2012) and Slovene (Ljubešic et al., 2012). Other approaches to enhance NER include that of transfer of linguistic structure from one language to another (Täckström et al., 2012; Faruqui and Dyer, 2013) by aligning word clusters across languages. Green et al. (2011) exploits the fact that NEs retain their shape across languages and tries to group NEs across language together.",
      "startOffset" : 67,
      "endOffset" : 348
    }, {
      "referenceID" : 0,
      "context" : "In a broader perspective this can be framed as a problem of resource sharing (Bateman et al., 2005) among different languages.",
      "startOffset" : 77,
      "endOffset" : 99
    }, {
      "referenceID" : 24,
      "context" : "closely related like Hindi and Urdu benefit from sharing resources for NLP tasks (Visweswariah et al., 2010).",
      "startOffset" : 81,
      "endOffset" : 108
    }, {
      "referenceID" : 12,
      "context" : "Guo and Diab, 2010) and cross-lingual annotation projection applied to bootstrapping parsers (Hwa et al., 2005; Smith and Smith, 2007).",
      "startOffset" : 93,
      "endOffset" : 134
    }, {
      "referenceID" : 18,
      "context" : "Multilingual guidance has also been used for training Part-ofSpeech (POS) taggers (Snyder et al., 2008; Snyder et al., 2009; Das and Petrov, 2011).",
      "startOffset" : 82,
      "endOffset" : 146
    }, {
      "referenceID" : 19,
      "context" : "Multilingual guidance has also been used for training Part-ofSpeech (POS) taggers (Snyder et al., 2008; Snyder et al., 2009; Das and Petrov, 2011).",
      "startOffset" : 82,
      "endOffset" : 146
    } ],
    "year" : 2014,
    "abstractText" : "Named Entities (NEs) are often written with no orthographic changes across different languages that share a common alphabet. We show that this can be leveraged so as to improve named entity recognition (NER) by using unsupervised word clusters from secondary languages as features in state-of-the-art discriminative NER systems. We observe significant increases in performance, finding that person and location identification is particularly improved, and that phylogenetically close languages provide more valuable features than more distant languages.",
    "creator" : "LaTeX with hyperref package"
  }
}