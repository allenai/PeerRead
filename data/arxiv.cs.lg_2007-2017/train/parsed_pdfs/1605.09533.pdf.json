{
  "name" : "1605.09533.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Robust Deep-Learning-Based Road-Prediction for Augmented Reality Navigation Systems",
    "authors" : [ "Matthias Limmer", "Julian Forster", "Dennis Baudach", "Florian Schüle", "Roland Schweiger", "Hendrik P.A. Lensch" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "I. Introduction Augmented reality navigation applications that support drivers navigating in unknown environments are one example of future advanced driver assistance systems (ADAS). Although this ADAS function is aimed mostly at urban navigation, where the difficulty lies in navigating in a complex road network, another use case is inter-urban navigation, especially for poor visibility conditions (e.g., fog, snow, night, . . . ). Regular navigation applications leverage a map database and a GPS sensor for coarse localization. Augmented reality applications, however, not only require a precise localization but also a precise road course estimation. Accurate lateral localization and shorter distance road course estimation is particularly important for realistic augmentations of the camera image. Common approaches exploit existing lane and road markings for this task (cf. [1], [2]). Lane and\nThis work was partially funded by the European Commission under the ECSEL Joint Undertaking in the scope of the DESERVE project. http://www.deserve-project.eu/\n1 M. Limmer, J. Forster, D. Baudach and R. Schweiger are with Daimler AG R&D, Ulm, Germany\n2 F. Schüle is with the Institute of Measurement, Control and Microtechnology, University of Ulm, Germany\n3 H. Lensch is with the Department of Computer Graphics, Eberhard Karls Universität, Tübingen, Germany\n*These authors contributed equally to this work\nroad markings, though, might not be usable or available for all inter-urban roads because of damage, soiling or simple absence.\nThe image-based road detection approach presented in this paper classifies each pixel with a deep multiscale convolutional neural network (CNN). The CNN learns feature extractors to identify road pixels in an integrated fashion. It is therefore capable of reliably classifying road pixels disregarding the presence of lane markings. Classified road pixels are homogenized by a floodfill algorithm to create a coherent road segment. A road contour is extracted from that segment and fitted into a spline-based road model, the optical map. This optical map is fused with processed data from a map database, the digital map, and a grid map from a radar sensor to conduct a precise localization and road course estimation. This is used in the augmented reality navigation system depicted in Fig. 1.\nThe approach is trained on a large number of fullscene-labeled near infrared (NIR) images showing nighttime road scenes including adverse weather conditions. Localization and road prediction results are evaluated in extensive experiments against ground truth trajectories measured by a high precision inertial measurement unit (IMU) and differential GPS (D-GPS). Evaluation results show state-of-the-art performance compared to a baseline approach [3], but no failures when lane markings are not available. This increases the robustness and availability of the application.\nar X\niv :1\n60 5.\n09 53\n3v 1\n[ cs\n.C V\n] 3\n1 M\nay 2\n01 6"
    }, {
      "heading" : "II. Related Work",
      "text" : "Using digital maps as a source for road course estimation at longer distances requires accurate localization. The precision of common GPS sensors of up to 10m for localization satisfies the needs of regular navigation systems but not those of a precise road course estimation, especially for augmented reality navigation [1]. To achieve a higher precision for longitudinal and lateral localization, road course estimation applications fuse multiple sensors with longer and shorter perception ranges. An exhaustive overview of different sensor fusion approaches is collated in [4]. In the following, a few approaches are introduced that are closer related to the scope of this paper.\nTsogas et al. [5] fuse measurements of a camera, laser scanner and a digital map based on the clothoid road model. A Sugeno-fuzzy system determines appropriate weightings for each of the different sensors dependent on the prediction distance from the ego-vehicle and the range of the sensor. The clothoid model, though, is only able to model cubic road curvatures. Complex curvatures, which commonly reside in arbitrary rural roads, can only be represented by joining several clothoids together. This, however, would increase the parameter space considerably and is not modeled by the aforementioned approach.\nThe sensor fusion system of Deusch et al. [2] is not dependent on the clothoid model and the digital map. It belongs to the category of systems that record a custom map containing landmarks and sensor data that can be used to localize the car later on. Coordinates from a DGPS sensor are mapped to landmarks extracted from forward and backward looking cameras and the occupancy grid of a laser scanner. In a recall phase, the regular GPSposition is refined by matching concurrently extracted landmarks to those in the database. The creation of a landmark database, though, is a procedure that needs to be completed in advance. Moreover, maintenance of the database has to be performed on a regular basis to remove landmark errors because of construction works, etc.\nSchüle et. al. [1] describe a framework that fuses a NIR camera sensor, a radar sensor and a digital map. It performs longitudinal localization by fusing a radar grid map and a digital map using a particle filter. Precise lateral localization is then accomplished by fusing the longitudinally mapped digital map with an optical lane recognition algorithm [3] in the camera image. In subsequent works [6], a Bayesian fusion system that performs the final road course estimation is introduced. In both systems, the road course model is not a clothoid but rather lists of connected 2D points sampling the right and left borders of the lane. This approach, as well as all aforementioned approaches, relies on lane marking detectors for the estimation of an optical map. To increase the robustness and availability of such a system, an optical road course recognition is desired that\nworks independent of lane or road markings. Seo et al. [7] describe a road boundary estimator based on intensity distribution thresholding from camera images. The thresholded intensity distribution is extracted from a region of interest (ROI) on the inverse perspective mapped camera image. Extracted road boundaries are tracked over time by a Bayes filter. Although the thresholding method is a simple and efficient approach for detecting road pixels, it might fail for roads with a high illumination variance (e.g. containing sharp shadows).\nFernández et al. [8] perform road detection by training decision trees. They use the disparity features of a stereo camera for a ground plane detection and several handcrafted color and texture features to classify superpixels segmented by a watershed transform. This approach, though, strongly relies on features not available for grayscale monocular camera images.\nAlvarez et al. [9] describe a road scene segmentation from single images using a convolutional neural network. The CNN is trained on publicly available annotated road scenes that are not necessarily images from the camera used in the application. To overcome this and allow adapting to immediate situations, the CNN classification is fused with the color intensity distribution from an ROI ahead of the vehicle through a Bayesian framework. Recent developments of CNN classifiers, though, show that such a fusion step is not necessary if a network is pretrained on a large amount of data from a similar sensor and only adjusted to the current sensor by providing a smaller set of training data (c.v. [10], [11]). Apart from that, this approach is not directly suitable for a road course extraction. Other road users that possibly occlude parts of the road are not explicitly classified, what complicates road border extractions in these cases.\nThe framework proposed in this paper is based on [1], but replaces the optical lane detection module from [3] with a road segmentation module based on deep multiscale CNNs. It is trained on a dataset of night-time images with a large variety of road and weather situations with and without lane markings. This approach therefore increases the robustness and availability of shorter distance road course estimations to situations without lane markings or adverse weather situations.\nThe remainder of this paper is structured as follows: Section III describes the framework, while Section IV presents the road detection module in more detail. Extensive experiments are described and discussed in Section V. Section VI summarizes the results."
    }, {
      "heading" : "III. Framework",
      "text" : "To compute a reliable localization and road course estimation, a framework that fuses different sensor inputs is needed. This paper leverages a derivation of the framework from [1] and is depicted in Fig. 2. The modules of the framework are as follows: First, radar data in combination with a tracked ego-motion estimation produces a grid map. Second, initialized by the GPS\nposition, the rough location in a commercially available map database is determined and map parameters for that location are transformed into a compliant digital map model. Third, the grid map and the digital map are fused to produce a longitudinally matched digital map. The fourth module performs road detection in a corresponding camera image and produces an optical map."
    }, {
      "heading" : "A. Grid Mapping",
      "text" : "A grid map is a 2D map representing the local environment quantized into equally sized cells representing occupancy (see Fig. 3). Each cell temporally integrates respective sensor measurements from a distance measuring sensor and thereby reduces the inherent noise and uncertainties of singular measurements. In the proposed framework, data from an imaging automotive radar, which returns both, the distances of reflections and their velocities, is stored in the grid map. Since the egovehicle is moving, its relative position on the grid map needs to be determined by estimating the ego-motion. An extended Kalman filter with a CTRV-model (constant turn rate and velocity [12]) leverages the wheel speeds and yaw rate measurements to accomplish an ego-motion estimation. The ego-motion estimation is then used to determine the correct cells where static radar objects are stored and integrated over time."
    }, {
      "heading" : "B. Digital Mapping",
      "text" : "A commercial map database commonly stores its information in annotated discrete shape points using the UTM (Universal Transverse Mercator) coordinate system. The amount of points per road, the accuracy of such points and the meta-information per point varies greatly, since major roads are better sampled and maintained by database providers. To obtain a continuous local digital road model, shape points around the current egovehicle’s location are interpolated by a cubic hermite spline. This creates the digital map, which serves as the base for the following fusion modules."
    }, {
      "heading" : "C. Map Matching",
      "text" : "To estimate the orientation and longitudinal position of the ego-vehicle on the digital map, the grid map is fitted into the digital map using a particle filter. Each\nparticle of the filter represents the position and orientation of the vehicle and is weighted by how well the digital map and the grid map fit using various features [13]. The sampling of the particles is initialized by the previous position or the GPS position if no previous position is available."
    }, {
      "heading" : "D. Road Detection",
      "text" : "The original Optical Lane Detection module [3] in the framework of [1] is replaced with the lane-independent Road Detection module proposed in this paper. In this processing step, pixels in a camera image belonging to the currently traveled road are identified. These detected pixels are used to determine the road boundaries which are then transformed into and tracked by the optical map. Further details of this processing step are described in Section IV."
    }, {
      "heading" : "E. Lane Course Fusion",
      "text" : "To increase the precision of the lateral localization, the optical map is fused with the digital map. Therefore, lateral coordinates in the ego-vehicle’s coordinate system are sampled from both maps along the longitudinal trajectories of lane or road borders. Corresponding lateral positions are linearly interpolated by weighting each sensor according to its reliability for different distances from the ego vehicle. The optical map is very reliable for close distances while the digital map is more reliable for larger distances. The specific weighting scheme is described in [1]."
    }, {
      "heading" : "IV. Road Detection",
      "text" : "The road detection module described in the following identifies the currently traveled road in a camera image by performing a pixel classification using deep learning techniques. It then extracts and tracks the left and right road border taking into account uncertainties and border-occlusions by other road users. It then computes the optical map that can be fused with the digital map."
    }, {
      "heading" : "A. Scene Labeling",
      "text" : "The scene labeling module proposed in this paper is a deep multi-scale CNN. It combines the approach of [14] with the multi-scale scheme of [15]. [14] introduces network topologies characterized by many convolution layers with small convolution kernels and comparatively\nfew pooling layers. Many convolution layers increase the amount of non-linearities and thus the capability of the network to learn complex classification functions. If small convolution kernel sizes are used, the increase of convolution layers does not necessarily lead to a drastic increase of computational complexity. Multiple scales further improve the scale-invariance of the network without increasing the depth of the network. Since realtime performance is needed for augmented reality applications, techniques from [16] and [17] are implemented for a computational efficient application of a CNN to entire images.\nMulti-scale neural networks process multiple scales of the same input data concurrently. An image pyramid of nl levels is constructed by reducing the image resolution by 0.5 in both dimensions for each new level. Each pyramid level is normalized to zero-mean unit-variance in a local neighborhood, which enhances the texture and equalizes bright and dark areas in the image. The normalized image pyramid levels are then fed to their respective branches of the neural network. All branches of the network are constructed with the same structure and are finally joined in a fully-connected layer that also serves as the output layer of the network. A diagram of possible network topologies of the above defined multiscale CNN is depicted in Fig. 4.\nThough each branch is structurally identical, no weights are shared between the branches. A branch overall consists of alternating np pooling layers and nb = np + 1 convolution layer blocks. Every convolution layer block consists of nc convolution layers that use the ReLU function: ReLU(x) = max(0, x) as activation function. The size of the filter bank nf is identical within each convolution layer block and is doubled after each pooling layer. The kernel size of the convolution kernels kc is the same in all convolution layers.\nIn a patch-based application of the proposed network, correctly sized image patches need to be extracted from the normalized image pyramid levels prior to feeding them to the branches. Applying the CNN efficiently to complete images while retaining the full image resolution requires slight changes in various layers and the introduction of several helper layers into the network (see [16], [17]). These changes are explained in the following.\n1) Overlapping Pooling: In an image-based application, pooling layers, which are normally strided ( ∏\nstridekp > 1) according to their kernel size kp, need to be applied in an overlapping fashion ( ∏ stridekp = 1), so that no resolution is lost. 2) Fragmentation: Fragmentation layers need to be inserted after each pooling layer. They split the oversized feature maps of the preceding layer into ∏ stridekp feature maps of reduced resolution, which are processed individually afterwards. This ensures that the subsampling property of the pooling functions is preserved without loosing resolution. Fig. 5 depicts a 2× 2 fragmentation. 3) Defragmentation: A defragmentation layer is needed after the last convolution block before all branches are joined. It reverts all performed fragmentations and transforms the fragmented feature map arrays into one cohesive feature map array that has the same1 resolution as the corresponding input pyramid level. 4) Upscaling: After defragmentation, the feature map arrays of lower pyramid levels need to be sampled up and eventually cropped so that they match the resolution of the lowest pyramid level. In this manner, the feature maps of all scales can be concatenated and used as an input to the fully-connected layer. 5) Convolutional Fully-Connected Layer: The fullyconnected layer is applied in a convolutional fashion to emulate the patch-based functionality. This transforms fully-connected layers into 1× 1 convolution layers with as many input channels as incoming feature maps."
    }, {
      "heading" : "B. Road Segmentation",
      "text" : "A CNN, such as outlined above, generates a class membership map, in which every pixel is assigned to 1Valid convolutions might crop some border pixels.\none of the trained classes. The class membership map of the preceding step needs to be segmented such that a cohesive road segment can be extracted. Fig. 6 displays a road segmentation generated by the following steps.\nAssuming that the biggest connected group of classified road pixels approximates the actual connected group of road pixels, detached road pixel clusters can be neglected. Holes in the connected group of pixels are then filled leveraging a flood-fill algorithm. The algorithm is seeded at the bottom of the image, since that is supposed to be part of the road in most of the cases.\nA contour is extracted from the segmented road pixels by using the snake algorithm of [18]. The left and right road border is then determined by splitting the contour in half at the highest central contour point. The road contours are ignored at all border pixels of the camera image. Contour pixels that are adjacent to pixels classified as other road-users, such as vehicle pixels, are ignored as well, since road users might conceal parts of the correct road border. Finally, the remaining contour pixels are transformed into the digital map’s coordinate system and stored for tracking in the following frames."
    }, {
      "heading" : "C. Road Border Shaping",
      "text" : "To compute the optical map needed in the following fusion step, the tracked road border estimates need to be fitted into a conclusive road model. All estimates are longitudinally binned, with each bin representing a road border shape point. The values of each bin are analyzed to compute a reliability measure of that particular road border shape point. The bin medians are used as the shape points for fitting a spline, while the interquartile range determines if a shape point is used in the spline computation. Exploiting meta-information contained in the digital map, other track splines, like lane borders, can be interpolated from the left and the right border splines. If preceding processing steps continuously fail to deliver usable measurements for one road border (e.g., in sharp curves) that road border can be extrapolated by the other road border and the other track splines. Fig. 7 displays a spline fitting for unreliable measurements."
    }, {
      "heading" : "V. Experiments",
      "text" : "The performed experiments are twofold. First, various network topologies were redundantly trained and eval-\nuated with respect to their classification performance (Section V-B). The dataset for training and evaluating the classifiers consists of 7095 full scene labeled images from an NIR camera of rural road sequences at night containing a large variety of weather situations, seasons and landscapes. Second, the optical map of the best performing classifiers were compared to the standard optical map from [3] using the fusion framework from [1] (Section V-C). The evaluation is performed on five nighttime sequences resulting in 13.5 km of driven distance with a ground-truth trajectory taken from a D-GPS sensor and a high accurate IMU. Fig. 8 shows examples images of these sequences."
    }, {
      "heading" : "A. Training of the CNN",
      "text" : "Only certain combinations of parameters mentioned in section IV-A are used in the experiments. The influence of the number of pyramid levels (nl), the deepness of the network while retaining the input patch size (nc, kc) and the initial number of filters of the first convolution block (nf ) were evaluated. The topologies are therefore denoted as topo-nl-nc-nf , with a parameter range of nl ∈ [1..5], (nc, kc) ∈ {(1, 7), (3, 3)} and nf ∈ {16, 32}. Parameters nc and kc have to be chosen such that the input patch size stays the same, which holds for the above defined tuples. The kernel size of the max pooling layers is fixed at 2×2 pixels for all pooling layers in all topology variants. Topology topo-4-1-32, for example, has the following parameters: nl = 4, (nc, kc) = (1, 7), nf = 32.\nAll scene-labeled images are split into a set of 6895 images for training and a set of 200 images for evaluation. To train the topologies, multinomial logistic regression performs a stochastic gradient descent leveraging the backpropagation algorithm [19] with linear learn rate annealing. The target classes consist of the default class background and specific classes: road, vehicle, sky, vru (vulnerable road users) and infrastructure. Training examples are sampled patch-wise and class-balanced for an equal but random distribution of examples per class. The trainable parameters of the networks are initialized by random-sampling a Gaussian distribution. Learn rates for each topology are empirically determined by choosing the best performing learn rate in various mini-trainings. With the selected learn rate full train-\nings are performed. After completion, the biases of the fully connected layers are adjusted such that the multiclass extension of the Matthews Correlation Coefficient (MCC) [20] is optimized. All trainings were conducted with cuda-convnet [21]."
    }, {
      "heading" : "B. Scene Labeling Results",
      "text" : "Table I shows the classifier performances with regard to several measures. These measures are the MCC [20], the overall accuracy (ACC), the intersection over union (IU) as an average over all classes (IUglobal) and specifically for the road (IUroad) and vehicle class (IUveh). The IU measure is defined as:\nIU = TPTP ∪ FP ∪ FN (1)\nwhere TP (true positives) is the amount of correctly classified pixels and FP ∪ FN (false positives and false negatives) the amount of wrongly classified pixels regarding one specific class. To ensure that equal topologies perform similarly, each topology is trained three times. The table shows the average result for one topology of the individually evaluated classifiers. Topology topo-1-1-16 is comparable to the best performing topology of [9]. According to Table I, this topology achieves the lowest performance. Other topologies are therefore encouraged for road segmentation tasks.\nTopologies topo-[3..5]-3-32 perform best regarding most of the measures. This implies that an increase of pyramid levels after level 3 has almost no effect to the best performing topology variant. Fig. 9 shows this effect in a graphical display of the MCC performances dependent on the pyramid levels."
    }, {
      "heading" : "C. Lane Course Prediction Results",
      "text" : "In the following, the fusion system performance is evaluated using the best performing classifier for each topology variant in relation to the optical lane recognition from [3] and the system without the optical map. The performance measure is taken from [1]. It compares the deviations of the road course estimations from the ground truth trajectory for different distances to the ego-vehicle. Since [1] have shown that their fusion algorithm benefits primarily short range estimations, only the average performances of the five sequences for short range estimations (0-30m) are displayed in Table II. The final row displays the failure rates of the lanebased recognition (percentage of frames, where no lane markings are detected). It should be noted that the lanebased recognition measure is solely computed for frames, which contain detected lane markings. Table II displays that the CNN-based optical map approach performs slightly worse for sequences containing good lane markings (A,B), but better for sequences with bad weather (C,D) or bad lane markings (E). Considering the better performing topologies (topo-[3..5]-*-*), the CNN-based optical maps show a similar range of performance values (~ 28 cm) for sequences A-C and E . Contrary to that, the lane-based approach shows a greater variance there, ranging from 19 cm (seq. A) to 50 cm (seq. C). This implies that the CNN-based approach is performing more robustly than\nthe lane-based approach, although the peak performance of the latter might not be reached.\nSequence D is an exception regarding this robustness. Its performance ranges from 50 cm to 60 cm for the CNN-based and 89 cm for the lane-based approach. This sequence contains severe snowfall and thus, poses a big challenge to sensor processing and detection algorithms. While the CNN-based approach robustly delivers road course estimations for all frames (see Fig. 10 for an example augmented image), the failure rate of the lane marking detection exceeds 90%. This means that the lane-based approach is practically inapplicable and needs to switch to a mode without optical map. The performance of that mode, though, is always much lower than with an optical map (see first row of Table II).\nFurther, it should be noted that the best performing classifiers are not necessarily the best performing road course estimators, although a trend can be detected, when comparing topologies topo-[1,2]-*-* and topo-[3..5]-*-*."
    }, {
      "heading" : "VI. Conclusion",
      "text" : "This paper presented a deep multi-scale convolutional neural network based approach for camera-based road course prediction and localization. Various network topologies were trained that reliably detect road and vehicle pixels, from which an optical map is extracted. Deeper topologies with a higher number of filters per convolution layer perform better, while an increase of pyramid levels after level three does not increase the performance considerably. The extracted optical maps\nhave been successfully fused with a digital map to refine the lateral localization. Compared to a baseline lanebased algorithm, the approach proposed in this paper shows a slightly worse performance for optimal road and weather conditions. However, contrary to the baseline, our approach performs consistently well for various weather conditions, even if lane markings are missing. This demonstrates that state-of-the-art performance can be achieved while increasing the robustness and application scope to situations, where traditional lane marking detection is not possible."
    }, {
      "heading" : "ACKNOWLEDGMENT",
      "text" : "The authors would like to thank Markus Thom and Oliver Hartmann for their valuable support."
    } ],
    "references" : [ {
      "title" : "Augmenting night vision video images with longer distance road course information",
      "author" : [ "F. Schüle", "R. Schweiger", "K. Dietmayer" ],
      "venue" : "IEEE Intelligent Vehicles Symposium, 2013, pp. 1233–1238.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Multi-sensor self-localization based on maximally stable extremal regions",
      "author" : [ "H. Deusch", "J. Wiest", "S. Reuter", "D. Nuss", "M. Fritzsche", "K. Dietmayer" ],
      "venue" : "IEEE Intelligent Vehicles Symposium, 2014, pp. 555–560.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Robust lane recognition embedded in a real-time driver assistance system",
      "author" : [ "R. Risack", "P. Klausmann", "W. Krüger", "W. Enkelmann" ],
      "venue" : "IEEE Intelligent Vehicles Symposium, 1998, pp. 35–40.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Recent progress in road and lane detection: a survey",
      "author" : [ "A. Bar Hillel", "R. Lerner", "D. Levi", "G. Raz" ],
      "venue" : "Machine Vision and Applications, vol. 25, no. 3, pp. 727–745, 2012.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Combined lane and road attributes extraction by fusing data from digital map, laser scanner and camera",
      "author" : [ "M. Tsogas", "N. Floudas", "P. Lytrivis", "A. Amditis", "A. Polychronopoulos" ],
      "venue" : "Information Fusion, vol. 12, no. 1, pp. 28 – 36, 2011, special Issue on Intelligent Transportation Systems.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Probabilistic fusion of rural road course estimations",
      "author" : [ "F. Schüle", "C. Koch", "O. Hartmann", "R. Schweiger", "K. Dietmayer" ],
      "venue" : "Proceedings of the IEEE International Conference on Intelligent Transportation Systems, 2013, pp. 1701–1706.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Detection and tracking of boundary of unmarked roads",
      "author" : [ "Y.W. Seo", "R.R. Rajkumar" ],
      "venue" : "International Conference on Information Fusion, 2014.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "A comparative analysis of decision trees based classifiers for road detection in urban environments",
      "author" : [ "C. Fernández", "R. Izquierdo", "D.F. Llorca", "M.A. Sotelo" ],
      "venue" : "Proceedings of the IEEE International Conference on Intelligent Transportation Systems, 2015, pp. 719–724.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Road scene segmentation from a single image",
      "author" : [ "J.M. Alvarez", "T. Gevers", "Y. LeCun", "A.M. Lopez" ],
      "venue" : "Proceedings of the European Conference on Computer Vision. Springer Berlin Heidelberg, 2012, pp. 376–389.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Fully convolutional networks for semantic segmentation",
      "author" : [ "J. Long", "E. Shelhamer", "T. Darrell" ],
      "venue" : "Proceedings of the Conference on Computer Vision and Pattern Recognition, 2015.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "The cityscapes dataset for semantic urban scene understanding",
      "author" : [ "M. Cordts", "M. Omran", "S. Ramos", "T. Rehfeld", "M. Enzweiler", "R. Benenson", "U. Franke", "S. Roth", "B. Schiele" ],
      "venue" : "Proceedings of the Conference on Computer Vision and Pattern Recognition, 2016.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Comparison and evaluation of advanced motion models for vehicle tracking",
      "author" : [ "R. Schubert", "E. Richter", "G. Wanielik" ],
      "venue" : "International Conference on Information Fusion, 2008.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Global positioning using a digital map and an imaging radar sensor",
      "author" : [ "M. Szczot", "M. Serfling", "O. Löhlein", "F. Schüle", "M. Konrad", "K. Dietmayer" ],
      "venue" : "IEEE Intelligent Vehicles Symposium, 2010, pp. 406–411.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Very deep convolutional networks for large-scale image recognition",
      "author" : [ "K. Simonyan", "A. Zisserman" ],
      "venue" : "International Conference on Learning Representations, 2014, arXiv:1509.01951.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Learning hierarchical features for scene labeling",
      "author" : [ "C. Farabet", "C. Couprie", "L. Najman", "Y. LeCun" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 35, no. 8, pp. 1915–1929, 2013.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 1915
    }, {
      "title" : "Fast image scanning with deep max-pooling convolutional neural networks",
      "author" : [ "A. Giusti", "D.C. Ciresan", "J. Masci", "L.M. Gambardella", "J. Schmidhuber" ],
      "venue" : "Proceedings of the IEEE International Conference on Image Processing, 2013, pp. 4034– 4038.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "A theory for rapid exact signal scanning with deep multi-scale convolutional neural networks",
      "author" : [ "M. Thom", "F. Gritschneder" ],
      "venue" : "Tech. Rep. arXiv:1508.06904, 2016.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Algorithms for Graphics and Image Processing, 1982",
      "author" : [ "T. Pavlidis" ],
      "venue" : "ch. Contour Tracing,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 1982
    }, {
      "title" : "Gradientbased learning applied to document recognition",
      "author" : [ "Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner" ],
      "venue" : "Proceedings of the IEEE, vol. 86, no. 11, pp. 2278 –2324, 1998.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "A comparison of mcc and cen error measures in multi-class prediction",
      "author" : [ "G. Jurman", "S. Riccadonna", "C. Furlanello" ],
      "venue" : "PLoS ONE, vol. 7, no. 8, 2012.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "A. Krizhevsky", "I. Sutskever", "G.E. Hinton" ],
      "venue" : "Advances in Neural Information Processing Systems 25. Curran Associates, Inc., 2012, pp. 1097–1105.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "[1], [2]).",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 1,
      "context" : "[1], [2]).",
      "startOffset" : 5,
      "endOffset" : 8
    }, {
      "referenceID" : 2,
      "context" : "Evaluation results show state-of-the-art performance compared to a baseline approach [3], but no failures when lane markings are not available.",
      "startOffset" : 85,
      "endOffset" : 88
    }, {
      "referenceID" : 0,
      "context" : "localization satisfies the needs of regular navigation systems but not those of a precise road course estimation, especially for augmented reality navigation [1].",
      "startOffset" : 158,
      "endOffset" : 161
    }, {
      "referenceID" : 3,
      "context" : "overview of different sensor fusion approaches is collated in [4].",
      "startOffset" : 62,
      "endOffset" : 65
    }, {
      "referenceID" : 4,
      "context" : "[5] fuse measurements of a camera, laser",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 1,
      "context" : "[2] is not dependent on the clothoid model and the digital map.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 0,
      "context" : "[1] describe a framework that fuses a NIR camera sensor, a radar sensor and a digital map.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 2,
      "context" : "Precise lateral localization is then accomplished by fusing the longitudinally mapped digital map with an optical lane recognition algorithm [3] in the camera image.",
      "startOffset" : 141,
      "endOffset" : 144
    }, {
      "referenceID" : 5,
      "context" : "subsequent works [6], a Bayesian fusion system that performs the final road course estimation is introduced.",
      "startOffset" : 17,
      "endOffset" : 20
    }, {
      "referenceID" : 6,
      "context" : "[7] describe a road boundary estimator based on intensity distribution thresholding from camera images.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 7,
      "context" : "[8] perform road detection by training decision trees.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 8,
      "context" : "[9] describe a road scene segmentation from single images using a convolutional neural network.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 9,
      "context" : "[10], [11]).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 10,
      "context" : "[10], [11]).",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 0,
      "context" : "The framework proposed in this paper is based on [1], but replaces the optical lane detection module from [3] with a road segmentation module based on deep multiscale CNNs.",
      "startOffset" : 49,
      "endOffset" : 52
    }, {
      "referenceID" : 2,
      "context" : "The framework proposed in this paper is based on [1], but replaces the optical lane detection module from [3] with a road segmentation module based on deep multiscale CNNs.",
      "startOffset" : 106,
      "endOffset" : 109
    }, {
      "referenceID" : 0,
      "context" : "This paper leverages a derivation of the framework from [1] and is depicted in Fig.",
      "startOffset" : 56,
      "endOffset" : 59
    }, {
      "referenceID" : 11,
      "context" : "An extended Kalman filter with a CTRV-model (constant turn rate and velocity [12]) leverages the wheel speeds and yaw rate measurements to accomplish an ego-motion estimation.",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 12,
      "context" : "particle of the filter represents the position and orientation of the vehicle and is weighted by how well the digital map and the grid map fit using various features [13].",
      "startOffset" : 166,
      "endOffset" : 170
    }, {
      "referenceID" : 2,
      "context" : "The original Optical Lane Detection module [3] in the",
      "startOffset" : 43,
      "endOffset" : 46
    }, {
      "referenceID" : 0,
      "context" : "framework of [1] is replaced with the lane-independent Road Detection module proposed in this paper.",
      "startOffset" : 13,
      "endOffset" : 16
    }, {
      "referenceID" : 0,
      "context" : "The specific weighting scheme is described in [1].",
      "startOffset" : 46,
      "endOffset" : 49
    }, {
      "referenceID" : 13,
      "context" : "It combines the approach of [14] with the multi-scale scheme of [15].",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 14,
      "context" : "It combines the approach of [14] with the multi-scale scheme of [15].",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 13,
      "context" : "[14] introduces network topologies characterized by many convolution layers with small convolution kernels and comparatively",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 15,
      "context" : "Since realtime performance is needed for augmented reality applications, techniques from [16] and [17] are implemented",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 16,
      "context" : "Since realtime performance is needed for augmented reality applications, techniques from [16] and [17] are implemented",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 15,
      "context" : "Applying the CNN efficiently to complete images while retaining the full image resolution requires slight changes in various layers and the introduction of several helper layers into the network (see [16], [17]).",
      "startOffset" : 200,
      "endOffset" : 204
    }, {
      "referenceID" : 16,
      "context" : "Applying the CNN efficiently to complete images while retaining the full image resolution requires slight changes in various layers and the introduction of several helper layers into the network (see [16], [17]).",
      "startOffset" : 206,
      "endOffset" : 210
    }, {
      "referenceID" : 17,
      "context" : "by using the snake algorithm of [18].",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 2,
      "context" : "performing classifiers were compared to the standard optical map from [3] using the fusion framework from [1] (Section V-C).",
      "startOffset" : 70,
      "endOffset" : 73
    }, {
      "referenceID" : 0,
      "context" : "performing classifiers were compared to the standard optical map from [3] using the fusion framework from [1] (Section V-C).",
      "startOffset" : 106,
      "endOffset" : 109
    }, {
      "referenceID" : 18,
      "context" : "To train the topologies, multinomial logistic regression performs a stochastic gradient descent leveraging the backpropagation algorithm [19] with linear learn rate annealing.",
      "startOffset" : 137,
      "endOffset" : 141
    }, {
      "referenceID" : 19,
      "context" : "fully connected layers are adjusted such that the multiclass extension of the Matthews Correlation Coefficient (MCC) [20] is optimized.",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 20,
      "context" : "All trainings were conducted with cuda-convnet [21].",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 19,
      "context" : "These measures are the MCC [20], the overall accuracy (ACC), the intersection over union (IU) as an average over all classes (IUglobal) and specifically for the road (IUroad) and vehicle class (IUveh).",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 8,
      "context" : "Topology topo-1-1-16 is comparable to the best performing topology of [9].",
      "startOffset" : 70,
      "endOffset" : 73
    }, {
      "referenceID" : 2,
      "context" : "In the following, the fusion system performance is evaluated using the best performing classifier for each topology variant in relation to the optical lane recognition from [3] and the system without the optical map.",
      "startOffset" : 173,
      "endOffset" : 176
    }, {
      "referenceID" : 0,
      "context" : "The performance measure is taken from [1].",
      "startOffset" : 38,
      "endOffset" : 41
    }, {
      "referenceID" : 0,
      "context" : "Since [1] have shown that their fusion algorithm benefits primarily short range estimations, only",
      "startOffset" : 6,
      "endOffset" : 9
    }, {
      "referenceID" : 2,
      "context" : "Lane based estimation [3] performs better on scenes where the lane is clearly visible (A, B) but has a significant failure in all other conditions (C-E).",
      "startOffset" : 22,
      "endOffset" : 25
    }, {
      "referenceID" : 2,
      "context" : "lane-based [3] 0.",
      "startOffset" : 11,
      "endOffset" : 14
    }, {
      "referenceID" : 0,
      "context" : "Further, it should be noted that the best performing classifiers are not necessarily the best performing road course estimators, although a trend can be detected, when comparing topologies topo-[1,2]-*-* and",
      "startOffset" : 194,
      "endOffset" : 199
    }, {
      "referenceID" : 1,
      "context" : "Further, it should be noted that the best performing classifiers are not necessarily the best performing road course estimators, although a trend can be detected, when comparing topologies topo-[1,2]-*-* and",
      "startOffset" : 194,
      "endOffset" : 199
    } ],
    "year" : 2016,
    "abstractText" : "This paper proposes an approach that predicts the road course from camera sensors leveraging deep learning techniques. Road pixels are identified by training a multi-scale convolutional neural network on a large number of full-scene-labeled nighttime road images including adverse weather conditions. A framework is presented that applies the proposed approach to longer distance road course estimation, which is the basis for an augmented reality navigation application. In this framework long range sensor data (radar) and data from a map database are fused with short range sensor data (camera) to produce a precise longitudinal and lateral localization and road course estimation. The proposed approach reliably detects roads with and without lane markings and thus increases the robustness and availability of road course estimations and augmented reality navigation. Evaluations on an extensive set of high precision ground truth data taken from a differential GPS and an inertial measurement unit show that the proposed approach reaches state-of-the-art performance without the limitation of requiring existing lane markings.",
    "creator" : "LaTeX with hyperref package"
  }
}