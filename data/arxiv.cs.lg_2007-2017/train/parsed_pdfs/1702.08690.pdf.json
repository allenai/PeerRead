{
  "name" : "1702.08690.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Borrowing Treasures from the Wealthy: Deep Transfer Learning through Selective Joint Fine-tuning*",
    "authors" : [ "Weifeng Ge", "Yizhou Yu" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Experiments demonstrate that our selective joint finetuning scheme achieves state-of-the-art performance on multiple visual classification tasks with insufficient training data for deep learning. Such tasks include Caltech 256, MIT Indoor 67, Oxford Flowers 102 and Stanford Dogs 120. In comparison to fine-tuning without a source domain, the proposed method can improve the classification accuracy by 2% - 10% using a single model."
    }, {
      "heading" : "1. Introduction",
      "text" : "Convolutional neural networks (CNNs) have become deeper and larger to pursue increasingly better performance on classification and recognition tasks [25, 21, 44, 16, 15]. Looking at the successes of deep learning in computer vison, we find that a large amount of training or pretraining data is essential in training deep neural networks. Large-scale image datasets, such as the ImageNet ILSVRC\n*To appear in 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2017).\ndataset [40], Places [53], and MS COCO [27], have led to a series of breakthroughs in visual recognition, including image classification [28], object detection [12], and semantic segmentation [29]. Many other related visual tasks have benefited from these breakthroughs.\nNonetheless, researchers face a dilemma when using deep convolutional neural networks to perform visual tasks that do not have sufficient training data. Training a deep network with insufficient data might even give rise to inferior performance in comparison to traditional classifiers fed with handcrafted features. Fine-grained classification problems, such as Oxford Flowers 102 [33] and Stanford Dogs 120 [22], are such examples. The number of training samples in these datasets is far from being enough for training large-scale deep neural networks because a large number of parameters need to be learnt and the networks would become overfit quickly.\nSolving the overfitting problem for deep convolutional neural networks on learning tasks without sufficient training data is very challenging [43]. Transfer learning techniques that apply knowledge learnt from one task to other related tasks have been proven helpful [34]. In the context of deep learning, fine-tuning a deep network pre-trained on the ImageNet or Places dataset is a common strategy to learn taskspecific deep features [19, 51, 17]. This strategy is considered a simple transfer learning technique for deep learning. However, since the ratio between the number of learnable parameters and the number of training samples still remains the same, fine-tuning needs to be terminated after a relatively small number of iterations; otherwise, overfitting is still going to occur.\nIn this paper, we attempt to tackle the problem of training deep neural networks for learning tasks that have insufficient training data. We adopt the source-target joint training methodology [49] when fine-tuning deep neural networks. The original learning task without sufficient training data is called the target learning task, T t. To boost its performance, the target learning task is teamed up with another learning task with rich training data. The latter is called the source learning task, T s. Suppose the source learning task has a large-scale training set Ds, and the target learn-\nar X\niv :1\n70 2.\n08 69\n0v 1\n[ cs\n.C V\n] 2\n8 Fe\nb 20\n17\ning task has a small-scale training set Dt. Since the target learning task is likely a specialized task, we envisage the image signals in its dataset possess certain unique low-level characteristics (e.g. fur textures in Stanford Dogs 120 [22]), and the learned kernels in the convolutional layers of a deep network need to grasp such characteristics in order to generate highly discriminative features. Thus supplying sufficient training images with similar low-level characteristics becomes the most important mission of the source learning task. Our core idea is to identify a subset of training images from Ds whose low-level characteristics are similar to those from Dt, and then jointly fine-tune a shared set of convolutional layers for both source and target learning tasks. The source learning task is fine-tuned using the selected training images only. Hence, this process is called selective joint fine-tuning. The rationale behind this is that the unique low-level characteristics of the images from Dt might be overwhelmed if all images from Ds were taken as training samples for the source learning task.\nHow do we select images from Ds that share similar low-level characteristics as those from Dt? Since kernels followed with nonlinear activation in a deep convolutional neural network (CNN) are actually nonlinear spatial filters, to find sufficient data for training high-quality kernels, we use the responses from existing linear or nonlinear filter banks to define similarity in low-level characteristics. Gabor filters [32] form an example of a linear filter bank, and the complete set of kernels from certain layers of a pretrained CNN form an example of a nonlinear filter bank. We use histograms of filter bank responses as image descriptors to search for images with similar low-level characteristics.\nThe motivation behind selecting images according to their low-level characteristics is two fold. First, low-level characteristics are extracted by kernels in the lower convolutional layers of a deep network. These lower convolutional layers form the foundation of an entire network, and the quality of features extracted by these layers determines the quality of features at higher levels of the deep network. Sufficient training images sharing similar low-level characteristics could strength the kernels in these layers. Second, images with similar low-level characteristics could have very different high-level semantic contents. Therefore, searching for images using low-level characteristics has less restrictions and can return much more training images than using high-level semantic contents.\nThe above source-target selective joint fine-tuning scheme is expected to benefit the target learning task in two different ways. First, since convolutional layers are shared between the two learning tasks, the selected training samples for the source learning task prevent the deep network from overfitting quickly. Second, since the selected training samples for the source learning task share similar lowlevel characteristics as those from the target learning task,\nkernels in their shared convolutional layers can be trained more robustly to generate highly discriminative features for the target learning task.\nThe proposed source-target selective joint fine-tuning scheme is easy to implement. Experimental results demonstrate state-of-the-art performance on multiple visual classification tasks with much less training samples than what is required by recent deep learning architectures. These visual classification tasks include fine-grained classification on Stanford Dogs 120 [22] and Oxford Flowers 102 [33], image classification on Caltech 256 [14], and scene classification on MIT Indoor 67 [37].\nIn summary, this paper has the following contributions:\n• We introduce a source-target selective joint fine-tuning scheme for improving the performance of deep learning tasks with insufficient training data.\n• We develop a novel pipeline for implementing this selective joint fine-tuning scheme. Specifically, we compute descriptors from linear or nonlinear filter bank responses on training images from both tasks, and use such descriptors to search for a desired subset of training samples for the source learning task.\n• Experiments demonstrate that our selective joint finetuning scheme achieves state-of-the-art performance on multiple visual classification tasks with insufficient training data for deep learning."
    }, {
      "heading" : "2. Related Work",
      "text" : "Multi-Task Learning. Multi-task learning (MTL) learns shared feature representations or classifiers for related tasks [4]. In comparison to learning individual tasks independently, features and classifiers learned with MTL often have better generalization capability. The focus of [10] is on learning a shared feature representation that generalizes well on related tasks. Multiple tasks were learned in a joint model in [5] by explicitly optimizing shared parameters and task-specific parameters. In deep learning, faster RCNN [38] jointly learns object locations and labels using shared convolutional layers but different loss functions for these two tasks. In [9], the same multi-scale convolutional architecture was used to predict depth, surface normals and semantic labels. This indicates that convolutional neural networks can be adapted to different tasks easily. While previous work [10, 38] attempts to find a shared feature space that benefits multiple learning tasks, the proposed joint training scheme in this paper focuses on learning a shared feature space that improves the performance of the target learning task only. Feature Extraction and Fine-tuning. Off-the-shelf CNN features [41, 7] have been proven to be powerful in various\ncomputer vision problems. Pre-training convolutional neural networks on ImageNet [40] or Places [53] has been the standard practice for other vision problems. However, features learnt in pre-trained models are not tailored for the target learning task. Fine-tuning pre-trained models [12] has become a commonly used method to learn task-specific features. The transfer ability of different convolutional layers in CNNs has been investigated in [51]. However, for tasks that do not have sufficient training data, overfitting occurs quickly. Li et al. [26] proposed to preserve the output for old learning tasks while learning a new task. Their method prevents the network from overfitting quickly on the new task while keeping its discriminative power on the old tasks. The proposed MTL pipeline in this paper not only alleviates overfitting, but also tries to find a more discriminative feature space for the target learning task.\nTransfer Learning. Different from MTL, transfer learning (or domain adaptation) [34] applies knowledge learnt in one domain to other related tasks. Much work has been done to transfer knowledge between different domains using supervised or unsupervised learning [2, 13, 45]. Domain adaptation algorithms can be divided into three categories, including instance adaption [18, 1], feature adaption [30, 45], and model adaption [8]. Hong et al. [?] transferred rich semantic information from source categories to target categories via the attention model. Tzeng et al. [45] performed feature adaptation using a shared convolutional neural network by transferring the class relationship in the source domain to the target domain. To make our pipeline more flexible, this paper does not assume the source and target label spaces\nare the same as in [45]. Different from the work in [1] which randomly resamples training classes or images in the source domain, this paper conducts a special type of transfer learning by selecting source training samples that are nearest neighbors of samples in the target domain in the space of certain low-level image descriptor.\nKrause et al. [24] directly performed Google image search using keywords associated with categories from the target domain, and download a noisy collection of images to form a training set. Then they apply the classifiers learnt on this training set to images in the target domain. In our method, we search for nearest neighbors in a large-scale labeled dataset using low-level features instead of high-level semantic information. It has been shown in [31] that lowlevel features computed in the bottom layers of a CNN encode very rich information, which can completely reconstruct the original image. Our experimental results show that nearest neighbor search using low-level features can outperform that using high-level semantic information as in [24]."
    }, {
      "heading" : "3. Selective Joint Fine-tuning",
      "text" : ""
    }, {
      "heading" : "3.1. Overview",
      "text" : "Fig. 1 shows the overall pipeline for our proposed source-target selective joint fine-tuning scheme. Given a target learning task T t that has insufficient training data, we perform selective joint fine-tuning as follows. The entire training dataset associated with the target learning task is called the target domain. The source domain is defined\nsimilarly.\nSource Domain : The minimum requirement is that the number of images in the source domain, Ds ={( xsi , y s i )}ns i=1\n, should be large enough to train a deep convolutional neural network from scratch. Ideally, these training images should present diversified low-level characteristics. That is, running a filter bank on them give rise to as diversified responses as possible. There exist a few large-scale visual recognition datasets that can serve as the source domain, including ImageNet ILSVRC dataset [40], Places [53], and MS COCO [27].\nSource Domain Training Images : In our selective joint fine-tuning, we do not use all images in the source domain as training images. Instead, for each image from the target domain, we search a certain number of images with similar low-level characteristics from the source domain. Only images returned from these searches are used as training images for the source learning task in selective joint finetuning. We apply a filter bank to all images in both source domain and target domain. Histograms of filter bank responses are used as image descriptors during search. We associate an adaptive number of source domain images with each target domain image. Hard training samples in the target domain might be associated with a larger number of source domain images. Two filter banks are used in our experiments. One is the Gabor filter bank, and the other consists of kernels in the convolutional layers of AlexNet pre-trained on ImageNet [25].\nCNN Architecture : Almost any existing deep convolutional neural network, such as AlexNet [25], GoogleNet [44], VggNet [21], and ResidualNet [15], can be used in our selective joint fine-tuning. We use the 152-layer residual network with identity mappings [16] as the CNN architecture in our experiments. The entire residual network is shared by the source and target learning tasks. An extra output layer is added on top of the residual network for each of the two learning tasks. This output layer is not shared because the two learning tasks may not share the same label space. The residual network is pre-trained either on ImageNet or Places.\nSource-Target Joint Fine-tuning : Each task uses its own cost function during selective joint fine-tuning, and every training image only contributes to the cost function corresponding to the domain it comes from. The source domain images selected by the aforementioned searches are used as training images for the source learning task only while the entire target domain is used as the training set for the target learning task only. Since the residual network (with all its convolutional layers) is shared by these two learning tasks, it is fine-tuned by both training sets. And the output layers on top of the residual network are fine-tuned\nby its corresponding training set only. Thus we conduct end-to-end joint fine-tuning to minimize the original loss functions of the source learning task and the target learning task simultaneously."
    }, {
      "heading" : "3.2. Similar Image Search",
      "text" : "There is a unique step in our pipeline. For each image from the target domain, we search a certain number of images with similar low-level characteristics from the source domain. Only images returned from these searches are used as training images for the source learning task in selective joint fine-tuning. We elaborate this image search step below.\nFilter Bank We use the responses to a filter bank to describe the low-level characteristics of an image. The first filter bank we use is the Gabor filter bank. Gabor filters are commonly used for feature description, especially texture description [32]. Gabor filter responses are powerful lowlevel features for image and pattern analysis. We use the parameter setting in [32] as a reference. For each of the real and imaginary parts, we use 24 convolutional kernels with 4 scales and 6 orientations. Thus there are 48 Gabor filters in total.\nKernels in a deep convolutional neural network are actually spatial filters. When there is nonlinear activation following a kernel, the combination of the kernel and nonlinear activation is essentially a nonlinear filter. A deep CNN can extract low/middle/high level features at different convolutional layers [51]. Convolutional layers close to the input data focus on extract low-level features while those further away from the input extract middle- and high-level features. In fact, a subset of the kernels in the first convolutional layer of AlexNet trained on ImageNet exhibit oriented stripes, similar to Gabor filters [25]. When trained on a large-scale diverse dataset, such as ImageNet, such kernels can be used for describing generic low-level image characteristics. In practice, we use all kernels (and their following nonlinear activation) from the first and second convolutional layers of AlexNet pre-trained on ImageNet as our second choice of a filter bank.\nImage Descriptor Let Ci(m,n) denote the response map to the i-th convolutional kernel or Gabor filter in our filter bank, and φi its histogram. To obtain more discriminative histogram features, we first obtain the upper bound hui and lower bound hli of the i-th response map by scanning the entire target domain Dt. Then the interval hli, hui is divided into a set of small bins. We adaptively set the width of every histogram bin so that each of them contains a roughly equal percentage of pixels. In this manner, we can avoid a large percentage of pixels falling into the same bin. We concatenate the histograms of all filter response maps to form a feature vector, φk = { φ1,φ2, ,φD } , for image xk.\nNearest Neighbor Ranking Given the histogram-based descriptor of a training image xti in the target domain, we search for its nearest neighbors in the source domain Ds. Note that the number of kernels in different convolutional layers of AlexNet might be different. To ensure equal weighting among different convolutional layers during nearest neighbor search, each histogram of kernel responses is normalized by the total number of kernels in the corresponding layer. Thus the distance between the descriptor of a source image xsj and that of a target image x t i is computed as follows.\nH ( xti,x s j ) = D∑ h=1 wh[κ(φ i,t h ,φ j,s h ) + κ(φ j,s h ,φ i,t h )],\nwhere wh = 1/Nh, Nh is the number of convolutional kernels in the corresponding layer, φi,th and φ j,s h are the hth histogram for images xti and x s j , and κ(·, ·) is the KLdivergence.\nHard Samples in the Target Domain The labels of training samples in the target domain have varying degrees of difficulty to satisfy. Intuitively, we would like to seek extra help for those hard training samples in the target domain by searching for more and more nearest neighbors in the source domain. We propose an iterative scheme for this purpose. We calculate the information entropy to measure the classification uncertainty of training samples in the target domain after the m-th iteration as follows.\nHmi = − C∑\nc=1\npmi,c log(p m i,c),\nwhere C is the number of classes, pmi,c is the probability that the i-th training sample belongs to the c-th class after a softmax layer in the m-th iteration.\nTraining samples that have high classification uncertainty are considered hard training samples. In the next iteration, we increase the number of nearest neighbors of the hard training samples as in Eq. (3.2), and continue finetuning the model trained in the current iteration. For a training sample xti in the target domain, the number of its nearest neighbors in the next iteration is defined as follows.\nKm+1i =  K m i + σ0, ŷ t i 6= yti\nKmi + σ1, ŷti = yti and Hmi ≥ δ Kmi , ŷti = yti and Hmi < δ (6)\nwhere σ0, σ1 and δ are constants, ŷti is predicted label of xti, and Kmi is the number of nearest neighbors in the mth iteration. By changing the number of nearest neighbors for samples in the target domain, the subset of the source domain used as training data evolves over iterations, which in turn gradually changes the feature representation learned\nin the deep network. In the above equation, we typically set δ = 0.1, σ0 = 4K0 and σ1 = 2K0, where K0 is the initial number of nearest neighbors for all samples in the target domain. In our experiments, we stop after five iterations.\nIn Table 1, we compare the effectiveness of Gabor filters and various combinations of kernels from AlexNet in our selective joint fine-tuning. In this experiment, we use the 50-layer residual network [15] with half of the convolutional kernels in the original architecture."
    }, {
      "heading" : "4. Experiments",
      "text" : ""
    }, {
      "heading" : "4.1. Implementation",
      "text" : "In all experiments, we use the 152-layer residual network [15] as the deep convolutional architecture. To use GPU memory more efficiently, we modify the implementation of the 152-layer residual network with identity mappings [16] such that more images can be included in a minibatch using Caffe [20]. We use the pre-trained model released in [15] to initialize the residual network, and choose either ImageNet or the combination of ImageNet and Places as the source domain. During selective joint fine-turning, source and target samples are mixed together in each minibatch. Once the data has passed the average pooling layer in the residual network, we split the source and target samples, and send them to their corresponding softmax classifier layer respectively. Both the source and target classifiers are initialized randomly.\nWe run all our experiments on a TITAN X GPU with 12GB memory. All training data is augmented as in [35] first, and we follow the training and testing settings in [15]. Every mini-batch can include 20 224×224 images using the reimplemented residual network. We include randomly chosen samples from the target domain in a mini-batch. Then for each of the chosen target sample, we further include one of its retrieved nearest neighbors from the source domain in the same mini-batch. We set the iter size to 10 for each iteration in Caffe [20]. The momentum parameter is set to 0.9 and the weight decay is 0.0001 in SGD. During selective joint fine-tuning, the learning rate starts from 0.01 and is divided by 10 after every 2400 − 5000 iterations in\nall the experiments. Most of the experiments can finish in 16000 iterations."
    }, {
      "heading" : "4.2. Source Image Retrieval",
      "text" : "We use the ImageNet ILSVRC 2012 training set [40] as the source domain for Stanford Dogs [22], Oxford Flowers [33], and Caltech 256 [14], and the combination of the ImageNet and Places 205 [53] training sets as the source domain for MIT Indoor 67 [37]. Fig. 2 shows the retrieved 1-st, 10-th, 20-th, 30-th, and 40-th nearest neighbors from ImageNet [40] or Places [53]. It can be observed that corresponding source and target images share similar colors, local patterns and global structures. Since low-level filter bank responses do not encode strong semantic information, the 50 nearest neighbors from a target domain include images from various and sometimes completely unrelated categories.\nWe find out experimentally that there should be at least 200,000 retrieved images from the source domain. Too few source images give rise to overfitting quickly. Therefore, the initial number of retrieved nearest neighbors (K0) for each target training sample is set to meet this requirement. On the other hand, a surprising result is that setting K0 too large would make the performance of the target learning\ntask drop significantly. In our experiments, we setK0 to different values for Stanford Dogs (K0 = 100), Oxford Flowers (K0 = 300), Caltech 256 (K0 = 50 − 100), and MIT Indoor 67 (K0 = 100). Since there exists much overlap among the nearest neighbors of different target samples, the retrieved images typically do not cover the entire ImageNet or Places datasets."
    }, {
      "heading" : "4.3. Fine-grained Object Recognition",
      "text" : "Stanford Dogs 120. Stanford Dogs 120 [22] contains 120 categories of dogs. There are 12000 images for training, and 8580 images for testing. We do not use the parts information during selective joint fine-tuning, and use the commonly used mean class accuracy to evaluate the performance as in [14].\nAs shown in Table 2, the mean class accuracy achieved by fine-tuning the residual network using the training samples of this dataset only and without a source domain is 80.4%, which is already better than most previous stateof-the-art results except [24]. It shows that the 152-layer residual network [15, 16] pre-trained on the ImageNet dataset [40] has a strong generalization capability on this fine-grained classification task. Using the entire ImageNet dataset during regular joint fine-tuning can improve the performance by 5.1%. When we finally perform our proposed selective joint fine-tuning using a subset of source domain images retrieved using histograms of low-level convolutional features, the performance is further improved to 90.2%, which is 9.8% higher than the performance of conventional fine-tuning without a source domain and 4.3% higher than the result reported in [24], which expands the original target training set using Google image search. This comparison demonstrates that selective joint fine-tuning can significantly outperform conventional fine-tuning. Oxford Flowers 102. Oxford Flowers 102 [33] consists of 102 flower categories. 1020 images are used for training, 1020 for validation, and 6149 images are used for testing. There are only 10 training images in each category.\nAs shown in Table 3, the mean class accuracy achieved by conventional fine-tuning using the training samples of this dataset only and without a source domain is 92.3%. Se-\nlective joint fine-tuning further improves the performance to 94.7%, 3.3% higher than previous best result from a single network [39]. To compare with previous state-of-the-art results obtained using an ensemble of different networks, we also average the performance of multiple models obtained during iterative source image retrieval for hard training samples in the target domain. Experiments show that the performance of our ensemble model is 95.8%, 1.3% higher than previous best ensemble performance reported in [23]. Note that Simon et al. [42] used the validation set in this dataset as additional training data. To verify the effectiveness of our joint fine-tuning strategy, we have also conducted experiments with this setting during training and our result from a single network outperforms that of [42] by 1.7%."
    }, {
      "heading" : "4.4. General Object Recognition",
      "text" : "Caltech 256. Caltech 256 [14] has 256 object categories and 1 background cluster class. In every category, there are at least 80 images used for training, validation and testing. Researchers typically report results with the number of training samples per class falling between 5 and 60. We follow the testing procedure in [46] to compare with stateof-the-art results.\nWe conduct four experiments with the number of training samples per class set to 15, 30, 45 and 60, respectively. According to Table 4, in comparison to conventional finetuning without using a source domain, selective joint finetuning improves classification accuracy in all four experiments, and the degree of improvement varies between 2.6% and 4.1%. Performance improvement due to selective joint fine-tuning is more obvious when a smaller number of target training image per class are used. This is because limited diversity in the target training data imposes a greater need to seek help from the source domain. In most of these experiments, the classification performance of our selective joint fine-tuning is also significantly better than previous stateof-the-art results."
    }, {
      "heading" : "4.5. Scene Classification",
      "text" : "MIT Indoor 67. MIT Indoor 67 [37] has 67 scene categories. In each category, there are 80 images for training and 20 images for testing. Since MIT Indoor 67 is a scene dataset, in addition to the ImageNet ILSVRC 2012 training set [40], the Places-205 training set [53] is also a potential source domain. We compare three settings during slective joint fine-tuning: ImageNet as the source domain, Places as the source domain, and the combination of both ImageNet and Places as the source domain.\nAs shown in Table 5, the mean class accuracy of selective joint fine-tuning with ImageNet as the source domain is 82.8%, 1.1% higher than that of conventional finetuning without using a source domain. Since ImageNet is an object-centric dataset while MIT Indoor 67 is a scene\ndataset, it is hard for training images in the target domain to retrieve source domain images with similar low-level characteristics. But source images retrieved from ImageNet still prevent the network from overfitting too heavily and help achieve a performance gain. When the Places dataset serves as the source domain, the mean class accuracy reaches 85.8%, which is 4.1% higher than the performance of finetuning without a source domain and 4.8% higher than previous best result from a single network [6]. And the hybrid source domain based on both ImageNet and Places does not further improve the performance. Once averaging the output from the networks jointly fine-tuned with Places and the hybrid source domain, we obtain a classification accuracy 0.9% higher than previous best result from an ensemble model [17]."
    }, {
      "heading" : "4.6. Ablation Study",
      "text" : "We perform an ablation study on both Stanford Dogs 120 [22] and Oxford Flowers 102 [33] by replacing or removing a single component from our pipeline. First, instead of using a subset of retrieved training images from the source domain, we simply use all training images in the source domain. Table 2 and Table 3 show that joint fine-tuning with the entire source domain decrease the performance by 4.6% and 1.3% respectively. This demonstrates that using more training data from the source domain is not always better.\nOn the contrary, using less but more relevant data from the source domain is actually more helpful. Second, instead of using a subset of retrieved training images, we use the same number of randomly chosen training images from the source domain. Again, the performance drops by 4.7% and 1.5% respectively. Third, to validate the effectiveness of iteratively increasing the number of retrieved images for hard training samples in the target domain, we turn off this feature and only use the same number (K0) of retrieved images for all training samples in the target domain. The performance drops by 1.9% and 0.5% respectively. This indicates that our adaptive scheme for hard samples is useful in improving the performance. Fourth, we use convolutional kernels in the two bottom layers of a pre-trained AlexNet as our filter bank. If we replace this filter bank with the Gabor filter bank, the overall performance drops by 2.7% and 0.9% respectively, which indicates a filter bank learned from a diverse dataset could be more powerful than an analytically defined one. Finally, if we perform conventional fine-tuning without using a source domain, the performance drop becomes quite significant and reaches 9.8% and 2.4% respectively."
    }, {
      "heading" : "5. Conclusions",
      "text" : "In this paper, we address deep learning tasks with insufficient training data by introducing selective joint fine-tuning, which performs a target learning task with insufficient training data simultaneously with another source learning task with abundant training data. Different from previous work which directly adds extra training data to the target learning task, we try to reduce the amount of data annotation effort when solving visual recognition problems. We borrow samples from a large-scale labeled database such as ImageNet and Places, and do not require additional labeling effort beyond the existing datasets. Experiments show that our selective joint fine-tuning strategy achieves state-of-theart performance on multiple visual classification tasks with insufficient training data for deep learning. Nevertheless, how to find the most suitable source domain for a specific target learning task remains an open problem for future investigation."
    } ],
    "references" : [ {
      "title" : "From generic to specific deep representations for visual recognition",
      "author" : [ "H. Azizpour", "A. Sharif Razavian", "J. Sullivan", "A. Maki", "S. Carlsson" ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 36–45",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Discriminative learning under covariate shift",
      "author" : [ "S. Bickel", "M. Brückner", "T. Scheffer" ],
      "venue" : "Journal of Machine Learning Research, 10(Sep):2137–2155",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Multipath sparse coding using hierarchical matching pursuit",
      "author" : [ "L. Bo", "X. Ren", "D. Fox" ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 660–667",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Multitask learning",
      "author" : [ "R. Caruana" ],
      "venue" : "Learning to learn, pages 95–133. Springer",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Boosted multi-task learning",
      "author" : [ "O. Chapelle", "P. Shivaswamy", "S. Vadrevu", "K. Weinberger", "Y. Zhang", "B. Tseng" ],
      "venue" : "Machine learning, 85(1-2):149–173",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Deep filter banks for texture recognition",
      "author" : [ "M. Cimpoi", "S. Maji", "I. Kokkinos", "A. Vedaldi" ],
      "venue" : "description, and segmentation. International Journal of Computer Vision, 118(1):65– 94",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Decaf: A deep convolutional activation feature for generic visual recognition",
      "author" : [ "J. Donahue", "Y. Jia", "O. Vinyals", "J. Hoffman", "N. Zhang", "E. Tzeng", "T. Darrell" ],
      "venue" : "ICML, pages 647–655",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Visual event recognition in videos by learning from web data",
      "author" : [ "L. Duan", "D. Xu", "I.W.-H. Tsang", "J. Luo" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence, 34(9):1667–1680",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Predicting depth",
      "author" : [ "D. Eigen", "R. Fergus" ],
      "venue" : "surface normals and semantic labels with a common multi-scale convolutional architecture. In Proceedings of the IEEE International Conference on Computer Vision, pages 2650–2658",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Multi-task feature learning",
      "author" : [ "A. Evgeniou", "M. Pontil" ],
      "venue" : "Advances in neural information processing systems, 19:41",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Local alignments for fine-grained categorization",
      "author" : [ "E. Gavves", "B. Fernando", "C.G. Snoek", "A.W. Smeulders", "T. Tuytelaars" ],
      "venue" : "International Journal of Computer Vision, 111(2):191– 212",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Rich feature hierarchies for accurate object detection and semantic segmentation",
      "author" : [ "R. Girshick", "J. Donahue", "T. Darrell", "J. Malik" ],
      "venue" : "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 580–587",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Connecting the dots with landmarks: Discriminatively learning domain-invariant features for unsupervised domain adaptation",
      "author" : [ "B. Gong", "K. Grauman", "F. Sha" ],
      "venue" : "ICML (1), pages 222–230",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Caltech-256 object category dataset",
      "author" : [ "G. Griffin", "A. Holub", "P. Perona" ],
      "venue" : null,
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2007
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "K. He", "X. Zhang", "S. Ren", "J. Sun" ],
      "venue" : "arXiv preprint arXiv:1512.03385",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Identity mappings in deep residual networks",
      "author" : [ "K. He", "X. Zhang", "S. Ren", "J. Sun" ],
      "venue" : "arXiv preprint arXiv:1603.05027",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Scene recognition with cnns: objects",
      "author" : [ "L. Herranz", "S. Jiang", "X. Li" ],
      "venue" : "scales and dataset bias. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 571–579",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Correcting sample selection bias by unlabeled data",
      "author" : [ "J. Huang", "A. Gretton", "K.M. Borgwardt", "B. Schölkopf", "A.J. Smola" ],
      "venue" : "Advances in neural information processing systems, pages 601–608",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "and A",
      "author" : [ "M. Huh", "P. Agrawal" ],
      "venue" : "A. Efros. What makes imagenet good for transfer learning? arXiv preprint arXiv:1608.08614",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Caffe: Convolutional architecture for fast feature embedding",
      "author" : [ "Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell" ],
      "venue" : "Proceedings of the 22nd ACM international conference on Multimedia, pages 675–678. ACM",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Very deep convolutional networks for large-scale image recognition",
      "author" : [ "S. Karen", "A. Zisserman" ],
      "venue" : "arXiv preprint arXiv:1409.1556",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Novel dataset for fine-grained image categorization: Stanford dogs",
      "author" : [ "A. Khosla", "N. Jayadevaprakash", "B. Yao", "F.-F. Li" ],
      "venue" : "Proc. CVPR Workshop on Fine-Grained Visual Categorization (FGVC), volume 2",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Learning to select pre-trained deep representations with bayesian evidence framework",
      "author" : [ "Y.-D. Kim", "T. Jang", "B. Han", "S. Choi" ],
      "venue" : "arXiv preprint arXiv:1506.02565",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "The unreasonable effectiveness of noisy data for fine-grained recognition",
      "author" : [ "J. Krause", "B. Sapp", "A. Howard", "H. Zhou", "A. Toshev", "T. Duerig", "J. Philbin", "L. Fei-Fei" ],
      "venue" : "arXiv preprint arXiv:1511.06789",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "A. Krizhevsky", "I. Sutskever", "G.E. Hinton" ],
      "venue" : "Advances in neural information processing systems",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Learning without forgetting",
      "author" : [ "Z. Li", "D. Hoiem" ],
      "venue" : "European Conference on Computer Vision, pages 614–629. Springer",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Microsoft coco: Common objects in context",
      "author" : [ "T.-Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Dollár", "C.L. Zitnick" ],
      "venue" : "European Conference on Computer Vision, pages 740–755. Springer",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Bilinear cnn models for fine-grained visual recognition",
      "author" : [ "T.-Y. Lin", "A. RoyChowdhury", "S. Maji" ],
      "venue" : "Proceedings of the IEEE International Conference on Computer Vision, pages 1449–1457",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Fully convolutional networks for semantic segmentation",
      "author" : [ "J. Long", "E. Shelhamer", "T. Darrell" ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3431–3440",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Learning transferable features with deep adaptation networks",
      "author" : [ "M. Long", "J. Wang" ],
      "venue" : "CoRR, abs/1502.02791, 1:2",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Understanding deep image representations by inverting them",
      "author" : [ "A. Mahendran", "A. Vedaldi" ],
      "venue" : "2015 IEEE conference 9  on computer vision and pattern recognition (CVPR), pages 5188–5196. IEEE",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Texture features for browsing and retrieval of image data",
      "author" : [ "B.S. Manjunath", "W.-Y. Ma" ],
      "venue" : "IEEE Transactions on pattern analysis and machine intelligence, 18(8):837–842",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "Automated flower classification over a large number of classes",
      "author" : [ "M.-E. Nilsback", "A. Zisserman" ],
      "venue" : "Computer Vision, Graphics & Image Processing, 2008. ICVGIP’08. Sixth Indian Conference on, pages 722–729. IEEE",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "A survey on transfer learning",
      "author" : [ "S.J. Pan", "Q. Yang" ],
      "venue" : "IEEE Transactions on knowledge and data engineering, 22(10):1345–1359",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Transformation pursuit for image classification",
      "author" : [ "M. Paulin", "J. Revaud", "Z. Harchaoui", "F. Perronnin", "C. Schmid" ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3646–3653",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Fine-grained visual categorization via multi-stage metric learning",
      "author" : [ "Q. Qian", "R. Jin", "S. Zhu", "Y. Lin" ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3716–3724",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Recognizing indoor scenes",
      "author" : [ "A. Quattoni", "A. Torralba" ],
      "venue" : "Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages 413–420. IEEE",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Faster r-cnn: Towards real-time object detection with region proposal networks",
      "author" : [ "S. Ren", "K. He", "R. Girshick", "J. Sun" ],
      "venue" : "Advances in neural information processing systems, pages 91–99",
      "citeRegEx" : "38",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Metric learning with adaptive density discrimination",
      "author" : [ "O. Rippel", "M. Paluri", "P. Dollar", "L. Bourdev" ],
      "venue" : "stat, 1050:2",
      "citeRegEx" : "39",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "et al",
      "author" : [ "O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein" ],
      "venue" : "Imagenet large scale visual recognition challenge. International Journal of Computer Vision, 115(3):211–252",
      "citeRegEx" : "40",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Cnn features off-the-shelf: an astounding baseline for recognition",
      "author" : [ "A. Sharif Razavian", "H. Azizpour", "J. Sullivan", "S. Carlsson" ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 806– 813",
      "citeRegEx" : "41",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Neural activation constellations: Unsupervised part model discovery with convolutional networks",
      "author" : [ "M. Simon", "E. Rodner" ],
      "venue" : "Proceedings of the IEEE International Conference on Computer Vision, pages 1143–1151",
      "citeRegEx" : "42",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Dropout: a simple way to prevent neural networks from overfitting",
      "author" : [ "N. Srivastava", "G.E. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov" ],
      "venue" : "Journal of Machine Learning Research, 15(1):1929–1958",
      "citeRegEx" : "43",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Going deeper with convolutions",
      "author" : [ "Szegedy", "Christian", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "44",
      "shortCiteRegEx" : "44",
      "year" : 2015
    }, {
      "title" : "Simultaneous deep transfer across domains and tasks",
      "author" : [ "E. Tzeng", "J. Hoffman", "T. Darrell", "K. Saenko" ],
      "venue" : "Proceedings  of the IEEE International Conference on Computer Vision, pages 4068–4076",
      "citeRegEx" : "45",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Locality-constrained linear coding for image classification",
      "author" : [ "J. Wang", "J. Yang", "K. Yu", "F. Lv", "T. Huang", "Y. Gong" ],
      "venue" : "Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on, pages 3360–3367. IEEE",
      "citeRegEx" : "46",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Harvesting discriminative meta objects with deep cnn features for scene classification",
      "author" : [ "R. Wu", "B. Wang", "W. Wang", "Y. Yu" ],
      "venue" : "Proceedings of the IEEE International Conference on Computer Vision, pages 1287–1295",
      "citeRegEx" : "47",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Hyper-class augmented and regularized deep learning for fine-grained image classification",
      "author" : [ "S. Xie", "T. Yang", "X. Wang", "Y. Lin" ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2645– 2654",
      "citeRegEx" : "48",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Multitask learning for classification with dirichlet process priors",
      "author" : [ "Y. Xue", "X. Liao", "L. Carin", "B. Krishnapuram" ],
      "venue" : "Journal of Machine Learning Research, 8(Jan):35–63",
      "citeRegEx" : "49",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Multi-scale pyramid pooling for deep convolutional representation",
      "author" : [ "D. Yoo", "S. Park", "J.-Y. Lee", "I. So Kweon" ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 71–80",
      "citeRegEx" : "50",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "How transferable are features in deep neural networks? In Advances in neural information processing systems",
      "author" : [ "J. Yosinski", "J. Clune", "Y. Bengio", "H. Lipson" ],
      "venue" : "pages 3320–3328",
      "citeRegEx" : "51",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Visualizing and understanding convolutional networks",
      "author" : [ "M.D. Zeiler", "R. Fergus" ],
      "venue" : "European Conference on Computer Vision, pages 818–833. Springer",
      "citeRegEx" : "52",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Learning deep features for scene recognition using places database",
      "author" : [ "B. Zhou", "A. Lapedriza", "J. Xiao", "A. Torralba", "A. Oliva" ],
      "venue" : "Advances in neural information processing systems, pages 487–495",
      "citeRegEx" : "53",
      "shortCiteRegEx" : null,
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 24,
      "context" : "Convolutional neural networks (CNNs) have become deeper and larger to pursue increasingly better performance on classification and recognition tasks [25, 21, 44, 16, 15].",
      "startOffset" : 149,
      "endOffset" : 169
    }, {
      "referenceID" : 20,
      "context" : "Convolutional neural networks (CNNs) have become deeper and larger to pursue increasingly better performance on classification and recognition tasks [25, 21, 44, 16, 15].",
      "startOffset" : 149,
      "endOffset" : 169
    }, {
      "referenceID" : 43,
      "context" : "Convolutional neural networks (CNNs) have become deeper and larger to pursue increasingly better performance on classification and recognition tasks [25, 21, 44, 16, 15].",
      "startOffset" : 149,
      "endOffset" : 169
    }, {
      "referenceID" : 15,
      "context" : "Convolutional neural networks (CNNs) have become deeper and larger to pursue increasingly better performance on classification and recognition tasks [25, 21, 44, 16, 15].",
      "startOffset" : 149,
      "endOffset" : 169
    }, {
      "referenceID" : 14,
      "context" : "Convolutional neural networks (CNNs) have become deeper and larger to pursue increasingly better performance on classification and recognition tasks [25, 21, 44, 16, 15].",
      "startOffset" : 149,
      "endOffset" : 169
    }, {
      "referenceID" : 39,
      "context" : "dataset [40], Places [53], and MS COCO [27], have led to a series of breakthroughs in visual recognition, including image classification [28], object detection [12], and semantic segmentation [29].",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 52,
      "context" : "dataset [40], Places [53], and MS COCO [27], have led to a series of breakthroughs in visual recognition, including image classification [28], object detection [12], and semantic segmentation [29].",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 26,
      "context" : "dataset [40], Places [53], and MS COCO [27], have led to a series of breakthroughs in visual recognition, including image classification [28], object detection [12], and semantic segmentation [29].",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 27,
      "context" : "dataset [40], Places [53], and MS COCO [27], have led to a series of breakthroughs in visual recognition, including image classification [28], object detection [12], and semantic segmentation [29].",
      "startOffset" : 137,
      "endOffset" : 141
    }, {
      "referenceID" : 11,
      "context" : "dataset [40], Places [53], and MS COCO [27], have led to a series of breakthroughs in visual recognition, including image classification [28], object detection [12], and semantic segmentation [29].",
      "startOffset" : 160,
      "endOffset" : 164
    }, {
      "referenceID" : 28,
      "context" : "dataset [40], Places [53], and MS COCO [27], have led to a series of breakthroughs in visual recognition, including image classification [28], object detection [12], and semantic segmentation [29].",
      "startOffset" : 192,
      "endOffset" : 196
    }, {
      "referenceID" : 32,
      "context" : "Fine-grained classification problems, such as Oxford Flowers 102 [33] and Stanford Dogs 120 [22], are such examples.",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 21,
      "context" : "Fine-grained classification problems, such as Oxford Flowers 102 [33] and Stanford Dogs 120 [22], are such examples.",
      "startOffset" : 92,
      "endOffset" : 96
    }, {
      "referenceID" : 42,
      "context" : "Solving the overfitting problem for deep convolutional neural networks on learning tasks without sufficient training data is very challenging [43].",
      "startOffset" : 142,
      "endOffset" : 146
    }, {
      "referenceID" : 33,
      "context" : "Transfer learning techniques that apply knowledge learnt from one task to other related tasks have been proven helpful [34].",
      "startOffset" : 119,
      "endOffset" : 123
    }, {
      "referenceID" : 18,
      "context" : "In the context of deep learning, fine-tuning a deep network pre-trained on the ImageNet or Places dataset is a common strategy to learn taskspecific deep features [19, 51, 17].",
      "startOffset" : 163,
      "endOffset" : 175
    }, {
      "referenceID" : 50,
      "context" : "In the context of deep learning, fine-tuning a deep network pre-trained on the ImageNet or Places dataset is a common strategy to learn taskspecific deep features [19, 51, 17].",
      "startOffset" : 163,
      "endOffset" : 175
    }, {
      "referenceID" : 16,
      "context" : "In the context of deep learning, fine-tuning a deep network pre-trained on the ImageNet or Places dataset is a common strategy to learn taskspecific deep features [19, 51, 17].",
      "startOffset" : 163,
      "endOffset" : 175
    }, {
      "referenceID" : 48,
      "context" : "We adopt the source-target joint training methodology [49] when fine-tuning deep neural networks.",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 21,
      "context" : "fur textures in Stanford Dogs 120 [22]), and the learned kernels in the convolutional layers of a deep network need to grasp such characteristics in order to generate highly discriminative features.",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 31,
      "context" : "Gabor filters [32] form an example of a linear filter bank, and the complete set of kernels from certain layers of a pretrained CNN form an example of a nonlinear filter bank.",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 21,
      "context" : "These visual classification tasks include fine-grained classification on Stanford Dogs 120 [22] and Oxford Flowers 102 [33], image classification on Caltech 256 [14], and scene classification on MIT Indoor 67 [37].",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 32,
      "context" : "These visual classification tasks include fine-grained classification on Stanford Dogs 120 [22] and Oxford Flowers 102 [33], image classification on Caltech 256 [14], and scene classification on MIT Indoor 67 [37].",
      "startOffset" : 119,
      "endOffset" : 123
    }, {
      "referenceID" : 13,
      "context" : "These visual classification tasks include fine-grained classification on Stanford Dogs 120 [22] and Oxford Flowers 102 [33], image classification on Caltech 256 [14], and scene classification on MIT Indoor 67 [37].",
      "startOffset" : 161,
      "endOffset" : 165
    }, {
      "referenceID" : 36,
      "context" : "These visual classification tasks include fine-grained classification on Stanford Dogs 120 [22] and Oxford Flowers 102 [33], image classification on Caltech 256 [14], and scene classification on MIT Indoor 67 [37].",
      "startOffset" : 209,
      "endOffset" : 213
    }, {
      "referenceID" : 3,
      "context" : "Multi-task learning (MTL) learns shared feature representations or classifiers for related tasks [4].",
      "startOffset" : 97,
      "endOffset" : 100
    }, {
      "referenceID" : 9,
      "context" : "The focus of [10] is on learning a shared feature representation that generalizes well on related tasks.",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 4,
      "context" : "Multiple tasks were learned in a joint model in [5] by explicitly optimizing shared parameters and task-specific parameters.",
      "startOffset" : 48,
      "endOffset" : 51
    }, {
      "referenceID" : 37,
      "context" : "In deep learning, faster RCNN [38] jointly learns object locations and labels using shared convolutional layers but different loss functions for these two tasks.",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 8,
      "context" : "In [9], the same multi-scale convolutional architecture was used to predict depth, surface normals and semantic labels.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 9,
      "context" : "While previous work [10, 38] attempts to find a shared feature space that benefits multiple learning tasks, the proposed joint training scheme in this paper focuses on learning a shared feature space that improves the performance of the target learning task only.",
      "startOffset" : 20,
      "endOffset" : 28
    }, {
      "referenceID" : 37,
      "context" : "While previous work [10, 38] attempts to find a shared feature space that benefits multiple learning tasks, the proposed joint training scheme in this paper focuses on learning a shared feature space that improves the performance of the target learning task only.",
      "startOffset" : 20,
      "endOffset" : 28
    }, {
      "referenceID" : 40,
      "context" : "Off-the-shelf CNN features [41, 7] have been proven to be powerful in various",
      "startOffset" : 27,
      "endOffset" : 34
    }, {
      "referenceID" : 6,
      "context" : "Off-the-shelf CNN features [41, 7] have been proven to be powerful in various",
      "startOffset" : 27,
      "endOffset" : 34
    }, {
      "referenceID" : 39,
      "context" : "Pre-training convolutional neural networks on ImageNet [40] or Places [53] has been the standard practice for other vision problems.",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 52,
      "context" : "Pre-training convolutional neural networks on ImageNet [40] or Places [53] has been the standard practice for other vision problems.",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 11,
      "context" : "Fine-tuning pre-trained models [12] has become a commonly used method to learn task-specific features.",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 50,
      "context" : "The transfer ability of different convolutional layers in CNNs has been investigated in [51].",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 25,
      "context" : "[26] proposed to preserve the output for old learning tasks while learning a new task.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 33,
      "context" : "Different from MTL, transfer learning (or domain adaptation) [34] applies knowledge learnt in one domain to other related tasks.",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 1,
      "context" : "Much work has been done to transfer knowledge between different domains using supervised or unsupervised learning [2, 13, 45].",
      "startOffset" : 114,
      "endOffset" : 125
    }, {
      "referenceID" : 12,
      "context" : "Much work has been done to transfer knowledge between different domains using supervised or unsupervised learning [2, 13, 45].",
      "startOffset" : 114,
      "endOffset" : 125
    }, {
      "referenceID" : 44,
      "context" : "Much work has been done to transfer knowledge between different domains using supervised or unsupervised learning [2, 13, 45].",
      "startOffset" : 114,
      "endOffset" : 125
    }, {
      "referenceID" : 17,
      "context" : "Domain adaptation algorithms can be divided into three categories, including instance adaption [18, 1], feature adaption [30, 45], and model adaption [8].",
      "startOffset" : 95,
      "endOffset" : 102
    }, {
      "referenceID" : 0,
      "context" : "Domain adaptation algorithms can be divided into three categories, including instance adaption [18, 1], feature adaption [30, 45], and model adaption [8].",
      "startOffset" : 95,
      "endOffset" : 102
    }, {
      "referenceID" : 29,
      "context" : "Domain adaptation algorithms can be divided into three categories, including instance adaption [18, 1], feature adaption [30, 45], and model adaption [8].",
      "startOffset" : 121,
      "endOffset" : 129
    }, {
      "referenceID" : 44,
      "context" : "Domain adaptation algorithms can be divided into three categories, including instance adaption [18, 1], feature adaption [30, 45], and model adaption [8].",
      "startOffset" : 121,
      "endOffset" : 129
    }, {
      "referenceID" : 7,
      "context" : "Domain adaptation algorithms can be divided into three categories, including instance adaption [18, 1], feature adaption [30, 45], and model adaption [8].",
      "startOffset" : 150,
      "endOffset" : 153
    }, {
      "referenceID" : 44,
      "context" : "[45] performed feature adaptation using a shared convolutional neural network by transferring the class relationship in the source domain to the target domain.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 44,
      "context" : "To make our pipeline more flexible, this paper does not assume the source and target label spaces are the same as in [45].",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 0,
      "context" : "Different from the work in [1] which randomly resamples training classes or images in the source domain, this paper conducts a special type of transfer learning by selecting source training samples that are nearest neighbors of samples in the target domain in the space of certain low-level image descriptor.",
      "startOffset" : 27,
      "endOffset" : 30
    }, {
      "referenceID" : 23,
      "context" : "[24] directly performed Google image search using keywords associated with categories from the target domain, and download a noisy collection of images to form a training set.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 30,
      "context" : "It has been shown in [31] that lowlevel features computed in the bottom layers of a CNN encode very rich information, which can completely reconstruct the original image.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 23,
      "context" : "Our experimental results show that nearest neighbor search using low-level features can outperform that using high-level semantic information as in [24].",
      "startOffset" : 148,
      "endOffset" : 152
    }, {
      "referenceID" : 39,
      "context" : "There exist a few large-scale visual recognition datasets that can serve as the source domain, including ImageNet ILSVRC dataset [40], Places [53], and MS COCO [27].",
      "startOffset" : 129,
      "endOffset" : 133
    }, {
      "referenceID" : 52,
      "context" : "There exist a few large-scale visual recognition datasets that can serve as the source domain, including ImageNet ILSVRC dataset [40], Places [53], and MS COCO [27].",
      "startOffset" : 142,
      "endOffset" : 146
    }, {
      "referenceID" : 26,
      "context" : "There exist a few large-scale visual recognition datasets that can serve as the source domain, including ImageNet ILSVRC dataset [40], Places [53], and MS COCO [27].",
      "startOffset" : 160,
      "endOffset" : 164
    }, {
      "referenceID" : 24,
      "context" : "One is the Gabor filter bank, and the other consists of kernels in the convolutional layers of AlexNet pre-trained on ImageNet [25].",
      "startOffset" : 127,
      "endOffset" : 131
    }, {
      "referenceID" : 24,
      "context" : "CNN Architecture : Almost any existing deep convolutional neural network, such as AlexNet [25], GoogleNet [44], VggNet [21], and ResidualNet [15], can be used in our selective joint fine-tuning.",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 43,
      "context" : "CNN Architecture : Almost any existing deep convolutional neural network, such as AlexNet [25], GoogleNet [44], VggNet [21], and ResidualNet [15], can be used in our selective joint fine-tuning.",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 20,
      "context" : "CNN Architecture : Almost any existing deep convolutional neural network, such as AlexNet [25], GoogleNet [44], VggNet [21], and ResidualNet [15], can be used in our selective joint fine-tuning.",
      "startOffset" : 119,
      "endOffset" : 123
    }, {
      "referenceID" : 14,
      "context" : "CNN Architecture : Almost any existing deep convolutional neural network, such as AlexNet [25], GoogleNet [44], VggNet [21], and ResidualNet [15], can be used in our selective joint fine-tuning.",
      "startOffset" : 141,
      "endOffset" : 145
    }, {
      "referenceID" : 15,
      "context" : "We use the 152-layer residual network with identity mappings [16] as the CNN architecture in our experiments.",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 31,
      "context" : "Gabor filters are commonly used for feature description, especially texture description [32].",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 31,
      "context" : "We use the parameter setting in [32] as a reference.",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 50,
      "context" : "A deep CNN can extract low/middle/high level features at different convolutional layers [51].",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 24,
      "context" : "In fact, a subset of the kernels in the first convolutional layer of AlexNet trained on ImageNet exhibit oriented stripes, similar to Gabor filters [25].",
      "startOffset" : 148,
      "endOffset" : 152
    }, {
      "referenceID" : 14,
      "context" : "In this experiment, we use the 50-layer residual network [15] with half of the convolutional kernels in the original architecture.",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 14,
      "context" : "In all experiments, we use the 152-layer residual network [15] as the deep convolutional architecture.",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 15,
      "context" : "To use GPU memory more efficiently, we modify the implementation of the 152-layer residual network with identity mappings [16] such that more images can be included in a minibatch using Caffe [20].",
      "startOffset" : 122,
      "endOffset" : 126
    }, {
      "referenceID" : 19,
      "context" : "To use GPU memory more efficiently, we modify the implementation of the 152-layer residual network with identity mappings [16] such that more images can be included in a minibatch using Caffe [20].",
      "startOffset" : 192,
      "endOffset" : 196
    }, {
      "referenceID" : 14,
      "context" : "We use the pre-trained model released in [15] to initialize the residual network, and choose either ImageNet or the combination of ImageNet and Places as the source domain.",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 34,
      "context" : "All training data is augmented as in [35] first, and we follow the training and testing settings in [15].",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 14,
      "context" : "All training data is augmented as in [35] first, and we follow the training and testing settings in [15].",
      "startOffset" : 100,
      "endOffset" : 104
    }, {
      "referenceID" : 19,
      "context" : "We set the iter size to 10 for each iteration in Caffe [20].",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 21,
      "context" : "The first column shows target images from Stanford Dogs 120 [22], Oxford Flowers 102 [33], Caltech 256 [14], and MIT Indoor 67 [37].",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 32,
      "context" : "The first column shows target images from Stanford Dogs 120 [22], Oxford Flowers 102 [33], Caltech 256 [14], and MIT Indoor 67 [37].",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 13,
      "context" : "The first column shows target images from Stanford Dogs 120 [22], Oxford Flowers 102 [33], Caltech 256 [14], and MIT Indoor 67 [37].",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 36,
      "context" : "The first column shows target images from Stanford Dogs 120 [22], Oxford Flowers 102 [33], Caltech 256 [14], and MIT Indoor 67 [37].",
      "startOffset" : 127,
      "endOffset" : 131
    }, {
      "referenceID" : 39,
      "context" : "We use the ImageNet ILSVRC 2012 training set [40] as the source domain for Stanford Dogs [22], Oxford Flowers [33], and Caltech 256 [14], and the combination of the ImageNet and Places 205 [53] training sets as the source domain for MIT Indoor 67 [37].",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 21,
      "context" : "We use the ImageNet ILSVRC 2012 training set [40] as the source domain for Stanford Dogs [22], Oxford Flowers [33], and Caltech 256 [14], and the combination of the ImageNet and Places 205 [53] training sets as the source domain for MIT Indoor 67 [37].",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 32,
      "context" : "We use the ImageNet ILSVRC 2012 training set [40] as the source domain for Stanford Dogs [22], Oxford Flowers [33], and Caltech 256 [14], and the combination of the ImageNet and Places 205 [53] training sets as the source domain for MIT Indoor 67 [37].",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 13,
      "context" : "We use the ImageNet ILSVRC 2012 training set [40] as the source domain for Stanford Dogs [22], Oxford Flowers [33], and Caltech 256 [14], and the combination of the ImageNet and Places 205 [53] training sets as the source domain for MIT Indoor 67 [37].",
      "startOffset" : 132,
      "endOffset" : 136
    }, {
      "referenceID" : 52,
      "context" : "We use the ImageNet ILSVRC 2012 training set [40] as the source domain for Stanford Dogs [22], Oxford Flowers [33], and Caltech 256 [14], and the combination of the ImageNet and Places 205 [53] training sets as the source domain for MIT Indoor 67 [37].",
      "startOffset" : 189,
      "endOffset" : 193
    }, {
      "referenceID" : 36,
      "context" : "We use the ImageNet ILSVRC 2012 training set [40] as the source domain for Stanford Dogs [22], Oxford Flowers [33], and Caltech 256 [14], and the combination of the ImageNet and Places 205 [53] training sets as the source domain for MIT Indoor 67 [37].",
      "startOffset" : 247,
      "endOffset" : 251
    }, {
      "referenceID" : 39,
      "context" : "2 shows the retrieved 1-st, 10-th, 20-th, 30-th, and 40-th nearest neighbors from ImageNet [40] or Places [53].",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 52,
      "context" : "2 shows the retrieved 1-st, 10-th, 20-th, 30-th, and 40-th nearest neighbors from ImageNet [40] or Places [53].",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 47,
      "context" : "HAR-CNN [48] 49.",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 10,
      "context" : "4 Local Alignment [11] 57.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 35,
      "context" : "0 Multi scale metric learning [36] 70.",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 38,
      "context" : "3 MagNet [39] 75.",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 23,
      "context" : "1 Web Data + Original Data [24] 85.",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 21,
      "context" : "Stanford Dogs 120 [22] contains 120 categories of dogs.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 13,
      "context" : "We do not use the parts information during selective joint fine-tuning, and use the commonly used mean class accuracy to evaluate the performance as in [14].",
      "startOffset" : 152,
      "endOffset" : 156
    }, {
      "referenceID" : 23,
      "context" : "4%, which is already better than most previous stateof-the-art results except [24].",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 14,
      "context" : "It shows that the 152-layer residual network [15, 16] pre-trained on the ImageNet dataset [40] has a strong generalization capability on this fine-grained classification task.",
      "startOffset" : 45,
      "endOffset" : 53
    }, {
      "referenceID" : 15,
      "context" : "It shows that the 152-layer residual network [15, 16] pre-trained on the ImageNet dataset [40] has a strong generalization capability on this fine-grained classification task.",
      "startOffset" : 45,
      "endOffset" : 53
    }, {
      "referenceID" : 39,
      "context" : "It shows that the 152-layer residual network [15, 16] pre-trained on the ImageNet dataset [40] has a strong generalization capability on this fine-grained classification task.",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 23,
      "context" : "3% higher than the result reported in [24], which expands the original target training set using Google image search.",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 32,
      "context" : "Oxford Flowers 102 [33] consists of 102 flower categories.",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 49,
      "context" : "MPP [50] 91.",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 0,
      "context" : "3 Multi-model feature contact [1] 91.",
      "startOffset" : 30,
      "endOffset" : 33
    }, {
      "referenceID" : 38,
      "context" : "3 MagNet [39] 91.",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 22,
      "context" : "4 VGG-19 + GoogleNet + AlexNet [23] 94.",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 41,
      "context" : "VGG-19 + Part Constellation Model [42] 95.",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 38,
      "context" : "3% higher than previous best result from a single network [39].",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 22,
      "context" : "3% higher than previous best ensemble performance reported in [23].",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 41,
      "context" : "[42] used the validation set in this dataset as additional training data.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 41,
      "context" : "To verify the effectiveness of our joint fine-tuning strategy, we have also conducted experiments with this setting during training and our result from a single network outperforms that of [42] by 1.",
      "startOffset" : 189,
      "endOffset" : 193
    }, {
      "referenceID" : 13,
      "context" : "Caltech 256 [14] has 256 object categories and 1 background cluster class.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 45,
      "context" : "We follow the testing procedure in [46] to compare with stateof-the-art results.",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 36,
      "context" : "MIT Indoor 67 [37] has 67 scene categories.",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 39,
      "context" : "Since MIT Indoor 67 is a scene dataset, in addition to the ImageNet ILSVRC 2012 training set [40], the Places-205 training set [53] is also a potential source domain.",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 52,
      "context" : "Since MIT Indoor 67 is a scene dataset, in addition to the ImageNet ILSVRC 2012 training set [40], the Places-205 training set [53] is also a potential source domain.",
      "startOffset" : 127,
      "endOffset" : 131
    }, {
      "referenceID" : 2,
      "context" : "M-HMP [3] 40.",
      "startOffset" : 6,
      "endOffset" : 9
    }, {
      "referenceID" : 51,
      "context" : "Net [52] 65.",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 20,
      "context" : "3 VGG-19 [21] 85.",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 22,
      "context" : "3 VGG-19 + GoogleNet +AlexNet [23] 86.",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 20,
      "context" : "1 VGG-19 + VGG-16 [21] 86.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 46,
      "context" : "MetaObject-CNN [47] 78.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 49,
      "context" : "9 MPP + DFSL [50] 80.",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 5,
      "context" : "8 VGG-19 + FV [6] 81.",
      "startOffset" : 14,
      "endOffset" : 17
    }, {
      "referenceID" : 22,
      "context" : "0 VGG-19 + GoogleNet [23] 84.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 16,
      "context" : "7 Multi scale + multi model ensemble [17] 86.",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 5,
      "context" : "8% higher than previous best result from a single network [6].",
      "startOffset" : 58,
      "endOffset" : 61
    }, {
      "referenceID" : 16,
      "context" : "9% higher than previous best result from an ensemble model [17].",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 21,
      "context" : "We perform an ablation study on both Stanford Dogs 120 [22] and Oxford Flowers 102 [33] by replacing or removing a single component from our pipeline.",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 32,
      "context" : "We perform an ablation study on both Stanford Dogs 120 [22] and Oxford Flowers 102 [33] by replacing or removing a single component from our pipeline.",
      "startOffset" : 83,
      "endOffset" : 87
    } ],
    "year" : 2017,
    "abstractText" : "Deep neural networks require a large amount of labeled training data during supervised learning. However, collecting and labeling so much data might be infeasible in many cases. In this paper, we introduce a source-target selective joint fine-tuning scheme for improving the performance of deep learning tasks with insufficient training data. In this scheme, a target learning task with insufficient training data is carried out simultaneously with another source learning task with abundant training data. However, the source learning task does not use all existing training data. Our core idea is to identify and use a subset of training images from the original source learning task whose low-level characteristics are similar to those from the target learning task, and jointly fine-tune shared convolutional layers for both tasks. Specifically, we compute descriptors from linear or nonlinear filter bank responses on training images from both tasks, and use such descriptors to search for a desired subset of training samples for the source learning task. Experiments demonstrate that our selective joint finetuning scheme achieves state-of-the-art performance on multiple visual classification tasks with insufficient training data for deep learning. Such tasks include Caltech 256, MIT Indoor 67, Oxford Flowers 102 and Stanford Dogs 120. In comparison to fine-tuning without a source domain, the proposed method can improve the classification accuracy by 2% 10% using a single model.",
    "creator" : "LaTeX with hyperref package"
  }
}