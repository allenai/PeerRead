{
  "name" : "1702.04488.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Transfer Learning for Low-Resource Chinese Word Segmentation with a Novel Neural Network",
    "authors" : [ "Jingjing Xu" ],
    "emails" : [ "xusun}@pku.edu.cn" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Chinese word segmentation (CWS) is an important step in Chinese natural language processing.\n1Our code is publicly available at https://github.com/jincy520/Low-Resource-CWS\nThe most widely used approaches (Xue and Shen, 2003; Peng et al., 2004) treat CWS as a sequence labelling problem in which each character is assigned with a tag. Formally, given an input sequence x = x1x2...xn, it produces a tag sequence y = y1y2...yn. Many exsiting techniques, such as conditional random fields, have been successfully applied to CWS (Lafferty et al., 2001; Tseng, 2005; Zhao et al., 2010; Sun and Xu, 2011; Sun et al., 2014). However, these approaches incorporate many handcrafted features. Therefore, the generalization ability is restricted.\nIn recent years, neural networks have become increasingly popular in CWS, which focused more on the ability of automated feature extraction. Collobert et al. (2011) developed a general neural architecture for sequence labelling tasks. Pei et al. (2014) used convolutioanl neural networks to capture local features within a fixed size window. Chen et al. (2015a) proposed gated recursive neural networks to model feature combinations. The gating mechanism was also used by Cai and Zhao (2016).\nHowever, this success relies on massive labelled data and are less effective on low-resource datasets. The major problem is that a small amount of labelled data leads to inadequate training and negatively impacts the ability of generalization. However, there are enough corpora which consist of massive annotated texts. All can be used to improve the task. Thus, we propose a transfer learning method to address the problem by leveraging high-resource datasets.\nFirst, we train a teacher model on high-resource datasets and then use the learned knowledge to initialize a student model. Previous neural network models usually use random initialization which relies on massive labelled data. It is hard for a randomly initialized model to achieve the expected results on low-resource datasets. Motivated by\nar X\niv :1\n70 2.\n04 48\n8v 1\n[ cs\n.C L\n] 1\n5 Fe\nb 20\n17\nthat, we propose a teacher-student framework to initialize the student model. Second, the student model is trained on a low-resource dataset with the help of high-resource corpora. However, it is hard to directly make use of high-resource datasets to train the student model because different corpora have different data distributions. The shift of data distributions is a major problem. To address the problem, we propose a weighted data similarity method which computes a similarity of each highresource sample with a low-resource dataset. Experiment results show that using our transfer learning method, we substantially improve results on low-resource datasets.\nOn the other hand, insufficient data puts forward higher requirements for feature extraction. A single feature trained on insufficient training data is weak. Our key idea is to combine several kinds of weak features to achieve the better performance. Motivated by that, we propose an unified globallocal neural network (UGL) in this paper and apply our transfer learning method to it. Compared with previous networks, our network has an advantage of capturing abundant and useful features. Unlike our transfer learning method which is proposed from the training’s point of view, the new neural network is proposed to improve the task in view of the model structure. Experiment results show that our model achieves large improvements on low-resource datasets.\nWith the increase of layers which are designed to improve the ability of feature extraction, the training speed is becoming a limit. To speed up training, we explore mini-batch asynchronous parallel (MAP) learning on neural segmentation in this paper. Existing asynchronous parallel learning methods are mainly for sparse models (Recht et al., 2011; Mcmahan and Streeter, 2014). For\ndense models, like neural networks, asynchronous parallel methods bring inevitable gradient noises. However, the theoretical analysis by Sun (2016) showed that the learning process with gradient errors can still be convergent on neural models. Motivated by that, we explore the MAP approach on neural segmentation in this paper. The parallel method accelerates training substantially and is almost five times faster than a serial mode.\nThe main contributions of the paper are as follows:\n• A transfer learning method is proposed to improve low-resource word segmentation by leveraging high-resource corpora.\n• The transfer learning method is realized through a novel neural network which improves feature learning.\n• To speed up training, we explore minibatch asynchronous parallel learning on neural word segmentation in this paper."
    }, {
      "heading" : "2 Transfer Learning by Leveraging High-Resource Datasets",
      "text" : "Table 1 shows that previous neural word segmentation models are less effective on low-resource datasets since these models only focus on indomain supervised learning. Furthermore, there are enough corpora which consist of massive annotated texts. For scenarios where we have insufficient labelled data, transfer learning is an effective way to improve the task. Motivated by that, we propose a transfer learning method to leverage high-resource corpora.\nFirst, we propose a teacher-student framework to initialize a model with the learned knowledge. We train a teacher model on a dataset where there\nis a large amount of training data (e.g., MSR). The learned parameters are used to initialize a student model. Therefore, the student model is trained from the learned parameters, rather than randomly initialization.\nSecond, the student model is trained on lowresource datasets with the help of high-resource corpora. However, since different corpora have different data distributions, it is hard to directly make use of high-resource datasets to train the student model. Thus, to avoid the shift of data distributions, high-resource corpora are used to train the student model based on the weighted data similarity method. This method identifies the similarity of each high-resource sample with a lowresource dataset. We use different learning rates for different samples. A learning rate is adjusted by the weighted data similarity automatically. The weighted data similarity wti is updated as follows.\nFirst, calculate the update rate at:\net = (1− 2 ∗ p t ∗ rt\npt + rt ) (1)\nat = 1\n2 log\n1− et\net (2)\nwhere pt and rt are precision and recall of the student model on high-resource data. The update rate at is determined by the error rate et. The error rate is a simple and effective way to evaluate the data similarity.\nNext, update the data similarity after t iterations:\nSt+1 = (wt+11 , . . ., w t+1 i , . . ., w t+1 N ) (3)\nwt+1i = wti\nZt ∗m m∑ j=1 exp(atI(yi,j = pi,j)) (4)\nwhere m is the length of sample i, I() is the indicator function which evaluates if the prediction pi,j is equal with the gold label yi,j and Zt is the regularization factor which is computed as:\nZt = ∑ i wti m m∑ j=1 exp(atI(yi,j = pi,j)) (5)\nFinally, the weighted data similarity is used to compute the learning rate αti:\nαti = α t ∗ wti (6)\nwhere αt is the fixed learning rate for a lowresource dataset, wti indicates the similarity between sentence i and a low-resource corpus, which is from 0 to 1."
    }, {
      "heading" : "3 Unified Global-Local Neural Networks for Feature Extraction",
      "text" : "Insufficient data puts forward higher requirements for feature extraction. Our key idea is to combine several kinds of weak features to achieve the better performance. Unlike previous networks which focus on a single kind of feature: either complicated local features or global dependencies, our network has an advantage of combing complicated local features with long dependencies together. Both of them are necessary for CWS and should not be neglected. Our network is built on a simple encoderdecoder structure. A encoder is designed to model local combinations and a decoder is used to capture long distance dependencies.\nFigure 1 illustrates the model architecture. First, words are represented by embeddings stored in a lookup table D|v|∗d where v is the number of words in the vocabulary and d is the embedding size. The lookup table is pre-trained on gigaword corpus where unknown words are mapped to a special symbol. The inputs to our model are x1, x2, ..., xn which are represented by D = Dx1 , Dx2,...,Dxn .\nWe first extract a window context H0∈Rn,k,d from an input sequence which is padded with special symbols according to the window size:\nH0i,j = D[i+ j] (7)\nwhere n is the sentence length, k is the window size and d is the embedding length. H0 will be input to the encoder to produce complicated local feature representations.\nEncoder. The encoder is composed by filter recursive networks. Unlike GRNN (Chen et al., 2015a) which has a limit that inputs must be two vectors, filter recursive networks are proposed to break this limit by introducing filter mechanism which controls the input size. Motivated by Chen et al. (2015a) and Cai and Zhao (2016), the gating mechanism has been shown effective to model feature combinations. Thus, we adjust the gate function to fit our model. According to the filter size, we first choose every patch and input it to gate function to get next layer H1∈Rn,k−f1+1,d where f1 is the filter size of 1th hidden layer.\nIn a gate cell of filter recursive networks, output H1 of the ith, jth hidden node is computed as:\nH1i,j = zh h ′ + f1−1∑ di=0 (zdi H 0 i,j+di) (8)\nwhere zh and zdi are update gates for new activation h ′ and inputs, while means element-wise multiplication. To simplify the cell, zh, zdi are computed as: zh z0 ... zdi ...\nzf1−1\n = sigmoid(U \nh ′\nH0i,j . . .\nH0i,j+di . . .\nH0i,j+f1−1\n ) (9)\nwhere U∈R(f1+1)d∗(f1+1)d and the new activation h ′ is computed as:\nh ′ = tanh(W  r0 H0i,j . . . rdi H0i,j+di . . .\nrf1−1 H0i,j+f1−1\n) (10)\nwhere W∈Rd∗f1d and r0, ..., rf1−1 are reset gates for inputs, which can be formalized as:\nr0 . . . rdi . . . rf1−1\n = sigmoid(G \nH0i,j . . .\nH0i,j+di . . .\nH0i,j+f1−1\n) (11)\nThese operations will repeat until we get H l∈Rn,1,d which is reduced dimension to H l∈Rn,d.\nDecoder. The decoder is composed by bidirectional long shor-term memory network (BiLSTM) (Hochreiter and Schmidhuber 1997). The local features encoded by filter recursive neural networks are refined into global dependencies and then decoded to tag sequences in this stage."
    }, {
      "heading" : "4 Mini-Batch Asynchronous Parallel Learning",
      "text" : "With the development of multicore computers, there is a growing interest in parallel techniques.\nResearchers have proposed several schemes (Zhao and Huang, 2013; Zinkevich et al., 2010), but most of them require locking so the speedup is limited. Asynchronous parallel learning methods without locking can maximize the speedup ratio. However, existing asynchronous parallel learning methods are mainly for sparse models. For dense models, like neural networks, asynchronous parallel learning brings gradient noises which are very common and inevitable (Figure 2). Read-read conflicts break the sequentiality of training procedure, read-write and write-write conflicts lead to incorrect gradients. Nevertheless, Sun (2016) proved that the learning process with gradient errors can still be convergent. Motivated by that, we train our model in the asynchronous parallel way.\nWe find that Adam (Kingma and Ba, 2014) is a practical method to train large neural networks. Then, we run the asynchronous parallel method on Adam training algorithm.\nFirst, it recursively calculates mt and vt, based on the gradient gt. β1 and β2 are used to control the ratio of previous states.\nmt = β1 ·mt−1 + (1− β1) · gt (12)\nvt = β2 · vt−1 + (1− β2) · g2t (13)\nSecond, ∆Wt is calculated based on vt and mt. and µ are both smooth parameters.\nM(w, t) = vt −m2t (14)\n∆wt = gt,i√\nM(w, t) + µ (15)\nFinally, weight matrix is updated based on ∆wt. The parameter Θt+1 at time step t+ 1 is:\nΘt+1 = Θt −∆wt (16)\nFollowing experimental results on development datasets, hyper parameters of optimization method are set as follows: β1 = β2 = 0.95, = 1× 10−4.\nThe training algorithm is realized without any locking (see Table 2). For each mini-batch, we uniformly distribute it into different processors. Processors compute the increment of gradient ∆wt in parallel, where wt is stored in a shared memory and each processor can read and update it.\nWe find that each processor always waits the end of the slowest processor because the length\nof sentence varies. Motivated by structure regularization (Sun, 2014), we split each sentence into several fixed-length mini-sentences. The task of each processor is almost the same, so the waiting time would be reduced greatly."
    }, {
      "heading" : "5 Experiments",
      "text" : "The proposed model is evaluated on three datasets: MSR, PKU and CTB. Table 3 shows the details of these datasets. We treat MSR as a highresource dataset, PKU and CTB as low-resurce datasets. MSR and PKU are provided by the second International Chinese Word Segmentation Bakeoff (Emerson, 2005). CTB is from Chinese TreeBank 8.02 and split to training and testing sets in this paper. We randomly split 10% of the training sets to development sets which are used to choose the suitable hyper-parameters.\nAll idioms, numbers and continuous English\n2https://catalog.ldc.upenn.edu/LDC2013T21\ncharacters are replaced to special flags. The improvements achieved by an idiom dictionary are very limited, less than 0.1% F-score on all datasets. The character embeddings are pretrained on Chinese gigaword3 by word2vec4.\nAll results are evaluated by F1-score which is computed by the standard Bakeoff scoring program. Besides, we also consider an error rate reduction as one of the evaluation criterion. The error reduction evaluates what percentage of errors are corrected. The error rate reduction R between model a and model b is calculated as follows:\nR = ea − eb ea\n(17)\nwhere ea and eb are error rates of model a and model b. ea and eb are computed as:\nea = 1− fa (18)\neb = 1− fb (19)\nwhere fa and fb are F-score values of model a and model b."
    }, {
      "heading" : "5.1 Setup",
      "text" : "Hyper-parameters are set according to the performance on development sets. We evaluate the minibatch size m in a serial mode and choose m = 16. Similarly, the window size w is set as 5, the fixed learning rate α is set as 0.01, the dimension of\n3https://catalog.ldc.upenn.edu/LDC2003T09 4https://code.google.com/archive/p/word2vec\ncharacter embeddings and hidden layers d is set as 100. d = 100 is a good balance between model speed and performance.\nInspired by Pei et al. (2014), bigram features are applied to our model as well. Specifically, each bigram embedding is represented as a single vector. Bigram embeddings are initialized randomly. We ignore lots of bigram features which only appear once or twice since these bigram features not only are useless, but also make a bigram lookup table huge.\nAll experiments are performed on a commodity 64-bit Dell Precision T5810 workstation with one 3.0GHz 16-core CPU and 64GB RAM. The C# multiprocessing module is used in this paper."
    }, {
      "heading" : "5.2 Results and Discussions",
      "text" : "Table 5 shows the improvements of our proposal. The proposal is compared with Bi-LSTM which is a competitive and widely used model for neural word segmentation. Experiment result show that our proposal achieves substantial improvements on low-resource datasets: 2.3% and 1.5% F-score on PKU and CTB datasets. Besides, the error rate is decreased by 34.3% and 26.3%.\nTransfer Learning. The improvements of transfer learning are shown in Table 5. We choose MSR as a high-resource dataset. Results on PKU and CTB datasets all show improvements: 1.0% Fscore on PKU dataset and 0.5% on CTB dataset. A high-resource dataset not only decreases the number of out-of-vocabulary words, but also improves results of in-vocabulary words. The size of PKU dataset is far less than that of CTB dataset and we achieve the better improvements on PKU dataset. It shows that our transfer learning method is more efficient on datasets with fewer resource.\nUnified Global-Local Neural Networks. We reconstruct some of state-of-the-art neural models in this paper: Bi-LSTM and GRNN. Bi-LSTM is used for capturing global and simple local features. GRNN is used for capturing complicated local features. Table 4 shows that our model out-\nperforms baselines on low-resource datasets: PKU and CTB. It proves that combining several weak features is an effective way to improve the performance on low-resource datasets.\nBesides, we compare our model with baselines on a high-resource dataset: MSR. Table 8 shows that our model achieves the best results. It can prove that our model is also applied to highresource datasets.\nComparisons with State-of-the-art Models. Table 6 shows comparisons between our work and latest neural models on low-resource datasets: PKU and CTB. Experiment results show that our work largely outperforms state-of-the-art models which are very competitive. Given that a dictionary used in Chen et al. (2015a) is not publicly released, our work is not comparable with it.\nWe also compare our work with traditional models on low-resource datasets: PKU and CTB, several of which take advantage of a variety of feature templates and dictionaries. As shown in Table\n7, our work achieves state-of-the-art results. Although our model only uses simple bigram features, it outperforms the previous state-of-theart methods which use more complex features.\nMini-Batch Asynchronous Parallel Learning. We run the proposed model in asynchronous, synchronous and serial modes to analyze the parallel efficiency. The number of threads used in asynchronous and synchronous modes is 15. The comparisons are shown in Figure 3 and 4. It can be clearly seen that the asynchronous algorithm achieves the best speedup ratio without decreasing F-score compared with synchronous and serial algorithms. The asynchronous parallel algorithm is almost 5x faster than the serial algorithm."
    }, {
      "heading" : "6 Related Work",
      "text" : "Next, we briefly review neural word segmentation, transfer learning in CWS and asynchronous parallel learning.\nNeural Word Segmentation. Zheng et al. (2013) used a two-layer network and adapted a general neural network architecture in Collobert et al. (2011). Pei et al. (2014) used a tensor framework to capture feature combination. Chen et al. (2015a) proposed gated recursive neural networks to model feature combinations of context characters. Chen et al. (2015b) used LSTM to model long distance dependencies in a sentence.\nZhang et al. (2016) applied the transition-based neural framework to Chinese segmentation. Cai and Zhao (2016) proposed a novel neural framework which thoroughly eliminated context windows and utilized complete segmentation history.\nTransfer Learning in CWS. Sun and Xu (2011) introduced many statistical features from unlabeled in-domain data to enhance supervised method in CWS. Zhang et al.(2014) used type-supervised domain adaptation for joint Chinese word segmentation and POS-tagging. Liu et al. (2014) adopted freely available data to help improve the performance on CWS. However, these transfer learning methods involved in too many handcrafted features. In our work, deep neural networks are used to reduce handcrafted efforts.\nAsynchronous Parallel Learning. Recently, a viriety of asynchronous parallel learning methods have been developed (Recht et al., 2011; Mcmahan and Streeter, 2014). Those asynchronous methods have shown to be more effective than synchronous parallel learning. However, existing asynchronous parallel learning methods are\nmainly for sparse parameter models to avoid the problem of gradient error. For dense parameter models like neural networks, asynchronous parallel methods bring gradient errors. The theoretical analysis work of Sun (2016) showed that the learning process with gradient errors can still be convergent on neural models."
    }, {
      "heading" : "7 Conclusions",
      "text" : "The major problem of low-resource word segmentation is insufficient training data. Thus, we propose a transfer learning method to improve the task by leveraging high-resource datasets. First, it is hard for a randomly initialized model to achieve the expected results on low-resource datasets. Therefore, we propose a teacher-student framework to initialize a student model. Second, the student model is trained on a low-resource dataset with the help of high-resource corpora. A weighted data similarity is proposed to avoid the shift of data distributions. Our transfer learning method is realised through a novel neural network which improves feature learning. Experiment results show that our work largely improves the performance on low-resource datasets compared with state-of-the-art models. Finally, our parallel training method brings substantial speedup and is almost 5x faster than a serial mode."
    } ],
    "references" : [ {
      "title" : "Neural word segmentation learning for chinese",
      "author" : [ "Deng Cai", "Hai Zhao." ],
      "venue" : "Meeting of the Association for Computational Linguistics.",
      "citeRegEx" : "Cai and Zhao.,? 2016",
      "shortCiteRegEx" : "Cai and Zhao.",
      "year" : 2016
    }, {
      "title" : "Gated recursive neural network for chinese word segmentation",
      "author" : [ "Xinchi Chen", "Xipeng Qiu", "Chenxi Zhu", "Xuanjing Huang." ],
      "venue" : "ACL (1). The Association for Computer Linguistics, pages 1744–1753.",
      "citeRegEx" : "Chen et al\\.,? 2015a",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2015
    }, {
      "title" : "Long short-term memory neural networks for chinese word segmentation",
      "author" : [ "Xinchi Chen", "Xipeng Qiu", "Chenxi Zhu", "Pengfei Liu", "Xuanjing Huang." ],
      "venue" : "EMNLP. The Association for Computational Linguistics, pages 1197–1206.",
      "citeRegEx" : "Chen et al\\.,? 2015b",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2015
    }, {
      "title" : "Natural language processing (almost) from scratch",
      "author" : [ "Ronan Collobert", "Jason Weston", "Léon Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa." ],
      "venue" : "J. Mach. Learn. Res. 12:2493–2537.",
      "citeRegEx" : "Collobert et al\\.,? 2011",
      "shortCiteRegEx" : "Collobert et al\\.",
      "year" : 2011
    }, {
      "title" : "The second international chinese word segmentation bakeoff",
      "author" : [ "Thomas Emerson." ],
      "venue" : "Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing. pages 123–133.",
      "citeRegEx" : "Emerson.,? 2005",
      "shortCiteRegEx" : "Emerson.",
      "year" : 2005
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik Kingma", "Jimmy Ba." ],
      "venue" : "Computer Science .",
      "citeRegEx" : "Kingma and Ba.,? 2014",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
      "author" : [ "John D. Lafferty", "Andrew McCallum", "Fernando C.N. Pereira." ],
      "venue" : "Proceedings of the Eighteenth International Conference on Machine Learning. Num-",
      "citeRegEx" : "Lafferty et al\\.,? 2001",
      "shortCiteRegEx" : "Lafferty et al\\.",
      "year" : 2001
    }, {
      "title" : "Domain adaptation for crf-based chinese word segmentation using free annotations",
      "author" : [ "Yijia Liu", "Yue Zhang", "Wanxiang Che", "Ting Liu", "Fan Wu." ],
      "venue" : "Alessandro Moschitti, Bo Pang, and Walter Daelemans, editors, EMNLP. ACL, pages 864–874.",
      "citeRegEx" : "Liu et al\\.,? 2014",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2014
    }, {
      "title" : "Accurate linear-time chinese word segmentation via embedding matching",
      "author" : [ "Jianqiang Ma", "Erhard W Hinrichs." ],
      "venue" : "ACL (1). pages 1733–1743.",
      "citeRegEx" : "Ma and Hinrichs.,? 2015",
      "shortCiteRegEx" : "Ma and Hinrichs.",
      "year" : 2015
    }, {
      "title" : "Delaytolerant algorithms for asynchronous distributed online learning",
      "author" : [ "H.B. Mcmahan", "M. Streeter." ],
      "venue" : "Advances in Neural Information Processing Systems 4:2915–2923.",
      "citeRegEx" : "Mcmahan and Streeter.,? 2014",
      "shortCiteRegEx" : "Mcmahan and Streeter.",
      "year" : 2014
    }, {
      "title" : "Maxmargin tensor neural network for chinese word segmentation",
      "author" : [ "Wenzhe Pei", "Tao Ge", "Baobao Chang." ],
      "venue" : "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computa-",
      "citeRegEx" : "Pei et al\\.,? 2014",
      "shortCiteRegEx" : "Pei et al\\.",
      "year" : 2014
    }, {
      "title" : "Chinese segmentation and new word detection using conditional random fields",
      "author" : [ "Fuchun Peng", "Fangfang Feng", "Andrew McCallum." ],
      "venue" : "Proceedings of the 20th International Conference on Computational Linguistics. Association for Computational",
      "citeRegEx" : "Peng et al\\.,? 2004",
      "shortCiteRegEx" : "Peng et al\\.",
      "year" : 2004
    }, {
      "title" : "Hogwild: A lock-free approach to parallelizing stochastic gradient descent",
      "author" : [ "Benjamin Recht", "Christopher Ré", "Stephen J. Wright", "Feng Niu." ],
      "venue" : "NIPS. pages 693–701.",
      "citeRegEx" : "Recht et al\\.,? 2011",
      "shortCiteRegEx" : "Recht et al\\.",
      "year" : 2011
    }, {
      "title" : "Enhancing chinese word segmentation using unlabeled data",
      "author" : [ "Weiwei Sun", "Jia Xu." ],
      "venue" : "Conference",
      "citeRegEx" : "Sun and Xu.,? 2011",
      "shortCiteRegEx" : "Sun and Xu.",
      "year" : 2011
    }, {
      "title" : "Structure regularization for structured prediction",
      "author" : [ "Xu Sun." ],
      "venue" : "Advances in Neural Information Processing Systems 27. pages 2402–2410.",
      "citeRegEx" : "Sun.,? 2014",
      "shortCiteRegEx" : "Sun.",
      "year" : 2014
    }, {
      "title" : "Asynchronous parallel learning for neural networks and structured models with dense features",
      "author" : [ "Xu Sun." ],
      "venue" : "COLING.",
      "citeRegEx" : "Sun.,? 2016",
      "shortCiteRegEx" : "Sun.",
      "year" : 2016
    }, {
      "title" : "Feature-frequency-adaptive on-line training for fast and accurate natural language processing",
      "author" : [ "Xu Sun", "Wenjie Li", "Houfeng Wang", "Qin Lu." ],
      "venue" : "Computational Linguistics 40(3):563–586.",
      "citeRegEx" : "Sun et al\\.,? 2014",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2014
    }, {
      "title" : "Fast online training with frequency-adaptive learning rates for chinese word segmentation and new word detection",
      "author" : [ "Xu Sun", "Houfeng Wang", "Wenjie Li." ],
      "venue" : "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics",
      "citeRegEx" : "Sun et al\\.,? 2012",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2012
    }, {
      "title" : "A conditional random field word segmenter",
      "author" : [ "Huihsin Tseng." ],
      "venue" : "In Fourth SIGHAN Workshop on Chinese Language Processing.",
      "citeRegEx" : "Tseng.,? 2005",
      "shortCiteRegEx" : "Tseng.",
      "year" : 2005
    }, {
      "title" : "Chinese Word Segmentation as LMR Tagging",
      "author" : [ "N. Xue", "L. Shen." ],
      "venue" : "Proceedings of the 2nd SIGHAN Workshop on Chinese Language Processing.",
      "citeRegEx" : "Xue and Shen.,? 2003",
      "shortCiteRegEx" : "Xue and Shen.",
      "year" : 2003
    }, {
      "title" : "Type-supervised domain adaptation for joint segmentation and pos-tagging",
      "author" : [ "Meishan Zhang", "Yue Zhang", "Wanxiang Che", "Ting Liu." ],
      "venue" : "EACL. pages 588–597.",
      "citeRegEx" : "Zhang et al\\.,? 2014",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2014
    }, {
      "title" : "Transition-based neural word segmentation",
      "author" : [ "Meishan Zhang", "Yue Zhang", "Guohong Fu." ],
      "venue" : "Meeting of the Association for Computational Linguistics. pages 421–431.",
      "citeRegEx" : "Zhang et al\\.,? 2016",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2016
    }, {
      "title" : "Subword-based tagging by conditional random fields for chinese word segmentation",
      "author" : [ "Ruiqiang Zhang", "Genichiro Kikui", "Eiichiro Sumita." ],
      "venue" : "Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume:",
      "citeRegEx" : "Zhang et al\\.,? 2006",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2006
    }, {
      "title" : "Chinese segmentation with a word-based perceptron algorithm",
      "author" : [ "Yue Zhang", "Stephen Clark." ],
      "venue" : "Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics. Association for Computational Linguistics, Prague, Czech Re-",
      "citeRegEx" : "Zhang and Clark.,? 2007",
      "shortCiteRegEx" : "Zhang and Clark.",
      "year" : 2007
    }, {
      "title" : "A unified character-based tagging framework for chinese word segmentation",
      "author" : [ "Hai Zhao", "Changning Huang", "Mu Li", "Bao-Liang Lu." ],
      "venue" : "ACM Trans. Asian Lang. Inf. Process. 9(2).",
      "citeRegEx" : "Zhao et al\\.,? 2010",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2010
    }, {
      "title" : "Minibatch and parallelization for online large margin structured learning",
      "author" : [ "Kai Zhao", "Liang Huang." ],
      "venue" : "HLT-NAACL. The Association for Computational Linguistics, pages 370–379.",
      "citeRegEx" : "Zhao and Huang.,? 2013",
      "shortCiteRegEx" : "Zhao and Huang.",
      "year" : 2013
    }, {
      "title" : "Deep learning for chinese word segmentation and pos tagging",
      "author" : [ "Xiaoqing Zheng", "Hanyang Chen", "Tianyu Xu." ],
      "venue" : "EMNLP. ACL, pages 647–657.",
      "citeRegEx" : "Zheng et al\\.,? 2013",
      "shortCiteRegEx" : "Zheng et al\\.",
      "year" : 2013
    }, {
      "title" : "Parallelized stochastic gradient descent",
      "author" : [ "Martin Zinkevich", "Markus Weimer", "Lihong Li", "Alex J. Smola." ],
      "venue" : "J.D. Lafferty, C.K.I. Williams, J. Shawe-Taylor, R.S. Zemel, and A. Culotta, editors, Advances in Neural Information Processing",
      "citeRegEx" : "Zinkevich et al\\.,? 2010",
      "shortCiteRegEx" : "Zinkevich et al\\.",
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 19,
      "context" : "com/jincy520/Low-Resource-CWS The most widely used approaches (Xue and Shen, 2003; Peng et al., 2004) treat CWS as a sequence labelling problem in which each character is assigned with a tag.",
      "startOffset" : 62,
      "endOffset" : 101
    }, {
      "referenceID" : 11,
      "context" : "com/jincy520/Low-Resource-CWS The most widely used approaches (Xue and Shen, 2003; Peng et al., 2004) treat CWS as a sequence labelling problem in which each character is assigned with a tag.",
      "startOffset" : 62,
      "endOffset" : 101
    }, {
      "referenceID" : 6,
      "context" : "Many exsiting techniques, such as conditional random fields, have been successfully applied to CWS (Lafferty et al., 2001; Tseng, 2005; Zhao et al., 2010; Sun and Xu, 2011; Sun et al., 2014).",
      "startOffset" : 99,
      "endOffset" : 190
    }, {
      "referenceID" : 18,
      "context" : "Many exsiting techniques, such as conditional random fields, have been successfully applied to CWS (Lafferty et al., 2001; Tseng, 2005; Zhao et al., 2010; Sun and Xu, 2011; Sun et al., 2014).",
      "startOffset" : 99,
      "endOffset" : 190
    }, {
      "referenceID" : 24,
      "context" : "Many exsiting techniques, such as conditional random fields, have been successfully applied to CWS (Lafferty et al., 2001; Tseng, 2005; Zhao et al., 2010; Sun and Xu, 2011; Sun et al., 2014).",
      "startOffset" : 99,
      "endOffset" : 190
    }, {
      "referenceID" : 13,
      "context" : "Many exsiting techniques, such as conditional random fields, have been successfully applied to CWS (Lafferty et al., 2001; Tseng, 2005; Zhao et al., 2010; Sun and Xu, 2011; Sun et al., 2014).",
      "startOffset" : 99,
      "endOffset" : 190
    }, {
      "referenceID" : 16,
      "context" : "Many exsiting techniques, such as conditional random fields, have been successfully applied to CWS (Lafferty et al., 2001; Tseng, 2005; Zhao et al., 2010; Sun and Xu, 2011; Sun et al., 2014).",
      "startOffset" : 99,
      "endOffset" : 190
    }, {
      "referenceID" : 0,
      "context" : "Collobert et al. (2011) developed a general neural architecture for sequence labelling tasks.",
      "startOffset" : 0,
      "endOffset" : 24
    }, {
      "referenceID" : 0,
      "context" : "Collobert et al. (2011) developed a general neural architecture for sequence labelling tasks. Pei et al. (2014) used convolutioanl neural networks to capture local features within a fixed size window.",
      "startOffset" : 0,
      "endOffset" : 112
    }, {
      "referenceID" : 0,
      "context" : "Chen et al. (2015a) proposed gated recursive neural networks to model feature combinations.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 0,
      "context" : "The gating mechanism was also used by Cai and Zhao (2016).",
      "startOffset" : 38,
      "endOffset" : 58
    }, {
      "referenceID" : 17,
      "context" : "MSR 86919 3985 Zhang and Clark (2007) 97.",
      "startOffset" : 15,
      "endOffset" : 38
    }, {
      "referenceID" : 12,
      "context" : "2 Sun et al. (2012) 97.",
      "startOffset" : 2,
      "endOffset" : 20
    }, {
      "referenceID" : 9,
      "context" : "4 Pei et al. (2014) 97.",
      "startOffset" : 2,
      "endOffset" : 20
    }, {
      "referenceID" : 0,
      "context" : "2 Cai and Zhao (2016) 96.",
      "startOffset" : 2,
      "endOffset" : 22
    }, {
      "referenceID" : 17,
      "context" : "PKU 19055 1945 Zhang and Clark (2007) 94.",
      "startOffset" : 15,
      "endOffset" : 38
    }, {
      "referenceID" : 12,
      "context" : "5 Sun et al. (2012) 95.",
      "startOffset" : 2,
      "endOffset" : 20
    }, {
      "referenceID" : 9,
      "context" : "4 Pei et al. (2014) 95.",
      "startOffset" : 2,
      "endOffset" : 20
    }, {
      "referenceID" : 0,
      "context" : "2 Cai and Zhao (2016) 95.",
      "startOffset" : 2,
      "endOffset" : 22
    }, {
      "referenceID" : 12,
      "context" : "Existing asynchronous parallel learning methods are mainly for sparse models (Recht et al., 2011; Mcmahan and Streeter, 2014).",
      "startOffset" : 77,
      "endOffset" : 125
    }, {
      "referenceID" : 9,
      "context" : "Existing asynchronous parallel learning methods are mainly for sparse models (Recht et al., 2011; Mcmahan and Streeter, 2014).",
      "startOffset" : 77,
      "endOffset" : 125
    }, {
      "referenceID" : 9,
      "context" : ", 2011; Mcmahan and Streeter, 2014). For dense models, like neural networks, asynchronous parallel methods bring inevitable gradient noises. However, the theoretical analysis by Sun (2016) showed that the learning process with gradient errors can still be convergent on neural models.",
      "startOffset" : 8,
      "endOffset" : 189
    }, {
      "referenceID" : 1,
      "context" : "Unlike GRNN (Chen et al., 2015a) which has a limit that inputs must be two vectors, filter recursive networks are proposed to break this limit by introducing filter mechanism which controls the input size.",
      "startOffset" : 12,
      "endOffset" : 32
    }, {
      "referenceID" : 0,
      "context" : "Unlike GRNN (Chen et al., 2015a) which has a limit that inputs must be two vectors, filter recursive networks are proposed to break this limit by introducing filter mechanism which controls the input size. Motivated by Chen et al. (2015a) and Cai and Zhao (2016), the gating mechanism has been shown effective to model feature combinations.",
      "startOffset" : 13,
      "endOffset" : 239
    }, {
      "referenceID" : 0,
      "context" : "(2015a) and Cai and Zhao (2016), the gating mechanism has been shown effective to model feature combinations.",
      "startOffset" : 12,
      "endOffset" : 32
    }, {
      "referenceID" : 25,
      "context" : "Researchers have proposed several schemes (Zhao and Huang, 2013; Zinkevich et al., 2010), but most of them require locking so the speedup is limited.",
      "startOffset" : 42,
      "endOffset" : 88
    }, {
      "referenceID" : 27,
      "context" : "Researchers have proposed several schemes (Zhao and Huang, 2013; Zinkevich et al., 2010), but most of them require locking so the speedup is limited.",
      "startOffset" : 42,
      "endOffset" : 88
    }, {
      "referenceID" : 5,
      "context" : "We find that Adam (Kingma and Ba, 2014) is a practical method to train large neural networks.",
      "startOffset" : 18,
      "endOffset" : 39
    }, {
      "referenceID" : 13,
      "context" : "Nevertheless, Sun (2016) proved that the learning process with gradient errors can still be convergent.",
      "startOffset" : 14,
      "endOffset" : 25
    }, {
      "referenceID" : 14,
      "context" : "Motivated by structure regularization (Sun, 2014), we split each sentence into several fixed-length mini-sentences.",
      "startOffset" : 38,
      "endOffset" : 49
    }, {
      "referenceID" : 4,
      "context" : "MSR and PKU are provided by the second International Chinese Word Segmentation Bakeoff (Emerson, 2005).",
      "startOffset" : 87,
      "endOffset" : 102
    }, {
      "referenceID" : 10,
      "context" : "Inspired by Pei et al. (2014), bigram features are applied to our model as well.",
      "startOffset" : 12,
      "endOffset" : 30
    }, {
      "referenceID" : 24,
      "context" : "Unigram Zheng et al. (2013) 92.",
      "startOffset" : 8,
      "endOffset" : 28
    }, {
      "referenceID" : 9,
      "context" : "4 * * * Pei et al. (2014) 94.",
      "startOffset" : 8,
      "endOffset" : 26
    }, {
      "referenceID" : 0,
      "context" : "0 * * * Cai and Zhao (2016) 95.",
      "startOffset" : 8,
      "endOffset" : 28
    }, {
      "referenceID" : 9,
      "context" : "Bigram Pei et al. (2014) * * 95.",
      "startOffset" : 7,
      "endOffset" : 25
    }, {
      "referenceID" : 8,
      "context" : "2 * * * Ma and Hinrichs (2015) * * 95.",
      "startOffset" : 8,
      "endOffset" : 31
    }, {
      "referenceID" : 8,
      "context" : "2 * * * Ma and Hinrichs (2015) * * 95.1 * * * Zhang et al. (2016) * * 95.",
      "startOffset" : 8,
      "endOffset" : 66
    }, {
      "referenceID" : 14,
      "context" : "5 * * * Sun et al. (2012) * * 95.",
      "startOffset" : 8,
      "endOffset" : 26
    }, {
      "referenceID" : 1,
      "context" : "Given that a dictionary used in Chen et al. (2015a) is not publicly released, our work is not comparable with it.",
      "startOffset" : 32,
      "endOffset" : 52
    }, {
      "referenceID" : 22,
      "context" : "Zheng et al. (2013) used a two-layer network and adapted a general neural network architecture in Collobert et al.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 1,
      "context" : "(2013) used a two-layer network and adapted a general neural network architecture in Collobert et al. (2011). Pei et al.",
      "startOffset" : 85,
      "endOffset" : 109
    }, {
      "referenceID" : 1,
      "context" : "(2013) used a two-layer network and adapted a general neural network architecture in Collobert et al. (2011). Pei et al. (2014) used a tensor framework to capture feature combination.",
      "startOffset" : 85,
      "endOffset" : 128
    }, {
      "referenceID" : 1,
      "context" : "Chen et al. (2015a) proposed gated recursive neural networks to model feature combinations of context characters.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 1,
      "context" : "Chen et al. (2015a) proposed gated recursive neural networks to model feature combinations of context characters. Chen et al. (2015b) used LSTM to model long distance dependencies in a sentence.",
      "startOffset" : 0,
      "endOffset" : 134
    }, {
      "referenceID" : 0,
      "context" : "Cai and Zhao (2016) proposed a novel neural framework which thoroughly eliminated context win-",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 12,
      "context" : "Sun and Xu (2011) introduced many statistical features from unlabeled in-domain data to enhance supervised method in CWS.",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 12,
      "context" : "Sun and Xu (2011) introduced many statistical features from unlabeled in-domain data to enhance supervised method in CWS. Zhang et al.(2014) used type-supervised domain adaptation for joint Chinese word segmentation and POS-tagging.",
      "startOffset" : 0,
      "endOffset" : 141
    }, {
      "referenceID" : 7,
      "context" : "Liu et al. (2014) adopted freely available data to help improve the performance on CWS.",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 12,
      "context" : "Recently, a viriety of asynchronous parallel learning methods have been developed (Recht et al., 2011; Mcmahan and Streeter, 2014).",
      "startOffset" : 82,
      "endOffset" : 130
    }, {
      "referenceID" : 9,
      "context" : "Recently, a viriety of asynchronous parallel learning methods have been developed (Recht et al., 2011; Mcmahan and Streeter, 2014).",
      "startOffset" : 82,
      "endOffset" : 130
    }, {
      "referenceID" : 9,
      "context" : ", 2011; Mcmahan and Streeter, 2014). Those asynchronous methods have shown to be more effective than synchronous parallel learning. However, existing asynchronous parallel learning methods are mainly for sparse parameter models to avoid the problem of gradient error. For dense parameter models like neural networks, asynchronous parallel methods bring gradient errors. The theoretical analysis work of Sun (2016) showed that the learning process with gradient errors can still be convergent on neural models.",
      "startOffset" : 8,
      "endOffset" : 414
    } ],
    "year" : 2017,
    "abstractText" : "Recent works have been shown effective in using neural networks for Chinese word segmentation. However, these models rely on large-scale data and are less effective for low-resource datasets because of insufficient training data. Thus, we propose a transfer learning method to improve low-resource word segmentation by leveraging high-resource corpora. First, we train a teacher model on high-resource corpora and then use the learned knowledge to initialize a student model. Second, a weighted data similarity method is proposed to train the student model on low-resource data with the help of highresource corpora. Finally, given that insufficient data puts forward higher requirements for feature extraction, we propose a novel neural network which improves feature learning. Experiment results show that our work significantly improves the performance on low-resource datasets: 2.3% and 1.5% F-score on PKU and CTB datasets. Furthermore, this paper achieves state-of-the-art results: 96.1%, and 96.2% F-score on PKU and CTB datasets1. Besides, we explore an asynchronous parallel method on neural word segmentation to speed up training. The parallel method accelerates training substantially and is almost five times faster than a serial mode.",
    "creator" : "LaTeX with hyperref package"
  }
}