{
  "name" : "1112.5246.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Combining One-Class Classifiers via Meta-Learning",
    "authors" : [ "Eitan Menahem", "Lior Rokach", "Yuval Elovici" ],
    "emails" : [ "ELOVICI}@BGU.AC.IL" ],
    "sections" : [ {
      "heading" : "1 Introduction and Background",
      "text" : "One-class classification aims to differentiate instances of class of interest from all other instances. The one-class classifier is trained from a training set containing only the instances of that class. Many one-class classification algorithms have been investigated [1, 2]. The variety of classification algorithms is both positive and negative. On the one hand, there are plenty of techniques to choose from, but on the other hand, choosing the right one can be difficult. This is because evaluating one-class classifiers performance is problematic. By definition, the data collections contain only one-class examples, and thus performance metrics, such as false-positive (FP ) and true negative (TN), cannot be computed. In the absence of FP and TN , derived performance metrics, such as classification accuracy, precision, AUC and others cannot be computed. One way to address this difficulty is to use an ensemble of classifiers as opposed to choosing the best classifier. The main idea behind the ensemble methodology is to weigh several individual classifiers and combine them in order to obtain a classifier that outperforms them all. Indeed, previous works in supervised ensemble learning show that combining classification models can produce a better classifier in terms of prediction accuracy [3].\nCompared to supervised-learning, research in the one-class ensemble research is limited [4]. In particular, only one combining method, the Fix-rule ensemble, was considered for one-class ensemble [5, 6, 7]. In this method, the combiner regards each participating classifier’s output as a single vote, upon which it applies some aggregative function (a combining rule) to produce a final classification. Henceforth we use the notation I(·) as the indicator function, Pk (x|ωTc) as the estimated probability of instance x given the target class ωTc, fT,k as the fraction of the target class that should be accepted for classifier k and θk notates\nar X\niv :1\n11 2.\n52 46\nv1 [\ncs .L\nG ]\n2 2\nD ec\nthe classification threshold for classifier k. A list of fixed combining rules is presented in Table 1. The fixed rule ensemble techniques, however, are not optimal, as the rules are\nusually assigned statically and independent of the training data. In order to find better ensemble solutions, researchers tend to import ideas from other learning domains. Unfortunately, the absence of negative examples in the training-sets of one-class problems make the adoption of many fine ensemble schemes difficult, especially with those who rely on some performance metrics, e.g. Weighting Performance and Grading. The ensemble method we pursue here, TUPSO, is based on meta-learning, aided by one-class performance evaluator. We show this method is indeed practical and worthwhile for one-class ensemble learning. Moreover, it significantly outperforms the Fix-rule technique."
    }, {
      "heading" : "2 Meta-Learning-Based Ensembles for One-Class Problems",
      "text" : "The proposed ensemble method, TUPSO, is intended to function in one-class scenarios where multiple, and possibly diverse, classifiers exist. Its main task is to combine one-class base-classifiers via meta-classifier. TUPSO is aided by a heuristic performance evaluator which estimates the classification performance of each of the base-classifiers and output a performance vector, Perfvect = {Pref1, ..., P refm}. The estimated performance is then translated into static weights, αi, which the meta-learning algorithm uses. αi = Perfi∗1/ ∑m j=1 Perfj , ∀i = 1...m, where m is the number of instance. TUPSO ensemble, as shown in Figure 1, is made up of four major components: (1) Base-Classifiers, (2) Performance Evaluator, (3) Aggregate Features Extractor and (4) Meta Classifier.\nOne of the building blocks of meta-learning is the meta-features, which are measured properties of the base-classifier(s). A collection of meta-features makes a meta-instance, which is used for training the meta-classifier and upon which the trained meta-classifier produces the ensemble prediction. The Aggregate Features Extractor produces meta-features by using multiple aggregations of the\nbase-classifiers outputs. Let Pm =< pm1 , ..., pmk > be the vector containing the base-classifiers outputs pm1 , ..., pmk , where k is the number of base-classifiers. In Table 2 we define six such aggregate meta-features Faggr = {f1, ..., f6}. During the training phase, the meta-features are applied for training the meta-classifier. In the on-line phase they are used by the meta-classifier that produces the final prediction."
    }, {
      "heading" : "3 Estimating the Performance of One-Class Classifiers",
      "text" : "The base-classifiers performance is a key factor for producing effective aggregate meta-features. Unfortunately, the inherent absence of negative examples makes the performance assessing difficult. This is because the two important values, the false positive (FP ) and true negative (TN) rates, cannot be measured, and thus, calculating some performance metrics such as Accuracy, Precision and F - score becomes impossible. Notice that each of the following performance metrics misses one or more values: Accuracy= TP+TNTP+TN+FP+FN , Precision = TP TP+FP\nand F -score = 2PRP+R , where P is the Precision and R is the Recall. Alternatively, we propose using a one-class approach for estimating classifiers performance. A criterion proposed in [8] estimates the F -score by using classifier’s prediction on positive instances. The criterion, rpPr[Y=+1] , shares some characters with the F -Score; note that Pr[f(x) = +1|Y = +1]Pr[Y = +1] = Pr[Y = +1|f(x) = +1]Pr[f(x) = 1) ⇔ Pr[f(x) = +1|Y = +1]/Pr[f(x) = +1] = Pr[Y = +1|f(x) = +1]/Pr[Y = +1]⇔ rPr[f(x)=+1] = p Pr[Y=+1] By multiplying both sides by r, we get the desired measure, henceforth denoted as Positive-Only F -score (Fix-rule): POF ≡ r 2\nPr[f(x)=+1]\nNotice that the recall r = Pr[f(x) = +1|y = +1] can be estimated from the classifier predictions on positive labeled examples, and Pr[f(x) = 1] can be estimated with Pr[f(x) = +1] = m(−1) ∗ ∑m i=1 f(x) = +1 from the classifier output on the validation set."
    }, {
      "heading" : "4 Evaluation",
      "text" : "In this section we examine two aspects concerning TUPSO. First, we test the proposed aggregation functions under various datasets. Secondly, we test how well TUPSO performs in comparison to some common Fix-rule methods. In order to estimate the generalized classification performance of the mentioned ensemble schemes, a 5x2 cross-validation procedure was performed [9]."
    }, {
      "heading" : "4.1 Experiment Setup",
      "text" : "The evaluation of TUPSO is consist of three dimensions: Datasets (12 popular datasets from the UCI collection [10]), combining method (TUPSO or fixe-rule method) and base-classifier performance metric (POF or none). We used six inducer setups (i.e. base-classifiers), induced by four algorithms: (i) ADIFA-HM and (ii) ADIFA-GM [11], (iii) GDE [12], (iv) PGA [13], (v) OC-SVM1 and (vi) OC-SVM2 [1]. The base-classifiers properties were kept static during the entire evaluation. Afore mentioned base-classifiers were selected as they represent three prominent families of one-class classifiers: density (ADIFA) nearest-neighbor (GDE and PGA) and boundary (SVM). Table 3 present the setup parameters.\nFor the Mean-Vote rule we used the classification cutoff θk = 0.75."
    }, {
      "heading" : "4.2 Measured Metrics",
      "text" : "For evaluating the real performance of both individual classifiers and ensemble methods we used a two-class performance metric, rather than POF . In contrast to the training process, in which the absence of negative examples dictates the use of one-class evaluation metrics (i.e. POF ), the dataset in the testing phase included instances of both classes, and thus allows the use two-class performance measure. Specifically, we used the Area under the ROC curve (AUC)."
    }, {
      "heading" : "4.3 Experimental Results",
      "text" : "In Table 4 we present the evaluation results. We use ’All’, ’WF’ and ’Non-WF’ to denote all-, weighted- and non-weighted- aggregative features respectively (see Table 2). Inside the parenthesis is the AUC rank of the corresponding ensemble method.\nWe can see that the weighted features (SumWP and V arWP ) together produced the best performance. It seems that mixing together the non-weighted and the weighted features produces a marginally weaker ensemble.\nIn Figure 2 we present the ROC graphs of the tested ensemble methods generated using the Optical-Digits dataset. The best curve belongs to a TUPSO version that uses all the aggregate features defined in Table 2. A weakness of the Fix-rule technique is demonstrated in Figure 3. Having 60 base-classifiers,\nwe plotted the density of the sum of votes for both the ’normal’ and ’anomaly’ classes. The classification cutoff value which produces the least classification errors is around 55. Voting, however, will classify any sum of votes above 30 as ’normal’ and since most of ’anomaly’ instances receive more than 30 votes, using it will result in a high FNR and a low TPR. The Max-rule will perform equally poorly because the probability that no base-classifier will vote ”normal’ is very low for ’anomaly’ instances. TUPSO, in contrast, will assign a low probability for sum of votes lower than 55 and hence classify instances more proficiently."
    }, {
      "heading" : "5 Conclusions and Future Work",
      "text" : "In this paper we proposed a new meta-learning based ensemble scheme for oneclass problems. The ensemble scheme learns a combining function upon aggregates of the base-classifiers’ predictions. To improve the aggregates inductive power, we implemented a classification performance evaluator, which we found very effective. Further on, we would like to examine TUPSO on more datasets and investigate additional one-class performance evaluators. Finally, we would like to discover how TUPSO performs compared to the ensemble’s best-classifier."
    } ],
    "references" : [ {
      "title" : "Novelty detection and neural network validation",
      "author" : [ "Chris M. Bishop" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1994
    }, {
      "title" : "Metric anomaly detection via asymmetric risk minimization",
      "author" : [ "Aryeh Kontorovich", "Danny Hendler", "Eitan Menahem" ],
      "venue" : "In SIMBAD,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2011
    }, {
      "title" : "Troika - an improved stacking schema for classification",
      "author" : [ "Eitan Menahem", "Lior Rokach", "Yuval Elovici" ],
      "venue" : "tasks. Inf. Sci.,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2009
    }, {
      "title" : "Intrusion detection in computer networks by a modular ensemble of one-class classifiers",
      "author" : [ "Giorgio Giacinto", "Roberto Perdisci", "Mauro Del Rio", "Fabio Roli" ],
      "venue" : "Information Fusion,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2008
    }, {
      "title" : "Combining one-class classifiers",
      "author" : [ "David M.J. Tax", "Robert P.W. Duin" ],
      "venue" : "In in Proc. Multiple Classifier Systems,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2001
    }, {
      "title" : "Combining one-class classifiers to classify missing data",
      "author" : [ "Piotr Juszczak", "Robert P.W. Duin" ],
      "venue" : "In Multiple Classifier Systems,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2004
    }, {
      "title" : "Weighted bagging for graph based one-class classifiers",
      "author" : [ "Santi Segúı", "Laura Igual", "Jordi Vitrià" ],
      "venue" : "In MCS,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2010
    }, {
      "title" : "Learning with positive and unlabeled examples using weighted logistic regression",
      "author" : [ "Wee Sun Lee", "Bing Liu" ],
      "venue" : "In ICML,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2003
    }, {
      "title" : "Approximate statistical test for comparing supervised classification learning algorithms",
      "author" : [ "Thomas G. Dietterich" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1998
    }, {
      "title" : "Anomaly detection via attribute distribution function approximation",
      "author" : [ "E. Menahem", "L. Roakach", "Y. Elovici" ],
      "venue" : null,
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2011
    }, {
      "title" : "A geometric framework for unsupervised anomaly detection: Detecting intrusions in unlabeled data",
      "author" : [ "Eleazar Eskin", "Andrew Arnold", "Michael Prerau", "Leonid Portnoy", "Sal Stolfo" ],
      "venue" : "In Applications of Data Mining in Computer Security. Kluwer,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2002
    }, {
      "title" : "A unified notion of outliers: Properties and computation",
      "author" : [ "Edwin M. Knorr", "Raymond T. Ng" ],
      "venue" : "Proc. of the International Conference on Knowledge Discovery and Data Mining,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 1997
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Many one-class classification algorithms have been investigated [1, 2].",
      "startOffset" : 64,
      "endOffset" : 70
    }, {
      "referenceID" : 1,
      "context" : "Many one-class classification algorithms have been investigated [1, 2].",
      "startOffset" : 64,
      "endOffset" : 70
    }, {
      "referenceID" : 2,
      "context" : "Indeed, previous works in supervised ensemble learning show that combining classification models can produce a better classifier in terms of prediction accuracy [3].",
      "startOffset" : 161,
      "endOffset" : 164
    }, {
      "referenceID" : 3,
      "context" : "Compared to supervised-learning, research in the one-class ensemble research is limited [4].",
      "startOffset" : 88,
      "endOffset" : 91
    }, {
      "referenceID" : 4,
      "context" : "In particular, only one combining method, the Fix-rule ensemble, was considered for one-class ensemble [5, 6, 7].",
      "startOffset" : 103,
      "endOffset" : 112
    }, {
      "referenceID" : 5,
      "context" : "In particular, only one combining method, the Fix-rule ensemble, was considered for one-class ensemble [5, 6, 7].",
      "startOffset" : 103,
      "endOffset" : 112
    }, {
      "referenceID" : 6,
      "context" : "In particular, only one combining method, the Fix-rule ensemble, was considered for one-class ensemble [5, 6, 7].",
      "startOffset" : 103,
      "endOffset" : 112
    }, {
      "referenceID" : 7,
      "context" : "A criterion proposed in [8] estimates the F -score by using classifier’s prediction on positive instances.",
      "startOffset" : 24,
      "endOffset" : 27
    }, {
      "referenceID" : 8,
      "context" : "In order to estimate the generalized classification performance of the mentioned ensemble schemes, a 5x2 cross-validation procedure was performed [9].",
      "startOffset" : 146,
      "endOffset" : 149
    }, {
      "referenceID" : 9,
      "context" : "base-classifiers), induced by four algorithms: (i) ADIFA-HM and (ii) ADIFA-GM [11], (iii) GDE [12], (iv) PGA [13], (v) OC-SVM1 and (vi) OC-SVM2 [1].",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 10,
      "context" : "base-classifiers), induced by four algorithms: (i) ADIFA-HM and (ii) ADIFA-GM [11], (iii) GDE [12], (iv) PGA [13], (v) OC-SVM1 and (vi) OC-SVM2 [1].",
      "startOffset" : 94,
      "endOffset" : 98
    }, {
      "referenceID" : 11,
      "context" : "base-classifiers), induced by four algorithms: (i) ADIFA-HM and (ii) ADIFA-GM [11], (iii) GDE [12], (iv) PGA [13], (v) OC-SVM1 and (vi) OC-SVM2 [1].",
      "startOffset" : 109,
      "endOffset" : 113
    }, {
      "referenceID" : 0,
      "context" : "base-classifiers), induced by four algorithms: (i) ADIFA-HM and (ii) ADIFA-GM [11], (iii) GDE [12], (iv) PGA [13], (v) OC-SVM1 and (vi) OC-SVM2 [1].",
      "startOffset" : 144,
      "endOffset" : 147
    } ],
    "year" : 2017,
    "abstractText" : "We examine various methods for combining the output of one-class models. In particular, we show that simple meta-learning based ensemble achieves better result than weighting methods. Furthermore we propose a new one-class ensemble scheme, called TUPSO that uses metalearning for combining multiple one-class classifiers. We also present a new one-class classification performance measures to weigh the base-classifiers, a process that proved helpful for increasing the classification performance of the induced ensemble. Our experimental study shows that the proposed method significantly outperforms exiting methods.",
    "creator" : "LaTeX with hyperref package"
  }
}