{
  "name" : "1402.6859.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Outlier Detection using Improved Genetic K-means",
    "authors" : [ "M. H. Marghny", "Ahmed I. Taloba" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "clustering-based outlier detection algorithms is to find clusters and outliers, which are often regarded as noise that should be removed in order to make more reliable clustering.\nIn this article, we present an algorithm that provides outlier detection and data clustering simultaneously. The algorithmimprovesthe estimation of centroids of the generative distribution during the process of clustering and outlier discovery. The proposed algorithm consists of two stages. The first stage consists of improved genetic k-means algorithm (IGK) process, while the second stage iteratively removes the vectors which are far from their cluster centroids.\nGeneral Terms Data Mining.\nKeywords Outlier detection, Genetic algorithms, Clustering, K-means algorithm, Improved Genetic K-means (IGK)"
    }, {
      "heading" : "1. INTRODUCTION",
      "text" : "Data mining, in general, deals with the discovery of nontrivial, hidden and interesting knowledge from different types of data. With the development of information technologies, the number of databases, as well as their dimension and complexity, grow rapidly. It is necessary what we need automated analysis of great amount of information. The analysis results are then used for making a decision by a human or program. One of the basic problems of data mining is the outlier detection.\nAn outlier is an observation of the data that deviates from other observations so much that it arouses suspicions that it was generated by a different mechanism from the most part of data [1]. Inlier, on the other hand, is defined as an observation that is explained by underlying probability density function. This function represents probability distribution of main part of data observations [2].\nMany data-mining algorithms manipulate outliers as a sideproduct of clustering algorithms. However these techniques define outliers as points, which do not lie in clusters. Thus, the techniques implicitly define outliers as the background noise in which the clusters are embedded. Another class of techniques defines outliers as points, which are neither a part of a cluster nor a part of the background noise; rather they are specifically points which behave very differently from the norm [3]. Some noisy points may be far away from the data points, whereas the others may be close. The faraway noisy points would affect the result more significantly because they are more different from the data points. It is desirable to identify and remove the outliers, which are far away from all the other points in cluster [4]. So, to improve the clustering\nsuch algorithms use the same process and functionality to solve both clustering and outlier discovery [2].\nIn this paper, we propose a clustering-based technique to identify outliers and simultaneously produce data clustering. Proposed outlier detection process at the same time is effective for extracting clusters and very efficient in finding outliers."
    }, {
      "heading" : "2. IMPROVED GENETIC K-MEANS (IGK)",
      "text" : "IGK is an efficient clustering algorithm to handle large scale data, which can select initial clustering center purposefully using Genetic algorithms (GAs), reduce the sensitivity to isolated point, avoid dissevering big cluster, and overcome deflexion of data in some degree that caused by the disproportion in data partitioning owing to adoption of multisampling [5].\nThe mean steps of proposed algorithm can be summarized as follows:\nAlgorithm: Improved Genetic K-means (S, k), S = {x1, x2,…,xn}.\nInput: The number of clusters K'(K'> K) and a dataset containing n objects xi.\nOutput: A set of k clusters Cj that minimize the squared-error criterion."
    }, {
      "heading" : "3. OUTLIER DETECTION METHODS",
      "text" : "In outlier detection methods based on clustering, outlier is defined to be an observation that does not fit to the overall clustering pattern [6]. The ability to detect outliers can be improved using a combined perspective of outlier detection and clustering. Some clustering algorithms [7-16] handle outliers as special observations, but their main concern is clustering the dataset, not detecting outliers. The following techniques have been proposed to detect outliers:\nBegin 1. Multiple sub-samples {S1, S2, ...,Sj}; 2. For m = 1 to j do\nGenetic K-means(Sm, K'); //executing Genetic Kmeans, produce K' clusters and j groups.\n3. Compute\n;\n4. Choose min{Jc} as the refined initial points Zj , j [1, K']; 5. Genetic K-means(S, K'); //executing Genetic K-means again with chosen initial, producing K'mediods. 6. Repeat\nCombining two near clusters into one cluster, and recalculate the new center generated by two centers merged.\n7. Until the number of clusters reduces into k //Merging (K' +K) End\nOutlier Detection using Indegree Number (ODIN)[15] is a local density-based outlier detection algorithm. Local density based scheme can be used in cluster thinning. Outlier removal algorithm can remove vectors from the overlapping regions between clusters, if the assumption holds that the regions are of relatively low density. Higher density is found near the cluster centroid. An obvious approach to use outlier rejection in the cluster thinning is as follows: (i) eliminate outliers (ii) cluster the data using any method.\nIn ODIN, the outliers are defined using a k-nearest neighbour (kNN) graph, in which every vertex represents a data vector, and the edges are pointers to neighbouring k vectors. The weight of an edge eij is ||xi – xj||. In ODIN, the outlyingness of xi is defined as:\n1\n( ) 1 iO ind x  \nWhere ind(xi) is the indegree of the vertex xi, i.e. the number of edges pointing to xi. In the first step of ODIN, a kNN graph is created for the dataset X. Then, each vertex is visited to check if its outlyingness is above threshold T. An algorithm shows the ODIN method as follows:\nAlgorithm ODIN+K-means (k, T):\nOutlier removal clustering (ORC)[16], it consists of two consecutive stages, which are repeated several times. In the first stage, K-means algorithm is performed until convergence, and in the second stage, the outlyingness factor is assigned for each vector. Factor depends on its distance from the cluster centroid. Then algorithm iterations start, with first finding the vector with maximum distance to the partition centroid dmax:\nmax max , 1,...,i i pid x c i N  \nThe outlyingness of a vector xi is defined as follows:\nmax\ni pi\ni\nx c O\nd\n \nAn algorithm shows the ORC method as follows: Algorithm ORC (I, T)"
    }, {
      "heading" : "4. PROPOSED METHOD",
      "text" : "The ability to detect outliers can be improved using a combined perspective from outlier detection and cluster identification. Unlike the traditional clustering-based methods, the proposed algorithm provides much efficient outlier detection and data clustering capabilities in the presence of outliers. This approach is based on filtering of the data after clustering process. The purpose of our method is not only to produce data clustering but at the same time to find outliers from the resulting clusters.\nThe algorithm of our outlier detection method is divided into two stages. The first stage carried out using IGK process and the second stage removing outliers according to a chosen threshold.\nSo, the proposed method is like the ORC method except in the first stage, ORC was applying K-means. But K-means suffers from some drawbacks such as K-means is sensitive to initial choice of cluster centers, the clustering can be very different by starting from different centers, K-means can’t deal with massive data, K-means is sensitive with respect to outliers and noise. Hence, we employed IGK instead K-means which has many advantages as we described before.\nIn our method, the outliers are defined using outlyingness factor that assigned for each vector. Factor depends on its distance from the cluster centroid. Outlyingness factors for each vector are defined as follows:\nmax\ni pi\ni\nx c O\nd\n \nWhere xi is the vector and dmax is the maximum distance between vector xi and the partition centroid cpi and defined as follows:\nmax max , 1,...,i i pid x c i N  \nWe see that all outlyingness factors of the dataset are\nnormalized to the scale [0, 1]. The greater value, the more\nlikely the vector is an outlier. An example of dataset clustered\nin three clusters and calculated outlyingness factors is shown\nin Figure 1.\nBegin {ind(xi)|i = 1, ...,N}←Calculate kNN graph fori←1,...,N do\nOi←1/(ind(xi) + 1) ifOi> T then X←X \\ {xi} end if\nend for (C,P) ←K-means(X) //C is the centers and P is the partition. End\nBegin C ← Run K-means with multiple initial solutions, pick best C for j← 1,...,I do //I is no of iterations\ndmax← maxi{||xi − cpi||} for i ← 1,...,N do\nOi = ||xi–cpi||/dmax ifOi> T then X ← X \\ {xi} end if\nend for (C,P) ← K-means(X,C) //C is the centers and P is the partition.\nend for End\nThe proposed algorithm can be summarized as follows:\nAlgorithm Outlier Detection (I, T)\nThe principle of outliers removing depends on the difference between threshold T and outlyingness factors Oi if Oi> T, the outlier is removed from the dataset. Threshold is set by user in range between 0 and 1. At the end of each iteration, IGK is run with the previous C as the initial codebook, so new solution will be a fine-tuned solution for the reduced dataset. By setting the threshold to T < 1, at least one vector is removed. Thus, increasing the number of iterations and decreasing the threshold will in effect remove more vectors from the dataset, possibly all vectors.\nThe performance of proposed method for 10 iterations with threshold 0.9 is illustrated in Figure 2. In the original data, there are the most distant vectors from centroids, some of such furthest vectors are labeled by arrows in Figure 2 on the left. The algorithm proceeds by removing the furthest vectors from all partitions.Figure 2 on right demonstrates the resulting data after the algorithm."
    }, {
      "heading" : "5. EXPERIMENTS",
      "text" : "We run experiments on three synthetic datasets denoted as A1, A2 and A3 [17], which are shown in Figure 3 and summarized in Table 1 with threshold equal to 0.9 and number of iterations equal to 10, where N is number of examples and M is no of clusters.\nFig3: Datasets, A1 (left), A2 (center) and A3 (right).\nThe mean square error (MSE) used to differentiate the difference between outlier points. The distance between the estimated centroids and the true centroids has been calculated to computes the error.\nFigure 4, shows that the error is decreasing very fast when the threshold becomes bigger, thus removing more and more number of objects implies to decrease the distance between the points and cluster centroids.\nBegin C ←IGK for j← 1,...,I do //I is no of iterations\ndmax← maxi{||xi − cpi||} for i ← 1,...,N do\nOi = ||xi–cpi||/dmax ifOi> T then\nX ← X \\ {xi}\nend if end for (C,P) ←IGK(X,C) //C is the centers and P is the partition.\nend for End\nIn Table 2, we show the smallest MSE between original centroids and those obtained by using K-means, ORC and proposed method. It is obvious that the MSE obtained by the proposed method for all data set (A114.77, A2 26.37, A3 800.69) is better than the calculated MSE by K-means (A1 1632.88, A2 2516.83, A3 3052.36) and ORC algorithm (A1 1311.88, A2 2138.64, A3 2470.51).\nIn summary, the above experimental results on A1, A2, and A3 datasets show that the proposed algorithm can identify outliers more successfully than existing algorithms."
    }, {
      "heading" : "6. CONCLUSION",
      "text" : "In this paper, we have proposed to integrate outlier removal into clustering for nonparametric model estimation. The proposed method was also compared with the standard Kmeans without outlier removal, and a simple approach in which outlier removal precedes the actual clustering. The proposed method employs both clustering and outlier discovery to improve estimation of the centroids of the generative distribution.\nThe experimental results show that the proposed method can identify outliers more successfully than existing algorithms and indicate that our method has a lower error on datasets with overlapping clusters than the competing methods."
    }, {
      "heading" : "7. ACKNOWLEDGMENTS",
      "text" : "We want to give our special thanks to Prof. Adel Abo ElMagd for his encouragement and support with the evaluation."
    }, {
      "heading" : "8. REFERENCES",
      "text" : "[1] Williams, G., Baxter, R., He, H., Hawkins, S., and Gu,\nL.2002. A Comparative Study for RNN for Outlier Detection in Data Mining. In Proceedings of the 2nd IEEE International Conference on Data Mining, Maebashi City, Japan, pp.709.\n[2] He,Z., Xu, X., and Deng,S. 2003. Discovering Clusterbased Local Outliers. Pattern Recognition Letters, vol.24, pp.1641-1650.\n[3] Aggarwal, C., and Yu,P.2001. Outlier Detection for High Dimensional Data. In Proceedings of the ACM SIGMOD International Conference on Management of Data, vol.30, pp.37-46.\n[4] Jaing, M., Tseng, S., and Su, C.2001. Two-phase Clustering Process for Outlier Detection. Pattern Recognition Letters, vol.22, pp.691-700.\n[5] Taloba, A. I. 2008. Data Clustering Using Evolutionary Algorithms. Master thesis, Assiut University, Assiut,Egypt.\n[6] Zhang, T.,Ramakrishnan, R., and Livny, M.1997. BIRCH: A new data clustering algorithm and its applications. Data Mining and Knowledge Discovery, vol.1,pp.141-182.\n[7] Ester, M.,Kriegel, H. P., Sander J., and Xu, X.1996. A density-based algorithm for discovering clusters in large\nspatial databases with noise. In:2nd International Conference on Knowledge Discovery and Data Mining, pp.226-231.\n[8] Guha, S.,Rastogi, R., and Shim, K.1999. A robust clustering algorithm for categorical attributes. In 15th International Conference on Data Engineering, pp.512521.\n[9] Pamula, R., Deka, J.K., Nandi, S. 2011. An Outlier Detection Method Based on Clustering. Emerging Applications of Information Technology (EAIT), pp. 253 – 256.\n[10] Al-Zoubi, M., Al-Dahoud, A. and Yahya, A.A. 2010. New Outlier Detection Method Based on Fuzzy Clustering, WSEAS Transactions on Information Science and Applications, pp.681-690.\n[11] Murugavel, P., and Punithavalli, M. 2011. Improved Hybrid Clustering and Distance-based Technique for Outlier Removal, International Journal on Computer Science and Engineering (IJCSE).\n[12] Karmaker, A. and Rahman, S. 2009 Outlier Detection in Spatial Databases Using Clustering Data Mining, Sixth International Conference on Information Technology: New Generations, pp.1657-1658.\n[13] Loureiro,A., Torgo, L. and Soares, C. 2004. Outlier Detection using Clustering Methods: a Data Cleaning Application, in Proceedings of KDNet Symposium on Knowledge-based Systems for the Public Sector. Bonn, Germany.\n[14] Niu, K., Huang, C., Zhang, S., and Chen, J. 2007. ODDC: Outlier Detection Using Distance Distribution Clustering, T. Washio et al. (Eds.): PAKDD 2007 Workshops, Lecture Notes in Artificial Intelligence (LNAI) 4819, pp. 332–343.\n[15] Hautamaki, V., Karkkainen, I., and Franti, P.2004. Outlier detection using knearestneighbour graph. In 17th International Conference on Pattern Recognition (ICPR 2004), Cambridge, United Kingdom, pp.430-433.\n[16] Hautamaki,V.Cherednichenko, S.,Karkkainen, I.,Kinnunen, T.,and Franti, P.2005. Improving K-Means by Outlier Removal. In: SCIA 2005, pp.978-987.\n[17] Virmajoki, O. 2004. Pairwise Nearest Neighbor Method Revisited. PhD thesis, University of Joensuu, Joensuu, Finland."
    } ],
    "references" : [ {
      "title" : "A Comparative Study for RNN for Outlier Detection in Data Mining",
      "author" : [ "G. Williams", "R. Baxter", "H. He", "S. Hawkins", "Gu" ],
      "venue" : "In Proceedings of the 2nd IEEE International Conference on Data Mining, Maebashi City, Japan,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2002
    }, {
      "title" : "Discovering Clusterbased Local Outliers",
      "author" : [ "Z. He", "X. Xu", "S. Deng" ],
      "venue" : "Pattern Recognition Letters,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2003
    }, {
      "title" : "Outlier Detection for High Dimensional Data",
      "author" : [ "C. Aggarwal" ],
      "venue" : "In Proceedings of the ACM SIGMOD International Conference on Management of Data,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2001
    }, {
      "title" : "Two-phase Clustering Process for Outlier Detection",
      "author" : [ "M. Jaing", "S. Tseng", "Su" ],
      "venue" : "Pattern Recognition Letters,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2001
    }, {
      "title" : "Data Clustering Using Evolutionary Algorithms. Master thesis, Assiut University, Assiut,Egypt",
      "author" : [ "A.I. Taloba" ],
      "venue" : null,
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2008
    }, {
      "title" : "BIRCH: A new data clustering algorithm and its applications. Data Mining and Knowledge Discovery, vol.1,pp.141-182",
      "author" : [ "T. Zhang", "R. Ramakrishnan", "Livny" ],
      "venue" : null,
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1997
    }, {
      "title" : "A density-based algorithm for discovering clusters in large spatial databases with noise",
      "author" : [ "M. Ester", "H.P. Kriegel", "Sander J", "Xu" ],
      "venue" : "In:2nd International Conference on Knowledge Discovery and Data Mining,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1996
    }, {
      "title" : "A robust clustering algorithm for categorical attributes",
      "author" : [ "S. Guha", "R. Rastogi", "Shim" ],
      "venue" : "In 15th International Conference on Data Engineering,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1999
    }, {
      "title" : "An Outlier Detection Method Based on Clustering",
      "author" : [ "R. Pamula", "J.K. Deka", "S. Nandi" ],
      "venue" : "Emerging Applications of Information Technology (EAIT), pp",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2011
    }, {
      "title" : "New Outlier Detection Method Based on Fuzzy Clustering",
      "author" : [ "M. Al-Zoubi", "A. Al-Dahoud", "A.A. Yahya" ],
      "venue" : "WSEAS Transactions on Information Science and Applications,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2010
    }, {
      "title" : "Improved Hybrid Clustering and Distance-based Technique for Outlier Removal, International Journal on Computer Science and Engineering (IJCSE)",
      "author" : [ "P. Murugavel", "M. Punithavalli" ],
      "venue" : null,
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2011
    }, {
      "title" : "Outlier Detection in Spatial Databases Using Clustering Data Mining",
      "author" : [ "A. Karmaker", "S. Rahman" ],
      "venue" : "Sixth International Conference on Information Technology: New Generations,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2009
    }, {
      "title" : "Outlier Detection using Clustering Methods: a Data Cleaning Application, in Proceedings of KDNet Symposium on Knowledge-based Systems for the Public Sector",
      "author" : [ "A. Loureiro", "L. Torgo", "C. Soares" ],
      "venue" : null,
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2004
    }, {
      "title" : "ODDC: Outlier Detection Using Distance Distribution Clustering, T",
      "author" : [ "K. Niu", "C. Huang", "S. Zhang", "J. Chen" ],
      "venue" : "Washio et al. (Eds.): PAKDD 2007 Workshops, Lecture Notes in Artificial Intelligence (LNAI)",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2007
    }, {
      "title" : "Outlier detection using knearestneighbour graph",
      "author" : [ "V. Hautamaki", "I. Karkkainen", "Franti" ],
      "venue" : "In 17th International Conference on Pattern Recognition (ICPR",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2004
    }, {
      "title" : "Improving K-Means by Outlier Removal",
      "author" : [ "Hautamaki", "V.Cherednichenko" ],
      "venue" : "Franti,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2005
    }, {
      "title" : "Pairwise Nearest Neighbor Method Revisited",
      "author" : [ "O. Virmajoki" ],
      "venue" : "PhD thesis,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2004
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "An outlier is an observation of the data that deviates from other observations so much that it arouses suspicions that it was generated by a different mechanism from the most part of data [1].",
      "startOffset" : 188,
      "endOffset" : 191
    }, {
      "referenceID" : 1,
      "context" : "This function represents probability distribution of main part of data observations [2].",
      "startOffset" : 84,
      "endOffset" : 87
    }, {
      "referenceID" : 2,
      "context" : "Another class of techniques defines outliers as points, which are neither a part of a cluster nor a part of the background noise; rather they are specifically points which behave very differently from the norm [3].",
      "startOffset" : 210,
      "endOffset" : 213
    }, {
      "referenceID" : 3,
      "context" : "It is desirable to identify and remove the outliers, which are far away from all the other points in cluster [4].",
      "startOffset" : 109,
      "endOffset" : 112
    }, {
      "referenceID" : 1,
      "context" : "So, to improve the clustering such algorithms use the same process and functionality to solve both clustering and outlier discovery [2].",
      "startOffset" : 132,
      "endOffset" : 135
    }, {
      "referenceID" : 4,
      "context" : "IMPROVED GENETIC K-MEANS (IGK) IGK is an efficient clustering algorithm to handle large scale data, which can select initial clustering center purposefully using Genetic algorithms (GAs), reduce the sensitivity to isolated point, avoid dissevering big cluster, and overcome deflexion of data in some degree that caused by the disproportion in data partitioning owing to adoption of multisampling [5].",
      "startOffset" : 396,
      "endOffset" : 399
    }, {
      "referenceID" : 5,
      "context" : "OUTLIER DETECTION METHODS In outlier detection methods based on clustering, outlier is defined to be an observation that does not fit to the overall clustering pattern [6].",
      "startOffset" : 168,
      "endOffset" : 171
    }, {
      "referenceID" : 6,
      "context" : "Some clustering algorithms [7-16] handle outliers as special observations, but their main concern is clustering the dataset, not detecting outliers.",
      "startOffset" : 27,
      "endOffset" : 33
    }, {
      "referenceID" : 7,
      "context" : "Some clustering algorithms [7-16] handle outliers as special observations, but their main concern is clustering the dataset, not detecting outliers.",
      "startOffset" : 27,
      "endOffset" : 33
    }, {
      "referenceID" : 8,
      "context" : "Some clustering algorithms [7-16] handle outliers as special observations, but their main concern is clustering the dataset, not detecting outliers.",
      "startOffset" : 27,
      "endOffset" : 33
    }, {
      "referenceID" : 9,
      "context" : "Some clustering algorithms [7-16] handle outliers as special observations, but their main concern is clustering the dataset, not detecting outliers.",
      "startOffset" : 27,
      "endOffset" : 33
    }, {
      "referenceID" : 10,
      "context" : "Some clustering algorithms [7-16] handle outliers as special observations, but their main concern is clustering the dataset, not detecting outliers.",
      "startOffset" : 27,
      "endOffset" : 33
    }, {
      "referenceID" : 11,
      "context" : "Some clustering algorithms [7-16] handle outliers as special observations, but their main concern is clustering the dataset, not detecting outliers.",
      "startOffset" : 27,
      "endOffset" : 33
    }, {
      "referenceID" : 12,
      "context" : "Some clustering algorithms [7-16] handle outliers as special observations, but their main concern is clustering the dataset, not detecting outliers.",
      "startOffset" : 27,
      "endOffset" : 33
    }, {
      "referenceID" : 13,
      "context" : "Some clustering algorithms [7-16] handle outliers as special observations, but their main concern is clustering the dataset, not detecting outliers.",
      "startOffset" : 27,
      "endOffset" : 33
    }, {
      "referenceID" : 14,
      "context" : "Some clustering algorithms [7-16] handle outliers as special observations, but their main concern is clustering the dataset, not detecting outliers.",
      "startOffset" : 27,
      "endOffset" : 33
    }, {
      "referenceID" : 15,
      "context" : "Some clustering algorithms [7-16] handle outliers as special observations, but their main concern is clustering the dataset, not detecting outliers.",
      "startOffset" : 27,
      "endOffset" : 33
    }, {
      "referenceID" : 14,
      "context" : "34 Outlier Detection using Indegree Number (ODIN)[15] is a local density-based outlier detection algorithm.",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 15,
      "context" : "Outlier removal clustering (ORC)[16], it consists of two consecutive stages, which are repeated several times.",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 0,
      "context" : "We see that all outlyingness factors of the dataset are normalized to the scale [0, 1].",
      "startOffset" : 80,
      "endOffset" : 86
    }, {
      "referenceID" : 16,
      "context" : "EXPERIMENTS We run experiments on three synthetic datasets denoted as A1, A2 and A3 [17], which are shown in Figure 3 and summarized in Table 1 with threshold equal to 0.",
      "startOffset" : 84,
      "endOffset" : 88
    } ],
    "year" : 2011,
    "abstractText" : "The outlier detection problem in some cases is similar to the classification problem. For example, the main concern of clustering-based outlier detection algorithms is to find clusters and outliers, which are often regarded as noise that should be removed in order to make more reliable clustering. In this article, we present an algorithm that provides outlier detection and data clustering simultaneously. The algorithmimprovesthe estimation of centroids of the generative distribution during the process of clustering and outlier discovery. The proposed algorithm consists of two stages. The first stage consists of improved genetic k-means algorithm (IGK) process, while the second stage iteratively removes the vectors which are far from their cluster centroids. General Terms Data Mining.",
    "creator" : "Microsoft® Office Word 2007"
  }
}