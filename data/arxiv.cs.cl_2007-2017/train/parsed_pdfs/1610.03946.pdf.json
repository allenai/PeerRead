{
  "name" : "1610.03946.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Neural Network for Coordination Boundary Prediction",
    "authors" : [ "Jessica Ficler" ],
    "emails" : [ "jessica.ficler@gmail.com", "yoav.goldberg@gmail.com" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Coordination is a common syntactic phenomena, appearing in 38.8% of the sentences in the Penn Treebank (PTB) (Marcus et al., 1993), and in 60.71% of the sentences in the Genia Treebank (Ohta et al., 2002). However, predicting the correct conjuncts span remain one of the biggest challenges for stateof-the-art syntactic parsers. Both the Berkeley and Zpar phrase-structure parsers (Petrov et al., 2006; Zhang and Clark, 2011) achieve F1 scores of around 69% when evaluated on their ability to recover coordination boundaries on the PTB test set. For example, in:\n“He has the government’s blessing to [build churches] and [spread Unificationism] in that country.”\nthe conjuncts are incorrectly predicted by both parsers:\nBerkeley: “He has the government’s blessing to [build churches] and [spread Unificationism in that country].” Zpar: “He [has the government’s blessing to build churches] and [spread Unificationism in that country].”\nIn this work we focus on coordination boundary prediction, and suggest a specialized model for this task. We treat it as a ranking task, and learn a scoring function over conjuncts candidates such that the correct candidate pair is scored above all other candidates. The scoring model is a neural network with two LSTM-based components, each modeling a different linguistic principle: (1) conjuncts tend to be similar (“symmetry”); and (2) replacing the coordination phrase with each of the conjuncts usually result in a coherent sentence (“replacement”). The symmetry component takes into account the conjuncts’ syntactic structures, allowing to capture similarities that occur in different levels of the syntactic structure.\nThe replacement component considers the coherence of the sequence that is produced when connecting the participant parts. Both of these signals are syntactic in nature, and are learned solely based on information in the Penn Treebank. Our model substantially outperforms both the Berkeley and Zpar parsers on the coordination prediction task, while using the exact same training corpus. Semantic signals (which are likely to be based on resources external to the treebank) are also relevant for coordination disambiguation (Kawahara and Kurohashi, 2008; Hogan, 2007) and provide complementary information. We plan to incorporate such signals in future work. ar X\niv :1\n61 0.\n03 94\n6v 1\n[ cs\n.C L\n] 1\n3 O\nct 2"
    }, {
      "heading" : "2 Background",
      "text" : "Coordination is a very common syntactic construction in which several sentential elements (called conjuncts) are linked. For example, in:\n“The Jon Bon Jovi Soul Foundation [was founded in 2006] and1 [exists to combat issues that force (families) and2 (individuals) into economic despair].”\nThe coordinator and1 links the conjuncts surrounded with square brackets and the coordinator and2 links the conjuncts surrounded with round brackets.\nCoordination between NPs and between VPs are the most common, but other grammatical functions can also be coordinated: “[relatively active]ADJP but [unfocused]ADJP” ; “[in]IN and [out]IN the market”. While coordination mostly occurs between elements with the same syntactic category, cross-category conjunctions are also possible: (“Alice will visit Earth [tomorrow]NP or [in the next decade]PP”). Less common coordinations involve non-constituent elements “[equal to] or [higher than]”, argument clusters (“Alice visited [4 planets] [in 2014] and [3 more] [since then]”), and gapping (“[Bob lives on Earth] and [Alice on Saturn]”) (Dowty, 1988)."
    }, {
      "heading" : "2.1 Symmetry between conjuncts",
      "text" : "Coordinated conjuncts tend to be semantically related and have a similar syntactic structure. For example, in (a) and (b) the conjuncts include similar words (China/Asia, marks/yen) and have identical syntactic structures.\nPP\nPP\nIN\nfor\nNP\nNNP\nChina\nCC\nand\nPP\nIN\nfor\nNP\nNNP\nAsia\n(a)\nNP\nNP\nCD\n1.8690\nNNS\nmarks\nCC\nand\nNP\nCD\n139.75\nNNS\nyen\n(b)\nSymmetry holds also in larger conjuncts, such as in:\n(c)\nNP\nNP\nNN\nincome\nPP\nIN of\nNP\nNP\nQP\n429.9 billion\nNNS rubles\nPRN\n(US$ 693.4)\nCC and\nNP\nVBZ\nexpenditures\nPP\nIN of\nNP\nNP\nQP\n489.9 billion\nNNS rubles\nPRN\n(US$ 790.2)\nSimilarity between conjuncts was used as a guiding principle in previous work on coordination disambiguation (Hogan, 2007; Shimbo and Hara, 2007; Hara et al., 2009)."
    }, {
      "heading" : "2.2 Replaceability",
      "text" : "Replacing a conjunct with the whole coordination phrase usually produce a coherent sentence (Huddleston et al., 2002). For example, in “Ethan has developed [new products] and [a new strategy]”, replacement results in: “Ethan has developed new products”; and “Ethan has developed a new strategy”, both valid sentences. Conjuncts replacement holds also for conjuncts of different syntactic types, e.g.: “inactivation of tumor-suppressor genes, [alone] or [in combination], appears crucial to the development of such scourges as cancer.”.\nWhile both symmetry and replacebility are strong characteristics of coordination, neither principle holds universally. Coordination between syntactically dissimilar conjuncts is possible (“tomorrow and for the entirety of the next decade”), and the replacement principle fails in cases of ellipsis, gapping and others (“The bank employs [8,000 people in Spain] and [2,000 abroad]”)."
    }, {
      "heading" : "2.3 Coordination in the PTB",
      "text" : "Coordination annotation in the Penn Treebank (Marcus et al., 1993) is inconsistent (Hogan, 2007) and lacks internal structure for NPs with nominal modifiers (Bies et al., 1995). In addition, conjuncts in the PTB are not explicitly marked. These deficiencies led previous works on coordination disambiguation (Shimbo and Hara, 2007; Hara et al., 2009; Hanamoto et al., 2012) to use the Genia treebank of biomedical text (Ohta et al., 2002) which explicitly marks coordination phrases. However, using the Genia corpus is not ideal since it is in a specialized\ndomain and much smaller than the PTB. In this work we rely on a version of the PTB released by Ficler and Goldberg (2016) in which the above deficiencies are manually resolved. In particular, coordinating elements, coordination phrases and conjunct boundaries are explicitly marked with specialized function labels."
    }, {
      "heading" : "2.4 Neural Networks and Notation",
      "text" : "We use w1:n to indicate a list of vectors w1, w2, . . . wn and wn:1 to indicate the reversed list. We use ◦ for vector concatenation. When a discrete symbol w is used as a neural network’s input, the corresponding embedding vector is assumed.\nA multi-layer perceptron (MLP) is a non linear classifier. In this work we take MLP to mean a classifier with a single hidden layer: MLP (x) = V · g(Wx + b) where x is the network’s input, g is an activation function such as ReLU or Sigmoid, and W , V and b are trainable parameters. Recurrent Neural Networks (RNNs) (Elman, 1990) allow the representation of arbitrary sized sequences. In this work we use LSTMs (Hochreiter and Schmidhuber, 1997), a variant of RNN that was proven effective in many NLP tasks. LSTM(w1:n) is the outcome vector resulting from feeding the sequence w1:n into the LSTM in order. A bi-directional LSTM (biLSTM) takes into account both the past w1:i and the future wi:n when representing the element in position i:\nbiLSTM(w1:n, i) = LSTMF (w1:i) ◦ LSTMB(wn:i)\nwhere LSTMF reads the sequence in its regular order and LSTMB reads it in reverse."
    }, {
      "heading" : "3 Task Definition and Architecture",
      "text" : "Given a coordination word in a sentence, the coordination prediction task aims to returns the two conjuncts that are connected by it, or NONE if the word does not function as a coordinating conjunction of a relevant type.1 Figure 1 provides an example.\nOur system works in three phases: first, we determine if the coordinating word is indeed part of a conjunction of a desired type. We then extract a ranked list of candidate conjuncts, where a candidate is a\n1We consider and, or, but, nor as coordination words. In case of more than two coordinated elements (conjuncts), we focus on the two conjuncts which are closest to the coordinator.\npair of spans of the form ((i, j), (l,m)). The candidates are then scored and the highest scoring pair is returned. Section 4 describes the scoring model, which is the main contribution of this work. The coordination classification and candidate extraction components are described in Section 5."
    }, {
      "heading" : "4 Candidate Conjunctions Scoring",
      "text" : "Our scoring model takes into account two signals, symmetry between conjuncts and the possibility of replacing the whole coordination phrase with its participating conjuncts."
    }, {
      "heading" : "4.1 The Symmetry Component",
      "text" : "As noted in Section 2.1, many conjuncts spans have similar syntactic structure. However, while the similarity is clear to human readers, it is often not easy to formally define, such as in:\n“about/IN half/NN its/PRP$ revenue/NN and/CC\nmore/JJR than/IN half/NN its/PRP$ profit/NN” If we could score the amount of similarity between two spans, we could use that to identify correct coordination structures. However, we do not know the similarity function. We approach this by training the similarity function in a data-dependent manner. Specifically, we train an encoder that encodes spans into vectors such that vectors of similar spans will have a small Euclidean distance between them. This architecture is similar to Siamese Networks, which are used for learning similarity functions in vision tasks (Chopra et al., 2005).\nVP\nVB cut NP PRP$\ntheir\nNNS risks\nVP\nVB take NP NNS\nprofits\nR VP VB cut cut V B V P R\nL VP R NP\nPRP$ their\ntheir PR P$ N P R V P L\nVP L NP NNS risks risks N N S N P L V P\nR VP VB take take V B V P R\nL VP NP NNS profits\nprofits N N S N P V P L\nEuclidean Distance\nFigure 2: Illustration of the symmetry scoring component that takes into account the conjuncts syntactic structures. Each conjunct tree is decomposed into paths that are fed into the path-LSTMs (squares). The resulting vectors are fed into the symmetry LSTM function (circles). The outcome vectors (blue circles) are then fed into the euclidean distance function.\nGiven two spans of lengths k and m with corresponding vector sequences u1:k and v1:m we encode each sequences using an LSTM, and take the euclidean distance between the resulting representations:\nSym(u1:k, v1:m) = ||LSTM(u1:k)− LSTM(v1:m)||\nThe network is trained such that the distance is minimized for compatible spans and large for incompatible ones in order to learn that vectors that represent correct conjuncts are closer than vectors that do not represent conjuncts.\nWhat are the elements in the sequences to be compared?\nOne choice is to take the vectors ui to correspond to embeddings of the ith POS in the span.\nThis approach works reasonably well, but does not consider the conjuncts’ syntactic structure, which may be useful as symmetry often occurs on a higher level than POS tags. For example, in:\nNP\nNP\nNN\ntomorrow\nPP\nIN\nat\nCD\n16:00\nCC\nor\nNP\nNP\nNP\nthe day\nPP\nafter tomorrow\nPP\nIN\nat\nCD\n12:00\nthe similarity is more substantial in the third level of the tree than in the POS level.\nA way to allow the model access to higher levels of syntactic symmetry is to represent each word as the projection of the grammatical functions from the word to the root.2\nFor example, the projections for the first conjunct in Figure 2 are:\n2Similar in spirit to the spines used in Carreras et al. (2008) and Shen et al. (2003).\nVP\nVB\ncut\nVP\nNP\nPRP$\ntheir\nVP\nNP\nNNS\nrisks\nThis decomposition captures the syntactic context of each word, but does not uniquely determine the structure of the tree. To remedy this, we add to the paths special symbols, R and L, which marks the lowest common ancestors with the right and left words respectively.\nThese are added to the path above the corresponding nodes. For example consider the following paths which corresponds to the above syntactic structure:\nR\nVP\nVB\ncut\nL\nVP\nR\nNP\nPRP$\ntheir\nVP\nL\nNP\nNNS\nrisks\nThe lowest common ancestor of “their” and “risks” is NP. Thus, R is added after NP in the path of “their” and L is added after NP in the path of “risks”. Similarly, L and R are added after the VP in the “their” and “cut” paths.\nThe path for each word is encoded using an LSTM receiving vector embeddings of the elements in the path from the word to the root. We then use the resulting encodings instead of the POS-tag embeddings as input to the LSTMs in the similarity function.\nFigure 2 depicts the complete process for the spans “cut their risks” and “take profits”.\nUsing syntactic projections requires the syntactic structures of the conjuncts. This is obtained by running the Berkeley parser over the sentence and taking the subtree with the highest probability from the\ncorresponding cell in the CKY chart.3 In both approaches, the POS embeddings are initialized with vectors that are pre-trained by running word2vec (Mikolov et al., 2013) on the POS sequences in PTB training set."
    }, {
      "heading" : "4.2 The Replacement Component",
      "text" : "The replacement component is based on the observation that, in many cases, the coordination phrase can be replaced with either one of its conjuncts while still preserving a grammatical and semantically coherent sentence (Section 2.2)\nWhen attempting such a replacement on incorrect conjuncts, the resulting sentence is likely to be either syntactically or semantically incorrect. For example, in the following erroneous analysis: “Rudolph Agnew, [55 years old] and [former chairman] of Consolidated Gold Fields PLC” replacing the conjunction with the first conjunct results in the semantically incoherent sequence “Rudolph Agnew, 55 years old of Consolidated Golden Fields, PLC”.4\nOur goal is to distinguish replacements resulting from correct conjuncts from those resulting from erroneous ones. To this end, we focus on the connection points. A connection point in a resulting sentence is the point where the sentence splits into two sequences that were not connected in the original sentence. For example, consider the sentence in Figure 3. It has four parts, marked as Pre, Conj1, Conj2 and Post. Replacing the coordination phrase Conj1 and Conj2 with Conj2 results in a connection point\n3The parser’s CKY chart did not include a tree for 10% of the candidate spans, which have inside probability 0 and outside probability > 0. For those, we obtained the syntactic structure by running the parser on the span words only.\n4While uncommon, incorrect conjuncts may also result in valid sentences, e.g. “He paid $ 7 for cold [drinks] and [pizza] that just came out of the oven.”\nbetween Pre and Conj2. Likewise, replacing the coordination phrase with Conj1 results in connection point between Conj1 and Post.\nIn order to model the validity of the connection points, we represent each connection point as the concatenation of a forward and reverse LSTMs centered around that point. Specifically, for the spans in Figure 3 the two connection points are represented as: LSTMF (Rudolph,...,old)◦LSTMB(director,...,was,,) and LSTMF (Rudolph,Agnew,,)◦LSTMB(director,...,former)\nFormally, assuming wordsw1:n in a sentence with coordination at position k and conjuncts wi:j and wl:m,5\nthe connection points are between w1:j and wm+1:n; and between w1:i−1 and wl:n. The two connection points representations are then concatenated, resulting in a replacement vector:\nREPL(w1:n, i, j, l,m) =\nCONPOINT(w1:n, i− 1, l) ◦ CONPOINT(w1:n, j,m+ 1)\nwhere: CONPOINT(w1:n, i, j) =\nLSTMF (w1:i) ◦ LSTMB(wn:j)\nWe use two variants of the replacement vectors, corresponding to two levels of representation. The first variant is based on the sentence’s words, while the second is based on its POS-tags."
    }, {
      "heading" : "4.3 Parser based Features",
      "text" : "In addition to the symmetry and replacement signals, we also incorporate some scores that are derived from the Berkeley parser. As detailed in Section 5, a list of conjuncts candidates are extracted\n5Usually j = k − 1 and l = k + 1, but in some cases punctuation symbols may interfere.\nfrom the CKY chart of the parser. The candidates are then sorted in descending order according to the multiplication of inside and outside scores of the candidate’s spans:6 I(i,j) ×O(i,j) × I(l,m) ×O(l,m). Each candidate {(i, j), (l,m)} is assigned two numerical features based on this ranking: its position in the ranking, and the ratio between its score and the score of the adjacent higher-ranked candidate. We add an additional binary feature indicating whether the candidate spans are in the 1-best tree predicted by the parser. These three features are denoted as Feats(i, j, l,m)."
    }, {
      "heading" : "4.4 Final Scoring and Training",
      "text" : "Finally, the score of a candidate {(i, j), (l,m)} in a sentence with words w1:n and POS tags p1:n is computed as:\nSCORE(w1:n, p1:n, {(i, j), (l,m)}) = MLP (\nSym(vPathi:j , v Path l:m ) ◦Repl(w1:n, i, j, l,m) ◦Repl(p1:n, i, j, l,m) ◦ Feats(i, j, l,m) )\nwhere vPathi:j and v Path l:m are the vectors resulting from the path LSTMs, and Sym, Repl and Feats are the networks defined in Sections 4.1 – 4.3 above. The network is trained jointly, attempting to minimize a pairwise ranking loss function, where the loss for each training case is given by:\nloss = max(0, 1− (ŷ − yg)) where ŷ is the highest scoring candidate and yg is the correct candidate. The model is trained on all the coordination cases in Section 2–21 in the PTB."
    }, {
      "heading" : "5 Candidates Extraction and Supporting Classifiers",
      "text" : "Candidates Extraction We extract candidate spans based on the inside-outside probabilities assigned by the Berkeley parser. Specifically, to obtain\n6Inside-Outside probabilities (Goodman, 1998) represent the probability of a span with a given non-terminal symbol. The inside probability I(N,i,j) is the probability of generating words wi, wi+1, ..., wj given that the root is the non-terminal N . The outside probability O(N,i,j) is the probability of generating words w1, w2, ..., wi−1, the non-terminal N and the words wj+1, wj+2, ..., wn with the root S.\ncandidates for conjunct span we collect spans that are marked with COORD, are adjacent to the coordinating word, and have non-zero inside or outside probabilities. We then take as candidates all possible pairs of collected spans.\nOn the PTB dev set, this method produces 6.25 candidates for each coordinating word on average and includes the correct candidates for 94% of the coordinations.\nCoordination Classification We decide whether a coordination word wk in a sentence w1:n functions as a coordinator by feeding the biLSTM vector centered around wk into a logistic classifier: σ(v · biLSTM(w1:n, k) + b). The training examples are all the coordination words (marked with CC) in the PTB training set. The model achieves 99.46 F1 on development set and 99.19 F1 on test set.\nNP coordinations amount to about half of the coordination cases in the PTB, and previous work is often evaluated specifically on NP coordination. When evaluating on NP coordination, we depart from the unrealistic scenario used in most previous work where the type of coordination is assumed to be known a-priori, and train a specialized model for predicting the coordination type. For a coordination candidate {(i, j), (l,m)} with a coordinator wk, we predict if it is NP coordination or not by feeding a logistic classifier with a biLSTM vector centered around the coordinator and constrained to the candidate spans:\nσ(v · biLSTM(wi:m, k) + b). The training examples are coordinations in the PTB training set, where where a coordinator is considered of type NP if its head is labeled with NP or NX. Evaluating on gold coordinations results in F1 scores of 95.06 (dev) and 93.89 (test)."
    }, {
      "heading" : "6 Experiments",
      "text" : "We evaluate our models on their ability to identify conjunction boundaries in the extended Penn Treebank (Ficler and Goldberg, 2016) and Genia Treebank (Ohta et al., 2002)7.\nWhen evaluating on the PTB, we compare to the conjunction boundary predictions of the generative\n7http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA\nBerkeley parser (Petrov et al., 2006) and the discriminative Zpar parser (Zhang and Clark, 2011). When evaluating on the Genia treebank, we compare to the results of the discriminative coordination-prediction model of Hara et al. (2009).8"
    }, {
      "heading" : "6.1 Evaluation on PTB",
      "text" : "Baseline Our baseline is the performance of the Berkeley and Zpar parsers on the task presented in Section 3, namely: for a given coordinating word, determine the two spans that are being conjoined by it, and return NONE if the coordinator is not conjoining spans or conjoins spans that are not of the expected type. We convert predicted trees to conjunction predictions by taking the two phrases that are immediately adjacent to the coordinator on both sides (ignoring phrases that contain solely punctuation). For example, in the following Zparpredicted parse tree the conjunct prediction is (“Feb. 8, 1990”,“May 10, 1990”).\nNP\nNP\nFeb. 8, 1990\nCC\nand\nNP\nMay 10, 1990\n,\n,\nADJP\nrespectively\nCases in which the coordination word is the leftmost or right-most non-punctuation element in its phrase (e.g. (PRN (P -)(CC and)(S it’s been painful)(P -))) are considered as nocoordination (“None”).\n8Another relevant model in the literature is (Hanamoto et al., 2012), however the results are not directly comparable as they use a slightly different definition of conjuncts, and evaluate on a subset of the Genia treebank, containing only trees that were properly converted to an HPSG formalism.\nWe consider two setups. In the first we are interested in all occurrences of coordination, and in the second we focus on NP coordination. The second scenario requires typed coordinations. We take the type of a parser-predicted coordination to be the type of the phrase immediately dominating the coordination word. Evaluation Metrics We measure precision and recall compared to the gold-annotated coordination spans in the extended PTB, where an example is considered correct if both conjunct boundaries match exactly. When focusing on NPs coordinations, the type of the phrase above the CC level must match as well, and phrases of type NP/NX are considered as NP coordination. Results Tables (1) and (2) summarize the results. The Berkeley and Zpar parsers perform similarly on the coordination prediction task. Our proposed model outperforms both parsers, with a test-set F1 score of 72.7 (3.78 F1 points gain over the better parser) when considering all coordinations, and testset F1 score of 76.1 (4.77 F1 points gain) when considering NP coordination."
    }, {
      "heading" : "6.2 Evaluation on Genia",
      "text" : "To compare our model to previous work, we evaluate also on the Genia treebank (Beta), a collection of constituency trees for 4529 sentences from Medline abstracts. The Genia treebank coordination annotation explicitly marks coordination phrases with a special function label (COOD), making the corpus an appealing resource for previous work on coordination boundary prediction (Shimbo and Hara, 2007; Hara et al., 2009; Hanamoto et al., 2012).\nFollowing Hara et al. (2009), we evaluate the models’ ability to predict the span of the entire coordination phrase, disregarding the individual conjuncts. For example, in “My plan is to visit Seychelles, ko Samui and Sardinia by the end of the year” the goal is to recover “Seychelles, ko Samui and Sardinia”. This is a recall measure. We follow the exact protocol of Hara et al. (2009) and train and evaluate the model on 3598 coordination phrases in Genia Treebank Beta and report the micro-averaged results of a five-fold cross validation run.9 As shown\n9We thank Kazuo Hara for providing us with the exact details of their splits.\nby Hara et al. (2009), syntactic parsers do not perform well on the Genia treebank. Thus, in our symmetry component we opted to not rely on predicted tree structures, and instead use the simpler option of representing each conjunct by its sequence of POS tags. To handle coordination phrases with more than two conjuncts, we extract candidates which includes up to 7 spans and integrate the first and the last span in the model features. Like Hara et al., we use gold POS. Results Table 3 summarizes the results. Our proposed model achieves Recall score of 64.14 (2.64 Recall points gain over Hara et al.) and significantly improves the score of several coordination types."
    }, {
      "heading" : "6.3 Technical Details",
      "text" : "The neural networks (candidate scoring model and supporting classifiers) are implemented using the pyCNN package.10.\nIn the supporting models we use words embedding of size 50 and the Sigmoid activation function. The LSTMs have a dimension of 50 as well. The models are trained using SGD for 10 iterations over the train-set, where samples are randomly shuffled before each iteration. We choose the model with the highest F1 score on the development set.\nAll the LSTMs in the candidate scoring model have a dimension of 50. The input vectors for the\n10https://github.com/clab/cnn/tree/master/pycnn\nsymmetry LSTM is of size 50 as well. The MLP in the candidate scoring model uses the Relu activation function, and the model is trained using the Adam optimizer. The words and POS embeddings are shared between the symmetry and replacment components. The syntactic label embeddings are for the path-encoding LSTM, We perform grid search with 5 different seeds and the following: [1] MLP hidden layer size (100, 200, 400); [2] input embeddings size for words, POS and syntactic labels (100, 300). We train for 20 iterations over the train set, randomly shuffling the examples before each iteration. We choose the model that achieves the highest F1 score on the dev set."
    }, {
      "heading" : "7 Analysis",
      "text" : "Our model combines four signals: symmetry, wordlevel replacement, POS-level replacement and features from Berkeley parser. Table 4 shows the PTB dev-set performance of each sub-model in isolation. On their own, each of the components’ signals is relatively weak, seldom outperforming the parsers. However, they provide complementary information, as evident by the strong performance of the joint model. Figure 4 lists correct and incorrect predictions by each of the components, indicating that the individual models are indeed capturing the patterns they were designed to capture – though these patterns do not always lead to correct predictions."
    }, {
      "heading" : "8 Related Work",
      "text" : "The similarity property between conjuncts was explored in several previous works on coordination disambiguation. Hogan (2007) incorporated this principle in a generative parsing model by changing the generative process of coordinated NPs to condition on properties of the first conjunct when generating the second one.\nShimbo and Hara (2007) proposed a discriminative sequence alignment model to detect similar conjuncts. They focused on disambiguation of nonnested coordination based on the learned edit distance between two conjuncts. Their work was extended by Hara et al. (2009) to handle nested coordinations as well. The discriminative edit distance model in these works is similar in spirit to our symmetry component, but is restricted to sequences of POS-tags, and makes use of a sequence alignment algorithm. We compare our results to Hara et al.’s in Section 6.2. Hanamoto et al. (2012) extended the previous method with dual decomposition and HPSG parsing. In contrast to these symmetry-directed efforts, Kawahara et al. (2008) focuses on the dependency relations that surround the conjuncts. This kind of semantic information provides an additional signal which is complementary to the syntactic signals explored in our work. Our neural-network based model easily supports incorporation of additional signals, and we plan to explore such semantic signals in future work."
    }, {
      "heading" : "9 Conclusions",
      "text" : "We presented an neural-network based model for resolving conjuncts boundaries. Our model is based on the observation that (a) conjuncts tend to be similar and (b) that replacing the coordination phrase with a conjunct results in a coherent sentence. Our models rely on syntactic information and do not incorporate resources external to the training treebanks, yet improve over state-of-the-art parsers on the coordination boundary prediction task."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work was supported by The Israeli Science Foundation (grant number 1555/15) as well as the German Research Foundation via the German-\nIsraeli Project Cooperation (DIP, grant DA 1600/1- 1)."
    } ],
    "references" : [ {
      "title" : "Bracketing guidelines for treebank ii style penn treebank project",
      "author" : [ "Bies et al.1995] Ann Bies", "Mark Ferguson", "Karen Katz", "Robert MacIntyre", "Victoria Tredinnick", "Grace Kim", "Mary Ann Marcinkiewicz", "Britta Schasberger" ],
      "venue" : null,
      "citeRegEx" : "Bies et al\\.,? \\Q1995\\E",
      "shortCiteRegEx" : "Bies et al\\.",
      "year" : 1995
    }, {
      "title" : "Tag, dynamic programming, and the perceptron for efficient, feature-rich parsing",
      "author" : [ "Michael Collins", "Terry Koo" ],
      "venue" : "In Proceedings of the Twelfth Conference on Computational Natural Language Learning,",
      "citeRegEx" : "Carreras et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Carreras et al\\.",
      "year" : 2008
    }, {
      "title" : "Learning a similarity metric discriminatively, with application to face verification",
      "author" : [ "Chopra et al.2005] Sumit Chopra", "Raia Hadsell", "Yann LeCun" ],
      "venue" : "In Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Chopra et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Chopra et al\\.",
      "year" : 2005
    }, {
      "title" : "Type raising, functional composition, and non-constituent conjunction",
      "author" : [ "David Dowty" ],
      "venue" : "In Categorial grammars and natural language structures,",
      "citeRegEx" : "Dowty.,? \\Q1988\\E",
      "shortCiteRegEx" : "Dowty.",
      "year" : 1988
    }, {
      "title" : "Finding structure in time",
      "author" : [ "Jeffrey L Elman" ],
      "venue" : "Cognitive science,",
      "citeRegEx" : "Elman.,? \\Q1990\\E",
      "shortCiteRegEx" : "Elman.",
      "year" : 1990
    }, {
      "title" : "Coordination annotation extension in the penn tree bank. Association for Computational Linguistics",
      "author" : [ "Ficler", "Goldberg2016] Jessica Ficler", "Yoav Goldberg" ],
      "venue" : null,
      "citeRegEx" : "Ficler et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Ficler et al\\.",
      "year" : 2016
    }, {
      "title" : "Parsing insideout. arXiv preprint cmp-lg/9805007",
      "author" : [ "Joshua Goodman" ],
      "venue" : null,
      "citeRegEx" : "Goodman.,? \\Q1998\\E",
      "shortCiteRegEx" : "Goodman.",
      "year" : 1998
    }, {
      "title" : "Coordination structure analysis using dual decomposition",
      "author" : [ "Takuya Matsuzaki", "Jun’ichi Tsujii" ],
      "venue" : "In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics,",
      "citeRegEx" : "Hanamoto et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Hanamoto et al\\.",
      "year" : 2012
    }, {
      "title" : "Coordinate structure analysis with global structural constraints and alignment-based local features",
      "author" : [ "Hara et al.2009] Kazuo Hara", "Masashi Shimbo", "Hideharu Okuma", "Yuji Matsumoto" ],
      "venue" : "In Proceedings of the Joint Conference of the 47th Annual",
      "citeRegEx" : "Hara et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Hara et al\\.",
      "year" : 2009
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "Jürgen Schmidhuber" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "Hochreiter et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Hochreiter et al\\.",
      "year" : 1997
    }, {
      "title" : "Coordinate noun phrase disambiguation in a generative parsing model. Association for Computational Linguistics",
      "author" : [ "Deirdre Hogan" ],
      "venue" : null,
      "citeRegEx" : "Hogan.,? \\Q2007\\E",
      "shortCiteRegEx" : "Hogan.",
      "year" : 2007
    }, {
      "title" : "The cambridge grammar of english",
      "author" : [ "Geoffrey K Pullum" ],
      "venue" : null,
      "citeRegEx" : "Huddleston and Pullum,? \\Q2002\\E",
      "shortCiteRegEx" : "Huddleston and Pullum",
      "year" : 2002
    }, {
      "title" : "Coordination disambiguation without any similarities",
      "author" : [ "Kawahara", "Kurohashi2008] Daisuke Kawahara", "Sadao Kurohashi" ],
      "venue" : "In Proceedings of the 22nd International Conference on Computational Linguistics-Volume",
      "citeRegEx" : "Kawahara et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Kawahara et al\\.",
      "year" : 2008
    }, {
      "title" : "Building a large annotated corpus of english: The penn treebank",
      "author" : [ "Mary Ann Marcinkiewicz", "Beatrice Santorini" ],
      "venue" : null,
      "citeRegEx" : "Marcus et al\\.,? \\Q1993\\E",
      "shortCiteRegEx" : "Marcus et al\\.",
      "year" : 1993
    }, {
      "title" : "Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781",
      "author" : [ "Kai Chen", "Greg Corrado", "Jeffrey Dean" ],
      "venue" : null,
      "citeRegEx" : "Mikolov et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "The genia corpus: An annotated research abstract corpus in molecular biology domain",
      "author" : [ "Ohta et al.2002] Tomoko Ohta", "Yuka Tateisi", "JinDong Kim" ],
      "venue" : "In Proceedings of the second international conference on Human Language Technology Research,",
      "citeRegEx" : "Ohta et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Ohta et al\\.",
      "year" : 2002
    }, {
      "title" : "Learning accurate, compact, and interpretable tree annotation",
      "author" : [ "Petrov et al.2006] Slav Petrov", "Leon Barrett", "Romain Thibaux", "Dan Klein" ],
      "venue" : "In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting",
      "citeRegEx" : "Petrov et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Petrov et al\\.",
      "year" : 2006
    }, {
      "title" : "Using ltag based features in parse reranking",
      "author" : [ "Shen et al.2003] Libin Shen", "Anoop Sarkar", "Aravind K Joshi" ],
      "venue" : "In Proceedings of the 2003 conference on Empirical methods in natural language processing,",
      "citeRegEx" : "Shen et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2003
    }, {
      "title" : "A discriminative learning model for coordinate conjunctions",
      "author" : [ "Shimbo", "Hara2007] Masashi Shimbo", "Kazuo Hara" ],
      "venue" : "In EMNLP-CoNLL,",
      "citeRegEx" : "Shimbo et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Shimbo et al\\.",
      "year" : 2007
    }, {
      "title" : "Syntactic processing using the generalized perceptron and beam search",
      "author" : [ "Zhang", "Clark2011] Yue Zhang", "Stephen Clark" ],
      "venue" : "Computational linguistics,",
      "citeRegEx" : "Zhang et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 13,
      "context" : "8% of the sentences in the Penn Treebank (PTB) (Marcus et al., 1993), and in 60.",
      "startOffset" : 47,
      "endOffset" : 68
    }, {
      "referenceID" : 15,
      "context" : "71% of the sentences in the Genia Treebank (Ohta et al., 2002).",
      "startOffset" : 43,
      "endOffset" : 62
    }, {
      "referenceID" : 16,
      "context" : "Both the Berkeley and Zpar phrase-structure parsers (Petrov et al., 2006; Zhang and Clark, 2011) achieve F1 scores of around 69% when evaluated on their ability to recover coordination boundaries on the PTB test set.",
      "startOffset" : 52,
      "endOffset" : 96
    }, {
      "referenceID" : 10,
      "context" : "Semantic signals (which are likely to be based on resources external to the treebank) are also relevant for coordination disambiguation (Kawahara and Kurohashi, 2008; Hogan, 2007) and provide complementary information.",
      "startOffset" : 136,
      "endOffset" : 179
    }, {
      "referenceID" : 3,
      "context" : "Less common coordinations involve non-constituent elements “[equal to] or [higher than]”, argument clusters (“Alice visited [4 planets] [in 2014] and [3 more] [since then]”), and gapping (“[Bob lives on Earth] and [Alice on Saturn]”) (Dowty, 1988).",
      "startOffset" : 234,
      "endOffset" : 247
    }, {
      "referenceID" : 10,
      "context" : "Similarity between conjuncts was used as a guiding principle in previous work on coordination disambiguation (Hogan, 2007; Shimbo and Hara, 2007; Hara et al., 2009).",
      "startOffset" : 109,
      "endOffset" : 164
    }, {
      "referenceID" : 8,
      "context" : "Similarity between conjuncts was used as a guiding principle in previous work on coordination disambiguation (Hogan, 2007; Shimbo and Hara, 2007; Hara et al., 2009).",
      "startOffset" : 109,
      "endOffset" : 164
    }, {
      "referenceID" : 13,
      "context" : "3 Coordination in the PTB Coordination annotation in the Penn Treebank (Marcus et al., 1993) is inconsistent (Hogan, 2007) and lacks internal structure for NPs with nominal modifiers (Bies et al.",
      "startOffset" : 71,
      "endOffset" : 92
    }, {
      "referenceID" : 10,
      "context" : ", 1993) is inconsistent (Hogan, 2007) and lacks internal structure for NPs with nominal modifiers (Bies et al.",
      "startOffset" : 24,
      "endOffset" : 37
    }, {
      "referenceID" : 0,
      "context" : ", 1993) is inconsistent (Hogan, 2007) and lacks internal structure for NPs with nominal modifiers (Bies et al., 1995).",
      "startOffset" : 98,
      "endOffset" : 117
    }, {
      "referenceID" : 8,
      "context" : "These deficiencies led previous works on coordination disambiguation (Shimbo and Hara, 2007; Hara et al., 2009; Hanamoto et al., 2012) to use the Genia treebank of biomedical text (Ohta et al.",
      "startOffset" : 69,
      "endOffset" : 134
    }, {
      "referenceID" : 7,
      "context" : "These deficiencies led previous works on coordination disambiguation (Shimbo and Hara, 2007; Hara et al., 2009; Hanamoto et al., 2012) to use the Genia treebank of biomedical text (Ohta et al.",
      "startOffset" : 69,
      "endOffset" : 134
    }, {
      "referenceID" : 15,
      "context" : ", 2012) to use the Genia treebank of biomedical text (Ohta et al., 2002) which explicitly marks coordination phrases.",
      "startOffset" : 53,
      "endOffset" : 72
    }, {
      "referenceID" : 4,
      "context" : "Recurrent Neural Networks (RNNs) (Elman, 1990) allow the representation of arbitrary sized sequences.",
      "startOffset" : 33,
      "endOffset" : 46
    }, {
      "referenceID" : 2,
      "context" : "This architecture is similar to Siamese Networks, which are used for learning similarity functions in vision tasks (Chopra et al., 2005).",
      "startOffset" : 115,
      "endOffset" : 136
    }, {
      "referenceID" : 1,
      "context" : "2 For example, the projections for the first conjunct in Figure 2 are: Similar in spirit to the spines used in Carreras et al. (2008) and Shen et al.",
      "startOffset" : 111,
      "endOffset" : 134
    }, {
      "referenceID" : 1,
      "context" : "2 For example, the projections for the first conjunct in Figure 2 are: Similar in spirit to the spines used in Carreras et al. (2008) and Shen et al. (2003). VP",
      "startOffset" : 111,
      "endOffset" : 157
    }, {
      "referenceID" : 14,
      "context" : "3 In both approaches, the POS embeddings are initialized with vectors that are pre-trained by running word2vec (Mikolov et al., 2013) on the POS sequences in PTB training set.",
      "startOffset" : 111,
      "endOffset" : 133
    }, {
      "referenceID" : 6,
      "context" : "Inside-Outside probabilities (Goodman, 1998) represent the probability of a span with a given non-terminal symbol.",
      "startOffset" : 29,
      "endOffset" : 44
    }, {
      "referenceID" : 15,
      "context" : "We evaluate our models on their ability to identify conjunction boundaries in the extended Penn Treebank (Ficler and Goldberg, 2016) and Genia Treebank (Ohta et al., 2002)7.",
      "startOffset" : 152,
      "endOffset" : 171
    }, {
      "referenceID" : 16,
      "context" : "Berkeley parser (Petrov et al., 2006) and the discriminative Zpar parser (Zhang and Clark, 2011).",
      "startOffset" : 16,
      "endOffset" : 37
    }, {
      "referenceID" : 8,
      "context" : "When evaluating on the Genia treebank, we compare to the results of the discriminative coordination-prediction model of Hara et al. (2009).8",
      "startOffset" : 120,
      "endOffset" : 139
    }, {
      "referenceID" : 7,
      "context" : "Another relevant model in the literature is (Hanamoto et al., 2012), however the results are not directly comparable as they use a slightly different definition of conjuncts, and evaluate on a subset of the Genia treebank, containing only trees that were properly converted to an HPSG formalism.",
      "startOffset" : 44,
      "endOffset" : 67
    }, {
      "referenceID" : 8,
      "context" : "The Genia treebank coordination annotation explicitly marks coordination phrases with a special function label (COOD), making the corpus an appealing resource for previous work on coordination boundary prediction (Shimbo and Hara, 2007; Hara et al., 2009; Hanamoto et al., 2012).",
      "startOffset" : 213,
      "endOffset" : 278
    }, {
      "referenceID" : 7,
      "context" : "The Genia treebank coordination annotation explicitly marks coordination phrases with a special function label (COOD), making the corpus an appealing resource for previous work on coordination boundary prediction (Shimbo and Hara, 2007; Hara et al., 2009; Hanamoto et al., 2012).",
      "startOffset" : 213,
      "endOffset" : 278
    }, {
      "referenceID" : 7,
      "context" : ", 2009; Hanamoto et al., 2012). Following Hara et al. (2009), we evaluate the models’ ability to predict the span of the entire coordination phrase, disregarding the individual conjuncts.",
      "startOffset" : 8,
      "endOffset" : 61
    }, {
      "referenceID" : 7,
      "context" : ", 2009; Hanamoto et al., 2012). Following Hara et al. (2009), we evaluate the models’ ability to predict the span of the entire coordination phrase, disregarding the individual conjuncts. For example, in “My plan is to visit Seychelles, ko Samui and Sardinia by the end of the year” the goal is to recover “Seychelles, ko Samui and Sardinia”. This is a recall measure. We follow the exact protocol of Hara et al. (2009) and train and evaluate the model on 3598 coordination phrases in Genia Treebank Beta and report the micro-averaged results of a five-fold cross validation run.",
      "startOffset" : 8,
      "endOffset" : 420
    }, {
      "referenceID" : 8,
      "context" : "by Hara et al. (2009), syntactic parsers do not perform well on the Genia treebank.",
      "startOffset" : 3,
      "endOffset" : 22
    }, {
      "referenceID" : 8,
      "context" : "Hogan (2007) incorporated this principle in a generative parsing model by changing the generative process of coordinated NPs to condition on properties of the first conjunct when generating the second one.",
      "startOffset" : 0,
      "endOffset" : 13
    }, {
      "referenceID" : 8,
      "context" : "Hogan (2007) incorporated this principle in a generative parsing model by changing the generative process of coordinated NPs to condition on properties of the first conjunct when generating the second one. Shimbo and Hara (2007) proposed a discriminative sequence alignment model to detect similar conjuncts.",
      "startOffset" : 0,
      "endOffset" : 229
    }, {
      "referenceID" : 7,
      "context" : "Their work was extended by Hara et al. (2009) to handle nested coordinations as well.",
      "startOffset" : 27,
      "endOffset" : 46
    }, {
      "referenceID" : 7,
      "context" : "Hanamoto et al. (2012) extended the previous method with dual decomposition and HPSG parsing.",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 7,
      "context" : "Hanamoto et al. (2012) extended the previous method with dual decomposition and HPSG parsing. In contrast to these symmetry-directed efforts, Kawahara et al. (2008) focuses on the dependency relations that surround the conjuncts.",
      "startOffset" : 0,
      "endOffset" : 165
    } ],
    "year" : 2016,
    "abstractText" : "We propose a neural-network based model for coordination boundary prediction. The network is designed to incorporate two signals: the similarity between conjuncts and the observation that replacing the whole coordination phrase with a conjunct tends to produce a coherent sentences. The modeling makes use of several LSTM networks. The model is trained solely on conjunction annotations in a Treebank, without using external resources. We show improvements on predicting coordination boundaries on the PTB compared to two state-of-the-art parsers; as well as improvement over previous coordination boundary prediction systems on the Genia corpus.",
    "creator" : "LaTeX with hyperref package"
  }
}