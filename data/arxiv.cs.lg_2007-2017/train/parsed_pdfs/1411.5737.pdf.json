{
  "name" : "1411.5737.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ "damelin@umich.edu." ],
    "sections" : [ {
      "heading" : null,
      "text" : "i i\ni i\nKeywords and phrases: diffusion maps, nonlinear dimensionality reduction, spectral, fuzzy adaptive resonance theory, fuzzy adaptive resonance diffusion, clustering, biclustering\nMathematics Subject Classification: 94A15, 62H30, 60J20, 68T05, 68T45, 68T10"
    }, {
      "heading" : "1. Introduction",
      "text" : "In this paper, we describe an algorithm FARDiff (Fuzzy Adaptive Resonance Diffusion) which combines Diffusion Maps and Fuzzy Adaptive Resonance Theory to do clustering on high dimensional data. This algorithm was introduced in the papers [9, 10, 18, 19, 21]. We describe some applications of this method and some problems for future research.\nThe dimensionality reduction part of FARDiff is achieved via a nonlinear diffusion map, which interprets the eigenfunctions of Markov matrices as a system of coordinates on the dataset in order to obtain\n∗Corresponding author. E-mail: damelin@umich.edu.\nar X\niv :1\n41 1.\n57 37\n“Clustering˙and˙Biclustering” — 2015/10/7 — 4:03 — page 2 — #2i i\ni i\ni i\ni i\nS. B. Damelin, Y. Gu, D. C. Wunsch II, R. Xu ART, Diffusion Maps and App. to Clustering and Biclustering\nan efficient representation of certain geometric descriptions of the data. Our algorithm is sensitive to the connectivity of the data points. Clustering was achieved using Fuzzy Adaptive Resonance Theory.\nThe structure of this paper is as follows. Section 2 will describe the dimension reduction, section 3 will describe the clustering method, section 4 will discuss some applications, and section 5 will discuss work in progress."
    }, {
      "heading" : "2. Diffusion Maps",
      "text" : "In this section, we describe the nonlinear dimension reduction spectral method part of FARDiff. We use nonlinear diffusion maps. See [6, 15] and references sited there in.\nLet m be a large enough fixed non-negative integer, and let X = {xi, i = 1, . . . , N} ⊂ Rm be a data set consisting of N ≥ 1 distinct points. A finite graph with N nodes corresponding to N data points can be constructed on X as follows. Every two nodes in the graph are connected by an edge weighted through a Gaussian kernel, w : Rm × Rm → R, defined by\nw(z, z′) = exp ( − ∥∥z− z′∥∥2\nσ2\n) , z, z′ ∈ Rm,\nwhere ‖ · ‖ is the Euclidean norm in Rm and σ is a fixed nonzero real number to be chosen in a moment. w here satisfies symmetric property w(z, z′) = w(z′, z) and positivity preserving property w(z, z′) ≥ 0. The first property is useful for performing spectral analysis, and the second property allows it to be interpreted as a scaled probability.\nWe choose σ to be a parameter depending on the data set X, and we shall write for each 1 ≤ i, j ≤ N ,\nw(xi,xj) = exp\n( − ∥∥xi − xj∥∥2\nσ2\n) , xi, xj ∈ X. (2.1)\nw reflects the degree of similarity between xi and xj . The resulting symmetric semi-positive definite matrix W = {w(xi,xj)}N×N is called the affinity matrix.\nLet d : X→ R be defined by d(x) = ∑ x′∈X w(x,x′), x ∈ X, (2.2) be the degree of x. For each 1 ≤ i, j ≤ N , a Markov or affinity matrix PN×N is then constructed by calculating each entry of P as\np(xi,xj) = w(xi,xj)\nd(xi) . (2.3)\nFrom the definition of w, p(xi,xj) can be interpreted as the transition probability from xi to xj in one time step. From the definition of the Gaussian kernel it can be seen that the transition probability will be high for similar elements. This idea can be further extended by considering pt(xi,xj) in the t\nth power Pt of P as the probability of transition from xi to xj in t time steps [6], where t is a fixed non-negative integer. Here, pt(xi,xj) sums all paths of length t from point xi to point xj . Hence, the parameter t defines the granularity of the analysis. With the increase of the value of t, local geometric information of data is also integrated. The change in size of t makes it possible to control the generation of more specific or broader clusters.\nThe diffusion distance is related to P and is given by\nDt(xi,xj) 2 = ∑ x∈X ∣∣∣pt(xi,x)− pt(xj ,x)∣∣∣2. (2.4) It can be seen that the more paths that connect two points in the graph, the higher probability of paths of length t between two points is, and the smaller the diffusion distance is. Therefore, the diffusion distance can be used as a measurement of a specific geometric structure.\n2\n“Clustering˙and˙Biclustering” — 2015/10/7 — 4:03 — page 3 — #3i i\ni i\ni i\ni i\nS. B. Damelin, Y. Gu, D. C. Wunsch II, R. Xu ART, Diffusion Maps and App. to Clustering and Biclustering\nNext, we aim to find a mapping that reassembles the data in a lower dimensional space but preserves the diffusion distances between data points optimally. Consider a mapping\nΨ ′t(xi) = ( pt(xi,x1), . . . , p t(xi,xN ) )T , 1 ≤ i ≤ N. (2.5)\nThe Euclidean distance between two points xi and xj is∥∥Ψ ′t(xi)− Ψ ′t(xj)∥∥2 = ∑ x∈X ∣∣∣pt(xi,x)− pt(xj ,x)∣∣∣2, which is also the diffusion distance. The mapping thus preserves the original data structure in the sense of diffusion distance.\nNext, we need to perform dimensionality reduction. Because of the symmetry property of w, for each t, we may obtain a sequence of N eigenvalues of P, 1 = λ1 > λ2 > · · · > λN ≥ 0, with corresponding eigenvectors {Φj , j = 1, . . . , N}, satisfying,\nPtΦj = λ t jΦj . (2.6)\nSuppose we wish to reduce our data from Rm to RL where 1 ≤ L < m1. To achieve it, we choose the first L eigenvalues and their corresponding eigenvectors of P, and rewrite the mapping (2.5) as\nΨ t(xi) = ( λt1Φ1(xi), . . . , λ t LΦL(xi) )T , (2.7)\nwhere Φj(xi) is the i th element of the eigenvector Φj . We observe that the new data point xi’s dimension has been reduced to L. The new mapping reduces the dimensionality of the original data set by retaining the important geometric structure associated with dominant eigenvalues and eigenvectors. Correspondingly, the diffusion distance between a pair of points xi and xj is approximated with the Euclidean distance in RL, written as\nDt(xi,xj) 2 = ∥∥∥Ψ t(xi)− Ψ t(xj)∥∥∥2. (2.8) The kernel width parameter σ represents the rate at which the similarity between two points decays2. One of the main reasons for using spectral clustering methods is that, with sparse kernel matrices, long range affinities are accommodated through the chaining of many local interactions as opposed to standard Euclidean distance methods - e.g. correlation - that impute global influence into each pair-wise affinity metric, making long range interactions dominate local interactions.\nWe have used a Gaussian Kernel scaled with parameter σ. This kernel works well for the two applications we have studied given both trade sparseness and affinity of the point set. FARDIFF however can be defined by way of a wide class of positive definite kernels, see [6,7] where the choice of kernel is typically application dependent. In addition to the choice of kernel, the trade off in sparsity can be handled by using a restricted isometry, see for example [1]."
    }, {
      "heading" : "3. Fuzzy Adaptive Resonance Theory",
      "text" : "In this section we describe the second part of FARDiff which uses Fuzzy Adaptive Resonance Theory (FA) [3,17,20] for clustering data points whose dimension has been reduced using the method of section 2. See [9, 10,18,19,21].\n1Note that here we require that N ≥ L. This is not a strong requirement given the reduced dimension L is typically small and the number of points are always larger.\n2Our choice of σ is determined by a trade off between sparseness of the kernel matrix (small σ) with adequate characterization of true affinity of two points.\n3\n“Clustering˙and˙Biclustering” — 2015/10/7 — 4:03 — page 4 — #4i i\ni i\ni i\ni i\nS. B. Damelin, Y. Gu, D. C. Wunsch II, R. Xu ART, Diffusion Maps and App. to Clustering and Biclustering\nFA allows stable recognition of clusters in response to both binary and real-valued input patterns with either fast or slow learning. The basic FA architecture consists of two-layer nodes or neurons, the feature representation field F1, and the category representation field F2, as shown in Figure 1. The neurons in layer F1 are activated by the input pattern, while the prototypes of the formed clusters are stored in layer F2. The neurons in layer F2 that are already being used as representations of input patterns are said to be committed. Correspondingly, the uncommitted neuron encodes no input patterns. The two layers are connected via adaptive weights wj , emanating from node j in layer F2. After an input pattern is presented, the neurons (including a certain number of committed neurons and one uncommitted neuron) in layer F2 compete by calculating the category choice function\nTj = Tj(x,wj , α) = ∣∣x ∧wj∣∣ α+\n∣∣wj∣∣ , (3.1) where ∧ is the fuzzy AND operator defined as follows. Given y = {yi, i = 1, . . . , N} ⊂ Rm and y′ = {y′i, i = 1, . . . , N} ⊂ Rm, define (\ny ∧ y′ ) i\n= min i\n( yi,y ′ i ) , (3.2)\nand α > 0 is the choice parameter to break the tie when more than one prototype vector is a fuzzy subset of the input pattern, based on the winner-take-all rule,\nTJ = max j {Tj}. (3.3)\nThe winning neuron J then becomes activated, and an expectation is reflected in layer F1 and compared with the input pattern. The orienting subsystem with the pre-specified vigilance parameter ρ (0 ≤ ρ ≤ 1) determines whether the expectation and the input pattern are closed matched. If the match meets the vigilance criterion,\nρ ≤ ∣∣x ∧wj∣∣∣∣x∣∣ , (3.4)\nweight adaptation occurs, where learning starts and the weights are updated using the following learning rule, wj(new) = β ( x ∧wj(old) ) + ( 1− β ) wj(old), (3.5)\nwhere β ∈ [0, 1] is the learning rate parameter. On the other hand, if the vigilance criterion is not met, a reset signal is sent back to layer F2 to shut off the current winning neuron, which will remain disabled for the entire duration of the presentation of this input pattern, and a new competition is performed among the rest of the neurons. This new expectation is then projected into layer F1, and this process repeats until the vigilance criterion is met. In the case that an uncommitted neuron is selected for coding, a new uncommitted neuron is created to represent a potential new cluster.\nSome of ART’s (Adaptive Resonance Theory) advantages are stability, biological plausibility, and responsiveness to the stability-plasticity dilemma. Further advantages are scalability, speed, configurability, and potential for parallelization. ART has been found to have good ability to interpret well, results on neural net learning and decision based networks with low complexity and good robustness [16]."
    }, {
      "heading" : "4. Applications in Cancer Detection and Hyperspectral Clustering",
      "text" : "The algorithm which we discuss in this paper which we call FARDiff (Fuzzy Adaptive Resonance Diffusion) combines 1. Diffusion Maps and 2. Fuzzy Adaptive Resonance Theory to do clustering on high dimensional data. This algorithm was introduced in the papers [9, 10,18,19,21].\nIn [18, 21], we applied FARDiff to investigate cancer detection. Early detection of the site of the origin of a tumor is particularly important for cancer diagnosis and treatment. The employment of\n4\n“Clustering˙and˙Biclustering” — 2015/10/7 — 4:03 — page 5 — #5i i\ni i\ni i\ni i\ngene expression profiles for different cancer types or subtypes has already shown significant advantages over traditional cancer classification methods. We applied FARDiff to the small round blue-cell tumor (SRBCT) data set, which is published from the diagnostic research of small round blue-cell tumors in children, and compared results with other widely-used clustering algorithms. The experimental results demonstrate the effectiveness of our method in extracting useful information from the high-dimensional data sets.\nIn [9,10,19], we applied FARDiff to study hyperspectral imaging. The images are sampled at hundreds of frequencies, and the bands are regularly spaced, thus a continuous spectrum can be drawn for every pixel in the image. We reassembled our data into a “hypercube,” called the hyperspectral data. Due to the fact that each pixel of the image contains more than a hundred bands, the occurrence of large amounts of hyperspectral data brings important challenges to storage and processing. We used FARDiff to investigate clustering of high dimensional hyperspectral image data from core samples provided by AngloCold Ashanti, and compared to results obtained by AngloGold Ashanti’s proprietary method. The experimental results demonstrate the potential of our method in addressing the complicated hyperspectral data and identifying the minerals in core samples."
    }, {
      "heading" : "5. Open Questions For Future Research",
      "text" : "In this section, we discuss some on going work. We are interested in extending the FARDiff algorithm to a biclustering framework and high dimensional data reduction. Biclustering is a technique which performs simultaneous clustering in many dimensions automatically integrating feature selection to clustering without any prior information. Two examples of good biclustering algorithms are BARTMAP and HBiFAM (Hierarchical Biclustering FA algorithm) [16,22].\nIt is well known that clustering has been used extensively in the analysis of high-throughput messenger RNA (mRNA) expression profiling with microarrays. This technique is restrictive, in part, due to the existence of many uncorrelated genes with respect to sample or condition clustering, or many unrelated samples or conditions with respect to gene clustering. Biclustering offers a solution to such problems by performing simultaneous clustering on both dimensions, or automatically integrating feature selection to clustering without any prior information, so that the relations of clusters of genes (generally, features) and clusters of samples or conditions (data objects) are established. Challenges which need to be addressed using this method include computational complexity and high dimensional data reduction. Current work focuses on developing a natural framework for an analog of FARDiff for biclustering and related computational complexity challenges, for example, traveling salesman problems. A second question relates to applications of FARDiff to more generalized clustering performance. A third research project investigates unified learning schemes and hardware implementation with ART and FARDiff in clustering and biclustering. A valuable new area of innovation will be the application of FARDiff to more generalized data structures such as trees and grammars. Continued progress on distributed representations is valu-\n5\n“Clustering˙and˙Biclustering” — 2015/10/7 — 4:03 — page 6 — #6i i\ni i\ni i\ni i\nS. B. Damelin, Y. Gu, D. C. Wunsch II, R. Xu ART, Diffusion Maps and App. to Clustering and Biclustering\nable because of increased data representation capability, both in terms of system capacity and template complexity. Several aspects of these questions are being investigated in ongoing work.\nAcknowledgements. Damelin gratefully acknowledges support from the School of Computational and Applied Mathematics at Wits, the American Mathematical Society, the Center for High Performance Computing and the National Science Foundation. Xu and Wunsch gratefully acknowledge support from the Missouri University of Science & Technology Intelligent Systems Center, and the M. K. Finley Missouri Endowment. Wunsch additionally acknowledges support from the National Science Foundation. Gu gratefully acknowledges support from the University of Michigan."
    } ],
    "references" : [ {
      "title" : "Near-optimal signal recovery from random projections: universal encoding strategies",
      "author" : [ "E.J. Candés", "T. Tao" ],
      "venue" : "IEEE Trans. Inform. Theory, (52)",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Fuzzy ARTMAP: A neural network architecture for incremental supervised learning of analog multidimensional maps",
      "author" : [ "G. Carpenter", "S. Grossberg", "N. Markuzon", "J. Reynolds", "D. Rosen" ],
      "venue" : "IEEE Transactions on Neural Networks, (3)",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "Fuzzy ART: Fast Stable Learning and Categorization of Analog Patterns by an Adaptive Resonance",
      "author" : [ "G. Carpenter", "S. Grossberg", "D. Rosen" ],
      "venue" : "Neural Networks, (4)",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 1991
    }, {
      "title" : "L",
      "author" : [ "K. Cawse", "S.B. Damelin", "R. McIntyre", "M. Mitchley" ],
      "venue" : "du Plessis and M. Sears, An Investigation of data compression for Hyperspectral core image data, Proceedings of the Mathematics in Industry Study Group, South Africa",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Using random matrix theory to determine the number of endmembers in a hyperspectral image",
      "author" : [ "K. Cawse", "M. Sears", "A. Robin", "S.B. Damelin", "K. Wessels", "F. van den Bergh", "R. Mathieu" ],
      "venue" : "Proceedings of WHISPERS",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2010
    }, {
      "title" : "Energies",
      "author" : [ "S.B. Damelin", "J. Levesley", "D.L. Ragozin", "X. Sun" ],
      "venue" : "Group Invariant Kernels and Numerical Integration on Compact Manifolds, Journal of Complexity, (25)",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Reducing dimensionality of hyperspectral data with diffusion maps and clustering with K-means and fuzzy art",
      "author" : [ "L. du Plessis", "R. Xu", "S.B. Damelin", "M. Sears", "D.C. Wunsch" ],
      "venue" : "Int. J. Systems Control and Communications,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2011
    }, {
      "title" : "Reducing dimensionality of hyperspectral data with diffusion maps and clustering with K-means and fuzzy art",
      "author" : [ "L. du Plessis", "R. Xu", "S.B. Damelin", "M. Sears", "D.C. Wunsch" ],
      "venue" : "Proceedings of IJCNN",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2009
    }, {
      "title" : "BMO Theorems for ε distorted diffeomorphisms on RD and an application to comparing manifolds of speech and sound",
      "author" : [ "C. Fefferman", "S.B. Damelin", "W. Glover" ],
      "venue" : "Involve 5-2",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Biclustering algorithms for biological data analysis: A survey",
      "author" : [ "S. Madeira", "A. Oliveira" ],
      "venue" : "IEEE Transactions on Computational Biology and Bioinformatics, 1-1",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Target detection I Hyperpectral mineral data using wavelet analysis",
      "author" : [ "M. Mitchley", "M. Sears", "S.B. Damelin" ],
      "venue" : "Proceedings of the 2009 IEEE Geosciences and Remote Sensing Symposium,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2009
    }, {
      "title" : "Million city traveling salesman problem solution by divide and conquer clustering with adaptive resonance neural networks",
      "author" : [ "S. Mulder", "D. Wunsch II" ],
      "venue" : "Neural Networks, (16)",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "An Introduction to Diffusion Maps, The 19th Symposium of the Pattern Recognition Association of South Africa",
      "author" : [ "J. de la Porte", "B.M. Herbst", "W. Hereman", "S.J. van der Walt" ],
      "venue" : null,
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2008
    }, {
      "title" : "ART Properties of Interest in Engineering Applications",
      "author" : [ "II D. Wunsch" ],
      "venue" : "Proceedings of International Conference of Neural Networks, Atlanta",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Clustering",
      "author" : [ "R. Xu", "D. Wunsch II" ],
      "venue" : "IEEE/Wiley",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Applications of diffusion maps in gene expression data-based cancer diagnosis analysis",
      "author" : [ "R. Xu", "S.B. Damelin", "D.C. Wunsch II" ],
      "venue" : "Proceedings of the 29th Annual International Conference of IEEE Engineering in Medicine and Biology Society, Lyon, France",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Survey of Clustering Algorithms",
      "author" : [ "R. Xu", "D. Wunsch II" ],
      "venue" : "IEEE Transactions on Neural Networks, 16-3",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Clustering of High-Dimensional Gene Expression Data with Feature Filtering Methods and Diffusion Maps",
      "author" : [ "R. Xu", "S.B. Damelin", "B. Nadler", "D.C. Wunsch II" ],
      "venue" : "Bio-Medical Engineering and Informatics, (1)",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "BARTMAP: A Viable Structure for Biclustering",
      "author" : [ "R. Xu", "D.C. Wunsch II" ],
      "venue" : "Neural Networks, (24)(7)",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 6,
      "context" : "This algorithm was introduced in the papers [9, 10, 18, 19, 21].",
      "startOffset" : 44,
      "endOffset" : 63
    }, {
      "referenceID" : 7,
      "context" : "This algorithm was introduced in the papers [9, 10, 18, 19, 21].",
      "startOffset" : 44,
      "endOffset" : 63
    }, {
      "referenceID" : 15,
      "context" : "This algorithm was introduced in the papers [9, 10, 18, 19, 21].",
      "startOffset" : 44,
      "endOffset" : 63
    }, {
      "referenceID" : 17,
      "context" : "This algorithm was introduced in the papers [9, 10, 18, 19, 21].",
      "startOffset" : 44,
      "endOffset" : 63
    }, {
      "referenceID" : 12,
      "context" : "See [6, 15] and references sited there in.",
      "startOffset" : 4,
      "endOffset" : 11
    }, {
      "referenceID" : 5,
      "context" : "FARDIFF however can be defined by way of a wide class of positive definite kernels, see [6,7] where the choice of kernel is typically application dependent.",
      "startOffset" : 88,
      "endOffset" : 93
    }, {
      "referenceID" : 0,
      "context" : "In addition to the choice of kernel, the trade off in sparsity can be handled by using a restricted isometry, see for example [1].",
      "startOffset" : 126,
      "endOffset" : 129
    }, {
      "referenceID" : 2,
      "context" : "In this section we describe the second part of FARDiff which uses Fuzzy Adaptive Resonance Theory (FA) [3,17,20] for clustering data points whose dimension has been reduced using the method of section 2.",
      "startOffset" : 103,
      "endOffset" : 112
    }, {
      "referenceID" : 14,
      "context" : "In this section we describe the second part of FARDiff which uses Fuzzy Adaptive Resonance Theory (FA) [3,17,20] for clustering data points whose dimension has been reduced using the method of section 2.",
      "startOffset" : 103,
      "endOffset" : 112
    }, {
      "referenceID" : 16,
      "context" : "In this section we describe the second part of FARDiff which uses Fuzzy Adaptive Resonance Theory (FA) [3,17,20] for clustering data points whose dimension has been reduced using the method of section 2.",
      "startOffset" : 103,
      "endOffset" : 112
    }, {
      "referenceID" : 6,
      "context" : "See [9, 10,18,19,21].",
      "startOffset" : 4,
      "endOffset" : 20
    }, {
      "referenceID" : 7,
      "context" : "See [9, 10,18,19,21].",
      "startOffset" : 4,
      "endOffset" : 20
    }, {
      "referenceID" : 15,
      "context" : "See [9, 10,18,19,21].",
      "startOffset" : 4,
      "endOffset" : 20
    }, {
      "referenceID" : 17,
      "context" : "See [9, 10,18,19,21].",
      "startOffset" : 4,
      "endOffset" : 20
    }, {
      "referenceID" : 0,
      "context" : "5) where β ∈ [0, 1] is the learning rate parameter.",
      "startOffset" : 13,
      "endOffset" : 19
    }, {
      "referenceID" : 13,
      "context" : "ART has been found to have good ability to interpret well, results on neural net learning and decision based networks with low complexity and good robustness [16].",
      "startOffset" : 158,
      "endOffset" : 162
    }, {
      "referenceID" : 6,
      "context" : "This algorithm was introduced in the papers [9, 10,18,19,21].",
      "startOffset" : 44,
      "endOffset" : 60
    }, {
      "referenceID" : 7,
      "context" : "This algorithm was introduced in the papers [9, 10,18,19,21].",
      "startOffset" : 44,
      "endOffset" : 60
    }, {
      "referenceID" : 15,
      "context" : "This algorithm was introduced in the papers [9, 10,18,19,21].",
      "startOffset" : 44,
      "endOffset" : 60
    }, {
      "referenceID" : 17,
      "context" : "This algorithm was introduced in the papers [9, 10,18,19,21].",
      "startOffset" : 44,
      "endOffset" : 60
    }, {
      "referenceID" : 15,
      "context" : "In [18, 21], we applied FARDiff to investigate cancer detection.",
      "startOffset" : 3,
      "endOffset" : 11
    }, {
      "referenceID" : 17,
      "context" : "In [18, 21], we applied FARDiff to investigate cancer detection.",
      "startOffset" : 3,
      "endOffset" : 11
    }, {
      "referenceID" : 6,
      "context" : "In [9,10,19], we applied FARDiff to study hyperspectral imaging.",
      "startOffset" : 3,
      "endOffset" : 12
    }, {
      "referenceID" : 7,
      "context" : "In [9,10,19], we applied FARDiff to study hyperspectral imaging.",
      "startOffset" : 3,
      "endOffset" : 12
    }, {
      "referenceID" : 13,
      "context" : "Two examples of good biclustering algorithms are BARTMAP and HBiFAM (Hierarchical Biclustering FA algorithm) [16,22].",
      "startOffset" : 109,
      "endOffset" : 116
    }, {
      "referenceID" : 18,
      "context" : "Two examples of good biclustering algorithms are BARTMAP and HBiFAM (Hierarchical Biclustering FA algorithm) [16,22].",
      "startOffset" : 109,
      "endOffset" : 116
    } ],
    "year" : 2015,
    "abstractText" : "In this paper, we describe an algorithm FARDiff (Fuzzy Adaptive Resonance Diffusion) which combines Diffusion Maps and Fuzzy Adaptive Resonance Theory to do clustering on high dimensional data. We describe some applications of this method and some problems for future research.",
    "creator" : "LaTeX with hyperref package"
  }
}