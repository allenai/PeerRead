{
  "name" : "1604.03247.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Thesis: Multiple Kernel Learning for Object Categorization",
    "authors" : [ "Dinesh Govindaraj" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Object Categorization is a challenging problem, especially when the images have clutter background, occlusions or different lighting conditions. In the past, many descriptors have been proposed which aid object categorization even in such adverse conditions. Each descriptor has its own merits and de-merits. Some descriptors are invariant to transformations while the others are more discriminative [1, 2]. Past research has shown that, employing multiple descriptors rather than any single descriptor leads to better recognition [3, 4]. The problem of learning the optimal combination of the available descriptors for a particular classification task is studied. Multiple Kernel Learning (MKL) framework has been developed for learning an optimal combination of descriptors for object categorization. Existing MKL formulations often employ block l-1 norm regularization which is equivalent to selecting a single kernel from a library of kernels [5, 6, 7, 8, 9]. Since essentially a single descriptor is selected, the existing formulations maybe suboptimal for object categorization. A MKL formulation based on block l-∞ norm regularization has been developed, which chooses an optimal combination of kernels as opposed to selecting a single kernel. A Composite Multiple Kernel Learning(CKL) formulation based on mixed l-∞ and l-1 norm regularization has been developed. These formulations end in Second Order Cone Programs(SOCP). Other efficient alternative algorithms for these formulation have been implemented. Empirical results on benchmark datasets show significant improvement using these new MKL formulations."
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "Object Categorization is a challenging problem, especially when the images have clutter background, occlusions or different lighting conditions. In the past, many descriptors have been proposed which aid object\ncategorization even in such adverse conditions. Each descriptor has its own merits and de-merits. Some descriptors are invariant to transformations while the others are more discriminative. For example, Scale Invariant Feature Transformation (SIFT [1]) is invariant to affine transformations, geometric blur descriptor [2] is robust to shape deformation and pyramid histogram of gradient [10] is invariant to geometric and photometric transformations. Past research has shown that, employing multiple descriptors rather than any single descriptor leads to better recognition [3, 4]. The project focuses on the problem of learning the optimal combination of the available descriptors for a particular classification task.\nAdaBoost for combining the descriptors has been developed which is inspired by the MKL work where each kernel is formed with different descriptors. Difference between AdaBoost SVM with different descriptors and MKL is AdaBoost gives weight on the SVM classifier, each SVM with different descriptors in kernel, where MKL gives weights on each kernel.\nIn [3, 11, 12, 4], the authors employ the Multiple Kernel Learning (MKL) framework [5] to find the optimal combination of descriptors (kernels). The goal of MKL is to simultaneously optimize the combination of kernels and the usual classification objective. Most of the existing MKL formulations perform a l-1 regularization [8, 3] over the kernels. This is equivalent to selecting the best kernel from the given set of kernels; which, as discussed earlier, might be suboptimal for object categorization tasks. One way to circumvent this problem of optimal weights being zero for many of the kernels, was introduced in [3], where an additional constraint to employ prior information is included.\nA new formulation for the MKL problem based on block l-∞ and mixed norm(l-∞ and l-1 norm)regularization has been developed. It is well known that such a regularization would induce “equal weightage” to all the kernels rather than sparsity is developed. Hence would be ideal for applications such\nar X\niv :1\n60 4.\n03 24\n7v 1\n[ cs\n.C V\n] 1\n2 A\npr 2\n01 6\nas object categorization, in which a combination of the descriptors is known to perform better than any single descriptor.\nThese new MKL formulations are Second Order Cone Programs(SOCP) which can be solved using solvers like Mosek, SeDuMi, etc. Other efficient alternative algorithms are also developed which alternates between SVM optimization parameter and kernel weights. Empirical results on Caltech-4, Caltech101 and Oxford flower datasets show significant improvement using these new MKL formulations.\nThe outline of the report is as follows: section 2 briefly reviews the work on object recognition. Existing MKL formulations is given section 3 and the new MKL formulations, is presented in section 4 5. In the subsequent section, efficient algorithms for solving the proposed MKL formulation are discussed. Section 6 presents experimental results on synthetic and real-world datasets which illustrate the merits of the new MKL formulation. The results show that the new formulations achieves better recognition compared to state-of-the-art, which is an l-1 regularization based formulation. Video change detection problem is presented in section ??. The report concludes in section 7 by summarizing the work."
    }, {
      "heading" : "2 Related Work",
      "text" : "This section provides some of work done in area of Machine learning involved in Object categorization. SVM-KNN [13] gets motivation from Local learning which uses K-Nearest neighbor to select local training point and uses SVM algorithm in those local training points for classification of object. Main problem here is time taken for classification.\nMultiple kernel learning considers the scenario where several descriptors (kernels) for a particular classification task are available. It aims to simultaneously learn the optimal combination of the given kernels and the optimal classifier parameters that maximize the generalization ability. Most of the work on MKL, since it was first introduced in [5], concentrates on the employment of a block l-1 regularization. The main features of it being: a) l-1 regularization leads to sparse combination of the kernels, and hence automatically performs feature selection b) very efficient algorithms to solve the formulation exist [6, 7, 8, 9].\nThere has been lot of work on combining descriptors for the object categorization task [3, 11, 10, 4, 12, 14, 15, 16, 17, 18, 19, 20]. In [10], the authors introduce spatial pyramid kernel and combine shape (pyramid histogram of gradient),\nappearance descriptors for object classification. In [4], the Support Kernel Machine [6], which is again based on l-1 regularization, is employed for combining descriptors for object categorization. In [12], a sample dependent local ensemble kernel machine is learned for object categorization. In [3], the authors use six descriptors for object categorization and employ a MKL formulation for learning the optimal combination of descriptors. However, as observed by the authors, most of the (important) kernels get eliminated in the optimal combination. This, as discussed above, is a consequence of employing the l-1 regularization. In order to circumvent the problem of optimal weights being zero for most of the kernels, the authors introduce additional constraints and parameters to utilize additional prior information regarding the kernels. This MKL formulation [3] is known to achieve state-of-the-art performance for many object recognition tasks. In [11], four descriptors for flower classification task were combined using the multiple kernel learning formulation in [3] and this is shown to achieve state-of-the-art performance on such tasks.\nIn summary, most of the existing methodologies for object categorization employ the l-1 regularization based MKL formulation and its variants. As discussed earlier, such a regularization leads to kernel selection rather than kernel combination and hence suboptimal for object categorization tasks. MKL formulation with block l-∞ regularization and CKL, which is more suited for combining kernels as opposed to selecting kernels is presented in this report."
    }, {
      "heading" : "3 Multiple kernel learning",
      "text" : "This section gives brief introduction about MKL. Let xki denote the feature vector of the i\nth training datapoint in the kth feature space (kth kernel). Suppose yi denotes its label. Let Xk represent the matrix whose columns are the training datapoints in the kth feature space. Also, let Kk ≡ X>kXk, be the gram matrix of the training datapoints in the kth feature space. Note that Xk may not be explicitly known; the gram matrices, Kk, are assumed to be known. Let y,Y represent the column vector, diagonal matrix with entries as labels of the training datapoints respectively. Let the discriminating hyperplane be ∑l k=1 w > k x\nk − b = 0 (here, xk denotes the kth feature space representation of the datapoint, x. l is the number of kernels given). The usual soft-margin Support Vector Machine (SVM) [21, 17] formulation with this notation\nis:\nmin wk,b,ξi\n1 2 [∑l k=1 ‖wk‖22 ] + C ∑ i ξi\ns.t. yi (∑l k=1 w > k x k i − b ) ≥ 1− ξi,\nξi ≥ 0, ∀ i = 1, . . . ,m (L2-MKL)\nThis is evident by considering w ≡ [ w>1 . . .w > l ]> and\nxi ≡ [( x1i )> . . . ( xli )>]> in the usual SVM formulation. It is easy to see that in this case the gram-matrix of the datapoints xi, i = 1, . . . ,m (m is the number of\ntraining datapoints) is nothing but ∑l k=1 Kk. Hence, in the context of MKL, the optimal kernel with this formulation is nothing but a simple sum of the given kernels. Another alternative which has been extensively explored in the past [5, 8] was to employ a block l-1 regularization in order to perform kernel selection. This formulation can be written as:\nmin wk,b,ξi\n1 2 [∑l k=1 ‖wk‖2 ]2 + C ∑ i ξi\ns.t. yi (∑l k=1 w > k x k i − b ) ≥ 1− ξi,\nξi ≥ 0, ∀ i = 1, . . . ,m (L1-MKL)\nNote that this formulation performs a l-1 regularization over ‖wk‖2, k = 1, . . . , l. Hence it automatically performs kernel selection and is equivalent to selecting one (the best) of the given kernels. Since the formulation promotes sparsity in the usage of the given kernels, it is best suited for feature selection applications rather than for applications like object categorization where each kernel is believed to provide important information regarding the classification problem at hand."
    }, {
      "heading" : "4 L-∞ regularization MKL For-",
      "text" : "mulation\nThe alternative to that of l-1 regularization is to perform block l-∞ regularization. Such a regularization promotes the use of all the kernels while assuming they are equally preferable. The proposed MKL formulation can be written as:\nmin wk,b,ξi\n[ maxlk=1 ‖wk‖22 ] + C ∑ i ξi\ns.t. yi (∑l k=1 w > k x k i − b ) ≥ 1− ξi,\nξi ≥ 0, ∀ i = 1, . . . ,m (Li-MKL)\nIn the remainder of this section, the ranges of the indices i, k are omitted for convenience. The (Li-MKL) formulation is same as:\nmin t,wk,b,ξi\nt+ C ∑ i ξi\ns.t. yi( ∑ kw > k x k i − b) ≥ 1− ξi, ξi ≥ 0, ‖wk‖22 ≤ t\nIn the following text the dual of the proposed MKL formulation is derived. The Lagrangian turns out to be: L = t+C ∑ i ξi+ ∑ i αi ( 1− ξi − yi (∑ k w>k x k i − b )) −\n∑ i βiξi + ∑ k λk ( ‖wk‖22 − t ) where αi, βi, λk ≥ 0 are the Lagrange multipliers. From the KKT conditions:\n∇wkL = 0⇒ λkwk = 1\n2 ∑ i αiyix k i (1)\n∂L ∂t = 0⇒ ∑ k λk = 1 (2) ∂L ∂b = 0⇒ ∑ i αiyi = 0 (3) ∂L ∂ξi = 0⇒ C = αi + βi (4)\nNow, suppose that all the gram-matrices Kk are positive-definite (add a small ridge if singular, see also [8]). Then, (1) implies that if λk = 0 for some k, then αi = 0, ∀ i. Clearly, in this case rest of the λk must also be zero — which is not possible since∑ k λk = 1. Hence λk > 0 ∀ k. Eliminating the primal variables, the dual can be written as:\nmin α,λ\n1 2α >Q(λ)α− 1>α\ns.t. 0 ≤ α ≤ C1,y>α = 0, λ ≥ 0,1>λ = 1 (5)\nwhere Q(λ) ≡ 12 ∑ k YKkY λk\nand λ, α denote the column vectors with entries as λk, αi respectively.\nThough the dual in (12) has more variables, it gives more insight into the structure of the solution. Consider re-writing the dual (12) in the following way:\nmin λ\nJ(λ)\ns.t. λ ≥ 0,1>λ = 1 (6)\nwhere J(λ) is the optimal value of the following convex QP:\nJ(λ) ≡min α\n1 2 α>Q(λ)α− 1>α\ns.t. 0 ≤ α ≤ C1,y>α = 0 (7)\nNote that (7) is nothing but a usual SVM problem and hence the optimal α is very sparse. Infact, algorithms which exploit this sparsity in solution and outperform standard QP solvers exist [22]."
    }, {
      "heading" : "4.1 Algorithms for solving the Li-MKL Formulation",
      "text" : "The Li-MKL formulation, can be solved using standard Second Order Cone Program (SOCP) solvers(e.g., SeDuMi1, Mosek2). However the optimization problem would involve l conic quadratic constraints (l, the number of kernels, can be large). Also, the size of the optimization problem (m, the number of training datapoints), can be large. Hence generic cone solvers fail to solve for large l or m. Interestingly, there are more efficient ways to solve the dual formulations (12). The following sections explain in brief the possible methodologies."
    }, {
      "heading" : "4.2 Alternating Minimization Algorithm",
      "text" : "The dual (12) can be solved efficiently using an alternating minimization algorithm in the variables α and λ. Note that for a fixed value of λ, (12) is nothing but the SVM dual (which has very efficient scalable solvers). Also, for fixed value of α, the minimization wrt. λ is the following simple problem:\nmin λk\n∑ k Dk λk\ns.t. λk ≥ 0, ∑ k λk = 1 (8)\nwhere Dk ≡ α>Qkα. It is easy to show that the optimal values of λk for the problem (8) are (λk > 0 ∀ k):\nλk =\n√ Dk∑\nk\n√ Dk\n(9)\nHence the following iterative algorithm can be employed to solve (12) efficiently:\n1. Initialize with λ (0) k = 1 l where l is the number of\nkernels.\n1http://sedumi.ie.lehigh.edu/ 2www.mosek.com\n2. At iteration i ≥ 1, solve a standard SVM problem with Hessian as Q(λ(i−1)) for α(i).\n3. Using α(i) compute D (i) k . Update λ (i) using (9).\n4. repeat until, say, change in objective value of (12) is negligible."
    }, {
      "heading" : "5 Composite MKL Formulation",
      "text" : "This section explains Composite MKL. Suppose n descriptors are available. Further, for each of these descriptors Kernels (linear, polynomial, Gaussian) are defined. Let number of Kernels of the jth descriptor be denoted by nj . Also, let φjk denote the mapping induced by the kth Kernel of the jth descriptor. The hyperplane classifier to be learnt has the form∑n j=1 ∑nj k=1 w > jkφjk(x) − b = 0. The objective is to choose the “best” combination of these Kernels in order to maximize the generalization. The idea is to combine the Kernels in such a way that: a) all descriptors are given equal priority (weightage) b) best of the Kernels in each descriptor are selected. In other words, perform an l-∞ regularization over the parameters (wjk) such that each descriptor is given equal priority. Further, perform an l-1 regularization such that sparsity in selection of Kernels belonging to each descriptor is encouraged. Mathematically, the formulation can be written as:\nmin wjk,b,ξi\n[ maxj (∑nj k=1 ‖wjk‖2 )2] + C ∑ i ξi\ns.t. yi (∑n j=1 ∑nj k=1 w > jkφjk(xi)− b ) (10)\n≥ 1− ξi ξi ≥ 0 (CKL) (11)\nwhere {(xi, yi), i = 1, . . . ,m} is the training dataset. C is the regularization parameter.\nLet y denote the vector with entries as the labels. Let Sm be the set {α ∈ Rm | 0 ≤ α ≤ C, y>α = 0}. Denote the set {γ ∈ Rn | γ ≥ 0, 1>γ = 1} by ∆n. The dual of the above formulation can be written as:\nmin λj∈∆nj max α∈Sm, γ∈∆n 1>α−1 4 α>  n∑ j=1 (∑nj k=1 λjkQjk γj )α (12) where Qjk ≡ YKjkY. Here, Kjk is the gram-matrix of the training datapoints with the kth Kernel of the jth descriptor and Y is the diagonal matrix with entries as the labels. The gram-matrices are assumed to be positive definite.\nOne can solve the above dual (12) using a simple alternating algorithm described below. Due to com-\npactness of feasibility sets and convexity of the objective, the order of min., max. can be rearranged. Also since the variables λj1 are not inter-linked with the variables λj2 , for j1 6= j2, instead of minimizing sum over j index one can sum the minima:\n≡ max α∈Sm max γ∈∆n min λj∈∆nj 1>α−1 4 α>  n∑ j=1 (∑nj k=1 λjkQjk γj )α\n≡ max α∈Sm 1>α−1 4 min γ∈∆n n∑ j=1\nmaxλj∈∆nj ∑nj k=1 α >λjkQjkα\nγj\nNow it is easy to see that for fixed values of γ, λj ∀ j, the problem wrt. α is same as the SVM problem. Also, for fixed values of α, the problem wrt. γ, λj ∀ j is the following simple problem:\nmin γ∈∆n n∑ j=1\nmaxλj∈∆nj ∑nj k=1 α >λjkQjkα\nγj (13)\nwhich has a closed form solution described below. Consider solving\nmax λj∈∆nj nj∑ k=1 α>λjkQjkα\nfor a particular j. This amounts to just picking the maximum among α>Qjkα for k = 1, . . . , nj . Let these maxima be denoted by Dj(≥ 0). Hence (13) is equivalent to the following problem:\nmin γ∈∆n n∑ j=1 Dj γj\nThe optimal solution for this problem is: γj =√ Dj∑\nj\n√ Dj .\nThe overall algorithm is as follows:\n• Initialize with γ(0) = 1n and λ (0) j = 1 nj .\n• At iteration i ≥ 1, solve an SVM taking kernel as∑n j=1 ∑nj k=1 λ (i−1) jk Qjk\nγ (i−1) j\n. Update α(i) as the solution\nof this SVM.\n• Using updated values of α(i), compute the closed form solution of (13) using the methodology described above.\n• Repeat until convergence."
    }, {
      "heading" : "6 Numerical Experiments",
      "text" : "This section presents the experimental results on standard object categorization. Various experiments also conducted using the Adaboost for combining descriptors on standard Object categorization dataset. The key idea is to show that the proposed l-∞ regularization and Composite regularization based MKL formulation leads to better generalization than the l-1 regularization based MKL formulations, which represent state-of-the-art methodologies for object recognition. The results on synthetic and real-world data are summarized in sections 6.2.1 and 6.2.3 respectively. In all cases, the parameters for the respective methods were tuned on a validation set. Also, the accuracies reported are on unseen testsets and hence represent a true estimate of the generalization performance of the respective classifiers. All multi-class problems were handled using the one-vs-one scheme."
    }, {
      "heading" : "6.1 Results using Adaboost",
      "text" : "Adaboost mentioned in the result is performed with following setup:\n1. Set of classifier for Adaboost is SVM.\n2. Each SVM in that set is build on different base kernel Ki mentioned in previous section.\n3. Here each Ki are build with descriptors like pyramid histograms of gradient, scale-invariant feature descriptors.\n4. Adaboost provides weight on the SVM classifier which is build on each such kernel mentioned above.\nThis AdaBoost is inspired by the Multiple kernel learning work where each kernel is formed with different descriptors. Difference between AdaBoost with SVM(with different descriptors) and Multiple kernel learning is AdaBoost gives weight on the SVM classifier(each SVM with different descriptors in kernel) where in multiple kernel learning gives weights on each kernel in a SVM problem. Following table shows result."
    }, {
      "heading" : "6.2 Results of L-∞ MKL Experiments",
      "text" : "This section provides experimental results for the LiMKL."
    }, {
      "heading" : "6.2.1 Synthetic Data",
      "text" : "In this section, results on synthetic datasets showing the benefit of the proposed methodology is presented. The key result to establish is that the Li-MKL formulation achieves better generalization, especially in cases where the redundancy in the given kernels is less, such as in applications like object categorization. For this, the experimental strategy given by [23]. We repeat the description of the experimental set-up here for the sake of completeness.\nWe wish to create l kernels whose degree of redundancy is controlled by a single parameter ρ. First, m datapoints are sampled from two independent Normal distributions with covariance as the identity matrix (dimensionality of data is n). Here, datapoints sampled from different Normals are assumed to belong to different classes. Now the features are grouped into p disjoint sets (p varies from 1 to l): X1, . . . ,Xp where Xk ∈ R n p×m. We then sample l − p copies from these disjoint sets, by randomly picking one by one from X1, . . . ,Xp with replacement. For each of these l sets randomly generate a linear transformation matrix Ai ∈ Rτ n p× n p (τ is a parameter). The gram-matrices are computed as Kk = X > kA > kAkXk. Clearly, by varying ρ = pl , the redundancy in the kernels can be varied. More specifically, ρ = 1 represents the extreme case where the redundancy in kernels is zero, and hence represents the best-suited scenario for the proposed methodology. The other extreme case is ρ = 0, where the redundancy is maximum, and hence an ideal scenario for employing the l-1 regularization based MKL.\nFigure 1 shows the plot of ratio of testset accuracies achieved by Li-MKL and L1-MKL vs. redundancy in the given kernels (vertical bars represent variance in accuracy). As a baseline for comparison, plot a similar graph for the ratio of testset accuracy achieved by Li-MKL and L2-MKL. Note that as the degree of redundancy decreases the ratio in case of both graphs increases; proving that Li-MKL is well-suited for ap-\nplications like object categorization. In fact, observed a huge improvement in generalization over the L1MKL when ρ is near 1 (as high as 8% and 2% in case of L2-MKL). Also, in cases where the redundancy is high (ρ is near 0), Li-MKL achieves generalization comparable to the other MKL formulations."
    }, {
      "heading" : "6.2.2 Results on Caltech4 dataset",
      "text" : "This section presents results on Caltech-43. Caltech4 dataset contains images of airplanes, cars, faces and bikes. We have taken 80 images for each class, of which 40 are randomly taken as the training/validation data and the remaining as test data. We have used Pyramid Histogram Of Gradient (PHOG) features generated4 at various levels (1,2,3) and angles (180,360). We have generated kernels on these six PHOG features using different parameters for the polynomial and Gaussian kernel (9 for each feature, totally 54 kernels). This experimental procedure was repeated for 20 times with different training-test data splits. The mean testset accuracies obtained with L1-MKL and Li-MKL were 92.00±2.44% and 93.50±2.14% respectively. This shows that the Li-MKL achieves better generalization. Following figures 3 5 6 7 8 9 10 shows ratio of accuracies of Li-MKL to L1-MKL as function of number of kernels on Caltech-4 dataset. Figure 4 shows confusion for Caltech-4 dataset."
    }, {
      "heading" : "6.2.3 Results on Oxford dataset",
      "text" : "The task in the Oxford flower dataset is to categorize images of 17 varieties of flowers. This dataset\n3http://www.robots.ox.ac.uk/~vgg/data/data-cats.html 4Code available at http://www.robots.ox.ac.uk/~vgg/\nresearch/caltech/phog.html\ncontains 80 examples for each class. In [11], the authors introduced four different features color, SIFT for foreground region, SIFT for foreground boundary, Histogram of Gradients for flowers. We have used the χ2 distances given in [11, 24]5 for our experimentation on this dataset. We have used same training, validation and test splits as used in [11]. The mean testset accuracy achieved by L1-MKL and Li-MKL are 85.88±1.83% and 87.35±1.72% respectively. Again, the results confirm that the proposed methodology achieves better generalization than state-of-the-art. The accuracy achieved by the proposed formulation is comparable to the best accuracy reported in [11], which is 88.33±0.3%. Note that this state-of-the-art accuracy was achieved after tuning the parameters for the various descriptors [11] and incorporating prior information following the strategy of [3]. As mentioned earlier, incorporating such prior information may further improve testset accuracies of the proposed formulation. Following figures 11 13 14 15 shows ratio of accuracies of Li-MKL to L1-MKL as function of number of kernels on Oxford flower dataset. Figure 12 shows confusion for Oxford flower dataset.\nIn the subsequent set of experiments the generalization performance of the Li-MKL and L1-MKL as a function of the number of base kernels is compared. The plots are shown in figures 11 3 for the two benchmark datasets. Figures show that in most of the cases Li-MKL achieves better generalization than L1-MKL. Also, in some cases the improvement is as high as 7.5%. Note that the base kernels were derived from the fixed sets of descriptors and hence have some degree of redundancy. These results show that the\n5http://www.robots.ox.ac.uk/~vgg/data/flowers/17/\nindex.html\nproposed formulation does achieve good improvement generalization even in these cases. The next set of experiments compare the performance of the methodologies at various values of the regularization parameter C (see figure ??). Note that at performance of L1-MKL drastically decreases for low values of C. In some cases the difference in accuracy between LiMKL and L1-MKL is as high as 9%. Hence, the proposed formulation is less sensitive to the variation in the regularization parameter."
    }, {
      "heading" : "6.2.4 Results on Caltech-101 dataset",
      "text" : "This section presents results on Caltech-1016. Caltech-101 dataset contains 101 object categories. We have taken 30 images for each class, of which 15 are randomly taken as the training/validation data and the remaining as test data. We have used Pyramid Histogram Of Gradient (PHOG) features generated7 at various levels (1,2,3) and angles (180,360). We have generated kernels on these six PHOG features using different parameters for the polynomial and Gaussian kernel (9 for each feature, totally 54 kernels). This experimental procedure was repeated for 3 times with different training-test data splits. The mean testset accuracies obtained with L1-MKL and Li-MKL were 31.45% and 27.12% respectively. This shows that the Li-MKL achieves better generalization.\n6http://www.vision.caltech.edu/Image_Datasets/\nCaltech101/ 7Code available at http://www.robots.ox.ac.uk/~vgg/ research/caltech/phog.html"
    }, {
      "heading" : "6.3 Results of CKL Experiments",
      "text" : "All the experiments in this section is carried out using descriptors available from ColorDescriptor software8. The general procedure for the experiment is given in the figrue ??. All the experiments in this section follows:\n• Generate for all training images.\n• Generate all the 14 descriptors provided by the software(RGB histogram, Opponent histogram, Hue histogram, rg histogram, Transformed Color histogram, Color moments, Color moment invariants, SIFT, HueSIFT, HSV-SIFT, OpponentSIFT, rgSIFT, C-SIFT, Transformed Color SIFT).\n• Use no spatial pyramids.\n• Clusters all points from all training images to form a codebook for each descriptors.\n• Generate histogram of codebook of both training and testing images.\n• Train the classifier using these histograms as features.\n8http://staff.science.uva.nl/~ksande/research/\ncolordescriptors/\nResults obtained using above procedure is given in following sections."
    }, {
      "heading" : "6.4 Results on Caltech-5 dataset",
      "text" : "This section presents results on Caltech-59 using new MKL formulation and descriptors(csift, opponentsift, rgsift, sift, transformedcolorsift) provided from ColorDescriptor software. Caltech-5 dataset contains images of airplanes, cars, faces, leopards and bikes. This section follows same procedure discussed in previous section. We have used cluster size of 100 to form codebook. Clusters are found using k-means algorithm. Note here we have not run k-means multiple times to find the best cluster or codebook. We have taken 100 images for each class, of which 15 are randomly taken to form a codebook. We have generated kernels on 5 descriptors provided using different parameters for Gaussian kernel (10 for each descriptor, totally 50 kernels). This experimental procedure was repeated for 5 times with different training-test data splits. Figure ?? reports mean accuracy as number of training size increases. Note here we have used same codebook generated on 15 training points for all training sizes.\n9http://www.robots.ox.ac.uk/~vgg/data/data-cats.html"
    }, {
      "heading" : "6.5 Results on Oxford dataset",
      "text" : "The task in the Oxford flower dataset is to categorize images of 17 varieties of flowers. This dataset contains 80 examples for each class. In [11], the authors introduced four different features color, SIFT for foreground region, SIFT for foreground boundary, Histogram of Gradients for flowers. We have used the χ2 distances given in [11, 24]10 for our experimentation on this dataset. We have used same training, validation and test splits as used in [11]. The mean testset accuracy achieved by L1-MKL, Li-MKL and CKL are 85.3922%, 86.6667% and 86.6667% respectively."
    }, {
      "heading" : "6.6 Results on Caltech-101 dataset",
      "text" : "This section presents results on Caltech-10111 using new MKL formulations and all 14 descriptors provided from ColorDescriptor software. We have taken 30 images for each class, of which 15 are randomly taken as the training/validation data and the remaining as test data. We have generated kernels on 14 descriptors provided using different parameters for Gaussian kernel (2 for each descriptor, totally 28 kernels). With cluster size 600 accuracy obtained is around 24.1% and with cluster size 300, accuracy obtained is 23.21%. Main problem in Caltech-101, is problem of clustering for\n10http://www.robots.ox.ac.uk/~vgg/data/flowers/17/\nindex.html 11http://www.vision.caltech.edu/Image_Datasets/ Caltech101/\nforming codebook. Because of huge size and dimension in data, we followed 2-level kmeans. Our guess is that codebook formed is not good because clustering is not good."
    }, {
      "heading" : "7 Conclusions and Future Work",
      "text" : "The project addressed the issue of combining various descriptors for a given object categorization problem in order to achieve better generalization. The project also briefly addressed problem of video change detection.\nAdaboost has been designed for combining descriptors. State-of-the-art methodologies for object categorization employ a l-1 regularization based MKL formulation, which is more suitable for selecting descriptors rather than combining them. The key idea is to employ a l-∞ regularization and mixed l-∞ and l-1 regularization for combining the descriptors in an MKL framework. The new MKL formulation is better suited for object categorization and highly efficient algorithms which solve the corresponding convex optimization problem were derived.\nEmpirical results performed on synthetic and realworld benchmark datasets clearly establish the efficacy of the proposed MKL formulation. In some cases, the increase in accuracy when compared to the standard l-1 regularization was as high as 9%. The results also show that there is a consistant improvement in accuracy in almost all the cases, however, the improvement is maximized when the redundancy in the base kernels is low. Another advantage with the proposed formulation is that it is less sensitive to variation in the regularization parameter, C.\nWork is going on to experiment the new MKL formulations for Caltech-101 dataset using codebook models described. Experiments is going on to form codebooks with different size as size of codebook largely affect classification accuracy. Novality of new MKL formulations will be known once experimentation on the bigger dataset has been done namely Pascal and Caltech-256. Future work also includes to experiment the new MKL formulations for these bigger datasets."
    } ],
    "references" : [ {
      "title" : "Distinctive image features from scaleinvariant keypoints",
      "author" : [ "David G. Lowe" ],
      "venue" : "International Journal of Computer Vision,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2004
    }, {
      "title" : "Geometric blur for template matching",
      "author" : [ "A. Berg", "J. Malik" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2001
    }, {
      "title" : "Learning the discriminative power-invariance trade-off",
      "author" : [ "M. Varma", "D. Ray" ],
      "venue" : "In ICCV, pages",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2007
    }, {
      "title" : "Support kernel machines for object recognition",
      "author" : [ "A. Kumar", "C. Sminchisescu" ],
      "venue" : "In ICCV,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2007
    }, {
      "title" : "Learning the kernel matrix with semidefinite programming",
      "author" : [ "Gert Lanckriet", "Nello Cristianini", "Peter Bartlett", "Laurent El Ghaoui" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2004
    }, {
      "title" : "Multiple kernel learning, conic duality, and the smo algorithm",
      "author" : [ "F.R. Bach", "G.R.G. Lanckriet", "M.I. Jordan" ],
      "venue" : "In ICML,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2004
    }, {
      "title" : "Large scale multiple kernel learning",
      "author" : [ "Sören Sonnenburg", "Gunnar Rätsch", "Christin Schäfer", "Bernhard Schölkopf" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2006
    }, {
      "title" : "Simple MKL",
      "author" : [ "Alain Rakotomamonjy", "Francis R. Bach", "Stephane Canu", "Yves Grandvalet" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2008
    }, {
      "title" : "An Extended Level Method for Efficient Multiple Kernel Learning",
      "author" : [ "Z. Xu", "R. Jin", "I. King", "M. Lyu" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2008
    }, {
      "title" : "Representing shape with a spatial pyramid kernel",
      "author" : [ "A. Zisserman A. Bosch", "X. Munoz" ],
      "venue" : "In CIVR,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2007
    }, {
      "title" : "Automated flower classification over a large number of classes",
      "author" : [ "M-E. Nilsback", "A Zisserman" ],
      "venue" : "In Proceedings of the Indian Conference on Computer Vision, Graphics and Image Processing,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2008
    }, {
      "title" : "Local ensemble kernel learning for object category recognition",
      "author" : [ "Y.Y. Lin", "T.Y. Liu", "C.S. Fuh" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2007
    }, {
      "title" : "Svm-knn: Discriminative nearest neighbor classification for visual category recognition",
      "author" : [ "Hao Zhang", "Alexander C. Berg", "Michael Maire", "Jitendra Malik" ],
      "venue" : "Proceedings of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2006
    }, {
      "title" : "Local features and kernels for classification of texture and object categories: a comprehensive study",
      "author" : [ "S. Lazebnik J. Zhang", "M. Marszalek", "C. Schmid" ],
      "venue" : "In IJCV,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2007
    }, {
      "title" : "On the algorithmics and applications of a mixed-norm based kernel learning formulation",
      "author" : [ "Saketha N. Jagarlapudi", "Dinesh Govindaraj", "Raman S", "Chiranjib Bhattacharyya", "Aharon Ben-tal", "Ramakrishnan K.r" ],
      "venue" : null,
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2009
    }, {
      "title" : "Controlled sparsity kernel learning",
      "author" : [ "Dinesh Govindaraj", "Sankaran Raman", "Sreedal Menon", "Chiranjib Bhattacharyya" ],
      "venue" : "CoRR, abs/1401.0116,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2014
    }, {
      "title" : "Sparse classifier design based on the shapley value",
      "author" : [ "Prashanth Ravipally", "Dinesh Govindaraj" ],
      "venue" : "In Proceedings of the World Congress on Engineering,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2010
    }, {
      "title" : "Moneybee: Towards enabling a ubiquitous, efficient, and easyto-use mobile crowdsourcing service in the emerging market",
      "author" : [ "Dinesh Govindaraj", "Naidu K.V.M", "Animesh Nandi", "Girija Narlikar", "Viswanath Poosala" ],
      "venue" : "Bell Labs Technical Journal,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2011
    }, {
      "title" : "Modeling attractiveness and multiple clicks in sponsored search results",
      "author" : [ "Dinesh Govindaraj", "Tao Wang", "S.V.N. Vishwanathan" ],
      "venue" : "CoRR, abs/1401.0255,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2014
    }, {
      "title" : "Application of active appearance model to automatic face replacement",
      "author" : [ "Dinesh Govindaraj" ],
      "venue" : "Journal of Applied Statistics,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2011
    }, {
      "title" : "Fast Training of Support Vector Machines using Sequential Minimal Optimization",
      "author" : [ "J. Platt" ],
      "venue" : "In Advances in Kernel Methods—Support Vector Learning,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 1999
    }, {
      "title" : "Non-sparse multiple kernel learning",
      "author" : [ "Marius Kloft", "Ulf Brefeld", "Pavel Laskov", "Soren Sonnenburg" ],
      "venue" : "In Workshop on Kernel Learning: Automatic Selection of Optimal Kernels,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2008
    }, {
      "title" : "A visual vocabulary for flower classification",
      "author" : [ "M-E. Nilsback", "A. Zisserman" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2006
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Some descriptors are invariant to transformations while the others are more discriminative [1, 2].",
      "startOffset" : 91,
      "endOffset" : 97
    }, {
      "referenceID" : 1,
      "context" : "Some descriptors are invariant to transformations while the others are more discriminative [1, 2].",
      "startOffset" : 91,
      "endOffset" : 97
    }, {
      "referenceID" : 2,
      "context" : "Past research has shown that, employing multiple descriptors rather than any single descriptor leads to better recognition [3, 4].",
      "startOffset" : 123,
      "endOffset" : 129
    }, {
      "referenceID" : 3,
      "context" : "Past research has shown that, employing multiple descriptors rather than any single descriptor leads to better recognition [3, 4].",
      "startOffset" : 123,
      "endOffset" : 129
    }, {
      "referenceID" : 4,
      "context" : "Existing MKL formulations often employ block l-1 norm regularization which is equivalent to selecting a single kernel from a library of kernels [5, 6, 7, 8, 9].",
      "startOffset" : 144,
      "endOffset" : 159
    }, {
      "referenceID" : 5,
      "context" : "Existing MKL formulations often employ block l-1 norm regularization which is equivalent to selecting a single kernel from a library of kernels [5, 6, 7, 8, 9].",
      "startOffset" : 144,
      "endOffset" : 159
    }, {
      "referenceID" : 6,
      "context" : "Existing MKL formulations often employ block l-1 norm regularization which is equivalent to selecting a single kernel from a library of kernels [5, 6, 7, 8, 9].",
      "startOffset" : 144,
      "endOffset" : 159
    }, {
      "referenceID" : 7,
      "context" : "Existing MKL formulations often employ block l-1 norm regularization which is equivalent to selecting a single kernel from a library of kernels [5, 6, 7, 8, 9].",
      "startOffset" : 144,
      "endOffset" : 159
    }, {
      "referenceID" : 8,
      "context" : "Existing MKL formulations often employ block l-1 norm regularization which is equivalent to selecting a single kernel from a library of kernels [5, 6, 7, 8, 9].",
      "startOffset" : 144,
      "endOffset" : 159
    }, {
      "referenceID" : 0,
      "context" : "For example, Scale Invariant Feature Transformation (SIFT [1]) is invariant to affine transformations, geometric blur descriptor [2] is robust to shape deformation and pyramid histogram of gradient [10] is invariant to geometric and photometric transformations.",
      "startOffset" : 58,
      "endOffset" : 61
    }, {
      "referenceID" : 1,
      "context" : "For example, Scale Invariant Feature Transformation (SIFT [1]) is invariant to affine transformations, geometric blur descriptor [2] is robust to shape deformation and pyramid histogram of gradient [10] is invariant to geometric and photometric transformations.",
      "startOffset" : 129,
      "endOffset" : 132
    }, {
      "referenceID" : 9,
      "context" : "For example, Scale Invariant Feature Transformation (SIFT [1]) is invariant to affine transformations, geometric blur descriptor [2] is robust to shape deformation and pyramid histogram of gradient [10] is invariant to geometric and photometric transformations.",
      "startOffset" : 198,
      "endOffset" : 202
    }, {
      "referenceID" : 2,
      "context" : "Past research has shown that, employing multiple descriptors rather than any single descriptor leads to better recognition [3, 4].",
      "startOffset" : 123,
      "endOffset" : 129
    }, {
      "referenceID" : 3,
      "context" : "Past research has shown that, employing multiple descriptors rather than any single descriptor leads to better recognition [3, 4].",
      "startOffset" : 123,
      "endOffset" : 129
    }, {
      "referenceID" : 2,
      "context" : "In [3, 11, 12, 4], the authors employ the Multiple Kernel Learning (MKL) framework [5] to find the optimal combination of descriptors (kernels).",
      "startOffset" : 3,
      "endOffset" : 17
    }, {
      "referenceID" : 10,
      "context" : "In [3, 11, 12, 4], the authors employ the Multiple Kernel Learning (MKL) framework [5] to find the optimal combination of descriptors (kernels).",
      "startOffset" : 3,
      "endOffset" : 17
    }, {
      "referenceID" : 11,
      "context" : "In [3, 11, 12, 4], the authors employ the Multiple Kernel Learning (MKL) framework [5] to find the optimal combination of descriptors (kernels).",
      "startOffset" : 3,
      "endOffset" : 17
    }, {
      "referenceID" : 3,
      "context" : "In [3, 11, 12, 4], the authors employ the Multiple Kernel Learning (MKL) framework [5] to find the optimal combination of descriptors (kernels).",
      "startOffset" : 3,
      "endOffset" : 17
    }, {
      "referenceID" : 4,
      "context" : "In [3, 11, 12, 4], the authors employ the Multiple Kernel Learning (MKL) framework [5] to find the optimal combination of descriptors (kernels).",
      "startOffset" : 83,
      "endOffset" : 86
    }, {
      "referenceID" : 7,
      "context" : "Most of the existing MKL formulations perform a l-1 regularization [8, 3] over the kernels.",
      "startOffset" : 67,
      "endOffset" : 73
    }, {
      "referenceID" : 2,
      "context" : "Most of the existing MKL formulations perform a l-1 regularization [8, 3] over the kernels.",
      "startOffset" : 67,
      "endOffset" : 73
    }, {
      "referenceID" : 2,
      "context" : "One way to circumvent this problem of optimal weights being zero for many of the kernels, was introduced in [3], where an additional constraint to employ prior information is included.",
      "startOffset" : 108,
      "endOffset" : 111
    }, {
      "referenceID" : 12,
      "context" : "SVM-KNN [13] gets motivation from Local learning which uses K-Nearest neighbor to select local training point and uses SVM algorithm in those local training points for classification of object.",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 4,
      "context" : "Most of the work on MKL, since it was first introduced in [5], concentrates on the employment of a block l-1 regularization.",
      "startOffset" : 58,
      "endOffset" : 61
    }, {
      "referenceID" : 5,
      "context" : "The main features of it being: a) l-1 regularization leads to sparse combination of the kernels, and hence automatically performs feature selection b) very efficient algorithms to solve the formulation exist [6, 7, 8, 9].",
      "startOffset" : 208,
      "endOffset" : 220
    }, {
      "referenceID" : 6,
      "context" : "The main features of it being: a) l-1 regularization leads to sparse combination of the kernels, and hence automatically performs feature selection b) very efficient algorithms to solve the formulation exist [6, 7, 8, 9].",
      "startOffset" : 208,
      "endOffset" : 220
    }, {
      "referenceID" : 7,
      "context" : "The main features of it being: a) l-1 regularization leads to sparse combination of the kernels, and hence automatically performs feature selection b) very efficient algorithms to solve the formulation exist [6, 7, 8, 9].",
      "startOffset" : 208,
      "endOffset" : 220
    }, {
      "referenceID" : 8,
      "context" : "The main features of it being: a) l-1 regularization leads to sparse combination of the kernels, and hence automatically performs feature selection b) very efficient algorithms to solve the formulation exist [6, 7, 8, 9].",
      "startOffset" : 208,
      "endOffset" : 220
    }, {
      "referenceID" : 2,
      "context" : "There has been lot of work on combining descriptors for the object categorization task [3, 11, 10, 4, 12, 14, 15, 16, 17, 18, 19, 20].",
      "startOffset" : 87,
      "endOffset" : 133
    }, {
      "referenceID" : 10,
      "context" : "There has been lot of work on combining descriptors for the object categorization task [3, 11, 10, 4, 12, 14, 15, 16, 17, 18, 19, 20].",
      "startOffset" : 87,
      "endOffset" : 133
    }, {
      "referenceID" : 9,
      "context" : "There has been lot of work on combining descriptors for the object categorization task [3, 11, 10, 4, 12, 14, 15, 16, 17, 18, 19, 20].",
      "startOffset" : 87,
      "endOffset" : 133
    }, {
      "referenceID" : 3,
      "context" : "There has been lot of work on combining descriptors for the object categorization task [3, 11, 10, 4, 12, 14, 15, 16, 17, 18, 19, 20].",
      "startOffset" : 87,
      "endOffset" : 133
    }, {
      "referenceID" : 11,
      "context" : "There has been lot of work on combining descriptors for the object categorization task [3, 11, 10, 4, 12, 14, 15, 16, 17, 18, 19, 20].",
      "startOffset" : 87,
      "endOffset" : 133
    }, {
      "referenceID" : 13,
      "context" : "There has been lot of work on combining descriptors for the object categorization task [3, 11, 10, 4, 12, 14, 15, 16, 17, 18, 19, 20].",
      "startOffset" : 87,
      "endOffset" : 133
    }, {
      "referenceID" : 14,
      "context" : "There has been lot of work on combining descriptors for the object categorization task [3, 11, 10, 4, 12, 14, 15, 16, 17, 18, 19, 20].",
      "startOffset" : 87,
      "endOffset" : 133
    }, {
      "referenceID" : 15,
      "context" : "There has been lot of work on combining descriptors for the object categorization task [3, 11, 10, 4, 12, 14, 15, 16, 17, 18, 19, 20].",
      "startOffset" : 87,
      "endOffset" : 133
    }, {
      "referenceID" : 16,
      "context" : "There has been lot of work on combining descriptors for the object categorization task [3, 11, 10, 4, 12, 14, 15, 16, 17, 18, 19, 20].",
      "startOffset" : 87,
      "endOffset" : 133
    }, {
      "referenceID" : 17,
      "context" : "There has been lot of work on combining descriptors for the object categorization task [3, 11, 10, 4, 12, 14, 15, 16, 17, 18, 19, 20].",
      "startOffset" : 87,
      "endOffset" : 133
    }, {
      "referenceID" : 18,
      "context" : "There has been lot of work on combining descriptors for the object categorization task [3, 11, 10, 4, 12, 14, 15, 16, 17, 18, 19, 20].",
      "startOffset" : 87,
      "endOffset" : 133
    }, {
      "referenceID" : 19,
      "context" : "There has been lot of work on combining descriptors for the object categorization task [3, 11, 10, 4, 12, 14, 15, 16, 17, 18, 19, 20].",
      "startOffset" : 87,
      "endOffset" : 133
    }, {
      "referenceID" : 9,
      "context" : "In [10], the authors introduce spatial pyramid kernel and combine shape (pyramid histogram of gradient), appearance descriptors for object classification.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 3,
      "context" : "In [4], the Support Kernel Machine [6], which is again based on l-1 regularization, is employed for combining descriptors for object categorization.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 5,
      "context" : "In [4], the Support Kernel Machine [6], which is again based on l-1 regularization, is employed for combining descriptors for object categorization.",
      "startOffset" : 35,
      "endOffset" : 38
    }, {
      "referenceID" : 11,
      "context" : "In [12], a sample dependent local ensemble kernel machine is learned for object categorization.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 2,
      "context" : "In [3], the authors use six descriptors for object categorization and employ a MKL formulation for learning the optimal combination of descriptors.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 2,
      "context" : "This MKL formulation [3] is known to achieve state-of-the-art performance for many object recognition tasks.",
      "startOffset" : 21,
      "endOffset" : 24
    }, {
      "referenceID" : 10,
      "context" : "In [11], four descriptors for flower classification task were combined using the multiple kernel learning formulation in [3] and this is shown to achieve state-of-the-art performance on such tasks.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 2,
      "context" : "In [11], four descriptors for flower classification task were combined using the multiple kernel learning formulation in [3] and this is shown to achieve state-of-the-art performance on such tasks.",
      "startOffset" : 121,
      "endOffset" : 124
    }, {
      "referenceID" : 16,
      "context" : "The usual soft-margin Support Vector Machine (SVM) [21, 17] formulation with this notation",
      "startOffset" : 51,
      "endOffset" : 59
    }, {
      "referenceID" : 4,
      "context" : "Another alternative which has been extensively explored in the past [5, 8] was to employ a block l-1 regularization in order to perform kernel selection.",
      "startOffset" : 68,
      "endOffset" : 74
    }, {
      "referenceID" : 7,
      "context" : "Another alternative which has been extensively explored in the past [5, 8] was to employ a block l-1 regularization in order to perform kernel selection.",
      "startOffset" : 68,
      "endOffset" : 74
    }, {
      "referenceID" : 7,
      "context" : "Now, suppose that all the gram-matrices Kk are positive-definite (add a small ridge if singular, see also [8]).",
      "startOffset" : 106,
      "endOffset" : 109
    }, {
      "referenceID" : 20,
      "context" : "Infact, algorithms which exploit this sparsity in solution and outperform standard QP solvers exist [22].",
      "startOffset" : 100,
      "endOffset" : 104
    }, {
      "referenceID" : 21,
      "context" : "For this, the experimental strategy given by [23].",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 10,
      "context" : "In [11], the authors introduced four different features color, SIFT for foreground region, SIFT for foreground boundary, Histogram of Gradients for flowers.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 10,
      "context" : "We have used the χ distances given in [11, 24] for our experimentation on this dataset.",
      "startOffset" : 38,
      "endOffset" : 46
    }, {
      "referenceID" : 22,
      "context" : "We have used the χ distances given in [11, 24] for our experimentation on this dataset.",
      "startOffset" : 38,
      "endOffset" : 46
    }, {
      "referenceID" : 10,
      "context" : "We have used same training, validation and test splits as used in [11].",
      "startOffset" : 66,
      "endOffset" : 70
    }, {
      "referenceID" : 10,
      "context" : "The accuracy achieved by the proposed formulation is comparable to the best accuracy reported in [11], which is 88.",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 10,
      "context" : "Note that this state-of-the-art accuracy was achieved after tuning the parameters for the various descriptors [11] and incorporating prior information following the strategy of [3].",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 2,
      "context" : "Note that this state-of-the-art accuracy was achieved after tuning the parameters for the various descriptors [11] and incorporating prior information following the strategy of [3].",
      "startOffset" : 177,
      "endOffset" : 180
    }, {
      "referenceID" : 10,
      "context" : "In [11], the authors introduced four different features color, SIFT for foreground region, SIFT for foreground boundary, Histogram of Gradients for flowers.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 10,
      "context" : "We have used the χ distances given in [11, 24] for our experimentation on this dataset.",
      "startOffset" : 38,
      "endOffset" : 46
    }, {
      "referenceID" : 22,
      "context" : "We have used the χ distances given in [11, 24] for our experimentation on this dataset.",
      "startOffset" : 38,
      "endOffset" : 46
    }, {
      "referenceID" : 10,
      "context" : "We have used same training, validation and test splits as used in [11].",
      "startOffset" : 66,
      "endOffset" : 70
    } ],
    "year" : 2016,
    "abstractText" : "Object Categorization is a challenging problem, especially when the images have clutter background, occlusions or different lighting conditions. In the past, many descriptors have been proposed which aid object categorization even in such adverse conditions. Each descriptor has its own merits and de-merits. Some descriptors are invariant to transformations while the others are more discriminative [1, 2]. Past research has shown that, employing multiple descriptors rather than any single descriptor leads to better recognition [3, 4]. The problem of learning the optimal combination of the available descriptors for a particular classification task is studied. Multiple Kernel Learning (MKL) framework has been developed for learning an optimal combination of descriptors for object categorization. Existing MKL formulations often employ block l-1 norm regularization which is equivalent to selecting a single kernel from a library of kernels [5, 6, 7, 8, 9]. Since essentially a single descriptor is selected, the existing formulations maybe suboptimal for object categorization. A MKL formulation based on block l-∞ norm regularization has been developed, which chooses an optimal combination of kernels as opposed to selecting a single kernel. A Composite Multiple Kernel Learning(CKL) formulation based on mixed l-∞ and l-1 norm regularization has been developed. These formulations end in Second Order Cone Programs(SOCP). Other efficient alternative algorithms for these formulation have been implemented. Empirical results on benchmark datasets show significant improvement using these new MKL formulations.",
    "creator" : "LaTeX with hyperref package"
  }
}