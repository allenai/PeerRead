{
  "name" : "1511.06407.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "SPEECH RECOGNITION", "Suyoun Kim" ],
    "emails" : [ "suyoun@cmu.edu", "lane@cmu.edu" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "Many real-world speech recognition applications, including teleconferencing, robotics and in-car spoken dialog systems, must deal with speech from distant microphones in noisy environments. When a human voice is captured with far-field microphones in these environments, the audio signal is severely degraded by reverberation and background noise. This makes the distant speech recognition task far more challenging than near-field speech recognition, which is commonly used for voice-based interaction today.\nAcoustic signals from multiple microphones can be used to enhance recognition accuracy due to the availability of additional spatial information. Many researchers have proposed techniques to efficiently integrate inputs from multiple distant microphones. The most representative multi-channel processing technique is the beamforming approach (Van Compernolle et al., 1990; Seltzer et al., 2004; Kumatani et al., 2012; Pertilä & Nikunen, 2015), which generates an enhanced single output signal by aligning multiple signals through digital delays that compensate for the different distances of the input signals. However, the performance of beamforming is highly dependant on prior information about microphone location and the location of the target source. For downstream tasks such as speech recognition, this preprocessing step is suboptimal because it is not directly optimized towards the final objective of interest: speech recognition accuracy (Seltzer, 2008).\nOver the past few years, deep neural networks (DNNs) have been successfully applied to acoustic models in speech recognition (Seide et al., 2011; Mohamed et al., 2012; Hinton et al., 2012). Other works (Liu et al., 2014; Renals & Swietojanski, 2014; Swietojanski et al., 2014; Yoshioka et al., 2015; Himawan et al., 2015) have shown that DNNs can learn suitable representations for distant\nar X\niv :1\n51 1.\n06 40\n7v 2\n[ cs\n.L G\n] 7\nJ an\n2 01\nspeech recognition by directly using multi-channel input. These approaches however, simply concatenated acoustic features from multiple microphones without considering the spatial properties of acoustic signal propagation, or used convolutional neural networks (CNNs) to implicitly account for spatial relationships between channels (Renals & Swietojanski, 2014; Swietojanski et al., 2014).\nRecently, an ”attention mechanism” in neural networks has been proposed to address the problem of learning variable-length input and output sequences (Bahdanau et al., 2014). At each output step, the previous output history is used to generate an attention vector over the input sequence. This attention vector enables models to learn to focus attention on specific parts of their input. These attentionequipped frameworks have shown very promising results on many challenging tasks involving inputs and outputs with variable length, including machine translation (Bahdanau et al., 2014), parsing (Vinyals et al., 2014), image captioning (Xu et al., 2015) and conversational modeling (Vinyals & Le, 2015). Specifically, for the speech recognition tasks, (Chorowski et al., 2014; Chan et al., 2015; Bahdanau et al., 2015) attempted to align the input features and the desired character sequence using an attention mechanism. However, no attention mechanisms have been applied to learn to integrate multiple inputs.\nIn this work, we propose a novel attention-based model that enables to learn misaligned and nonstationary multiple input sources for distant speech recognition. We embed an attention mechanism within a Recurrent Neural Network (RNN) based acoustic model to automatically tune its attention to a more reliable input source among misaligned and non-stationary input sources at each output step. The attention module is learned with the normal acoustic model and is jointly optimized towards phonetic state accuracy. Our attention module is unique in the way that we 1) deal with the problem of integrating different qualities and misalignment of multiple sources, and 2) exploit spatial information between multiple sources to accelerate learning of auditory attention. Our system plays a similar role to traditional multichannel preprocessing through deep neural network architecture, but bypasses the limitations of preprocessing, which requires an expensive, separate step and depends on prior information.\nThrough a series of experiments on the CHiME-3 (Jon Barker, 2015) dataset, we show that our proposed approach improves recognition accuracy in various types of noisy environments. In addition, we also compare our approach with the beamforming technique(Jon Barker, 2015; Loesch & Yang, 2010; Blandin et al., 2012; Mestre et al., 2003). The paper is organized as follows: in Section 2 we describe our proposed attention based model. In section 3, we evaluate the performance of our model. Finally, in Section 4 we draw conclusions."
    }, {
      "heading" : "2 MODEL",
      "text" : "In this section, we describe our neural attention model, which allows neural networks to focus more on reliable input sources across different temporal locations. We formulate the proposed framework with applications in multi-channel distant speech recognition. While there has been some recent work on end-to-end neural speech recognition systems - from speech directly to transcripts (Graves et al., 2006; Graves & Jaitly, 2014; Hannun et al., 2014; Chorowski et al., 2014) - our model is based on typical hybrid DNN-HMM frameworks (Morgan & Bourlard, 1994; Hinton et al., 2012), wherein the acoustic model estimates hidden Markov model (HMM) state posteriors, because we focus on dealing with the re-weighted input representation of misaligned multiple input sources.\nGiven a set of input sequences X = {Xch1 , · · · ,XchN }, where Xchi is an input sequence (xchi1 , · · · , x chi T ) from the ith microphone, i ∈ {1, · · · , N}, our system computes a corresponding sequence of HMM acoustic states, y = (y1, · · · , yT ). We model each output yt at time t as a conditional distribution over the previous outputs y<t and the multiple inputs Xt at time t using the chain rule:\nP (y|X) = ∏ t P (yt|X, y<t) (1)\nOur system consists of two subnetworks: AttendMultiSource and LSTM-AM. AttendMultiSource is an attention-equipped Recurrent Neural Network (RNN) for learning to determine and focus on reliable channels and temporal locations among the candidate multiple\ninput sequences. AttendMultiSource produces re-weighted inputs, X̂, based on the learned attention. This X̂ is used for the next subnetwork LSTM-AM, which is a Long Short-Term Memory (LSTM) acoustic model to estimate the probability of the output HMM state y. Figure 1 visualizes our overall model with these two components. We describe more details of each component in the following subsections 2.1 and 2.2.\nX̂ = AttendMultiSource(X,y) (2)\nP (y|X) = LSTM-AM(X̂,y) (3)"
    }, {
      "heading" : "2.1 ATTENTION MECHANISM FOR MULTIPLE SOURCES",
      "text" : "The challenge we attempt to address with the neural attention mechanism is the problem of misaligned multiple input sources with non-stationary quality over time. Specifically, in multi-channel distant speech recognition, the arrival time of each channel is different because the acoustic path length of each signal differs according to the location of the microphone. This results in the misalignment of input features. These differences in arrival time are even greater when the space between microphones is larger. Even worse, signal quality across channels can also vary over time because the speaker and interfering noise sources may keep changing. Figure 1 describes the asynchronous arrival of multiple inputs due to acoustic path length differences.\nWe now introduce an attention mechanism to cope with the misaligned input problem, and formulate the AttendMultiSource. At every output step t, the AttendMultiSource function produces a re-weighted input representation X̂c, given cth candidate input set Xc. Xc is a subsequence of time frames. As proposed by (Bahdanau et al., 2015), we perform similar windowing to limit the exploring temporal location of inputs for computational efficiency and scalability. We limit the range of attention to l=7 time frames (±3). In our experiments, longer time steps had little impact on overall performance and would rather benefit from microphones placed further apart from each other.\nFor re-weighting the input Xc, AttendMultiSource predicts an attention weight matrix A time,ch t at each output step t. Unlike previous attention mechanisms, we produce a weight matrix rather than\na vector, because our attention mechanism additionally identifies which channel, in a given time step, is more relevant. Therefore, Atime,cht is the (number of channels) by (number of candidate input frames) matrix - here it is N x l matrix. Attention weights are calculated based on four different information sources: 1) attention history Atime,cht−1 , 2) content in the candidate sequences Xc, 3) decoding history st−1, and 4) additional spatial information between multiple microphones based on phase difference information PDc corresponding to Xc. The following three formulations describe the AttendMultiSource function:\nEtime,cht = MLP(st−1,A time,ch t−1 ,PDc,Xc) (4)\nAtime,cht = softmax(E time,ch t ) (5)\nX̂c = A time,ch t ·Xc (6)\nSpecifically, MLP (in equation 4) computes an energy matrix Etime,cht (N x l) by the following equation:\nEtime,cht = tanh(Ws · st−1 +Wa ·A time,ch t−1 +Wp ·PDc +Wx ·Xc + b) (7)\nwhere Ws, Wa, Wp, and Wx are parameter matrices, and b is a parameter vector. Once we compute the energy Etime,cht at time t, then we obtain A time,ch t by normalizing\nexp(Etime,cht )/ ∑ time,ch exp(E time,ch t ), such that, ∀t, A time,ch t ≥ 0, and ∑ time,ch A time,ch t = 1\n(in equation 5). Finally, re-weighted output X̂c is generated by calculating the dot product of the attention weights Atime,cht and candidate input Xc (in equation 6). Typically, the selection of elements from input candidates is a weighted sum. However, we only calculate the dot product in order to avoid losing information.\nTo accelerate the learning of the attention mechanism, we use additional spatial information based on analysis of differences in arrival time. It is generally assumed that the human auditory system can localize multiple sounds and attend to the desired signal using information from the interaural time difference (ITD) (Stern et al., 2008; Park & Stern, 2009). A previous study (Kim et al., 2009) attempted to emulate human binaural processing and estimate ITD indirectly by comparing the phase difference between two microphones at each frequency domain. The authors identified a ”close” time-frequency component to the speaker based on the estimated ITD. Similarly, we use the phase difference between two microphones to infer spatial information. The following equations are used to compute phase difference between two microphones i and j, where i 6= j, i, j ∈ {1 · · ·N}:\npdchi−chj = min |∠xchi − ∠xchj − 2πr| (8)\nPDchi−chj = (pd chi−chj 1 , · · · , pd chi−chj T ) (9)\nPD = {PDch1−ch2 , · · · ,PDch4−ch5} (10)\nFrom these equations, we calculate the phase differences of each time-frequency bin of each pair of multiple microphones. In our work, we use 256 frequency bins for 25ms windows. The phase feature PD is calculated in every pair of channels, then the MLP network accepts the PDc corresponding to the input candidates, with Xc as an additional input."
    }, {
      "heading" : "2.2 LSTM ACOUSTIC MODEL",
      "text" : "Our next subnetwork LSTM-AM serves as a typical RNN-based acoustic model, except that it accepts the re-weighted input X̂c instead of the original input Xc. LSTM-AM uses a Long Short-Term Memory RNN (LSTM)(Hochreiter & Schmidhuber, 1997), which has been successfully applied to speech recognition tasks due to its ability to handle long-term dependencies. The LSTM contains special units called memory blocks in the recurrent hidden layer, and each block has memory cells ct with special three-gates (input it, output ot, and forget ft) to control the flow of information.\nIn our work, we use a simplified version of an LSTM without peephole connections and biases to reduce the computational expense of learning the standard LSTM models. Although LSTMs have many variations for enhancing their performance, such as BLSTM (Graves et al., 2013), LSTMP (Sak et al., 2014), and PBLSTM (Chan et al., 2015), in our work, we focus on verifying an additional attention mechanism with a simple LSTM architecture, instead of improving LSTM acoustic modeling overall.\nLSTM-AM maps a re-weighted input sequence based on the attention mechanism X̂ =\n{x̂ch1 , · · · , x̂chN }, where x̂chi = (x̂chi1 , · · · , x̂ chi T ), to an output sequence yt = (y1, · · · , yT ) by calculating the network unit activations using the following equations iteratively from t = 1 to T :\nit = σ(x̂cWxi + ht−1Whi) (11) ft = σ(x̂cWxf + ht−1Whf ) (12) ct = ft · ct−1 + it · tanh(x̂cWxc + ht−1Whc) (13) ot = σ(x̂cWxo + ht−1Who) (14) st = ot · tanh(ct) (15)\nwhere W terms denote weight matrices, and σ the logistic sigmoid function. it, ft, ot, and ct are the input gate, forget gate, output gate and cell activation vectors, respectively. Finally, the output st is used to predict the current HMM state label by softmax (in equation 16). st is also used to predict the next t+ 1 attention matrix as well as the next ct+1 hidden state of LSTM-AM.\nyt = argmaxiP (y = i|st) (16)"
    }, {
      "heading" : "3 EXPERIMENTS",
      "text" : ""
    }, {
      "heading" : "3.1 DATASET",
      "text" : "We evaluated the performance of our architecture on the CHiME-3 task. The CHiME-3 (Jon Barker, 2015) task is automatic speech recognition for a multi-microphone tablet device in an everyday environment - a cafe, a street junction, public transport, and a pedestrian area. There are two types of datasets: REAL and SIMU. The REAL data consists of 6-channel recordings. 12 US English speakers were asked to read the sentences from the WSJ0 corpus (Garofalo et al., 2007) while using the multimicrophone tablet. They were encouraged to adjust their reading positions, so that the target distance kept changing over time. The simulated data SIMU was generated by mixing clean utterances from WSJ0 into background recordings. To verify our method in a real noisy environment, we first chose not to use the simulated dataset but rather to use only the REAL dataset, with 5 channels from the five microphones, which were located in each corner of tablet, about 10cm to 20cm away from each other (we excluded one microphone, which faced backward in the tablet device). We then evaluated our system on the full CHiME3 dataset, MULTI, including REAL and SIMU."
    }, {
      "heading" : "3.2 SYSTEM TRAINING",
      "text" : "All the networks were trained on the 1,600 utterance (about 2.9 hours) REAL dataset and then on the 8,738 utterance (about 18 hours) MULTI dataset. The dataset was represented with 25ms frames of 40-dimensional log-filterbank energy features computed every 10ms. We produced 1,992 HMM state labels from a trained GMM-HMM system using near-field microphone data, and these state labels were used in all subsequent experiments. We use one layer of LSTM architecture with 512 cells. The weights in all the networks were initialized to the range (-0.03, 0.03) with a uniform distribution, and the initial attention weights were initialized to 1/n in n dimensions. We set the configuration of the learning rate to 0.4 and after two epochs it decays during training. All models resulted in a stable convergence range from 1e-04 to 5e-04. To avoid the exploding gradient problem, we limited the norm of the gradient to 1 (Pascanu et al., 2012). Apart from the gradient clipping, we did not limit the activations of the weights.\nDuring training, we evaluated frame accuracies (i.e. phone state labeling accuracy of acoustic frames) on the development set of 1,640 utterances in REAL and 3,280 utterances in MULTI. The trained models were evaluated in a speech recognition system on a test set of 1,320 utterances. For all the decoding experiments, we used a size 18 beam and size 10 lattices. There is a mismatch between the Kaldi baseline (Povey et al., 2011) and our results because we did not perform sequence training (sMBR) or language model rescoring (5-gram rescoring or RNNLM). The inputs for all networks were log-filterbank features, with 5 channels stacking, and then with 7 frames stacking (+3-3)."
    }, {
      "heading" : "3.3 RESULTS",
      "text" : "In Table 1 and 2, we summarize word error rates (WERs) obtained on the subset of the CHiME3 task. ALSTM is our proposed model, which has an attention mechanism for multiple inputs as described in 2.1, and ALSTM (with phase) used phase information in addition to ALSTM.\nAs our baselines, we built three models on the REAL dataset and used the same simple version of the LSTM architecture that we described in Section 2.2 with three different inputs. LSTM (Preprocessing 5 noisy-channel) was trained on the enhanced signal from 5 noisy channels. We obtained the enhanced signal from the beamforming toolkit, which was provided by the CHiME3 organizer (Jon Barker, 2015; Loesch & Yang, 2010; Blandin et al., 2012; Mestre et al., 2003). LSTM (single noisy-channel) was trained on a single noisy channel, and LSTM (5 noisy-channels) used the concatenated 5 noisy channels. We also built LSTM (Preprocessing 5 noisy-channel) on the MULTI dataset.\nAs expected, LSTM (Preprocessing 5 noisy-channel) provided a substantial improvement in WER compared to LSTM (single noisy-channel) and LSTM (5 noisy-channel), showing a 13.3% and 5.0% relative improvement in WER, respectively. We also found that the model, which simply combined 5 features across microphones, did not perform very well. It showed poorer results than even the model trained with single microphone data. This result underscores the importance of integrating channels based on analysis of differences in arrival times.\nOur model with the attention mechanism provided a significant improvement in WER compared to LSTM (5 noisy-channel). Compared to LSTM (5 noisy-channel), ALSTM (with phase) achieved a 17% reduction in relative error rate on the evaluation set, and ALSTM achieved a 13% relative\nerror rate. These results suggest that we can leverage the attention mechanism to integrate multiple channels efficiently. To ensure the improvement of the system was coming from our time-channel attention mechanism, we compared our model to a model with an attention mechanism across time only on single-channel input. This comparison model helped to improve accuracy by 3%, a lower gain than that achieved by the time-channel attention mechanism.\nWe also found that the additional phase information can help to learn attention and WER improved by 4.6% relatively. In comparison with LSTM (Preprocessing 5 noisy-channel), we found that our proposed model achieved comparable performance to beamforming without any preprocessing. Although ALSTM shows a slightly lower performance as compared to LSTM (Preprocessing 5 noisy-channel), a 4.0% relative error rate was obtained by ALSTM (with phase). When we used LSTM-AM with the additional phase features without any attention mechanism, it had a negative influence on learning. Thus, using the phase features for the attention mechanism is more effective than using the phase features as direct inputs of the acoustic model.\nWe also evaluated the models on the MULTI dataset. We found that our system outperformed LSTM (Preprocessing 5 noisy-channel) by 5%, and the gain from the time-channel attention mechanism increased.\nWe then analyzed the computational aspects of our system. As the multi-microphone is performed as part of the acoustic model computation we have actually found it to be more computationally efficient than performing beamforming followed by an LSTM acoustic model. On our development machine (Intel(R) Xeon(R) CPU E5-2690 v3 @ 2.60GHz), the proposed multi-microphone model with attention and phase operated 0.08 real-time, which was significantly faster than the beamforming followed by acoustic model computation which operated at 0.6 real-time."
    }, {
      "heading" : "4 CONCLUSIONS",
      "text" : "We proposed an attention-based model (ALSTM) that uses asynchronous and non-stationary inputs from multiple channels to generate outputs. For a distant speech recognition task, we embedded a novel attention mechanism within a RNN-based acoustic model to automatically tune its attention to a more reliable input source. We presented our results on the CHiME3 task and found that ALSTM showed a substantial improvement in WER. Our model achieved comparable performance to beamforming without any prior knowledge of the microphone layout or any explicit preprocessing.\nThe implications of this work are significant and far-reaching. Our work suggests a way to build a more efficient ASR system by bypassing preprocessing. Our findings suggest that this approach will likely do well on tasks that need to exploit misaligned and non-stationary inputs from multiple sources, such as multimodal problems and sensory fusion. We believe that our attention framework can greatly improve these tasks by maximizing the benefits of using inputs from multiple sources."
    }, {
      "heading" : "ACKNOWLEDGMENTS",
      "text" : "The authors would like to acknowledge Richard M. Stern and William Chan for their valuable and constructive suggestions. This research was supported by LGE."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2016,
    "abstractText" : "Integration of multiple microphone data is one of the key ways to achieve ro-<lb>bust speech recognition in noisy environments or when the speaker is located at<lb>some distance from the input device. Signal processing techniques such as beam-<lb>forming are widely used to extract a speech signal of interest from background<lb>noise. These techniques, however, are highly dependent on prior spatial informa-<lb>tion about the microphones and the environment in which the system is being used.<lb>In this work, we present a neural attention network that directly combines multi-<lb>channel audio to generate phonetic states without requiring any prior knowledge<lb>of the microphone layout or any explicit signal preprocessing for speech enhance-<lb>ment. We embed an attention mechanism within a Recurrent Neural Network<lb>(RNN) based acoustic model to automatically tune its attention to a more reli-<lb>able input source. Unlike traditional multi-channel preprocessing, our system can<lb>be optimized towards the desired output in one step. Although attention-based<lb>models have recently achieved impressive results on sequence-to-sequence learn-<lb>ing, no attention mechanisms have previously been applied to learn potentially<lb>asynchronous and non-stationary multiple inputs. We evaluate our neural atten-<lb>tion model on the CHiME-3 challenge task, and show that the model achieves<lb>comparable performance to beamforming using a purely data-driven method.",
    "creator" : "LaTeX with hyperref package"
  }
}