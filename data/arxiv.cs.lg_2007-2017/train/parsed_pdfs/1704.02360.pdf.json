{
  "name" : "1704.02360.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Voice Conversion Using Sequence-to-Sequence Learning of Context Posterior Probabilities",
    "authors" : [ "Hiroyuki Miyoshi", "Yuki Saito" ],
    "emails" : [ "mathma1306@gmail.com", "saruwatari}@ipc.i.u-tokyo.ac.jp" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n70 4.\n02 36\n0v 1\n[ cs\n.S D\n] 1\n0 A\npr 2\n01 7\nof context posterior probabilities is proposed. Conventional VC using shared context posterior probabilities predicts target speech parameters from the context posterior probabilities estimated from the source speech parameters. Although conventional VC can be built from non-parallel data, it is difficult to convert speaker individuality such as phonetic property and speaking rate contained in the posterior probabilities because the source posterior probabilities are directly used for predicting target speech parameters. In this work, we assume that the training data partly include parallel speech data and propose sequence-to-sequence learning between the source and target posterior probabilities. The conversion models perform non-linear and variable-length transformation from the source probability sequence to the target one. Further, we propose a joint training algorithm for the modules. In contrast to conventional VC, which separately trains the speech recognition that estimates posterior probabilities and the speech synthesis that predicts target speech parameters, our proposed method jointly trains these modules along with the proposed probability conversion modules. Experimental results demonstrate that our approach outperforms the conventional VC. Index Terms: voice conversion, context posterior probabilities, sequence-to-sequence learning"
    }, {
      "heading" : "1. Introduction",
      "text" : "Voice conversion (VC) is a technique for converting para- and non-linguistic information while keeping linguistic information. VC is used in a variety of applications such as speech enhancement [1, 2] and language education for non-native speakers [3]. It is mainly classified into two types: text-independent VC and text-dependent VC. Text-independent VC directly predicts target speech parameters from the source speech parameters, and acoustic models such as Gaussian mixture models [4, 5] or deep neural network [6] are trained using only speech data. Since the models are often trained using parallel speech data, the conversion quality of the performance is typically highly accurate. However, in most cases, parallel data is not readily available. Text-dependent VC [7, 8], in contrast, converts speech parameters through textual information. This type consists of two modules: speech recognition that estimates the textual information from the source speech, and speech synthesis that predicts target speech from the textual information. Basically, parallel data are not required to build the VC, and the training data are easily available. However, the conversion units of this method are rougher (e.g., phoneme, word, or other linguistic units) than those of text-independent VC (e.g., frame). VC using shared context posterior probabilities [9] is classified\nin text-dependent VC, but the conversion unit is frame level. The context posterior probability of source speech parameters is estimated frame by frame, and then the target speech parameters are predicted from the estimated posterior probabilities. This VC is interpreted as soft text-dependent VC and can be extended to cross-lingual text-to-speech [10, 11]. However, it cannot convert speaker individuality (e.g., speaking rates and phonetic properties) included in the context posterior probabilities because the posterior probabilities of the source speech are directly used for predicting target speech parameters.\nIn light of the above, we propose a sequence-to-sequence learning of the context posterior probabilities. Assuming that the training data partly include parallel speech data (parallel utterances of phrases), we build an encoder-decoder model [12] that converts the posterior probabilities of the source speech parameters into those of the target speech parameters. The proposed posterior probability conversion module is inserted between conventional speech recognition and synthesis. When we do not build the conversion model or do not have parallel data, conventional VC [9] is available. Further, we propose a joint training algorithm. Whereas the conventional VC [9] separately trains speech recognition and speech synthesis, our approach jointly trains these modules (like auto-encoding) and the proposed conversion module. We found through experiment that the proposed methods outperform the conventional VC [9]."
    }, {
      "heading" : "2. VC Using Shared Context Posterior Probabilities",
      "text" : "Conventional VC using shared context posterior probabilities [9] contains two modules: speech recognition and speech synthesis. They are separately trained, and the voice conversion is performed by concatenating them. Figure 1 shows an example of context posterior probabilities. The upper and middle parts of Fig. 2 show the details of these processes."
    }, {
      "heading" : "2.1. Training Stage",
      "text" : "Recognition models estimate context posterior probability sequence from the speech parameter sequence. Let x = [x⊤1 , · · · ,x ⊤ Tx ]⊤ and y = [y⊤1 , · · · , y ⊤ Ty ]⊤ be source and target speech parameter sequences, respectively. xt and yt are the parameters at frame t. Tx and Ty are their frame lengths. Also, let l(x) = [l (x) 1 , · · · , l (x) Tx ]⊤ and l(y) = [l (y) 1 , · · · , l (y) Ty ]⊤ be the context label sequence (such as quin-phone) corresponding to x and y, respectively. Speaker-independent neural network R(·) is trained using speech data including x and y, and the training criterion is minimizing the cross entropy LC(lx,R(x)).\nSynthesis models predict target speech parameter sequence y from the corresponding context posterior probability se-\nquence p̂y , using the trained recognition models, i.e., p̂y = R(y). The target-speaker-dependent neural networks G(·) are trained to minimize the mean squared error LG(y,G(p̂y)) between y and G(p̂y)."
    }, {
      "heading" : "2.2. Conversion Stage",
      "text" : "In conversion, the converted speech parameter sequence ŷ is predicted by concatenating speech recognition and speech synthesis, i.e., ŷ = G(p̂x) = G(R(x)), where p̂x is the context posterior probability sequence of x. Note that the frame lengths of x, p̂x and ŷ are the same, i.e., Tx."
    }, {
      "heading" : "2.3. Problems",
      "text" : "Since the posterior probabilities estimated in speech recognition are directly used for speech synthesis, it is difficult to convert speaker individuality included in the posterior probabilities, such as the speaking rate (frame length) and phonetic characteristics (see Fig. 1). Also, improving recognition accuracy does not always improve speech quality in converted speech (except zero error in recognition)."
    }, {
      "heading" : "3. Proposed VC using Sequence-to-Sequence Learning of Context Posterior Probabilities",
      "text" : "To overcome the limitation of the conventional method, we propose an approach for converting source context posterior probabilities to target context posterior probabilities using sequenceto-sequence learning."
    }, {
      "heading" : "3.1. Sequence-to-Sequence Learning",
      "text" : "Sequence-to-sequence learning using recurent neural networks (RNNs) [13] can be applied to the problem that the source and target sequences have different lengths. An encoder-decoder model we adopt here maps a variable-length source sequence to a fixed-length vector, and maps the vector to the variable-length target sequence. At each frame, the source side RNN (encoder) and target side RNN (decoder) predict the source and target features of the next frame, respectively. As discussed below, we adopt this in order to convert a source posterior probability sequence to a target that has a different length. The proposed procedure is shown in the lower part of Fig. 2."
    }, {
      "heading" : "3.2. Training Stage",
      "text" : "We propose two algorithms to perform the posterior probability conversion. The first algorithm separately trains speech recog-\n! \" # $\n#%&'('()\n!\"#$\n% & ' () $ * + $ $ ) , -.$ / !* 0\n1 / (2 $ ! * + $ $ ) , -.$ / !* 0 ! \" # $\n3&*!$(\"&(-+(&4*0 / \"\n'\n*+(,-%.'+(/01+(,-(2'+(&34\n0\n1 / (2 $ ! + & * !$ (\" & (- + (& 4 * 0\nnition and synthesis the same as the conventional algorithms and separately trains probability conversion models using the source and target posterior probabilities. The second algorithm jointly trains the recognition and synthesis and trains the conversion models considering not only posterior probabilities conversion but also speech synthesis. In conversion, we concatenate the three models for converting input speech parameters."
    }, {
      "heading" : "3.2.1. Training of probability conversion models",
      "text" : "Given the parallel sequences of source and target context posterior probabilities, we train encoder-decoder models C(·) that convert the source and target sequences. The loss function to be minimized is as follows:\nL(ly, p̂x, p̂y) = LG(p̂y ,C(p̂x)) + LC(ly ,C(p̂x)). (1)\nThe first term minimizes the conversion error between the predicted and target sequences. The second term minimizes the cross-entropy using ly , which was obtained by the training of R(·), and can decrease the recognition error included in p̂y. Our preliminary evaluation demonstrated that using this formulation results in better conversion accuracy than using only the first term.\nSequence-to-sequence learning suffers from long-term dependencies, i.e., error accumulation, so in our approach, we implement phoneme-by-phoneme probability conversion. Given the phoneme boundary of the source and target probability sequence, phoneme-independent encoder-decoder models are trained to convert the probability sequence within the current phoneme."
    }, {
      "heading" : "3.2.2. Jointly training of recognition, synthesis, and conversion",
      "text" : "Since the final goal of the method is to minimize synthesis error, its pre-processes (i.e., recognition and conversion) must be trained by considering this error. We train speech recognition R(·) to minimize not only recognition error but also synthesis\nerror (e.g., reconstruction error of auto-encoders): the loss function is LC(lx,R(x))+LG(x,G(R(x))). The speech synthesis G(·) is trained in the conventional manner. We further train the conversion models to minimize not only conversion error but also synthesis error: the loss function is the sum of Eq. (1) and LG(G(y,C(p̂(x))))."
    }, {
      "heading" : "3.3. Discussion",
      "text" : "Text-dependent VC forcibly aligns the input speech feature segment into a single context (e.g., phoneme, syllable, or word unit) and generates output speech features from the context sequence. Although this method can flexibly transform the context sequence (e.g., variable-length conversion), it cannot avoid the effect of time quantization with mapping from speech feature segments. Meanwhile, text-independent VC with dynamic time warping (DTW) [4] aligns speech features in a frame level, but it limits the transformation of (implicitly considered) context sequence, e.g., the sequence length is fixed. The conventional method [9, 10] also corresponds to the latter because the source posterior probability is directly used for synthesizing the target speech. In comparison with these methods, since the proposed algorithm performs frame-level conversion without forced alignments, it can avoid the effect of time quantization and convert context sequence flexibly.\nJoint training of recognition and synthesis, which is proposed in Section 3.2.2, is similar to auto-encoding processes [14] with the referred class labels and dual learning [15]. Therefore, we expect that these processes can be extended to the supervised learning of variational auto-encoders [16] that have not only class labels (e.g., context labels) but also the hidden variables [17, 18]."
    }, {
      "heading" : "4. Experiments",
      "text" : ""
    }, {
      "heading" : "4.1. Experimental Setup",
      "text" : "Although the conventional VC [9] and proposed VC accept non-parallel speech data and partly included parallel data, we used fully parallel data in this evaluation. We prepared speech data of eight speakers taken from the ATR Japanese speech database [19]. The speaker uttered 503 phonetically balanced sentences. We built the speaker-independent speech recognition module with speaker codes by using the speech data of eight speakers including source and target speakers. We built conversion and synthesis modules by using speech data of source female and target male speakers. We used 450 sentences (subsets A to I) for the training and 53 sentences (subset J) for the evaluation. Speech signals were sampled at a rate of 16 kHz, and the shift length was set to 5 ms. The 0th–through–24th melcepstral coefcients were used as a spectral parameter and F0 and 5 band-aperiodicity [20, 21] were used as excitation parameters. We used the STRAIGHT analysis-synthesis system [22] for the parameter extraction and waveform synthesis. To improve training accuracy, speech parameter trajectory smoothing [23] with a 50 Hz cutoff modulation frequency was applied to the spectral parameters in the training data. In the training phase, spectral features were normalized to have zero-mean unit-variance, and 80 % of the silent frames were removed from the training data. We used AdaGrad [24] as the optimization algorithm, setting the learning rate to 0.01. Both in speech recognition and generation stage, we used bi-directional long short-term memory (LSTM) [9, 10] and each hidden layer contained 256 units. For probability conversion, we used bi-directional LSTM in encoder, and LSTM in decoder. Each hidden layers contained 256\nunits.\nQuin-phone was used as the context labels. For training the recognition models, we divided the quin-phone into five groups: previous phoneme, current phoneme, and next phoneme, and so on. The cross-entropy loss was calculated for each group, and the loss function for training the recognition models was the sum of each loss [25]. Only spectral and their delta features were used for recognition and synthesis. In the proposed method, F0 was linearly transformed [4] first, and we modified its length using DTW between the source context posterior probability sequence and the converted posterior probability sequence. Only DTW was used for band-aperiodicity conversion. This evaluation uses reference phoneme duration for converting posterior probabilities in order to address the sequence length determination problem to which sequence-to-sequence learning is prone [26]. Given the phoneme duration of the target natural speech parameter sequence in conversion data, we performed phoneme-level probability conversion. Given the phoneme duration of the source and target speech parameters in the training and conversion data, the conversion models converted the probabilities within the current phoneme. The finally generated posterior probability sequence was then calculated by concatenating the converted probabilities.\nTwo evaluations were performed to compare the conventional and proposed VC. First, we evaluated the effectiveness of the proposed posterior probability conversion, and then, we evaluated the effect of the proposed joint training algorithms."
    }, {
      "heading" : "4.2. Evaluations",
      "text" : "We discuss the effectiveness of the proposed posterior probability conversion. The separately trained modules were used here."
    }, {
      "heading" : "4.2.1. Objective Evaluation",
      "text" : "We calculated mel-cepstral distortion between the target and converted speech parameters of the conventional VC [9] and proposed VC. DTW was used to align the target and converted parameters by the conventional VC. The difference between the two methods is the time warping method, i.e., DTW or sequence-to-sequence learning. The results (Fig. 3) clearly show that the proposed VC outperforms the conventional VC, we demonstrate that spectral distortion caused by DTW can be alleviated by the use of sequence-to-sequence learning."
    }, {
      "heading" : "4.2.2. Subjective Evaluation",
      "text" : "To subjectively evaluate the conventional and proposed VC, we performed a preference AB test to evaluate the converted speech quality. We presented every pair of converted speech of the two sets in random order and had listeners select the speech sample that sounded better. Similarly, we performed an XAB test on the speaker individuality using the natural speech as a reference “X.” Seven listeners participated in each assessment.\nThe results are shown in Fig. 4. Althogh the proposed VC performed better in speaker similarity (Fig. 4(a)) thanks to posterior probability conversion, it degrades speech quality (Fig. 4(b)). It seems that the probability conversion caused con-\nversion error that missed the phonetic properties of the source speech parameters, which is probably what resulted in the degraded quality."
    }, {
      "heading" : "4.3. Evaluation of Joint Training",
      "text" : ""
    }, {
      "heading" : "4.3.1. Joint Training of Recognition and Synthesis",
      "text" : "We evaluated the effectiveness of joint training of recognition and synthesis modules, in comparison to conventional separately trained modules [9]. We calculated mel-cepstral distortion in the auto-encoding case, i.e., reconstruction error of source speech parameters through recognition and synthesis. As shown in Fig. 5, the proposed joint training achieved better distortion than conventional separated training.\nWe also performed an AB test on speech quality and XAB test on speaker similarity in the VC case, as similar as in Section 4.2.2. As shown in Fig. 6, the proposed joint-training overcomes the conventional separated training in both speaker similarity and speech quality."
    }, {
      "heading" : "4.3.2. Joint Training of Recognition, Conversion, and Synthesis",
      "text" : "To evaluate of the joint training of recognition, conversion, and synthesis, we calculated the mel-cepstral distortion of three systems: (1) conventional VC [9], (2) separately trained modules (equal to ”Proposed” in Section 4.2.1), and (3) jointly trained modules.\nThe results are shown in Fig. 7. The joint training scored better than conventional VC but worse than separated training. To clarify this, we show in Fig. 8 an example of probability sequence estimated through speech recognition. The separately trained recognition module outputs harder probabilities, i.e., the values are close to 0 or 1 for all frames. However, we can see that the joint training of recognition and synthesis tends to make the values soft. This requires deeper investigation, but we suspect this tendency is one of the reasons."
    }, {
      "heading" : "5. Conclusion",
      "text" : "In this paper, we proposed voice conversion (VC) using sequence-to-sequence learning of context posterior probabilities. Since conventional VC directly uses the posterior probabilities of source speech for predicting target speech, it is difficult to convert the speaker individuality included in the posterior probabilities. To address this, we built sequence-to-sequence conversion models that convert the source context posterior probability sequence into a target one. Further, we proposed joint training algorithms for speech recognition, speech synthesis, and posterior probability conversion. Experimental results demonstrated that (1) the proposed algorithms outperformed the conventional VC in speaker similarity, and (2) joint training of recognition and synthesis outperformed the conventional VC in both speaker similarity and speech quality. As future work, we will investigate how to determine the frame length of the converted posterior probability sequence.\nAcknowledgements: Part of this work was supported by ImPACT Program of Council for Science, Technology and Innovation (Cabinet Ofce, Government of Japan), SECOM Science and Technology Foundation, and JSPS KAKENHI Grant Number 16H06681."
    }, {
      "heading" : "6. References",
      "text" : "[1] A. B. Kain, J.-P. Hosom, X. Niu, J. P. H. van Santen, M. F.Oken, and J. Staehely, “Improving the intelligibility of dysarthric speech,” Speech Communication, vol. 49, no. 9, pp. 743–759, 2007.\n[2] F. Rudzicz, “Acoustic transformations to improve the intelligibility of dysarthric speech,” in Proc. SLPAT, Edinburgh, Scotland, Jul. 2011, pp. 11–21.\n[3] S. Aryal and R. G.-Osuna, “Can voice conversion be used to reduce non-native accents?” in Proc. ICASSP, Florence, Italy, May 2014, pp. 7929–7933.\n[4] T. Toda, A. W. Black, and K. Tokuda, “Voice conversion based on maximum likelihood estimation of spectral parameter trajectory,” IEEE Transactions on Audio, Speech, and Language Processing, vol. 15, no. 8, pp. 2222–2235, 2007.\n[5] Y. Stylianou, O. Cappé, and E. Moulines, “Continuous probabilistic transform for voice conversion,” IEEE Transactions on Speech and Audio Processing, vol. 6, no. 2, pp. 131–142, 1998.\n[6] S. Desai, E. V. Raghavendra, B. Yegnanarayana, A. W. Black, and K. Prahallad, “Voice conversion using artificial neural networks,” in Proc. ICASSP, Taipei, Taiwan, Apr. 2009, pp. 3893–3896.\n[7] A. Kain and M. W. Macon, “Spectral voice conversion for text-tospeech synthesis,” in Proc. ICASSP, Seattle, U.S.A., May 1998, pp. 285–288.\n[8] D. Sunderman, H. Hoge, A. Bonafonte, H. Ney, A. W. Black, and S. Narayanan, “Text-independent voice conversion based on unit selection,” in Proc. ICASSP, Toulouse, France, May 2006.\n[9] L. Sun, K. Li, H. Wang, S. Kang, and H. Meng, “Phonetic posteriorgrams for many-to-one voice conversion without parallel data training,” in Proc. ICME, Seattle, U.S.A., Jul. 2016.\n[10] L. Sun, S. Kang, K. Li, and H. Meng, “Personalized, cross-lingual TTS using phonetic posteriorgrams,” in Proc. INTERSPEECH, San Francisco, U.S.A., Sep. 2016, pp. 322–326.\n[11] F.-L. Xie, F. K. Soong, and H. Li, “A KL divergence and DNNbased approach to voice conversion without parallel training sentences,” in Proc. INTERSPEECH, San Francisco, U.S.A., Sep. 2016, pp. 287–291.\n[12] K. Cho, D. Bahdanau, F. Vougares, H. Schwenk, and Y. Bengio, “Learning phrase representations using RNN encoderdecoder for statistical machine translation,” in Proc. EMNLP, Doha, Qatar, Oct. 2014, pp. 1724–1734.\n[13] I. Sutskever, O. Vinyals, and Q. V. Le, “Sequence to sequence learning with neural networks,” in Proc. NIPS, Montreal, Canada, Dec. 2014, pp. 3104–3112.\n[14] Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle, “Greedy layer-wise training of deep networks,” in Proc. NIPS, Vancouver, Canada, Dec. 2006, pp. 153–160.\n[15] D. He, Y. Xia, T. Qin, L. Wang, N. Yu, T. Liu, and W.-Y. Ma, “Dual learning for machine translation,” in Proc. NIPS, Barcelona, Spain, Dec. 2016, pp. 820–828.\n[16] D. P. Kingma and M. Welling, “Auto-encoding variational bayes,” in Proc. ICLR, Banff, Canada, Apr. 2014.\n[17] C.-C. Hsu, H.-T. Hwang, Y.-C. Wu, Y. Tsao, and H.-M. Wang, “Voice conversion from non-parallel corpora using variational auto-encoder,” in Proc. APSIPA ASC, Jeju, Korea, Dec. 2016.\n[18] D. P. Kingma, D. J. Rezende, S. Mohamed, and M. Welling, “Semi-supervised learning with deep generative models,” in Proc. NIPS, Montreal, Canada, Dec. 2014, pp. 3581–3589.\n[19] Y. Sagisaka, K. Takeda, M. Abe, S. Katagiri, T. Umeda, and H. Kuawhara, “A large-scale Japanese speech database,” in ICSLP90, Kobe, Japan, Nov. 1990, pp. 1089–1092.\n[20] H. Kawahara, J. Estill, and O. Fujimura, “Aperiodicity extraction and control using mixed mode excitation and group delay manipulation for a high quality speech analysis, modification and synthesis system STRAIGHT,” in MAVEBA 2001, Firentze, Italy, Sep. 2001, pp. 1–6.\n[21] Y. Ohtani, T. Toda, H. Saruwatari, and K. Shikano, “Maximum likelihood voice conversion based on GMM with STRAIGHT mixed excitation,” in Proc. INTERSPEECH, Pittsburgh, U.S.A., Sep. 2006, pp. 2266–2269.\n[22] H. Kawahara, I. Masuda-Katsuse, and A. D. Cheveigne, “Restructuring speech representations using a pitch-adaptive timefrequency smoothing and an instantaneous-frequency-based F0 extraction: Possible role of a repetitive structure in sounds,” Speech Communication, vol. 27, no. 3–4, pp. 187–207, 1999.\n[23] S. Takamichi, K. Kobayashi, K. Tanaka, T. Toda, and S. Nakamura, “The NAIST text-to-speech system for the Blizzard Challenge 2015,” in Proc. Blizzard Challenge workshop, Berlin, Germany, Sep. 2015.\n[24] J. Duchi, E. Hazan, and Y. Singer, “Adaptive subgradient methods for online learning and stochastic optimization,” Journal of Machine Learning Research, vol. 12, pp. 2121–2159, Jul. 2011.\n[25] B. Huang, D. Ke, H. Zheng, B. Xu, Y. Xu, and K. Su, “Multi-task learning deep neural networks for speech feature denoising,” in Proc. INTERSPEECH, Dresden, Germany, Sep. 2015, pp. 2464– 2468.\n[26] W. Wang, S. Xu, and B. Xu, “First step towards end-to-end parametric TTS synthesis: Generating spectral parameters with neural attention,” in Proc. INTERSPEECH, San Francisco, U.S.A., Sep. 2016, pp. 2243–2247."
    } ],
    "references" : [ {
      "title" : "Improving the intelligibility of dysarthric speech",
      "author" : [ "A.B. Kain", "J.-P. Hosom", "X. Niu", "J.P.H. van Santen", "M.F.- Oken", "J. Staehely" ],
      "venue" : "Speech Communication, vol. 49, no. 9, pp. 743–759, 2007.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Acoustic transformations to improve the intelligibility of dysarthric speech",
      "author" : [ "F. Rudzicz" ],
      "venue" : "Proc. SLPAT, Edinburgh, Scotland, Jul. 2011, pp. 11–21.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "G.-Osuna, “Can voice conversion be used to reduce non-native accents?",
      "author" : [ "R.S. Aryal" ],
      "venue" : "in Proc. ICASSP, Florence,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2014
    }, {
      "title" : "Voice conversion based on maximum likelihood estimation of spectral parameter trajectory",
      "author" : [ "T. Toda", "A.W. Black", "K. Tokuda" ],
      "venue" : "IEEE Transactions on Audio, Speech, and Language Processing, vol. 15, no. 8, pp. 2222–2235, 2007.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Continuous probabilistic transform for voice conversion",
      "author" : [ "Y. Stylianou", "O. Cappé", "E. Moulines" ],
      "venue" : "IEEE Transactions on Speech and Audio Processing, vol. 6, no. 2, pp. 131–142, 1998.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Voice conversion using artificial neural networks",
      "author" : [ "S. Desai", "E.V. Raghavendra", "B. Yegnanarayana", "A.W. Black", "K. Prahallad" ],
      "venue" : "Proc. ICASSP, Taipei, Taiwan, Apr. 2009, pp. 3893–3896.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Spectral voice conversion for text-tospeech synthesis",
      "author" : [ "A. Kain", "M.W. Macon" ],
      "venue" : "Proc. ICASSP, Seattle, U.S.A., May 1998, pp. 285–288.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Text-independent voice conversion based on unit selection",
      "author" : [ "D. Sunderman", "H. Hoge", "A. Bonafonte", "H. Ney", "A.W. Black", "S. Narayanan" ],
      "venue" : "Proc. ICASSP, Toulouse, France, May 2006.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Phonetic posteriorgrams for many-to-one voice conversion without parallel data training",
      "author" : [ "L. Sun", "K. Li", "H. Wang", "S. Kang", "H. Meng" ],
      "venue" : "Proc. ICME, Seattle, U.S.A., Jul. 2016.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Personalized, cross-lingual TTS using phonetic posteriorgrams",
      "author" : [ "L. Sun", "S. Kang", "K. Li", "H. Meng" ],
      "venue" : "Proc. INTERSPEECH, San Francisco, U.S.A., Sep. 2016, pp. 322–326.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "A KL divergence and DNNbased approach to voice conversion without parallel training sentences",
      "author" : [ "F.-L. Xie", "F.K. Soong", "H. Li" ],
      "venue" : "Proc. INTERSPEECH, San Francisco, U.S.A., Sep. 2016, pp. 287–291.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Learning phrase representations using RNN encoderdecoder for statistical machine translation",
      "author" : [ "K. Cho", "D. Bahdanau", "F. Vougares", "H. Schwenk", "Y. Bengio" ],
      "venue" : "Proc. EMNLP, Doha, Qatar, Oct. 2014, pp. 1724–1734.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "I. Sutskever", "O. Vinyals", "Q.V. Le" ],
      "venue" : "Proc. NIPS, Montreal, Canada, Dec. 2014, pp. 3104–3112.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Greedy layer-wise training of deep networks",
      "author" : [ "Y. Bengio", "P. Lamblin", "D. Popovici", "H. Larochelle" ],
      "venue" : "Proc. NIPS, Vancouver, Canada, Dec. 2006, pp. 153–160.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Dual learning for machine translation",
      "author" : [ "D. He", "Y. Xia", "T. Qin", "L. Wang", "N. Yu", "T. Liu", "W.-Y. Ma" ],
      "venue" : "Proc. NIPS, Barcelona, Spain, Dec. 2016, pp. 820–828.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Auto-encoding variational bayes",
      "author" : [ "D.P. Kingma", "M. Welling" ],
      "venue" : "Proc. ICLR, Banff, Canada, Apr. 2014.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Voice conversion from non-parallel corpora using variational auto-encoder",
      "author" : [ "C.-C. Hsu", "H.-T. Hwang", "Y.-C. Wu", "Y. Tsao", "H.-M. Wang" ],
      "venue" : "Proc. APSIPA ASC, Jeju, Korea, Dec. 2016.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Semi-supervised learning with deep generative models",
      "author" : [ "D.P. Kingma", "D.J. Rezende", "S. Mohamed", "M. Welling" ],
      "venue" : "Proc. NIPS, Montreal, Canada, Dec. 2014, pp. 3581–3589.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "A large-scale Japanese speech database",
      "author" : [ "Y. Sagisaka", "K. Takeda", "M. Abe", "S. Katagiri", "T. Umeda", "H. Kuawhara" ],
      "venue" : "IC- SLP90, Kobe, Japan, Nov. 1990, pp. 1089–1092.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 1990
    }, {
      "title" : "Aperiodicity extraction and control using mixed mode excitation and group delay manipulation for a high quality speech analysis, modification and synthesis system STRAIGHT",
      "author" : [ "H. Kawahara", "J. Estill", "O. Fujimura" ],
      "venue" : "MAVEBA 2001, Firentze, Italy, Sep. 2001, pp. 1–6.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Maximum likelihood voice conversion based on GMM with STRAIGHT mixed excitation",
      "author" : [ "Y. Ohtani", "T. Toda", "H. Saruwatari", "K. Shikano" ],
      "venue" : "Proc. INTERSPEECH, Pittsburgh, U.S.A., Sep. 2006, pp. 2266–2269.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Restructuring speech representations using a pitch-adaptive timefrequency smoothing and an instantaneous-frequency-based F0 extraction: Possible role of a repetitive structure in sounds",
      "author" : [ "H. Kawahara", "I. Masuda-Katsuse", "A.D. Cheveigne" ],
      "venue" : "Speech Communication, vol. 27, no. 3–4, pp. 187–207, 1999.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "The NAIST text-to-speech system for the Blizzard Challenge 2015",
      "author" : [ "S. Takamichi", "K. Kobayashi", "K. Tanaka", "T. Toda", "S. Nakamura" ],
      "venue" : "Proc. Blizzard Challenge workshop, Berlin, Germany, Sep. 2015.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Adaptive subgradient methods for online learning and stochastic optimization",
      "author" : [ "J. Duchi", "E. Hazan", "Y. Singer" ],
      "venue" : "Journal of Machine Learning Research, vol. 12, pp. 2121–2159, Jul. 2011.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Multi-task learning deep neural networks for speech feature denoising",
      "author" : [ "B. Huang", "D. Ke", "H. Zheng", "B. Xu", "Y. Xu", "K. Su" ],
      "venue" : "Proc. INTERSPEECH, Dresden, Germany, Sep. 2015, pp. 2464– 2468.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "First step towards end-to-end parametric TTS synthesis: Generating spectral parameters with neural attention",
      "author" : [ "W. Wang", "S. Xu", "B. Xu" ],
      "venue" : "Proc. INTERSPEECH, San Francisco, U.S.A., Sep. 2016, pp. 2243–2247.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "VC is used in a variety of applications such as speech enhancement [1, 2] and language education for non-native speakers [3].",
      "startOffset" : 67,
      "endOffset" : 73
    }, {
      "referenceID" : 1,
      "context" : "VC is used in a variety of applications such as speech enhancement [1, 2] and language education for non-native speakers [3].",
      "startOffset" : 67,
      "endOffset" : 73
    }, {
      "referenceID" : 2,
      "context" : "VC is used in a variety of applications such as speech enhancement [1, 2] and language education for non-native speakers [3].",
      "startOffset" : 121,
      "endOffset" : 124
    }, {
      "referenceID" : 3,
      "context" : "Text-independent VC directly predicts target speech parameters from the source speech parameters, and acoustic models such as Gaussian mixture models [4, 5] or deep neural network [6] are trained using only speech data.",
      "startOffset" : 150,
      "endOffset" : 156
    }, {
      "referenceID" : 4,
      "context" : "Text-independent VC directly predicts target speech parameters from the source speech parameters, and acoustic models such as Gaussian mixture models [4, 5] or deep neural network [6] are trained using only speech data.",
      "startOffset" : 150,
      "endOffset" : 156
    }, {
      "referenceID" : 5,
      "context" : "Text-independent VC directly predicts target speech parameters from the source speech parameters, and acoustic models such as Gaussian mixture models [4, 5] or deep neural network [6] are trained using only speech data.",
      "startOffset" : 180,
      "endOffset" : 183
    }, {
      "referenceID" : 6,
      "context" : "Text-dependent VC [7, 8], in contrast, converts speech parameters through textual information.",
      "startOffset" : 18,
      "endOffset" : 24
    }, {
      "referenceID" : 7,
      "context" : "Text-dependent VC [7, 8], in contrast, converts speech parameters through textual information.",
      "startOffset" : 18,
      "endOffset" : 24
    }, {
      "referenceID" : 8,
      "context" : "VC using shared context posterior probabilities [9] is classified in text-dependent VC, but the conversion unit is frame level.",
      "startOffset" : 48,
      "endOffset" : 51
    }, {
      "referenceID" : 9,
      "context" : "This VC is interpreted as soft text-dependent VC and can be extended to cross-lingual text-to-speech [10, 11].",
      "startOffset" : 101,
      "endOffset" : 109
    }, {
      "referenceID" : 10,
      "context" : "This VC is interpreted as soft text-dependent VC and can be extended to cross-lingual text-to-speech [10, 11].",
      "startOffset" : 101,
      "endOffset" : 109
    }, {
      "referenceID" : 11,
      "context" : "Assuming that the training data partly include parallel speech data (parallel utterances of phrases), we build an encoder-decoder model [12] that converts the posterior probabilities of the source speech parameters into those of the target speech parameters.",
      "startOffset" : 136,
      "endOffset" : 140
    }, {
      "referenceID" : 8,
      "context" : "When we do not build the conversion model or do not have parallel data, conventional VC [9] is available.",
      "startOffset" : 88,
      "endOffset" : 91
    }, {
      "referenceID" : 8,
      "context" : "Whereas the conventional VC [9] separately trains speech recognition and speech synthesis, our approach jointly trains these modules (like auto-encoding) and the proposed conversion module.",
      "startOffset" : 28,
      "endOffset" : 31
    }, {
      "referenceID" : 8,
      "context" : "We found through experiment that the proposed methods outperform the conventional VC [9].",
      "startOffset" : 85,
      "endOffset" : 88
    }, {
      "referenceID" : 8,
      "context" : "Conventional VC using shared context posterior probabilities [9] contains two modules: speech recognition and speech synthesis.",
      "startOffset" : 61,
      "endOffset" : 64
    }, {
      "referenceID" : 12,
      "context" : "Sequence-to-sequence learning using recurent neural networks (RNNs) [13] can be applied to the problem that the source and target sequences have different lengths.",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 3,
      "context" : "Meanwhile, text-independent VC with dynamic time warping (DTW) [4] aligns speech features in a frame level, but it limits the transformation of (implicitly considered) context sequence, e.",
      "startOffset" : 63,
      "endOffset" : 66
    }, {
      "referenceID" : 8,
      "context" : "The conventional method [9, 10] also corresponds to the latter because the source posterior probability is directly used for synthesizing the target speech.",
      "startOffset" : 24,
      "endOffset" : 31
    }, {
      "referenceID" : 9,
      "context" : "The conventional method [9, 10] also corresponds to the latter because the source posterior probability is directly used for synthesizing the target speech.",
      "startOffset" : 24,
      "endOffset" : 31
    }, {
      "referenceID" : 13,
      "context" : "2, is similar to auto-encoding processes [14] with the referred class labels and dual learning [15].",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 14,
      "context" : "2, is similar to auto-encoding processes [14] with the referred class labels and dual learning [15].",
      "startOffset" : 95,
      "endOffset" : 99
    }, {
      "referenceID" : 15,
      "context" : "Therefore, we expect that these processes can be extended to the supervised learning of variational auto-encoders [16] that have not only class labels (e.",
      "startOffset" : 114,
      "endOffset" : 118
    }, {
      "referenceID" : 16,
      "context" : ", context labels) but also the hidden variables [17, 18].",
      "startOffset" : 48,
      "endOffset" : 56
    }, {
      "referenceID" : 17,
      "context" : ", context labels) but also the hidden variables [17, 18].",
      "startOffset" : 48,
      "endOffset" : 56
    }, {
      "referenceID" : 8,
      "context" : "Although the conventional VC [9] and proposed VC accept non-parallel speech data and partly included parallel data, we used fully parallel data in this evaluation.",
      "startOffset" : 29,
      "endOffset" : 32
    }, {
      "referenceID" : 18,
      "context" : "We prepared speech data of eight speakers taken from the ATR Japanese speech database [19].",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 19,
      "context" : "The 0th–through–24th melcepstral coefcients were used as a spectral parameter and F0 and 5 band-aperiodicity [20, 21] were used as excitation parameters.",
      "startOffset" : 109,
      "endOffset" : 117
    }, {
      "referenceID" : 20,
      "context" : "The 0th–through–24th melcepstral coefcients were used as a spectral parameter and F0 and 5 band-aperiodicity [20, 21] were used as excitation parameters.",
      "startOffset" : 109,
      "endOffset" : 117
    }, {
      "referenceID" : 21,
      "context" : "We used the STRAIGHT analysis-synthesis system [22] for the parameter extraction and waveform synthesis.",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 22,
      "context" : "To improve training accuracy, speech parameter trajectory smoothing [23] with a 50 Hz cutoff modulation frequency was applied to the spectral parameters in the training data.",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 23,
      "context" : "We used AdaGrad [24] as the optimization algorithm, setting the learning rate to 0.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 8,
      "context" : "Both in speech recognition and generation stage, we used bi-directional long short-term memory (LSTM) [9, 10] and each hidden layer contained 256 units.",
      "startOffset" : 102,
      "endOffset" : 109
    }, {
      "referenceID" : 9,
      "context" : "Both in speech recognition and generation stage, we used bi-directional long short-term memory (LSTM) [9, 10] and each hidden layer contained 256 units.",
      "startOffset" : 102,
      "endOffset" : 109
    }, {
      "referenceID" : 24,
      "context" : "The cross-entropy loss was calculated for each group, and the loss function for training the recognition models was the sum of each loss [25].",
      "startOffset" : 137,
      "endOffset" : 141
    }, {
      "referenceID" : 3,
      "context" : "In the proposed method, F0 was linearly transformed [4] first, and we modified its length using DTW between the source context posterior probability sequence and the converted posterior probability sequence.",
      "startOffset" : 52,
      "endOffset" : 55
    }, {
      "referenceID" : 25,
      "context" : "This evaluation uses reference phoneme duration for converting posterior probabilities in order to address the sequence length determination problem to which sequence-to-sequence learning is prone [26].",
      "startOffset" : 197,
      "endOffset" : 201
    }, {
      "referenceID" : 8,
      "context" : "We calculated mel-cepstral distortion between the target and converted speech parameters of the conventional VC [9] and proposed VC.",
      "startOffset" : 112,
      "endOffset" : 115
    }, {
      "referenceID" : 8,
      "context" : "We evaluated the effectiveness of joint training of recognition and synthesis modules, in comparison to conventional separately trained modules [9].",
      "startOffset" : 144,
      "endOffset" : 147
    }, {
      "referenceID" : 8,
      "context" : "To evaluate of the joint training of recognition, conversion, and synthesis, we calculated the mel-cepstral distortion of three systems: (1) conventional VC [9], (2) separately trained modules (equal to ”Proposed” in Section 4.",
      "startOffset" : 157,
      "endOffset" : 160
    } ],
    "year" : 2017,
    "abstractText" : "Voice conversion (VC) using sequence-to-sequence learning of context posterior probabilities is proposed. Conventional VC using shared context posterior probabilities predicts target speech parameters from the context posterior probabilities estimated from the source speech parameters. Although conventional VC can be built from non-parallel data, it is difficult to convert speaker individuality such as phonetic property and speaking rate contained in the posterior probabilities because the source posterior probabilities are directly used for predicting target speech parameters. In this work, we assume that the training data partly include parallel speech data and propose sequence-to-sequence learning between the source and target posterior probabilities. The conversion models perform non-linear and variable-length transformation from the source probability sequence to the target one. Further, we propose a joint training algorithm for the modules. In contrast to conventional VC, which separately trains the speech recognition that estimates posterior probabilities and the speech synthesis that predicts target speech parameters, our proposed method jointly trains these modules along with the proposed probability conversion modules. Experimental results demonstrate that our approach outperforms the conventional VC.",
    "creator" : "LaTeX with hyperref package"
  }
}