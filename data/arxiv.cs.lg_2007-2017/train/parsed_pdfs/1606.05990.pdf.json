{
  "name" : "1606.05990.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A New Training Method for Feedforward Neural Networks Based on Geometric Contraction Property of Activation Functions",
    "authors" : [ "Petre Birtea", "Cosmin Cernăzanu–Glăvan", "Alexandru Şişu" ],
    "emails" : [ "birtea@math.uvt.ro", "cosmin.cernazanu@cs.upt.ro;", "sisu.eugen@gmail.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "MSC: 92B20, 68T05 Keywords: feedforward neural network, training algorithm, backpropagation, contraction, opti-\nmization"
    }, {
      "heading" : "1 Introduction",
      "text" : "From the mathematical point of view a feedforward neural network (FNN) is a sequence of mathematical operations (of type vector multiplication and non-linear activation) able to transform vectors from the input space into vectors from the output space.\nA feed forward neural network with n hidden layers can be seen as a minimization problem for the following cost function:\nE(W1,W2, ...,Wn) := T∑ α=1 ||sn(Wn · ... · s2(W2 · s1(W1 · xα))...)− yα||2, (1.1)\nwhere:\n• Wi represents the weights matrices between the (i− 1)th layer and ith layer\n• the vectorial activation function si : Idi → Jdi is defined by si(x) := (s(x1), ..., s(xdi)), where Idi = I × · · · × I︸ ︷︷ ︸\ndi−times and Jdi = J × · · · × J︸ ︷︷ ︸ di−times with di being the number of neurons on the i th layer,\ns : I ⊆ R→ J ⊆ R is a neuron activation function, and x = (x1, x2, ..., xdi) is the numeric codified neural network input\n• xα is the αth input vector of the training set\n• yα is the αth output vector of the training set\n• T is the number of samples in the training set.\nar X\niv :1\n60 6.\n05 99\n0v 1\n[ cs\n.N E\n] 2\n0 Ju\nThe training of a neural network means to find the weights matrices W1,W2, ...,Wn that render the minimum value of the cost function E. The most common approach is to apply an iterative numerical algorithm that in the end will generate a set of approximated optimal weights matrices (W (opt) 1 , ..,W (opt) n ).\nUsually the activation function s is a non-linear function that increases the amount and the complexity of the computations necessary to solve the above minimization problem.\nThis cost function can be seen as a distance function in the Euclidean space of the weights variables. Hence the minimization of the cost function E can be translated into the problem of minimizing the distance between the target vectors and the output vectors of the neural network.\nFigure 1 presents an intuitive depiction of the linear transformation followed by the non-linear transformation (the activation function) occurring between two consecutive layers. The cost function can be seen at the end of the transformation process.\nSince the development of the backpropagation algorithm (BP) there was a lot of effort in order to address the shortcoming of applying the BP in really large networks. The shortcomings were vanishing gradient problem and the algorithm complexity translated into CPU time. These problems were tackled by improvements that followed several directions that we might classify into: mathematical, structural and algorithmic.\nWhen talking about mathematical improvements we have to mention: Quasi Newton methods [3], [7], [9], [28]; conjugate gradient descent [11], [20]. However, this methods turned out to be expensive from a computational point of view. Methods like Hessian-free optimization [20], [23], [27], [19] address the problem of vanishing gradient in feedforward neural networks. Levenberg-Marquardt [15], [18], [25] improved the convergence speed.\nAnother set of improvements came from modifying the structure of a FNN like in the Dropout method [12], [1], [2] or by modifying the activation function from non-linear to linear like in the case of Rectified Linear Units (ReLU) [17], [21], [16], [8], [13], [4].\nNumeric and algorithmic optimization techniques were also employed when trying to optimize the whole process of running BP: regularization and weight decay [10, 30, 14], momentum [24], [6], [31], Nesterov accelerated gradient descent [22]. Other gradient based optimization methods turned out to be really efficient: RmsProp [29] [26], Adagrad [5], Adadelta [33]."
    }, {
      "heading" : "2 Canceling the non-linearity on the output layer",
      "text" : "Our idea is to modify the initial cost function by taking away the non-linearity of the activation function on the output layer. This leads to reduce the amount of computation for finding the optimal values of the weights matrices that renders a minimum value of the cost function. In order to do this, we need the following hypotheses:\n• the activation function s is a diffeomorphism on its domain of definition and moreover it is a contraction (i.e. the derivative of s is strictly smaller than 1 on the whole interval of definition)\n• the coordinates of output vectors ỹα belong to the co-domain of the activation function s. The output vectors ỹα can be the same with the output vectors yα if the activation function s allows it, see the discussion in next section.\nMore precisely, we propose the following modified cost function:\nC̃(W̃1,W̃2, ...,W̃n) := T∑ 1 ||W̃n · sn−1(W̃n−1 · ... · s2(W̃2 · s1(W̃1 · xα))...)− s−1n (ỹα)||2, (2.1)\nwhere:\n• s−1n : Jdn → Idn is the inverse of the vectorial activation function sn\n• W̃i represents the weights matrices between the (i− 1)th layer and ith layer\n• ỹα is the αth output vector of the training set.\nWe propose to train the neural network in the same manner, but using the newly introduced cost\nfunction C̃. At the end, we will obtain a set of approximated optimal values (W̃ (opt) 1 , ...,W̃ (opt) n ) for the variables that we are interested, in our case the weights matrices.\nWe will show that introducing the computed weights (W̃ (opt) 1 , ...,W̃ (opt) n ) in the typical feedforward\nerror function (1.1)\nẼ(W̃1,W̃2, ...,W̃n) := T∑ 1 ||sn(W̃n · ... · s2(W̃2 · s1(W̃1 · xα))...)− ỹα||2,\nwhere the new output vectors are ỹα, will yield even better values. Indeed, due to the fact that vectorial function sn is a contraction, this implies that it will contract the distances. Expressed in a mathematical form we have:\nC̃(W̃ (opt) 1 , ..,W̃ (opt) n ) > Ẽ(W̃ (opt) 1 , ..,W̃ (opt) n ).\nStarting from an initial set of conditions and applying the standard training procedure (using the\nclassical cost function E) we obtain a set of weight matrices (W (opt) 1 , ..,W (opt) n ), which in general\nare different than the weights matrices (W̃ (opt) 1 , ..,W̃ (opt) n ) obtained by training the network using the modified cost function C̃. A direct comparison between the two training methods, the classical one using the cost function E and the newly proposed one using the cost function C̃ cannot be performed, as in general the inequality\nE(W (opt) 1 , ..,W (opt) n ) > Ẽ(W̃ (opt) 1 , ..,W̃ (opt) n ), (2.2)\ncannot be proved mathematically. Experimentally, when we take as a criteria of performance the number of rightfully classified examples (accuracy measure), we prove that our newly proposed training method yields better classification results than the classical one. Also, the experiments show that as a byproduct of this method we obtain also a faster classification for the chosen datasets.\nThe comparison and the implementation procedure of the classical training method and the newly proposed method follows the logical diagram depicted in Figure 3. Thus, we experimentally compare C1 (the accuracy percentage for the classical method) with C2 (the accuracy percentage for the newly proposed method). The newly proposed method needs a supplementary preprocessing step where we need to define the new output vectors ỹα. This intermediate step is necessary only when the coordinates of the output vectors yα in the classical method does not belong to the co-domain J of the activation function s. The rest of the steps remain the same with the difference that we minimize a different cost function C̃, defined in (2.1).\nExperimentally, we observe that after Step 3 we have a faster learning (lower value of the corresponding cost functions) using the new method. This advantage is reflected at the end of the Step 4, where one compares the actual accuracies (C1 and C2) obtained by the two methods."
    }, {
      "heading" : "3 Experiments",
      "text" : "We have done three experiments, in all three we have used the MNIST dataset [32]. This is the most common dataset (actually is a subset of a larger available set - NIST) used in this field and it contains a large set of images representing handwrite digits. The training set contains 60.000 examples and\nthe testing set 10.000 examples. Each experiment consists of training a neural network using the two different cost functions, the classical cost function E and respectively, the newly proposed cost function C̃. In all three experiments we have chosen neural network configurations already used in literature, see [32]. During the three experiments we have obtained a faster learning and also a better classification\nerror using the newly proposed cost function C̃. For the first experiment we consider a feedforward neural network having the following configuration: the input layer contains 784 neurons, followed by two hidden layers with 300, respectively 100 neurons, and an output layer containing 10 neurons (one neuron for each digit).\nFor the classical training of the first experiment all neurons are classical neurons having as activation function the sigmoid, s : R → (0, 1) , s(x) = (1 + e−x)−1. Each target is a vector yα ∈ R10 with components 0 or 1.\nFor the second training of the first experiment, we start by verifying that the sigmoid function obeys the first hypothesis discussed in the previous section, meaning it is a contracting diffeomorphism on its domain of definition.\nIn order to implement our method, the second hypothesis needs to be verified. We need to apply the inverse sigmoid on each component of the target vector yα. As ”s\n−1(1) =∞” and ”s−1(0) = −∞” one must replace the coordinate entries 0 and 1 in yα with other two reference values belonging to (0, 1). To find a set of suitable values, a series of experiments were conducted and we obtain the best results (from the training point of view) when replacing 0 with 0.2227 and 1 with 0.7773. Thus, the coordinate entries of the new output vectors ỹα are 0.2227 and 0.7773. Following the logical diagram depicted in Figure 3, we implement our method accordingly:\n• Pre-processing step: replace the vector components 0 and 1 of the target vectors yα with 0.2227, respectively 0.7773, thus obtaining the new output vectors ỹα. Compute the vectors s −1 n (ỹα) that\nare used for defining the cost function C̃.\n• Step 1: apply the gradient descent algorithm for the new cost function C̃.\n• Step 2: after a sufficient number of iterations we obtain the weights matrices W̃(opt)1 ,W̃ (opt) 2 ,W̃ (opt) 3 ,\nwhere W̃ (opt) 1 is the weights matrix of 784 × 300 elements between the input layer and the first hidden layer, W̃ (opt) 2 is the weights matrix of 300 × 100 elements between the first hidden layer and the second hidden layer and W̃ (opt) 3 is the weights matrix of 300 × 10 elements between the second hidden layer and the output layer.\n• Step 3: compute the value Ẽ(W̃(opt)1 ,W̃ (opt) 2 ,W̃ (opt) 3 ) and compare with E(W (opt) 1 , ..,W (opt) n )\nobtained using the classical training, see Figure 4.\n• Step 4: compute the accuracy value C2 obtained with the newly proposed method and compare with accuracy value C1 obtained using the classical training, see Figure 5. For this configuration, LeCun et al. [32] report a classification error of 3.05, value which we have obtained after 47.750 epochs with classical training. After the same number of epochs, using the newly proposed method, we have obtain a classification error of 2.59.\nFor the second experiment we have changed the architecture of the neural network in the following way: the input layer contains 784 neurons, followed by two hidden layers with 500, respectively 150 neurons, and an output layer containing 10 neurons (one neuron for each digit). Both classical and the newly proposed training had followed the same procedure as in the first experiment, which we have described in details. The comparison between the two trainings of the second experiment can be seen in Figure 6 and Figure 7. For this configuration, LeCun et al. [32] report a classification error of 2.95, value which we have obtained after 77.800 epochs with classical training. After the same number of epochs, using the newly proposed method, we have obtained a classification error of 2.00.\nIn the third experiment we have considered an architecture of the neural network with a single hidden layer of 1000 neurons. Thus, the architecture is as follows: the input layer contains 784 neurons, followed by one hidden layer with 1000 neurons, and an output layer containing 10 neurons (one neuron for each digit). Both classical and the newly proposed training had followed the same procedure as in the first two experiments. The comparison between the two trainings of the third experiment can be seen in Figure 8 and Figure 9. For this configuration, LeCun et al. [32] report a classification error of 4.5, value which we have obtained after 77.050 epochs with classical training. After the same number of epochs, using the newly proposed method, we have obtained a classification error of 2.79."
    }, {
      "heading" : "4 Conclusions",
      "text" : "We have presented a new method for training a feedforward neural network. The core of the method relies on the contraction property of some activation functions (e.g. sigmoid function) and the geometry underlying the training of FNN. As a result, we have obtained a new cost function that needs to be minimized during the training process. The main advantage of the new functional resides in the fact that we have less non-linearities introduced by activation functions.\nThe experiments that we have conducted show that our method leads to a faster learning convergence and also to a better recognition rate (classification error). In the first experiment, after 47.750 epochs, the classical method gives a classification error of 3.05 versus our method, which renders a classification error of 2.59. This behaviour remains valid for the next two experiments where we classification error for classical method was 2.97 versus 2.00 obtained with the new method, respectively 4.5 classification error obtained with classical method versus 2.79 the classification error obtained with the new method."
    } ],
    "references" : [ {
      "title" : "Adaptive dropout for training deep neural networks",
      "author" : [ "J. Ba", "B. Frey" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2013
    }, {
      "title" : "The dropout learning algorithm",
      "author" : [ "P. Baldi", "P. Sadowski" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2014
    }, {
      "title" : "A class of methods for solving nonlinear simultaneous equations",
      "author" : [ "Broyden", "C. G" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1965
    }, {
      "title" : "Improving deep neural networks for LVCSR using rectified linear units and dropout",
      "author" : [ "G.E. Dahl", "T.N. Sainath", "G.E. Hinton" ],
      "venue" : "In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2013
    }, {
      "title" : "Adaptive subgradient methods for online learning and stochastic optimization",
      "author" : [ "J. Duchi", "E. Hazan", "Y. Singer" ],
      "venue" : "The Journal of Machine Learning,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2011
    }, {
      "title" : "An empirical study of learning speed in back-propagation networks",
      "author" : [ "S.E. Fahlman" ],
      "venue" : "Technical Report CMU-CS-88-162, Carnegie-Mellon Univ",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1988
    }, {
      "title" : "A rapidly convergent descent method for minimization",
      "author" : [ "R. Fletcher", "M.J. Powell" ],
      "venue" : "The Computer Journal,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1963
    }, {
      "title" : "Deep sparse rectifier networks",
      "author" : [ "X. Glorot", "A. Bordes", "Y. Bengio" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2011
    }, {
      "title" : "A family of variable-metric methods derived by variational means",
      "author" : [ "D. Goldfarb" ],
      "venue" : "Mathematics of computation,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1970
    }, {
      "title" : "Comparing biases for minimal network construction with back-propagation",
      "author" : [ "S.J. Hanson", "L.Y. Pratt" ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS)",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1989
    }, {
      "title" : "Methods of conjugate gradients for solving linear systems",
      "author" : [ "M.R. Hestenes", "E. Stiefel" ],
      "venue" : "Journal of research of the National Bureau of Standards,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1952
    }, {
      "title" : "Improving neural networks by preventing co-adaptation of feature detectors",
      "author" : [ "G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R.R. Salakhutdinov" ],
      "venue" : "Technical Report arXiv:1207.0580",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2012
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "A. Krizhevsky", "I. Sutskever", "G.E. Hinton" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2012
    }, {
      "title" : "A simple weight decay can improve generalization",
      "author" : [ "A. Krogh", "J.A. Hertz" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1992
    }, {
      "title" : "A method for the solution of certain problems in least squares",
      "author" : [ "K. Levenberg" ],
      "venue" : "Quarterly of applied mathematics,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 1944
    }, {
      "title" : "Rectifier nonlinearities improve neural network acoustic models",
      "author" : [ "A.L. Maas", "A.Y. Hannun", "A.Y. Ng" ],
      "venue" : "In International Conference on Machine Learning (ICML)",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2013
    }, {
      "title" : "Preattentive texture discrimination with early vision mechanisms",
      "author" : [ "J. Malik", "P. Perona" ],
      "venue" : "Journal of the Optical Society of America",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 1990
    }, {
      "title" : "An algorithm for least-squares estimation of nonlinear parameters",
      "author" : [ "D.W. Marquardt" ],
      "venue" : "Journal of the Society for Industrial & Applied Mathematics,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 1963
    }, {
      "title" : "Deep learning via Hessian-free optimization",
      "author" : [ "J. Martens" ],
      "venue" : "Proceedings of the 27th International Conference on Machine Learning",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2010
    }, {
      "title" : "Exact calculation of the product of the Hessian matrix of feed-forward network error functions and a vector in O(N) time",
      "author" : [ "M.F. Møller" ],
      "venue" : "Technical Report PB-432,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 1993
    }, {
      "title" : "Rectified linear units improve restricted Boltzmann machines",
      "author" : [ "V. Nair", "G.E. Hinton" ],
      "venue" : "In International Conference on Machine Learning (ICML)",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2010
    }, {
      "title" : "A method of solving a convex programming problem with convergence rate of 1k2",
      "author" : [ "Y. Nesterov" ],
      "venue" : "Soviet Mathematics Doklady,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 1983
    }, {
      "title" : "Fast exact multiplication by the Hessian",
      "author" : [ "B.A. Pearlmutter" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 1994
    }, {
      "title" : "Learning internal representations by error propagation",
      "author" : [ "D.E. Rumelhart", "G.E. Hinton", "R.J. Williams" ],
      "venue" : "Parallel Distributed Processing,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 1986
    }, {
      "title" : "No more pesky learning rates",
      "author" : [ "T. Schaul", "S. Zhang", "Y. LeCun" ],
      "venue" : "In Proc. 30th International Conference on Machine Learning (ICML)",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2013
    }, {
      "title" : "Conjugate directions for stochastic gradient descent",
      "author" : [ "N.N. Schraudolph", "T. Graepel" ],
      "venue" : "Proc. Intl. Conf. Artificial Neural Networks (ICANN),",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2002
    }, {
      "title" : "Conditioning of quasi-Newton methods for function minimization",
      "author" : [ "D.F. Shanno" ],
      "venue" : "Mathematics of computation,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 1970
    }, {
      "title" : "Lecture 6.5—RmsProp: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural Networks for Machine Learning",
      "author" : [ "T. Tieleman", "G. Hinton" ],
      "venue" : null,
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2012
    }, {
      "title" : "Generalization by weightelimination with application to forecasting",
      "author" : [ "A.S. Weigend", "D.E. Rumelhart", "B.A. Huberman" ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS)",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 1991
    }, {
      "title" : "Adaptive back-propagation in on-line learning of multilayer networks",
      "author" : [ "A.H.L. West", "D. Saad" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 1995
    }, {
      "title" : "Gradient-Based Learning Applied to Document Recognition",
      "author" : [ "Y. LeCun", "Y.B.L. Bottou", "P. Haffner" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 1998
    }, {
      "title" : "ADADELTA: An Adaptive Learning Rate Method",
      "author" : [ "M.D. Zeiler" ],
      "venue" : "CoRR, abs/1212.5701",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "When talking about mathematical improvements we have to mention: Quasi Newton methods [3], [7], [9], [28]; conjugate gradient descent [11], [20].",
      "startOffset" : 86,
      "endOffset" : 89
    }, {
      "referenceID" : 6,
      "context" : "When talking about mathematical improvements we have to mention: Quasi Newton methods [3], [7], [9], [28]; conjugate gradient descent [11], [20].",
      "startOffset" : 91,
      "endOffset" : 94
    }, {
      "referenceID" : 8,
      "context" : "When talking about mathematical improvements we have to mention: Quasi Newton methods [3], [7], [9], [28]; conjugate gradient descent [11], [20].",
      "startOffset" : 96,
      "endOffset" : 99
    }, {
      "referenceID" : 26,
      "context" : "When talking about mathematical improvements we have to mention: Quasi Newton methods [3], [7], [9], [28]; conjugate gradient descent [11], [20].",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 10,
      "context" : "When talking about mathematical improvements we have to mention: Quasi Newton methods [3], [7], [9], [28]; conjugate gradient descent [11], [20].",
      "startOffset" : 134,
      "endOffset" : 138
    }, {
      "referenceID" : 19,
      "context" : "When talking about mathematical improvements we have to mention: Quasi Newton methods [3], [7], [9], [28]; conjugate gradient descent [11], [20].",
      "startOffset" : 140,
      "endOffset" : 144
    }, {
      "referenceID" : 19,
      "context" : "Methods like Hessian-free optimization [20], [23], [27], [19] address the problem of vanishing gradient in feedforward neural networks.",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 22,
      "context" : "Methods like Hessian-free optimization [20], [23], [27], [19] address the problem of vanishing gradient in feedforward neural networks.",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 25,
      "context" : "Methods like Hessian-free optimization [20], [23], [27], [19] address the problem of vanishing gradient in feedforward neural networks.",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 18,
      "context" : "Methods like Hessian-free optimization [20], [23], [27], [19] address the problem of vanishing gradient in feedforward neural networks.",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 14,
      "context" : "Levenberg-Marquardt [15], [18], [25] improved the convergence speed.",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 17,
      "context" : "Levenberg-Marquardt [15], [18], [25] improved the convergence speed.",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 11,
      "context" : "Another set of improvements came from modifying the structure of a FNN like in the Dropout method [12], [1], [2] or by modifying the activation function from non-linear to linear like in the case of Rectified Linear Units (ReLU) [17], [21], [16], [8], [13], [4].",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 0,
      "context" : "Another set of improvements came from modifying the structure of a FNN like in the Dropout method [12], [1], [2] or by modifying the activation function from non-linear to linear like in the case of Rectified Linear Units (ReLU) [17], [21], [16], [8], [13], [4].",
      "startOffset" : 104,
      "endOffset" : 107
    }, {
      "referenceID" : 1,
      "context" : "Another set of improvements came from modifying the structure of a FNN like in the Dropout method [12], [1], [2] or by modifying the activation function from non-linear to linear like in the case of Rectified Linear Units (ReLU) [17], [21], [16], [8], [13], [4].",
      "startOffset" : 109,
      "endOffset" : 112
    }, {
      "referenceID" : 16,
      "context" : "Another set of improvements came from modifying the structure of a FNN like in the Dropout method [12], [1], [2] or by modifying the activation function from non-linear to linear like in the case of Rectified Linear Units (ReLU) [17], [21], [16], [8], [13], [4].",
      "startOffset" : 229,
      "endOffset" : 233
    }, {
      "referenceID" : 20,
      "context" : "Another set of improvements came from modifying the structure of a FNN like in the Dropout method [12], [1], [2] or by modifying the activation function from non-linear to linear like in the case of Rectified Linear Units (ReLU) [17], [21], [16], [8], [13], [4].",
      "startOffset" : 235,
      "endOffset" : 239
    }, {
      "referenceID" : 15,
      "context" : "Another set of improvements came from modifying the structure of a FNN like in the Dropout method [12], [1], [2] or by modifying the activation function from non-linear to linear like in the case of Rectified Linear Units (ReLU) [17], [21], [16], [8], [13], [4].",
      "startOffset" : 241,
      "endOffset" : 245
    }, {
      "referenceID" : 7,
      "context" : "Another set of improvements came from modifying the structure of a FNN like in the Dropout method [12], [1], [2] or by modifying the activation function from non-linear to linear like in the case of Rectified Linear Units (ReLU) [17], [21], [16], [8], [13], [4].",
      "startOffset" : 247,
      "endOffset" : 250
    }, {
      "referenceID" : 12,
      "context" : "Another set of improvements came from modifying the structure of a FNN like in the Dropout method [12], [1], [2] or by modifying the activation function from non-linear to linear like in the case of Rectified Linear Units (ReLU) [17], [21], [16], [8], [13], [4].",
      "startOffset" : 252,
      "endOffset" : 256
    }, {
      "referenceID" : 3,
      "context" : "Another set of improvements came from modifying the structure of a FNN like in the Dropout method [12], [1], [2] or by modifying the activation function from non-linear to linear like in the case of Rectified Linear Units (ReLU) [17], [21], [16], [8], [13], [4].",
      "startOffset" : 258,
      "endOffset" : 261
    }, {
      "referenceID" : 9,
      "context" : "Numeric and algorithmic optimization techniques were also employed when trying to optimize the whole process of running BP: regularization and weight decay [10, 30, 14], momentum [24], [6], [31], Nesterov accelerated gradient descent [22].",
      "startOffset" : 156,
      "endOffset" : 168
    }, {
      "referenceID" : 28,
      "context" : "Numeric and algorithmic optimization techniques were also employed when trying to optimize the whole process of running BP: regularization and weight decay [10, 30, 14], momentum [24], [6], [31], Nesterov accelerated gradient descent [22].",
      "startOffset" : 156,
      "endOffset" : 168
    }, {
      "referenceID" : 13,
      "context" : "Numeric and algorithmic optimization techniques were also employed when trying to optimize the whole process of running BP: regularization and weight decay [10, 30, 14], momentum [24], [6], [31], Nesterov accelerated gradient descent [22].",
      "startOffset" : 156,
      "endOffset" : 168
    }, {
      "referenceID" : 23,
      "context" : "Numeric and algorithmic optimization techniques were also employed when trying to optimize the whole process of running BP: regularization and weight decay [10, 30, 14], momentum [24], [6], [31], Nesterov accelerated gradient descent [22].",
      "startOffset" : 179,
      "endOffset" : 183
    }, {
      "referenceID" : 5,
      "context" : "Numeric and algorithmic optimization techniques were also employed when trying to optimize the whole process of running BP: regularization and weight decay [10, 30, 14], momentum [24], [6], [31], Nesterov accelerated gradient descent [22].",
      "startOffset" : 185,
      "endOffset" : 188
    }, {
      "referenceID" : 29,
      "context" : "Numeric and algorithmic optimization techniques were also employed when trying to optimize the whole process of running BP: regularization and weight decay [10, 30, 14], momentum [24], [6], [31], Nesterov accelerated gradient descent [22].",
      "startOffset" : 190,
      "endOffset" : 194
    }, {
      "referenceID" : 21,
      "context" : "Numeric and algorithmic optimization techniques were also employed when trying to optimize the whole process of running BP: regularization and weight decay [10, 30, 14], momentum [24], [6], [31], Nesterov accelerated gradient descent [22].",
      "startOffset" : 234,
      "endOffset" : 238
    }, {
      "referenceID" : 27,
      "context" : "Other gradient based optimization methods turned out to be really efficient: RmsProp [29] [26], Adagrad [5], Adadelta [33].",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 24,
      "context" : "Other gradient based optimization methods turned out to be really efficient: RmsProp [29] [26], Adagrad [5], Adadelta [33].",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 4,
      "context" : "Other gradient based optimization methods turned out to be really efficient: RmsProp [29] [26], Adagrad [5], Adadelta [33].",
      "startOffset" : 104,
      "endOffset" : 107
    }, {
      "referenceID" : 31,
      "context" : "Other gradient based optimization methods turned out to be really efficient: RmsProp [29] [26], Adagrad [5], Adadelta [33].",
      "startOffset" : 118,
      "endOffset" : 122
    }, {
      "referenceID" : 30,
      "context" : "We have done three experiments, in all three we have used the MNIST dataset [32].",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 30,
      "context" : "In all three experiments we have chosen neural network configurations already used in literature, see [32].",
      "startOffset" : 102,
      "endOffset" : 106
    }, {
      "referenceID" : 30,
      "context" : "[32] report a classification error of 3.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 30,
      "context" : "[32] report a classification error of 2.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 30,
      "context" : "[32] report a classification error of 4.",
      "startOffset" : 0,
      "endOffset" : 4
    } ],
    "year" : 2016,
    "abstractText" : "We propose a new training method for a feedforward neural network having the activation functions with the geometric contraction property. The method consists of constructing a new functional that is less nonlinear in comparison with the classical functional by removing the nonlinearity of the activation functions from the output layer. We validate this new method by a series of experiments that show an improved learning speed and also a better classification error. MSC: 92B20, 68T05",
    "creator" : "LaTeX with hyperref package"
  }
}