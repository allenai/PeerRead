{
  "name" : "1301.5348.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Why Size Matters: Feature Coding as Nyström Sampling",
    "authors" : [ "Oriol Vinyals", "Yangqing Jia" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Why Size Matters: Feature Coding as Nyström Sampling\nOriol Vinyals UC Berkeley Berkeley, CA\nYangqing Jia UC Berkeley Berkeley, CA\nTrevor Darrell UC Berkeley Berkeley, CA"
    }, {
      "heading" : "1 Introduction",
      "text" : "Recently, the computer vision and machine learning community has been in favor of feature extraction pipelines that rely on a coding step followed by a linear classifier, due to their overall simplicity, well understood properties of linear classifiers, and their computational efficiency. In this paper we propose a novel view of this pipeline based on kernel methods and Nyström sampling. In particular, we focus on the coding of a data point with a local representation based on a dictionary with fewer elements than the number of data points, and view it as an approximation to the actual function that would compute pair-wise similarity to all data points (often too many to compute in practice), followed by a Nyström sampling step to select a subset of all data points.\nFurthermore, since bounds are known on the approximation power of Nyström sampling as a function of how many samples (i.e. dictionary size) we consider, we can derive bounds on the approximation of the exact (but expensive to compute) kernel matrix, and use it as a proxy to predict accuracy as a function of the dictionary size, which has been observed to increase but also to saturate as we increase its size. This model may help explaining the positive effect of the codebook size [2, 7] and justifying the need to stack more layers (often referred to as deep learning), as flat models empirically saturate as we add more complexity."
    }, {
      "heading" : "2 The Nyström View",
      "text" : "We specifically consider forming a dictionary by sampling our training set. To encode a new sample x ∈ Rd, we apply a (generally non-linear) coding function c so that c(x) ∈ Rc. Note that d is the dimensionality of the original feature space, while c is the dictionary size. The standard classification pipeline considers c(x) as the new feature space, and typically uses a linear classifier on this space. For example, one may use the threshold encoding function [2] as an example: c(x) = max(0,x>D − α) where D ∈ Rd×c is the dictionary. Note that our discussion on coding is valid for many different feed-forward coding schemes.\nIn the ideal case (infinite computation and memory), we encode each sample x using the whole training set X ∈ Rd×N , which can be seen as the best local coding of the training set X (as long as over-fitting is handled by the classification algorithm). In general, larger dictionary sizes yield better performance assuming the linear classifier is well regularized, as it can be seen as a way to do manifold learning [6]. We define the new coded feature space as C = max(0,X>X − α), where the i-th row of C corresponds to coding the i-th sample c(xi). The linear kernel function between samples i and j is k(xi,xj) = c(xi)>c(xj). The kernel matrix is then K = CC>. Naively applying Nyström sampling to the matrix K does not save any computation, as every column of K requires computing an inner product with N samples. However, if we decompose the matrix C with Nyström sampling (i.e., with a subsampled dictionary) we obtain C′ ≈ C, and as a consequence K′ ≈ K:\nC′ = EW−1E>, K′ = C′C′> = EW−1E>EW−1E> = EΛE>\nar X\niv :1\n30 1.\n53 48\nv2 [\ncs .L\nG ]\n1 6\nA pr\n2 01\n3\nwhere the first equation comes from applying Nyström sampling to C, E is a random subsample of the columns of C, and W the corresponding square matrix with the same random subsample of both columns and rows of C."
    }, {
      "heading" : "3 Main Results on Approximation Bounds",
      "text" : "More interestingly, many bounds on the error made in estimating C by C′ exist, and finding better sampling schemes that improve such bounds is an active topic in the machine learning community (see e.g. [4]). The bound we start with is [4]:\n||C−C′||F ≤ ||C−Ck||F + max(nCii) (1) valid if c ≥ 64k/ 4 (c is the number of columns that we sample from C to form E, i.e. the codebook size), where k is the sufficient rank to estimate the structure of C, and Ck is the optimal rank k approximation (given by Singular Value Decomposition (SVD), which we cannot compute in practice). Note that, if we assume that our training set can be explained by a manifold of dimension k (i.e. the first term in the right hand side of eq. 1 vanishes), then the error is proportional to times a constant (that is dataset dependent).\nThus, if we fix k to the value that retains enough energy from C, we get a bound that for every c (dimension of code), gives a minimum to plug in equation 1. This gives us a useful bound of the form ≥Mc− 14 for some constant M (that depends on k). Putting it all together, we get:\n||C−C′||F ≤ O +Mc− 1 4\nwith O and M constants that are dataset specific.\nHaving bounded the error C is not sufficient to establish how the code size will affect the classifier performance. In particular, it is not clear how the error on C affect the error on the kernel matrix K. However, we are able to prove that the error bound on K′ is in the same format as that on C:\n||K−K′||F ≤ O +Mc− 1 4 (2)\nEven though we are not aware of an easy way to formally link degradation in Frobenius norm of our approximation K′ to K to classification accuracy, the bound above is informative as one may reasonably expect kernel matrices of different quality to have classification performances in the same trend."
    }, {
      "heading" : "4 Experiments",
      "text" : "We empirically evaluate the bound on the kernel matrix, used as a proxy to model classification accuracy, which is the measure of interest. To estimate the constants in the bound, we do interpolation of the observed accuracy in the first two samples of accuracy versus codebook size, which is of practical interest: one may want to quickly run a new dataset through the pipeline with small codebook sizes, and then quickly estimate what the accuracy would be when running a full experiment with a much larger dictionary size.\nFigure 1 shows the results on on the CIFAR-10 image classification and TIMIT speech recognition datasets respectively. It is observed that the derived model closely follow our own empirical observations, with red dashed line serving as a lower bound of the actual accuracy and following the\nshape of the empirical accuracy, predicting its saturation. The model is never too tight though, due to various factors of our approximation, e.g., the analytical relationship between the approximation of K and the classification accuracy is not clear.\nThe Nyström view of feature encoding and the approximation bounds we proposed helps understanding several key observations in the recent literature: (1) the linear classifier performance is always bounded when using a fixed codebook, and performance increases when the codebook grows [2], even with a huge codebook [7], and (2) simple dictionary learning techniques have been found efficient in some classification pipelines [1, 5], and K-means works particularly well as a dictionary learning algorithm albeit its simplicity, a phenomenon that is common in the Nyström sampling context [4].\nIn addition, in many image classification tasks the feature extraction pipeline is composed of more than feature encoding. For example, recent state-of-the-art methods pool locally encoded features spatially to form the final feature vector. The Nyström view presented in the paper inspires us to employ findings in the machine learning field to learn better, pooling-aware dictionaries. In one of our related work [3], we form a dictionary by first “overshooting” the coding stage with a larger dictionary, and then pruning it using K-centers with pooled features. Figure 2 shows an increase in the final classification accuracy compared with the baseline that only learns the dictionary on the patch-level, with no additional computation cost for either feature extraction or classification."
    } ],
    "references" : [ {
      "title" : "An analysis of single-layer networks in unsupervised feature learning",
      "author" : [ "A Coates", "H Lee", "AY Ng" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2011
    }, {
      "title" : "The importance of encoding versus training with sparse coding and vector quantization",
      "author" : [ "A Coates", "AY Ng" ],
      "venue" : "In ICML,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2011
    }, {
      "title" : "Pooling-invariant image feature learning",
      "author" : [ "Y Jia", "O Vinyals", "T Darrell" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2013
    }, {
      "title" : "Sampling methods for the nyström method",
      "author" : [ "S Kumar", "M Mohri", "A Talwalkar" ],
      "venue" : "JMLR, 13(4):981–1006,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2012
    }, {
      "title" : "On random weights and unsupervised feature learning",
      "author" : [ "A Saxe", "PW Koh", "Z Chen", "M Bhand", "B Suresh", "AY Ng" ],
      "venue" : "In ICML,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2011
    }, {
      "title" : "Locality-constrained linear coding for image classification",
      "author" : [ "J Wang", "J Yang", "K Yu", "F Lv", "T Huang", "Y Gong" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2010
    }, {
      "title" : "Efficient highly over-complete sparse coding using a mixture model",
      "author" : [ "J Yang", "K Yu", "T Huang" ],
      "venue" : "In ECCV,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "This model may help explaining the positive effect of the codebook size [2, 7] and justifying the need to stack more layers (often referred to as deep learning), as flat models empirically saturate as we add more complexity.",
      "startOffset" : 72,
      "endOffset" : 78
    }, {
      "referenceID" : 6,
      "context" : "This model may help explaining the positive effect of the codebook size [2, 7] and justifying the need to stack more layers (often referred to as deep learning), as flat models empirically saturate as we add more complexity.",
      "startOffset" : 72,
      "endOffset" : 78
    }, {
      "referenceID" : 1,
      "context" : "For example, one may use the threshold encoding function [2] as an example: c(x) = max(0,x>D − α) where D ∈ Rd×c is the dictionary.",
      "startOffset" : 57,
      "endOffset" : 60
    }, {
      "referenceID" : 5,
      "context" : "In general, larger dictionary sizes yield better performance assuming the linear classifier is well regularized, as it can be seen as a way to do manifold learning [6].",
      "startOffset" : 164,
      "endOffset" : 167
    }, {
      "referenceID" : 3,
      "context" : "[4]).",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 3,
      "context" : "The bound we start with is [4]:",
      "startOffset" : 27,
      "endOffset" : 30
    }, {
      "referenceID" : 2,
      "context" : "We refer to our tech report [3] for more details.",
      "startOffset" : 28,
      "endOffset" : 31
    }, {
      "referenceID" : 1,
      "context" : "The Nyström view of feature encoding and the approximation bounds we proposed helps understanding several key observations in the recent literature: (1) the linear classifier performance is always bounded when using a fixed codebook, and performance increases when the codebook grows [2], even with a huge codebook [7], and (2) simple dictionary learning techniques have been found efficient in some classification pipelines [1, 5], and K-means works particularly well as a dictionary learning algorithm albeit its simplicity, a phenomenon that is common in the Nyström sampling context [4].",
      "startOffset" : 284,
      "endOffset" : 287
    }, {
      "referenceID" : 6,
      "context" : "The Nyström view of feature encoding and the approximation bounds we proposed helps understanding several key observations in the recent literature: (1) the linear classifier performance is always bounded when using a fixed codebook, and performance increases when the codebook grows [2], even with a huge codebook [7], and (2) simple dictionary learning techniques have been found efficient in some classification pipelines [1, 5], and K-means works particularly well as a dictionary learning algorithm albeit its simplicity, a phenomenon that is common in the Nyström sampling context [4].",
      "startOffset" : 315,
      "endOffset" : 318
    }, {
      "referenceID" : 0,
      "context" : "The Nyström view of feature encoding and the approximation bounds we proposed helps understanding several key observations in the recent literature: (1) the linear classifier performance is always bounded when using a fixed codebook, and performance increases when the codebook grows [2], even with a huge codebook [7], and (2) simple dictionary learning techniques have been found efficient in some classification pipelines [1, 5], and K-means works particularly well as a dictionary learning algorithm albeit its simplicity, a phenomenon that is common in the Nyström sampling context [4].",
      "startOffset" : 425,
      "endOffset" : 431
    }, {
      "referenceID" : 4,
      "context" : "The Nyström view of feature encoding and the approximation bounds we proposed helps understanding several key observations in the recent literature: (1) the linear classifier performance is always bounded when using a fixed codebook, and performance increases when the codebook grows [2], even with a huge codebook [7], and (2) simple dictionary learning techniques have been found efficient in some classification pipelines [1, 5], and K-means works particularly well as a dictionary learning algorithm albeit its simplicity, a phenomenon that is common in the Nyström sampling context [4].",
      "startOffset" : 425,
      "endOffset" : 431
    }, {
      "referenceID" : 3,
      "context" : "The Nyström view of feature encoding and the approximation bounds we proposed helps understanding several key observations in the recent literature: (1) the linear classifier performance is always bounded when using a fixed codebook, and performance increases when the codebook grows [2], even with a huge codebook [7], and (2) simple dictionary learning techniques have been found efficient in some classification pipelines [1, 5], and K-means works particularly well as a dictionary learning algorithm albeit its simplicity, a phenomenon that is common in the Nyström sampling context [4].",
      "startOffset" : 587,
      "endOffset" : 590
    }, {
      "referenceID" : 2,
      "context" : "In one of our related work [3], we form a dictionary by first “overshooting” the coding stage with a larger dictionary, and then pruning it using K-centers with pooled features.",
      "startOffset" : 27,
      "endOffset" : 30
    } ],
    "year" : 2013,
    "abstractText" : "Recently, the computer vision and machine learning community has been in favor of feature extraction pipelines that rely on a coding step followed by a linear classifier, due to their overall simplicity, well understood properties of linear classifiers, and their computational efficiency. In this paper we propose a novel view of this pipeline based on kernel methods and Nyström sampling. In particular, we focus on the coding of a data point with a local representation based on a dictionary with fewer elements than the number of data points, and view it as an approximation to the actual function that would compute pair-wise similarity to all data points (often too many to compute in practice), followed by a Nyström sampling step to select a subset of all data points.",
    "creator" : "TeX"
  }
}