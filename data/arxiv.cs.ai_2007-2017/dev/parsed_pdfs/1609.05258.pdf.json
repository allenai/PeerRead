{
  "name" : "1609.05258.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "The ACRV Picking Benchmark: A Robotic Shelf Picking Benchmark to Foster Reproducible Research",
    "authors" : [ "Jürgen Leitner", "Adam W. Tow", "Niko Sünderhauf", "Jake E. Dean", "Joseph W. Durham", "Matthew Cooper", "Markus Eich", "Christopher Lehnert", "Ruben Mangels", "Christopher McCool", "Peter T. Kujala", "Lachlan Nicholson", "Trung Pham", "James Sergeant", "Fangyi Zhang", "Ben Upcroft", "Peter Corke" ],
    "emails" : [ "j.leitner@roboticvision.org" ],
    "sections" : [ {
      "heading" : null,
      "text" : "I. INTRODUCTION\nRobotic picking of randomly presented objects is an application with considerable economic potential in applications such as e-shopping and logistics. Despite a long history of research into dexterous manipulation [1], much of the observable recent progress has been driven by technological developments such as quality low-cost arms (Rethink’s Baxter, Kinova’s MICO and JACO arms, etc.) and simple compliant grippers based on coffee grounds [2] or vacuum grippers [3], [4], [5]. This in turn has revealed the deficiencies in state-ofthe-art hand-eye coordination and robust perception in realworld environments.\nOver more than a decade the robotics community has embraced challenges as a means to drive progress – from self-driving cars, to humanoids, to robotic picking. The methodology of using standard problems is powerful but the challenge events occur only occasionally and the test conditions are difficult to replicate outside the event. Alongside challenges there is growing interest in reproducible research [6] which is relevant and related.\nIn parallel to activity within the robotics community, the vision research community has made enormous progress in\nThis research was supported by the Australian Research Council Centre of Excellence for Robotic Vision (ACRV) (project number CE140100016). The participation at the APC was supported by Amazon Robotics LLC.\n1Authors are with the Australian Centre for Robotic Vision (ACRV) 2Authors are with the Queensland University of Technology (QUT), Brisbane, QLD 4001 Australia. 3JWD is with Amazon Robotics LLC, North Reading, MA 01864 USA. 4TP is with the University of Adelaide, Adelaide, SA 5005 Australia. j.leitner@roboticvision.org\nproblems such as object recognition and detection using dataset challenges such as the ImageNet-based ILSVRC [7], COCO [8], and Pascal-VOC [9]. However, pure image dataset challenges are of limited use for robotics where capturing perceptually different images of the same scene is a likelihood, data might be heavily biased, and top-5 classification performance may not be the most appropriate for action execution.\nThis paper is motivated by our experience in the recent 2016 Amazon Picking Challenge (APC). The challenge is a very effective way to drive progress but it occurs only annually and is limited to less than 20 participant team who are provided with the appropriate physical artifacts: standard shelf, standard set of objects. To really drive progress we believe that it is essential to make the challenge conditions more widely available. We propose benchmarking to allow for an easier reproduction of robotic picking task, hopefully allowing for more thorough analysis, and better comparison. The benchmark is based on commonly available shelves and objects. In addition we describe a baseline system for this shelf picking benchmark. Our system is based on a Baxter robot (depicted picking from two shelves in Fig. 1). We report our systems performance on four specific setups of our benchmark and during the 2016 APC.\nar X\niv :1\n60 9.\n05 25\n8v 1\n[ cs\n.R O\n] 1\n7 Se\np 20\n16"
    }, {
      "heading" : "II. TOWARDS REPRODUCIBLE RESEARCH IN ROBOTICS",
      "text" : "Inspired by the trends in other disciplines, e.g. datasets for computer vision, there has also been in an increase in creating more reproducible setups for robotic research. Datasets in computer vision are quite helpful and have enabled the easy comparison of various techniques for image processing problems such as object recognition and detection.\nChallenges like ILSVRC [7] and Pascal-VOC [9] in particular, have spurred interest in the computer vision community and highlighted the challenges that remain. A wealth of algorithms and approaches have been published that aim to outperform the state-of-the-art on these image sets. To be able to “reason” about images has become more interesting for computer vision. The MS COCO challenge, for example, requires algorithms to create a caption for a scene [8], while VQA (visual question answering) [10] asks open ended questions about more than 250 000 images.\nFor robotics, perception and manipulation in cluttered environments is of considerable interest. For instance [11] introduced a 2D/3D database for multiple-instance detection with considerable clutter and foreground occlusion. The increasing number of RGB-D datasets, e.g. for object segmentation [12] and 6D pose estimation [13], can be contributed to the ubiquity of low-cost depth sensors. The above datasets have helped advance the field but provide no complete solution. In addition to datasets like KITTI [14] or the Marulan dataset [15], the robotics community have embraced challenges like the DARPA Robotics Challenge to compare different approaches to solving complete tasks. These challenges are often limited to a small number of participants and do not operate open-ended like computer vision challenges.\nOne such event, the Amazon Picking Challenge (APC), inspired the benchmark proposed in this paper. For the APC, Amazon Robotics provides a standardised shelf and set of objects to each qualifying team. The shelf consists of 12 bins from the middle section of a real Amazon shelf with six 28 × 23 × 43cm bins and six 28 × 27 × 43 cm bins. The tote is a standard, red Uline tote. The first iteration of the APC ran in 2015. In 2016 it included not just a picking – remove 12 specific items from a shelf and place them in a container – but also a stowing task – move 12 objects from a container into a partially full shelf. The challenge lead to the creation of vision datasets, e.g. 2D images and point clouds were created [16]1, and raised questions about how to trade off generality versus engineered solutions in the research community [17].\nThe 2016 APC item set consisted of 39 unique products, selected to present a variety of shapes, sizes, weights, and materials. Seven of the products contained two instances, making 46 total products in the challenge. During the competition, each team had 15 minutes to fulfil a specific work order provided by the organisers. Each task defined the contents of each bin and how the objects should be arranged within. Additional information for the judges included descriptions\n1http://rll.berkeley.edu/amazon_picking_challenge\nfor object placement such as: leaning against wall, occluded, etc. Points were given for successful picks and stows; points were deducted for dropped or destroyed objects.\nWhile the APC showcases advances in robotics, the challenges are hard to reproduce in between the offical runs. Furthermore the improved scores show that the objects and setups can be more challenging."
    }, {
      "heading" : "III. THE ACRV PICKING BENCHMARK (APB)",
      "text" : "Inspired by the Amazon Picking Challenge (APC), we introduce a new physical benchmark and evaluation protocol for robotic picking from shelving units that fosters reproducible and thus comparable results. In contrast to existing datasets, our benchmark allows robotic systems to be evaluated on the complete task, not just sub-tasks such as object detection, grasp selection, or path planning. The APB is designed to highlight current research questions but also to accommodate more complex item arrangements and reasoning as progress is made.\nA major focus through the design of this benchmark was to maximise reproducibility: a number of carefully chosen scenarios with precise instructions on how to place, orient, and align objects with the help of printable stencils are defined. To make the benchmark as accessible as possible to the research community, a white IKEA shelf (Fig. 1A) is used for all picking tasks. Furthermore, we carefully curated a set of 42 objects (Fig. 1B) to ensure global availability and reduced chance of import restrictions.2\nThe defined benchmark scenarios vary in difficulty and challenge both the manipulation capabilities, and the perception pipeline, of the evaluated robotic system: the objects vary in weight and size, and comprise transparent, reflective, black and deformable objects. Some of the objects are prone to shifting their centre of mass when manipulated, and others must adhere to strong constraints on orientation, i.e. they are not allowed to be turned over.\nThis selection makes the APB benchmark a challenge for the state of the art in robotic picking. We hope the research community will take on this challenge and propose novel solutions to complete systems that improve over the baseline results described in Section V. In the following, we describe selected aspects of the benchmark in greater detail."
    }, {
      "heading" : "A. The Shelf",
      "text" : "Our benchmark uses the commonly available white IKEA Kallax shelf with eight 33 × 33 × 39 cm bins (w×h×d). The shelf can be ordered from IKEA3 and is conveniently available worldwide, unlike the proprietary Kiva shelf used during the APC. For the tasks described in this paper, only the upper four bins are used and referred to as ‘Bin A’ to ‘Bin D’. To improve reproducibility and make the bins perceptually similar to the APC shelves, we cover the back of the shelf with cardboard from the packaging (Fig. 2).\n2Food items in the YCB dataset and the rawhide dog chew toy could not be shipped to Australia for example and will violate import regulations in a number of other countries around the world as well.\n3IKEA article number: 102.758.95, price: $75 AUD"
    }, {
      "heading" : "B. Object Dataset",
      "text" : "This benchmark consists of 42 unique objects (Fig. 1B). The items chosen present various challenges for both perception and manipulation. Each item is categorised into easy, medium, and hard difficulties for both detection and manipulability. Item categorisations were selected based on our experience in the recent APC and with respect to our solution to the task. Each item was chosen to broaden the set of challenges presented. Some items, like the tissue box, are both easy to manipulate and relatively easy to detect. Other items, like the nail, are both difficult to detect (small, reflective) and difficult to manipulate. The set includes items similar in appearance to the background, items similar in appearance to others and items invisible to low-cost depth sensors. Transparent, reflective, deformable and odd-shaped items are also included."
    }, {
      "heading" : "C. General Benchmark Task Description",
      "text" : "The benchmark evaluates the capabilities of a robotic system to identify and pick objects from a shelf and place them into a tote. It defines a number of setups and calculates an evaluation score based on the robot’s performance averaged over a number of these setups.\nA setup in the benchmark consists of two parts: • a layout specification, defining the exact placement and\nalignment of objects in the shelf, • a task description (or work order), defining which\nobjects should be picked from the shelf The benchmark description, task definitions, object dataset, placement stencils, and shelf model are available for download at: http://Juxi.net/dataset/apb\na) Object Placement and Stencils: To precisely control the placement of objects in the shelf bins, we created a set of stencils. Stencils can be printed on A4 paper and feature light-grey, numbered markers ( 1©- 9©). Objects are placed on these markers according to a layout specification. For instance, a setup might call for the placement of object cheezit so that its front edge covers markers 4© and 5©, with the front corner of the object just covering marker 5© (Fig. 3). A high-resolution photo of each setup will also be provided to remove ambiguity from the layout specification. We aim to fully cover all markers of a stencil, so that object detection algorithms are not influenced by the stencil itself.\nThe edges of the stencils are marked with compass directions (N,E,S,W) and the layout description specifies how each stencil must be placed in a bin. The four APB task setups presented require the stencils to be placed as follows:\n• SW corner of Crux at the front, left corner of ‘Bin A’; • NE Cassiopeia at front, right of ‘Bin B’; • NE Crux at the front, right of ‘Bin C’; • SE Cassiopeia at the back, left of ‘Bin D’. Images of each task setup can be seen in Figure 4. Predefined stencils allow us to create a multitude of precisely reproducible configurations of objects in the shelf. More scenarios can be easily defined in the future by combining or adding stencils.\nb) Layout and Task Description: A setup is defined by a set of objects and stencils, and includes a description of how objects are placed in the bins. The work order defines which objects are in each bin, and the objects to be picked and placed in the tote. This description is given to the robot in a JSON file, which will need to be updated by the robot at the end of the run. Each bin has n objects to pick (n = 1, except in the hard task, where for two bins n = 2). A speficic example of how such a setup is defined can be found in Section V. The object orientation can be determined from the image provided for each setup. The markers can also be used to specify precise orientation, e.g. centre object sharpie marker over 2© with the tip pointing to 3©.\nc) Evaluation Score: A simple success rate is used to evaluate the overall system performance. A successfully picked object is an item that is in the work order and placed in the tote without being dropped from higher than 35cm and has not accrued damage during manipulation. To ensure a certain robustness and reduce cherry-picking of results, system performance is reported as the percentage of successful picks from the list of objects-to-pick, averaged over three consecutive runs. In addition a clock is started as soon as the robot begins with the task. The time for each successful pick shall be reported. The robot may not take more than 15 minutes to fulfil the full work order."
    }, {
      "heading" : "D. Evaluation Guidelines",
      "text" : "We will curate a list of submissions and a leaderboard on our webpage, where teams can submit scores (and new tasks). A video to verify the robots score and show the robot in operation will be required. In addition, the teams can provide links to systems descriptions, publications, and code. The following guidelines are proposed for fair comparing and ranking of submissions.\nA full run starts with first moving the shelf slightly (each corner’s position can change within a square of 2cm). By moving the shelf position slightly, we hope to foster development of more generic, more robust solutions. The stencils are then placed in the shelf bins before aligning the objects with the correct makers. To check the rotation of the items consult the image provided with the task description. The tote where the objects will be placed can be positioned manually anywhere within a 2m workspace in front of the shelf. The tote is not allowed to touch the shelf or be attached or fixed to the ground. Any part of the robot shall not be closer than 0.5m to any part of the shelf at start. Once the robot is ready to pick objects the clock starts. There is no predefined order for the task, the system can choose in which order to pick. A run is over when the work order is fulfilled (all items are picked) or the maximum time has elapsed. The results for 3 consecutive runs shall be reported."
    }, {
      "heading" : "IV. A BASELINE SYSTEM FOR SHELF PICKING",
      "text" : "We present a benchmark shelf picking system primarily composed of inexpensive, readily available components. The platform leverages a single seven degree of freedom arm of a static Baxter robot. To promote comparison against a variety of systems, a single-arm setup was chosen."
    }, {
      "heading" : "A. Approach",
      "text" : "The baseline system extends Baxter with additional perception capabilities, a custom end-effector with two suction cups at right angles, a small kinetic vacuum pump, and an Intel NUC PC mounted on the robot’s elbow.\nFirst the robot localises the shelf by using the Kinect2. The robot then chooses the next object to pick from the work order provided (in a JSON file) and moves the robot’s end effector to pre-recorded scan pose. The robot begins the Kinect Fusion algorithm and performs a diamond-shaped scanning operation parallel to the shelf’s front face. The fused point cloud is then sent into the perception pipeline, which segments the objects and identifies the target object.\nThe grasping subsystem detects grasp points (and which of the two suction cups to use) in the provided segmented point cloud. Following grasp point selection, the robot turns on the vacuum pump and plans and attempts to execute motions to pre-grasp and grasp poses. Successful object attachment is detected with an in-line pressure sensor. When a successful grasp is detected, trajectories to remove the object from the shelf and position the item over the tote are performed. When over the tote, the item is released by switching the vacuum pump off and the robot chooses the next item to attempt. If a grasp is unsuccessful, the robot returns to scan the scene to accommodate for times when items have moved within the bin. After three failed grasp attempts, an item is skipped.\nComputation was provided by multiple computers running ROS. Alongside the computer within Baxter, the Intel NUC mounted on Baxter’s elbow drives the additional on-board cameras, provides point clouds over the network and reads the pressure sensor switch to detect successful grasps. An off-board computer with an NVIDIA TitanX GPU was used for perception, motion planning and to run the state machine."
    }, {
      "heading" : "B. End-effector Design",
      "text" : "The robot’s gripper is a custom piece of hardware that provides a mounting point for two suction cups, two vacuum lines and a small RGB-D camera. The gripper consists of two 3D-printed components connected by PVC pipes. Existing gripper mount points allowed for our gripper to be attached to Baxter non-destructively. It was designed with the following features and considerations:\n• Small cross-section for reaching into filled bins • Two suction cups mounted at right-angles for grasping\nobjects frontward, downward and sideward\n• Camera mount to maximise closeness to shelf front whilst minimising the joint-space distance between look-into and grasp-ready poses • Mechanically simple design that was easy to manufacture quickly and optimize iteratively\nSuction cups provide a simple and effective way to grasp a wide variety of objects. Our design includes two suction cups to increase the number of reachable grasp points. Considering an object-filled shelf, the additional right-angled suction cup enables object grasps from above. Furthermore, aligning the suction cup with the axis of rotation of the wrist creates the capability to grasp objects from the side (Fig. 5B).\nAn RGB-D camera, mounted at a fixed-angle location, provides local shelf sensing. The angle chosen was empirically found by maximising closeness to the front face of the shelf and reducing the joint-angle distance required to reposition between look-into-shelf and pre-grasp pose."
    }, {
      "heading" : "C. Perception Pipeline",
      "text" : "1) Shelf Localisation: The system leverages a headmounted depth sensor (a Kinect 2) to localise the shelf wrt. the robot. A single point cloud of the empty shelf was captured and the four front corners annotated by a human. These were then used to extract the grid structure of the shelf’s front face. A one-off, semi-autonomous pre-alignment is finalised using ICP [18] registration between the grid points and a 3D model of the shelf.\nThe autonomous alignment stage registers the prerecorded point cloud of the shelf wrt. to the current point cloud provided by the sensor by NDT [19]. Combining this with the transform found in the pre-alignment step a 3D model of the shelf is aligned to the ’live’ shelf position.\nThis approach is widely applicable to varying shelf configurations though was primarily chosen to combat shelves with reflective floors (such as the APC shelves). In such cases, the difference between a Kinect cloud and a cloud generated from a 3D CAD model is large, necessitating some pre-processing to robustly align the points.\n2) Scan, Propose, Classify: Our perception pipeline was split into three distinct stages: a Kinect Fusion (KinFu) [20] approach whilst running a scan motion, a point-cloud-based segmentation algorithm and a deep convolutional neural network (CNN) classification stage.\nA point-cloud-based segmentation algorithm [21] yields distinct object clouds. These are projected into the current\nimage frame to generate non-overlapping 2D object proposals, significantly reducing overall computation compared to EdgeBoxes [22]. To further reduce the number of proposals by the chosen segmentation algorithm, we integrated a KinFu scanning motion, which creates a dense point cloud of the bin, leading to larger, more complete object segments. In addition it reduces the number of erroneous points from it.\nFollowing the scan and segmentation, a pre-trained GoogLeNet [23] fine-tuned with 150 images per class (for 38 of the 39 object classes, as we were not able to import one object), was used to classify each object proposal."
    }, {
      "heading" : "D. Grasping Pipeline",
      "text" : "The objective of the grasping pipeline is to provide reachable grasp points on the object to pick. Provided a point cloud segment, candidate grasp points are generated by smoothing the cloud, estimating it’s boundaries and computing point normals in a grid-like pattern across the surface. This step is fast to perform and includes hyper-parameters for the suction cup dimensions to reduce/increase the number of candidate points generated.\nA grasp selection process follows. First, inverse kinematics for each candidate are checked at both pre-grasp (5 cm above) and grasp poses to ensure the robot can reach the object along the candidate point normal. Reachable grasps are then ranked with heuristics such as distance to the clouds boundary, curvature at the normal and distance to the walls of the bin. These heuristics were chosen to give our system the best chance of grasping an object in its current pose."
    }, {
      "heading" : "E. Motion Planning",
      "text" : "Reaching is performed in open loop on an estimate of the real environment. Facilitated by a localised shelf model, RRT∗ [24] coupled with the TRAC-IK inverse kinematics solver was used to compute collision-free paths for moving the end-effector to the desired target pose. This combination of planner and IK-solver was empirically found to produce more reliable and consistent paths than the other planners and IK solvers tested. In order to optimise the workspace of Baxter’s arm, Baxter was rotated roughly 45 degrees with respect to the shelf (see Fig. 1C)."
    }, {
      "heading" : "F. Software",
      "text" : "We leverage the Robot Operating System (ROS) framework and additional open source software to speed up development of the system and promote modularity,\nstandardisation and reproducibility. The baseline system’s source code is publicly available at: https://github. com/amazon-picking-challenge/team_acrv. Software packages integrated in the system include:\n• MoveIt! and OMPL - Motion Planning • TRAC-IK - Inverse Kinematics Solver • PCL - Point Cloud Library [25] • Iai kinect2 - Microsoft Kinect 2 Driver • Librealsense - Intel RealSense SR300 Driver • SMACH - State Machine"
    }, {
      "heading" : "V. BASELINE RESULTS",
      "text" : "The robotic shelf-picking system described above was initially developed for our participation in the Amazon Picking Challenge 2016 where we ranked 6th of 16 in the picking task. We evaluate our system on the benchmark proposed in Section III. Only minor adjustments to the shelf registration and additional scripted way-points for out-ofshelf movements were added to transfer from APC to APB."
    }, {
      "heading" : "A. ACRV Picking Benchmark",
      "text" : "The system was placed in front of the IKEA shelf, with the centre of Baxter’s arm base 1.5m away from the rear of the shelf and rotated by about 45◦ to maximise shelf reachability.\nThe baseline system was tested on four setups (Fig. 4) with increasing complexity. For the easy task, our systems results were quite consistent, while for the more complicated tasks, our system was unable to produce object segments for the reflective or black objects and so resulted in a poor score.\nIn addition to our systems success rate over three runs in Table I, we report the quickest time-to-first-pick. This metric was chosen to foster research into faster robot systems.\nAn example of a task description using the stencil placement as described above, is given for Setup 1 below:\n• ‘Bin A’: place object cheeze-it 388g so that its front corner is over marker 5©, its front edge just covers marker 4© and 5©; • ‘Bin B’: place rawlings baseball centred over 5©; expo dry erase board eraser front left corner over 3©, front edge just covering 2©; • ‘Bin C’: place i am a bunny book front left corner over 2© and left edge aligned with 3©; laugh out loud joke book centred on 1©, aligned with the front of the shelf, and leaning on the left wall; • ‘Bin D’: elmers washable no run school glue centred over 1©; kleenex tissue box left front corner over 3© and front edge parallel to the back wall.\nOn this setup our baseline system achieved a performance of 75%, ie. it picked 9 out of 12 objects, in three consecutive runs. Our shelf localisation system was continuously running during all experiments, but results presented do not include a movement of the shelf between the runs.\nThe setups are ordered by complexity. Setup 2 adds complexity by introducing deformable objects to the list of picks (cherokee easy tee shirt from ‘Bin C’) and occlusions (platinum pets dog bowl from ‘Bin B’ and kleenex tissue box from ‘Bin D’). In ‘Bin A’\nthe elmers washable no run school glue is to be picked. Our first run needed to be aborted as the robot failed to execute any motion after starting. This result highlights the need to have multiple consecutive runs reported instead of single-shot, cherry-picked results. Overall we picked four objects.\nThe complexity of the picking task is further increased in Setup 3. Objects that are visually hard to differentiate are added, e.g. the object to pick from ‘Bin A’ is champion sports official softball. Also items that produce noisy point clouds were introduced, e.g. picking plastic spatula from ‘Bin B’. In ‘Bin C’ the target is easter turtle sippy cup, which has challenging geometry for manipulation. Multi-object occlusions are also present throughout this setup (cloud b plush bear from ‘Bin D’), which focus not just on improved perception but also robust path planning. We were able to pick one item through the three runs (8.33% success rate), due to perception problems (black objects) and inaccuracies that lead to planning problems.\nSetup 4 is particularly hard. It aims to highlight the shortcomings of current systems and direction for robotics research. The setup contains densely packed bins, very small objects, transparent objects and deformable objects. The work order is to pick the deformable usb cable 1m from ‘Bin A’. In ‘Bin B’ the two objects need to be picked, the green toothbrush which can not be detected as green in its current configuration, requiring some reasoning or verification after the pick. Second the jane eyre dvd stacked between two books, requiring higher level planning. In ‘Bin C’ the single nail needs to be picked, as well as, the pair of scissors, which are visually very similar to the spoon placed just a couple of centimetres away. Finally the ICRA duckie, which is small and of complex shape needs to be picked. For this setup our baseline system was not able to perform any successful grasps, due to the complexity of the perception, grasp detection, and motion planning."
    }, {
      "heading" : "B. Amazon Picking Challenge 2016",
      "text" : "During the APC 2016 in Leipzig, 16 teams competed. This year’s picking task was significantly more difficult than the 2015 task, with denser packed bins, occluded items, and products that were more difficult to see and grasp. For the pick task, all 46 objects started in the shelf. The work order specified one item from each of the 12 bins to be picked and placed into the tote. Before the challenge the teams were given a rough distribution of how many items would be per\nbin, but not told specifics of how these would be arranged. The bins and items to be picked are designed to present a set of trials. For example, each template features a cuboid item at the back of a five item bin and bins where the target item is a book, a soft object, and a bagged object. Points are awarded for successful picks and can vary based on how many items were in the bin. Our baseline system achieved a score of 42 points during the competition, picking four objects successfully from picking template C and placing sixth overall. On runs of the other picking templates in the lab, scores between 5 and 25 points were achieved.4\nThe results during the APC, where some teams picked almost all items successfully, warrants the extension to a wider range of objects as presented in our benchmark. More complex setups – more clutter, more picks per bin – are also introduced to increase the complexity level, especially to create more generic, less engineered solutions."
    }, {
      "heading" : "VI. CONCLUSIONS",
      "text" : "This paper is motivated by our experience in the recent Amazon Picking Challenge (APC). These challenges are effective in driving reseach but are sometimes hard to reproduce. We propose a benchmark, with easily available physical artifacts: a standard shelf, standard set of objects, and reproducible task setups. This benchmarking task will allow for more thorough analysis, better comparison and easier reproduction of complete robotic picking tasks.\nA major focus of the benchmark design was maximising reproducibility: a number of carefully chosen scenarios with precise instructions on how to place, orient, and align objects with the help of printable stencils are defined. In addition a multitude of configurations can be created by combining the various stencils with all possible objects. We carefully selected 42 objects which vary in weight and size, and include deformable, transparent, and closely related items (baseball and softball, red and green tooth brush, full and half full water bottle). Our hope is that the community will expand on these setups.\nThe defined benchmark scenarios vary in difficulty and challenge both the manipulation capabilities as well as the perception pipeline of the evaluated robotic system. A baseline system using a custom-off-the-shelf robot with extensions is described. It is able to perform the easy setups proposed but fails at the difficult ones. We hope the research community will take on this challenge and propose novel solutions to complete systems that improve over the baseline results presented in this paper."
    } ],
    "references" : [ {
      "title" : "The opengrasp benchmarking suite: An environment for the comparative analysis of grasping and dexterous manipulation",
      "author" : [ "S. Ulbrich", "D. Kappler", "T. Asfour", "N. Vahrenkamp", "A. Bierbaum", "M. Przybylski", "R. Dillmann" ],
      "venue" : "International Conference on Intelligent Robots and Systems, 2011, pp. 1761–1767.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Universal robotic gripper based on the jamming of granular material",
      "author" : [ "E. Brown", "N. Rodenberg", "J. Amend", "A. Mozeika", "E. Steltz", "M.R. Zakin", "H. Lipson", "H.M. Jaeger" ],
      "venue" : "Proceedings of the National Academy of Sciences, vol. 107, no. 44, pp. 18 809–18 814, 2010. 4http://amazonpickingchallenge.org/results.shtml",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Vacuum tool for handling microobjects with a nanorobot",
      "author" : [ "W. Zesch", "M. Brunner", "A. Weber" ],
      "venue" : "IEEE International Conference on Robotics and Automation (ICRA), vol. 2, 1997, pp. 1761–1766.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Lessons from the amazon picking challenge",
      "author" : [ "N. Correll", "K.E. Bekris", "D. Berenson", "O. Brock", "A. Causo", "K. Hauser", "K. Okada", "A. Rodriguez", "J.M. Romano", "P.R. Wurman" ],
      "venue" : "CoRR, 2016, arXiV: 1601.05484.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Replicable and measurable robotics research [Special Issue",
      "author" : [ "F. Bonsignorio", "J. Hallam", "A.P. del Pobil (eds." ],
      "venue" : "IEEE Robotics Automation Magazine, vol. 22, no. 3, Sept 2015.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "ImageNet Large Scale Visual Recognition Challenge",
      "author" : [ "O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein", "A.C. Berg", "L. Fei-Fei" ],
      "venue" : "International Journal of Computer Vision, vol. 115, no. 3, pp. 211– 252, 2015.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Microsoft COCO: Common objects in context",
      "author" : [ "T.-Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Dollár", "C.L. Zitnick" ],
      "venue" : "European Conference on Computer Vision (ECCV), 2014, pp. 740–755.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "The pascal visual object classes challenge: A retrospective",
      "author" : [ "M. Everingham", "S.M.A. Eslami", "L. Van Gool", "C.K.I. Williams", "J. Winn", "A. Zisserman" ],
      "venue" : "International Journal of Computer Vision, vol. 111, no. 1, pp. 98–136, Jan. 2015.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "VQA: Visual question answering",
      "author" : [ "S. Antol", "A. Agrawal", "J. Lu", "M. Mitchell", "D. Batra", "C. Lawrence Zitnick", "D. Parikh" ],
      "venue" : "International Conference on Computer Vision (ICCV), 2015, pp. 2425–2433.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Latent-class hough forests for 3d object detection and pose estimation",
      "author" : [ "A. Tejani", "D. Tang", "R. Kouskouridas", "T.-K. Kim" ],
      "venue" : "European Conference on Computer Vision (ECCV), 2014, pp. 462–477.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Rgb-d object recognition: Features, algorithms, and a large scale benchmark",
      "author" : [ "K. Lai", "L. Bo", "X. Ren", "D. Fox" ],
      "venue" : "Consumer Depth Cameras for Computer Vision, 2013, pp. 167–192.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "A dataset for improved rgbd-based object detection and pose estimation for warehouse pick-and-place",
      "author" : [ "C. Rennie", "R. Shome", "K.E. Bekris", "A.F.D. Souza" ],
      "venue" : "IEEE Robotics and Automation Letters, vol. 1, no. 2, pp. 1179–1185, July 2016.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Vision meets robotics: The KITTI dataset",
      "author" : [ "A. Geiger", "P. Lenz", "C. Stiller", "R. Urtasun" ],
      "venue" : "International Journal of Robotics Research, 2013.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "The Marulan Data Sets: Multi-Sensor Perception in Natural Environment with Challenging Conditions",
      "author" : [ "T. Peynot", "S. Scheding", "S. Terho" ],
      "venue" : "The International Journal of Robotics Research, vol. 29, no. 13, pp. 1602–1607, November 2010.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Bigbird: A large-scale 3d database of object instances",
      "author" : [ "A. Singh", "J. Sha", "K.S. Narayan", "T. Achim", "P. Abbeel" ],
      "venue" : "IEEE International Conference on Robotics and Automation (ICRA), 2014, pp. 509–516.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Lessons from the amazon picking challenge: Four aspects of building robotic systems",
      "author" : [ "C. Eppner", "S. Höfer", "R. Jonschkowski", "R. Martın-Martın", "A. Sieverling", "V. Wall", "O. Brock" ],
      "venue" : "Robotics: Science and Systems, 2016.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Method for registration of 3-d shapes",
      "author" : [ "P.J. Besl", "N.D. McKay" ],
      "venue" : "Robotics-DL tentative. International Society for Optics and Photonics, 1992, pp. 586–606.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "The three-dimensional normal-distributions transform: an efficient representation for registration, surface analysis, and loop detection",
      "author" : [ "M. Magnusson" ],
      "venue" : "Ph.D. dissertation, Örebro universitet, 2009.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Kinectfusion: Real-time dense surface mapping and tracking",
      "author" : [ "R.A. Newcombe", "S. Izadi", "O. Hilliges", "D. Molyneaux", "D. Kim", "A.J. Davison", "P. Kohi", "J. Shotton", "S. Hodges", "A. Fitzgibbon" ],
      "venue" : "IEEE Int’l. Symp. on Mixed & Augmented Reality, 2011, p. 127.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Geometrically Consistent Plane Extraction for Dense Indoor 3D Maps Segmentation",
      "author" : [ "T.T. Pham", "M. Eich", "I. Reid", "G. Wyeth" ],
      "venue" : "International Conference on Intelligent Robots and Systems, 2016.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Edge boxes: Locating object proposals from edges",
      "author" : [ "P.D. Larry Zitnick" ],
      "venue" : "European Conference on Computer Vision (ECCV), 2014.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Going deeper with convolutions",
      "author" : [ "C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich" ],
      "venue" : "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2015.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Sampling-based algorithms for optimal motion planning",
      "author" : [ "S. Karaman", "E. Frazzoli" ],
      "venue" : "The International Journal of Robotics Research, vol. 30, no. 7, pp. 846–894, 2011.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "3d is here: Point cloud library (PCL)",
      "author" : [ "R.B. Rusu", "S. Cousins" ],
      "venue" : "IEEE International Conference on Robotics and Automation (ICRA), 2011, pp. 1–4.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Despite a long history of research into dexterous manipulation [1], much of the observable recent progress has been driven by technological developments such as quality low-cost arms (Rethink’s Baxter,",
      "startOffset" : 63,
      "endOffset" : 66
    }, {
      "referenceID" : 1,
      "context" : ") and simple compliant grippers based on coffee grounds [2] or vacuum grippers [3], [4], [5].",
      "startOffset" : 56,
      "endOffset" : 59
    }, {
      "referenceID" : 2,
      "context" : ") and simple compliant grippers based on coffee grounds [2] or vacuum grippers [3], [4], [5].",
      "startOffset" : 79,
      "endOffset" : 82
    }, {
      "referenceID" : 3,
      "context" : ") and simple compliant grippers based on coffee grounds [2] or vacuum grippers [3], [4], [5].",
      "startOffset" : 89,
      "endOffset" : 92
    }, {
      "referenceID" : 4,
      "context" : "side challenges there is growing interest in reproducible research [6] which is relevant and related.",
      "startOffset" : 67,
      "endOffset" : 70
    }, {
      "referenceID" : 5,
      "context" : "problems such as object recognition and detection using dataset challenges such as the ImageNet-based ILSVRC [7], COCO [8], and Pascal-VOC [9].",
      "startOffset" : 109,
      "endOffset" : 112
    }, {
      "referenceID" : 6,
      "context" : "problems such as object recognition and detection using dataset challenges such as the ImageNet-based ILSVRC [7], COCO [8], and Pascal-VOC [9].",
      "startOffset" : 119,
      "endOffset" : 122
    }, {
      "referenceID" : 7,
      "context" : "problems such as object recognition and detection using dataset challenges such as the ImageNet-based ILSVRC [7], COCO [8], and Pascal-VOC [9].",
      "startOffset" : 139,
      "endOffset" : 142
    }, {
      "referenceID" : 5,
      "context" : "Challenges like ILSVRC [7] and Pascal-VOC [9] in partic-",
      "startOffset" : 23,
      "endOffset" : 26
    }, {
      "referenceID" : 7,
      "context" : "Challenges like ILSVRC [7] and Pascal-VOC [9] in partic-",
      "startOffset" : 42,
      "endOffset" : 45
    }, {
      "referenceID" : 6,
      "context" : "requires algorithms to create a caption for a scene [8], while VQA (visual question answering) [10] asks open ended questions about more than 250 000 images.",
      "startOffset" : 52,
      "endOffset" : 55
    }, {
      "referenceID" : 8,
      "context" : "requires algorithms to create a caption for a scene [8], while VQA (visual question answering) [10] asks open ended questions about more than 250 000 images.",
      "startOffset" : 95,
      "endOffset" : 99
    }, {
      "referenceID" : 9,
      "context" : "For instance [11] introduced a 2D/3D database for multiple-instance detection with considerable clutter and foreground occlusion.",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 10,
      "context" : "segmentation [12] and 6D pose estimation [13], can be contributed to the ubiquity of low-cost depth sensors.",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 11,
      "context" : "segmentation [12] and 6D pose estimation [13], can be contributed to the ubiquity of low-cost depth sensors.",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 12,
      "context" : "In addition to datasets like KITTI [14] or the Marulan dataset [15], the robotics community have embraced challenges like the DARPA Robotics Challenge",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 13,
      "context" : "In addition to datasets like KITTI [14] or the Marulan dataset [15], the robotics community have embraced challenges like the DARPA Robotics Challenge",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 14,
      "context" : "2D images and point clouds were created [16]1, and raised questions about how to trade off generality versus engineered solutions in the research community [17].",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 15,
      "context" : "2D images and point clouds were created [16]1, and raised questions about how to trade off generality versus engineered solutions in the research community [17].",
      "startOffset" : 156,
      "endOffset" : 160
    }, {
      "referenceID" : 16,
      "context" : "A one-off, semi-autonomous pre-alignment is finalised using ICP [18] registration between the grid",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 17,
      "context" : "to the current point cloud provided by the sensor by NDT [19].",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 18,
      "context" : "2) Scan, Propose, Classify: Our perception pipeline was split into three distinct stages: a Kinect Fusion (KinFu) [20]",
      "startOffset" : 114,
      "endOffset" : 118
    }, {
      "referenceID" : 19,
      "context" : "A point-cloud-based segmentation algorithm [21] yields distinct object clouds.",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 20,
      "context" : "These are projected into the current image frame to generate non-overlapping 2D object proposals, significantly reducing overall computation compared to EdgeBoxes [22].",
      "startOffset" : 163,
      "endOffset" : 167
    }, {
      "referenceID" : 21,
      "context" : "Following the scan and segmentation, a pre-trained GoogLeNet [23] fine-tuned with 150 images per class (for 38 of the 39 object classes, as we were not able to import one object), was used to classify each object proposal.",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 22,
      "context" : "Facilitated by a localised shelf model, RRT∗ [24] coupled with the TRAC-IK inverse kinematics solver was used to compute collision-free paths for moving the end-effector to the desired target pose.",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 23,
      "context" : "• MoveIt! and OMPL - Motion Planning • TRAC-IK - Inverse Kinematics Solver • PCL - Point Cloud Library [25] • Iai kinect2 - Microsoft Kinect 2 Driver • Librealsense - Intel RealSense SR300 Driver",
      "startOffset" : 103,
      "endOffset" : 107
    } ],
    "year" : 2017,
    "abstractText" : "Robotic challenges like the Amazon Picking Challenge (APC) or the DARPA Challenges are an established and important way to drive scientific progress as they make research comparable on a well-defined benchmark with equal test conditions for all participants. However, such challenge events occur only occasionally, are limited to a small number of contestants, and the test conditions are very difficult to replicate after the main event. We present a new physical benchmark challenge for robotic picking. The ACRV Picking Benchmark is designed to be reproducible by using a set of 42 common objects, a widely available shelf, and exact guidelines for object arrangement using stencils. A well-defined evaluation protocol enables the comparability of complete robotic systems – including perception and manipulation – instead of sub-systems only. Our paper describes this new benchmark challenge and presents results acquired by a baseline system based on a Baxter robot.",
    "creator" : "LaTeX with hyperref package"
  }
}