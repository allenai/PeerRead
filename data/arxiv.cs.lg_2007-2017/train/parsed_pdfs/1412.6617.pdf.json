{
  "name" : "1412.6617.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "UNDERSTANDING MINIMUM PROBABILITY FLOW FOR RBMS UNDER VARIOUS KINDS OF DYNAMICS",
    "authors" : [ "Daniel Jiwoong Im", "Ethan Buchman", "Graham W. Taylor" ],
    "emails" : [ "imj@uoguelph.ca", "ebuchman@uoguelph.ca", "gwtaylor@uoguelph.ca" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "A common problem in machine learning is to estimate the parameters of a high-dimensional probabilistic model using gradient descent on the model’s negative log likelihood. For exponential models where p(x) is defined by an energy function F (x), the gradient of the data negative log-likelihood takes the form\n∇θ = ∑ x∈D ∂F (x) ∂θ − ∑ x p(x) ∂F (x) ∂θ (1)\nwhere the sum in the first term is over the dataset, D, and the sum in the second term is over the entire domain of x. The first term has the effect of pushing the parameters in a direction that increases the likelihood of the model on the data, while the second term serves to balance this by decreasing the likelihood of all possible states. Since the second term is intractable for all but trivial models, we cannot, in practice, accommodate for every state of x, but rather resort to sampling. We call states in the sum in the first term positive particles and those in the second term negative particles, in accordance with their effect on the likelihood. Thus, the intractability of the second term becomes a problem of negative particle selection (NPS).\nThe most famous approach to NPS is Contrastive Divergence (CD) (Hinton, 2002), which is the kernel of unsupervised neural network learning in energy-based models. CD proposes to sample the negative particles by employing a Markov chain Monte Carlo (MCMC) transition operator a small number of times to each data state. This is in contrast to taking an unbiased sample from the distribution, by applying the MCMC operator a large number of times, until the distribution reaches equilibrium, which is often prohibitive for practical applications. Much research has attempted to better understand this approach and the reasoning behind its success (Sutskever & Tieleman, 2009), leading to many variations being proposed from the perspective of improving the MCMC chain. Here, we take a more general approach to the problem of NPS, in particular, through the lens of the Minimum Probability Flow (MPF) algorithm (Sohl-Dickstein et al., 2011).\nMPF works by introducing an explicit dynamics over the model, yielding an equilibrium distribution as a function of that dynamics. The objective of learning is to minimize the flow of probability from data states to non-data states after infinitesimal evolution under the model’s dynamics. In MPF, NPS is replaced by a more explicit notion of\nar X\niv :1\n41 2.\n66 17\nv1 [\ncs .L\nG ]\n2 0\nD ec\n2 01\n4\nconnectivity between states. Connected states are ones between which probability can flow under the dynamics. Thus, rather than attempting to approximate an intractable function, we run a simple optimization over an explicit dynamics.\nInterestingly, MPF and CD have gradients with remarkably similar form, where the latter uses a somewhat incomprehensible transition operator and the former a well formulated and understood dynamics. Thus, in one aspect, MPF solves the problem of contrastive divergence by reconceptualizing it as probability flow under an explicit dynamics, rather than the convenient but biased sampling of an intractable function. The challenge thus becomes one of how to design those dynamics. This paper aims to contrast variations on the dynamics, and experimentally demonstrate that MPF training outperforms CD on Restricted Boltzmann Machines."
    }, {
      "heading" : "1.1 RESTRICTED BOLTZMANN MACHINES",
      "text" : "While the learning methods we discuss apply to undirected probabilistic graphical models in general, we will use the Restricted Boltzmann Machine (RBM) as a canonical example. An RBM is an undirected bipartite graph with visible (observed) variables v ∈ {0, 1}D and hidden (latent) variables h ∈ {0, 1}H (Smolensky, 1986). The RBM is an energy-based model where the energy of state v,h is given by\nE(v,h; θ) = − ∑ i ∑ j Wijvihj − ∑ i bivi − ∑ j cjhj (2)\nwhere θ = {W,b, c} are the parameters of the model. The marginalized probability over visible variables is formulated from the Boltzmann distribution,\np(v; θ) = p∗(v; θ)\nZ(θ) =\n1\nZ(θ) ∑ h exp ( −1 τ E(v,h; θ) ) (3)\nsuch that Z(θ) = ∑ v,h exp (−1 τ E(v,h; θ) ) is a normalizing constant and τ is the thermodynamic temperature. We can marginalize over the binary hidden states in Equation 2 and re-express in terms of a new energy F (v),\nF (v; θ) = − log ∑ h exp ( −1 τ E(v,h) ) = 1 τ vTb− 1 τ H∑ j=1 log ( 1 + exp ( vTW + b )) (4)\np(v; θ) = exp\n( − F (v; θ) ) Z(θ)\n(5)\nFollowing physics, this form of the energy is better known as a free energy, as it expresses the difference between the average energy and the entropy of a distribution, in this case, that of p(h|v). Defining the distribution in terms of free energy as p(v; θ) is convenient since it naturally copes with the presence of latent variables.\nThe key characteristic of an RBM is the simplicity of inference due to conditional independence between visible and hidden states:\np(h|v) = ∏ j p(hj |v), p(hj = 1|v) = σ( ∑ i Wijvi + cj)\np(v|h) = ∏ i p(vi|h), p(vi = 1|h) = σ( ∑ j Wijhj + bi)\nwhere σ(·) = 1/(1 + exp (−·)). This leads naturally to a block Gibbs sampling dynamics, used universally for sampling from RBMs. Hence, in an RBM trained by CD, the connectivity (NPS) is determined with probability given by the one step block Gibbs sampling transition.\nWe can formalize this by writing the learning updates for CD as follows ∆θCD ∝ − ∑ j∈D ∑ i 6∈D (∂Ej(θ) ∂θ − ∂Ei(θ) ∂θ ) Tij (6)\nwhere Tij is the probability of transitioning from state j to state i in one step of block Gibbs sampling. We can in principle replace Tij by any other transition operator, so long as it preserves the equilibrium distribution. Indeed, this is what alternative methods, like Persistent CD (Tieleman & Hinton, 2009), achieve."
    }, {
      "heading" : "2 MINIMUM PROBABILTY FLOW",
      "text" : "The key intuition behind MPF is that NPS can be reformulated in a firm theoretical context by treating the model distribution as the end point of some explicit dynamics, and minimizing probability flow under those dynamics, rather than treating the dynamics as a sampling procedure employed to approximate an intractable function of that distribution. That is, MPF provides a theoretical environment for the formal treatment of Tij that offers a much more general perspective of that operator than CD can, while formalizing the notion of minimizing divergence between positive and negative particles."
    }, {
      "heading" : "2.1 DYNAMICS OF THE MODEL",
      "text" : "The primary mathematical apparatus for MPF is a continuous time Markov chain known as the master equation, ṗi = ∑ j 6=i [Γijp (t) j − Γjip (t) i ] (7)\nwhere j are the data states and i are the non-data states and Γij is probability flow rate from state j and state i. Note that a state is full vector of variables, and we are theoretically enumerating all states. ṗi is the rate of change of the probability of state i, that is, the difference between the probability flowing out of any state j into state i and the probability flowing out of state i to any other state j at time t. We can re-express ṗi in a simple matrix form as\nṗ = Γp (8) by setting Γii = − ∑ i 6=j Γjip (t) i . We note that if the transition matrix Γ is ergodic, then the model has a unique stationary distribution.\nThis is a common model for exploring statistical mechanical systems, but it is unwieldly in practice for two reasons, namely, the continuous time dynamics, and exponential size of the state space. For our purposes, we will actually find the former an advantage, and the latter irrelevant.\nThe objective of MPF is to minimize the KL divergence between the data distribution and the distribution after evolving an infinitesimal amount of time under the dynamics:\nθMPF = argminθJ(θ), J(θ) = DKL(p (0)||p( )(θ))\nApproximating J(θ) up to a first order Taylor expansion with respect to time t, our objective function reduces to\nJ(θ) = |D| ∑ j∈D ∑ i 6∈D Γij (9)\nand θ can be optimized by taking gradient descent of J(θ). Since Γij captures probability flow from state j to state i, this objective function has the quite elegant interpretation of minimizing the probability flow from data states to non-data states (Sohl-Dickstein et al., 2011)."
    }, {
      "heading" : "2.2 FORM OF THE TRANSITION MATRIX",
      "text" : "MPF does not propose to actually simulate these dynamics. There is, in fact, no need to, as the problem formulation reduces to a rather simple optimization problem with no intractable component. However, we must provide a means for computing the matrix coefficients Γij . Since our target distribution is the distribution defined by the RBM, we require Γ to be a function of the energy, or more particularly, the parameters of the energy function.\nA sufficient (but not necessary) means to guarantee that the distribution p∞ (θ) is a fixed point of the dynamics is to choose Γ to satisfy detailed balance, that is\nΓjip (∞) i (θ) = Γijp (∞) j (θ). (10)\nThe following theorem provides a general form for the transition matrix such that the equilibrium distribution is that of the RBM:\nTheorem 1. Suppose p(∞)j is the probability of state j and p (∞) i is the probability of state i. Let the transition matrix be\nΓij = gij exp\n( o(Fi − Fj) + 1\n2 (Fj − Fi)\n) (11)\nsuch that o(·) is any odd function, where gij is the symmetric connectivity between the states i and j. Then this transition matrix satisfies detailed balance in Equation 18.\nThe proof is provided in Appendix A.1. The transition matrix proposed by (Sohl-Dickstein et al., 2011) is thus the simplest case of Theorem 1, found by setting o(·) = 0 and gij = gji:\nΓij = gij exp ( 1\n2 (Fj(θ)− Fi(θ)). (12)\nGiven a form for the transition matrix, we can now evaluate the gradient of J(θ)\n∂J(θ)\n∂θ = |D| ∑ j∈D ∑ i 6∈D (∂Ej(θ) ∂θ − ∂Ei(θ) ∂θ ) Tij\nTij = gij exp (1\n2 (Ej(θ)− Ei(θ)) ) and observe the similarity to the formulation given for the RBM trained by CD (Equation 6). Unlike with CD, however, this expression was derived through an explicit dynamics and well-formalized minimization objective."
    }, {
      "heading" : "3 PROBABILITY FLOW RATES Γ",
      "text" : "At first glance, MPF might appear doomed, due to the size of Γ, namely 2D × 2D, and the problem of enumerating all of the states. However, the objective function in Equation 9 sums over the Γij’s, thus considering only transitions between data states i (limited in size by our data set) and non-data state j (limited by the sparseness of our design). By specifying Γ to be sparse, the intractability disappears, and complexity is dominated by the size of the dataset.\nUsing traditional methods, an RBM can be trained in two ways, either with sampled negative particles, like in CD, PCD, or stochastic maximum likelihood (Hinton, 2002; Tieleman & Hinton, 2009), or via an inductive principle, with fixed sets of “fantasy cases”, like in general score matching, ratio matching, or pseudolikelihood (Hyvärinen, 2005; Marlin & Freitas, 2011; Besag, 1975). In a similar manner, we can define Γ by specifying the connectivity function gij either as a distribution from which to sample or as fixed and deterministic.\nIn this section, we examine various kinds of connectivity functions and their consequences on the probability flow dynamics."
    }, {
      "heading" : "3.1 1-BIT FLIP CONNECTIONS",
      "text" : "It can be shown that score matching is a special case of MPF, where the connectivity function is set to connect all states within a small hamming distance r in the limit of r → 0 (Sohl-Dickstein et al., 2011). For simplicity, we can fix the hamming distance to one instead, and consider that data states are connected to all other states 1-bit flip away:\ngij = { 1, if state i, j differs by single bit flip 0, otherwise\n(13)\n1-bit flip connectivity gives us an extremely sparse Γ with D2 non-zero terms rather than 4D, and may be seen as NPS where the only negative particles are those which are 1-bit flip away from data states. This was the only connectivity function pursued in (Sohl-Dickstein et al., 2011) and is a natural starting point for the approach."
    }, {
      "heading" : "3.2 FACTORIZED MINIMUM PROBABILITY FLOW",
      "text" : "Previously, we considered connectivity gij as a function of both states i and j. Instead, we may wish to let gij = gi, yielding an independence chain (Tierney, 1994). This is an alternative way of constructing a probability matrix\nwith a factorized objective function (Sohl-Dickstein, 2011),\nJ(θ) = 1 |D| ∑ j∈D ∑ i 6∈D gi ( gj gi ) 1 2 exp ( 1 2 (Fj(x; θ)− Fi(x; θ)) ) (14)\n=  1 |D| ∑ j∈D exp ( 1 2 Fj(x; θ) + log gj ) 1 |Dc| ∑ i6∈D exp ( 1 2 − Fi(x; θ) + log gi) ) (15) where Dc is complement of dataset, gi is the probability of choosing state i, and ( gj gi ) 1 2\nis a scaling term. By the independence in the connectivity function, we can eliminate the inner sum and treat each summation separately. However, this assumes that the connectivity of state j to state i is independent of initial state j, which amounts to a design decision of the probability flow matrix (Hasting, 1970). In practice, the second term in Equation 15 can be sampled from p(v; θ). We can then rewrite the objective function as\nJ(θ) =\n( 1\n|D| ∑ x∈D exp [ 1 2 F (x; θ)− F (x; θn−1) ])( 1 |S| ∑ x′∈S exp [ 1 2 F (x′; θ)− F (x′; θn−1) ]) (16)\nwhere S is the sampled set from p(v; θn−1) , and log gj and log gi become F (x; θn−1) and F (x′; θn−1). The purpose of the F (x; θn−1) term is to reweight the states by the square root of the probability of them being chosen as non-zero entries in the connectivity matrix. Notice that the 1-bit flip connection cannot be factorized since the connection to state i depends on it being a neighbor of state j."
    }, {
      "heading" : "3.3 PERSISTENT MINIMUM PROBABILITY FLOW",
      "text" : "There are several ways of sampling “fantasy particles” from p(v; θn−1). Notice that taking the data distribution with respect to θn−1 is important.\nPreviously, persistent contrastive divergence (PCD) was developed to improve CD learning (Tieleman & Hinton, 2009). Similarly, persistence can be applied to sampling in MPF connectivity functions. For each update, we pick a new sample based on a MCMC sampler which starts from previous samples. Then we update θn, which satsifies J(θn) ≤ J(θn−1) (Sohl-Dickstein, 2011). The pseudo-code for persistent MPF is the same as Factored MPF except for drawing new samples, which is indicated by square brackets in Algorithm 1.\nAs we will show, using persistence in MPF is important for achieving faster convergence in learning. While the theoretical formulation of MPF guarantees eventual convergence, the focus on minimizing the initial probability flow will have little effect if the sampler mixes too slowly. In practice, combining the persistent samples and non-persistent samples gave better performance.\nAlgorithm 1 Factored [Persistent] minimum probability flow learning.\n• Initialize the parameters θ0\n• Initialize the samples S0 for number of iterations do\n1. Draw a new sample Sn based on S0 [ Sn−1 ] using an MCMC sampler.\n2. Compute θn ← θn−1 − λ5 J(θn−1) end for"
    }, {
      "heading" : "4 EXPERIMENTS",
      "text" : "We conducted the first empirical study of MPF under different types of connectivity as discussed in Section 3. We compared our results to CD with different numbers of MCMC steps. We analyzed the MPF variants based on training RBMs and assessed them quantitatively and qualitatively by comparing the log-liklihoods of the test data and samples generated from model. For the experiments, we denote the 1-bit flip, factorized, and persistent methods as MPF-1flip, FMPF, and PMPF, respectively.\nThe goals of these experiments are to\n1. Compare the performance between MPF algorithms under different connectivities; and 2. Compare the performance between MPF and CD.\nIn our experiments, we considered the MNIST and CalTech Silhouette datasets. MNIST consists of 60,000 training and 10,000 test images of size 28 × 28 pixels containing handwritten digits from the classes 0 to 9. The pixels in MNIST are binarized based on thresholding. From the 60,000 training examples, we set aside 10,000 as validation examples to tune the hyperparameters in our models. The CalTech Silhouette dataset contains the outlines of objects from the CalTech101 dataset, which are centered and scaled on a 28 × 28 image plane and rendered as filled black regions on a white background creating a silhouette of each object. The training set consists of 4,100 examples, with at least 20 and at most 100 examples in each category. The remaining instances were split evenly between validation and testing1."
    }, {
      "heading" : "4.1 MNIST - EXACT LOG LIKELIHOOD",
      "text" : "In our first experiment, we trained eleven RBMs on the MNIST digits. All RBMs consisted of 20 hidden units and 784 (28×28) visible units. Due to the small number of hidden variables, we calculated the exact value of the partition function by explicitly summing over all visible configurations. Four RBMs were learned by CD1, CD10, CD15, and CD25. Seven RBMs were learned by 1 bit flip, FMPF, and PMPF. Block Gibbs sampling is required for FMPF-k and PMPF-k similar to CD training, where the number of steps is given by k.\nThe average log test likelihood values of RBMs with 20 hidden units are presented in Table 1. This table gives a sense of the performance under different types of MPF dynamics when the partition function can be calculated exactly. We observed that PMPF consistently achieved a higher log-likelihood than FMPF. MPF with 1 bit flip was very fast but gave poor performance compared to FMPF and PMPF. We also observed that MPF-1flip outperformed CD1. FMPF always performed slightly worse than CD training with the same number of Gibbs steps. However, PMPF always outperformed CD.\n1More details on pre-processing the CalTech Silhouettes can be found in http://people.cs.umass.edu/∼marlin/data.shtml\nOne advantage of FMPF is that it converges much quicker than CD or PMPF. This is because we used twice many samples as PMPF as mentioned in Section 3.3. Figure 1 shows initial data and the generated samples after running 100 Gibbs steps from each RBM. PMPF produces samples that are visually more appealing than the other methods."
    }, {
      "heading" : "4.2 MNIST - ESTIMATING LOG LIKELIHOOD",
      "text" : "In our second set of experiments, we trained RBMs with 200 hidden units. We trained them exactly as described in Section 4.1. These RBMs are able to generate much higher-quality samples from the data distribution, however, the partition function can no longer be computed exactly.\nIn order to evaluate the model quantitatively, we estimated the test log-likelihood using the Conservative Samplingbased Likelihood estimator (CSL) (Bengio et al., 2013). Given well-defined conditional probabilities P (v|h) of a model and a set of latent variable samples S collected from a Markov chain, CSL computes\nlog f̂(v) = log meanh∈SP (v|h). The advantage of CSL is that sampling latent variables h instead of v has the effect of reducing the variance of the CSL estimator. Also, in contrast to annealed importance sampling (AIS) (Salakhutdinov & Murray, 2008), which tends to overestimate, CSL is much more conservative in its estimates.\nTable 2 demonstrates the test log-likelihood of various RBMs with 200 hidden units. The ranking of the different training paradigms with respect to performance was similar to what we observed in Section 4.1 with PMPF emerging as the winner. However, contrary to the first experiment, we observed that MPF with 1 bit flip did not perform well. Moreover, FMPF and PMPF both tended to give higher test log-likelihoods than CD training. As a result of cross-validation, we noticed that smaller batch sizes worked better with MPF when the number of hiddens was increased. Once again, we observed smaller variances compared to CD with both forms of MPF, especially with FMPF. We noted that FMPF and PMPF always have smaller variance compared to CD. This implies that FMPF and PMPF are less sensitive to random weight initialization. Figure 2 shows initial data and generated samples after running 100 Gibbs steps for each RBM. PMPF clearly produces samples that look more like digits."
    }, {
      "heading" : "4.3 CALTECH 101 SILHOUETTES - ESTIMATING LOG LIKELIHOOD",
      "text" : "Finally, we evaluated the same set of RBMs on the Caltech101 Silhouettes dataset. Compared to MNIST, this dataset contains much more diverse structures with richer correlation among the pixels. It has 10 times more categories, contains less training data per category, and each object covers more of the image. For these reasons, we set the number of hidden units to be 500. The estimated average log-likelihood of train and test data is presented in Table 3. The results for Caltech 101 Silhouettes are consistent with the results on MNIST.\nIn all sets of experiments, we observed a larger margin between PMPF and CD when the number of sampling steps was smaller. In addition, the single bit flip technique was not particularly successful, especially as the number of latent variables grew. We speculate that the reason for this might have to do with the slow rate of convergence for the dynamic system. Moreover, PMPF works better than FMPF for similar reasons. By having persistent samples as the learning progresses, the rate of convergence must be smaller since the dynamics start from samples to where it is most likely the model assigns probability mass. Figure 3 shows initial data and generated samples after running 100 Gibbs steps for each RBM on Caltech101 dataset."
    }, {
      "heading" : "5 CONCLUSION",
      "text" : "Minimum Probability Flow is an unsupervised algorithm that can be employed off-the-shelf to any energy-based models. It has a number of favorable properties but has not seen application proportional to its potential. In this paper, we first expounded on MPF and its connections to CD training, which allowed us to gain a better understanding and perspective to CD. We proved a general form for the transition matrix such that the equilibrium distribution converges to that of an RBM. This may lead to future extensions of MPF based on the choice of o(·) in Equation 11.\nOne of the merits of MPF is that the choice of designing a dynamic system by defining a connectivity function is left open as long as it satisfies the fixed point equation. We thoroughly explored three different connectivity structures,\nnoting that connectivity can be designed inductively or by sampling. Finally, we showed empirically that MPF, and in particular, PMPF, outperforms CD for training generative models. Until now, RBM training was dominated by methods based on CD; however, our results indicate that MPF is a practical and effective alternative."
    }, {
      "heading" : "A MINIMUM PROBABILITY FLOW",
      "text" : "A.1 DYNAMICS OF THE MODEL\nTheorem 1. Suppose p(∞)j is the probability of state j and p (∞) i is the probability of state i. Let the transition matrix be\nΓij = gij exp\n( o(Fi − Fj) + 1\n2 (Fj − Fi)\n) (17)\nsuch that o(·) is any odd function, where gij is the symmetric connectivity between the states i and j. Then this transition matrix satisfies detailed balance in Equation 18.\nProof. By cancalling out the partition function, the detailed balance Equation 18 can be formulated to be\nΓji exp (−Fi) = Γij exp (−Fj) (18)\nwhere Fi = F (v = i; θ) We substitute transition matrix defined in Equation 11, then we get the following after straight forward formula manipulation.\nΓji exp (−Fi)/Γij exp (−Fj)) = 1\nexp\n( o(Fi − Fj) + 1\n2 (Fj − Fi)− Fi\n) / exp ( o(Fj − Fi) + 1\n2 (Fi − Fj)− Fj\n) = 1\nexp\n( o(Fi − Fj) + 1\n2 (Fj − Fi)− Fi − o(Fj − Fi) + 1 2\n(Fi − Fj) + Fj ) = 1\no(Fi − Fj) + 1 2 (Fj − Fi)− Fi − o(Fj − Fi) + 1 2 (Fi − Fj) + Fj = 0\n(Fi − Fj) ( o(Fi − Fj) + 1\n2 + o(Fj − Fi) + 1 2 − 1 ) = 0\n(Fi − Fj) ( o(Fi − Fj)\n2 + o(Fj − Fi) 2\n) = 0\nNotice that since o(·) is an odd function, this makes the term ( o(Fi−Fj)\n2 + o(Fj−Fi) 2\n) = 0. Therefore, the detailed\nbalance criterion is satisfied."
    } ],
    "references" : [ {
      "title" : "Bounding the test log-likelihood of generative models",
      "author" : [ "Bengio", "Yoshua", "Yao", "Li", "Cho", "Kyunghyun" ],
      "venue" : "In Proceedings of the International Conference of Learning Representations (ICLR),",
      "citeRegEx" : "Bengio et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2013
    }, {
      "title" : "Statistical analysis of non-lattice data",
      "author" : [ "Besag", "Julian" ],
      "venue" : "The Statistician,",
      "citeRegEx" : "Besag and Julian.,? \\Q1975\\E",
      "shortCiteRegEx" : "Besag and Julian.",
      "year" : 1975
    }, {
      "title" : "Monte carlo sampling methods using markov chains and their applications",
      "author" : [ "Hasting", "W. Keith" ],
      "venue" : null,
      "citeRegEx" : "Hasting and Keith.,? \\Q1970\\E",
      "shortCiteRegEx" : "Hasting and Keith.",
      "year" : 1970
    }, {
      "title" : "Training products of experts by minimizing contrastive divergence",
      "author" : [ "Hinton", "Geoffrey. E" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Hinton and E.,? \\Q2002\\E",
      "shortCiteRegEx" : "Hinton and E.",
      "year" : 2002
    }, {
      "title" : "Estimation of non-normalized statistical models by score matching",
      "author" : [ "Hyvärinen", "Aapo" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Hyvärinen and Aapo.,? \\Q2005\\E",
      "shortCiteRegEx" : "Hyvärinen and Aapo.",
      "year" : 2005
    }, {
      "title" : "Asymptotic efficiency of deterministic estimators for discrete energybased models: Ratio matching and pseudolikelihood",
      "author" : [ "Marlin", "Benjamin M", "Freitas", "Nando de" ],
      "venue" : "In Proceedings of the Uncertainty in Artificial Intelligence (UAI),",
      "citeRegEx" : "Marlin et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Marlin et al\\.",
      "year" : 2011
    }, {
      "title" : "On the quantitative analysis of deep belief networks",
      "author" : [ "Salakhutdinov", "Ruslan", "Murray", "Iain" ],
      "venue" : "In Proceedings of the International Conference of Machine Learning (ICML),",
      "citeRegEx" : "Salakhutdinov et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Salakhutdinov et al\\.",
      "year" : 2008
    }, {
      "title" : "Information processing in dynamical systems: Foundations of harmony theory. In Parallel Distributed Processing: Volume 1: Foundations, pp. 194–281",
      "author" : [ "Smolensky", "Paul" ],
      "venue" : null,
      "citeRegEx" : "Smolensky and Paul.,? \\Q1986\\E",
      "shortCiteRegEx" : "Smolensky and Paul.",
      "year" : 1986
    }, {
      "title" : "Persistent minimum probability flow",
      "author" : [ "Sohl-Dickstein", "Jascha" ],
      "venue" : "Technical report, Redwood Centre for Theoretical Neuroscience,",
      "citeRegEx" : "Sohl.Dickstein and Jascha.,? \\Q2011\\E",
      "shortCiteRegEx" : "Sohl.Dickstein and Jascha.",
      "year" : 2011
    }, {
      "title" : "Minimum probability flow learning",
      "author" : [ "Sohl-Dickstein", "Jascha", "Battaglino", "Peter", "DeWeese", "Michael R" ],
      "venue" : "In Proceedings of the International Conference of Machine Learning (ICML),",
      "citeRegEx" : "Sohl.Dickstein et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Sohl.Dickstein et al\\.",
      "year" : 2011
    }, {
      "title" : "On the convergence properties of contrastive divergence",
      "author" : [ "Sutskever", "Ilya", "Tieleman", "Tijmen" ],
      "venue" : "In Proceedings of the AI & Statistics (AI STAT),",
      "citeRegEx" : "Sutskever et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2009
    }, {
      "title" : "Using fast weights to improve persistent contrastive divergence",
      "author" : [ "Tieleman", "Tijmen", "Hinton", "Geoffrey E" ],
      "venue" : "In Proceedings of the International Conference of Machine Learning (ICML),",
      "citeRegEx" : "Tieleman et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Tieleman et al\\.",
      "year" : 2009
    }, {
      "title" : "Markov chains for exploring posterior distributions",
      "author" : [ "Tierney", "Luke" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "Tierney and Luke.,? \\Q1994\\E",
      "shortCiteRegEx" : "Tierney and Luke.",
      "year" : 1994
    } ],
    "referenceMentions" : [ {
      "referenceID" : 9,
      "context" : "Here, we take a more general approach to the problem of NPS, in particular, through the lens of the Minimum Probability Flow (MPF) algorithm (Sohl-Dickstein et al., 2011).",
      "startOffset" : 141,
      "endOffset" : 170
    }, {
      "referenceID" : 9,
      "context" : "Since Γij captures probability flow from state j to state i, this objective function has the quite elegant interpretation of minimizing the probability flow from data states to non-data states (Sohl-Dickstein et al., 2011).",
      "startOffset" : 193,
      "endOffset" : 222
    }, {
      "referenceID" : 9,
      "context" : "The transition matrix proposed by (Sohl-Dickstein et al., 2011) is thus the simplest case of Theorem 1, found by setting o(·) = 0 and gij = gji: Γij = gij exp ( 1 2 (Fj(θ)− Fi(θ)).",
      "startOffset" : 34,
      "endOffset" : 63
    }, {
      "referenceID" : 9,
      "context" : "1 1-BIT FLIP CONNECTIONS It can be shown that score matching is a special case of MPF, where the connectivity function is set to connect all states within a small hamming distance r in the limit of r → 0 (Sohl-Dickstein et al., 2011).",
      "startOffset" : 204,
      "endOffset" : 233
    }, {
      "referenceID" : 9,
      "context" : "This was the only connectivity function pursued in (Sohl-Dickstein et al., 2011) and is a natural starting point for the approach.",
      "startOffset" : 51,
      "endOffset" : 80
    }, {
      "referenceID" : 0,
      "context" : "Likelihood estimates are made with CSL (Bengio et al., 2013).",
      "startOffset" : 39,
      "endOffset" : 60
    }, {
      "referenceID" : 0,
      "context" : "In order to evaluate the model quantitatively, we estimated the test log-likelihood using the Conservative Samplingbased Likelihood estimator (CSL) (Bengio et al., 2013).",
      "startOffset" : 148,
      "endOffset" : 169
    }, {
      "referenceID" : 0,
      "context" : "Likelihood estimates are made with CSL (Bengio et al., 2013).",
      "startOffset" : 39,
      "endOffset" : 60
    } ],
    "year" : 2017,
    "abstractText" : "Energy-based models are popular in machine learning due to the elegance of their formulation and their relationship to statistical physics. Among these, the Restricted Boltzmann Machine (RBM) has been the prototype for some recent advancements in the unsupervised training of deep neural networks. However, the contrastive divergence training algorithm, so often used for such models, has a number of drawbacks and ineligancies both in theory and in practice. Here, we investigate the performance of Minimum Probability Flow learning for training RBMs. This approach reconceptualizes the nature of the dynamics defined over a model, rather than thinking about Gibbs sampling, and derives a simple, tractable, and elegant objective function using a Taylor expansion, allowing one to learn the parameters of any distribution over visible states. In the paper, we expound the Minimum Probability Flow learning algorithm under various dynamics. We empirically analyze its performance on these dynamics and demonstrate that MPF algorithms outperform CD on various RBM configurations.",
    "creator" : "LaTeX with hyperref package"
  }
}