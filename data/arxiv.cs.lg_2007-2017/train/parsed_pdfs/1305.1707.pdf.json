{
  "name" : "1305.1707.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Class Imbalance Problem in Data Mining: Review",
    "authors" : [ "Snehlata S. Dongre", "Latesh Malik" ],
    "emails" : [ "rushi.longadge@gmail.com", "dongre.sneha@gmail.com", "latesh.malik@raisoni.net" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "In many real time applications large amount of data is generated with skewed distribution. A data set said to be highly skewed if sample from one class is in higher number than other [1] [16]. In imbalance data set the class having more number of instances is called as major class while the one having relatively less number of instances are called as minor class [16]. Applications such as medical diagnosis prediction of rare but important disease is very important than regular treatment. Similar\nsituations are observed in other areas, such as detecting fraud in banking operations, detecting network intrusions [10], managing risk and predicting failures of technical equipment. In such situation most of the classifier are biased towards the major classes and hence show very poor classification rates on minor classes. It is also possible that classifier predicts everything as major class and ignores the minor class. various techniques have been proposed to solve the problems associated with class imbalance [9], which divided into three basic categories, the algorithmic approach, data-preprocessing and feature selection approach. In data-preprocessing technique sampling is applied on data in which either new samples are added or existing samples are removed. Process of adding new sample in existing is known as over-sampling and process of removing a sample known as under-sampling. Second method for solving class imbalance problem is creating or modifying algorithm. The algorithms include the costsensitive method and recognition-based approaches, kernel-based learning, such as support vector machine (SVM) and radial basis function [16]. Applying an algorithm alone is not good idea because size of data and class imbalance ratio is high and hence a new technique i.e. the combination of sampling method with algorithm is used [12]. In classification, algorithm generally gives more important to correctly classify the majority class samples. In many applications miss-classifying a rare event can be\nresult in more serious problem than common event [11]. For example in medical diagnosis in case of cancerous cell detection, misclassifying non-cancerous cells may leads to some additional clinical testing but misclassifying cancerous cells leads to very serious health risks. However in classification problems with imbalanced data, the minority class examples are more likely to be misclassified than the majority class examples, due to their design principles, most of the machine learning algorithms optimizes the overall classification accuracy which results in misclassification minority classes. The paper is organized as follows: section 2 contains current approaches which gives basic techniques that used to solve the problem of imbalance dataset. Section 3 gives the review of related work that handle class imbalance problem. Section 4 gives comparative study of some algorithm and finally end with concluding conclusion in Section 5."
    }, {
      "heading" : "2. Current approaches",
      "text" : "The literature survey suggests many algorithm and techniques that solve the problem of imbalance distribution of sample. These approaches are mainly dividing into three methods such as sampling, algorithms, and feature selection.\n2.1 Sampling\nSampling techniques used to solve the problems with the distribution of a dataset, sampling techniques involve artificially re-sampling the data set, it also known as data preprocessing method. Sampling can be achieved by two ways, Under-sampling the majority class, oversampling the minority class, or by combining over and undersampling techniques. . Under-sampling: The most important method in undersampling is random under-sampling method which trying to balance the distribution of class by randomly removing majority class sample. Figure 1 show the random undersampling method [4]. The problem with this method is loss of valuable information.\nFig.1.Randomly removes the majority sample.\nOver-sampling: Random Oversampling methods also help to achieve balance class distribution by replication minority class sample.\nFig.2. Replicate the minority class samples There is no need to add extra information, it reuse the data [12]. This problem can be solving by generating new synthetic data of minority sample. SMOTE generates synthetic minority examples to over-sample the minority class. In this method learning process consume more time because original data set contain very small number of minority samples. 2.2 Algorithms A several new algorithms have been created for solving the class imbalance problem. The goal of this approach is to optimize the performance of learning algorithm on unseen data. One-class learning methods recognized the sample belongs to that class and reject others. Under certain condition such as multi-dimensional data set oneclass learning gives better performance than others [5]. Instated of changing class distribution applying cost in decision making is another away to improve the performance of classifier. Cost-sensitive learning methods try to maximize a loss function associated with a data set. These learning methods are motivated by the finding that most real-world applications do not have uniform costs for misclassifications. The actual costs associated with each kind of error are unknown typically,\nso these methods need to determine the cost matrix based on the data and apply that to the learning stage. A closely related idea to cost-sensitive learners is shifting the bias of a machine to favor the minority class [8]. The goal of cost sensitive classification is to minimize the cost of misclassification, which can be realized by choosing the class with the minimum conditional risk. Table 1 gives the cost matrix which contains two classes i&j. λij cost of misclassification. Diagonal element are Zero indicate that cost of correct classification has no cost. Another algorithmic approach for skewed distribution of data is modifying the classifier [8]. Kernelbased approach borrows the idea of support vector machines to map the imbalanced dataset into a higher dimension space. Then by combining with oversampling technique or ensemble method, the classifier is supposedly to perform much better than learning from the original dataset.\nIn terms of SVMs, several changes have been made to improve their class prediction accuracy and result suggests that SVM have ability to solve the problem of skewed vector without introducing noise [9]. Boosting methods can be combined with SVMs very effectively in the presence of imbalanced data [16]. 2.3 Feature Selection The goal of feature selection, in general, is to select a subset of j features that allows a classifier to reach optimal performance, where j is a user-defined parameter. For high-dimensional data sets, it uses filters that score each feature independently based on a rule. Feature selection is a key step for many machine learning algorithms, especially when the data is high-dimensional. Because the class imbalance problem is commonly accompanied by the issue of high dimensionality of the data set, hence applying feature selection techniques is essential. Sampling techniques and algorithmic methods may not be enough to solve high dimensional class imbalance problems [5]. Feature selection as a general part of machine learning and data mining algorithms has been thoroughly researched, but its importance to resolving the\nclass imbalance problem is a recent development with most research appearing in the previous several years [18]. In this time period, a number of researchers have conducted research on using feature selection to combat the class imbalance problem. Ertekin [17] studied the performance of feature selection metrics in classifying text data drawn from the Yahoo Web hierarchy. They applied nine different metrics to the data set and measured the power of the best features using the naıve Bayes classifier."
    }, {
      "heading" : "3. Related work",
      "text" : "Data sampling has received much attention in data mining related to class imbalance problem. Data sampling tries to overcome imbalanced class distributions problem by adding samples to or removing sampling from the data set [2]. This method improves the classification accuracy of minority class but, because of infinite data streams and continuous concept drifting, this method cannot suitable for skewed data stream classification. Most existing imbalance learning techniques are only designed for twoclass problem. Multiclass imbalance problem mostly solve by using class decomposition. AdaBoost.NC [1-4] is an ensemble learning algorithm that combines the strength of negative correlation learning and boosting method. This algorithm mainly used in multiclass imbalance data set. The results suggest that AdaBoost.NC combined with random oversampling can improve the prediction accuracy on the minority class without losing the overall performance compared to other existing class imbalance learning methods. Wang et al. proposed the classification algorithm for skewed data stream in [2], which shows that clustering sampling outperforms the traditional undersampling, since clustering helps to reserve more useful information. However, the method cannot detect concept drifting. Chris [2] proposed that both sampling and ensemble technique are effective for improving the classification accuracy of skewed data streams. SVMbased one-class skewed data streams learning method was proposed in [6], which cannot work with concept drifting. Liu et al. [16] proposed one class data streams algorithm, which follows the single classifier approach and can be used to classify text streams. One of the most common data sampling techniques is Random Under-sampling. RUS simply removes examples from the majority class at random until a desired class distribution is achieved. RUSBoost is a new hybrid sampling and boosting algorithm for learning from skewed training data. RUSBoost provides a simpler and faster alternative to SMOTEBoost which is another algorithm that combines boosting and data sampling [2]. RUS decreases the time required to construct a model, which is benefit when creating an ensemble of models that is use in boosting.\nRUSBoost presents a simpler, faster, and less complex than SMOTEBoost for learning from imbalanced data. SMOTEBoost combines a popular oversampling technique (SMOTE) with AdaBoost, resulting in a hybrid technique that increases the performance of its components. Infinitely imbalanced logistic regression [8] a recently developed classification technique that is named infinitely imbalanced logistic regression (IILR) acknowledges the problem of class imbalance in its formulation. Logistic regression (LR) is a commonly used approach for performing binary classification. It learns a set of parameters, {w0, and w1}, that maximizes the likelihood of the class labels for a given set of training data. When the number of data points belonging to one class far exceeds the number belonging to the other class, the standard LR approach can lead to poor classification performance. Cost-sensitive neural networks use sampling and threshold-moving method [8], this technique modify the distribution of training data such that cost of example calculated based on appearance of example. Thresholdmoving tries to move the output threshold toward low cost classes such that examples with higher costs become harder to be misclassified. Threshold-moving is a good choice which is effective on all the data sets and can perform cost-sensitive learning even with seriously imbalanced data sets. Boosting SVM [20] in this algorithm, the classifier is produced from the current weight observation. For given instance, class prediction function which is design in terms of kernel function K. Algorithm calculates the G-mean of classifier by applying different weight and generates new set of classifier. The weight is calculated in iteration of boosting algorithm. Finally, G-mean is used for prediction of good classifier from ensemble classifier. SVM boosting algorithm is still unable to handle the issue of imbalance distribution of data. For online classification of data streams with imbalanced class distribution, Lei [7] proposed an incremental LPSVM termed DCIL-IncLPSVM that has robust learning performance under class imbalance. Linear Proximal support vector machines [LPSVM], like decision trees, classic SVM, etc. are originally not design to handle drifting data streams that exhibit high and varying degrees of class imbalance. Learning from class imbalance data stream, incremental learning algorithm is desirable to pose a capability for dynamic class imbalance learning (DCIL), i.e. learning from data to adjust itself adaptively to handle varied class imbalances. Lei [7] proposes a new incremental learning of wLPSVM for DCIL, where non-stationary imbalanced stream data mining problem is formalized as learning from data chunks of imbalanced class ratio, which are becoming available\nin an incremental manner. The proposed DCILIncLPSVM updates its weights and LPSVM simultaneously whenever a chunk of data is presented or removed."
    }, {
      "heading" : "4. Discussion",
      "text" : "Analysis drawn from comparative study of each of the algorithm is shown in following table.\nMany areas are affected by class imbalance problems. The solution provided by many techniques in data mining is helpful but not enough. The consideration of which technique is best for handling a problem of data distribution is highly depends upon the nature of data used for experiment."
    }, {
      "heading" : "5. Conclusion",
      "text" : "Practically, it is reported that data preprocessing provide better solution than other methods because it allow adding new information or deleting the redundant information, which helps to balance the data. Another method that helpful to solve the problem of class imbalance is boosting. Boosting is powerful ensemble learning algorithm that improved the performance of weak classifier. The algorithm such as RUSBoost, SMOTEBoost is an example of boosting algorithm. Feature selection method can also used for classification of imbalance data. The\nperformance of a feature selection algorithm depends on the nature of the problem. Finally, this paper suggests that applying two or more technique i.e. hybrid approach gives better solution for class imbalance problem."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2013,
    "abstractText" : "In last few years there are major changes and evolution has been done on classification of data. As the application area of technology is increases the size of data also increases. Classification of data becomes difficult because of unbounded size and imbalance nature of data. Class imbalance problem become greatest issue in data mining. Imbalance problem occur where one of the two classes having more sample than other classes. The most of algorithm are more focusing on classification of major sample while ignoring or misclassifying minority sample. The minority samples are those that rarely occur but very important. There are different methods available for classification of imbalance data set which is divided into three main categories, the algorithmic approach, datapreprocessing approach and feature selection approach. Each of this technique has their own advantages and disadvantages. In this paper systematic study of each approach is define which gives the right direction for research in class imbalance problem.",
    "creator" : null
  }
}