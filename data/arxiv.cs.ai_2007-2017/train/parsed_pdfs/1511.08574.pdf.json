{
  "name" : "1511.08574.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Stochastic Process Model of Classical Search",
    "authors" : [ "Dimitri Klimenko", "Hanna Kurniawati" ],
    "emails" : [ "dimitri.klimenko@uqconnect.edu.au", "hannakur@uq.edu.au", "marcusg@uq.edu.au" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ation, with sufficient memory A* is essentially as fast as possible in finding a proven optimal solution. However, in many situations optimal solutions are simply infeasible, and thus search algorithms that trade solution quality for speed are desirable. In this paper, we formalize the process of classical search as a metalevel decision problem, the Abstract Search MDP. For any given optimization criterion, this establishes a well-defined notion of the best possible behaviour for a search algorithm and offers a theoretical approach to the design of algorithms for that criterion. We proceed to approximately solve a version of the Abstract Search MDP for anytime algorithms and thus derive a novel search algorithm, Search by Maximizing the Incremental Rate of Improvement (SMIRI). SMIRI is shown to outperform current state-of-the-art anytime search algorithms on a parametrized stochastic tree model for most of the tested parameter values."
    }, {
      "heading" : "1 Introduction",
      "text" : "Since the early 1960’s, the general idea of best-first search has been fundamental to the design of a great many informed search algorithms. Of particular note is A* (Hart et al., 1968), which remains ubiquitous. At a basic level, all best-first search algorithms work in the same way: they incrementally build a search tree, in which nodes represent states in the problem’s state space, and edges represent state transitions. The key atomic task in this process is edge expansion, which is the addition of a new node to the search tree via a previously unvisited edge.\nThe design of any best-first search algorithm comes down to a single fundamental question—how does the algorithm decide which edge to expand\nar X\niv :1\n51 1.\n08 57\n4v 1\n[ cs\n.A I]\n2 7\nin each iteration? In doing so there are trade-offs between minimizing cost of solutions and the amount of time taken to find the solution. Most work addresses the question in an ad hoc manner, on the basis of diverse heuristic arguments. However, some researchers have taken more formal approaches to such questions; of particular note is recent work (Hay et al., 2012) which formulates the Bayesian selection problem as a metalevel decision problem. Unfortunately, the selection problem formalism is not sufficient to theoretically model the behaviour of tree-searching algorithms. In the words of Hay et al., “A more ambitious goal is extending the formalism to trees—in particular, achieving better sampling at non-root nodes, for which the purpose of sampling differs from that at the root.”\nIn this paper, we propose a decision-theoretic framework, the Abstract Search Markov Decision Process (ASMDP), that achieves this ambitious goal for classical search problems (in which results of actions are fully deterministic). To show its benefit, we apply the framework to derive a novel model-based anytime search algorithm, called Search by Maximizing the Incremental Rate of Improvement (SMIRI). Experimental results on a random tree model indicate that SMIRI offers state-of-the-art anytime performance in this domain."
    }, {
      "heading" : "2 Related Work",
      "text" : ""
    }, {
      "heading" : "2.1 Metareasoning and Models of Search",
      "text" : "Although we would like search algorithms to exhibit perfect rationality, in the real world this is not not practical due to the high computational complexity of many decision problems. To develop the most rational agent under limited computational resources, metareasoning applies decision-theoretic principles. The basic idea is simple—as with object-level decisions, when choosing between computations an agent should select whichever one has the highest expected utility. Matheson (1968) showed that metareasoning can be formalized by combining an object-level model with a model of the computations to form the metalevel decision problem. The Decision-Theoretic A* algorithm (Russell and Wefald, 1988) applies metareasoning to real-time problem-solving search, but the utility estimates used in DTA* cannot obey the standard axioms of probability and utility theory and thus the approach lacks solid theoretical grounding (Russell and Wefald, 1988). More recent work (Hay et al., 2012) has formalized the metalevel decision problem for Bayesian selection problems. However, this formalism can only be applied at the root node of a search tree, and thus is not a full-fledged decision-theoretic framework for tree search algorithms. In contrast, we propose a probabilistic approach that can be applied to tree search.\nProbabilistic models of search have been proposed in the past, but they have typically been used to analyze time complexity of pre-existing\nalgorithms (Karp and Pearl, 1983; Knuth and Moore, 1975; Nau, 1980) rather than to formulate a metalevel decision problem. One exception is the modelling by Mutchler (1986) of search with severely limited edge expansions, who showed theoretically that a simple best-first search algorithm was approximately optimal in a simple random tree model.\nThe aforementioned prior works have offered interesting domains to study the behaviour of search algorithms, and we continue this trend by formulating a novel random tree model that should be more representative of classical search problems. On the other hand another line of research focused on predicting the sizes of search trees has resulted in much more realistic models of search problems, primarily due to a focus on predicting real-world performance. The earliest work on this problem is that of Knuth (1975); later, Korf et al. (2001) were able to accurately predict the number of nodes expanded by the Iterative Deepening A* algorithm (Korf, 1985) on a number of different problems. This work was later extended by Zahavi et al. (2010) to increase its accuracy, by using conditional distributions over what Zahavi et al. refer to as a type system. In essence, the theoretical framework of this paper can be seen as building on the “type system” idea by viewing it as a Bayesian model of the underlying search problem, and building a metalevel decision problem (the Abstract Search MDP) on top of it."
    }, {
      "heading" : "2.2 Existing Algorithms",
      "text" : "As mentioned in Section 1, different requirements with regard to solution quality and execution time result in a spectrum of very different search algorithms to meet them. The A* algorithm dominates one extreme within this spectrum—when used with an admissible heuristic, A* ensures with complete certainty that the solution it returns will be optimal. Moreover, Dechter and Pearl (1985) demonstrate that A* is (down to the tie-breaking criterion) essentially the fastest general algorithm that can make this guarantee—other algorithms can only do better by “cheating” on specific problem instances. However, despite many adaptations of A*, the issue of computational complexity remains hard to evade. For many kinds of problems, the time taken to find optimal solution will grow exponentially (or worse!) with the size of the problem, no matter how good the search algorithm.\nThe only way to avoid this fundamental issue is to relax the optimality requirement, e.g. to w -admissibility which requires the solution to be within a factor w of optimal. For a long time, the best-performing algorithm for this purpose was Weighted A* (Pohl, 1970); this approach multiples the heuristic in A* by a constant factor of w , resulting in an algorithm that typically runs faster than A∗ but is w -admissible. However, it tends to waste time exploring equally meritorious solutions in parallel, even when any one of those solutions would be acceptable (Kowalski, 1972; Pearl and Kim, 1982).\nExplicit Estimation Search (EES) (Thayer and Ruml, 2011) resolves this issue by maintaining multiple queues, so as to maintain admissibility while focusing search effort on particular candidates. An alternative suboptimality criterion is bounded-cost search, which requires an algorithm to find a solution with cost strictly less than a fixed cost bound C . The most prominent algorithm for this criterion is Potential Search (Stern et al., 2011), which aims to expand the node with the highest probability of having a path cost below the bound.\nIn this paper, we focus on the design of anytime algorithms (Dean and Boddy, 1988), which quickly find initial solutions and then gradually improve upon them. A number of anytime heuristic search algorithms have been proposed in the literature; the most basic is Anytime Weighted A* (Hansen et al., 1997), which is Weighted A* but continues to search after a solution is found, and prunes nods that cannot lead to an improvement. Anytime Repairing A* (Likhachev et al., 2003) adapts this by also decreasing the weight every time a solution is found. Finally, the latest state-of-the-art anytime algorithms have made further improvements by obviating the need for tuning the weight parameters; these are APTS/ANA* (Stern et al., 2011; van den Berg et al., 2011), an anytime version of Potential Search, and AEES (Thayer et al., 2012), an anytime version of EES. Experiments on random trees indicate that SMIRI solidly outperforms these other algorithms."
    }, {
      "heading" : "3 Framework",
      "text" : "To define the ASMDP—the proposed metalevel decision problem for classical search—we first define the search problem and abstract search problem, upon which it is built."
    }, {
      "heading" : "3.1 The Classical Search Problem",
      "text" : "Here we use a simplified version of the extensive form game (Osborne and Rubinstein, 1994, p. 89), so that the definition can easily be extended for all perfect information games, including MDPs and stochastic games. This definition is based on the idea of a history, which is a sequence of actions. In particular, for a given set of possible actions A, we denote by A≤ω the set of all possible histories; this can be split into the set of all infinite histories Aω and the set of all finite histories A<ω. Additionally, we use the symbol a to denote appending an action to the end of a history, e.g. h a a.\nAt the core of any problem is a graph structure which we refer to as a search tree;\nDefinition 1. A search tree T is a set of histories which satisfies the following:\n• The empty sequence or root 〈 〉 is a member of T .\n• Any prefix of a sequence in T is also in T .\n• Any infinite history all of whose prefixes are in T must also be in T .\nThus a search tree represents a valid tree structure, in which the nodes of the tree are sequences of actions and the edges are actions.\nDefinition 2. A search problem is a tuple G = (A, T , TGoal, c), where:\n• A is the action space—the finite set of all possible actions.\n• T is a search tree, which we refer to as the complete search tree of the search problem. Histories in T are called legal, and finite legal histories are called states, which are members of the state space U = T ∩A<ω.\n• TGoal ⊂ U is the set of solutions to the search problem.\n• c : U → (0,∞) is the step cost function, which maps each history to the cost of the last step taken. From c, the path cost function g is defined as the sum of c(n ′) over all histories n ′ such that n ′ is a prefix of n.\nNote that we represent the problem by a tree of histories, the complete search tree, rather than the usual state-space graph (Russell and Norvig, 2010). Since the complete tree can be infinite, problems with cyclic state spaces can still be correctly represented; thus, this approach can still be used for searching on graphs, although it does limit the ability of the theoretical model to account for redundant paths."
    }, {
      "heading" : "3.2 The Abstract Search Problem",
      "text" : "Decision-theoretic analysis of classical search may appear to be straightforward, but in reality there is a subtle issue that needs to be resolved (Russell and Wefald, 1991): any information obtained solely as the result of a computation is information that, from the point of view of utility and probability theory, that agent already had. This fundamental issue is a rather difficult one; it continues to be an area of active research, sometimes referred to as the question of “logical uncertainty” (Soares and Fallenstein, 2015).\nTo address the issue, we reformulate the problem into a more standard question of environmental uncertainty, to which Bayesian probability and decision theory can be applied. The idea is based on the simple observation that a search algorithm never actually makes full use of the information available about the states in the search tree. Therefore, a search algorithm only operates on an abstracted representation of a state which we refer to as an abstract state residing in the feature space F . For generality, we consider the feature space to be an arbitrary measurable space, i.e. F = (F ,Φ), where F is the set of all features, and Φ is a σ-algebra of subsets of F , which defines the measurable subsets of F . The information available to the search algorithm is defined by the abstraction function φ : U → F , which maps a\nstate to the features the search algorithm will actually use—a well-known example is the Manhattan distance heuristic function in the sliding tile puzzle. This idea can be seen as building upon the “type system” idea of Lelis et al. (2013) by viewing abstract states as residing in an arbitrary space, and viewing the type system as representing a whole class of problems rather than just one. This means that the same partial search tree can be consistent with distinctly different search problems and thus the algorithm must be uncertain about the true underlying search problem. In this way, the process of search can be formally modeled as a process of Bayesian inference on a probability distribution over search problems. In particular, we view a search algorithm as starting with a prior distribution over search problems, which encodes initial knowledge about how features of states are related to one another; this distribution is continually narrowed via Bayesian updating upon observing the features of states.\nIn order to make Bayesian inference amenable, we require the algorithm’s prior knowledge to satisfy the local directed Markov property over the structure of the search tree. In other words, the distribution of a subtree rooted at a particular node should be conditionally independent of everything outside of that subtree, given the features of that node. Although this might appear to be a severe restriction, it has intuitive appeal and still has the capacity to model complex dependencies by adding extra features to the feature space. Given the local directed Markov property, it follows that the prior can be defined entirely in terms of the conditional distributions of child abstract states given parent abstract states, i.e. a Markov kernel. With this, we define the abstract problem as:\nDefinition 3. An abstract problem is a tuple (A,F ,FGoal, x0, κ, c̄), where\n• A is the action space, as per Definition 2.\n• F = (F ,Φ) is the feature space. This is a measurable space, i.e. F is the set of all possible abstract states, and Φ is a σ-algebra of subsets of F . This becomes the extended feature space F∗ = (F ∗,Φ∗) by adding the illegal state Γ to represent illegal actions.\n• FGoal ⊂ F specifies the goal states. These are analogous to set of solutions TGoal of a non-abstract search problem, but reside in the feature space rather than the state space.\n• x0 ∈ F is the initial state; as with the goal states, this is analogous to the root history 〈 〉 but lies in the feature space.\n• κ : (F ∗ ×A)× Φ∗ → [0, 1] is the transition function, a Markov kernel from state-action pairs to the extended feature space, such that:\n– Any state/action pair must either map to the illegal state with probability 1, in which case it is illegal, or with probability 0, in which case it is legal.\n– Any action leading out of the illegal state is also illegal, i.e. κ(Γ, ·, {Γ}) ≡ 1.\nStates with no legal actions are referred to as a sink states, which includes Γ as well as the terminal states FTerm.\n• c̄ : F → (0,∞) specifies step costs, as in Definition 2.\nFor a given abstract problem (A,F ,FGoal, x0, κ, c̄), the induced tree generation process is a stochastic process {Ψn | n ∈ A<ω}, where each Ψn represents the abstract state associated with a history n.\nDefinition 4. A realization of the tree generation process (or, equivalently, the underlying abstract problem), is a pair (U , φ), where U specifies which histories from A<ω are considered to be legal, and φ : U → F is an abstraction function (per the previous definition), which maps each legal history to an abstract state.\nNotably, a realization (U , φ) fully defines a search problem per Definition 2; the action space is the same, and the complete search tree T is simply U with the addition of any viable infinite histories. The solutions to the realized search problem are TGoal = φ−1(FGoal), and its step cost function is c(n) = c̄(φ(n)). Thus, with the tree generation process we have fulfilled the task of constructing a well-defined prior probability distribution over search problems. Moreover, this distribution factorizes neatly into a tree of conditional distributions (in essence, a finite or infinite Bayesian network); since the features of a state are observed by the search algorithm, a Bayesian update simply replaces unknown values for the random variables Ψn in the conditional distributions with known ones."
    }, {
      "heading" : "3.3 The Abstract Search MDP (ASMDP)",
      "text" : "In the context of the tree generation process as defined in the previous section, the process of search can be viewed as a Partially Observable Markov Decision Process (POMDP) in which the agent is unaware of the true underlying problem, but can observe the abstract states within its search tree.\nHowever, to avoid solving the full-blown POMDP directly, we instead apply the concept of sufficient information states (Hauskrecht, 2000) to define the ASMDP. Due to the Markov assumption (Section 3.2), the structure of the search tree and the observed feature values within that tree are a sufficient statistic.\nDefinition 5. Given an action set A, a partial search tree is a finite set of finite histories H ⊂ A<ω that is also a search tree (per Definition 1). A partial realization (H , φH ) of an abstract problem Ḡ is a partial search tree H and a mapping φH : H → F such that x0 = φH (〈 〉), and for any n a a ∈ H we have κ(φH (n), a, {Γ}) = 0. In other words, it is a partial search tree labelled with abstract states; the condition on κ ensures that all histories in the partial realization must be legal, as illegal histories are not considered part of the search tree.\nNow, we can define the ASMDP.\nDefinition 6. The ASMDP for a given abstract problem Ḡ is an MDP (S ,A ,T ,R, s0, γ) over the space of partial realizations, where\n• S = {H , φH | (H , φH ) is a partial realization of Ḡ} is the state space; the tree H is the current search tree of the searching agent.\n• A = A<ω is the action space. Legal actions in the ASMDP correspond to expanding previously unexpanded out-edges within the search tree H , i.e. an action n a a is legal if and only if n ∈ H , n a a 6∈ H , and κ(φH (n), a, {Γ}) = 0.\n• T is the transition function, which is a Markov kernel. For any given state s = (H , φH ) and legal action n a a, the next state s ′ takes the form s ′ = (H ∪ {n a a}, φH ∪ {(n a a,X )}), where X is a random variable whose distribution is specified by the transition function of the abstract game, κ(φH (n), a, ·).\n• R is the reward function, which specifies the reward obtained by the searching agent for every time step. The choice of reward function will depend upon what we wish to optimize for in the design of the search algorithm.\n• s0 = ({〈 〉}, φ0) is the initial state, where φ0(〈 〉) = x0. In other words, the initial state is a tree consisting of only the root node, labelled with the initial abstract state x0.\n• γ is the discount factor, which depends on the type of search being modeled.\nWithin this formalism, an optimal search algorithm corresponds directly to an optimal policy in the ASMDP.\nFor the case of anytime search, we view the search as being halted at an arbitrary time (which the agent is uncertain about), and evaluated based on the cost of the incumbent solution when the search is terminated. In particular, we assume that there is a constant probability p of the search terminating at time k + 1 given that it has continued for k steps. Notably,\nrather than explicitly build this probability of halting into the ASMDP, we can model this by using a discounting scheme where γ = 1 − p, and the reward is\nR(H , φH ) = −min{Cmax , min n∈TGoal∩H g(n)}, (1)\ni.e. the negation of the cost of the best solution present within the search tree, up to an upper bound of Cmax . This reward is accumulated over time steps, but in conjunction with the discounting scheme the effect is that the total expected reward for the MDP is essentially a weighted average of solution costs at different times, weighted by how likely the search is to stop at that time."
    }, {
      "heading" : "4 Approximately Solving the ASMDP",
      "text" : "Although the ASMDP results in a well-defined notion of what constitutes an optimal search algorithm, most general-purpose MDP solvers are too slow to handle the ASMDP due to state and action spaces that grow hyperexponentially with the number of steps taken.\nHowever, by exploiting the local directed Markov property, we can derive an efficient approximate solver. The idea is analogous to index policies (Gittins and Jones, 1979) in multi-armed bandits: it independently computes a single quantity for each action and selects the action with the greatest index. Our index policy is derived based on the notion of incremental rate of improvement, denoted as r , which is the rate at which the incumbent cost Cinc improves over coming time steps (thus “rate of improvement”), but only in the near future (thus “incremental”). 1\nTo derive an approximate solution to the ASMDP, we first note that an optimal policy of the ASMDP (i.e., an optimal search algorithm) should only ever expand edges that might either lead to a better solution, or be “followed up” on by the policy, as these are the only ways to gain utility. Consequently, the choice to expand an edge can be viewed as a “macro-action” with outcomes o ∈ O ; which are either successes o ∈ Os , in which a better solution is found in the subtree for that edge and the cost bound is reduced, or failures o ∈ Of in which the algorithm switches over to a different subtree without finding an improved solution. In particular, let s be a state of the ASMDP with incumbent cost Cinc , and π be a policy that selects out-edge n a a in state s, and acts optimally otherwise. We describe an outcome o ∈ O by three key parameters: its probability p(o), the subsequent resulting state s ′(o), the amount ∆(o) by which the cost bound is reduced (if at all), and the number of steps t(o) taken to reach that outcome. This means that\n1Thayer et al. (2012) have argued against maximizing the rate of improvement, but their argument applies to long-term improvement and not the short-term criterion used by SMIRI.\nany outcome o is a series of t(o) steps during which the agent receives a reward of −Cinc on every step, until finally reaching a new ASMDP state s ′ which might (or might not) have an improved cost bound. Then the value function of π at state s can be expressed as\nV π(s) = ∑ o p(o)\n[ γt(o)V π(s ′(o))− Cinc 1− γt(o)\n1− γ\n] . (2)\nAs there will typically be a large number of available out-edges with few leading to improvements, for a failed outcome it is likely that V π(s ′) ≈ V π(s). On the other hand, if the solution is improved the algorithm will gain by having a better solution for some time. In principle, the amount of utility this gains could depend on the particular cost bound and state, but as a rough approximation we assume that the utility gained is proportional to the reduction in the cost bound, ∆, i.e. V π(s ′) ≈ V π(s) + ξ∆ for some ξ. Hence Eq. (2) reduces to\nV π(s) ≈\n∑ o p(o) [ γt(o)ξ∆(o)− Cinc 1−γ t(o) 1−γ ] 1− ∑ o p(o)γ t(o) . (3)\nTo further simplify this equation, we also assume that the times t(o) are much smaller than the discounting horizon, such that γt(o) ≈ 1 + t(o) ln γ. In general, we expect that this assumption will be reasonable, as the overall time spent searching should be much greater than the time spent focusing on any one particular subtree for any one particular cost bound. Thus, we expect that Eq. (2) can be approximated as\nV π(s) ≈ −ξ ln γ ∑ o p(o)∆(o)∑ o p(o)t(o) − Cinc 1− γ . (4)\nThe incremental rate of imporvement r is then the expected value of ∆ divided by the expected value of t , i.e.\nr(s,n a a) = ∑ o p(o)∆(o)∑ o p(o)t(o) , (5)\nwhich is also the only term in Eq. (4) that depends on π. Consequently, a policy that always expands the edge with the maximal value of r is an approximately optimal policy for the ASMDP. At this point, we have not yet precisely defined r , since the outcomes o in Eq. (5) depend on the r -values themselves. In particular, under our current definition a “failure” is when the algorithm switches over to a different subtree without improving, and this only happens when all unexpanded edges in the subtree being searched have lower r -values than the best out-edge outside of that subtree.\nAs specified, there is no obvious way to calculate r without knowing the optimal policy, since the stopping criterion for failure introduces an interdependency between r -values of parallel out-edges. However, we can derive an alternative form of r without this flaw by specifying a different stopping criterion for failures o ∈ Of . In particular, define the thresholded incremental rate of improvement rt as the incremental rate of improvement obtained by searching optimally in a particular subtree until all out-edges in that subtree are below the threshold t , i.e. have rt(· · · ) < t . Finally, we define the peak incremental rate of improvement r∗ as the maximum value of rt over all possible thresholds, i.e. r∗(· · · ) = maxt∈[0,∞) rt(· · · ). This corresponds to expanding all edges in a subtree that are equal to or better than the root edge of that subtree. The thresholded and peak r -values still depend on other r -values, but only those of their children; this results in a well-defined (via induction) notion of rt and r∗.\nMoreover, it is clear that selecting whichever out-edge has the highest r∗ value will maximise the overall incremental rate of improvement—a policy that selects another edge first is clearly dominated by a similar policy that first expands the max-r∗ edge up to a threshold of r∗, and then does whatever the former policy did whenever that fails. This policy cannot be improved by including any edges below the threshold, or failing to include any above the threshold, since the form of Eq. (5) means that this necessarily results in a lower rate of improvement.\nFinally, due to the local directed Markov property, the distribution of edges within the subtree for n a a depends only on φ(n) and a, while ∆ additionally depends on the relative cost bound Cinc − g(n) within that subtree. Since the peak rate of improvement does not depend on any other aspects of the ASMDP state s, it can be expressed as a function r∗(Cinc − g(n), φ(n), a). Consequently, a policy that always chooses to expand the out-edge naa with the maximal value of r∗(Cinc−g(n), φ(n), a) is an approximately optimal policy for the ASMDP. This is essentially a best-first search on r∗, although it is important to note that the values of r∗ will change whenever an improved solution is found, reducing the incumbent cost Cinc ."
    }, {
      "heading" : "5 Search by Maximizing the Incremental Rate of Improvement",
      "text" : "In order to make practical use of the approximately optimal policy of Section 4, we need an efficient method for computing the peak incremental rate of improvement r∗ for each equivalence class of edges e = (C , x , a); where C = Cinc−g(n) is the cost bound relative to the parent node of the out-edge, x is the abstract state of that node, and a is the action along the edge. When path costs g(·) and abstract states x are limited to finite sets, Algorithm 1\ndescribes how to pre-compute r∗ in polynomial time. The key idea is that since g(n) can only increase along a path, the relative cost bound C will always decrease; thus r∗ can be computed via dynamic programming along its first argument. In particular, line 1 loops in increasing order of C , ensuring that r∗ values of descendant edge types are always computed before their ancestors.\nThe inner loop on line 2 loops over all x , a pairs for a given value of C , thus covering all possible equivalence classes of edges e = (C , x , a). In order to compute the true r∗ value of e, it is necessary to find which descendants of e fall below the threshold, and sum the possible outcomes over all of those edges. Lines 3-10 of the algorithm set all of the initial values for this calculation, and then lines 11-16 initialise the possible descendants to include with the children of (x , a), using the transition function κ for the distribution of possible next-states y resulting from the x , a-transition. In order to correctly find the optimal threshold value r∗, the value of r∗(C , x , a) is calculated incrementally, gradually including all possible decendants (C − c̄(y), y , b) in order from the highest r∗(· · · ) values to the lowest. This is the core function of the loop over lines 17-22; the process stops as soon as the current value of r∗ exceeds the r∗ values of all unused descendants.\nFor each equivalence class of edges e = (C , x , a), the following variables are used to store cumulative values that are updated as additional descendants are included in the threshold:\n• ps ≈ ∑\no∈Os p(o), the probability of success. • ts ≈ ∑\no∈Os p(o)t(o), the unnormalized expected number of steps in a successful outcome.\n• tf ≈ ∑\no∈Of p(o)t(o), the same for a failed outcome.\n• ∆ ≈ ∑\no∈O p(o)∆(o): the expected improvement.\n• r∗ = ∆ts+tf : the peak incremental rate of improvement.\n• edgesf : the expected frequencies of resulting edges given a failed outcome.\nThese quantities are updated by the ProcessDescendant subroutine, which adds the success and failure statistics of one particular descendant e ′ to the overall stats for e. This also adds new descendants to the queue, since the descendants of e ′ from edgesf (e ′, ∗) are added. Critically, any particular edge e can only be evaluated once in lines 17-22, as they are evaluated in descending order and the descendants of any edge always have lower r∗ values than the edge itself.\nAlso essential is that the quantities ps , ts , . . . are are approximated. Rather than computing them for hyper-exponentially many possibilities for\nexpanded subtrees and taking expectations over those, we view the subtrees as having only a single distinct outcome for each possible child abstract state y resulting from the transition (x , a), i.e. all y ∈ F | κ(x , a, {y}) > 0. For this, we also store the quantities p(y) and t(y) to represent the total probability of failure and aggregate time spent on failure within the subtree for y . Within each of those subtrees, out-edges that may or may not appear are evaluated as though they occur the expected number of times, represented by non-integer exponents in lines 28 and 29 of Algorithm 1. Following this assumption, the expected frequencies for each type of edge e, descendant edge e ′ and child state y are stored in the array freq(e, y , e ′), which is updated on lines 27 and 37.\nOverall, if D is the number of distinct possible values that g(·) can take, then W = D |F| |A| is the number of possible distinct equivalence classes of edges or (C , x , a) tuples; the final output of SMIRI is a lookup table consisting of at most W entries. SMIRI has worst-case space complexity of O(W 2) (needed to store the descendant frequencies edgesf ), and worst-case time complexity of O(|F|W 3 logW ). For homogeneous actions this reduces further as W = D |F|, and for typical cases with sparse transition functions the leading |F| term reduces to a constant K ."
    }, {
      "heading" : "6 Experiments",
      "text" : "In order to evaluate the performance of SMIRI, a random binary tree model T (p, h0) was designed to exhibit the typical characteristics of classical search problems. We define T as having a feature space of the natural numbers N, initial state h0, goal state 0, and legal actions Left and Right having cost c̄ ≡ 1. The transition function for T (p, ·) from state h with either action leads to state h − 1 with probability p and h + 1 with probability 1− p. This model has two key properties that make it a good testbed for anytime classical search. First of all, the goal state can occur many times at many different depths in the tree, resulting in solutions that vary widely in quality. Secondly, for this problem φ is, in a natural manner, the most\naccurate admissible and consistent heuristic that can be constructed from the feature space. Thus, although SMIRI does not require any kind of admissible heuristic, the model T (p, h0) offers a framework within which methods that rely on admissibility can also be evaluated. This model was implemented with 7 anytime classical search algorithms:\n• SMIRI, using r∗ as precomputed by Algorithm 1.\n• Anytime Potential Search (APTS/ANA*), as per Stern et al. (2014), which selects the node with maximal C−g(n)h(n) .\n• Anytime Generalized Potential Search (AGPTS), as per Stern et al. (2014). For AGPTS, we explicitly precomputed a table for the potential PTC (n) for all n and C .\n• Anytime Explicit Estimation Search (AEES), as per Thayer et al. (2012). For AEES, we precomputed an unbiased inadmissible heuristic ĥ ≡ d̂ by calculating the expected value of the shortest-cost path from any node given h.\n• Anytime Repairing A* (ARA*) per Likachev et al. (2003); with weights 5, 3, 2, 1.5, 1 per Richter et al. (2010).\nPerformance was evaluated in terms of solution cost vs number of edges expanded in the search, thus the details of hardware and algorithm implementations should not be relevant to the results. Although using step counts instead of time neglects the relative overhead of the different algorithms, the average time per step was very similar for most of them; the only notable exception was EES, which can be several times slower per expansion if node generation operations are cheap (Thayer and Ruml, 2011).\nEach of the algorithms was run on 6 different sets of parameter values, which are specified in detail in Table 1. The cost bound Cmax was chosen to make it relatively easy to find an improved initial solution in each test case, whereas the edge expansion limit N was chosen to allow the higherquality algorithms sufficient time to closely approach optimal solutions where possible. The discount factor was then chosen as γ = 1− 2/N (γN ≈ 0.135). Tables for r∗, PTC and ĥ were precomputed, as was the expected cost of an optimal solution E [Copt ], which was used to estimate suboptimality as calculating optimal solution costs for particular instances quickly becomes impractical. For each test case 1000 instances (except in test cases 1 and 2, which only used 100) of T (p, h0) were created as generative models—this is necessary since instances of T (p, h0) are typically infinite and cannot be explicitly instantiated. Each algorithm was then run on each instance of the random tree model for N iterations, recording the sequences of improved solutions and the time step at which each was attained. Note that each model instance was shared between all of the algorithms, which significantly reduces variance in the relative performance of different algorithms due to luck on particular model instances.\nTable 1 depicts the profile of incumbent solution cost C vs. the number of edges expanded for each algorithm and test case. For easier comparison, the y-axes are presented as the estimated suboptimality, i.e. C/E [Copt ], while the x -axes are the number of edge expansions divided by the maximum N allowed for each test case. Finally, Table 2 depicts mean discounted total cost; these values are normalized, with the total discounted cost for each policy being divided by the expected cost of a hypothetical “perfectly rational” anytime algorithm which has a solution of mean quality E [Copt ] after 0 time steps."
    }, {
      "heading" : "7 Discussion",
      "text" : "The decreasing differences between algorithms indicate that the test cases become progressively easier from 1 to 6, with a large decrease in difficulty\nbetween p = 0.6 and p = 0.4; this suggests the model may have a complexity transition at p = 0.5, as with the random tree model of Karp and Pearl (Karp and Pearl, 1983) Overall, SMIRI and APTS/ANA* are quite clearly the bestperforming algorithms, particularly for the hardest test cases. By contrast, ARA* performed poorly for these cases, suggesting that for difficult problems it is much better to use algorithms that don’t rely on parameter tuning.\nAs can be seen in cases 1, 2, and 3, APTS appears to level off at highercost solutions than SMIRI, and thus is likely could be far slower to attain solutions of similar quality than SMIRI. More critically, there is a significant disparity between APTS and AGPTS, which is a significant theoretical concern for APTS, because if one follows the theoretical derivation given by Stern et al. (2014) AGPTS does the “correct thing” in always selecting for maximum potential PTC , whereas the linear-relative assumption APTS relies on clearly fails for the random tree model we have used. In general the results for APTS in this domain are much better than those of AGPTS, although for later timesteps in case 3 AGPTS overtakes APTS in solution quality. The most likely explanation for this issue is one that has been raised by Stern et al. themselves: although AGPTS correctly selects for maximum potential, this is not actually the correct thing for an anytime search algorithm to do. In particular, considering only the probability of a solution in a given subtree neglects the expected search effort to find a solution there. For example, a node with 10 20-step step branches, each of which has a 50% chance of being a solution path, has a 99.9% being part of a solution, whereas a node with a single 5-step branch that has a 90% chance of being a solution path has a 90% chance of being part of a solution. Clearly the potential of the first node is higher, but a reasonable search algorithm should always expand the latter node first as it is likely to lead to a better solution than the former, and more quickly to boot.\nThe idea of minimising search effort is one of the key motivations for AEES (Thayer and Ruml, 2011), but unfortunately it performed significantly worse than SMIRI or APTS and sometimes ARA*. It is difficult to say why this occurs, but it is likely that the problem is related to the nature of the inadmissible heuristic ĥ—it appears that, despite being an unbiased estimator, the statistical properties of ĥ and/or the underlying random tree model cause issues for AEES. Nevertheless, AEES has one key advantage, which is the use of a distance-to-go estimate d(n) to better estimate search effort; this does not give any advantage in our test domains since every edge has unit cost, and thus d̂ ≡ ĥ. However, SMIRI also makes an explicit distinction between estimates of cost and distance, as evidenced by the distinct quantities ∆, ts and tf in Algorithm 1.\nIn summary, SMIRI was the best overall algorithm in this test domain, with a strong edge over all other algorithms except APTS (over which it has only a slight edge). The very narrow losses to APTS in test cases 5 and 6 suggest that both algorithms may be acting quite close to optimally;\nthe approximations made in Sections 4 and 5 are likely to be the cause of suboptimality in SMIRI. More critically, the results for AGPTS demonstrate that the good performance for APTS comes in spite of the justification by Stern et al. for selecting on the basis of potential, and not because of it. This is highlighted by the benchmarking results of Thayer et al. (2012), which show AEES significantly outperforming APTS in all but one domain. On the whole, SMIRI has shown solid results in the synthetic benchmark, while also lacking some of the theoretical shortcomings of APTS."
    }, {
      "heading" : "8 Conclusion",
      "text" : "Overall, the experimental results for SMIRI are quite promising, although SMIRI is not without limitations—it requires pre-processing time, and is limited to domains in which the possible path costs (up to Cmax ) and the abstract states are finite sets. However, such pre-processing costs can become insignificant when compared to a large search problem or amortized over many searches that share an abstract model. Furthermore, one possible approach that could resolve both of these issues is functional approximation methods, which could be used to capture the structure of r∗. A greater problem with the results is that they are limited to an artificial problem domain, rather than real-world problems or typical benchmark problems from the literature. It would be very informative to examine the performance of SMIRI in more realistic problem domains, and particularly ones with non-uniform edge costs to check the effectiveness of SMIRI’s use of estimated search effort.\nHowever, the main purpose of this paper is not to evaluate the SMIRI algorithm, but rather to demonstrate the effectiveness of applying explicit metareasoning techniques to the problem of classical search. The benchmarks, although limited, are solid evidence that metareasoning in general, and the ASMDP in particular, are a useful tool for developing better search algorithms. In particular, this paper demonstrates how formal decision-theoretic methods can be used to formulate the problem of heuristic search as a metalevel decision problem with a well-defined optimal solution, rather than relying on ad hoc heuristic arguments to justify a particular method. Furthermore, although the benchmarking here is limited, many of the latest results in probabilistic models of deterministic search (Lelis et al., 2014) indicate that such models can be effectively applied in estimating the performance of search algorithms, and there is little reason to believe that the additional step of applying metareasoning to such models is fundamentally flawed.\nAlthough SMIRI is derived from a particular application of the ASMDP framework to anytime search, the ASMDP framework applies more generally to other kinds of design criteria for search algorithms. Moreover, the core framework should not be difficult to extend to search with non-classical\nelements such as adversaries or stochastic environments. A far more serious limitation is the assumption of a tree structure and the local directed Markov property in the construction of the tree generation process, as this restricts the ASMDP’s ability to handle correlations that are induced by cycles in graph-structured problems. Nevertheless, the success of probabilistic prediction methods for tree search in domains with cyclic graphs (Lelis et al., 2014) indicates that such models can be useful even if they don’t account for those factors. Moreover, despite this limitation of the theory, SMIRI can still be adapted to function as a graph search algorithm in the usual way, by adding a closed list and re-expanding edges when necessary. On the whole, both the theoretical and experimental results of this paper are quite promising, and indicate a clear need both for further experimental results with the SMIRI algorithm, as well as a broader theoretical investigation of the ASMDP framework.\nAlgorithm 1 SMIRI: computing r∗ 1: for C ← possible path costs from 0 to Cmax do 2: for all legal (x , a) ∈ F ×A do 3: e ← (C , x , a) 4: ts(e)← 0; tf (e)← 0 5: edgesf (e, ∗)← 0 6: if x ∈ FGoal then 7: ps(e)← 1; ∆(e)← C ; r∗(e)←∞ 8: continue 9: ps(e)← 0; ∆(e)← 0; r∗(e)← 0 10: queue ← ∅; freq(e, ∗, ∗)← 0 11: for all y ∈ F : py = κ(x , a, {y}) > 0 do 12: p(y)← py ; t(y)← 1 13: for all legal actions b from y do 14: e ′ ← (C − c̄(y), y , b) 15: queue ← queue ∪ {(C − c̄(y), y , b)} 16: freq(e, y , e ′)← 1 17: while maxe′∈queue r∗(e ′) ≥ r∗(e) do 18: e ′ ← arg maxe′∈queue r∗(e ′) 19: Remove e ′ from queue 20: ProcessDescendant(e, e’) 21: tf (e)← ∑ y∈F p(y)t(y) 22: r∗(e)← ∆(e)/(ts(e) + tf (e)) 23: for all y , e ′ : m = freq(e, y , e ′) > 0 do 24: edgesf (e, e ′)← edgesf (e, e ′) + m p(y)1−ps(e) 25: procedure ProcessDescendant(e, e ′) 26: for all y ∈ F : m = freq(e, y , e ′) > 0 do 27: freq(e, y , e ′)← 0 28: psuc ← 1− (1− ps(e ′))m\n29: tsuc ← ∑m−1\nk=0\n[ ts(e ′) + ktf (e ′) ps(e ′) 1−ps(e′) ] [1− ps(e ′)]k\n30: ps(e)← ps(e) + p(y)psuc 31: ts(e)← ts(e) + p(y) (tsuc + psuct(y)) 32: ∆(e)← ∆(e) + p(y)∆(e ′)psuc/ps(e ′) 33: p(y)← p(y)(1− psuc) 34: t(y)← t(y) + tf (e ′)/(1− ps(e ′)) 35: for all e ′′ : m2 = edgesf (e ′, e ′′) > 0 do 36: Add e ′′ to the queue if it isn’t there. 37: freq(e, y , e ′′)← freq(e, y , e ′′) + mm2"
    } ],
    "references" : [ {
      "title" : "An analysis of time-dependent planning",
      "author" : [ "Thomas L. Dean", "Mark S. Boddy" ],
      "venue" : "In Proc. AAAI. St. Paul, MN, August 21-26,",
      "citeRegEx" : "Dean and Boddy.,? \\Q1988\\E",
      "shortCiteRegEx" : "Dean and Boddy.",
      "year" : 1988
    }, {
      "title" : "Generalized best-first search strategies and the optimality of a",
      "author" : [ "Rina Dechter", "Judea Pearl" ],
      "venue" : "J. ACM,",
      "citeRegEx" : "Dechter and Pearl.,? \\Q1985\\E",
      "shortCiteRegEx" : "Dechter and Pearl.",
      "year" : 1985
    }, {
      "title" : "A dynamic allocation index for the discounted multiarmed bandit problem",
      "author" : [ "J.C. Gittins", "D.M. Jones" ],
      "venue" : null,
      "citeRegEx" : "Gittins and Jones.,? \\Q1979\\E",
      "shortCiteRegEx" : "Gittins and Jones.",
      "year" : 1979
    }, {
      "title" : "Anytime heuristic search: First results",
      "author" : [ "Eric A. Hansen", "Shlomo Zilberstein", "Victor A. Danilchenko" ],
      "venue" : "Technical Report 97-50,",
      "citeRegEx" : "Hansen et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Hansen et al\\.",
      "year" : 1997
    }, {
      "title" : "A formal basis for the heuristic determination of minimum cost paths",
      "author" : [ "Peter E. Hart", "Nils J. Nilsson", "Bertram Raphael" ],
      "venue" : "IEEE Trans. Systems Science and Cybernetics,",
      "citeRegEx" : "Hart et al\\.,? \\Q1968\\E",
      "shortCiteRegEx" : "Hart et al\\.",
      "year" : 1968
    }, {
      "title" : "Value-function approximations for partially observable markov decision processes",
      "author" : [ "Milos Hauskrecht" ],
      "venue" : "JAIR, 13:33–94,",
      "citeRegEx" : "Hauskrecht.,? \\Q2000\\E",
      "shortCiteRegEx" : "Hauskrecht.",
      "year" : 2000
    }, {
      "title" : "Selecting computations: Theory and applications",
      "author" : [ "Nicholas Hay", "Stuart J. Russell", "David Tolpin", "Solomon Eyal Shimony" ],
      "venue" : "In Proc. of UAI, August 14-18,",
      "citeRegEx" : "Hay et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Hay et al\\.",
      "year" : 2012
    }, {
      "title" : "Searching for an optimal path in a tree with random costs",
      "author" : [ "Richard M. Karp", "Judea Pearl" ],
      "venue" : "Artif. Intell.,",
      "citeRegEx" : "Karp and Pearl.,? \\Q1983\\E",
      "shortCiteRegEx" : "Karp and Pearl.",
      "year" : 1983
    }, {
      "title" : "Estimating the efficiency of backtrack programs",
      "author" : [ "Donald E. Knuth" ],
      "venue" : "Math. Comp.,",
      "citeRegEx" : "Knuth.,? \\Q1975\\E",
      "shortCiteRegEx" : "Knuth.",
      "year" : 1975
    }, {
      "title" : "An analysis of alpha-beta pruning",
      "author" : [ "Donald E. Knuth", "Ronald W. Moore" ],
      "venue" : "Artif. Intell.,",
      "citeRegEx" : "Knuth and Moore.,? \\Q1975\\E",
      "shortCiteRegEx" : "Knuth and Moore.",
      "year" : 1975
    }, {
      "title" : "Depth-first iterative-deepening: An optimal admissible tree search",
      "author" : [ "Richard E. Korf" ],
      "venue" : "Artif. Intell.,",
      "citeRegEx" : "Korf.,? \\Q1985\\E",
      "shortCiteRegEx" : "Korf.",
      "year" : 1985
    }, {
      "title" : "Time complexity of iterative-deepening-a",
      "author" : [ "Richard E. Korf", "Michael Reid", "Stefan Edelkamp" ],
      "venue" : "Artif. Intell.,",
      "citeRegEx" : "Korf et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Korf et al\\.",
      "year" : 2001
    }, {
      "title" : "And-or graphs, theorem-proving graphs and bi-directional graphs",
      "author" : [ "Robert Kowalski" ],
      "venue" : "In Machine Intelligence 7, Proceedings of the Seventh Machine Intelligence Workshop,",
      "citeRegEx" : "Kowalski.,? \\Q1972\\E",
      "shortCiteRegEx" : "Kowalski.",
      "year" : 1972
    }, {
      "title" : "Predicting the size of ida*’s search",
      "author" : [ "Levi H.S. Lelis", "Sandra Zilles", "Robert C. Holte" ],
      "venue" : "tree. Artif. Intell.,",
      "citeRegEx" : "Lelis et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Lelis et al\\.",
      "year" : 2013
    }, {
      "title" : "Predicting optimal solution cost with conditional probabilities predicting optimal solution cost",
      "author" : [ "Levi H.S. Lelis", "Roni Stern", "Ariel Felner", "Sandra Zilles", "Robert C. Holte" ],
      "venue" : "Ann. Math. Artif. Intell.,",
      "citeRegEx" : "Lelis et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Lelis et al\\.",
      "year" : 2014
    }, {
      "title" : "Ara*: Anytime a* with provable bounds on suboptimality",
      "author" : [ "Maxim Likhachev", "Geoffrey J. Gordon", "Sebastian Thrun" ],
      "venue" : "NIPS",
      "citeRegEx" : "Likhachev et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Likhachev et al\\.",
      "year" : 2003
    }, {
      "title" : "The economic value of analysis and computation",
      "author" : [ "James E. Matheson" ],
      "venue" : "IEEE Trans. Systems Science and Cybernetics,",
      "citeRegEx" : "Matheson.,? \\Q1968\\E",
      "shortCiteRegEx" : "Matheson.",
      "year" : 1968
    }, {
      "title" : "Optimal allocation of very limited search resources",
      "author" : [ "David Mutchler" ],
      "venue" : "In Proc. AAAI. Philadelphia, PA, August 11-15,",
      "citeRegEx" : "Mutchler.,? \\Q1986\\E",
      "shortCiteRegEx" : "Mutchler.",
      "year" : 1986
    }, {
      "title" : "Pathology on game trees: A summary of results",
      "author" : [ "Dana S. Nau" ],
      "venue" : "In Proc. AAAI. Stanford University, August 18-21,",
      "citeRegEx" : "Nau.,? \\Q1980\\E",
      "shortCiteRegEx" : "Nau.",
      "year" : 1980
    }, {
      "title" : "A course in game theory",
      "author" : [ "Martin J Osborne", "Ariel Rubinstein" ],
      "venue" : "MIT press,",
      "citeRegEx" : "Osborne and Rubinstein.,? \\Q1994\\E",
      "shortCiteRegEx" : "Osborne and Rubinstein.",
      "year" : 1994
    }, {
      "title" : "Studies in semi-admissible heuristics",
      "author" : [ "Judea Pearl", "Jin H. Kim" ],
      "venue" : "IEEE Trans. Pattern Anal. Mach. Intell.,",
      "citeRegEx" : "Pearl and Kim.,? \\Q1982\\E",
      "shortCiteRegEx" : "Pearl and Kim.",
      "year" : 1982
    }, {
      "title" : "Heuristic search viewed as path finding in a graph",
      "author" : [ "Ira Pohl" ],
      "venue" : "Artif. Intell.,",
      "citeRegEx" : "Pohl.,? \\Q1970\\E",
      "shortCiteRegEx" : "Pohl.",
      "year" : 1970
    }, {
      "title" : "The joy of forgetting: Faster anytime search via restarting",
      "author" : [ "Silvia Richter", "Jordan Tyler Thayer", "Wheeler Ruml" ],
      "venue" : "In Proc. ICAPS",
      "citeRegEx" : "Richter et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Richter et al\\.",
      "year" : 2010
    }, {
      "title" : "Decision-theoretic control of reasoning: General theory and an application to game-playing",
      "author" : [ "Stuart Russell", "Eric Wefald" ],
      "venue" : "Technical Report UCB/CSD-88-435,",
      "citeRegEx" : "Russell and Wefald.,? \\Q1988\\E",
      "shortCiteRegEx" : "Russell and Wefald.",
      "year" : 1988
    }, {
      "title" : "Artificial Intelligence - A Modern Approach (3",
      "author" : [ "Stuart J. Russell", "Peter Norvig" ],
      "venue" : "internat. ed.). Pearson Education,",
      "citeRegEx" : "Russell and Norvig.,? \\Q2010\\E",
      "shortCiteRegEx" : "Russell and Norvig.",
      "year" : 2010
    }, {
      "title" : "Do the right thing - studies in limited rationality",
      "author" : [ "Stuart J. Russell", "Eric Wefald" ],
      "venue" : null,
      "citeRegEx" : "Russell and Wefald.,? \\Q1991\\E",
      "shortCiteRegEx" : "Russell and Wefald.",
      "year" : 1991
    }, {
      "title" : "Questions of reasoning under logical uncertainty",
      "author" : [ "Nate Soares", "Benja Fallenstein" ],
      "venue" : "Technical Report 2015-1, Machine Intelligence Research Institute,",
      "citeRegEx" : "Soares and Fallenstein.,? \\Q2015\\E",
      "shortCiteRegEx" : "Soares and Fallenstein.",
      "year" : 2015
    }, {
      "title" : "Potential-based bounded-cost search and anytime nonparametric a",
      "author" : [ "Roni Stern", "Ariel Felner", "Jur van den Berg", "Rami Puzis", "Rajat Shah", "Ken Goldberg" ],
      "venue" : "Artif. Intell.,",
      "citeRegEx" : "Stern et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Stern et al\\.",
      "year" : 2014
    }, {
      "title" : "Potential search: A boundedcost search algorithm",
      "author" : [ "Roni Tzvi Stern", "Rami Puzis", "Ariel Felner" ],
      "venue" : "In Proc. ICAPS",
      "citeRegEx" : "Stern et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Stern et al\\.",
      "year" : 2011
    }, {
      "title" : "Bounded suboptimal search: A direct approach using inadmissible estimates",
      "author" : [ "Jordan Tyler Thayer", "Wheeler Ruml" ],
      "venue" : "IJCAI",
      "citeRegEx" : "Thayer and Ruml.,? \\Q2011\\E",
      "shortCiteRegEx" : "Thayer and Ruml.",
      "year" : 2011
    }, {
      "title" : "Better parameter-free anytime search by minimizing time between solutions",
      "author" : [ "Jordan Tyler Thayer", "J. Benton", "Malte Helmert" ],
      "venue" : "In Proc. SOCS",
      "citeRegEx" : "Thayer et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Thayer et al\\.",
      "year" : 2012
    }, {
      "title" : "Anytime nonparametric A",
      "author" : [ "Jur van den Berg", "Rajat Shah", "Arthur Huang", "Kenneth Y. Goldberg" ],
      "venue" : "In Proc. AAAI",
      "citeRegEx" : "Berg et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Berg et al\\.",
      "year" : 2011
    }, {
      "title" : "Predicting the performance of ida* using conditional distributions",
      "author" : [ "Uzi Zahavi", "Ariel Felner", "Neil Burch", "Robert C. Holte" ],
      "venue" : "JAIR, 37:41–83,",
      "citeRegEx" : "Zahavi et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Zahavi et al\\.",
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "Of particular note is A* (Hart et al., 1968), which remains ubiquitous.",
      "startOffset" : 25,
      "endOffset" : 44
    }, {
      "referenceID" : 6,
      "context" : "However, some researchers have taken more formal approaches to such questions; of particular note is recent work (Hay et al., 2012) which formulates the Bayesian selection problem as a metalevel decision problem.",
      "startOffset" : 113,
      "endOffset" : 131
    }, {
      "referenceID" : 16,
      "context" : "Matheson (1968) showed that metareasoning",
      "startOffset" : 0,
      "endOffset" : 16
    }, {
      "referenceID" : 23,
      "context" : "The Decision-Theoretic A* algorithm (Russell and Wefald, 1988) applies metareasoning to real-time problem-solving search, but the utility estimates used in DTA* cannot obey the standard axioms of probability and utility theory and thus the approach lacks solid theoretical grounding (Russell and Wefald, 1988).",
      "startOffset" : 36,
      "endOffset" : 62
    }, {
      "referenceID" : 23,
      "context" : "The Decision-Theoretic A* algorithm (Russell and Wefald, 1988) applies metareasoning to real-time problem-solving search, but the utility estimates used in DTA* cannot obey the standard axioms of probability and utility theory and thus the approach lacks solid theoretical grounding (Russell and Wefald, 1988).",
      "startOffset" : 283,
      "endOffset" : 309
    }, {
      "referenceID" : 6,
      "context" : "More recent work (Hay et al., 2012) has formalized the metalevel decision problem for Bayesian selection problems.",
      "startOffset" : 17,
      "endOffset" : 35
    }, {
      "referenceID" : 7,
      "context" : "algorithms (Karp and Pearl, 1983; Knuth and Moore, 1975; Nau, 1980) rather than to formulate a metalevel decision problem.",
      "startOffset" : 11,
      "endOffset" : 67
    }, {
      "referenceID" : 9,
      "context" : "algorithms (Karp and Pearl, 1983; Knuth and Moore, 1975; Nau, 1980) rather than to formulate a metalevel decision problem.",
      "startOffset" : 11,
      "endOffset" : 67
    }, {
      "referenceID" : 18,
      "context" : "algorithms (Karp and Pearl, 1983; Knuth and Moore, 1975; Nau, 1980) rather than to formulate a metalevel decision problem.",
      "startOffset" : 11,
      "endOffset" : 67
    }, {
      "referenceID" : 10,
      "context" : "(2001) were able to accurately predict the number of nodes expanded by the Iterative Deepening A* algorithm (Korf, 1985) on a number of different problems.",
      "startOffset" : 108,
      "endOffset" : 120
    }, {
      "referenceID" : 7,
      "context" : "algorithms (Karp and Pearl, 1983; Knuth and Moore, 1975; Nau, 1980) rather than to formulate a metalevel decision problem. One exception is the modelling by Mutchler (1986) of search with severely limited edge expansions, who showed theoretically that a simple best-first search algorithm was approximately optimal in a simple random tree model.",
      "startOffset" : 12,
      "endOffset" : 173
    }, {
      "referenceID" : 7,
      "context" : "algorithms (Karp and Pearl, 1983; Knuth and Moore, 1975; Nau, 1980) rather than to formulate a metalevel decision problem. One exception is the modelling by Mutchler (1986) of search with severely limited edge expansions, who showed theoretically that a simple best-first search algorithm was approximately optimal in a simple random tree model. The aforementioned prior works have offered interesting domains to study the behaviour of search algorithms, and we continue this trend by formulating a novel random tree model that should be more representative of classical search problems. On the other hand another line of research focused on predicting the sizes of search trees has resulted in much more realistic models of search problems, primarily due to a focus on predicting real-world performance. The earliest work on this problem is that of Knuth (1975); later, Korf et al.",
      "startOffset" : 12,
      "endOffset" : 863
    }, {
      "referenceID" : 7,
      "context" : "algorithms (Karp and Pearl, 1983; Knuth and Moore, 1975; Nau, 1980) rather than to formulate a metalevel decision problem. One exception is the modelling by Mutchler (1986) of search with severely limited edge expansions, who showed theoretically that a simple best-first search algorithm was approximately optimal in a simple random tree model. The aforementioned prior works have offered interesting domains to study the behaviour of search algorithms, and we continue this trend by formulating a novel random tree model that should be more representative of classical search problems. On the other hand another line of research focused on predicting the sizes of search trees has resulted in much more realistic models of search problems, primarily due to a focus on predicting real-world performance. The earliest work on this problem is that of Knuth (1975); later, Korf et al. (2001) were able to accurately predict the number of nodes expanded by the Iterative Deepening A* algorithm (Korf, 1985) on a number of different problems.",
      "startOffset" : 12,
      "endOffset" : 890
    }, {
      "referenceID" : 7,
      "context" : "algorithms (Karp and Pearl, 1983; Knuth and Moore, 1975; Nau, 1980) rather than to formulate a metalevel decision problem. One exception is the modelling by Mutchler (1986) of search with severely limited edge expansions, who showed theoretically that a simple best-first search algorithm was approximately optimal in a simple random tree model. The aforementioned prior works have offered interesting domains to study the behaviour of search algorithms, and we continue this trend by formulating a novel random tree model that should be more representative of classical search problems. On the other hand another line of research focused on predicting the sizes of search trees has resulted in much more realistic models of search problems, primarily due to a focus on predicting real-world performance. The earliest work on this problem is that of Knuth (1975); later, Korf et al. (2001) were able to accurately predict the number of nodes expanded by the Iterative Deepening A* algorithm (Korf, 1985) on a number of different problems. This work was later extended by Zahavi et al. (2010) to increase its accuracy, by using conditional distributions over what Zahavi et al.",
      "startOffset" : 12,
      "endOffset" : 1092
    }, {
      "referenceID" : 21,
      "context" : "For a long time, the best-performing algorithm for this purpose was Weighted A* (Pohl, 1970); this approach multiples the heuristic in A* by a constant factor of w , resulting in an algorithm that typically runs faster than A∗ but is w -admissible.",
      "startOffset" : 80,
      "endOffset" : 92
    }, {
      "referenceID" : 12,
      "context" : "However, it tends to waste time exploring equally meritorious solutions in parallel, even when any one of those solutions would be acceptable (Kowalski, 1972; Pearl and Kim, 1982).",
      "startOffset" : 142,
      "endOffset" : 179
    }, {
      "referenceID" : 20,
      "context" : "However, it tends to waste time exploring equally meritorious solutions in parallel, even when any one of those solutions would be acceptable (Kowalski, 1972; Pearl and Kim, 1982).",
      "startOffset" : 142,
      "endOffset" : 179
    }, {
      "referenceID" : 1,
      "context" : "Moreover, Dechter and Pearl (1985) demonstrate that A* is (down to the tie-breaking criterion) essentially the fastest general algorithm that can make this guarantee—other algorithms can only do better by “cheating” on specific problem instances.",
      "startOffset" : 10,
      "endOffset" : 35
    }, {
      "referenceID" : 29,
      "context" : "Explicit Estimation Search (EES) (Thayer and Ruml, 2011) resolves this issue by maintaining multiple queues, so as to maintain admissibility while focusing search effort on particular candidates.",
      "startOffset" : 33,
      "endOffset" : 56
    }, {
      "referenceID" : 28,
      "context" : "The most prominent algorithm for this criterion is Potential Search (Stern et al., 2011), which aims to expand the node with the highest probability of having a path cost below the bound.",
      "startOffset" : 68,
      "endOffset" : 88
    }, {
      "referenceID" : 0,
      "context" : "In this paper, we focus on the design of anytime algorithms (Dean and Boddy, 1988), which quickly find initial solutions and then gradually improve upon them.",
      "startOffset" : 60,
      "endOffset" : 82
    }, {
      "referenceID" : 3,
      "context" : "A number of anytime heuristic search algorithms have been proposed in the literature; the most basic is Anytime Weighted A* (Hansen et al., 1997), which is Weighted A* but continues to search after a solution is found, and prunes nods that cannot lead to an improvement.",
      "startOffset" : 124,
      "endOffset" : 145
    }, {
      "referenceID" : 15,
      "context" : "Anytime Repairing A* (Likhachev et al., 2003) adapts this by also decreasing the weight every time a solution is found.",
      "startOffset" : 21,
      "endOffset" : 45
    }, {
      "referenceID" : 28,
      "context" : "Finally, the latest state-of-the-art anytime algorithms have made further improvements by obviating the need for tuning the weight parameters; these are APTS/ANA* (Stern et al., 2011; van den Berg et al., 2011), an anytime version of Potential Search, and AEES (Thayer et al.",
      "startOffset" : 163,
      "endOffset" : 210
    }, {
      "referenceID" : 30,
      "context" : ", 2011), an anytime version of Potential Search, and AEES (Thayer et al., 2012), an anytime version of EES.",
      "startOffset" : 58,
      "endOffset" : 79
    }, {
      "referenceID" : 24,
      "context" : "Note that we represent the problem by a tree of histories, the complete search tree, rather than the usual state-space graph (Russell and Norvig, 2010).",
      "startOffset" : 125,
      "endOffset" : 151
    }, {
      "referenceID" : 25,
      "context" : "Decision-theoretic analysis of classical search may appear to be straightforward, but in reality there is a subtle issue that needs to be resolved (Russell and Wefald, 1991): any information obtained solely as the result of a computation is information that, from the point of view of utility and probability theory, that agent already had.",
      "startOffset" : 147,
      "endOffset" : 173
    }, {
      "referenceID" : 26,
      "context" : "This fundamental issue is a rather difficult one; it continues to be an area of active research, sometimes referred to as the question of “logical uncertainty” (Soares and Fallenstein, 2015).",
      "startOffset" : 160,
      "endOffset" : 190
    }, {
      "referenceID" : 13,
      "context" : "This idea can be seen as building upon the “type system” idea of Lelis et al. (2013) by viewing abstract states as residing in an arbitrary space, and viewing the type system as representing a whole class of problems rather than just one.",
      "startOffset" : 65,
      "endOffset" : 85
    }, {
      "referenceID" : 5,
      "context" : "However, to avoid solving the full-blown POMDP directly, we instead apply the concept of sufficient information states (Hauskrecht, 2000) to define the ASMDP.",
      "startOffset" : 119,
      "endOffset" : 137
    }, {
      "referenceID" : 2,
      "context" : "The idea is analogous to index policies (Gittins and Jones, 1979) in multi-armed bandits: it independently computes a single quantity for each action and selects the action with the greatest index.",
      "startOffset" : 40,
      "endOffset" : 65
    }, {
      "referenceID" : 30,
      "context" : "1Thayer et al. (2012) have argued against maximizing the rate of improvement, but their argument applies to long-term improvement and not the short-term criterion used by SMIRI.",
      "startOffset" : 1,
      "endOffset" : 22
    }, {
      "referenceID" : 27,
      "context" : "• Anytime Potential Search (APTS/ANA*), as per Stern et al. (2014), which selects the node with maximal C−g(n) h(n) .",
      "startOffset" : 47,
      "endOffset" : 67
    }, {
      "referenceID" : 27,
      "context" : "• Anytime Generalized Potential Search (AGPTS), as per Stern et al. (2014). For AGPTS, we explicitly precomputed a table for the potential PTC (n) for all n and C .",
      "startOffset" : 55,
      "endOffset" : 75
    }, {
      "referenceID" : 30,
      "context" : "• Anytime Explicit Estimation Search (AEES), as per Thayer et al. (2012). For AEES, we precomputed an unbiased inadmissible heuristic ĥ ≡ d̂ by calculating the expected value of the shortest-cost path from any node given h.",
      "startOffset" : 52,
      "endOffset" : 73
    }, {
      "referenceID" : 22,
      "context" : "5, 1 per Richter et al. (2010).",
      "startOffset" : 9,
      "endOffset" : 31
    }, {
      "referenceID" : 29,
      "context" : "exception was EES, which can be several times slower per expansion if node generation operations are cheap (Thayer and Ruml, 2011).",
      "startOffset" : 107,
      "endOffset" : 130
    }, {
      "referenceID" : 7,
      "context" : "5, as with the random tree model of Karp and Pearl (Karp and Pearl, 1983) Overall, SMIRI and APTS/ANA* are quite clearly the bestperforming algorithms, particularly for the hardest test cases.",
      "startOffset" : 51,
      "endOffset" : 73
    }, {
      "referenceID" : 29,
      "context" : "The idea of minimising search effort is one of the key motivations for AEES (Thayer and Ruml, 2011), but unfortunately it performed significantly worse than SMIRI or APTS and sometimes ARA*.",
      "startOffset" : 76,
      "endOffset" : 99
    }, {
      "referenceID" : 7,
      "context" : "5, as with the random tree model of Karp and Pearl (Karp and Pearl, 1983) Overall, SMIRI and APTS/ANA* are quite clearly the bestperforming algorithms, particularly for the hardest test cases. By contrast, ARA* performed poorly for these cases, suggesting that for difficult problems it is much better to use algorithms that don’t rely on parameter tuning. As can be seen in cases 1, 2, and 3, APTS appears to level off at highercost solutions than SMIRI, and thus is likely could be far slower to attain solutions of similar quality than SMIRI. More critically, there is a significant disparity between APTS and AGPTS, which is a significant theoretical concern for APTS, because if one follows the theoretical derivation given by Stern et al. (2014) AGPTS does the “correct thing” in always selecting for maximum potential PTC , whereas the linear-relative assumption APTS relies on clearly fails for the random tree model we have used.",
      "startOffset" : 36,
      "endOffset" : 752
    }, {
      "referenceID" : 27,
      "context" : "More critically, the results for AGPTS demonstrate that the good performance for APTS comes in spite of the justification by Stern et al. for selecting on the basis of potential, and not because of it. This is highlighted by the benchmarking results of Thayer et al. (2012), which show AEES significantly outperforming APTS in all but one domain.",
      "startOffset" : 125,
      "endOffset" : 274
    }, {
      "referenceID" : 14,
      "context" : "probabilistic models of deterministic search (Lelis et al., 2014) indicate that such models can be effectively applied in estimating the performance of search algorithms, and there is little reason to believe that the additional step of applying metareasoning to such models is fundamentally flawed.",
      "startOffset" : 45,
      "endOffset" : 65
    }, {
      "referenceID" : 14,
      "context" : "Nevertheless, the success of probabilistic prediction methods for tree search in domains with cyclic graphs (Lelis et al., 2014) indicates that such models can be useful even if they don’t account for those factors.",
      "startOffset" : 108,
      "endOffset" : 128
    } ],
    "year" : 2015,
    "abstractText" : "Among classical search algorithms with the same heuristic information, with sufficient memory A* is essentially as fast as possible in finding a proven optimal solution. However, in many situations optimal solutions are simply infeasible, and thus search algorithms that trade solution quality for speed are desirable. In this paper, we formalize the process of classical search as a metalevel decision problem, the Abstract Search MDP. For any given optimization criterion, this establishes a well-defined notion of the best possible behaviour for a search algorithm and offers a theoretical approach to the design of algorithms for that criterion. We proceed to approximately solve a version of the Abstract Search MDP for anytime algorithms and thus derive a novel search algorithm, Search by Maximizing the Incremental Rate of Improvement (SMIRI). SMIRI is shown to outperform current state-of-the-art anytime search algorithms on a parametrized stochastic tree model for most of the tested parameter values.",
    "creator" : "LaTeX with hyperref package"
  }
}