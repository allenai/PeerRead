{
  "name" : "1509.05765.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "“Oddball SGD”: Novelty Driven Stochastic Gradient Descent for Training Deep Neural Networks",
    "authors" : [ "Andrew J.R. Simpson" ],
    "emails" : [ "Andrew.Simpson@Surrey.ac.uk" ],
    "sections" : [ {
      "heading" : null,
      "text" : "most popular of the machine learning methods applied to training deep neural networks (DNN) today. It has recently been demonstrated that SGD can be statistically biased so that certain elements of the training set are learned more rapidly than others. In this article, we place SGD into a feedback loop whereby the probability of selection is proportional to error magnitude. This provides a novelty-driven oddball SGD process that learns more rapidly than traditional SGD by prioritising those elements of the training set with the largest novelty (error). In our DNN example, oddball SGD trains some 50x faster than regular SGD.\nIndex terms—Deep learning, novelty detection, dropout,\nparallel dither.\nI. INTRODUCTION\nA less-than-obvious founding assumption of stochastic gradient descent (SGD) is that the learning necessary for each element of the training set should be ‘uniform’. I.e., the paths of descent should be of similar characteristics across the training set. Thus, by iteratively and randomly updating the weights for the various training examples, a stochastic search is conducted which converges on a general minimum. However, if the assumption of uniform learning does not hold, then some proportion of steps taken might be unnecessary or even counter productive. Thus, it might be useful to let the DNNs state of knowledge drive the SGD process.\nDuring training, the error in predictions (made at the output layer) is computed with respect to the supervision data (e.g., classification data) for back propagation. This error might be interpreted as a measure of novelty – what the network cannot predict it does not know, so unknown can be interpreted as novel in the context of prior learning (or forgetting [1]). It has been demonstrated [1] that SGD may be biased towards selective learning of specific elements of a training set by frequentist statistical biases representing the probability of a given training element being selected for an SGD update step. By combining this statistical selectivity with the measure of novelty (prediction error), we may prioritise the most novel training examples for update steps during SGD.\nIn this article, we introduce oddball SGD – a noveltydriven SGD process which selectively learns those elements of the training set which are least well predicted. Using\nparallel dither [2,3] to enable non-batch SGD, we show that oddball SGD speeds up learning by around 50x.\nII. METHOD\nAs example case, we use the well-known computer vision problem of hand-written digit classification using the MNIST dataset [4]. For the input layer we unpacked the images of 28x28 pixels into vectors of length 784. An example digit is given in Fig. 1. Pixel intensities were normalized to zero mean. Replicating Hinton’s [5] architecture, but using the biasedsigmoid [6] activation function (which is optimised for demodulation), we built a fully connected network of size 784x100x10 units, where the 10-unit softmax output layer corresponds to the 10-way digit classification problem.\nOperating within the so-called ‘small-data regime’ (as in [3]), we used only the first 256 training examples of the MNIST dataset and tested on the full 10,000 test examples. We trained several instances of the model with non-batch SGD (equivalent to a batch size of 1 in batch-averaged SGD). The first was a baseline model without regularisation. The second was the baseline model regularised with dropout. The third was the baseline model regularised with dither [2]. The fourth was the baseline model regularised with 100x parallel dither [3]. The fifth was the baseline model regularised using 100x parallel dither w/ dropout [3]. The final model was trained using oddball SGD and regularised with 100x parallel dither w/ dropout [3].\nOddball SGD – novelty. Each training iteration of oddball SGD began with a feed-forward pass over the 256-element training set. Absolute prediction error (the absolute difference between the prediction of the model and the training data for the output layer) was then computed, for each training element,\nin the output layer with respect to the training data. Then, for each element of the training set, the sum of the absolute error (across the 10-way output layer) was computed and placed in a vector (length 256) corresponding to the training examples. This vector represents the state of novelty of each training element.\nNovelty-driven selection statistics. The novelty vector was then normalised so that it summed to 1 (i.e., it could be interpreted in terms of instantaneous probabilities). The resulting normalised selection probability vector was then used to assign instantaneous selection probabilities to each training element (so that selection probability was proportional to the novelty).\nDuring each iterative step of oddball SGD, an element of the training set was randomly selected according to the selection probabilities. Note that this is in contrast to the traditional SGD method where the entire training set is used in random order for each full-sweep iteration.\nParallel dither and dropout. During non-batch SGD, each training example was replicated 100 times to form a parallel\nset. For parallel dither, each element of this set was dithered independently by adding uniform random noise of zero mean and unit scale and the gradients computed for each element independently. For parallel dither w/ dropout [7], both dither and dropout were applied at the same time (i.e., the parallel set was still of size: 100). Then, each parallel set of gradients (representing a single training example) was averaged and applied. Batch averaging across training examples was not applied.\nEach separate instance of the model was trained for 100 full-sweep iterations of non-batch SGD (without momentum) and the test error computed (over the 10,000 test examples) at each iteration. For the oddball SGD model, iterations were counted cumulatively (i.e., a full sweep of SGD is 256 steps) and may be compared like for like (equal number of iterations used).\nFor reliable comparison, each instance of the model was trained from the exact same random starting weights. A learning rate (SGD step size) of 1 was used for all training. All dropout was at the 50% level.\nIII. RESULTS\nFig. 2 plots the test-error rates, as a function of full-sweep SGD iterations, for the various non-batch-SGD trained models and for the oddball SGD trained model. For convenience, the oddball SGD total iteration count is divided by 256, giving equivalent full-sweep cost. The oddball SGD model learns far more rapidly than the equivalent model trained with typical non-batch SGD. In fact, the best non-batch SGD model reaches peak performance at 100 iterations, whereas the oddball SGD model reaches the same performance after the equivalent of around two full-sweep iterations – i.e, it learns\naround 50x faster. Thus, it would seem that non-uniform novelty-based selective learning is effective.\nWe note, in passing, that this entire training method fails without parallel dither [3, see 8]. We also note that (data not shown) the same oddball SGD similarly improved the equivalent ReLU model (as in [8]) but the performance with ReLU was not as good as with the optimally-biased sigmoid [6] function of the present experiments [see 8 for discussion of why].\nIV. DISCUSSION AND CONCLUSION\nIn this paper we have described a novelty-driven learning algorithm – oddball SGD – where the running (dynamic) selection probability for each training element is proportional to the respective prediction error of the model. Thus, we have interpreted prediction error as novelty. Training with oddball SGD resulting in a speed-up of around 50x. Given that similar novelty-driven adaptation has been observed in human perception [9], it may be that a similar learning strategy is used in the brain.\nACKNOWLEDGMENT\nAJRS did this work on the weekends and was supported by his wife and children.\nREFERENCES\n[1] Simpson AJR (2015) “Use it or Lose it: Selective Memory and Forgetting in a Perpetual Learning Machine”, arxiv.org abs/1509.03185. [2] Simpson AJR (2015) “Dither is Better than Dropout for Regularising Deep Neural Networks”, arxiv.org abs/1508.04826. [3] Simpson AJR (2015) “Parallel Dither and Dropout for Regularising Deep Neural Networks”, arxiv.org abs/1508.07130. [4] LeCun Y, Bottou L, Bengio Y, Haffner P (1998) “Gradient-based learning applied to document recognition”, Proc. IEEE 86: 2278–2324. [5] Hinton GE, Osindero S, Teh Y (2006). “A fast learning algorithm for deep belief nets”, Neural Computation 18: 1527–1554. [6] Simpson AJR (2015) “Abstract Learning via Demodulation in a Deep Neural Network”, arxiv.org abs/1502.04042. [7] Hinton GE, Srivastava N, Krizhevsky A, Sutskever I, Salakhutdinov R (2012) “Improving neural networks by preventing co-adaptation of feature detectors”, The Computing Research Repository (CoRR), abs/1207.0580. [8] Simpson AJR (2015) “Taming the ReLU with Parallel Dither in a Deep Neural Network”, arxiv.org abs/1509.05173. [9] Simpson AJR, Harper NS, Reiss JD, McAlpine D (2014) “Selective Adaptation to “Oddball” Sounds by the Human Auditory System”, J Neurosci 34:1963-1969."
    } ],
    "references" : [ {
      "title" : "Use it or Lose it: Selective Memory and Forgetting in a Perpetual Learning Machine”, arxiv.org abs/1509.03185",
      "author" : [ "AJR Simpson" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2015
    }, {
      "title" : "Dither is Better than Dropout for Regularising Deep Neural Networks”, arxiv.org abs/1508.04826",
      "author" : [ "AJR Simpson" ],
      "venue" : null,
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2015
    }, {
      "title" : "Parallel Dither and Dropout for Regularising Deep Neural Networks”, arxiv.org abs/1508.07130",
      "author" : [ "AJR Simpson" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2015
    }, {
      "title" : "Gradient-based learning applied to document recognition",
      "author" : [ "Y LeCun", "L Bottou", "Y Bengio", "P Haffner" ],
      "venue" : "Proc. IEEE",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1998
    }, {
      "title" : "A fast learning algorithm for deep belief nets",
      "author" : [ "GE Hinton", "S Osindero", "Y Teh" ],
      "venue" : "Neural Computation",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2006
    }, {
      "title" : "Abstract Learning via Demodulation in a Deep Neural Network”, arxiv.org abs/1502.04042",
      "author" : [ "AJR Simpson" ],
      "venue" : null,
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2015
    }, {
      "title" : "Improving neural networks by preventing co-adaptation of feature detectors”, The Computing Research Repository (CoRR), abs/1207.0580",
      "author" : [ "GE Hinton", "N Srivastava", "A Krizhevsky", "I Sutskever", "R Salakhutdinov" ],
      "venue" : null,
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2012
    }, {
      "title" : "Taming the ReLU with Parallel Dither in a Deep Neural Network”, arxiv.org abs/1509.05173",
      "author" : [ "AJR Simpson" ],
      "venue" : null,
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2015
    }, {
      "title" : "Selective Adaptation to “Oddball” Sounds by the Human Auditory System",
      "author" : [ "AJR Simpson", "NS Harper", "JD Reiss", "D McAlpine" ],
      "venue" : "J Neurosci 34:1963-1969",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "This error might be interpreted as a measure of novelty – what the network cannot predict it does not know, so unknown can be interpreted as novel in the context of prior learning (or forgetting [1]).",
      "startOffset" : 195,
      "endOffset" : 198
    }, {
      "referenceID" : 0,
      "context" : "It has been demonstrated [1] that SGD may be biased towards selective learning of specific elements of a training set by frequentist statistical biases representing the probability of a given training element being selected for an SGD update step.",
      "startOffset" : 25,
      "endOffset" : 28
    }, {
      "referenceID" : 1,
      "context" : "Using parallel dither [2,3] to enable non-batch SGD, we show that oddball SGD speeds up learning by around 50x.",
      "startOffset" : 22,
      "endOffset" : 27
    }, {
      "referenceID" : 2,
      "context" : "Using parallel dither [2,3] to enable non-batch SGD, we show that oddball SGD speeds up learning by around 50x.",
      "startOffset" : 22,
      "endOffset" : 27
    }, {
      "referenceID" : 3,
      "context" : "As example case, we use the well-known computer vision problem of hand-written digit classification using the MNIST dataset [4].",
      "startOffset" : 124,
      "endOffset" : 127
    }, {
      "referenceID" : 4,
      "context" : "Replicating Hinton’s [5] architecture, but using the biasedsigmoid [6] activation function (which is optimised for demodulation), we built a fully connected network of size 784x100x10 units, where the 10-unit softmax output layer corresponds to the 10-way digit classification problem.",
      "startOffset" : 21,
      "endOffset" : 24
    }, {
      "referenceID" : 5,
      "context" : "Replicating Hinton’s [5] architecture, but using the biasedsigmoid [6] activation function (which is optimised for demodulation), we built a fully connected network of size 784x100x10 units, where the 10-unit softmax output layer corresponds to the 10-way digit classification problem.",
      "startOffset" : 67,
      "endOffset" : 70
    }, {
      "referenceID" : 2,
      "context" : "Operating within the so-called ‘small-data regime’ (as in [3]), we used only the first 256 training examples of the MNIST dataset and tested on the full 10,000 test examples.",
      "startOffset" : 58,
      "endOffset" : 61
    }, {
      "referenceID" : 1,
      "context" : "The third was the baseline model regularised with dither [2].",
      "startOffset" : 57,
      "endOffset" : 60
    }, {
      "referenceID" : 2,
      "context" : "The fourth was the baseline model regularised with 100x parallel dither [3].",
      "startOffset" : 72,
      "endOffset" : 75
    }, {
      "referenceID" : 2,
      "context" : "The fifth was the baseline model regularised using 100x parallel dither w/ dropout [3].",
      "startOffset" : 83,
      "endOffset" : 86
    }, {
      "referenceID" : 2,
      "context" : "The final model was trained using oddball SGD and regularised with 100x parallel dither w/ dropout [3].",
      "startOffset" : 99,
      "endOffset" : 102
    }, {
      "referenceID" : 6,
      "context" : "For parallel dither w/ dropout [7], both dither and dropout were applied at the same time (i.",
      "startOffset" : 31,
      "endOffset" : 34
    }, {
      "referenceID" : 7,
      "context" : "We also note that (data not shown) the same oddball SGD similarly improved the equivalent ReLU model (as in [8]) but the performance with ReLU was not as good as with the optimally-biased sigmoid [6] function of the present experiments [see 8 for discussion of why].",
      "startOffset" : 108,
      "endOffset" : 111
    }, {
      "referenceID" : 5,
      "context" : "We also note that (data not shown) the same oddball SGD similarly improved the equivalent ReLU model (as in [8]) but the performance with ReLU was not as good as with the optimally-biased sigmoid [6] function of the present experiments [see 8 for discussion of why].",
      "startOffset" : 196,
      "endOffset" : 199
    }, {
      "referenceID" : 8,
      "context" : "Given that similar novelty-driven adaptation has been observed in human perception [9], it may be that a similar learning strategy is used in the brain.",
      "startOffset" : 83,
      "endOffset" : 86
    } ],
    "year" : 2015,
    "abstractText" : "Stochastic Gradient Descent (SGD) is arguably the most popular of the machine learning methods applied to training deep neural networks (DNN) today. It has recently been demonstrated that SGD can be statistically biased so that certain elements of the training set are learned more rapidly than others. In this article, we place SGD into a feedback loop whereby the probability of selection is proportional to error magnitude. This provides a novelty-driven oddball SGD process that learns more rapidly than traditional SGD by prioritising those elements of the training set with the largest novelty (error). In our DNN example, oddball SGD trains some 50x faster than regular SGD.",
    "creator" : "PDFCreator Version 1.7.1"
  }
}