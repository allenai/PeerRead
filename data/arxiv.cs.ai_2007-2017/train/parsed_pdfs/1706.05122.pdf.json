{
  "name" : "1706.05122.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Bib2vec: An Embedding-based Search System for Bibliographic Information",
    "authors" : [ "Takuma Yoneda", "Koki Mori Makoto Miwa", "Yutaka Sasaki" ],
    "emails" : [ "sd14084@toyota-ti.ac.jp", "sd15435@toyota-ti.ac.jp", "makoto-miwa@toyota-ti.ac.jp", "yutaka.sasaki@toyota-ti.ac.jp" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n70 6.\n05 12\n2v 1\n[ cs\n.C L\n] 1\n6 Ju\nn 20\n17\nWe propose a novel embedding model that represents relationships among several elements in bibliographic information with high representation ability and flexibility. Based on this model, we present a novel search system that shows the relationships among the elements in the ACLAnthology Reference Corpus. The evaluation results show that our model can achieve a high prediction ability and produce reasonable search results."
    }, {
      "heading" : "1 Introduction",
      "text" : "Modeling relationships among several types of information, such as nodes in information networks, has attracted great interests in natural language processing (NLP) and data mining (DM), since it can uncover hidden information in data. Topic models such as author-topic model (Rosen-Zvi et al., 2004) have been widely studied to represent relationships among these types of information. These models, however, need a considerable effort to incorporate new types and do not scale well in increasing the number of types since they explicitly model the relationships between types in the generating process.\nWord representation models, such as skipgram and continuous bag-of-word (CBOW) models (Mikolov et al., 2013), have made a great success in NLP. They have been widely used to represent texts, but recent studies started to apply these methods to represent other types of information, e.g., authors or papers in citation networks (Tang et al., 2015).\nWe propose a novel embedding model that represents relationships among several elements in bibliographic information, which is useful to discover hidden relationships such as authors’ interests and similar authors. We built a novel\nsearch system that enables to search for authors and words related to other authors based on the model using the ACL Anthology Reference Corpus (Bird et al., 2008). Based on skip-gram and CBOW models, our system embeds vectors to not only words but also other elements of bibliographic information such as authors and references and provides a great representation ability and flexibility. The vectors can be used to calculate distances among the elements using similarity measures such as the cosine distance and inner products. For example, the distances can be used to findwords or authors related to a specific author. Our model can easily incorporate new types without changing the model structure and scale well in the number of types."
    }, {
      "heading" : "2 Related work",
      "text" : "Most of previous studies on modeling several elements in bibliographic information have been based on topic models such as author-topic model (Rosen-Zvi et al., 2004). Although the models work fairly well, they have comparably low flexibility and scalability since they explicitly model the generation process. Our model employs word representation-based models instead of topic models.\nSome previous studies embedded vectors to the elements. Among them, large-scale information network embedding (LINE) (Tang et al., 2015) embedded a vector to each node in information network. LINE handles single type of information and prepares a network for each element separately. By contrast, our model simultaneously handles all the types of information."
    }, {
      "heading" : "3 Method",
      "text" : "We propose a novel method to represent bibliographic information by embedding vectors to elements based on the skip-gram and CBOWmodels."
    }, {
      "heading" : "3.1 Task definition",
      "text" : "We assume the bibliographic data set have the following structure. The data set is composed of bibliographic information of papers. Each paper consists of several categories. Categories are divided into two groups: a textual category Ψ (e.g., titles and abstracts1) and non-textual categories Φ (e.g., authors and references). Figure 1 illustrates an example structure of bibliographic information of a paper. Each category has one or more elements; the textual category usually has many elements while a non-textual category has a few elements (e.g., authors are not many for a paper)."
    }, {
      "heading" : "3.2 Proposed model",
      "text" : "Our model focuses on a target element, and predicts a context element from the target element. We use only the elements in non-textual categories as contexts to reduce the computational cost. Figure 1 shows the case when we use an element in a non-textual category as a target. For the blackpainted target element in category Φ2, the shaded elements in the same paper are used as its contexts.\nWhen we use elements in the textual category as a target, instead of treating each element as a target, we consider that the textual category has only one element that represents all the elements in the category like CBOW. Figure 1 illustrates the case that we consider the averaged vector of the vectors of all the elements in the textual category as a target.\nWe describe our probabilistic model to predict\na context element e j O from a target e i I in a certain paper. We define two d-dimensional vectors υit and ωit to represent an element e i t as a target and context, respectively. Similarly to the skip-gram model, the probability to predict element e j O in the context from input eiI is defined as follows:\np(ejO|e i I) =\nexp(ωjO·υ i I + β j O)∑\n(ωjs ,β j s)∈Sj\nexp(ωjs·υiI + β j s) ,\ne j O ∈ Φ, e i I ∈ Ψ ∪ Φ, (1)\nwhere βjs denotes a bias corresponds to ω j s , and S j denotes pairs of ωjs and β j s that belong to a category Φj . As we mentioned, our model considers that the textual category Ψ has only one averaged\n1Note that we have only one textual category since the categories for texts are usually not distinguished in most word representation models.\nvector. The vector υjrep can be described as:\nυjrep = 1\nn\nn∑\nq=1\nυjq , e j ∈ Ψ (2)\nOur target loss can be defined as:\n− ∑\n(ea,eb)∈D\nlog p(eb|ea), (3)\nwhere D denotes a set of all the correct pairs of the elements in the data set. To reduce the cost of the summation in Eq. (1), we applied the noisecontrastive estimation (NCE) to minimize the loss (Gutmann and Hyvärinen, 2010)."
    }, {
      "heading" : "3.3 Predicting related elements",
      "text" : "We predict the top k elements related to a query element by calculating their similarities to the query element. We calculate the similarities using one of three similarity measures: the linear function in Eq. (1), dot product, and cosine distance."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Evaluation settings",
      "text" : "We built our data set from the ACL Anthology Reference Corpus version\n20160301 (Bird et al., 2008). The statistics of the data set and our model settings are summarized in Table 1.\nAs pre-processing, we deleted commas and periods that sticked to the tails of words and removed non-alphabetical words such as numbers and brackets from abstracts and titles. We then lowercased the words, and made phrases using the word2phrase tool2.\nWe prepared five categories: author, paper-id, reference, year and text. author consists of the list of authors without distinguishing the order of the authors. paper-id is an unique identifier assigned to each paper, and this mimics the paragraph vector model (Le and Mikolov, 2014). reference includes the paper ids of reference papers in this data set. Although ids in paper-id and reference are shared, we did not assign the same vectors to the ids since they are different categories. year is the publication year of the paper. text includes words and phrases in both abstracts and titles, and it belongs to the textual category Ψ, while each other category is treated as a non-textual category Φi. We regard elements as unknown elements when they appear less than minimum frequencies in Table 1.\nWe split the data set into training and test. We prepared 17,475 papers for training and the remaining 2,000 papers for evaluation. For the test set, we regarded the elements that do not appear in the training set as unknown elements.\nWe set the dimension d of vectors to 300 and\nshow the results with the linear function."
    }, {
      "heading" : "4.2 Evaluation",
      "text" : "We automatically built multiple choice questions and evaluate the accuracy of our model. We also compared some results of our model with those of author-topic model.\nOur method models elements in several categories and allows us to estimate relationships among the elements with high flexibility, but this\n2 https://github.com/tmikolov/word2vec\nmakes the evaluation complex. Since it is tough to evaluate all the possible combinations of inputs and targets, we focused on relationships between authors and other categories. We prepared an evaluation data set that requires to estimate an author from other elements. We removed an (not unknown) author from each paper in the evaluation set to ask the system to predict the removed author considering all the other elements in the paper. To choose a correct author from all the authors can be insanely difficult, so we prepared 10 selection candidates. In order to evaluate the effectiveness of our model, we compared the accuracy on this data set with that by logistic regression. As a result, when we use our model, we got 74.3% (1,486 / 2,000) in accuracy, which was comparable to 74.1% (1,482 / 2,000) by logistic regression.\nTable 2 shows the examples of the search results using our model. The leftmost column shows the authors we input to our model. In the rightmost two columns, we manually picked up words and authors belonging to a certain topic described in Sim et al. (2015) that can be considered to correspond to the input author. This table shows that our model can predict relative words or similar authors favorably well although the words are inconsistent with those by the author topic model.\nFigure 3 shows the screenshot of our system. The lefthand box shows words in the word cloud related to the query and the righthand box shows the close authors. We can input a query by putting it in the textbox or click one of the authors in the righthand box and select a similarity measure by selecting a radio button."
    }, {
      "heading" : "4.3 Discussion",
      "text" : "When we train the model, we did not use elements in category Ψ as context. This reduced the computational costs, but this might disturbed the accuracy of the embeddings. Furthermore, we used the averaged vector for the textual category Ψ, so we do not consider the importance of each word. Our model might ignore the inter-dependency among elements since we applied skip-grams. To resolve these problems, we plan to incorporate attentions (Ling et al., 2015) so that the model can pay more attentions to certain elements that are important to predict other elements.\nWe also found that some elements have several aspects. For example, words related to an author spread over several different tasks in NLP.Wemay\nbe able to model this by embedding multiple vectors (Neelakantan et al., 2014)."
    }, {
      "heading" : "5 Conclusions",
      "text" : "This paper proposed a novel embedding method that represents several elements in bibliographic information with high representation ability and flexibility, and presented a system that can search for relationships among the elements in the bibliographic information. Experimental results in Table 2 show that our model can predict relative words or similar authors favorably well. We plan to extend our model by other modifications such as incorporating attention and embedding multiple vectors to an element. Since this model has high flexibility and scalability, it can be applied to not only papers but also a variety of bibliographic information in broad fields."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We would like to thank the anonymous reviewer for helpful comments and suggestions."
    } ],
    "references" : [ {
      "title" : "The ACL anthology reference corpus: A reference dataset for bib",
      "author" : [ "Bird et al.2008] Steven Bird", "Robert Dale", "Bonnie J. Dorr", "Bryan R. Gibson", "Mark Thomas Joseph", "MinYen Kan", "Dongwon Lee", "Brett Powley", "Dragomir R. Radev", "Yee Fan Tan" ],
      "venue" : null,
      "citeRegEx" : "Bird et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Bird et al\\.",
      "year" : 2008
    }, {
      "title" : "Noise-contrastive estimation: A new estimation principle for unnormalized statistical models",
      "author" : [ "Gutmann", "Hyvärinen2010] Michael Gutmann", "Aapo Hyvärinen" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "Gutmann et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Gutmann et al\\.",
      "year" : 2010
    }, {
      "title" : "Not all contexts are created equal: Better word representations with variable attention",
      "author" : [ "Black", "Isabel Trancoso", "Chu-Cheng Lin." ],
      "venue" : "EMNLP, pages 1367–1372.",
      "citeRegEx" : "Black et al\\.,? 2015",
      "shortCiteRegEx" : "Black et al\\.",
      "year" : 2015
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Ilya Sutskever", "Kai Chen", "Greg S. Corrado", "Jeff Dean" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Mikolov et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Efficient non-parametric estimation of multiple embeddings per word in vector space",
      "author" : [ "Jeevan Shankar", "Alexandre Passos", "Andrew McCallum" ],
      "venue" : null,
      "citeRegEx" : "Neelakantan et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Neelakantan et al\\.",
      "year" : 2014
    }, {
      "title" : "The author-topic model for authors and documents",
      "author" : [ "Thomas L. Griffiths", "Mark Steyvers", "Padhraic Smyth" ],
      "venue" : "In UAI,",
      "citeRegEx" : "Rosen.Zvi et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Rosen.Zvi et al\\.",
      "year" : 2004
    }, {
      "title" : "A utility model of authors in the scientific community",
      "author" : [ "Sim et al.2015] Yanchuan Sim", "Bryan R. Routledge", "Noah A. Smith" ],
      "venue" : "In EMNLP,",
      "citeRegEx" : "Sim et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Sim et al\\.",
      "year" : 2015
    }, {
      "title" : "LINE: large-scale information network embedding",
      "author" : [ "Tang et al.2015] Jian Tang", "Meng Qu", "Mingzhe Wang", "Ming Zhang", "Jun Yan", "Qiaozhu Mei" ],
      "venue" : "In WWW,",
      "citeRegEx" : "Tang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Tang et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : "Topic models such as author-topic model (Rosen-Zvi et al., 2004) have been widely studied to represent relationships among these types of information.",
      "startOffset" : 40,
      "endOffset" : 64
    }, {
      "referenceID" : 3,
      "context" : "Word representation models, such as skipgram and continuous bag-of-word (CBOW) models (Mikolov et al., 2013), have made a great success in NLP.",
      "startOffset" : 86,
      "endOffset" : 108
    }, {
      "referenceID" : 7,
      "context" : ", authors or papers in citation networks (Tang et al., 2015).",
      "startOffset" : 41,
      "endOffset" : 60
    }, {
      "referenceID" : 0,
      "context" : "We built a novel search system that enables to search for authors and words related to other authors based on the model using the ACL Anthology Reference Corpus (Bird et al., 2008).",
      "startOffset" : 161,
      "endOffset" : 180
    }, {
      "referenceID" : 5,
      "context" : "Most of previous studies on modeling several elements in bibliographic information have been based on topic models such as author-topic model (Rosen-Zvi et al., 2004).",
      "startOffset" : 142,
      "endOffset" : 166
    }, {
      "referenceID" : 7,
      "context" : "Among them, large-scale information network embedding (LINE) (Tang et al., 2015) embedded a vector to each node in information network.",
      "startOffset" : 61,
      "endOffset" : 80
    }, {
      "referenceID" : 0,
      "context" : "20160301 (Bird et al., 2008).",
      "startOffset" : 9,
      "endOffset" : 28
    }, {
      "referenceID" : 6,
      "context" : "In the rightmost two columns, we manually picked up words and authors belonging to a certain topic described in Sim et al. (2015) that can be considered to correspond to the input author.",
      "startOffset" : 112,
      "endOffset" : 130
    }, {
      "referenceID" : 4,
      "context" : "be able to model this by embedding multiple vectors (Neelakantan et al., 2014).",
      "startOffset" : 52,
      "endOffset" : 78
    } ],
    "year" : 2017,
    "abstractText" : "We propose a novel embedding model that represents relationships among several elements in bibliographic information with high representation ability and flexibility. Based on this model, we present a novel search system that shows the relationships among the elements in the ACLAnthology Reference Corpus. The evaluation results show that our model can achieve a high prediction ability and produce reasonable search results.",
    "creator" : "LaTeX with hyperref package"
  }
}