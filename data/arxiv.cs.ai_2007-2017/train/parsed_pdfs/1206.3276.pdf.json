{
  "name" : "1206.3276.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Explanation Trees for Causal Bayesian Networks",
    "authors" : [ "Ulf H. Nielsen", "Jean-Philippe Pellet", "André Elissee" ],
    "emails" : [ "uln@zurich.ibm.com", "jep@zurich.ibm.com", "ael@zurich.ibm.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Bayesian networks can be used to extract explanations about the observed state of a subset of variables. In this paper, we explicate the desiderata of an explanation and confront them with the concept of explanation proposed by existing methods. The necessity of taking into account causal approaches when a causal graph is available is discussed. We then introduce causal explanation trees, based on the construction of explanation trees using the measure of causal information ow (Ay and Polani, 2006). This approach is compared to several other methods on known networks."
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "A Bayesian network (BN, Pearl, 1988) is an algebraic tool to compactly represent the joint probability distribution of a set of variables V by exploiting conditional independence amongst variables. It represents all variables in a directed acyclic graph (DAG), where the absence of arcs between nodes denotes (conditional) independence. In addition to graphically representing the structure of the dependencies between the variables, BNs allow inference tasks to be solved more e ciently. In this paper, we discuss the extraction of explanations in causal BNs (Pearl, 2000; Spirtes et al., 2001) BNs where the arcs depict direct cause e ect relationships between variables.\nGenerally, explanations in BNs can be classi ed in three categories (Lacave and Diez, 2002) depending on the focus of the explanation:\n• Explanation of evidence. Given a subset of observed (instantiated) variables O ( V, what is the state of (some of) the other variables V \\ O that best explains O = o?\n• Explanation of the reasoning process. When we have received some evidence and belief states are updated by probabilistic inference, how was the reasoning process by which we arrive at this state?\n• Explanation of the model, which provides insight into the static components of a network such as (conditional) independence relationships, causal mechanisms, etc.\nWe shall focus our attention on the explanation of evidence: we wish to explain why variables in O took on speci c observed values using assignments in V \\ O. To this purpose, we discuss in section 2 the requirements of such an explanation. In section 3, we list the standard approaches to evidence explanation as well as some recent methods to make explanations more concise, and explain some of their drawbacks. We then present causal information trees in section 4, and detail experiments and comparisons in section 5."
    }, {
      "heading" : "NOTATION",
      "text" : "Boldface capitals denote sets of random variables or nodes in a graph, depending on the context. V is the set of all variables in the analysis. Italicized capitals like X, Xi, Y are random variables or nodes and elements of V; calligraphic capitals such as X ,Y are their respective domains. Vectors are denoted boldface lowercase, as e or p; scalars in italics. Unless otherwise stated, the scalars x, y are assumed to be a value of their respective uppercase random variable. The probability distribution of a random variable X is denoted by p(X), and we write p(X = x) or p(x) the probability of x. We only work with discrete variables."
    }, {
      "heading" : "2 AN IDEALIZED EXPLANATION",
      "text" : "Even though many variables O may be observed, the explanation can be focused only on a speci c subset E ⊆ O. The state E = e is then called explanandum.\nThe set of explanatory variables H ( V can include both observed and unobserved variables, and an explanation is an assignment H = h (compatible with O = o for variables both in H and in O).\nall variables V\nobserved variables O = o\nexplanandum E = e\nexplanatory variables H\nWe insist on the distinction between the explanandum e and the observations o (Chajewska and Halpern, 1997). Observations are all our knowledge about the current state of a system, and this might not coincide exactly with what we want explained. Consider for example the case where we wish to know why the grass is wet while we know it has been raining. We do not seek an explanation for why it has rained, only for why the grass is wet. A perfectly valid explanation is that the grass is wet because it is raining if no other factors can su ciently explain the facts.\nAn algorithm respecting this should then determine, for each variable in O, whether its observed state is relevant to explain e, and for each unobserved variable in V \\O, whether knowing its state adds explanatory power to the proposed explanation. This excludes methods which marginalize out O, preventing these variables from being part of an explanation.\nTo explain why a given system is observed in a given state, we must intuitively convey some information about the causal mechanisms that lead to the observation made. If we observe that it is raining and some explanation tells us that it rains because the grass is wet, we do not nd it a good explanation as it contradicts our understanding of how the system works; that the grass being wet cannot make it rain. Suppose we have an explanation H = h for E = e: an intuitive interpretation of this result is that manually setting H = h will be a favourable con guration to observe E = e. As Halpern and Pearl (2005) discuss, explanations need to be causal to be consistent with users' knowledge of the mechanisms of the system. It is therefore important that the explanations are given in a data-generating direction, such that users can infer interventional rules from the given explanations (for instance, if I can make it rain somehow, then I know that the grass will be wet, as opposed to an impossible let me make the grass wet so as to make it rain ).\nCausal methods are subject to the availability of causal information. In this paper, we extract the causal information from causal BNs, but in general, the approach\nis adaptable to any causal model that can predict the e ect of interventions on certain variables.\nIn addition to assuming that the relationships between the variables V can be represented by a fully oriented causal BN, we assume that the corresponding joint probability distribution is faithful and causally su - cient (Pearl, 2000; Spirtes et al., 2001). Faithfulness of the distribution ensures that there is a unique graph whose arcs depict all (conditional) dependencies of the distribution, and only those. Causal su ciency forbids hidden common causes for variables in V, such that we can build a DAG whose arcs represent direct causation. Although most expert-designed BN are naturally oriented causally, the output of structure causal learning algorithms are often partially directed graphs and may need additional expert knowledge to be fully oriented.\nTo summarize, we wish our explanations to give us causal information by detailing the mechanisms that lead to the explanandum, using all the available information we have about the state of the network."
    }, {
      "heading" : "3 EXISTING METHODS",
      "text" : "This section reviews and discusses some of the major techniques to nd explanations."
    }, {
      "heading" : "3.1 MOST PROBABLE EXPLANATION & VARIANTS",
      "text" : "A common noncausal measure of explanatory power is the conditional probability of the explanatory variables H given the explanandum e. The most probable explanation (MPE) approach (Pearl, 1988) then considers h∗ = arg maxh p(h | e) as the best explanation (or, alternatively, looks for the k best explanations by maximizing this probability). The explanandum e is in the case of MPE equal to the full set of observations O = o, and the set H is V \\E. This list can be long and uninformative because of lack of conciseness; moreover, it is hard to distinguish between long explanations, whose respective probabilities are low anyway and close to one another.\nIn the partial abduction approach (Shimony, 1991), the set of explanatory variables is a strict subset H ( V \\ E. The set of variables X = V \\ H \\ E excluded from the explanation is then marginalized out before the maximum is computed: we look for arg maxh ∑ x p(h,x | e). This is the maximum a posteriori (MAP) model approach. The excluded variables X are selected either by a user, or via automated analysis of the network. Automatically selecting the relevant explanatory variable is a nontrivial issue (Shimony, 1991).\nPartial abduction is computationally more expensive than standard MPE, because it cannot be readily solved by message passing algorithms, but approximations exist (e.g., Park, 2002). On the other hand, it globally leads to more concise explanations than MPE.\nFurther e orts to make explanations more concise include de Campos et al. (2001), where the k most probable explanations are found and then simpli ed based on relevance and probabilistic criteria; and Henrion and Druzdzel (1990), where also partial assignments are allowed but only within a prede ned tree that limits the set of possible explanations. An explanation is then a path from the root of the tree to a leaf, denoting variable assignments for each branch taken. This is known as scenario-based explanation. The best explanation is the one with the highest posterior probability.\nThere are several concerns with these approaches MPE/MAP or scenario-based maximizing some conditional probability of the explanatory variables (Chajewska and Halpern, 1997). First, they do not distinguish the explanandum and the observations, such that the additional state information that is not meant to be explained is excluded from a possible explanation. Furthermore, there is no distinction between observing an explanatory variable X in a certain state x, and forcing it to have the value x.1 Thus, depending on the choice of the explanatory variables, the intuitive interpretation (as described in the previous section) stating that setting H = h∗ will be a favourable con guration for observing E = e does not hold.\nMPEs and, to a lesser extent, MAP model explanations, are not robust: little changes in the network will often change the result of the analysis, even though the changes occur in parts of the network largely independent of the explanandum (Chan and Darwiche, 2006). Common to the methods in this subsection is that they order explanations by p(h, e) (this is equivalent to p(h | e) as p(e) is constant for a given e): this joint probability cannot be considered alone to determine the explanatory power of h on e. Some of these problems are illustrated by the experiments in section 5."
    }, {
      "heading" : "3.2 SE ANALYSIS",
      "text" : "In SE analysis, Jensen (2001) additionally considers the sensitivity of an explanation h with respect to the explanandum. Less sensitive explanations ensure that little changes in the network's parameters will not lead to severely di erent explanations, so that the explanation is stable with respect to the speci cation of the network.\n1The di erence between observation and intervention is fundamental to causality and is best described with the example of Simpson's paradox in Pearl (2000), chap. 6.\nSE analysis also works by comparing two explanations hi and hj , usually with Bayes' factor or the likelihood ratio (Je reys, 1961): Bayes' factor =\nposterior ratio\nprior ratio = p(hi | e) / p(hj | e) p(hi) / p(hj) = p(e |hi) p(e |hj) .\nThe empirical interpretation of Bayes' factor given by Je reys (1961) is that if it is less than 1 it is in favor of hj , if less than 3 it is a slight support for hi. If it is between 3 and 12, it is a positive support; and higher than 12, it is a strong support for hi.\nIn Yuan and Lu (2007), Bayes' factor is used to search for explanations consisting of only a few variables by ranking them by their Bayes' factor computed as the ratio between the probability of the explanation given the explanandum and its opposite:\nBayes' factor = p(h | e)\n1− p(h | e) .\nAn exhaustive search is performed over all subsets of the hypothesis, and the explanations are shown to be more concise in a sample network than MPE, Shimony's (1991) MAP, and the simpli cations described by de Campos et al. (2001).\nA similar criticism as before can be applied to these methods: additional observations are discarded, and the causal directionality is ignored in the selection of the relevant explanatory variables."
    }, {
      "heading" : "3.3 EXPLANATION TREES",
      "text" : "The method of Flores (2005) constructs a set of best explanations while at the same time giving a preference for concise explanations, summarizing the results of the analysis in an explanation tree. We describe this method in more detail, as the causal information tree method (described in section 4) is based on a similar representation.\nDe nition 1 An explanation tree for an explanandum E = e is a tree in which every node X is an explanatory variable (with X ∈ V \\E), and each branch out of X is a speci c instantiation x ∈ X of X. A path from the root to a leaf is then a series of assignments X = x, Y = y, · · · , Z = z, summarized as P = p, which constitutes a full explanation.\nFlores's (2005) algorithm, summarized in Algorithm 1, builds such an explanation tree. Starting with an empty tree, the variable to use as the next node is selected to be the one that, given the explanandum, reduces the uncertainty the most in the rest of the explanatory variables according to some measure. The nodes that are on the path being grown are added to\na conditioning set, so that the part of the explanatory space they already account for is taken into account. Two stopping criteria are used to determine when to stop growing the tree: the minimum posterior probability β of the current branch, and the minimum amount of uncertainty reduction α that must be achieved by adding a new variable. Among all explanations represented by the nal tree, the best one is the one with the largest posterior probability p(p | e).\nAlgorithm 1 Flores's (2005) Explanation Tree\n1: function T = ExplanationTree(H, e,p; α, β) Input: H : set of explanatory variables\nE = e : explanandum P = p : path of variable assignments α, β : stopping criteria\nOutput: T : an explanation tree 2: X∗ ← arg maxX∈H P\nY ∈H Inf(X;Y | e,p) 3: if max\nY ∈H\\X∗ Inf(X;Y | e,p) < α or p(p | e) < β then\n4: return ∅ 5: end if\n6: T ← new tree with root X∗ 7: for each x ∈ domain(X∗) do 8: T ′ ← ExplanationTree(H \\X∗, e,p ∪ {x}) 9: add a branch x to T with subtree T ′ and 10: assign it the label p(p, x | e) 11: end for 12: return T\nThe algorithm is parametrized with the measure of uncertainty reduction Inf(X;Y | e,p).2 For our implementation, we used the conditional mutual information. The mutual information I(X;Y | z) is a symmetrical measure of how much reduction in uncertainty about Y we get by knowing X in the context Z = z, and is de ned as: I(X;Y | z) =∑\nx∈X p(x | z) ∑ y∈Y p(y |x, z) log p(y |x, z) p(y | z) . (1)\nIf X and Y are independent given Z = z, we have I(X;Y | z) = 0. IfX fully determines Y , then knowing one is enough to know the other and full information is shared.\nExplanation trees are interesting in that they can present many mutually exclusive explanations in a compact form. Flores (2005) also argues that explanations as constructed by Algorithm 1 are reasonable and more sensible than (k-)MPE in the sense that on simple networks, the returned explanations are those that we expect. Four elements, however, are subject to discussion.\nFirst, on line 2 of Algorithm 1, variables are added to the tree in order of how much information they provide\n2See Flores (2005) for additional cases where max at line 3 is replaced by min or avg, and Inf is the Gini index.\nabout the remaining variables in the set of explanatory variables. But this does not measure the information of the added variables shared with the explanandum. Moreover, the explanandum actually grows as the tree is constructed, since there is no di erence between the constructed path p and e at line 2. Thus, this maximization cannot be interpreted as selecting variables reducing the uncertainty in the explanandum.\nSecond, the algorithm makes no distinction between explanandum and observations. To try to x this, we could either additionally condition on observations O = o, or marginalize out O altogether. The former case is no di erent from adding o to the explanandum e, and the latter case excludes all X ∈ O from explanations, such that both cases are unsatisfactory.\nThird, the criterion to choose the best explanation is the probability of the explanation path given the evidence p(p | e), and not how likely the system is to have produced the evidence we are trying to explain with a con guration p, p(e |p). Both measures are linked, but since several explanations can cover almost an equal share of the explanation space and often only one will be included in the explanation tree, the criterion p(p | e) will miss explanations which could have explained the evidence well, but do not cover as large a fraction of the explanation space.\nFourth, causal considerations are ignored: there is no distinction between ancestors and descendants of a variables, such that we can get explanations of the type it rains because the grass is wet.\nFrom an end-user perspective though, trees are a good solution for representing several competing explanations compactly and readably. We introduce in section 4 a modi ed approach, which can address the issues discussed here."
    }, {
      "heading" : "4 CAUSAL EXPLANATION TREES",
      "text" : "Like the previous method, causal explanation trees take advantage of a tree representation. The tree is grown so as to ensure that explanations in any path are causal: variables can be selected as explanatory only if they causally in uence the explanandum.\nBefore de ning the causal criterion used in this approach, we need to de ne the concept of postintervention distribution (Pearl, 2000, p. 72). A standard conditional probability of the form p(e |x) gives the probability (or probability density) of e when X = x is observed. It does not represent, however, the probability of e if we manually force variableX to have value x. Causally, we are interested in the intervention on X, which we denote by do(X = x), rather the observation of x. In causal BNs, the tool used to evaluate\nthe e ect of these conditionings is Pearl's (1995) docalculus, which uses the structure of the causal graph to evaluate the postintervention distribution.\nDe nition 2 Given a causal Bayesian network B in the sense of Pearl (2000, p. 23) over variables V = {X1, · · ·, Xd}, the postintervention distribution p(v | do(Xi = x′i)), also denoted p(v | x̂′i), after an intervention do(Xi = x′i) can be expressed as:\np(x1, · · ·, xd | x̂′i) =\n{∏ j 6=i p(xj |paj) if xi = x′i,\n0 if xi 6= x′i, (2)\nwhere paj is the values of the graphical parents (i.e., direct causes) of the node Xj in B.\nThe truncated factorization of (2) states that the probability distribution is computed as if the manipulated variable Xi had no incoming causal in uence (i.e., no direct causes), and as if p(Xi = x′i) had probability one. This makes sense, as forcing Xi to have a certain value e ectively ignores its direct causes and natural distribution.\nWith this concept, we can now de ne the causal information ow (Ay and Polani, 2006), which will be our measure of causal contribution of explanatory variables towards our explanandum.\nDe nition 3 The causal information ow from X to Y given the interventions do(Z = z), written I(X→Y | ẑ), is: I(X→Y | ẑ) =\n∑ x∈X p(x | ẑ) ∑ y∈Y p(y | x̂, ẑ) log p(y | x̂, ẑ) p∗(y | ẑ) , (3)\nwhere p∗(y | ẑ) = ∑\nx′∈X p(x′| ẑ)p(y | x̂′, ẑ).\nThe expression I(X→ Y | ẑ) measures the amount of information owing from X to Y if we intervene on Z, setting it to z (i.e., if we block the causal ow on all paths going through Z). Note that (3) is, in essence, similar to (1). For faithful probability distributions, I(X → Y | ẑ) = 0 if and only if all directed paths (if any) from X to Y go through Z in the corresponding causal graph. For binary variables, if Y is a deterministic function of X regardless of Z, then I(X→Y | ẑ) = 1.\nIn our application, we use the causal information ow to decide which variable should be added to the tree being built. This is shown in Algorithm 2. At line 2, we use the do-conditioning on the already build path p, and we allow inputting additional observed variables O in an additional conditioning set. We replace Y from (3) with the state of the explanandum e, suppress the corresponding summation and divide by the\nprior probability p(e |o, p̂), so that we end up computing I(X→e |o, p̂) =∑ x∈X p(x |o, p̂)p(e |o, x̂, p̂) p(e |o, p̂) log p(e |o, x̂, p̂)∑\nx′∈X p(x′|o, p̂)p(e |o, x̂′, p̂)\n,\nensuring that the expected value EE [ I(X→e |o, p̂) ] =∑\ne∈E p(e |o, p̂)I(X→e |o, p̂) equals I(X→E |o, p̂).\nUsing this criterion, the explanation tree is then built as follows: the root node is selected as being arg maxX I(X → e |o); i.e., the node which has the maximum information ow to the state of the explanandum. The important part is that we may condition on o without confusing observation and explanandum. Furthermore, we also allow selection of an observed variable X ∈ O in addition to unobserved variables, consistently with our desiderata. When X ∈ O, it is observed and we know its value x. We must then compute the pointwise causal information ow from x to e, with o′ being o without the observation X = x:\nI(x→e |o′, p̂) = log p(e |o ′, p̂, x̂)∑\nx′∈X p(x′|o′, p̂)p(e |o′, x̂′, p̂)\n.\nThe tree is then grown recursively: for each possible value for X, a branch is added to the root. For each new leaf, the next explanatory variable is selected as being arg maxY I(Y → e |o, x̂), and so on, where the do-conditioning set always re ects the selected variable values from the root to the current leaf. We use only one stopping criterion, the minimum information ow α we accept as a causal information contribution. The algorithm furthermore allows explicitly to restrict the search set for explanatory variables H (defaulting to V \\ {E}). Finally, each leaf is labeled with log ( p(e |o, p̂)/p(e |o) ) (where we make sure that variables selected in p̂ are removed from o if needed). This measures how much performing the interventions p̂ changes the probability of the explanandum (given the observations) with respect to the prior probability of the explanandum. Higher values indicate better explanations; negative values indicate that the probability of the explanandum actually decreases with the proposed explanation.\nUsing the information ow criterion brings us two advantages over standard (conditional) mutual information: rst, we automatically only consider variables that can causally in uence the explanandum. Second, when selecting the ith variable on a tree branch, we take into account the previously selected variables 1 through i − 1 causally, as they enter the conditioning set of variables that have been intervened on.\nIn practice, computing a causal information ow of the type I(X→ e |o, p̂) at line 2 of Algorithm 2 requires\nAlgorithm 2 Causal Explanation Tree\n1: function T = CausalExplTree(H,o, e, p̂; α) Input: H : set of explanatory variables\nO = o : observation set E = e : explanandum\np̂ : path of interventions α : stopping criterion\nOutput: T : a causal explanation tree\n2: X∗ ← arg maxX∈H I(X→e |o, p̂) 3: if I(X∗→e |o, p̂) < α then return ∅\n4: T ← new tree with root X∗ 5: for each x ∈ domain(X∗) do 6: T ′ ← CausalExplTree(H \\ {X∗},o, e, p̂ ∪ {x̂}) 7: add a branch x to T with subtree T ′ and 8: assign it the contribution log ` p(e |o, p̂, x̂)/p(e |o)\n´ 9: end for 10: return T\nto know the distributions p(X |o, p̂), p(E |o, p̂), and p(E |o, X̂, p̂) (i.e., p(E |o, x̂, p̂) for all x ∈ X ). Additionally, p(e |o) is needed to label the leaves, but as it is only dependent on e and o, we compute it only once. We can further avoid unnecessary computations by using the graphical reachability criterion from a candidate nodeX to E, blocking paths going through O∪P. The inference steps were implemented using the factor graph message-passing algorithm (Frey et al., 2001).\nThe complexity of this algorithm, in terms of number of calls to an inference engine per node in the constructed tree, is O(nd), where n is the number of explanatory variables |H| and d is the average domain size of the variables, e.g., 2 for binary variables. For comparison, Flores's (2005) approach is O(n2d2)."
    }, {
      "heading" : "5 EXPERIMENTS",
      "text" : "We compare causal explanation trees (CET) with parameter α = 0 to Most Probable Explanation (MPE), Bayes' factor (BF) following Yuan and Lu (2007), and standard (noncausal) explanation trees (ET) with parameters α = 0.02 and β = 0. We test the approach on three simple networks3 to compare the relevance of explanations. A more extended version of these experiments and comments can be found in Nielsen (2007).\nDrug (Figure 1). This network comes from Pearl (2000, chap. 6). It represents the outcome of an experiment designed to check the e ciency of a new drug on male and female patients. The males have a natural recovery rate of 70%; taking the drug decreases it to 60%. Similarly, 30% of females recover naturally,\n3The conditional probability tables have been omitted in the gures of the two larger BNs due to lack of space and can be found at http://www.zurich.ibm.com/~uln/ causalexpl/.\nbut only 20% when given the drug. Thus, both the absence of drug and being a male can explain a good recovery rate.\nIn Figure 2, we try to explain a recovery. All approaches correctly realize that Sex = m largely accounts for the recovery. However, ET selects Sex = m ∧ Drug = yes as the best explanation according to the leaves' labels, just like MPE. This contradicts the natural idea of explanation, since the drug has a negative impact on the recovery. CET labels the leaves more sensibly: branches where the drug was not given have a higher rank. Moreover, the branches where Sex = f have a negative label, indicating that they actually decrease the probability of recovery. Although the rst two BF explanations are sensible, the third one is mistakenly selects Drug = yes as an explanation.\nAcademe (Figure 3). This network depicts the relationships between various marks given to students following a course. The Final mark is determined by some Other outside factors and an intermediate mark (T.P. mark), which is in turn determined by the student's abilities in Theory and Practice as well as Extra curricular activities in this tested subject.\nIn Figure 4, the explanandum was set to Final mark = fail ; i.e., we want to explain why a student failed the course. T.P. mark and Global mark have been excluded from the possible explanatory variables in the two tree algorithms as they are modeling artifacts.\nET tells us that Theory = bad is the best explanation. We could have expected Practice to also be part of\nalternate explanations, as it in uences the nal mark very similarly to Theory. This is what CET does, including Practice to explain the nal failure when Theory is average or good. MPE includes Practice = good in its long list of states, which does not seem intuitively likely. BF provides more concise explanations, but, like ET, ignores Practice altogether, although a bad practice can account for failure equally well.\nAsia (Figure 5). This network (Lauritzen and Spiegelhalter, 1988) models the relationships between two indicators, X-ray results and dyspnea, of severe diseases for a person. Tuberculosis (more likely if a visit to Asia occurred) and lung cancer (more likely when the person smokes) both increase abnormal Xray results and dyspnea; bronchitis also causes increased dyspnea. TbOrCa is a modeling artifact, excluded from the the analysis in the two tree algorithms.\nIn Figure 6, we try to explain abnormal X-ray results. Whereas both tree algorithms select a Lung cancer as the best explanation, they di er on how to explain\nwhen it is absent : CET selects, justi ably, Tuberculosis, but ET uses Dyspnea and then Bronchitis, which are not causes of X-ray and cannot explain it, especially not when we know that no lung cancer is present. MPE surprisingly excludes a visit to Asia, when this is expected to make abnormal X-rays more likely through Tuberculosis. BF, while still providing very concise explanations, is stuck on the middle node TbOrCa and, like ET, ignore the important Tuberculosis.\nIn Figure 7, we try to explain the presence of dyspnea for a smoker. While CET is still able to select Smoker as explanatory variable, ET can only add this observation to the explanandum and thus cannot select it. Instead, the best explanation according to ET is normal X-rays, which does not seem very likely. Although ET does select the important Lung cancer and Tuberculosis, it ignores the largest factor according to BF and CET, namely Bronchitis. Here too, CET selects the more intuitively interpretable explanations.\n0.343\n-1.396\n-2.037\n0.895\n0.878\nSmoking\nBronchitis\nSmoker\nLung cancer\nAbsent\nLung cancer\nPresent\nTuberculosis\nAbsent\n0.683\nPresent\n-2.124\nAbsent\n0.683\nPresent\nTuberculosis\nAbsent\n1.046\nPresent\n0.876\nAbsent\n1.046\nPresent\n0.201\n0.055\nX-ray result\n0.799\nNormal\nLung cancer\nAbnormal\nTuberculosis\nAbsent\n0.145\nPresent\n0.042\nAbsent\n0.014\nPresent\n(a) Causal explanation tree (b) Explanation tree\n(c) MPE: p(Bronchitis = yes, X-ray = normal, Lung cancer = no, TbOrCa = no, Smoker = yes, Tuberculosis = no, Visit to Asia = no |Dyspnea = yes) = 0.46 (d) BF: BF(Bronchitis = yes) = 6.14 BF(Bronchitis = yes, Visit to Asia = no) = 5.89 BF(Bronchitis = yes, Tuberculosis = no) = 5.84\nFigure 7: Asia: explain Dyspnea = yes |Smoker = yes."
    }, {
      "heading" : "6 CONCLUSION",
      "text" : "We have presented an approach to explanation in causal BNs, causal explanation trees. Explanations are presented as a tree, compactly representing several explanations and making it more readable than a (possibly long) list. Assuming that the BN is causal allows us to use the causal information ow criterion to build the tree. This leads to more sensible explanations, in that we only explain a given state with variables that can causally in uence it. The approach makes an explicit distinction between observation and explanandum. This lets the user input all available knowledge about the network as observation, while still focusing on explaining one of them and allowing the observed variables to be selected as part of a good explanation. The algorithm labels the leaves so as to re ect how a proposed explanation changes the probability of the explanandum, making the tree easy to interpret.\nCausal explanation trees, unlike other techniques, do not condition on the explanandum to maximize the probability of the explanatory variables p(h | e), but focus on p(e |h) instead by means of the causal information ow. This allows it to compare favorably to MPE, Bayes' factor, and Flores's (2005) noncausal explanation trees on the tested networks because the returned explanations are intuitive, and those with a positive label in the tree ensure that they increase the probability of the explanandum."
    }, {
      "heading" : "Ay, N. & Polani, D. (2006). Information ows in causal",
      "text" : "networks. Technical report, Max Planck Institute for Mathematics in the Sciences.\nChajewska, U. & Halpern, J. Y. (1997). De ning explanation in probabilistic systems. In: UAI-97, pages 62 71. Morgan Kaufmann."
    }, {
      "heading" : "Chan, H. & Darwiche, A. (2006). On the robustness of",
      "text" : "most probable explanations. In: UAI-06.\nde Campos, L. M., Gémez, J. A., & Moral, S. (2001). Simplifying explanations in Bayesian belief networks. International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems 9.\nFlores, M. J. (2005). Bayesian networks Inference: Advanced algorithms for triangulation and partial abduction. PhD thesis, Universidad De Castilla-La Mancha.\nFrey, B. J., Kschischang, F. R., & Loeliger, H.-A. (2001). Factor graphs and the sum-product algorithm. IEEE Transactions on Information Theory, 47."
    }, {
      "heading" : "Halpern, J. Y. & Pearl, J. (2005). Causes and explanations:",
      "text" : "A structural-model approach. Part II: Explanations. The British Journal for the Philosophy of Science.\nHenrion, M. & Druzdzel, M. J. (1990). Qualitative propagation and scenario-based scheme for exploiting probabilistic reasoning. In: UAI, pages 17 32.\nJe reys, H. (1961). Theory of Probability. Oxford University Press.\nJensen, F. V. (2001). Bayesian Networks and Decision Graphs. Springer.\nLacave, C. & Diez, F. J. (2002). A review of explanation methods of Bayesian networks. Knowledge Engineering Review, 17(2):107 127.\nLauritzen, S. L. & Spiegelhalter, D. J. (1988). Local computations with probabilities on graphical structures and their application to expert systems. Journal of the Royal Statistical Society, Series B, 50:157 224.\nNielsen, U. H. (2007). On causal explanations in Bayesian networks. Master's thesis, IT University of Copenhagen.\nPark, J. D. (2002). Map complexity results and approximation methods. In: UAI-02, pages 388 396.\nPearl, J. (1988). Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan Kaufmann.\nPearl, J. (1995). Causal diagrams for empirical research. Biometrika, 82(4):669 709.\nPearl, J. (2000). Causality: Models, Reasoning, and Inference. Cambridge University Press.\nShimony, S. E. (1991). Explanation, irrelevance, and statistical independence. In: AAAI, pages 482 487.\nSpirtes, P., Glymour, C., & Scheines, R. (2001). Causation, Prediction, and Search, Second Edition. The MIT Press."
    }, {
      "heading" : "Yuan, C. & Lu, T.-C. (2007). Finding explanations in",
      "text" : "bayesian networks. In: The 18th International Workshop on Principles of Diagnosis."
    } ],
    "references" : [ {
      "title" : "Information ows in causal networks. Technical report, Max Planck Institute for Mathematics in the Sciences",
      "author" : [ "N. Ay", "D. Polani" ],
      "venue" : null,
      "citeRegEx" : "Ay and Polani,? \\Q2006\\E",
      "shortCiteRegEx" : "Ay and Polani",
      "year" : 2006
    }, {
      "title" : "De ning explanation in probabilistic systems",
      "author" : [ "U. Chajewska", "J.Y. Halpern" ],
      "venue" : "In: UAI-97,",
      "citeRegEx" : "Chajewska and Halpern,? \\Q1997\\E",
      "shortCiteRegEx" : "Chajewska and Halpern",
      "year" : 1997
    }, {
      "title" : "On the robustness of most probable explanations",
      "author" : [ "H. Chan", "A. Darwiche" ],
      "venue" : null,
      "citeRegEx" : "Chan and Darwiche,? \\Q2006\\E",
      "shortCiteRegEx" : "Chan and Darwiche",
      "year" : 2006
    }, {
      "title" : "Simplifying explanations in Bayesian belief networks",
      "author" : [ "L.M. de Campos", "J.A. Gémez", "S. Moral" ],
      "venue" : "International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems",
      "citeRegEx" : "Campos et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Campos et al\\.",
      "year" : 2001
    }, {
      "title" : "Bayesian networks Inference: Advanced algorithms for triangulation and partial abduction",
      "author" : [ "M.J. Flores" ],
      "venue" : "PhD thesis,",
      "citeRegEx" : "Flores,? \\Q2005\\E",
      "shortCiteRegEx" : "Flores",
      "year" : 2005
    }, {
      "title" : "Factor graphs and the sum-product algorithm",
      "author" : [ "B.J. Frey", "F.R. Kschischang", "Loeliger", "H.-A" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "Frey et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Frey et al\\.",
      "year" : 2001
    }, {
      "title" : "Causes and explanations: A structural-model approach. Part II: Explanations",
      "author" : [ "J.Y. Halpern", "J. Pearl" ],
      "venue" : "The British Journal for the Philosophy of Science",
      "citeRegEx" : "Halpern and Pearl,? \\Q2005\\E",
      "shortCiteRegEx" : "Halpern and Pearl",
      "year" : 2005
    }, {
      "title" : "Qualitative propagation and scenario-based scheme for exploiting probabilistic reasoning",
      "author" : [ "M. Henrion", "M.J. Druzdzel" ],
      "venue" : "UAI, pages",
      "citeRegEx" : "Henrion and Druzdzel,? \\Q1990\\E",
      "shortCiteRegEx" : "Henrion and Druzdzel",
      "year" : 1990
    }, {
      "title" : "Theory of Probability",
      "author" : [ "H. Je reys" ],
      "venue" : null,
      "citeRegEx" : "reys,? \\Q1961\\E",
      "shortCiteRegEx" : "reys",
      "year" : 1961
    }, {
      "title" : "Bayesian Networks and Decision Graphs",
      "author" : [ "F.V. Jensen" ],
      "venue" : null,
      "citeRegEx" : "Jensen,? \\Q2001\\E",
      "shortCiteRegEx" : "Jensen",
      "year" : 2001
    }, {
      "title" : "A review of explanation methods of Bayesian networks",
      "author" : [ "C. Lacave", "F.J. Diez" ],
      "venue" : "Knowledge Engineering Review,",
      "citeRegEx" : "Lacave and Diez,? \\Q2002\\E",
      "shortCiteRegEx" : "Lacave and Diez",
      "year" : 2002
    }, {
      "title" : "Local computations with probabilities on graphical structures and their application to expert systems",
      "author" : [ "S.L. Lauritzen", "D.J. Spiegelhalter" ],
      "venue" : "Journal of the Royal Statistical Society, Series B,",
      "citeRegEx" : "Lauritzen and Spiegelhalter,? \\Q1988\\E",
      "shortCiteRegEx" : "Lauritzen and Spiegelhalter",
      "year" : 1988
    }, {
      "title" : "On causal explanations in Bayesian networks. Master's thesis, IT University of Copenhagen",
      "author" : [ "U.H. Nielsen" ],
      "venue" : null,
      "citeRegEx" : "Nielsen,? \\Q2007\\E",
      "shortCiteRegEx" : "Nielsen",
      "year" : 2007
    }, {
      "title" : "Map complexity results and approximation methods",
      "author" : [ "J.D. Park" ],
      "venue" : "In: UAI-02,",
      "citeRegEx" : "Park,? \\Q2002\\E",
      "shortCiteRegEx" : "Park",
      "year" : 2002
    }, {
      "title" : "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference",
      "author" : [ "J. Pearl" ],
      "venue" : null,
      "citeRegEx" : "Pearl,? \\Q1988\\E",
      "shortCiteRegEx" : "Pearl",
      "year" : 1988
    }, {
      "title" : "Causal diagrams for empirical research",
      "author" : [ "J. Pearl" ],
      "venue" : null,
      "citeRegEx" : "Pearl,? \\Q1995\\E",
      "shortCiteRegEx" : "Pearl",
      "year" : 1995
    }, {
      "title" : "Causality: Models, Reasoning, and Inference",
      "author" : [ "J. Pearl" ],
      "venue" : null,
      "citeRegEx" : "Pearl,? \\Q2000\\E",
      "shortCiteRegEx" : "Pearl",
      "year" : 2000
    }, {
      "title" : "Explanation, irrelevance, and statistical independence",
      "author" : [ "S.E. Shimony" ],
      "venue" : "In: AAAI,",
      "citeRegEx" : "Shimony,? \\Q1991\\E",
      "shortCiteRegEx" : "Shimony",
      "year" : 1991
    }, {
      "title" : "Causation, Prediction, and Search, Second Edition",
      "author" : [ "P. Spirtes", "C. Glymour", "R. Scheines" ],
      "venue" : null,
      "citeRegEx" : "Spirtes et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Spirtes et al\\.",
      "year" : 2001
    }, {
      "title" : "Finding explanations in bayesian networks",
      "author" : [ "C. Yuan", "Lu", "T.-C" ],
      "venue" : "The 18th International Workshop on Principles of Diagnosis",
      "citeRegEx" : "Yuan et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Yuan et al\\.",
      "year" : 2007
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "We then introduce causal explanation trees, based on the construction of explanation trees using the measure of causal information ow (Ay and Polani, 2006).",
      "startOffset" : 134,
      "endOffset" : 155
    }, {
      "referenceID" : 16,
      "context" : "In this paper, we discuss the extraction of explanations in causal BNs (Pearl, 2000; Spirtes et al., 2001) BNs where the arcs depict direct cause e ect relationships between variables.",
      "startOffset" : 71,
      "endOffset" : 106
    }, {
      "referenceID" : 18,
      "context" : "In this paper, we discuss the extraction of explanations in causal BNs (Pearl, 2000; Spirtes et al., 2001) BNs where the arcs depict direct cause e ect relationships between variables.",
      "startOffset" : 71,
      "endOffset" : 106
    }, {
      "referenceID" : 10,
      "context" : "Generally, explanations in BNs can be classi ed in three categories (Lacave and Diez, 2002) depending on the focus of the explanation: • Explanation of evidence.",
      "startOffset" : 68,
      "endOffset" : 91
    }, {
      "referenceID" : 1,
      "context" : "We insist on the distinction between the explanandum e and the observations o (Chajewska and Halpern, 1997).",
      "startOffset" : 78,
      "endOffset" : 107
    }, {
      "referenceID" : 6,
      "context" : "As Halpern and Pearl (2005) discuss, explanations need to be causal to be consistent with users' knowledge of the mechanisms of the system.",
      "startOffset" : 3,
      "endOffset" : 28
    }, {
      "referenceID" : 16,
      "context" : "In addition to assuming that the relationships between the variables V can be represented by a fully oriented causal BN, we assume that the corresponding joint probability distribution is faithful and causally su cient (Pearl, 2000; Spirtes et al., 2001).",
      "startOffset" : 219,
      "endOffset" : 254
    }, {
      "referenceID" : 18,
      "context" : "In addition to assuming that the relationships between the variables V can be represented by a fully oriented causal BN, we assume that the corresponding joint probability distribution is faithful and causally su cient (Pearl, 2000; Spirtes et al., 2001).",
      "startOffset" : 219,
      "endOffset" : 254
    }, {
      "referenceID" : 14,
      "context" : "The most probable explanation (MPE) approach (Pearl, 1988) then considers h∗ = arg maxh p(h | e) as the best explanation (or, alternatively, looks for the k best explanations by maximizing this probability).",
      "startOffset" : 45,
      "endOffset" : 58
    }, {
      "referenceID" : 17,
      "context" : "In the partial abduction approach (Shimony, 1991), the set of explanatory variables is a strict subset H ( V \\ E.",
      "startOffset" : 34,
      "endOffset" : 49
    }, {
      "referenceID" : 17,
      "context" : "Automatically selecting the relevant explanatory variable is a nontrivial issue (Shimony, 1991).",
      "startOffset" : 80,
      "endOffset" : 95
    }, {
      "referenceID" : 3,
      "context" : "Further e orts to make explanations more concise include de Campos et al. (2001), where the k most probable explanations are found and then simpli ed based on relevance and probabilistic criteria; and Henrion and Druzdzel (1990), where also partial assignments are allowed but only within a prede ned tree that limits the set of possible explanations.",
      "startOffset" : 60,
      "endOffset" : 81
    }, {
      "referenceID" : 3,
      "context" : "Further e orts to make explanations more concise include de Campos et al. (2001), where the k most probable explanations are found and then simpli ed based on relevance and probabilistic criteria; and Henrion and Druzdzel (1990), where also partial assignments are allowed but only within a prede ned tree that limits the set of possible explanations.",
      "startOffset" : 60,
      "endOffset" : 229
    }, {
      "referenceID" : 1,
      "context" : "There are several concerns with these approaches MPE/MAP or scenario-based maximizing some conditional probability of the explanatory variables (Chajewska and Halpern, 1997).",
      "startOffset" : 144,
      "endOffset" : 173
    }, {
      "referenceID" : 2,
      "context" : "MPEs and, to a lesser extent, MAP model explanations, are not robust: little changes in the network will often change the result of the analysis, even though the changes occur in parts of the network largely independent of the explanandum (Chan and Darwiche, 2006).",
      "startOffset" : 239,
      "endOffset" : 264
    }, {
      "referenceID" : 9,
      "context" : "In SE analysis, Jensen (2001) additionally considers the sensitivity of an explanation h with respect to the explanandum.",
      "startOffset" : 16,
      "endOffset" : 30
    }, {
      "referenceID" : 13,
      "context" : "The di erence between observation and intervention is fundamental to causality and is best described with the example of Simpson's paradox in Pearl (2000), chap.",
      "startOffset" : 142,
      "endOffset" : 155
    }, {
      "referenceID" : 8,
      "context" : "The empirical interpretation of Bayes' factor given by Je reys (1961) is that if it is less than 1 it is in favor of hj , if less than 3 it is a slight support for hi.",
      "startOffset" : 58,
      "endOffset" : 70
    }, {
      "referenceID" : 16,
      "context" : "An exhaustive search is performed over all subsets of the hypothesis, and the explanations are shown to be more concise in a sample network than MPE, Shimony's (1991) MAP, and the simpli cations described by de Campos et al.",
      "startOffset" : 150,
      "endOffset" : 167
    }, {
      "referenceID" : 3,
      "context" : "An exhaustive search is performed over all subsets of the hypothesis, and the explanations are shown to be more concise in a sample network than MPE, Shimony's (1991) MAP, and the simpli cations described by de Campos et al. (2001).",
      "startOffset" : 211,
      "endOffset" : 232
    }, {
      "referenceID" : 4,
      "context" : "The method of Flores (2005) constructs a set of best explanations while at the same time giving a preference for concise explanations, summarizing the results of the analysis in an explanation tree.",
      "startOffset" : 14,
      "endOffset" : 28
    }, {
      "referenceID" : 4,
      "context" : "Algorithm 1 Flores's (2005) Explanation Tree 1: function T = ExplanationTree(H, e,p; α, β) Input: H : set of explanatory variables E = e : explanandum P = p : path of variable assignments α, β : stopping criteria Output: T : an explanation tree 2: X∗ ← arg maxX∈H P Y ∈H Inf(X;Y | e,p) 3: if max Y ∈H\\X∗ Inf(X;Y | e,p) < α or p(p | e) < β then 4: return ∅ 5: end if 6: T ← new tree with root X∗ 7: for each x ∈ domain(X∗) do 8: T ′ ← ExplanationTree(H \\X∗, e,p ∪ {x}) 9: add a branch x to T with subtree T ′ and 10: assign it the label p(p, x | e) 11: end for 12: return T",
      "startOffset" : 12,
      "endOffset" : 28
    }, {
      "referenceID" : 4,
      "context" : "Flores (2005) also argues that explanations as constructed by Algorithm 1 are reasonable and more sensible than (k-)MPE in the sense that on simple networks, the returned explanations are those that we expect.",
      "startOffset" : 0,
      "endOffset" : 14
    }, {
      "referenceID" : 4,
      "context" : "First, on line 2 of Algorithm 1, variables are added to the tree in order of how much information they provide See Flores (2005) for additional cases where max at line 3 is replaced by min or avg, and Inf is the Gini index.",
      "startOffset" : 115,
      "endOffset" : 129
    }, {
      "referenceID" : 14,
      "context" : "the e ect of these conditionings is Pearl's (1995) docalculus, which uses the structure of the causal graph to evaluate the postintervention distribution.",
      "startOffset" : 36,
      "endOffset" : 51
    }, {
      "referenceID" : 0,
      "context" : "With this concept, we can now de ne the causal information ow (Ay and Polani, 2006), which will be our measure of causal contribution of explanatory variables towards our explanandum.",
      "startOffset" : 62,
      "endOffset" : 83
    }, {
      "referenceID" : 5,
      "context" : "The inference steps were implemented using the factor graph message-passing algorithm (Frey et al., 2001).",
      "startOffset" : 86,
      "endOffset" : 105
    }, {
      "referenceID" : 4,
      "context" : "For comparison, Flores's (2005) approach is O(nd).",
      "startOffset" : 16,
      "endOffset" : 32
    }, {
      "referenceID" : 12,
      "context" : "A more extended version of these experiments and comments can be found in Nielsen (2007).",
      "startOffset" : 74,
      "endOffset" : 89
    }, {
      "referenceID" : 11,
      "context" : "This network (Lauritzen and Spiegelhalter, 1988) models the relationships between two indicators, X-ray results and dyspnea, of severe diseases for a person.",
      "startOffset" : 13,
      "endOffset" : 48
    } ],
    "year" : 2008,
    "abstractText" : "Bayesian networks can be used to extract explanations about the observed state of a subset of variables. In this paper, we explicate the desiderata of an explanation and confront them with the concept of explanation proposed by existing methods. The necessity of taking into account causal approaches when a causal graph is available is discussed. We then introduce causal explanation trees, based on the construction of explanation trees using the measure of causal information ow (Ay and Polani, 2006). This approach is compared to several other methods on known networks.",
    "creator" : "LaTeX with hyperref package"
  }
}