{
  "name" : "1511.06410.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Yuandong Tian", "Yan Zhu" ],
    "emails" : [ "yuandong@fb.com", "yz328@cs.rutgers.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Competing with top human players in the ancient game of Go has been a longterm goal of artificial intelligence. Go’s high branching factor makes traditional search techniques ineffective, even on leading-edge hardware, and Go’s evaluation function could change drastically with one stone change. Recent works [Maddison et al. (2015); Clark & Storkey (2015)] show that search is not strictly necessary for machine Go players. A pure pattern-matching approach, based on a Deep Convolutional Neural Network (DCNN) that predicts the next move, can perform as well as Monte Carlo Tree Search (MCTS)-based open source Go engines such as Pachi [Baudis & Gailly (2012)] if its search budget is limited. We extend this idea in our bot named darkforest, which relies on a DCNN designed for long-term predictions. Darkforest substantially improves the win rate for patternmatching approaches against MCTS-based approaches, even with looser search budgets. Against human players, darkforest achieves a stable 1d-2d level on KGS Go Server, estimated from free games against human players. This substantially improves the estimated rankings reported in Clark & Storkey (2015), where DCNN-based bots are estimated at 4k-5k level based on performance against other machine players. Adding MCTS to darkforest creates a much stronger player: with only 1000 rollouts, darkforest+MCTS beats pure darkforest 90% of the time; with 5000 rollouts, our best model plus MCTS beats Pachi with 10,000 rollouts 95.5% of the time."
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "For a long time, computer Go is considered to be a grand challenge in artificial intelligence. Fig. 1 shows a simple illustration of the game of Go. Two players, black and white, place stones at intersections in turn on a 19x19 board (Fig. 1(a)). Black plays first on an empty board. A 4-connected component of the same color is called a group. The liberties of a group is the number of its neighboring empty intersections (Fig. 1(b)). A group is captured if its liberties are zero. The goal of the game is to control more territory than the opponent (Fig. 1(c)). Fig. 1(d)) shows the Go rating system, ranging from kyu level (beginner to decent amateur, 30k-1k) to dan level (advanced amateur, 1d-7d) and to professional levels (1p-9p) [Silver (2009)].\nGo is difficult due to its high branching factors (typically on the order of hundred on a 19x19 board) and subtle board situations that are sensitive to small changes (adding/removing one stone could alter the life/death situation of a large group of stone and thus completely changes the final score). A combination of the two implies that the only solution is to use massive search that requires a prohibitive amount of resources, which is not attainable with cutting-edge hardware.\nFortunately, recent works [Maddison et al. (2015); Clark & Storkey (2015)] in Computer Go have shown that the Go board situation could be deciphered with Deep Convolutional Neural Network (DCNN). They can predict the next move that a human would play 55.2% of the time. However, whether this accuracy leads to a strong Go AI is not yet well understood. It is possible that DCNN correctly predicts most regular plays by looking at the correlation of local patterns, but still fails to predict the critical one or two moves and loses the game. Indeed, a DCNN-based\nar X\niv :1\n51 1.\n06 41\n0v 1\n[ cs\n.L G\n] 1\n9 N\nov 2\n01 5\nplayer is still behind compared to traditional open-source engines based on Monte-Carlo Tree Search (MCTS) [Browne et al. (2012); Kocsis & Szepesvári (2006)], let alone commercial ones.\nIn this paper, we show that DCNN-based move predictions indeed give a strong Go AI, if properly trained. In particular, we carefully design the training process and choose to predict next k moves rather than the immediate next move to enrich the gradient signal. Despite our prediction giving a mere 2% boost for accuracy of move predictions, the win rate against open-source engines (e.g., Pachi and Fuego) in heavy search scenarios (e.g., 100k rollouts) is more than 6 times higher (Pachi: 11.0% vs 73.7%, Fuego: 12.5% vs 91.0%) than current state-of-the-art DCNN-based player [Maddison et al. (2015)]. In addition, we release our two bots, called darkforest and darkfores1, to public KGS Go server for free games. A collection of more than 2000 games against humans over three months show that the search-less darkforest holds a decent 1d-2d level, much better than the neural network based AI proposed by Clark & Storkey (2015) that holds 4-5 kyu estimated from games against other MCTS-based Go engines.\nOur bots also share the common weakness of DCNN-based methods in local tactics. We show that, by combining DCNN with MCTS, our bots can address the local tactics weakness and greatly boost the strength. With only 1000 rollouts, our synchronized version of DCNN+MCTS runs 6 seconds per move and achieves an impressive 90% win rate against pure DCNN. Our best model, darkfores2, combined with MCTS of 5000 rollouts, has a 95.5% win rate over Pachi with 10,000 rollouts."
    }, {
      "heading" : "2 METHOD",
      "text" : "Using Neural Network as a function approximator and pattern matcher to predict the next move of Go is a long-standing idea [Sutskever & Nair (2008); Richards et al. (1998); Schraudolph et al. (1994); Enzenberger (1996)]. Recent progress [Maddison et al. (2015); Clark & Storkey (2015)] uses Deep Convolutional Neural Network (DCNN) for move prediction, and shows substantial improvement over shallow networks or linear function approximators based on manually designed features or simple patterns extracted from previous games [Silver (2009)].\nIn this paper, we train a DCNN that predicts the next k moves given the current board situation as an input. We treat the 19× 19 board as a 19× 19 image with multiple channels. Each channel encodes a different aspect of board information, e.g., liberties (Fig. 1(b)). Compared to previous works, we use a more compact feature set and predict long-term moves, and show that they lead to a substantial performance boost in terms of win rate against open source engines."
    }, {
      "heading" : "2.1 FEATURE CHANNELS",
      "text" : "Table 1 shows the features extracted from the current board situation. Each feature is a binary 19×19 map except for history information and position mask, which are real numbers in [0, 1]. History is encoded as exp(−t ∗ 0.1), where t is how long the stone has been placed. The exponential temporal decay is meant to enable the network to focus on the recent battle. Position mark is defined as exp(− 12 l\n2), where l2 is the squared L2 distance to the board center. It is used to encode the relative position of each intersection.\nThere are two differences between our features and those in Maddison et al. (2015). First, we use relative coding (our/opponent) for almost all the features. In contrast, the features in Maddison et al. (2015) are largely player-agnostic. Second, our feature set is simpler and compact (25 vs. 36 input\nplanes), in particular, free from one step forward simulation. In comparison, Maddison et al. (2015) uses such features like liberties after the move, captures after the move, etc.\nWe use a similar way to encode rank in 9 planes as in Maddison et al. (2015). That is, all kyu-players have all nine planes zero, 1d players has their first plane all-1, 2d players have their second plane all-1, etc. For 9d and professional players, all the planes are filled with 1."
    }, {
      "heading" : "2.2 NETWORK ARCHITECTURE",
      "text" : "Fig. 3 shows the architecture of the network for our best model. We use a 12-layered (d = 12) full convolutional network. Each convolution layer is followed by a ReLU nonlinearity. Except for the first layer, all layers use the same width w = 384. No weight sharing is used. We do not use pooling since they negatively affect the performance. Instead of using two softmax outputs [Maddison et al. (2015)] to predict black and white moves, we only use one softmax layer to predict the next move, reducing the number of parameters."
    }, {
      "heading" : "2.3 LONG TERM PLANNING",
      "text" : "Predicting only the immediate next move limits the information received by the lower layers. Instead, we predict next k moves (self and opponent, alternatively) from the current board situation. Each move is a separate softmax output. The motivation is two-fold. First, we want our network to focus on a strategic plan rather than the immediate next move. Second, with multiple softmax outputs, we expect to have more supervisions to train the network. Table 2 computes the ratio of average gradient L2 norm (over the first 9 epochs, first 1000 mini-batches removed) between 1-step and 3-step predictions at each convolutional layer. As expected, the gradient magnitudes of the top layers (layers closer to softmax) are higher in 3-step prediction. However, the gradient magnitudes of the lower layers are approximately the same, showing that the lower gradients are canceled out in 3-step prediction, presumably leaving only the most important gradient for training. Empirically, DCNN trained with 3 steps gives high win rate than that with 1 step."
    }, {
      "heading" : "2.4 TRAINING",
      "text" : "When training, we use 16 CPU threads to prepare the minibatch, each simulating 300 random selected games from the dataset. In each minibatch, for each thread, randomly select one game out of 300, simulate one step according to the game record, and extract features and next k moves as the input/output pair in the batch. If the game has ended (or fewer than k moves are left), we randomly pick one (with replacement) from the training set and continue. The batch size is 256. We use data augmentation with rotation at 90-degree intervals and horizontal/vertical flipping. For each board situation, data augmentation could generate up to 8 different situations.\nBefore training, we randomly initialize games into different stages. This ensures that each batch contains situations corresponding to different stages of games. Without this, the network will quickly overfit and get trapped into poor local minima.\nBecause of our training style, it is not clear when the training set has been thoroughly processed once. Therefore, we just define an epoch as 10,000 mini-batches. Unlike Maddison et al. (2015) that uses asynchronous stochastic gradient descent, we just use vanilla SGD on 4 NVidia K40m GPUs in a single machine to train the entire network (for some models we use 3 GPUs with 255 as the batch size). Each epoch lasts about 5 to 6 hours. The learning rate is set to be 0.05. Typically, the model starts to converge within one epoch and shows good performance after 50-60 epochs (around two weeks). Reducing the learning rate after the performance stalls will increase the performance even further."
    }, {
      "heading" : "2.5 MONTE CARLO TREE SEARCH",
      "text" : "From the experiments, we clearly show that DCNN is tactically weak due to the lack of search. Search is a way to explore the solution space conditioned on the current board situation, and build a non-parametric local model for the game. The local model is more flexible than the global model learned from massive training data and more adapted to the current situation. The state-of-the-art approach in computer Go is Monte-Carlo Tree Search (MCTS). Fig. 4 shows its basic principle.\nCombining DCNN with MCTS requires nontrivial engineering efforts because each rollout of MCTS is way much faster than DCNN evaluation. Therefore, these two must run in parallel with frequent communications. Our basic implementation of MCTS gives 16k rollouts per second (for 16 threads on a machine with Intel Xeon CPU E5-2680 v2 at 2.80GHz) while it typically takes 0.2s for DCNN to give board evaluations of a batch size of 128 with 4 GPUs.\nThere are two ways to address this problem. In asynchronized implementation used in Maddison et al. (2015), MCTS sends the newly expanded node to DCNN but is not blocked by DCNN evaluation. MCTS will use its own tree policy until DCNN evaluation is finished. This gives high rollout rate, but there is a time lag for the DCNN evaluation to take effect, and it is not clear how many board\nsituations have been evaluated for a given number of MCTS rollouts. In synchronized implementation, MCTS will wait until DCNN evaluates the board situation of a leaf node, and then expands the leaf. Default policy can be executed before or after DCNN evaluation. This is much slower but guarantees that each node is expanded according to the suggested moves given by DCNN.\nIn our experiments, we evaluate the synchronized case, which achieves 90% win rate against its raw DCNN player with only 1000 rollouts. Note that our implementation is not directly comparable to the asynchronized version in Maddison et al. (2015), achieving 86.7% with 100k rollouts."
    }, {
      "heading" : "3 EXPERIMENTS",
      "text" : ""
    }, {
      "heading" : "3.1 SETUP",
      "text" : "We use the public KGS dataset (∼170k games), which is used in Maddison et al. (2015). We use all games before 2012 as the training set and 2013-2015 games as the test set. This leads to 144,748 games for training and 26,814 games for testing. We also use GoGoD dataset1 (∼80k games), which is also used in Clark & Storkey (2015). 75,172 games are used for training and 2,592 for testing.\nFor evaluation, our model competes with GnuGo, Pachi [Baudis & Gailly (2012)] and Fuego [Enzenberger et al. (2010)]. We use GnuGo 3.8 level 10, Pachi 11.99 (Genjo-devel) with the pattern files, and Fuego 1.1 throughout our experiments."
    }, {
      "heading" : "3.2 MOVE PREDICTION",
      "text" : "Table 3 shows the performance comparison for move prediction. For models that predict the next k moves, we only evaluate their prediction accuracy for the immediate next move.\nWith our training framework, we are able to achieve slightly higher Top-1 prediction accuracy (after hundreds of epochs) compared to Maddison et al. (2015). Note that using standard or extended features seem to have marginal gains (Fig. 5). For the remaining experiments, we thus use d = 12 and w = 384, as shown in Fig. 3.\n1We used GoGoD 2015 summer version, purchased from http://www.gogod.co.uk. We skip ancient games and only use game records after 1800 AD."
    }, {
      "heading" : "3.3 WIN RATE",
      "text" : "Surprisingly, win rate does not correlate well with move prediction accuracy. Regarding move prediction accuracy in the test set, all the numbers are approximately the same except for a few percentage difference. However, their win rates against other game engines are very different. Table 6 shows how the win rate increases over time. From the figure, our DCNN trained with 2 or 3 steps is about 10%− 15% (in absolute difference) better than DCNN trained with 1 step. For more steps, the performance shows diminishing returns. On the other hand, the win rate of the standard feature set is comparable to the extended one. Table 4 shows that win rate of our approach is substantially higher than that of previous works.\nWe also train a smaller model with w = 144 whose number of parameters are comparable to Maddison et al. (2015). Our smaller model achieves 43.3% in 300 games against Pachi 10k when Pachi’s\n3We also test darkfores2 against Fuego under this setting, and its win rate is 93%± 1%.\npondering is on (keep searching when the opponent plays), and 55.7% when it is off. In contrast, Maddison et al. (2015) reports 47.4% and does not mention pondering status.\nDarkforest AI Bots. We build three bots from the trained models. Our first bot darkforest is trained using standard features, 1 step prediction on KGS dataset. The second bot darkfores1 is trained using extended features, 3 step prediction on GoGoD dataset. Both bots are trained with constant learning rate 0.05. Based on darkfores1, we fine-tuned the learning rate to create an even stronger DCNN player, darkfores2. Table 4 shows their strengths against open source engines. It seems that despite the fact that GoGoD is smaller, our model can be trained faster with better performance, presumably because GoGoD contains all professional games, while games from KGS Server are a bit noisy. Win rates between pairs of the three bots (Table 5) are also consistent with their performances against open source engines.\nWe also compare darkforest with a public DCNN model4. Since both models are deterministic, we sample the moves according to the softmax probability. We played two sets of 100 games; the win rate is 100% and 99% respectively. Darkforest always wins if using their strongest moves or sampling from top 5 confident moves."
    }, {
      "heading" : "3.4 EVALUATION ON KGS GO SERVER",
      "text" : "We also put our bots onto KGS Go server and check their performance over three months period. Darkforest became publicly available on Aug 31, 2015. Since then it has played about 2000 games. Recently we also release the improved version darkfores1 on Nov 2, 2015. To score the endgame board situations, we randomly run 1000 trials of default policy to find the dead stones, followed by standard Tromp-Taylor scoring. If all 1000 trials show losing by 10+ points, they resign.\nIn KGS Go server, their levels are around KGS 1d-2d, while darkfores1 is slightly stronger, consistent with its better win rate against open source engines. Table 6 shows the statistics. In addition, they also played against 4d players and had won 3 games out of 9. This is a major improvement upon the AI developed in Clark & Storkey (2015) that holds 4k-5k level, estimated by playing against Go engines. Although the two bots only play free games that have no effect on ranking and might not draw full attention from human players, their overall game statistics are quite impressive as pure DCNN models. Fig. 7 shows one example game between darkfores1 and a KGS 1d human player. Note that we are still way behind commercial state-of-the-art Go AIs, e.g., Zen, CrazyStone and Ginsei Igo, which use MCTS and achieve 6d in KGS.\nOverall, our bots have a very good understanding of global board situations, and tend to play “good shapes” but might not get the local basic life/death situations right. When they lost, they usually failed to make a large connected group alive during capturing race, or failed to make two eyes. Rarely they lost in ladder capture, a special case in Go (Fig. 2(c)) in which one keeps chasing the opponent’s stones until the board’s border and kill them. Ladder is usually a gamble: if ladder fails, then the stones used by the attacker are disconnected and are vulnerable to being attacked. Apparently the network failed to capture the ladder pattern due to its rarity in actual games.\nReaction on KGS Go server. Darkforest and darkfores1 are quite popular on KGS Go server, playing around 100 games a day. One comment from experienced players around 3d-4d is “the bot played just like human, but a bit weak in local tactics.” This shows that our bots have great potential and could be substantially better with the help of search.\n4From http://physik.de/net.tgz by Detlef Schmicker. He released the model in Computer-Go forum. See http://computer-go.org/pipermail/computer-go/2015-April/007573.html\n2015-11-12 (KGS Go Server) W+13.5 (komi: 7.5)\ngugun (1d)\ndarkfores1 (?)\n()\nB+Resign\n(komi: 7.5)\ngo_player_v2_mcts (?)\ngo_player_v2 (?)"
    }, {
      "heading" : "3.5 COMBINATION WITH MONTE CARLO TREE SEARCH (MCTS",
      "text" : "We build an MCTS framework and study how DCNN+MCTS could affect the win rate. Our MCTS implementation is pretty standard. Tree policy: For a list of moves sorted by softmax probability from DCNN, we keep picking the moves from the most probable one until the accumulated probability exceed 0.8, and use UCT [Browne et al. (2012)] to select moves for tree expansion. Note that the DCNN confidences of the selected moves are not used in UCT. Default policy: Following Pachi’s implementation [Baudis & Gailly (2012)], we use 3x3 patterns, opponent atari points, detection of nakade points and avoidance of self-atari for default policy. The default policy is played until no player can play a move that is not self-destructive.\nWe test our synchronized MCTS implementation with darkforest, darkfores1 and darkfores2. With 1000 rollouts, it runs on 16 threads and produces a move every 6 seconds on a machine with NVidia K40m GPU and Intel Xeon E5-2680 v2 at 2.80GHz. Our implementation can be further optimized to yield faster rollout rate (GPU utility is around 30%). Due to the non-deterministic nature of multi-threading, the game between DCNN+MCTS and DCNN is always different for each trial.\nVersus Pure DCNN. Table 7 summarizes the performance. Darkforest+MCTS gives the highest performance boost over darkforest (90% win rate), while MCTS also gives decent boost for darkfores1 and darkfores2. This indicates that MCTS mitigates the weakness of DCNN. Table 8 shows how likely MCTS picks DCNN’s best move, if DCNN offers three or more choices. MCTS tends to agree more with DCNN towards the end of the game, when the situation becomes clearer via search. Finally, Fig. 7 shows one sample game between darkforest and darkforest+MCTS.\nVersus Pachi 10k. With 1000 rollouts, the win rate improvement over darkforest is about 3%, compared to pure DCNN. For darkfores1 and darkfores2, their combined performances are worse than their pure DCNN counterparts. This shows that 1000 rollouts probably is not sufficient. With 5000 rollouts, all combinations beats their pure DCNN version, even for our best model darkfores2. Note that for these experiments, we turn off pondering (Pachi will not search during the opponent round). A tentative reason why using 1000 rollouts is not working well is that adding MCTS to DCNN make it stronger in local tactics but might negatively affect its global board understanding, which is a significant advantage over pure MCTS-based bot such as Pachi. Therefore, sufficient number of rollouts are needed to compensate for that. With 10,000 rollouts, win rate against Pachi 10k are ≥ 98% for all three bots. Comparison with previous works. In comparison, an asynchronized version is used in Maddison et al. (2015) that achieves 86.7% with 100k rollouts, with faster CPU and GPU (Intel Xeon E5-2643 v2 at 3.50GHz and GeForce GTX Titan Black). The two numbers are not directly comparable since (1) in asynchronized implementations, the number of game states sent to DCNN for evaluation is unknown during 100k rollouts, and (2) Table 7 shows that stronger DCNN model benefits less when combined with MCTS. Section 2.5 gives a detailed comparison between the two implementations."
    }, {
      "heading" : "4 CONCLUSION AND FUTURE WORK",
      "text" : "In this paper, we have substantially improved the performance of DCNN-based Go AI, extensively evaluated it against both open source engines and strong amateur human players, and shown its potentials if combined with Monte-Carlo Tree Search (MCTS).\nIdeally, we want to construct a system that combines both pattern matching and search, and can be trained jointly in an online fashion. Pattern matching with DCNN is good at global board reading, but might fail to capture special local situations. On the other hand, search is excellent in modeling arbitrary situations, by building a local non-parametric model for the current state, only when the computation cost is affordable. One paradigm is to update DCNN weights (i.e., Policy Gradient [Sutton et al. (1999)]) after MCTS completes and chooses a different best move than DCNN’s proposal. To increase the signal bandwidth, we could also update weights using all the board situations along the trajectory of the best move. Alternatively, we could update the weights when MCTS is running. Actor-Critics algorithms [Konda & Tsitsiklis (1999)] can also be used to train two models simultaneously, one to predict the next move (actor) and the other to evaluate the current board situation (critic). Finally, local tactics training (e.g., Life/Death practice) focuses on local board situation with fewer variations. Human players have benefited from it and can generalize the local principle to other similar situations. DCNN approaches should also be benefited from it as well.\nAcknowledgement We thank Rob Fergus and Keith Adams for constructive suggestions, and Vincent Cheung for engineering help."
    } ],
    "references" : [ {
      "title" : "Pachi: State of the art open source go program",
      "author" : [ "Baudis", "Petr", "Gailly", "Jean-loup" ],
      "venue" : null,
      "citeRegEx" : "Baudis et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Baudis et al\\.",
      "year" : 2012
    }, {
      "title" : "A survey of monte carlo tree search methods",
      "author" : [ "Browne", "Cameron B", "Powley", "Edward", "Whitehouse", "Daniel", "Lucas", "Simon M", "Cowling", "Peter", "Rohlfshagen", "Philipp", "Tavener", "Stephen", "Perez", "Diego", "Samothrakis", "Spyridon", "Colton", "Simon" ],
      "venue" : "Computational Intelligence and AI in Games, IEEE Transactions on,",
      "citeRegEx" : "Browne et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Browne et al\\.",
      "year" : 2012
    }, {
      "title" : "Training deep convolutional neural networks to play go",
      "author" : [ "Clark", "Christopher", "Storkey", "Amos" ],
      "venue" : "In Proceedings of the 32nd International Conference on Machine Learning",
      "citeRegEx" : "Clark et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2015
    }, {
      "title" : "The integration of a priori knowledge into a go playing neural network",
      "author" : [ "Enzenberger", "Markus" ],
      "venue" : "URL: http://www. markus-enzenberger. de/neurogo. html,",
      "citeRegEx" : "Enzenberger and Markus.,? \\Q1996\\E",
      "shortCiteRegEx" : "Enzenberger and Markus.",
      "year" : 1996
    }, {
      "title" : "Fuegoan opensource framework for board games and go engine based on monte carlo tree search",
      "author" : [ "Enzenberger", "Markus", "Müller", "Martin", "Arneson", "Broderick", "Segal", "Richard" ],
      "venue" : "Computational Intelligence and AI in Games, IEEE Transactions on,",
      "citeRegEx" : "Enzenberger et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Enzenberger et al\\.",
      "year" : 2010
    }, {
      "title" : "Bandit based monte-carlo planning",
      "author" : [ "Kocsis", "Levente", "Szepesvári", "Csaba" ],
      "venue" : "In Machine Learning: ECML",
      "citeRegEx" : "Kocsis et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Kocsis et al\\.",
      "year" : 2006
    }, {
      "title" : "Move evaluation in go using deep convolutional neural networks",
      "author" : [ "Maddison", "Chris J", "Huang", "Aja", "Sutskever", "Ilya", "Silver", "David" ],
      "venue" : null,
      "citeRegEx" : "Maddison et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Maddison et al\\.",
      "year" : 2015
    }, {
      "title" : "Evolving neural networks to play go",
      "author" : [ "Richards", "Norman", "Moriarty", "David E", "Miikkulainen", "Risto" ],
      "venue" : "Applied Intelligence,",
      "citeRegEx" : "Richards et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Richards et al\\.",
      "year" : 1998
    }, {
      "title" : "Temporal difference learning of position evaluation in the game of go",
      "author" : [ "Schraudolph", "Nicol N", "Dayan", "Peter", "Sejnowski", "Terrence J" ],
      "venue" : "Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Schraudolph et al\\.,? \\Q1994\\E",
      "shortCiteRegEx" : "Schraudolph et al\\.",
      "year" : 1994
    }, {
      "title" : "Reinforcement learning and simulation-based search",
      "author" : [ "Silver", "David" ],
      "venue" : "Doctor of philosophy, University of Alberta,",
      "citeRegEx" : "Silver and David.,? \\Q2009\\E",
      "shortCiteRegEx" : "Silver and David.",
      "year" : 2009
    }, {
      "title" : "Mimicking go experts with convolutional neural networks",
      "author" : [ "Sutskever", "Ilya", "Nair", "Vinod" ],
      "venue" : "In Artificial Neural Networks-ICANN",
      "citeRegEx" : "Sutskever et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2008
    }, {
      "title" : "Policy gradient methods for reinforcement learning with function approximation",
      "author" : [ "Sutton", "Richard S", "McAllester", "David A", "Singh", "Satinder P", "Mansour", "Yishay" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Sutton et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Sutton et al\\.",
      "year" : 1999
    } ],
    "referenceMentions" : [ {
      "referenceID" : 6,
      "context" : "Recent works [Maddison et al. (2015); Clark & Storkey (2015)] show that search is not strictly necessary for machine Go players.",
      "startOffset" : 14,
      "endOffset" : 37
    }, {
      "referenceID" : 6,
      "context" : "Recent works [Maddison et al. (2015); Clark & Storkey (2015)] show that search is not strictly necessary for machine Go players.",
      "startOffset" : 14,
      "endOffset" : 61
    }, {
      "referenceID" : 6,
      "context" : "Recent works [Maddison et al. (2015); Clark & Storkey (2015)] show that search is not strictly necessary for machine Go players. A pure pattern-matching approach, based on a Deep Convolutional Neural Network (DCNN) that predicts the next move, can perform as well as Monte Carlo Tree Search (MCTS)-based open source Go engines such as Pachi [Baudis & Gailly (2012)] if its search budget is limited.",
      "startOffset" : 14,
      "endOffset" : 365
    }, {
      "referenceID" : 6,
      "context" : "Recent works [Maddison et al. (2015); Clark & Storkey (2015)] show that search is not strictly necessary for machine Go players. A pure pattern-matching approach, based on a Deep Convolutional Neural Network (DCNN) that predicts the next move, can perform as well as Monte Carlo Tree Search (MCTS)-based open source Go engines such as Pachi [Baudis & Gailly (2012)] if its search budget is limited. We extend this idea in our bot named darkforest, which relies on a DCNN designed for long-term predictions. Darkforest substantially improves the win rate for patternmatching approaches against MCTS-based approaches, even with looser search budgets. Against human players, darkforest achieves a stable 1d-2d level on KGS Go Server, estimated from free games against human players. This substantially improves the estimated rankings reported in Clark & Storkey (2015), where DCNN-based bots are estimated at 4k-5k level based on performance against other machine players.",
      "startOffset" : 14,
      "endOffset" : 866
    }, {
      "referenceID" : 6,
      "context" : "Fortunately, recent works [Maddison et al. (2015); Clark & Storkey (2015)] in Computer Go have shown that the Go board situation could be deciphered with Deep Convolutional Neural Network (DCNN).",
      "startOffset" : 27,
      "endOffset" : 50
    }, {
      "referenceID" : 6,
      "context" : "Fortunately, recent works [Maddison et al. (2015); Clark & Storkey (2015)] in Computer Go have shown that the Go board situation could be deciphered with Deep Convolutional Neural Network (DCNN).",
      "startOffset" : 27,
      "endOffset" : 74
    }, {
      "referenceID" : 1,
      "context" : "player is still behind compared to traditional open-source engines based on Monte-Carlo Tree Search (MCTS) [Browne et al. (2012); Kocsis & Szepesvári (2006)], let alone commercial ones.",
      "startOffset" : 108,
      "endOffset" : 129
    }, {
      "referenceID" : 1,
      "context" : "player is still behind compared to traditional open-source engines based on Monte-Carlo Tree Search (MCTS) [Browne et al. (2012); Kocsis & Szepesvári (2006)], let alone commercial ones.",
      "startOffset" : 108,
      "endOffset" : 157
    }, {
      "referenceID" : 1,
      "context" : "player is still behind compared to traditional open-source engines based on Monte-Carlo Tree Search (MCTS) [Browne et al. (2012); Kocsis & Szepesvári (2006)], let alone commercial ones. In this paper, we show that DCNN-based move predictions indeed give a strong Go AI, if properly trained. In particular, we carefully design the training process and choose to predict next k moves rather than the immediate next move to enrich the gradient signal. Despite our prediction giving a mere 2% boost for accuracy of move predictions, the win rate against open-source engines (e.g., Pachi and Fuego) in heavy search scenarios (e.g., 100k rollouts) is more than 6 times higher (Pachi: 11.0% vs 73.7%, Fuego: 12.5% vs 91.0%) than current state-of-the-art DCNN-based player [Maddison et al. (2015)].",
      "startOffset" : 108,
      "endOffset" : 789
    }, {
      "referenceID" : 1,
      "context" : "player is still behind compared to traditional open-source engines based on Monte-Carlo Tree Search (MCTS) [Browne et al. (2012); Kocsis & Szepesvári (2006)], let alone commercial ones. In this paper, we show that DCNN-based move predictions indeed give a strong Go AI, if properly trained. In particular, we carefully design the training process and choose to predict next k moves rather than the immediate next move to enrich the gradient signal. Despite our prediction giving a mere 2% boost for accuracy of move predictions, the win rate against open-source engines (e.g., Pachi and Fuego) in heavy search scenarios (e.g., 100k rollouts) is more than 6 times higher (Pachi: 11.0% vs 73.7%, Fuego: 12.5% vs 91.0%) than current state-of-the-art DCNN-based player [Maddison et al. (2015)]. In addition, we release our two bots, called darkforest and darkfores1, to public KGS Go server for free games. A collection of more than 2000 games against humans over three months show that the search-less darkforest holds a decent 1d-2d level, much better than the neural network based AI proposed by Clark & Storkey (2015) that holds 4-5 kyu estimated from games against other MCTS-based Go engines.",
      "startOffset" : 108,
      "endOffset" : 1118
    }, {
      "referenceID" : 1,
      "context" : "player is still behind compared to traditional open-source engines based on Monte-Carlo Tree Search (MCTS) [Browne et al. (2012); Kocsis & Szepesvári (2006)], let alone commercial ones. In this paper, we show that DCNN-based move predictions indeed give a strong Go AI, if properly trained. In particular, we carefully design the training process and choose to predict next k moves rather than the immediate next move to enrich the gradient signal. Despite our prediction giving a mere 2% boost for accuracy of move predictions, the win rate against open-source engines (e.g., Pachi and Fuego) in heavy search scenarios (e.g., 100k rollouts) is more than 6 times higher (Pachi: 11.0% vs 73.7%, Fuego: 12.5% vs 91.0%) than current state-of-the-art DCNN-based player [Maddison et al. (2015)]. In addition, we release our two bots, called darkforest and darkfores1, to public KGS Go server for free games. A collection of more than 2000 games against humans over three months show that the search-less darkforest holds a decent 1d-2d level, much better than the neural network based AI proposed by Clark & Storkey (2015) that holds 4-5 kyu estimated from games against other MCTS-based Go engines. Our bots also share the common weakness of DCNN-based methods in local tactics. We show that, by combining DCNN with MCTS, our bots can address the local tactics weakness and greatly boost the strength. With only 1000 rollouts, our synchronized version of DCNN+MCTS runs 6 seconds per move and achieves an impressive 90% win rate against pure DCNN. Our best model, darkfores2, combined with MCTS of 5000 rollouts, has a 95.5% win rate over Pachi with 10,000 rollouts. 2 METHOD Using Neural Network as a function approximator and pattern matcher to predict the next move of Go is a long-standing idea [Sutskever & Nair (2008); Richards et al.",
      "startOffset" : 108,
      "endOffset" : 1820
    }, {
      "referenceID" : 1,
      "context" : "player is still behind compared to traditional open-source engines based on Monte-Carlo Tree Search (MCTS) [Browne et al. (2012); Kocsis & Szepesvári (2006)], let alone commercial ones. In this paper, we show that DCNN-based move predictions indeed give a strong Go AI, if properly trained. In particular, we carefully design the training process and choose to predict next k moves rather than the immediate next move to enrich the gradient signal. Despite our prediction giving a mere 2% boost for accuracy of move predictions, the win rate against open-source engines (e.g., Pachi and Fuego) in heavy search scenarios (e.g., 100k rollouts) is more than 6 times higher (Pachi: 11.0% vs 73.7%, Fuego: 12.5% vs 91.0%) than current state-of-the-art DCNN-based player [Maddison et al. (2015)]. In addition, we release our two bots, called darkforest and darkfores1, to public KGS Go server for free games. A collection of more than 2000 games against humans over three months show that the search-less darkforest holds a decent 1d-2d level, much better than the neural network based AI proposed by Clark & Storkey (2015) that holds 4-5 kyu estimated from games against other MCTS-based Go engines. Our bots also share the common weakness of DCNN-based methods in local tactics. We show that, by combining DCNN with MCTS, our bots can address the local tactics weakness and greatly boost the strength. With only 1000 rollouts, our synchronized version of DCNN+MCTS runs 6 seconds per move and achieves an impressive 90% win rate against pure DCNN. Our best model, darkfores2, combined with MCTS of 5000 rollouts, has a 95.5% win rate over Pachi with 10,000 rollouts. 2 METHOD Using Neural Network as a function approximator and pattern matcher to predict the next move of Go is a long-standing idea [Sutskever & Nair (2008); Richards et al. (1998); Schraudolph et al.",
      "startOffset" : 108,
      "endOffset" : 1844
    }, {
      "referenceID" : 1,
      "context" : "player is still behind compared to traditional open-source engines based on Monte-Carlo Tree Search (MCTS) [Browne et al. (2012); Kocsis & Szepesvári (2006)], let alone commercial ones. In this paper, we show that DCNN-based move predictions indeed give a strong Go AI, if properly trained. In particular, we carefully design the training process and choose to predict next k moves rather than the immediate next move to enrich the gradient signal. Despite our prediction giving a mere 2% boost for accuracy of move predictions, the win rate against open-source engines (e.g., Pachi and Fuego) in heavy search scenarios (e.g., 100k rollouts) is more than 6 times higher (Pachi: 11.0% vs 73.7%, Fuego: 12.5% vs 91.0%) than current state-of-the-art DCNN-based player [Maddison et al. (2015)]. In addition, we release our two bots, called darkforest and darkfores1, to public KGS Go server for free games. A collection of more than 2000 games against humans over three months show that the search-less darkforest holds a decent 1d-2d level, much better than the neural network based AI proposed by Clark & Storkey (2015) that holds 4-5 kyu estimated from games against other MCTS-based Go engines. Our bots also share the common weakness of DCNN-based methods in local tactics. We show that, by combining DCNN with MCTS, our bots can address the local tactics weakness and greatly boost the strength. With only 1000 rollouts, our synchronized version of DCNN+MCTS runs 6 seconds per move and achieves an impressive 90% win rate against pure DCNN. Our best model, darkfores2, combined with MCTS of 5000 rollouts, has a 95.5% win rate over Pachi with 10,000 rollouts. 2 METHOD Using Neural Network as a function approximator and pattern matcher to predict the next move of Go is a long-standing idea [Sutskever & Nair (2008); Richards et al. (1998); Schraudolph et al. (1994); Enzenberger (1996)].",
      "startOffset" : 108,
      "endOffset" : 1871
    }, {
      "referenceID" : 1,
      "context" : "player is still behind compared to traditional open-source engines based on Monte-Carlo Tree Search (MCTS) [Browne et al. (2012); Kocsis & Szepesvári (2006)], let alone commercial ones. In this paper, we show that DCNN-based move predictions indeed give a strong Go AI, if properly trained. In particular, we carefully design the training process and choose to predict next k moves rather than the immediate next move to enrich the gradient signal. Despite our prediction giving a mere 2% boost for accuracy of move predictions, the win rate against open-source engines (e.g., Pachi and Fuego) in heavy search scenarios (e.g., 100k rollouts) is more than 6 times higher (Pachi: 11.0% vs 73.7%, Fuego: 12.5% vs 91.0%) than current state-of-the-art DCNN-based player [Maddison et al. (2015)]. In addition, we release our two bots, called darkforest and darkfores1, to public KGS Go server for free games. A collection of more than 2000 games against humans over three months show that the search-less darkforest holds a decent 1d-2d level, much better than the neural network based AI proposed by Clark & Storkey (2015) that holds 4-5 kyu estimated from games against other MCTS-based Go engines. Our bots also share the common weakness of DCNN-based methods in local tactics. We show that, by combining DCNN with MCTS, our bots can address the local tactics weakness and greatly boost the strength. With only 1000 rollouts, our synchronized version of DCNN+MCTS runs 6 seconds per move and achieves an impressive 90% win rate against pure DCNN. Our best model, darkfores2, combined with MCTS of 5000 rollouts, has a 95.5% win rate over Pachi with 10,000 rollouts. 2 METHOD Using Neural Network as a function approximator and pattern matcher to predict the next move of Go is a long-standing idea [Sutskever & Nair (2008); Richards et al. (1998); Schraudolph et al. (1994); Enzenberger (1996)].",
      "startOffset" : 108,
      "endOffset" : 1891
    }, {
      "referenceID" : 1,
      "context" : "player is still behind compared to traditional open-source engines based on Monte-Carlo Tree Search (MCTS) [Browne et al. (2012); Kocsis & Szepesvári (2006)], let alone commercial ones. In this paper, we show that DCNN-based move predictions indeed give a strong Go AI, if properly trained. In particular, we carefully design the training process and choose to predict next k moves rather than the immediate next move to enrich the gradient signal. Despite our prediction giving a mere 2% boost for accuracy of move predictions, the win rate against open-source engines (e.g., Pachi and Fuego) in heavy search scenarios (e.g., 100k rollouts) is more than 6 times higher (Pachi: 11.0% vs 73.7%, Fuego: 12.5% vs 91.0%) than current state-of-the-art DCNN-based player [Maddison et al. (2015)]. In addition, we release our two bots, called darkforest and darkfores1, to public KGS Go server for free games. A collection of more than 2000 games against humans over three months show that the search-less darkforest holds a decent 1d-2d level, much better than the neural network based AI proposed by Clark & Storkey (2015) that holds 4-5 kyu estimated from games against other MCTS-based Go engines. Our bots also share the common weakness of DCNN-based methods in local tactics. We show that, by combining DCNN with MCTS, our bots can address the local tactics weakness and greatly boost the strength. With only 1000 rollouts, our synchronized version of DCNN+MCTS runs 6 seconds per move and achieves an impressive 90% win rate against pure DCNN. Our best model, darkfores2, combined with MCTS of 5000 rollouts, has a 95.5% win rate over Pachi with 10,000 rollouts. 2 METHOD Using Neural Network as a function approximator and pattern matcher to predict the next move of Go is a long-standing idea [Sutskever & Nair (2008); Richards et al. (1998); Schraudolph et al. (1994); Enzenberger (1996)]. Recent progress [Maddison et al. (2015); Clark & Storkey (2015)] uses Deep Convolutional Neural Network (DCNN) for move prediction, and shows substantial improvement over shallow networks or linear function approximators based on manually designed features or simple patterns extracted from previous games [Silver (2009)].",
      "startOffset" : 108,
      "endOffset" : 1933
    }, {
      "referenceID" : 1,
      "context" : "player is still behind compared to traditional open-source engines based on Monte-Carlo Tree Search (MCTS) [Browne et al. (2012); Kocsis & Szepesvári (2006)], let alone commercial ones. In this paper, we show that DCNN-based move predictions indeed give a strong Go AI, if properly trained. In particular, we carefully design the training process and choose to predict next k moves rather than the immediate next move to enrich the gradient signal. Despite our prediction giving a mere 2% boost for accuracy of move predictions, the win rate against open-source engines (e.g., Pachi and Fuego) in heavy search scenarios (e.g., 100k rollouts) is more than 6 times higher (Pachi: 11.0% vs 73.7%, Fuego: 12.5% vs 91.0%) than current state-of-the-art DCNN-based player [Maddison et al. (2015)]. In addition, we release our two bots, called darkforest and darkfores1, to public KGS Go server for free games. A collection of more than 2000 games against humans over three months show that the search-less darkforest holds a decent 1d-2d level, much better than the neural network based AI proposed by Clark & Storkey (2015) that holds 4-5 kyu estimated from games against other MCTS-based Go engines. Our bots also share the common weakness of DCNN-based methods in local tactics. We show that, by combining DCNN with MCTS, our bots can address the local tactics weakness and greatly boost the strength. With only 1000 rollouts, our synchronized version of DCNN+MCTS runs 6 seconds per move and achieves an impressive 90% win rate against pure DCNN. Our best model, darkfores2, combined with MCTS of 5000 rollouts, has a 95.5% win rate over Pachi with 10,000 rollouts. 2 METHOD Using Neural Network as a function approximator and pattern matcher to predict the next move of Go is a long-standing idea [Sutskever & Nair (2008); Richards et al. (1998); Schraudolph et al. (1994); Enzenberger (1996)]. Recent progress [Maddison et al. (2015); Clark & Storkey (2015)] uses Deep Convolutional Neural Network (DCNN) for move prediction, and shows substantial improvement over shallow networks or linear function approximators based on manually designed features or simple patterns extracted from previous games [Silver (2009)].",
      "startOffset" : 108,
      "endOffset" : 1957
    }, {
      "referenceID" : 1,
      "context" : "player is still behind compared to traditional open-source engines based on Monte-Carlo Tree Search (MCTS) [Browne et al. (2012); Kocsis & Szepesvári (2006)], let alone commercial ones. In this paper, we show that DCNN-based move predictions indeed give a strong Go AI, if properly trained. In particular, we carefully design the training process and choose to predict next k moves rather than the immediate next move to enrich the gradient signal. Despite our prediction giving a mere 2% boost for accuracy of move predictions, the win rate against open-source engines (e.g., Pachi and Fuego) in heavy search scenarios (e.g., 100k rollouts) is more than 6 times higher (Pachi: 11.0% vs 73.7%, Fuego: 12.5% vs 91.0%) than current state-of-the-art DCNN-based player [Maddison et al. (2015)]. In addition, we release our two bots, called darkforest and darkfores1, to public KGS Go server for free games. A collection of more than 2000 games against humans over three months show that the search-less darkforest holds a decent 1d-2d level, much better than the neural network based AI proposed by Clark & Storkey (2015) that holds 4-5 kyu estimated from games against other MCTS-based Go engines. Our bots also share the common weakness of DCNN-based methods in local tactics. We show that, by combining DCNN with MCTS, our bots can address the local tactics weakness and greatly boost the strength. With only 1000 rollouts, our synchronized version of DCNN+MCTS runs 6 seconds per move and achieves an impressive 90% win rate against pure DCNN. Our best model, darkfores2, combined with MCTS of 5000 rollouts, has a 95.5% win rate over Pachi with 10,000 rollouts. 2 METHOD Using Neural Network as a function approximator and pattern matcher to predict the next move of Go is a long-standing idea [Sutskever & Nair (2008); Richards et al. (1998); Schraudolph et al. (1994); Enzenberger (1996)]. Recent progress [Maddison et al. (2015); Clark & Storkey (2015)] uses Deep Convolutional Neural Network (DCNN) for move prediction, and shows substantial improvement over shallow networks or linear function approximators based on manually designed features or simple patterns extracted from previous games [Silver (2009)].",
      "startOffset" : 108,
      "endOffset" : 2214
    }, {
      "referenceID" : 1,
      "context" : "player is still behind compared to traditional open-source engines based on Monte-Carlo Tree Search (MCTS) [Browne et al. (2012); Kocsis & Szepesvári (2006)], let alone commercial ones. In this paper, we show that DCNN-based move predictions indeed give a strong Go AI, if properly trained. In particular, we carefully design the training process and choose to predict next k moves rather than the immediate next move to enrich the gradient signal. Despite our prediction giving a mere 2% boost for accuracy of move predictions, the win rate against open-source engines (e.g., Pachi and Fuego) in heavy search scenarios (e.g., 100k rollouts) is more than 6 times higher (Pachi: 11.0% vs 73.7%, Fuego: 12.5% vs 91.0%) than current state-of-the-art DCNN-based player [Maddison et al. (2015)]. In addition, we release our two bots, called darkforest and darkfores1, to public KGS Go server for free games. A collection of more than 2000 games against humans over three months show that the search-less darkforest holds a decent 1d-2d level, much better than the neural network based AI proposed by Clark & Storkey (2015) that holds 4-5 kyu estimated from games against other MCTS-based Go engines. Our bots also share the common weakness of DCNN-based methods in local tactics. We show that, by combining DCNN with MCTS, our bots can address the local tactics weakness and greatly boost the strength. With only 1000 rollouts, our synchronized version of DCNN+MCTS runs 6 seconds per move and achieves an impressive 90% win rate against pure DCNN. Our best model, darkfores2, combined with MCTS of 5000 rollouts, has a 95.5% win rate over Pachi with 10,000 rollouts. 2 METHOD Using Neural Network as a function approximator and pattern matcher to predict the next move of Go is a long-standing idea [Sutskever & Nair (2008); Richards et al. (1998); Schraudolph et al. (1994); Enzenberger (1996)]. Recent progress [Maddison et al. (2015); Clark & Storkey (2015)] uses Deep Convolutional Neural Network (DCNN) for move prediction, and shows substantial improvement over shallow networks or linear function approximators based on manually designed features or simple patterns extracted from previous games [Silver (2009)]. In this paper, we train a DCNN that predicts the next k moves given the current board situation as an input. We treat the 19× 19 board as a 19× 19 image with multiple channels. Each channel encodes a different aspect of board information, e.g., liberties (Fig. 1(b)). Compared to previous works, we use a more compact feature set and predict long-term moves, and show that they lead to a substantial performance boost in terms of win rate against open source engines. 2.1 FEATURE CHANNELS Table 1 shows the features extracted from the current board situation. Each feature is a binary 19×19 map except for history information and position mask, which are real numbers in [0, 1]. History is encoded as exp(−t ∗ 0.1), where t is how long the stone has been placed. The exponential temporal decay is meant to enable the network to focus on the recent battle. Position mark is defined as exp(− 12 l ), where l is the squared L2 distance to the board center. It is used to encode the relative position of each intersection. There are two differences between our features and those in Maddison et al. (2015). First, we use relative coding (our/opponent) for almost all the features.",
      "startOffset" : 108,
      "endOffset" : 3318
    }, {
      "referenceID" : 1,
      "context" : "player is still behind compared to traditional open-source engines based on Monte-Carlo Tree Search (MCTS) [Browne et al. (2012); Kocsis & Szepesvári (2006)], let alone commercial ones. In this paper, we show that DCNN-based move predictions indeed give a strong Go AI, if properly trained. In particular, we carefully design the training process and choose to predict next k moves rather than the immediate next move to enrich the gradient signal. Despite our prediction giving a mere 2% boost for accuracy of move predictions, the win rate against open-source engines (e.g., Pachi and Fuego) in heavy search scenarios (e.g., 100k rollouts) is more than 6 times higher (Pachi: 11.0% vs 73.7%, Fuego: 12.5% vs 91.0%) than current state-of-the-art DCNN-based player [Maddison et al. (2015)]. In addition, we release our two bots, called darkforest and darkfores1, to public KGS Go server for free games. A collection of more than 2000 games against humans over three months show that the search-less darkforest holds a decent 1d-2d level, much better than the neural network based AI proposed by Clark & Storkey (2015) that holds 4-5 kyu estimated from games against other MCTS-based Go engines. Our bots also share the common weakness of DCNN-based methods in local tactics. We show that, by combining DCNN with MCTS, our bots can address the local tactics weakness and greatly boost the strength. With only 1000 rollouts, our synchronized version of DCNN+MCTS runs 6 seconds per move and achieves an impressive 90% win rate against pure DCNN. Our best model, darkfores2, combined with MCTS of 5000 rollouts, has a 95.5% win rate over Pachi with 10,000 rollouts. 2 METHOD Using Neural Network as a function approximator and pattern matcher to predict the next move of Go is a long-standing idea [Sutskever & Nair (2008); Richards et al. (1998); Schraudolph et al. (1994); Enzenberger (1996)]. Recent progress [Maddison et al. (2015); Clark & Storkey (2015)] uses Deep Convolutional Neural Network (DCNN) for move prediction, and shows substantial improvement over shallow networks or linear function approximators based on manually designed features or simple patterns extracted from previous games [Silver (2009)]. In this paper, we train a DCNN that predicts the next k moves given the current board situation as an input. We treat the 19× 19 board as a 19× 19 image with multiple channels. Each channel encodes a different aspect of board information, e.g., liberties (Fig. 1(b)). Compared to previous works, we use a more compact feature set and predict long-term moves, and show that they lead to a substantial performance boost in terms of win rate against open source engines. 2.1 FEATURE CHANNELS Table 1 shows the features extracted from the current board situation. Each feature is a binary 19×19 map except for history information and position mask, which are real numbers in [0, 1]. History is encoded as exp(−t ∗ 0.1), where t is how long the stone has been placed. The exponential temporal decay is meant to enable the network to focus on the recent battle. Position mark is defined as exp(− 12 l ), where l is the squared L2 distance to the board center. It is used to encode the relative position of each intersection. There are two differences between our features and those in Maddison et al. (2015). First, we use relative coding (our/opponent) for almost all the features. In contrast, the features in Maddison et al. (2015) are largely player-agnostic.",
      "startOffset" : 108,
      "endOffset" : 3445
    }, {
      "referenceID" : 6,
      "context" : "In comparison, Maddison et al. (2015) uses such features like liberties after the move, captures after the move, etc.",
      "startOffset" : 15,
      "endOffset" : 38
    }, {
      "referenceID" : 6,
      "context" : "In comparison, Maddison et al. (2015) uses such features like liberties after the move, captures after the move, etc. We use a similar way to encode rank in 9 planes as in Maddison et al. (2015). That is, all kyu-players have all nine planes zero, 1d players has their first plane all-1, 2d players have their second plane all-1, etc.",
      "startOffset" : 15,
      "endOffset" : 195
    }, {
      "referenceID" : 6,
      "context" : "In comparison, Maddison et al. (2015) uses such features like liberties after the move, captures after the move, etc. We use a similar way to encode rank in 9 planes as in Maddison et al. (2015). That is, all kyu-players have all nine planes zero, 1d players has their first plane all-1, 2d players have their second plane all-1, etc. For 9d and professional players, all the planes are filled with 1. 2.2 NETWORK ARCHITECTURE Fig. 3 shows the architecture of the network for our best model. We use a 12-layered (d = 12) full convolutional network. Each convolution layer is followed by a ReLU nonlinearity. Except for the first layer, all layers use the same width w = 384. No weight sharing is used. We do not use pooling since they negatively affect the performance. Instead of using two softmax outputs [Maddison et al. (2015)] to predict black and white moves, we only use one softmax layer to predict the next move, reducing the number of parameters.",
      "startOffset" : 15,
      "endOffset" : 831
    }, {
      "referenceID" : 6,
      "context" : "Unlike Maddison et al. (2015) that uses asynchronous stochastic gradient descent, we just use vanilla SGD on 4 NVidia K40m GPUs in a single machine to train the entire network (for some models we use 3 GPUs with 255 as the batch size).",
      "startOffset" : 7,
      "endOffset" : 30
    }, {
      "referenceID" : 6,
      "context" : "Unlike Maddison et al. (2015) that uses asynchronous stochastic gradient descent, we just use vanilla SGD on 4 NVidia K40m GPUs in a single machine to train the entire network (for some models we use 3 GPUs with 255 as the batch size). Each epoch lasts about 5 to 6 hours. The learning rate is set to be 0.05. Typically, the model starts to converge within one epoch and shows good performance after 50-60 epochs (around two weeks). Reducing the learning rate after the performance stalls will increase the performance even further. 2.5 MONTE CARLO TREE SEARCH From the experiments, we clearly show that DCNN is tactically weak due to the lack of search. Search is a way to explore the solution space conditioned on the current board situation, and build a non-parametric local model for the game. The local model is more flexible than the global model learned from massive training data and more adapted to the current situation. The state-of-the-art approach in computer Go is Monte-Carlo Tree Search (MCTS). Fig. 4 shows its basic principle. Combining DCNN with MCTS requires nontrivial engineering efforts because each rollout of MCTS is way much faster than DCNN evaluation. Therefore, these two must run in parallel with frequent communications. Our basic implementation of MCTS gives 16k rollouts per second (for 16 threads on a machine with Intel Xeon CPU E5-2680 v2 at 2.80GHz) while it typically takes 0.2s for DCNN to give board evaluations of a batch size of 128 with 4 GPUs. There are two ways to address this problem. In asynchronized implementation used in Maddison et al. (2015), MCTS sends the newly expanded node to DCNN but is not blocked by DCNN evaluation.",
      "startOffset" : 7,
      "endOffset" : 1595
    }, {
      "referenceID" : 5,
      "context" : "Note that our implementation is not directly comparable to the asynchronized version in Maddison et al. (2015), achieving 86.",
      "startOffset" : 88,
      "endOffset" : 111
    }, {
      "referenceID" : 5,
      "context" : "Note that our implementation is not directly comparable to the asynchronized version in Maddison et al. (2015), achieving 86.7% with 100k rollouts. 3 EXPERIMENTS 3.1 SETUP We use the public KGS dataset (∼170k games), which is used in Maddison et al. (2015). We use all games before 2012 as the training set and 2013-2015 games as the test set.",
      "startOffset" : 88,
      "endOffset" : 257
    }, {
      "referenceID" : 5,
      "context" : "Note that our implementation is not directly comparable to the asynchronized version in Maddison et al. (2015), achieving 86.7% with 100k rollouts. 3 EXPERIMENTS 3.1 SETUP We use the public KGS dataset (∼170k games), which is used in Maddison et al. (2015). We use all games before 2012 as the training set and 2013-2015 games as the test set. This leads to 144,748 games for training and 26,814 games for testing. We also use GoGoD dataset1 (∼80k games), which is also used in Clark & Storkey (2015). 75,172 games are used for training and 2,592 for testing.",
      "startOffset" : 88,
      "endOffset" : 501
    }, {
      "referenceID" : 5,
      "context" : "Note that our implementation is not directly comparable to the asynchronized version in Maddison et al. (2015), achieving 86.7% with 100k rollouts. 3 EXPERIMENTS 3.1 SETUP We use the public KGS dataset (∼170k games), which is used in Maddison et al. (2015). We use all games before 2012 as the training set and 2013-2015 games as the test set. This leads to 144,748 games for training and 26,814 games for testing. We also use GoGoD dataset1 (∼80k games), which is also used in Clark & Storkey (2015). 75,172 games are used for training and 2,592 for testing. For evaluation, our model competes with GnuGo, Pachi [Baudis & Gailly (2012)] and Fuego [Enzenberger et al.",
      "startOffset" : 88,
      "endOffset" : 637
    }, {
      "referenceID" : 4,
      "context" : "For evaluation, our model competes with GnuGo, Pachi [Baudis & Gailly (2012)] and Fuego [Enzenberger et al. (2010)].",
      "startOffset" : 89,
      "endOffset" : 115
    }, {
      "referenceID" : 4,
      "context" : "For evaluation, our model competes with GnuGo, Pachi [Baudis & Gailly (2012)] and Fuego [Enzenberger et al. (2010)]. We use GnuGo 3.8 level 10, Pachi 11.99 (Genjo-devel) with the pattern files, and Fuego 1.1 throughout our experiments. 3.2 MOVE PREDICTION Table 3 shows the performance comparison for move prediction. For models that predict the next k moves, we only evaluate their prediction accuracy for the immediate next move. Maddison et al. (2015) d=12,w=384 d=12,w=512 d=16,w=512 d=17,w=512 55.",
      "startOffset" : 89,
      "endOffset" : 455
    }, {
      "referenceID" : 4,
      "context" : "For evaluation, our model competes with GnuGo, Pachi [Baudis & Gailly (2012)] and Fuego [Enzenberger et al. (2010)]. We use GnuGo 3.8 level 10, Pachi 11.99 (Genjo-devel) with the pattern files, and Fuego 1.1 throughout our experiments. 3.2 MOVE PREDICTION Table 3 shows the performance comparison for move prediction. For models that predict the next k moves, we only evaluate their prediction accuracy for the immediate next move. Maddison et al. (2015) d=12,w=384 d=12,w=512 d=16,w=512 d=17,w=512 55.2 57.1 57.3 56.6 56.4 Table 3: Comparison of Top-1 accuracies for next move predictions using standard features. d is the depth of the model while w is the number of filters at convolutional layers (except the first layer). With our training framework, we are able to achieve slightly higher Top-1 prediction accuracy (after hundreds of epochs) compared to Maddison et al. (2015). Note that using standard or extended features seem to have marginal gains (Fig.",
      "startOffset" : 89,
      "endOffset" : 882
    }, {
      "referenceID" : 6,
      "context" : "0 Maddison et al. (2015) 97.",
      "startOffset" : 2,
      "endOffset" : 25
    }, {
      "referenceID" : 6,
      "context" : "0 Maddison et al. (2015) 97.2 47.4 11.0 23.3 12.5 darkforest 96.7± 1.7 67.3± 3.2 23.3± 3.0 86.6± 0.5 52.2± 1.9 darkfores1 99.3± 0.7 86.7± 3.1 51.7± 0.1 94.3± 0.9 78.9± 2.6 darkfores2 99.7± 0.3 94.3± 1.7 73.7± 0.9 99.3± 0.7 91.0± 4.1 Table 4: Win rate comparison against open source engines between our model and previous works. For each setting, 3 groups of 100 games are played. We report the average win rate and standard deviation computed from group averages. All the game experiments mentioned in this paper use komi 7.5 and Chinese rules. Pondering (keep searching when the opponent is thinking) in Pachi and Fuego are on. Note that in Clark & Storkey (2015), they control the time per move as 10 sec/move on 2x 1.",
      "startOffset" : 2,
      "endOffset" : 665
    }, {
      "referenceID" : 6,
      "context" : "0 Maddison et al. (2015) 97.2 47.4 11.0 23.3 12.5 darkforest 96.7± 1.7 67.3± 3.2 23.3± 3.0 86.6± 0.5 52.2± 1.9 darkfores1 99.3± 0.7 86.7± 3.1 51.7± 0.1 94.3± 0.9 78.9± 2.6 darkfores2 99.7± 0.3 94.3± 1.7 73.7± 0.9 99.3± 0.7 91.0± 4.1 Table 4: Win rate comparison against open source engines between our model and previous works. For each setting, 3 groups of 100 games are played. We report the average win rate and standard deviation computed from group averages. All the game experiments mentioned in this paper use komi 7.5 and Chinese rules. Pondering (keep searching when the opponent is thinking) in Pachi and Fuego are on. Note that in Clark & Storkey (2015), they control the time per move as 10 sec/move on 2x 1.6 GHz cores, instead of fixing the rollout number.3 We also train a smaller model with w = 144 whose number of parameters are comparable to Maddison et al. (2015). Our smaller model achieves 43.",
      "startOffset" : 2,
      "endOffset" : 883
    }, {
      "referenceID" : 6,
      "context" : "In contrast, Maddison et al. (2015) reports 47.",
      "startOffset" : 13,
      "endOffset" : 36
    }, {
      "referenceID" : 6,
      "context" : "In contrast, Maddison et al. (2015) reports 47.4% and does not mention pondering status. Darkforest AI Bots. We build three bots from the trained models. Our first bot darkforest is trained using standard features, 1 step prediction on KGS dataset. The second bot darkfores1 is trained using extended features, 3 step prediction on GoGoD dataset. Both bots are trained with constant learning rate 0.05. Based on darkfores1, we fine-tuned the learning rate to create an even stronger DCNN player, darkfores2. Table 4 shows their strengths against open source engines. It seems that despite the fact that GoGoD is smaller, our model can be trained faster with better performance, presumably because GoGoD contains all professional games, while games from KGS Server are a bit noisy. Win rates between pairs of the three bots (Table 5) are also consistent with their performances against open source engines. We also compare darkforest with a public DCNN model4. Since both models are deterministic, we sample the moves according to the softmax probability. We played two sets of 100 games; the win rate is 100% and 99% respectively. Darkforest always wins if using their strongest moves or sampling from top 5 confident moves. 3.4 EVALUATION ON KGS GO SERVER We also put our bots onto KGS Go server and check their performance over three months period. Darkforest became publicly available on Aug 31, 2015. Since then it has played about 2000 games. Recently we also release the improved version darkfores1 on Nov 2, 2015. To score the endgame board situations, we randomly run 1000 trials of default policy to find the dead stones, followed by standard Tromp-Taylor scoring. If all 1000 trials show losing by 10+ points, they resign. In KGS Go server, their levels are around KGS 1d-2d, while darkfores1 is slightly stronger, consistent with its better win rate against open source engines. Table 6 shows the statistics. In addition, they also played against 4d players and had won 3 games out of 9. This is a major improvement upon the AI developed in Clark & Storkey (2015) that holds 4k-5k level, estimated by playing against Go engines.",
      "startOffset" : 13,
      "endOffset" : 2075
    }, {
      "referenceID" : 1,
      "context" : "8, and use UCT [Browne et al. (2012)] to select moves for tree expansion.",
      "startOffset" : 16,
      "endOffset" : 37
    }, {
      "referenceID" : 1,
      "context" : "8, and use UCT [Browne et al. (2012)] to select moves for tree expansion. Note that the DCNN confidences of the selected moves are not used in UCT. Default policy: Following Pachi’s implementation [Baudis & Gailly (2012)], we use 3x3 patterns, opponent atari points, detection of nakade points and avoidance of self-atari for default policy.",
      "startOffset" : 16,
      "endOffset" : 221
    }, {
      "referenceID" : 6,
      "context" : "In comparison, an asynchronized version is used in Maddison et al. (2015) that achieves 86.",
      "startOffset" : 51,
      "endOffset" : 74
    }, {
      "referenceID" : 11,
      "context" : ", Policy Gradient [Sutton et al. (1999)]) after MCTS completes and chooses a different best move than DCNN’s proposal.",
      "startOffset" : 19,
      "endOffset" : 40
    }, {
      "referenceID" : 11,
      "context" : ", Policy Gradient [Sutton et al. (1999)]) after MCTS completes and chooses a different best move than DCNN’s proposal. To increase the signal bandwidth, we could also update weights using all the board situations along the trajectory of the best move. Alternatively, we could update the weights when MCTS is running. Actor-Critics algorithms [Konda & Tsitsiklis (1999)] can also be used to train two models simultaneously, one to predict the next move (actor) and the other to evaluate the current board situation (critic).",
      "startOffset" : 19,
      "endOffset" : 369
    } ],
    "year" : 2015,
    "abstractText" : "Competing with top human players in the ancient game of Go has been a longterm goal of artificial intelligence. Go’s high branching factor makes traditional search techniques ineffective, even on leading-edge hardware, and Go’s evaluation function could change drastically with one stone change. Recent works [Maddison et al. (2015); Clark & Storkey (2015)] show that search is not strictly necessary for machine Go players. A pure pattern-matching approach, based on a Deep Convolutional Neural Network (DCNN) that predicts the next move, can perform as well as Monte Carlo Tree Search (MCTS)-based open source Go engines such as Pachi [Baudis & Gailly (2012)] if its search budget is limited. We extend this idea in our bot named darkforest, which relies on a DCNN designed for long-term predictions. Darkforest substantially improves the win rate for patternmatching approaches against MCTS-based approaches, even with looser search budgets. Against human players, darkforest achieves a stable 1d-2d level on KGS Go Server, estimated from free games against human players. This substantially improves the estimated rankings reported in Clark & Storkey (2015), where DCNN-based bots are estimated at 4k-5k level based on performance against other machine players. Adding MCTS to darkforest creates a much stronger player: with only 1000 rollouts, darkforest+MCTS beats pure darkforest 90% of the time; with 5000 rollouts, our best model plus MCTS beats Pachi with 10,000 rollouts 95.5% of the time. 1 INTRODUCTION For a long time, computer Go is considered to be a grand challenge in artificial intelligence. Fig. 1 shows a simple illustration of the game of Go. Two players, black and white, place stones at intersections in turn on a 19x19 board (Fig. 1(a)). Black plays first on an empty board. A 4-connected component of the same color is called a group. The liberties of a group is the number of its neighboring empty intersections (Fig. 1(b)). A group is captured if its liberties are zero. The goal of the game is to control more territory than the opponent (Fig. 1(c)). Fig. 1(d)) shows the Go rating system, ranging from kyu level (beginner to decent amateur, 30k-1k) to dan level (advanced amateur, 1d-7d) and to professional levels (1p-9p) [Silver (2009)]. Go is difficult due to its high branching factors (typically on the order of hundred on a 19x19 board) and subtle board situations that are sensitive to small changes (adding/removing one stone could alter the life/death situation of a large group of stone and thus completely changes the final score). A combination of the two implies that the only solution is to use massive search that requires a prohibitive amount of resources, which is not attainable with cutting-edge hardware. Fortunately, recent works [Maddison et al. (2015); Clark & Storkey (2015)] in Computer Go have shown that the Go board situation could be deciphered with Deep Convolutional Neural Network (DCNN). They can predict the next move that a human would play 55.2% of the time. However, whether this accuracy leads to a strong Go AI is not yet well understood. It is possible that DCNN correctly predicts most regular plays by looking at the correlation of local patterns, but still fails to predict the critical one or two moves and loses the game. Indeed, a DCNN-based 1 ar X iv :1 51 1. 06 41 0v 1 [ cs .L G ] 1 9 N ov 2 01 5 Under review as a conference paper at ICLR 2016 30k\t\r   1k\t\r   1d\t\r   7d\t\r   1p\t\r   9p (b) (a) (d) B W (c) Figure 1: A simple illustrations on Go rules and rating system. Images are from Internet. player is still behind compared to traditional open-source engines based on Monte-Carlo Tree Search (MCTS) [Browne et al. (2012); Kocsis & Szepesvári (2006)], let alone commercial ones. In this paper, we show that DCNN-based move predictions indeed give a strong Go AI, if properly trained. In particular, we carefully design the training process and choose to predict next k moves rather than the immediate next move to enrich the gradient signal. Despite our prediction giving a mere 2% boost for accuracy of move predictions, the win rate against open-source engines (e.g., Pachi and Fuego) in heavy search scenarios (e.g., 100k rollouts) is more than 6 times higher (Pachi: 11.0% vs 73.7%, Fuego: 12.5% vs 91.0%) than current state-of-the-art DCNN-based player [Maddison et al. (2015)]. In addition, we release our two bots, called darkforest and darkfores1, to public KGS Go server for free games. A collection of more than 2000 games against humans over three months show that the search-less darkforest holds a decent 1d-2d level, much better than the neural network based AI proposed by Clark & Storkey (2015) that holds 4-5 kyu estimated from games against other MCTS-based Go engines. Our bots also share the common weakness of DCNN-based methods in local tactics. We show that, by combining DCNN with MCTS, our bots can address the local tactics weakness and greatly boost the strength. With only 1000 rollouts, our synchronized version of DCNN+MCTS runs 6 seconds per move and achieves an impressive 90% win rate against pure DCNN. Our best model, darkfores2, combined with MCTS of 5000 rollouts, has a 95.5% win rate over Pachi with 10,000 rollouts. 2 METHOD Using Neural Network as a function approximator and pattern matcher to predict the next move of Go is a long-standing idea [Sutskever & Nair (2008); Richards et al. (1998); Schraudolph et al. (1994); Enzenberger (1996)]. Recent progress [Maddison et al. (2015); Clark & Storkey (2015)] uses Deep Convolutional Neural Network (DCNN) for move prediction, and shows substantial improvement over shallow networks or linear function approximators based on manually designed features or simple patterns extracted from previous games [Silver (2009)]. In this paper, we train a DCNN that predicts the next k moves given the current board situation as an input. We treat the 19× 19 board as a 19× 19 image with multiple channels. Each channel encodes a different aspect of board information, e.g., liberties (Fig. 1(b)). Compared to previous works, we use a more compact feature set and predict long-term moves, and show that they lead to a substantial performance boost in terms of win rate against open source engines. 2.1 FEATURE CHANNELS Table 1 shows the features extracted from the current board situation. Each feature is a binary 19×19 map except for history information and position mask, which are real numbers in [0, 1]. History is encoded as exp(−t ∗ 0.1), where t is how long the stone has been placed. The exponential temporal decay is meant to enable the network to focus on the recent battle. Position mark is defined as exp(− 12 l ), where l is the squared L2 distance to the board center. It is used to encode the relative position of each intersection. There are two differences between our features and those in Maddison et al. (2015). First, we use relative coding (our/opponent) for almost all the features. In contrast, the features in Maddison et al. (2015) are largely player-agnostic. Second, our feature set is simpler and compact (25 vs. 36 input 2 Under review as a conference paper at ICLR 2016",
    "creator" : "LaTeX with hyperref package"
  }
}