{
  "name" : "1709.00643.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Fast Image Processing with Fully-Convolutional Networks",
    "authors" : [ "Qifeng Chen", "Jia Xu", "Vladlen Koltun" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "∗Joint first authors\nmodels, multiscale tone and detail manipulation, photographic style transfer, nonlocal dehazing, and nonphotorealistic stylization. All operators are approximated by the same model. Experiments demonstrate that the presented approach is significantly more accurate than prior approximation schemes. It increases approximation accuracy as measured by PSNR across the evaluated operators by 8.5 dB on the MIT-Adobe dataset (from 27.5 to 36 dB) and reduces DSSIM by a multiplicative factor of 3 compared to the most accurate prior approximation scheme, while being the fastest. We show that our models generalize across datasets and across resolutions, and investigate a number of extensions of the presented approach.\n1\nar X"
    }, {
      "heading" : "1. Introduction",
      "text" : "Research in image processing has yielded a variety of advanced operators that produce visually striking effects. Techniques developed in the last decade can dramatically enhance detail [24, 69, 26, 28, 60], transform the image by applying a master photographer’s style [7, 5], smooth the image for the purpose of abstraction [73, 76, 79], and eliminate the effects of atmospheric scattering [25, 35, 27, 9]. This is accomplished by a variety of algorithmic approaches, including variational methods, gradient-domain processing, high-dimensional filtering, and manipulation of multiscale representations.\nThe computational demands and running times of existing operators vary greatly. Some operators, such as bilateral filtering, have benefitted from more than a decade of concerted investment in their acceleration. Others still take seconds or even minutes for high-resolution images. While most existing techniques can be accelerated by experts given sufficient research and development time, such acceleration schemes often require significant expertise and may not generalize across operators.\nOne general approach to accelerating a broad range of image processing operators is well-known: downsample the image, execute the operator at low resolution, and upsample [45, 34, 14]. This approach suffers from two significant drawbacks. First, the original operator must still be evaluated on a lower-resolution image. This can be a severe handicap because some operators are slow and existing implementations cannot be executed at interactive rates even at low resolution. Second, since the operator is never evaluated at the original resolution, its effects on the highfrequency content of the image may not be modeled properly. This can limit the accuracy of the approximation.\nIn this paper, we investigate an alternative approach to accelerating image processing operators. Like the downsample-evaluate-upsample approach, the presented method approximates the original operator. Unlike the downsampling approach, the method operates on fullresolution images, is trained end-to-end to maximize accuracy, and does not require running the original operator at all. To approximate the operator, we use a convolutional network that is trained on input-output pairs that demonstrate the action of the operator. After training, the network is used in place of the original operator, which need not be run at all.\nWe investigate the effects of different network architectures in terms of three properties that are important for accelerating image processing operators: approximation accuracy, runtime, and compactness. We identify a specific architecture that satisfies all three criteria and show that it approximates a wide variety of standard image processing operators extremely accurately. We evaluate the presented approach on ten advanced image processing opera-\ntors, including multiple forms of variational image smoothing, adaptive detail enhancement, photographic style transfer, and dehazing. All operators are approximated using an identical architecture with no hyperparameter tuning. Five of the trained approximators are demonstrated in Figure 1, which shows their action on images from the MIT-Adobe 5K test set (not seen during training).\nFor all evaluated operators, the presented approximation scheme outperforms the downsampling approach. For example, the PSNR of our approximators across the ten considered operators on the MIT-Adobe test set is 36 dB, compared to 25 dB for the high-accuracy variant of bilateral guided upsampling [14]. At the same time, our approximators are faster than the fastest variant of that scheme. Our approximators run in constant time, independent of the runtime of the original operator.\nWe conduct extensive experiments that demonstrate that our simple approach outperforms a large number of recent and contemporary baselines, and that trained approximators generalize across datasets and to image resolutions not seen during training. We also investigate a number of extensions and show that the presented approach can be used to create parameterized networks that expose parameters that can be used to interactively control the effect of the image processing operator at test time; to train a single network that can emulate many diverse image processing operators and combine their effects; and to process video."
    }, {
      "heading" : "2. Related Work",
      "text" : "Many schemes have been developed for accelerating image processing operators. The bilateral filter in particular has benefitted from long-term investment in its acceleration [21, 72, 15, 59, 2, 1, 29, 8]. Another family of dedicated acceleration schemes addresses the median filter and its variants [72, 61, 54, 80]. Other work has examined the acceleration of variational methods [6, 62, 13, 17], gradient-domain techniques [46], convolutions with large spatial support [23], and local Laplacian filters [5]. (Deep mathematical connections between these families of operators exist [57].) While many of these schemes successfully accelerate their intended families of operators, they do not have the generality we seek.\nA general approach to accelerating image processing operators is to downsample the image, evaluate the operator at low resolution, and upsample [45, 34, 14]. This approach accelerates a broad range of operators by approximating them. It is largely agnostic to the operator but requires that the operator avoid spatial transformation so that the original image can be used to guide the upsampling. (E.g., no spatial warping such as perspective correction.) Our method shares a number of characteristics with the downsampling approach: it targets a broad range of operators, uses an approximation, and assumes that the spatial layout of the im-\nage is preserved. However, our approximation has a much richer parameterization that can model the operator’s effect on the high-frequency content of the image. Once trained, the approximator does not need to execute the original operator at all. We will show that our method is more accurate than the downsampling approach on a wide range of tasks, while being faster.\nOther work on accelerating image processing considers the system infrastructure and programming language. Given a powerful cloud backend and a bandwidth-limited network connection, high-resolution processing can be offloaded to the cloud [32]. Domain-specific languages can be used to schedule image processing pipelines to better utilize available hardware resources [63, 36]. Our work is complementary and provides an approach to approximating a wide variety of operators with a uniform parameterization. Such uniform parameterization and predictable flow of computation can assist further acceleration using dedicated hardware.\nThe closest works to ours are due to Xu et al. [75], Liu et al. [51], and Yan et al. [77]. We review each in turn. Xu et al. [75] used deep networks to approximate a variety of edge-preserving filters. Our work also uses deep networks, but differs in key technical decisions, leading to substantially broader scope and better performance. Specifically, the approach of Xu et al. operates in the gradient domain and requires reconstructing the output image by integrating the gradient field produced by the network. Since their networks produce non-integrable gradient fields, the authors had to constrain the final image reconstruction by introducing an additional data term that forces the output to be similar to the input. For this and other reasons, the approach of Xu et al. only applies to edge-preserving smoothing, has limited approximation accuracy, exhibits high running times (seconds for 1 MP images), and requires operatorspecific hyperparameter tuning. In comparison, we train an approximator end-to-end, pixels to pixels, using a parameterization that is deeper and more context-aware while being more compact. We will demonstrate experimentally that the presented approach yields higher accuracy and lower runtimes while fitting a much bigger family of operators.\nLiu et al. [51] combined a convolutional network and a set of recurrent networks to approximate a variety of image filters. This approach is quite flexible and outperforms the approach of Xu et al. on some operators, but does not achieve the approximation accuracy and speed we seek. We will show that a single convolutional network can achieve higher accuracy, while being faster and more compact.\nYan et al. [77] also applied deep networks to image adjustment. This work is also related to ours in its idea of approximating image transformations by deep networks. However, our work differs substantially in scope, technical approach, and results. Yan et al. use a fully-connected\nnetwork that operates on each pixel separately. The network itself has a receptive field of a single pixel. Contextual information is only provided by hand-crafted input features, instead of being collected adaptively by the network. This places a substantial burden on manual feature design. In contrast, our approximator is a single convolutional network that is trained end-to-end, aggregates spatial context from the image as needed, and does not rely on extraneous modules or preprocessing. This leads to much greater generality, higher accuracy, and faster runtimes.\nDeep networks have been used for denoising [39, 11, 3], super-resolution [10, 19, 40, 42, 41, 48, 50], deblurring [74], restoration of images corrupted by dirt or rain [22], example-based non-photorealistic stylization [30, 70, 40], joint image filtering [49], dehazing [64], and demosaicking [31]. None of the approaches described in these works were intended as broadly applicable replacements for the standard downsample-evaluate-upsample approach to image processing acceleration. Indeed, our experiments have shown that many approaches lack in either speed, accuracy, or compactness when applied across a broad range of operators. These criteria will be explored further in the next section."
    }, {
      "heading" : "3. Method",
      "text" : ""
    }, {
      "heading" : "3.1. Preliminaries",
      "text" : "Let I be an image, represented in the RGB color space. Let f be an operator that transforms the content of an image without modifying its dimensions: that is, I and f(I) have the same resolution. We will consider a variety of operators f that use a broad range of algorithmic techniques. Our goal is to approximate f with another operator f̂ , such that f̂(I) ≈ f(I) for all images I. Note that the resolution of I is not restricted: both the operator f and its approximation f̂ are assumed to operate on variable-resolution images. Furthermore, we will consider many operators {fi} but require that our corresponding approximations {f̂i} all share the same parameterization: same set of parameters, same flow of computation. The approximations will differ only in their parameters, which will be fit for each operator during training.\nOur goal is to find a broadly applicable approach to accelerating image processing operators. We have identified three desirable criteria for such an approach. Accuracy: We seek an approach that provides high approximation accuracy across a broad range of popular image processing operators. Speed: The approach must be fast, ideally achieving interactive rates on HD images. Compactness: We seek an approach that can potentially be deployed within the constraints of mobile devices. An ideal network would have a very compact parameterization that can fit into on-chip SRAM, and a small memory footprint [33].\nOur basic approach is to approximate the operator using a convolutional network [47]. The network must operate on variable-resolution images and must produce an output image at the same resolution as the input. This is known as dense prediction [52]. In principle, any fully-convolutional network architecture can be used for this purpose. Specifically, any network that has been used for a pixelwise classification problem such as semantic segmentation can instead be trained with a regression loss to produce continuous color rather than a discrete label per pixel. However, not all network architectures will yield high accuracy in this regime and most are not compact.\nWe have experimented with a large number of network architectures derived from prior work in high-level vision, specifically on semantic segmentation. We found that when some of these high-level networks are applied to low-level image processing problems, they generally outperform dedicated architectures previously designed for these image processing problems. The key advantage of architectures designed for high-level vision is their large receptive field. Many image processing operators are based on global optimization over the entire image, analysis of global image properties, or nonlocal information aggregation. To model such operators faithfully, the network must collect data from spatially distributed locations, aggregating information at multiple scales that are ultimately large enough to provide a global view of the image.\nIn Section 3.2 we describe an architecture that strikes the best balance between the different desiderata according to our experiments. Three alternative fully-convolutional architectures are described in the supplement."
    }, {
      "heading" : "3.2. Context aggregation networks",
      "text" : "Our primary architecture is the multi-scale context aggregation network (CAN), developed in the context of semantic image analysis [78]. Its intermediate representations and its output have the same resolution as the input. Contextual information is gradually aggregated at increasingly larger scales, such that the computation of each output pixel takes into account all input pixels within a window of size exponential in the network’s depth. This accomplishes global information aggregation for high-resolution images with a very compact parameterization. We will see that this architecture fulfills all of the desiderata outlined above.\nWe now describe the parameterization in detail. The data is laid out over multiple consecutive layers: {L0, . . . ,Ld}. The first and last layers L0,Ld have dimensionality m×n×3. These represent the input and output images. The resolutionm×n varies and is not given in advance.\nEach intermediate layer Ls (1 ≤ s ≤ d− 1) has dimensionality m×n×w, where w is the width of (i.e., the number of feature maps in) each layer. The content of intermediate layer Ls is computed from the content of the previous\nlayer Ls−1 as follows:\nLsi = Φ\n Ψs  bsi + ∑\nj\nLs−1j ∗rs Ksi,j\n    . (1)\nHere Lsi is the i th feature map of layer Ls, Ls−1j is the j th feature map of layer Ls−1, bsi is a scalar bias, and K s i,j is a 3×3 convolution kernel. The operator ∗rs is a dilated convolution with dilation rs. The dilated convolution operator is the means by which the network aggregates long-range contextual information without losing resolution. Specifically, for image coordinates x: ( Ls−1j ∗rs Ksi,j ) (x) = ∑\na+rsb=x\nLs−1j (a)K s i,j(b). (2)\nThe effect of dilation is that the filter is tapped not at adjacent locations in the feature map, but at locations separated by the factor rs. The dilation is increased exponentially with depth: rs = 2s−1 for 1 ≤ s ≤ d− 2. For Ld−1, we do not use dilation. For the output layer Ld we use a linear transformation (1×1 convolution with no nonlinearity) that projects the final layer into the RGB color space.\nFor the pointwise nonlinearity Φ, we use the leaky rectified linear unit (LReLU) [55]: Φ(x) = max(αx, x), where α = 0.2. Ψs is an adaptive normalization function, described in Section 3.3. Additional specification of the CAN architecture is provided in the supplement.\nThe network aggregates global context via fullresolution intermediate layers. It has a large receptive field while being extremely compact. It also has a small memory footprint during the forward pass. Since no skip connections across non-consecutive layers are employed, only two layers need to be kept in memory at any one time. Since the layers are all structurally identical, two fixed memory buffers are sufficient, with data flowing back and forth between them."
    }, {
      "heading" : "3.3. Adaptive normalization",
      "text" : "We have found that using batch normalization improves approximation accuracy on challenging image processing operators such as style transfer and pencil drawing, but degrades performance on other image processing operators. We thus employ adaptive normalization that combines batch normalization and the identity mapping:\nΨs(x) = λsx+ µsBN(x), (3)\nwhere λs, µs ∈ R are learned scalar weights and BN is the batch normalization operator [37]. The weights {λs, µs} are learned by backpropagation alongside all other parameters of the network [67]. Learning these weights allows the model to adapt to the characteristics of the approximated operator, adjusting the strengths of the identity branch and the batch normalization branch as needed."
    }, {
      "heading" : "3.4. Training",
      "text" : "The network is trained on a set of input-output pairs that contain images before and after the application of the original operator: D = {Ii, f(Ii)}. The parameters of the network are the kernel weights K = {Ksi,j}s,i,j and the biases B = {bsi}s,i. These parameters are optimized to fit the action of the operator f across all images in the training set. We train with an image-space regression loss:\nℓ(K,B) = ∑\ni\n1\nNi\n∥∥f̂(Ii;K,B)− f(Ii) ∥∥2, (4)\nwhere Ni is the number of pixels in image Ii. This loss minimizes the mean-squared error (MSE) in the RGB color space across the training set. Although MSE is known to have limited correlation with perceptual image fidelity [71], experiments will demonstrate that training an approximator to minimize MSE will also yield high accuracy in terms of other measures such as PSNR and SSIM.\nWe have also experimented with more sophisticated losses, including perceptual losses that match feature activations in a visual perception network [10, 20, 40, 48, 16] and adversarial training [20, 38, 48]. We found that the higherlevel feature matching losses did not increase approximation accuracy in our tasks; the image processing operators we target are not semantic in nature and can be approximated well by directly fitting the operator’s action on the photographic content of the image. Adversarial training is known to be unstable [4, 56, 16] and we found that it also did not increase the already excellent results that we were able to obtain with an appropriate network architecture and a direct image-space loss.\nCreating the training set D only requires running the original operator f on a set of images. Training can thus be conducted on extremely large datasets that can be generated automatically without human intervention, although we found that training on a few thousand images already produces approximators that generalize well.\nIn order to expose the training to the effects of the operator f on images of different resolutions, we use images of varying resolution for training. Specifically, given a set of high-resolution images, each is automatically resized to a random resolution between 320p and 1440p (e.g., 517p) while preserving its aspect ratio. These resized images are used for training. Training uses the Adam solver [43] and proceeds for 500K iterations (one randomly sampled image per iteration). This takes roughly one day on our test workstation."
    }, {
      "heading" : "4. Experiments",
      "text" : "Experimental setup. We evaluate the presented approach on ten image processing operators: Rudin-OsherFatemi [66], TV-L1 image restoration [58], L0 smooth-\ning [73], relative total variation [76], image enhancement by multiscale tone manipulation [24], multiscale detail manipulation based on local Laplacian filtering [5, 60], photographic style transfer from a reference image [5], darkchannel dehazing [35], nonlocal dehazing [9], and pencil drawing [53]. The operators, their effect on images, and our reference implementations are described in the supplement.\nWe use two image processing datasets: MIT-Adobe 5K and RAISE [12, 18]. MIT-Adobe 5K contains 5,000 highresolution photographs covering a broad range of scenes, subjects, and lighting conditions. We use the default 2.5K/2.5K training/test split. The RAISE dataset contains 8,156 high-resolution RAW images captured by four photographers over a period of three years, depicting different scenes and moments across Europe. We use 2.5K randomly sampled images for training and 1K other randomly sampled images for testing.\nWe ran all ten operators on all images from the training and test sets of both datasets. For each operator, the inputoutput pairs from the MIT-Adobe training set were used for training. The same models and training procedures were used for all operators. The only difference between the ten approximators is in the output images that were provided in the training set. For each architecture, this procedure yielded ten identically parameterized models, trained to approximate the respective operators. These approximators are used for most of the experiments, which are conducted on the MIT-Adobe test set.\nThe same procedure was performed using the RAISE training set. This yielded models trained to approximate the same operators on the RAISE dataset. These models will be used to test cross-dataset generalization.\nMain results. Our primary baseline is bilateral guided upsampling (BGU) [14], the state-of-the-art form of the downsample-evaluate-upsample scheme for accelerating image processing operators. There are two variants of the BGU approach, both with publicly available implementations. The first uses global optimization and is designed to approximate the original operator as closely as possible. The second is an approximation scheme designed to maximize speed, which was implemented in Halide [63] with specific attention to parallelization, vectorization, and data locality. We will compare to both variants of BGU, referred to respectively as BGU-opt and BGU-fast. We use the public implementations with the default parameters.\nWe also compare to a large number of baseline approaches that have used deep networks for related problems. The closest of these are the deep edge-aware filters of Xu et al. [75] and the recursive filters of Liu et al. [51]. Beyond this, we also evaluate the image transformation approach of Johnson et al. [40], which was developed for style transfer and superresolution but can be applied more broadly. Finally, we compare to the contemporaneous work of Isola\net al. [38], who proposed an approach to “image-to-image translation” based on adversarial training. The approach of Isola et al. differs from the other baselines in that it is not fully-convolutional and is set up to operate at fixed resolution (256×256). We report results for two versions of this baseline: one in which the output images are upsampled to the original resolution by bilinear interpolation, and one in which the output is upsampled using BGU-opt.\nApproximation accuracy achieved by each approach for each of the ten operators is visualized in Figure 2. All the numerical results are listed in the supplement. Our default model is a CAN with adaptive normalization, using d = 9 and w = 24 for the depth and width, respectively. This is the model referred to as ‘Ours’ in Figure 2 and Table 1. For each image, the output of each approach is compared to the output of the original reference operator, and the distance between the two images is evaluated in terms of PSNR and SSIM [71]. For each operator, the results are averaged over the MIT-Adobe test set. We also use a trivial baseline for calibration, referred to as Input. This trivial baseline simply uses the input image with no modification and thus evaluates the distance between the input image and the output of the reference operator. The Input baseline shows how a trivial approximation scheme (doing nothing) would fare and also provides an indication of how strongly the reference operator alters the image.\nDue to the high computational demands of some of the reference operators, all images were scaled to 1080p resolution (∼1.75 MP) for this comprehensive experiment. We will evaluate cross-resolution performance in a subsequent experiment. Note that a resolution of 1080p had no special significance during training: the models were trained on images with randomly sampled resolution.\nAverage accuracy and runtime for each approach across all ten operators is summarized in Table 1. The runtime of\neach approach on each specific operator is reported in the supplement. The CAN parameterization is extremely compact: the network has a total of 37K parameters. It approximates the reference operators extremely accurately, achieving SSIM above 0.99 on four of the operators and SSIM above 0.96 on eight of them. (See the supplement for detailed results on the individual operators.)\nCompared to our main baselines, BGU-opt and BGUfast, our approach increases PNSR by 11 dB (from ∼25 to 36) and reduces DSSIM (=(1-SSIM)/2) by a multiplicative factor of 3. The downsampling approach does not perform well when the action of the operator at high resolution cannot be recovered from its output at low resolution. In contrast, our approach models the action of the operator directly at the original resolution. Our approach is also faster than BGU-fast and is more than an order of magnitude faster than BGU-opt. Runtime was measured on a workstation with an Intel i7-5960X 3.0GHz CPU and an Nvidia Titan X GPU. The runtime of BGU varies across operators, see the sup-\nplement for detailed results. The runtime of our approach is constant. It is 40 ms (25 fps) for 480p images, 190 ms for 1080p images, and scales linearly in the number of pixels. We used a standard deep learning library (TensorFlow) with no additional performance tuning.\nOf the prior approaches that use deep networks, Liu et al. [51] and Johnson et al. [40] achieve the best approximation accuracy. Our approach outperforms these baselines by 8.5 dB in PNSR, and reduces DSSIM by a multiplicative factor of 3. Our approach is also the fastest and has the most compact parameterization. Qualitative results are shown in Figure 3 and in the supplement.\nAdditional experiments. We now compare a number of different CAN configurations to alternative fullyconvolutional architectures. These alternative architectures – Plain, Encoder-decoder [65], and FCN-8s [52, 68] – are described in detail in the supplement. All these models are trained by the same procedure as the CAN.\nThe results are summarized in Table 2. Here CAN24+AN is our primary model, referred to as ‘Ours’ in Table 1 (d = 9, w = 24, adaptive normalization). CAN32+AN is a more accurate but slower configuration (d = 10, w = 32, adaptive normalization). This configuration benefits from a receptive field of 513×513 versus the 257× 257 receptive field of CAN24. We also evalu-\nate two other variants of CAN32, controlling for the effect of adaptive normalization: CAN32 (no normalization) and CAN32+BN (BatchNorm). Finally, Table 2 also reports the performance of a single network (CAN32+AN) that represents all ten operators; this network is described in Section 5.\nCross-resolution generalization. We now test how the trained approximators generalize across resolutions. To\nkeep the time of the experiment manageable, we focus on the L0 smoothing operator for this purpose. Recall that our approximator was trained on images resized to random resolutions between 320p and 1440p. We now compare the trained model to baselines on a set of specific resolutions: 320p, 480p, 720p, 1080p, 1440p, and 2160p. For this purpose, the MIT-Adobe test set was resized to each of these resolutions, the reference operator was executed on these images, and all methods were evaluated at each resolution. The results are shown in the supplement. They indicate that the accuracy of our approximator is stable and outperforms the other approaches across resolutions. Note that the 2160p condition (∼7MP) tests the generalization of our model to resolutions never seen during training. (The maximal resolution used during training was 1440p.)\nCross-dataset generalization. We have also evaluated how the trained operators generalize across datasets. To this end, for each operator, we tested two models on the MITAdobe test set: one trained on the MIT-Adobe training set and one trained on the RAISE training set. Similarly, for each operator, we tested two models on the RAISE test set: one trained on the RAISE training set and one trained on the MIT-Adobe training set. The detailed results are given in the supplement. They indicate that the trained approximators generalize extremely well and effectively represent the underlying action of the reference operators. The accuracy in corresponding conditions (e.g., MIT→MIT and RAISE→MIT) is virtually identical. Ablation studies. Additional controlled experiments on network depth and width are reported in the supplement."
    }, {
      "heading" : "5. Extensions",
      "text" : "We now describe three extensions of the presented approach: representing parameterized operators, representing multiple operators by a single network, and video processing.\nParameterized operators. An image processing operator can have parameters that control its action. For example, variational image smoothing operators [66, 58, 73] commonly have a parameter λ that controls the relative strength of the regularizer: higher λ leads to more aggressive smoothing. Other operators, such as multiscale tone manipulation, have multiple meaningful parameters that can be used to control the operator’s effect [24]. Our approach extends naturally to creating parameterized approximators that expose these degrees of freedom at test time. To this end, we add channels to the input layer. For each parameter we wish to expose, we add an input channel that is used to communicate the parameter’s value to the network. During training, we apply the operator with randomly sampled parameter values, thus showing the network the effect of the parameter on the operator. Quantitative results are reported\nin the supplement and qualitative results are shown in the video.\nOne network to represent them all. So far, we have trained separate networks for different operators, albeit with identical parameterizations. We now show that all 10 operators can be represented by a single network, which can emulate any of the individual operators at test time. This shows that a single compact network can execute a large number of advanced image processing effects at high accuracy. To this end, we augment the input layer by adding 10 additional channels, where each channel is a binary indicator that corresponds to one of the 10 operators. During training, we randomly sample an operator and an input image in each iteration. Training proceeds for 500K iterations total, as in the other experiments. For this experiment we use the CAN32 configuration with adaptive normalization.\nThe approximation accuracy achieved by the trained network across the 10 operators is reported in Table 2. The accuracy on each individual operator is given in the supplement. Remarkably, a single compact network that represents all 10 operators achieves high accuracy, well above the most accurate prior approximation scheme (compare to the results in Table 1). The trained network is demonstrated in the supplementary video. As shown in the video, the network can also smoothly transition between the operators when it is given continuous values in the auxiliary input channels, even though it was trained with one-hot vectors only.\nVideo processing. We also apply the trained models to videos from the Tanks and Temples dataset [44]. This further demonstrates cross-dataset generalization. (The models were trained on the MIT-Adobe dataset.) We simply apply the approximator to each frame. Although no provisions are made for temporal coherence, the results are as coherent as the original operators. The results are shown in the supplementary video."
    }, {
      "heading" : "6. Conclusion",
      "text" : "We have presented an approach to approximating a wide range of image processing operators. All operators are approximated with the same parameterization and the same flow of computation. We have shown that the presented approach significantly outperforms prior approximation schemes.\nWe see the uniform and regular flow of computation in the presented model as a strong advantage. While the model is already faster than baselines using a generic implementation, we expect that significant further acceleration can be achieved."
    }, {
      "heading" : "A. Operators",
      "text" : "In this appendix, we describe in more detail the ten image processing operators used in our experiments. Our approach approximates all operators using the same model. Rudin-Osher-Fatemi. Rudin-Osher-Fatemi (ROF) [18] is a seminal model for variational image restoration. The model aims to remove noise while preserving veridical image features by optimizing a variational objective over the image. Let I : Ω→ R be a grayscale image. A restored image J : Ω → R can be computed by minimizing the following objective:\n∫\nΩ\n|∇J |+ λ ∫\nΩ\n(I − J)2, (1)\nwhere λ is a free parameter that controls the smoothness of J . The first term ∫ Ω |∇J | is the total variation regularization\nand the second term ∫ Ω (I − J)2 is a data term that uses the L2 norm. Equation (1) is strictly convex, so there is a unique global minimum. TV-L1. TV-L1 [15] is a variational image restoration model that uses the following objective:\n∫\nΩ\n|∇J |+ λ ∫\nΩ\n|I − J |. (2)\nUnlike the ROF model, TV-L1 uses the more robust L1 norm in the data term. Objective (2) is convex but not strictly convex, so the global minimizer may not be unique. L0 smoothing. The L0 smoothing operator [21] makes use of the L0 norm in the regularization term. This operator globally identifies the most important edges by penalizing the number of non-zero gradients in the image. The objective has the following form:\n∫\nΩ\n|∇J |0 + λ ∫\nΩ\n(I − J)2. (3)\nThe objective is highly non-convex and cannot be optimized by traditional gradient-based methods. We use the solver provided by Xu et al. [21].\nObjective (3) dates back to the work of Geman and Geman [5] and Mumford and Shah [13]. This objective is known as the Potts model or the piecewise-constant Mumford-Shah model. In addition to the solver of Xu et al. [21], which we use as the reference operator in our work, there are other recent solvers that optimize this objective [20, 14]. Relative total variation. Relative total variation (RTV) [23] is a model for extracting image structure by suppressing detail. This is also a variational model. It differs from the preceding ones by the form of the regularizer. The objective is\n∫\nΩ\n( Dx\nLx + ε + Dy Ly + ε\n) + λ ∫\nΩ\n(I − J)2, (4)\nwhere Dx = G ∗ |∂xJ |, Dy = G ∗ |∂yJ |, Lx = |G ∗ ∂xJ |, Lx = |G ∗ ∂xJ |, G is a Gaussian kernel, and ε is a small positive number. This objective is non-convex. We use the solver provided by Xu et al. [23]. Multiscale tone manipulation. This operator enhances an image by boosting features at multiple scales [4]. The method constructs a three-level image decomposition: a base layer B and two detail layers, D1 and D2. The base layer is simply the LAB lightness channel of the input image I . The detail layers are constructed asD1 = B −Ψ(B) and D2 = Ψ(B)−Ψ(Ψ(B)), where Ψ(·) denotes edgepreserving smoothing via weighted least-squares optimization. A new image can be constructed by nonlinearly combining these layers:\nM + S (δ0(B −M)) + S(δ1D1) + S(δ2D2), (5)\nwhere (δ0,δ1,δ2) are parameters,M is a constant image with the mean intensity ofB, and S(·) is a sigmoid function. Different sets of parameters boost features at different scales. We use the implementation of Farbman et al. [4] and use the default parameters to generate coarse-scale, medium-scale, and fine-scale images. These are then averaged to yield the final output. Detail manipulation. This is another approach to multiscale detail manipulation, based on local Laplacian filtering [16]. We use the accelerated implementation of Aubry et al. [1]. Style transfer. This operator transfers the photographic style of a reference image to the input image [1]. The operator is designed to transfer both local and global contrast and proceeds iteratively, alternating between local Laplacian filtering and histogram matching. We use the implementation of Aubry et al. [1] with their default style image. Dark-channel dehazing. The goal of image dehazing is to remove some of the effects of atmospheric absorption and scattering. The standard image formation model used for this task is\nI(x) = t(x)J(x) + (1− t(x))A, (6)\nwhere x is a pixel, I is the sensor irradiance, J is the scene radiance, A is the global atmospheric light, and t is the transmission factor. Equation (6) is underconstrained and different dehazing techniques use different prior assumptions. Haze removal using the dark channel prior [7] is based on the observation that the atmospheric light can often be computed by identifying color channels that would have been dark in the absence of haze. We use the implementation of He et al. [7]. Nonlocal dehazing. This is a recent dehazing technique that uses a nonlocal prior [2]. It is based on the observation that pixel colors in haze-free images are clustered in color\nspace, and that haze spreads these clusters into radial lines. The atmospheric light and transmission factors are recovered by identifying these lines in color space, and haze is removed using Equation (6). We use the implementation of Berman et al. [2].\nPencil drawing. This is a nonphotorealistic image stylization technique that aims to reproduce the appearance of a color pencil drawing while retaining the spatial structure of the image [12]. The technique computes a stroke layer from the gradient map and combines it with a tone layer, computed by a parametric model that represents tone distributions of pencil sketches. We use the implementation of Lu et al. [12]."
    }, {
      "heading" : "B. Context Aggregation Networks",
      "text" : "Here we provide an illustration and a further specification of the context aggregation network (CAN), our primary architecture for approximating image processing operators. The context aggregation architecture is illustrated schematically in Figure 4. For the purpose of this figure, we use depth d = 6 and width w = 8. The dilation is increased from r1 = 1 in L1 to r4 = 8 in L4. The commensurate growth in the receptive field of each element in each layer can be seen in the figure. For Ld−1 (L5 in Figure 4), we do not use dilation. For the output layer Ld (L6 in the figure) we use a linear transformation (1×1 convolution with no nonlinearity) that projects the final feature layer into the RGB color space.\nFigure 4 provides only a schematic visualization. The network we use is deeper and has a much larger receptive field. Table 3 provides a specification of the CAN32 configuration, which uses d = 10 and w = 32 and provides a receptive field of 513×513."
    }, {
      "heading" : "C. Alternative Fully-Convolutional Architectures",
      "text" : "In this appendix, we describe a number of fullyconvolutional architectures that are evaluated alongside the CAN.\nPlain. The first alternative architecture is a plain feedforward convolutional network that operates at full resolution. Specifically, we take the context aggregation network presented above and remove dilation. The network structure is the same, but all dilated convolutions are replaced by regular convolutions. The receptive field in the final layers of the network is 19×19. We use this architecture as a distinct baseline for two reasons. First, it isolates the effect of dilation (and therefore large receptive field) while retaining all the other desirable properties of the previously presented architecture. Second, it is analogous to an architecture that has recently been used for demosaicking and denoising [6],\nand thus illustrates the performance characteristics of this architecture when applied to a broad range of operators.\nEncoder-decoder. The next architecture highlights an alternative way to achieve a large receptive field: progressively reducing the resolution of the feature layers and then increasing them back to the original resolution. Such hourglass-shaped networks are sometimes referred to as encoder-decoders. Of course, the high-frequency content that is lost in the internal layers due to downsampling must be recovered somehow. A standard solution is to add skip connections across non-consecutive layers, for example connecting all layers that have the same resolution on the two sides of the hourglass. Our reference encoderdecoder architecture is the u-net [17]. The network has 23 convolutional layers. Each encoding layer applies 3×3 convolutions, followed by truncation, max pooling, and downsampling. With each downsampling step, the number of feature channels is doubled. The decoder performs upsampling by 2×2 upconvolutions, concatenates the result with the corresponding feature maps from one of the encoding layers, and applies 3×3 convolutions and truncations. The final layer applies a 1×1 convolution that projects each feature column into the RGB color space.\nWe make two modifications to the original u-net architecture [17]. First, to reduce computation time and memory footprint, we use half of the filters in each layer (e.g., 32 rather than 64 in the first layer): we found that this is sufficient to get high accuracy and it matches our configuration of the other baselines. Second, we pad each layer when necessary to make the output image match the size of the input. This makes our implementation agnostic to aspect ratio, whereas the original u-net requires the image to be square.\nWe will see that the encoder-decoder achieves comparable accuracy to the context aggregation network across operators. Furthermore, due to the low resolution of the intermediate layers, it is even faster. However, due to the high width of the intermediate layers, its capacity (number of parameters), is two orders of magnitude higher: roughly 7.7 million as opposed to 75 thousand for CAN32. Furthermore, due to the skip connections across the network, up to half of the layers must be kept in memory during the\nforward pass, increasing the network’s memory footprint. FCN-8s. As a reference baseline, we also use the fullyconvolutional setup of the VGG-16 network (specifically, FCN-8s) [11, 19]. This network also performs downsampling and then upsampling, although asymmetrically: most of the capacity is in the downsampling layers. This network is fast, but is far from compact: more than 30 million parameters. The biggest issue, however, is that its approximation accuracy is low, due to the severe internal downsampling and limited support for recovering the lost high-frequency content during upsampling. This will be illustrated in the experiments."
    }, {
      "heading" : "D. Accuracy and Runtime",
      "text" : "Here we provide the complete quantitative results for the approximation accuracy and runtime of different approaches. The approximation accuracy of each approach on each operator is given in Table 4. These are the numerical results that are visualized in Figure 2 and summarized in Table 1 in the paper. The approximation accuracy for different CAN configurations and alternative fully-convolutional architectures is given in Table 5; these are the numerical results that are summarized in Table 2 in the paper.\nThe running time of each approach on each operator is given in Table 6. The operators are arranged in the same order as in Table 4. Runtime was measured on a workstation with an Intel i7-5960X 3.0GHz CPU and an Nvidia Titan X GPU. Our approach is faster than BGU-opt by more than an order of magnitude. It is faster than BGU-fast on eight of the ten operators."
    }, {
      "heading" : "E. Cross-Resolution Generalization",
      "text" : "The cross-resolution generalization results on L0 smoothing are shown in Figure 5."
    }, {
      "heading" : "F. Cross-Dataset Generalization",
      "text" : "Here we provide the precise results of the cross-dataset generalization experiment. For each operator, we tested two models on the MIT-Adobe test set: one trained on the MITAdobe training set and one trained on the RAISE training set. Similarly, for each operator, we tested two models on\nthe RAISE test set: one trained on the RAISE training set and one trained on the MIT-Adobe training set. The results for all operators are shown in Table 7. They indicate that the trained approximators generalize extremely well. The accuracy in corresponding conditions (e.g., MIT→MIT and RAISE→MIT) is virtually identical. On the MIT test set, the SSIM achieved by models trained on RAISE is within 1% of the SSIM achieved by models trained on the MIT training set, for all operators. The same is true on the RAISE test set. This indicates that our approximators rep-\nresent the underlying action of the reference operators effectively."
    }, {
      "heading" : "G. Ablation Studies",
      "text" : "Here we report the results of additional controlled experiments that study different aspects of our model’s structure and their effect on approximation accuracy. For these experiments, we again use the L0 smoothing operator on the MIT-Adobe dataset.\nDepth. We begin by training and testing the context ag-\ngregation network with different depths d. The results are given in Table 8. Note that smaller depth implies a smaller receptive field. As shown in the table, the results are good even for shallow networks: for example, the model achieves higher SSIM than BGU-opt even with depth 4. (With this depth, the running time on 1080p images is 67 ms.) The accuracy further improves with depth and saturates at d = 9.\nWidth. We now evaluate the effect of width (the number of feature maps in each intermediate layer) on approximation\naccuracy. The experimental setup is the same as in the previous experiment. The results are reported in Table 9. The accuracy is again good even with a network that has fairly low capacity (8 feature maps per layer, 84 ms runtime at 1080p). Accuracy further increases with width."
    }, {
      "heading" : "H. Parameterized Operators",
      "text" : "Here we report in more detail the results on representing parameterized operators. We use the L0 smoothing operator. We sample different hyperparamters λ in Equation 3: λ = λ̄ exp(x) where x is a random variable with uniform distribution U (− ln(10), ln(10)) and λ̄ = 0.01 is the default value, so λ ∈ [0.1λ̄, 10λ̄]. We train and test the approximator with randomly sampled parameters λ. The approximation accuracy achieved by our approach is 21.0 in MSE, 36.2 in PSNR, and 0.984 in SSIM."
    }, {
      "heading" : "I. Qualitative Results",
      "text" : "Extensive qualitative results are provided in a separate supplement. Our method consistently outperforms the other approaches. The most sophisticated prior downsampleevaluate-upsample scheme, BGU-opt, does not perform well when the action of the operator at high resolution cannot be recovered from its output at low resolution. In contrast, our method operates directly at the original resolution. Our direct approach is also more accurate than prior approaches that use deep networks."
    } ],
    "references" : [ {
      "title" : "Fast local Laplacian filters: Theory and applications",
      "author" : [ "M. Aubry", "S. Paris", "S.W. Hasinoff", "J. Kautz", "F. Durand" ],
      "venue" : "ACM Transactions on Graphics,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2014
    }, {
      "title" : "Non-local image dehazing",
      "author" : [ "D. Berman", "T. Treibitz", "S. Avidan" ],
      "venue" : "In CVPR, 2016",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2016
    }, {
      "title" : "Bilateral guided upsampling",
      "author" : [ "J. Chen", "A. Adams", "N. Wadhwa", "S.W. Hasinoff" ],
      "venue" : "ACM Transactions on Graphics,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2016
    }, {
      "title" : "Edgepreserving decompositions for multi-scale tone and detail manipulation",
      "author" : [ "Z. Farbman", "R. Fattal", "D. Lischinski", "R. Szeliski" ],
      "venue" : "ACM Transactions on Graphics,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2008
    }, {
      "title" : "Stochastic relaxation, Gibbs distributions, and the bayesian restoration",
      "author" : [ "S. Geman", "D. Geman" ],
      "venue" : "of images. PAMI,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1984
    }, {
      "title" : "Deep joint demosaicking and denoising",
      "author" : [ "M. Gharbi", "G. Chaurasia", "S. Paris", "F. Durand" ],
      "venue" : "ACM Transactions on Graphics,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2016
    }, {
      "title" : "Single image haze removal using dark channel",
      "author" : [ "K. He", "J. Sun", "X. Tang" ],
      "venue" : "prior. PAMI,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2011
    }, {
      "title" : "Image-to-image translation with conditional adversarial networks",
      "author" : [ "P. Isola", "J. Zhu", "T. Zhou", "A.A. Efros" ],
      "venue" : "In CVPR, 2017",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2017
    }, {
      "title" : "Perceptual losses for real-time style transfer and super-resolution",
      "author" : [ "J. Johnson", "A. Alahi", "L. Fei-Fei" ],
      "venue" : "In ECCV, 2016",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2016
    }, {
      "title" : "Learning recursive filters for low-level vision via a hybrid neural network",
      "author" : [ "S. Liu", "J. Pan", "M. Yang" ],
      "venue" : "In ECCV,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2016
    }, {
      "title" : "Fully convolutional networks for semantic segmentation",
      "author" : [ "J. Long", "E. Shelhamer", "T. Darrell" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2015
    }, {
      "title" : "Combining sketch and tone for pencil drawing production",
      "author" : [ "C. Lu", "L. Xu", "J. Jia" ],
      "venue" : "In Non-Photorealistic Animation and Rendering,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2012
    }, {
      "title" : "Optimal approximations by piecewise smooth functions and associated variational problems",
      "author" : [ "D. Mumford", "J. Shah" ],
      "venue" : "Communications on Pure and Applied Mathematics,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 1989
    }, {
      "title" : "Fast and effective L0 gradient minimization by region fusion",
      "author" : [ "R.M.H. Nguyen", "M.S. Brown" ],
      "venue" : "In ICCV,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2015
    }, {
      "title" : "A variational approach to remove outliers and impulse noise",
      "author" : [ "M. Nikolova" ],
      "venue" : "Journal of Mathematical Imaging and Vision,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2004
    }, {
      "title" : "Local Laplacian filters: Edge-aware image processing with a Laplacian pyramid",
      "author" : [ "S. Paris", "S.W. Hasinoff", "J. Kautz" ],
      "venue" : "ACM Transactions on Graphics,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2011
    }, {
      "title" : "U-Net: Convolutional networks for biomedical image segmentation",
      "author" : [ "O. Ronneberger", "P. Fischer", "T. Brox" ],
      "venue" : "In MIC- CAI,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2015
    }, {
      "title" : "Nonlinear total variation based noise removal algorithms",
      "author" : [ "L.I. Rudin", "S. Osher", "E. Fatemi" ],
      "venue" : "Physica D,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 1992
    }, {
      "title" : "Very deep convolutional networks for large-scale image recognition",
      "author" : [ "K. Simonyan", "A. Zisserman" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2015
    }, {
      "title" : "Fast partitioning of vectorvalued images",
      "author" : [ "M. Storath", "A. Weinmann" ],
      "venue" : "SIAM Journal on Imaging Sciences,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2014
    }, {
      "title" : "Image smoothing via L0 gradient minimization",
      "author" : [ "L. Xu", "C. Lu", "Y. Xu", "J. Jia" ],
      "venue" : "ACM Transactions on Graphics,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2011
    }, {
      "title" : "Deep edgeaware filters",
      "author" : [ "L. Xu", "J.S.J. Ren", "Q. Yan", "R. Liao", "J. Jia" ],
      "venue" : "In ICML,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2015
    }, {
      "title" : "Structure extraction from texture via relative total variation",
      "author" : [ "L. Xu", "Q. Yan", "Y. Xia", "J. Jia" ],
      "venue" : "ACM Transactions on Graphics,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2012
    }, {
      "title" : "Multi-scale context aggregation by dilated convolutions",
      "author" : [ "F. Yu", "V. Koltun" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 23,
      "context" : "Techniques developed in the last decade can dramatically enhance detail [24, 69, 26, 28, 60], transform the image by applying a master photographer’s style [7, 5], smooth the image for the purpose of abstraction [73, 76, 79], and eliminate the effects of atmospheric scattering [25, 35, 27, 9].",
      "startOffset" : 72,
      "endOffset" : 92
    }, {
      "referenceID" : 6,
      "context" : "Techniques developed in the last decade can dramatically enhance detail [24, 69, 26, 28, 60], transform the image by applying a master photographer’s style [7, 5], smooth the image for the purpose of abstraction [73, 76, 79], and eliminate the effects of atmospheric scattering [25, 35, 27, 9].",
      "startOffset" : 156,
      "endOffset" : 162
    }, {
      "referenceID" : 4,
      "context" : "Techniques developed in the last decade can dramatically enhance detail [24, 69, 26, 28, 60], transform the image by applying a master photographer’s style [7, 5], smooth the image for the purpose of abstraction [73, 76, 79], and eliminate the effects of atmospheric scattering [25, 35, 27, 9].",
      "startOffset" : 156,
      "endOffset" : 162
    }, {
      "referenceID" : 8,
      "context" : "Techniques developed in the last decade can dramatically enhance detail [24, 69, 26, 28, 60], transform the image by applying a master photographer’s style [7, 5], smooth the image for the purpose of abstraction [73, 76, 79], and eliminate the effects of atmospheric scattering [25, 35, 27, 9].",
      "startOffset" : 278,
      "endOffset" : 293
    }, {
      "referenceID" : 13,
      "context" : "One general approach to accelerating a broad range of image processing operators is well-known: downsample the image, execute the operator at low resolution, and upsample [45, 34, 14].",
      "startOffset" : 171,
      "endOffset" : 183
    }, {
      "referenceID" : 13,
      "context" : "For example, the PSNR of our approximators across the ten considered operators on the MIT-Adobe test set is 36 dB, compared to 25 dB for the high-accuracy variant of bilateral guided upsampling [14].",
      "startOffset" : 194,
      "endOffset" : 198
    }, {
      "referenceID" : 20,
      "context" : "The bilateral filter in particular has benefitted from long-term investment in its acceleration [21, 72, 15, 59, 2, 1, 29, 8].",
      "startOffset" : 96,
      "endOffset" : 125
    }, {
      "referenceID" : 14,
      "context" : "The bilateral filter in particular has benefitted from long-term investment in its acceleration [21, 72, 15, 59, 2, 1, 29, 8].",
      "startOffset" : 96,
      "endOffset" : 125
    }, {
      "referenceID" : 1,
      "context" : "The bilateral filter in particular has benefitted from long-term investment in its acceleration [21, 72, 15, 59, 2, 1, 29, 8].",
      "startOffset" : 96,
      "endOffset" : 125
    }, {
      "referenceID" : 0,
      "context" : "The bilateral filter in particular has benefitted from long-term investment in its acceleration [21, 72, 15, 59, 2, 1, 29, 8].",
      "startOffset" : 96,
      "endOffset" : 125
    }, {
      "referenceID" : 7,
      "context" : "The bilateral filter in particular has benefitted from long-term investment in its acceleration [21, 72, 15, 59, 2, 1, 29, 8].",
      "startOffset" : 96,
      "endOffset" : 125
    }, {
      "referenceID" : 5,
      "context" : "Other work has examined the acceleration of variational methods [6, 62, 13, 17], gradient-domain techniques [46], convolutions with large spatial support [23], and local Laplacian filters [5].",
      "startOffset" : 64,
      "endOffset" : 79
    }, {
      "referenceID" : 12,
      "context" : "Other work has examined the acceleration of variational methods [6, 62, 13, 17], gradient-domain techniques [46], convolutions with large spatial support [23], and local Laplacian filters [5].",
      "startOffset" : 64,
      "endOffset" : 79
    }, {
      "referenceID" : 16,
      "context" : "Other work has examined the acceleration of variational methods [6, 62, 13, 17], gradient-domain techniques [46], convolutions with large spatial support [23], and local Laplacian filters [5].",
      "startOffset" : 64,
      "endOffset" : 79
    }, {
      "referenceID" : 22,
      "context" : "Other work has examined the acceleration of variational methods [6, 62, 13, 17], gradient-domain techniques [46], convolutions with large spatial support [23], and local Laplacian filters [5].",
      "startOffset" : 154,
      "endOffset" : 158
    }, {
      "referenceID" : 4,
      "context" : "Other work has examined the acceleration of variational methods [6, 62, 13, 17], gradient-domain techniques [46], convolutions with large spatial support [23], and local Laplacian filters [5].",
      "startOffset" : 188,
      "endOffset" : 191
    }, {
      "referenceID" : 13,
      "context" : "A general approach to accelerating image processing operators is to downsample the image, evaluate the operator at low resolution, and upsample [45, 34, 14].",
      "startOffset" : 144,
      "endOffset" : 156
    }, {
      "referenceID" : 10,
      "context" : "Deep networks have been used for denoising [39, 11, 3], super-resolution [10, 19, 40, 42, 41, 48, 50], deblurring [74], restoration of images corrupted by dirt or rain [22], example-based non-photorealistic stylization [30, 70, 40], joint image filtering [49], dehazing [64], and demosaicking [31].",
      "startOffset" : 43,
      "endOffset" : 54
    }, {
      "referenceID" : 2,
      "context" : "Deep networks have been used for denoising [39, 11, 3], super-resolution [10, 19, 40, 42, 41, 48, 50], deblurring [74], restoration of images corrupted by dirt or rain [22], example-based non-photorealistic stylization [30, 70, 40], joint image filtering [49], dehazing [64], and demosaicking [31].",
      "startOffset" : 43,
      "endOffset" : 54
    }, {
      "referenceID" : 9,
      "context" : "Deep networks have been used for denoising [39, 11, 3], super-resolution [10, 19, 40, 42, 41, 48, 50], deblurring [74], restoration of images corrupted by dirt or rain [22], example-based non-photorealistic stylization [30, 70, 40], joint image filtering [49], dehazing [64], and demosaicking [31].",
      "startOffset" : 73,
      "endOffset" : 101
    }, {
      "referenceID" : 18,
      "context" : "Deep networks have been used for denoising [39, 11, 3], super-resolution [10, 19, 40, 42, 41, 48, 50], deblurring [74], restoration of images corrupted by dirt or rain [22], example-based non-photorealistic stylization [30, 70, 40], joint image filtering [49], dehazing [64], and demosaicking [31].",
      "startOffset" : 73,
      "endOffset" : 101
    }, {
      "referenceID" : 21,
      "context" : "Deep networks have been used for denoising [39, 11, 3], super-resolution [10, 19, 40, 42, 41, 48, 50], deblurring [74], restoration of images corrupted by dirt or rain [22], example-based non-photorealistic stylization [30, 70, 40], joint image filtering [49], dehazing [64], and demosaicking [31].",
      "startOffset" : 168,
      "endOffset" : 172
    }, {
      "referenceID" : 9,
      "context" : "We have also experimented with more sophisticated losses, including perceptual losses that match feature activations in a visual perception network [10, 20, 40, 48, 16] and adversarial training [20, 38, 48].",
      "startOffset" : 148,
      "endOffset" : 168
    }, {
      "referenceID" : 19,
      "context" : "We have also experimented with more sophisticated losses, including perceptual losses that match feature activations in a visual perception network [10, 20, 40, 48, 16] and adversarial training [20, 38, 48].",
      "startOffset" : 148,
      "endOffset" : 168
    }, {
      "referenceID" : 15,
      "context" : "We have also experimented with more sophisticated losses, including perceptual losses that match feature activations in a visual perception network [10, 20, 40, 48, 16] and adversarial training [20, 38, 48].",
      "startOffset" : 148,
      "endOffset" : 168
    }, {
      "referenceID" : 19,
      "context" : "We have also experimented with more sophisticated losses, including perceptual losses that match feature activations in a visual perception network [10, 20, 40, 48, 16] and adversarial training [20, 38, 48].",
      "startOffset" : 194,
      "endOffset" : 206
    }, {
      "referenceID" : 3,
      "context" : "Adversarial training is known to be unstable [4, 56, 16] and we found that it also did not increase the already excellent results that we were able to obtain with an appropriate network architecture and a direct image-space loss.",
      "startOffset" : 45,
      "endOffset" : 56
    }, {
      "referenceID" : 15,
      "context" : "Adversarial training is known to be unstable [4, 56, 16] and we found that it also did not increase the already excellent results that we were able to obtain with an appropriate network architecture and a direct image-space loss.",
      "startOffset" : 45,
      "endOffset" : 56
    }, {
      "referenceID" : 23,
      "context" : "We evaluate the presented approach on ten image processing operators: Rudin-OsherFatemi [66], TV-L image restoration [58], L0 smoothing [73], relative total variation [76], image enhancement by multiscale tone manipulation [24], multiscale detail manipulation based on local Laplacian filtering [5, 60], photographic style transfer from a reference image [5], darkchannel dehazing [35], nonlocal dehazing [9], and pencil drawing [53].",
      "startOffset" : 223,
      "endOffset" : 227
    }, {
      "referenceID" : 4,
      "context" : "We evaluate the presented approach on ten image processing operators: Rudin-OsherFatemi [66], TV-L image restoration [58], L0 smoothing [73], relative total variation [76], image enhancement by multiscale tone manipulation [24], multiscale detail manipulation based on local Laplacian filtering [5, 60], photographic style transfer from a reference image [5], darkchannel dehazing [35], nonlocal dehazing [9], and pencil drawing [53].",
      "startOffset" : 295,
      "endOffset" : 302
    }, {
      "referenceID" : 4,
      "context" : "We evaluate the presented approach on ten image processing operators: Rudin-OsherFatemi [66], TV-L image restoration [58], L0 smoothing [73], relative total variation [76], image enhancement by multiscale tone manipulation [24], multiscale detail manipulation based on local Laplacian filtering [5, 60], photographic style transfer from a reference image [5], darkchannel dehazing [35], nonlocal dehazing [9], and pencil drawing [53].",
      "startOffset" : 355,
      "endOffset" : 358
    }, {
      "referenceID" : 8,
      "context" : "We evaluate the presented approach on ten image processing operators: Rudin-OsherFatemi [66], TV-L image restoration [58], L0 smoothing [73], relative total variation [76], image enhancement by multiscale tone manipulation [24], multiscale detail manipulation based on local Laplacian filtering [5, 60], photographic style transfer from a reference image [5], darkchannel dehazing [35], nonlocal dehazing [9], and pencil drawing [53].",
      "startOffset" : 405,
      "endOffset" : 408
    }, {
      "referenceID" : 11,
      "context" : "We use two image processing datasets: MIT-Adobe 5K and RAISE [12, 18].",
      "startOffset" : 61,
      "endOffset" : 69
    }, {
      "referenceID" : 17,
      "context" : "We use two image processing datasets: MIT-Adobe 5K and RAISE [12, 18].",
      "startOffset" : 61,
      "endOffset" : 69
    }, {
      "referenceID" : 13,
      "context" : "Our primary baseline is bilateral guided upsampling (BGU) [14], the state-of-the-art form of the downsample-evaluate-upsample scheme for accelerating image processing operators.",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 23,
      "context" : "From 1 to 10: RudinOsher-Fatemi [66], TV-L image restoration [58], L0 smoothing [73], relative total variation [76], image enhancement by multiscale tone manipulation [24], multiscale detail manipulation based on local Laplacian filtering [5, 60], nonlocal dehazing [9], dark-channel dehazing [35], photographic style transfer from a reference image [5], and pencil drawing [53].",
      "startOffset" : 167,
      "endOffset" : 171
    }, {
      "referenceID" : 4,
      "context" : "From 1 to 10: RudinOsher-Fatemi [66], TV-L image restoration [58], L0 smoothing [73], relative total variation [76], image enhancement by multiscale tone manipulation [24], multiscale detail manipulation based on local Laplacian filtering [5, 60], nonlocal dehazing [9], dark-channel dehazing [35], photographic style transfer from a reference image [5], and pencil drawing [53].",
      "startOffset" : 239,
      "endOffset" : 246
    }, {
      "referenceID" : 8,
      "context" : "From 1 to 10: RudinOsher-Fatemi [66], TV-L image restoration [58], L0 smoothing [73], relative total variation [76], image enhancement by multiscale tone manipulation [24], multiscale detail manipulation based on local Laplacian filtering [5, 60], nonlocal dehazing [9], dark-channel dehazing [35], photographic style transfer from a reference image [5], and pencil drawing [53].",
      "startOffset" : 266,
      "endOffset" : 269
    }, {
      "referenceID" : 4,
      "context" : "From 1 to 10: RudinOsher-Fatemi [66], TV-L image restoration [58], L0 smoothing [73], relative total variation [76], image enhancement by multiscale tone manipulation [24], multiscale detail manipulation based on local Laplacian filtering [5, 60], nonlocal dehazing [9], dark-channel dehazing [35], photographic style transfer from a reference image [5], and pencil drawing [53].",
      "startOffset" : 350,
      "endOffset" : 353
    }, {
      "referenceID" : 13,
      "context" : "BGU-fast [14] 521.",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 13,
      "context" : "BGU-opt [14] 413.",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 13,
      "context" : "(a) Input (b) Ours (c) BGU-opt [14] (d) Xu et al.",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 13,
      "context" : "For each operator, we show the input image, the result of the original reference operator, the result produced by our approximator, and results produced by BGU-opt [14], Xu et al.",
      "startOffset" : 164,
      "endOffset" : 168
    }, {
      "referenceID" : 23,
      "context" : "Other operators, such as multiscale tone manipulation, have multiple meaningful parameters that can be used to control the operator’s effect [24].",
      "startOffset" : 141,
      "endOffset" : 145
    } ],
    "year" : 2017,
    "abstractText" : "We present an approach to accelerating a wide variety of image processing operators. Our approach uses a fullyconvolutional network that is trained on input-output pairs that demonstrate the operator’s action. After training, the original operator need not be run at all. The trained network operates at full resolution and runs in constant time. We investigate the effect of network architecture on approximation accuracy, runtime, and memory footprint, and identify a specific architecture that balances these considerations. We evaluate the presented approach on ten advanced image processing operators, including multiple variational ∗Joint first authors models, multiscale tone and detail manipulation, photographic style transfer, nonlocal dehazing, and nonphotorealistic stylization. All operators are approximated by the same model. Experiments demonstrate that the presented approach is significantly more accurate than prior approximation schemes. It increases approximation accuracy as measured by PSNR across the evaluated operators by 8.5 dB on the MIT-Adobe dataset (from 27.5 to 36 dB) and reduces DSSIM by a multiplicative factor of 3 compared to the most accurate prior approximation scheme, while being the fastest. We show that our models generalize across datasets and across resolutions, and investigate a number of extensions of the presented approach. 1 ar X iv :1 70 9. 00 64 3v 1 [ cs .C V ] 2 S ep 2 01 7",
    "creator" : "LaTeX with hyperref package"
  }
}