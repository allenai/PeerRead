{
  "name" : "1609.07724.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "The RNN-ELM Classifier",
    "authors" : [ "Athanasios Vlontzos" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Deep Learning, using convolutional neural networks with multiple layers of hidden units has in recent years achieved human-competitive or even better than human performance in image classification tasks [2],[18] at the expense of long training times and specialised hardware [3]. In this paper we combine the Random Neural Network (RNN)[6],[12],[13] and the Extreme Learning Machine (ELM)[16] in shallow and deep classifiers and compare their performance.\nThe RNN: The RNN is a stochastic integer state, integrate and fire system [14], initially developed to model biological neurons [7] and extended to model soma-to-soma interactions [15]. It consists of M interconnected neurons, each of which can receive positive (excitatory) or negative (inhibitory) signals from external sources such as sensory sources or other cells. The RNN can be described by equations that are possible to be solved analytically. It provides useful mathematical properties and algorithmic efficiency as seen in [14] :\n– The state of each neuron i is represented at a given time t by a integer ki ≥ 0 which can describe the neuron’s level of excitation. – Each neuron i receives excitatory and inhibitory spikes in the form of independent Poisson processes of rate λ+i and λ − i . A neuron when excited (i.e. ki > 0) can fire after a delay characterised\nby exponential distribution whose average value µ−1i depends on the specific neuron. – A neuron j which fires, sends an excitatory or inhibitory spike to a neuron i with probability p+ji, p − ji. We write w + ji = rjp + ji and w − ji = rjp − ji – The state of the system is the joint probability distribution p(k, t) = Prob[k1(t), ...kn(t) = (k1, .., kn)] and it satisfies a coupled system of Chapman-Kolmogorov equations – The RNN has a “product form” solution [6], meaning that in steady state, the joint probability distribution of network state is equal to the product of marginal probabilities\nwhere the marginal : lim t→∞ Pr[ki(t) = ki] = q ki i (1− qi) (1)\nar X\niv :1\n60 9.\n07 72\n4v 1\n[ cs\n.N E\n] 2\n5 Se\np 20\n16\nand\nqi = λ+i\n∑n j=1 qjw + ij\nri + λ − i ∑n j=1 qjw − ji\n(2)\nThe RNN was initially developed to model biological neurons [7] and has been used for landmine detection[20],[9], video and image processing [5],[4],[1], combinatorial optimisation [10], network routing[8] and emergency management citeemergency.\nThe ELM: The Extreme Learning Machine [16] is a Single Layer Feedforward Network (SLFN) with one layer of hidden neurons. Input weights W1 to the hidden neurons are assigned randomly in the range [0,1] and never changed while the output weights W2 are estimated in one step by observing that its output is calculated as in eq. 3 where ζ is the hidden neuron activation function. Then:\nY = W2ζ(W1x) W2 = ζ(W1x) +Y (3, 4)\nwhere a least squares fit to Y is calculated and ()+ is the Moore-Penrose pseudo-inverse. ELMs have been shown to achieve very good classification results and with their one-step weight estimation procedure, achieve very fast learning times. However, ELMs tend to produce good results when very large numbers of hidden neurons are used thus reducing their computational complexity advantage since the computation time is dominated by the calculation of the pseudo-inverse of a very large matrix."
    }, {
      "heading" : "2 The RNN-ELM and the PCA-RNN-ELM",
      "text" : "RNN-ELM: Inspired by the fact that in mammalian brains, among other communication mechanisms, cells exhibit a quasi-simultaneous firing pattern through soma-to-soma interactions[15], in [14] an extension of the RNN was presented. A special network was considered that contained n identical connected neurons, each having a firing rate r and external excitatory and inhibitory spikes are denoted by λ+ and λ−. The state of each neuron was denoted by q and each neuron receives an inhibitory input from some external neuron u which is not part of the cluster, thus any cell i inside the cluster has an inhibitory weight w−u ≡ w−u,i > 0 from the external neuron u to i. Also the internal spiking rate weights were set to zero w+i,j = w − i,j = 0. Whenever one of the neurons fires it triggers the firing of the others with p(i, j) = pn and Q(i, j) = (1−p)\nn . In this way instead of exiting or inhibiting other neurons in the cluster through spikes, the packed neurons excite each other and provoke firing through soma-to-soma interactions. The result that [14] has reached is :\nq = λ+ + rq(n−1)(1−p)n−qp(n−1)\nr + λ− + quw − u + rqp(n− 1) n− qp(n− 1)\n(5)\nwhich is a second degree polynomial in q that can be solved for its positive root which is the only of interest since q is a probability.\nq2p(n− 1)[λ− + quw−u ]− q(n− 1)[r(1− p)− λ+p] + n[λ+ − r − λ− − quwu] = 0 (6)\nFrom the standard method of solving quadratic equation we can define the activation function of the cth cluster as:\nζC(x) = −bc\n2pc(n− 1)[λ−c + x] +\n√ bc\n2 − 4pc(n− 1)[λ− + x]n[λc+ − rc − λc− − x] 2pc(n− 1)[λc− + x]\n(7)\nwhere:\nx = U∑ u=1 w−u,cqu (8)\nThe RNN-ELM therefore is defined as an ELM using equations (7) and (8) as the activation function of the hidden neurons.\nAn Update Rule for ELM Output: In [14] and [11] to achieve better accuracies in classification tasks an update rule was introduced. Instead of updating the ELM output weights based on the desired output, the desired output itself was updated and the weights were updated via the Moore-Penrose pseudo inverse. Denoting the labels of the dataset as L = [l1l2...lD] T and the desired output as a D ×K matrix , Y = [yd,k] where the ldth element in [yd,1...yd,K ]is initially 1 while the rest of the are set to 0. Then the hidden-layer output is then H = ζ(XW (1)) where W (1) denotes the randomly generated input weights while let W (2) which is determined by\nW (2) = H+Y (9)\nThen the output of the ELM is O = HW (2). The rule dictates an iterative approach to adjust Y based on the output O using the negative log-likelihood function at the cost function:\nfd = −ln( eO(d,ld)∑K k=1 e O(d,k) ) (10)\nthen taking the partial derivative:\n∂fd ∂O(d, k) =  1∑M k̂=1 eO(d,k̂)−O(k,d) − 1 if k = ld 1∑K\nk̂=1 eO(d,k̂)−O(k,d)\nif k 6= ld (11)\nthen:\nY (i+1)(d, k) = O(i)(d, k)− s ∂fd ∂O(i)(d, k)\n(12)\nwhere O(i) denotes the output after the i-th iteration based on Y (i) and s > 0 is the step size chosen by the user.\nThe PCA-RNN-ELM algorithm: The PCA algorithm is using Singular Value Decomposition (SVD), in that we decompose the input X and its covariance matrix as :\nX = UΓVT C = 1\nN XXT =\n1 N UΓ2UT (13, 14)\nWhere U is an N × N matrix, Γ is a N ×M matrix and V is a M ×M matrix. Comparing the factorisation of X with that of C we conclude that the right vectors U are equivalent to the eigenvectors of XXT. So the transformed data are denoted as Y and after selecting only the the eigenvectors corresponding to the m largest eigenvalues the data are denoted as Ym. Both can be expressed as\nY = XV = UΓ Ym = UmΓm = XVm (15, 16)\nBased on the above the complete PCA-ELM algorithm with Nh hidden neurons, Vm the matrix of the first m principal components and training set X is:\nX′m = XVm W1 = rand(Nh,M) (17, 18)\nH = ζ(W1X ′ m) W2 = H +T (19, 20)\nwhere T is the matrix of target labels. Finally repeat for I iterations starting with O = HW2\nDi = ∂fd\n∂O(i)(d, k) Y (i)(d, k) = O(i−1)(d, k)− sDi (21, 22)\nWi2 = H +Yi (23)\nOn the testing set Z the algorithm executed is:\nZ′m = ZVm Y = W2ζ(W1Z ′ m) (24, 25)\nWhen using the iterative output adaptation method, one must be careful to avoid model overfitting. The training accuracy rapidly converges to 100% but the training accuracy starts decreasing. In our simulations we limit the training accuracy to 98.5% for MNIST and 99% for NORB dataset."
    }, {
      "heading" : "3 Simulation Results",
      "text" : "Before we present our simulation results we must note that most algorithms achieving better performance are using image pre-processing (e.g. [19]) or require much larger computational resources (e.g. [2],[18],[17]).\nRNN-ELM vs. ELM: The first experiment run was to compare the accuracy of the ELM with various activation functions with the RNN-ELM using the activation function of Eq.7. In [17] an ELM structure 784-15000-10 was used to achieve 97% testing accuracy with a sigmoid activation function. In contrast, the RNN-ELM can achieve the same level of performance with 5000 output neurons. Table 1 provides a more detailed comparison using 1000 hidden neurons using various activation functions and the activation function of Eq.7 (RNN). These results clearly demonstrate that the use of the RNN activation function leads to much better accuracies with a small increase in computational time.\nPCA-RNN-ELM: The next experiment run was to test the performance of the PCA-ELM varying the number of principal components and the number of hidden neurons in the ELM. In our testing we used this process with 30 iterations and a step size of s = 5 getting equivalent results but using less neurons as seen in figure Fig. 1.. Also its important to note that the time needed to achieve 30 iterations of the simulation seems to be constant and independent of the number of neurons used, in contrast to the method used in [14] where we observe an increase in the time needed as the number of neurons increases, as seen in table 2.\nPCA-RNN-ELM vs. Autoencoder-ELM: We compared PCA-RNN-ELM with Autoencoder-ELM[11] using the same number of PCs and autoencoder neurons while varying the ELM size. Essentially this compares the performance of the ELM given the dimensionality reduction obtained by the two methods. Figure 1 and Table 2 show that the performance of the two methods is roughly equivalent for any ELM size. The PCA-ELM enjoys a slight advantage in testing time while the accuracy is essentially the same. It must be noted that due to the randomness of the ELM weights, results vary from run to run and differences beyond the second decimal point should be ignored.\nNORB with PCA-RNN-ELM: In testing the PCA-ELM with the NORB dataset the input data were presented to the algorithms by concatenating binocular images of the same object. Therefore, each\npair of images had 2048 features (pixel values), similarly to [14]. We ran two sets of experiments: 1) setting ELM hidden neurons to 500 and varying the number of PCs used (Table 3) and 2) keeping the number of PCs fixed and varying the number of ELM hidden neurons . Finally we ran the Deep RNN-ELM network of [14] and obtained a training time of 34.88s, training accuracy 99.02%, testing time 9.53 and testing accuracy 86.99%. All results are averages of 50 trials to minimise the effect of the randomised initialisation of the input weights in the ELM. We observed also that the training times are faster than the Deep Autoencoder RNN-ELM while producing better testing accuracy by 3% at best. The PCA-RNN-ELM always enjoys an advantage in testing time. Given that the PCA-RNN-ELM training and testing times are quite fast, it is possible to run the algorithm on the same data multiple times and select the parameters (random input weights) that produce the best results."
    }, {
      "heading" : "4 Conclusion",
      "text" : "In this paper we compared the RNN-ELM to the ELM with various activation functions and observed that the RNN-ELM achieves far superior results with far fewer hidden neurons. We also demonstrated that the RNN-ELM network with PCA preprocessing is a viable alternative to other image classification algorithms by comparing three versions of the RNN-ELM network and a Deep RNN architecture on the standard MNIST and NORB datasets. The results were obtained without any prior feature extraction or image processing apart from the PCA algorithm in order to concentrate on the raw performance of the algorithms tested. We observed that the relatively simple PCA-RNN-ELM can provide high accuracy and very fast training and testing times while the deep autoencoder-ELM algorithm can achieve similar results on the MNIST dataset."
    } ],
    "references" : [ {
      "title" : "A laser intensity image based automatic vehicle classification system",
      "author" : [ "H. Abdelbaki", "K. Hussain", "E. Gelenbe" ],
      "venue" : "IEEE Proc. on Intelligent Transportation Systems pp. 460–465",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Flexible, high performance convolutional neural networks for image classification",
      "author" : [ "D.C. Ciresan", "U. Meier", "J. Masci", "L.M. Gambardella", "J. Schmidhuber" ],
      "venue" : "International Joint Conference on Artificial Intelligence (IJCAI-2011, Barcelona)",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Multi-column deep neural network for traffic sign classification",
      "author" : [ "D.C. Ciresan", "U. Meier", "J. Masci", "J. Schmidhuber" ],
      "venue" : "Neural Networks",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Video quality and traffic qos in learning based subsampled and receiver interpolated video sequences",
      "author" : [ "C. Cramer", "E. Gelenbe" ],
      "venue" : "IEEE J. on Selected Areas in Communications 18(2), 150–167",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Low bit-rate video compression with neural networks and temporal subsampling",
      "author" : [ "C. Cramer", "E. Gelenbe", "H. Bakircloglu" ],
      "venue" : "Proceedings of the IEEE 84(10), 1529–1543",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "Random neural networks with negative and positive signalsand product form solution",
      "author" : [ "E. Gelenbe" ],
      "venue" : "Neural Computation 1(4), 502–510",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 1989
    }, {
      "title" : "Oscillatory corticothalamic response to somatosensory input",
      "author" : [ "E. Gelenbe", "C. Cramer" ],
      "venue" : "Biosystems 48(1), 67–75",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Cognitive packet network for bilateral asymmetric connections",
      "author" : [ "E. Gelenbe", "Z. Kazhmaganbetova" ],
      "venue" : "IEEE Trans. Industrial Informatics 10(3), 1717–1725",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Area-based results for mine detection",
      "author" : [ "E. Gelenbe", "T. Kocak" ],
      "venue" : "IEEE Trans. Geoscience and Remote Sensing 38(1), 12–14",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Dynamical random neural network approach to the traveling salesman problem",
      "author" : [ "E. Gelenbe", "V. Koubi", "F. Pekegrin" ],
      "venue" : "IEEE Trans Sys. Man, Cybernetics pp. 630–635",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 1993
    }, {
      "title" : "Deep learning with random neural networks",
      "author" : [ "E. Gelenbe", "Y. Yin" ],
      "venue" : "IEEE World Conference on Computational Intelligence, IJCNN",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "G-networks with triggered customer movement",
      "author" : [ "E. Gelenbe" ],
      "venue" : "Journal of Applied Probability pp. 742–748",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 1993
    }, {
      "title" : "Random neural networks with synchronized interactions",
      "author" : [ "E. Gelenbe", "S. Timotheou" ],
      "venue" : "Neural Computation vol. 20(no. 9), pp. 2308–2324,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Deep learning with random neural networks",
      "author" : [ "E. Gelenbe", "Y. Yin" ],
      "venue" : "SAI Intelligent Systems Conference 2016",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Neuronal soma-satellite glial cell interactions in sensory ganglia and the participation of purigenic receptors",
      "author" : [ "Y. Gu", "Y. Chen", "X. Zhang", "G. Li", "C. Wang", "L. Huang" ],
      "venue" : "Neuron Glia Biology 6(1), 53–62",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Extreme learning machine: theory and applications",
      "author" : [ "G.B. Huang", "Q.Y. Zhu", "C.K. Siew" ],
      "venue" : "Neurocomputing 70(1), 489 – 501",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Representational learning with elms for big data",
      "author" : [ "L. Kasun", "H. Zhou", "G. Huang", "C. Vong" ],
      "venue" : "IEEE Intelligent Systems pp. 31–34",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "A. Krizhevsky", "I. Sutskever", "G. Hinton" ],
      "venue" : "NIPS 2012: Neural Information Processing Systems, Lake Tahoe, Nevada",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Gradient-based learning applied to document recognition",
      "author" : [ "Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner" ],
      "venue" : "Proceedings of the IEEE 86(5), 755 – 824",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Autonomous search for mines",
      "author" : [ "W.Gelenbe", "Y. Cao" ],
      "venue" : "European J. Oper. Research 108(2), 319–333",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 1998
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "Deep Learning, using convolutional neural networks with multiple layers of hidden units has in recent years achieved human-competitive or even better than human performance in image classification tasks [2],[18] at the expense of long training times and specialised hardware [3].",
      "startOffset" : 203,
      "endOffset" : 206
    }, {
      "referenceID" : 17,
      "context" : "Deep Learning, using convolutional neural networks with multiple layers of hidden units has in recent years achieved human-competitive or even better than human performance in image classification tasks [2],[18] at the expense of long training times and specialised hardware [3].",
      "startOffset" : 207,
      "endOffset" : 211
    }, {
      "referenceID" : 2,
      "context" : "Deep Learning, using convolutional neural networks with multiple layers of hidden units has in recent years achieved human-competitive or even better than human performance in image classification tasks [2],[18] at the expense of long training times and specialised hardware [3].",
      "startOffset" : 275,
      "endOffset" : 278
    }, {
      "referenceID" : 5,
      "context" : "In this paper we combine the Random Neural Network (RNN)[6],[12],[13] and the Extreme Learning Machine (ELM)[16] in shallow and deep classifiers and compare their performance.",
      "startOffset" : 56,
      "endOffset" : 59
    }, {
      "referenceID" : 11,
      "context" : "In this paper we combine the Random Neural Network (RNN)[6],[12],[13] and the Extreme Learning Machine (ELM)[16] in shallow and deep classifiers and compare their performance.",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 12,
      "context" : "In this paper we combine the Random Neural Network (RNN)[6],[12],[13] and the Extreme Learning Machine (ELM)[16] in shallow and deep classifiers and compare their performance.",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 15,
      "context" : "In this paper we combine the Random Neural Network (RNN)[6],[12],[13] and the Extreme Learning Machine (ELM)[16] in shallow and deep classifiers and compare their performance.",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 13,
      "context" : "The RNN: The RNN is a stochastic integer state, integrate and fire system [14], initially developed to model biological neurons [7] and extended to model soma-to-soma interactions [15].",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 6,
      "context" : "The RNN: The RNN is a stochastic integer state, integrate and fire system [14], initially developed to model biological neurons [7] and extended to model soma-to-soma interactions [15].",
      "startOffset" : 128,
      "endOffset" : 131
    }, {
      "referenceID" : 14,
      "context" : "The RNN: The RNN is a stochastic integer state, integrate and fire system [14], initially developed to model biological neurons [7] and extended to model soma-to-soma interactions [15].",
      "startOffset" : 180,
      "endOffset" : 184
    }, {
      "referenceID" : 13,
      "context" : "It provides useful mathematical properties and algorithmic efficiency as seen in [14] : – The state of each neuron i is represented at a given time t by a integer ki ≥ 0 which can describe the neuron’s level of excitation.",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 5,
      "context" : ", kn)] and it satisfies a coupled system of Chapman-Kolmogorov equations – The RNN has a “product form” solution [6], meaning that in steady state, the joint probability distribution of network state is equal to the product of marginal probabilities",
      "startOffset" : 113,
      "endOffset" : 116
    }, {
      "referenceID" : 6,
      "context" : "The RNN was initially developed to model biological neurons [7] and has been used for landmine detection[20],[9], video and image processing [5],[4],[1], combinatorial optimisation [10], network routing[8] and emergency management citeemergency.",
      "startOffset" : 60,
      "endOffset" : 63
    }, {
      "referenceID" : 19,
      "context" : "The RNN was initially developed to model biological neurons [7] and has been used for landmine detection[20],[9], video and image processing [5],[4],[1], combinatorial optimisation [10], network routing[8] and emergency management citeemergency.",
      "startOffset" : 104,
      "endOffset" : 108
    }, {
      "referenceID" : 8,
      "context" : "The RNN was initially developed to model biological neurons [7] and has been used for landmine detection[20],[9], video and image processing [5],[4],[1], combinatorial optimisation [10], network routing[8] and emergency management citeemergency.",
      "startOffset" : 109,
      "endOffset" : 112
    }, {
      "referenceID" : 4,
      "context" : "The RNN was initially developed to model biological neurons [7] and has been used for landmine detection[20],[9], video and image processing [5],[4],[1], combinatorial optimisation [10], network routing[8] and emergency management citeemergency.",
      "startOffset" : 141,
      "endOffset" : 144
    }, {
      "referenceID" : 3,
      "context" : "The RNN was initially developed to model biological neurons [7] and has been used for landmine detection[20],[9], video and image processing [5],[4],[1], combinatorial optimisation [10], network routing[8] and emergency management citeemergency.",
      "startOffset" : 145,
      "endOffset" : 148
    }, {
      "referenceID" : 0,
      "context" : "The RNN was initially developed to model biological neurons [7] and has been used for landmine detection[20],[9], video and image processing [5],[4],[1], combinatorial optimisation [10], network routing[8] and emergency management citeemergency.",
      "startOffset" : 149,
      "endOffset" : 152
    }, {
      "referenceID" : 9,
      "context" : "The RNN was initially developed to model biological neurons [7] and has been used for landmine detection[20],[9], video and image processing [5],[4],[1], combinatorial optimisation [10], network routing[8] and emergency management citeemergency.",
      "startOffset" : 181,
      "endOffset" : 185
    }, {
      "referenceID" : 7,
      "context" : "The RNN was initially developed to model biological neurons [7] and has been used for landmine detection[20],[9], video and image processing [5],[4],[1], combinatorial optimisation [10], network routing[8] and emergency management citeemergency.",
      "startOffset" : 202,
      "endOffset" : 205
    }, {
      "referenceID" : 15,
      "context" : "The ELM: The Extreme Learning Machine [16] is a Single Layer Feedforward Network (SLFN) with one layer of hidden neurons.",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 0,
      "context" : "Input weights W1 to the hidden neurons are assigned randomly in the range [0,1] and never changed while the output weights W2 are estimated in one step by observing that its output is calculated as in eq.",
      "startOffset" : 74,
      "endOffset" : 79
    }, {
      "referenceID" : 14,
      "context" : "RNN-ELM: Inspired by the fact that in mammalian brains, among other communication mechanisms, cells exhibit a quasi-simultaneous firing pattern through soma-to-soma interactions[15], in [14] an extension of the RNN was presented.",
      "startOffset" : 177,
      "endOffset" : 181
    }, {
      "referenceID" : 13,
      "context" : "RNN-ELM: Inspired by the fact that in mammalian brains, among other communication mechanisms, cells exhibit a quasi-simultaneous firing pattern through soma-to-soma interactions[15], in [14] an extension of the RNN was presented.",
      "startOffset" : 186,
      "endOffset" : 190
    }, {
      "referenceID" : 13,
      "context" : "The result that [14] has reached is :",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 13,
      "context" : "An Update Rule for ELM Output: In [14] and [11] to achieve better accuracies in classification tasks an update rule was introduced.",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 10,
      "context" : "An Update Rule for ELM Output: In [14] and [11] to achieve better accuracies in classification tasks an update rule was introduced.",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 18,
      "context" : "[19]) or require much larger computational resources (e.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 1,
      "context" : "[2],[18],[17]).",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 17,
      "context" : "[2],[18],[17]).",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 16,
      "context" : "[2],[18],[17]).",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 16,
      "context" : "In [17] an ELM structure 784-15000-10 was used to achieve 97% testing accuracy with a sigmoid activation function.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 13,
      "context" : "Also its important to note that the time needed to achieve 30 iterations of the simulation seems to be constant and independent of the number of neurons used, in contrast to the method used in [14] where we observe an increase in the time needed as the number of neurons increases, as seen in table 2.",
      "startOffset" : 193,
      "endOffset" : 197
    }, {
      "referenceID" : 10,
      "context" : "Autoencoder-ELM: We compared PCA-RNN-ELM with Autoencoder-ELM[11] using the same number of PCs and autoencoder neurons while varying the ELM size.",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 13,
      "context" : "Table 2: MNIST Simulation results: Classifier of [14] with 500-500-X structure",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 13,
      "context" : "pair of images had 2048 features (pixel values), similarly to [14].",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 13,
      "context" : "Finally we ran the Deep RNN-ELM network of [14] and obtained a training time of 34.",
      "startOffset" : 43,
      "endOffset" : 47
    } ],
    "year" : 2016,
    "abstractText" : "In this paper we examine learning methods combining the Random Neural Network, a biologically inspired neural network and the Extreme Learning Machine that achieve state of the art classification performance while requiring much shorter training time. The Random Neural Network is a integrate and fire computational model of a neural network whose mathematical structure permits the efficient analysis of large ensembles of neurons. An activation function is derived from the RNN and used in an Extreme Learning Machine. We compare the performance of this combination against the ELM with various activation functions, we reduce the input dimensionality via PCA and compare its performance vs. autoencoder based versions of the RNN-ELM.",
    "creator" : "LaTeX with hyperref package"
  }
}