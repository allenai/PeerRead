{
  "name" : "1703.10371.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Born to Learn: the Inspiration, Progress, and Future of Evolved Plastic Artificial Neural Networks",
    "authors" : [ "Andrea Soltoggio", "Kenneth O. Stanley", "Sebastian Risi" ],
    "emails" : [ "a.soltoggio@lboro.ac.uk", "kstanley@cs.ucf.edu", "sebr@itu.dk" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Index Terms—Artificial Neural Networks, Learning, Plasticity, Evolution.\nI. INTRODUCTION\nOver the course of millions of years, evolution has led to the emergence of innumerable biological systems, and intelligence itself, crowned by the discovery of the human brain. Evolution, development, and learning are the fundamental processes that underpin biological intelligence. Thus, it is no surprise that scientists have modeled artificial systems to reproduce such phenomena (Sanchez et al., 1996; Sipper et al., 1997; Dawkins, 2003). However, our current knowledge of evolution, biology, and neuroscience remains insufficient to provide clear guidance on the essential mechanisms that are key to the emergence of such complex systems.\nFields such as artificial intelligence and artificial life (Langton, 1997) address questions about the emergence of life and intelligence from a computational perspective, thus abstracting the principles from the medium, i.e. biological or in-silico, and attempting to find algorithms that reproduce properties of their biological counterparts. The simulation of natural evolution in-silico is a tool that, besides being an optimization\nDepartment of Computer Science, Loughborough University, LE11 3TU, Loughborough, UK, a.soltoggio@lboro.ac.uk\nDepartment of Computer Science, University of Central Florida, Orlando, FL, USA, kstanley@cs.ucf.edu\nIT University of Copenhagen, Copenhagen, Denmark, sebr@itu.dk\nalgorithm (Bäck and Schwefel, 1993), attempts to discover pivotal elements to artificially evolve life and intelligence. Thus, the field of evolutionary computation (Holland, 1975; Eiben and Smith, 2015) includes areas such as evolutionary robotics (Harvey et al., 1997; Nolfi and Floreano, 2000) and neuroevolution (Yao, 1999) to investigate the autonomous design of complex, bio-inspired adaptive physical and neural systems.\nThis paper frames the field that attempts to evolve plastic artificial neural networks, and introduces the acronym EPANN. EPANNs are evolved because parts of their design are determined by an evolutionary algorithm; they are plastic because they undergo various time-scale changes, beyond neural activity, while experiencing sensory-motor information streams during a lifetime simulation. The final capabilities of such networks are a result of genetic instructions, determined by evolution, that enable learning once the network is placed in an environment. Static ANNs with evolved connection weights (Yao, 1999) are not considered EPANN.\nThe field studying EPANNs is motivated by the longterm ambitious promise of discovering principles of neural adaptation, learning, and memory, an endeavor that also entails a number of challenges. A primary challenge is that the interactions of dynamics across the evolutionary and learning dimensions make the whole system difficult to design and analyze. Moreover, it is not clear what key aspects are necessary and what abstractions can be made to achieve the objective. A second problem is the high computational cost required to simulate even simple models of lifetime learning and evolution. Thirdly, open-ended evolutionary experiments are difficult to benchmark because they are based on loosely defined objectives such as increasing behavioral complexity, intelligence, adaptability, and evolvability (Miconi, 2008). For this reason, EPANNs rarely compete with ad-hoc machine learning algorithms specifically designed to improve performance on well-defined and narrow problems. The lack of benchmarks and commercially viable applications make it difficult to assess the scientific value or the pathways to impact for many open-ended evolutionary systems. However, in this paper we argue that the above challenges can reveal untapped directions of scientific investigation with a high potential for discovery. Advances in this field have improved our understanding of artificial and biological intelligence and will continue to do so.\nEPANNs rely on two autonomous search processes: evolution and learning, which arguably place them among the most advanced AI and machine learning systems in terms of open-endedness and human-free design. Those systems rely\nar X\niv :1\n70 3.\n10 37\n1v 1\n[ cs\n.N E\n] 3\n0 M\nar 2\n01 7\n2 the least on pre-programmed instructions, and the most on the autonomous discovery of intelligence and adaptation from experience and from the interaction with a real or simulated world.\nIn recent years, progress in a number of relevant areas have set the stage for renewed advancement of EPANNs: more focus has been devoted to learning networks and deep neural networks; there has been a remarkable increase in computational power by means of parallel GPU computing; theories on search, complexity, and genotype-phenotype mapping now allow for less naive evolutionary approaches; high-bandwidth sensors, actuators, dedicated hardware, and big data are becoming increasingly more available; new hardware paradigms such as neuromorphic chips are being developed; and finally, neuroscience and genetics provide us with an increasingly large set of inspirational principles. This progress has changed the theoretical and technological landscape in which EPANNs first emerged, providing greater research opportunities than in the past.\nThis paper brings together the principles that inspire and justify research in EPANNs to delineate a unique research field (Section II), it reviews the progress so far (Section III), and it outlines current and future challenges and opportunities (Section IV).\nII. INSPIRATION\nEPANNs are strongly inspired by the biological evidence of natural computation and intelligence (Floreano and Mattiussi, 2008; Downing, 2015). However, the notion of “natureinspired” extends to a wide breadth of knowledge, is often loose, and is subject to different interpretations. This section aims to pinpoint the main ideas that motivate research specifically in EPANNs.\nBiological neural systems, the primary inspiration for EPANNs, are complex computational structures responsible for a variety of functions from perception, to motor skills, to behavior. In neuroscience, an important question is, what makes a brain do what it does? In the computational field that includes EPANNs, more relevant is to understand what are the fundamental ingredients and design principles discovered by nature that resulted in high levels of adaptation and intelligence. Those principles might then be abstracted and implemented in original ways in-silico to achieve similar results to those observed in nature. Crucially, artificial systems do not need to implement the same constraints and limitation as biological systems (Bullinaria, 2003). Thus, inspiration is not simple imitation.\nWe now know that genetic instructions determine the ultimate capabilities of a brain (Deary et al., 2009; Hopkins et al., 2014). Different animal species manifest different levels of skills and intelligence because of their genetic blueprint, given by the phylogenetic path of a particular species (Schreiweis et al., 2014). We also know that the vastly intricate structure of the brain emerges from one single zygote cell through a developmental process (Kolb and Gibb, 2011). Finally, the developmental process is strongly shaped by input-output learning experiences throughout early life (Hensch et al., 1998;\nKolb and Gibb, 2011), and yet high levels of plasticity are maintained throughout the entire lifespan (Merzenich et al., 1984; Kiyota, 2017).\nThese dimensions, evolution, development and learning, also known as the phylogenetic (evolution), ontogenetic (development) and epigenetic (learning) (POE) dimensions (Sipper et al., 1997) are the pillars of biological plastic brains. This review focuses on evolution and learning, but less on artificial developmental systems (Stanley and Miikkulainen, 2003b) because development may occur independently of, or dependent upon, external factors. In models where it occurs independently of external factors, development is entirely driven by genetic instructions, and does not include learning; in the second case, when development is affected by external factors, part of the process can be interpreted as learning. Thus, learning in EPANNs may occur by means of a variety of plasticity mechanisms, some of which could be seen as developmental processes.\nIn the design of bio-inspired artificial intelligence, an interesting issue is whether all three POE dimensions are equally important, or even necessary at all. Opinions are diverse, in particular in relation to what key aspects produce adaptation and intelligence. Recent advances in neuroscience suggest that plasticity in the brain is far more responsible for the manifestation of behaviors and skills than previously thought (LeDoux, 2003; Doidge, 2007; Grossberg, 2012), meaning that behavior is less determined by genetics and more by stimulidriven plasticity. A variety of social behaviors are leaned with experience in different circumstances. Skills such as reading, playing a musical instrument, or driving a car, are mastered even if none of those behaviors existed during evolutionary time. Thus, genetic instructions are responsible mainly for setting recipes to learn.\nRecent developments in a different field, that of machine learning (Michalski et al., 2013; Alpaydin, 2014) and neural learning (LeCun et al., 2015), also suggest the importance of learning from large input-output data. Deep reinforcement learning has demonstrated a remarkable capability to learn complex input-output relations using extensive training (Mnih et al., 2015; Silver et al., 2016). Deep learning has pioneered a remarkable progress in image classification and speech recognition (Schmidhuber, 2015; Deng et al., 2013). Other areas of cognition such as the capabilities to make predictions (Hawkins and Blakeslee, 2007), establish associations (Rescorla, 2014) and regulate behaviors (Carver and Scheier, 2012) are also based on learning from experience but have seen a slower progress in computational methods.\nIt is commonly agreed that EPANN’ final objective is not to perform well on one specific task, but rather to perform well on adaptation processes such as learning the structure of a particular world, its associations, patterns, and possible behaviors. In this endeavor, the evolutionary computation community, and particularly the field that studies neuroevolution, is extending research efforts to include open-ended evolutionary environments in which many tasks and behaviors might be required (Miconi, 2008). In this way, both biological observations and computational tools are yielding inspirational principles for EPANNs.\n3 Whilst the range of inspiring ideas is large and heterogeneous, the analysis in this review proposes that EPANNs build upon five main concepts:\n• evolutionary processes, • inspiration from biological neural networks, • abstractions in brain simulations, • artificial plastic neural networks, and • intelligence-testing environments.\nThese broad categories encompass a number of research areas that range from large fields such as brain physiology to specific themes such as artificial genetic encoding. Figure 1 illustrates the five main concepts and the relevant research areas to give a graphical representation of the variety and overlap of ideas that contribute to EPANNs. The sections that follow describe each of these main concepts.\nA. Evolutionary processes\nA central idea in evolutionary computation (Goldberg and Holland, 1988; Back et al., 1997) is that evolutionary processes, i.e. those that occurred in nature during the course of billions of years, can also be simulated with computer software. This idea led to the belief that intelligent computer programs could emerge with little human intervention by means of evolution in-silico (Holland and Reitman, 1977; Koza, 1992; Fogel, 2006).\nThe emergence of intelligent evolved software, however, did not occur as easily as initially hoped. The reasons for the slow progress are not completely understood, but a number of problems have been identified in relation to levels of abstraction, diversity, selection criteria, fitness and simulation environments, genotype-phenotype mapping, and available computation power. Moreover, in the biological evolution of intelligent and complex forms of life, it is not clear yet which stepping stones were more challenging for natural evolution to overcome (Stanley and Lehman, 2015). Similarly, current evolutionary algorithms might not yet focus on the pivotal search aspects that are necessary to evolve intelligence.\nTo address these questions, research topics in evolutionary computation, particularly those that contribute to EPANNs, investigate the concepts of evolvability and scalability (Wagner and Altenberg, 1996; Pigliucci, 2008; Lehman and Stanley, 2013), the encoding of genetic information through the genotype-phenotype mapping processes (Hornby et al., 2002), and novelty search and the deception of fitness objectives (Lehman and Stanley, 2008; Stanley and Lehman, 2015). Popular algorithms applied to EPANNs are reviewed later in Section III-A.\nB. Inspiration from biological neural networks\nBiological networks are a source of inspiration for their complexity and the variety of phenomena they produce. Lifetime learning includes astonishing skills such as social behavior, playing an instrument, and speaking one or more languages. Many of the skills we possess are acquired by experiencing stimuli and actions.\nThe fact that experiences guide life-long learning is an old and intuitive notion. Scientific evidence of animal learning\nfrom experience was documented initially in the works of behaviorism with scientists such as Thorndike (1911), Pavlov (1927), Skinner (1938, 1953), and Hull (1943). The main idea was to experiment and verify scientifically how experiences cause a change in behavior, in particular as a result of learning associations and observable behavioral patterns (Staddon, 1983). This approach means linking behavior to brain mechanisms and dynamics, an idea initially entertained by Freud (KϕPPE, 1983) and later by other illustrious scientists trying to bring physiology and psychology together (Hebb, 1949; Kandel, 2007). A seminal contribution to link physiology to psychology comes from Hebb (1949), whose ubiquitous principle that neurons that fire together, wire together is relevant to both low level neural wiring and high level psychological theories (Doidge, 2007).\nThe seminal work of Kandel and Tauc (1965), and following studies (Clark and Kandel, 1984), were the first to demonstrate that changes in the strength of connectivity among neurons related to behavior learning. Walters and Byrne (1983) showed that a single neuron can perform associative learning such as classical conditioning, a class of learning that is observed in simple neural systems such as by the Aplysia (Carew et al., 1981). Plasticity driven by local neural stimuli, i.e. compatible with the Hebb synapse (Hebb, 1949), is responsible not only for fine tuning, but also for building a working visual system in the cat’s visual cortex (Rauschecker and Singer, 1981). Those advances in the 1970s and 1980s lead to the mathematical formulation of synaptic learning dynamics (Lisman, 1989; Brown et al., 1990), e.g. the BCM rule, inspired by the cat’s visual cortex (Bienenstock et al., 1982), and the Oja’s rule (Oja, 1982). A Hebbian-compatible rule that regulates synaptic changes according to the firing times of the presynaptic and postsynaptic neurons was observed by Markram et al. (1997) and named Spike-Timing-Dependent Plasticity (STDP).\nBesides fine tuning of synaptic connections, a plastic brain is capable of establishing new pathways and connections by means of what is referred to as structural plasticity (Lamprecht and LeDoux, 2004; Pascual-Leone et al., 2005; Russo et al., 2010). Structural plasticity occurs primarily by means of processes taking place during development. Axon growth is known to be regulated by neurotrophic nerve growth factors that guide axons to extend in particular directions (TessierLavigne and Goodman, 1996). Developmental processes and neural plasticity are often indistinguishable (Kolb, 1989; Pascual-Leone et al., 2005) because the brain is highly plastic during development. There is evidence that structural plasticity continues well into adulthood (Pascual-Leone et al., 2005). Our understanding of structural plasticity in shaping both thoughts and behavior has grown significantly. Neuroscientific advances reviewed in Damasio (1999); LeDoux (2003); Pascual-Leone et al. (2005); Doidge (2007); Draganski and May (2008) reveal that fundamental sensations, motor patterns, associations, and ways of thinking are less instinctive and hardwired than previously thought. Both structural and functional plasticity in biology is essential to acquiring long-lasting new skills, and for this reason appears to be an important point of inspiration for EPANN.\nOne further plasticity mechanism that has attracted attention\n4 C O N C E P T S intelligence-testing environments evolutionary processes abstractions in brain simulations inspiration from biological brains artificial plastic neural networks Evolved Plastic Artificial Neural Networks\nC O N T R I B U T I N G A R E A S A N D T O P I C S\nmutation/ recombination\nprocesses\nfitness objectives\nnovelty search and diversity\nencodings\nevolvability\nmodels of pathways and\nfunctions\nbio-physical neural models\ncomputational neuroscience\nlearning & memory\nbehavior and brain functions\nphysiology of the brain reinforcement learning\nmachine learning\ndeep/neural learning\nAI in games\ncontrol problems\ncognitive problems\nmodels of plasticity\ncognitive models\nFigure 1: The five main concepts that contribute to EPANNs draw from diverse research areas and topics.\nis neuromodulation, or heterosynaptic plasticity (Marder and Thirumalai, 2002; Gu, 2002; Bailey et al., 2000). Neuromodulation appears to mediate plasticity by increasing or decreasing its effect according to a modulatory signal. A number of modulatory chemicals such as acetylcholine (ACh), norepinephrine (NE), serotonin (5-HT) and dopamine (DA) appear to regulate a large variety of neural functions, from arousal and behavior (Harris-Warrick and Marder, 1991; Hasselmo and Schnell, 1994; Marder, 1996; Katz, 1995; Katz and Frost, 1996), to pattern generation (Katz et al., 1994), to memory consolidation (Kupfermann, 1987; Hasselmo, 1995; Marder, 1996; Hasselmo, 1999). Learning by reward in monkeys was linked to dopaminergic activity during the 1990s with studies by Schultz et al. (1993, 1997); Schultz (1998). For these reasons, neuromodulation is considered an essential element in cognitive and behavioral processes, and has been the topic of a considerable amount of work in EPANN (Section III-E).\nIn short, plasticity in the brain appears driven by an extraordinarily rich set of dynamics and chemicals. From these biological observations, computational questions relevant to EPANN include: (1) How does the brain structure grow – driven both by genetic instructions and neural activity – to acquire functions and behavior? (2) What are the key plasticity mechanisms from biology that can be applied to EPANN? (3) Are memories, skills, and behaviors stored in plastic synaptic connections, in patterns of activities, or a combination of both? Whilst neuroscience continues to provide inspiration and insight into plasticity in biological brains, EPANNs serve the complementary objective of implementing, seeking, and verifying designs of bio-inspired methods for adaptation, learning, and intelligent behavior.\nC. Abstractions in brain simulations If an accurate brain simulator existed, the model would manifest most, if not all, the behavioral and cognitive properties of a biological brain. Thus, the attempt to implement intelligence by simulating a complex biological network of neurons appears justified. However, despite considerable progress, current neural models are far from exhibiting sophisticated cognitive abilities. It is thus a legitimate question to ask what is missing from current models of the brain to match animal and human intelligence.\nNeural models range from small directed graphs in which nodes are simple functions of the input, to complex and sometimes large spiking neural models and bio-physical models (Markram, 2006, 2012; Eliasmith et al., 2012). The variety and range of neural models derive from their diverse aims, but also from the current uncertainty on the level of abstraction required to reproduce those behaviors typical of human and animal intelligence.\nWhilst modeling neural cells and their connectivity is one approach to mimicking brain function, a second approach is based on information processing and statistics. Computational neuroscience seeks to make sense of the computation in the brain from a theoretical view point, thus understanding neural dynamics, processes and computation from a mathematical and statistical perspective. Computational neuroscience is concerned primarily with biological brains, so adhering to biological properties and measurements is of particular importance. Although brain models and computational neuroscience do not seek to create EPANNs, they can provide tools to design such systems.\nD. Artificial plastic neural networks Artificial neural networks are plastic whenever a learning algorithm is applied to modify their properties such as the\n5 topology or the connectivity. Because networks perform correctly only with specific weight configurations, learning rules are an integral part of most neural networks; see Widrow and Lehr (1990) for an early review. One exception, not discussed in this paper, is when evolution is used to search static connection weights of a network, i.e. as a surrogate of learning rules (Yao, 1999).\nEPANNs are networks that, as opposed to hand-designed ANNs, employ evolution as a meta-learning process in which the learning mechanisms are evolved. Testing occurs with behavioral simulations that often apply online learning, i.e. the learning is active at each time step. Established learning rules are often used as ingredients or inspiration in EPANNs, which then search for good parameter configurations, efficient combinations of rules and network topologies, new functions representing novel learning rules, etc. EPANN experiments are suited to include the largest possible variety of rules because of (1) the variety of possible tasks in a simulated behavioral experiment and (2) the flexibility of evolution to combine rules with no assumptions about their dynamics. The review that follows gives a snapshot of the extent and scope of various learning algorithms for ANN that can effectively be building blocks of EPANN.\nIn supervised learning, backpropagation is the most popular learning rule used to train both shallow and deep networks (Rumelhart et al., 1988; LeCun et al., 2015), typically for classification in feed-forward networks. Unsupervised learning is implemented in neural networks with self-Organising Maps (SOM) (Kohonen, 1982, 1990), auto-encoders (Bourlard and Kamp, 1988), restricted Boltzmann machines (RBM) (Hinton and Salakhutdinov, 2006), and Hebbian plasticity (Hebb, 1949; Gerstner and Kistler, 2002a; Cooper, 2005). Hebbian rules, in particular, given their biological plausibility and unsupervised flavor of learning, are a particularly important inspirational principle. Variations have been proposed to include, e.g. terms to achieve stability (Oja, 1982; Bienenstock et al., 1982) and various constraints (Miller and Mackay, 1994), or more advanced update dynamics such as dual weights for fast and slow decay (Levy and Bairaktaris, 1995; Soltoggio, 2015). Those rules depend often on the precise topology of the network and are hand-designed, although cost-minimization Hebbian rules have been proposed recently (Pehlevan et al., 2015; Bahroun and Soltoggio, 2017).\nOften, behavior learning requires a measure of the quality of such behaviors, implemented as positive or negative values that represent the consequences of stimulus-action sequences. This type of learning is also referred to as trial and error, or operant reward learning in animal learning, and was formalized mathematically as Reinforcement Learning (RL) (Sutton and Barto, 1998). RL in neural networks (Pennartz, 1997) makes use of robotic or agent-environment problems with a reward signal that affects plasticity in the network (Lin, 1993; Niv et al., 2002; Soltoggio and Stanley, 2012; Auerbach et al., 2014; Pugh et al., 2014)\nNeuromodulated plasticity (Fellous and Linster, 1998) is an important mechanism in EPANNs (reviewed later in Section III-E). Modulation of signals and gated learning (Abbott, 1990) is inspired by the idea that behavior learning and reg-\nulation of behavior require neuromodulatory activity (Baxter et al., 1999; Suri et al., 2001; Birmingham, 2001; Alexander and Sporns, 2002; Doya, 2002; Fujii et al., 2002; Suri, 2002; Ziemke and Thieme, 2002; Sporns and Alexander, 2003).\nPlastic neural models are used also to demonstrate how behavior can emerge from a particular circuitry modeled after biological brains. Computational models of, e.g., the basal ganglia and modulatory systems may propose plasticity mechanisms and aim to demonstrate the computational relations among various nuclei, pathways, and learning processes (Krichmar, 2008; Vitay and Hamker, 2010; Schroll and Hamker, 2015).\nFinally, plasticity rules for spiking neural networks (Maass and Bishop, 2001) aim to demonstrate unique learning mechanisms that emerge from spiking dynamics (Izhikevich, 2007), as well as model biological synaptic plasticity (Gerstner and Kistler, 2002b).\nPlasticity in neural networks, when continuously active, was also observed to cause catastrophic forgetting (Robins, 1995). If learning occurs continuously, i.e. online, new information or skills have the potential to overwrite previously acquired information or skills (Abraham and Robins, 2005). Metaplasticity and consolidation mechanisms were suggested to address this problem (Finnie and Nader, 2012; Soltoggio, 2015).\nIn conclusion, plasticity is central to most approaches to ANNs. The variety of algorithms indicate that different approaches were developed to solve different problems. In this context, EPANNs offer a unique tool to test hypotheses on the effectiveness and suitability of different models, or their combination, in a variety of different scenarios.\nE. Intelligence-testing environments\nEvolution works on organisms within an environment to exploit its resources, find survival solutions, and adapt to changes (Darwin, 1859). However, what makes an environment conducive to the evolution of learning is not clear. In particular, what are the challenges faced by learning organisms in the natural world, and how can those be abstracted and ported to a virtual, simulated environment?\nIn the early phases of AI, logic and reasoning were thought to be key to intelligence (Cervier, 1993), so symbolic inputoutput mappings were employed as tests. Soon it became evident that intelligence is not only symbol manipulation, but resides also in subsymbolic problem solving abilities emerging from the interaction of brain, body and environment (Steels, 1993; Sims, 1994), so more complex simulators of real-life environment were developed. A further step in complexity is represented by high-level planning and strategies required, e.g., when applying AI to games (Allis et al., 1994; Millington and Funge, 2016) or articulated robotic tasks. Planning and decision making with high bandwidth sensory-motor information flow such as those required for humanoid robots or self-driving vehicles are current benchmarks of intelligent systems. Finally, environments in which affective dynamics and feelings play a role are recognized as important for human well being (De Botton, 2016). Detecting human emotions (Lee and Narayanan, 2005) and providing emotional support will\n6 increase in importance as a benchmark for AI. The increase in the variety and complexity of simulated environments offers new opportunities to develop EPANN as further outlined later in Section 4, on open challenges."
    }, {
      "heading" : "III. PROGRESS ON EVOLVING ARTIFICIAL PLASTIC NEURAL NETWORKS",
      "text" : "EPANNs may draw inspiration from one or more theories and techniques overviewed in the previous section. Specific research questions under investigation determine the sets of chosen algorithms. One common focus is using evolution to search for emergent mechanisms of adaptation, intelligence, and complexity. Thus, the objectives may not be only to achieve quality solutions, but primarily to investigate the search path, the evolutionary steps, the features of the final solutions, and other dynamical aspects of the evolutionary process. The hope is that counter-intuitive, distributed, subsymbolic and minimal algorithms will emerge from a combination of the right building blocks and initial conditions. Within this general idea of searching for emergent mechanisms of learning and adaptation, particular areas of focus, reviewed in the next sections, have contributed to progress with EPANNs.\nEPANN setups can be viewed as the interaction of simulated evolution and intelligent testing environments in which the various inspirational principles illustrated in this section are used to enrich the learning dynamics. As simplified graphical representation is given in Fig. 2.\nA. Evolutionary algorithms for EPANN When applied to EPANNs, artificial evolution is characterized by systems of growing complexity and changing search spaces (Holland, 1975; Michalewicz, 1994). As such, it differs from parameter optimization (Bäck and Schwefel, 1993) in which search spaces are often static. The growing and changing fitness landscapes affect the choice of the particular evolutionary algorithm to use to evolve plastic networks. Desired features for an evolutionary algorithm to evolve plastic networks are:\n• variable genotype length to allow for complexity to grow through evolution; • indirect genotype to phenotype encodings to enable efficient and evolvable representations; • mutations and/or recombination of genomes that allow exploration of new skills while maintaining basic functional properties of the solutions; • expressing regularity, repetition and patterns, which appear to be features in biological structures • flexibility to operate at different levels of abstraction, e.g. network, neuron, or synaptic level, to allow evolution to extend the search across those levels; • maintaining diversity in the population to take advantage of parallel search along diverse phylogenetic paths, and to allow increased exploratory search along paths of suboptimal fitness values; • devising fitness as an indication of general adaptive or intelligent behavior to provide an evolutionary advantage to solutions expressing those properties.\nAll points, except the last two, deal with the capability of the algorithm to mimic various biological phenomena within the evolving solution. The last two points deal with the modality of selection, how and which solutions survive.\nMany evolutionary algorithms include one or more of those mechanisms (Fogel, 2006). Due to the complexity of neural network design, the field of neuroevolution was the first to explore most extensively those mechanisms. Popular algorithms include early work of Angeline et al. (1994) and Yao and Liu (1997) to evolve neural networks, Neuroevolution of Augmenting Topologies (NEAT) (Stanley and Miikkulainen, 2002), Analog Genetic Encoding (AGE) (Mattiussi and Floreano, 2007), and HyperNEAT (Stanley et al., 2009). While these algorithms were not initially devised to evolve plastic networks, they are often adapted to operate with EPANNs.\nB. Evolution of plasticity rules\nEarly experiments in EPANN started with the attempt to evolve learning rules to change the connection strength among nodes in a connected graph, and thereby enable online lifetime learning. In contrast to back-propagation of errors (Widrob and Lehr, 1990), considered biologically implausible, the Hebbian principle (Hebb, 1949; Cooper, 2005) is a more popular choice in EPANN because is biologically plausible due its dependency solely on local activity values. In its simplest form, the synaptic strength w is updated by the product of presynaptic (x) and postsynaptic (y) activities, and a learning rate η:\n∆w = η · x · y . (1)\nMore generally, the synaptic change can be expressed as an arbitrary function of local values:\n∆w = f(x, y, w,θ) , (2)\nwhere θ are parameters of the function. The type of function and its parameters can be searched by evolution.\nBengio et al. (1990, 1992) proposed the optimisation of a learning rule via evolutionary search. The idea was to use evolutionary search to optimize parameters according to an error (or cost) function. Those studies are also among the first to include a modulatory term in the learning rules. Chalmers (1990) evolved a global learning rule (i.e. a rule that applies to every connection) and discovered that the evolved rule was similar to the well-known delta rule, or Widrow-Hoff rule (Widrow et al., 1960), used in backpropagation. Fontanari and Meir (1991) build on Chalmers (1990)’s approach to evolve a learning algorithm for single-layer networks with binary weights. Baxter (1992) evolved a local learning rule and the architecture for a network that learned four boolean functions of one variable. The network was called Local Binary Neural Net (LBNNs): each connection had a rule that took local neural values and changed its weight to one of two possible values (±1). The ideas of evolving learning was implemented in Nolfi and Parisi (1993) by evolving “auto-teaching” inputs, which could then provide an error signal for the network to adjust weights during lifetime. McQuesten and Miikkulainen (1997) showed that neuroevolution can benefit from parent networks teaching their offspring through backpropagation. In\n7 EPANN output\nenvironment\nse ns\nor y-\nm ot\nor\ndr iv\nen le\nar ni\nng\nfitness\n010110…\nmodulatory neurons\nplasticity functions\nlearning architectures\n/modules agent\ninput\nM\nevolved learning features\n101110…\n001110…\n111011…\nevolution via EPANN-specific\nalgorithms\n110111…\ngenotype to phenotype mapping\nevolved plasticity\ngenes\nlearning parameters\nlifetime learning\nlearning-enabling fixed architectures\nFigure 2: Main elements of an EPANN set-up in which simulated evolution (left box) and an environment (right box) allow for plastic networks to evolve through generations and learn within a lifetime in the environment (central box). The network may include diverse elements, e.g. modules or architectures, modulatory neurons, inputs/outputs, learning parameters, and other structures that enable learning.\nHusbands et al. (1998) and Smith et al. (2002), the authors propose a plastic network that adapts its activation functions by simulating the diffusion of a gas. Although the tasks did not involve learning, the network appears to evolve faster than a network evolved with fixed weights.\nSuch studies that evolve a network to learn seek dynamics that can be viewed as a meta-learning process. Abraham (2004) proposed a method called Meta-Learning Evolutionary Artificial Neural Networks (MLEANN) in which evolution searches for initial weights, neural architectures and transfer functions, and finally optimizes existing learning rules. Orchard and Wang (2016) encode the learning rule itself as a network that is then evolved. One general purpose in those studies is to find learning rules, via evolution, that permit a consistent, reliable, and scalable learning in specific problems (Soltoggio, 2008b; Risi and Stanley, 2012).\nC. EPANNs in Evolutionary Robotics\nEvolutionary Robotics (ER) brought the idea of autonomously designing robots and controllers by means of evolution (Cliff et al., 1993; Floreano and Mondada, 1994, 1996; Urzelai and Floreano, 2000; Floreano and Nolfi, 2004). One important principle is the idea of evolving brains with body and environment (Chiel and Beer, 1997), in which intelligence is embodied because it emerges from the interaction of brainbody-environment (Floreano et al., 2004).\nWhilst ER originated with the idea of evolving simulated and real robots with no specific assumptions on neural systems or plasticity (Smith, 2002), experiments suggested that neural control structures evolved with fixed weights perform less well than plastic networks in robotic applications (Nolfi and Parisi, 1996; Floreano and Urzelai, 2001a). In particular, plastic networks were shown to adapt better in the transition from simulation to real robots. This made sense as well as the intuitive concept that evolved plastic neural networks increase the adaptability to changing ER environments (Nolfi and Parisi, 1996). However, the precise nature and the magnitude of those changes that plasticity can cope with is not always easy to quantify in robotic settings. Another study reported\nthat networks evolved faster, i.e., had better evolvability, when synaptic plasticity and neural architectures are evolved simultaneously (Floreano and Urzelai, 2001b).\nThe behavioral changes required to switch behaviors in simple ER experiments can also take place with non-plastic recurrent neural networks because inputs can switch the network’s activity to different attractors. These dynamics were observed in particular in recurrent networks in which neurons are modeled as leaky integrators (Beer and Gallagher, 1992; Yamauchi and Beer, 1994), which implement different time constants across neurons. Such networks were called continuous time recurrent neural networks (CTRNN). A study in 2003 observed similar performance in learning when comparing plastic and non-plastic neural networks (Stanley and Miikkulainen, 2003a). That study put forward the idea that the learning of plastic networks as described in Blynel and Floreano (2002, 2003) was at that point still a proof-of-concept rather than a superior learning tool because networks with fixed weights could achieve similar performance. Attempts to evolve plastic neural controllers extended also to models of spiking neural networks (Maass and Bishop, 2001), which were tested in evolutionary learning experiments (Di Paolo, 2003; Federici, 2005). Spiking neural networks, however, require a more computationally intensive simulation, making evolution less feasible.\nIn short, ER sparked the discussion around design principles for robots and controllers, proposing the idea that bio-inspired evolutionary design may provide innovative solutions to control problems. In this attempt, evolving learning neural systems played a central role. Examples of the strong focus on learning in ER projects are provided, e.g., in The Cyber Rodent Project (Doya and Uchibe, 2005) that investigated the evolution of learning by seeking to implement a number of features such as (1) evolution of neural controllers, (2) learning of foraging and mating behaviors, (3) evolution of learning architectures and meta-parameters, (4) simultaneous learning of multiple agents in a body, and (5) learning and evolution in a self-sustained colony.\nExperiments in ER in the 1990s and early 2000s revealed\n8 the extent, complexity, and multitude of ideas behind the evolutionary design of learning robots. The challenges of evolving robotic hardware and control are combined with the challenges of setting the correct levels of abstraction for all parts of the solution, of evolving learning mechanisms, and of devising a conducive evolutionary and learning environment. For these reasons, the advances in ER led to a shift in focus more on specific problems. Among those are the interaction of evolution and learning, the evolution of neuromodulation, the evolution of learning architectures, and the evolution of indirectly encoded plasticity.\nD. The interaction of evolution and learning\nEPANNs are affected by the complex interaction of evolution and learning. Such interaction dynamics are present in any system that seeks the evolution of learning, and for this reason, it is worth presenting a few general concepts extending beyond EPANNs in the next paragraph.\nLifetime learning was first suggested to have an impact on accelerating evolution by Baldwin (1896). The studies in Smith (1986); Hinton and Nowlan (1987); Boers et al. (1995); Mayley (1996); Bullinaria (2001) demonstrated the Baldwin effect with computational simulations. For example, Mayley (1997) investigated the effects of learning on the rate of evolution, showing that a learning process during an individual lifetime, although costly, can improve the speed of evolution of the species. An important distinction can be drawn between (1) learning invariant environmental features and (2) learning variable environmental features. Hinton and Nowlan (1987) investigated the first case with a static environment and proved that learning can speed up the evolutionary acquisition of invariant knowledge by creating a gradient in the fitness landscape. In dynamic and uncertain environmental conditions, instead, knowledge cannot be acquired by the evolutionary process, as lifetime learning is required, and therefore the interaction of evolution and learning is less understood. A large body of work is dedicated to study the interaction and dynamics of all three POE dimensions, e.g. Paenke (2008); Paenke et al. (2009). Paenke et al. (2007) investigated the influence of plasticity and learning on evolution under directional selection; Mery and Burns (2010) demonstrate that behavioral plasticity affects evolution.\nIn EPANNs, similarly complex interaction dynamics between evolution and learning were also observed. One study (Soltoggio et al., 2007) suggests that evolution discovers learning by discrete stepping stones: when evolution casually discovers a weak mechanism of learning, it is sufficient to create an evolutionary advantage, so the neural mechanism is subsequently evolved quickly to perfect learning: Fig. 3 shows the best fitness in a foraging task when the agent suddenly evolves a learning strategy. When an environment changes over time, the frequency of those changes plays a role. With timescales comparable to a lifetime, evolution may lead to phenotypic plasticity, which is the capacity for a genotype to express different phenotypes in response to different environmental conditions (Lalejini and Ofria, 2016). The frequency of environmental changes was observed experimentally in\n25000 1000 2000\n234\n178\n190\n200\n210\n220\nGenerations\nFi tn\nes s/\nRe w\nar d\nA learning strategy is discovered by an EPANN\nA navigation strategy reward-learning is first evolved without\nFigure 3: Discovery of a learning strategy during evolution. After approximately 1,000 generation, a reward signal is used by one network to learn where is the changing location of a stochastic reward. Figure adapted from Soltoggio et al. (2007).\nplastic neural networks to affect the evolution of learning (Ellefsen, 2014), revealing a complex relationship between environmental variability and evolved learning.\nA focus on the deceptiveness of evolving to learn is presented in Risi et al. (2010); Lehman and Miikkulainen (2014). In Risi et al. (2009, 2010), EPANN-controlled simulated robots, evolved in a discrete T-Maze domain, reveal that the evolutionary stepping stones towards adaptive behaviors are often not rewarded by objective-based performance measures. In other words, the necessary evolutionary stepping stones towards achieving adaptive behavior receive a lower fitness score than more brittle solutions with low learning but effective behaviors. A solution to this problem (Risi et al., 2010, 2009) was devised in Lehman and Miikkulainen (2014), in which novelty search (Lehman and Stanley, 2011) was adopted as a substitute to performance in the fitness objective with the aim of finding novel behaviors. Novelty search was observed to perform significantly better in the T-Maze domain.\nInterestingly, by rewarding novel behaviors, novelty search validates the importance of exploration or curiosity, previously proposed in Schmidhuber (1991, 2006), also from an evolutionary viewpoint. With the aim of validating the same hypothesis, Soltoggio and Jones (2009) devised a simple EPANN experiment in which exploring was more advantageous than exploiting when reward information was not used, e.g. , before learning evolves; to do this, the reward at a particular location depleted itself if continuously visited, so that changing location at random in a T-maze became beneficial. Interestingly, evolution discovered first exploratory behavior, even if not linked to learning, which in turn lead to an earlier evolution of reward-based learning. In that experiment, a stepping stone to evolve reward-based learning was to evolved non-reward based exploration.\nThe problem of deception in the evolution of learning can be seen as becoming trapped in a local optimum from which escaping requires the evolution of complex strategies. Such a situation emerges when an agent evolves a non-learning strategy that consistently and reliably delivers an average reward, as opposed to an agent that attempts learning, and by doing so risks collecting less reward during the learning\n9 mod neuron\nstandard neuron standard neuron\nw\nm\nplastic connection\nFigure 4: A modulatory neuron gates plasticity of the synapses that connect to the postsynaptic neuron. The learning is local, but a learning signal can be created by part of the network and used to regulate learning elsewhere.\nprocess. The seminal work in Bullinaria (2003, 2007a, 2009b) proposes the more general hypothesis that learning requires the evolution of long periods of parental protection and late onset of maturity. Similarly, Ellefsen (2013b,a) investigates sensitive and critical periods of learning in evolved neural networks. This fascinating hypothesis has wider implications for experiments with EPANNs, and more generally for machine learning and AI. It is therefore foreseeable that future EPANNs will have a protected childhood during which parental guidance may be provided (Clutton-Brock, 1991; Klug and Bonsall, 2010).\nE. The evolution of neuromodulation\nGrowing neuroscientific evidence on the role of neuromodulation (previously outlined in Section II-B) inspired the design of experiments with neuromodulated plasticity to evolve control behavior and learning strategies. A general form of neuromodulated plasticity can be expressed as\n∆w = m · f(x, y, w,θ) , (3)\nwhere m is a modulatory signal used a multiplication factor to gate plasticity. Graphically, modulation can be represented as a different type of signal affecting plasticity at the synaptic connections of the afferent neuron (Fig. 4).\nEvolutionary search was used to find the parameters of a neuromodulated Hebbian learning rule in a reward-based armed-bandit problem in Niv et al. (2002). The same problem was used later in Soltoggio et al. (2007) to evolve arbitrary learning architectures with a bio-inspired gene representation method called Analog Genetic Encoding (AGE) (Mattiussi and Floreano, 2007). Evolution was used to search modulatory topologies and parameters of a particular form of Eq. 3:\n∆w = m · (Axy +Bx+ Cy +D) , (4)\nwhere the parameters A to D determine the influence of four factors in the rule: a multiplicative Hebbian term A, a presynaptic term B, a postsynaptic term C, and pure modulatory, or heterosynaptic, term D. D allows the modulatory term alone to increase or decrease the afferent weights of the target neuron regardless of pre or postsynaptic activities. At the same time, Kondo (2007) proposed an evolutionary design and behavior analysis of neuromodulatory neural networks for mobile robot control, validating the potential of the method. Soltoggio et al. (2008) tested the hypothesis of whether modulatory dynamics held an evolutionary advantage. In their algorithm, modulatory\nneurons are freely inserted or deleted by mutations, effectively allowing the system to autonomously select the computational components that give an evolutionary advantage. The evolving networks demonstrated that modulatory neurons were inserted into control networks by evolution with functional learning properties, providing an evolutionary advantage and better learning for those networks. In another study, Soltoggio (2008c) suggested that evolved modulatory neurons may be essential to separate the learning circuity from the input-output controller, effectively contributing to shortening the inputoutput pathways and speeding up decision processes. The fundamental question of what learning rules and dynamics are required to solve which problems remains mostly unanswered (Soltoggio, 2008b), mainly because learning dynamics are affected by tight coupling between rules and architectures in a search space with many equivalent but different control structures. Fig.4 also suggests that modulatory networks require evolution to find two essential topological structures: what signals or combination of signals triggers modulation, and what neurons are to be targeted by modulatory signals.\nA number of further studies on the evolution of neuromodulatory dynamics confirmed the evolutionary advantages in learning scenarios (Soltoggio, 2008a). Silva et al. (2012a) used simulations of 2-wheel robots performing a dynamic concurrent foraging task, in which scattered food items periodically change their nutritive value or become poisonous, similarly to the setup in Soltoggio and Stanley (2012). The results showed that when neuromodulation is enabled, learning is evolved faster than when neuromodulation is not enabled, also with multi-robot distributed systems (Silva et al., 2012b). Nogueira et al. (2013) also reported evolutionary advantages in foraging behavior of an autonomous virtual robot when equipped with neuromodulated plasticity. Harrington et al. (2013) demonstrated how evolved neuromodulation applied to a gene regulatory network consistently generalizes better than agents trained with fixed parameter settings. Surprisingly, Arnold et al. (2013b) showed that neuromodulatory architectures provide an evolutionary advantage also in reinforcementfree environments. The evolution of social representations in neural networks was shown to be facilitated by neuromodulatory dynamics in Arnold et al. (2013a). An artificial life simulation environment called Polyword (Yoder and Yaeger, 2014) helped to assess the advantage of neuromodulated plasticity in various scenarios. The authors found that neuromodulation may be able to enhance or diminish foraging performance in a competitive, dynamic environment.\nNeuromodulation was evolved in Ellefsen et al. (2015) in combination with modularity to address the problem of catastrophic forgetting. In Gustafsson (2016), networks evolved with AGE (Mattiussi and Floreano, 2007) to play a game were shown to perform better with the addition of neuromodulation. In Norouzzadeh and Clune (2016), the authors demonstrated that neuromodulation produces forward models that can adapt to changes significantly better than the controls. They verified that evolution exploited variable learning rates to perform adaptation when needed.\nThe evidence in these studies suggests that neuromodulation is a key ingredient to facilitate the evolution of learning in\n10\nEPANN. They also indirectly suggest that neural systems with more than one type of signal (i.e. activation as well as other modulatory signals) are beneficial in the neuroevolution of learning.\nF. Evolving learning architectures\nJust as neuromodulation is one particular plasticity mechanism that enables effective reward learning, other principles may benefit particular types of learning behavior, as outlined in the review by (Floreano et al., 2008) and in a recent book by Downing (2015). Evolving architectures might be as simple as optimizing the number of hidden neurons, or the number of layers in a network, but in general, it refers to the discovery of neural pathways, learning rules, and other mechanisms whose synergetic matching enables particular learning dynamics. It is also important to note that EPANN do not have the neurophysiological constraints of biological systems, e.g., short neural connections, sparsity, brain size limits, etc., which impacts the evolution of particular architectures such as modularity (Bullinaria, 2007b, 2009a).\nOne seminal early study by Happel and Murre (1994) proposed the evolutionary design of modular neural networks in which modules could perform unsupervised learning, and the intermodule connectivity was shaped by Hebbian rules. In Khan et al. (2008), the authors propose an evolutionary developmental system. The resulting neural network is an example of an architecture that adapts with learning: the network has a dynamic morphology in which neurons can be inserted or deleted, and synaptic connections form and change in response to stimuli. Downing (2007) looks at different computational models of neurogenesis to propose an evolutionary developmental system, focusing in particular on abstraction levels and principles such as Neural Darwinism (Edelman and Tononi, 2000). In Khan et al. (2011); Khan and Miller (2014), the authors introduce a large number of bioinspired mechanisms to evolve networks with rich learning dynamics. The idea is to use evolution to design a network that is capable of advanced plasticity such as dendrite branch and axon growth and shrinkage, neuron insertion and destruction, and many others.\nIn short, learning mechanisms and neural architectures appear strongly interdependent: EPANNs offer a tool to study the evolution of both simultaneously. This dependency is currently of particular interest also outside the field of EPANNs: in deep networks, learning rules and architectures need to be manually and carefully tuned to achieve good performance.\nG. Indirectly Encoded Plasticity\nJust as connection weights in EPANN may not be individually encoded in the genome, also plasticity may be coded in the genome to be expressed indirectly in the phenotype. Using a T-Maze domain as learning task, Risi and Stanley (2010) showed that HyperNEAT, which usually implements a compact encoding of weight patterns for large-scale ANNs (Fig. 5a), can also encode patterns of local learning rules. The approach, called adaptive HyperNEAT, can encode arbitrary learning rules for each connection in an evolving\nANN based on a function of the ANN’s geometry (Fig. 5b). Further flexibility was added in Risi and Stanley (2012) to simultaneously encode the density and placement of nodes in substrate space. The approach, called adaptive evolvablesubstrate HyperNEAT, makes it possible to indirectly encode plastic ANNs with thousands of connections that exhibit regularities and repeating motifs. Adaptive ES-HyperNEAT allows each individual synaptic connection, rather than neuron, to be standard or modulatory, thus introducing further design flexibility.\nMore recently, Risi and Stanley (2014) showed how adaptive HyperNEAT can be seeded to produce a specific lateral connectivity pattern, thereby allowing the weights to self-organize to form a genuine topographic map of the input space. This advance is important because it shows how evolution can be seeded with specific plasticity mechanisms that could facilitate the evolution of higher cognitive abilities.\nThe effect of encoding plasticity rules as patterns on the learning and on the evolutionary process was considered by Tonelli and Mouret (2013). Using an operant conditioning task, i.e., learning by reward, the authors showed that indirect encodings that produce more regular neural structures also improve the general EPANN learning abilities when compared to direct encodings."
    }, {
      "heading" : "IV. OPEN CHALLENGES AND NEW DIRECTIONS",
      "text" : "The body of work reviewed in the previous section contributed from different angles to produce the idea of EPANNs. The principles at play draw from a large variety of bio-inspired concepts and technological advances. Thus, new advances in AI, machine learning and neural networks are currently setting the groundwork for new directions for EPANNs. This section presents promising research themes that have recently emerged, and have the potential to extend and radically change the field of EPANN.\nA. Levels of abstraction and representations for neural adaptivity\nChoosing the right level of abstraction and the right representation (Bengio et al., 2013) are themes at the heart of many problems in AI. Because EPANN draw inspiration from many different research areas (see Fig. 1), the choices of abstraction level and representation are multiple.\nLevels of abstraction are influenced by two main factors: low levels require increased computational costs, while high levels require an intuition of the essential dynamics that are necessary in the model. In the first case, the increase of computational power with modern hardware provides increasingly more resources to model large-scale neural systems at low levels, and to operate within complex environments. However, low level models are appropriate to model low level dynamics, but might not appropriately answer questions on the high level mechanisms on which evolution and learning operate.\nOne case in which the abstraction level is still clearly an open question is that of two fields simulating large neural networks: deep learning and bio-physical neural models. The progress in learning performance with deep networks\n11\n(Krizhevsky et al., 2012; LeCun et al., 2015; Schmidhuber, 2015), based mainly on rate-based neurons and low precision in the computation appears to contrast with the low-level simulations of accurate bio-physical models such as the Blue Brain Project (Markram, 2006), intended to reveal the computational and learning properties of a cortical column. Although the two approaches have different aims, they both represent abstractions of neural computation. Research in EPANN is best placed to address the problem of levels of abstractions because it can autonomously discover the most appropriate abstraction levels to evolve neural adaptation and intelligence.\nIn the context of genotype-to-phenotype mappings, Compositional Patterns Producing Networks (CPPNs) (Stanley, 2007), and also the previous work of Sims (1991), demonstrated that structured phenotypes can be generated through a function mapping without going through the dynamic developmental process typical of multicellular organisms. Relatedly, Hornby et al. (2002) showed that the different phenotypical representations led to considerably different results in the evolution of furniture. Miller (2014) discussed explicitly the effect of abstraction levels for evolved developmental learning networks, in particular in relation to two approaches that model development at the neuron level or at the network level.\nThis body of work suggests that finding appropriate representations, just as it was fundamental in the advances in deep learning to represent input spaces and hierarchical features (Bengio et al., 2013; Oquab et al., 2014), can also extend to representations of internal models, learning mechanisms, and genetic encodings, affecting the algorithms’ capabilities of evolving learning abilities.\nB. Evolving general learning One challenge in the evolution of learning is that evolved learning may simply result in the switch among a finite set of evolved behaviors, e.g. turning left or right in a T-Maze in a finite sequence, which is all that evolving solutions encounter during lifetime. A challenge for EPANNs is to acquire general\nlearning abilities in which the network is capable of learning problems not encountered during evolution. Mouret and Tonelli (2014) propose the distinction between the evolution of behavioral switches and the evolution of synaptic general learning abilities, and suggest conditions that favor these types of learning. General learning can be intuitively understood as the capability to learn any association among input, internal, and output patterns, both in the spatial and temporal dimensions, regardless of the complexity of the problem. Such objective clearly poses practical and philosophical challenges. Although humans are considered better at general learning than machines, human learning skills are specific and not unlimited (Ormrod and Davis, 2004). Nevertheless, moving from behavior switches to more general learning is a desirable feature for EPANN. Encouraging the emergence of general learners may likely involve (1) an increased computational cost for testing in rich environments, which include a large variety of uncertain and stochastic scenarios with problems of various complexity, and (2) an increased search space to explore the evolution of complex strategies and avoid deception.\nOne promising direction is to evolve the ability to learn algorithms and programs instead of simple associations. This approach has recently shown promise in the context of deep learning; methods such as the Neural Turing Machine (Graves et al., 2014) can learn simple algorithms such as sort, copy, etc. and approaches such as the Neural Programmer-Interface (Reed and de Freitas, 2015) can learn to represent and execute programs.\nC. Incremental and social learning An important open challenge for machine learning in general is the creation of neural systems that can continuously integrate new knowledge and skills without forgetting what they previously learned. A promising deep learning approach is progressive neural networks (Rusu et al., 2016), in which a new network is created for each new task, and lateral connections between networks allow the system to leverage\n12\npreviously learned features. Weight consolidation, formalized in a rule called Hypothesis Testing Plasticity, was shown in Soltoggio (2015) to permit learning of multiple tasks without the catastrophic forgetting typical of neural networks. The method suggested the consolidation of weights when associations became established after a period of learning. A similar idea was later named elastic weight consolidation in Kirkpatrick et al. (2017). Plasticity rules that implement weight consolidation (Soltoggio, 2015; Kirkpatrick et al., 2017) have proven to enhance networks to learn new tasks without suffering from catastrophic forgetting. Such approaches may become standard practice in EPANNs.\nEncouraging modularity (Ellefsen et al., 2015; Durr et al., 2010) or augmenting evolving networks with a dedicated external memory component (Lüders et al., 2016) have been proposed recently. The scalability of these approaches and their impact on the evolutionary path will be a further topic of investigation. However, an evolutionary advantage is likely to emerge for networks that can elaborate on previously learned sub-skills during their lifetime to learn more complex tasks.\nOne interesting case in which incremental learning may play a role is social learning (Best, 1999). EPANNs may learn both from the environment and from other individuals, from scratch or incrementally (Offerman and Sonnemans, 1998). When social, learning may involve imitation, language or communication, or other social behaviors. Bullinaria (2017) proposes an EPANN framework to simulate the evolution of culture and social learning. It is reasonable to assume that future AI learning systems, whether based on EPANNs or not, will acquire knowledge through different modalities, involving direct experience with the environment, but also social interaction, and possibly with complex incremental learning phases.\nD. Fast learning through generalization\nAnimal learning does not always require myriads of trials. Humans can very quickly generalize from only a few given examples, possibly leveraging previous experiences and a long learning process during infancy. This type of learning is currently missing in EPANNs. Inspiration for new approaches could come from complementary learning systems (Kumaran et al., 2016) that humans seem to possess, which include fast and slow learning components. Additionally, approaches such as probabilistic program induction seem to be able to learn concepts in one-shot at a human-level in some tasks (Lake et al., 2015). Fast learning is likely to derive not just from trial-and-error, but also from mental models that can be applied to diverse problems, similarly to transfer learning (Pan and Yang, 2010). Reusable mental models, once learned, will allow agents to make predictions and plan in new and uncertain scenarios with similarities to previously learned ones. If EPANNs can discover neural structures or learning rules that allow for generalization, an evolutionary advantage of such a discovery will lead to its full emergence and further optimization of such a property.\nE. The loci of memory\nThe consequence of learning is memory, both explicit and implicit (Anderson, 2013), and its consolidation (Dudai, 2012). EPANNs may reach solutions in which memory evolved in different fashions, e.g., preserved as self-sustained neural activity, encoded by connection weights modified by plasticity rules, stored with an external memory (e.g. Neural Turing Machine), or a combination of these approaches. Neuromodulation and weight consolidation target areas of the network where information is stored (Soltoggio et al., 2008; Soltoggio, 2015). Recurrent neural architectures based on long short-term memory (LSTM) allow very complex tasks to be solved through gradient descent training (Greff et al., 2015; Hochreiter and Schmidhuber, 1997) and have recently shown promise when combined with evolution (Rawal and Miikkulainen, 2016). A different approach was introduced by Graves et al. (2014), called Neural Turing Machine (NTM), in which networks are augmented with an external memory that allows long-term memory storage. NTMs have shown promise when trained through evolution (Greve et al., 2016) or gradient descent (Graves et al., 2014, 2016). However, imposing such an explicit representation of memories may not lead to evolvable networks when problems require distributed, possibly implicit memory representations. Research in this area will reveal which computational systems are more evolvable and how memories will self organize and form in EPANN.\nF. EPANN and deep learning\nDeep learning has shown remarkable results in a variety of different fields (Krizhevsky et al., 2012; Schmidhuber, 2015; LeCun et al., 2015). However, the model structures of these networks are mostly hand-designed, include a large number of parameters, and require extensive experiments to discover optimal configurations. Advances in deep networks have come from manually testing various design aspects such as: different transfer functions, e.g. the ReLU (Krizhevsky et al., 2012); generalization mechanisms, e.g. the dropout (Srivastava et al., 2014); networks with different depth, e.g. (Simonyan and Zisserman, 2014), and others (Schmidhuber, 2015). With increased computational resources, it is now possible to search those design aspects with evolution, effectively in what can be called deep EPANNs. It is worth noting that the evolution of static weights for deep networks are not EPANNs. In contrast, certain types of EPANNs that evolve architectures with arbitrary depth, e.g. (Risi and Stanley, 2012), are effectively deep EPANNs.\nKoutnı́k et al. (2014) used evolution to design a controller that combined evolved recurrent neural networks, for the control part, and a deep max-pooling convolutional neural network to reduce the input dimensionality. The study does not use evolution on the deep preprocessing networks itself, but in combination with it, which nevertheless demonstrates the evolutionary design of a deep neural controller. Young et al. (2015) used an evolutionary algorithm to optimize two parameters of a deep network: the size (range [1,8]) and the number (range [16,126]) of the filters in a convolutional neural network. The study showed that the optimized parameters\n13\ncan vary considerably from the standard best-practice values. An established evolutionary computation technique, the Covariance Matrix Adaptation Evolution Strategy (CMA-ES), was used in Loshchilov and Hutter (2016) to optimize the parameters of a deep network to learn to classify the MNIST dataset. The authors reported performance close to the state-ofthe-art using 30 GPU devices. Fernando et al. (2016) was able to rediscover convolutional networks by means of evolution of Differentiable Pattern Producing Networks (Stanley, 2007).\nRecently, Real et al. (2017) and Miikkulainen et al. (2017) showed that evolutionary search can be used to determine the topology, hyperparameters and building blocks of deep networks trained through gradient descent. The performance were shown to rival those of hand-designed architectures in the CIFAR-10 classification task and a language modeling task (Miikkulainen et al., 2017), while Real et al. (2017) also tested the method on the larger CIFAR-100 dataset. A different approach was presented by Fernando et al. (2017), in which evolution determines a subset of pathways through a network that are trained through backpropagation, allowing the same network to learn a variety of different tasks. While these results were only possible with significant computational resources, they demonstrate the potential of combining evolution and deep learning approaches.\nThe flexibility of evolutionary search, which can in some cases work better than gradient descent in search spaces with local optima, allows the design of search processes that not only tune certain parts of the networks, but also combine different mechanisms in an innovative and counterintuitive fashion. An important point is that EPANNs do not necessarily implement error-based rules like backpropagation. Thus, the network need not be differentiable, as it would need to be in gradient descent based approaches, thereby allowing a greater variety in architectures (Greve et al., 2016). Finally, the deception in the evolution of learning suggests that gradient descent approaches can encounter limitations when searching for complex learning strategies, leaving EPANN as a compelling alternative to discover more advanced cognitive learning models.\nG. GPU implementations and neuromorphic hardware\nThe progress of EPANNs will crucially depend on implementations that take advantage of the increased computational power of parallel computation with GPUs and neuromorphic hardware (Jo et al., 2010; Monroe, 2014). Deep learning greatly benefited from GPU-accelerated machine learning but also standardized tools (e.g. Torch, Tensorflow, Theano, etc.) that made it easy for anybody to download, experiment, and extend promising deep learning models.\nHoward et al. (2011, 2012, 2014) devised experiments to evolve plastic spiking networks implemented as memristors for simulated robotic navigation tasks. Memristive plasticity was observed consistently to enable higher performance than constant-weighted connections in both static and dynamic reward scenarios. This body of work consolidates the idea that EPANNs have considerable advantages over non-learning networks.\nIn the context of newly emerging technologies, it is worth noting that, just as GPUs were not developed initially for deep learning, so novel neural computation tools and hardware systems, not developed for EPANNs, can now be exploited to setup the articulated processes and computationally expensive experiments required by EPANN.\nH. Measuring progress\nMuch of the recent progress in neural learning comes from having standardized benchmarks and training sets that algorithms can be evaluated on. However, the measurement rarely considers the learning process itself, but rather the performance when the learning has ended. The development of criteria to measure learning skills (Miconi, 2008), specifically designed to measure the agent’s learning capabilities, and not only a task-specific performance, is necessary to appreciate the scientific significance of, and progress in, EPANNs. A number of platforms for testing AI controllers are becoming available, e.g., the Atari or General Video Game Playing Benchmark (GVGAI, 2017), or the OpenAI Universe, “a software platform for measuring and training an AI’s general intelligence across the world’s supply of games, websites and other applications” (OpenAI, 2017). For EAPNN using such benchmarks, the goal is not to be good at playing one or more games, but rather to test the capability to evolve the learning required for such control problems. Measuring such skills involves not only looking at the final performance, but also at the dynamics of learning (where fast learning is not always better), at the robustness, generalization, initial conditions, type of datasets or richness of the environments, size of the network, and computational resources used. Complex benchmarks could allow testing increasingly complex problems or scenarios, requiring the agent to build upon previously acquired skills."
    }, {
      "heading" : "V. CONCLUSION",
      "text" : "This paper first described the broad inspiration and aspirations of evolved plastic neural networks (EPANNs), which are shown to draw from large, diverse, and interdisciplinary areas. The inspirations also reveal the ambitious and long-term research objectives of EPANNs with important implications for Artificial Intelligence and biology.\nEPANNs saw considerable progress in the last two decades, which led to the discovery of a number of further research themes, such as the design of evolutionary algorithms to promote the evolution of learning, a better understanding of the complex interaction dynamics between evolution and learning, the advantages of multi-signal networks such as modulatory networks, and the attempt to evolve architectures and appropriate representations of learning mechanisms.\nFinally, the paper outlined that recent scientific and technical progress may enable a step change in the potential of EPANNs. In light of current advances, a pivotal moment for EPANNs may be at hand as the next AI tool to find flexible algorithms, capable of discovering principles for general adaptation and intelligent systems.\n14"
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "Biological neural networks are systems of extraordinary computational capabilities shaped by evolution, development, and lifetime learning. The interplay of these elements leads to the emergence of adaptive behavior and intelligence, but the complexity of the whole system of interactions is an obstacle to the understanding of the key factors at play. Inspired by such intricate natural phenomena, Evolved Plastic Artificial Neural Networks (EPANNs) use simulated evolution in-silico to breed plastic neural networks, artificial systems composed of sensors, outputs, and plastic components that change in response to sensory-output experiences in an environment. These systems may reveal key algorithmic ingredients of adaptation, autonomously discover novel adaptive algorithms, and lead to hypotheses on the emergence of biological adaptation. EPANNs have seen considerable progress over the last two decades. Current scientific and technological advances in artificial neural networks are now setting the conditions for radically new approaches and results. In particular, the limitations of hand-designed structures and algorithms currently used in most deep neural networks could be overcome by more flexible and innovative solutions. This paper brings together a variety of inspiring ideas that define the field of EPANNs. The main computational methods and results are reviewed. Finally, new opportunities and developments are presented.",
    "creator" : "LaTeX with hyperref package"
  }
}