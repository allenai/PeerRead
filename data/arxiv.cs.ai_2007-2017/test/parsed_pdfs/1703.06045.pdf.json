{
  "name" : "1703.06045.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Approximation Complexity of Maximum A Posteriori Inference in Sum-Product Networks",
    "authors" : [ "Denis Deratani Mauá" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n70 3.\n06 04\n5v 1\n[ cs\n.A I]\n1 7\nM ar\n2 01\n7"
    }, {
      "heading" : "Approximation Complexity of",
      "text" : ""
    }, {
      "heading" : "Maximum A Posteriori Inference",
      "text" : "in Sum-Product Networks"
    }, {
      "heading" : "Denis Deratani Mauá",
      "text" : ""
    }, {
      "heading" : "Universidade de São Paulo, Brazil",
      "text" : ""
    }, {
      "heading" : "Cassio P. de Campos",
      "text" : ""
    }, {
      "heading" : "Queen’s University Belfast, UK",
      "text" : "We discuss the computational complexity of approximating maximum a posteriori inference in sum-product networks. We first show np-hardness in three-level tree networks by a reduction from maximum independent set; this implies non-approximability within a sublinear factor. We show that this is a tight bound, as we can find an approximation within a linear factor in any three-level networks. We then show that in four-level tree networks it is np-hard to approximate the problem within a factor 2f(n) for any sublinear function f of the size of the input n. Again, this is bound is tight, as we prove that the usual max-product algorithm finds (in any network) approximations within factor 2cn from some constant c < 1. Last, we present a simple algorithm, and show that provably produces solutions at least as good as, and potentially much better, than the max-product algorithm."
    }, {
      "heading" : "1 Introduction",
      "text" : "Finding the mode of a probability distribution is a key step of many solutions to problems in machine learning such as image segmentation, 3D image reconstruction, natural language parsing, statistical machine translation, speech recognition, sentiment analysis, protein design and multicomponent fault diagnosis, to name a few. This problem is often called maximum a posteriori (map) inference, or most likely explanation (mpe).\nSum-Product Networks (spns) are a relatively new class of graphical models that allow marginal inference in linear time in their size [13]. This is therefore in sharp difference with other graphical models such as Bayesian networks and Markov Random Fields that require #p-hard effort to produce marginal inference [5]. Intuitively, an spn encodes\nHeight Lower bound Upper bound\nan arithmetic circuit whose evaluation produces a marginal inference [4]. spns have received increasing popularity in applications of machine learning due to their ability to represent complex and highly multidimensional distributions [13, 11, 3, 8, 1].\nIn his PhD thesis, Peharz showed a direct proof of np-hardness of map in spns by a reduction from maximum satisfiability; his proof however is not correct as it encodes clauses as products [9, Theorem 5.3]. Later, Peharz et al. [10] noted that np-hardness can be proved by transforming a Bayesian network with a Naive Bayes structure into a distribution-equivalent spn of height two (this is done by adding a sum node to represent the latent root variable and its marginal distribution and product nodes as children to represent the conditional distributions). As map inference in the former is np-hard [6], the result follows.\nIn this paper, we show a direct proof of np-hardness of map inference by a reduction from maximum independent set, the problem of deciding whether there is a subset of vertices of a certain size such that no two vertices in the set are connected. This new proof is quite simple, and (as with the reduction from Bayesian networks) uses a sum-product network with three levels. As a corollary of our proof, we obtain the nonapproximability of map inference within a sublinear factor in networks of height two (height is defined as the maximum distance, counted as number of arcs, from the root to a leaf). We show that this is a tight bound, as there is polynomial-time algorithm that produces approximations within a linear factor in networks of height two. We show that, in networks of height three, it is np-hard to approximate the problem within any factor 2f(n) for any sublinear function f of the input size n. This a tight bound, as we show that the usual max-product algorithm by Poon and Domingos [13], which replaces sums with maximizations, finds an approximation within a factor 2c·n for some constant c < 1. Table 1 summarizes these results. As far as we are concerned, these are the first results about the complexity of approximating map in spns.\nWe also show that simple modification to the max-product algorithm results in an algorithm that is consistently and potentially significantly better than the max-product algorithm. We expect this result to foster research in new approximation algorithms for map in spns.\nBefore presenting the complexity results (Section 3), we first review the definition of sum-product networks, and comment on a few selected results from the literature (Section 2). We conclude the paper with a review of the main results (Section 4)."
    }, {
      "heading" : "2 Sum-Product Networks",
      "text" : "We use capital letters without subscripts to denote random vectors (e.g. X), and capital letters with subscripts to denote random variables (e.g., X1). If X is a random vector, we call the set X composed of the random variables Xi in X its scope. The scope of a function of a random vector is the scope of the respective random vector. In this work, we constrain our discussion to random variables with finite domains.\nPoon and Domingos [13] originally defined spns as multilinear functions of indicator variables that allow for space and time efficient representation and inference. In its original definition spns were not constrained to represent valid distributions; this was achieved by imposing properties of consistency and completeness. This definition more closely resembles Darwiche’s arithmetic circuits which represent the network polynomial of a Bayesian network [4], and also allow inference in the size of the circuit.\nLater, Gens and Domingos [7] re-stated spns as complex mixture distributions as follows.\n• Any univariate distribution is an spn.\n• Any weighted sum of spns with the same scope and nonnegative weights is an spn.\n• Any product of spns with disjoint scopes is an spn.\nThis alternative definition (called generalized spns by Peharz [9]) implies decomposability, a stricter requirement than consistency. Peharz et al. [12] showed that any consistent spn over discrete random variables can be transformed in an equivalent decomposable spn with a polynomial increase in size, and that weighted sums can be restricted to convex combinations without loss of expressivity. Hence, we assume in the following that spns are normalized : the weights of a weighted sum add up to one. This implies that spns specify (normalized) distributions. A similar result was obtained by Zhao, Melibari and Poupart [16]. We note that the base of the inductive definition can also be extended to accommodate any class of tractable distributions (e.g., Chow-Liu trees) [14, 15]. For this note, however, it suffices to consider univariate distributions.\nAn spn is usually represented graphically as a weighted rooted graph where each internal node is associated with an operation + or ×, and leaves are associated with variables and distributions. The arcs from a sum node to its children are weighted according to the corresponding convex combinations. The remaining arcs have weight 1. Figure 1 shows an example of an spn with scope {X1,X2}. Unit weights are omitted in the figure. Note that by definition every node represents an spn (hence a distribution) on its own; we refer to nodes and their corresponding spns interchangeably.\nConsider an spn S(X) over a random vector X = (X1, . . . ,Xn). The value of S at a point x = (x1, . . . , xn) in its domain is denoted by S(x) and defined recursively as follows. The value of a leaf node is the value of its corresponding distribution at the point obtained by projecting x onto the scope of the node, or the value 1 if the variable in the leaf is not in the scope of X. The value of a product node is the product of the values of its children at x. Finally, the value of a sum node is the weighted average of\nits children’s values at x. For example, the value of the spn S(X1,X2) in Figure 1 at the point (1, 0) is S(1, 0) = 0.2 · 0.3 · 0.4 + 0.5 · 0.4 · 0.8 + 0.3 · 0.8 · 0.9 = 0.4. Note that since we assumed spns to be normalized, we have that ∑\nx S(x) = 1. Let E ⊆ {1, . . . , n} and consider a random vector XE with scope {Xi : i ∈ E}, and an assignment e = {Xi = ei : i ∈ E}. We write x ∼ e to denote a value of X consistent with e (i.e., the projection of x on E is e). Given an spn S(X) representing a distribution P(X), we denote the marginal probability P(e) = ∑\nx∼e S(x) by S(e). This value can be computed by first marginalizing the variables {Xj : j 6∈ E} from every (distribution in a) leaf and then propagating values as before. Thus marginal probabilities can be computed in time linear in the network size (considering univariate distributions are represented as tables). The marginal probability P(X2 = 0) = 0.7 induced by the spn in Figure 1 can be obtained by first marginalizing leaves without {X2} (thus producing values 1 at respective leaves), and then propagating values as before.\nIn this work, we are interested in the following computational problem with spns:\nDefinition 1 (Functional map inference problem). Given an spn S specified with rational weights and an assignment e, compute maxx∼e S(x).\nA more general version of the problem would be to allow some of the variables to be summed out, while others are maximized. However, the marginalization (i.e., summing out) of variables can performed in polynomial time as a preprocessing step, the result of which is a map problem as stated above. We stick with the above definition for simplicity (bearing in mind that complexity is not changed).\nTo prove np-completeness, we use the decision variant of the problem:\nDefinition 2 (Decision map inference problem). Given an spn S specified with rational weights, an assignment e and a rational γ, decide if maxx∼e S(x) ≥ γ.\nWe denote both problems by map, as the distinction to which particular (functional or decision) version we refer should be clear from context. Clearly, np-completeness of the decision version establishes np-hardness of the functional version.\nThe support of an spn S is the set of configurations of its domain with positive values: supp(S) = {x : S(x) > 0}. An spn is selective if for every sub-spn T corresponding to\na sum node in S it follows that the supports of any two children are disjoint. Peharz et al. [10] recently showed that map is tractable in selective spns. Here, we discuss the complexity of (approximately) solving map in general spns. We assume that instances of the MAP problem are represented as bitstrings using a reasonable encoding; for instance, weights and probabilities are rational values represented by two integers in binary notation, and graphs are represented by (a binary encoding of their) adjacency lists."
    }, {
      "heading" : "3 Complexity Results",
      "text" : "As we show in this section, there is a strong connection between the height of a spn and the complexity of map inferences. First, note that an spn of height 0 is just a marginal distribution. So consider an spn of height 1. If the root is a sum node, then the network encodes a sum of univariate distributions (over the same variable), and map can be solved trivially by enumerating all values of that variable. If on the other hand the root is a product node, then the network encodes a distribution of fully independent variables. Also in this case, we can solve map easily by optimizing independently for each variable. So map in networks of height 1 is solvable in polynomial time.\nLet us now consider spns of height 2. As already discussed in the introduction, Peharz et al. [10] briefly observed that the map problem is np-hard even for tree-shaped networks of height 2. Here, we give the following alternative, direct proof of np-hardness of map in spns, that allows us to obtain results on non-approximability.\nTheorem 1. map in sum-product networks is np-complete even if there is no evidence, and the underlying graph is a tree of height 2.\nProof. Membership is straightforward as we can evaluate the probability of an assignment in polynomial time.\nWe show hardness by reduction from the np-hard problem maximum independent set (see e.g. [17]): Given an undirected graph G = (V,E) with vertices {1, . . . , n} and an integer v, decide whether there is an independent set of size v. An independent set is a subset V ′ ⊆ V such that no two vertices are connected by an edge in E.\nLet Ni denote the neighbors of i in V . For each i ∈ V , build a product node Si whose children are leaf nodes Si1, . . . , Sin with scopes X1, . . . ,Xn, respectively. If j ∈ Ni then associate Sij with distribution P(Xi = 1) = 0; if j /∈ Ni ∪ {i} associate Sij with P(Xj = 1) = 1/2; finally, associate Sii with distribution P(Xi = 1) = 1. See Figure 2 for an example. Let ni = |Ni| be the number of neighbors of i. Then Si(x) = 1/2\nn−ni−1 if xi = 1 and xj = 0 for all j ∈ Ni; and Si(x) = 0 otherwise. That is, Si(x) > 0 if there is a set V ′ which contains i and does not contain any of its neighbors. Now connect all product nodes Si with a root sum node parent S; specify the weight from S to Si as wi = 2 n−ni−1/c, where c = ∑ i 2 n−ni−1. Suppose there is an independent set I of size v. Take x such that xi = 1 if i ∈ I and xi = 0 otherwise. Then S(x) = v/c. For any configuration x of the variables, let I(x) = {i : Si(x) > 0}. Then I(x) is an independent set of size c·S(x). So suppose that there is no independent set of size v. Then maxx S(x) < v/c. Thus, there is an independent set if and only if maxx S(x) ≥ v/c.\nConsider a real-valued function f(S, e) of the input, that is a real-valued function f of (the encodings of) network S and evidence e. An algorithm for map in spns is a f(S, e)approximation if it runs in time polynomial in the size of its input spn S (which specifies the graph, the weights, the distributions, the evidence) and outputs a value γ such that γ ·f(S, e) ≥ maxx∼e S(x). That is, a f(S, e)-approximation algorithm provides, for every instance (S, e) of the problem, a solution whose value is at most a factor f(S, e) from the optimum value. The value f(S, e) is called the approximation factor. We have the following consequence of Theorem 1:\nCorollary 1. Unless p equals np, there is no (m − 1)ε-approximation algorithm for map in spns for any 0 ≤ ε < 1, where m is the number of internal nodes of the spn, even if there is no evidence and the underlying graph is a tree of height 2.\nProof. The proof of Theorem 1 encodes a maximum independent set problem and the reduction is a weighted reduction (see Definition 1 in [2]), which is sufficient to prove the result. To dispense with weighted reductions, we now give a direct proof. So suppose that there is a (m− 1)ε-approximation algorithm for map with 0 ≤ ε < 1. Let γ be the result of such algorithm when applied to the spn S created in the proof of Theorem 1 for a graph G given as input of the maximum independent set problem. We have that\nγ · c · (m− 1)ε ≥ c ·max x S(x) = max I∈I(G) |I| ,\nwhere c = ∑ i 2 n−ni−1, I(G) is the collection of independent sets of G, n is the number of vertices in G and ni is the number of neighbors of vertex i in G. Consequently, γ · c is a polynomial-time nε-approximation for maximum independent set (note that n = m−1 by construction). We know that there is no nε-approximation for maximum independent set with 0 ≤ ε < 1 unless p equals np [17], so the result follows.\nCorollary 1 shows that there is probably no approximation algorithm for map with sublinear approximation factor in the size of the input. The following result shows that this lower bound is tight:\nTheorem 2. There exists a (m− 1)-approximation algorithm for map in sum-product networks whose underlying graph has height at most 2, where m is the number of internal nodes.\nProof. Consider a sum-product network of height 2. If the root is a product node then the problem decomposes into independent map problems in spns of height 1; each of those problems can be solved exactly. So assume that the root S is a sum node connected to either leaf nodes or to nodes which are connected to leaf nodes. Solve the respective map problem for each child Si independently (which is exact, as the corresponding spn has height at most 1); denote by vi the corresponding value. Note that vi is an upper bound on the value of Si at the (global) map configuration. Let w1, . . . , wm−1 denote the weights from the root to children S1, . . . , Sm−1. Return v = maxi viwi. It follows that (m− 1)v ≥ maxx∼e S(x). Note that this is the same value returned by the max-product algorithm by Poon and Domingos [13].\nThus for networks of height 2, we have a clear divide: there is an approximation algorithm with linear approximation factor in the number of internal nodes, and no approximation algorithm with sublinear approximation factor in the input size. Allowing an additional level of nodes reduces drastically the quality of the approximations in the worst case:\nTheorem 3. Unless p equals np, there is no 2s ε -approximation algorithm for map in spns for any 0 ≤ ε < 1, where s is the size of the input, even if there is no evidence and the underlying graph is a tree of height 3.\nProof. First, we show how to build an spn for deciding satisfiability: Given a Boolean formula φ with in conjunctive normal form, decide if there is a satisfying truth-value assignment. We assume that each clause contains exactly 3 distinct variables (np-completeness is not altered by this assumption, but if one would like to drop it, then the weights of the sum node we define below could be easily adjusted to account for clauses with less than 3 variables).\nLet X1, . . . ,Xn denote the Boolean variables and φ1, . . . , φm denote the clauses in the formula. For i = 1, . . . ,m, consider the conjunctions φi1, . . . , φi7 over the variables of clause φi, representing all the satisfying assignments of that clause. For each such assignment, introduce a product node Sij encoding the respective assignment: there is a leaf node with scope Xk whose distribution assigns all mass to value 1 (resp., 0) if and only if Xk appears nonnegated (resp., negated) in φij ; and there is a leaf node with uniform distribution over Xk if and only if Xk does not appear on φij . See Figure 3 for an example. For a fixed configuration of the random variables, the clause φi is true if and only if one of the product nodes Si1, . . . , Si7 evaluates to 1. And since these products encode disjoint assignments, at most one such product is nonzero for each configuration. We thus have that ∑\nij Sij(x) = m/2 n−3 if φ(x) is true, and\n∑\nij Sij(x) < m/2 n−3 if φ(x)\nis false. So introduce a sum node S with all product nodes as children and with uniform weights 1/7m. There is a satisfying assignment for φ if and only if maxx S(x) ≥ 2\n3−n/7. Now take the spn above and make q copies of it with disjoint scopes: each copy contains different random variables Xtk, t = 1, . . . , q, at the leaves, but otherwise represents the very same distribution/satisfiability problem. Name each copy St, t = 1, . . . q, and let its size be st. Denote by s\n′ = maxt st (note that, since they are copies, their size is the same, apart from possible indexing, etc). Connect these copies using a product node S with networks S1, . . . , Sq as children, so that S(x) = ∏q t=1 St(x). Note that maxx St(x) ≥ 2 3−n/7 if there is a satisfying assignment to the Boolean formula, and maxx St(x) ≤ m−1 m · 2\n3−n/7 if there is no satisfying assignment. Hence, maxx S(x) ≥ (23−n/7)q if there is a satisfying assignment and maxx S(x) ≤ ((m − 1)2\n3−n/(7m))q if there is no satisfying assignment. Specify\nq = 1 + ⌊ (ln(2) ·m · (s′ + 2)ε) 1 1−ε ⌋ ,\nwhich is polynomial in s′, so the spn S can be constructed in polynomial time and space and has size s < q(s′ + 2). From the definition of q, we have that\nq > ( ln(2) ·m · (s′ + 2)ε ) 1 1−ε .\nRaising both sides to 1− ε, we get\nq > qε ln(2) ·m · (s+ 2)ε = m ln(2(q(s ′+2))ε) > m ln 2s ε .\nSince 1m ≤ ln m m−1 for any integer m > 1, it follows that\nq > ln 2s\nε\nln mm−1 =⇒ q ln\n(\nm\nm− 1\n)\n> ln 2s ε =⇒\n(\nm\nm− 1\n)q\n> 2s ε =⇒\n(\nm− 1\nm\n)q\n< 1\n2sε ,\nwhence\n2s ε\n(\n23−n(m− 1)\n7m\n)q\n<\n(\n23−n\n7\n)q\n.\nHence, if we can obtain an 2s ε -approximation for maxx S(x), then we can decide satisfiability: there is a satisfying assignment to the Boolean formula if and only if the approximation returns a value strictly greater than (23−n(m− 1)/(7m))q .\nAccording to Theorem 3, there is no 2f(s)-approximation (unless p equals np) for any sublinear function f of the input size s. The following result is used to show that this lower bound is tight.\nTheorem 4. Let S+ denote the sum nodes in spn S, and di be the number of children of sum node Si ∈ S +. Then there exists a ( ∏\nSi∈S+ di)-approximation algorithm for map\nwith input S and evidence e.\nProof. There are two cases to consider, based on the value of S(e), which can be checked in polynomial time. If S(e) = 0, then we can return any assignment consistent with e, as the result will be exact (and equal to zero). If S(e) > 0, then take the max-product algorithm by Poon and Domingos [13], which replaces sums by maximizations in the evaluation of an spn. Let pd(S) represent the output of that algorithm for an spn S and evidence e, while map(S) = maxx∼e S(x) (we omit the dependence of pd and map on e, for simplicity). Denote by S1, . . . , St the children of (the root node of) S. If S is a leaf then pd(S) = map(S). Otherwise, it follows that\npd(S) = max j=1,...,t wj · pd(Sj) , if S is a sum node, and\npd(S) = t ∏\nj=1\npd(Sj) , if S is a product node.\nWe prove by induction in the height of the spn that the max-product algorithm is a ( ∏\nSi∈S+ di)-approximation, that is, that\npd(S) ≥\n\n\n∏\nSi∈S+\n1\ndi\n\nmap(S) .\nTo show the base of the induction, take a single node network S. Then pd(S) = map(S) trivially. So take a network S with children S1, . . . , St, and suppose (by inductive hypothesis) that pd(Sj) ≥ ( ∏\nSi∈S +\nj\n1 di )map(Sj) for every child Sj. If S is a product node,\nthen\npd(S) = t ∏\nj=1\npd(Sj) ≥ t ∏\nj=1\n\n \n∏\nSi∈S +\nj\n1\ndi\n\n  map(Sj) =\n\n\n∏\nSi∈S+\n1\ndi\n\n\nt ∏\nj=1\nmap(Sj)\n=\n\n\n∏\nSi∈S+\n1\ndi\n\nmap(S) ,\nwhere the last two equalities follow as the scopes of products are disjoints, which implies\nthat the children do not share any node. If S is a sum node, then\npd(S) = max j=1,...,t wj · pd(Sj) ≥ max j=1,...,t\n\n \n∏\nSi∈S +\nj\n1\ndi\n\n  wj ·map(Sj)\n= max j=1,...,t\n(\nt\nt ∏\nSi∈S + j di\n)\nwj ·map(Sj)\nwhich, since 1/(t · ∏\nSi∈S + j di) ≥ 1/(t ·\n∏\nSi∈S+,Si 6=S di) = 1/\n∏\nSi∈S+ di,\n≥ max j=1,...,t\nt ∏\nSi∈S+ di\n· wj ·map(Sj)\n= t ·maxj=1,...,twj ·map(Sj) ∏\nSi∈S+ di\n≥\n\n\n∏\nSi∈S+\n1\ndi\n\nmap(S) .\nThe last inequality follows as maxj wj ·map(Sj) is an upper bound on the value of any child of S. This concludes the proof.\nWe have the following immediate consequence, showing the tightness of Theorem 3.\nCorollary 2. There exists a 2ε·s-approximation algorithm for map for some 0 < ε < 1, where s is the size of the spn.\nProof. Assume the network has at least one sum node (otherwise we can find an exact solution in polynomial time). Given the result of Theorem 4, we only need to show that there is ε < 1 such that ∏\nSi∈S+ di < 2 ε·s, with S+ the sum nodes in spn S and di be the number of children of sum node Si ∈ S\n+. Because s is strictly greater than the number of nodes and arcs in the network (as we must somehow encode the graph of S), we know that s > ∑\nSi∈S+ di. One can show that 3\nx/3 > x for any positive integer. Hence,\n∏\nSi∈S+\ndi ≤ ∏\nSi∈S+\n3di/3 = ∏\nSi∈S+\n2di log2(3)/3 = 2 log2(3)/3· ∑ Si∈S + di < 2s log2(3)/3 < 2ε·s ,\nfor some ε < 0.5284.\nThe previous result shows that the max-product algorithm by Poon and Domingos [13] achieves tight upper bounds on the approximation factor. This however does not rule the existence of approximation algorithms that achieve the same (worst-case) upper bound but perform significantly better on average. For instance, consider the following algorithm that takes an spn S and evidence e, and returns amap(S) as follows, where\namap is short for argmax-product algorithm. If S is a sum node with children S1, . . . , St, then compute\namap(S) = max k=1,...,t\nt ∑\nj=1\nwj · Sj(x k) ,\nwhere xk is such that Sk(x k) = amap(Sk), that is, x k is the solution of the map problem obtained by argmax-product for network Sk (argmax-product is run bottom-up). If S is a product node with children S1, . . . , St, then compute\namap(S) =\nt ∏\nj=1\namap(Sj) .\nElse, S is a leaf, so return maxx∼e S(x). The argmax-product has a worst-case time complexity quadratic in the size of the network; for comparison the max-product takes linear time. That is because the evaluation of the children of a sum node with the argument which maximizes each of the children takes linear time. While this is a drawback of this algorithm, quadratic complexity is still very efficient. Moreover, argmax-product always produces an approximation at least as good as that of max-product, and possibly exponentially better:\nTheorem 5. For any spn S and evidence e, we have that amap(S) ≥ pd(S). Moreover, there exists S and e such that amap(S) > 2mpd(S), where m is the number of sum nodes in S and pd(S) is the result returned by the max-product algorithm.\nProof. It is easy to see that amap(S) ≥ pd(S), because the configuration that is selected by pd(S) in a sum node is one of the configurations that are tried by the maximization of amap(S) for that same sum node (both algorithms perform the same operation over leaves and product nodes). To see that this improvement can be exponentially better, consider the spn Si in Figure 4. It is not difficult to see that pd(Si) = 5/16 while\namap(Si) = 3 · 11/48 = 11/16 > 2 · 5/16 .\nNow, create an spn S with a product root node connected to children S1, . . . , Sm as described (note that the scope of S is X1, . . . ,Xm). Then,\namap(S) = (11/16)m > 2m(5/16)m = 2mpd(S) ,\nso the result follows.\nAs an immediate result, the solutions produced by argmax-product achieve the upper bound on the complexity of approximating map. We hope that this simple result motivates researchers to seek for more sophisticated algorithms that exhibit the time performance of max-product while achieving the accuracy of argmax-product."
    }, {
      "heading" : "4 Conclusion",
      "text" : "We analyzed the complexity of maximum a posteriori inference in sum-product networks and showed that it relates with the height of the underlying graph. We first provided an alternative (and more direct) proof of np-hardness of maximum a posteriori inference in sum-product networks. Our proof uses a reduction from maximum independent set in undirected graphs, from which we obtain the non-approximability for any sublinear factor in the size of input, even in networks of height 2. We then showed that this limit is tight, that is, there is polynomial-time algorithm that produces solutions which are at most a linear factor for networks of height 2. We also showed that in networks of height 3 or more, complexity of approximation increases considerably: there is no approximation within a factor 2f(n), for any sublinear function f of the input size n. This is also tight lower bound, as we showed that the usual max-product algorithm by Poon and Domingos [13] finds an approximation within factor 2c·n for some constant c < 1. Last, we showed that a simple modification to max-product results in an algorithm that is at least as good, and possibly greatly superior to max-product. We hope that this result will foster research on approximation algorithms for maximum a posteriori inference in sum-product networks."
    }, {
      "heading" : "Acknowledgements",
      "text" : "The first author received financial support from the São Paulo Research Foundation (FAPESP) grant # 2016/01055-1 and CNPq grant PQ #303920/2016-5."
    } ],
    "references" : [ {
      "title" : "Sum product networks for activity recognition",
      "author" : [ "M.R. Amer", "S. Todorovic" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2016
    }, {
      "title" : "The complexity of weighted and unweighted #CSP",
      "author" : [ "A. Bulatov", "M. Dyer", "L.A. Goldberg", "M. Jalsenius", "M. Jerrum", "D. Richerby" ],
      "venue" : "Journal of Computer and System Sciences,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2012
    }, {
      "title" : "Language modeling with sum-product networks",
      "author" : [ "W.-C. Cheng", "S. Kok", "H.V. Pham", "H.L. Chieu", "K.M.A. Chai" ],
      "venue" : "In Proceedings of the 15th Annual Conference of the International Speech Communication Association,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2014
    }, {
      "title" : "A differential approach to inference in bayesian networks",
      "author" : [ "A. Darwiche" ],
      "venue" : "Journal of the ACM,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2003
    }, {
      "title" : "Modeling and Reasoning with Bayesian Networks",
      "author" : [ "A. Darwiche" ],
      "venue" : null,
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2009
    }, {
      "title" : "New complexity results for MAP in bayesian networks",
      "author" : [ "C.P. de Campos" ],
      "venue" : "In Proceedings of the Twenty-Second International Joint Conference on Artificial Intelligence,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2011
    }, {
      "title" : "Learning the structure of sum-product networks",
      "author" : [ "R. Gens", "P. Domings" ],
      "venue" : "In Proceedings of 30th International Conferecen on Machine Learning,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2013
    }, {
      "title" : "Learning tractable probabilistic models for fault localization",
      "author" : [ "A. Nath", "P. Domingos" ],
      "venue" : "In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2016
    }, {
      "title" : "Foundations of sum-product networks for probabilistic modeling",
      "author" : [ "R. Peharz" ],
      "venue" : "PhD thesis,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2015
    }, {
      "title" : "On the latent variable interpretation in sum-product networks",
      "author" : [ "R. Peharz", "R. Gens", "F. Pernkopf", "P. Domingos" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2016
    }, {
      "title" : "Modeling speech with sumproduct networks: Application to bandwidth extension",
      "author" : [ "R. Peharz", "G. Kapeller", "P. Mowlaee", "F. Pernkopf" ],
      "venue" : "In IEEE International Conference on Acoustics, Speech and Signal Processing,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2014
    }, {
      "title" : "On theoretical properties of sum-product networks",
      "author" : [ "R. Peharz", "S. Tschiatschek", "F. Pernkopf", "P. Domingos" ],
      "venue" : "In Proceedings of the 18th International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2015
    }, {
      "title" : "Sum-product networks: A new deep architecture",
      "author" : [ "H. Poon", "P. Domingos" ],
      "venue" : "In Proceedings of 27th Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2011
    }, {
      "title" : "Learning sum-product networks with direct and indirect variable interactions",
      "author" : [ "A. Rooshenas", "D. Lowd" ],
      "venue" : "In Proceedings of the 31st International Conference on Machine Learning,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2014
    }, {
      "title" : "Simplifying, regularizing and strengthening sum-product network structure learning",
      "author" : [ "A. Vergari", "N.D. Mauro", "F. Esposito" ],
      "venue" : "In Proocedings of the European Converence on Machine Learning and Knowledge Discovery in Databases,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2015
    }, {
      "title" : "On the relationship between sum-product networks and bayesian networks",
      "author" : [ "H. Zhao", "M. Melibari", "P. Poupart" ],
      "venue" : "In Proceedings of the 32nd International Conference on Machine Learning,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2015
    }, {
      "title" : "Linear degree extractors and the inapproximability of max clique and chromatic number",
      "author" : [ "D. Zuckerman" ],
      "venue" : "Theory of Computing,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2007
    } ],
    "referenceMentions" : [ {
      "referenceID" : 12,
      "context" : "Sum-Product Networks (spns) are a relatively new class of graphical models that allow marginal inference in linear time in their size [13].",
      "startOffset" : 134,
      "endOffset" : 138
    }, {
      "referenceID" : 4,
      "context" : "This is therefore in sharp difference with other graphical models such as Bayesian networks and Markov Random Fields that require #p-hard effort to produce marginal inference [5].",
      "startOffset" : 175,
      "endOffset" : 178
    }, {
      "referenceID" : 3,
      "context" : "an arithmetic circuit whose evaluation produces a marginal inference [4].",
      "startOffset" : 69,
      "endOffset" : 72
    }, {
      "referenceID" : 12,
      "context" : "spns have received increasing popularity in applications of machine learning due to their ability to represent complex and highly multidimensional distributions [13, 11, 3, 8, 1].",
      "startOffset" : 161,
      "endOffset" : 178
    }, {
      "referenceID" : 10,
      "context" : "spns have received increasing popularity in applications of machine learning due to their ability to represent complex and highly multidimensional distributions [13, 11, 3, 8, 1].",
      "startOffset" : 161,
      "endOffset" : 178
    }, {
      "referenceID" : 2,
      "context" : "spns have received increasing popularity in applications of machine learning due to their ability to represent complex and highly multidimensional distributions [13, 11, 3, 8, 1].",
      "startOffset" : 161,
      "endOffset" : 178
    }, {
      "referenceID" : 7,
      "context" : "spns have received increasing popularity in applications of machine learning due to their ability to represent complex and highly multidimensional distributions [13, 11, 3, 8, 1].",
      "startOffset" : 161,
      "endOffset" : 178
    }, {
      "referenceID" : 0,
      "context" : "spns have received increasing popularity in applications of machine learning due to their ability to represent complex and highly multidimensional distributions [13, 11, 3, 8, 1].",
      "startOffset" : 161,
      "endOffset" : 178
    }, {
      "referenceID" : 9,
      "context" : "[10] noted that np-hardness can be proved by transforming a Bayesian network with a Naive Bayes structure into a distribution-equivalent spn of height two (this is done by adding a sum node to represent the latent root variable and its marginal distribution and product nodes as children to represent the conditional distributions).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 5,
      "context" : "As map inference in the former is np-hard [6], the result follows.",
      "startOffset" : 42,
      "endOffset" : 45
    }, {
      "referenceID" : 12,
      "context" : "This a tight bound, as we show that the usual max-product algorithm by Poon and Domingos [13], which replaces sums with maximizations, finds an approximation within a factor 2c·n for some constant c < 1.",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 12,
      "context" : "Poon and Domingos [13] originally defined spns as multilinear functions of indicator variables that allow for space and time efficient representation and inference.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 3,
      "context" : "This definition more closely resembles Darwiche’s arithmetic circuits which represent the network polynomial of a Bayesian network [4], and also allow inference in the size of the circuit.",
      "startOffset" : 131,
      "endOffset" : 134
    }, {
      "referenceID" : 6,
      "context" : "Later, Gens and Domingos [7] re-stated spns as complex mixture distributions as follows.",
      "startOffset" : 25,
      "endOffset" : 28
    }, {
      "referenceID" : 8,
      "context" : "This alternative definition (called generalized spns by Peharz [9]) implies decomposability, a stricter requirement than consistency.",
      "startOffset" : 63,
      "endOffset" : 66
    }, {
      "referenceID" : 11,
      "context" : "[12] showed that any consistent spn over discrete random variables can be transformed in an equivalent decomposable spn with a polynomial increase in size, and that weighted sums can be restricted to convex combinations without loss of expressivity.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 15,
      "context" : "A similar result was obtained by Zhao, Melibari and Poupart [16].",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 13,
      "context" : ", Chow-Liu trees) [14, 15].",
      "startOffset" : 18,
      "endOffset" : 26
    }, {
      "referenceID" : 14,
      "context" : ", Chow-Liu trees) [14, 15].",
      "startOffset" : 18,
      "endOffset" : 26
    }, {
      "referenceID" : 9,
      "context" : "[10] recently showed that map is tractable in selective spns.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 9,
      "context" : "[10] briefly observed that the map problem is np-hard even for tree-shaped networks of height 2.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 16,
      "context" : "[17]): Given an undirected graph G = (V,E) with vertices {1, .",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 1,
      "context" : "The proof of Theorem 1 encodes a maximum independent set problem and the reduction is a weighted reduction (see Definition 1 in [2]), which is sufficient to prove the result.",
      "startOffset" : 128,
      "endOffset" : 131
    }, {
      "referenceID" : 16,
      "context" : "We know that there is no nε-approximation for maximum independent set with 0 ≤ ε < 1 unless p equals np [17], so the result follows.",
      "startOffset" : 104,
      "endOffset" : 108
    }, {
      "referenceID" : 12,
      "context" : "Note that this is the same value returned by the max-product algorithm by Poon and Domingos [13].",
      "startOffset" : 92,
      "endOffset" : 96
    }, {
      "referenceID" : 12,
      "context" : "If S(e) > 0, then take the max-product algorithm by Poon and Domingos [13], which replaces sums by maximizations in the evaluation of an spn.",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 12,
      "context" : "The previous result shows that the max-product algorithm by Poon and Domingos [13] achieves tight upper bounds on the approximation factor.",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 0,
      "context" : "Xi [0, 1] Xi [1, 0] Xi [1, 0] Xi [1, 0] 5/16 11/48 11/48 11/48",
      "startOffset" : 3,
      "endOffset" : 9
    }, {
      "referenceID" : 0,
      "context" : "Xi [0, 1] Xi [1, 0] Xi [1, 0] Xi [1, 0] 5/16 11/48 11/48 11/48",
      "startOffset" : 13,
      "endOffset" : 19
    }, {
      "referenceID" : 0,
      "context" : "Xi [0, 1] Xi [1, 0] Xi [1, 0] Xi [1, 0] 5/16 11/48 11/48 11/48",
      "startOffset" : 23,
      "endOffset" : 29
    }, {
      "referenceID" : 0,
      "context" : "Xi [0, 1] Xi [1, 0] Xi [1, 0] Xi [1, 0] 5/16 11/48 11/48 11/48",
      "startOffset" : 33,
      "endOffset" : 39
    }, {
      "referenceID" : 12,
      "context" : "This is also tight lower bound, as we showed that the usual max-product algorithm by Poon and Domingos [13] finds an approximation within factor 2c·n for some constant c < 1.",
      "startOffset" : 103,
      "endOffset" : 107
    } ],
    "year" : 2017,
    "abstractText" : "We discuss the computational complexity of approximating maximum a posteriori inference in sum-product networks. We first show np-hardness in three-level tree networks by a reduction from maximum independent set; this implies non-approximability within a sublinear factor. We show that this is a tight bound, as we can find an approximation within a linear factor in any three-level networks. We then show that in four-level tree networks it is np-hard to approximate the problem within a factor 2f(n) for any sublinear function f of the size of the input n. Again, this is bound is tight, as we prove that the usual max-product algorithm finds (in any network) approximations within factor 2cn from some constant c < 1. Last, we present a simple algorithm, and show that provably produces solutions at least as good as, and potentially much better, than the max-product algorithm.",
    "creator" : "LaTeX with hyperref package"
  }
}