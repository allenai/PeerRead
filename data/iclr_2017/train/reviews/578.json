{"conference": "ICLR 2017 conference submission", "title": "Improving Invariance and Equivariance Properties of Convolutional Neural Networks", "abstract": "Convolutional Neural Networks (CNNs) learn highly discriminative representations from data, but how robust and structured are these representations? How does the data shape the internal network representation? We shed light on these questions by empirically measuring the invariance and equivariance properties of a large number of CNNs trained with various types of input transformations. We find that CNNs learn invariance wrt all 9 tested transformation types and that invariance extends to transformations outside the training range. We also measure the distance between CNN representations and show that similar input transformations lead to more similar internal representations. Transforms can be grouped by the way they affect the learned representation. Additionally, we also propose a loss function that aims to improve CNN equivariance.", "reviews": [{"is_meta_review": true, "comments": "This paper is an extension of Lenc&Vedaldi15 paper, showing CNN representations at FC7 layer are to certain extent equivariant to various classes of transformations and that training with a certain group of transformation makes the representations more equivalent.\n\nAuthors performed a large amount of experiments, training over 30 networks with different forms of jitter, which is quite impressive. However it is rather difficult to find a main message of this work. Yes, authors measured the properties on a different layer than the Lenc&Vedaldi15, however it is hard to find some novel insights other than the known fact that jitter helps to achieve invariance. The evaluation seems to be mostly correct, however the paper does not seem to be solving the task advertised in its title really well.\n\nMajor issues are in the experiments with the representation distances:\n* The selection of only FC7 is a bit controversial - it is followed only by a single classification layer (FC8) to the common output - class likelyhoods. Because the FC8 is just a linear projections, what the equivalence map does is just to re-project the FC8 weights of the attached network to the weights of the original network. Probably performing similar experiments but on more layers may be more useful (as the networks are already trained).\n* The experiment with representation distance is missing what is the classification error on the testing dataset. This would answer whether the representations are actually compatible up to linear transformation at all...\n* It is not clear for the experiment with K-NN whether this is measured per each test set example? After training the equivalence map? More clear would be to show that networks trained on similar group of jitter transformations are more compatible on the target task.\n* The proposed method does not seem to improve equivariance consistently on all tasks. Especially with \\lambda_1 and \\lambda_2 having such small values, the loss is basically equal to simple data jitter as it just adds up the loss of the original and transformed image. Maybe the issue is in the selection of the FC7 layer?\n\nIn general, this paper shows some interesting results on the FC7 equivariance, but it does not seem to be drawing many interesting new observations out of these experiments. Due to some issues with the equivalence experiments and the finetuning of equivariance, I would not recommend acceptance of this manuscript. However, refining the experiments on already trained networks and restructuring this manuscript into more investigative work may lead to interesting contribution to the field.\n\nThere are also few minor issues:\n* It is not experimentally verified that the new criterion for equivariance mapping helps to gain better results.\n* The angles on page 1 and 5 are missing units (degrees?).\n* On page three, \"In practice, it is difficult... \", it is not M_g which is maximised/minimised, but the loss over the M_g\n* Page 4, footnote 2 - if you are just halving the activations, it is hard to call it a dropout as this constant factor can be passed to the following/preceding weights\n* Is the network for RVL-CDIP the same architecture as Alexnet?\n* On page 7, Figure 3a+3b - in my opinion, turning the diagonal elements to white is really misleading, and probably even incorrect, as the distance between the same representations should be zero (which is also a way how to verify that the experiments are performed correctly).", "IS_META_REVIEW": true}, {"TITLE": "ICLR committee final decision", "OTHER_KEYS": "ICLR 2017 pcs", "comments": "The authors have withdrawn the submission.", "IS_META_REVIEW": false, "DATE": "06 Feb 2017", "is_meta_review": false}, {"TITLE": "Response to Reviews", "OTHER_KEYS": "Christopher Tensmeyer", "comments": "We thank the reviewers for their helpful and candid feedback on our work.  We will withdraw the paper from submission at ICLR 2017 and perform major revisions to address reviewer feedback before submission to another venue.\n\nIn particular, we will remove the portion of the paper dealing with improving invariance/equivariance and focus more on the empirical study and conclusions that can be drawn from it.  We appreciate the suggestion that more meaningful conclusions could be drawn from a comparative study of the invariances of different layers or architectures.  A more meaningful contribution in characterizing the structure of CNN representations would be finding some relationship between the M_g's for a particular transform.  For example, if M_g1 is rotation by 5 degrees and M_g2 is rotation by 10 degrees, does M_g2(x) = M_g1(M_g1(x)).\n\nAgain, we thank you for your time and attention.", "IS_META_REVIEW": false, "DATE": "20 Dec 2016", "is_meta_review": false}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Not clear what we learn from the results", "is_meta_review": false, "comments": "This paper empirically studies the invariance, equivariance and equivalence properties of representations learned by convolutional networks under various kinds of data augmentation. Additional loss terms are presented which can make a representation more invariant or equivariant.\n\nThe idea of measuring invariance, equivariance and equivalence of representations is not new (Lenc & Vedaldi). The authors are the first to systematically study the effect of data augmentation on these properties, but it is unclear in what way the results are surprising, interesting, or useful. It is not really surprising that data augmentation increases invariance, or that training with the same augmentation leads to more similar representations than training with different augmentations.\n\nRegarding the presented method to increase invariance and equivariance: while it could be that a representation will generalize better if it is invariant or equivariant, it is not clear why one would want to increase in/equivariance if it does not indeed lead to improvements in performance. The paper presents no evidence that training for increased invariance / equivariance leads to substantial improvements in performance. Combined with the fact that the loss (eq. 6) would substantially increase the computational burden, I don\u2019t think this technique will be very useful.\n\nMinor comments:\n-R^{nxn} should be R^{n \\times n}\n-In eq. 2: \u2018equivaraince\u2019\n-In 3.3, argmax is not properly formatted\n-I think data augmentation was already considered essential before Krizhevsky et al. Not really correct to attribute this to them.\n- About the claim \u201cThis is related to the idea of whether CNNs collapse (invariance) or linearize (equivariance) view manifolds of 3D objects\u201d. The idea that equivariance means that the manifold (orbit) is linearized, is incorrect. A linear representation M_g can create nonlinear manifolds. A simple example is given by a rotation matrix in 2D (clearly linear), generating a nonlinear manifold (the circle). \n- Equivariance in eq. 2 should be called \u201cnon-equivariance\u201d. If the value is low, the representation is equivariant, while if it is high it is non-equivariant.\n- \u201cEq. 2  also uses the paradigm that\u201d, uses the word paradigm in a strange manner\n- In the definition of x\u2019_ij, should one of the g_j be inverted? Otherwise it seems like the transformation is applied twice, instead of being undone.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Review", "is_meta_review": false, "comments": "This work presents an empirical study of the influence of different types of data augmentation on the performance of CNNs. It also proposes to incorporate additional loss functions to encourage approximate invariance or equivariance, and shows there are some benefits.\n\nThe paper reads well and the objectives are clear. The study of invariances in CNNs is a very important topic, and advances in this area are greatly appreciated. The paper splits itself in two very different parts -- the empirical study of equivariances in existing CNNs, and the proposal of equivariance objectives. However, taken separately each of these two parts could be better executed.\n\nOn the empirical study, its breath is relatively limited, and it's hard to draw any far-reaching conclusions from it:\n- Only one network is studied; at least one other architecture would have made for better generalization.\n- Only one layer (fc7) is studied; this presents issues as the top layer is the most invariant. At least one convolutional layer (possibly more) should have been considered.\n- The reliance on the scanned text dataset does not help; however the ImageNet results are definitely very encouraging.\n\nIt is nice to see how performance degrades with the degree of transformations, and the authors do interpret the results, but it would be better to see more analysis. There is only a limited set of conclusions that can be drawn from evaluating networks with jittered data. If the authors could propose some other interesting ways to assess the invariance and equivariance, they would potentially draw more insightful conclusions from it.\n\nOn the proposed loss function, only a very quick treatment of it is given (Section 4, half a page). It does not differ too much from known invariance/equivariance objectives studied in the literature previously, e.g. Decoste and Scholkopf, \"Training Invariant Support Vector Machines\", Machine Learning, 2002.\n\nI'm not sure that dividing the paper into these two different contributions is the best approach; they both feel a bit incomplete, and a full treatment of only one of them would make for an overall better paper.\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "Few novel outcomes with too many issues", "is_meta_review": false, "comments": "This paper is an extension of Lenc&Vedaldi15 paper, showing CNN representations at FC7 layer are to certain extent equivariant to various classes of transformations and that training with a certain group of transformation makes the representations more equivalent.\n\nAuthors performed a large amount of experiments, training over 30 networks with different forms of jitter, which is quite impressive. However it is rather difficult to find a main message of this work. Yes, authors measured the properties on a different layer than the Lenc&Vedaldi15, however it is hard to find some novel insights other than the known fact that jitter helps to achieve invariance. The evaluation seems to be mostly correct, however the paper does not seem to be solving the task advertised in its title really well.\n\nMajor issues are in the experiments with the representation distances:\n* The selection of only FC7 is a bit controversial - it is followed only by a single classification layer (FC8) to the common output - class likelyhoods. Because the FC8 is just a linear projections, what the equivalence map does is just to re-project the FC8 weights of the attached network to the weights of the original network. Probably performing similar experiments but on more layers may be more useful (as the networks are already trained).\n* The experiment with representation distance is missing what is the classification error on the testing dataset. This would answer whether the representations are actually compatible up to linear transformation at all...\n* It is not clear for the experiment with K-NN whether this is measured per each test set example? After training the equivalence map? More clear would be to show that networks trained on similar group of jitter transformations are more compatible on the target task.\n* The proposed method does not seem to improve equivariance consistently on all tasks. Especially with \\lambda_1 and \\lambda_2 having such small values, the loss is basically equal to simple data jitter as it just adds up the loss of the original and transformed image. Maybe the issue is in the selection of the FC7 layer?\n\nIn general, this paper shows some interesting results on the FC7 equivariance, but it does not seem to be drawing many interesting new observations out of these experiments. Due to some issues with the equivalence experiments and the finetuning of equivariance, I would not recommend acceptance of this manuscript. However, refining the experiments on already trained networks and restructuring this manuscript into more investigative work may lead to interesting contribution to the field.\n\nThere are also few minor issues:\n* It is not experimentally verified that the new criterion for equivariance mapping helps to gain better results.\n* The angles on page 1 and 5 are missing units (degrees?).\n* On page three, \"In practice, it is difficult... \", it is not M_g which is maximised/minimised, but the loss over the M_g\n* Page 4, footnote 2 - if you are just halving the activations, it is hard to call it a dropout as this constant factor can be passed to the following/preceding weights\n* Is the network for RVL-CDIP the same architecture as Alexnet?\n* On page 7, Figure 3a+3b - in my opinion, turning the diagonal elements to white is really misleading, and probably even incorrect, as the distance between the same representations should be zero (which is also a way how to verify that the experiments are performed correctly).", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"TITLE": "Clarifications", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "", "IS_META_REVIEW": false, "DATE": "13 Dec 2016", "is_meta_review": false}, {"TITLE": "Pre-review Questions", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "", "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "is_meta_review": false}, {"TITLE": "Meaning of \"structured wrt\"", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "", "IS_META_REVIEW": false, "DATE": "27 Nov 2016", "is_meta_review": false}], "SCORE": 4, "authors": "Christopher Tensmeyer, Tony Martinez", "KEYWORDS": "Data augmentation shapes internal network representation and makes predictions robust to input transformations.", "accepted": false, "id": ""}
