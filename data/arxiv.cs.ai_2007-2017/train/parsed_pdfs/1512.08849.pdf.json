{
  "name" : "1512.08849.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Learning Natural Language Inference with LSTM",
    "authors" : [ "Shuohang Wang", "Jing Jiang" ],
    "emails" : [ "shwang.2014@phdis.smu.edu.sg", "jingjiang@smu.edu.sg" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Natural language inference (NLI) is the problem of determining whether from a sentence P one can infer another sentenceH (MacCartney, 2009). Here P is called the premise and H the hypothesis. NLI is a fundamentally important problem that has applications in many tasks including question answering, semantic search and automatic text summarization. There has been much interest in NLI in the past decade, especially sur-\nrounding the PASCAL Recognizing Textual Entailment (RTE) Challenge (Dagan et al., 2005). Existing solutions to NLI range from shallow approaches based on lexical similarities (Glickman et al., 2005) to advanced methods that consider syntax (Mehdad et al., 2009), perform explicit sentence alignment (MacCartney et al., 2008) or use formal logic (Clark and Harrison, 2009).\nRecently, Bowman et al. (2015) released the Stanford Natural Language Inference (SNLI) corpus for the purpose of encouraging more learningcentered approaches to NLI. This corpus contains around 570K sentence pairs with three labels: entailment, contradiction and neutral. The size of the corpus makes it now feasible to train deep neural network models, which typically require a large amount of training data. Bowman et al. (2015) tested a straightforward architecture of deep neural networks for NLI. In their architecture, the premise and the hypothesis are each represented by a sentence embedding vector. The two vectors are then fed into a multi-layer neural network to train a classifier. Bowman et al. (2015) achieved an accuracy of 77.6% on the SNLI corpus when long short-term memory (LSTM) networks were used to obtain the sentence embeddings.\nA more recent work by Rocktäschel et al. (2015) improved the performance by applying a neural attention model. While their basic architecture is still based on sentence embeddings for the premise and the hypothesis, a key difference is that the embedding of the premise takes into consideration the alignment between the premise and the hypothesis. This so-called attention-weighted representation of the premise was shown to push the accuracy to 83.5% on the SNLI corpus.\nA limitation of the aforementioned two models is that they reduce both the premise and the hypothesis to a single embedding vector before matching them, i.e., in the end, they use two embedding vectors to perform sentence-level match-\nar X\niv :1\n51 2.\n08 84\n9v 1\n[ cs\n.C L\n] 3\n0 D\nec 2\n01 5\ning. However, not all word or phrase-level matching results are equally important. For example, the matching between stop words in the two sentences is not likely to contribute much to the final prediction. Another example is that for a hypothesis to contradict a premise, a single word or phraselevel mismatch (e.g., a mismatch of the subjects of the two sentences) may be sufficient and other word or phrase-level matching results are less important, but this intuition is hard to be captured if we directly match two sentence embeddings.\nIn this paper, we propose a new LSTM-based architecture for learning natural language inference. Different from the models in (Bowman et al., 2015) and (Rocktäschel et al., 2015), our prediction is not based on whole sentence embeddings of the premise and the hypothesis. Instead, we use an LSTM to perform word-by-word matching of the hypothesis with the premise. Our LSTM sequentially processes the hypothesis, and at each position, it tries to match the current word in the hypothesis with an attention-weighted representation of the premise. Matching results that are critical for the final prediction will be “remembered” by the LSTM while less important matching results will be “forgotten.” We refer to this architecture for natural language inference a matchLSTM, or mLSTM for short.\nUsing the SNLI corpus, we show that our mLSTM model improves the state-of-the-art performance on this data set by achieving a classification accuracy of 86.1%. Through qualitative analyses, we also show that the mLSTM architecture can indeed pick up the more important word-level matching results that need to be remembered for the final prediction. In particular, we observe that “good” word-level matching results are generally forgotten but important mismatches, which often indicate a contradiction or a neutral relationship, tend to be remembered."
    }, {
      "heading" : "2 Model",
      "text" : "In this section, we present our mLSTM architecture for natural language inference."
    }, {
      "heading" : "2.1 Background",
      "text" : "We first present the model by Rocktäschel et al. (2015) because our model builds on top of theirs. Their model uses LSTMs to process the premise and the hypothesis separately, with a neural attention component to find a soft alignment between\nthe two sentences.\nLSTM Let us first briefly review LSTM. LSTM is a special form of recurrent neural networks (RNNs), which process sequence data. LSTM uses a few gate vectors at each position to control the passing of information along the sequence and thus improves the modeling of long-range dependencies. While there are different variations of LSTMs, here we present the one adopted in (Rocktäschel et al., 2015). Specifically, let us use X = (x1,x2, . . . ,xN ) to denote an input sequence, where xk ∈ Rl (1 ≤ k ≤ N ). At each position k, there is a set of internal vectors, including an input gate ik, a forget gate fk, an output gate ok and a memory cell ck. All these vectors will be used together to generate a hidden state hk. The following transition equations define the LSTM architecture:\nik = σ(W ixk +V ihk−1 + b i), fk = σ(W fxk +V fhk−1 + b f ),\nok = σ(W oxk +V ohk−1 + b o), ck = fk ck−1 + ik tanh(Wcxk +Vchk−1 + bc), hk = ok tanh(ck), (1)\nwhere σ is the sigmoid function, is the elementwise multiplication of two vectors, and all W ∈ Rd×l,V ∈ Rd×d and b ∈ Rd are weight matrices and weight vectors to be learned.\nNeural Attention Model For the natural language inference task, we have two sentences Xs = (xs1,x s 2, . . . ,x s M ) and X\nt = (xt1,x t 2, . . . ,x t N ), where X s is the premise and Xt is the hypothesis. Here each x is an embedding vector of the corresponding word and can be initialized using some pre-trained word embedding vectors. The goal is to predict a label y that indicates the relationship between Xs and Xt. In this paper, we assume y is one of entailment, contradiction and neutral.\nRocktäschel et al. (2015) first used two LSTMs to process the premise and the hypothesis, respectively, but initialized the second LSTM (for the hypothesis) with the last cell state of the first LSTM (for the premise). Let us use hsj (1 ≤ j ≤ M ) and htk (1 ≤ k ≤ N ) to denote the resulting hidden states corresponding to xsj and x t k, respectively. The main idea of the word-by-word attention model proposed by Rocktäschel et al. (2015)\nis to introduce a series of attention-weighted combinations of the hidden states of the premise, where each weighted version of the premise is for a particular word in the hypothesis. Let us use ak to denote such a vector for word xtk in the hypothesis. We call these vectors {ak}Nk=1 the attention vectors. Specifically, ak is defined as follows1:\nak = M∑ j=1 αkjh s j , (2)\nwhere αkj is an attention weight that encodes the degree to which xtk in the hypothesis is aligned with xsj in the premise. The attention weight αkj is generated in the following way:\nαkj = exp(ekj)∑ j′ exp(ekj′) , (3)\nwhere\nekj = w e · tanh(Wshsj +Wthtk +Wahak−1). (4)\nHere · is the dot-product between two vectors, the vector we ∈ Rd and all matrices W ∈ Rd×d contain weights to be learned, and hak−1 is another hidden state which we will explain below.\nThe attention-weighted premise ak essentially tries to model the relevant parts in the premise with respect to xtk, i.e., the k\nth word in the hypothesis. Rocktäschel et al. (2015) further built an RNN model over {ak}Nk=1 by defining the following hidden states:\nhak = ak + tanh(V ahak−1), (5)\nwhere Va ∈ Rd×d is a weight matrix to be learned. We can see that the last haN aggregates all the previous ak and can be seen as an attentionweighted representation of the whole premise. Rocktäschel et al. (2015) then used this haN , which represents the whole premise, together with htN , which is an aggregated representation of the whole hypothesis2, to predict the label y.\n1We present the word-by-word attention model by Rocktäschel et al. (2015) in a different way but the underlying model is the same. Our presentation is close to the one by Bahdanau et al. (2015), with our attention vectors a corresponding to the context vectors c in their paper.\n2Strictly speaking, in (Rocktäschel et al., 2015), htN encodes both the premise and the hypothesis because the two sentences are chained. But htN places a higher emphasis on the hypothesis given the nature of RNNs."
    }, {
      "heading" : "2.2 Our Model",
      "text" : "Although the neural attention model by Rocktäschel et al. (2015) achieved better results than Bowman et al. (2015), we see two limitations. First, the model still uses a single vector representation of the premise, namely haN , to match the entire hypothesis. We speculate that if we instead use each of the attention-weighted representations of the premise for matching, i.e. use ak at position k to match the hidden state htk of the hypothesis while we go through the hypothesis, we could achieve better matching quality. This can be done using an RNN which at each position takes in both ak and htk as its input and determines how well the overall matching of the two sentences is up to the current position. In the end the RNN will produce a single vector representing the matching of the two entire sentences.\nThe second limitation is that the model by Rocktäschel et al. (2015) does not explicitly allow us to place more emphasis on the more important matches between the premise and the hypothesis and down-weight the less critical matches. For example, matching of stop words is presumably less important than matching of content words. Also, some matching results may be particularly critical for making the final prediction and thus should be remembered. For example, consider the premise “A dog jumping for a Frisbee in the snow.” and the hypothesis “A cat washes his face and whiskers with his front paw.” When we sequentially process the hypothesis, once we see that the subject of the hypothesis cat does not match the subject of the premise dog, we have a high probability to believe that there is a contradiction. So this mismatch should be remembered.\nBased on the two observations above, we propose to use an LSTM to sequentially match the two sentences. At each position the LSTM takes in both ak and htk as its input. The LSTM is expected to remember the more important matches between the two sentences for predicting the final label y and forget the less important ones. Figure 1 gives an overview of our model in contrast to the model by Rocktäschel et al. (2015).\nSpecifically, our model works as follows. First, similar to Rocktäschel et al. (2015), we process the premise and the hypothesis using two LSTMs, but we do not feed the last cell state of the premise to the LSTM of the hypothesis. This is because\nwe do not need the LSTM for the hypothesis to encode any knowledge about the premise but we will match the premise with the hypothesis using the hidden states of the two LSTMs. Again, we use hsj and h t k to represent these hidden states.\nNext, we generate the attention vectors ak similarly to Eqn (2). However, Eqn (4) will be replaced by the following equation:\nekj = w e · tanh(Wshsj +Wthtk +Wmhmk−1).(6)\nThe only difference here is that we use a hidden state hm instead of ha, and the way we define hm is very different from the definition of ha. Our hmk is the hidden state at position k generated from our mLSTM. This LSTM models the matching between the premise and the hypothesis. Important matches will be “remembered” by the LSTM while non-essential ones will be “forgotten.” We use the concatenation of ak, which is the attention-weighted version of the premise for the kth word in the hypothesis, and htk, the hid-\nden state for the kth word itself, as input to the mLSTM.\nSpecifically, let us define\nmk = [ ak htk ] . (7)\nWe then build the mLSTM as follows:\nimk = σ(W mimk +V mihmk−1 + b mi), fmk = σ(W mfmk +V mfhmk−1 + b mf ), omk = σ(W momk +V mohmk−1 + b mo), cmk = f m k cmk−1 + imk tanh(Wmcmk +Vmchmk−1 +bmc), hmk = o m k tanh(cmk ). (8)\nWith this mLSTM, finally we use only hmN to predict the label y."
    }, {
      "heading" : "2.3 Implementation Details",
      "text" : "Besides the mLSTM architecture, which is the main difference of our model from the model by Rocktäschel et al. (2015), we also introduce a few other changes.\nFirst, we insert a special word NULL to the premise, and we allow words in the hypothesis to be aligned with this NULL. This is inspired by common practice in machine translation. Specifically, we introduce a vector hs0, which is fixed to be a vector of 0s of dimension d. This hs0 represents NULL and is used together with other hsj to derive the attention vectors {ak}Nk=1.\nSecond, we use word embeddings trained from GloVe (Pennington et al., 2014) instead of word2vec vectors. The main reason is that GloVe word embeddings cover more words in the SNLI corpus than word2vec3.\nThird, for words which do not have pre-trained word embeddings, we take the average of the embeddings of all the words (in GloVe) surrounding the unseen word within a window size of 9 (4 on the left and 4 on the right) as an approximation of the embedding of this unseen word. Then we do not update any word embedding when learning our model. Although this is a very crude approximation, it reduces the number of parameters we need to update, and as it turns out, we can still achieve better performance than Rocktäschel et al. (2015).\n3The SNLI corpus contains 37K unique tokens. Around 12.1K of them cannot be found in word2vec but only around 4.1K of them cannot be found in GloVe."
    }, {
      "heading" : "3 Experiments",
      "text" : "In this section, we present the evaluation of our model. We first perform quantitative evaluation, comparing our model with the model by Rocktäschel et al. (2015). We then conduct some qualitative analyses to understand how our mLSTM model works in matching the premise and the hypothesis."
    }, {
      "heading" : "3.1 Experiment Settings",
      "text" : "Data: We use the SNLI corpus to test the effectiveness of our model. The original data set contains 570,152 sentence pairs, each labeled with one of the following relationships: entailment, contradiction, neutral and –, where – indicates a lack of consensus from the human annotators. We discard the sentence pairs labeled with – and keep the remaining ones for our experiments. In the end, we have 549,367 pairs for training, 9,842 pairs for development and 9,824 pairs for testing. This follows the same data partition used by Bowman et al. (2015) in their experiments. We perform three-class classification and use accuracy as our evaluation metric. Parameters: We use the Adam method (Kingma and Ba, 2014) with hyperparameters β1 set to 0.9 and β2 set to 0.999 for optimization. The initial learning rate is set to be 0.001 with a decay ratio of 0.95 for each iteration. The batch size is set to be 30. We experiment with d = 150 and d = 300 where d is the dimension of all the hidden states of the LSTMs. Methods for comparison: We mainly want to compare our model with the word-by-word attention model by Rocktäschel et al. (2015) because this model achieved the state-of-the-art performance on the SNLI corpus. To ensure fair com-\nparison, besides comparing with the accuracy reported by Rocktäschel et al. (2015), we also reimplemented their word-by-word attention model ourselves and report the performance of our implementation. We also consider a few variations of our model. Specifically, the following models are implemented and tested in our experiments:\n• Word-by-word attention (d = 150): This is our implementation of the word-by-word attention model by Rocktäschel et al. (2015), where we set the dimension of the hidden states to 150. The differences between our implementation and the original implementation by Rocktäschel et al. (2015) are the following: (1) We also add a NULL token to the premise for matching. (2) We do not feed the last cell state of the LSTM for the premise to the LSTM for the hypothesis, to keep it consistent with the implementation of our model. (3) For word representation, we also use the GloVe word embeddings as in the implementation of our model, and we do not update the word embeddings. For unseen words, we adopt the same strategy as described in Section 2.3.\n• mLSTM (d = 150): This is our mLSTM model with d set to 150.\n• mLSTM with bi-LSTM sentence modeling (d = 150): This is the same as the model above except that when we derive the hidden states hsj and h t k of the two sentences, we use\nbi-LSTMs instead of LSTMs. We implement this model to see whether bi-LSTMs allow us to better align the sentences.\n• mLSTM (d = 300): This is our mLSTM model with d set to 300.\n• mLSTM with word embedding (d = 300): This is the same as the model above except that we directly use the word embedding vectors xsj and x t k instead of the hidden\nstates hsj and h t k in our model. In this case, each attention vector ak is a weighted sum of {xsj}Mj=1. We experiment with this setting because we hypothesize that the effectiveness of our model is largely related to the mLSTM that models the matching rather than the use of LSTMs to process the original sentences."
    }, {
      "heading" : "3.2 Quantitative Results",
      "text" : "Table 1 compares the performance of the various models we tested together with some previously reported results. We have the following observations: (1) First of all, we can see that when we set d to 300, our model achieves an accuracy of 86.1% on the test data, which to the best of our knowledge is the highest on this data set. (2) If we compare our mLSTM model with our implementation of the word-by-word attention model by Rocktäschel et al. (2015) under the same setting with d = 150, we can see that our performance on the test data (85.7%) is still higher than that of the other model (82.6%). We also tested statistical significance and found the improvement to be statistically significant at the 0.001 level. (3) The performance of mLSTM with bi-LSTM sentence modeling compared with the model with standard LSTM sentence modeling when d is set to 150 shows that using bi-LSTM to process the original sentences helps (86.0% vs. 85.7% on the test data), but the difference is small. Therefore when we increased d to 300 we did not experiment with bi-LSTM sentence modeling. (4) Interestingly, when we experimented with the mLSTM model using the pre-trained word embeddings instead of LSTM-generated hidden states as initial representations of the premise and the hypothesis, we were able to achieve an accuracy of 85.3% on the test data, which is still better than previously reported state of the art. This suggests that the mLSTM architecture coupled with the attention model works well, regardless of whether or not we use LSTM to process the original sentences."
    }, {
      "heading" : "3.3 Qualitative Analyses",
      "text" : "To obtain a better understanding of how our proposed model actually performs the matching between a premise and a hypothesis, we further conduct the following analyses. First, we look at the\nlearned word-by-word alignment weights αkj to check whether the soft alignment makes sense. This is the same as what was done in (Rocktäschel et al., 2015). We then look at the values of the various gate vectors of the mLSTM. By looking at these values, we are able to check (1) whether the model is able to differentiate between more important and less important word-level matching results, and (2) whether the model forgets certain matches and remembers certain other matches.\nWhile we have looked at a random sample of sentence pairs, here we show only three examples. These three sentence pairs share the same premise but have different hypotheses and different relationship labels. They are given in Table 2. We can see that the first hypothesis is an entailment. The second hypothesis is a contradiction because it mentions a completely different event. The third hypothesis is neutral to the premise because the phrase “with his owner” cannot be inferred from the premise.\nWord Alignment First, let us look at the top-most plots of Figure 2, Figure 3 and Figure 4. These plots show the alignment weights αkj between the hypothesis and the premise, where a darker color corresponds to a larger value of αkj . Recall that αkj is the degree to which the word xtk in the hypothesis is aligned with the word xsj in the premise. Also recall that the weights αkj are configured such that for the same k all the αkj add up to 1. This means the weights in the same row in these plots add up to 1.\nFrom the three plots we can see that the alignment weights generally make sense. For example, in Example 1, “animal” is strongly aligned with “dog” while “toy” aligned with “Frisbee.” The phrase “cold weather” is aligned with “snow.” In Example 3, we also see that “pet” is strongly aligned with “dog” while “game” aligned with “Frisbee.”\nIn Example 2, “cat” is strongly aligned with “dog” and “washes” is aligned with “jumping.” It may appear that these matches are wrong. However, “dog” is likely the best match for “cat” among all the words in the premise, and as we will show later, this match between “cat” and “dog” is actually a strong indication of a contradiction between the two sentences. The same explanation applies to the match between “washes” and “jumping.”\nWe also observe that some words are aligned\nwith the NULL token we inserted. For example, the word “is” in the hypothesis in Example 1 does not correspond to any word in the premise and is therefore aligned with NULL. The words “face” and “whiskers” in Example 2 and “owner” in Example 3 are also aligned with NULL. Intuitively, if some important content words in the hypothesis cannot find a match in the premise and are therefore aligned with NULL, then we should have a higher chance to believe that the relationship label is either contradiction or neutral.\nValues of Gate Vectors Next, let us look at the values of the learned gate vectors of our mLSTM for the three examples. We show these values under the setting where d is set to 150. Each row of these plots corresponds to one of the 150 dimensions. Again, a darker color indicates a higher value.\nAn input gate controls whether the input at the current position should be used in deriving the final hidden state of the current position. From the three plots of the input gates of the three examples, we can observe that generally for stop words such as prepositions and articles the input gates have lower values, suggesting that the matches of these words in the hypothesis with the premise are less important for deriving the hidden state of the current position, and hence less important for predicting the final relationship label. On the other hand, content words such as nouns and verbs tend to have higher values of the input gates, which also makes sense because these words are generally more important for determining the relationship label between the two sentences. Overall, the observation with the input gates verifies our assumption that the mLSTM helps differentiate the more important word-level matching results from the less important ones.\nNext, let us look at the forget gates. Recall that a forget gate controls the importance of the previous cell state in deriving the final hidden state of the current position. Higher values of a forget gate indicate that we need to remember the previous cell state and pass it on whereas lower values\nindicate that we should probably forget the previous cell. From the three plots of the forget gates of the three examples, we can see that overall the colors are the lightest for Example 1, which is an entailment. Light colors correspond to low values, and in this case they suggest that when the hypothesis is an entailment of the premise, the mLSTM tends to forget the previous matching results. On the other hand, for Example 2 and Example 3, which are contradiction and neutral, we see generally darker colors, which correspond to higher values of the forget gates. In particular, in Example 2, we can see that the colors are consistently dark starting from the word “his” in the hypothesis until the end. We believe the explanation is that after the mLSTM processes the first three words of the hypothesis, “A cat washes,” it sees that the matchings between “cat” and “dog” and between “washes” and “jumping” are both strong indications of a contradiction, and therefore these matching results need to be remembered until the end of the mLSTM for the final softmax-based classifier to make a prediction.\nWe have also checked the plots for the forget gates of some other sentence pairs, and we observe that generally for entailment, the forget gates have low values, while for contradiction and neutral, the forget gates start to have high values from certain position of the hypothesis. We therefore hypothesize that the way the mLSTM works is as follows. It remembers important mismatches, which are useful for predicting the contradiction or the neutral relationship, and forgets good matches. At the end of the mLSTM, if no important mismatch is remembered, the final classifier then will likely predict entailment by default. Otherwise, depending on the kind of mismatch remembered, the classifier will predict either contradiction or neutral.\nIt is also interesting to point out that the values of the input gates seem to have a negative correlation with the values of the forget gates. In other words, at each position of the hypothesis, if the input gate has high values, then the forget gate tends to have low values, and vice versa.\nFor the output gates, we are not able to draw\nany important conclusion except that the output gates seem to be positively correlated with the input gates but they tend to be darker than the input gates."
    }, {
      "heading" : "4 Related Work",
      "text" : "There has been much work on natural language inference. Shallow methods rely mostly on lexical similarities but are shown to be robust. For example, Bowman et al. (2015) experimented with a lexicalized classifier-based method, which only uses lexical information to extract features used by a classifier, and the method achieves an accuracy of 78.2% on the SNLI corpus. More advanced methods use syntactic structures of the sentences to help matching them. For example, Mehdad et al. (2009) applied syntactic-semantic tree kernels for recognizing textual entailment. Because inference is essentially a logic problem, methods based on formal logic (Clark and Harrison, 2009) or natural logic (MacCartney, 2009) have also been proposed. A comprehensive review on existing work on natural language inference can be found in (Sammons et al., 2011).\nThe work most relevant to ours is the recently proposed neural attention model-based method by Rocktäschel et al. (2015), which we have detailed in previous sections. Neural attention models have recently been applied to some natural language processing tasks including machine translation (Bahdanau et al., 2014), abstractive summarization (Rush et al., 2015) and question answering (Hermann et al., 2015). Rocktäschel et al. (2015) showed that the neural attention model could help derive a better representation of the premise to be used to match the hypothesis, whereas in our work we also use it to derive representations of the premise that are used to sequentially match the words in the hypothesis.\nThe Stanford Natural Language Inference corpus is new and so far it has only been used in a few studies. Besides the work by Bowman et al. (2015) themselves and by Rocktäschel et al. (2015), there is another study by Vendrov et al. (2015) in which a Skip-Thought model by Kiros et al. (2015) was applied to the NLI task. The authors reported an accuracy of 81.5% on the data set. Because this accuracy is lower than the best performance reported by Rocktäschel et al. (2015) and because our main focus was to examine the effectiveness of our match-LSTM compared with the model by\nRocktäschel et al. (2015), we did not include their study for comparison in our experiments."
    }, {
      "heading" : "5 Conclusions",
      "text" : "In this paper, we proposed a special LSTM architecture for the task of natural language inference. Based on a recent work by Rocktäschel et al. (2015), we first used neural attention models to derive attention-weighted vector representations of the premise. We then designed a match-LSTM that processes the hypothesis word by word while trying to match the hypothesis with the premise. Specifically the mLSTM takes in as input both the current hidden state of the hypothesis and an attention-weighted representation of the premise, and generates an output that represents the matching between the premise and the hypothesis up to the current position. As a result, the last hidden state of the mLSTM can be used for predicting the relationship between the premise and the hypothesis.\nExperiments on the SNLI corpus showed that the mLSTM model outperformed the state-of-theart performance reported so far on this data set. Moreover, closer analyses on the gate vectors revealed that our mLSTM indeed remembers and passes on important matching results, which are typically mismatches that indicate a contradiction or a neutral relationship between the premise and the hypothesis."
    } ],
    "references" : [ {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "arXiv preprint arXiv:1409.0473.",
      "citeRegEx" : "Bahdanau et al\\.,? 2014",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2014
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "HyungHyun Cho", "Yoshua Bengio." ],
      "venue" : "Proceedings of the International Conference on Learning Representations.",
      "citeRegEx" : "Bahdanau et al\\.,? 2015",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2015
    }, {
      "title" : "A large annotated corpus for learning natural language inference",
      "author" : [ "Samuel R Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D Manning." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages",
      "citeRegEx" : "Bowman et al\\.,? 2015",
      "shortCiteRegEx" : "Bowman et al\\.",
      "year" : 2015
    }, {
      "title" : "An inferencebased approach to recognizing entailment",
      "author" : [ "Peter Clark", "Phil Harrison." ],
      "venue" : "Proceedings of the Text Analysis Conference.",
      "citeRegEx" : "Clark and Harrison.,? 2009",
      "shortCiteRegEx" : "Clark and Harrison.",
      "year" : 2009
    }, {
      "title" : "The PASCAL Recognising Textual Entailment Challenge",
      "author" : [ "Ido Dagan", "Oren Glickman", "Bernardo Magnini." ],
      "venue" : "Proceedings of the PASCAL Challenges Workshop on Recognizing Textual Entailment.",
      "citeRegEx" : "Dagan et al\\.,? 2005",
      "shortCiteRegEx" : "Dagan et al\\.",
      "year" : 2005
    }, {
      "title" : "Web based probabilistic textual entailment",
      "author" : [ "Oren Glickman", "Ido Dagan", "Moshe Koppel." ],
      "venue" : "Proceedings of the PASCAL Challenges Workshop on Recognizing Textual Entailment.",
      "citeRegEx" : "Glickman et al\\.,? 2005",
      "shortCiteRegEx" : "Glickman et al\\.",
      "year" : 2005
    }, {
      "title" : "Teaching machines to read and comprehend",
      "author" : [ "Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom." ],
      "venue" : "Advances in Neural Information Processing Systems, pages 1684–",
      "citeRegEx" : "Hermann et al\\.,? 2015",
      "shortCiteRegEx" : "Hermann et al\\.",
      "year" : 2015
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik Kingma", "Jimmy Ba." ],
      "venue" : "arXiv preprint arXiv:1412.6980.",
      "citeRegEx" : "Kingma and Ba.,? 2014",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Skip-thought vectors",
      "author" : [ "Ryan Kiros", "Yukun Zhu", "Ruslan R Salakhutdinov", "Richard Zemel", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler." ],
      "venue" : "Advances in Neural Information Processing Systems, pages 3276–3284.",
      "citeRegEx" : "Kiros et al\\.,? 2015",
      "shortCiteRegEx" : "Kiros et al\\.",
      "year" : 2015
    }, {
      "title" : "A phrase-based alignment model for natural language inference",
      "author" : [ "Bill MacCartney", "Michel Galley", "Christopher D Manning." ],
      "venue" : "Proceedings of the Conference on Empirical Methods in Natural Language Processing.",
      "citeRegEx" : "MacCartney et al\\.,? 2008",
      "shortCiteRegEx" : "MacCartney et al\\.",
      "year" : 2008
    }, {
      "title" : "Natural Language Inference",
      "author" : [ "Bill MacCartney." ],
      "venue" : "Ph.D. thesis, Stanford University.",
      "citeRegEx" : "MacCartney.,? 2009",
      "shortCiteRegEx" : "MacCartney.",
      "year" : 2009
    }, {
      "title" : "SemKer: Syntactic/semantic kernels for recognizing textual entailment",
      "author" : [ "Yashar Mehdad", "Alessandro Moschitti", "Fabio Massiomo Zanzotto" ],
      "venue" : "In Proceedings of the Text Analysis Conference",
      "citeRegEx" : "Mehdad et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Mehdad et al\\.",
      "year" : 2009
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher D Manning." ],
      "venue" : "Proceedings of the Conference on Empirical Methods in Natural Language Processing.",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Reasoning about entailment with neural attention",
      "author" : [ "Tim Rocktäschel", "Edward Grefenstette", "Karl Moritz Hermann", "Tomáš Kočiskỳ", "Phil Blunsom." ],
      "venue" : "arXiv preprint arXiv:1509.06664.",
      "citeRegEx" : "Rocktäschel et al\\.,? 2015",
      "shortCiteRegEx" : "Rocktäschel et al\\.",
      "year" : 2015
    }, {
      "title" : "A neural attention model for abstractive sentence summarization",
      "author" : [ "Alexander M Rush", "Sumit Chopra", "Jason Weston." ],
      "venue" : "arXiv preprint arXiv:1509.00685.",
      "citeRegEx" : "Rush et al\\.,? 2015",
      "shortCiteRegEx" : "Rush et al\\.",
      "year" : 2015
    }, {
      "title" : "Recognizing textual entailment",
      "author" : [ "Mark Sammons", "VG Vinod Vydiswaran", "Dan Roth." ],
      "venue" : "Multilingual Natural Language Applications: From Theory to Practice. Prentice Hall, Jun.",
      "citeRegEx" : "Sammons et al\\.,? 2011",
      "shortCiteRegEx" : "Sammons et al\\.",
      "year" : 2011
    }, {
      "title" : "Order-embeddings of images and language",
      "author" : [ "Ivan Vendrov", "Ryan Kiros", "Sanja Fidler", "Raquel Urtasun." ],
      "venue" : "arXiv preprint arXiv:1511.06361.",
      "citeRegEx" : "Vendrov et al\\.,? 2015",
      "shortCiteRegEx" : "Vendrov et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 10,
      "context" : "can infer another sentenceH (MacCartney, 2009).",
      "startOffset" : 28,
      "endOffset" : 46
    }, {
      "referenceID" : 4,
      "context" : "There has been much interest in NLI in the past decade, especially surrounding the PASCAL Recognizing Textual Entailment (RTE) Challenge (Dagan et al., 2005).",
      "startOffset" : 137,
      "endOffset" : 157
    }, {
      "referenceID" : 5,
      "context" : "Existing solutions to NLI range from shallow approaches based on lexical similarities (Glickman et al., 2005) to advanced methods that consider syntax (Mehdad et al.",
      "startOffset" : 86,
      "endOffset" : 109
    }, {
      "referenceID" : 11,
      "context" : ", 2005) to advanced methods that consider syntax (Mehdad et al., 2009), perform explicit sen-",
      "startOffset" : 49,
      "endOffset" : 70
    }, {
      "referenceID" : 9,
      "context" : "tence alignment (MacCartney et al., 2008) or use formal logic (Clark and Harrison, 2009).",
      "startOffset" : 16,
      "endOffset" : 41
    }, {
      "referenceID" : 3,
      "context" : ", 2008) or use formal logic (Clark and Harrison, 2009).",
      "startOffset" : 28,
      "endOffset" : 54
    }, {
      "referenceID" : 2,
      "context" : "Recently, Bowman et al. (2015) released the Stanford Natural Language Inference (SNLI) corpus for the purpose of encouraging more learningcentered approaches to NLI.",
      "startOffset" : 10,
      "endOffset" : 31
    }, {
      "referenceID" : 2,
      "context" : "Bowman et al. (2015)",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 2,
      "context" : "Bowman et al. (2015) achieved an accuracy of 77.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 13,
      "context" : "A more recent work by Rocktäschel et al. (2015) improved the performance by applying a neural attention model.",
      "startOffset" : 22,
      "endOffset" : 48
    }, {
      "referenceID" : 2,
      "context" : "Different from the models in (Bowman et al., 2015) and (Rocktäschel et al.",
      "startOffset" : 29,
      "endOffset" : 50
    }, {
      "referenceID" : 13,
      "context" : ", 2015) and (Rocktäschel et al., 2015), our prediction is not based on whole sentence embed-",
      "startOffset" : 12,
      "endOffset" : 38
    }, {
      "referenceID" : 13,
      "context" : "We first present the model by Rocktäschel et al. (2015) because our model builds on top of theirs.",
      "startOffset" : 30,
      "endOffset" : 56
    }, {
      "referenceID" : 13,
      "context" : "While there are different variations of LSTMs, here we present the one adopted in (Rocktäschel et al., 2015).",
      "startOffset" : 82,
      "endOffset" : 108
    }, {
      "referenceID" : 13,
      "context" : "Rocktäschel et al. (2015) further built an",
      "startOffset" : 0,
      "endOffset" : 26
    }, {
      "referenceID" : 13,
      "context" : "Rocktäschel et al. (2015) then used this hN , which represents the whole premise, together with hN , which is an aggregated representation of the whole hypothesis2, to predict the label y.",
      "startOffset" : 0,
      "endOffset" : 26
    }, {
      "referenceID" : 13,
      "context" : "Strictly speaking, in (Rocktäschel et al., 2015), hN encodes both the premise and the hypothesis because the two sentences are chained.",
      "startOffset" : 22,
      "endOffset" : 48
    }, {
      "referenceID" : 11,
      "context" : "We present the word-by-word attention model by Rocktäschel et al. (2015) in a different way but the underlying model is the same.",
      "startOffset" : 47,
      "endOffset" : 73
    }, {
      "referenceID" : 0,
      "context" : "Our presentation is close to the one by Bahdanau et al. (2015), with our attention vectors a corresponding to the context vectors c in their paper.",
      "startOffset" : 40,
      "endOffset" : 63
    }, {
      "referenceID" : 12,
      "context" : "Although the neural attention model by Rocktäschel et al. (2015) achieved better results than Bowman et al.",
      "startOffset" : 39,
      "endOffset" : 65
    }, {
      "referenceID" : 2,
      "context" : "(2015) achieved better results than Bowman et al. (2015), we see two limitations.",
      "startOffset" : 36,
      "endOffset" : 57
    }, {
      "referenceID" : 13,
      "context" : "The second limitation is that the model by Rocktäschel et al. (2015) does not explicitly allow us to place more emphasis on the more important",
      "startOffset" : 43,
      "endOffset" : 69
    }, {
      "referenceID" : 13,
      "context" : "Figure 1 gives an overview of our model in contrast to the model by Rocktäschel et al. (2015).",
      "startOffset" : 68,
      "endOffset" : 94
    }, {
      "referenceID" : 13,
      "context" : "First, similar to Rocktäschel et al. (2015), we process the premise and the hypothesis using two LSTMs, but we do not feed the last cell state of the premise to the LSTM of the hypothesis.",
      "startOffset" : 18,
      "endOffset" : 44
    }, {
      "referenceID" : 13,
      "context" : "Figure 1: The top figure depicts the model by Rocktäschel et al. (2015) and the bottom figure depicts our model.",
      "startOffset" : 46,
      "endOffset" : 72
    }, {
      "referenceID" : 13,
      "context" : "Figure 1: The top figure depicts the model by Rocktäschel et al. (2015) and the bottom figure depicts our model. Here Hs represents all the hidden states hj . We can see that in the model by Rocktäschel et al. (2015), each hk represents a weighted version of the premise only, while in our",
      "startOffset" : 46,
      "endOffset" : 217
    }, {
      "referenceID" : 13,
      "context" : "Besides the mLSTM architecture, which is the main difference of our model from the model by Rocktäschel et al. (2015), we also introduce a few",
      "startOffset" : 92,
      "endOffset" : 118
    }, {
      "referenceID" : 12,
      "context" : "Second, we use word embeddings trained from GloVe (Pennington et al., 2014) instead of",
      "startOffset" : 50,
      "endOffset" : 75
    }, {
      "referenceID" : 13,
      "context" : "Although this is a very crude approximation, it reduces the number of parameters we need to update, and as it turns out, we can still achieve better performance than Rocktäschel et al. (2015).",
      "startOffset" : 166,
      "endOffset" : 192
    }, {
      "referenceID" : 2,
      "context" : "Model d |θ|W+M |θ|M Train Dev Test LSTM [Bowman et al. (2015)] 100 10M 221K 84.",
      "startOffset" : 41,
      "endOffset" : 62
    }, {
      "referenceID" : 2,
      "context" : "Model d |θ|W+M |θ|M Train Dev Test LSTM [Bowman et al. (2015)] 100 10M 221K 84.4 - 77.6 Classifier [Bowman et al. (2015)] - - 99.",
      "startOffset" : 41,
      "endOffset" : 121
    }, {
      "referenceID" : 2,
      "context" : "Model d |θ|W+M |θ|M Train Dev Test LSTM [Bowman et al. (2015)] 100 10M 221K 84.4 - 77.6 Classifier [Bowman et al. (2015)] - - 99.7 - 78.2 LSTM shared [Rocktäschel et al. (2015)] 159 3.",
      "startOffset" : 41,
      "endOffset" : 177
    }, {
      "referenceID" : 2,
      "context" : "Model d |θ|W+M |θ|M Train Dev Test LSTM [Bowman et al. (2015)] 100 10M 221K 84.4 - 77.6 Classifier [Bowman et al. (2015)] - - 99.7 - 78.2 LSTM shared [Rocktäschel et al. (2015)] 159 3.9M 252K 84.4 83.0 81.4 Word-by-word attention [Rocktäschel et al. (2015)] 100 3.",
      "startOffset" : 41,
      "endOffset" : 257
    }, {
      "referenceID" : 2,
      "context" : "This follows the same data partition used by Bowman et al. (2015) in their experiments.",
      "startOffset" : 45,
      "endOffset" : 66
    }, {
      "referenceID" : 7,
      "context" : "Parameters: We use the Adam method (Kingma and Ba, 2014) with hyperparameters β1 set to 0.",
      "startOffset" : 35,
      "endOffset" : 56
    }, {
      "referenceID" : 13,
      "context" : "Methods for comparison: We mainly want to compare our model with the word-by-word attention model by Rocktäschel et al. (2015) because this model achieved the state-of-the-art performance on the SNLI corpus.",
      "startOffset" : 101,
      "endOffset" : 127
    }, {
      "referenceID" : 13,
      "context" : "ported by Rocktäschel et al. (2015), we also reimplemented their word-by-word attention model ourselves and report the performance of our implementation.",
      "startOffset" : 10,
      "endOffset" : 36
    }, {
      "referenceID" : 13,
      "context" : "• Word-by-word attention (d = 150): This is our implementation of the word-by-word attention model by Rocktäschel et al. (2015),",
      "startOffset" : 102,
      "endOffset" : 128
    }, {
      "referenceID" : 13,
      "context" : "The differences between our implementation and the original implementation by Rocktäschel et al. (2015) are the following: (1) We also add a NULL token to the premise for matching.",
      "startOffset" : 78,
      "endOffset" : 104
    }, {
      "referenceID" : 13,
      "context" : "we compare our mLSTM model with our implementation of the word-by-word attention model by Rocktäschel et al. (2015) under the same setting with d = 150, we can see that our performance on the test data (85.",
      "startOffset" : 90,
      "endOffset" : 116
    }, {
      "referenceID" : 2,
      "context" : "For example, Bowman et al. (2015) experimented with a lexicalized classifier-based method, which only uses lexical information to extract features used by a classifier, and the method achieves an accuracy of 78.",
      "startOffset" : 13,
      "endOffset" : 34
    }, {
      "referenceID" : 2,
      "context" : "For example, Bowman et al. (2015) experimented with a lexicalized classifier-based method, which only uses lexical information to extract features used by a classifier, and the method achieves an accuracy of 78.2% on the SNLI corpus. More advanced methods use syntactic structures of the sentences to help matching them. For example, Mehdad et al. (2009) applied syntactic-semantic tree kernels for recognizing textual entailment.",
      "startOffset" : 13,
      "endOffset" : 355
    }, {
      "referenceID" : 3,
      "context" : "ence is essentially a logic problem, methods based on formal logic (Clark and Harrison, 2009) or natural logic (MacCartney, 2009) have also been proposed.",
      "startOffset" : 67,
      "endOffset" : 93
    }, {
      "referenceID" : 10,
      "context" : "ence is essentially a logic problem, methods based on formal logic (Clark and Harrison, 2009) or natural logic (MacCartney, 2009) have also been proposed.",
      "startOffset" : 111,
      "endOffset" : 129
    }, {
      "referenceID" : 15,
      "context" : "A comprehensive review on existing work on natural language inference can be found in (Sammons et al., 2011).",
      "startOffset" : 86,
      "endOffset" : 108
    }, {
      "referenceID" : 0,
      "context" : "Neural attention models have recently been applied to some natural language processing tasks including machine translation (Bahdanau et al., 2014), abstractive summarization (Rush et al.",
      "startOffset" : 123,
      "endOffset" : 146
    }, {
      "referenceID" : 14,
      "context" : ", 2014), abstractive summarization (Rush et al., 2015) and question an-",
      "startOffset" : 35,
      "endOffset" : 54
    }, {
      "referenceID" : 6,
      "context" : "swering (Hermann et al., 2015).",
      "startOffset" : 8,
      "endOffset" : 30
    }, {
      "referenceID" : 6,
      "context" : "swering (Hermann et al., 2015). Rocktäschel et al. (2015) showed that the neural attention model could help derive a better representation of the premise to be used to match the hypothesis, whereas in our work we also use it to derive representations of the premise that are used to sequentially match the words in the hypothesis.",
      "startOffset" : 9,
      "endOffset" : 58
    }, {
      "referenceID" : 2,
      "context" : "Besides the work by Bowman et al. (2015) themselves and by Rocktäschel et al.",
      "startOffset" : 20,
      "endOffset" : 41
    }, {
      "referenceID" : 2,
      "context" : "Besides the work by Bowman et al. (2015) themselves and by Rocktäschel et al. (2015), there is another study by Vendrov et al.",
      "startOffset" : 20,
      "endOffset" : 85
    }, {
      "referenceID" : 2,
      "context" : "Besides the work by Bowman et al. (2015) themselves and by Rocktäschel et al. (2015), there is another study by Vendrov et al. (2015) in which a Skip-Thought model by Kiros et al.",
      "startOffset" : 20,
      "endOffset" : 134
    }, {
      "referenceID" : 2,
      "context" : "Besides the work by Bowman et al. (2015) themselves and by Rocktäschel et al. (2015), there is another study by Vendrov et al. (2015) in which a Skip-Thought model by Kiros et al. (2015) was applied to the NLI task.",
      "startOffset" : 20,
      "endOffset" : 187
    }, {
      "referenceID" : 2,
      "context" : "Besides the work by Bowman et al. (2015) themselves and by Rocktäschel et al. (2015), there is another study by Vendrov et al. (2015) in which a Skip-Thought model by Kiros et al. (2015) was applied to the NLI task. The authors reported an accuracy of 81.5% on the data set. Because this accuracy is lower than the best performance reported by Rocktäschel et al. (2015) and because our main focus was to examine the effectiveness of our match-LSTM compared with the model by Rocktäschel et al.",
      "startOffset" : 20,
      "endOffset" : 370
    }, {
      "referenceID" : 2,
      "context" : "Besides the work by Bowman et al. (2015) themselves and by Rocktäschel et al. (2015), there is another study by Vendrov et al. (2015) in which a Skip-Thought model by Kiros et al. (2015) was applied to the NLI task. The authors reported an accuracy of 81.5% on the data set. Because this accuracy is lower than the best performance reported by Rocktäschel et al. (2015) and because our main focus was to examine the effectiveness of our match-LSTM compared with the model by Rocktäschel et al. (2015), we did not include their study for comparison in our experiments.",
      "startOffset" : 20,
      "endOffset" : 501
    }, {
      "referenceID" : 13,
      "context" : "Based on a recent work by Rocktäschel et al. (2015), we first used neural attention models to derive attention-weighted vector representations of the premise.",
      "startOffset" : 26,
      "endOffset" : 52
    } ],
    "year" : 2015,
    "abstractText" : "Natural language inference (NLI) is a fundamentally important task in natural language processing that has many applications. The recently released Stanford Natural Language Inference (SNLI) corpus has made it possible to develop and evaluate learning-centered methods such as deep neural networks for the NLI task. In this paper, we propose a special long short-term memory (LSTM) architecture for NLI. Our model builds on top of a recently proposed neutral attention model for NLI but is based on a significantly different idea. Instead of deriving sentence embeddings for the premise and the hypothesis to be used for classification, our solution uses a matching-LSTM that performs word-by-word matching of the hypothesis with the premise. This LSTM is able to place more emphasis on important word-level matching results. In particular, we observe that this LSTM remembers important mismatches that are critical for predicting the contradiction or the neutral relationship label. Our experiments on the SNLI corpus show that our model outperforms the state of the art, achieving an accuracy of 86.1% on the test data.",
    "creator" : "TeX"
  }
}