{
  "name" : "1501.02432.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Sanjit S. Batra", "Siddarth Sabharwal" ],
    "emails" : [ "jayadeva@ee.iitd.ac.in" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n50 1.\n02 43\n2v 1\n[ cs\n.L G\n] 1\nThe Vapnik-Chervonenkis (VC) dimension measures the complexity of a learning machine, and a low VC dimension leads to good generalization. The recently proposed Minimal Complexity Machine (MCM) learns a hyperplane classifier by minimizing an exact bound on the VC dimension. This paper extends the MCM classifier to the fuzzy domain. The use of a fuzzy membership is known to reduce the effect of outliers, and to reduce the effect of noise on learning. Experimental results show, that on a number of benchmark datasets, the the fuzzy MCM classifier outperforms SVMs and the conventional MCM in terms of generalization, and that the fuzzy MCM uses fewer support vectors. On several benchmark datasets, the fuzzy MCM classifier yields excellent test set accuracies while using one-tenth the number of support vectors used by SVMs.\nKeywords: Machine Learning, Support Vector Machines, VC dimension, complexity, generalization, fuzzy SVMs"
    }, {
      "heading" : "1. Introduction",
      "text" : "Support vector machines are amongst the most widely used machine learning techniques today. The most commonly used variants are the maximum margin L1 norm SVM [1], and the least squares SVM (LSSVM) [2], both of which require\n∗Corresponding Author Email address: jayadeva@ee.iitd.ac.in (Jayadeva)\npreprint submitted to arXiv January 13, 2015\nthe solution of a quadratic programming problem. The proximal SVM [3] is also similar in spirit to the LSSVM. SVMs were motivated by the celebrated work of Vapnik and his colleagues on generalization, and the complexity of learning. The capacity of a learning machine may be measured by its VC dimension, and a small VC dimension leads to good generalization and low error rates on test data.\nHowever, according to Burges [4], SVMs can have a very large VC dimension, and that “at present there exists no theory which shows that good generalization performance is guaranteed for SVMs”. In recent work [5], we have shown how to learn a bounded margin hyperplane classifier, termed as the Minimal Complexity Machine (MCM) by minimizing an exact bound on its VC dimension. Experimental results on many benchmark datasets confirm that in comparison to SVMs, the MCM generalizes well while using significantly fewer support vectors, often lower by a factor between 10 and 50.\nClassically, each training sample in a binary classification setting is treated equally and is associated with a unique class. However, in reality, some training samples may be corrupted by noise; this could be noise in the sample’s location or in its label. Such samples may be thought of as not lying entirely in one class, but belonging to both classes to a certain degree [6]. It is well known that SVMs are very sensitive to outliers [7, 8, 9]. Fuzzy support vector machines (FSVM) [8] were proposed to address this problem. In FSVMs, each sample is assigned a fuzzy membership which indicates the extent to which belongs to any one class. The membership also determines the importance of the sample in determining the separating hyperplane. Consequently, the measurement of the empirical error in a fuzzy setting does not treat all samples equally. Discounting errors on outlier samples can allow hyperplanes with larger margins to be learnt, and can also obviate the effect of noise to a considerable degree.\nThis paper extends the MCM into the fuzzy domain, by attempting to learn a gap tolerant, or fat margin fuzzy classifier with low VC dimension. The fuzzy MCM objective function consists of two terms. The first term is related to the VC dimension of the classifier, and minimization of this term yields a classi-\nfier with good generalization properties. The second term is a weighted sum of misclassification errors over the training samples; the weights are dependent on the fuzzy memberships of the samples, and samples that are outliers contribute less to the overall error measure. The fuzzy MCM optimization problem thus tries to find a hyperplane with a small VC dimension, that minimizes the fuzzy weighted empirical error over training data samples. The use of fuzzy memberships allows importance to be attached to individual samples, and hence helps improve generalization by not assigning equal importance to the misclassification error contributions of different samples; this reduces the effect of outliers. The fuzzy Minimal Complexity Machine, as the proposed approach is termed, dramatically outperforms conventional SVMs in terms of support vectors used, while yielding better test set accuracy. The effect of the approach to minimizing VC dimension may be guaged from the fact that on several datasets, the number of support vectors is more than fifteen times smaller than those used by SVMs. As we show in the sequel, an interesting example is that of the ’haberman’ dataset from the UCI machine learning repository [10], that has 306 samples. A fuzzy MCM classifier learnt using 80% of the dataset yields a classifier that can be written as a closed form expression involving only 4 support vectors. In comparison, a SVM classifier uses about 73 support vectors.\nThe rest of the paper is organized as follows. Section 2 briefly describes the MCM classifier, for the sake of completeness. Section 3 shows how to extend the approach to learn a linear fuzzy MCM classifier, and section 4 then extends this work to the kernel case. Section 5 is devoted to a discussion of results obtained on selected benchmark datasets. Section 6 contains concluding remarks."
    }, {
      "heading" : "2. The Linear Minimal Complexity Machine Classifier",
      "text" : "The motivation for the MCM originates from some outstanding work on\ngeneralization [11, 12, 13, 14].\nConsider a binary classification dataset with n-dimensional samples xi, i =\n1, 2, ...,M , where each sample is associated with a label yi ∈ {+1,−1}. Vapnik [13] showed that the VC dimension γ for fat margin hyperplane classifiers with margin d ≥ dmin satisfies\nγ ≤ 1 +Min( R2\nd2min , n) (1)\nwhere R denotes the radius of the smallest sphere enclosing all the training samples. Burges, in [4], stated that “the above arguments strongly suggest that algorithms that minimize R 2\nd2 can be expected to give better generalization perfor-\nmance. Further evidence for this is found in the following theorem of (Vapnik, 1998), which we quote without proof”.\nFollowing this line of argument leads us to the formulations for a hyperplane classifier with minimum VC dimension; we term the same as the MCM classifier. We now summarize the MCM classifier formulation for the sake of completeness. Details may be found in [5].\nConsider the case of a linearly separable dataset. By definition, there exists a hyperplane that can classify these points with zero error. Let the separating hyperplane be given by\nuTx+ v = 0. (2)\nLet us denote\nh = Maxi=1,2,...,M yi(u\nTxi + v)\nMini=1,2,...,M yi(uTxi + v) . (3)\nIn [5], we show that there exist constants α, β > 0, α, β ∈ ℜ such that\nαh2 ≤ γ ≤ βh2, (4)\nor, in other words, h2 constitutes a tight or exact (θ) bound on the VC dimension γ. An exact bound implies that h2 and γ are close to each other.\nFigure 1 illustrates this notion. It is known that the number of degrees of freedom in a learning machine is related to the VC dimension, but the connection is tenuous and usually abstruse. Even though the VC dimension γ may\nhave a complicated dependence on the variables defining the learning machine, the VC dimension γ is bounded by multiples of h2 from both above and below. The exact bound h2 is thus always “close” to the VC dimension, and minimizing h2 with respect to the variables defining the learning machine allows us to find one that has a small VC dmension. The use of a continuous and differentiable exact bound on the VC dimension allows us to find a learning machine with small VC dimension; this may be achieved by minimizing h over the space of variables defining the separating hyperplane. In the case of a hyperplane classifier, the only variables are u and v, and a hyperplane classifier with a small VC dimension is obtained by minimizing h2 with respect to these variables.\nThe MCM classifier solves an optimization problem, that tries to minimize the machine capacity, while classifying all training points of the linearly separable dataset correctly. This problem is given by\nMinimize u,v\nh = Maxi=1,2,...,M yi(u\nTxi + v)\nMini=1,2,...,M yi(uTxi + v) , (5)\nthat attempts to minimize h instead of h2, the square function (·)2 being a monotonically increasing one.\nThis optimization problem is both quasiconvex and pseudoconvex. In [5], we further show that the optimization problem (5) may be reduced to the linear programming problem\nMin w,b,h h (6)\nh ≥ yi · [w Txi + b], i = 1, 2, ...,M (7) yi · [w Txi + b] ≥ 1, i = 1, 2, ...,M, (8)\nwhere w ∈ ℜn, and b, h ∈ ℜ. We refer to the problem (6) - (8) as the hard margin Linear Minimum Complexity Machine (Linear MCM).\nIn practice, the datasets may not be linearly separable. In such a case, we seek a classifier with a minimal VC dimension that has a small mis-classification error on the training samples. Such a hyperplane may be found by solving the soft margin MCM formulation, that is given by\nMin w,b,h,q h+ C ·\nM ∑\ni=1\nqi (9)\nh ≥ yi · [w Txi + b] + qi, i = 1, 2, ...,M (10) yi · [w Txi + b] + qi ≥ 1, i = 1, 2, ...,M, (11)\nqi ≥ 0, i = 1, 2, ...,M. (12)\nOnce w and b have been determined by solving (9)-(12), the class of a test\nsample x may be determined from the sign of the discriminant function\nf(x) = wTx+ b (13)"
    }, {
      "heading" : "3. The Fuzzy Minimal Complexity Machine Classifier",
      "text" : "In the linear soft margin MCM formulation (9)-(12), the error variables qi, i = 1, 2, ...,M measure the mis-classification error on the respective data\nsamples, and the second term of the objective function in (9) is a weighted sum of all the mis-classification errors. In this case, the hyper-parameter C equally weights all variables qi; this effectively means that errors made on all samples are equally important. In reality, noise tends to corrupt training samples, and robust learning requires us to ignore outliers, by assigning reduced importance to samples on which one has less confidence.\nSome samples may not be representative of a class. For example, a person showing some symptoms of a disease may have characteristics that overlap with both healthy subjects as well as unhealthy ones. Therefore, the membership of the class to which a sample belongs tends to be fuzzy, with a fuzzy membership (a value between 0 and 1) indicating the extent to which the sample may be said to belong to one class or the other. Samples with a higher membership value can be thought of as more representative of that class, while those with a smaller membership value should be given less importance when building a classifier.\nConsider the samples in Fig. 2. Four outlier samples have been highlighted by arrows. The hyperplanes found before and after discounting outlier samples have been shown in (a) and (b), respectively. Two of the outliers are in black, and discounting them would allow us to obtain a hyperplane with a smaller VC dimension. Two of the outliers are marked in red, and these make the data set linearly non-separable. Discounting classification errors on these red coloured samples would allow for a more robust classifier to be learnt.\nLin and Wang proposed fuzzy SVMs in [8], wherein they suggested that each sample be associated with a fuzzy membership si. This membership value determines how important it is to classify a data sample correctly; samples with lower values of the membership function are less representative of the class to which they have been assigned, and can therefore be mis-classified without incurring the same penalty. In the example of Fig. 2, outlier samples would have a small membership value; the optimization problem being solved factors in these membership values, thus allowing a more robust classifier to be learnt.\nThe fuzzy MCM classifier aims to learn a hyperplane classifier that has a\nsmall VC dimension, and that also minimizes a weighted measure of the classification error on training samples. The linear fuzzy MCM (FMCM) classifier does this by solving the following optimization problem.\nMin w,b,h,q\nh+ C · M ∑\ni=1\nsiqi (14)\nh ≥ yi · [w Txi + b] + qi, i = 1, 2, ...,M (15) yi · [w Txi + b] + qi ≥ 1, i = 1, 2, ...,M, (16)\nqi ≥ 0 (17)\nHere, the fuzzy membership si for the i− th sample is used to determine the importance of the sample in terms of its possible mis-classification. This implies that samples with a small value of the fuzzy membership, such as outliers, can be ignored or accorded less importance when learning the classifier. This makes the classifier less sensitive to outliers, leading to more robust learning. In the example of Fig. 2, the values of si for the outlier samples are small. This implies that the objective function (14) discounts the errors caused when learning such samples, because of the small values of the weights si.\nIn the following section, we show how the linear fuzzy MCM can be extended\nto the kernel case."
    }, {
      "heading" : "4. The Fuzzy Kernel MCM",
      "text" : "We consider a map φ(x) that maps the input samples from ℜn to ℜr, where\nr > n. The separating hyperplane in the image space is given by\nuTφ(x) + v = 0. (18)\nFollowing (14) - (17), the optimization problem for the fuzzy kernel MCM\nmay be shown to be\nMin w,b,h,q\nh+ C · M ∑\ni=1\nsiqi (19)\nh ≥ yi · [w Tφ(xi) + b] + qi, i = 1, 2, ...,M (20) yi · [w T φ(xi) + b] + qi ≥ 1, i = 1, 2, ...,M (21)\nqi ≥ 0, i = 1, 2, ...,M. (22)\nThe image vectors φ(xi), i = 1, 2, ...,M form an overcomplete basis in the\nempirical feature space, in which w also lies. Hence, we can write\nw =\nM ∑\nj=1\nλjφ(x j). (23)\nNote that in (23), the φ(xj)’s for which the corresponding λj ’s are non-zero may be termed as support vectors.\nTherefore,\nwTφ(xi) + b =\nM ∑\nj=1\nλjφ(x j)Tφ(xi) + b =\nM ∑\nj=1\nλjK(x i, xj) + b, (24)\nwhere K(p, q) denotes the Kernel function with input vectors p and q, and\nis defined as\nK(p, q) = φ(p)T φ(q). (25)\nSubstituting from (24) into (19) - (21), we obtain the following optimization\nproblem.\nMin w,b,h,q h+ C ·\nM ∑\ni=1\nsiqi (26)\nh ≥ yi · [ M ∑\nj=1\nλjK(x i, xj) + b] + qi, i = 1, 2, ...,M (27)\nyi · [\nM ∑\nj=1\nλjK(x i, xj) + b] + qi ≥ 1, i = 1, 2, ...,M (28)\nqi ≥ 0, i = 1, 2, ...,M. (29)\nOnce the variables λj , j = 1, 2, ...,M and b are obtained, the class of a test\npoint x can be determined by evaluating the sign of\nf(x) = wTφ(x) + b =\nM ∑\nj=1\nλjK(x, x j) + b. (30)\nResults on benchmark datasets indicate that the use of fuzzy memberships in the FMCM can reduce the number of support vectors and also lead to improved accuracies on test data. In the sequel, we present results on the linear and kernel versions of the fuzzy MCM."
    }, {
      "heading" : "5. Experimental results",
      "text" : "The FMCM was coded in MATLAB. The code is available on request from the author. Fuzzy membership values were computed by using the approach outlined in [8]. In this case, the membership value si of the i-th sample is a function of its distance from its class centre. Lin and Wang suggested the formula\nsi =\n  \n \n1− ||x+ − xi|| r+ + δ , if yi = 1 , i.e., the sample belongs to class 1 1− ||x− − xi|| r − + δ , if yi = −1 , i.e., the sample belongs to class -1 (31)\nHere, r+ and r− are the radii of the two classes, and x+ and x− are the respective class centres. The scalar δ is a small number used to ensure that si does not become zero. Figure 3 illustrates the computation of the fuzzy membership.\nIn order to evaluate the FMCM, we chose a number of benchmark datasets from the UCI machine learning repository [10]. Table 1 summarizes information about the number of samples and features of each dataset.\nTable 2 summarizes five fold cross validation results of the fuzzy linear MCM on a number of datasets taken from the UCI machine learning repository. Accuracies refer to the test sets, and are indicated as mean ± standard deviation, computed using a standard five fold cross validation methodology. The table compares the linear MCM with LIBSVM using a linear kernel. The values of C were determined for the FMCM by performing a grid search.\nTable 3 summarizes five fold cross validation results of the Fuzzy kernel MCM on a number of datasets. A Gaussian kernel was used for both the FMCM and the FSVM. The width of the Gaussian kernel was chosen by using a grid search.\nA comparison with the fuzzy SVM indicates that the fuzzy MCM yields better generalization with fewer support vectors. An examination of the table indicates that the proposed approach shows a lower test set error, and also uses a smaller number of support vectors. It is also interesting to note that the fuzzy MCM outperforms the classical MCM in terms of the number of support vectors and test set accuracies. The results of the classical MCM have not been duplicated from [5] for the sake of brevity; an added reason is that a fair comparison would be between two methods that use a fuzzy methodology.\nAs an interesting illustration of the sparsity of the fuzzy MCM, consider a fuzzy kernel MCM classifier using a randomly chosen subset comprising 80% samples of the ’haberman’ dataset, that employs a Gaussian kernel. This classifier may be tested by the reader on any randomly chosen set of training samples. It is interesting because it uses only four support vectors and can be written\ndown as the following closed form expression.\nf(x1, x2, x3) =sign{−105.8063 exp[−10 −4 ∗ ((x1 − 36)2 + (x2− 69)2 + x32)]\n+ 90.5143 exp[−10−4((x1 − 43)2 + (x2− 58)2 + (x3 − 52)2)] + 129.7232 exp[−10−4((x1 − 54)2 + (x2 − 67)2 + (x3− 46)2)] − 113.7966 exp[−10−4((x1 − 62)2 + (x2 − 58)2 + x32)]\n− 0.7661} (32)\nHere, the input samples are in three dimensions, and given by (x1, x2, x3)."
    }, {
      "heading" : "6. Conclusion",
      "text" : "In this paper, we propose a way to build a fuzzy hyperplane classifier, termed as the fuzzy Minimal Complexity Machine (MCM), that learns a fuzzy cassifier with small VC dimension. The fuzzy MCM involves the solution of a linear programming problem. Experimental results show that the fuzzy MCM outperforms the fuzzy SVM in terms of test set accuracies on a number of selected benchmark datasets. At the same time, the number of support vectors is less, often by a substantial factor, often as large as 10 or more. It has not escaped our attention that the proposed approach can be extended to fuzzy least squares classifiers, as well as to tasks such as fuzzy regression and fuzzy time series prediction; in fact, a large number of variants of fuzzy SVMs can be re-examined from the perspective of the fuzzy MCM."
    } ],
    "references" : [ {
      "title" : "Support-vector networks",
      "author" : [ "C. Cortes", "V. Vapnik" ],
      "venue" : "Machine learning, vol. 20, no. 3, pp. 273–297, 1995.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "Least squares support vector machine classifiers",
      "author" : [ "J.A. Suykens", "J. Vandewalle" ],
      "venue" : "Neural processing letters, vol. 9, no. 3, pp. 293–300, 1999. 15",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Proximal support vector machine classifiers",
      "author" : [ "G. Fung", "O.L. Mangasarian" ],
      "venue" : "Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2001, pp. 77–86.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "A tutorial on support vector machines for pattern recognition",
      "author" : [ "C.J. Burges" ],
      "venue" : "Data mining and knowledge discovery, vol. 2, no. 2, pp. 121–167, 1998.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Learning a hyperplane classifier by minimizing an exact bound on the {VC} dimension",
      "author" : [ "Jayadeva" ],
      "venue" : "Neurocomputing, vol. 149, Part B, no. 0, pp. 683 – 689, 2015. [Online]. Available: http://www.sciencedirect.com/science/article/pii/S0925231214010194",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Fuzzy svm with a new fuzzy membership function",
      "author" : [ "X. Jiang", "Z. Yi", "J.C. Lv" ],
      "venue" : "Neural Computing & Applications, vol. 15, no. 3-4, pp. 268–276, 2006.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Robust support vector machine with bullet hole image classification",
      "author" : [ "Q. Song", "W. Hu", "W. Xie" ],
      "venue" : "Systems, Man, and Cybernetics, Part C: Applications and Reviews, IEEE Transactions on, vol. 32, no. 4, pp. 440– 448, 2002.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Fuzzy support vector machines",
      "author" : [ "C.-F. Lin", "S.-D. Wang" ],
      "venue" : "Neural Networks, IEEE Transactions on, vol. 13, no. 2, pp. 464–471, 2002.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Discovering informative patterns and data cleaning.",
      "author" : [ "I. Guyon", "N. Matic", "V. Vapnik" ],
      "venue" : null,
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1996
    }, {
      "title" : "UCI machine learning repository",
      "author" : [ "K. Bache", "M. Lichman" ],
      "venue" : "2013. [Online]. Available: http://archive.ics.uci.edu/ml",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "A framework for structural risk minimisation",
      "author" : [ "J. Shawe-Taylor", "P.L. Bartlett", "R.C. Williamson", "M. Anthony" ],
      "venue" : "Proceedings of the ninth annual conference on Computational learning theory. ACM, 1996, pp. 68–76. 16",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "Structural risk minimization over data-dependent hierarchies",
      "author" : [ "——" ],
      "venue" : "Information Theory, IEEE Transactions on, vol. 44, no. 5, pp. 1926–1940, 1998.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 1926
    }, {
      "title" : "Statistical learning theory",
      "author" : [ "V.N. Vapnik" ],
      "venue" : "1998.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Learning with kernels",
      "author" : [ "B. Schölkopf", "A.J. Smola" ],
      "venue" : null,
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2002
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "The most commonly used variants are the maximum margin L1 norm SVM [1], and the least squares SVM (LSSVM) [2], both of which require",
      "startOffset" : 67,
      "endOffset" : 70
    }, {
      "referenceID" : 1,
      "context" : "The most commonly used variants are the maximum margin L1 norm SVM [1], and the least squares SVM (LSSVM) [2], both of which require",
      "startOffset" : 106,
      "endOffset" : 109
    }, {
      "referenceID" : 2,
      "context" : "The proximal SVM [3] is also similar in spirit to the LSSVM.",
      "startOffset" : 17,
      "endOffset" : 20
    }, {
      "referenceID" : 3,
      "context" : "However, according to Burges [4], SVMs can have a very large VC dimension, and that “at present there exists no theory which shows that good generalization performance is guaranteed for SVMs”.",
      "startOffset" : 29,
      "endOffset" : 32
    }, {
      "referenceID" : 4,
      "context" : "In recent work [5], we have shown how to learn a bounded margin hyperplane classifier, termed as the Minimal Complexity Machine (MCM) by minimizing an exact bound on its VC dimension.",
      "startOffset" : 15,
      "endOffset" : 18
    }, {
      "referenceID" : 5,
      "context" : "Such samples may be thought of as not lying entirely in one class, but belonging to both classes to a certain degree [6].",
      "startOffset" : 117,
      "endOffset" : 120
    }, {
      "referenceID" : 6,
      "context" : "It is well known that SVMs are very sensitive to outliers [7, 8, 9].",
      "startOffset" : 58,
      "endOffset" : 67
    }, {
      "referenceID" : 7,
      "context" : "It is well known that SVMs are very sensitive to outliers [7, 8, 9].",
      "startOffset" : 58,
      "endOffset" : 67
    }, {
      "referenceID" : 8,
      "context" : "It is well known that SVMs are very sensitive to outliers [7, 8, 9].",
      "startOffset" : 58,
      "endOffset" : 67
    }, {
      "referenceID" : 7,
      "context" : "Fuzzy support vector machines (FSVM) [8] were proposed to address this problem.",
      "startOffset" : 37,
      "endOffset" : 40
    }, {
      "referenceID" : 9,
      "context" : "As we show in the sequel, an interesting example is that of the ’haberman’ dataset from the UCI machine learning repository [10], that has 306 samples.",
      "startOffset" : 124,
      "endOffset" : 128
    }, {
      "referenceID" : 10,
      "context" : "The motivation for the MCM originates from some outstanding work on generalization [11, 12, 13, 14].",
      "startOffset" : 83,
      "endOffset" : 99
    }, {
      "referenceID" : 11,
      "context" : "The motivation for the MCM originates from some outstanding work on generalization [11, 12, 13, 14].",
      "startOffset" : 83,
      "endOffset" : 99
    }, {
      "referenceID" : 12,
      "context" : "The motivation for the MCM originates from some outstanding work on generalization [11, 12, 13, 14].",
      "startOffset" : 83,
      "endOffset" : 99
    }, {
      "referenceID" : 13,
      "context" : "The motivation for the MCM originates from some outstanding work on generalization [11, 12, 13, 14].",
      "startOffset" : 83,
      "endOffset" : 99
    }, {
      "referenceID" : 12,
      "context" : "Vapnik [13] showed that the VC dimension γ for fat margin hyperplane classifiers with margin d ≥ dmin satisfies γ ≤ 1 +Min( R d2min , n) (1)",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 3,
      "context" : "Burges, in [4], stated that “the above arguments strongly suggest that algorithms that minimize R 2 d2 can be expected to give better generalization performance.",
      "startOffset" : 11,
      "endOffset" : 14
    }, {
      "referenceID" : 4,
      "context" : "Details may be found in [5].",
      "startOffset" : 24,
      "endOffset" : 27
    }, {
      "referenceID" : 4,
      "context" : "In [5], we show that there exist constants α, β > 0, α, β ∈ R such that αh ≤ γ ≤ βh, (4) or, in other words, h constitutes a tight or exact (θ) bound on the VC dimension γ.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 4,
      "context" : "In [5], we further show that the optimization problem (5) may be reduced to the linear programming problem",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 7,
      "context" : "Lin and Wang proposed fuzzy SVMs in [8], wherein they suggested that each sample be associated with a fuzzy membership si.",
      "startOffset" : 36,
      "endOffset" : 39
    }, {
      "referenceID" : 7,
      "context" : "Fuzzy membership values were computed by using the approach outlined in [8].",
      "startOffset" : 72,
      "endOffset" : 75
    }, {
      "referenceID" : 9,
      "context" : "In order to evaluate the FMCM, we chose a number of benchmark datasets from the UCI machine learning repository [10].",
      "startOffset" : 112,
      "endOffset" : 116
    }, {
      "referenceID" : 4,
      "context" : "The results of the classical MCM have not been duplicated from [5] for the sake of brevity; an added reason is that a fair comparison would be between two methods that use a fuzzy methodology.",
      "startOffset" : 63,
      "endOffset" : 66
    } ],
    "year" : 2015,
    "abstractText" : "The Vapnik-Chervonenkis (VC) dimension measures the complexity of a learning machine, and a low VC dimension leads to good generalization. The recently proposed Minimal Complexity Machine (MCM) learns a hyperplane classifier by minimizing an exact bound on the VC dimension. This paper extends the MCM classifier to the fuzzy domain. The use of a fuzzy membership is known to reduce the effect of outliers, and to reduce the effect of noise on learning. Experimental results show, that on a number of benchmark datasets, the the fuzzy MCM classifier outperforms SVMs and the conventional MCM in terms of generalization, and that the fuzzy MCM uses fewer support vectors. On several benchmark datasets, the fuzzy MCM classifier yields excellent test set accuracies while using one-tenth the number of support vectors used by SVMs.",
    "creator" : "LaTeX with hyperref package"
  }
}