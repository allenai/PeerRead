{
  "name" : "1701.06549.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Learning to Decode for Future Success",
    "authors" : [ "Jiwei Li", "Will Monroe", "Dan Jurafsky" ],
    "emails" : [ "jiweil@stanford.edu", "wmonroe4@stanford.edu", "jurafsky@stanford.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Neural generation models (Sutskever et al., 2014; Bahdanau et al., 2015; Cho et al., 2014; Kalchbrenner and Blunsom, 2013) learn to map source to target sequences in applications such as machine translation (Sennrich et al., 2015; Gulcehre et al., 2015), conversational response generation (Vinyals and Le, 2015; Sordoni et al., 2015), abstractive summarization (Nallapati et al., 2016; Rush et al., 2015).\nNeural generation models are standardly trained by maximizing the likelihood of target sequences given source sequences in a training dataset. At test time, a decoder incrementally generates a sequence with the highest probability using search strategies such as beam search. This locally incremental nature of the decoding model leads to the following issueL Decoders cannot be tailored to\ngenerate target sequences with specific properties of interest, such as pre-specified length constraints (Shao et al., 2017; Shi et al., 2016), which might be useful in tasks like conversational response generation or non-factoid question answering, and cannot deal with important objectives, such as the mutual information between sources and targets (Li et al., 2016a), that require knowing the full target sequence in advance.\nTo address this issue, we propose a general strategy that allows the decoder to incrementally generate output sequences that, when complete, will have specific properties of interest. Such properties can take various forms, such as length, diversity, mutual information between sources and targets, and BLEU/ROUGE scores. The proposed framework integrates two models: the standard seq2seq model, trained to incrementally predict the next token, and a future output estimation model, trained to estimate future properties solely from a prefix string (or the representation associated with this string), and incorporated into the decoder to encourage it to make decisions that lead to better long-term future outcomes.\nMaking decoding decisions based on future success resembles the central idea of reinforcement learning (RL), that of training a policy that leads to better long-term reward. Our work is thus related to a variety of recent work inspired by or using reinforcement learning (e.g., REINFORCE or actorcritic models) for sequence generation (Wiseman and Rush, 2016; Shen et al., 2015; Bahdanau et al., 2016; Ranzato et al., 2016). The proposed model can be viewed as a simpler but more effective version of the actor-critic RL model (?) in sequence generation: it does not rely on the critic to update the policy (the actor), but rather, uses a linear interpolation of the actor (the policy) and the critic (the value function) to make final decisions. Such a strategy comes with the following benefits: (1) It naturally avoids the known problems such as large ar X iv :1\n70 1.\n06 54\n9v 2\n[ cs\n.C L\n] 3\nF eb\n2 01\n7\nvariance and instability with the use of reinforcement learning in tasks with enormous search spaces like sequence generation. As will be shown in the experiment sections, the simplified take on reinforcement without policy updates yields consistent improvements, not only outperforming standard SEQ2SEQ models, but also the RL models themselves in a wide range of sequence generation tasks; (2) training RL-based generation models using specific features like sequence length as rewards not only increases the model’s instability but may also lead to suboptimal generated utterances, for example, sequences that satisfy a length constraint but are irrelevant, incoherent or even ungrammatical.1\nWe study how to incorporate different properties into the decoder different properties of the future output sequence: (1) sequence length: the approach provides the flexibility of controlling the output length, which in turns addresses sequence models’ bias towards generating short sequences (Sountsov and Sarawagi, 2016); (2) mutual information between sources and targets: the approach enables modeling the bidirectional dependency between sources and targets at each decoding timestep, significantly improving response quality on a task of conversational response generation and (3) the properties can also take the form of the BLEU and ROUGE scores, yielding consistent improvements in machine translation and summarization, yielding the state-of-the-art result on the IWSLT German-English translation task."
    }, {
      "heading" : "2 Model Overview",
      "text" : "In this section, we first review the basics of training and decoding in standard neural generation models. Then we give a sketch of the proposed model."
    }, {
      "heading" : "2.1 Basics",
      "text" : "Neural sequence-to-sequence (SEQ2SEQ) generation models aim to generate a sequence of tokens Y given input sequence X . Using recurrent nets, LSTMs (Hochreiter and Schmidhuber, 1997) or CNNs (Krizhevsky et al., 2012; Kim, 2014), X is first mapped to a vector representation, which is then used as the initial input to the decoder. A neural generation model defines a distribution over outputs by sequentially predicting tokens using a\n1A workaround is to use the linear interpolation of the MLE-based policy and the value function for a specific property as a reward for RL training. This strategy comes with the following disadvantages: it requires training different models for different interpolation weights, and again, suffers from large training variance.\nsoftmax function:\np(Y |X) = nY∏ t=1 p(yt|X, y1:t−1)\nDecoding typically seeks to find the maximumprobability sequence Y ∗ given input X:\nY ∗ = argmax Y p(Y |X) (1)\nThe softmax function that computes p(yt|X, y1:t−1) takes as input the hidden representation at time step t− 1, denoted by ht−1. The hidden representation ht−1 is computed using a recurrent net that combines the previously built representation ht−2 and the word representation et−1 for word yt−1. It is infeasible to enumerate the large space of possible sequence outputs, so beam search is normally employed to find an approximately optimal solution. Given a partially generated sequence y1:t−1, the score for choosing token yt (denoted by S(yt)) is thus given by\nS(yt) = log p(yt|ht−1) (2)"
    }, {
      "heading" : "2.2 The Value Function Q",
      "text" : "The core of the proposed architecture is to train a future outcome prediction function (or value function) Q, which estimates the future outcome of taking an action (choosing a token) yt in the present. The function Q is then incorporated into S(yt) at each decoding step to push the model to generate outputs that lead to future success. This yields the following definition for the score S(yt) of taking action yt:\nS(yt) = log p(yt|ht−1) + γQ(X, y1:t) (3)\nwhere γ denotes the hyperparameter controlling the trade-off between the local probability prediction p(yt|ht−1) and the value function Q(X, y1:t). The input to Q can take various forms, such as the vector representation of the decoding step after yt has been considered (i.e., ht) or the raw strings (X and y1:t).2\nQ can be trained either jointly with or independently of the SEQ2SEQ model. When training Q, we provide it with source-target pairs (X,Y ), where Y is a full sequence. Y = {y1, y2, ..., yN} can either be sampled or decoded using a trained model (making Q dependent on the pre-trained\n2One can think of ht as the output of a function that takes as input X and y1:t.\nSEQ2SEQ model) or can be taken from the training set (making Q independent of the SEQ2SEQ model). However, Y must always be a full sequence. The future outcome of generating each of the tokens of Y (y1, y2, ..., yN ) is the feature score (BLEU, length, mutual information, etc.) associated with the full sequence Y , denoted by q(Y ). The future outcome function Q is trained to predict q(Y ) from (X, y1:t), where 1 ≤ t ≤ N . Q estimates the long-term outcome of taking an action yt. It is thus similar to the value function in Q-learning, the role of the critic in actor-critic reinforcement learning (Sutton, 1988; Grondman et al., 2012), the value network for position evaluation in the Monte-Carlo tree search of AlphaGo (Silver et al., 2016), or the h* function in A∗ search. (Och et al., 2001).\nIn this paper, value function and future outcome prediction function and Q are interchangeable\nIn the sections below, we will describe how to adapt this general framework to various features with different properties and different kinds of input to the future outcome prediction function."
    }, {
      "heading" : "3 Q for Controlling Sequence Length",
      "text" : "For tasks like machine translation, abstractive summarization and image caption generation, the information required to generate the target sequences is already embedded in the input. Usually we don’t have to worry about the length of targets, since the model can figure it out itself; this is a known, desirable property of neural generation models (Shi et al., 2016).\nHowever, for tasks like conversational response generation and non-factoid question answering, in which there is no single correct answer, it is useful to be able to control the length of the targets. Additionally, in tasks like conversational response generation, SEQ2SEQ models have a strong bias towards generating short sequences (Sountsov and Sarawagi, 2016). This is because the standard search algorithm at decoding time can only afford to explore very a small action space. As decoding proceeds, only a small number of hypotheses can be maintained. By Zipf’s law, short sequences are significantly more frequent than longer ones. Therefore, the prefixes of shorter responses are usually assigned higher probability by the model. This makes prefixes of longer sequences fall off the beam after a few decoding steps, leaving only short sequences.\nOne can still force the model to keep generating\ntokens (simply by prohibiting the EOS token and forcing the decoding to proceed). However, since the previous decoded tokens were chosen with a shorter sentence in mind, artificially lengthening the response this way will result in low-quality responses. In particular, problems arise with repetition (“no, no, no, no, no, no”) or incoherence (“i like fish, but i don’t like fish but I do like fish”)."
    }, {
      "heading" : "3.1 Training Q for Sequence Length",
      "text" : "Shao et al. (2017) give one efficient method of generating long sequences, consisting of a stochastic search algorithm and segment-by-segment reranking of hypotheses. The fundamental idea is to keep a diverse list of hypotheses on the beam and remove those that are similar to each other, so as to explore the space more adequately. While more adequately exploring the search space can increase the likelihood of generating long sequences, since the beam is more likely to include a prefix of a long sequence, this method doesn’t offer direct control over sequence length. Length information seems to be embedded in the hidden representations of neural models in some implicit way (Shi et al., 2016). We therefore build another neural model to expose this length information and use it to estimate the number of words left to be generated.\nGiven a pre-trained sequence-to-sequence model, an input sequence X , and a target Y = {y1, y2, ..., yN}, where N denotes the length of y, we first run a forward pass to compute the hidden representation ht associated with each time step on the target side (1 ≤ t ≤ N ). Then we build a regression model Q(ht), which takes as input ht to predict the length of the remaining sequence, i.e., N − t. The model first passes ht to two non-linear layers, on top of which is a linear regression model which outputs the predicted number of tokens left to decode. The regression model is optimized by minimizing the mean squared loss between the predicted sequence length and the goldstandard length N − t on source–target pairs taken from the training set."
    }, {
      "heading" : "3.2 Decoding",
      "text" : "Given an inputX , suppose that we wish to generate a target sequence of a pre-specified length N . At decoding time step t− 1, we first obtain the vector representation ht−1 for the current time step. The score used to rank choices for the next token yt is a linear combination of the log probability outputted from the sequence model and the mean square loss\nbetween the number of words left to generate (N − t) and the output from Q(ht):\nyt = argmax y\nlog p(y1:t|X)\n− λ||(N − t)−Q(ht)||2 (4)\nwhere y1:t is the concatenation of yt and previously decoded (given) sequence y1:t−1, and ht is obtained from the sequence model by combining ht−1 and the word representation of yt as if it were the true next token. λ is a hyperparameter controlling the influence of the future length estimator."
    }, {
      "heading" : "3.3 Experiments",
      "text" : "We evaluate the proposed model on the task of open-domain conversation response generation (Vinyals and Le, 2015; Sordoni et al., 2015; Serban et al., 2015a; Mei et al., 2016; Serban et al., 2015b), in which a model must predict the next turn of a dialogue given the preceding ones. We use the OpenSubtitles (OSDb) dataset (Tiedemann, 2009).\nWe compare the proposed model with the standard SEQ2SEQ beam search (SBS) decoder. We first group test dialogue pairs by target length and decode each group. At decoding time, for an input with a gold target of length L, we force the model to generate an output of length L. This can be achieved by selecting a hypothesis that predicts an EOS token at time step L + 1.3 If no EOS is predicted at time step L + 1, we continue decoding and stop once an EOS token is generated. We report BLEU scores on the concatenation of the\n3If multiple hypotheses satisfy this requirement, we pick the one with the largest likelihood.\noutputs from each length cluster.4\nWe also report adversarial success (AdverSuc) and machine-vs-random accuracy, evaluation metrics proposed in Li et al. (2016c). Adversarial success refers to the percentage of machine-generated responses that are able to fool a trained evaluator model into believing that they are generated by a human; machine-vs-random accuracy denotes the accuracy of a (different) trained evaluator model at distinguishing between machinegenerated responses and randomly-sampled responses.5 Higher values of adversarial success and machine-vs-random accuracy indicate the superiority of a model. We refer readers to Li et al. (2016c) for more details. Table 2 presents the quantitative results: adding predictor rankers increases the general quality of generated responses.\nSampled responses (from a random batch, without cherry-picking) are shown in Table 1, with more examples shown in Table 7 in the Appendix. We force the decoding model to generate 20 tokens using the strategy described above. We can\n4In this setup, both algorithms are allowed to know the length of the gold-standard targets. The results from different models are thus comparable. This is to remove the effect of target length on the evaluation metrics (all metrics employed are sensitive to target length).\n5The estimators for AdverSuc and machine-vs-random accuracy are trained using a hierarchical network (Serban et al., 2016) See Li et al. (2016c) for details.\nclearly identify problems with the standard beam search algorithm: the decoder produces tokens that are optimal for shorter sequences, eliminating candidates from the beam that would lead to longer possibilities. Once the length reaches the point where the intended shorter sequence would naturally conclude, it has no option but to fill space with repetitions of tokens (e.g., “go, go, go” or strings of punctuation) and phrases (e.g., i don ’t know who i am or where i am or where i am or where i am), or addenda that are sometimes contradictory (e.g., it was the only way to make it to the top of the island . . . . . . but it wasn ’t). This issue is alleviated by the proposed length-prediction algorithm, which plans ahead and chooses tokens that lead to meaningful sequences with the desired length. More coherent responses are observed when the hyperparameter λ is set to 1 than when it is set to 5, as expected, since the decoding algorithm deviates more from the pre-trained model when λ takes larger values."
    }, {
      "heading" : "4 Q for Mutual Information",
      "text" : ""
    }, {
      "heading" : "4.1 Background",
      "text" : "Maximum mutual information (MMI) has been shown to be better than maximum likelihood estimation (MLE) as an decoding objective for conversational response generation tasks (Li et al., 2016a). The mutual information between source X and target Y is given by log[p(X,Y )/p(X)p(Y )],\nwhich measures bidirectional dependency between sources and targets, as opposed to the unidirectional dependency of targets on sources in the maximum likelihood objective. Modeling the bidirectional dependency between sources and targets reduces the prevalence of generic responses and leads to more diverse and interesting conversations.6 Maximizing a weighted generalization of mutual information between the source and target can be shown using Bayes’ rule to be equivalent to maximizing a linear combination of the forward probability log p(Y |X) (the standard objective function for SEQ2SEQ models) and the backward probability log p(X|Y ):7\nY ∗ = argmax Y log p(Y |X) + λ log p(X|Y ) (5)\nUnfortunately, direct decoding using Eq.5 is infeasible, since it requires completion of target generation before p(X|Y ) can be effectively computed, and the enormous search space for target y prevents exploring all possibilities. An approximation approach is commonly adopted, in which an Nbest list is first generated based on p(Y |X) and then reranked by adding p(X|Y ). The problem with this reranking strategy is that the beam search\n6This is because although it is easy to produce a sensible generic response Y regardless of the input sequence X , it is much harder to guess X given Y if Y is generic.\n7When using this objective, p(Y |X) and p(X|Y ) are separately trained models with different sets of parameters.\nstep gives higher priority to optimizing the forward probability, resulting in solutions that are not globally optimal. Since hypotheses in beam search are known to lack diversity (Li et al., 2016b; Vijayakumar et al., 2016; Gimpel et al., 2013), after decoding is finished, it is sometimes too late for the reranking model to have significant impact. Shao et al. (2017) confirm this problem and show that the reranking approach helps for short sequences but not longer ones."
    }, {
      "heading" : "4.2 Training Q for Mutual Information",
      "text" : "The first term of Eq. 5 is the same as standard SEQ2SEQ decoding. We thus focus our attention on the second term, log p(X|Y ). To incorporate the backward probability into intermediate decoding steps, we use a model to estimate the future value of p(X|Y ) when generating each token yt.\nFor example, suppose that we have a sourcetarget pair with source X = “what ’s your name” and target Y = “my name is john”. The future backward probability of the partial sequences “my”, “my name”, “my name is” is thus p(X|Y ). Again, we use Q(yt) to denote the function that maps a partially generated sequence to its future backward probability, and we can factorize Eq. 5 as follows:\nyt = argmax y\nlog p(y1:t−1, y|X) + λQ(y) (6)\nWe propose two ways to obtain the future backward-probability estimation function Q(yt).\n(1) As in the strategies described in Sections 5 and 3, we first pretrain a SEQ2SEQ model for both p(Y |X) and p(X|Y ). The training of the latter is the same as a standard SEQ2SEQ model but with sources and targets swapped. Then we train an additional future backward-probability estimation function Q(X, y1:t), which takes as inputs the hidden representation of intermediate decoding steps (i.e., ht) from the forward probability model and predicts the backward probability for the entire target sequence Y using the pretrained backward SEQ2SEQ model (i.e., log p(X|Y ) with Y being the full target).\n(2) We can directly train models to calculate Q(yt) = p(X|y1:t), i.e., the probability of generating a full source given a partial target. To do this, we first break y into a series of partial sequences, i.e., y1:1, y1:2, ..., y1:N , which is {“i”, “i am”, “i am john”} in the example above. Then we pair each partial sequence y1:t (1 ≤ t ≤ N ) with the\nsource and use each pair (y1:t, X) as a training example to a train SEQ2SEQ model, with y1:t as the source and X as the target. Since we are increasing the size of the training set by roughly a factor of 10 (the average target length is about 10), training is extremely computation-intensive. We reduce the training time by grouping y1:t by length and training a separate SEQ2SEQ model for each length group. At decoding time, we use the score from the model corresponding to the length of the current partially decoded target to generate the next token. Since SEQ2SEQ models for different target lengths are independent, they can be trained in parallel.\nWe find that option (2) generally outperforms option (1), but option (2) requires training in parallel on a large number of machines."
    }, {
      "heading" : "4.3 Experimental Results",
      "text" : "We compare the results for the approach with standard beam search using the MLE objective and the MMI reranking approach of Li et al. (2016a), which performs reranking only after decoding is finished. We report BLEU scores and AdverSuc scores8 on the test set. We also report diversity scores (denoted by Distinct-1 and Distinct-2); these are defined as in Li et al. (2016a) to be the the number of distinct unigrams and bigrams (respectively) in generated responses, divided by the total number of generated tokens (to avoid favoring long sentences). Addi-\n8The machine-vs-random scores for the three models are very similar, respectively 0.947, 0.939, 0.935.\ntionally, we split the dev set into a subset containing longer targets (with length larger than 8) and a subset containing shorter ones (smaller than 8). During decoding, we force the model to generate targets of the same length as the gold standard targets using the strategy described in Section 3.3.\nTable 3 presents quantitative results for the different decoding strategies. On the full test set, the future backward-probability prediction model outperforms the approach of reranking when decoding is fully finished. Specifically, a larger performance improvement is observed on examples with longer targets than on those with shorter ones. This effect is consistent with the intuition that for short responses, due to the relatively smaller search space, doing reranking at the end of decoding is sufficient, whereas this is not the case with longer sequences: as beam search proceeds, a small number of prefixes gradually start to dominate, with hypotheses differing only in punctuation or minor morphological variations. Incorporating mutual information in the early stages of decoding maintains diverse hypotheses, leading to better final results.\nTable 4 presents sampled outputs from each strategy, with more results shown in the Table 8 (Ap-\npendix). As can be seen, the results from reranking are generally better than those from MLE, but sometimes both approaches still generate the same generic outputs. This is due to the fact that reranking is performed only after more interesting outputs have fallen off the beam. Using smaller values of λ, the future backward-probability prediction approach generally yields better results than reranking. When using larger values of λ, the algorithm tends to produce more diverse and interesting outputs but has a greater risk of generating irrelevant responses."
    }, {
      "heading" : "5 Q for BLEU/ROUGE",
      "text" : "The future outcome function Q can be trained to predict arbitrary features. These features include BLEU (Papineni et al., 2002) or ROUGE (Lin, 2004) scores. We thus trainQ to directly predict future BLEU or ROUGE values. In this situation, the future prediction function is able to reduce the discrepancy between training (using maximum likelihood objective) and testing (using BLEU or ROUGE) (Wiseman and Rush, 2016; Shen et al., 2015; Ranzato et al., 2016)."
    }, {
      "heading" : "5.1 Model",
      "text" : "Given a pre-trained sequence generation model, an input sequence X , and a partially decoded sequence y1:t−1, we want to estimate the future reward for taking the action of choosing word yt for the current time-step. We denote this estimate Q({yt, y1:t−1, X}), abbreviated Q(yt) where possible.\nThe future prediction network is trained as follows: we first sample yt from the distribution p(yt|X, y1:t−1), then decode the remainder of the sequence Y using beam search. The future outcome for the action yt is thus the score of the final decoded sequence, q(Y ). Having obtained pairs (q(Y ), {X, y1:t}), we train a neural network model that takes as input X and y1:t to predict q(Y ). The network first maps the input sequence X and the partially decoded sequence y1:t to vector representations using LSTMs, and then uses another network that takes the concatenation of the two vectors to output the final outcome q(Y ). The future prediction network is optimized by minimizing the mean squared loss between the predicted value and the real q(Y ) during training.\nAt decoding time, Q(yt) is incorporated into the decoding model to push the model to take actions that lead to better future outcomes. An action yt is\nthus evaluated by the following function:\nyt = argmax y\nlog p(y1:t−1, y|X) + λQ(y) (7)\nλ is a hyperparameter that is tuned on the development set."
    }, {
      "heading" : "5.2 Experiments",
      "text" : "We evaluate the decoding model on two sequence generation tasks, machine translation and abstractive summarization.\nMachine Translation We use the GermanEnglish machine translation track of the IWSLT 2014 (Cettolo et al., 2014), which consists of sentence-aligned subtitles of TED and TEDx talks. For fair comparison, we followed exactly the data processing protocols defined in Ranzato et al. (2016), which have also been adopted by Bahdanau et al. (2016) and Wiseman and Rush (2016). The training data consists of roughly 150K sentence pairs, in which the average English sentence is 17.5 words long and the average German sentence is 18.5 words long. The test set is a concatenation of dev2010, dev2012, tst2010, tst2011 and tst2012, consisting of 6750 sentence pairs. The English dictionary has 22822 words, while the German has 32009 words.\nWe train two models, a vanilla LSTM (Sutskever et al., 2014) and an attention-based model (Bahdanau et al., 2015). For the attention model, we use the input-feeding model described in Luong et al. (2015) with one minor modification: the weighted attention vectors that are used in the softmax token predictions and those fed to the recurrent net at the next step use different sets of parameters. Their values can therefore be different, unlike in Luong et al. (2015). We find that this small modification significantly improves the capacity of attention models, yielding more than a +1.0 BLEU score improvement. We use structure similar to that of Wiseman and Rush (2016), a single-layer sequenceto-sequence model with 256 units for each layer. We use beam size 7 for both standard beam search (SBS) and future outcome prediction.\nResults are shown in Table 5, with SBS standing for the standard beam search model and future func as the proposed future prediction model. Baselines employed include the REINFORCE model described in Ranzato et al. (2016), the actor-critic RL model described in Bahdanau et al. (2016) and the beam-search training scheme described in Wise-\nman and Rush (2016). Results are reprinted from the best setting in the corresponding paper.\nOur implementation of the attention model itself already achieves state-of-the-art performance on this benchmark. The proposed future outcome model adds +0.4 BLEU, pushing the SOTA performance up to 28.3. Since the trained SEQ2SEQ model is already quite strong, there is less room for improvement. For the vanilla LSTM, however, due to its relative inferiority, we observe a more significant improvement from the future outcome prediction approach.\nAbstractive Summarization We follow the protocols described in Rush et al. (2015), in which the source input is the first sentence of a new article and the target output is the headline. Our training dataset consists of 2M pairs. We train a two-layer word-level attention model with 512 units for each layer. Experimental results are shown in Table 6. We observe a +1.0 ROUGE performance improvement from the proposed model over standard beam search."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this paper, we propose a general strategy that enables a neural decoder to generate outputs that have specific properties of interest. We show how to use a model Q to optimize three useful properties of the output—sequence length, mutual information and BLEU/ROUGE scores —and investigate the effects of different designs for the predictor model and decoding algorithm. Our model provides a general and easy-to-implement way to control neural generation models to meet their specific needs, while improving results on a variety of generation tasks."
    } ],
    "references" : [ {
      "title" : "An actor-critic algorithm for sequence prediction",
      "author" : [ "Dzmitry Bahdanau", "Philemon Brakel", "Kelvin Xu", "Anirudh Goyal", "Ryan Lowe", "Joelle Pineau", "Aaron Courville", "Yoshua Bengio." ],
      "venue" : "arXiv preprint arXiv:1607.07086 .",
      "citeRegEx" : "Bahdanau et al\\.,? 2016",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2016
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "Proceedings of the International Conference on Learning Representations (ICLR).",
      "citeRegEx" : "Bahdanau et al\\.,? 2015",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2015
    }, {
      "title" : "Report on the 11th IWSLT evaluation campaign, IWSLT 2014",
      "author" : [ "Mauro Cettolo", "Jan Niehues", "Sebastian Stüker", "Luisa Bentivogli", "Marcello Federico." ],
      "venue" : "Proceedings of the International Workshop on Spoken Language Translation, Hanoi, Vietnam.",
      "citeRegEx" : "Cettolo et al\\.,? 2014",
      "shortCiteRegEx" : "Cettolo et al\\.",
      "year" : 2014
    }, {
      "title" : "Microsoft COCO captions: Data collection and evaluation server",
      "author" : [ "Xinlei Chen", "Hao Fang", "Tsung-Yi Lin", "Ramakrishna Vedantam", "Saurabh Gupta", "Piotr Dollár", "C Lawrence Zitnick." ],
      "venue" : "arXiv preprint arXiv:1504.00325 .",
      "citeRegEx" : "Chen et al\\.,? 2015",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2015
    }, {
      "title" : "Noisy parallel approximate decoding for conditional recurrent language model",
      "author" : [ "Kyunghyun Cho." ],
      "venue" : "arXiv preprint arXiv:1605.03835 .",
      "citeRegEx" : "Cho.,? 2016",
      "shortCiteRegEx" : "Cho.",
      "year" : 2016
    }, {
      "title" : "Learning phrase representations using RNN encoder-decoder for statistical machine translation",
      "author" : [ "Kyunghyun Cho", "Bart Van Merriënboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio." ],
      "venue" : "arXiv preprint",
      "citeRegEx" : "Cho et al\\.,? 2014",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2014
    }, {
      "title" : "Abstractive sentence summarization with attentive recurrent neural networks",
      "author" : [ "Sumit Chopra", "Michael Auli", "Alexander M. Rush." ],
      "venue" : "NAACL-HLT .",
      "citeRegEx" : "Chopra et al\\.,? 2016",
      "shortCiteRegEx" : "Chopra et al\\.",
      "year" : 2016
    }, {
      "title" : "A systematic exploration of diversity in machine translation",
      "author" : [ "Kevin Gimpel", "Dhruv Batra", "Chris Dyer", "Gregory Shakhnarovich." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Gimpel et al\\.,? 2013",
      "shortCiteRegEx" : "Gimpel et al\\.",
      "year" : 2013
    }, {
      "title" : "A survey of actor-critic reinforcement learning: Standard and natural policy gradients",
      "author" : [ "Ivo Grondman", "Lucian Busoniu", "Gabriel AD Lopes", "Robert Babuska." ],
      "venue" : "IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)",
      "citeRegEx" : "Grondman et al\\.,? 2012",
      "shortCiteRegEx" : "Grondman et al\\.",
      "year" : 2012
    }, {
      "title" : "On using monolingual corpora in neural machine translation",
      "author" : [ "Caglar Gulcehre", "Orhan Firat", "Kelvin Xu", "Kyunghyun Cho", "Loic Barrault", "Huei-Chi Lin", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio." ],
      "venue" : "arXiv preprint arXiv:1503.03535 .",
      "citeRegEx" : "Gulcehre et al\\.,? 2015",
      "shortCiteRegEx" : "Gulcehre et al\\.",
      "year" : 2015
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural Computation 9(8):1735–1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Recurrent continuous translation models",
      "author" : [ "Nal Kalchbrenner", "Phil Blunsom." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Kalchbrenner and Blunsom.,? 2013",
      "shortCiteRegEx" : "Kalchbrenner and Blunsom.",
      "year" : 2013
    }, {
      "title" : "Convolutional neural networks for sentence classification",
      "author" : [ "Yoon Kim." ],
      "venue" : "arXiv preprint arXiv:1408.5882 .",
      "citeRegEx" : "Kim.,? 2014",
      "shortCiteRegEx" : "Kim.",
      "year" : 2014
    }, {
      "title" : "ImageNet classification with deep convolutional neural networks",
      "author" : [ "Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton." ],
      "venue" : "NIPS.",
      "citeRegEx" : "Krizhevsky et al\\.,? 2012",
      "shortCiteRegEx" : "Krizhevsky et al\\.",
      "year" : 2012
    }, {
      "title" : "2016a. A diversity-promoting objective function for neural conversation models",
      "author" : [ "Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan" ],
      "venue" : null,
      "citeRegEx" : "Li et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2016
    }, {
      "title" : "A simple, fast diverse decoding algorithm for neural generation",
      "author" : [ "Jiwei Li", "Will Monroe", "Dan Jurafsky." ],
      "venue" : "arXiv preprint arXiv:1611.08562 .",
      "citeRegEx" : "Li et al\\.,? 2016b",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2016
    }, {
      "title" : "Adversarial reinforcement learning for neural dialogue generation",
      "author" : [ "Jiwei Li", "Will Monroe", "Tianlin Shi", "Dan Jurafsky" ],
      "venue" : null,
      "citeRegEx" : "Li et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2016
    }, {
      "title" : "ROUGE: A package for automatic evaluation of summaries",
      "author" : [ "Chin-Yew Lin." ],
      "venue" : "Text Summarization Branches Out: Proceedings of the ACL-04 Workshop.",
      "citeRegEx" : "Lin.,? 2004",
      "shortCiteRegEx" : "Lin.",
      "year" : 2004
    }, {
      "title" : "LSTM based conversation models",
      "author" : [ "Yi Luan", "Yangfeng Ji", "Mari Ostendorf." ],
      "venue" : "arXiv preprint arXiv:1603.09457 .",
      "citeRegEx" : "Luan et al\\.,? 2016",
      "shortCiteRegEx" : "Luan et al\\.",
      "year" : 2016
    }, {
      "title" : "Effective approaches to attentionbased neural machine translation",
      "author" : [ "Minh-Thang Luong", "Hieu Pham", "Christopher D Manning." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Luong et al\\.,? 2015",
      "shortCiteRegEx" : "Luong et al\\.",
      "year" : 2015
    }, {
      "title" : "Coherent dialogue with attention-based language models",
      "author" : [ "Hongyuan Mei", "Mohit Bansal", "Matthew R Walter." ],
      "venue" : "arXiv preprint arXiv:1611.06997 .",
      "citeRegEx" : "Mei et al\\.,? 2016",
      "shortCiteRegEx" : "Mei et al\\.",
      "year" : 2016
    }, {
      "title" : "Abstractive text summarization using sequence-to-sequence RNNs and beyond",
      "author" : [ "Ramesh Nallapati", "Bowen Zhou", "Çağlar Gülçehre", "Bing Xiang." ],
      "venue" : "CoNLL.",
      "citeRegEx" : "Nallapati et al\\.,? 2016",
      "shortCiteRegEx" : "Nallapati et al\\.",
      "year" : 2016
    }, {
      "title" : "An efficient A* search algorithm for statistical machine translation",
      "author" : [ "Franz Josef Och", "Nicola Ueffing", "Hermann Ney." ],
      "venue" : "Proceedings of the Workshop on Data-Driven Methods in Machine Translation.",
      "citeRegEx" : "Och et al\\.,? 2001",
      "shortCiteRegEx" : "Och et al\\.",
      "year" : 2001
    }, {
      "title" : "BLEU: A method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu." ],
      "venue" : "ACL.",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "Sequence level training with recurrent neural networks. In ICLR",
      "author" : [ "Marc’Aurelio Ranzato", "Sumit Chopra", "Michael Auli", "Wojciech Zaremba" ],
      "venue" : null,
      "citeRegEx" : "Ranzato et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Ranzato et al\\.",
      "year" : 2016
    }, {
      "title" : "A neural attention model for abstractive sentence summarization",
      "author" : [ "Alexander M Rush", "Sumit Chopra", "Jason Weston." ],
      "venue" : "arXiv preprint arXiv:1509.00685 .",
      "citeRegEx" : "Rush et al\\.,? 2015",
      "shortCiteRegEx" : "Rush et al\\.",
      "year" : 2015
    }, {
      "title" : "Neural machine translation of rare words with subword units",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "arXiv preprint arXiv:1508.07909 .",
      "citeRegEx" : "Sennrich et al\\.,? 2015",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2015
    }, {
      "title" : "Building end-to-end dialogue systems using generative hierarchical neural network models",
      "author" : [ "Iulian V Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau." ],
      "venue" : "arXiv preprint arXiv:1507.04808 .",
      "citeRegEx" : "Serban et al\\.,? 2015a",
      "shortCiteRegEx" : "Serban et al\\.",
      "year" : 2015
    }, {
      "title" : "Hierarchical neural network generative models for movie dialogues",
      "author" : [ "Iulian V Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau." ],
      "venue" : "arXiv preprint arXiv:1507.04808 .",
      "citeRegEx" : "Serban et al\\.,? 2015b",
      "shortCiteRegEx" : "Serban et al\\.",
      "year" : 2015
    }, {
      "title" : "Building end-to-end dialogue systems using generative hierarchical neural network models",
      "author" : [ "Iulian V Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau." ],
      "venue" : "Proceedings of the 30th AAAI Conference on Artificial Intelligence",
      "citeRegEx" : "Serban et al\\.,? 2016",
      "shortCiteRegEx" : "Serban et al\\.",
      "year" : 2016
    }, {
      "title" : "Generating long and diverse responses with neural conversational models",
      "author" : [ "Louis Shao", "Stephan Gouws", "Denny Britz", "Anna Goldie", "Brian Strope", "Ray Kurzweil." ],
      "venue" : "arXiv preprint arXiv:1701.03185 .",
      "citeRegEx" : "Shao et al\\.,? 2017",
      "shortCiteRegEx" : "Shao et al\\.",
      "year" : 2017
    }, {
      "title" : "Minimum risk training for neural machine translation",
      "author" : [ "Shiqi Shen", "Yong Cheng", "Zhongjun He", "Wei He", "Hua Wu", "Maosong Sun", "Yang Liu." ],
      "venue" : "arXiv preprint arXiv:1512.02433 .",
      "citeRegEx" : "Shen et al\\.,? 2015",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2015
    }, {
      "title" : "Why neural translations are the right length",
      "author" : [ "Xing Shi", "Kevin Knight", "Deniz Yuret." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Shi et al\\.,? 2016",
      "shortCiteRegEx" : "Shi et al\\.",
      "year" : 2016
    }, {
      "title" : "Mastering the game of Go with deep neural networks and tree",
      "author" : [ "David Silver", "Aja Huang", "Chris J Maddison", "Arthur Guez", "Laurent Sifre", "George Van Den Driessche", "Julian Schrittwieser", "Ioannis Antonoglou", "Veda Panneershelvam", "Marc Lanctot" ],
      "venue" : null,
      "citeRegEx" : "Silver et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Silver et al\\.",
      "year" : 2016
    }, {
      "title" : "A neural network approach to context-sensitive generation of conversational responses",
      "author" : [ "Alessandro Sordoni", "Michel Galley", "Michael Auli", "Chris Brockett", "Yangfeng Ji", "Margaret Mitchell", "Jian-Yun Nie", "Jianfeng Gao", "Bill Dolan." ],
      "venue" : "arXiv preprint",
      "citeRegEx" : "Sordoni et al\\.,? 2015",
      "shortCiteRegEx" : "Sordoni et al\\.",
      "year" : 2015
    }, {
      "title" : "Length bias in encoder decoder models and a case for global conditioning",
      "author" : [ "Pavel Sountsov", "Sunita Sarawagi." ],
      "venue" : "arXiv preprint arXiv:1606.03402 .",
      "citeRegEx" : "Sountsov and Sarawagi.,? 2016",
      "shortCiteRegEx" : "Sountsov and Sarawagi.",
      "year" : 2016
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc V Le." ],
      "venue" : "NIPS.",
      "citeRegEx" : "Sutskever et al\\.,? 2014",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "Learning to predict by the methods of temporal differences",
      "author" : [ "Richard S Sutton." ],
      "venue" : "Machine learning 3(1):9–44.",
      "citeRegEx" : "Sutton.,? 1988",
      "shortCiteRegEx" : "Sutton.",
      "year" : 1988
    }, {
      "title" : "News from OPUS – A collection of multilingual parallel corpora with tools and interfaces",
      "author" : [ "Jörg Tiedemann." ],
      "venue" : "Recent Advances in Natural Language Processing. volume 5, pages 237–248.",
      "citeRegEx" : "Tiedemann.,? 2009",
      "shortCiteRegEx" : "Tiedemann.",
      "year" : 2009
    }, {
      "title" : "Diverse beam",
      "author" : [ "Ashwin K Vijayakumar", "Michael Cogswell", "Ramprasath R Selvaraju", "Qing Sun", "Stefan Lee", "David Crandall", "Dhruv Batra" ],
      "venue" : null,
      "citeRegEx" : "Vijayakumar et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Vijayakumar et al\\.",
      "year" : 2016
    }, {
      "title" : "A neural conversational model",
      "author" : [ "Oriol Vinyals", "Quoc Le." ],
      "venue" : "Proceedings of the ICML Deep Learning Workshop.",
      "citeRegEx" : "Vinyals and Le.,? 2015",
      "shortCiteRegEx" : "Vinyals and Le.",
      "year" : 2015
    }, {
      "title" : "Sequence-to-sequence learning as beam-search optimization",
      "author" : [ "Sam Wiseman", "Alexander M Rush." ],
      "venue" : "arXiv preprint arXiv:1606.02960 .",
      "citeRegEx" : "Wiseman and Rush.,? 2016",
      "shortCiteRegEx" : "Wiseman and Rush.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 36,
      "context" : "Neural generation models (Sutskever et al., 2014; Bahdanau et al., 2015; Cho et al., 2014; Kalchbrenner and Blunsom, 2013) learn to map source to target sequences in applications such as machine translation (Sennrich et al.",
      "startOffset" : 25,
      "endOffset" : 122
    }, {
      "referenceID" : 1,
      "context" : "Neural generation models (Sutskever et al., 2014; Bahdanau et al., 2015; Cho et al., 2014; Kalchbrenner and Blunsom, 2013) learn to map source to target sequences in applications such as machine translation (Sennrich et al.",
      "startOffset" : 25,
      "endOffset" : 122
    }, {
      "referenceID" : 5,
      "context" : "Neural generation models (Sutskever et al., 2014; Bahdanau et al., 2015; Cho et al., 2014; Kalchbrenner and Blunsom, 2013) learn to map source to target sequences in applications such as machine translation (Sennrich et al.",
      "startOffset" : 25,
      "endOffset" : 122
    }, {
      "referenceID" : 11,
      "context" : "Neural generation models (Sutskever et al., 2014; Bahdanau et al., 2015; Cho et al., 2014; Kalchbrenner and Blunsom, 2013) learn to map source to target sequences in applications such as machine translation (Sennrich et al.",
      "startOffset" : 25,
      "endOffset" : 122
    }, {
      "referenceID" : 26,
      "context" : ", 2014; Kalchbrenner and Blunsom, 2013) learn to map source to target sequences in applications such as machine translation (Sennrich et al., 2015; Gulcehre et al., 2015), conversational response generation (Vinyals and Le, 2015; Sordoni et al.",
      "startOffset" : 124,
      "endOffset" : 170
    }, {
      "referenceID" : 9,
      "context" : ", 2014; Kalchbrenner and Blunsom, 2013) learn to map source to target sequences in applications such as machine translation (Sennrich et al., 2015; Gulcehre et al., 2015), conversational response generation (Vinyals and Le, 2015; Sordoni et al.",
      "startOffset" : 124,
      "endOffset" : 170
    }, {
      "referenceID" : 40,
      "context" : ", 2015), conversational response generation (Vinyals and Le, 2015; Sordoni et al., 2015), abstractive summarization (Nallapati et al.",
      "startOffset" : 44,
      "endOffset" : 88
    }, {
      "referenceID" : 34,
      "context" : ", 2015), conversational response generation (Vinyals and Le, 2015; Sordoni et al., 2015), abstractive summarization (Nallapati et al.",
      "startOffset" : 44,
      "endOffset" : 88
    }, {
      "referenceID" : 21,
      "context" : ", 2015), abstractive summarization (Nallapati et al., 2016; Rush et al., 2015).",
      "startOffset" : 35,
      "endOffset" : 78
    }, {
      "referenceID" : 25,
      "context" : ", 2015), abstractive summarization (Nallapati et al., 2016; Rush et al., 2015).",
      "startOffset" : 35,
      "endOffset" : 78
    }, {
      "referenceID" : 30,
      "context" : "This locally incremental nature of the decoding model leads to the following issueL Decoders cannot be tailored to generate target sequences with specific properties of interest, such as pre-specified length constraints (Shao et al., 2017; Shi et al., 2016), which might be useful in tasks like conversational response generation or non-factoid question answering, and cannot deal with important objectives, such as the mutual information between sources and targets (Li et al.",
      "startOffset" : 220,
      "endOffset" : 257
    }, {
      "referenceID" : 32,
      "context" : "This locally incremental nature of the decoding model leads to the following issueL Decoders cannot be tailored to generate target sequences with specific properties of interest, such as pre-specified length constraints (Shao et al., 2017; Shi et al., 2016), which might be useful in tasks like conversational response generation or non-factoid question answering, and cannot deal with important objectives, such as the mutual information between sources and targets (Li et al.",
      "startOffset" : 220,
      "endOffset" : 257
    }, {
      "referenceID" : 41,
      "context" : ", REINFORCE or actorcritic models) for sequence generation (Wiseman and Rush, 2016; Shen et al., 2015; Bahdanau et al., 2016; Ranzato et al., 2016).",
      "startOffset" : 59,
      "endOffset" : 147
    }, {
      "referenceID" : 31,
      "context" : ", REINFORCE or actorcritic models) for sequence generation (Wiseman and Rush, 2016; Shen et al., 2015; Bahdanau et al., 2016; Ranzato et al., 2016).",
      "startOffset" : 59,
      "endOffset" : 147
    }, {
      "referenceID" : 0,
      "context" : ", REINFORCE or actorcritic models) for sequence generation (Wiseman and Rush, 2016; Shen et al., 2015; Bahdanau et al., 2016; Ranzato et al., 2016).",
      "startOffset" : 59,
      "endOffset" : 147
    }, {
      "referenceID" : 24,
      "context" : ", REINFORCE or actorcritic models) for sequence generation (Wiseman and Rush, 2016; Shen et al., 2015; Bahdanau et al., 2016; Ranzato et al., 2016).",
      "startOffset" : 59,
      "endOffset" : 147
    }, {
      "referenceID" : 35,
      "context" : "(Sountsov and Sarawagi, 2016); (2) mutual information between sources and targets: the approach enables modeling the bidirectional dependency between sources and targets at each decoding timestep, significantly improving response quality on a task of conversational response generation and (3) the properties can also take the form of the BLEU and ROUGE scores, yielding consistent improvements in machine translation and summarization, yielding the state-of-the-art result on the IWSLT German-English translation task.",
      "startOffset" : 0,
      "endOffset" : 29
    }, {
      "referenceID" : 10,
      "context" : "Using recurrent nets, LSTMs (Hochreiter and Schmidhuber, 1997) or CNNs (Krizhevsky et al.",
      "startOffset" : 28,
      "endOffset" : 62
    }, {
      "referenceID" : 13,
      "context" : "Using recurrent nets, LSTMs (Hochreiter and Schmidhuber, 1997) or CNNs (Krizhevsky et al., 2012; Kim, 2014), X is first mapped to a vector representation, which is then used as the initial input to the decoder.",
      "startOffset" : 71,
      "endOffset" : 107
    }, {
      "referenceID" : 12,
      "context" : "Using recurrent nets, LSTMs (Hochreiter and Schmidhuber, 1997) or CNNs (Krizhevsky et al., 2012; Kim, 2014), X is first mapped to a vector representation, which is then used as the initial input to the decoder.",
      "startOffset" : 71,
      "endOffset" : 107
    }, {
      "referenceID" : 37,
      "context" : "It is thus similar to the value function in Q-learning, the role of the critic in actor-critic reinforcement learning (Sutton, 1988; Grondman et al., 2012), the value network for position evaluation in the Monte-Carlo tree search of AlphaGo (Silver et al.",
      "startOffset" : 118,
      "endOffset" : 155
    }, {
      "referenceID" : 8,
      "context" : "It is thus similar to the value function in Q-learning, the role of the critic in actor-critic reinforcement learning (Sutton, 1988; Grondman et al., 2012), the value network for position evaluation in the Monte-Carlo tree search of AlphaGo (Silver et al.",
      "startOffset" : 118,
      "endOffset" : 155
    }, {
      "referenceID" : 33,
      "context" : ", 2012), the value network for position evaluation in the Monte-Carlo tree search of AlphaGo (Silver et al., 2016), or the h* function in A∗ search.",
      "startOffset" : 93,
      "endOffset" : 114
    }, {
      "referenceID" : 22,
      "context" : "(Och et al., 2001).",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 32,
      "context" : "Usually we don’t have to worry about the length of targets, since the model can figure it out itself; this is a known, desirable property of neural generation models (Shi et al., 2016).",
      "startOffset" : 166,
      "endOffset" : 184
    }, {
      "referenceID" : 35,
      "context" : "Additionally, in tasks like conversational response generation, SEQ2SEQ models have a strong bias towards generating short sequences (Sountsov and Sarawagi, 2016).",
      "startOffset" : 133,
      "endOffset" : 162
    }, {
      "referenceID" : 32,
      "context" : "Length information seems to be embedded in the hidden representations of neural models in some implicit way (Shi et al., 2016).",
      "startOffset" : 108,
      "endOffset" : 126
    }, {
      "referenceID" : 40,
      "context" : "(Vinyals and Le, 2015; Sordoni et al., 2015; Serban et al., 2015a; Mei et al., 2016; Serban et al., 2015b), in which a model must predict the next turn of a dialogue given the preceding ones.",
      "startOffset" : 0,
      "endOffset" : 106
    }, {
      "referenceID" : 34,
      "context" : "(Vinyals and Le, 2015; Sordoni et al., 2015; Serban et al., 2015a; Mei et al., 2016; Serban et al., 2015b), in which a model must predict the next turn of a dialogue given the preceding ones.",
      "startOffset" : 0,
      "endOffset" : 106
    }, {
      "referenceID" : 27,
      "context" : "(Vinyals and Le, 2015; Sordoni et al., 2015; Serban et al., 2015a; Mei et al., 2016; Serban et al., 2015b), in which a model must predict the next turn of a dialogue given the preceding ones.",
      "startOffset" : 0,
      "endOffset" : 106
    }, {
      "referenceID" : 20,
      "context" : "(Vinyals and Le, 2015; Sordoni et al., 2015; Serban et al., 2015a; Mei et al., 2016; Serban et al., 2015b), in which a model must predict the next turn of a dialogue given the preceding ones.",
      "startOffset" : 0,
      "endOffset" : 106
    }, {
      "referenceID" : 28,
      "context" : "(Vinyals and Le, 2015; Sordoni et al., 2015; Serban et al., 2015a; Mei et al., 2016; Serban et al., 2015b), in which a model must predict the next turn of a dialogue given the preceding ones.",
      "startOffset" : 0,
      "endOffset" : 106
    }, {
      "referenceID" : 38,
      "context" : "We use the OpenSubtitles (OSDb) dataset (Tiedemann, 2009).",
      "startOffset" : 40,
      "endOffset" : 57
    }, {
      "referenceID" : 14,
      "context" : "We also report adversarial success (AdverSuc) and machine-vs-random accuracy, evaluation metrics proposed in Li et al. (2016c). Adversarial success refers to the percentage of machine-generated responses that are able to fool a trained evaluator model into believing that they are generated by a human; machine-vs-random accuracy denotes the accuracy of a (different) trained evaluator model at distinguishing between machinegenerated responses and randomly-sampled responses.",
      "startOffset" : 109,
      "endOffset" : 127
    }, {
      "referenceID" : 14,
      "context" : "We also report adversarial success (AdverSuc) and machine-vs-random accuracy, evaluation metrics proposed in Li et al. (2016c). Adversarial success refers to the percentage of machine-generated responses that are able to fool a trained evaluator model into believing that they are generated by a human; machine-vs-random accuracy denotes the accuracy of a (different) trained evaluator model at distinguishing between machinegenerated responses and randomly-sampled responses.5 Higher values of adversarial success and machine-vs-random accuracy indicate the superiority of a model. We refer readers to Li et al. (2016c) for more details.",
      "startOffset" : 109,
      "endOffset" : 621
    }, {
      "referenceID" : 29,
      "context" : "The estimators for AdverSuc and machine-vs-random accuracy are trained using a hierarchical network (Serban et al., 2016) See Li et al.",
      "startOffset" : 100,
      "endOffset" : 121
    }, {
      "referenceID" : 14,
      "context" : ", 2016) See Li et al. (2016c) for details.",
      "startOffset" : 12,
      "endOffset" : 30
    }, {
      "referenceID" : 15,
      "context" : "Since hypotheses in beam search are known to lack diversity (Li et al., 2016b; Vijayakumar et al., 2016; Gimpel et al., 2013), after decoding is finished, it is sometimes too late for the reranking model to have significant impact.",
      "startOffset" : 60,
      "endOffset" : 125
    }, {
      "referenceID" : 39,
      "context" : "Since hypotheses in beam search are known to lack diversity (Li et al., 2016b; Vijayakumar et al., 2016; Gimpel et al., 2013), after decoding is finished, it is sometimes too late for the reranking model to have significant impact.",
      "startOffset" : 60,
      "endOffset" : 125
    }, {
      "referenceID" : 7,
      "context" : "Since hypotheses in beam search are known to lack diversity (Li et al., 2016b; Vijayakumar et al., 2016; Gimpel et al., 2013), after decoding is finished, it is sometimes too late for the reranking model to have significant impact.",
      "startOffset" : 60,
      "endOffset" : 125
    }, {
      "referenceID" : 7,
      "context" : ", 2016; Gimpel et al., 2013), after decoding is finished, it is sometimes too late for the reranking model to have significant impact. Shao et al. (2017) confirm this problem and show that the reranking approach helps for short sequences but not longer ones.",
      "startOffset" : 8,
      "endOffset" : 154
    }, {
      "referenceID" : 14,
      "context" : "We compare the results for the approach with standard beam search using the MLE objective and the MMI reranking approach of Li et al. (2016a), which performs reranking only after decoding is finished.",
      "startOffset" : 124,
      "endOffset" : 142
    }, {
      "referenceID" : 14,
      "context" : "We compare the results for the approach with standard beam search using the MLE objective and the MMI reranking approach of Li et al. (2016a), which performs reranking only after decoding is finished. We report BLEU scores and AdverSuc scores8 on the test set. We also report diversity scores (denoted by Distinct-1 and Distinct-2); these are defined as in Li et al. (2016a) to be the the number of distinct unigrams and bigrams (respectively) in generated responses, divided by the total number of generated tokens (to avoid favoring long sentences).",
      "startOffset" : 124,
      "endOffset" : 375
    }, {
      "referenceID" : 14,
      "context" : "Table 4: Sample of responses generated by (1) standard beam search (SBS); (2) the MMI reranking approach of Li et al. (2016a), which performs reranking only after decoding is complete (denoted by MMI); and (3) the future prediction model Q(MMI) with different values of future prediction weight λ.",
      "startOffset" : 108,
      "endOffset" : 126
    }, {
      "referenceID" : 23,
      "context" : "These features include BLEU (Papineni et al., 2002) or ROUGE (Lin, 2004) scores.",
      "startOffset" : 28,
      "endOffset" : 51
    }, {
      "referenceID" : 17,
      "context" : ", 2002) or ROUGE (Lin, 2004) scores.",
      "startOffset" : 17,
      "endOffset" : 28
    }, {
      "referenceID" : 41,
      "context" : "In this situation, the future prediction function is able to reduce the discrepancy between training (using maximum likelihood objective) and testing (using BLEU or ROUGE) (Wiseman and Rush, 2016; Shen et al., 2015; Ranzato et al., 2016).",
      "startOffset" : 172,
      "endOffset" : 237
    }, {
      "referenceID" : 31,
      "context" : "In this situation, the future prediction function is able to reduce the discrepancy between training (using maximum likelihood objective) and testing (using BLEU or ROUGE) (Wiseman and Rush, 2016; Shen et al., 2015; Ranzato et al., 2016).",
      "startOffset" : 172,
      "endOffset" : 237
    }, {
      "referenceID" : 24,
      "context" : "In this situation, the future prediction function is able to reduce the discrepancy between training (using maximum likelihood objective) and testing (using BLEU or ROUGE) (Wiseman and Rush, 2016; Shen et al., 2015; Ranzato et al., 2016).",
      "startOffset" : 172,
      "endOffset" : 237
    }, {
      "referenceID" : 2,
      "context" : "Machine Translation We use the GermanEnglish machine translation track of the IWSLT 2014 (Cettolo et al., 2014), which consists of sentence-aligned subtitles of TED and TEDx talks.",
      "startOffset" : 89,
      "endOffset" : 111
    }, {
      "referenceID" : 0,
      "context" : "Machine Translation We use the GermanEnglish machine translation track of the IWSLT 2014 (Cettolo et al., 2014), which consists of sentence-aligned subtitles of TED and TEDx talks. For fair comparison, we followed exactly the data processing protocols defined in Ranzato et al. (2016), which have also been adopted by Bahdanau et al.",
      "startOffset" : 90,
      "endOffset" : 285
    }, {
      "referenceID" : 0,
      "context" : "(2016), which have also been adopted by Bahdanau et al. (2016) and Wiseman and Rush (2016).",
      "startOffset" : 40,
      "endOffset" : 63
    }, {
      "referenceID" : 0,
      "context" : "(2016), which have also been adopted by Bahdanau et al. (2016) and Wiseman and Rush (2016). The training data consists of roughly 150K sentence pairs, in which the average English sentence is 17.",
      "startOffset" : 40,
      "endOffset" : 91
    }, {
      "referenceID" : 36,
      "context" : "We train two models, a vanilla LSTM (Sutskever et al., 2014) and an attention-based model (Bahdanau et al.",
      "startOffset" : 36,
      "endOffset" : 60
    }, {
      "referenceID" : 1,
      "context" : ", 2014) and an attention-based model (Bahdanau et al., 2015).",
      "startOffset" : 37,
      "endOffset" : 60
    }, {
      "referenceID" : 0,
      "context" : ", 2014) and an attention-based model (Bahdanau et al., 2015). For the attention model, we use the input-feeding model described in Luong et al. (2015) with one minor modification: the weighted attention vectors that are used in the softmax token predictions and those fed to the recurrent net at the next step use different sets of parameters.",
      "startOffset" : 38,
      "endOffset" : 151
    }, {
      "referenceID" : 0,
      "context" : ", 2014) and an attention-based model (Bahdanau et al., 2015). For the attention model, we use the input-feeding model described in Luong et al. (2015) with one minor modification: the weighted attention vectors that are used in the softmax token predictions and those fed to the recurrent net at the next step use different sets of parameters. Their values can therefore be different, unlike in Luong et al. (2015). We find that this small modification significantly improves the capacity of attention models, yielding more than a +1.",
      "startOffset" : 38,
      "endOffset" : 415
    }, {
      "referenceID" : 0,
      "context" : ", 2014) and an attention-based model (Bahdanau et al., 2015). For the attention model, we use the input-feeding model described in Luong et al. (2015) with one minor modification: the weighted attention vectors that are used in the softmax token predictions and those fed to the recurrent net at the next step use different sets of parameters. Their values can therefore be different, unlike in Luong et al. (2015). We find that this small modification significantly improves the capacity of attention models, yielding more than a +1.0 BLEU score improvement. We use structure similar to that of Wiseman and Rush (2016), a single-layer sequenceto-sequence model with 256 units for each layer.",
      "startOffset" : 38,
      "endOffset" : 620
    }, {
      "referenceID" : 24,
      "context" : "(2016) and the beam-search training scheme described in WiseREINFORCE (Ranzato et al., 2016) 20.",
      "startOffset" : 70,
      "endOffset" : 92
    }, {
      "referenceID" : 0,
      "context" : "7 Actor-Critic (Bahdanau et al., 2016) 22.",
      "startOffset" : 15,
      "endOffset" : 38
    }, {
      "referenceID" : 15,
      "context" : "Baselines employed include the REINFORCE model described in Ranzato et al. (2016), the actor-critic RL model described in Bahdanau et al.",
      "startOffset" : 4,
      "endOffset" : 82
    }, {
      "referenceID" : 0,
      "context" : "(2016), the actor-critic RL model described in Bahdanau et al. (2016) and the beam-search training scheme described in WiseREINFORCE (Ranzato et al.",
      "startOffset" : 47,
      "endOffset" : 70
    }, {
      "referenceID" : 0,
      "context" : "(2016), the actor-critic RL model described in Bahdanau et al. (2016) and the beam-search training scheme described in WiseREINFORCE (Ranzato et al., 2016) 20.7 Actor-Critic (Bahdanau et al., 2016) 22.4 Wiseman and Rush (2016) 26.",
      "startOffset" : 47,
      "endOffset" : 227
    }, {
      "referenceID" : 24,
      "context" : "Abstractive Summarization We follow the protocols described in Rush et al. (2015), in which the source input is the first sentence of a new article and the target output is the headline.",
      "startOffset" : 63,
      "endOffset" : 82
    } ],
    "year" : 2017,
    "abstractText" : "We introduce a simple, general strategy to manipulate the behavior of a neural decoder that enables it to generate outputs that have specific properties of interest (e.g., sequences of a pre-specified length). The model can be thought of as a simple version of the actor-critic model that uses an interpolation of the actor (the MLE-based token generation policy) and the critic (a value function that estimates the future values of the desired property) for decision making. We demonstrate that the approach is able to incorporate a variety of properties that cannot be handled by standard neural sequence decoders, such as sequence length and backward probability (probability of sources given targets), in addition to yielding consistent improvements in abstractive summarization and machine translation when the property to be optimized is BLEU or ROUGE scores.",
    "creator" : "LaTeX with hyperref package"
  }
}