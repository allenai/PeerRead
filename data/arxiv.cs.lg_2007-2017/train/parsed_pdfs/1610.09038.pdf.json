{
  "name" : "1610.09038.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Professor Forcing: A New Algorithm for Training Recurrent Networks",
    "authors" : [ "Alex Lamb", "Aaron Courville" ],
    "emails" : [ "anirudhgoyal9119@gmail.com", "lambalex@iro.umontreal.ca", "ying.zhlisa@gmail.com", "saizhenglisa@gmail.com", "aaron.courville@gmail.com", "yoshua.umontreal@gmail.com" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Recurrent neural networks (RNNs) have become to be the generative models of choice for sequential data (Graves, 2012) with impressive results in language modeling (Mikolov, 2010; Mikolov and Zweig, 2012), speech recognition (Bahdanau et al., 2015; Chorowski et al., 2015), Machine Translation (Cho et al., 2014a; Sutskever et al., 2014; Bahdanau et al., 2014), handwriting generation (Graves, 2013), image caption generation (Xu et al., 2015; Chen and Lawrence Zitnick, 2015), etc.\n29th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nar X\niv :1\n61 0.\n09 03\n8v 1\nThe RNN models the data via a fully-observed directed graphical model: it decomposes the distribution over the discrete time sequence y1, y2, . . . yT into an ordered product of conditional distributions over tokens\nP (y1, y2, . . . yT ) = P (y1) T∏ t=1 P (yt | y1, . . . yt−1).\nBy far the most popular training strategy is via the maximum likelihood principle. In the RNN literature, this form of training is also known as teacher forcing (Williams and Zipser, 1989), due to the use of the ground-truth samples yt being fed back into the model to be conditioned on for the prediction of later outputs. These fed back samples force the RNN to stay close to the ground-truth sequence.\nWhen using the RNN for prediction, the ground-truth sequence is not available conditioning and we sample from the joint distribution over the sequence by sampling each yt from its conditional distribution given the previously generated samples. Unfortunately, this procedure can result in problems in generation as small prediction error compound in the conditioning context. This can lead to poor prediction performance as the RNN’s conditioning context (the sequence of previously generated samples) diverge from sequences seen during training.\nRecently, (Bengio et al., 2015) proposed to remedy that issue by mixing two kinds of inputs during training: those from the ground-truth training sequence and those generated from the model. However, when the model generates several consecutive yt’s, it is not clear anymore that the correct target (in terms of its distribution) remains the one in the ground truth sequence. This is mitigated in various ways, by making the self-generated subsequences short and annealing the probability of using self-generated vs ground truth samples. However, as remarked by Huszár (2015), scheduled sampling yields a biased estimator, in that even as the number of examples and the capacity go to infinity, this procedure may not converge to the correct model. It is however good to note that experiments with scheduled sampling clearly showed some improvements in terms of the robustness of the generated sequences, suggesting that something indeed needs to be fixed (or replaced) with maximum-likelihood (or teacher forcing) training of generative RNNs.\nIn this paper, we propose an alternative way of training RNNs which explicitly seeks to make the generative behavior and the teacher-forced behavior match as closely as possible. This is particularly important to allow the RNN to continue generating robustly well beyond the length of the sequences it saw during training. More generally, we argue that this approach helps to better model long-term dependencies by using a training objective that is not solely focused on predicting the next observation, one step at a time.\nOur work provides the following contributions regarding this new training framework:\n• We introduce a novel method for training generative RNNs called Professor Forcing, meant to improve long-term sequence sampling from recurrent networks. We demonstrate this with human evaluation of sample quality by performing a study with human evaluators.\n• We find that Professor Forcing can act as a regularizer for recurrent networks. This is demonstrated by achieving improvements in test likelihood on character-level Penn Treebank, Sequential MNIST Generation, and speech synthesis. Interestingly, we also find that training performance can also be improved, and we conjecture that it is because longer-term dependencies can be more easily captured.\n• When running an RNN in sampling mode, the region occupied by the hidden states of the network diverges from the region occupied when doing teacher forcing. We empirically study this phenomenon using T-SNEs and show that it can be mitigated by using Professor Forcing.\n• In some domains the sequences available at training time are shorter than the sequences that we want to generate at test time. This is usually the case in long-term forecasting tasks (climate modeling, econometrics). We show how using Professor Forcing can be used to improve performance in this setting. Note that scheduled sampling cannot be used for this task, because it still uses the observed sequence as targets for the network."
    }, {
      "heading" : "2 Proposed Approach: Professor Forcing",
      "text" : "The basic idea of Professor Forcing is simple: while we do want the generative RNN to match the training data, we also want the behavior of the network (both in its outputs and in the dynamics of its hidden states) to be indistinguishable whether the network is trained with its inputs clamped to a training sequence (teacher forcing mode) or whether its inputs are self-generated (free-running generative mode). Because we can only compare the distribution of these sequences, it makes sense to take advantage of the generative adversarial networks (GANs) framework (Goodfellow et al., 2014) to achieve that second objective of matching the two distributions over sequences (the one observed in teacher forcing mode vs the one observed in free-running mode).\nHence, in addition to the generative RNN, we will train a second model, which we call the discriminator, and that can also process variable length inputs. In the experiments we use a bidirectional RNN architecture for the discriminator, so that it can combine evidence at each time step t from the past of the behavior sequence as well as from the future of that sequence."
    }, {
      "heading" : "2.1 Definitions and Notation",
      "text" : "Let the training distribution provide (x,y) pairs of input and output sequences (possibly there are no inputs at all). An output sequence y can also be generated by the generator RNN when given an input sequence x, according to the sequence to sequence model distribution Pθg (y|x). Let θg be the parameters of the generative RNN and θd be the parameters of the discriminator. The discriminator is trained as a probabilistic classifier that takes as input a behavior sequence b derived from the generative RNN’s activity (hiddens and outputs) when it either generates or is constrained by a sequence y, possibly in the context of an input sequence x (often but not necessarily of the same length). The behavior sequence b is either the result of running the generative RNN in teacher forcing mode (with y from a training sequence with input x), or in free-running mode (with y self-generated according to Pθg (y|x), with x from the training sequence). The function B(x,y,θg) outputs the behavior sequence (chosen hidden states and output values) given the appropriate data (where x always comes from the training data but y either comes from the data or is self-generated). Let D(b) be the output of the discriminator, estimating the probability that b was produced in teacher-forcing mode, given that half of the examples seen by the discriminator are generated in teacher forcing mode and half are generated in the free-running mode.\nNote that in the case where the generator RNN does not have any conditioning input, the sequence x is empty. Note also that the generated output sequences could have a different length then the conditioning sequence, depending of the task at hand."
    }, {
      "heading" : "2.2 Training Objective",
      "text" : "The discriminator parameters θd are trained as one would expect, i.e., to maximize the likelihood of correctly classifying a behavior sequence:\nCd(θd|θg) = E(x,y)∼data[− logD(B(x,y,θg),θd)+Ey∼Pθg (y|x)[− log(1−D(B(x,y,θg),θd)]]. (1) Practically, this is achieved with a variant of stochastic gradient descent with minibatches formed by combining N sequences obtained in teacher-forcing mode and N sequences obtained in free-running mode, with y sampled from Pθg (y|x). Note also that as θg changes, the task optimized by the discriminator changes too, and it has to track the generator, as in other GAN setups, hence the notation Cd(θd|θg). The generator RNN parameters θg are trained to (a) maximize the likelihood of the data and (b) fool the discriminator. We considered two variants of the latter. The negative log-likelihood objective (a) is the usual teacher-forced training criterion for RNNs:\nNLL(θg) = E(x,y)∼data[− logPθg (y|x)]. (2)\nRegarding (b) we consider a training objective that only tries to change the free-running behavior so that it better matches the teacher-forced behavior, considering the latter fixed:\nCf (θg|θd) = Ex∼data,y∼Pθg (y|x)[− logD(B(x,y,θg),θd)]. (3)\nIn addition (and optionally), we can ask the teacher-forced behavior to be indistinguishable from the free-running behavior:\nCt(θg|θd) = E(x,y)∼data[− log(1−D(B(x,y,θg),θd))]. (4)\nIn our experiments we either perform stochastic gradient steps on NLL+Cf or on NLL+Cf +Ct to update the generative RNN parameters, while we always do gradient steps on Cd to update the discriminator parameters."
    }, {
      "heading" : "3 Related Work",
      "text" : "Professor Forcing is an adversarial method for learning generative models that is closely related to Generative Adversarial Networks (Goodfellow et al., 2014) and Adversarial Domain Adaptation Ajakan et al. (2014); Ganin et al. (2015). Our approach is similar to generative adversarial networks (GANs) because both use a discriminative classifier to provide gradients for training a generative model. However, Professor Forcing is different because the classifier discriminates between hidden states from sampling mode and teacher forcing mode, whereas the GAN’s classifier discriminates between real samples and generated samples. One practical advantage of Professor Forcing over GANs is that Professor Forcing can be used to learn a generative model over discrete random variables without requiring to approximate backpropagation through discrete spaces Bengio et al. (2013).\nThe Adversarial Domain Adaptation uses a classifier to discriminate between the hidden states of the network with inputs from the source domain and the hidden states of the network with inputs from the target domain. However this method was not applied in the context of generative models, more specifically, was not applied to the task of improving long-term generation from recurrent networks.\nAlternative non-adversarial methods have been explored for improving long-term generation from recurrent networks. The scheduled sampling method Bengio et al. (2015), which is closely related to SEARN (Daumé et al., 2009) and DAGGER Ross et al. (2010), involves randomly using the network’s predictions as its inputs (as in sampling mode) with some probability that increases over the course of training. This forces the network to be able to stay in a reasonable regime when receiving the network’s predictions as inputs instead of observed inputs. While Scheduled Sampling shows improvement on some tasks, it is not a consistent estimation strategy. This limitation arises because the outputs sampled from the network could correspond to a distribution that is not consistent with the sequence that the network is trained to generate. This issue is discussed in detail in Huszár (2015). A practical advantage of Scheduled Sampling over Professor Forcing is that Scheduled Sampling does not require the additional overhead of having to train a discriminator network.\nActor-critic methods have also been explored for improving modeling of long-term dependencies in generative recurrent neural networks Bahdanau et al. (2016).\nFinally, the idea of matching the behavior of the model when it is generating in a free-running way with its behavior when it is constrained by the observed data (being clamped on the \"visible units\") is precisely that which one obtains when zeroing the maximum likelihood gradient on undirected graphical models with latent variables such as the Boltzmann machine. Training Boltzmann machines amounts to matching the sufficient statistics (which summarize the behavior of the model) in both \"teacher forced\" (positive phase) and \"free-running\" (negative phase) modes."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Networks Architecture and Professor Forcing Setup",
      "text" : "The neural networks and Professor Forcing setup used in the experiments is the following. The generative RNN has single hidden layer of gated recurrent units (GRU), previously introduced by (Cho et al., 2014b) as a computationally cheaper alternative to LSTM units (Hochreiter and Schmidhuber, 1997). At each time step, the generative RNN reads an element xt of the input sequence (if any) and an element of the output sequence yt (which either comes from the training data or was generated at the previous step by the RNN). It then updates its state ht as a function of its previous state ht−1 and of the current input (xt,yt). It then computes a probability distribution Pθg (yt+1|ht) = Pθg (yt+1|x1, . . . ,xt,y1, . . . ,yt) over the next element of the output. For discrete outputs this is achieved by a softmax / affine layer on top of ht, with as many outputs as the size of\nthe set of values that yt can take. In free-running mode, yt+1 is then sampled from this distribution and will be used as part of the input for the next time step. Otherwise, the ground truth yt is used.\nThe behavior function B used in the experiments outputs the pre-tanh activation of the GRU states for the whole sequence considered, and optionally the softmax outputs for the next-step prediction, again for the whole sequence.\nThe discriminator architecture we used for these experiments is based on a bidirectional recurrent neural network, which comprises two RNNs (again, two GRU networks), one running forward in time on top of the input sequence b, and one running backwards in time, with the same input. The hidden states of these two RNNs are concatenated at each time step and fed to a multi-layer neural network shared across time (the same network is used for all time steps). That MLP has three layers, each composing an affine transformation and a rectifier (ReLU). Finally, the output layer composes an affine transformation and a sigmoid that outputs D(b).\nWhen the discriminator is too poor, the gradient it propagates into the generator RNN could be detrimental. For this reason, we back-propagate from the discriminator into the generator RNN only when the discriminator classification accuracy is greater than 75%. On the other hand, when the discriminator is too successful at identifying fake inputs, we found that it would also hurt to continue training it. So when its accuracy is greater than 99%, we do not update the discriminator.\nBoth networks are trained by minibatch stochastic gradient descent with adaptive learning rates and momentum determined by the Adam algorithm (Kingma and Ba, 2014). All of our experiments were implemented using the Theano framework (Al-Rfou et al., 2016)."
    }, {
      "heading" : "4.2 Character-Level Language Modeling",
      "text" : "We evaluate Professor Forcing on character-level language modeling on Penn-Treebank corpus, which has an alphabet size of 50 and consists of 5059k characters for training, 396k characters for validation and 446k characters for test. We divide the training set into non-overlapping sequences with each length of 500. During training, we monitor the negative log-likelihood (NLL) of the output\nsequences. The final model are evaluated by bits-per-character (BPC) metric. The generative RNN\nimplements an 1 hidden layer GRU with 1024 hidden units. We use Adam algorithm for optimization with a learning rate of 0.0001. We feed both the hidden states and char level embeddings into the discriminator. All the layers in the discriminator consists of 2048 hidden units. Output activation of the last layer is clipped between -10 and 10. We see that training cost of Professor Forcing network decreases faster compared to teacher forcing network. The training time of our model is 3 times more as compared to teacher forcing, since our model includes sampling phase, as well as passing the hidden distributions corresponding to free running and teacher forcing phase to the discriminator. The final BPC on validation set using our baseline was 1.50 while using professor forcing it is 1.48.\nOn word level Penn Treebank we did not observe any difference between Teacher Forcing and Professor Forcing. One possible explanation for this difference is the increased importance of long-term dependencies in character-level language modeling."
    }, {
      "heading" : "4.3 Sequential MNIST",
      "text" : "We evaluated Professor Forcing on the task of sequentially generating the pixels in MNIST digits. We use the standard binarized MNIST dataset Murray and Salakhutdinov (2009). We selected hyperparameters for our model on the validation set and elected to use 512 hidden states and a learning rate of 0.0001. For all experiments we used a 3-layer GRU as our generator. Unlike our other experiments, we used a convolutional network for the discriminator instead of a bi-directional RNN, as the pixels have a 2D spatial structure. We note that our model achieves the second best reported likelihood on this task, after the PixelRNN, which used a significantly more complicated architecture for its generator van den Oord et al. (2016). Combining Professor Forcing with the PixelRNN would be an interesting area for future research. However, the PixelRNN parallelizes computation in the teacher forcing network in a way that doesn’t work in the sampling network. Because Professor Forcing requires running the sampling network during training, naively combining Professor Forcing with the PixelRNN would be very slow."
    }, {
      "heading" : "4.4 Handwriting Generation",
      "text" : "With this task we wanted to investigate if Professor Forcing could be used to perform domain adaptation from a training set with short sequences to sampling much longer sequences. We train the Teacher Forcing model on only 50 steps of text-conditioned handwriting (corresponding to a few letters) and then sample for 1000 time steps . We let the model learn a sequence of (x, y) coordinates together with binary indicators of pen-up vs. pen-down, using the standard handwriting IAM-OnDB dataset, which consists of 13,040 handwritten lines written by 500 writers Liwicki and Bunke (2005). For our teacher forcing model, we use the open source implementation Brebisson (2016) and use their hyperparameters which is based on the model in Graves (2013). For the professor forcing model, we sample for 1000 time steps and run a separate discriminator on non-overlapping segments of length 50 (the number of steps used in the teacher forcing model). For both Teacher Forcing and Professor Forcing we sample with a bias of 0.5.\nWe performed a human evaluation study on handwriting samples. We gave 48 volunteers 16 randomly selected Prof. Forcing samples randomly paired with 16 Teacher Forcing samples and asked them to\nindicate which sample was higher quality and whether it was “much better” or “slightly better”. Both models had equal training time and samples were drawn using the same procedure. Volunteers were not aware of which samples came from which model."
    }, {
      "heading" : "4.5 Music Synthesis on Raw Waveforms",
      "text" : "We considered the task of vocal synthesis on raw waveforms. For this task we used three hours of monk chanting audio scraped from YouTube (https://www.youtube.com/watch?v=9-pD28iSiTU). We sampled the audio at a rate of 1 kHz and took four seconds for each training and validation example. On each time step of the raw audio waveform we binned the signal’s value into 8000 bins with boundaries drawn uniformly between the smallest and largest signal values in the dataset. We then model the raw audio waveform as a 4000-length sequence with 8000 potential values on each time step.\nWe evaluated the quality of our vocal synthesis model using two criteria. First, we demonstrated a regularizing effect and improvement in negative log-likelihood. Second, we observed improvement in the quality of samples. We included a few randomly selected samples in the supplementary material and also performed human evaluation of the samples.\nVisual inspection of samples is known to be a flawed method for evaluating generative models, because a generative model could simply memorize a small number of examples from the training set (or slightly modified examples from the training set) and achieve high sample quality. This issue was discussed in Theis et al. (2015). However, this is unlikely to be an issue with our evaluation because our method also improved validation set likelihood, whereas a model that achieves quality samples by dropping coverage would have poorer validation set likelihood.\nWe performed human evaluation by asking 29 volunteers to listen to five randomly selected teacher forcing samples and five randomly selected professor forcing samples (included in supplementary materials and then rate each sample from 1-3 on the basis of quality. The annotators were given the samples in random order and were not told which samples came from which algorithm. The human annotators gave the Professor Forcing samples an average score of 2.20, whereas they gave the Teacher Forcing samples an average score of 1.30."
    }, {
      "heading" : "4.6 Negative Results on Shorter Sequences",
      "text" : "On word level Penn Treebank we did not observe any difference between Teacher Forcing and Professor Forcing. One possible explanation for this difference is the increased importance of longterm dependencies in character-level language modeling. Also, for speech synthesis, we did not observe any difference between Teacher Forcing and Professor Forcing while training on sequences of length less than 100."
    }, {
      "heading" : "5 Conclusion",
      "text" : "The idea of matching behavior of a model when it is running on its own, making predictions, generating samples, etc. vs when it is forced to be consistent with observed data is an old and powerful one. In this paper we introduce Professor Forcing, a novel instance of this idea when the model of interest is a recurrent generative one, and which relies on training an auxiliary model, the discriminator to spot the differences in behavior between these two modes of behavior. A major motivation for this approach is that the discriminator can look at the statistics of the behavior and not just at the single-step predictions, forcing the generator to behave the same when it is constrained by the data and when it is left generating outputs by itself for sequences that can be much longer than the training sequences. This naturally produces better generalization over sequences that are much longer than the training sequences, as we have found. We have also found that it helped to generalize better in terms of one-step prediction (log-likelihood), even though we are adding a possibly conflicting term to the log-likelihood training objective. This suggests that it acts like a regularizer but a very interesting one because it can also greatly speed up convergence in terms of number of training updates. We validated the advantage of Professor Forcing over traditional teacher forcing on a variety of sequential learning and generative tasks, with particularly impressive results in acoustic generation,\nwhere the training sequences are much shorter (because of memory constraints) than the length of the sequences we actually want to generate."
    }, {
      "heading" : "6 Acknowledgments",
      "text" : "We thank Martin Arjovsky, Dzmitry Bahdanau, Nan Rosemary Ke, Kyle Kastner, José Manuel Rodríguez Sotelo, Alexandre de Brébisson, Olexa Bilaniuk, Hal Daumé III, Kari Torkkola, and David Krueger for their helpful feedback."
    } ],
    "references" : [ {
      "title" : "Domain-Adversarial Neural Networks",
      "author" : [ "H. Ajakan", "P. Germain", "H. Larochelle", "F. Laviolette", "M. Marchand" ],
      "venue" : "ArXiv e-prints.",
      "citeRegEx" : "Ajakan et al\\.,? 2014",
      "shortCiteRegEx" : "Ajakan et al\\.",
      "year" : 2014
    }, {
      "title" : "Theano: A python framework for fast computation of mathematical expressions. CoRR, abs/1605.02688",
      "author" : [ "R. Al-Rfou", "G. Alain", "A. Almahairi" ],
      "venue" : null,
      "citeRegEx" : "Al.Rfou et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Al.Rfou et al\\.",
      "year" : 2016
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "D. Bahdanau", "K. Cho", "Y. Bengio" ],
      "venue" : "arXiv preprint arXiv:1409.0473.",
      "citeRegEx" : "Bahdanau et al\\.,? 2014",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2014
    }, {
      "title" : "End-to-end attention-based large vocabulary speech recognition",
      "author" : [ "D. Bahdanau", "J. Chorowski", "D. Serdyuk", "P. Brakel", "Y. Bengio" ],
      "venue" : "arXiv preprint arXiv:1508.04395.",
      "citeRegEx" : "Bahdanau et al\\.,? 2015",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2015
    }, {
      "title" : "An Actor-Critic Algorithm for Sequence Prediction",
      "author" : [ "D. Bahdanau", "P. Brakel", "K. Xu", "A. Goyal", "R. Lowe", "J. Pineau", "A. Courville", "Y. Bengio" ],
      "venue" : "ArXiv e-prints.",
      "citeRegEx" : "Bahdanau et al\\.,? 2016",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2016
    }, {
      "title" : "Scheduled sampling for sequence prediction with recurrent neural networks",
      "author" : [ "S. Bengio", "O. Vinyals", "N. Jaitly", "N. Shazeer" ],
      "venue" : "Advances in Neural Information Processing Systems, pages 1171–1179.",
      "citeRegEx" : "Bengio et al\\.,? 2015",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2015
    }, {
      "title" : "Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation",
      "author" : [ "Y. Bengio", "N. Léonard", "A. Courville" ],
      "venue" : "ArXiv e-prints.",
      "citeRegEx" : "Bengio et al\\.,? 2013",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2013
    }, {
      "title" : "Conditional handwriting generation in theano",
      "author" : [ "A. Brebisson" ],
      "venue" : "https://github.com/adbrebs/ handwriting.",
      "citeRegEx" : "Brebisson,? 2016",
      "shortCiteRegEx" : "Brebisson",
      "year" : 2016
    }, {
      "title" : "Mind’s eye: A recurrent visual representation for image caption generation",
      "author" : [ "X. Chen", "C. Lawrence Zitnick" ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2422–2431.",
      "citeRegEx" : "Chen and Zitnick,? 2015",
      "shortCiteRegEx" : "Chen and Zitnick",
      "year" : 2015
    }, {
      "title" : "Learning phrase representations using RNN encoder–decoder for statistical machine translation",
      "author" : [ "K. Cho", "B. Van Merriënboer", "Ç. Gülçehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio" ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1724–1734, Doha, Qatar. Association for Computational Linguistics.",
      "citeRegEx" : "Cho et al\\.,? 2014a",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2014
    }, {
      "title" : "Learning phrase representations using rnn encoder-decoder for statistical machine translation",
      "author" : [ "K. Cho", "B. Van Merriënboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio" ],
      "venue" : "arXiv preprint arXiv:1406.1078.",
      "citeRegEx" : "Cho et al\\.,? 2014b",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2014
    }, {
      "title" : "Attention-based models for speech recognition",
      "author" : [ "J.K. Chorowski", "D. Bahdanau", "D. Serdyuk", "K. Cho", "Y. Bengio" ],
      "venue" : "Advances in Neural Information Processing Systems, pages 577–585.",
      "citeRegEx" : "Chorowski et al\\.,? 2015",
      "shortCiteRegEx" : "Chorowski et al\\.",
      "year" : 2015
    }, {
      "title" : "Search-based Structured Prediction",
      "author" : [ "III Daumé", "H.", "J. Langford", "D. Marcu" ],
      "venue" : "ArXiv e-prints.",
      "citeRegEx" : "Daumé et al\\.,? 2009",
      "shortCiteRegEx" : "Daumé et al\\.",
      "year" : 2009
    }, {
      "title" : "Domain-Adversarial Training of Neural Networks",
      "author" : [ "Y. Ganin", "E. Ustinova", "H. Ajakan", "P. Germain", "H. Larochelle", "F. Laviolette", "M. Marchand", "V. Lempitsky" ],
      "venue" : "ArXiv e-prints.",
      "citeRegEx" : "Ganin et al\\.,? 2015",
      "shortCiteRegEx" : "Ganin et al\\.",
      "year" : 2015
    }, {
      "title" : "Made: Masked autoencoder for distribution estimation",
      "author" : [ "M. Germain", "K. Gregor", "I. Murray", "H. Larochelle" ],
      "venue" : "arXiv preprint arXiv:1502.03509.",
      "citeRegEx" : "Germain et al\\.,? 2015",
      "shortCiteRegEx" : "Germain et al\\.",
      "year" : 2015
    }, {
      "title" : "Generative adversarial networks",
      "author" : [ "I.J. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio" ],
      "venue" : "NIPS’2014.",
      "citeRegEx" : "Goodfellow et al\\.,? 2014",
      "shortCiteRegEx" : "Goodfellow et al\\.",
      "year" : 2014
    }, {
      "title" : "Supervised Sequence Labelling with Recurrent Neural Networks",
      "author" : [ "A. Graves" ],
      "venue" : "Studies in Computational Intelligence. Springer.",
      "citeRegEx" : "Graves,? 2012",
      "shortCiteRegEx" : "Graves",
      "year" : 2012
    }, {
      "title" : "Generating sequences with recurrent neural networks",
      "author" : [ "A. Graves" ],
      "venue" : "Technical report, arXiv:1308.0850.",
      "citeRegEx" : "Graves,? 2013",
      "shortCiteRegEx" : "Graves",
      "year" : 2013
    }, {
      "title" : "Generating Sequences With Recurrent Neural Networks",
      "author" : [ "A. Graves" ],
      "venue" : "ArXiv e-prints.",
      "citeRegEx" : "Graves,? 2013",
      "shortCiteRegEx" : "Graves",
      "year" : 2013
    }, {
      "title" : "Draw: A recurrent neural network for image generation",
      "author" : [ "K. Gregor", "I. Danihelka", "A. Graves", "D. Wierstra" ],
      "venue" : "arXiv preprint arXiv:1502.04623.",
      "citeRegEx" : "Gregor et al\\.,? 2015",
      "shortCiteRegEx" : "Gregor et al\\.",
      "year" : 2015
    }, {
      "title" : "Long short-term memory",
      "author" : [ "S. Hochreiter", "J. Schmidhuber" ],
      "venue" : "Neural Comput., 9(8), 1735–1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber",
      "year" : 1997
    }, {
      "title" : "How (not) to Train your Generative Model: Scheduled Sampling, Likelihood, Adversary? ArXiv e-prints",
      "author" : [ "F. Huszár" ],
      "venue" : null,
      "citeRegEx" : "Huszár,? \\Q2015\\E",
      "shortCiteRegEx" : "Huszár",
      "year" : 2015
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "D. Kingma", "J. Ba" ],
      "venue" : "arXiv preprint arXiv:1412.6980.",
      "citeRegEx" : "Kingma and Ba,? 2014",
      "shortCiteRegEx" : "Kingma and Ba",
      "year" : 2014
    }, {
      "title" : "The neural autoregressive distribution estimator",
      "author" : [ "H. Larochelle", "I. Murray" ],
      "venue" : null,
      "citeRegEx" : "Larochelle and Murray,? \\Q2011\\E",
      "shortCiteRegEx" : "Larochelle and Murray",
      "year" : 2011
    }, {
      "title" : "Iam-ondb - an on-line english sentence database acquired from handwritten text on a whiteboard",
      "author" : [ "M. Liwicki", "H. Bunke" ],
      "venue" : "Eighth International Conference on Document Analysis and Recognition (ICDAR’05), pages 956–961 Vol. 2.",
      "citeRegEx" : "Liwicki and Bunke,? 2005",
      "shortCiteRegEx" : "Liwicki and Bunke",
      "year" : 2005
    }, {
      "title" : "Recurrent neural network based language model",
      "author" : [ "T. Mikolov" ],
      "venue" : null,
      "citeRegEx" : "Mikolov,? \\Q2010\\E",
      "shortCiteRegEx" : "Mikolov",
      "year" : 2010
    }, {
      "title" : "Context dependent recurrent neural network language model",
      "author" : [ "T. Mikolov", "G. Zweig" ],
      "venue" : null,
      "citeRegEx" : "Mikolov and Zweig,? \\Q2012\\E",
      "shortCiteRegEx" : "Mikolov and Zweig",
      "year" : 2012
    }, {
      "title" : "Evaluating probabilities under high-dimensional latent variable models",
      "author" : [ "I. Murray", "R.R. Salakhutdinov" ],
      "venue" : "D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, editors, Advances in Neural Information Processing Systems 21, pages 1137–1144. Curran Associates, Inc.",
      "citeRegEx" : "Murray and Salakhutdinov,? 2009",
      "shortCiteRegEx" : "Murray and Salakhutdinov",
      "year" : 2009
    }, {
      "title" : "Iterative neural autoregressive distribution estimator NADE-k",
      "author" : [ "T. Raiko", "L. Yao", "K. Cho", "Y. Bengio" ],
      "venue" : "Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 27 (NIPS 2014), pages 325–333. Curran Associates, Inc.",
      "citeRegEx" : "Raiko et al\\.,? 2014",
      "shortCiteRegEx" : "Raiko et al\\.",
      "year" : 2014
    }, {
      "title" : "A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning",
      "author" : [ "S. Ross", "G.J. Gordon", "J.A. Bagnell" ],
      "venue" : "ArXiv e-prints.",
      "citeRegEx" : "Ross et al\\.,? 2010",
      "shortCiteRegEx" : "Ross et al\\.",
      "year" : 2010
    }, {
      "title" : "Markov chain monte carlo and variational inference: Bridging the gap",
      "author" : [ "T. Salimans", "D.P. Kingma", "M. Welling" ],
      "venue" : "arXiv preprint arXiv:1410.6460.",
      "citeRegEx" : "Salimans et al\\.,? 2014",
      "shortCiteRegEx" : "Salimans et al\\.",
      "year" : 2014
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "I. Sutskever", "O. Vinyals", "Q.V. Le" ],
      "venue" : "Advances in neural information processing systems, pages 3104–3112.",
      "citeRegEx" : "Sutskever et al\\.,? 2014",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "A note on the evaluation of generative models. ArXiv e-prints",
      "author" : [ "L. Theis", "A. van den Oord", "M. Bethge" ],
      "venue" : null,
      "citeRegEx" : "Theis et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Theis et al\\.",
      "year" : 2015
    }, {
      "title" : "A learning algorithm for continually running fully recurrent neural networks",
      "author" : [ "R.J. Williams", "D. Zipser" ],
      "venue" : "Neural computation, 1(2), 270–280.",
      "citeRegEx" : "Williams and Zipser,? 1989",
      "shortCiteRegEx" : "Williams and Zipser",
      "year" : 1989
    }, {
      "title" : "Show, attend and tell: Neural image caption generation with visual attention",
      "author" : [ "K. Xu", "J. Ba", "R. Kiros", "A. Courville", "R. Salakhutdinov", "R. Zemel", "Y. Bengio" ],
      "venue" : "arXiv preprint arXiv:1502.03044.",
      "citeRegEx" : "Xu et al\\.,? 2015",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 16,
      "context" : "Recurrent neural networks (RNNs) have become to be the generative models of choice for sequential data (Graves, 2012) with impressive results in language modeling (Mikolov, 2010; Mikolov and Zweig, 2012), speech recognition (Bahdanau et al.",
      "startOffset" : 103,
      "endOffset" : 117
    }, {
      "referenceID" : 25,
      "context" : "Recurrent neural networks (RNNs) have become to be the generative models of choice for sequential data (Graves, 2012) with impressive results in language modeling (Mikolov, 2010; Mikolov and Zweig, 2012), speech recognition (Bahdanau et al.",
      "startOffset" : 163,
      "endOffset" : 203
    }, {
      "referenceID" : 26,
      "context" : "Recurrent neural networks (RNNs) have become to be the generative models of choice for sequential data (Graves, 2012) with impressive results in language modeling (Mikolov, 2010; Mikolov and Zweig, 2012), speech recognition (Bahdanau et al.",
      "startOffset" : 163,
      "endOffset" : 203
    }, {
      "referenceID" : 3,
      "context" : "Recurrent neural networks (RNNs) have become to be the generative models of choice for sequential data (Graves, 2012) with impressive results in language modeling (Mikolov, 2010; Mikolov and Zweig, 2012), speech recognition (Bahdanau et al., 2015; Chorowski et al., 2015), Machine Translation (Cho et al.",
      "startOffset" : 224,
      "endOffset" : 271
    }, {
      "referenceID" : 11,
      "context" : "Recurrent neural networks (RNNs) have become to be the generative models of choice for sequential data (Graves, 2012) with impressive results in language modeling (Mikolov, 2010; Mikolov and Zweig, 2012), speech recognition (Bahdanau et al., 2015; Chorowski et al., 2015), Machine Translation (Cho et al.",
      "startOffset" : 224,
      "endOffset" : 271
    }, {
      "referenceID" : 9,
      "context" : ", 2015), Machine Translation (Cho et al., 2014a; Sutskever et al., 2014; Bahdanau et al., 2014), handwriting generation (Graves, 2013), image caption generation (Xu et al.",
      "startOffset" : 29,
      "endOffset" : 95
    }, {
      "referenceID" : 31,
      "context" : ", 2015), Machine Translation (Cho et al., 2014a; Sutskever et al., 2014; Bahdanau et al., 2014), handwriting generation (Graves, 2013), image caption generation (Xu et al.",
      "startOffset" : 29,
      "endOffset" : 95
    }, {
      "referenceID" : 2,
      "context" : ", 2015), Machine Translation (Cho et al., 2014a; Sutskever et al., 2014; Bahdanau et al., 2014), handwriting generation (Graves, 2013), image caption generation (Xu et al.",
      "startOffset" : 29,
      "endOffset" : 95
    }, {
      "referenceID" : 17,
      "context" : ", 2014), handwriting generation (Graves, 2013), image caption generation (Xu et al.",
      "startOffset" : 32,
      "endOffset" : 46
    }, {
      "referenceID" : 34,
      "context" : ", 2014), handwriting generation (Graves, 2013), image caption generation (Xu et al., 2015; Chen and Lawrence Zitnick, 2015), etc.",
      "startOffset" : 73,
      "endOffset" : 123
    }, {
      "referenceID" : 33,
      "context" : "In the RNN literature, this form of training is also known as teacher forcing (Williams and Zipser, 1989), due to the use of the ground-truth samples yt being fed back into the model to be conditioned on for the prediction of later outputs.",
      "startOffset" : 78,
      "endOffset" : 105
    }, {
      "referenceID" : 5,
      "context" : "Recently, (Bengio et al., 2015) proposed to remedy that issue by mixing two kinds of inputs during training: those from the ground-truth training sequence and those generated from the model.",
      "startOffset" : 10,
      "endOffset" : 31
    }, {
      "referenceID" : 5,
      "context" : "Recently, (Bengio et al., 2015) proposed to remedy that issue by mixing two kinds of inputs during training: those from the ground-truth training sequence and those generated from the model. However, when the model generates several consecutive yt’s, it is not clear anymore that the correct target (in terms of its distribution) remains the one in the ground truth sequence. This is mitigated in various ways, by making the self-generated subsequences short and annealing the probability of using self-generated vs ground truth samples. However, as remarked by Huszár (2015), scheduled sampling yields a biased estimator, in that even as the number of examples and the capacity go to infinity, this procedure may not converge to the correct model.",
      "startOffset" : 11,
      "endOffset" : 576
    }, {
      "referenceID" : 15,
      "context" : "Because we can only compare the distribution of these sequences, it makes sense to take advantage of the generative adversarial networks (GANs) framework (Goodfellow et al., 2014) to achieve that second objective of matching the two distributions over sequences (the one observed in teacher forcing mode vs the one observed in free-running mode).",
      "startOffset" : 154,
      "endOffset" : 179
    }, {
      "referenceID" : 15,
      "context" : "Professor Forcing is an adversarial method for learning generative models that is closely related to Generative Adversarial Networks (Goodfellow et al., 2014) and Adversarial Domain Adaptation Ajakan et al.",
      "startOffset" : 133,
      "endOffset" : 158
    }, {
      "referenceID" : 12,
      "context" : "(2015), which is closely related to SEARN (Daumé et al., 2009) and DAGGER Ross et al.",
      "startOffset" : 42,
      "endOffset" : 62
    }, {
      "referenceID" : 0,
      "context" : ", 2014) and Adversarial Domain Adaptation Ajakan et al. (2014); Ganin et al.",
      "startOffset" : 42,
      "endOffset" : 63
    }, {
      "referenceID" : 0,
      "context" : ", 2014) and Adversarial Domain Adaptation Ajakan et al. (2014); Ganin et al. (2015). Our approach is similar to generative adversarial networks (GANs) because both use a discriminative classifier to provide gradients for training a generative model.",
      "startOffset" : 42,
      "endOffset" : 84
    }, {
      "referenceID" : 0,
      "context" : ", 2014) and Adversarial Domain Adaptation Ajakan et al. (2014); Ganin et al. (2015). Our approach is similar to generative adversarial networks (GANs) because both use a discriminative classifier to provide gradients for training a generative model. However, Professor Forcing is different because the classifier discriminates between hidden states from sampling mode and teacher forcing mode, whereas the GAN’s classifier discriminates between real samples and generated samples. One practical advantage of Professor Forcing over GANs is that Professor Forcing can be used to learn a generative model over discrete random variables without requiring to approximate backpropagation through discrete spaces Bengio et al. (2013). The Adversarial Domain Adaptation uses a classifier to discriminate between the hidden states of the network with inputs from the source domain and the hidden states of the network with inputs from the target domain.",
      "startOffset" : 42,
      "endOffset" : 727
    }, {
      "referenceID" : 0,
      "context" : ", 2014) and Adversarial Domain Adaptation Ajakan et al. (2014); Ganin et al. (2015). Our approach is similar to generative adversarial networks (GANs) because both use a discriminative classifier to provide gradients for training a generative model. However, Professor Forcing is different because the classifier discriminates between hidden states from sampling mode and teacher forcing mode, whereas the GAN’s classifier discriminates between real samples and generated samples. One practical advantage of Professor Forcing over GANs is that Professor Forcing can be used to learn a generative model over discrete random variables without requiring to approximate backpropagation through discrete spaces Bengio et al. (2013). The Adversarial Domain Adaptation uses a classifier to discriminate between the hidden states of the network with inputs from the source domain and the hidden states of the network with inputs from the target domain. However this method was not applied in the context of generative models, more specifically, was not applied to the task of improving long-term generation from recurrent networks. Alternative non-adversarial methods have been explored for improving long-term generation from recurrent networks. The scheduled sampling method Bengio et al. (2015), which is closely related to SEARN (Daumé et al.",
      "startOffset" : 42,
      "endOffset" : 1290
    }, {
      "referenceID" : 0,
      "context" : ", 2014) and Adversarial Domain Adaptation Ajakan et al. (2014); Ganin et al. (2015). Our approach is similar to generative adversarial networks (GANs) because both use a discriminative classifier to provide gradients for training a generative model. However, Professor Forcing is different because the classifier discriminates between hidden states from sampling mode and teacher forcing mode, whereas the GAN’s classifier discriminates between real samples and generated samples. One practical advantage of Professor Forcing over GANs is that Professor Forcing can be used to learn a generative model over discrete random variables without requiring to approximate backpropagation through discrete spaces Bengio et al. (2013). The Adversarial Domain Adaptation uses a classifier to discriminate between the hidden states of the network with inputs from the source domain and the hidden states of the network with inputs from the target domain. However this method was not applied in the context of generative models, more specifically, was not applied to the task of improving long-term generation from recurrent networks. Alternative non-adversarial methods have been explored for improving long-term generation from recurrent networks. The scheduled sampling method Bengio et al. (2015), which is closely related to SEARN (Daumé et al., 2009) and DAGGER Ross et al. (2010), involves randomly using the network’s predictions as its inputs (as in sampling mode) with some probability that increases over the course of training.",
      "startOffset" : 42,
      "endOffset" : 1376
    }, {
      "referenceID" : 0,
      "context" : ", 2014) and Adversarial Domain Adaptation Ajakan et al. (2014); Ganin et al. (2015). Our approach is similar to generative adversarial networks (GANs) because both use a discriminative classifier to provide gradients for training a generative model. However, Professor Forcing is different because the classifier discriminates between hidden states from sampling mode and teacher forcing mode, whereas the GAN’s classifier discriminates between real samples and generated samples. One practical advantage of Professor Forcing over GANs is that Professor Forcing can be used to learn a generative model over discrete random variables without requiring to approximate backpropagation through discrete spaces Bengio et al. (2013). The Adversarial Domain Adaptation uses a classifier to discriminate between the hidden states of the network with inputs from the source domain and the hidden states of the network with inputs from the target domain. However this method was not applied in the context of generative models, more specifically, was not applied to the task of improving long-term generation from recurrent networks. Alternative non-adversarial methods have been explored for improving long-term generation from recurrent networks. The scheduled sampling method Bengio et al. (2015), which is closely related to SEARN (Daumé et al., 2009) and DAGGER Ross et al. (2010), involves randomly using the network’s predictions as its inputs (as in sampling mode) with some probability that increases over the course of training. This forces the network to be able to stay in a reasonable regime when receiving the network’s predictions as inputs instead of observed inputs. While Scheduled Sampling shows improvement on some tasks, it is not a consistent estimation strategy. This limitation arises because the outputs sampled from the network could correspond to a distribution that is not consistent with the sequence that the network is trained to generate. This issue is discussed in detail in Huszár (2015). A practical advantage of Scheduled Sampling over Professor Forcing is that Scheduled Sampling does not require the additional overhead of having to train a discriminator network.",
      "startOffset" : 42,
      "endOffset" : 2012
    }, {
      "referenceID" : 0,
      "context" : ", 2014) and Adversarial Domain Adaptation Ajakan et al. (2014); Ganin et al. (2015). Our approach is similar to generative adversarial networks (GANs) because both use a discriminative classifier to provide gradients for training a generative model. However, Professor Forcing is different because the classifier discriminates between hidden states from sampling mode and teacher forcing mode, whereas the GAN’s classifier discriminates between real samples and generated samples. One practical advantage of Professor Forcing over GANs is that Professor Forcing can be used to learn a generative model over discrete random variables without requiring to approximate backpropagation through discrete spaces Bengio et al. (2013). The Adversarial Domain Adaptation uses a classifier to discriminate between the hidden states of the network with inputs from the source domain and the hidden states of the network with inputs from the target domain. However this method was not applied in the context of generative models, more specifically, was not applied to the task of improving long-term generation from recurrent networks. Alternative non-adversarial methods have been explored for improving long-term generation from recurrent networks. The scheduled sampling method Bengio et al. (2015), which is closely related to SEARN (Daumé et al., 2009) and DAGGER Ross et al. (2010), involves randomly using the network’s predictions as its inputs (as in sampling mode) with some probability that increases over the course of training. This forces the network to be able to stay in a reasonable regime when receiving the network’s predictions as inputs instead of observed inputs. While Scheduled Sampling shows improvement on some tasks, it is not a consistent estimation strategy. This limitation arises because the outputs sampled from the network could correspond to a distribution that is not consistent with the sequence that the network is trained to generate. This issue is discussed in detail in Huszár (2015). A practical advantage of Scheduled Sampling over Professor Forcing is that Scheduled Sampling does not require the additional overhead of having to train a discriminator network. Actor-critic methods have also been explored for improving modeling of long-term dependencies in generative recurrent neural networks Bahdanau et al. (2016). Finally, the idea of matching the behavior of the model when it is generating in a free-running way with its behavior when it is constrained by the observed data (being clamped on the \"visible units\") is precisely that which one obtains when zeroing the maximum likelihood gradient on undirected graphical models with latent variables such as the Boltzmann machine.",
      "startOffset" : 42,
      "endOffset" : 2349
    }, {
      "referenceID" : 10,
      "context" : "The generative RNN has single hidden layer of gated recurrent units (GRU), previously introduced by (Cho et al., 2014b) as a computationally cheaper alternative to LSTM units (Hochreiter and Schmidhuber, 1997).",
      "startOffset" : 100,
      "endOffset" : 119
    }, {
      "referenceID" : 20,
      "context" : ", 2014b) as a computationally cheaper alternative to LSTM units (Hochreiter and Schmidhuber, 1997).",
      "startOffset" : 64,
      "endOffset" : 98
    }, {
      "referenceID" : 22,
      "context" : "Both networks are trained by minibatch stochastic gradient descent with adaptive learning rates and momentum determined by the Adam algorithm (Kingma and Ba, 2014).",
      "startOffset" : 142,
      "endOffset" : 163
    }, {
      "referenceID" : 1,
      "context" : "All of our experiments were implemented using the Theano framework (Al-Rfou et al., 2016).",
      "startOffset" : 67,
      "endOffset" : 89
    }, {
      "referenceID" : 14,
      "context" : "Method MNIST NLL DBN 2hl (Germain et al., 2015) ≈ 84.",
      "startOffset" : 25,
      "endOffset" : 47
    }, {
      "referenceID" : 23,
      "context" : "55 NADE (Larochelle and Murray, 2011) 88.",
      "startOffset" : 8,
      "endOffset" : 37
    }, {
      "referenceID" : 28,
      "context" : "33 EoNADE-5 2hl (Raiko et al., 2014) 84.",
      "startOffset" : 16,
      "endOffset" : 36
    }, {
      "referenceID" : 30,
      "context" : "68 DLGM 8 leapfrog steps (Salimans et al., 2014) ≈ 85.",
      "startOffset" : 25,
      "endOffset" : 48
    }, {
      "referenceID" : 19,
      "context" : "51 DARN 1hl (Gregor et al., 2015) ≈ 84.",
      "startOffset" : 12,
      "endOffset" : 33
    }, {
      "referenceID" : 19,
      "context" : "13 DRAW (Gregor et al., 2015) ≤ 80.",
      "startOffset" : 8,
      "endOffset" : 29
    }, {
      "referenceID" : 27,
      "context" : "We use the standard binarized MNIST dataset Murray and Salakhutdinov (2009). We selected hyperparameters for our model on the validation set and elected to use 512 hidden states and a learning rate of 0.",
      "startOffset" : 44,
      "endOffset" : 76
    }, {
      "referenceID" : 27,
      "context" : "We use the standard binarized MNIST dataset Murray and Salakhutdinov (2009). We selected hyperparameters for our model on the validation set and elected to use 512 hidden states and a learning rate of 0.0001. For all experiments we used a 3-layer GRU as our generator. Unlike our other experiments, we used a convolutional network for the discriminator instead of a bi-directional RNN, as the pixels have a 2D spatial structure. We note that our model achieves the second best reported likelihood on this task, after the PixelRNN, which used a significantly more complicated architecture for its generator van den Oord et al. (2016). Combining Professor Forcing with the PixelRNN would be an interesting area for future research.",
      "startOffset" : 44,
      "endOffset" : 633
    }, {
      "referenceID" : 20,
      "context" : "pen-down, using the standard handwriting IAM-OnDB dataset, which consists of 13,040 handwritten lines written by 500 writers Liwicki and Bunke (2005). For our teacher forcing model, we use the open source implementation Brebisson (2016) and use their hyperparameters which is based on the model in Graves (2013).",
      "startOffset" : 125,
      "endOffset" : 150
    }, {
      "referenceID" : 7,
      "context" : "For our teacher forcing model, we use the open source implementation Brebisson (2016) and use their hyperparameters which is based on the model in Graves (2013).",
      "startOffset" : 69,
      "endOffset" : 86
    }, {
      "referenceID" : 7,
      "context" : "For our teacher forcing model, we use the open source implementation Brebisson (2016) and use their hyperparameters which is based on the model in Graves (2013). For the professor forcing model, we sample for 1000 time steps and run a separate discriminator on non-overlapping segments of length 50 (the number of steps used in the teacher forcing model).",
      "startOffset" : 69,
      "endOffset" : 161
    }, {
      "referenceID" : 32,
      "context" : "This issue was discussed in Theis et al. (2015). However, this is unlikely to be an issue with our evaluation because our method also improved validation set likelihood, whereas a model that achieves quality samples by dropping coverage would have poorer validation set likelihood.",
      "startOffset" : 28,
      "endOffset" : 48
    } ],
    "year" : 2016,
    "abstractText" : "The Teacher Forcing algorithm trains recurrent networks by supplying observed sequence values as inputs during training and using the network’s own one-stepahead predictions to do multi-step sampling. We introduce the Professor Forcing algorithm, which uses adversarial domain adaptation to encourage the dynamics of the recurrent network to be the same when training the network and when sampling from the network over multiple time steps. We apply Professor Forcing to language modeling, vocal synthesis on raw waveforms, handwriting generation, and image generation. Empirically we find that Professor Forcing acts as a regularizer, improving test likelihood on character level Penn Treebank and sequential MNIST. We also find that the model qualitatively improves samples, especially when sampling for a large number of time steps. This is supported by human evaluation of sample quality. Trade-offs between Professor Forcing and Scheduled Sampling are discussed. We produce T-SNEs showing that Professor Forcing successfully makes the dynamics of the network during training and sampling more similar.",
    "creator" : "LaTeX with hyperref package"
  }
}