{
  "name" : "1511.08951.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "MidRank: Learning to rank based on subsequences",
    "authors" : [ "Basura Fernando", "Efstratios Gavves", "Damien Muselet", "Jean Monnet", "Tinne Tuytelaars" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "The objective of supervised learning-to-rank is to learn from training sequences a method that correctly orders unknown test sequences. This topic has been widely studied over the last years. Some applications include video analysis [35], person re-identification [38], zeroshot recognition [27], active learning [23], dimensionality reduction [9], 3D feature analysis [37], binary code learning [24], learning from privileged information [33],\n∗The work was conducted at KU Leuven, ESAT-PSI.\ninterestingness prediction [16] and action recognition using rank pooling [12]. In particular, we focus on image re-ranking. Image re-ranking is useful in modern image search tools to facilitate user specific interests by re-ordering images based on some user specific criteria. In this context, we re-order the top-k retrieved images for example from an image search [15, 2] based on some criteria such as interestingness [17] or chronology [21, 14, 29, 13]. In this paper, we propose a new, efficient and accurate supervised learning-to-rank method that uses the information in image subsequences effectively for image re-ranking.\nMost learning-to-rank methods rely on pair-wise cues and constraints. However, pairs of ranked images provide rather weak, and often ambiguous, constraints, as illustrated in Fig. 1. Especially when complex and structured data is involved, considering only pairs during training may confuse the learning of rankers as shown in [7, 25, 42]. Learning-to-rank is in fact a prediction task on lists of data/images. Treatment of pairs of images as independent and identically distributed random variables during training is not ideal [7]. It is, therefore, better, to consider longer subsequences within a sequence, which contain more information than pair of elements, see Fig. 1.\nTo exploit the structure in long sequences, list-wise methods [7, 36, 39, 40, 41, 42] optimize for ranking losses defined over sequences. Although such an approach can exploit the structure in sequences, working on long se-\nar X\niv :1\n51 1.\n08 95\n1v 1\n[ cs\n.C V\n] 2\nquences introduces a new problem. More specifically, as the number of wrong permutations grows exponentially with respect to the sequence length, list-wise methods often end up with a more difficult learning problem [25]. Hence, list-wise methods often lead to over-fitting, as we also observe in our experimental evaluations.\nTo overcome the above limitations, we propose to use subsequences to learn-to-rank. On one hand, the increased length of subsequences brings more information and less uncertainty than pairs. On the other hand, compared to full sequences learning on subsequences allows more regularity and better learnability.\nNote that the training subsequences can be generated without any additional labeling cost: if the training set consists of sequences, we sub-sample; if the training set consists of pairs, we build subsequences by exploiting the transitivity property. We argue that every subsequence of any length sampled from a correctly ordered sequence also has the correct ordering i.e. all subsequences of a correctly ordered sequence are also correctly ordered. We exploit this property as follows. Given the training subsequences, we learn rankers that minimize the zero-one loss\nper subsequence length (or scale). During testing, using the above property, we evaluate all ranking functions of different lengths over the full test sequences using convolution. Then, to obtain the final ordering, we fuse the ranking results of different rankers.\nOur major contributions are threefold. First, we propose a method, MidRank, that exploits subsequences to improve ranking both quantitatively and qualitatively. Second, we present a novel difference based vector representation that exploits the total ordering of subsequences. This representation, which we will refer to as stacked difference vectors, is discriminative and results in learning accurate rankers. Third, we introduce an accurate and efficient polynomial time testing algorithm for the NPhard [30] linear ordering problem to re-rank images in moderately sized sequences.\nWe evaluate our method on three different applications: ranking images of famous people according to relative visual attributes [27], ranking images according to how interesting they are [16] and ranking car images according to the chronology [21]. Given an image search result obtained from an image search engine, we can use our method to re-rank images in a page to satisfy user specific criteria such as interestingness or chronology. Results show a consistent and significant accuracy improvement for an extensive palette of ranking criteria. This paper extends our the conference paper [11]."
    }, {
      "heading" : "2 Related work",
      "text" : "Supervised learning-to-rank algorithms are categorized as point-wise, pair-wise and list-wise methods. Point-wise methods [8], which process each element in the sequence individually, are easy to train but prone to over-fitting. Pair-wise methods [18, 19, 31] compute the differences between two input elements at a time and learn a binary decision function that outputs whether one element precedes the other or vice-versa. These methods are restricted to pair-wise loss functions. Naturally, pair-wise methods do not explicitly exploit any structural information beyond what a pair of elements can yield. List-wise methods [7, 39, 41, 42, 36, 40], on the other hand formulate a loss on whole lists, thus being able to optimize more relevant ranking measures like the NDCG or the KendallTau.\nWe present MidRank, which belongs to a fourth family of learning to rank methods, that is positioned between pair-wise and list-wise methods. Similar to pair-wise methods, MidRank uses pairwise relations but extends to more informative subsequences, by considering multiple pairs within a subsequence simultaneously. Similar to list-wise methods, MidRank optimizes a list-wise ranking loss, but unlike most list-wise methods we use zero-one sequence loss. This is done at sub-sequences thus allowing to exploit the regularity in them during learning.\nIn [10] Dokania et al. propose to optimize average precision information retrieval loss using point-wise and pair-wise feature representations. However, this method only focuses on information retrieval.\nMidRank is also different from existing methods that use multiple weak rankers, such as LambdaMART [39] and AdaRank [41], which propose a linear combination of weak rankers, with iterative re-weighting of the training samples and rankers during training. In contrast, MidRank learns multiple ranking functions, one for each subsequence length, therefore focusing more on the latent structure inside the subsequences."
    }, {
      "heading" : "3 MidRank",
      "text" : "We start from a training set of ordered image sequences. Each sequence orders the images according to a predetermined criterion, e.g. images ranging from the most to the least happy face or from the oldest to the most modern car. Our goal is to learn from data in a supervised manner a ranker, such that we can order a new list of unseen images according to the same criterion.\nBasic notations. Our training set is composed of N ordered image sequences, D = {Xi,Yi, `i}, i = 1, . . . , N . Xi stands for an image sequence [xi1,x i 2, . . . ,x i `i ] containing `i images, where `i can vary for different sequences Xi. Yi is a permutation vector Yi = [π(1), ..., π(`i)], and represents that the correct order of the images in the sequence is xiπ(1) x i π(2) · · · xiπ(`i). Henceforth, whenever we speak of a sequence X i, we imply that it is unordered, and when we speak of an ordered sequence, we imply a tuple {Xi,Yi, `i}. To reduce notation clutter, whenever it is clear from the context we drop the superscript i referring to the i-th sequence."
    }, {
      "heading" : "3.1 Ranking sequences",
      "text" : "Assume a new list of previously unseen images, X′ = [x′1, ...,x ′ `′ ]. We define a ranking score function z(X′,Y′), which should return the highest score for the correct order of images Y′∗. Given an appropriate loss function δ(·, ·) our learning objective is\narg min ϑ\nδ(Y′ ∗ , Ŷ′), (1)\nŶ ′ = arg max\nY′ z(X\n′ ,Y ′ ;ϑ), (2)\nwhere Ŷ ′ is the highest scoring order for X′ and ϑ are the parameters for our ranking score function.\nThe score function z(X′,Y′;ϑ) should be applicable for any length `′ that a new sequence X′ might have. To this end we decompose the sequence X′ into a set of subsequences of a particular length, λ. We only consider consecutive subsequences, i.e. subsequences of the form Xj ′ = [x′j:j+λ]. The following proposition holds for any λ ∈ [2...`′]:\nProposition 1. A sequence of length ` is correctly ordered if and only if all of its `−λ+1 consecutive subsequences of length λ are correctly ordered.\nThis proposition follows easily from the transitivity property of inequalities. We have, therefore, transformed our goal from ranking an unconstrained sequence of images, to ranking images inside each of the constrained, length-specific subsequences. To get to the final ranking of the original sequence we need to combine the rankings from the subsequences. Based on proposition 1, we define the ranking inference as\nŶ′ = arg max Y′ `−λ+1∑ j=1 z(X ′ j ,Y ′ j ;ϑ). (3)\nwith Y ′ j = [π ′ (j)...π ′ (j + λ − 1)] and z(·) the ranking score function for fixed-length subsequences. The simplest choice for z(·) would be a linear classifier, i.e. z(X ′ ,Y ′ ;ϑ) = ϑTψ(X ′ ,Y ′ ), where ψ(Xi,Yi) is a feature function that we will revisit in the next subsection. However, since eq. (3) sums the ranking scores for all subsequences together, a non-linear feature mapping is to be preferred, otherwise the effect of different\nsubsequences will be cancelled out. In practice, we use z(X ′ ,Y ′ ;ϑ) = sign(ϑTψ(X ′ ,Y ′ )).|ϑTψ(X′ ,Y′)| 12 . It is worth noting that the ranking inference of eq. (3) is a generalization of the inference in pairwise ranking methods, like RankSVM [19], where λ = 2. Moreover, eq. (3) is similar in spirit to convolutional neural network models [20], which decompose the input of unconstrained size to a series of overlapping operations."
    }, {
      "heading" : "3.2 Training λ-subsequence rankers",
      "text" : "Having decomposed an unconstrained ranking problem into a combination of constrained, length-specific subsequence ranking problems, we need a learning algorithm for optimizing ϑ. Considering prop. (1) and eq. (3) we train the parameters ϑ for a given subsequence length, as follows\narg min ϑ\nµ 2 ‖ϑ‖2 + ∑ i,j Li(λ)j\nLi(λ)j =max{0, 1− δ(Y i j ,Y i∗ j ) ϑ T · ψ(Xij ,Yij)}. (4)\nThe loss function Li(λ)j measures the loss of the j-th subsequence of length λ of the i-th original sequence. For discriminative learning in eq. (4) we need both positive and negative subsequences. During training we sample subsequences of standardized lengths. Although we can mine positive subsequences of all possible lengths, in practice we focus on subsequences up to length 7- 10. To generate negative subsequences during training, we scramble the correct order randomly sampled subsequences. For each positive subsequence of length λ, we can generate theoretically up to λ! − 1 negatives. However, this would create a heavily imbalanced dataset, which might influence the generalization of the learnt rankers [1]. Furthermore, keeping all possible negative subsequences would have a severe impact on memory. Instead, we generate as many negative subsequences as our positives. For the optimization of eq. (4) we use the stochastic dual coordinate ascent (SDCA) method [32], which can handle imbalanced or very large training problems [32]. In practice, when adding more negatives we did not witness any significant differences.\nThe δ(·) function operates as a weight coefficient. The optimization uses indirectly the ranking disagreements to emphasize the wrong classifications proportionally to the\nmagnitude of the δ(·) disagreement value. At the same time, the optimization is expressed as a zero-one loss based classification using hinge loss. This allows to maximize the margin between the correct and the incorrect subsequences of specific length.\nAs we are interested in obtaining the optimal ranking, we could implement δ(·) using any ranking metric, such as sequence accuracy, Kendall-Tau or the NDCG. From the above metrics the sequence accuracy, δ(Y,Y′∗) = 2[Y = Y′ ∗ ] − 1 is the strictest one, where [·] is the Iverson’s bracket. List-wise methods usually employ relatively relaxed ranking metrics, e.g. based on the NDCG or Kendall-Tau measure. This is mostly because for longer, unconstrained sequences on which they operate directly, the zero-one loss is too restrictive. In our case, the advantage is we have standardized, relatively small, and lengthspecific subsequences, on which the zero-one loss can be easily applied. With zero-one loss we observe better discrimination of correct subsequences from incorrect ones and, therefore, better generalization. To this end in our implementation we use the zero-one loss, although other measures can also be easily introduced."
    }, {
      "heading" : "3.3 Ranking-friendly feature maps",
      "text" : "To learn accurate rankers we need discriminative feature representations ψ(Xij ,Y i j). We discuss three different representations for X(λ).\nMean pairwise difference representation. Herbrich et al. [18] eloquently showed that the difference of vector representation yields accurate results for learning a pairwise ranking function.\nFor this representation we have ψ(X,Y) = 1 |{(i,j)|xπ(i) xπ(j)}| ∑ ∀{i,j|xπ(i) xπ(j)}(xπ(i) − xπ(j)). The mean pairwise difference representation is perhaps the most popular choice in the learning-to-rank literature [18, 34, 19, 27, 24, 10]. In the specific case of λ = 2 we end up with the standard rank SVM [19] and the SVR [34] learning objectives.\nStacked representation. Standard pairwise max margin rankers make the simplifying assumption that a sequence is equivalent to a collection of pairwise inequalities. To exploit the structural information beyond\npairs, we propose to use the stacked representation as ψ(X,Y) = [xTπ(1), . . . ,x T π(`)] T for subsequences.\nAn interesting property of stacked representations comes from the field of combinatorial geometry [4]. From combinatorial geometry we know that all permutations of a vector are vertices of a Birkhoff polytope [4]. As the Birkhoff polytope is provably convex, there exists at least one hyperplane that separates each vertex/permutation from all others. Hence, there exists also at least one hyperplane to separate the optimally permutated sequence X∗l from all others, i.e., we have a linearly separable problem. Of course this linear separability applies only for the different permutations of a particular list of elements Xil . Thus is not guaranteed that all correctly permuted Xi∗l ,∀i training sequences will be linearly separable from all incorrectly permuted ones. However, this property ensures that from all possible permutations of a sequence, the correct one can always be linearly separated from the incorrect ones.\nThe same advantage of better separability between different orderings of the same sequence could be obtained by nonlinear kernels. Such kernels, however, are too expensive to apply on many realistic scenarios, when thousands of sequences are considered at a time. Furthermore, the design of such kernels is application dependent, thus making them less general.\nStacked difference representation. Inspired from [18] and the stacked representations, we can also represent a sequence of images as ψ(X,Y) = [(xπ(1) − xπ(2))\nT , . . . , (xπ(λ−1)−xπ(λ))T ]T . Similar to mean pairwise difference representations, they model only the difference between neighboring elements in a rank, thus being invariant to the absolute magnitudes of the elements in X`. Furthermore it is easy to show that stacked difference representations maintain total order structure (proof in supplementary material1). As a result, if there is some latent structure in the subsequence explaining why a particular order is correct, the stacked difference representation will capture it, to the extent of the feature’s capacity.\n1users.cecs.anu.edu.au/˜basura/ICCV15_sup.pdf"
    }, {
      "heading" : "3.4 Multi-length MidRank",
      "text" : "So far we focused on subsequences of fixed length λ. A natural extension is to consider multiple subsequence lengths, as different lengths are likely to capture different aspects of the example sequences and subsequences. To train a multi-length ranker we simply consider each length in eq. (4) separately, namely λ = 2, 3, 4, . . . , L. To infer the final ranking we need to fuse the output from the different length rankers. To this end we propose a weighted majority voting scheme.\nFor each test instance X′ we obtain a ranking per λ and the respective ranking score from z(·). As a result we have rankings for each of the L − 1 rankers. Then, each image in the test sequence gets a vote for its particular position as returned from each ranker. Also, each image gets a voting score that is proportional to the ranking score from z(·), and, therefore, the confidence of the ranker for placing that image to the particular position. We weight the image position with the voting scores and compute the weighted votes for all images for all positions. Then, we decide the final position of each image starting rank 1, selecting the image with the highest weighted vote at rank 1. Then we eliminate this image from the subsequence comparisons and continue iteratively, until there are no images to be ranked."
    }, {
      "heading" : "4 Efficient inference",
      "text" : "Solving eq. (3) requires an explicit search over the space of all possible permutations in Y, which amounts to l!. Hence, for a successful as well as practical inference we need to resolve this combinatorial issue. Inspired by random forests [6] and the best-bin-first search strategies [3], we propose the following greedy search algorithm. For a visual illustration of the algorithm we refer to Figure 2.\nWe start from an initial ranking solution Ŷ′(0) obtained from a less accurate, but straightforward ranking algorithm (i.e. RankSVM). Given Ŷ′(0), we generate a set of permutations denoted by {Ŷ′(1)}, such that the new permutations are obtained by only swapping a pair of elements of Ŷ′(0). From all permutations of {Ŷ′(1)}, we compute the ranking scores using Eq. 3 and pick the permutation with the maximum score denoted by Ŷ′(1). If this score is larger than the score of the parent permutation\n(i.e. ∑ z(X ′\nj , Ŷ ′(1) j ;ϑ, λ) >\n∑ z(X ′\nj , Ŷ ′(0) j ;ϑ, λ)), we\nset the permutation Ŷ′(1) as the new parent and traverse the solution space recursively using the same strategy. Permutations that have already been visited are removed from any future searches. The procedure of traversing through the above strategy forms a tree of permutations (see Figure 2). We stop when i) no other solution with a higher score can be found (score criterion) or ii) when we reach the l-th level of the permutation tree (depth criterion).\nAt each level of the tree, we traverse a maximum of `i · (`i − 1)/2 nodes. Experimentally, we observe that after only a few levels we satisfy the score criterion, thus having an average complexity of O(`i2). When the depth criterion is satisfied, we have the worst case complexity of O(`i3).\nTo further ensure that the search algorithm has not converged to a poor local optimum, we repeat the above procedure starting from different initial solutions. For maximum coverage of the solution space, we carefully select the new Ŷ′(0), such that Ŷ′(0) was not seen in the previous trees. As we also show in the experiments, the proposed search strategy allows for obtaining good results using very few trees. Inarguably, our efficient inference enables ranking with subsequences, as the\nexhaustive search is impractical for moderately sized sequences (8-15 elements long), and intractable for longer sequences."
    }, {
      "heading" : "5 Experiments",
      "text" : "We select three supervised image re-ranking applications to compare MidRank with other supervised learning-torank algorithms, namely, ranking public figures (section 5.2), ordering images based on interestingness (section 5.3) and chronological ordering of car images (section 5.4). Next, we analyze the properties and the efficiency of MidRank under different parameterizations in section 5.5."
    }, {
      "heading" : "5.1 Evaluation criteria & implementation details",
      "text" : "We evaluate all methods with respect to the following ranking metrics. First, we use the normalized discounted cumulative gain NDCG, commonly used to evaluate ranking algorithms [25]. The discounted cumulative gain at position k is defined as DCG@k = ∑k i=1 2reli−1 log2(i+1)\n, where reli is the relevance of the image at position i. To obtain the normalized DCG, theDCG@k score is divided by the ideal DCG score. NDCG, whose range is [0, 1], is strongly non-linear. For example going from 0.940 to 0.950 indicates a significant improvement.\nWe also use the Kendall-Tau, which captures better how close we are to the perfect ranking. The KendallTau accuracy is defined as KT = l\n+−l− 0.5l(l−1) , where l +, l−\nstand for the number of all pairs in the sequence that are correctly and incorrectly ordered respectively, and l = l+ + l−. Kendall-Tau varies between −1 and +1 where a perfect ranker will have a score of +1. For completeness we also use pairwise accuracy as an evaluation criterion in which we count the percentage of correctly ranked pairs of elements in all sequences.\nWe compare our MidRank with point-wise methods such as SVR [34], McRank [22], pair-wise methods such as RankSVM [19], Relative Attributes [27] and CRR [31]. We also compare with list-wise methods such as AdaRank [41], LambdaMART [39], ListNET [7] and\nListMLE [40]. For all methods we use the publically available code as provided by the authors, the same features and recommended settings for fine-tuning. All these baselines and MidRank take the same set of training sequences as input. There is no overlap between elements of train and test sequences. All training sequences are sampled from the training set of each dataset and testing sequences are sampled from the testing set. We make these train and test sequences along with the data publicly available. We evaluate all methods on a large number of 20,000 test sequences. We experimentally found that the standard deviations are quite small for all methods.\nFor MidRank we pick the values for any hyperparameters (e.g. the cost parameter in eq. (4)) after crossvalidation. We investigate MidRank rankers of length 3−8 and merge the results with the majority weighted voting, as described in Sect. 3.4. For the efficient inference, we initialize the parent node with the solution obtained from the pair-wise RankSVM [19]. We also tried various ranking score normalizations between the different length rankers, but we found experimentally that results did not improve significantly. Consequently, we opted for directly using the unnormalized ranking scores from different length rankers. In all cases the feature vectors are L2-normalized."
    }, {
      "heading" : "5.2 Ranking Public Figures",
      "text" : "First we evaluate MidRank on ranking public figure images with respect to relative visual attributes [27], using the features and the train/test splits provided by [27]. The dataset consists of images from eight public figures and eleven visual attributes of theirs, such as big lips, white and chubby. Our goal is to learn rankers for these attributes. Since there 8 public figures, we report results on the longest possible test sequence size composed of 8 images. For each of the 11 attributes we sample 10,000 train sequences of length 8 and 20,000 test sequences, totaling to 220,000 test sequences for all attributes. We use the standard GIST features provided with the dataset. The results are reported by taking the average over all eleven attributes and over all test sequences. See results in Table 1.\nWe observe that MidRank improves the accuracy of the ranking significantly for all the evaluation criteria. For Kendall-Tau, MidRank brings a +10.5% absolute improvement. It is worth mentioning that for this dataset the best individual MidRank function was of length 7, which in isolation scored a 0.684 Kendall-Tau accuracy."
    }, {
      "heading" : "5.3 Ranking Interestingness",
      "text" : "Next, we evaluate MidRank on ranking images according to how interesting they are. We consider train and test sequences of size 20. It is not possible to consider much\nlonger sequences as the annotation pool for interestingness is limited in this dataset. In practice even for humans rating more than 20 images based on interestingness would be difficult. We use the scene categories dataset from [26], whose images were later annotated with respect to interestingness by [17]. We extract GIST [26] features and construct 10,000 train sequences and 20,000 test sequences. Note that this is a difficult task, as interestingness is a subjective criterion which can be attributed to many different factors within an image. See results in Table 1.\nWe observe that also for this dataset MidRank has a significantly better accuracy than the competitor methods for all the evaluation criteria. For visual results obtained from our method, see Fig. 3 (random example). As we can see, MidRank returns a rank which visually is very close to the ground truth."
    }, {
      "heading" : "5.4 Chronological ordering of cars",
      "text" : "As a final application, we consider the task of re-ranking images chronologically. We use the car dataset of [21]. The chronology of the cars is in the range 1920, 1921, ..., 1999. As image representation we use 64-Gaussian Fisher vectors [28] computed on dense SIFT features, after being reduced to 64 dimensions with PCA. To control the dimensionality we also reduce the Fisher vectors to 1000 dimensions using PCA again. Similar to the pre-\nvious experiment, we generate 10,000 training sequences and 20,000 test sequences of length 20. See results in Table 1.\nAgain, MidRank obtains significantly better results than the competitor methods for all the evaluation criteria and especially for the Kendall-Tau accuracy (+7.1%). We show some visual results in Fig. 3. We also experimented with training sequence lengths of 5, 10, 15, 20 and testing sequence lengths of 5, 10, 20, 80. Due to brevity and space, we report on test sequences of length 20 only (which seems a more practical scenario in image search applications). However, in all these cases MidRank outperforms all other methods. Note that despite the uncanny resemblance between the cars, especially the older ones, MidRank returns a ranking very close to the true one."
    }, {
      "heading" : "5.5 Detailed analysis of MidRank",
      "text" : "Effect of subsequence length on MidRank. Next, we evaluate the relation between the MidRank accuracy and the training subsequence sizes. We use sequences of size 20 for training and testing generated from the car dataset. We evaluate different MidRank ranking functions of size 3 up to 8. We plot the results in Fig. 4(a).\nFor all ranking measures the best MidRank is of size 7. Interestingly, the ranking performance gradually increases up to a point as the training subsequence size increases. This indicates that MidRank ranking models\ntrained on moderately sized subsequences perform better than very small or very large ones. Small MidRank models are easy to train (small training errors), but solve a relatively easy ranking sub-problem. In contrast, large MidRank models are more difficult to train (larger training errors). Our experiments suggest that moderately sized subsequences are the most suitable for MidRank. It is also interesting to see that all three ranking measures used are consistent (–see Fig. 4(a)). However, the sensitivity of Kendall Tau seems to be larger than the other two ranking criteria (NDCG and pair-accuracy). Evaluating subsequence representations. In this experiment we evaluate the effectiveness of the stacked difference representation introduced in section 3.3 compared to other alternatives see Fig. 5 (left). The max-pooling or the mean pooling of difference vectors of a subsequence hinders useful ranking information, such as subtle variations between neighboring elements. Full stacked difference vectors (option (c)) does not perform as well as other stacked versions (d) and (e), probably due to the curse of dimensionality. Note that we also evaluated the mean representations on longer subsequences (20 images per sequence) and obtained 0.05 points lower in KT than the proposed stacked representations. This shows the best results are obtained with our stacked difference representation. Evaluating efficient inference vs exhaustive inference.\nIn this section we compare our efficient inference strategy with the exhaustive inference. As explained earlier, the exhaustive strategy has a complexity of O(`i!), whereas the proposed efficient inference strategy has an average complexity of O(`i2) and a worst case complexity of O(`i 3 ).\nFirst, we plot how the execution time varies during inference for different test sequence sizes. We compare our inference method with the exhaustive search in Fig. 6 (a). For moderately long test sequences, e.g. up to size 8 in this experiment, our method is 50 times faster than exhaustive search. For longer sequences exhaustive inference is not even an option, as the number of possible combinations that need to be explored becomes very impractical, or even intractable for longer sequences.\nOur inference method discovers the optimal order for a sequence of length 20 in 0.75 ± 0.1 seconds, and in practice sequences of up to 500 elements can be easily processed. Hence, MidRank may easily be employed in the standard supervised image ranking and re-ranking scenarios, e.g. improving the image search results based on user preferences.\nIn Fig. 6(b) we show significant improvement in\nexecution time does not hurt the accuracy with respect to the exhaustive search. In this experiment we vary the number of trees used in our efficient algorithm and report the Kendal-Tau score and the percentage of sequences that agrees with the solution obtained with exhaustive search (blue line in Fig. 6(b)). Interestingly, using a single tree, we obtain a better Kendall-Tau score than the one obtained with the exhaustive search. We attribute this to some degree of over-fitting that might occur during learning. With 3 trees we obtain the same solution as exhaustive search for 97% of the times, whereas with five trees we obtain exactly the same results as the exhaustive search.\nEvaluating Majority Voting Scheme. Last, we evaluate the effectiveness of different rank fusion strategies for MidRank using the cars dataset. We compare the proposed weighted majority voting with winner-takes-all strategy in which the ranker with the highest ranking score is used to define the final ordering. We also compare with the best individual ranker, where we use cross-validation to find the best ranker given a test sequence length.\nAs can be seen from Fig. 4(b), the weighted majority voting scheme works best. The results indicate that each ranker from different mid-level structure sizes exploits different types of structural information. Similar conclusions were derived for the other datasets. Evaluating on sequences of different lengths. In this experiment we evaluate how pair accuracy and KT vary for different train and test sequence sizes. We use the cars dataset for this experiment. From results reported in Fig. 5 (right) we see that for smaller test sequences of 5 and 10 the best results are obtained using train subsequences of size 3 or 4. However, for larger test sequences of size 15 and 20 the best results are obtained for train subsequences of size 5, 6 and 7. Interestingly, the largest train subsequence size of 8 reports the worst results. These observations are valid for both pair accuracy as well as Kendall-Tau performance."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this paper we present a supervised learning to rank method, MidRank, that learns from sub-sequences. A novel stacked difference vectors representation and an\neffective ranking algorithm that uses sub-sequences during the learning is presented. The proposed method obtains significant improvements over state-of-the-art pair-wise and list-wise ranking methods. Moreover, we show that by exploiting the structural information and the regularity in sub-sequences, MidRank allows for a better learning of ranking functions on several image ordering tasks.\nAcknowledgement Authors acknowledge the support from the Australian Research Council Centre of Excellence for Robotic Vision (project number CE140100016), the PARIS project (IWT-SBO-Nr.110067), iMinds project HiViz and ANR project SoLStiCe (ANR-13-BS02-0002-01)."
    } ],
    "references" : [ {
      "title" : "Good practice in large-scale learning for image classification",
      "author" : [ "Z. Akata", "F. Perronnin", "Z. Harchaoui", "C. Schmid" ],
      "venue" : "PAMI, 36:507–520",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Three things everyone should know to improve object retrieval",
      "author" : [ "R. Arandjelović", "A. Zisserman" ],
      "venue" : "Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on, pages 2911–2918. IEEE",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Shape indexing using approximate nearest-neighbour search in high-dimensional spaces",
      "author" : [ "J.S. Beis", "D.G. Lowe" ],
      "venue" : "CVPR",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Tres observaciones sobre el algebra lineal",
      "author" : [ "D. Birkhoff" ],
      "venue" : "Universidad Nacional de Tucuman Revista , Serie A, 5:147151",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 1946
    }, {
      "title" : "A theoretical analysis of feature pooling in visual recognition",
      "author" : [ "Y. Boureau", "J. Ponce", "Y. LeCun" ],
      "venue" : "ICML",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Random forests",
      "author" : [ "L. Breiman" ],
      "venue" : "Machine Learning, 45(1):5– 32",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Learning to rank: from pairwise approach to listwise approach",
      "author" : [ "Z. Cao", "T. Qin", "T. Liu", "M. Tsai", "H. Li" ],
      "venue" : "ICML",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Probabilistic retrieval based on staged logistic regression",
      "author" : [ "W.S. Cooper", "F.C. Gey", "D.P. Dabney" ],
      "venue" : "SIGIR",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "Linear ranking analysis",
      "author" : [ "W. Deng", "J. Hu", "J. Guo" ],
      "venue" : "CVPR",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Learning to rank using high-order information",
      "author" : [ "P. Dokania", "A. Behl", "C. Jawahar", "M. Kumar" ],
      "venue" : "ECCV",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Learning to rank based on subsequences",
      "author" : [ "B. Fernando", "E. Gavves", "D. Muselet", "T. Tuytelaars" ],
      "venue" : "ICCV",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Modeling video evolution for action recognition",
      "author" : [ "B. Fernando", "E. Gavves", "J. Oramas", "A. Ghodrati", "T. Tuytelaars" ],
      "venue" : "CVPR",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Color features for dating historical color",
      "author" : [ "B. Fernando", "D. Muselet", "R. Khan", "T. Tuytelaars" ],
      "venue" : null,
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2014
    }, {
      "title" : "Location recognition over large time lags",
      "author" : [ "B. Fernando", "T. Tommasi", "T. Tuytelaars" ],
      "venue" : "Computer Vision and Image Understanding, 139:21–28",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Mining multiple queries for image retrieval: On-the-fly learning of an object-specific mid-level representation",
      "author" : [ "B. Fernando", "T. Tuytelaars" ],
      "venue" : "ICCV",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Interestingness prediction by robust learning to rank",
      "author" : [ "Y. Fu", "T. Hospedales", "T. Xiang", "S. Gong", "Y. Yao" ],
      "venue" : "ECCV",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "The interestingness of images",
      "author" : [ "M. Gygli", "H. Grabner", "H. Riemenschneider", "F. Nater", "L. Van Gool" ],
      "venue" : "ICCV",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Large margin rank boundaries for ordinal regression",
      "author" : [ "R. Herbrich", "T. Graepel", "K. Obermayer" ],
      "venue" : "NIPS, pages 115–132",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Training linear svms in linear time",
      "author" : [ "T. Joachims" ],
      "venue" : "SIGKDD",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "A. Krizhevsky", "I. Sutskever", "G.E. Hinton" ],
      "venue" : "In NIPS",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2012
    }, {
      "title" : "Style-aware mid-level representation for discovering visual connections in space and time",
      "author" : [ "Y.J. Lee", "A. Efros", "M. Hebert" ],
      "venue" : "ICCV",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Mcrank: Learning to rank using multiple classification and gradient boosting",
      "author" : [ "P. Li", "Q. Wu", "C.J. Burges" ],
      "venue" : "NIPS, pages 897–904",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Beyond comparing image pairs: Setwise active learning for relative attributes",
      "author" : [ "L. Liang", "K. Grauman" ],
      "venue" : "CVPR",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Optimizing ranking measures for compact binary code learning",
      "author" : [ "G. Lin", "C. Shen", "J. Wu" ],
      "venue" : "ECCV",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Learning to rank for information retrieval",
      "author" : [ "T.-Y. Liu" ],
      "venue" : "Foundations and Trends in Information Retrieval, 3(3):225– 331",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Modeling the shape of the scene: A holistic representation of the spatial envelope",
      "author" : [ "A. Oliva", "A. Torralba" ],
      "venue" : "IJCV, 42(3):145–175",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Relative attributes",
      "author" : [ "D. Parikh", "K. Grauman" ],
      "venue" : "ICCV",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Improving the fisher kernel for large-scale image classification",
      "author" : [ "F. Perronnin", "J. Sánchez", "T. Mensink" ],
      "venue" : "ECCV",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "and T",
      "author" : [ "K. Rematas", "B. Fernando", "T. Tommasi" ],
      "venue" : "Tuytelaars. Does evolution cause a domain shift? In International Workshop on Visual Domain Adaptation and Dataset Bias - ICCV 2013",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "The linear ordering problem: Instances",
      "author" : [ "T. Schiavinotto", "T. Stützle" ],
      "venue" : "search space analysis and algorithms. Journal of Mathematical Modelling and Algorithms, 3(4):367–402",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Large scale learning to rank",
      "author" : [ "D. Sculley" ],
      "venue" : "NIPS",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Pegasos: Primal estimated sub-gradient solver for svm",
      "author" : [ "S. Shalev-Shwartz", "Y. Singer", "N. Srebro", "A. Cotter" ],
      "venue" : "Mathematical programming, 127(1):3–30",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Learning to rank using privileged information",
      "author" : [ "V. Sharmanska", "N. Quadrianto", "C.H. Lampert" ],
      "venue" : "ICCV",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "A tutorial on support vector regression",
      "author" : [ "A.J. Smola", "B. Schölkopf" ],
      "venue" : "Statistics and computing, 14(3):199–222",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Ranking domainspecific highlights by analyzing edited videos",
      "author" : [ "M. Sun", "A. Farhadi", "S.M. Seitz" ],
      "venue" : "ECCV",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Softrank: optimizing non-smooth rank metrics",
      "author" : [ "M. Taylor", "J. Guiver", "S. Robertson", "T. Minka" ],
      "venue" : "Proceedings of the 2008 International Conference on Web Search and Data Mining, pages 77–86. ACM",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Learning to rank 3d features",
      "author" : [ "O. Tuzel", "M. Liu", "Y. Taguchi", "A. Raghunathan" ],
      "venue" : "ECCV",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Person reidentification by video ranking",
      "author" : [ "T. Wang", "S. Gong", "X. Zhu", "S. Wang" ],
      "venue" : "ECCV",
      "citeRegEx" : "38",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Adapting boosting for information retrieval measures",
      "author" : [ "Q. Wu", "C.J. Burges", "K.M. Svore", "J. Gao" ],
      "venue" : "Information Retrieval, 13(3):254–270",
      "citeRegEx" : "39",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Listwise approach to learning to rank: theory and algorithm",
      "author" : [ "F. Xia", "T.-Y. Liu", "J. Wang", "W. Zhang", "H. Li" ],
      "venue" : "ICML, pages 1192–1199. ACM",
      "citeRegEx" : "40",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Adarank: a boosting algorithm for information retrieval",
      "author" : [ "J. Xu", "H. Li" ],
      "venue" : "SIGIR",
      "citeRegEx" : "41",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "A support vector method for optimizing average precision",
      "author" : [ "Y. Yue", "T. Finley", "F. Radlinski", "T. Joachims" ],
      "venue" : "ACM SIGIR, pages 271–278. ACM",
      "citeRegEx" : "42",
      "shortCiteRegEx" : null,
      "year" : 2007
    } ],
    "referenceMentions" : [ {
      "referenceID" : 34,
      "context" : "Some applications include video analysis [35], person re-identification [38], zeroshot recognition [27], active learning [23], dimensionality reduction [9], 3D feature analysis [37], binary code learning [24], learning from privileged information [33],",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 37,
      "context" : "Some applications include video analysis [35], person re-identification [38], zeroshot recognition [27], active learning [23], dimensionality reduction [9], 3D feature analysis [37], binary code learning [24], learning from privileged information [33],",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 26,
      "context" : "Some applications include video analysis [35], person re-identification [38], zeroshot recognition [27], active learning [23], dimensionality reduction [9], 3D feature analysis [37], binary code learning [24], learning from privileged information [33],",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 22,
      "context" : "Some applications include video analysis [35], person re-identification [38], zeroshot recognition [27], active learning [23], dimensionality reduction [9], 3D feature analysis [37], binary code learning [24], learning from privileged information [33],",
      "startOffset" : 121,
      "endOffset" : 125
    }, {
      "referenceID" : 8,
      "context" : "Some applications include video analysis [35], person re-identification [38], zeroshot recognition [27], active learning [23], dimensionality reduction [9], 3D feature analysis [37], binary code learning [24], learning from privileged information [33],",
      "startOffset" : 152,
      "endOffset" : 155
    }, {
      "referenceID" : 36,
      "context" : "Some applications include video analysis [35], person re-identification [38], zeroshot recognition [27], active learning [23], dimensionality reduction [9], 3D feature analysis [37], binary code learning [24], learning from privileged information [33],",
      "startOffset" : 177,
      "endOffset" : 181
    }, {
      "referenceID" : 23,
      "context" : "Some applications include video analysis [35], person re-identification [38], zeroshot recognition [27], active learning [23], dimensionality reduction [9], 3D feature analysis [37], binary code learning [24], learning from privileged information [33],",
      "startOffset" : 204,
      "endOffset" : 208
    }, {
      "referenceID" : 32,
      "context" : "Some applications include video analysis [35], person re-identification [38], zeroshot recognition [27], active learning [23], dimensionality reduction [9], 3D feature analysis [37], binary code learning [24], learning from privileged information [33],",
      "startOffset" : 247,
      "endOffset" : 251
    }, {
      "referenceID" : 15,
      "context" : "interestingness prediction [16] and action recognition using rank pooling [12].",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 11,
      "context" : "interestingness prediction [16] and action recognition using rank pooling [12].",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 14,
      "context" : "In this context, we re-order the top-k retrieved images for example from an image search [15, 2] based on some criteria such as interestingness [17] or chronology [21, 14, 29, 13].",
      "startOffset" : 89,
      "endOffset" : 96
    }, {
      "referenceID" : 1,
      "context" : "In this context, we re-order the top-k retrieved images for example from an image search [15, 2] based on some criteria such as interestingness [17] or chronology [21, 14, 29, 13].",
      "startOffset" : 89,
      "endOffset" : 96
    }, {
      "referenceID" : 16,
      "context" : "In this context, we re-order the top-k retrieved images for example from an image search [15, 2] based on some criteria such as interestingness [17] or chronology [21, 14, 29, 13].",
      "startOffset" : 144,
      "endOffset" : 148
    }, {
      "referenceID" : 20,
      "context" : "In this context, we re-order the top-k retrieved images for example from an image search [15, 2] based on some criteria such as interestingness [17] or chronology [21, 14, 29, 13].",
      "startOffset" : 163,
      "endOffset" : 179
    }, {
      "referenceID" : 13,
      "context" : "In this context, we re-order the top-k retrieved images for example from an image search [15, 2] based on some criteria such as interestingness [17] or chronology [21, 14, 29, 13].",
      "startOffset" : 163,
      "endOffset" : 179
    }, {
      "referenceID" : 28,
      "context" : "In this context, we re-order the top-k retrieved images for example from an image search [15, 2] based on some criteria such as interestingness [17] or chronology [21, 14, 29, 13].",
      "startOffset" : 163,
      "endOffset" : 179
    }, {
      "referenceID" : 12,
      "context" : "In this context, we re-order the top-k retrieved images for example from an image search [15, 2] based on some criteria such as interestingness [17] or chronology [21, 14, 29, 13].",
      "startOffset" : 163,
      "endOffset" : 179
    }, {
      "referenceID" : 6,
      "context" : "Especially when complex and structured data is involved, considering only pairs during training may confuse the learning of rankers as shown in [7, 25, 42].",
      "startOffset" : 144,
      "endOffset" : 155
    }, {
      "referenceID" : 24,
      "context" : "Especially when complex and structured data is involved, considering only pairs during training may confuse the learning of rankers as shown in [7, 25, 42].",
      "startOffset" : 144,
      "endOffset" : 155
    }, {
      "referenceID" : 41,
      "context" : "Especially when complex and structured data is involved, considering only pairs during training may confuse the learning of rankers as shown in [7, 25, 42].",
      "startOffset" : 144,
      "endOffset" : 155
    }, {
      "referenceID" : 6,
      "context" : "Treatment of pairs of images as independent and identically distributed random variables during training is not ideal [7].",
      "startOffset" : 118,
      "endOffset" : 121
    }, {
      "referenceID" : 6,
      "context" : "To exploit the structure in long sequences, list-wise methods [7, 36, 39, 40, 41, 42] optimize for ranking losses defined over sequences.",
      "startOffset" : 62,
      "endOffset" : 85
    }, {
      "referenceID" : 35,
      "context" : "To exploit the structure in long sequences, list-wise methods [7, 36, 39, 40, 41, 42] optimize for ranking losses defined over sequences.",
      "startOffset" : 62,
      "endOffset" : 85
    }, {
      "referenceID" : 38,
      "context" : "To exploit the structure in long sequences, list-wise methods [7, 36, 39, 40, 41, 42] optimize for ranking losses defined over sequences.",
      "startOffset" : 62,
      "endOffset" : 85
    }, {
      "referenceID" : 39,
      "context" : "To exploit the structure in long sequences, list-wise methods [7, 36, 39, 40, 41, 42] optimize for ranking losses defined over sequences.",
      "startOffset" : 62,
      "endOffset" : 85
    }, {
      "referenceID" : 40,
      "context" : "To exploit the structure in long sequences, list-wise methods [7, 36, 39, 40, 41, 42] optimize for ranking losses defined over sequences.",
      "startOffset" : 62,
      "endOffset" : 85
    }, {
      "referenceID" : 41,
      "context" : "To exploit the structure in long sequences, list-wise methods [7, 36, 39, 40, 41, 42] optimize for ranking losses defined over sequences.",
      "startOffset" : 62,
      "endOffset" : 85
    }, {
      "referenceID" : 24,
      "context" : "More specifically, as the number of wrong permutations grows exponentially with respect to the sequence length, list-wise methods often end up with a more difficult learning problem [25].",
      "startOffset" : 182,
      "endOffset" : 186
    }, {
      "referenceID" : 29,
      "context" : "Third, we introduce an accurate and efficient polynomial time testing algorithm for the NPhard [30] linear ordering problem to re-rank images in moderately sized sequences.",
      "startOffset" : 95,
      "endOffset" : 99
    }, {
      "referenceID" : 26,
      "context" : "We evaluate our method on three different applications: ranking images of famous people according to relative visual attributes [27], ranking images according to how interesting they are [16] and ranking car images according to the chronology [21].",
      "startOffset" : 128,
      "endOffset" : 132
    }, {
      "referenceID" : 15,
      "context" : "We evaluate our method on three different applications: ranking images of famous people according to relative visual attributes [27], ranking images according to how interesting they are [16] and ranking car images according to the chronology [21].",
      "startOffset" : 187,
      "endOffset" : 191
    }, {
      "referenceID" : 20,
      "context" : "We evaluate our method on three different applications: ranking images of famous people according to relative visual attributes [27], ranking images according to how interesting they are [16] and ranking car images according to the chronology [21].",
      "startOffset" : 243,
      "endOffset" : 247
    }, {
      "referenceID" : 10,
      "context" : "This paper extends our the conference paper [11].",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 7,
      "context" : "Point-wise methods [8], which process each element in the sequence individually, are easy to train but prone to over-fitting.",
      "startOffset" : 19,
      "endOffset" : 22
    }, {
      "referenceID" : 17,
      "context" : "Pair-wise methods [18, 19, 31] compute the differences between two input elements at a time and learn a binary decision function that outputs whether one element precedes the other or vice-versa.",
      "startOffset" : 18,
      "endOffset" : 30
    }, {
      "referenceID" : 18,
      "context" : "Pair-wise methods [18, 19, 31] compute the differences between two input elements at a time and learn a binary decision function that outputs whether one element precedes the other or vice-versa.",
      "startOffset" : 18,
      "endOffset" : 30
    }, {
      "referenceID" : 30,
      "context" : "Pair-wise methods [18, 19, 31] compute the differences between two input elements at a time and learn a binary decision function that outputs whether one element precedes the other or vice-versa.",
      "startOffset" : 18,
      "endOffset" : 30
    }, {
      "referenceID" : 6,
      "context" : "List-wise methods [7, 39, 41, 42, 36, 40], on the other hand formulate a loss on whole lists, thus being able to optimize more relevant ranking measures like the NDCG or the KendallTau.",
      "startOffset" : 18,
      "endOffset" : 41
    }, {
      "referenceID" : 38,
      "context" : "List-wise methods [7, 39, 41, 42, 36, 40], on the other hand formulate a loss on whole lists, thus being able to optimize more relevant ranking measures like the NDCG or the KendallTau.",
      "startOffset" : 18,
      "endOffset" : 41
    }, {
      "referenceID" : 40,
      "context" : "List-wise methods [7, 39, 41, 42, 36, 40], on the other hand formulate a loss on whole lists, thus being able to optimize more relevant ranking measures like the NDCG or the KendallTau.",
      "startOffset" : 18,
      "endOffset" : 41
    }, {
      "referenceID" : 41,
      "context" : "List-wise methods [7, 39, 41, 42, 36, 40], on the other hand formulate a loss on whole lists, thus being able to optimize more relevant ranking measures like the NDCG or the KendallTau.",
      "startOffset" : 18,
      "endOffset" : 41
    }, {
      "referenceID" : 35,
      "context" : "List-wise methods [7, 39, 41, 42, 36, 40], on the other hand formulate a loss on whole lists, thus being able to optimize more relevant ranking measures like the NDCG or the KendallTau.",
      "startOffset" : 18,
      "endOffset" : 41
    }, {
      "referenceID" : 39,
      "context" : "List-wise methods [7, 39, 41, 42, 36, 40], on the other hand formulate a loss on whole lists, thus being able to optimize more relevant ranking measures like the NDCG or the KendallTau.",
      "startOffset" : 18,
      "endOffset" : 41
    }, {
      "referenceID" : 9,
      "context" : "In [10] Dokania et al.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 38,
      "context" : "MidRank is also different from existing methods that use multiple weak rankers, such as LambdaMART [39] and AdaRank [41], which propose a linear combination of weak rankers, with iterative re-weighting of the training samples and rankers during training.",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 40,
      "context" : "MidRank is also different from existing methods that use multiple weak rankers, such as LambdaMART [39] and AdaRank [41], which propose a linear combination of weak rankers, with iterative re-weighting of the training samples and rankers during training.",
      "startOffset" : 116,
      "endOffset" : 120
    }, {
      "referenceID" : 18,
      "context" : "(3) is a generalization of the inference in pairwise ranking methods, like RankSVM [19], where λ = 2.",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 19,
      "context" : "(3) is similar in spirit to convolutional neural network models [20], which decompose the input of unconstrained size to a series of overlapping operations.",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 0,
      "context" : "However, this would create a heavily imbalanced dataset, which might influence the generalization of the learnt rankers [1].",
      "startOffset" : 120,
      "endOffset" : 123
    }, {
      "referenceID" : 31,
      "context" : "(4) we use the stochastic dual coordinate ascent (SDCA) method [32], which can handle imbalanced or very large training problems [32].",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 31,
      "context" : "(4) we use the stochastic dual coordinate ascent (SDCA) method [32], which can handle imbalanced or very large training problems [32].",
      "startOffset" : 129,
      "endOffset" : 133
    }, {
      "referenceID" : 17,
      "context" : "[18] eloquently showed that the difference of vector representation yields accurate results for learning a pairwise ranking function.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 17,
      "context" : "The mean pairwise difference representation is perhaps the most popular choice in the learning-to-rank literature [18, 34, 19, 27, 24, 10].",
      "startOffset" : 114,
      "endOffset" : 138
    }, {
      "referenceID" : 33,
      "context" : "The mean pairwise difference representation is perhaps the most popular choice in the learning-to-rank literature [18, 34, 19, 27, 24, 10].",
      "startOffset" : 114,
      "endOffset" : 138
    }, {
      "referenceID" : 18,
      "context" : "The mean pairwise difference representation is perhaps the most popular choice in the learning-to-rank literature [18, 34, 19, 27, 24, 10].",
      "startOffset" : 114,
      "endOffset" : 138
    }, {
      "referenceID" : 26,
      "context" : "The mean pairwise difference representation is perhaps the most popular choice in the learning-to-rank literature [18, 34, 19, 27, 24, 10].",
      "startOffset" : 114,
      "endOffset" : 138
    }, {
      "referenceID" : 23,
      "context" : "The mean pairwise difference representation is perhaps the most popular choice in the learning-to-rank literature [18, 34, 19, 27, 24, 10].",
      "startOffset" : 114,
      "endOffset" : 138
    }, {
      "referenceID" : 9,
      "context" : "The mean pairwise difference representation is perhaps the most popular choice in the learning-to-rank literature [18, 34, 19, 27, 24, 10].",
      "startOffset" : 114,
      "endOffset" : 138
    }, {
      "referenceID" : 18,
      "context" : "In the specific case of λ = 2 we end up with the standard rank SVM [19] and the SVR [34] learning objectives.",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 33,
      "context" : "In the specific case of λ = 2 we end up with the standard rank SVM [19] and the SVR [34] learning objectives.",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 3,
      "context" : "An interesting property of stacked representations comes from the field of combinatorial geometry [4].",
      "startOffset" : 98,
      "endOffset" : 101
    }, {
      "referenceID" : 3,
      "context" : "From combinatorial geometry we know that all permutations of a vector are vertices of a Birkhoff polytope [4].",
      "startOffset" : 106,
      "endOffset" : 109
    }, {
      "referenceID" : 17,
      "context" : "Inspired from [18] and the stacked representations, we can also represent a sequence of images as ψ(X,Y) = [(xπ(1) − xπ(2)) T , .",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 5,
      "context" : "Inspired by random forests [6] and the best-bin-first search strategies [3], we propose the following greedy search algorithm.",
      "startOffset" : 27,
      "endOffset" : 30
    }, {
      "referenceID" : 2,
      "context" : "Inspired by random forests [6] and the best-bin-first search strategies [3], we propose the following greedy search algorithm.",
      "startOffset" : 72,
      "endOffset" : 75
    }, {
      "referenceID" : 24,
      "context" : "First, we use the normalized discounted cumulative gain NDCG, commonly used to evaluate ranking algorithms [25].",
      "startOffset" : 107,
      "endOffset" : 111
    }, {
      "referenceID" : 0,
      "context" : "NDCG, whose range is [0, 1], is strongly non-linear.",
      "startOffset" : 21,
      "endOffset" : 27
    }, {
      "referenceID" : 33,
      "context" : "We compare our MidRank with point-wise methods such as SVR [34], McRank [22], pair-wise methods such as RankSVM [19], Relative Attributes [27] and CRR [31].",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 21,
      "context" : "We compare our MidRank with point-wise methods such as SVR [34], McRank [22], pair-wise methods such as RankSVM [19], Relative Attributes [27] and CRR [31].",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 18,
      "context" : "We compare our MidRank with point-wise methods such as SVR [34], McRank [22], pair-wise methods such as RankSVM [19], Relative Attributes [27] and CRR [31].",
      "startOffset" : 112,
      "endOffset" : 116
    }, {
      "referenceID" : 26,
      "context" : "We compare our MidRank with point-wise methods such as SVR [34], McRank [22], pair-wise methods such as RankSVM [19], Relative Attributes [27] and CRR [31].",
      "startOffset" : 138,
      "endOffset" : 142
    }, {
      "referenceID" : 30,
      "context" : "We compare our MidRank with point-wise methods such as SVR [34], McRank [22], pair-wise methods such as RankSVM [19], Relative Attributes [27] and CRR [31].",
      "startOffset" : 151,
      "endOffset" : 155
    }, {
      "referenceID" : 40,
      "context" : "We also compare with list-wise methods such as AdaRank [41], LambdaMART [39], ListNET [7] and",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 38,
      "context" : "We also compare with list-wise methods such as AdaRank [41], LambdaMART [39], ListNET [7] and",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 6,
      "context" : "We also compare with list-wise methods such as AdaRank [41], LambdaMART [39], ListNET [7] and",
      "startOffset" : 86,
      "endOffset" : 89
    }, {
      "referenceID" : 26,
      "context" : "Table 1: Evaluating ranking methods on three datasets and applications: ranking public figures using relative attributes of [27], ranking scenes according to how interesting they look [17] and ranking cars according to their manufacturing date [21].",
      "startOffset" : 124,
      "endOffset" : 128
    }, {
      "referenceID" : 16,
      "context" : "Table 1: Evaluating ranking methods on three datasets and applications: ranking public figures using relative attributes of [27], ranking scenes according to how interesting they look [17] and ranking cars according to their manufacturing date [21].",
      "startOffset" : 184,
      "endOffset" : 188
    }, {
      "referenceID" : 20,
      "context" : "Table 1: Evaluating ranking methods on three datasets and applications: ranking public figures using relative attributes of [27], ranking scenes according to how interesting they look [17] and ranking cars according to their manufacturing date [21].",
      "startOffset" : 244,
      "endOffset" : 248
    }, {
      "referenceID" : 39,
      "context" : "ListMLE [40].",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 18,
      "context" : "For the efficient inference, we initialize the parent node with the solution obtained from the pair-wise RankSVM [19].",
      "startOffset" : 113,
      "endOffset" : 117
    }, {
      "referenceID" : 26,
      "context" : "First we evaluate MidRank on ranking public figure images with respect to relative visual attributes [27], using the features and the train/test splits provided by [27].",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 26,
      "context" : "First we evaluate MidRank on ranking public figure images with respect to relative visual attributes [27], using the features and the train/test splits provided by [27].",
      "startOffset" : 164,
      "endOffset" : 168
    }, {
      "referenceID" : 25,
      "context" : "We use the scene categories dataset from [26], whose images were later annotated with respect to interestingness by [17].",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 16,
      "context" : "We use the scene categories dataset from [26], whose images were later annotated with respect to interestingness by [17].",
      "startOffset" : 116,
      "endOffset" : 120
    }, {
      "referenceID" : 25,
      "context" : "We extract GIST [26] features and construct 10,000 train sequences and 20,000 test sequences.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 20,
      "context" : "We use the car dataset of [21].",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 27,
      "context" : "As image representation we use 64-Gaussian Fisher vectors [28] computed on dense SIFT features, after being reduced to 64 dimensions with PCA.",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 4,
      "context" : ": (a) max pooling of difference vectors as in [5], (b) Mean pairwise difference representations, (c) the full stacked difference vector representation between all elements i, j in a sequence, (d) the stacked representation, and (e) the stacked difference vector representation.",
      "startOffset" : 46,
      "endOffset" : 49
    } ],
    "year" : 2015,
    "abstractText" : "We present a supervised learning to rank algorithm that effectively orders images by exploiting the structure in image sequences. Most often in the supervised learning to rank literature, ranking is approached either by analyzing pairs of images or by optimizing a list-wise surrogate loss function on full sequences. In this work we propose MidRank, which learns from moderately sized sub-sequences instead. These sub-sequences contain useful structural ranking information that leads to better learnability during training and better generalization during testing. By exploiting sub-sequences, the proposed MidRank improves ranking accuracy considerably on an extensive array of image ranking applications and datasets.",
    "creator" : "LaTeX with hyperref package"
  }
}