{
  "name" : "1509.01270.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Machine Learning Methods to Analyze Arabidopsis Thaliana Plant Root Growth",
    "authors" : [ "*Hamidreza Farhidzadeh" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "mutation. Arabidopsis Thaliana is a plant that is so interesting, because its genetic structure has some similarities with that of human beings. Biologists classify the type of this plant to mutated and not mutated (wild) types. Phenotypic analysis of these types is a time-consuming and costly effort by individuals. In this paper, we propose a modified feature extraction step by using velocity and acceleration of root growth. In the second step, for plant classification, we employed different Support Vector Machine (SVM) kernels and two hybrid systems of neural networks. Gated Negative Correlation Learning (GNCL) and Mixture of Negatively Correlated Experts (MNCE) are two ensemble methods based on complementary feature of classical classifiers; Mixture of Expert (ME) and Negative Correlation Learning (NCL). The hybrid systems conserve of advantages and decrease the effects of disadvantages of NCL and ME. Our Experimental shows that MNCE and GNCL improve the efficiency of classical classifiers, however, some SVM kernels function has better performance than classifiers based on neural network ensemble method. Moreover, kernels consume less time to obtain a classification rate.\nINTRODUCTION\nIn biology, study on root growth and organisms operation is a significant issue. Understanding of effective factors on organism operation can be useful in various fields, from treatment of diseases up to increasing the fertility of plants. Studies about usage of organisms’ morphological characteristics to obtain information about genetic operation of these organisms are called phenotypic analysis.\nMany genetic factors effect on each morphological characteristics of an organism. For instance, the manner of gene operation can be discovered with comparison of differences between two organisms that are distinct in a specific gene. Technology advances in the field of obtaining genetic characteristics and morphological characteristics improve phenotypic analysis. Because of high amount of data and required accuracy in these studies, there is increasing demand for fast and accurate computational methods to extract phenotypic information and distinguish between different species.\nUnlike the past that an extensive collection of organisms in biological studies was investigated, only a few special organisms are reviewed for such researches now. The reason of this issue is to simplify the experiments, to reduce the required time and cost of experiments. The Arabidopsis Thaliana plant is one of the instances that have attracted a lot of research interest, recently. Arabidopsis Thaliana is a small plant that grows in short period and by possessing 5 chromosomes and 150 base pairs, it has one of the smallest genome between plants. It is the first plant that its genetic sequence is determined [1]. In this plant, two specific genes are mutated and the behaviours of mutated species and wild type species are analysed.\nWe target to apply a new approach for feature extraction and classification. We apply velocity and acceleration of root growth and use sliding window to decrease classification error rate. We also obtain classification accuracy rate by classifiers combination. Combining classifiers is one of the methods to improve performance in classification issues, especially, in complicated cases that have few numbers of training data, feature vector with high dimension or overlapped classes [12]. In accordance to experiments results, “divide and conquer” idea improves performance\nof classification by dividing main problem to some easier computational problems and then by combining the results. In supervised learning classification issues, divide and conquer principle is implemented by separating data space to some sub-problems and attributing the experts to model each sub problems [7].\nThe rest of this article is organized as follows. Section 2 reviews the SVM methods and its kernels. Section 3 presents some related works are done on Arabidopsis Thaliana root growth and analysis of swing of tip angle. In the Section 4 the learning and combining procedure of the ME and NCL methods are investigated. In Section 5, the new feature extraction approach is explained. Section 6 presents the results of our experimental study on Arabidopsis Thaliana plant. Finally, Section 6 concludes the article.\nREVIEW OF SVM AND KERNEL FUNCTIONS\nThe important issue in classification is to find the best decision boundary. We can suppose a two-class problem that is linearly separable. We can find out there is many decision boundaries. However, the main question is: Are all decision boundaries equally efficient?\nClass 1\nClass2\nSupport Vector Machines (SVMs) are a class of supervised learning algorithms first introduced by Vapnik[?? Therodosis] . Given a set of labeled training vectors (positive and negative input examples), SVMs learn a linear decision boundary to discriminate between the two classes. The result is a linear classification rule that can be used to classify new test examples. SVMs have exhibited excellent generalization performance (accuracy on test sets) in practice and have strong theoretical motivation in statistical learning theory 19. Let X with {x1, x2… xn} R n be our data set and yi{-1,+1} be the class label of xi. We can specify a linear classification rule f by a pair (w, b),\n(1) ,)( bxwxf T \nwhere w Rn and b Rn, a point x is classified as positive (negative) if f(x) > 0 (f(x) < 0). Geometrically, the decision boundary is the hyper plane\n{xRn: wTx + b=0 } (2)\nwhere w is a normal vector to the hyper plane and b is the bias. The margin m of the classifier with respect to our data set members is:\n))(( bxwyMinm Tixi  (3)\nWhen classes are linearly separable, the classifier can classify the dataset and m is the distance of decision boundary and nearest training points.\nw\nm 1bxw T\n0 bw T\n1 bxw T\nOne types of SVM is that appropriate decision boundary has maximum margin m of both classes’ data as far as possible. Therefore, the SVM discovers the decision boundary that separates data and maximizes the distance to nearest training points.\nHowever, generally, training sets are not linearly separable and we should enhance the SVM efficiency to balance a trade-off between maximizing margin and minimizing classification error rate on training set.\nKernels in SVMs\nA key feature of any SVM optimization problem is that it is equivalent to solving a dual quadratic programming problem. For example, in the linearly separable case, the maximal margin classifier is found by solving for the optimal “weights\" αi, I = 1,…, m, in the dual problem:\n(4)\n\n\n\n\n\n0,0\n2\n1 )(\n1,111\niii\nj T ij\nn\nji\niji\nn\ni\ni\nySubject\nxxyyWMaximize\nw and b of classifier are then determined by the optimal αi. The dual problem not only modifies SVMs to various efficient optimization algorithms, but also, since the dual problem depends only on the inner products xi and xj allows for the introduction of kernel techniques.\nThe key idea is to transform xi to a higher dimensional space to make classification easier. The input space is space that xi is belong to. The feature space is a space that any given feature map Ф(xi) from X to higher dimensional vector space.\nnRX  :\nSo, the kernel K on X*X is as follows:\nK(xi,xj) = Ф T (xi) * Ф(xi) (5)\nWe can use SVM in feature vector by replacing inner products with kernel function in dual programming.\nGaussian and Sigmoid Kernel\nWe need a simple way to test whether a function constitutes a valid kernel without having to construct the function Ф (x) explicitly. A necessary and sufficient condition for a function K(xi, xj) to be a valid kernel (Shawe-Taylor and Cristianini, 2004) is that the Gram matrix K, whose elements are given by k(xi, xj), should be positive semi definite for all possible choices of the set {xn}.Note that a positive semi definite matrix is not the same thing as a matrix whose elements are nonnegative. We require that the kernel K(xi , xj) be symmetric and positive semidefinite and that it expresses the appropriate form of similarity between x and x_ according to the intended application.\nOne of commonly used kernel takes the form\n(6) )2/| || |exp(),( 22  jiji xxxxK\nand is often called a ‘Gaussian’ kernel. Note, however, that in this context, it is not interpreted as a probability density, and hence the normalization coefficient is omitted. We can see that this is a valid kernel by expanding the square:\n(7) j T ij T ji T iji xxxxxxxx 2| || | 2 \nNote that the feature vector that corresponds to the Gaussian kernel has infinite dimensionality. The Gaussian kernel is not restricted to the use of Euclidean distance. If we use kernel substitution in (6.24) to replace xi T xj with a nonlinear kernel κ(xi, xj), we obtain:\n(8) ))},(2),(),(( 2\n1 exp{),(\n2 jijjiiji xxxxxxxxk   \nSigmoid kernel is another common kernel function as follows:\n(9) )*tanh(),( bxxaxxk j T iji \nwhose Gram matrix in general is not positive semi-definite. This form of kernel has, however, been used in practice (Vapnik, 1995), possibly because it gives kernel expansions such as the support vector machine a superficial resemblance to neural network models. For a > 0, we can view as a scaling parameter of the input data, and b as a shifting parameter that controls the threshold of mapping. For a < 0, the dot-product of the input data is not only scaled but reversed.(Ref.1). The value of a is usually equal to reverse of number of features.\nRELATED WORKS\nEvans and Ishikawa [6] presented an algorithm to measure growth rate and swing rate of tip angle automatically and they implemented it by ADAPT 1 software. Miller [14] provided an accurate method to obtain mid line of plant image. Dashti [3] undermined the significance of phenotypic traits that are implicit in patterns of dynamics in plant root response to sudden changes of its environmental conditions. Siahpirani[15] designed an PCA-based algorithm to obtain midline of root and number of root hairs. Miller [13] employed LDA and wavelet differentiation to analyse root gravitropism phenotypic traits. D.Brook [4] differentiated plasticity of root plant in different conditions."
    }, {
      "heading" : "ANALYSIS OF MIXTURE OF EXPERT AND NEGATIVE CORRELATION LEARNING MODELS",
      "text" : "In this section, the ME and NCL methods are investigated and reviewed.\nA. ME\nThe ME method was introduced by Jacobs et al.[9,10]. The authors examined the use of different error functions in the learning process for expert networks in the ME method. Jacobs proposed making NN experts local in different distributions of data space; as a result, the increased diversity among the experts led to improvements in the performance of this method. Various error functions were investigated with respect to a performance criterion and then Jacobs introduced a new error function based on the negative log probability of generating the desired output vector under the mixture of Gaussian models:\n(10) ),)( 2\n1 exp(log\n2\n  j jj OygE\nwhere gj is the proportional contribution of expert j to the combined output vector and Oi and y are the actual and\ndesired outputs of the i th NN, respectively.\nTo evaluate this error function, its deviation with respect to i th expert is analysed:\n(11 ) ).(\n))( 2\n1 exp(\n))( 2\n1 exp(\n2\n2\ni\nj\njj\nii\ni\nOy\nOyg\nOyg\nO\nE      \n\n\n     \n\n\n\n \n\n\n1 Automatic Degree And Position Technique\nAccording to the derivation term, the training of each expert is based on its individual error. Moreover, the weight-updating factor for each expert is proportional to the ratio of its error value to the total error. These two features in the proposed error function that cause the localization of each expert in their corresponding and authors claimed that the ME method has better efficiency with this error function.\nIn addition, a gating network is used to complete a system of competing local experts. The learning rule for the gating network attempts to maximize the likelihood of the training set by assuming a Gaussian mixture model in which each expert is responsible for one component of the mixture.\nThe ME method has special characteristics that distinguish it from the other combining methods. This method differs from the others due to its dynamic combination method. In the literature on combining methods, ME refers to the methods in which complex problems based on a “divide and conquer” approach are partitioned into a set of simpler sub problems and are distributed among the experts. In this method, instead of assigning a set of fixed combinational weights to the experts, as described previously, an extra gating network is used to compute these weights from the inputs dynamically."
    }, {
      "heading" : "B. NCL",
      "text" : "In neural network ensemble methods, the individual networks are usually trained independently. One of the disadvantages of such an approach is the loss of interactions among the individual networks during the learning process. It is thus possible that some of the independently designed individual networks contribute slightly to the whole ensemble.\nLiu and Yao [11] proposed the NCL method that trains NNs in the ensemble simultaneously and interactively\nthrough the correlation penalty terms in their error functions. In NCL, the error function of the i th NN is expressed by the equation:\n(12) ,)( 2\n1 2 iii POyE \nwhere Oi and y are the actual and desired outputs of the i th NN, respectively. The first term in (3) is the empirical risk function of the i th\nNN. The second term, Pi, is the correlation penalty function, which can be expressed as:\n),()(    ji ensjensii OOOOP (13)\nwhere Oens is the average of NNs outputs in the ensemble. Here, Pi can be regarded as a regularization term that is incorporated into the error function of each ensemble network. This regularization parameter provides a convenient way to balance the bias-variance-covariance trade-off [11]. This term is meant to quantify the amount of error correlation, so it can be minimized explicitly during training, which leads to negatively correlated NNs. The term λ is a scaling coefficient parameter that controls the trade-off between the objective and penalty functions. When λ = 0, the penalty function is removed, resulting in an ensemble in which each network trains independently of the others, using simple Back-Propagation (BP). Therefore, the interaction and correlation among NNs of the ensemble is controlled explicitly by the value of λ. The minimization of this penalty function encourages different individual networks in an ensemble to learn different parts or aspects of the training data so that the ensemble can better learn the whole training dataset."
    }, {
      "heading" : "C. Comparison the strengths and weaknesses",
      "text" : "In this part we compare the strengths and weaknesses of ME and NCL models.\nFrist, we regard common characteristics of these two models. Both ensemble methods train base NNs in a process with multiple communication and cooperation among experts simultaneously. As already mentioned, different and special functions of these models have characteristics that lead experts to learn sub problems or different aspect of problem in a comparative and cooperation process, and finally the hybrid system learns all the data space. In other\nword, the error functions of these methods have characteristics that base NNs are produced with bias that possess negative correlation by dividing data space between experts explicitly [8].\nBeside these similarities, there are some differences. Hybrid system contains two major parts. In fist part, training of base NNs, NCL has better efficiency than ME. Regularization term, which is used in this method, provides a convenient way to balance the bias-variance–covariance trade-off and thus improves the generalization ability, whereas ME does not include such control over the trade-off. On the other hand, one of the superiors of base MEs is its unique technique to combine output of experts.ME uses a trainable combiner that, according to the input x, dynamically selects the best expert(s) and combines their outputs to create the final output. The combining function of ME includes a dynamic weighted average in which the local competences of the experts with respect to the input are estimated by the weights produced by the gating network."
    }, {
      "heading" : "GATING NETWORK TO COMBINE NCL EXPERTS OUTPUTS",
      "text" : "According to complement characteristics of these two models, an integrated system can reserve strengths points and reduce their weaknesses. Based on this idea, the proposed hybrid system [5] contains two stages of training. In first stage, NNs are trained by NCL algorithm and in second stage; gating network is used for combining base NNs which is resulted from pervious stage. From another point of view, we can regard this method as improved NCL method. In NCL, error function orients base NNs to train different aspects or parts of problem. The local expertise characteristic of base NNs of NCL should be regarded in their combination method. Using gating network, as a combiner of base NNs output, can provide this required ensemble NN characteristic in NCL method. In this method, after training of neural networks by NCL, in the next stage, gating network is trained to model local expertise and to combine them; therefore, this method is called “using gating network to combine NCL experts”.\nFor implementation of this idea, gating network should be train by some target points that show the efficiency and local expertise of experts in different sub problems. Like base mixture of expert is proposed to measure local and proportional expertise of base NNs.\n(14) . ))(\n2\n1 exp(\n))( 2\n1 exp(\n2\n2\n,\n \n\n\nj\nj\ni\niNClG\nOy\nOy\nh\nIf these values be in used as the target points to train gating network, gating network can estimate base NNs local expertise proportion in accordance to system input. With respect to and calculating target points in (5), error function of gating network is expressed as follows:\n(15 ) .)( 2\n1 2ghE NCLGNCLG  \nThis error function is used to train Multilayer Perceptron network with Back Propagation algorithm. After output\nproduction of gating network, Og,i, output of neuron i th , softmax function applies to calculate gi:\n(16) .\n)exp(\n)exp(\n1\n,\n,\n \n N\nj\njg\nig i\nO\nO g\nHere, the gi values are nonnegative and sum to unity, and they can be interpreted as estimates of the prior probability that expert i can generate the desired output y. Learning algorithm for updating weights of input layer to hidden layer and hidden layer to output layer based on Back Propagation is as follow:\n(17 )T hgggNCLGgyg OOOghw ))1()((  \n(18) ihghg\nggNCLG T ygghg\nxOO\nOOghww\n)1(*\n))1()((\n\n \nwhere ηg is learning rate, g gating network output after applying softmax function, whg and wyg are weights of hidden layer and weights of output layer, recpectively. Ohg T is transposed vector of Ohg neurons outputs of hidden layer of gating network. Finally, to combine of NCL experts outputs, the gate assigns a weight gi as a function of x to each of the experts’ output, Oj and the final combined output of the ensemble is:\n(19) .\n1\ni\nN\nj iT gOO   \nThe idea of using gating network is an efficient method to combine output of experts’ based on local expertise. In this idea, weights which are assigned to each expert output are estimated based on local expertise rate. In this approach, the combining weights are estimated dynamically from the inputs based on the different competences of each expert regarding different parts of the problem. Hence, the combination of NCL experts using this approach is superior to the previous static methods. The whole process is show in Fig.1a, b. The first stage is shown in Fig. 1a. In the first training stage, the expert networks are trained using the NCL error function. Fig. 1b shows the second stage of the training algorithm. After training the NCL experts, in the second stage, a gating network is trained to model the local competence of the NCL expert.\nFig. 1 a, b. Diagram of the two training stages of Gated NCL."
    }, {
      "heading" : "INCORPORATION OF THE NCL TRAINING ALGORITHM INTO ME",
      "text" : "ME and NCL methods use different error functions to incorporate negative correlation between the experts. Although ME can produce negatively correlated experts, it does not include a control parameter like NCL to adjust this parameter explicitly and so make a near-optimal balance in the bias-variance-covariance trade-off. To introduce this advantage of NCL into the training algorithm of ME, we incorporated this control parameter in the ME error function. The modified error function of ME was obtained by adding the penalty term from NCL to the error function. The proposed method is named Mixture of Negatively Correlated Experts (MNCE). The new error function thus takes the form of Equations (20) and (21):\n),)( 2\n1 exp(log\n2\nj\nj\njj POygE  \n(20)\n),()(    ji ensjensii OOOOP (21)\nThe introduced penalty term, similarly to NCL, provides a control parameter leading to a near-optimal balance in the bias-variance-covariance trade-off. In this ensemble architecture, each expert network is an MLP network with one hidden layer that computes an output Oi as a function of the input vector x and the weights of hidden and output layers with a sigmoid activation function. To train MLP experts based on the new error function using the BP training algorithm, the weights for each expert i are updated according to the following rules:\n(22) . ))(\n2\n1 exp(\n))( 2\n1 exp(\n2\n2\n,\n     \n\n     \n\n\n\n\n j iji\niii\niMNCE\nPOyg\nPOyg\nh\n(23 )T hiii\ni\ni iiMNCEeiy OOO\nO\nP Oyhw ))1(()(,, \n       \n(24 ) ihihiii i\ni i T yiMNCEeih xOOOO\nO\nP Oywhw )1())1(()(,, \n       \n(25 )   \n\n  \n\n     ))(1()( OOMgOOg O P ij iiji i i\n  \nN\ni\niO M nO\n1\n1 )( (26)\nwhere ηe is the learning rate, is the NCL control parameter, gi is the i th output of the gating network after applying the softmax function, wh and wy are the weights of the inputs to the hidden layers and those of the hidden to the output layers of the expert networks, respectively.\nis the transpose of Ok , the outputs of the hidden layers of the expert networks. Similar to original ME, the gate\nis composed of two layers: the first layer is a MLP network, and the second layer is a softmax nonlinear operator.\nThus, the gating network computes Og, which is the output of the MLP layer of the gating network, then applies the softmax function (Eq. 16). According to the modified ME error function (Eq. 20), the modified error function of the gating network can be written as:\n2 , )(\n2\n1 ghE MNCEMNCEG  (27)\nBased on this error function, the weights of the gating network in the MNCE method are determined using the BP error algorithm according to the following rules:\nT hgggMNCEgyg OOOghw ))1()((  (28)\nihghgggMNCE T ygghg xOOOOghww )1())1()((  (29)\nFinally, to combine the experts’ outputs, the gate assigns a weight gi as function of x to each of expert’s output Oj , and the final mixed output of the ensemble is:\nj\nj\njT gOO  (30)\nThe MNCE is composed of the experts and a gating network. The expert training process and the gating network work simultaneously to minimize the modified error functions. The experts compete to learn the training patterns, and the gating network mediates the competition. The added control parameter λPi provides an explicit control for efficiently adjusting the measure of negative correlation between the experts. The structure of MNCE and its simultaneous training algorithm for the experts and gating network are shown in Figure 2.\nAccording to this MNCE training procedure, in the network’s learning process, the expert networks compete for each input pattern and the gating network rewards the winner of each competition with stronger error feedback signals. This competitive learning procedure leads to a localization of the experts into possibly overlapping regions. Additionally, incorporating the control parameter of NCL into the error function of ME provides an explicit control for efficiently adjusting the measure of negative correlation between the experts. Thus, it yields a better balance in the bias-variance-covariance trade-off, which improves generalization ability.\nUSING VELOCITY AND ACCELERATION FOR FEATURE EXTRACTION\nIn this part we explain our feature extraction approach which is applied on time series dataset. Frist we use Principal Component Analysis (PCA) to reduce the dimensions of data in dataset. We also obtain principal components of data in this stage. In datasets that have some motions we can calculate velocity and acceleration of motions in consecutive data as follows:\n(11) ),)(()1)(()( jiPjiPiVelocity \n(12) ),)(()1)(( jiVelocityjiVelocityonAccelerati \nfor i = 1,…,m and j= 1,…, n-1\nwhere P is the main matrix , m is the number of samples and n is the length of each samples. We used sliding\nwindow, a method used for data mining in time series in machine learning, to improve classification rate. Sliding window moves throughout the feature vector and the classifiers in these slices will be trained. In this way we can find best slices to obtain best features to classify dataset."
    }, {
      "heading" : "EXPERIMENTAL",
      "text" : "To evaluate the results, first we define our dataset. Then, the approach for dimension reduction is principal component analysis. In next step, we define our feature vector and give it to different classifiers to classify our dataset."
    }, {
      "heading" : "A. Dataset",
      "text" : "Main purpose of studying Arabidopsis Thaliana model plant is to find the operation of each 25000 genes. An extensive and indexed library of mutated T-DNA input leads to use reverse-genetic to find these operations. When a mutation causes observable change in phenotype to obtain clue of disabled gene function, reverse-genetic can be efficient. However, many disabled gene in Arabidopsis Thaliana do not make any observable changes in phenotype."
    }, {
      "heading" : "B. Size of seeds",
      "text" : "Seeds of Arabidopsis Thaliana were screened based on size 522 , 582 , 300 , 355 . Seeds which\nare between 522 - 582 , are in small group, and seeds which are 300 - 355 are in large group. Seeds grain vertically in special circumstances between 2 and 4 days."
    }, {
      "heading" : "C. Genetic Mutation",
      "text" : "Seeds of plant which obtain mutated T-DNA in GLR.3.3 gene are provided by Salk association. Used product are Salk_040458 (glr3.3-1, mutated second axon) and Salk_066009 (glr3.3-2, mutated first antron) and seeds are divided by the method identified in pervious section."
    }, {
      "heading" : "D. Genetic Mutation",
      "text" : "As already discussed, dataset contains groups in Table 1.It is necessary for classification that at least one wild type species places against mutated type species. For this purpose, groups place against each other what is demonstrated Table 2.\nEach group contains some samples which each sample contains 300 pictures. Resolution of each frame is 700*900 pixels that can be shown by a 630000-ordered pair which means each frame belongs to a 630000 dimensions. So the dimension of each sample is—630000 ×300 = 189000000\nalready mentioned, the dimensions of these points are 630000 that we transform them to the 30-40 dimensions. It can be argued that the difference between two consecutive frames is very small. It means the seed tip moves just few pixels, so this difference is small in low dimension too. As a result, these determining points of each frame emerges a curve in space. This curve is a dynamic sequence of root growth. However, based on experiments used the curve with this length, we cannot obtain helpful answers. We use 2D array to show this curve that multiple pair ordered of each point place in one direction and time places in oppose direction.\nFor each pair that contains a mutated group and a wild type group 30 through 40 principal components is considered. Then a 2D array which is corresponding of merging of number of frame and number of principal component is obtained by a 3D matrix of “number of samples” × “number of frames” × “number of principal component”. Velocity and acceleration is obtained from this matrix and is added to end of this matrix.\nFor classification rate of SVM we used cross-validation method with 5 folds. MLP neural networks are used with one hidden layer with 4 neurons in it when gating network NCL experts’ method is applied. All methods are trained by Back Propagation algorithm. In Gated-NCL learning rate for experts and gating networks are 0.15 and 0.1, respectively. λ is obtained by trial and error procedure in Gated-NCL and MNCE. Also, here we used crossvalidation method with 5 folds for correct rate of this classifier. On average the best results were obtained using a window length of 40 that it is used for all other experiments. Results are shown in Table 3.\nFeature Extraction and Classification Procedure for Analysis of Root Growth\n1. Compute Feature Vector with PCA method 2. Make 2D array ‘P’ by merging of Number of Frame and Number of PCs\n- Compute velocity and acceleration growth rate of each frame of samples - add these features to the end of the ‘P’\n3. Move Sidling window on feature vector end of file\nDo training and testing operation\n4. output : lowest error rate and best frames for training of classifiers"
    }, {
      "heading" : "CONCLUSION",
      "text" : "In this paper, we proposed a new approach for feature extraction base on velocity and acceleration of motion and\nemployed a method based on NCL and ME applicable for classification.\nWe used a Gated-NCL and MNCE hybrid system by regarding weaknesses and strengths of NCL and ME models. NCL encourages experts by “divide and conquer” principal that consider different parts and aspects of data space.\nWe also employed different kinds of SVM classifiers to compare their efficiency with each and other ensemble methods. According to result, the MNCE hybrid system can obtain better performance than GNCL. Moreover, Gaussian kernel worsens the efficiency of SVM. Finally, sigmoidal kernel could obtain best classification error rate between our classifiers. In addition, these hybrid system spend more time than kernel SVMs, because dividing the problem to some sub problems and then integrating the result is time-consuming.\nIn future work, we want to apply semi-supervised algorithms. SVM will be exploited with generative models such as Parzen windows and probabilistic neural network (PNN) in order to improve the training process and reduce the time of testing. It is also interesting to study for using different kernel functions for the SVM with semi-supervised algorithms."
    } ],
    "references" : [ {
      "title" : "Support-Vector Networks",
      "author" : [ "C. Cortes", "V. Vapnik" ],
      "venue" : "Mach. Learn., vol. 20, pp. 273- 297, 1995.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "Improving combination method of NCL experts using gating network",
      "author" : [ "R. Ebrahimpour", "S. Arani", "S. Masoudnia" ],
      "venue" : "Neural Computing and Applications, pp. 1-7, 2011/12/01 2011.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Adaptive mixtures of local experts",
      "author" : [ "R.A. Jacobs", "M.I. Jordan", "S.J. Nowlan", "G.E. Hinton" ],
      "venue" : "Neural Comput., vol. 3, pp. 79-87, 1991.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 1991
    }, {
      "title" : "On combining classifiers",
      "author" : [ "J. Kittler", "M. Hatef", "R.P.W. Duin", "J. Matas" ],
      "venue" : "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 20, pp. 226-239, 1998.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Combining Pattern Classifiers: Methods and Algorithms: Wiley- Interscience",
      "author" : [ "L.I. Kuncheva" ],
      "venue" : null,
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2004
    }, {
      "title" : "Simultaneous training of negatively correlated neural networks in an ensemble",
      "author" : [ "Y. Liu", "X. Yao" ],
      "venue" : "Trans. Sys. Man Cyber. Part B, vol. 29, pp. 716-725, 1999.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Incorporation of a Regularization Term to Control Negative Correlation in Mixture of Experts",
      "author" : [ "S. Masoudnia", "R. Ebrahimpour", "S. Arani" ],
      "venue" : "Neural Processing Letters, vol. 36, pp. 31- 47, 2012.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Kernel Methods for Pattern Analysis",
      "author" : [ "J. Shawe-Taylor", "N. Cristianini" ],
      "venue" : null,
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2004
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : "In supervised learning classification issues, divide and conquer principle is implemented by separating data space to some sub-problems and attributing the experts to model each sub problems [7].",
      "startOffset" : 191,
      "endOffset" : 194
    }, {
      "referenceID" : 4,
      "context" : "RELATED WORKS Evans and Ishikawa [6] presented an algorithm to measure growth rate and swing rate of tip angle automatically and they implemented it by ADAPT 1 software.",
      "startOffset" : 33,
      "endOffset" : 36
    }, {
      "referenceID" : 1,
      "context" : "Dashti [3] undermined the significance of phenotypic traits that are implicit in patterns of dynamics in plant root response to sudden changes of its environmental conditions.",
      "startOffset" : 7,
      "endOffset" : 10
    }, {
      "referenceID" : 2,
      "context" : "Brook [4] differentiated plasticity of root plant in different conditions.",
      "startOffset" : 6,
      "endOffset" : 9
    }, {
      "referenceID" : 7,
      "context" : "[9,10].",
      "startOffset" : 0,
      "endOffset" : 6
    }, {
      "referenceID" : 6,
      "context" : "word, the error functions of these methods have characteristics that base NNs are produced with bias that possess negative correlation by dividing data space between experts explicitly [8].",
      "startOffset" : 185,
      "endOffset" : 188
    }, {
      "referenceID" : 3,
      "context" : "Based on this idea, the proposed hybrid system [5] contains two stages of training.",
      "startOffset" : 47,
      "endOffset" : 50
    } ],
    "year" : 2015,
    "abstractText" : "AbstractOne of the challenging problems in biology is to classify plants based on their reaction on genetic mutation. Arabidopsis Thaliana is a plant that is so interesting, because its genetic structure has some similarities with that of human beings. Biologists classify the type of this plant to mutated and not mutated (wild) types. Phenotypic analysis of these types is a time-consuming and costly effort by individuals. In this paper, we propose a modified feature extraction step by using velocity and acceleration of root growth. In the second step, for plant classification, we employed different Support Vector Machine (SVM) kernels and two hybrid systems of neural networks. Gated Negative Correlation Learning (GNCL) and Mixture of Negatively Correlated Experts (MNCE) are two ensemble methods based on complementary feature of classical classifiers; Mixture of Expert (ME) and Negative Correlation Learning (NCL). The hybrid systems conserve of advantages and decrease the effects of disadvantages of NCL and ME. Our Experimental shows that MNCE and GNCL improve the efficiency of classical classifiers, however, some SVM kernels function has better performance than classifiers based on neural network ensemble method. Moreover, kernels consume less time to obtain a classification rate.",
    "creator" : "Microsoft® Word 2010"
  }
}