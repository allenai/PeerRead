{"conference": "ICLR 2017 conference submission", "title": "Submodular Sum-product Networks for Scene Understanding", "abstract": "Sum-product networks (SPNs) are an expressive class of deep probabilistic models in which inference takes time linear in their size, enabling them to be learned effectively. However, for certain challenging problems, such as scene understanding, the corresponding SPN has exponential size and is thus intractable. In this work, we introduce submodular sum-product networks (SSPNs), an extension of SPNs in which sum-node weights are defined by a submodular energy function. SSPNs combine the expressivity and depth of SPNs with the ability to efficiently compute the MAP state of a combinatorial number of labelings afforded by submodular energies. SSPNs for scene understanding can be understood as representing all possible parses of an image over arbitrary region shapes with respect to an image grammar. Despite this complexity, we develop an efficient and convergent algorithm based on graph cuts for computing the (approximate) MAP state of an SSPN, greatly increasing the expressivity of the SPN model class. Empirically, we show exponential improvements in parsing time compared to traditional inference algorithms such as alpha-expansion and belief propagation, while returning comparable minima.", "reviews": [{"is_meta_review": true, "comments": "This paper develops Submodular Sum Product Networks (SSPNs) and\nan efficient inference algorithm for approximately computing the\nmost probable labeling of variables in the model. The main\napplication in the paper is on scene parsing. In this context,\nSSPNs define an energy function with a grammar component for\nrepresenting a hierarchy of labels and an MRF for encoding\nsmoothness of labels over space. To perform inference, the\nauthors develop a move-making algorithm, somewhat in the spirit\nof fusion moves (Lempitsky et al., 2010) that repeatedly improves\na solution by considering a large neighborhood of alternative segmentations\nand solving an optimization problem to choose the best neighbor.\nEmpirical results show that the proposed algorithm achieves better\nenergy that belief propagation of alpha expansion and is much faster.\n\nThis is generally a well-executed paper. The model is interesting\nand clearly defined, the algorithm is well presented with proper\nanalysis of the relevant runtimes and guarantees on the\nbehavior. Overall, the algorithm seems effective at minimizing\nthe energy of SSPN models.\n\nHaving said that, I don't think this paper is a great fit for\nICLR. The model is even somewhat to the antithesis of the idea of\nlearning representations, in that a highly structured form of\nenergy function is asserted by the human modeller, and then\ninference is performed. I don't see the connection to learning\nrepresentations. One additional issue is that while the proposed\nalgorithm is faster than alternatives, the times are still on the\norder of 1-287 seconds per image, which means that the\napplicability of this method (as is) to something like training\nConvNets is limited.\n\nFinally, there is no attempt to argue that the model produces\nbetter segmentations than alternative models. The only\nevaluations in the paper are on energy values achieved and on\ntraining data.\n\nSo overall I think this is a good paper that should be published\nat a good machine learning conference, but I don't think ICLR is\nthe right fit.", "IS_META_REVIEW": true}, {"TITLE": "ICLR committee final decision", "OTHER_KEYS": "ICLR 2017 pcs", "comments": "This paper was reviewed by three experts. While they all find merits in the paper (interesting new model class SSPN, new MAP inference algorithm), they all consistently point to deficiencies in the current manuscript (lack of parameter learning, emphasis on evaluation on energies, lack of improvements in accuracy). \n \n One problem that (I believe) is that manuscript as it stands makes neither a compelling impact on the chosen application (semantic segmentation) nor does it convincing establish the broad applicability of the proposed model (how do I run SSPNs on activity recognition or social network modeling). \n \n To be clear, we all agree that there is promising content here. However, I agree with the reviewers that the significance has not been established.", "IS_META_REVIEW": false, "DATE": "06 Feb 2017", "is_meta_review": false}, {"TITLE": "General response to concern about our empirical results", "OTHER_KEYS": "Abram L. Friesen", "comments": "The main concern raised in each review is the lack of a direct comparison of SSPNs to existing semantic segmentation benchmarks. We address this point here and then respond to each reviewer's other points directly. We\u2019ve also made significant revisions to the structure and writing of the paper, particularly in the inference section, which we hope more clearly explains the intuitions behind InferSSPN.\n\nDespite not having comparisons on semantic segmentation benchmarks, SSPNs are a significant step forward in the state-of-the-art on scene understanding. SSPNs are (provably) more expressive than (and contain as a special case) MRFs and SPNs as commonly used for scene understanding and semantic segmentation, meaning that SSPNs are a richer model class than those already in use. Despite this increase in expressivity and complexity, InferSSPN is a very efficient (approximate) inference algorithm for the problem of parsing an image with respect to a grammar, which has been addressed by many previous works, but their approaches have all been very restrictive or inefficient. InferSSPN provably achieves low-order-polynomial time complexity for a problem with a state space that has size exponential in both the number of pixels and the height of the grammar. While the first version of the paper did not include accuracy results, as we thought the energy results were sufficient, we\u2019ve since updated the paper to include all accuracy, energy, and time complexity results for all test cases. These show empirically that InferSSPN provides exponential improvements in time-complexity with no loss in accuracy relative to alpha-expansion, which provably returns local optima that are within a constant factor of the global optimum.\n\nFurther, while we have characterized SSPNs in terms of scene understanding, they are as broadly applicable as SPNs and MRFs, and scene understanding is just the initial application we chose to focus on. Future applications may include activity recognition or social network modeling. Nonetheless, this paper presents a powerful new representation for efficiently modeling and reasoning about scenes at multiple levels of abstraction and is well within the scope of ICLR, which includes \u201chierarchical models\u201d and \u201capplications to vision\u201d in its list of relevant topics.\n\nLearning SSPNs is important \u2013 both for evaluating SSPNs on existing semantic segmentation benchmarks and for their adoption in general \u2013 and is something that we are already working on. Unfortunately, there do not yet exist any image grammars or relevant datasets with which to train SSPNs, so learning requires either creating such datasets or learning with a large number of hidden variables. This is ongoing and future work. However, we believe that one benefit of our contributions will be to spur the creation of such datasets and open up new research problems where reasoning about multiple levels of abstraction in scenes and higher-level relationships between objects is crucial. Finally, it\u2019s common in AI and ML for new representations to be introduced well before learning algorithms are properly developed for them. Notable cases include graphical models and previous work on image grammars, and this paper does the same for SSPNs.\n", "IS_META_REVIEW": false, "DATE": "14 Jan 2017", "is_meta_review": false}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "My thoughts", "is_meta_review": false, "comments": "The paper discusses sub modular sum-product networks as a tractable extension for classical sum-product networks. The proposed approach is evaluated on semantic segmentation tasks and some early promising results are provided.\n\nSummary:\n\u2014\u2014\u2014\nI think the paper presents a compelling technique for hierarchical reasoning in MRFs but the experimental results are not yet convincing. Moreover the writing is confusing at times. See below for details.\n\nQuality: I think some of the techniques could be described more carefully to better convey the intuition.\nClarity: Some of the derivations and intuitions could be explained in more detail.\nOriginality: The suggested idea is great.\nSignificance: Since the experimental setup is somewhat limited according to my opinion, significance is hard to judge at this point in time.\n\nDetailed comments:\n\u2014\u2014\u2014\n1. I think the clarity of the paper would benefit significantly from fixes to inaccuracies. E.g., \\alpha-expansion and belief propagation are not `scene-understanding algorithms\u2019 but rather approaches for optimizing energy functions. Computing the MAP state of an SSPN in time sub-linear in the network size seems counterintuitive because it means we are not allowed to visit all the nodes in the network. The term `deep probabilistic model\u2019 should probably be defined. The paper states that InferSSPN computes `the approximate MAP state of the SSPN (equivalently, the optimal parse of the image)\u2019 and I\u2019m wondering how the `approximate MAP state' can be optimal. Etc.\n\n2. Albeit being formulated for scene understanding tasks, no experiments demonstrate the obtained results of the proposed technique. To assess the applicability of the proposed approach a more detailed analysis is required. More specifically, the technique is evaluated on a subset of images which makes comparison to any other approach impossible. According to my opinion, either a conclusive experimental evaluation using, e.g., IoU metric should be given in the paper, or a comparison to publicly available results is possible.\n\n3. To simplify the understanding of the paper a more intuitive high-level description is desirable. Maybe the authors can even provide an intuitive visualization of their approach.", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Interesting idea that needs to be fully developed and evaluated", "is_meta_review": false, "comments": "This paper is about submodular sum product networks applied to scene understanding. SPNs have shown great success in deep linear models since the work of Poon 2011.  The authors propose an extension to the initial SPNs model to be submodular, introducing submodular unary and pairwise potentials.  The authors propose a new inference algorithm. The authors evaluated their results on Stanford Background Dataset and compared against multiple baselines.\n\nPros:\n+ New formulation of SPNs \n+ New inference algorithm\n\nCons:\n- The authors did not discuss how the SSPN structure is learned and how the generative process chooses the a symbol (operation) at each level)\n- The evaluations is lacking. The authors only showed results on their own approach and baselines, leaving out every other approach. Evaluations could have been also done on BSD for regular image segmentation (hierarchical segmentation). \n\nThe idea is great, however, the paper needs more work to be published.  I would also recommend for the authors to include more details about their approach and present a full paper with extended experiments and full learning approach.", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "Good paper, but not a good fit for ICLR", "is_meta_review": false, "comments": "This paper develops Submodular Sum Product Networks (SSPNs) and\nan efficient inference algorithm for approximately computing the\nmost probable labeling of variables in the model. The main\napplication in the paper is on scene parsing. In this context,\nSSPNs define an energy function with a grammar component for\nrepresenting a hierarchy of labels and an MRF for encoding\nsmoothness of labels over space. To perform inference, the\nauthors develop a move-making algorithm, somewhat in the spirit\nof fusion moves (Lempitsky et al., 2010) that repeatedly improves\na solution by considering a large neighborhood of alternative segmentations\nand solving an optimization problem to choose the best neighbor.\nEmpirical results show that the proposed algorithm achieves better\nenergy that belief propagation of alpha expansion and is much faster.\n\nThis is generally a well-executed paper. The model is interesting\nand clearly defined, the algorithm is well presented with proper\nanalysis of the relevant runtimes and guarantees on the\nbehavior. Overall, the algorithm seems effective at minimizing\nthe energy of SSPN models.\n\nHaving said that, I don't think this paper is a great fit for\nICLR. The model is even somewhat to the antithesis of the idea of\nlearning representations, in that a highly structured form of\nenergy function is asserted by the human modeller, and then\ninference is performed. I don't see the connection to learning\nrepresentations. One additional issue is that while the proposed\nalgorithm is faster than alternatives, the times are still on the\norder of 1-287 seconds per image, which means that the\napplicability of this method (as is) to something like training\nConvNets is limited.\n\nFinally, there is no attempt to argue that the model produces\nbetter segmentations than alternative models. The only\nevaluations in the paper are on energy values achieved and on\ntraining data.\n\nSo overall I think this is a good paper that should be published\nat a good machine learning conference, but I don't think ICLR is\nthe right fit.", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Questions", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "", "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "is_meta_review": false}], "SCORE": 4, "authors": "Abram L. Friesen, Pedro Domingos", "KEYWORDS": "A novel extension of sum-product networks that incorporates submodular Markov random fields into the sum nodes, resulting in a highly expressive class of models in which efficient inference is still possible.", "accepted": false, "id": ""}
