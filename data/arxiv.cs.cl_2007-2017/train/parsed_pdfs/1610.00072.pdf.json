{
  "name" : "1610.00072.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Vocabulary Selection Strategies for Neural Machine Translation",
    "authors" : [ "Gurvan L’Hostis", "David Grangier", "Michael Auli" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Neural Machine Translation (NMT) has made great progress in recent years and improved the state of the art on several benchmarks (Jean et al., 2015b; Sennrich et al., 2016a; Zhou et al., 2016). However, neural systems are typically less efficient than traditional phrase-based translation models (PBMT; Koehn et al. 2003), both at training and decoding time.\nThe efficiency of neural models depends on the size of the target vocabulary and previous work has shown that vocabularies of well over 50k word types are necessary to achieve good accuracy (Jean et al., 2015a; Zhou et al., 2016). Neural translation systems compute the probability of the next target word given both the previously generated target\nwords as well as the source sentence. Estimating this conditional distribution is linear in the size of the target vocabulary which can be very large for many language pairs (Grave et al., 2016). Recent work in neural translation has adopted sampling techniques from language modeling which do not leverage the input sentence (Mikolov et al., 2011; Jean et al., 2015a; Chen et al., 2016; Zhou et al., 2016).\nOn the other hand, classical translation models generate outputs in an efficient two-step selection procedure: first, a subset of promising translation rules is chosen by matching rules to the source sentence, and by pruning them based on local scores such as translation probabilities. Second, translation hypotheses are generated that incorporate non-local scores such as language model probabilities.\nRecently, Mi et al. (2016) proposed a similar strategy for neural translation: a selection method restricts the target vocabulary to a small subset, specific to the input sentence. The subset is then scored by the neural model. Their results demonstrate that vocabulary subsets that are only about 1% of the original size result in very little to no degradation in accuracy.\nThis paper complements their study by experimenting with additional selection techniques and by analyzing speed and accuracy in more detail. Similar to Mi et al. (2016), we consider selecting target words based either on a dictionary built from Viterbi word alignments, or by matching phrases in a traditional phrase-table, or by using the k most frequent words in the target language. In addition, we investigate methods that do not rely on a traditional phrase-based translation model or alignment model to select target words. We investigate bilingual co-occurrence counts, bilingual embeddings as well as a discriminative classifier to leverage context information ar X\niv :1\n61 0.\n00 07\n2v 1\n[ cs\n.C L\n] 1\nO ct\n2 01\n6\nvia features extracted from the entire source sentence (§2).\nOur experiments show speed-ups in CPU decoding by up to a factor of 10 at very little degradation in accuracy. Training speed on GPUs can be increased by a factor of 1.33. We find that word alignments as the sole selection method is sufficient to obtain good accuracy. This is in contrast to Mi et al. (2016) who used a combination of the 2, 000 most frequent words, word alignments as well as phrase-pairs.\nSelection methods often fall short in retrieving all words of the gold standard human translation. However, we find that with a reduced vocabulary of ∼ 600 words they can recover over 99% of the words that are actually chosen by a model that decodes over the entire vocabulary. Finally, the speed-ups obtained by vocabulary selection become even more significant if faster encoder models are used, since selection removes the burden of scoring large vocabularies (§4)."
    }, {
      "heading" : "2 Vocabulary Selection Strategies",
      "text" : "This section presents different selection strategies inspired by phrase-based translation. We improve on a simple word co-occurrence method by estimating bilingual word embeddings with Hellinger PCA and then by using word alignments instead of co-occurrence counts. Finally, we leverage richer context in the source via bilingual phrases from a phrase-table or by using the entire sentence in a Support Vector Machine classifier."
    }, {
      "heading" : "2.1 Word Co-occurrences",
      "text" : "This is the simplest approach we consider. We estimate a co-occurrence table which counts how many times each source word s co-occurs with each target word t in the training bitext. The table allows us to estimate the joint distribution P (s, t). Next, we create a list of the k target words that cooccur most with each source word, i.e., the words t which maximize P (s, t) for a given s. Vocabulary selection then simply computes the union of the target word lists associated with each source word in the input.\nWe were concerned that this strategy overselects frequent target words which have higher co-occurrence counts than rare words, regardless of the source word. Therefore, we experimented with selecting target words maximizing point-wise\nmutual information (PMI) instead, i.e.,\nPMI(s, t) = P (s, t)\nP (s)P (t)\nHowever, this estimate was deemed too unreliable for low P (t) in preliminary experiments and did not perform better than just P (s, t)."
    }, {
      "heading" : "2.2 Bilingual Embeddings",
      "text" : "We build bilingual embeddings by applying Hellinger Principal Component Analysis (PCA) to the bilingual co-occurrence count matrix Mi,j = P (t = i|s = j); this extends the work on monolingual embeddings of Lebret and Collobert (2014) to the bilingual case. The resulting low rank estimate of the matrix can be more robust for rare counts. Hellinger PCA has been shown to produce embeddings which perform similarly to word2vec but at higher speed (Mikolov et al., 2013; Gouws et al., 2015).\nFor selection, the estimated co-occurrence can be used instead of the raw counts as described in the above section. This strategy is equivalent to using the low rank representation of each source word (source embedding, i.e., column vectors from the PCA) and finding the target word with the closest low rank representation (target embeddings, i.e., row vectors from the PCA)."
    }, {
      "heading" : "2.3 Word Alignments",
      "text" : "This strategy uses word alignments learned from a bilingual corpus (Brown et al., 1993). Word alignment introduces latent variables to model P (t|s), the probability of source word t given target word s. Latent variables indicate the source position corresponding to each target position in a sentence pair (Koehn, 2010).\nWe use FastAlign, a popular reparameterization of IBM Model 2 (Dyer et al., 2013). For each source word s, we build a list of the top k target words maximizing P (t|s). The candidate target vocabulary is the union of the lists for all source words.\nCompared to co-occurrence counts, this strategy avoids selecting frequent target words when conditioning on a rare source word. Word alignments will only link a frequent target word to a rare source word if no better explanation is present in the source sentence."
    }, {
      "heading" : "2.4 Phrase Pairs",
      "text" : "This strategy relies on a phrase translation table, i.e., a table pairing source phrases with corresponding target phrases. The phrase table is constructed by reading off all bilingual phrases that are consistent with the word alignments according to an extraction heuristic (Koehn, 2010).\nFor selection, we restrict the phrase table to the phrases present in the source sentence and consider the union of the word types appearing in all corresponding target phrases (Mi et al., 2016). Compared to word alignments, we hope this strategy to fetch more relevant target words as it can rely on longer source phrases to leverage richer source context."
    }, {
      "heading" : "2.5 Support Vector Machines",
      "text" : "Support Vector Machines (SVMs) for vocabulary selection have been previously proposed in (Bangalore et al., 2007). The idea is to determine a target vocabulary based on the entire source sentence rather than individual words or phrases. In particular, we train one SVM per target word taking as input a sparse vector encoding the source sentence as a bag of words. The SVM then predicts whether the considered target word is present or absent from the target sentence.\nThis classifier-based method has several advantages compared to phrase alignments: the input is not restricted to a few contiguous source words and can leverage all words in the source sentence. The model can express anti-correlation with negative weights, marking that the presence of a source word is a negative indicator for the presence of a target word. A disadvantage of this approach is that we need to feed the source sentence to all SVMs in order to get scores, instead of just reading from a pre-computed table. However, SVMs can be evaluated efficiently since (i) the features are sparse, and (ii) only features corresponding to words from the source sentence are used at each evaluation. Finally, this framework formulates the selection of each target word as an independent binary classification problem which might not favor competition between target words."
    }, {
      "heading" : "2.6 Common Words",
      "text" : "Following Mi et al. (2016), we consider adding the k most frequent target words to the above selection methods. This set includes conjunctions, determiners, prepositions and frequent verbs. Prun-\ning any such word through restrictive vocabulary selection may adversely affect the system output and is addressed by this technique."
    }, {
      "heading" : "3 Related Work",
      "text" : "The selection of a limited target vocabulary from the source sentence is classical topic in machine translation. It is often referred to as lexical selection. As mentioned above, word-based and phrase-based systems perform implicit lexical selection by building a word or phrase table from alignments to constrain the possible target words. Other approaches to lexical selection include discriminative models such as SVMs and Maximum Entropy models (Bangalore et al., 2007) as well as rule-based systems (Tufis, 2002; Tyers et al., 2012).\nIn the context of neural machine translation, vocabulary size has always been a concern. Various strategies have been proposed to improve training and decoding efficiency. Approaches inspired by importance sampling reduce the vocabulary for training (Jean et al., 2015a), byte pair encoding segment words into more frequent sub-units (Sennrich et al., 2016b), while (Luong and Manning, 2016) proposes to segment words into characters. Related work in neural language modeling is also relevant (Bengio et al., 2003; Mnih and Hinton, 2008; Chen et al., 2016). One can refer to (Sennrich, 2016) for further references.\nCloser to our work, recent work (Mi et al., 2016) presents preliminary results on using lexical selection techniques in an NMT system. Compared to this work, we investigate more selection methods (SVM, PCA, co-occurrences) and analyze the speed/accuracy trade-offs at various operating points. We report efficiency gains and distinguish the impact of selection in training and decoding."
    }, {
      "heading" : "4 Experiments & Results",
      "text" : "This section presents our experimental setup, then discusses the impact of vocabulary selection at decoding time and then during training time."
    }, {
      "heading" : "4.1 Experimental Setup",
      "text" : "We use a an encoder-decoder style neural machine translation system based on Torch.1 Our encoder\n1We will release the code with the camera ready.\nis a bidirectional recurrent neural network (Long Short Term Memory, LSTM) and an LSTM decoder with attention. The resulting context vector is fed to an LSTM decoder which generates the output (Bahdanau et al., 2015; Luong et al., 2015a). We use a single layer setup both in the encoder and as well as the decoder, each with 512 hidden units. Decoding experiments are run on a CPU since this is the most common type of hardware for inference. For training we use GPUs which are the most common hardware for neural network fitting. Specifically, we rely on 2.5GHz Intel Xeon 5 CPUs and Nvidia Tesla M40 GPUs. Decoding times are based on a single CPU core and training times are based on a single GPU card.\nWord alignments are computed with FastAlign (Dyer et al., 2013) in both language directions and then symmetrized with ’grow-diag-final-and’. Phrase tables are computed with Moses (Koehn et al., 2007) and we train support vector machines with SvmSgd (Bottou, 2010). We also use the Moses preprocessing scripts to tokenize the training data.\nWe experiment on two language pairs. The majority of experiments are on WMT-15 English to German data (Bojar et al., 2015); we use newstest2013 for validation and newstest2010-2012 as well as newstest2014,2015 to present final test results. Training is restricted to sentences of no more than 50 words which results in 3.6m sentence pairs. We chose the vocabulary sizes following the same methodology. We use the 100k most frequent words both for the source and target vocabulary. At decoding time we use a standard beam search with a beam width of 5 in all experiments. Unknown output words are simply replaced with the source word whose attention score is largest (Luong et al., 2015b).\nWe also experiment with WMT-16 English to Romanian data using a similar setting but allowing sentences of up to 125 words (Bojar et al., 2016). Since the training set provided by WMT is limited to 600k sentence pairs, we add the synthetic training data provided by Sennrich et al. (2016a). This results in a total of 2.4m sentence pairs. Our source vocabulary comprises the 200k most frequent words and the target vocabulary contains 50k words."
    }, {
      "heading" : "4.2 Selection for Efficient Decoding",
      "text" : "Decoding efficiency of neural machine translation is still much lower than for traditional phrasebased translation. For NMT, the running time of beam search on a CPU is dominated by the last linear layer that computes a score for each target word. Vocabulary selection can therefore have a large impact on decoding speed. Figure 1 shows that a reduced vocabulary of ∼ 460 types (144 msec) can achieve a 10X speedup over using the full 100k-vocabulary (∼ 1, 600 msec).\nNext we investigate the impact of reduced vocabularies on accuracy. Figure 2 compares BLEU for the various selection strategies on a wide range of vocabulary sizes. Broadly, there are two groups of techniques: first, co-occurrence counts and bilingual embeddings (PCA) are not able to match the baseline performance (Full 100k) even with over 5k candidate words per sentence. Second, even with fewer than 1, 000 candidates per sentence, word alignments, phrase pairs and SVMs nearly match the full vocabulary accuracy.\nAlthough co-occurrence counts and PCA have shown useful to measure semantic relatedness (Brown et al., 1992; Lebret and Collobert, 2014), it seems that considering the whole source sentence as the explanation of a target word without latent alignment variables undermines their selection ability. Overall, word alignments work as well or better than the other techniques relying on a wider input context (phrase pairs and SVMs). Querying a word table is also more efficient than querying a phrase-table or evaluating SVMs. We\ntherefore use word alignment-based selection for the rest of our analysis.\nMi et al. (2016) suggest that adding common words to a selection technique could have a positive impact. We therefore consider adding the most frequent k words to our word alignmentbased selection. Figure 3 shows that this actually has little impact on BLEU in our setting. In fact, the overlap of the results for n = 0 and 50 indicates that most of the top 50 words are already selected, even with small candidate sets.\nNext we try to get a better sense of how precise selection is with respect to the words used by a human translator or with respect to the transla-\ntions generated by the full vocabulary model. We use word alignments for this experiment. Figure 4 shows coverage with respect to the reference (left) and with respect to the output of the full vocabulary system (right). We do not count unknown words (UNK) in all settings, even if they may later be replaced by a source word (§4.1). Not counting UNKs is the reason why the full vocabulary models do not achieve 100% coverage in either setting. The two graphs show different trends: On the left, coverage with respect to the reference for the full vocabulary is 95.1%, while selection achieves 87.5% with a vocabulary of 614 words (3rd point on graph). However, when coverage is measured with respect to the full vocabulary system output, the coverage of selection is very close to the full vocabulary model with respect to itself, i.e., when unknown words are not counted. In fact, the selection model covers over 99% of the nonUNK words in the full vocabulary output. This result shows that selection can recover almost all of the words which are effectively selected by a full vocabulary model while discarding many words which are not chosen by the full model.\nWhat is the exact speed and accuracy trade-off when reducing the output vocabulary? Figure 5 plots BLEU against decoding speed. We pick a number of operating points from this graph for\nour final test set experiments (Table 1). For our best methods (word alignments, phrase alignments and SVMs) we pick points such that vocabularies are kept small while maintaining good accuracy compared to the full vocabulary setting. For cooccurrence counts and bilingual PCA we choose settings with comparable speed.\nOur test results (Table 1) confirm the validation experiments. On English-German translation we achieve more than a 10-fold speed-up over the full vocabulary setting. Accuracy for the word alignment-based selection matches the full vocabulary setting on most test sets or decreases only slightly. For example with word alignment selection, the largest drop is on newstest2015 which achieves 22.2 BLEU compared to 22.5 BLEU for the full setting on English-German; the best single-system neural setup at WMT15 achieved 22.4 BLEU on this dataset (Jean et al., 2015b). On English-Romanian, we achieve a speed-up of over 5 times with word alignments at 28.1 BLEU versus 27.9 BLEU for the full vocabulary baseline. This matches the state-of-the-art on this dataset (Sennrich et al., 2016a) from WTM16. The smaller speed-up on English-Romanian is due to the smaller vocab of the baseline in this setting which is 50k compared to 100k for EnglishGerman."
    }, {
      "heading" : "4.3 Selection for Better Training",
      "text" : "So far our evaluation focused on vocabulary selection for decoding, relying on a model trained with\nthe full vocabulary. Next we address the question of whether the efficiency advantages observed for decoding translate to training as well. Selection at training may impact generalization performance either way: it assimilates training and testing conditions which could positively impact accuracy. However, training with a reduced vocabulary could result in worse parameter estimates, especially for rare words which would receive much fewer updates because they would be selected less often.\nWe run training experiments on WMT English to German with word alignment-based selection. In addition to the selected words, we include the target words of the reference and train a batch with the union of the sentence-specific vocabularies of all samples (Mi et al., 2016).\nFigure 6 compares validation accuracy of models trained with selection or with the full vocabulary. Selection in both training and decoding gives a small accuracy improvement. However, this improvements disappears for vocabulary sizes of 500 and larger; we found the same pattern on other test sets. Similar to our decoding experiments, adding common words during training did not improve accuracy.\nTable 2 shows the impact of selection on training speed. Our bi-directional LSTM model (BLSTM) can process the same number of samples in 25% less time on a GPU with a batch size of 32 sentences. We do not observe changes in\nthe number of epochs required to obtain the best validation BLEU.\nThe speed-ups for training are significantly smaller than for decoding (Table 1). This is because training scores the vocabulary exactly once per target position, while beam search has to score multiple hypotheses at each generation step.\nWe suspect that training is now dominated by the bi-directional LSTM encoder. To confirm this, we replaced the encoder with a simple average pooling model which encodes source words as the mean of word and position embeddings over a local context (Ranzato et al., 2016). Table 2 shows that in this setting the efficiency gains of vocabu-\nlary selection are more substantial (40% less time per epoch). This model is not as accurate and achieves only 18.5 BLEU on newstest2015 compared to 22.5 for the bi-directional LSTM encoder. However, it shows that improving the efficiency of the encoder is a promising future work direction."
    }, {
      "heading" : "5 Conclusions",
      "text" : "This paper presents a comprehensive analysis of vocabulary selection techniques for neural machine translation. Vocabulary selection constrains the output words to be scored to a small subset relevant to the current source sentence. The idea is to avoid scoring a high number of unlikely candidates with the full model which can be ruled out by simpler means.\nWe extend previous work by considering a wide range of simple and complex selection techniques including bilingual word co-occurrence counts, bilingual embeddings built with Hellinger PCA,\nword alignments, phrase pairs, and discriminative SVM classifiers. We explore the trade-off between speed and accuracy for different vocabulary sizes and validate results on two language pairs and several test sets.\nOur experiments show that decoding speed-up can be reduced by up to 90% without compromising accuracy. Word alignments, bilingual phrases and SVMs can achieve high accuracy, even when considering fewer than 1, 000 word types per sentence.\nAt training time, we achieve a speed-up of up to 1.33 with a bi-directional LSTM encoder and 1.66 with a faster alternative. Efficiency increases are less pronounced during training because of two combined factors. First, vocabulary scoring at the final layer of the model is a smaller part of the computation compared to beam search. Second, state-of-the-art bi-directional LSTM encoders (Bahdanau et al., 2015) are relatively costly compared to scoring the vocabulary on GPU hardware. Efficiency gains from vocabulary selection highlight the importance of progress towards efficient, accurate encoder and decoder architectures."
    } ],
    "references" : [ {
      "title" : "Kyunghyun Cho",
      "author" : [ "Dzmitry Bahdanau" ],
      "venue" : "and Yoshua Bengio.",
      "citeRegEx" : "Bahdanau et al.2015",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Patrick Haffner",
      "author" : [ "Srinivas Bangalore" ],
      "venue" : "and Stephan Kanthak.",
      "citeRegEx" : "Bangalore et al.2007",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Pascal Vincent",
      "author" : [ "Yoshua Bengio", "Réjean Ducharme" ],
      "venue" : "and Christian Janvin.",
      "citeRegEx" : "Bengio et al.2003",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "and Pavel Pecina",
      "author" : [ "Ondej Bojar", "Rajan Chatterjee", "Christian Federmann", "Barry Haddow", "Chris Hokamp", "Matthias Huck", "Varvara Logacheva" ],
      "venue" : "editors.",
      "citeRegEx" : "Bojar et al.2015",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Large-scale machine learning with stochastic gradient descent",
      "author" : [ "Léon Bottou" ],
      "venue" : "In International Conference on Computational Statistics",
      "citeRegEx" : "Bottou.,? \\Q2010\\E",
      "shortCiteRegEx" : "Bottou.",
      "year" : 2010
    }, {
      "title" : "and Jenifer C",
      "author" : [ "Peter F. Brown", "Peter V. deSouza", "Robert L. Mercer", "Vincent J. Della Pietra" ],
      "venue" : "Lai.",
      "citeRegEx" : "Brown et al.1992",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "and Robert L",
      "author" : [ "Peter F. Brown", "Vincent J. Della Pietra", "Stephen A. Della Pietra" ],
      "venue" : "Mercer.",
      "citeRegEx" : "Brown et al.1993",
      "shortCiteRegEx" : null,
      "year" : 1993
    }, {
      "title" : "David Grangier",
      "author" : [ "Wenlin Chen" ],
      "venue" : "and Michael Auli.",
      "citeRegEx" : "Chen et al.2016",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "and Noah A",
      "author" : [ "Chris Dyer", "Victor Chahuneau" ],
      "venue" : "Smith.",
      "citeRegEx" : "Dyer et al.2013",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Yoshua Bengio",
      "author" : [ "Stephan Gouws" ],
      "venue" : "and Greg Corrado.",
      "citeRegEx" : "Gouws et al.2015",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "David Grangier",
      "author" : [ "Edouard Grave", "Armand Joulin", "Moustapha Cissé" ],
      "venue" : "and Hervé Jégou.",
      "citeRegEx" : "Grave et al.2016",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "On using very large target vocabulary for neural machine translation. In Annual Meeting of the Association for Computational Linguistics (ACL)",
      "author" : [ "Jean et al.2015a] Sébastien Jean", "KyungHyun Cho", "Roland Memisevic", "Yoshua Bengio" ],
      "venue" : null,
      "citeRegEx" : "Jean et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Jean et al\\.",
      "year" : 2015
    }, {
      "title" : "2015b. Montreal neural machine translation systems for wmt15",
      "author" : [ "Jean et al.2015b] Sébastien Jean", "Orhan Firat", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio" ],
      "venue" : "In Workshop on Statistical Machine Translation (WMT)",
      "citeRegEx" : "Jean et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Jean et al\\.",
      "year" : 2015
    }, {
      "title" : "Franz Josef Och",
      "author" : [ "Philipp Koehn" ],
      "venue" : "and Daniel Marcu.",
      "citeRegEx" : "Koehn et al.2003",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Moses: Open source toolkit for statistical machine translation",
      "author" : [ "Shen", "Christine Moran", "Richard Zens" ],
      "venue" : "In Annual Meeting of the Association for Computational Linguistics (ACL)",
      "citeRegEx" : "Shen et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2007
    }, {
      "title" : "Statistical Machine Translation",
      "author" : [ "Philipp Koehn" ],
      "venue" : null,
      "citeRegEx" : "Koehn.,? \\Q2010\\E",
      "shortCiteRegEx" : "Koehn.",
      "year" : 2010
    }, {
      "title" : "Word embeddings through hellinger pca. In European Chapter of the Association for Computational Linguistics (EACL)",
      "author" : [ "Lebret", "Collobert2014] Rémi Lebret", "Ronan Collobert" ],
      "venue" : null,
      "citeRegEx" : "Lebret et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Lebret et al\\.",
      "year" : 2014
    }, {
      "title" : "Achieving open vocabulary neural machine translation with hybrid word-character models. In Annual Meeting of the Association for Computational Linguistics (ACL)",
      "author" : [ "Luong", "Manning2016] Minh-Thang Luong", "Christopher D. Manning" ],
      "venue" : null,
      "citeRegEx" : "Luong et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Luong et al\\.",
      "year" : 2016
    }, {
      "title" : "Effective approaches to attention-based neural machine translation",
      "author" : [ "Hieu Pham", "Christopher D. Manning" ],
      "venue" : "In Conference on Empirical Methods in Natural Language Processing (EMNLP)",
      "citeRegEx" : "Luong et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Luong et al\\.",
      "year" : 2015
    }, {
      "title" : "Addressing the rare word problem in neural machine translation. In Annual Meeting of the Association for Computational Linguistics (ACL)",
      "author" : [ "Luong et al.2015b] Thang Luong", "Ilya Sutskever", "Quoc V. Le", "Oriol Vinyals", "Wojciech Zaremba" ],
      "venue" : null,
      "citeRegEx" : "Luong et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Luong et al\\.",
      "year" : 2015
    }, {
      "title" : "Zhiguo Wang",
      "author" : [ "Haitao Mi" ],
      "venue" : "and Abe Ittycheriah.",
      "citeRegEx" : "Mi et al.2016",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Lukáš Burget",
      "author" : [ "Tomáš Mikolov", "Anoop Deoras", "Daniel Povey" ],
      "venue" : "and Jan Černocký.",
      "citeRegEx" : "Mikolov et al.2011",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Corrado",
      "author" : [ "Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Gregory S" ],
      "venue" : "and Jeffrey Dean.",
      "citeRegEx" : "Mikolov et al.2013",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "A scalable hierarchical distributed language model",
      "author" : [ "Mnih", "Hinton2008] Andriy Mnih", "Geoffrey E. Hinton" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS)",
      "citeRegEx" : "Mnih et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Mnih et al\\.",
      "year" : 2008
    }, {
      "title" : "Sequence Level Training with Recurrent Neural Networks",
      "author" : [ "Sumit Chopra", "Michael Auli", "Wojciech Zaremba" ],
      "venue" : null,
      "citeRegEx" : "Ranzato et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Ranzato et al\\.",
      "year" : 2016
    }, {
      "title" : "2016a. Edinburgh neural machine translation systems for WMT 16",
      "author" : [ "Barry Haddow", "Alexandra Birch" ],
      "venue" : "In Workshop on Machine Translation (WMT)",
      "citeRegEx" : "Sennrich et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2016
    }, {
      "title" : "Neural machine translation of rare words with subword units. In Annual Meeting of the Association for Computational Linguistics (ACL)",
      "author" : [ "Barry Haddow", "Alexandra Birch" ],
      "venue" : null,
      "citeRegEx" : "Sennrich et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2016
    }, {
      "title" : "Tutorial on neural machine translation. In Second Machine Translation Marathon in the Americas (MTMA)",
      "author" : [ "Rico Sennrich" ],
      "venue" : null,
      "citeRegEx" : "Sennrich.,? \\Q2016\\E",
      "shortCiteRegEx" : "Sennrich.",
      "year" : 2016
    }, {
      "title" : "A cheap and fast way to build useful translation lexicons",
      "author" : [ "Dan Tufis" ],
      "venue" : "In International Conference on Computational Linguistics (COLING)",
      "citeRegEx" : "Tufis.,? \\Q2002\\E",
      "shortCiteRegEx" : "Tufis.",
      "year" : 2002
    }, {
      "title" : "Felipe SánchezMartı́nez",
      "author" : [ "Francis M Tyers" ],
      "venue" : "Mikel L Forcada, et al.",
      "citeRegEx" : "Tyers et al.2012",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Peng Li",
      "author" : [ "Jie Zhou", "Ying Cao", "Xuguang Wang" ],
      "venue" : "and Wei Xu.",
      "citeRegEx" : "Zhou et al.2016",
      "shortCiteRegEx" : null,
      "year" : 2016
    } ],
    "referenceMentions" : [ ],
    "year" : 2016,
    "abstractText" : "Classical translation models constrain the space of possible outputs by selecting a subset of translation rules based on the input sentence. Recent work on improving the efficiency of neural translation models adopted a similar strategy by restricting the output vocabulary to a subset of likely candidates given the source. In this paper we experiment with context and embedding-based selection methods and extend previous work by examining speed and accuracy trade-offs in more detail. We show that decoding time on CPUs can be reduced by up to 90% and training time by 25% on the WMT15 English-German and WMT16 English-Romanian tasks at the same or only negligible change in accuracy. This brings the time to decode with a state of the art neural translation system to just over 140 msec per sentence on a single CPU core for English-German.",
    "creator" : "LaTeX with hyperref package"
  }
}