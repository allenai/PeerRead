{
  "name" : "1206.4685.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Sparse-GEV: Sparse Latent Space Model for Multivariate Extreme Value Time Series Modeling",
    "authors" : [ "Yan Liu", "Mohammad Taha Bahadori", "Hongfei Li" ],
    "emails" : [ "liho@us.ibm.com" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Time series analysis and modeling have been extensively studied in the literature and successfully found applications across domains (Box & Jenkins, 1990; Hamilton, 1994). In many applications, such as climate science, social media analysis and smart grid, we are mostly interested in revealing the temporal dependence and make predictions of extreme events. For\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nexample, climate change is mostly characterized by increasing probabilities of extreme weather patterns (IPCC, 2007), such as temperature or precipitation reaching extremely high value. Therefore quantifying the temporal dependence between the extreme events from different locations and make effective predictions are important for disaster prevention; in social media analysis, burst of topics, i.e., buzz, is reflected by extremely high frequency of related words. Uncovering the temporal dependencies between buzzes could reveal valuable insights into information propagation and achieve much better accuracy for buzz prediction.\nIdentifying temporal dependencies between multiple time-series data is a topic of significant interest (Arnold et al., 2007; Lozano et al., 2009a;b). Many algorithms are proposed to automatically recover the temporal structures, such as autocorrelation, crosscorrelations (Box & Jenkins, 1990), randomization test (Edgington & Onghena, 2007), Granger causality (Granger, 1980), transfer entropy (Beirlant et al., 1997; Barnett et al., 2009), and so on. However, uncovering temporal dependency for extreme values is much more challenging than classical observations since the distributions of extreme values are more complex and significantly different from the commonly used Gaussian distribution. In addition, the lack of sufficient past observations on extreme events poses difficulties in modeling and attributing such events.\nThe statistical approach we can utilize to solve these important problems is the theory of extreme value modeling (Coles, 2001; Beirlant et al., 2004), which provides a natural family of probability distributions for modeling the magnitude of the largest (or smallest) of a large number of events, and a canonical stochastic process model (Coles, 2001) for the occurrence of events above a very high (or below a very low) threshold. In the past decade, extreme value modeling has attracted a lot of research efforts in statistics, finance,\nand environmental science, particularly on modeling temporal and spatio-temporal extreme value (Coles & Tawn, 1996; Ferro & Segers, 2003; Huerta & Sanso, 2007). However, all of the work above model temporal or spatial dependence with predefined covariance structures (e.g. without independence considerations). Furthermore, most general discussions of dependencies in multivariate extreme value modeling has been focused on pairwise relationships. This is obviously unrealistic and demands a significant contribution on automatically learning the temporal structures from the data for better analysis and modeling.\nIn this paper, we propose a sparse latent space model, namely sparse-GEV model, to solve the problem. The basic idea of our approach is to model the multivariate extreme value time series as a latent space model. The latent variables, corresponding to the location parameters (which determine the mode) of extreme value distributions for time series at certain time, are modeled by the location parameters of all time series in history, through a dynamic linear model (DLM). By imposing an L1-penalty with respect to the regression coefficients in DLM, we could establish meaningful temporal dependencies between a small subset of time series and the concerned time series of extreme values. To estimate parameters of the model, we develop a iterative searching algorithm based on the generalized EMalgorithms and sampling with particle filtering. Our model is significant because it is among the first models to reveal the temporal dependencies between multiple extreme value time series. In addition, our experiment results demonstrate the superior performance of our model to other state-of-art methods on both learning temporal dependence and predicting future value.\nThe rest of the paper is organized as follows: we first describe the details of our proposed model in Section 2, then we review the existing work and discuss their connections to our model in Section 3, we show the experiment results in Section 4, and finally we summarize the paper with hints on future work."
    }, {
      "heading" : "2. Methodology",
      "text" : "Preliminaries Before diving into the details of our model, we first briefly review the extreme value theory (Coles, 2001). Let X1, · · · , Xm be a sequence of independent and identically distributed random variables, and let Mm = max{X1, · · · , Xm}. If there exist sequences of constants am > 0 and bm such that\nPr\n( Mm − bm am ≤ z ) → G(z) as m→∞ , (1)\nfor some non-degenerate distribution function G, then G should belong to the generalized extreme value (GEV) families, namely\nG(z) = exp { − [ 1 + ξ ( z − µ σ )]−1/ξ + } , (2)\ndefined on {z : 1 + ξ(z − µ)/σ > 0}, where µ (−∞ < µ < ∞) is the location parameter, σ (σ > 0) is the scale, and ξ (−∞ < ξ <∞) is the shape parameter ξ, which governs the tail behavior of the distribution.\nOne popular GEV distribution is the Gumbel distribution when ξ → 0, whose pdf is defined as\np(z|µ, σ) = 1 σ exp\n{ −z − µ\nσ − exp\n( −z − µ\nσ\n)} . (3)\nIt has been shown that the maximum value in a sample of a random variable following an exponential family distribution (such as Gaussian, Lognormal and Gamma distributions) converge to the Gumbel distribution. One special property of the Gumbel distribution is that the mode is determined solely by the location parameter µ."
    }, {
      "heading" : "2.1. Model Description",
      "text" : "Given multivariate time series data, our goal is to build an effective model that can recover temporal dependence between extreme value time series (block maxima or peaks over threshold) and make accurate predictions for future extreme events. To achieve a robust and interpretable model, a natural choice is to capture the temporal dependence via linear models; however, this is not directly achievable on extreme value variables since their temporal dependence is obviously nonlinear. To solve the problem, we propose latent models in which the location parameters of GEV distributions are latent variables and the temporal dependence between extreme value variables is captured via the latent variables through dynamic linear model. We choose the location parameters because they capture the mode of extreme value variables and can be modeled reasonably well by linear dependence.\nFormally, let x1, . . . ,xP denote P number of extreme value time series and each time series xi have T observations, i.e., xi = {xi1, . . . , xiT }1, we define the joint\n1 In extreme value theory, two main sets of methods, the Block Maxima method and the Peaks over Thresholds method have been developed to model extreme values (Coles, 2001). In the rest of the paper, we use Block Maxima method as an example to describe our model. Notice that our methodology is applicable to the Peaks over Thresholds approach by defining a point process model and in the experiments we have used both approaches.\nprobability of observations {xit} and their associated location parameter {µit} as:\np({xit}, {µit}|β,σ, c) = (4) P∏ i=1 T∏ t=L+1 p(xit|µit, σi)p(µit|{µ j t−l},β, c),\nwhere p(xit|µit, σi) can be modeled by a GEV distribution such as the Gumbel distribution in eq(3) with σi as the scale parameter specific to time series xi, {µjt−l} is the history of all time series at time t with a maximal lag of L, and p(µit|{µ j t−l},β, c) can be modeled by a dynamic linear model as follows,\nµit = c i + L∑ l=1 P∑ j=1 βij,lµ j t−l + . (5)\nwhere ci is the offset specific to time series i, βi are the coefficients, and is a Gaussian noise with variance τ2. As we can see, the temporal dependence between xi and xj is now captured via the coefficients β. By adding a shrinkage Laplace prior over β when maximizing the likelihood function, i.e.,\n{β̂, σ̂, ĉ} = arg max `(x1, . . . ,xP ;β,σ, c)+ P∑ i=1 λ‖βi‖1, (6) where λ is the regularization parameter, we can obtain the sparse solution of β. Finally, we determine that xi temporally depends on xj if the corresponding value of βij is non-zero. In this way, our model not only can provide better understanding of potential causes of the extreme events, but also helps to achieve more accurate prediction of the extreme events in the future. This model is later referred to as the SparseGEV model."
    }, {
      "heading" : "2.2. Inference and Learning",
      "text" : "Given the existence of hidden variables in the sparseGEV model, directly maximizing the likelihood as in eq(6) is not feasible. Therefore we applied the generalized EM-algorithm to solve the problem.\nNext, we use Gumbel distribution as an example to demonstrate how we can make efficient inference and learning in the proposed model. In the EM algorithm, we optimize the following function via two steps:\nQ(β,σ, c;βold,σold, cold) = − P∑\ni=1 T∑ t=L+1 ln(σi)\n− E{µ|X,βold,σold,cold} [ xit − µit σi + exp ( −x i t − µit σi )\n+ 1\n2\n( µit − ci − ∑L l=1 ∑P j=1 β i j,lµ j t−l\nτ )2 . (7) E Step Directly calculating the expectations in eq(7) is infeasible given the form of the posterior probability, therefore we apply sampling algorithms for approximation. In order to generate samples from p { µ|X,βold,σold, cold } , we use the particle filtering algorithm (Doucet & Johansen, 2009). The major challenge is that in each iteration of particle filtering, we need to draw samples from p(µit|xit, {µ j t−l}), which cannot be calculated analytically. Instead, we use the following proposal function:\nN ( µ̃it + γiτ − σiW0 ( γ2i exp ( µ̃it − xit σi + γ2i )) , τ2\nγ2i + 1\n) ,\nwhere W0 is the Lambert W function, γi = τ/σi, and µ̃it is calculated using the history, i.e., µ̃ i t =\nci,old + ∑L l=1 ∑P j=1 β i,old j,l µ̃ j t−l. The rationale behind\nthis choice is to approximate the posterior distribution p { µ|X,βold,σold, cold } with a Gaussian distribution with the same mode and similar variance.\nNotice that particle filtering may encounter the challenge of “miniscule weights” if the sequence length is long. Therefore the resampling step is usually applied at each time stamp to resolve the issue (Doucet & Johansen, 2009). For very long time series, particle filtering does face some other challenges, but can be fixed using particle smoothing (Doucet & Johansen, 2009).\nM Step The optimization problem for updating βi and ci is as follows:\nmin βi,ci\nEµ|x T∑\nt=L+1 µit − ci − L∑ l=1 P∑ j=1 βij,lµ j t−l 2 + λ ∥∥βi∥∥ 1 ,\nwhere the expectation is computed from the samples. As we can see, the optimization function has the Lasso format and can be solved efficiently by algorithms such as coordinate descent (Wu & Lange, 2008). The parameter estimation for the Gumbel distribution itself is not a trivial problem. In general, the MLE is the widely accepted approach to estimate the shape and scale parameters, and Newton-Raphson or quasiNewton methods can be applied to solve the resulting\noptimization problem (Evans et al., 2000). Therefore we estimate σ by the Newton-Raphson algorithm."
    }, {
      "heading" : "2.3. Prediction",
      "text" : "In order to make predictions on the future value of extreme events, for example xiT+1, given the extreme value time series up to time T , we can first estimate the mean µ̄iT+1 using the samples drawn from the posterior distribution with the learned parameters. Based on the model defined in eq(4), we can predict xiT+1 as\nx̂iT+1 = µ̄ i T+1 + γEσ i,\nwhere µ̄iT+1 = c i + ∑L l=1 ∑P j=1 β i j,lµ̄ j T−l+1, and γE(≈ 0.5771) is the Euler constant."
    }, {
      "heading" : "2.4. Scalability",
      "text" : "The computational complexity of Sparse-GEV depends on two factors: the number of EM iterations required for convergence and the scalability of E-Step and M-Step. In Section 5, we empirically show that EM usually converges within a small number of iterations. In the M-Step, while there are efficient solvers for both equations, the problems for different time series are independent and can be implemented in parallel. The particle filtering in E-Step is notoriously efficient for sampling from time series mainly due to three reasons: (i) it requires only one iteration to generate the samples, (ii) the generated samples are independent; no burn-in period or decoupling is required and (iii) at each time stamp the sampling procedures in different locations are independent from each other and can be implemented in parallel. Therefore our algorithm is scalable and could be easily applicable to practical applications."
    }, {
      "heading" : "3. Related Work and Discussions",
      "text" : "Very recently, a few advanced approaches have been explored to uncover temporal dependence from time series data, including Lasso-Granger (Arnold et al., 2007), transfer entropy (Schreiber, 2000), and the copula approach (Liu et al., 2009). In this section, we discuss how these algorithms can be applicable to extreme value time series analysis and their connections to Sparse-GEV."
    }, {
      "heading" : "3.1. Related Work",
      "text" : "Granger causality In (Arnold et al., 2007), the Lasso-Ganger algorithm, an effective and efficient approach to learn sparse temporal graphs, is developed by combining Granger causality with sparse neighborhood selection using L1-penalized regression. More\nspecifically, given p number of time series, x1, . . . ,xp, where xi = {xit : t = 0, . . . , T}, let X Lagged t,L represent the concatenated vector of all the lagged variables (with a maximal lag of L) of up to time t, i.e., {xt−lj : j = 1, . . . , p, l = 1, . . . , L}. Then the temporal graphs can be learned by the following regularized regression:\nβ̂i(λ) = arg min βi ( T∑ t=1 ‖xti−X Lagged t,L βi‖ 2+λ‖βi‖1), (8)\nwhere there is an edge from xj to xi if and only if at least one of the corresponding coefficients in β̂i is non-zero. The Lasso-Granger algorithm can be directly applied to extreme value observations to infer the dependency graph, but obviously this violates the common assumptions of linear dependency in Granger causality.\nTransfer Entropy Solution Transfer entropy is usually employed when the data do not follow the autoregressive model and a nonlinear generalization of the Granger causality framework is desirable. In the Transfer entropy framework (Schreiber, 2000), time series xi is thought to be a cause of another time series xj if the values of xi in the past significantly decrease the uncertainty in the future values of xj given its past. The amount of decrease in the uncertainty can be quantified as\nTxi→xj = H(x j t |xit−L:t−1)−H(x j t |x j t−L:t−1, x i t−L:t−1),\nwhere H(x) is the Shannon entropy of the random variable x. Since the transfer entropy is a pairwise quantity, we can use its output as input to a graph learning algorithm, for example, IAMB (Tsamardinos et al., 2003), to uncover the temporal dependency among multiple time series.\nThe transfer entropy approach can be used to uncover causality relationship among extreme value time series since it does not rely on any particular assumptions on the distribution of the time series.\nCopula Approach The copula approach has been proposed for dependency analysis of time series with non-Gaussian marginal distributions (Embrechts et al., 2002). It has been used for forecast in time series (Leong & Valdez, 2005) and learning sparase dependency structures (Liu et al., 2009). In a copula framework, e.g., Gaussian copula, the marginal distribution of the time series Xi are estimated as F̃i. Next the observations are transformed to the Gaussian copula domain as U it = Φ −1(F̃i(X i t)), where Φ is the cdf of the unit Gaussian distribution. Finally the temporal causal graph can be uncovered by analysis of dependency among U it using algorithms such as glasso\nalgorithm (Friedman et al., 2008). We report an edge from node i to node j if the precision matrix has at least one non-zero element from lagged U jt−` to U i t , for ` ≥ 1. The method in (Leong & Valdez, 2005) can be used for predicting the future values of the time series.\nIn order to uncover temporal dependencies among extreme value time series, we can either estimate the marginals with a non-parametric density estimator or use the GEV distribution to estimate the marginal distribution. For extreme value time series, the latter is preferred since the non-parametric approximation of the marginal distributions could lead to over-fitting when the number of observations is scarce."
    }, {
      "heading" : "3.2. Connections to existing algorithms",
      "text" : "The connections between our algorithm with existing algorithms can be established by considering SparseGEV, transfer entropy and the copula approach as extensions of the Granger causality framework. The copula approach leverages the marginal distribution of the time series to map the observations to another space and assumes linear dependence in the new space. Sparse-GEV discovers the Granger causality relationship among the latent variables from which the observations have been generated. The transfer entropy approach generalizes the Granger causality framework by finding the Granger causality type relationships from the uncertainty of the time series. In fact, when the data are distributed according to Gaussian linear model, transfer entropy is equivalent to Granger causality (Barnett et al., 2009).\nFor high-dimensional time series, the number of observations is much less than the parameters of the model. The Lasso-Granger algorithm benefits from the variable selection properties of Lasso. (Meinshausen & Bühlmann, 2006) show that the Lasso variable selection loss, and subsequently the Lasso-Granger’s loss (Arnold et al., 2007), vanishes with an exponential rate. For the copula approach, (Liu et al., 2009) show that when copula-based model is the true model, the copula-based structure learning algorithm with non-parametric estimation of marginals converges to\nthe true graph with a rate of O (√\nlog(n) n1−ξ\n) for some\nξ ∈ (0, 1), which is far slower than the exponential convergence of Lasso-Granger. The performance of transfer entropy heavily relies on the accuracy of entropy estimations, which require a large number of observations, especially for high dimensional distributions, to achieve robust estimation (Beirlant et al., 1997). For example, the Nearest Neighbor Estimator converges with root-n rate, which is again far slower\nthan the convergence rate of Lasso-Granger. However, Sparse-GEV inherits the variable selection advantages of Lasso-Granger while allows a more flexible marginal distribution for the observations. It is fully parametric, and together with proper `1 penalization can avoid over-fitting while capturing non-linear dependencies."
    }, {
      "heading" : "4. Experiment Results",
      "text" : "In order to evaluate the effectiveness of our algorithm, we conduct experiments on four datasets, including one synthetic dataset, one weather dataset and two Twitter datasets. The experiment results are evaluated on both how well we uncover the temporal dependence graphs and how accurately we can predict the future value of extreme events using the learned temporal dependence."
    }, {
      "heading" : "4.1. Datasets",
      "text" : "Synthetic Dataset We generate eight synthetic datasets, each composed of nine time-series with different types of temporal dependence, one of which is shown in Figure 1(a). Time series of length T = 40 are generated in two steps: (i) A set of observations of the location variables µ̃ is generated according to eq (4), with the offset {ci} generated from N(0.2, 0.05), the coefficients β set to have stationary time series, τ2 set to 0.1 and the time lag L set to 2; (ii) The observations x̃ are generated from a Gumbel distribution with the corresponding location parameters µ̃ and scale parameter σi = 0.05 for all time series.\nClimate Dataset The study of extreme value of wind speed and gust speed is of great interest to the climate scientists and wind power engineers. A collection of wind observations is provided by AWS Convergence Technologies, Inc. of Germantown, MD. It consists of the observations of surface wind speed (mph) and gust speed (mph) every five minutes. We choose 153 weather stations located on a grid laying in the 35N − 50N and 70W − 90W block. Following the traditions in this domain, we generated extreme value time series observations, i.e, daily maximum values, at different weather stations. The objective is examine how the wind speed (or gust speed) at different locations affects each other and how well we can make predictions on future wind speed.\nTwitter Dataset In social media analysis, “buzz” refers to those topics or memes that many people are talking about at the same time with rapid growth and impact. Buzz modeling and predictions are the fundamental problems in computational social science, but they are extremely challenging since the distributions\nSparse-GEV: Sparse Latent Space Model for Multivariate Extreme Value Time Series Modeling\nAc tu al    1   4   5   3  2   5  \n7  \n6  \n9  8  \n(a)\nGr an\nge r  \n(b)\nTr an\nsf er   E nt ro py\n  \n(c)\nC o p u la\n(d)\nSp ar se -‐G\nEV   \n(e)\nFigure 1. Illustration of (a) Ground truth and the inferred temporal dependence graphs by (b) Granger causality, (c) Transfer entropy, (d) Copula method, and (e) Sparse-GEV.\nof these time series observations have heavy tails and most existing models fail miserably. Given the definition of buzz, i.e., extremely high frequency of certain words within a time interval, it is natural to model them via extreme value theory. We collected two Twitter datasets to evaluate the effectiveness of our model: one is the most popular 20 meme phrases during a 28- day interval from Nov-Dec 2009, and the other is popular hashtags around “occupy wall street” during a 21- day interval in Oct-Nov 2011. Some example phrases in the first dataset are “Haiti earthquake”, “Grammy Awards”, “iPad release”, and “Scott Brown’s Senate Election”; some example hashtags in the second dataset are #OWS, #OccupyLA, #OccupySF, #OccupyDC and #OccupyBoston. For those phrases and hashtags, we count the number of mentions in tweets within a interval of one hour. We are interested in uncovering how different buzzes affect each other and how well we can make predictions on future buzz."
    }, {
      "heading" : "4.2. Performance Comparison",
      "text" : "We compare the performance of our Sparse-GEV model with three baselines, including Granger causality, transfer entropy, and the copula method (with Gaussian copula), on two tasks: uncovering the underlying dependency among time series, and predicting the future values of time series. The first one requires knowledge about the true dependency structure, which is only available in the synthetic dataset. For evaluation, we use the Area Under the Curve (AUC) score, i.e., the probability that the algorithm will assign a higher value to a randomly chosen positive (existing) edge than a randomly chosen negative (nonexisting) edge in the graph. In the prediction task, we conduct experiments via the sliding window approach: given time series observations of length T and a window size S, we train a model on observations of xs, . . . , xT−S+s−1 and test it on the (T − S + s)th sample, for s = 1, . . . , S. We set S to 10 for all the datasets and use the root mean squared error (RMSE) measure (averaged over S experiments and all nodes) as the evaluation metric.\nIn the experiment, the regularization parameter λ is set via cross-validation. All the observations are normalized into interval [0, 1] prior the experiments.\nTemporal Dependence Discovery Table 1 lists the average accuracy of uncovering the underlying dependence structures by different algorithms on the synthetic data (consisting of 8 different datasets). As we can see, our Sparse-GEV model significantly outperforms the baseline methods. Figure 1 shows an example of the graphs learned by different algorithms: our model can recover the ground-truth graph more accurately than other methods.\nFig. 2 shows the inferred temporal dependencies from the extreme value time series of wind speed and wind gust speed by Sparse-GEV. Given the limited space, we limit our discussion on the new york region. The main observation is that the weather in the inland regions are heavily influenced by the coastline region. The wind gust graph (Fig. 2(b)) indicates two clusters. One is at the top of the graph, starting from Middletown to Danbury across Fishkill. The other one is located at the bottom of the graph, which passes through several cities, such as Stamford, Fairfield and Brookhaven, around Long Island Sound, then goes to inland cities in New Jersey through New York City. The top cluster gives an example of wind gust path in inland while the bottom one shows the coastal impact of Long Island Sound and the impact extends to inland New Jersey. Comparably, in addition to the Middle-\nPajek (a) (b)\nFigure 2. The temporal dependence graph learned by Sparse-GEV on the extreme value time series of (a) Wind in NY and (b) Gust in NY. Thicker edges imply stronger dependency.\ntown to Danbury inland cluster in wind gust graph, the wind graph (Fig. 2(a)) shows another inland cluster centered at Bridgewater, which has strong temporal dependencies with neighboring cities (confirmed by the climatologists).\nFig. 3 shows the inferred temporal dependencies from the extreme value time series of Twitter data on a subset of buzz. From Fig. 3(a), we can see that the temporal dependence between different buzz are sparse (since they are quite different topics); however, the buzz on “Haiti Earthquake” generates a huge impact on the whole Twitter universe and significantly changes the future mentions of other popular meme phrases. An interesting observation in Fig. 3(b) is that the hashtag on the general theme #OWS has direct temporal dependence with the city-specific hashtags, such as #O-LA and #O-DC, while city-specific hashtags do not affect each other.\nPrediction Performance As discussed in Section 2, Sparse-GEV can also be used for predicting future extreme events. For other baseline methods, we use the approaches discussed in Section 3.1 for predictions. Table 2 shows the prediction accuracy of different algorithms on all datasets. As we can see, SparseGEV outperforms all the other algorithms across all datasets. This can be attributed to two properties of Sparse-GEV: its flexibility in modeling complex distributions and its effectiveness in utilizing the samples. The assumptions of Lasso-Granger and copula methods about the distribution of the data can be responsible for their lower performance. Transfer entropy re-\n10 0\n10 1\n0.8\n0.85\n0.9\n0.95\nλ\nA U\nC S\nco re\n(a)\n5 10 15 20 −4\n−3\n−2\n−1\n0x 10 6\nEM Iteration\nL og\n− lik\nel ih\noo d\nV al\nue\n(b)\nquires many observations to perform well, which could be a potential issue in the real applications."
    }, {
      "heading" : "4.3. Parameter Sensitivity Assessment",
      "text" : "Like other latent state models, Sparse-GEV model has many parameters, which could affect its performance significantly. In our last experiment, we assess the parameter sensitivity. Fig. 4(a) shows that in a large range of values for the regularization parameter λ, the graph learning accuracy remains unchanged and little effort in selection of the regularization parameter leads to the optimal performance. Fig. 4(b) suggests that in less than 10 EM iterations, our algorithm converges to the optimal point. Fig. 4(c) illustrates the effect of τ on the performance of Sparse-GEV. Small values of τ result in smoother estimation of E[µ|X], while higher values lead to sensitive estimation (as a result E[µ|X] closely follows the observation time series). This observation suggests that we should monitor the sample mean of the latent variables and choose a value of τ that allows smooth latent variables to capture the trend of observations."
    }, {
      "heading" : "5. Conclusion",
      "text" : "In this paper, we propose sparse-GEV, a sparse latent space model, to uncover the sparse temporal de-\npendency from multivariate extreme value time series. To estimate the parameters of the model, we develop an iterative searching algorithm based on the generalized EM-algorithm and sampling with particle filtering. Through extensive experiments, we demonstrate that Sparse-GEV outperforms the state-of-the-art algorithms such as copula and transfer entropy. For future work, we are interested in the theoretical analysis on the consistency of the Sparse-GEV model."
    }, {
      "heading" : "Acknowledgement",
      "text" : "We thank the anonymous reviewers for their valuable comments. This research was supported by the NSF research grants IIS-1134990."
    } ],
    "references" : [ {
      "title" : "Temporal causal modeling with graphical Granger methods",
      "author" : [ "A. Arnold", "Y. Liu", "N. Abe" ],
      "venue" : "In Proceedings of SIGKDD,",
      "citeRegEx" : "Arnold et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Arnold et al\\.",
      "year" : 2007
    }, {
      "title" : "Granger causality and transfer entropy are equivalent for gaussian variables",
      "author" : [ "L. Barnett", "A.B. Barrett", "A.K. Seth" ],
      "venue" : "Phys. Rev. Lett.,",
      "citeRegEx" : "Barnett et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Barnett et al\\.",
      "year" : 2009
    }, {
      "title" : "Nonparametric Entropy Estimation: An Overview",
      "author" : [ "J. Beirlant", "E.J. Dudewicz", "L. Györfi", "E.C. Meulen" ],
      "venue" : "International Journal of the Mathematical Statistics Sciences,",
      "citeRegEx" : "Beirlant et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Beirlant et al\\.",
      "year" : 1997
    }, {
      "title" : "Statistics of extremes: Theory and applications",
      "author" : [ "J. Beirlant", "Y. Goegebeur", "J. Segers", "J. Teugels" ],
      "venue" : "New York: Wiley,",
      "citeRegEx" : "Beirlant et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Beirlant et al\\.",
      "year" : 2004
    }, {
      "title" : "Time Series Analysis, Forecasting and Control",
      "author" : [ "G. Box", "G. Jenkins", "G. Reinsel" ],
      "venue" : "Holden-Day, Incorporated,",
      "citeRegEx" : "Box et al\\.,? \\Q1990\\E",
      "shortCiteRegEx" : "Box et al\\.",
      "year" : 1990
    }, {
      "title" : "An Introduction to Statistical Modeling of Extreme Values",
      "author" : [ "S. Coles" ],
      "venue" : null,
      "citeRegEx" : "Coles.,? \\Q2001\\E",
      "shortCiteRegEx" : "Coles.",
      "year" : 2001
    }, {
      "title" : "Modeling extremes of areal rainfall process",
      "author" : [ "S. Coles", "J.A. Tawn" ],
      "venue" : "Journal of the Royal Statistical Society. Series B.,",
      "citeRegEx" : "Coles and Tawn.,? \\Q1996\\E",
      "shortCiteRegEx" : "Coles and Tawn.",
      "year" : 1996
    }, {
      "title" : "A tutorial on particle filtering and smoothing",
      "author" : [ "A. Doucet", "A.M. Johansen" ],
      "venue" : "Fifteen years later. The Oxford Handbook of Nonlinear Filtering,",
      "citeRegEx" : "Doucet and Johansen.,? \\Q2009\\E",
      "shortCiteRegEx" : "Doucet and Johansen.",
      "year" : 2009
    }, {
      "title" : "Granger causality and path diagrams for multivariate time series",
      "author" : [ "E. Michael" ],
      "venue" : "J. of Econometrics,",
      "citeRegEx" : "Michael.,? \\Q2007\\E",
      "shortCiteRegEx" : "Michael.",
      "year" : 2007
    }, {
      "title" : "Correlation and dependence in risk management: properties and pitfalls",
      "author" : [ "P. Embrechts", "A. Mcneil", "D. Straumann" ],
      "venue" : "In Risk Management: Value at Risk and Beyond,",
      "citeRegEx" : "Embrechts et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Embrechts et al\\.",
      "year" : 2002
    }, {
      "title" : "Inference for clusters of extreme values",
      "author" : [ "C.A. Ferro", "J. Segers" ],
      "venue" : "Journal of the Royal Statistical Society. Series B.,",
      "citeRegEx" : "Ferro and Segers.,? \\Q2003\\E",
      "shortCiteRegEx" : "Ferro and Segers.",
      "year" : 2003
    }, {
      "title" : "Sparse inverse covariance estimation with the graphical lasso",
      "author" : [ "J. Friedman", "T. Hastie", "R. Tibshirani" ],
      "venue" : null,
      "citeRegEx" : "Friedman et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Friedman et al\\.",
      "year" : 2008
    }, {
      "title" : "Testing for causality: A personal viewpoint",
      "author" : [ "C. Granger" ],
      "venue" : "Journal of Economic Dynamics and Control,",
      "citeRegEx" : "Granger.,? \\Q1980\\E",
      "shortCiteRegEx" : "Granger.",
      "year" : 1980
    }, {
      "title" : "Time Series Analysis",
      "author" : [ "J. Hamilton" ],
      "venue" : null,
      "citeRegEx" : "Hamilton.,? \\Q1994\\E",
      "shortCiteRegEx" : "Hamilton.",
      "year" : 1994
    }, {
      "title" : "Time-varying models for extreme values",
      "author" : [ "G. Huerta", "B. Sanso" ],
      "venue" : "Environmental and Ecological Statistics,",
      "citeRegEx" : "Huerta and Sanso.,? \\Q2007\\E",
      "shortCiteRegEx" : "Huerta and Sanso.",
      "year" : 2007
    }, {
      "title" : "Randomization tests for distinguishing social influence and homophily effects",
      "author" : [ "T. La Fond", "J. Neville" ],
      "venue" : "In Proceedings of WWW,",
      "citeRegEx" : "Fond and Neville.,? \\Q2010\\E",
      "shortCiteRegEx" : "Fond and Neville.",
      "year" : 2010
    }, {
      "title" : "Claims prediction with dependence using copula models",
      "author" : [ "Y.K. Leong", "E.A. Valdez" ],
      "venue" : "Insurance: Mathematics and Economics,",
      "citeRegEx" : "Leong and Valdez.,? \\Q2005\\E",
      "shortCiteRegEx" : "Leong and Valdez.",
      "year" : 2005
    }, {
      "title" : "The nonparanormal: Semiparametric estimation of high dimensional undirected graphs",
      "author" : [ "H. Liu", "J.D. Lafferty", "L.A. Wasserman" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Liu et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2009
    }, {
      "title" : "Grouped graphical Granger modeling for gene expression regulatory networks discovery",
      "author" : [ "A. Lozano", "N. Abe", "Y. Liu", "S. Rosset" ],
      "venue" : "In Proceedings of ISMB,",
      "citeRegEx" : "Lozano et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Lozano et al\\.",
      "year" : 2009
    }, {
      "title" : "Spatial-temporal causal modeling for climate change attribution",
      "author" : [ "A. Lozano", "H. Li", "A. Niculescu-Mizil", "Y. Liu", "C. Perlich", "J. Hosking", "N. Abe" ],
      "venue" : "In Proceedings of KDD,",
      "citeRegEx" : "Lozano et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Lozano et al\\.",
      "year" : 2009
    }, {
      "title" : "High-Dimensional Graphs and Variable Selection with the Lasso",
      "author" : [ "N. Meinshausen", "P. Bühlmann" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "Meinshausen and Bühlmann.,? \\Q2006\\E",
      "shortCiteRegEx" : "Meinshausen and Bühlmann.",
      "year" : 2006
    }, {
      "title" : "Robustly Estimating the Flow Direction of Information in Complex Physical Systems",
      "author" : [ "G. Nolte", "A. Ziehe", "V. Nikulin", "A. Schlögl", "N. Krämer", "T. Brismar", "K.-R. Müller" ],
      "venue" : "Physical Review Letters,",
      "citeRegEx" : "Nolte et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Nolte et al\\.",
      "year" : 2008
    }, {
      "title" : "Disk aware discord discovery: finding unusual time series in terabyte sized datasets",
      "author" : [ "D. Yankov", "E. Keogh", "U. Rebbapragada" ],
      "venue" : "Knowl. Inf. Syst.,",
      "citeRegEx" : "Yankov et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Yankov et al\\.",
      "year" : 2008
    }, {
      "title" : "Measuring Information Transfer",
      "author" : [ "T. Schreiber" ],
      "venue" : "Physical Review Letters,",
      "citeRegEx" : "Schreiber.,? \\Q2000\\E",
      "shortCiteRegEx" : "Schreiber.",
      "year" : 2000
    }, {
      "title" : "Algorithms for Large Scale Markov Blanket Discovery",
      "author" : [ "I. Tsamardinos", "C.F. Aliferis", "E. Statnikov" ],
      "venue" : "In Proceedings of FLAIRS,",
      "citeRegEx" : "Tsamardinos et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Tsamardinos et al\\.",
      "year" : 2003
    }, {
      "title" : "Coordinate descent algorithms for lasso penalized regression",
      "author" : [ "T-T. Wu", "K. Lange" ],
      "venue" : "The Annals of Applied Statistics,",
      "citeRegEx" : "Wu and Lange.,? \\Q2008\\E",
      "shortCiteRegEx" : "Wu and Lange.",
      "year" : 2008
    } ],
    "referenceMentions" : [ {
      "referenceID" : 13,
      "context" : "Time series analysis and modeling have been extensively studied in the literature and successfully found applications across domains (Box & Jenkins, 1990; Hamilton, 1994).",
      "startOffset" : 133,
      "endOffset" : 170
    }, {
      "referenceID" : 12,
      "context" : "Many algorithms are proposed to automatically recover the temporal structures, such as autocorrelation, crosscorrelations (Box & Jenkins, 1990), randomization test (Edgington & Onghena, 2007), Granger causality (Granger, 1980), transfer entropy (Beirlant et al.",
      "startOffset" : 211,
      "endOffset" : 226
    }, {
      "referenceID" : 2,
      "context" : "Many algorithms are proposed to automatically recover the temporal structures, such as autocorrelation, crosscorrelations (Box & Jenkins, 1990), randomization test (Edgington & Onghena, 2007), Granger causality (Granger, 1980), transfer entropy (Beirlant et al., 1997; Barnett et al., 2009), and so on.",
      "startOffset" : 245,
      "endOffset" : 290
    }, {
      "referenceID" : 1,
      "context" : "Many algorithms are proposed to automatically recover the temporal structures, such as autocorrelation, crosscorrelations (Box & Jenkins, 1990), randomization test (Edgington & Onghena, 2007), Granger causality (Granger, 1980), transfer entropy (Beirlant et al., 1997; Barnett et al., 2009), and so on.",
      "startOffset" : 245,
      "endOffset" : 290
    }, {
      "referenceID" : 5,
      "context" : "The statistical approach we can utilize to solve these important problems is the theory of extreme value modeling (Coles, 2001; Beirlant et al., 2004), which provides a natural family of probability distributions for modeling the magnitude of the largest (or smallest) of a large number of events, and a canonical stochastic process model (Coles, 2001) for the occurrence of events above a very high (or below a very low) threshold.",
      "startOffset" : 114,
      "endOffset" : 150
    }, {
      "referenceID" : 3,
      "context" : "The statistical approach we can utilize to solve these important problems is the theory of extreme value modeling (Coles, 2001; Beirlant et al., 2004), which provides a natural family of probability distributions for modeling the magnitude of the largest (or smallest) of a large number of events, and a canonical stochastic process model (Coles, 2001) for the occurrence of events above a very high (or below a very low) threshold.",
      "startOffset" : 114,
      "endOffset" : 150
    }, {
      "referenceID" : 5,
      "context" : ", 2004), which provides a natural family of probability distributions for modeling the magnitude of the largest (or smallest) of a large number of events, and a canonical stochastic process model (Coles, 2001) for the occurrence of events above a very high (or below a very low) threshold.",
      "startOffset" : 196,
      "endOffset" : 209
    }, {
      "referenceID" : 5,
      "context" : "Preliminaries Before diving into the details of our model, we first briefly review the extreme value theory (Coles, 2001).",
      "startOffset" : 108,
      "endOffset" : 121
    }, {
      "referenceID" : 5,
      "context" : ", xT }, we define the joint 1 In extreme value theory, two main sets of methods, the Block Maxima method and the Peaks over Thresholds method have been developed to model extreme values (Coles, 2001).",
      "startOffset" : 186,
      "endOffset" : 199
    }, {
      "referenceID" : 0,
      "context" : "Very recently, a few advanced approaches have been explored to uncover temporal dependence from time series data, including Lasso-Granger (Arnold et al., 2007), transfer entropy (Schreiber, 2000), and the copula approach (Liu et al.",
      "startOffset" : 138,
      "endOffset" : 159
    }, {
      "referenceID" : 23,
      "context" : ", 2007), transfer entropy (Schreiber, 2000), and the copula approach (Liu et al.",
      "startOffset" : 26,
      "endOffset" : 43
    }, {
      "referenceID" : 17,
      "context" : ", 2007), transfer entropy (Schreiber, 2000), and the copula approach (Liu et al., 2009).",
      "startOffset" : 69,
      "endOffset" : 87
    }, {
      "referenceID" : 0,
      "context" : "Granger causality In (Arnold et al., 2007), the Lasso-Ganger algorithm, an effective and efficient approach to learn sparse temporal graphs, is developed by combining Granger causality with sparse neighborhood selection using L1-penalized regression.",
      "startOffset" : 21,
      "endOffset" : 42
    }, {
      "referenceID" : 23,
      "context" : "In the Transfer entropy framework (Schreiber, 2000), time series x is thought to be a cause of another time series x if the values of x in the past significantly decrease the uncertainty in the future values of x given its past.",
      "startOffset" : 34,
      "endOffset" : 51
    }, {
      "referenceID" : 24,
      "context" : "Since the transfer entropy is a pairwise quantity, we can use its output as input to a graph learning algorithm, for example, IAMB (Tsamardinos et al., 2003), to uncover the temporal dependency among multiple time series.",
      "startOffset" : 131,
      "endOffset" : 157
    }, {
      "referenceID" : 9,
      "context" : "Copula Approach The copula approach has been proposed for dependency analysis of time series with non-Gaussian marginal distributions (Embrechts et al., 2002).",
      "startOffset" : 134,
      "endOffset" : 158
    }, {
      "referenceID" : 17,
      "context" : "It has been used for forecast in time series (Leong & Valdez, 2005) and learning sparase dependency structures (Liu et al., 2009).",
      "startOffset" : 111,
      "endOffset" : 129
    }, {
      "referenceID" : 11,
      "context" : "algorithm (Friedman et al., 2008).",
      "startOffset" : 10,
      "endOffset" : 33
    }, {
      "referenceID" : 1,
      "context" : "In fact, when the data are distributed according to Gaussian linear model, transfer entropy is equivalent to Granger causality (Barnett et al., 2009).",
      "startOffset" : 127,
      "endOffset" : 149
    }, {
      "referenceID" : 0,
      "context" : "(Meinshausen & Bühlmann, 2006) show that the Lasso variable selection loss, and subsequently the Lasso-Granger’s loss (Arnold et al., 2007), vanishes with an exponential rate.",
      "startOffset" : 118,
      "endOffset" : 139
    }, {
      "referenceID" : 17,
      "context" : "For the copula approach, (Liu et al., 2009) show that when copula-based model is the true model, the copula-based structure learning algorithm with non-parametric estimation of marginals converges to",
      "startOffset" : 25,
      "endOffset" : 43
    }, {
      "referenceID" : 2,
      "context" : "The performance of transfer entropy heavily relies on the accuracy of entropy estimations, which require a large number of observations, especially for high dimensional distributions, to achieve robust estimation (Beirlant et al., 1997).",
      "startOffset" : 213,
      "endOffset" : 236
    } ],
    "year" : 2012,
    "abstractText" : "In many applications of time series models, such as climate analysis and social media analysis, we are often interested in extreme events, such as heatwave, wind gust, and burst of topics. These time series data usually exhibit a heavy-tailed distribution rather than a Gaussian distribution. This poses great challenges to existing approaches due to the significantly different assumptions on the data distributions and the lack of sufficient past data on extreme events. In this paper, we propose the Sparse-GEV model, a latent state model based on the theory of extreme value modeling to automatically learn sparse temporal dependence and make predictions. Our model is theoretically significant because it is among the first models to learn sparse temporal dependencies among multivariate extreme value time series. We demonstrate the superior performance of our algorithm to the state-of-art methods, including Granger causality, copula approach, and transfer entropy, on one synthetic dataset, one climate dataset and two Twitter datasets.",
    "creator" : "LaTeX with hyperref package"
  }
}