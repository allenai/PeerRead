{
  "name" : "1709.01687.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Semi-Supervised Recurrent Neural Network for Adverse Drug Reaction Mention Extraction",
    "authors" : [ "Shashank Gupta", "Sachin Pawar", "Nitin Ramrakhiyani", "Girish Keshav Palshikar", "Vasudeva Varma" ],
    "emails" : [ "shashank.gupta@research.iiit.ac.in", "sachin7.p@tcs.com", "nitin.ramrakhiyani@tcs.com", "gk.palshikhar@tcs.com", "vv@iiit.ac.in" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n70 9.\n01 68\n7v 1\n[ cs\n.I R\n] 6\nS ep\nSocial media is an useful platform to share health-related information due to its vast reach. is makes it a good candidate for publichealth monitoring tasks, specifically for pharmacovigilance. We study the problem of extraction of Adverse-Drug-Reaction (ADR) mentions from social media, particularly from Twi er. Medical information extraction from social media is challenging, mainly due to short and highly informal nature of text, as compared to more technical and formal medical reports.\nCurrent methods in ADRmention extraction rely on supervised learning methods, which suffer from labeled data scarcity problem.\ne State-of-the-art method uses deep neural networks, specifically a class of Recurrent Neural Network (RNN) which are LongShort-Term-Memory networks (LSTMs) [6]. Deep neural networks, due to their large number of free parameters relies heavily on large annotated corpora for learning the end task. But in the real-world, it is hard to get large labeled data, mainly due to the heavy cost associated with the manual annotation. To this end, we propose a novel semi-supervised learning based RNN model, which can leverage unlabeled data also present in abundance on social media. rough experiments we demonstrate the effectiveness of our method, achieving state-of-the-art performance in ADR mention extraction.\nCCS CONCEPTS\n•Information systems→ Information extraction; • eory of computation→ Semi-supervisedlearning;Unsupervised learning and clustering;\nKEYWORDS\nPharmacovigilance, RNN, LSTM, semi-supervised learning\n∗Work done during internship at TRDDC, Pune.\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). WOODSTOCK’97, © 2016 Copyright held by the owner/author(s). 123-4567-24-567/08/06. . . $15.00 DOI: 10.475/123 4"
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "Adverse-Drug-Reactions (ADRs) are a leading cause of mortality and morbidity in health care. In a study, it was observed that from a death count in the range of (44,000-98,000) due to medical errors, 7000 deaths occurred due to ADRs.1. Postmarket drug surveillance is therefore required to identify such potential adverse reactions. e formal systems for postmarket surveillance can be slow and under-efficient. Studies show that 94% ADRs are underreported [5].\nSocial media presents a useful platform to conduct such postmarket surveillance, given the large audience and vast reach of such platforms. Such platforms have been used for real-time information retrieval and trends tracking, including digital disease surveillance system [12]. Recent study shows that twi er has 3 times more ADRs reported than were reported through FDA. Out of 61,000 tweets collected, 4400 had mention of ADRs as compared to 1400 ADRs reported through FDA during the same time-period [2]. is makes Twi er a great source for building a real-time postmarketing drug safety surveillance system. However, information extraction from social media comes with its own set of challenges. Some of them are: 1) Short nature of the text (twi er has a 142 character limit), making the language more ambiguous. 2) Sparsity of drug-related tweets 3) Highly colloquial language as compared to more technical and formal medical reports.\nConsider for example the tweets, ’Cymbalta, you’re driving me insane’; ’@<USER> Ugh, sorry. is effexor is not making me feel so awesome’. In the first tweet, ’driving me insane’ and in the second one, ’not making me feel so awesome’ are ADR mentions which indicate some level of discomfort in the user’s body. ese tweets clearly show how information extraction from social media suffers from above-mentioned problems.\nRecent work in deep learning has demonstrated its superiority over traditional hand-cra ed feature based machine learning models [8, 11]. However, due to a large number of free parameters, deep learning models rely heavily on large annotated dataset. In the real-world, it is o en the case that labeled data is sparse, making it challenging to train such models. Semi-supervised learning\n1h p://bit.ly/2vaWF6e\nbased methods provide a viable solution to this. ese methods rely on a small labeled data and a large unlabeled data for training.\nIn this work, we present a novel semi-supervised Recurrent Neural Network (RNN) [4] based method for ADR mention extraction, specifically leveraging a relatively large unlabeled data. We demonstrate the effectiveness of our method through experimentation on ADRmention annotated tweet corpus [1]. Ourmethod achieves superior results than the current state-of-the-art in ADR extraction from twi er. Our main contributions are :\n• Wepropose a novel semi-supervised sequence labelingmethod\nbased onRNN, specifically Long-Short-Term-Memory Network [6] which are known to capture long-term dependencies be er than vanilla RNN.\n• For the unsupervised learning part, we explore a novel\nproblem of drug name prediction given context from tweets.\ne goal is to predict the drug-name which is masked,\ngiven it’s context in the tweet.\n• For supervised learning, we explore different word embed-\nding initializing schemes and present results for the same. • We demonstrate that by training a semi-supervised model,\nADR extraction performance can be improved substantially as compared to current methods.\n• On the twi er dataset with ADR mentions annotated [1],\nour method achieves an F-score of 0.751 surpassing the current state-of-the-art method by 3.01%."
    }, {
      "heading" : "2 RELATED WORK",
      "text" : "e problem of ADRmention extraction falls under the category of sequence labeling problem. State-of-the-art method for sequence labeling problem is Conditional Random Fields (CRFs) [10]. ADRMine [16], is a CRF-based model for ADR extraction task. It uses a variety of hand-cra ed features, including word context, ADR lexicon, POS-tag and word embedding based features as input to CRF.\ne word embedding based features are trained on a large domainspecific tweet corpus. e problem with the above-mentioned approach is its dependency on hand-cra ed features, which is time and effort consuming. A Long-Short-Term-Memory (LSTM) network based model is proposed [1] to get around this problem. Instead of using human-engineered features, word embedding based features are passed to a Bi-directional LSTMmodelwhich is trained to generate a sequence of labels, given the input word sequence. State-of-the-art results are achieved, surpassing CRF-based ADRMine results.\nSome recent work also focuses on the problem of Adverse-DrugEvent (ADE) detection [7, 13]. e goal is to identify whether there is an Adverse-Drug-Event mentioned in the tweet based on its textual content."
    }, {
      "heading" : "3 ADR-MENTION EXTRACTION USING SEMI-SUPERVISED BI-DIRECTIONAL LSTM",
      "text" : "In this section, we present our approach for ADR extraction. Our method is based on a semi-supervised learningmethodwhich operates in two phases: 1) Unsupervised learning: In this phase, we train a Bidirectional LSTM model to predict the drug name given its context in the tweet. As training data for this task, we select tweets with exactly one mention of any prescription drug. Since\nwe already know the drug name beforehand, it doesn’t need any annotation effort. 2) Supervised learning: In this phase, we use the same bidirectional LSTM model from phase 1 and (re)train it to predict the sequence of labels, given the tweet text."
    }, {
      "heading" : "3.1 Unsupervised learning",
      "text" : "For this phase, we choose a novel task of drug name prediction from its context in the tweet. For training data, we use a large collection of tweets with exactly one mention of the drug name in them. Since we are predicting the drug name from a tweet which is already present in it, in order to avoid the network to learn a trivial function which maps drug-name in input to drug-name in output without considering the context in account, we mask the drugname in the tweet with a dummy token. For feature-extraction, we use a Bidirectional LSTM model. e model takes as input, a sequence of continuous word vectors as input and predicts a corresponding sequence of word vectors as output. e equations governing the dynamics of LSTMs are defined as follows:\n®дu = σ (W u ∗ ®ht−1 + I u ∗ ®xt )\n®дf = σ (W f ∗ ®ht−1 + I f ∗ ®xt )\n®дc = tanh(W c ∗ ®ht−1 + I c ∗ ®xt )\n®mt = ®д f ⊙ +®дu ⊙ ®дc\n®дo = σ (W o ∗ ®ht−1 + I o ∗ ®xt )\n®ht = tanh(®д o ⊙ ®mt−1)\n(1)\nhere σ is the logistic sigmoid function,Wu ,Wf ,Wo ,Wc are recurrent weight matrices and Iu , If , Io , Ic are projection matrices. In a conventional LSTM, the sequence order is from le to right. In Bidirectional LSTM, two sequence directions are considered, one from le to right and the other one opposite to it. e final hidden layers activation is the concatenation of vectors from both directions. Mathematically,\nht = [®ht ; ←− h t ] (2)\nTo generate the final representation of the tweet, average-pooling is applied over all hidden state vectors.\nh =\nT∑\nt=1\nht (3)\nwhere T is the maximum time-step. Finally a so max transformation is applied to generate a probability distribution over all drugnames followed by categorical cross-entropy loss."
    }, {
      "heading" : "3.2 Supervised Sequence Classification",
      "text" : "For this phase, we reuse the Bidirectional LSTM trained from the previous phase following the setup similar to state-of-the-art [1]. At each time-step of the sequence, a so max layer is applied which predicts a probability distribution over sequence labels. Formally,\nyt = so f tmax(Wh + b) (4)\nhere W and b are weight matrices for the so max layer. e final loss for the sequence labeling is sum of categorical cross-entropy\nloss at each time-step. e hidden state h and the parameters Wu , Wf , Wo ,Wc , Iu , If , Io , Ic are shared during training both phases.\ne intuition around the unsupervised task is that the network can learn the textual context where drug names appear, which can help in identifying Adverse Drug Reactions from drugs."
    }, {
      "heading" : "4 EXPERIMENTS",
      "text" : ""
    }, {
      "heading" : "4.1 Dataset",
      "text" : "We use the twi er dataset annotated with ADR mention collected during the period of 2007-2010. Tweets were collected using 81 drug names as keyword search terms. In the original dataset, a total of 960 tweets are annotated with ADR mentions. Due to Twitter’s search APIs license, only tweet ids were released. Out of the total of 960, we collected a total of 645 tweets using Python library tweepy2 . According to the given train-test split, 470 tweets are used for training and 170 tweets for testing.\nFor the unlabeled dataset, we used the Twi er’s search API 3 with the drug names used in the original study as keyword search terms 4. We crawled the tweets over a period of two months. For the sake of simplicity, we removed the tweets with more than one drug mentions, , resulting in a total of 0.1 Million tweets."
    }, {
      "heading" : "4.2 Implementation Details",
      "text" : "We use Keras5 for implementation. For text pre-processing, we applied several pre-processing steps, which are :\n• Normalizing HTML links and user-mentions:We re-\nplaced all HTML link mentions with the token ”<LINK>”. Similarly, we replaced all user handle mentions (for ex. @JonDoe) with the token ”<USER>”. • Special Character Removal: We removed all punctua-\ntions and special symbols like ’#’ from tweets.\n• Emoticons Removal: We removed all emoticons, in gen-\neral all non-ascii characters which are special types of emoticons.\n• Stop-word and rare words removal: We removed all\nstop-words and set the vocabulary size to top-15000 most frequent words in the corpus.\nWe used the word2vec [15] embeddings trained on a large generic twi er corpus [3] as input to the model. Word vector dimension is set to 400. BiLSTM parameters are set to the best reported se ing from [1], with hidden unit’s dimension equal to 500. For training\n2h ps://github.com/tweepy/tweepy 3h ps://dev.twi er.com/rest/public/search 4h p://diego.asu.edu/Publications/ADRMine.html Some example drug names used as keywords are: humira, dronedarone, lamictal, pradaxa, paxil, zoledronic acid, trazodone, enbrel, cymbalta, quetiapine 5h ps://keras.io/\nthe supervised model, we use the adam optimizer [9] with batchsize equal to 1 and for training the unsupervised model, we used the batch adam optimizer [9] with batch-size set to 128. e supervised model was trained for a total of 5 epochs, and unsupervised model trained for 30 epochs."
    }, {
      "heading" : "4.3 Results",
      "text" : "To convert ADR extraction problem into sequence labeling problem, we need to assign annotated entities with appropriate tag representations. We follow IO encoding scheme where each word belongs to either of the following categories: (1) I-ADR (inside ADR) (2) I-Indication (inside Indication ) (3) O (Outside any mention) (4) <PAD> (if the word is padding token). It should be noted that, similar to the baseline method [1] we report the performance on the ADR label only. is is because the number of Indication annotations are very less in number6. An example tweet annotatedwith IO-encoding:@BLENDOSO LamictalO andO trileptalO andO seroquelO ofO courseO theO seroquelO IO takeO inO severeO situationsO becauseO weightI-ADR gainI-ADR isO notO coolO For performance evaluation we use approximate-matching [18], which is used popularly in biomedical entity extraction tasks [1, 16]. We report the F1-score, Precision and Recall computed using approximate matching as follows:\nPrecision = #ADR approximately matched\n#ADR spans predicted (5)\nRecall = #ADR approximately matched\n#ADR spans in total (6)\nTable 1 presents the results of our approach along with comparisons. Since the number of tweets used for training and testing differs from the one used in baseline [1], we re-ran their model using the source-code released by them7. It should be noted that the original model used RMSProp [17] as an optimizer, so for a fair comparison we also report the baseline results with optimizer as adam instead of RMSProp. Replacing RMSProp with adam, although gives an improvement over the original baseline, still under-performs our method . Our approach gives the state-of-the-art results, giving an improvement of 2.97%F1 over the original baseline and an improvement of 1.88% F1 over the re-implemented baseline."
    }, {
      "heading" : "4.4 Analysis",
      "text" : "4.4.1 Effect of drug-mask. For the unsupervised learning phase, we select the task of drug-name prediction given its context. In order to avoid the network learning a degenerate function which maps input drug-name to output drug-name, we mask all drugnames in input with a single token. In order to verify this, we\n645 in training, 16 in testing 7h ps://github.com/chop-dbhi/twi er-adr-blstm\nreport the accuracy results without the drug-mask, i.e. with drugname included in the input. e result is presented in Table 2. It is clear that removing the drug mask from input degrades the endperformance by 0.535% in F-score. is further validates our claim that masking the drug-names is effective.\n4.4.2 Effect of embeddings and dictionary. We experiment with word embeddings trained on different corpus to observe its effect on the end-performance. We experiment with embeddings trained on part of Google News Dataset, which consists of around 100 billion words9. It can be observed that using Google News Corpus trained embeddings degrade the performance by 2.038% in F-score.\nis is due to the fact that these embeddings are trained on a large News Corpus, which is more grammatically sound and formal than the raw social media posts. Conceptually, the shi in the lexical data distribution of the News corpus as compared to tweets containing ADR causes the degradation in performance. We also experiment withword embeddings trained on a largemedical-concept terms related tweet corpus10 [14]. Intuitively, embeddings trained on similar domain (medical in this case) should perform be er, but surprisingly it performs worst amongst all methods. e generic embeddings trained on large tweet corpus captures potentially large variation of semantics and linguistic properties of text and due to the free-style nature of writing on social media, this helps more than domain-knowledge, as captured by medical-domain trained embeddings.\nWe also experimented with a different vocabulary initialization. In our proposed formulation, we construct vocabulary from both unlabeled and labeled corpus, resulting in a larger vocabulary size. When experimented with a restricted vocabulary (only from labeled training data), we observe that the F1-score drops by 0.8%.\nis suggests the use of a larger vocabulary with more coverage in\nsimilar se ings."
    }, {
      "heading" : "5 CONCLUSIONS",
      "text" : "We present a novel semi-supervised Bi-directional LSTM based model for ADR mention extraction. We evaluate our method on an annotated twi er corpus. By leveraging potentially large unlabeled corpus, our method outperforms the state-of-the-art method by 3.01% in F1-score.\nWe also demonstrate that word embeddings trained on a large domain-agnostic twi er corpus performs be er than more popular Google News Corpus trained word-embeddings and surprisingly even be er than medical domain-specific word embeddings\n9h ps://code.google.com/archive/p/word2vec/ 10h ps://zenodo.org/record/27354#.WWYph1ekW4A\ntrained on tweets, which suggests that language structure and semantics is more important in downstream information extraction tasks, compared to domain knowledge.\nIn future, we will explore drug and side-effect (adverse-effect) mention relation along with ADR extraction and explore if both can be formulated in a multi-task learning setup."
    } ],
    "references" : [ {
      "title" : "Deep learning for pharmacovigilance: recurrent neural network architectures for labeling adverse drug reactions in Twier posts",
      "author" : [ "Anne Cocos", "Alexander G Fiks", "Aaron J Masino" ],
      "venue" : "Journal of the American Medical Informatics Association (2017),",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2017
    }, {
      "title" : "Digital Drug Safety Surveillance: Monitoring Pharmaceutical Products in Twier",
      "author" : [ "Clark C. Freifeld", "John S. Brownstein", "Christopher M. Menone", "Wenjie Bao", "Ross Filice", "Taha Kass-Hout", "Nabarun Dasgupta" ],
      "venue" : "Drug Safety 37,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2014
    }, {
      "title" : "Multimedia lab@ acl w-nut ner shared task: named entity recognition for twier microposts using distributed word representations",
      "author" : [ "Fréderic Godin", "Baptist Vandersmissen", "Wesley De Neve", "Rik Van de Walle" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2015
    }, {
      "title" : "Sequence transduction with recurrent neural networks",
      "author" : [ "Alex Graves" ],
      "venue" : "arXiv preprint arXiv:1211.3711",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2012
    }, {
      "title" : "Under-reporting of Adverse Drug Reactions: A Systematic Review",
      "author" : [ "Lorna Hazell", "Saad Aw Shakir" ],
      "venue" : "Pharmacoepidemiology and Drug Safety",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2005
    }, {
      "title" : "Long short-termmemory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber" ],
      "venue" : "Neural computation 9,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1997
    }, {
      "title" : "Adverse drug reaction classification with deep neural networks",
      "author" : [ "Trung Huynh", "Yulan He", "Allistair Willis", "Stefan Rüger" ],
      "venue" : null,
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2016
    }, {
      "title" : "Convolutional neural networks for sentence classification",
      "author" : [ "Yoon Kim" ],
      "venue" : "arXiv preprint arXiv:1408.5882",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2014
    }, {
      "title" : "Adam: Amethod for stochastic optimization",
      "author" : [ "Diederik Kingma", "Jimmy Ba" ],
      "venue" : "arXiv preprint arXiv:1412.6980",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2014
    }, {
      "title" : "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
      "author" : [ "John Lafferty", "Andrew McCallum", "Fernando Pereira" ],
      "venue" : null,
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2001
    }, {
      "title" : "Sequential Short-Text Classification with Recurrent and Convolutional Neural Networks",
      "author" : [ "Ji Young Lee", "FranckDernoncourt" ],
      "venue" : "In Proceedings of NAACL- HLT",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2016
    }, {
      "title" : "Mining social media streams to improve public health allergy surveillance",
      "author" : [ "Kathy Lee", "Ankit Agrawal", "Alok Choudhary" ],
      "venue" : "ASONAM",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2015
    }, {
      "title" : "Adverse Drug Event Detection in Tweets with Semi-Supervised Convolutional Neural Networks",
      "author" : [ "Kathy Lee", "Ashequl Qadir", "Sadid A Hasan", "Vivek Datla", "Aaditya Prakash", "Joey Liu", "Oladimeji Farri" ],
      "venue" : null,
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2017
    }, {
      "title" : "Adapting phrase-based machine translation to normalise medical terms in social media",
      "author" : [ "Nut Limsopatham", "Nigel Collier" ],
      "venue" : null,
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2015
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean" ],
      "venue" : null,
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2013
    }, {
      "title" : "Pharmacovigilance from social media: mining adverse drug reaction mentions using sequence labeling with word embedding cluster features",
      "author" : [ "Azadeh Nikfarjam", "Abeed Sarker", "Karen OfiConnor", "Rachel Ginn", "Graciela Gonzalez" ],
      "venue" : "Journal of the American Medical Informatics Association 22,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2015
    }, {
      "title" : "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning",
      "author" : [ "Tijmen Tieleman", "Geoffrey Hinton" ],
      "venue" : null,
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2012
    }, {
      "title" : "Various criteria in the evaluation of biomedical named entity recognition",
      "author" : [ "Richard Tzong-Han Tsai", "Shih-Hung Wu", "Wen-Chi Chou", "Yu-Chun Lin", "Ding He", "Jieh Hsiang", "Ting-Yi Sung", "Wen-Lian Hsu" ],
      "venue" : "BMC bioinformatics 7,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2006
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : "Short-Term-Memory networks (LSTMs) [6].",
      "startOffset" : 35,
      "endOffset" : 38
    }, {
      "referenceID" : 4,
      "context" : "Studies show that 94% ADRs are underreported [5].",
      "startOffset" : 45,
      "endOffset" : 48
    }, {
      "referenceID" : 11,
      "context" : "surveillance system [12].",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 1,
      "context" : "Out of 61,000 tweets collected, 4400 had mention of ADRs as compared to 1400 ADRs reported through FDA during the same time-period [2].",
      "startOffset" : 131,
      "endOffset" : 134
    }, {
      "referenceID" : 7,
      "context" : "Recent work in deep learning has demonstrated its superiority over traditional hand-craed feature based machine learning models [8, 11].",
      "startOffset" : 129,
      "endOffset" : 136
    }, {
      "referenceID" : 10,
      "context" : "Recent work in deep learning has demonstrated its superiority over traditional hand-craed feature based machine learning models [8, 11].",
      "startOffset" : 129,
      "endOffset" : 136
    }, {
      "referenceID" : 3,
      "context" : "In this work, we present a novel semi-supervised Recurrent Neural Network (RNN) [4] based method for ADR mention extraction, specifically leveraging a relatively large unlabeled data.",
      "startOffset" : 80,
      "endOffset" : 83
    }, {
      "referenceID" : 0,
      "context" : "We demonstrate the effectiveness of our method through experimentation on ADRmention annotated tweet corpus [1].",
      "startOffset" : 108,
      "endOffset" : 111
    }, {
      "referenceID" : 5,
      "context" : "• Wepropose a novel semi-supervised sequence labelingmethod based onRNN, specifically Long-Short-Term-Memory Network [6] which are known to capture long-term dependencies beer than vanilla RNN.",
      "startOffset" : 117,
      "endOffset" : 120
    }, {
      "referenceID" : 0,
      "context" : "• On the twier dataset with ADR mentions annotated [1], our method achieves an F-score of 0.",
      "startOffset" : 52,
      "endOffset" : 55
    }, {
      "referenceID" : 9,
      "context" : "State-of-the-art method for sequence labeling problem is Conditional Random Fields (CRFs) [10].",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 15,
      "context" : "ADRMine [16], is a CRF-based model for ADR extraction task.",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 0,
      "context" : "A Long-Short-Term-Memory (LSTM) network based model is proposed [1] to get around this problem.",
      "startOffset" : 64,
      "endOffset" : 67
    }, {
      "referenceID" : 6,
      "context" : "Some recent work also focuses on the problem of Adverse-DrugEvent (ADE) detection [7, 13].",
      "startOffset" : 82,
      "endOffset" : 89
    }, {
      "referenceID" : 12,
      "context" : "Some recent work also focuses on the problem of Adverse-DrugEvent (ADE) detection [7, 13].",
      "startOffset" : 82,
      "endOffset" : 89
    }, {
      "referenceID" : 0,
      "context" : "For this phase, we reuse the Bidirectional LSTM trained from the previous phase following the setup similar to state-of-the-art [1].",
      "startOffset" : 128,
      "endOffset" : 131
    }, {
      "referenceID" : 0,
      "context" : "Baseline [1] 0.",
      "startOffset" : 9,
      "endOffset" : 12
    }, {
      "referenceID" : 14,
      "context" : "We used the word2vec [15] embeddings trained on a large generic twier corpus [3] as input to the model.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 2,
      "context" : "We used the word2vec [15] embeddings trained on a large generic twier corpus [3] as input to the model.",
      "startOffset" : 78,
      "endOffset" : 81
    }, {
      "referenceID" : 0,
      "context" : "BiLSTM parameters are set to the best reported seing from [1], with hidden unit’s dimension equal to 500.",
      "startOffset" : 59,
      "endOffset" : 62
    }, {
      "referenceID" : 8,
      "context" : "io/ the supervised model, we use the adam optimizer [9] with batchsize equal to 1 and for training the unsupervised model, we used the batch adam optimizer [9] with batch-size set to 128.",
      "startOffset" : 52,
      "endOffset" : 55
    }, {
      "referenceID" : 8,
      "context" : "io/ the supervised model, we use the adam optimizer [9] with batchsize equal to 1 and for training the unsupervised model, we used the batch adam optimizer [9] with batch-size set to 128.",
      "startOffset" : 156,
      "endOffset" : 159
    }, {
      "referenceID" : 0,
      "context" : "similar to the baseline method [1] we report the performance on the ADR label only.",
      "startOffset" : 31,
      "endOffset" : 34
    }, {
      "referenceID" : 17,
      "context" : "An example tweet annotatedwith IO-encoding:@BLENDOSO LamictalO andO trileptalO andO seroquelO ofO courseO theO seroquelO IO takeO inO severeO situationsO becauseO weightI-ADR gainI-ADR isO notO coolO For performance evaluation we use approximate-matching [18], which is used popularly in biomedical entity extraction tasks [1, 16].",
      "startOffset" : 255,
      "endOffset" : 259
    }, {
      "referenceID" : 0,
      "context" : "An example tweet annotatedwith IO-encoding:@BLENDOSO LamictalO andO trileptalO andO seroquelO ofO courseO theO seroquelO IO takeO inO severeO situationsO becauseO weightI-ADR gainI-ADR isO notO coolO For performance evaluation we use approximate-matching [18], which is used popularly in biomedical entity extraction tasks [1, 16].",
      "startOffset" : 323,
      "endOffset" : 330
    }, {
      "referenceID" : 15,
      "context" : "An example tweet annotatedwith IO-encoding:@BLENDOSO LamictalO andO trileptalO andO seroquelO ofO courseO theO seroquelO IO takeO inO severeO situationsO becauseO weightI-ADR gainI-ADR isO notO coolO For performance evaluation we use approximate-matching [18], which is used popularly in biomedical entity extraction tasks [1, 16].",
      "startOffset" : 323,
      "endOffset" : 330
    }, {
      "referenceID" : 0,
      "context" : "fers from the one used in baseline [1], we re-ran their model using the source-code released by them7.",
      "startOffset" : 35,
      "endOffset" : 38
    }, {
      "referenceID" : 16,
      "context" : "It should be noted that the original model used RMSProp [17] as an optimizer, so for a fair comparison we also report the baseline results with optimizer as adam instead of RMSProp.",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 13,
      "context" : "We also experiment withword embeddings trained on a largemedical-concept terms related tweet corpus10 [14].",
      "startOffset" : 102,
      "endOffset" : 106
    } ],
    "year" : 2017,
    "abstractText" : "Social media is an useful platform to share health-related information due to its vast reach. is makes it a good candidate for publichealth monitoring tasks, specifically for pharmacovigilance. We study the problem of extraction of Adverse-Drug-Reaction (ADR) mentions from social media, particularly from Twier. Medical information extraction from social media is challenging, mainly due to short and highly informal nature of text, as compared to more technical and formal medical reports. Current methods in ADRmention extraction rely on supervised learning methods, which suffer from labeled data scarcity problem. e State-of-the-art method uses deep neural networks, specifically a class of Recurrent Neural Network (RNN) which are LongShort-Term-Memory networks (LSTMs) [6]. Deep neural networks, due to their large number of free parameters relies heavily on large annotated corpora for learning the end task. But in the real-world, it is hard to get large labeled data, mainly due to the heavy cost associated with the manual annotation. To this end, we propose a novel semi-supervised learning based RNN model, which can leverage unlabeled data also present in abundance on social media. rough experiments we demonstrate the effectiveness of our method, achieving state-of-the-art performance in ADR mention",
    "creator" : "LaTeX with hyperref package"
  }
}