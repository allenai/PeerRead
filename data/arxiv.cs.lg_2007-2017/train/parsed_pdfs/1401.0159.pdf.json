{
  "name" : "1401.0159.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Speeding-Up Convergence via Sequential Subspace Optimization: Current State and Future Directions",
    "authors" : [ "Michael Zibulevsky" ],
    "emails" : [ "mzib@cs.technion.ac.il" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n40 1.\n01 59\nv1 [\ncs .N\nThis is an overview paper written in style of research proposal. In recent years we introduced a general framework for large-scale unconstrained optimization – Sequential Subspace Optimization (SESOP) and demonstrated its usefulness for sparsity-based signal/image denoising, deconvolution, compressive sensing, computed tomography, diffraction imaging, support vector machines. We explored its combination with Parallel Coordinate Descent and Separable Surrogate Function methods, obtaining state of the art results in above-mentioned areas.\nThere are several methods, that are faster than plain SESOP under specific conditions: Trust region Newton method - for problems with easily invertible Hessian matrix; Truncated Newton method - when fast multiplication by Hessian is available; Stochastic optimization methods - for problems with large stochastic-type data; Multigrid methods - for problems with nested multilevel structure. Each of these methods can be further improved by merge with SESOP. One can also accelerate Augmented Lagrangian method for constrained optimization problems and Alternating Direction Method of Multipliers for problems with separable objective function and non-separable constraints."
    }, {
      "heading" : "1 Background",
      "text" : "Many problems in science and engineering are handled as optimization tasks, often with very high dimensions. Solving them calls for the use of iterative methods of various sorts, and then convergence speed becomes crucial. Interior-point polynomial complexity methods provide fast and robust solution of convex problems, where Newton optimization is applicable [1, 2].\nWhen the problem size exceeds several thousand of variables, storage and inversion of Hessian matrix required for the Newton method become prohibitively expensive. Therefore increased attention to the methods which use gradient information only. Several of them possess optimal worst-case complexity [3]: ORTH method by Nemirovsky [4], Nesterov method [5, 6] and FISTA method by Beck and Teboulle [7].\nStill solving large ill-conditioned problems with high accuracy remains a challenge. Worst-case bounds are too pessimistic in many real-life cases. Recently we have developed a technique called SESOP, which constitutes a significant step towards this goal."
    }, {
      "heading" : "1.1 SESOP – Sequential Subspace Optimization",
      "text" : "The story of SESOP method [8] begins with the method of Conjugate Gradients (CG) [9]. Quadratic CG (i.e. CG applied to a quadratic function) has remarkable convergence properties: Its linear convergence rate (see for example [10]) is √ r−1√ r+1 , where r is the condition number of the Hessian of the objective. This rate is much better than the steepest descent rate, r−1 r+1 .\nOne can also rely on a 1/k2 sub-linear worst-case convergence of the quadratic CG, which does not depend on the Hessian conditioning (see for example [4, 3, 8]):\nf(xk+1)− foptimal ≤ L‖x0 − xoptimal‖\n2\nk2 (1)\nwhere k is the iteration index and L is the Lipschitz constant of the gradient of f . The presented convergence rates are intimately related to the well known expanding manifold property of quadratic CG: At every iteration the method minimizes the objective function over an affine subspace spanned by directions of all previous propagation steps and gradients.\nIn the case of a smooth convex function (not necessarily quadratic), one could propose a similar algorithm that preserves the expanding manifold property. Such an algorithm should minimize the objective function over\nan affine subspace spanned by directions of all previous propagation steps and the latest gradient. This method inherits the 1/k2 convergence of CG, however, the cost of an iteration of such a method will increase with iteration count.\nIn order to alleviate this problem, Nemirovski [4] suggested to restrict the optimization subspace just to three directions: the current gradient, the sum of all previous steps and a “cleverly” weighted sum of all previous gradients (a version of such weights is given in [8]). The resulting ORTH-method inherits the optimal worst case 1/k2 convergence (1), but it does not coincide with CG, when the objective is quadratic, and typically converges slower than CG, when the function become ”almost” quadratic in the neighborhood of solution.\nThe SESOP method [8] extends the ORTH subspaces with several directions of the last propagation steps. This way, the method, while preserving a 1/k2 convergence for smooth convex functions, becomes equivalent to the CG in the quadratic case. This property boosts the efficiency of the algorithm.\nQuite often the function we minimize has a form f(x) = φ(Ax), where multiplication by matrix A is costly, and computation of φ(·) is relatively cheap. The low-dimensional subspace optimization task at every iteration of SESOP can be addressed using the Newton algorithm. The main computational burden in this process is the need to multiply the spanning directions by A, but these multiplications can be stored in previous iterations, thus enabling the SESOP speed-up with minor additional cost.\nIn order to improve efficiency further we can substitute gradient direction with direction of parallel coordinate descent (PCD) or direction, provided by minimizer of a separable surrogate function (SSF) [11] . This approach provides state of the art results in the area of sparse signal approximation, where the objective function is\nf(x) = ‖Ax− b‖22 + µ‖x‖1 (2)\nParallel Coordinate Descent (PCD) Quite often coordinate descent is faster than gradient descent method in terms of total computational burden, because re-evaluating function value may be very cheap while changing one coordinate. In particular, for function (2) coordinate optimization can be calculated analytically and involves just one column of matrix A. Still for large dense matrices of size n× n we need 2n2 operations for one pass over all coordinates, i.e. the cost of two matrix-vector multiplications.\nOn the other hand in many problems fast matrix-vector multiplication is available, e.g. Fast Fourier Transform or Fast Wavelet Transform. For such problems we use Parallel Coordinate Descent [12]: Staying at current point we evaluate coordinate descent steps for all coordinates without moving along them (this can be performed analytically at cost of two fast matrix-vector multiplications). Then we obtain the next iterate of x via approximate minimization of the objective function (line search) along the obtained vector of coordinate steps.\nMethod of separable surrogate functions (SSF) is another efficient alternative to gradient descent proposed by [13, 14]. At every iteration the first term of (2) is substituted with diagonal quadratic function which has the same value as this term at current iterate and majorates it elsewhere. Combining this diagonal quadratic term with µ‖x‖1, we obtain a majorating separable surrogate function (SSF) for our problem. At every iteration we minimize SSF analytically, which always provides reduction of f(x). Similarly to PCD, SSF step costs about two matrix-vector multiplications, and the speed of convergence of two methods is also comparable. Still when the problem is ill-conditioned, the convergence may be slow.\nPCD-SESOP and SSF-SESOP method We can further accelerate the convergence if at every iteration we minimize the objective function over affine subspace spanned by current PCD or SSF direction and several previous propagation steps. PCD-SESOP and SSF-SESOP methods are asymptotically equivalent to the diagonally preconditioned CG. On the other hand, the PCD and SSF directions provide much faster progress at initial steps, when compared to the ordinary nonlinear CG. This partially explains extremal efficiency of these methods on difficult problems, where they are often significantly faster [15] than popular Nesterov method and FISTA [16, 7].\nIn Figure 1 we present results of an experiment [15] adopted from [17]. The problem uses an explicit matrix A of size 1848×8192, termed K4, which is built from a Gaussian random matrix, by shaping its singular values to fit a classical and highly ill-conditioned geo-science problem Beyond the natural goal of comparing various algorithms, in this experiment we aim to address two additional issues: (i) Run-time and its relation to number of iterations; and (ii) The behavior of the algorithms was explored for very small value of µ = 1e − 6, for which PCD/SSF methods are known to deteriorate. Figure 1 presents the objective function and the SNR of recovery of x, both as a function of the iteration count, and as a function of time. As we\ncan see, FISTA converges much faster then SSF, which corresponds to the observations in [17]. On the other hand, PCD-SESOP and SSF-SESOP are far superior to all other methods."
    }, {
      "heading" : "1.2 Newton-type optimization methods",
      "text" : "As a part of our proposal we are going to merge SESOP with several wellknown Newton methods in order to boost their performance. In this section we just set up the notions. There are two basic approaches in continuous multidimensional unconstrained optimization: trust region and line search. Each of them has its own strength and weakness. Suppose that we are looking for a minimum of a function of several variables\nmin f(x) (3)\nIf we have an easily minimizable approximate model q0(x) of our function around the initial point x0, we could find an approximate minimizer of (3) as x1 = argmin q0(x) In Newton method, for example, q0(·) is a second-order Taylor expansion of f(·) around x0. We can continue with this process iteratively in order to improve accuracy of solution xk+1 = argmin qk(x) The main problem with this approach is that our model qk(x) may become inaccurate far from its base point xk , therefore function value in the next point xk+1may even increase. In order to alleviate this problem, one can use line search or trust region strategy.\nLine search method At iteration k we first compute a minimizer of the model\nyk = argmin qk(x) (4)\nand then perform one-dimensional optimization in its direction\ndk = yk − xk (5)\nTrust region method A potential disadvantage of line search based optimization is that for ill-behaved functions, even though the model q fits the function quite well locally, the global minimum of the model may be far away from the minimum of the original function, and direction d may be ”poor”. Therefore an alternative trust region approach restricts search of the optimum of the model by a limited area around the current iterate xk\nxk+1 = argmin qk(x) (6)\nsubject to: ||x− xk|| ≤ rk\nParameter rk is adjusted dynamically from iteration to iteration based on progress in actual function value, see e.g. [10]. The problem with trust region approach is two fold. First, finding constrained minimum of the model is often 2 – 3 times more expensive than finding the unconstrained minimum. Secondly, the model can fit the original function well in some directions and poorly in others, therefore an ideal trust region should be a kind of ellipsoid instead of Euclidean ball. However adjusting parameters of such ellipsoid is rather difficult. In our proposal we will alleviate this difficulty using SESOP.\nTruncated Newton (TN) method [18, 19] is used when computing and inverting Hessian matrix is prohibitively expensive. Therefore at every outer iteration we minimize quadratic Taylor expansion qk(x) around the current iterate xk only approximately, using limited number of CG steps. The outer iteration of TN is accomplished with a line search, in order to guarantee function decrease. The overall effectiveness of the TN method is rather sensitive to the choice of stopping rule for the internal CG optimization. We attempt to overcome this difficulty, replacing the line search with subspace optimization. In this way we allow the CG iterations to stay matched through consequent TN steps."
    }, {
      "heading" : "1.3 Mini-batch stochastic optimization",
      "text" : "Machine learning poses data-driven optimization problems in which the objective function involves the summation of loss terms over a set of data to be modeled. Classical optimization techniques must compute this sum in its entirety for each evaluation of the objective, respectively its gradient. As available data sets grow ever larger, such ”batch” optimizers therefore become increasingly inefficient. They are also ill-suited for the online (incremental) setting, where partial data must be modeled as it arrives. Stochastic (online) gradient-based methods, by contrast, work with gradient estimates obtained from small subsamples (mini-batches) of training data. This can greatly reduce computational requirements: on large, redundant data sets, simple stochastic gradient descent routinely outperforms sophisticated second-order batch methods by orders of magnitude in spite of the slow convergence of first-order gradient descent.\nRecently there were partially successful attempts to adapt classical powerful unconstrained optimization methods to mini-batch mode. For example of mini-batch limited memory quasi-Newton L-BFGS [20, 21, 22]. It significantly outperforms stochastic gradient method, however still there is room\nfor improvement."
    }, {
      "heading" : "2 Merging existing algorithms with with SESOP",
      "text" : "The methods mentioned above are faster than plain SESOP under specific conditions: Trust region Newton method – for problems with easily invertible Hessian matrix; Truncated Newton method – when fast multiplication by Hessian is available; Stochastic optimization methods - for problems with large stochastic-type data; Multigrid methods – for problems with nested multilevel structure. Each of these methods has its own weakness, which may be alleviated using subspace optimization.\nAnother task is to accelerate Augmented Lagrangian method for constrained optimization problems and Alternating Direction Method of Multipliers for problems with separable objective function and non-separable constraints, using SESOP concept."
    }, {
      "heading" : "2.1 Merging trust region and line search approaches within SESOP framework",
      "text" : "The known problem with trust region approach (6) is two fold. First, finding constrained minimum of the model is often 2 – 3 times more expensive than finding the unconstrained minimum. Secondly, the model can fit the original function well in some directions and poorly in others, therefore an ideal trust region should be a kind of ellipsoid instead of Euclidean ball. However adjusting parameters of such ellipsoid is rather difficult. In our proposal we will alleviate this difficulty using SESOP.\nUsually the model qk in trust region step (6) is built in the way that its gradient at the point xk is equal to the gradient of the original function f . This is the case, for example, for Newton method, when the model is the second order Taylor expansion of f around xk. Therefore, if the radius rk of trust region goes to zero in (6), the obtained direction will correspond to the one of steepest descent. On the other hand, when the radius goes to infinity, the direction will correspond to the pure Newton step. For intermediate values of the radius the obtained direction gives a compromise between Newton and gradient step.\nMotivated by the above observation, we propose to compute the next iterate xk+1 as a minimizer of the original function f over affine subspace that touches xk and spans the current ”Newton” direction dk of the line search method given by (4), (5) and the direction of negative gradient −∇f(xk). In addition it may be very useful to include directions of several previous\nsteps xk−xk−1, xk−1−xk−2, ..., as well as previous gradients and ”Newton” directions into the subspace.\nThe proposed method takes advantage of several worlds. Whenever the Newton direction is nor effective, it is at least as efficient as SESOP [8, 8, 11, 15], which is usually much faster than steepest descent or nonlinear conjugate gradient method; when Newton direction is good, local quadratic convergence rate of Newton is preserved.\nWe should also note that quite often subspace optimization may be performed in very efficient way, so that additional computational burden is very small comparing to the evaluation of Newton direction. It is possible to incorporate the proposed approach in other newton-type schemes, for example into Levenberg-Marquardt method for nonlinear least squares, which is very popular e.g. in training of moderately-sized feed-forward neural networks."
    }, {
      "heading" : "2.2 Combining SESOP with Truncated Newton method",
      "text" : "Like we mentioned before, TN method is sensitive to change in early stopping of the internal CG optimization. We are going to resolve this problem replacing the line search in the outer step with subspace optimization. In this way the CG sequence will not break between outer iterations and will stay matched through consequent TN steps.\nWhen minimizing objective function f(x), k-th TN step will approximately minimize its quadratic model around current iterate xk\nqk(x) = f(x k) + gk T (x− xk) +\n1 2 (x− xk)THk(x− x k), (7)\nwhere gk = ∇f(xk) is the gradient and Hk = ∇ 2f(xk) – the Hessian of f at xk. Suppose that TN step k was truncated after l CG iterations. Coming back from the quadratic model to the original objective, we would like to imitate the next CG step, using f(x) instead of q(x). CG iteration l + 1 inside TN would perform optimization of the quadratic model q(x) over the affine subspace Sk,l, passing through the current inner iterate x\nk,l and spanned by the last CG step xk,l−xk,l−1 and the current gradient ∇q(xk,l). Instead, the next SESOP iteration will minimize f over the extended affine subspace Sk ⊃ Sk,l. In order to provide monotone descent of f , we add to Sk the TN direction\ndTN = x k,l − xk.\nNow xk ∈ Sk, and any monotone method used for the subspace optimization over Sk starting from x\nk, will reduce the objective relatively to f(xk). Optionally, we include several previous outer steps and gradients of f into\nSk, in order to improve the function descent, when the TN directions are not good enough.\nNext Truncated Newton step After performing the subspace optimization, we start a new TN iteration. At this stage, in order to keep alignment through the global CG sequence, we perform the first new CG step as an optimization of the new quadratic model qk+1(x) over the 2D subspace spanned by xk+1 − xk,l and g(xk+1).\nSummary of SESOP-TN algorithm: outer iteration k\n1. TN step Solve approximately Newton system∇2f(xk)dkTN = −∇f(x k),\ni.e. minimize quadratic model qk(x) in (7), using l steps of CG. Denote the last CG iterate as xk,l.\n2. Subspace optimization step xk+1 ≈ argminx∈Sk f(x),\nwhere affine subspace Sk passes through x k and is spanned by: • TN Direction dkTN = x k,l − xk; • Last value of the gradient of quadratic model ∇qk(x k,l) used in TN; • Last used CG direction in TN: (xk,l − xk,l−1); • [Optional] directions of several previous outer steps and gradients of f .\n3. Goto TN step, while performing the first new CG step as an optimization of quadratic model qk+1(x) over 2D subspace spanned by (xk+1 − xk,l) and ∇f(xk+1).\nThe presented procedure resolves the problem of TN sensitivity to early break of the CG process. For example, when the objective function is quadratic, SESOP-TN trajectory coincides with the trajectory of CG applied directly to the objective function, independently of the stopping rule in the TN step (see Figure 2 in Section 4 below) Standard TN lacks this property and converges more slowly when truncated too early."
    }, {
      "heading" : "2.3 Mini-batch stochastic optimization using SESOP",
      "text" : "Inspired by the recent success of mini-batch limited memory quasi-Newton method L-BFGS [20, 21, 22] in various machine learning tasks, we plan to adapt SESOP to similar conditions. SESOP has shown state of the art efficiency on many classes of large scale problems, see e.g [11, 15]. Basic SESOP iteration consists in computing minimum of the objective function over subspace spanned by the current gradient and several previous steps. In\nstochastic mode one can perform optimization of partial objective function, computed in current mini-batch, over subspace spanned by directions of steps in several previous mini-batches and the previous partial gradient. Restricting use of the latest gradient we hope to stabilize the method by preventing over-fitting to the latest mini-batch data."
    }, {
      "heading" : "2.4 Incorporating SESOP into multigrid/multilevel optimization",
      "text" : "In multigrid and multilevel techniques (see basic introduction in [23]) the problem whose solution is sought is represented in a hierarchy of resolutions, and each such version of the problem is “responsible” for eliminating errors on a scale compatible with its typical resolution. Multigrid methods accelerate computation in two ways. First, an approximate solution to the fine-grid problem may be cheaply obtained using coarse-grid formulation. Second, by ”clever” sequential iteration between fine and coarse grid, an accurate solution can be obtained much faster, then just iterating on fine grid with a good initial guess [23].\nMerging SESOP with multigrid Our approach is relevant to both, linear and non-linear multigrid problems, expressed in variational form as nested set of quadratic or general non-linear optimization problems, see e.g. [24, 25]. Quite often optimization at every level of resolution is carried by Conjugate Gradient (CG) method, which converges much faster than steepest-descent type methods on ill-conditioned problems. One the other hand, CG efficiency is greatly reduced when the method is restarted after small number of iterations (restarts happen because of interleave of fine and coarse grid steps in multigrid.) In order to alleviate this problem, we propose to use SESOP instead of CG. It is known that SESOP is equivalent to CG when used for minimization of quadratic function [8], but usually converges faster than CG, when the function is non-quadratic.\nStandard iteration of SESOP consists of optimization of the objective function over affine subspace spanned by directions of several previous steps, current [preconditioned] gradient (or other reasonable directions, like parallel coordinate descent, [11]). Additionally we are going to include descent direction, provided by most recent coarse grid solution, into current finegrid SESOP subspace. This should provide best combination of SESOP and multigrid advantages, because fine-grid iterations have no break, and CG-type acceleration properties are preserved.\nSaddle point problem and Alternating Direction Method of Multipliers As a general framework, SESOP allows acceleration of many other memoryless techniques. In particular, we are going to explore saddle point algorithms such as Augmented Lagrangian method for constrained optimization. In this case at every iteration of SESOP we will look for a saddle point in subspace of previous steps in primal and dual variables and current Augmented Lagrangian step. When the primal objective is separable and Alternating Direction Method of Multipliers is applicable (see e.g. [26]), we can include corresponding alternating directions into SESOP subspace."
    }, {
      "heading" : "3 Perspectives of convergence analysis",
      "text" : "In this section we just share several thoughts regarding possible convergence analysis of SESOP related methods.\nComposite functions As we already mentioned, similarly to ORTH and Nesterov method, SESOP possesses optimal 1/k2 worst-case convergence expressed in (1). The error in this formula is proportional to the Lipschitz constant L of the gradient of the objective function f . This bound become unsatisfactory when dealing with composite objective of type (2), which incorporates non-smooth L1-norm term. Nesterov method for composite objective and FISTA alleviate this difficulty using SSF direction instead of the gradient in the update formula for evaluating the next step. Therefore only Lipschitz constant of the gradient of the first smooth term in the objective is involved in error estimate.\nIn the same spirit SSF-SESOP method uses SSF direction and several previous steps when computing the next step. In all our numerical tests SSFSESOP outperformed FISTA in number of iterations. All the above rises hope that it is possible to develop a proof of optimal worst-case complexity of SSF-SESOP, which is similar to FISTA.\nOn the other hand, asymptotic linear convergence rate of SESOP is similar to the Conjugate Gradient method and is much better than the Nesterov one. We have some results for quadratic functions [11], however they could be extended for smooth nonlinear case as well as to the composite non-smooth functions.\nConstrained optimization with PCD-SSF-SESOP Substituting L1norm in (2) with L∞, and applying PCD-SSF-SESOP, one can obtain efficient method for problem with box-constraints. In similar way we can use\nSESOP-TN Standard TN\nindicator function of other simple feasible sets. The convergence results in this case should just follow from general results for composite functions."
    }, {
      "heading" : "4 Some preliminary experiments:",
      "text" : "Truncated Newton SESOP (SESOP-TN)\nQuadratic function First, let us demonstrate ”proof of the concept” using a pure quadratic function. We solve the linear least squares problem\nmin ||Ax− b||2 (8)\nwith n = 400 variables, where the square random matrix A has zero-mean i.i.d. Gaussian entries with variance 1/n. As we see in Figure 2, SESOP-TN trajectory, as expected, does not depend on the number of CG iterations in the TN step. Standard TN (the right plot) lacks this property.\nTwo non-linear examples The first problem is Exponents-and-Squares [27] with n = 200 variables:\nmin e−1 Tx +\n1\n2\nn∑\nj=1\nj2x2j .\nThe second example is Linear Support Vector Machine (SVM), see [28] for more details on unconstrained formulation of SVM. We used data set Astrophysics-29882 [29] with 99758 variables, and selected randomly 1495 training examples from there. In both problems (see Figure 3), SESOP-TN consistently outperformed classic TN, when restricted to 1, 10 or 40 CG iterations in TN step."
    }, {
      "heading" : "5 Conclusions",
      "text" : "In this paper we presented several perspective directions of acceleration of optimization algorithms using SESOP framework and shared some thoughts about convergence analysis."
    } ],
    "references" : [ {
      "title" : "Interior-point polynomial algorithms in convex programming",
      "author" : [ "Y. Nesterov", "A.S. Nemirovskii", "Y. Ye" ],
      "venue" : "vol. 13. SIAM,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1994
    }, {
      "title" : "Information-based complexity of mathematical programming (in Russian)",
      "author" : [ "A. Nemirovski", "D. Yudin" ],
      "venue" : "Izvestia AN SSSR, Ser. Tekhnicheskaya Kibernetika (the journal is translated to English as Engineering Cybernetics. Soviet J. Computer & Systems Sci.), vol. 1, 1983.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 1983
    }, {
      "title" : "Orth-method for smooth convex optimization (in Russian)",
      "author" : [ "A. Nemirovski" ],
      "venue" : "Izvestia AN SSSR, Ser. Tekhnicheskaya Kibernetika (the journal is translated to English as Engineering Cybernetics. Soviet J. Computer & Systems Sci.), vol. 2, 1982.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 1982
    }, {
      "title" : "A method for unconstrained convex minimization problem with the rate of convergence o(1/n) (in Russian)",
      "author" : [ "Y. Nesterov" ],
      "venue" : "Doklady AN SSSR (the journal is translated to English as Soviet Math. Docl.), vol. 269, no. 3, pp. 543–547, 1983.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 1983
    }, {
      "title" : "Smooth minimization of non-smooth functions",
      "author" : [ "Y. Nesterov" ],
      "venue" : "CORE discussion paper, Universitè catholique de Louvain, Belgium, 2003.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Sequential subspace optimization method for large-scale unconstrained problems",
      "author" : [ "G. Narkiss", "M. Zibulevsky" ],
      "venue" : "Technical Report CCIT 559, Technion – Israel Institute of Technology, Faculty of Electrical Engineering, 2005.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Methods of conjugate gradients for solving linear systems",
      "author" : [ "M.R. Hestenes", "E. Stiefel" ],
      "venue" : "J. Res. Natl. Bur. Stand., vol. 49, pp. 409–436, 1952.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 1952
    }, {
      "title" : "Numerical optimization. Series in operations research and financial engineering",
      "author" : [ "J. Nocedal", "S. Wright" ],
      "venue" : null,
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2006
    }, {
      "title" : "Coordinate and subspace optimization methods for linear least squares with non-quadratic regularization",
      "author" : [ "M. Elad", "B. Matalon", "M. Zibulevsky" ],
      "venue" : "Applied and Computational Harmonic Analysis, vol. 23, no. 3, pp. 346–367, 2007.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Why simple shrinkage is still relevant for redundant representations",
      "author" : [ "M. Elad" ],
      "venue" : "Information Theory, IEEE Transactions on, vol. 52, no. 12, pp. 5559–5569, 2006.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "An iterative thresholding algorithm for linear inverse problems with a sparsity constraint",
      "author" : [ "I. Daubechies", "M. Defrise", "C. De Mol" ],
      "venue" : "Communications on pure and applied mathematics, vol. 57, no. 11, pp. 1413– 1457, 2004.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "An em algorithm for wavelet-based image restoration",
      "author" : [ "M.A. Figueiredo", "R.D. Nowak" ],
      "venue" : "Image Processing, IEEE Transactions on, vol. 12, no. 8, pp. 906–916, 2003.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "L1-L2 optimization in signal and image processing",
      "author" : [ "M. Zibulevsky", "M. Elad" ],
      "venue" : "Signal Processing Magazine, IEEE, vol. 27, no. 3, pp. 76– 88, 2010.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Gradient methods for minimizing composite objective function",
      "author" : [ "Y. Nesterov" ],
      "venue" : "2007.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "On the performance of algorithms for the minimization of L1-penalized functionals",
      "author" : [ "I. Loris" ],
      "venue" : "Inverse Problems, vol. 25, no. 3, p. 035008, 2009.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Truncated-Newton algorithms for largescale unconstrained optimization",
      "author" : [ "R.S. Dembo", "T. Steihaug" ],
      "venue" : "Mathematical Programming, vol. 26, pp. 190–212, June 1983. 16",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 1983
    }, {
      "title" : "A survey of Truncated-Newton methods",
      "author" : [ "S.G. Nash" ],
      "venue" : "Journal of Computational and Applied Mathematics, no. 124, pp. 45–59, 2000.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "A stochastic quasi-Newton method for online convex optimization",
      "author" : [ "N. Schraudolph", "J. Yu", "S. Günter" ],
      "venue" : "2007.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "SGD-QN: Careful quasinewton stochastic gradient descent",
      "author" : [ "A. Bordes", "L. Bottou", "P. Gallinari" ],
      "venue" : "The Journal of Machine Learning Research, vol. 10, pp. 1737–1754, 2009.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "On optimization methods for deep learning",
      "author" : [ "J. Ngiam", "A. Coates", "A. Lahiri", "B. Prochnow", "A. Ng", "Q.V. Le" ],
      "venue" : "Proceedings of the 28th International Conference on Machine Learning (ICML-11), pp. 265– 272, 2011.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Why multigrid methods are so efficient",
      "author" : [ "I. Yavneh" ],
      "venue" : "Computing in science & engineering, vol. 8, no. 6, pp. 12–22, 2006.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "A multigrid approach to discretized optimization problems",
      "author" : [ "S.G. Nash" ],
      "venue" : "Optimization Methods and Software, vol. 14, no. 1-2, pp. 99–116, 2000.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Recursive trust-region methods for multiscale nonlinear optimization",
      "author" : [ "S. Gratton", "A. Sartenaer", "P.L. Toint" ],
      "venue" : "SIAM Journal on Optimization, vol. 19, no. 1, pp. 414–444, 2008.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Distributed optimization and statistical learning via the alternating direction method of multipliers",
      "author" : [ "S.", "N. Parikh", "E. Chu", "B. Peleato", "J. Eckstein" ],
      "venue" : "Foundations and Trends R  © in Machine Learning, vol. 3, no. 1, pp. 1–122, 2011.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "UCTP - test problems for unconstrained optimization",
      "author" : [ "H. Nielsen" ],
      "venue" : "Technical Report IMM-REP-2000-18, Department of Mathematical Modelling, DTU, 2000.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Support Vector Machine via sequential subspace optimization",
      "author" : [ "G. Narkiss", "M. Zibulevsky" ],
      "venue" : "Technical Report CCIT 557, Technion – Israel Institute of Technology, Faculty of Electrical Engineering, 2005.",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Training linear SVMs in linear time",
      "author" : [ "T. Joachims" ],
      "venue" : "Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD), 2006. 17",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2006
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Interior-point polynomial complexity methods provide fast and robust solution of convex problems, where Newton optimization is applicable [1, 2].",
      "startOffset" : 138,
      "endOffset" : 144
    }, {
      "referenceID" : 1,
      "context" : "Several of them possess optimal worst-case complexity [3]: ORTH method by Nemirovsky [4], Nesterov method [5, 6] and FISTA method by Beck and Teboulle [7].",
      "startOffset" : 54,
      "endOffset" : 57
    }, {
      "referenceID" : 2,
      "context" : "Several of them possess optimal worst-case complexity [3]: ORTH method by Nemirovsky [4], Nesterov method [5, 6] and FISTA method by Beck and Teboulle [7].",
      "startOffset" : 85,
      "endOffset" : 88
    }, {
      "referenceID" : 3,
      "context" : "Several of them possess optimal worst-case complexity [3]: ORTH method by Nemirovsky [4], Nesterov method [5, 6] and FISTA method by Beck and Teboulle [7].",
      "startOffset" : 106,
      "endOffset" : 112
    }, {
      "referenceID" : 4,
      "context" : "Several of them possess optimal worst-case complexity [3]: ORTH method by Nemirovsky [4], Nesterov method [5, 6] and FISTA method by Beck and Teboulle [7].",
      "startOffset" : 106,
      "endOffset" : 112
    }, {
      "referenceID" : 5,
      "context" : "The story of SESOP method [8] begins with the method of Conjugate Gradients (CG) [9].",
      "startOffset" : 26,
      "endOffset" : 29
    }, {
      "referenceID" : 6,
      "context" : "The story of SESOP method [8] begins with the method of Conjugate Gradients (CG) [9].",
      "startOffset" : 81,
      "endOffset" : 84
    }, {
      "referenceID" : 7,
      "context" : "CG applied to a quadratic function) has remarkable convergence properties: Its linear convergence rate (see for example [10]) is √ r−1 √ r+1 , where r is the condition number of the Hessian of the objective.",
      "startOffset" : 120,
      "endOffset" : 124
    }, {
      "referenceID" : 2,
      "context" : "One can also rely on a 1/k sub-linear worst-case convergence of the quadratic CG, which does not depend on the Hessian conditioning (see for example [4, 3, 8]):",
      "startOffset" : 149,
      "endOffset" : 158
    }, {
      "referenceID" : 1,
      "context" : "One can also rely on a 1/k sub-linear worst-case convergence of the quadratic CG, which does not depend on the Hessian conditioning (see for example [4, 3, 8]):",
      "startOffset" : 149,
      "endOffset" : 158
    }, {
      "referenceID" : 5,
      "context" : "One can also rely on a 1/k sub-linear worst-case convergence of the quadratic CG, which does not depend on the Hessian conditioning (see for example [4, 3, 8]):",
      "startOffset" : 149,
      "endOffset" : 158
    }, {
      "referenceID" : 2,
      "context" : "In order to alleviate this problem, Nemirovski [4] suggested to restrict the optimization subspace just to three directions: the current gradient, the sum of all previous steps and a “cleverly” weighted sum of all previous gradients (a version of such weights is given in [8]).",
      "startOffset" : 47,
      "endOffset" : 50
    }, {
      "referenceID" : 5,
      "context" : "In order to alleviate this problem, Nemirovski [4] suggested to restrict the optimization subspace just to three directions: the current gradient, the sum of all previous steps and a “cleverly” weighted sum of all previous gradients (a version of such weights is given in [8]).",
      "startOffset" : 272,
      "endOffset" : 275
    }, {
      "referenceID" : 5,
      "context" : "The SESOP method [8] extends the ORTH subspaces with several directions of the last propagation steps.",
      "startOffset" : 17,
      "endOffset" : 20
    }, {
      "referenceID" : 8,
      "context" : "In order to improve efficiency further we can substitute gradient direction with direction of parallel coordinate descent (PCD) or direction, provided by minimizer of a separable surrogate function (SSF) [11] .",
      "startOffset" : 204,
      "endOffset" : 208
    }, {
      "referenceID" : 9,
      "context" : "For such problems we use Parallel Coordinate Descent [12]: Staying at current point we evaluate coordinate descent steps for all coordinates without moving along them (this can be performed analytically at cost of two fast matrix-vector multiplications).",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 10,
      "context" : "Method of separable surrogate functions (SSF) is another efficient alternative to gradient descent proposed by [13, 14].",
      "startOffset" : 111,
      "endOffset" : 119
    }, {
      "referenceID" : 11,
      "context" : "Method of separable surrogate functions (SSF) is another efficient alternative to gradient descent proposed by [13, 14].",
      "startOffset" : 111,
      "endOffset" : 119
    }, {
      "referenceID" : 12,
      "context" : "This partially explains extremal efficiency of these methods on difficult problems, where they are often significantly faster [15] than popular Nesterov method and FISTA [16, 7].",
      "startOffset" : 126,
      "endOffset" : 130
    }, {
      "referenceID" : 13,
      "context" : "This partially explains extremal efficiency of these methods on difficult problems, where they are often significantly faster [15] than popular Nesterov method and FISTA [16, 7].",
      "startOffset" : 170,
      "endOffset" : 177
    }, {
      "referenceID" : 12,
      "context" : "In Figure 1 we present results of an experiment [15] adopted from [17].",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 14,
      "context" : "In Figure 1 we present results of an experiment [15] adopted from [17].",
      "startOffset" : 66,
      "endOffset" : 70
    }, {
      "referenceID" : 14,
      "context" : "can see, FISTA converges much faster then SSF, which corresponds to the observations in [17].",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 7,
      "context" : "[10].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 15,
      "context" : "Truncated Newton (TN) method [18, 19] is used when computing and inverting Hessian matrix is prohibitively expensive.",
      "startOffset" : 29,
      "endOffset" : 37
    }, {
      "referenceID" : 16,
      "context" : "Truncated Newton (TN) method [18, 19] is used when computing and inverting Hessian matrix is prohibitively expensive.",
      "startOffset" : 29,
      "endOffset" : 37
    }, {
      "referenceID" : 17,
      "context" : "For example of mini-batch limited memory quasi-Newton L-BFGS [20, 21, 22].",
      "startOffset" : 61,
      "endOffset" : 73
    }, {
      "referenceID" : 18,
      "context" : "For example of mini-batch limited memory quasi-Newton L-BFGS [20, 21, 22].",
      "startOffset" : 61,
      "endOffset" : 73
    }, {
      "referenceID" : 19,
      "context" : "For example of mini-batch limited memory quasi-Newton L-BFGS [20, 21, 22].",
      "startOffset" : 61,
      "endOffset" : 73
    }, {
      "referenceID" : 5,
      "context" : "Whenever the Newton direction is nor effective, it is at least as efficient as SESOP [8, 8, 11, 15], which is usually much faster than steepest descent or nonlinear conjugate gradient method; when Newton direction is good, local quadratic convergence rate of Newton is preserved.",
      "startOffset" : 85,
      "endOffset" : 99
    }, {
      "referenceID" : 5,
      "context" : "Whenever the Newton direction is nor effective, it is at least as efficient as SESOP [8, 8, 11, 15], which is usually much faster than steepest descent or nonlinear conjugate gradient method; when Newton direction is good, local quadratic convergence rate of Newton is preserved.",
      "startOffset" : 85,
      "endOffset" : 99
    }, {
      "referenceID" : 8,
      "context" : "Whenever the Newton direction is nor effective, it is at least as efficient as SESOP [8, 8, 11, 15], which is usually much faster than steepest descent or nonlinear conjugate gradient method; when Newton direction is good, local quadratic convergence rate of Newton is preserved.",
      "startOffset" : 85,
      "endOffset" : 99
    }, {
      "referenceID" : 12,
      "context" : "Whenever the Newton direction is nor effective, it is at least as efficient as SESOP [8, 8, 11, 15], which is usually much faster than steepest descent or nonlinear conjugate gradient method; when Newton direction is good, local quadratic convergence rate of Newton is preserved.",
      "startOffset" : 85,
      "endOffset" : 99
    }, {
      "referenceID" : 17,
      "context" : "Inspired by the recent success of mini-batch limited memory quasi-Newton method L-BFGS [20, 21, 22] in various machine learning tasks, we plan to adapt SESOP to similar conditions.",
      "startOffset" : 87,
      "endOffset" : 99
    }, {
      "referenceID" : 18,
      "context" : "Inspired by the recent success of mini-batch limited memory quasi-Newton method L-BFGS [20, 21, 22] in various machine learning tasks, we plan to adapt SESOP to similar conditions.",
      "startOffset" : 87,
      "endOffset" : 99
    }, {
      "referenceID" : 19,
      "context" : "Inspired by the recent success of mini-batch limited memory quasi-Newton method L-BFGS [20, 21, 22] in various machine learning tasks, we plan to adapt SESOP to similar conditions.",
      "startOffset" : 87,
      "endOffset" : 99
    }, {
      "referenceID" : 8,
      "context" : "g [11, 15].",
      "startOffset" : 2,
      "endOffset" : 10
    }, {
      "referenceID" : 12,
      "context" : "g [11, 15].",
      "startOffset" : 2,
      "endOffset" : 10
    }, {
      "referenceID" : 20,
      "context" : "In multigrid and multilevel techniques (see basic introduction in [23]) the problem whose solution is sought is represented in a hierarchy of resolutions, and each such version of the problem is “responsible” for eliminating errors on a scale compatible with its typical resolution.",
      "startOffset" : 66,
      "endOffset" : 70
    }, {
      "referenceID" : 20,
      "context" : "Second, by ”clever” sequential iteration between fine and coarse grid, an accurate solution can be obtained much faster, then just iterating on fine grid with a good initial guess [23].",
      "startOffset" : 180,
      "endOffset" : 184
    }, {
      "referenceID" : 21,
      "context" : "[24, 25].",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 22,
      "context" : "[24, 25].",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 5,
      "context" : "It is known that SESOP is equivalent to CG when used for minimization of quadratic function [8], but usually converges faster than CG, when the function is non-quadratic.",
      "startOffset" : 92,
      "endOffset" : 95
    }, {
      "referenceID" : 8,
      "context" : "Standard iteration of SESOP consists of optimization of the objective function over affine subspace spanned by directions of several previous steps, current [preconditioned] gradient (or other reasonable directions, like parallel coordinate descent, [11]).",
      "startOffset" : 250,
      "endOffset" : 254
    }, {
      "referenceID" : 23,
      "context" : "[26]), we can include corresponding alternating directions into SESOP subspace.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 8,
      "context" : "We have some results for quadratic functions [11], however they could be extended for smooth nonlinear case as well as to the composite non-smooth functions.",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 24,
      "context" : "Two non-linear examples The first problem is Exponents-and-Squares [27] with n = 200 variables:",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 25,
      "context" : "The second example is Linear Support Vector Machine (SVM), see [28] for more details on unconstrained formulation of SVM.",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 26,
      "context" : "We used data set Astrophysics-29882 [29] with 99758 variables, and selected randomly 1495 training examples from there.",
      "startOffset" : 36,
      "endOffset" : 40
    } ],
    "year" : 2014,
    "abstractText" : "This is an overview paper written in style of research proposal. In recent years we introduced a general framework for large-scale unconstrained optimization – Sequential Subspace Optimization (SESOP) and demonstrated its usefulness for sparsity-based signal/image denoising, deconvolution, compressive sensing, computed tomography, diffraction imaging, support vector machines. We explored its combination with Parallel Coordinate Descent and Separable Surrogate Function methods, obtaining state of the art results in above-mentioned areas. There are several methods, that are faster than plain SESOP under specific conditions: Trust region Newton method for problems with easily invertible Hessian matrix; Truncated Newton method when fast multiplication by Hessian is available; Stochastic optimization methods for problems with large stochastic-type data; Multigrid methods for problems with nested multilevel structure. Each of these methods can be further improved by merge with SESOP. One can also accelerate Augmented Lagrangian method for constrained optimization problems and Alternating Direction Method of Multipliers for problems with separable objective function and non-separable constraints.",
    "creator" : "LaTeX with hyperref package"
  }
}