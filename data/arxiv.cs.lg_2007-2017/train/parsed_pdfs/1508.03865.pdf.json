{
  "name" : "1508.03865.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Predicting Grades",
    "authors" : [ "Yannick Meier", "Jie Xu", "Onur Atan", "Mihaela van der Schaar" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Index Terms—Forecasting algorithms, online learning, grade prediction, data mining, digital signal processing education.\nI. INTRODUCTION\nEDUCATION is in a transformation phase; knowledgeis increasingly becoming freely accessible to everyone (through Massive Open Online Courses, Wikipedia, etc.) and is developed by a large number of contributors rather than by a single author [1]. Furthermore, new technology allows for personalized education enabling students to learn more efficiently and giving teachers the tools to support each student individually if needed, even if the class is large [2], [3].\nGrades are supposed to summarize in a single number or letter how well a student was able to understand and apply the knowledge conveyed in a course. Thus it is crucial for students to obtain the necessary support to pass and do well in a class. However, with large class sizes at universities and even larger class sizes in Massive Open Online Courses (MOOCs), which have undergone a rapid development in the past few years, it has become impossible for the instructor and teaching assistants to keep track of the performance of each student individually. This can lead to students failing in a class who could have passed if appropriate remedial actions had been taken early enough or excellent students not\nY. Meier, J. Xu, O. Atan and M. van der Schaar are with the Department of Electrical Engineering, University of California, Los Angeles, CA, 90095 USA. e-mail: (see http://medianetlab.ee.ucla.edu/people.html). This research is supported by the US Air Force Office of Scientific Research under the DDDAS Program.\nreceiving the necessary promotion to benefit maximally from the course. Remedial or promotional actions could consist of additional online study material presented to the student in a personalized and/or automated manner [4]. Hence, in both offline and online education, it is of great importance to develop automated personalized systems that predict the performance of a student in a course before the course is over and as soon as possible. While in online teaching systems a variety of data about a student such as responses to quizzes, activity in the forum and study time can be collected, the available data in a practical offline setting are limited to scores in early performance assessments such as homework assignments, quizzes and midterm exams.\nIn this paper we focus on predicting grades in traditional classroom-teaching where only the scores of students from past performance assessments are available. However, we believe that our methods can also be applied for online courses such as MOOCs. We design a grade prediction algorithm that finds for each student the best time to predict his/her grade such that, based on this prediction, a timely intervention can be made if necessary. Note that we analyze data from a digital signal processing course where no interventions were made; hence, we do not study the impact of inventions and consider only a single grade prediction for each student. However, our algorithm can be easily extended to multiple predictions per student.\nA timely prediction exclusively based on the limited data from the course itself is challenging for various reasons. First, since at the beginning most students are motivated, the score of students in early performance assessments (e.g. homework assignments) might have little correlation with their score in later performance assessments, in-class exams and the overall score. Second, even if the same material is covered in each year of the course, the assignments and exams change every year. Therefore, the informativeness of particular assignments with regard to predicting the final grade may change over the years. Third, the predictability of students having a variety of different backgrounds is very diverse. For some students an accurate prediction can be made very early based on the first few performance assessments. If for example a student shows an excellent performance in the first three homework assignments and in the midterm exam, it is highly likely that he/she will pass the class. For other students it might take more time to make an equally accurate prediction. If a student for example performs below average but not terribly at the beginning, it is risky to predict whether he/she is going to pass or fail and, therefore, to decide whether or not to intervene. This third challenge illustrates the necessity to make the prediction for each student individually and not for all at the same time.\nThe main contributions of this paper can be summarized as\nar X\niv :1\n50 8.\n03 86\n5v 1\n[ cs\n.L G\n] 1\n6 A\nug 2\n01 5\n2 follows. 1) We propose an algorithm that makes a personalized and\ntimely prediction of the grade of each student in a class. The algorithm can both be used in regression settings, where the overall score is predicted, and in classification settings, where the students are classified into two (e.g. do well/poorly) or more categories. 2) We accompany each prediction with a confidence estimate indicating the expected accuracy of the prediction. 3) We derive a bound for the probability that the prediction error is larger than a desired value . 4) We exclusively use the scores students achieve in early performance assessments such as homework assignments and midterm exams and do not use any other information such as age, gender or previous GPA. This makes our algorithm applicable in all practical traditional classroom and online teaching settings, where such information may not be available. 5) Since the algorithm is learning from past years, the predictions become more accurate when more data from previous years become available. 6) We demonstrate that the algorithm shows good robustness if different instructors have taught the course in past years. 7) We analyze real data from an introductory digital signal processing course taught at UCLA over 7 years and use the data to experimentally demonstrate the performance of our algorithm compared to benchmark prediction methods. As benchmark algorithms we use well known algorithms such as linear/logistic regression and kNearest Neighbors, which are still a current research topic [5]–[7]. 8) Based on our simulations, we suggest a preferred way of designing courses that enables early prediction and early intervention. Using data from a pilot course, we demonstrate the advantages of the suggested design.\nThe rest of the paper is organized as follows. Section II discusses related work in the field of grade and GPA prediction in education. In Section III we introduce notation, define data structures, formalize the problem and present the grade prediction algorithm. We analyze the data, describe benchmark methods and present simulation results including our and benchmark algorithms in Section IV. Finally, we draw conclusions in Section V."
    }, {
      "heading" : "II. RELATED WORK",
      "text" : "Various studies have investigated the value of standardized tests [8]–[10] admissions exams [11] and GPA in previous programs [9] in predicting the academic success of students in undergraduate or graduate schools. They agree on a positive correlation between these predictors and success measures such as GPA or degree completion. Besides standardized tests, the relevancy of other variables for predictions of a student’s GPA have been investigated, usually resulting in the conclusion that GPA from prior education and past grades in certain subjects (e.g. math, chemistry) [12], [13] have a strongly positive correlation as well. Reference [12] observes\nthat simple linear and more complex nonlinear (e.g. artificial neural network) models frequently lead to similar prediction accuracies and concludes that there is either no complex nonlinear pattern to be found in the underlying data or the pattern cannot be recognized by their approach. Our simulations support the statement that simple linear models show a similar accuracy in grade predictions as more complex methods.\nReference [14] argues that the accuracy of GPA predictions frequently is mediocre due to different grading standards used in different classes and shows a higher validity for grade predictions in single classes. Consequently, many works focus on identifying relationships between a student’s grade in a particular class and variables related to the student [15]–[19]. Relevant factors were found to include the student’s prior GPA [15]–[17], [19], performance in related courses [16], [17], [19], performance in early assignments of the class [17], [19], class attendance [15], self-efficacy [18] and whether the student is repeating the class [17]. To make the predictions, various data mining models such as regression models, decision trees, support vector machines and nearest neighbor techniques are used.\nA limitation of the algorithms in the previously discussed papers is that they are difficult to apply in many education scenarios. Frequently, variables related to the student such as performance in related classes, GPA or self-efficacy are not available to the instructor because the data has not been collected or is not accessible due to privacy reasons. However, the instructor always has access to data he/she collects from his/her own course, such as the performance of each student in early homework assignments or midterm exams. This paper, therefore, focuses on predicting the final grade based on this easily accessible data, which is collected anyway by the instructor.\nOther works [20]–[23], which also exclusively use data from the course itself, differ significantly from this paper in several aspects. First, they rely on logged data in online education or Massive Open Online Course (MOOC) systems such as information about video-watching behavior or time spent on specific questions. In contrast, our results are applicable to both online and offline courses, which include some kind of graded assignments or related feedback from the students during the course. Second, in order for the instructor to be able to take corrective actions it is of great importance to predict with a certain confidence the performance of students as early as possible. While our algorithm takes this into account by deciding for each student individually the best time to make the prediction using a confidence measure, related works do not provide a metric indicating the optimal time to predict. Third, while related works need training data from the course whose grades they want to predict, we show that we can use training data from past year classes of the same or even from different courses. Finally, in contrast to algorithms from related work, which are only shown to be applicable to classification settings (e.g. pass/fail or letter grade), our algorithm can be used both in regression and classification settings.\nTable I summarizes the comparison between our paper and related work investigating and predicting student performance in a course.\n3\nPerformance Assessments\nGrade Prediction\nAssessment 1\nAssessment 2\nAssessment K\nDatabase Feature Vectors & Grades\nfrom Past Years\nWait\nStore Overall Grade Store Score and Feature Vector Load Data form Past Years\nWait or Predict? Final Prediction for Current Student Made after Assessment 2Prediction & Confidence\n…\nGrade Prediction Algorithm\nTake Corrective Actions in Consequence of the Predicted Grade\nAssessment 3\nStore Score and Feature Vector No Need to Load Data\nWait Acknowledge Final Prediction\nFig. 1. System diagram for a single student."
    }, {
      "heading" : "III. FORMALISM, ALGORITHM AND ANALYSIS",
      "text" : "In this section we mathematically formalize the problem and propose an algorithm that predicts the final score or a classification according to the final grade of a student with a given confidence."
    }, {
      "heading" : "A. Definitions and System Description",
      "text" : "Consider a course which is taught for several years with only slight modifications. Students attending the course have to complete performance assessments such as graded homework assignments, course projects and in-class exams and quizzes throughout the entire course.1 Our goal is to predict with a certain confidence the overall performance of a student before all performance assessments have been taken. See Fig. 1 for a depiction of the system.\nWe consider a discrete time model with y = 1, 2, . . . , Y and k = 1, 2, . . . ,K where y denotes the year in which the course is taught and k the point in time in year y after the kth performance assessment has been graded. Y gives the total number of years during which the course is taught and K is the total number of performance assessments of each year. For a given year y we use index i as a representation of ith student\n1The performance assessments are usually graded by teaching assistants, by the instructor or even by other students through peer review [24].\nof the year and Iy to denote the total number of students attending in year y. Except for the rare case that a student retakes the course, the students in each year are different. Let ai,y,k ∈ [0, 1] denote the normalized score or grade of student i in performance assessment k of year y.\nThe feature vector of yth year student i after having taken performance assessment k is given by xi,y,k = (ai,y,1, . . . , ai,y,k). The normalized overall score zi,y ∈ [0, 1] of yth year student i is the weighted sum of all performance assessments\nzi,y = K∑ k=1 wkai,y,k (1)\nwhere the wk denote the weight of performance assessment k so that ∑K k=1 wk = 1. The weights are set by the instructor and we assume that in each year the number, sequence and weight of performance assessments is the same. This assumption is reasonable since the content of a course usually does not change drastically over the years and frequently the same course material (e.g. course book) is used.2 This is especially true in an introductory course such as the one we investigate in Section IV. The residual (overall score) ci,y,k of yth year student i after performance assessment k is defined as\nci,y,k =\n{∑K l=k+1 wlai,y,l k ∈ {1, . . . ,K − 1}\n0 k = K (2)\nUsing this definition we can write the overall score of yth year student i as\nzi,y = ci,y,k + k∑ l=1 wlai,y,l. (3)\nNote that after having taken the performance assessment k, the instructor has access to all the scores up to assignment k but the residual scores ci,y,k need to be estimated. We denote the estimate of the residual score for yth year student i at time k by ĉi,y,k and the corresponding estimate of the overall score by ẑi,y,k. In binary classification settings, where the goal is to predict whether a student achieves a letter grade above or below a certain threshold, we denote the class of yth year student i by bi,y ∈ {0, 1}.\nFor each student i we store the set of feature vectors Xi,y = {xi,y,k|k ∈ {1, . . . ,K}}, the set of residuals Ci,y = {ci,y,1, . . . , ci,y,K−1} and the student’s overall score zi,y . All\n2This assumption is made for simplicity. As we show in section IV-D2 we can apply our algorithm to settings where different instructors using different weights for each performance assessment teach the course. A prediction across courses with a different number of performance assessments is possible as well, for example by combining the scores of two or more performance assessments to a single score.\n4 feature vectors from all students of year y are given by Xy =⋃Iy i=1 Xi,y and X = ⋃Y y=1 Xy denotes all feature vectors\nof all completed years. Similarly C = ⋃Y\ny=1 ⋃Iy i=1 Ci,y and\nZ = ⋃Y\ny=1 ⋃Iy i=1 zi,y denote all residuals and overall scores of\nall completed years. Let Xk ′ = {xi,y,k|k = k′,∀i, y} denote the set of feature vectors and Ck ′ = {ci,y,k|k = k′,∀i, y} denote the set of residuals saved after performance assessment k′."
    }, {
      "heading" : "B. Problem Formulation",
      "text" : "Having introduced notations, definitions and data structures, we now formalize the grade prediction problem. We will investigate two different types of predictions. The objective of the first type, which we refer to as regression setting, is to accurately predict the overall score of each student individually in a timely manner. The second problem, referred to as classification setting, aims at making a binary prediction whether the student will do well or poorly or whether he/she will necessitate additional help or not. Again, the prediction is personalized and takes timeliness into account. For both types of predictions, the same algorithm can be used with only slight modifications, which we discuss in Section III-D. We will also show that the binary prediction problem can easily be generalized to a classification into three or more classes.\nIrrespective of the type of the prediction, the decision for a yth year student i consists of two parts. First, we decide after which performance assessment k∗i,y to predict for the given student and second we determine his/her estimated overall score ẑi,y or his/her estimated binary classification b̂i,y . At a point in time k of year y all scores including the overall scores of all students of past years 1, . . . , y − 1 are known. Thus all feature vectors x ∈ X, residuals c ∈ C and overall scores z ∈ Z of all completed years are known. Furthermore, the scores ai,y,1, . . . , ai,y,k of yth year student i up to assessment k are known as well and do not have to be estimated. However, to determine the overall score of the student we need to predict his/her residual score ci,y,k consisting of performance assessments k + 1, . . . ,K since they lie in the future and are unknown. At time k we have to decide for each student of the current year whether this is the optimal time k∗i,y = k to predict or whether it is better to wait for the next performance assessment. If we decide to predict, we determine the optimal prediction of the overall score ẑi,y = ẑi,y,k∗i,y . Both decisions are made based on the feature vector xi,y,k of the given student and the feature vectors x ∈ Xk and residuals c ∈ Ck of past students. To determine the optimal time to predict, we calculate a confidence qi,y(k) indicating the expected accuracy of the prediction for each student after each performance assessment. The prediction for a particular student is made as soon as the confidence exceeds a user-defined threshold qi,y(k) > qth. The problem of finding the optimal prediction time for yth year student i is formalized as follows:\nminimize k k subject to qi,y(k) > qth (4)\nThe optimization problem results in the optimal prediction time k∗i,y ."
    }, {
      "heading" : "C. Grade Prediction Algorithm, Regression Setting",
      "text" : "In this section we propose an algorithm that learns to predict a student’s overall performance based on data from classes held in past years and based on the student’s results in already graded performance assessments. We describe the algorithm for the regression setting and explain the changes needed to use the algorithm in the classification setting in Section III-D.\nSince at time k we know the scores ai,y,1, . . . , ai,y,k of the considered student from past performance assessments as well as the corresponding weights w1, . . . , wk, we only predict the residual ci,y,k and calculate the prediction of the overall score with (3). To make its prediction for the current residual of a student with feature vector xi,y,k, the algorithm finds all feature vectors from similar students of past years and their corresponding residuals ci,y,k. We define the similarity of students through their feature vectors. Two feature vectors xi,xj ∈ Xk are similar if 〈xi,xj〉k ≤ r where 〈., .〉k is a distance metric defined on the feature space Xk and r is a parameter. For two feature vectors x ∈ Xk1 and x′ ∈ Xk2 from different feature spaces (i.e. k1 6= k2) the distance metric is not defined since we only need to determine distances within a single feature space. Different feature spaces can have different definitions of the distance metric; we are going to define the distance metrics we use in Section IV-B. We define a neighborhood B (xc, r) with radius r of feature vector xc ∈ Xk as all feature vectors x ∈ Xk with 〈xc,x〉k ≤ r.\nLet Ck denote the random variable representing the residual score after performance assessment k. vk ( Ck|x ) denotes the probability distribution over the residual score for a student with feature vector x at time k and µk(x) denotes the student’s expected residual score. Let pk(x) denote the probability distribution of the students over the feature space Xk. Intuitively pk(x) is the fraction of students with feature vector x at time k. Note that the distributions vk ( Ck|x ) and pk(x) are not sampling distributions but unknown underlying distributions. We assume that the distributions do not change over the years.\nWe define the probability distribution of the students in a neighborhood B (xc, r) with center xc and radius r as\npkxc,r(x) := pk(x)∫\nx∈B(xc,r) dp k(x)\n1B(xc,r)(x),\nwhere 1 is the indicator function. Intuitively pkxc,r(x) is the fraction of students in neighborhood B(xc, r) with feature vector x. Let Ck(B(xc, r)) be the random variable representing the residual score of students in neighborhood B(xc, r) after having taken performance assessment k. The distribution of Ck(B(xc, r)) is given by\nfkxc,r ( Ck ) := ∫ x∈Xk vk(Ck|x)dpkxc,r(x)\nWe denote the true expected value of the residual scores after assignment k of students in a particular neighborhood by µk (xc, r) := E(Ck (B (xc, r))). Note that\nµk (xc, r) = Ex∼pkxc,r [ E [ Ck|x ]] = Ex∼pkxc,r [ µk (x) ] =\n∫ x∈Xk µk (x) dpkxc,r.\n5 Our estimation of the true expected residual of students within a particular neighborhood B(xi,y,k, r) is given by\nµ̂(Ck (B (xi,y,k, r))) =\n∑ x∈B(xi,y,k,r) cx,k\n|B (xi,y,k, r)| (5)\nwhere cx,k denotes the residual after time k of the student with feature vector x. For notational simplicity, we use µ̂k(xi,y,k, r) := µ̂(C\nk (B (xi,y,k, r))) to denote the estimated expectation. In the following we are going to derive how confident we are in the estimation of the residual score based on a given neighborhood B(x, r) and how we use this confidence q (B(x, r)) to both select the optimal radius of the neighborhood and to decide when to predict.\nIntuitively, if the feature vectors after performance assessment k in a neighborhood B(x, r) of x contain a lot of information about the residual cx,k, past students with feature vectors in this neighborhood should have had similar residuals. Hence, the variance of the residuals Var ( Ck(B(xi,y,k, r))\n) of the students in the neighborhood should be small. To mathematically support this intuition, we consider the residuals ci,y,k in a neighborhood B(x, r) of feature vector x with distribution fkxc,r ( Ck ) . For any confidence interval the probability that the absolute difference between the unknown residual cx,k of the student with feature vector x and the expected value of the residual distribution µk (x, r) in his/her neighborhood is smaller than can be bounded by P [ |Ck(B(x, r))− µk(x, r)| < ] > 1− V ar ( Ck(B(x, r)) ) 2 . (6) This statement directly follows from Chebyshev’s inequality.\nWe conclude that the lower the variance of the residual distribution in the neighborhood, the more confident we are that the true residual cx,k will be close to µk(x, r). Since both the expected value µk(x, r) and the variance Var ( Ck(B(x, r))\n) of the distribution are unknown, we estimate the two values through the sample mean from (5) and the sample variance V̂ ar ( Ck(B(x, r)) ) given by\nV̂ ar ( Ck(B(x, r)) ) =\n∑ x∈B(x,r) ( cx,k − µ̂k(x, r) )2 |B(x, r)| − 1 . (7)\nIn the following we use V ark(x, r) := Var ( Ck(B(x, r)) ) to denote the variance and V̂ ar k (x, r) := V̂ ar ( Ck(B(x, r))\n) to denote the sample variance of the residual distribution in neighborhood B(x, r). From the law of large number it follows that the sample mean and the sample variance converge to the true expected value and the true variance for |B(x, r)| → ∞. We will provide a bound for the probability that the prediction error is larger than a given value in the theorem below. Given a desired confidence interval , we define the confidence on the prediction of the residual as\nq (B(x, r)) = 1− V̂ ar k (x, r)\n2 . (8)\nUsing this confidence measure the radius of the optimal neighborhood after performance assessment k is given by r∗ =\nNeighborhood Confidence\n0.347\n0.502\n-0.092\n 1,iB x r\nStudent i with Unknown Residual\nStudent Database\n 2,iB x r  3,iB x r\n1r\n  ,iq B x r\n2r 3r\n  2,i thq B x r q\nWaitPredict\nYes No\nFig. 2. Illustration of the neighborhood selection process.\narg maxr q (B (xi,y,k, r)) = arg minr V̂ ar k\n(x, r). To estimate r∗ after each performance assessment k, our algorithm considers M different neighborhoods B(xi,y,k, rm),m = 1, . . . ,M with user-defined radii rm and chooses the best neighborhood m̂k(xi,y,k) according to our confidence measure m̂k(xi,y,k) = arg maxm q (B(xi,y,k, rm)). In the following we use m̂k := m̂k(xi,y,k) to denote the best neighborhood. Let\nĉi,y,k := µ̂ k (xi,y,k, rm̂k) (9)\ndenote the estimated residual of the best neighborhood at time k and ẑi,y,k denotes the corresponding estimated overall score\nẑi,y,k = ĉi,y,k + k∑ l=1 wlai,y,l. (10)\nIf the confidence bound for the best neighborhood qi,y(k) = q (B (xi,y,k, rm̂k)) is above a given threshold qi,y(k) ≥ qth, the algorithm returns the final prediction of the overall score ẑi,y = ẑi,y,k for the considered student.\nIf the confidence is below the threshold, we wait for the next performance assessment and start the next iteration. Fig. 2 illustrates the neighborhood selection process. Algorithm 1 provides a formal description of the grade prediction algorithm in pseudocode.\nTo conclude the discussion of the grade prediction algorithm in the regression setting, we derive a bound for the probability that the prediction error is larger than a value . Before we state the theorem, we introduce some further notations. Let m∗k(x) denote the index of the neighborhood with the smallest variance of residuals for the student with feature vector x at time k\nm∗k(x) = arg min 1≤m≤M V ark(x, rm). (11)\nNote that m∗k(x) is not necessarily equal to m̂k(x), the index of the neighborhood with the highest confidence chosen by our algorithm, since the confidence defined in (8) is calculated with the known sample variance of residuals V̂ ar(x, r) and not with the unknown true variance V ark(x, r) used in (11).\n6 Algorithm 1 Grade Prediction Algorithm, Regression Setting Input: All x and z from past years, qth, number M and radii\nr1, . . . , rM of neighborhoods Output: Predictions ẑ for the overall scores of the students\n1: for all years y do 2: for all performance assessments k do 3: for all current-year students i for whom the final prediction has not been made do 4: if k = K then 5: Calculate zi,y according to (1) 6: Return zi,y as final prediction for student i 7: end if 8: Create M neighborhoods with radii r1, . . . , rM 9: for all neighborhoods m do\n10: Estimate residual ĉ (B (xi,y,k, rm)) with (5) 11: Compute V̂ ar (x, rm) with (7) 12: Compute q (B(xi,y,k, rm)) with (8) 13: end for 14: Find m̂k = arg maxm q (B (xi,y,k, rm)) 15: if q (B (xi,y,k, rm̂k)) ≥ qth then 16: Compute ẑi,y with (3) 17: Return ẑi,y as final prediction for student i 18: end if 19: Add xi,y,k and ai,y,k to database 20: end for 21: end for 22: Calculate all ci,y,k of year y according to (2) 23: Add all ci,y,k to database 24: end for\nSimilarly m∗k,2(x) denotes the index of the neighborhood with the second highest confidence.\nm∗k,2(x) = arg min 1≤m≤M,m6=m∗k(x) V ark(x, rm).\nLet ∆k(x) denote the difference between the standard deviations of the residual distribution of neighborhoods m∗k(x) and m∗k,2(x)\n∆k(x) = √ V ark(x, rm∗k,2)− √ V ark(x, rm∗k). (12)\nTheorem. Without loss of generality we assume that all scores a are normalized to the range [0, 1]. Consider the prediction ẑi,y,k of the overall score of yth year student i with feature vector x made by algorithm 1. The probability that the absolute error the prediction exceeds is bounded by\nP [|zi,y − ẑi,y,k| ≥ ] ≤ 4V ark\n( x, rm∗k(x) ) 2\n+ 2 exp [ − 2 min\n1≤m≤M |B (x, rm)| 2 ] + 2M exp [ −∆k(x)2 min\n1≤m≤M |B(x, rm)| − 1 8\n]\nProof: See Appendix. This theorem illustrates two important aspects of algorithm 1. First, we see that for a given neighborhood the accuracy\nof our predictions increases with an increasing number of neighbors. Hence, our algorithm learns the best predictions online as the knowledge base is expanded after each year, when the feature vectors and results from the past-year students are added to the database. In Section IV-D1 we show that this learning can be experimentally illustrated with our data from the digital signal processing course taught at UCLA. Second, the term V ark(x, rm∗k)/\n2 shows that the prediction accuracy will be higher if the variance of the residuals in a neighborhood is small. With increasing time k we expect this variance to decrease since we have more information about the students and we expect the students in a neighborhood to be more similar and achieve similar (residual) scores.\nNote that it is possible to restrict the data kept in the knowledge base to recent years, which allows the algorithm to adapt faster to slowly changing students and to changes in the course."
    }, {
      "heading" : "D. Grade Prediction Algorithm, Classification Setting",
      "text" : "In the binary classification setting we predict the overall score analogously to the regression setting and then determine the class by comparing the predicted overall score ẑi,y with a threshold score zth. To illustrate how we find zth let us assume that we want to predict whether a student does well (letter grades ≥ B−) or does poorly (letter grades ≤ C+). To determine zth, we find the average zavg,B− of all students from past years who received a B− and the average zavg,C+ of all students from past years who achieved a C+. Subsequently, we define zth as zth = (zavg,B− + zavg,C+) /2. The predicted classification b̂i,y of yth year student i is then given by\nb̂i,y = { 0 ẑi,y ≥ zth 1 ẑi,y < zth.\n(13)\nWe are more confident in the classification not only if the variance of the neighbor-scores is small, which is the metric we used for the confidence in the regression setting, but also if the distance d (B(x, r)) = |ẑ (B(x, r))− zth| between the predicted score and the threshold score is large. Note that ẑ (B(x, r)) is the estimate of the overall score based on neighborhood B(x, r). Because of this intuition we use a modified confidence\nqbin (B(x, r)) = 1− e−d(B(x,r)) V̂ ar\n( Ck(B(x, r)) ) 2 (14)\nto decide whether to make the final prediction in binary classification settings. Since d (B(x, r)) should only influence whether the final prediction is made for a given neighborhood but not the neighborhood selection process, we still use the unmodified confidence from (8) to select the optimal neighborhood.\nIn summary, four changes have to be made to algorithm 1 to make it applicable to binary classification settings. First, zth has to be determined/updated at the beginning of each new year. Second, we calculate b̂i,y after line 16 according to (13). Third, we return b̂i,y at line 17 instead of ẑi,y . Fourth, we use the modified confidence qbin according to (14) in line 15\n7 instead of the unmodified confidence q. We use the unmodified confidence from (8) in line 14.\nThe described binary classification algorithm can easily be generalized to a larger number of categories. In a classification with L categories, we define L − 1 threshold values zth,1 < zth,2 < . . . < zth,L−1 and determine in which of the L score intervals {[0, zth,1), [zth,1, zth,2), . . . , [zth,L−1, 1]} the predicted overall score ẑi,y of a student lies. The index of the interval corresponds to the classification of the student. In this general classification setting, the modified confidence from (14) can be used as well by defining d as the distance of ẑi,y to the nearest threshold value.\nWe discuss the performance of the proposed algorithm 1 in both regression and classification settings in Section IV-D."
    }, {
      "heading" : "E. Confidence-Learning Prediction Algorithm",
      "text" : "Besides the radii of the neighborhoods ri, the only parameter to be chosen by the user in algorithm 1 is the desired confidence threshold qth. Since for an instructor it is more natural and practical to specify a desired prediction accuracy or error directly rather than the confidence threshold, we show in this section how to automatically learn the appropriate confidence threshold to achieve a certain prediction performance and what consequences this has on the average prediction time. We will discuss a possible way of choosing the radii ri of the neighborhoods in Section IV-B.\nFormally we define the problem as follows. Let p(k, qth) ∈ [0, 1] denote the proportion of current year students for which the grade prediction algorithm working with confidence threshold qth ≤ 1 has predicted the overall score by time (performance assessment) k ∈ [0,K]. pmin is the minimum percentage of current year students whose grade the user wants to predict with a specified accuracy. E(k, qth) ≥ 0 denotes the average absolute prediction error up to time k for the given confidence. Emax is the maximum error the user is willing to tolerate. k(p, qth) is the time necessary to predict the grade for proportion p of all students of the class using confidence threshold qth. Please note that since the variables p, E, qth and k are dependent, we can only independently specify two of the four variables. If we for example specify to predict all (p = 1) students with zero error (E = 0), the algorithm will have to wait until the end of the course when the overall score is known (k = K) and will use maximum confidence (qth = 1). Without making any assumptions on the dependence of the variables of each other, multiple pairs (k, qth) might lead to the same specified pair (p,E).\nOur goal is, therefore, to learn from past data the yth year estimate of the minimal time ky and corresponding confidence threshold qth,y necessary to achieve the desired share pmin of students predicted and the desired maximum average prediction error Emax. This is formally defined as:\nminimize qth k(p, qth) subject to p(k, qth) ≥ pmin E(k, qth) ≤ Emax\n(15)\nNote that while the goal of optimization problem (4) is to find the minimum time to predict the overall score of a particular\nAlgorithm 2 Confidence-Learning Prediction Algorithm Input: Emax, pmin, qth,0, all x and z from past years, number\nM and radii r1, . . . , rM of neighborhoods Output: Predictions ẑ, ky and qth,y for all years\n1: for all years y do 2: if y > 1 then 3: Find ky−1 and qth,y−1 according (15) by running algorithm 1 with various qth 4: Return ky−1 and qth,y−1 5: end if 6: Use algorithm 1 with qth = qth,y−1 to predict and return the grades of current year y students 7: end for\nstudent with a desired confidence, this problem (15) aims at finding the minimum time by which the overall scores of a specific percentage of all students can be predicted with a desired maximum error.\nAt a given year y we solve this optimization problem using a brute force approach using the all available data from years 1, . . . , y. For this purpose, we extract k, p and E from algorithm 2 for a large number of different confidence thresholds qth. We then select the confidence threshold qth,y which is optimal with respect to optimization problem (15) and determine the corresponding prediction time ky . To make the grade predictions for year y + 1 we use the learned confidence threshold qth,y as input to prediction algorithm 1. Since there are no training data available yet at year y = 1, the algorithm uses a user-defined starting value qth,0 for the grade predictions of the first year. Algorithm 2 summarizes the learning algorithm in pseudocode."
    }, {
      "heading" : "IV. EXPERIMENTS",
      "text" : "In this section, we present the data, discuss details of the application of algorithm 1 to our dataset, illustrate the functioning of the algorithm and evaluate its performance by comparing it against other prediction methods in both regression and binary classification settings. Due to space limitations, we will not show experimental results for classification settings with more than two categories."
    }, {
      "heading" : "A. Data Analysis",
      "text" : "Our experiments are based on a dataset from an undergraduate digital signal processing course (EE113) taught at UCLA over the past 7 years. The dataset contains the scores from all performance assessments of all students and their final letter grades. The number of students enrolled in the course for a given year varied between 30 and 156, in total the dataset contains the scores of approximately 700 students. Each year the course consists of 7 homework assignments, one in-class midterm exam taking place after the third homework assignment, one course project that has to be handed in after homework 7 and the final exam. The duration of the course is 10 weeks and in each week one performance assessments takes place. The weights of the performance assessments are given by: 20% homework assignments with equal weight on\n8 each assignment, 25% midterm exam, 15% course project and 40% final exam.3 Fig. 3a shows the distribution of the letter grades assigned over the 7 years. We observe that on average B is the grade the instructor assigned most frequently. A was assigned second most and C third most frequently. Surprisingly, however, the distribution varies drastically over the years; in year 1 for example only 18.75% received a B while in year 6 the frequency was 38.9%.\nTo understand the predictive power of the scores in different performance assessments, Fig. 3b shows the sample Pearson correlation coefficient between all performance assessments and the overall score. We make several important observations from this graph. First, on average the final exam has the strongest correlation to the overall score, followed by the midterm exam. This is not surprising, since the final contributes 40% and the midterm contributes 25% to the overall score. Second, the score from the course project on average does not have a higher correlation with the overall score than the homework assignments despite the fact that it accounts for 15% of the overall score. Third, all homework assignments have similar correlation coefficients. Fourth, the correlation between the individual performance assessments and the overall score varies greatly over the years. This indicates that predicting student scores based on training data from past years might be difficult.\nSince all performance assessments are part of the overall score and, therefore, a high correlation is expected, it is also informative to consider the correlation between the performance assessments and the final exam shown in Fig. 3c. It is interesting to observe that still the midterm exam shows, besides the overall score, the highest correlation with the final exam. A possible explanation for this is that both the midterm and final are in-class exams while the other performance assessments are take-home."
    }, {
      "heading" : "B. Our Algorithm",
      "text" : "In this section we discuss three important details of the application of algorithm 1 to the dataset from the undergraduate digital signal processing course.\nFirst, the rule we use to normalize all scores ai,y,k in our dataset is given by\nai,y,k = a∗i,y,k − µ̂y,k\nσ̂y , (16)\nwhere a∗i,y,k is the original score of the student, µ̂y,k is the sample mean of all yth year student’s original scores in performance assessment k and σ̂y is the standard deviation of all yth year student’s original overall scores. A normalization of the scores is needed for several reasons. First, the instructor-defined maximum score in a particular performance assessment may differ greatly across years and since we use data from past years to predict the performance of students in a given year, we need to make the data across years comparable. Second, also the difficulty of individual\n3As we explain in footnote 2 and show in section IV-D2, our algorithm can also be applied to settings where the number and weights of performance assessments change over the years.\nperformance assessments might be different across years, homework 2 might for instance be very easy in year 2 so that almost everyone achieves the maximum score and very difficult in year 3 so that few achieve half of the maximum score. The normalization according to (16) eliminates this bias by transforming the absolute scores of a student to scores relative to his/her classmates of the same year. Note that algorithm 1 does not require a specific normalization and it does not matter that the normalized scores according to (16) will not be in the interval [0, 1] as assumed in Section III for simplicity.\nSecond, we use feature vectors that simply contain the scores of all performance assessments student i has taken up to time k in the order they occurred xi,y,k = (ai,y,1, . . . , ai,y,k). To incorporate the fact that students who have performed similarly in a performance assessment with a lot of weight should be nearer to each other in the feature space than students that have had similar scores in a performance assessment (e.g. homework assignment) with low weight, we use a weighted metric to calculate the distance between two feature vectors. We define the distance of two feature vectors xi,xj ∈ Xk as\n〈xi,xj〉k = ∑k\nl=1 wl |xi,l − xj,l|∑k l=1 wl , (17)\nwhere k is the length of the feature vectors, wl is the weight of performance assessment l and xi,l denotes entry l of feature vector xi.\nThird, rather than specifying the radii of the neighborhoods to consider as an input, as suggested in the pseudocode of algorithm 1, we automatically adapt the radii of the neighborhoods such that they contain a certain number of neighbors. Since the sample variance gets more accurate with an increasing number of samples, we refrain from considering neighborhoods with only 2 neighbors. Therefore, the smallest radius considered r1 is the minimal radius such that the neighborhood includes 3 neighbors. For subsequent neighborhoods the minimal radius is chosen such that the neighborhood includes at least one neighbor more than the previous neighborhood. Formally, we define the selection of the radii recursively as\nr1 = min r, s.t. |B(xi,y,k, r)| ≥ 3 rm+1 = min r, s.t. |B(xi,y,k, r)| > |B(xi,y,k, rm)|. (18)"
    }, {
      "heading" : "C. Benchmarks",
      "text" : "We compare the performance of our algorithm against five different prediction methods. • We use the score ai,y,k student i has achieved in the\nmost recent performance assessment k alone to predict the overall grade. • A second simple benchmark makes the prediction based on the scores ai,y,1, . . . , ai,y,k student i has achieved up to performance assessment k taking into account the corresponding weights of the performance assessments. • Linear regression using the ordinary least squares (OLS) finds the least squares optimal linear mapping between the scores of first k performance assessments and the overall score.\n9 F D− D D+ C− C C+ B− B B+ A− A A+ 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4\nGrades\nF re\nqu en\ncy\nAll Years Year 1 Year 2 Year 3 Year 4 Year 5 Year 6 Year 7\n(a) Grade distribution\nH1 H2 H3 M H4 H5 H6 H7 P F 0\n0.2\n0.4\n0.6\n0.8\n1\nPerformance Assessments\nS am\npl e\nP ea\nrs on\nC or\nre la\ntio n\nC oe\nffi ci\nen t r\nAll Years Year 1 Year 2 Year 3 Year 4 Year 5 Year 6 Year 7\n(b) Correlation coefficients overall score\nH1 H2 H3 M H4 H5 H6 H7 P O 0\n0.2\n0.4\n0.6\n0.8\n1\nPerformance Assessments\nS am\npl e\nP ea\nrs on\nC or\nre la\ntio n\nC oe\nffi ci\nen t r\nAll Years Year 1 Year 2 Year 3 Year 4 Year 5 Year 6 Year 7\n(c) Correlation coefficients final score\nFig. 3. Data analysis: 3a shows the distribution of letter grades for all years. 3b and 3c present the sample Pearson correlation coefficient between individual performance assessments and the overall (3b) or final exam (3c) score. Note that we use the abbreviations Hi (homework assignment i), M (midterm exam), F (final exam) and O (overall score) in the figures.\nThe advantage of the method we use in our algorithm over linear and logistic regression is that being a nearest neighbor method, it is able to recognize certain patterns such as trends in the data that are missed in linear/logistic regression where a single parameter per performance assessment has to fit all students. In contrast, our algorithm is able to detect such patterns if there have been students in the past who have shown similar patterns.\nTable II illustrates this through a case study extracted from the UCLA undergraduate digital signal processing course data. We present cases from a simulation where we predicted whether students are going to do well (letter grade ≥ B−) or do poorly (letter grade ≤ C+) and consider the students for which our algorithm decided to predict after the midterm exam. The table shows 3 students whom logistic regression classified wrongly while our algorithm made the accurate prediction. In columns 2-4 we present the scores the students achieved up to the midterm exam and the last column shows the true classification of the students. These cases are typical examples of settings where our algorithm outperforms logistic regression. Student 1 and 2 both showed a good performance in homework assignment 1. However, in later assignments and especially at the midterm exam their performance successively deteriorated, an indication that the students might do poorly the class if they or the instructor and teaching assistants do not take corrective actions. Our algorithm is likely to have learned such patterns from past data and predicts the students to do poorly. On average, however, their performance in the\nfirst four performance assessments is still about average and, therefore, logistic regression predicts that the students will do well. For student 3 the situation is the other way around."
    }, {
      "heading" : "D. Results",
      "text" : "In this section we evaluate the performance of our algorithm 1 in different settings and compared to benchmarks in both regression and the classification tasks.\nAs a performance measure in the regression setting, we use the average of the absolute values of the prediction errors E. Since we normalized the overall score to have zero mean and a standard deviation of 1, E directly corresponds to the number of standard deviations the predictions on average are away from the true values. The overall performance measure in classification settings is the accuracy of the classification. Furthermore, we use the quantities precision, recall and false positive/negative rate besides accuracy to measure performance. Please note that positive in our case means that the student does poorly.\n1) Performance Comparison with Benchmarks in Regression Setting: Having discussed the various performance measures, we first address the regression setting. Fig. 4 visualizes the performance of the algorithm we presented in Section III-C and of benchmark methods. We generated Fig. 4 by predicting the overall scores of all students from years 2−7. To make the prediction for year y, we used the entire data from years 1 to y − 1 to learn from. Unlike our algorithms, the benchmark methods do not provide conditions to decide after which performance assessment the decision should be made. Therefore, for benchmark methods we specified the prediction time (performance assessment) k for an entire simulation and repeated the experiment for all k = 1, . . . , 10; the results are plotted in Fig. 4. To generate the curve of our algorithm 1, we ran simulations using different confidence thresholds qth and for each threshold we determined E and the performance assessment (time) k̄ after which the prediction was made on average.\nIrrespective of the prediction method, Fig. 4 shows the trade-off between timeliness and accuracy; the later we predict the more accurate our prediction gets. From the curve\n10\nfor the prediction using a single performance assessment we infer that there is a low correlation between homework assignments/course project and the overall score and a high correlation between the in-class assessments (midterm and final exam) and the overall score. This observation is congruent with the correlation analysis from Section IV-A. If the prediction is made early, before the midterm, all methods (except the prediction using a single performance assessment) lead to similar prediction errors. We observe that while the error decreases approximately linearly for our algorithm, the performance of benchmark methods steeply increases after the midterm and the final but stays approximately constant during the rest of the time. The reason for this is that we obtained the points of the curve for our algorithm by averaging the prediction time of all students. Therefore, the point of the curve above the midterm was not generated by predicting after the midterm for all students; some predictions were made earlier, some later. If on average the prediction is made after homework 4, our algorithm shows a significantly smaller error E than benchmark methods outperforming linear regression by up to 65%.\n2) Learning across Years and Instructors in Regression Setting: Consider Fig. 5 demonstrating the performance increase of our algorithm when more data to learn from become available. To generate the figure, we used our algorithm to predict the overall scores of all 7th year students for different confidence thresholds. The curves in dashed lines stem from simulations using only one of the years 1-5 as training data and the solid magenta curve uses all years 1-5 to learn from. We observe that the prediction performance strongly depends on the training data and differs if different years are used. Most importantly, the performance is highest irrespective of the average prediction time if the combination of the data from all 5 years is used. This shows that our algorithm is able to learn and improves its predictions over time.\nThe undergraduate digital signal processing course is taught twice a year by three different instructors at UCLA. While we used only the data from one instructor in the previous plots, Fig. 6 investigates the situation when we predict the grades for a class of instructor 1 based exclusively on past data from a different instructor 2. In practice this happens when a new\ninstructor takes over a course previously taught by someone else. It is interesting to see whether our grade prediction still works well in this setting. A good performance is not selfevident for several reasons. Different instructors might set a different focus concerning the knowledge imparted, they might use a different textbook and they might prefer different styles of homework assignments and in-class exams. Furthermore, the structure of the course, e.g. the number and sequence of homework assignments, the time when the midterm exam takes place, the weights of performance assessments and whether a course project and quizzes exist, might change drastically. To generate Fig. 6, we predicted the overall score for the year 7 class of instructor 1 based on two different sets of previous data. The solid blue curve was generated by using the data from the classes in years 1-5 from the same instructor 1 as training data. To obtain the dashed red curve, we used the data from classes in years 1-5 from instructor 2 to learn from. While the predictions using training data from the same instructor are slightly more accurate, the performance with training data from a different instructor is still very satisfying, showing a good robustness of our algorithm with respect to different instructors. For the subsequent results we again exclusively use data from one instructor.\n11\n3) Performance Comparison with Course Containing Early Quizzes in Regression Setting: The results in both the data analysis section (Fig. 3b) and Section IV-D1 (Fig. 4) indicate that scores in in-class exams are much better predictors of the overall score than homework assignments. To verify this, we consider two consecutive years of the UCLA course EE103, which contains four in-class quizzes in course weeks 2, 4, 6 and 8 instead of a midterm. Fig. 7 visualizes that, starting from the first quiz in week 2, indeed our algorithm is able to predict the same percentage of the students with an up to 22% smaller cumulative average prediction error by a certain week. We generated Fig. 7 by using algorithm 1 to predict for both courses the overall scores of the students in a particular year based on data from the previous year. Note that for the course with quizzes, the increase in the share of students predicted is larger in weeks that contain quizzes than in weeks without quizzes. This supports the thesis that quizzes are good predictors as well.\nAccording to this result, it is desirable to design courses with early in-class exams. This enables a timely and accurate grade prediction based on which the instructor can intervene if necessary.\n4) Performance Comparison with Benchmarks in Classification Setting: The performances in the binary classification settings are summarized in Fig. 8. Since logistic regression turns out to be the most challenging benchmark in the classification setting, we do not show the performance of the other benchmark algorithms for the sake of clarity. The goal was to predict whether a student is going to do well, still defined as letter grades equal to or above B−, or do poorly, defined as letter grades equal to or below C+. Again, to generate the curves for the benchmark method, logistic regression, we specified manually when to predict. For our algorithm we again averaged the prediction times of an entire simulation and varied qth to obtain different points of the curve. Up to homework 4, the performance of the two algorithms is very similar, both showing a high prediction accuracy even\nwith few performance assessments. Starting from homework 4, our algorithm performs significantly better, with an especially drastic improvement of recall. It is interesting to see that even with full information, the algorithms do not achieve a 100% prediction accuracy. The reason for this is that the instructor did not use a strict mapping between overall score and letter grade and the range of overall scores that lead to a particular letter grade changed slightly over the years.\n5) Decision Time and Accuracy in Classification Setting: To better understand when our algorithm makes decisions and with what accuracy, consider Fig. 9. We again investigate binary do well/poorly classifications as discussed above. The red curve shows (square markers) for what share of the total number of students the algorithm makes the prediction by a specific point in time. The remaining curves show different measures of cumulative performance. We can for example see that by the midterm exam we classify 85% of the students with an accuracy of 76%. These timely predictions are desirable since the earlier the prediction is made the more time an instructor has to take corrective action. The cumulative accuracy stays almost constant around 80% irrespective of the prediction time. We believe that the reason for this is that thanks to the confidence threshold, the easy decisions are made early and harder decisions are made later. Consequently,\n12\nthe expected accuracy of all predictions remains more or less constant irrespective of the prediction time."
    }, {
      "heading" : "V. CONCLUSION",
      "text" : "In this paper we develop an algorithm that allows for a timely and personalized prediction of the final grades of students exclusively based on their scores in early performance assessment such as homework assignments, quizzes or midterm exams. Using data from an undergraduate digital signal processing course taught at UCLA, we show that the algorithm is able to learn from past data, that it outperforms benchmark algorithms with regard to accuracy and timeliness both in classification and regression settings and that the predictions are robust even when the course is taught by different instructors.\nWe show that in-class exams are better predictors of the overall performance of a student than homework assignments. Hence, designing courses to have early in-class evaluations enables timely identification of students who, with a high probability, would do poorly without intervention and enables remedial actions to be adopted at an early stage.\nOur algorithm can easily be generalized to include context data from students such as their prior GPA or demographic data. If applied exclusively to MOOCs, the in-course data used for the predictions could be extended for example by the responses of students to multiple-choice questions, their forum activity, the course material they studied or the time they spent studying online. Another direction of future work is to apply our algorithm in practice and investigate to what extent the performance of students can be improved by a timely intervention based on the grade predictions. In this context, our algorithm could be extended to make multiple predictions for each student to monitor the trend in the predicted grade after an intervention.\nAPPENDIX In this Appendix, we proof the theorem from section III-C. Before we start with the proof, we discuss some preliminary results.\nFact 1. (Chernoff-Hoeffding Bound) Let X1, X2, . . . , Xn be independent and bounded random variables with range [0, 1] and expected value µ. Let µ̂n = (X1 + . . . + Xn)/n denote the sample mean of the random variables. Then, for all > 0\nP [|µ̂n − µ| ≥ ] ≤ 2 exp [ −2n 2 ] .\nProof: A proof of Fact 1 can be found in Hoeffding’s paper [25].\nFact 2. (Empirical Bernstein Bound) Let n ≥ 2 and X1, X2, . . . , Xn be independent and bounded random random variables with range [0, 1] and variance V ar. µ̂n denotes the n-sample mean µ̂n = 1n ∑n i=1Xi and V̂ arn denotes the n-\nsample variance V̂ arn = 1n−1 ∑N i=1 (xi − µ̂n) 2. Then, the following inequality bounds the probability that the error of the sample standard deviation, which is the square root of the sample variance, is larger than a given value\nP [∣∣∣∣√V̂ arn −√V ar∣∣∣∣ ≥ ] ≤ 2 exp [−n− 12 2 ]\ncan be derived.\nProof: See [26] for a proof of Fact 2.\nLemma 1. Let m̂k(x) denote the index of the neighborhood selected by our algorithm for the student with feature vector x at time k and m∗k(x) is given by (11). M denotes the total number of neighborhoods our algorithm considers and ∆k(x) is given by (12). We can bound the probability that our algorithm chooses the wrong neighborhood by\nP [m̂k(x) 6= m∗k(x)] ≤ 2e −∆k(x)2 min 1≤m≤M |B(x,rm)|−1 8 . (19)\nProof: Consider:\nP [m̂k(x) 6= m∗k(x)]\n= P [ arg min 1≤m≤M V̂ ar k (x, rm) 6= m∗k(x) ] = P\n[ arg min 1≤m≤M √ V̂ ar k (x, rm) 6= m∗k(x) ] .\nIf the estimation error of the standard deviation is smaller than ∆k(x)/2 for all neighborhoods∣∣∣∣√V̂ ark(x, rm)−√V ark(x, rm)∣∣∣∣ ≤ ∆k(x)2 , our algorithm chooses the optimal neighborhood m∗k(x). Therefore, we get\nP [ arg min 1≤m≤M √ V̂ ar k (x, rm) 6= m∗k(x) ]\n≤ P  ⋃ 1≤m≤M {∣∣∣∣√V̂ ark(x, rm) − √ V ark(x, rm) ∣∣∣∣ ≥ ∆k(x)2 }]\n(a) ≤ M∑\nm=1\nP [∣∣∣∣√V̂ ark(x, rm)−√V ark(x, rm)∣∣∣∣ ≥ ∆k(x)2 ]\n(b) ≤ M∑\nm=1\n2 exp [ −∆k(x)2\n|B(x, rm)| − 1 8 ] ≤ 2M exp [ −∆k(x)2 min\n1≤m≤M |B(x, rm)| − 1 8 ] where (a) is the union bound and (b) follows from Fact 2.\nProof of Theorem: Note that\n|zi,y − ẑi,y,k|\n(a) = ∣∣∣∣∣ci,y,k + k∑\nl=1\nwlai,y,l − ( ĉi,y,k +\nk∑ l=1 wlai,y,l )∣∣∣∣∣ = |ci,y,k − ĉi,y,k|\nwhere (a) follows from equations (3) and (10). There are three sources of error in the prediction of an overall score of algorithm 1: 1) The wrong neighborhood size may be selected due to\ninaccurate approximations of the true residual score variances of the neighborhoods through the sample variance.\n13\n2) If the optimal neighborhood is selected, the sample mean of the residual scores in the neighborhood may not be a good approximation of their true mean. 3) Even if the optimal neighborhood is selected and the sample mean equals the true mean, the residual score of the considered student may be different from the mean of the residual score distribution.\nIn the following we separate these three error sources and derive a bound for each one.\nWe have\nP [|zi,y − ẑi,y,k| ≥ ] = P [|ci,y,k − ĉi,y,k| ≥ ] (b) =P [∣∣ci,y,k − µ̂k (x, rm̂k(x))∣∣ ≥ ] (c) =P\n[∣∣ci,y,k − µ̂k (x, rm̂k(x))∣∣ ≥ , m̂k(x) = m∗k(x)] + P\n[∣∣ci,y,k − µ̂k (x, rm̂k(x))∣∣ ≥ , m̂k(x) 6= m∗k(x)] (d)\n≤P [∣∣∣ci,y,k − µ̂k (x, rm∗k(x))∣∣∣ ≥ , m̂k(x) = m∗k(x)]\n+ P [m̂k(x) 6= m∗k(x)] (e) ≤P [∣∣∣ci,y,k − µ̂k (x, rm∗k(x))∣∣∣ ≥ ]+ P [m̂k(x) 6= m∗k(x)]\nwhere (b) follows from (9), (c) is the law of total probability and (d) and (e) both follow from the fact that P [A,B] ≤ P [A]. Lemma 1 provides a bound for the second term. Therefore, we focus on the first term\nP [∣∣∣ci,y,k − µ̂k (x, rm∗k(x))∣∣∣ ≥ ]\n= P [∣∣∣ci,y,k − µk (x, rm∗k(x)) +µk (x, rm∗k(x))\n−µ̂k ( x, rm∗k(x) )∣∣∣ ≥ ] (f)\n≤ P [∣∣∣ci,y,k − µk (x, rm∗k(x))∣∣∣ ≥ 2]\n+ P [∣∣∣µk (x, rm∗k(x))− µ̂k (x, rm∗k(x))∣∣∣ ≥ 2]\n(g) ≤ 4V ark\n( x, rm∗k(x) ) 2 + 2 exp [ − 2\n2\n∣∣∣B (x, rm∗k(x))∣∣∣]\n≤ 4V ark\n( x, rm∗k(x) ) 2 + 2 exp [ − 2 min\n1≤m≤M |B (x, rm)| 2 ] where (f ) follows from the triangle inequality, the fact that P [X + Y ≥ x0 + y0] ≤ P [{X ≥ x0} ∪ {Y ≥ y0}] and the union bound. The bound for the first term in step (g) follows from Chebyshev’s inequality and the bound for the second term follows from the Chernoff-Hoeffding Bound from Fact 1.\nIncluding the second term again and using Lemma 1 we get\nP [|zi,y − ẑi,y,k| ≥ ] = P [|ci,y,k − ĉi,y,k| ≥ ] ≤ P [∣∣∣ci,y,k − µ̂k (x, rm∗k(x))∣∣∣ ≥ ]+ P [m̂k(x) 6= m∗k(x)]\n≤ 4V ark\n( x, rm∗k(x) ) 2 + 2 exp [ − 2 min\n1≤m≤M |B (x, rm)| 2 ] + 2M exp [ −∆k(x)2 min\n1≤m≤M |B(x, rm)| − 1 8\n] ,\nwhich concludes the proof."
    } ],
    "references" : [ {
      "title" : "Open education: New opportunities for signal processing",
      "author" : [ "R. Baraniuk" ],
      "venue" : "Plenary Speech, 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2015.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "etutor: Online learning for personalized education",
      "author" : [ "C. Tekin", "J. Braun", "M. van der Schaar" ],
      "venue" : "Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2015.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "l  {q} sparsity penalized linear regression with cyclic descent",
      "author" : [ "G. Marjanovic", "V. Solo" ],
      "venue" : "IEEE Transactions on Signal Processing, vol. 62, no. 6, pp. 1464–1475, 2014.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Nearest-neighbor distributed learning by ordered transmissions",
      "author" : [ "S. Marano", "V. Matta", "P. Willett" ],
      "venue" : "IEEE Transactions on Signal Processing, vol. 61, no. 21, pp. 5217–5230, 2013.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Distributed sparse linear regression",
      "author" : [ "G. Mateos", "J.A. Bazerque", "G.B. Giannakis" ],
      "venue" : "IEEE Transactions on Signal Processing, vol. 58, no. 10, pp. 5262–5276, 2010.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Standardized tests predict graduate students success",
      "author" : [ "N.R. Kuncel", "S.A. Hezlett" ],
      "venue" : "Science, vol. 315, pp. 1080–1081, 2007.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Determinants of undergraduate gpas: Sat scores, high-school gpa and high-school rank",
      "author" : [ "E. Cohn", "S. Cohn", "D.C. Balch", "J. Bradley" ],
      "venue" : "Economics of Education Review, vol. 23, no. 6, pp. 577–586, 2004.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Validity of the medical college admission test for predicting medical school performance",
      "author" : [ "E.R. Julian" ],
      "venue" : "Academic Medicine, vol. 80, no. 10, pp. 910–917, 2005.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Using an admissions exam to predict student success in an adn program",
      "author" : [ "P.A. Gallagher", "C. Bomba", "L.R. Crane" ],
      "venue" : "Nurse Educator, vol. 26, no. 3, pp. 132–135, 2001.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Comparative study of artificial neural network and statistical models for predicting student grade point averages",
      "author" : [ "W.L. Gorr", "D. Nagin", "J. Szczypula" ],
      "venue" : "International Journal of Forecasting, vol. 10, no. 1, pp. 17– 34, 1994.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "A comparative analysis of techniques for predicting academic performance",
      "author" : [ "N.T. Nghe", "P. Janecek", "P. Haddawy" ],
      "venue" : "Frontiers In Education Conference-Global Engineering: Knowledge Without Borders, Opportunities Without Passports, 2007. FIE’07. 37th Annual. IEEE, 2007, pp. T2G–7.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Why college grade point average is difficult to predict.",
      "author" : [ "R.D. Goldman", "R.E. Slaughter" ],
      "venue" : "Journal of Educational Psychology,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1976
    }, {
      "title" : "Using data mining to predict secondary school student performance",
      "author" : [ "P. Cortez", "A.M.G. Silva" ],
      "venue" : "2008.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Predicting student performance in a beginning computer science",
      "author" : [ "L.H. Werth" ],
      "venue" : "class. ACM,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1986
    }, {
      "title" : "Factors associated with grades in intermediate accounting",
      "author" : [ "J.L. Turner", "S.A. Holmes", "C.E. Wiggins" ],
      "venue" : "Journal of Accounting Education, vol. 15, no. 2, pp. 269–288, 1997.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Predictors of web-student performance: The role of self-efficacy and reasons for taking an on-line class",
      "author" : [ "A.Y. Wang", "M.H. Newlin" ],
      "venue" : "Computers in Human Behavior, vol. 18, no. 2, pp. 151–163, 2002.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Predicting students’performance in distance learning using machine learning techniques",
      "author" : [ "S. Kotsiantis", "C. Pierrakeas", "P. Pintelas" ],
      "venue" : "Applied Artificial Intelligence, vol. 18, no. 5, pp. 411–426, 2004.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Mooc performance prediction via clickstream data and social learning networks",
      "author" : [ "C.G. Brinton", "M. Chiang" ],
      "venue" : "34th INFOCOM IEEE. 2015, To appear.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Predicting student performance: an application of data mining methods with an educational web-based system",
      "author" : [ "B. Minaei-Bidgoli", "D.A. Kashy", "G. Kortemeyer", "W.F. Punch" ],
      "venue" : "Frontiers in education, 2003. FIE 2003 33rd annual, vol. 1. IEEE, 2003, pp. T2A–13.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "A promising classification method for predicting distance students performance.",
      "author" : [ "D. Garcıa-Saiz", "M. Zorrilla" ],
      "venue" : "EDM, pp",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2012
    }, {
      "title" : "Incentive design in peer review: Rating and repeated endogenous matching",
      "author" : [ "Y. Xiao", "F. Dörfler", "M. van der Schaar" ],
      "venue" : "Allerton Conference, 2014.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Probability inequalities for sums of bounded random variables",
      "author" : [ "W. Hoeffding" ],
      "venue" : "Journal of the American statistical association, vol. 58, no. 301, pp. 13–30, 1963.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 1963
    }, {
      "title" : "Empirical bernstein bounds and sample variance penalization",
      "author" : [ "A. Maurer", "M. Pontil" ],
      "venue" : "Proceedings of the Int. Conference on Learning Theory, 2009.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : ") and is developed by a large number of contributors rather than by a single author [1].",
      "startOffset" : 84,
      "endOffset" : 87
    }, {
      "referenceID" : 1,
      "context" : "Remedial or promotional actions could consist of additional online study material presented to the student in a personalized and/or automated manner [4].",
      "startOffset" : 149,
      "endOffset" : 152
    }, {
      "referenceID" : 2,
      "context" : "As benchmark algorithms we use well known algorithms such as linear/logistic regression and kNearest Neighbors, which are still a current research topic [5]–[7].",
      "startOffset" : 153,
      "endOffset" : 156
    }, {
      "referenceID" : 4,
      "context" : "As benchmark algorithms we use well known algorithms such as linear/logistic regression and kNearest Neighbors, which are still a current research topic [5]–[7].",
      "startOffset" : 157,
      "endOffset" : 160
    }, {
      "referenceID" : 5,
      "context" : "Various studies have investigated the value of standardized tests [8]–[10] admissions exams [11] and GPA in previous programs [9] in predicting the academic success of students in undergraduate or graduate schools.",
      "startOffset" : 66,
      "endOffset" : 69
    }, {
      "referenceID" : 7,
      "context" : "Various studies have investigated the value of standardized tests [8]–[10] admissions exams [11] and GPA in previous programs [9] in predicting the academic success of students in undergraduate or graduate schools.",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 8,
      "context" : "Various studies have investigated the value of standardized tests [8]–[10] admissions exams [11] and GPA in previous programs [9] in predicting the academic success of students in undergraduate or graduate schools.",
      "startOffset" : 92,
      "endOffset" : 96
    }, {
      "referenceID" : 6,
      "context" : "Various studies have investigated the value of standardized tests [8]–[10] admissions exams [11] and GPA in previous programs [9] in predicting the academic success of students in undergraduate or graduate schools.",
      "startOffset" : 126,
      "endOffset" : 129
    }, {
      "referenceID" : 9,
      "context" : "math, chemistry) [12], [13] have a strongly positive correlation as well.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 10,
      "context" : "math, chemistry) [12], [13] have a strongly positive correlation as well.",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 9,
      "context" : "Reference [12] observes that simple linear and more complex nonlinear (e.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 11,
      "context" : "Reference [14] argues that the accuracy of GPA predictions frequently is mediocre due to different grading standards used in different classes and shows a higher validity for grade predictions in single classes.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 12,
      "context" : "Consequently, many works focus on identifying relationships between a student’s grade in a particular class and variables related to the student [15]–[19].",
      "startOffset" : 145,
      "endOffset" : 149
    }, {
      "referenceID" : 16,
      "context" : "Consequently, many works focus on identifying relationships between a student’s grade in a particular class and variables related to the student [15]–[19].",
      "startOffset" : 150,
      "endOffset" : 154
    }, {
      "referenceID" : 12,
      "context" : "Relevant factors were found to include the student’s prior GPA [15]–[17], [19], performance in related courses [16], [17], [19], performance in early assignments of the class [17], [19], class attendance [15], self-efficacy [18] and whether the student is repeating the class [17].",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 14,
      "context" : "Relevant factors were found to include the student’s prior GPA [15]–[17], [19], performance in related courses [16], [17], [19], performance in early assignments of the class [17], [19], class attendance [15], self-efficacy [18] and whether the student is repeating the class [17].",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 16,
      "context" : "Relevant factors were found to include the student’s prior GPA [15]–[17], [19], performance in related courses [16], [17], [19], performance in early assignments of the class [17], [19], class attendance [15], self-efficacy [18] and whether the student is repeating the class [17].",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 13,
      "context" : "Relevant factors were found to include the student’s prior GPA [15]–[17], [19], performance in related courses [16], [17], [19], performance in early assignments of the class [17], [19], class attendance [15], self-efficacy [18] and whether the student is repeating the class [17].",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 14,
      "context" : "Relevant factors were found to include the student’s prior GPA [15]–[17], [19], performance in related courses [16], [17], [19], performance in early assignments of the class [17], [19], class attendance [15], self-efficacy [18] and whether the student is repeating the class [17].",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 16,
      "context" : "Relevant factors were found to include the student’s prior GPA [15]–[17], [19], performance in related courses [16], [17], [19], performance in early assignments of the class [17], [19], class attendance [15], self-efficacy [18] and whether the student is repeating the class [17].",
      "startOffset" : 123,
      "endOffset" : 127
    }, {
      "referenceID" : 14,
      "context" : "Relevant factors were found to include the student’s prior GPA [15]–[17], [19], performance in related courses [16], [17], [19], performance in early assignments of the class [17], [19], class attendance [15], self-efficacy [18] and whether the student is repeating the class [17].",
      "startOffset" : 175,
      "endOffset" : 179
    }, {
      "referenceID" : 16,
      "context" : "Relevant factors were found to include the student’s prior GPA [15]–[17], [19], performance in related courses [16], [17], [19], performance in early assignments of the class [17], [19], class attendance [15], self-efficacy [18] and whether the student is repeating the class [17].",
      "startOffset" : 181,
      "endOffset" : 185
    }, {
      "referenceID" : 12,
      "context" : "Relevant factors were found to include the student’s prior GPA [15]–[17], [19], performance in related courses [16], [17], [19], performance in early assignments of the class [17], [19], class attendance [15], self-efficacy [18] and whether the student is repeating the class [17].",
      "startOffset" : 204,
      "endOffset" : 208
    }, {
      "referenceID" : 15,
      "context" : "Relevant factors were found to include the student’s prior GPA [15]–[17], [19], performance in related courses [16], [17], [19], performance in early assignments of the class [17], [19], class attendance [15], self-efficacy [18] and whether the student is repeating the class [17].",
      "startOffset" : 224,
      "endOffset" : 228
    }, {
      "referenceID" : 14,
      "context" : "Relevant factors were found to include the student’s prior GPA [15]–[17], [19], performance in related courses [16], [17], [19], performance in early assignments of the class [17], [19], class attendance [15], self-efficacy [18] and whether the student is repeating the class [17].",
      "startOffset" : 276,
      "endOffset" : 280
    }, {
      "referenceID" : 17,
      "context" : "Other works [20]–[23], which also exclusively use data from the course itself, differ significantly from this paper in several aspects.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 19,
      "context" : "Other works [20]–[23], which also exclusively use data from the course itself, differ significantly from this paper in several aspects.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 13,
      "context" : "[16], [18] [19] [15], [17] [20] [21]–[23] Our Work",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 15,
      "context" : "[16], [18] [19] [15], [17] [20] [21]–[23] Our Work",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 16,
      "context" : "[16], [18] [19] [15], [17] [20] [21]–[23] Our Work",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 12,
      "context" : "[16], [18] [19] [15], [17] [20] [21]–[23] Our Work",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 14,
      "context" : "[16], [18] [19] [15], [17] [20] [21]–[23] Our Work",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 17,
      "context" : "[16], [18] [19] [15], [17] [20] [21]–[23] Our Work",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 18,
      "context" : "[16], [18] [19] [15], [17] [20] [21]–[23] Our Work",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 19,
      "context" : "[16], [18] [19] [15], [17] [20] [21]–[23] Our Work",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 20,
      "context" : "1The performance assessments are usually graded by teaching assistants, by the instructor or even by other students through peer review [24].",
      "startOffset" : 136,
      "endOffset" : 140
    }, {
      "referenceID" : 0,
      "context" : "Let ai,y,k ∈ [0, 1] denote the normalized score or grade of student i in performance assessment k of year y.",
      "startOffset" : 13,
      "endOffset" : 19
    }, {
      "referenceID" : 0,
      "context" : "The normalized overall score zi,y ∈ [0, 1] of yth year student i is the weighted sum of all performance assessments",
      "startOffset" : 36,
      "endOffset" : 42
    }, {
      "referenceID" : 0,
      "context" : "Without loss of generality we assume that all scores a are normalized to the range [0, 1].",
      "startOffset" : 83,
      "endOffset" : 89
    }, {
      "referenceID" : 0,
      "context" : "Let p(k, qth) ∈ [0, 1] denote the proportion of current year students for which the grade prediction algorithm working with confidence threshold qth ≤ 1 has predicted the overall score by time (performance assessment) k ∈ [0,K].",
      "startOffset" : 16,
      "endOffset" : 22
    }, {
      "referenceID" : 0,
      "context" : "Note that algorithm 1 does not require a specific normalization and it does not matter that the normalized scores according to (16) will not be in the interval [0, 1] as assumed in Section III for simplicity.",
      "startOffset" : 160,
      "endOffset" : 166
    }, {
      "referenceID" : 0,
      "context" : ", Xn be independent and bounded random variables with range [0, 1] and expected value μ.",
      "startOffset" : 60,
      "endOffset" : 66
    }, {
      "referenceID" : 21,
      "context" : "Proof: A proof of Fact 1 can be found in Hoeffding’s paper [25].",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 0,
      "context" : ", Xn be independent and bounded random random variables with range [0, 1] and variance V ar.",
      "startOffset" : 67,
      "endOffset" : 73
    }, {
      "referenceID" : 22,
      "context" : "Proof: See [26] for a proof of Fact 2.",
      "startOffset" : 11,
      "endOffset" : 15
    } ],
    "year" : 2017,
    "abstractText" : "To increase efficacy in traditional classroom courses as well as in Massive Open Online Courses (MOOCs), automated systems supporting the instructor are needed. One important problem is to automatically detect students that are going to do poorly in a course early enough to be able to take remedial actions. Existing grade prediction systems focus on maximizing the accuracy of the prediction while overseeing the importance of issuing timely and personalized predictions. This paper proposes an algorithm that predicts the final grade of each student in a class. It issues a prediction for each student individually, when the expected accuracy of the prediction is sufficient. The algorithm learns online what is the optimal prediction and time to issue a prediction based on past history of students’ performance in a course. We derive a confidence estimate for the prediction accuracy and demonstrate the performance of our algorithm on a dataset obtained based on the performance of approximately 700 UCLA undergraduate students who have taken an introductory digital signal processing over the past 7 years. We demonstrate that for 85% of the students we can predict with 76% accuracy whether they are going do well or poorly in the class after the 4th course week. Using data obtained from a pilot course, our methodology suggests that it is effective to perform early in-class assessments such as quizzes, which result in timely performance prediction for each student, thereby enabling timely interventions by the instructor (at the student or class level) when necessary.",
    "creator" : "LaTeX with hyperref package"
  }
}