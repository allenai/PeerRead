{
  "name" : "1106.0676.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Satinder Singh", "Diane Litman" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Journal of Arti ial Intelligen e Resear h 16 (2002) 105{133 Submitted 2/01; published 2/02Optimizing Dialogue Management with Reinfor ementLearning: Experiments with the NJFun SystemSatinder Singh baveja s. olorado.eduSyntek CapitalNew York, NY 10019Diane Litman litman s.pitt.eduDepartment of Computer S ien e and LRDCUniversity of PittsburghPittsburgh, PA 15260Mi hael Kearns mkearns is.upenn.eduDepartment of Computer and Information S ien eUniversity of PennsylvaniaPhiladelphia, PA 19104Marilyn Walker walker resear h.att. omAT&T Labs - Resear hFlorham Park, NJ 07932 Abstra tDesigning the dialogue poli y of a spoken dialogue system involves many nontrivial hoi es. This paper presents a reinfor ement learning approa h for automati ally optimiz-ing a dialogue poli y, whi h addresses the te hni al hallenges in applying reinfor ementlearning to a working dialogue system with human users. We report on the design, on-stru tion and empiri al evaluation of NJFun, an experimental spoken dialogue system thatprovides users with a ess to information about fun things to do in New Jersey. Our resultsshow that by optimizing its performan e via reinfor ement learning, NJFun measurably im-proves system performan e.1. Introdu tionRe ent advan es in spoken language understanding have made it possible to develop dialoguesystems for many appli ations. The role of the dialogue manager in su h systems is tointera t in a natural way to help the user omplete the tasks that the system is designed tosupport. Typi ally, an expert designs a dialogue management poli y by hand, and has tomake many nontrivial design hoi es. It is often diÆ ult to assess the rami ations of these hoi es be ause the performan e of a dialogue poli y depends on many other fa tors, su has the user population, the robustness of the automati spee h re ognizer (ASR), and taskdiÆ ulty (Kamm, Litman, & Walker, 1998; Danieli & Gerbino, 1995). This paper appliesreinfor ement learning (RL) to automati ally learn design hoi es that optimize systemperforman e for a hosen performan e measure (Levin, Piera ini, & E kert, 2000; Walker,Fromer, & Narayanan, 1998a). 2002 AI A ess Foundation and Morgan Kaufmann Publishers. All rights reserved.\nSingh, Litman, Kearns, & WalkerS1: Wel ome to NJFun. How may I help you?U1: I'd like to nd um winetasting in Lambertville in the morning.(ASR: I'd like to nd out wineries the in the Lambertville in the morning.)S2: Did you say you are interested in Lambertville?U2: Yes.S3: Did you say you want to go in the morning?U3: Yes.S4: I found a winery near Lambertville that is open in the morning.It is the Poor Ri hard's Winery in Lambertville.Thank you for using the system.Please give me feedba k by saying `good', `so-so', or `bad'.U4: Good.S5: Goodbye and have a ni e day!Figure 1: An example dialogue with NJFun.Consider the spoken dialogue system named NJFun, whi h we implemented to providetelephone a ess to a database of a tivities in New Jersey. A sample dialogue with NJFunis shown in Figure 1, with system utteran es labeled Si and user utteran es labeled Ui.In this dialogue, by starting with the open-ended greeting \\How may I help you?\", thesystem lets the user take the initiative in providing information about the a tivity they areinterested in. User responses in su h ases may be relatively un onstrained. In ontrast,the system ould take the initiative by saying the more restri tive phrase \\Please tell methe lo ation you are interested in\", thus onstraining the user to provide information aboutthe lo ation of the a tivity. Whi h of these ontrasting hoi es of user or system initiative issuperior may depend strongly on the properties of the underlying and imperfe t ASR, thepopulation of users, as well as the dialogue so far. This hoi e of initiative o urs repeatedlythroughout a dialogue, and is but one example of a lass of diÆ ult design de isions.In the main, previous resear h has treated the spe i ation of the dialogue managementpoli y as an iterative design problem: several versions of a system are reated (where ea hversion uses a single dialogue poli y, intuitively designed by an expert), dialogue orpora are olle ted with human users intera ting with di erent versions of the system, a number ofevaluation metri s are olle ted for ea h dialogue, and the di erent versions are statisti ally ompared (Danieli & Gerbino, 1995; Sanderman, Sturm, den Os, Boves, & Cremers, 1998;Kamm, 1995; Walker, Litman, Kamm, & Abella, 1998b). Due to the osts of experimen-tation, only a handful of poli ies are usually explored in any one experiment. Yet, manythousands of reasonable dialogue poli ies are typi ally possible. In NJFun, for example,there is a sear h spa e of 242 potential dialogue poli es, as will be detailed below.Re ent work has suggested that a dialogue poli y an be designed using the formalismsof Markov de ision pro esses (MDPs) and reinfor ement learning (RL) (Biermann & Long,1996; Levin et al., 2000; Walker et al., 1998a; Singh, Kearns, Litman, & Walker, 1999;Walker, 2000), whi h have be ome a standard approa h to many AI problems that involvean agent learning to improve performan e by intera tion with its environment (Sutton &106\nOptimizing Dialogue ManagementBarto, 1998; Kaelbling, Littman, & Moore, 1996). More spe i ally, the MDP and RLformalisms suggest a method for optimizing dialogue poli ies from sample dialogue data,and have many features well-suited to the problem of dialogue design. These featuresin lude the fa t that RL is designed to ope gra efully with noisy sensors (su h as theASR), sto hasti behavior in the environment (whi h in this ase is the user population),and delayed rewards (whi h are typi al in spoken dialogue systems).1 The main advantageof this approa h is the potential for omputing an optimal dialogue poli y within a mu hlarger sear h spa e, using a relatively small number of training dialogues. The RL approa his more data-eÆ ient be ause it evaluates a tions as a fun tion of state, while the traditionaliterative method evaluates entire poli ies.Unfortunately, the pra ti al appli ation of RL to the area of spoken dialogue manage-ment presents many te hni al hallenges. While the theory of RL is quite advan ed, ap-pli ations have been limited almost ex lusively to problems in ontrol, operations resear h,or game-playing (e.g., (Crites & Barto, 1996; Tesauro, 1995)). Dialogue management rep-resents a rather di erent type of problem, in whi h the MDP models a working system'sintera tion with a population of human users, and RL is used to optimize the system'sperforman e. For this type of appli ation, the amount of training data is severely limitedby the requirement that a human intera t with the system. Furthermore, the need forexploratory data must be balan ed with the need for a fun tioning system, i.e. ea h hoi ethat the system has available in a parti ular ontext must make sense in that ontext fromthe user's perspe tive.This paper presents a detailed methodology for using RL to optimize the design of adialogue management poli y based on limited intera tions with human users, and experi-mentally demonstrates the utility of the approa h in the ontext of the NJFun system. Ata high level, our RL methodology involves the hoi e of appropriate performan e riteria(i.e., reward measures) and estimates for dialogue state, the deployment of an initial trainingsystem that generates deliberately exploratory dialogue data, the onstru tion of an MDPmodel of user population rea tions to di erent a tion hoi es, and the redeployment of thesystem using the optimal dialogue poli y a ording to this learned or estimated model.Se tion 2 des ribes some of the dialogue poli y hoi es that a dialogue manager mustmake. Se tion 3 explains how reinfor ement learning an be used to optimize su h hoi esin a elded dialogue system with human users. Se tion 4 des ribes the ar hite ture ofthe NJFun system, while Se tion 5 des ribes how NJFun optimizes its dialogue poli yfrom experimentally obtained dialogue data. Se tion 6 reports empiri al results evaluatingthe performan e of NJFun's learned dialogue poli y, and demonstrates that our approa himproves NJFun's task ompletion rate (our hosen measure for performan e optimization).Se tion 6 also presents results establishing the vera ity of the learned MDP, and omparesthe performan e of the learned poli y to the performan e of standard hand-designed poli iesin the literature. Our results provide empiri al eviden e that, when properly applied, RL an quantitatively and substantially improve the performan e of a spoken dialogue system.1. Other work has explored the use of non-RL learning methods using more immediate kinds of re-wards (Chu-Carroll & Brown, 1997; Walker, Rambow, & Rogati, 2001).107"
    }, {
      "heading" : "Singh, Litman, Kearns, & Walker",
      "text" : "Policy Dialogue Database\nTTS\nASR\nUserFigure 2: A blo k diagram representation of a spoken dialogue system. The user gainsa ess to a database by speaking to the system in natural language through theautomati spee h re ognition system (ASR). The system talks ba k to the userthrough a text to spee h (TTS) system.2. Dialogue Management in Spoken Dialogue SystemsIn a typi al spoken dialogue system (shown in blo k-diagram form in Figure 2), the userspeaks to the system in real time through a telephone or mi rophone, using free-form naturallanguage, in order to retrieve desired information from a ba k-end su h as a database. Theuser's spee h is interpreted through an automati spee h re ognizer (ASR), and the system'snatural language responses are onveyed to the user via a text-to-spee h (TTS) omponent.The dialogue manager of the system uses a dialogue poli y to de ide what the system shouldsay (or in RL terminology, whi h a tion it should take) at ea h point in the dialogue.For our purposes, an ASR an be viewed as an imperfe t, noisy sensor with an adjustable\\parameter\" (the language model or grammar) that an be tuned to in uen e the types ofspee h re ognition mistakes made. In addition to any per eived mat hes in the utteran e,the ASR also returns a s ore (typi ally related to log-likelihood under a hidden Markovmodel) giving a subje tive estimate of on den e in the mat hes found. This s ore isimportant in interpreting the ASR results.Our work on entrates on automating two important types of de isions fa ed in dialoguepoli y design, both of whi h are heavily olored by the ASR fa ts above. The rst typeof de isions, of whi h we have already seen an example, is how mu h initiative the systemshould allow the user | namely, whether the system at any given point should prompt theuser in a relatively open-ended manner (often referred to as user initiative) or a relativelyrestri tive manner (system initiative).The se ond type of hoi e we investigate is how onservative the system should be in on rming its understanding of the user. After it has applied the ASR to a user utteran e,and obtained a value for some attribute of interest (for instan e, town = Lambertville), thesystem must de ide whether to on rm the per eived utteran e with the user. After theuser's response U1 in Figure 1, for example, NJFun must de ide whether it should expli itly on rm its understanding, as in utteran es S2 and S3. NJFun an also simply ontinue onwith the dialogue, as when it does not expli itly on rm that the user wants to nd outabout wineries. While we might posit that on rmation is unne essary for high values of theASR on den e, and ne essary for low values, the proper de nitions of \\high\" and \\low\"would ideally be determined empiri ally for the urrent state (for instan e, depending onwhether there has been diÆ ulty on previous ex hanges), and might depend on our measureof system su ess. 108\nOptimizing Dialogue ManagementAs will be detailed below, in the NJFun system, we identi ed many di erent dialoguestates for whi h we wanted to learn whether to take user or system initiative for the nextprompt. Similarly, we identi ed many di erent dialogue states in whi h we wanted tolearn whether to on rm the ASR-per eived user utteran e, or not to on rm.2 We notethat there is genuine and spirited debate over hoi es of initiative and on rmation amongdialogue system designers (Walker & Whittaker, 1990; Danieli & Gerbino, 1995; Haller &M Roy, 1998, 1999; Smith, 1998; Walker et al., 1998a). As a simple example, some usersenjoy systems that on rm frequently, even if unne essarily, sin e it provides on den ethat the system is understanding the user. These are not well-understood hoi es on whi hthere is a prevailing onsensus, whi h is pre isely why we wish to automate, in a prin ipledway, the pro ess of making su h hoi es on the basis of empiri al data.3. Reinfor ement Learning For Dialogue Poli y DesignIn this se tion, we des ribe the abstra t methodology we propose to apply RL to dia-logue poli y design. In the next se tion, we will des ribe in detail the instantiation of thismethodology in the NJFun system.In order to apply RL to the design of dialogue poli y, it is ne essary to de ne a state-basedrepresentation for dialogues. By this we simply mean that all or most of the informationabout the dialogue so far that is relevant for de iding what a tion the system should takenext is ontained in a single summarizing entity alled the state. One obvious but impra -ti al hoi e for this state is a trans ript or system log of the entire dialogue, whi h wouldin lude the audio so far, the utteran es mat hed by the ASR, the language models used,the on den e s ores returned by the ASR, and perhaps many other quantities. In pra ti e,we need to ompress this state as mu h as possible | representing states by the values ofa small set of features | without losing information ne essary for making good de isions.We view the design of an appropriate state spa e as appli ation-dependent , and a task fora skilled system designer.Given hoi es for the state features, the system designer an think in terms of the statespa e, and appropriate a tions to take in ea h state. We de ne a dialogue poli y to be amapping from the set of states in the state spa e to a set of a tions . For some states, theproper a tion to take may be lear (for instan e, greeting the user in the start state, orquerying the database when all informational attributes are instantiated). For other states, alled hoi e-states , there may be multiple reasonable a tion hoi es (su h as hoi es ofinitiative and on rmation). Ea h mapping from su h hoi e-states to a parti ular a tionis a distin t dialogue poli y. Typi ally the system designer uses intuition to hoose the besta tion to take in ea h hoi e-state. Our RL-based approa h is to instead make these hoi esby learning .In parti ular, a dialogue system that explores a tion hoi es in a systemati way anlearn to optimize its behavior by intera ting with representative human users. The system onverses with human users to perform a set of representative tasks in the dialogue domain.2. Although not learned in our work, there are obviously many other types of dialogue poli y de isions thatthe system made, e.g., how to present results of database queries (Litman, Pan, & Walker, 1998).109\nSingh, Litman, Kearns, & WalkerFor ea h dialogue intera tion, a s alar performan e measure, alled a reward, is al ulated.3The resulting dialogue orpus is used to onstru t a Markov de ision pro ess (MDP) whi hmodels the users' intera tion with the system. With this approa h, the problem of learninga good dialogue poli y is thus redu ed to omputing the optimal poli y for hoosing a tionsin an MDP| that is, the system's goal is to take a tions so as to maximize expe ted reward.The omputation of the optimal poli y given the learned MDP an be done eÆ iently usingstandard dynami programming algorithms (Bertsekas & Tsitsiklis, 1996; Sutton & Barto,1998).Sin e it is diÆ ult to predi t next a tions, states, and rewards in advan e, we buildthe desired MDP from sample dialogues. Following Singh et al. (1999), we an view adialogue as a traje tory in the hosen state spa e determined by the system a tions anduser responses: s1 !a1;r1 s2 !a2;r2 s3 !a3;r3 Here si !ai;ri si+1 indi ates that at the ith ex hange, the system was in state si, exe uteda tion ai, re eived reward ri, and then the state hanged to si+1. In our experiments onlyterminal dialogue states have nonzero rewards. Dialogue sequen es obtained from trainingdata an be used to empiri ally estimate the transition probabilities P (s0js; a) (denotingthe probability of a transition to state s0, given that the system was in state s and tooka tion a), and the reward fun tion R(s; a) (denoting the expe ted reward obtained, giventhat the system was in state s and took a tion a). For example, our estimate of thetransition probability is simply the number of times, in all of the dialogues, that the systemwas in s, took a, and arrived in s0, divided by the number of times the system was in sand took a (regardless of next state). The estimated transition probabilities and rewardfun tion onstitute an MDP model of the user population's intera tion with the system. It(hopefully) aptures the sto hasti behavior of the users when intera ting with the system.Note that in order to have any on den e in this model, in the sample dialogues the sys-tem must have tried many possible a tions from many possible states, and preferably manytimes. In other words, the training data must be exploratory with respe t to the hosenstates and a tions. If we never try an allowed a tion from some state, we annot expe t toknow the value of taking that a tion in that state. Perhaps the most straightforward way ofensuring exploratory training data is to take a tions randomly4. While this is the approa hwe will take in NJFun, it requires that we be ex eptionally areful in designing the a tionsallowed at ea h hoi e-state, in order to guarantee that the random hoi es made alwaysresult in a dialogue sensible to human users. (Keep in mind that there is no exploration innon hoi e-states where the appropriate a tion is already known and xed by the systemdesigner.) Other approa hes to generating exploratory data are possible.Next, given our MDP, the expe ted umulative reward (or Q-value) Q(s; a) of takinga tion a from state s an be al ulated in terms of the Q-values of su essor states via the3. We dis uss various hoi es for this reward measure later, but in our experiments the reward is always aquantity dire tly obtainable from the experimental set-up, su h as user-satisfa tion or task ompletion.4. Of ourse, even with random exploration, it is not possible in pra ti e to explore all states equally often.Some states will o ur more often than others. The net e e t is that states that o ur often will havetheir a tions tried more often than states that o ur rarely, and thus the transition probabilities forfrequent, and hen e potentially important, state-a tion pairs will be more a urate than the transitionprobabilities of infrequent state-a tion pairs. 110"
    }, {
      "heading" : "Optimizing Dialogue Management",
      "text" : "DB access Utterances Reward"
    }, {
      "heading" : "State",
      "text" : "State\nsemantic tags Dialogue\nPolicy\nASR/DB\nUser utterances log-likelihood\nEstimated\nUsers\nEstimator StateFigure 3: A dialogue system viewed as an MDP. The population of users orrespond tothe environment whose state is among other things de ned by the outputs of theautomati spee h re ognition (ASR) system and the database (DB). The dialoguepoli y de nes the agent, the state-estimator de nes the agent's sensors, and thedatabase a tions as well as the possible set of TTS utteran es de ne the agent'sa tion set.following re ursive equation (Watkins, 1989; Sutton & Barto, 1998):Q(s; a) = R(s; a) + Xs0 P (s0js; a)maxa0 Q(s0; a0): (1)where P (s0js; a) is our estimated transition model and R(s; a) our estimated reward model.Here 0 1 is a dis ount fa tor that if set to a value less than one would dis ountrewards obtained later in time. We found that for NJFun the poli y learned was insensitiveto reasonable hoi es of and therefore we used no dis ounting, or = 1, for the experimentsreported here. The Q-values de ned by Equation 1 an be estimated to within a desiredthreshold using the Q-value version of the standard value iteration algorithm (Bertsekas& Tsitsiklis, 1996), whi h iteratively updates the estimate of Q(s; a) based on the urrentQ-values of neighboring states and stops when the update yields a di eren e that is below athreshold. On e value iteration is ompleted, the optimal dialogue poli y (a ording to ourestimated model) is obtained by sele ting the a tion with the maximum Q-value at ea hdialogue state. To the extent that the estimated MDP is an a urate model of the userpopulation, this optimized poli y should maximize the reward obtained from future users.While this approa h is theoreti ally appealing, the ost of obtaining sample humandialogues makes it ru ial to limit the size of the state spa e, to minimize data sparsityproblems, while retaining enough information in the state to learn an a urate model. Ifsample data were in nite, the idealized state might in lude not only the dialogue so far,but also any derived features (e.g. ASR results, log-likelihood s ores representing ASR on den e, semanti analysis, the results of database queries, et .). Yet even a state basedon only a small number of features an yield an enormous state spa e. While others haveproposed simulating the user intera tions to obtain enough training data (Levin et al.,2000; Young, 2000), our approa h is to work dire tly in a small but arefully designedestimated state spa e (Singh et al., 1999), as shown in Figure 3. By using a minimal staterepresentation to approximate the true state, the amount of data required to learn theoptimal dialogue poli y for the learned MDP using value iteration an be greatly redu ed.111\nSingh, Litman, Kearns, & WalkerThe ontribution of this paper is to empiri ally validate this pra ti al methodology forusing reinfor ement learning to build a dialogue system that optimizes its behavior fromhuman- omputer training dialogue data. In a nutshell, our proposed approa h is:1. Choose an appropriate reward measure for dialogues, an appropriate representationfor dialogue states, and design a dialogue poli y that maps ea h state to a set ofreasonable a tions. In many states there may be only one reasonable a tion.2. Build an initial state-based training system that reates an exploratory data set (onethat tries, many times from ea h hoi e-state, ea h of the a tions we would like to hoose among). Despite being exploratory, this system should still provide the desiredbasi fun tionality.3. Use these training dialogues to build an empiri al MDP model on the state spa e. Thetransitions of this MDP will be modeling the user population's rea tions and rewardsfor the various system a tions.4. Compute the optimal dialogue poli y a ording to this learned MDP.5. Reimplement the system using the learned dialogue poli y.The next se tion details the use of this methodology to design the NJFun system.4. The NJFun SystemNJFun is a real-time spoken dialogue system that provides users with information aboutthings to do in New Jersey. NJFun is built using a general purpose platform for spokendialogue systems (Levin, Piera ini, E kert, Fabbrizio, & Narayanan, 1999), with supportfor modules for automati spee h re ognition (ASR), spoken language understanding, text-to-spee h (TTS), database a ess, and dialogue management. NJFun uses the Watsonspee h re ognizer with sto hasti language and understanding models trained from exam-ple user utteran es (Levin et al., 1999; Levin & Piera ini, 1995), and a TTS system basedon on atenative diphone synthesis (Sproat & Olive, 1995). Our mixed-initiative dialoguemanager was built using the DMD s ripting language (Levin et al., 1999). The NJFundatabase is populated from the nj.online webpage to ontain information about the fol-lowing a tivity types: amusement parks, aquariums, ruises, histori sites, museums, parks,theaters, wineries, and zoos. NJFun indexes this database using three attributes: a tivitytype, lo ation, and time of day (whi h an assume values morning, afternoon, or evening).Informally, the NJFun dialogue manager sequentially queries the user regarding thea tivity, lo ation and time attributes, respe tively. NJFun rst asks the user for the urrentattribute (and possibly the other attributes, depending on the initiative). If the urrentattribute's value is not obtained, NJFun asks for the attribute (and possibly the laterattributes) again. If NJFun still does not obtain a value, NJFun moves on to the nextattribute(s). Whenever NJFun su essfully obtains a value, it an on rm the value, ormove on to the next attribute(s). When NJFun has nished a quiring attributes, it queriesthe database (using a wild ard for ea h unobtained attribute value). For any given bindingof the three attributes, there may be multiple database mat hes, whi h will all be returned112\nOptimizing Dialogue ManagementPrompt TypeGrammar Open Dire tiveRestri tive Doesn't make sense System InitiativeNonRestri tive User Initiative Mixed InitiativeFigure 4: De nition of Initiative for System Prompts.to the user. The length of NJFun dialogues ranges from 1 to 12 user utteran es before thedatabase query. Although the NJFun dialogues are fairly short (sin e NJFun asks for anattribute at most twi e), the information a quisition part of the dialogue is similar to more omplex tasks su h as travel planning (Danieli & Gerbino, 1995; Sanderman et al., 1998).5As dis ussed above, our methodology for using reinfor ement learning to optimize di-alogue poli y requires that all potential a tions for ea h state be spe i ed. Re all that atsome states it is easy for a human to make the orre t a tion hoi e (e.g., we don't wantthe system to be able to say \\goodbye\" in the initial state, as in the simulations of Levinet al. (2000)). We made obvious dialogue poli y hoi es in advan e, and used learning onlyto optimize the diÆ ult hoi es (Walker et al., 1998a). In NJFun, we restri ted the a tion hoi es to 1) the type of initiative to use when asking or reasking for an attribute, and2) whether to on rm an attribute value on e obtained. The optimal a tions may varywith dialogue state, and are subje t to a tive debate in the literature. The a tion hoi esavailable to NJFun are shown in Figures 5 and 6.The three types of initiative that the system uses are de ned in Figure 4, based on the ombination of the wording of the system prompt (open versus dire tive (Kamm, 1995))6,and the type of grammar NJFun uses during ASR (restri tive versus non-restri tive). Theexamples in Figure 5 show that NJFun an ask the user about the rst two attributes7using the three types of initiative. If NJFun uses an open question with a non-restri tivegrammar, it is using user initiative (e.g., GreetU). The non-restri tive grammar is alwaysused with a user initiative prompt, be ause the hoi e of the restri tive grammar doesnot make sense in that ase. If NJFun instead uses a dire tive prompt with a restri tedgrammar, the system is using system initiative (e.g., GreetS). Here the system alls ASR onthe user utteran e using a grammar that re ognizes only the parti ular attribute mentionedin the prompt. If NJFun uses a dire tive question with a non-restri tive grammar, it isusing mixed initiative, be ause it allows the user to take the initiative by supplying extrainformation (e.g., ReAsk1M). The non-restri tive grammar is designed to re ognize boththe attribute expli itly mentioned in the dire tive prompt, as well as information o ered onthe other attributes. The last two rows of the gure show that NJFun always uses systeminitiative for the third attribute, be ause at that point the user an only provide the timeof day.5. To support ontinuous use, the system's fun tionality ould be extended in a number of ways su h as alarger live database and support for followup questions by the users.6. While there are other ways of de ning initiative (Walker & Whittaker, 1990; Chu-Carroll & Brown,1997), this operationalization is ommonly applied in spoken dialogue systems (Levin et al., 1999).7. \\Greet\" is equivalent to asking for the rst attribute.113\nSingh, Litman, Kearns, & WalkerA tion Prompt Prompt Type GrammarGreetS Wel ome to NJFun. Please say an a -tivity name or say `list a tivities' for alist of a tivities I know about. dire tive restri tiveGreetU Wel ome to NJFun. How may I helpyou? open nonrestri tiveReAsk1S I know about amusement parks,aquariums, ruises, histori sites, mu-seums, parks, theaters, wineries andzoos. Please say an a tivity namefrom this list. dire tive restri tiveReAsk1M Please tell me the a tivity type. You an also tell me the lo ation and time. dire tive nonrestri tiveAsk2S Please say the name of the town or ity that you are interested in. dire tive restri tiveAsk2U Please give me more information. open nonrestri tiveReAsk2S Please tell me the name of the town or ity that you are interested in. dire tive restri tiveReAsk2M Please tell me the lo ation that youare interested in. You an also tell methe time. dire tive nonrestri tiveAsk3S What time of the day do you want togo? dire tive restri tiveReAsk3S Do you want to go in the morning, inthe afternoon, or in the evening? dire tive restri tiveFigure 5: Initiative hoi es available to NJFun. The rst olumn spe i es the names of thea tions orresponding to the prompts in the se ond olumn. The third olumnspe i es the prompt type and the fourth olumn spe i es the type of grammarused. A tions that an be taken in the same state are grouped together.NJFun an also vary the a tions for on rming ea h attribute, as shown in Figure 6. IfNJFun asks the user to expli itly verify an attribute, it is using expli it on rmation (e.g.,ExpConf2 for the lo ation, exempli ed by S2 in Figure 1). All expli it on rmations aresystem initiative, as a restri tive yes/no grammar is used, and are generated from templates.For example, the prompt to on rm the time attribute is \\Did you say you want to go inthe < time >?\", where < time > is repla ed by the per eived value of the time attribute(morning, afternoon, or evening). If NJFun does not generate any on rmation prompt, itis using no on rmation (the NoConf a tion).Solely for the purposes of ontrolling its operation (as opposed to the learning, whi hwe dis uss in a moment), NJFun internally maintains a representation of the dialogue state,using an operations ve tor of 14 variables. 2 variables tra k whether the system has greeted114\nOptimizing Dialogue ManagementA tion Prompt Template Prompt Type GrammarExpConf1 Did you say you are interested in goingto < a tivity >? dire tive restri tiveNoConf -ExpConf2 Did you say you are interested in< lo ation > ? dire tive restri tiveNoConf -ExpConf3 Did you say you want to go in the< time >? dire tive restri tiveNoConf -Figure 6: Con rmation hoi es available to NJFun. The rst olumn spe i es the namesof the a tions orresponding to the prompts in the se ond olumn. The third olumn spe i es the prompt type and the fourth olumn spe i es the type ofgrammar used. The prompt for the NoConf (no- on rmation) a tion is empty.Feature Values ExplanationGreet (G) 0,1 Whether the system has greeted the userAttribute (A) 1,2,3,4 Whi h attribute is being worked onCon den e/Con rmed(C) 0,1,2,3,4 0,1,2 for low, medium, and high ASR on den e. 3,4for expli itly on rmed, and dis on rmedValue (V) 0,1 Whether value has been obtained for urrent attributeTries (T) 0,1,2 How many times urrent attribute has been askedGrammar (M) 0,1 Whether non-restri tive or restri tive grammar wasusedHistory (H) 0,1 Whether there was trouble on any previous attributeFigure 7: State features and values.the user, and whi h attribute the system is urrently attempting to obtain. For ea h of the3 attributes, 4 variables tra k whether the system has obtained the attribute's value andwhat the value is, the system's on den e in the value (if obtained), the number of timesthe system has asked the user about the attribute, and the type of ASR grammar mostre ently used to ask for the attribute.The formal state spa e S maintained by NJFun for the purposes of learning is mu hsimpler than the operations ve tor, due to the data sparsity on erns already dis ussed.The dialogue state spa e S ontains only 7 variables, as summarized in Figure 7. S is omputed from the operations ve tor using a hand-designed algorithm. The \\Greet\" featuretra ks whether the system has greeted the user or not (no=0, yes=1). \\Attribute\" spe i eswhi h attribute NJFun is urrently attempting to obtain or verify (a tivity=1, lo ation=2,time=3, done with attributes=4). \\Con den e/Con rmed\" represents the on den e thatNJFun has after obtaining a value for an attribute. The values 0, 1, and 2 represent the115\nSingh, Litman, Kearns, & Walkerlowest, middle and highest ASR on den e values.8 The values 3 and 4 are set whenASR hears \\yes\" or \\no\" after a on rmation question. \\Value\" tra ks whether NJFunhas obtained a value for the attribute (no=0, yes=1). \\Tries\" tra ks the number of timesthat NJFun has asked the user about the attribute. \\Grammar\" tra ks the type of ASRgrammar (language model) most re ently used to obtain the attribute (0=non-restri tive,1=restri tive). Finally, \\History\" represents whether NJFun had trouble understanding theuser in the earlier part of the onversation (bad=0, good=1). We omit the full de nition,but as an example, when NJFun is working on the se ond attribute (lo ation), the historyvariable is set to 0 if NJFun does not have an a tivity, has an a tivity but has no on den ein the value, or needed two queries to obtain the a tivity.We note that this state representation, in the interests of keeping the state spa e small,deliberately ignores potentially helpful information about the dialogue so far. For example,there is no state feature expli itly tra king the average ASR s ore over all user utteran es sofar, nor do we keep information about the raw feature values for previous states.9 However,as mentioned above, the goal is to design a small state spa e that makes enough riti aldistin tions to support learning. The use of S redu es the number of states to only 62, andsupports the onstru tion of an MDP model that is not sparse with respe t to S, even usinglimited training data.10 The state spa e that we utilize here, although minimal, allows us tomake initiative de isions based on the su ess of earlier ex hanges, and on rmation de i-sions based on ASR on den e s ores and grammars, as suggested by earlier work (Danieli& Gerbino, 1995; Walker et al., 1998b; Litman, Walker, & Kearns, 1999).With the state spa e and a tion hoi es pre isely de ned, we an now detail the poli y lass explored in our experiment, de ned to be the set of all deterministi mappings fromthe states in whi h the system has a hoi e to a parti ular, xed hoi e. The state/a tionmapping representing NJFun's dialogue poli y lass EIC (Exploratory for Initiative andCon rmation) is shown in Figure 8. For ea h hoi e-state, we list the two hoi es of a tionsavailable. (The a tion hoi es in boldfa e are the ones eventually identi ed as optimal bythe learning pro ess, and are dis ussed in detail later.) Sin e there are 42 hoi e-stateswith 2 a tion hoi es ea h, the total number of unique poli ies in this lass is 242. Inkeeping with the RL methodology des ribed above, our goal is to ompute and implementan approximately optimal poli y in this large lass on the basis of RL applied to exploratorytraining dialogues.The poli y lass in Figure 8 is obtained by allowing a hoi e of system or user initiativewhenever the system needs to ask or reask for an attribute, and by allowing a hoi e of on rming or simply moving on to the next attribute whenever the system has just obtaineda value for an attribute. For example, in the initial state where the user has not yet greetedthe user (\\Greet\" has the value 0), the system has a hoi e of uttering the system initiative8. For ea h utteran e, the ASR output in ludes not only the re ognized string, but also an asso iateda ousti on den e s ore. Based on data obtained during system development, we de ned a mappingfrom raw on den e values into 3 approximately equally populated partitions.9. As dis ussed above, the system uses its operations ve tor to store more information, su h as the a tualvalues of previous attributes for the eventual database query. As these do not in uen e future dialoguepoli y in any way, they are not stored as state features.10. 62 refers to those states that an a tually o ur in a dialogue. For example, greet=0 is only possible inthe initial dialogue state \\0 1 0 0 0 0 0\". Thus, all other states beginning with 0 (e.g. \\0 1 0 0 1 0 0\")will never o ur. 116\nOptimizing Dialogue ManagementChoi e-States A tion Choi esG A C V T M H0 1 0 0 0 0 0 GreetS,GreetU1 1 0 0 1 0 0 ReAsk1S,ReAsk1M1 1 0 1 0 0 0 NoConf,ExpConf11 1 0 1 0 1 0 NoConf,ExpConf11 1 1 1 0 0 0 NoConf,ExpConf11 1 1 1 0 1 0 NoConf,ExpConf11 1 2 1 0 0 0 NoConf,ExpConf11 1 2 1 0 1 0 NoConf,ExpConf11 1 4 0 0 0 0 ReAsk1S,ReAsk1M1 1 4 0 1 0 0 ReAsk1S,ReAsk1M1 2 0 0 0 0 0 Ask2S,Ask2U1 2 0 0 0 0 1 Ask2S,Ask2U1 2 0 0 1 0 0 ReAsk2S,ReAsk2M1 2 0 0 1 0 1 ReAsk2S,ReAsk2M1 2 0 1 0 0 0 NoConf,ExpConf21 2 0 1 0 0 1 NoConf,ExpConf21 2 0 1 0 1 0 NoConf,ExpConf21 2 0 1 0 1 1 NoConf,ExpConf21 2 1 1 0 0 0 NoConf,ExpConf21 2 1 1 0 0 1 NoConf,ExpConf21 2 1 1 0 1 0 NoConf,ExpConf21 2 1 1 0 1 1 NoConf,ExpConf21 2 2 1 0 0 0 NoConf,ExpConf21 2 2 1 0 0 1 NoConf,ExpConf21 2 2 1 0 1 0 NoConf,ExpConf21 2 2 1 0 1 1 NoConf,ExpConf21 2 4 0 0 0 0 ReAsk2S,ReAsk2M1 2 4 0 0 0 1 ReAsk2S,ReAsk2M1 2 4 0 1 0 0 ReAsk2S,ReAsk2M1 2 4 0 1 0 1 ReAsk2S,ReAsk2M1 3 0 1 0 0 0 NoConf,ExpConf31 3 0 1 0 0 1 NoConf,ExpConf31 3 0 1 0 1 0 NoConf,ExpConf31 3 0 1 0 1 1 NoConf,ExpConf31 3 1 1 0 0 0 NoConf,ExpConf31 3 1 1 0 0 1 NoConf,ExpConf31 3 1 1 0 1 0 NoConf,ExpConf31 3 1 1 0 1 1 NoConf,ExpConf31 3 2 1 0 0 0 NoConf,ExpConf31 3 2 1 0 0 1 NoConf,ExpConf31 3 2 1 0 1 0 NoConf,ExpConf31 3 2 1 0 1 1 NoConf,ExpConf3Figure 8: EIC Poli y Class. De nitions for state features are given in Figure 7.prompt \\Please say an a tivity name or say `list a tivities' for a list of a tivities I knowabout.\" or the user initiative prompt \\How may I help you?\" As another example, hoi esin on rmation are available at states for whi h the \\Value\" feature is 1. In these states,the system an either on rm the attribute value obtained from the ASR, or a ept the urrent binding and move on to the next attribute.117\nSingh, Litman, Kearns, & WalkerTo exe ute a parti ular poli y in the poli y lass EIC, NJFun hooses randomly betweenthe two a tions for whatever hoi e-state it is in, thus maximizing exploration and minimiz-ing data sparseness when onstru ting our MDP model. Note that due to the randomizationused for a tion hoi e, the prompts in Figures 5 and 6 are designed to ensure the oheren eof all possible a tion sequen es.State A tion Turn Rewardg a v t m h0 1 0 0 0 0 0 GreetU S1 01 1 2 1 0 0 0 NoConf - 01 2 2 1 0 0 1 ExpConf2 S2 01 3 2 1 0 0 1 ExpConf3 S3 01 4 0 0 0 0 0 Tell S4 1Figure 9: Generating the dialogue in Figure 1.Figure 9 illustrates how the dialogue poli y lass in Figure 8 generates the dialogue inFigure 1. Ea h row indi ates the state that NJFun is in, the a tion exe uted in this state,the orresponding turn in Figure 1, and the reward re eived. The initial state representsthat NJFun will rst attempt to obtain attribute 1. NJFun exe utes GreetU (althoughas shown in Figure 8, GreetS is also possible), generating the rst utteran e in Figure 1.After the user's response, the next state represents that NJFun has now greeted the userand obtained the a tivity value with high on den e, by using a non-restri tive grammar.NJFun then hooses the NoConf a tion, so it does not attempt to on rm the a tivity,whi h auses the state to hange but no prompt to be generated. The third state representsthat NJFun is now working on the se ond attribute (lo ation), that it already has this valuewith high on den e (lo ation was obtained with a tivity after the user's rst utteran e),and that the dialogue history is good.11 This time NJFun hooses the ExpConf2 a tion,and on rms the attribute with the se ond NJFun utteran e, and the state hanges again.The pro essing of time is similar to that of lo ation, whi h leads NJFun to the nal state,where it performs the a tion \\Tell\" ( orresponding to querying the database, presentingthe results to the user, and asking the user to provide a reward). Note that in NJFun, thereward is always 0 ex ept at the terminal state, as shown in the last olumn of Figure 9.Figure 10 illustrates another NJFun dialogue that was generated using the EIC poli y lass. Note that the dialogues in Figures 1 and 10 instantiate two di erent dialogue poli iesin the EIC poli y lass. For example, although NJFun begins both dialogues in the rststate in Figure 8, NJFun exe utes GreetU in one dialogue and GreetS in the other.5. Experimentally Optimizing a Poli yWe olle ted experimental dialogues for both training and testing our system. To obtaintraining dialogues, we implemented NJFun using the EIC dialogue poli y lass des ribed11. Re all that only the urrent attribute's features are in the state. However, the operations ve tor ontainsinformation regarding previous attributes. 118\nOptimizing Dialogue ManagementS1: Wel ome to NJFun. Please say an a tivity name or say `list a tivities' for a list ofa tivities I know about.U1: I would like to visit a histori site in Stanhope this morning.(ASR output: I would like to the zoo histori sites in stanhope histori )S2: Did you say you are interested in going to a zoo?U2: No.S3: I know about amusement parks, aquariums, ruises, histori sites, museums, parks,theaters, wineries, and zoos. Please say an a tivity name from this list.U3: I would like to visit a histori site. (ASR output: I would like to visit histori sites)S4: Did you say you are interested in going to a histori site?U4: Yes.S5: Please give me more information.U5: (user says nothing)S6: Please tell me the lo ation that you are interested in. You an also tell me the time.U6: Visit a histori . (ASR output: What histori )S7: What time of the day do you want to go?U7: Stanhope. (no ASR output)S8: Do you want to go in the morning, in the afternoon, or in the evening?U8: Morning.S9: Did you say you want to go in the morning?U9: Yes.S10: I found 11 histori sites in New Jersey that are open in the morning. The rst 3[. . . ℄ Would you like to hear more?U10:No.S11: Thank you for using the system. Please give me feedba k by saying `good', `so-so',or `bad'.U11:Bad. Figure 10: Another example dialogue with NJFun.in Se tion 4. We used these dialogues to build an empiri al MDP, and then omputed theoptimal dialogue poli y in this MDP (as des ribed in Se tion 3). In this se tion we des ribeour experimental design and the learned dialogue poli y. In the next se tion we presentresults from testing our learned poli y and show that it improves task ompletion rates, theperforman e measure we hose to optimize.Experimental subje ts were AT&T employees not asso iated with the NJFun proje t.There were 54 subje ts for training and 21 for testing. Subje ts were distributed so thetraining and testing pools were balan ed for gender, English as a rst language, and exper-tise with spoken dialogue systems.12 Training subje ts were informed at the beginning ofthe experiment that NJFun might hange its behavior during the experiment, via a set ofweb-based instru tions (see Appendix A).12. Subsequent analyses indi ated that system performan e did not depend signi antly on any of thesefa tors. 119\nSingh, Litman, Kearns, & Walker Task 1. You are bored at home in Morristown on a rainy afternoon. Use NJFun to nd a museum to go to. Task 2. You live in Cape May and want to take some friends on an evening ruise.Use NJFun to nd out what your options are. Task 3. You have lived in Stanhope for many years but have never managed to visitits histori sites. Today you are feeling virtuous. Use NJFun to nd out what you an see this morning. Task 4. You feel thirsty and want to do some winetasting in the morning. Are thereany wineries lose by your house in Lambertville? Task 5. After a hard day of work at AT&T in Florham Park, you would like to relaxwith an evening at the theatre. Use NJFun to nd out if it is possible to see a shownear Florham Park. Task 6. You live in Jersey City, and want to spend the afternoon enjoying nature asthe weather is beautiful. Are there any parks nearby?Figure 11: Task s enarios.During both training and testing, subje ts arried out free-form onversations withNJFun to omplete the six appli ation tasks in Figure 11. For example, the task exe utedby the user in Figure 1 was Task 4 in Figure 11. Subje ts read ea h task des ription bygoing to a separate web page for ea h task (a essible from the main experimental webpage), then alled NJFun from their oÆ e phone. At the end of the task, NJFun asked forfeedba k on their experien e (e.g., utteran e S4 in Figure 1). Users then hung up the phoneand lled out a user survey on the web, shown in Figure 12. Possible responses for questions1 and 2 are shown. The answers to the rst question (good, so-so, bad) are mapped to 1,0, and -1, respe tively. For the remaining questions, users indi ated the strength of theiragreement on a 5 point Likert s ale (Ja k, Foster, & Stentiford, 1992), with the responses(strongly agree, somewhat agree, neither agree nor disagree, somewhat disagree, stronglydisagree), whi h are mapped to 5 through 1, respe tively.As di tated by Step 2 of the RL methodology des ribed in Se tion 3, we rst built atraining version of the system, using the EIC state spa e and a tion hoi es outlined inthe pre eding se tion, that used random exploration. By this we mean that in any statefor whi h we had spe i ed a hoi e of system a tions, the training system hose randomlyamong the allowed a tions with uniform probability. We again emphasize the fa t thatthe allowed hoi es were designed in a way that ensured that any dialogue generated bythis exploratory training system was intuitively sensible to a human user, and permittedthe su essful ompletion of any task the system was intended to perform. Nevertheless, itis important to note that over their multiple alls to the system, training users may havee e tively experien ed multiple dialogue poli ies (as indu ed by the random exploration),while test users experien ed a single, xed, deterministi poli y.120\nOptimizing Dialogue Management Please repeat (or give) your feedba k on this onversation. (good, so-so, bad ) Did you omplete the task and get the information you needed? (yes, no ) In this onversation, it was easy to nd the pla e that I wanted. In this onversation, I knew what I ould say at ea h point in the dialogue. In this onversation, NJFun understood what I said. Based on my urrent experien e with using NJFun, I'd use NJFun regularly to nd apla e to go when I'm away from my omputer.Figure 12: User survey.The training phase of the experiment resulted in 311 omplete dialogues (not all subje ts ompleted all tasks), for whi h NJFun logged the sequen e of states and the orrespondingexe uted a tions. The shortest and longest dialogues obtained had 3 and 11 user utteran es,respe tively. In our training set, the number of samples per state for the initial ask hoi esare:0 1 0 0 0 0 0 GreetS=155 GreetU=1561 2 0 0 0 0 0 Ask2S=93 Ask2U=721 2 0 0 0 0 1 Ask2S=36 Ask2U=48Su h data illustrates that the random a tion hoi e method of exploration led to a fairlybalan ed a tion distribution per state. Similarly, the small state spa e, and the fa t thatwe only allowed 2 a tion hoi es per state, prevented a data sparseness problem. This isimportant be ause the optimal dialogue poli y obtained via RL is unreliable at infrequentlyvisited states. The rst state in Figure 8, the initial state for every dialogue, was the mostfrequently visited state (with 311 visits). Only 8 states that o ur near the end of a dialoguewere visited less than 10 times.The logged data was then used to onstru t the empiri al MDP. As we have mentioned,the measure we hose to optimize is a binary reward fun tion based on the strongest possiblemeasure of task ompletion, alled Binary Completion, that takes on value 1 if NJFunqueries the database using exa tly the attributes spe i ed in the task des ription, and -1otherwise. Sin e system logs ould be mat hed with whi h of the six tasks the user wasattempting, it was possible to dire tly ompute from the system logs whether or not theuser had ompleted the task. By \\ ompleted\" we mean binding all three attributes (a tivitytype, lo ation, and time of day) to the exa t values spe i ed in the task des ription given onthe asso iated web page. In this way, ea h training dialogue was automati ally labeled bya +1 in the ase of a ompleted task, or 1 otherwise. We note that this de nition of task ompletion guarantees that the user heard all and only the database entries mat hing thetask spe i ations. Relaxations of this reward measure, as well as other types of measuresthat ould have been used as our reward measure, are dis ussed in the next se tion.Finally, we omputed the optimal dialogue poli y in this learned MDP using Q-valueiteration ( f. Se tion 3). The a tion hoi es onstituting the learned poli y are in boldfa e121\nSingh, Litman, Kearns, & Walkerin Figure 8. Note that no hoi e was xed for several states (e.g., \\1 1 4 0 0 0 0\"), meaningthat the Q-values were identi al after value iteration. Thus, even when using the learnedpoli y, NJFun still sometimes hooses randomly between ertain a tion pairs.Intuitively, the learned poli y says that the optimal use of initiative is to begin with userinitiative, then ba k o to either mixed or system initiative when reasking for an attribute.Note, however, that the spe i ba ko method di ers with attribute (e.g., system initiativefor attribute 1, but generally mixed initiative for attribute 2). With respe t to on rmation,the optimal poli y is to mainly on rm at lower on den e values. Again, however, the pointwhere on rmation be omes unne essary di ers a ross attributes (e.g., on den e level 2for attribute 1, but sometimes lower levels for attributes 2 and 3), and also depends onother features of the state besides on den e (e.g., grammar and history). This use of ASR on den e by the dialogue poli y is more sophisti ated than previous approa hes, e.g. (Niimi& Kobayashi., 1996; Litman & Pan, 2000). NJFun an learn su h ne-grained distin tionsbe ause the optimal poli y is based on a omparison of 242 possible exploratory poli ies.Both the initiative and on rmation results suggest that the beginning of the dialogue wasthe most problemati for NJFun. Figure 1 is an example dialogue using the optimal poli y.6. Experimentally Evaluating the Optimized Poli yFor the testing phase, NJFun was reimplemented to use the (now deterministi ) learnedpoli y. 21 test subje ts then performed the same six tasks used during training, resultingin 124 omplete test dialogues. The primary empiri al test of the proposed methodologyis, of ourse, the extent and statisti al signi an e of the improvement in the allegedlyoptimized measure (binary task ompletion) from the training to test populations. In fa t,task ompletion as measured by Binary Completion does in rease, from 52% in trainingto 64% in testing. The following se tions are devoted to the analysis of this test, as well asseveral related tests.6.1 Comparing the Learned Poli y to the Training Poli yTable 1 summarizes the training versus testing performan e of NJFun, for various evalu-ation measures. Re all that in the 311 training dialogues, NJFun used randomly hosenpoli ies in the EIC poli y lass. In the 124 testing dialogues, NJFun used the single learnedpoli y. Although the learned poli y was optimized for only the task su ess measure Bi-nary Completion, many types of measures have been used to evaluate dialogue systems(e.g., task su ess, dialogue quality, eÆ ien y, usability (Danieli & Gerbino, 1995; Kammet al., 1998)). We thus evaluate the performan e of the learned poli y with respe t to boththe original reward measure and a number of other potential reward measures that we didnot optimize the test system for.Perhaps our most important results are summarized in the rst two rows of Table 1.In the rst row, we summarize performan e for the Binary Completion reward measure,dis ussed in the pre eding se tion. The average value of this reward measured a ross the311 dialogues generated using the randomized training system was 0:048 (re all the rangeis 1 to 1), while the average value of this same measure a ross the 124 dialogues using thelearned test system was 0:274, an improvement that has a p-value of 0:059 in a standard122\nOptimizing Dialogue ManagementEvaluation Measure Train Test p-valueBinary Completion 0:048 0:274 0:226 0:059Weak Completion 1:72 2:18 0:46 0:029ASR 2:48 2:67 0:19 0:038Web feedba k 0:18 0:11 0:07 0:42Easy 3:38 3:39 0:01 0:98What to say 3:71 3:64 0:07 0:71NJFun understood 3:42 3:52 0:1 0:58Reuse 2:87 2:72 0:15 0:55Table 1: Train versus test performan e for various evaluation measures. The rst olumnpresents the di erent measures onsidered (see text for detail); the se ond ol-umn is the average value of the measure obtained in the training data; the third olumn is the average value obtained in the test data; the fourth olumn showsthe di eren e between the test average and the train average (a positive numberis a \\win\", while a negative number is a \\loss\"); the fth olumn presents thestatisti al signi an e value obtained using the standard t-test.two-sample t-test over subje t means.13 This result orresponds to an improvement froma 52% ompletion rate among the training dialogues to a 64% ompletion rate among thetesting dialogues.The se ond row of Table 1 shows that performan e also improves from training to testfor the losely related measure Weak Completion14. Weak Completion is a relaxedversion of task ompletion that gives partial redit: if all attribute values are either orre t orwild ards, the value is the sum of the orre t number of attributes. Otherwise, at least oneattribute is wrong (e.g., the user says \\Lambertville\" but the system hears \\Morristown\"),and the value is -1. The motivation for this more re ned measure is that reward -1 indi atesthat the information desired was not ontained in the database entries presented to the user,while non-negative reward means that the information desired was present, but perhapsburied in a larger set of irrelevant items for smaller values of the reward. The trainingdialogue average of weak ompletion was 1:72 (where the range is 1 to 3), while thetest dialogue average was 2:18. Thus we have a large improvement, this time signi antat the 0:029 level. We note that the poli y di tated by optimizing the training MDP forbinary ompletion (whi h was implemented in the test system), and the poli y di tated byoptimizing the training MDP for weak ompletion (whi h was not implemented) were verysimilar, with only very minor di eren es in a tion hoi es.13. Conventionally, a p-value of less than .05 is onsidered to be statisti ally signi ant, while p-values lessthan .10 are onsidered indi ative of a statisti al trend.14. We emphasize that this is the improvement in weak ompletion in the system that was designed tooptimize binary ompletion | that is, we only elded a single test system, but examined performan e hanges for several di erent evaluation measures whi h ould also have been used as our reward measure.123\nSingh, Litman, Kearns, & WalkerThe measure in the third row, ASR, is another variation of Binary Completion.However, instead of evaluating task su ess, ASR evaluates dialogue quality. In parti ular,ASR approximates spee h re ognition a ura y for the database query, and is omputed byadding 1 for ea h orre t attribute value and .5 for every wild ard. Thus, if the task is to gowinetasting near Lambertville in the morning, and the system queries the database for ana tivity in New Jersey in the morning, Binary Completion=-1, Weak Completion=1,and ASR=2. Table 1 shows that the average value of ASR in reased from 2:48 duringtraining to 2:67 during testing (where the range is 0 to 3), a signi ant improvement (p <0:04). Again, this improvement o urred even though the learned poli y used for testingwas not optimized for ASR.The three measures onsidered so far are obje tive reward measures, in the sense thatthe reward is pre isely de ned as a fun tion of the system log on a dialogue, and an be omputed dire tly from this log. We now examine how performan e hanges from trainingto test when a set of subje tive usability measures (provided by the human user followingea h dialogue) are onsidered. Re all that ea h dialogue task was a ompanied by the websurvey in Figure 12. The measure Web feedba k is obtained from the rst question inthis survey (re all the range is 1 to 1). The measures Easy, What to say, NJFununderstood and Reuse are obtained from the last four questions (re all the range is 1to 5). Sin e we did not optimize for any of these subje tive measures, we had no a prioriexpe tations for improvement or degradation. The last ve rows of Table 1 shows we infa t did not nd any statisti ally signi ant hanges in the mean in either dire tion forthese measures. However, we observed a urious move to the middle e e t in that a smallerfra tion of users had extremely positive or extremely negative things to say about our testsystem than did about the training system. Figure 13, whi h shows the entire distributionof the values for both the train and test systems for these subje tive measures, shows thatin optimizing the test system for the task ompletion measure, we seem to have onsistentlyshifted weight away from the tails of the subje tive measures, and towards the intermediatevalues. Although we have no rm explanation for this phenomenon, its onsisten y (ito urs to varying degree for all 5 subje tive measures) is noteworthy.In sum, our empiri al results have demonstrated improvement in the optimized task ompletion measure, and also improvement in two non-optimized (but related) obje tivemeasures. In ontrast, our results show no statisti ally signi ant hanges for a number ofnon-optimized subje tive measures, but an interesting move to the middle e e t.6.2 E e t of ExpertiseIn addition to the task-independent performan e hanges from training to testing poli yjust dis ussed, there were also task-dependent performan e hanges. For example, therewas a signi ant intera tion e e t between poli y and task (p<.01) when performan ewas evaluated for Binary Completion.15 We believe that this ould be the e e t ofuser expertise with the system sin e previous work suggests that novi e users perform omparably to experts after only two tasks (Kamm et al., 1998). Sin e our learned poli y15. Our experimental design onsisted of two fa tors: the within-group fa tor poli y and the between-groupsfa tor task. We use a two-way analysis of varian e (ANOVA) to ompute intera tion e e ts betweenpoli y and task. 124\nOptimizing Dialogue Management(a) (b) −1 0 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Train Test 1 2 3 4 5 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Train Test ( ) (d) 1 2 3 4 5 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Train Test 1 2 3 4 5 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Train Test (e) 1 2 3 4 5 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 Train Test Figure 13: Distributions of the subje tive measures. (a) Web feedba k. (b) Easy. ( ) What to say.(d) NJFun understood. (e) Reuse. 125"
    }, {
      "heading" : "Singh, Litman, Kearns, & Walker",
      "text" : "Figure 14: Intera tion e e ts between task and poli y. The bar harts show the binary ompletion rate for the six tasks (in the order they were presented) for the testand train poli ies. The test poli y performan e is better for the last four taskswhile the train poli y performan e is better on the rst two tasks, providingeviden e that the learned test poli y is slightly optimized for expert users.was based on six tasks with ea h user, it is possible that the learned poli y is slightlyoptimized for expert users. To explore this hypothesis, we divided our orpus into dialogueswith \\novi e\" (tasks 1 and 2) and \\expert\" (tasks 3-6) users. We found that the learnedpoli y did in fa t lead to a large and signi ant improvement in Binary Completion forexperts, in reasing the number of ompleted dialogues from 46% during training to 69%during testing (p<.001). In ontrast, there was a non-signi ant degradation for novi es(train=66%, test=55%, p<.3). In parti ular, as shown in Figure 14, the test means arelower than the train means for the rst two tasks, but higher for the last four tasks. Thisis appropriate for a system that has repeat usage; however should it be the ase that oursystem is primarily used by novi e users, the system might need to be retrained.6.3 Comparison to Hand Designed Poli iesAlthough the results presented so far indi ate an improvement from training to testing, apotential limitation is that using a set of poli ies in the EIC lass may not be the bestbaseline for omparison to our learned poli y. A more standard alternative would be om-parison to the very best hand-designed xed poli y. However, there is no agreement in theliterature, nor amongst the authors, as to what the best hand-designed poli y might havebeen. Nevertheless, it is natural to ask how our optimized system ompares to systemsemploying a dialogue poli y pi ked by a human expert. Although implementing a numberof hand-pi ked poli ies, gathering dialogues from them, and omparing to our learned sys-tem would be time- onsuming and expensive (and in fa t, is exa tly the kind of repeated,sequential implement-and-test methodology we are attempting to repla e), our training sys-tem provides a onvenient and mathemati ally sound proxy. In this se tion we show that126\nOptimizing Dialogue Managementthe performan e of the learned poli y is better than several \\standard\" xed poli ies, by omputing the reward for all the traje tories in the empiri al MDP that are onsistent withea h alternative poli y. Then, be ause ea h of these alternatives has only a handful of on-sistent traje tories in the MDP, in the next se tion we present an analysis of the MDP'sa ura y.Sin e our training dialogues are generated making random hoi es, any dialogue in thetraining set that is onsistent with a poli y in our poli y lass provides an unbiased MonteCarlo trial of . By onsistent we mean that all the random hoi es in the dialogue agreewith those di tated by . We an average the rewards over the onsistent training dialoguesto obtain an unbiased estimate of the return of .Poli y # of Trajs. Emp. Avg. MDP Value p-valueTest 12 0:67 0:534SysNo on rm 11 0:08 0:085 0:06SysCon rm 5 0:6 0:006 0:01UserNo on rm 15 0:2 0:064 0:01UserCon rm 11 0:2727 0:32 0:30Mixed 13 0:077 0:063 0:06Table 2: Comparison to standard poli ies. Here we ompare our test poli y with severalstandard poli ies using the Monte Carlo method. The rst olumn presents thedi erent poli ies onsidered (see text for detail); the se ond olumn shows thenumber of onsistent traje tories in the training data; the third olumn showsthe empiri al average reward on these onsistent traje tories; the fourth olumnshows the estimated value of the poli y a ording to our learned MDP, and the fth olumn shows the statisti al signi an e (p-value) of the poli y's loss withrespe t to the test poli y.Table 2 ompares the performan e of our learned test system, on the Binary Com-pletion reward measure, to 5 xed poli ies in our lass that are ommon hoi es in thedialogue systems literature, or that were suggested to us by dialogue system designers.The SysNo on rm poli y always uses system initiative and never on rms; the SysCon rmpoli y always uses system initiative and on rms; the UserNo on rm poli y always usesuser initiative and never on rms; the UserCon rm poli y always uses user initiative and on rms; the Mixed poli y varies the initiative during the dialogue. For all but the User-Con rm poli y, the test poli y is better with a signi an e near or below the 0:05 level, andthe di eren e with UserCon rm is not signi ant. (Not surprisingly, the xed UserCon rmpoli y that fared best in this omparison is most similar to the poli y we learned.) Thus,in addition to optimizing over a large lass of poli y hoi es that is onsiderably more re- ned than is typi al, the reinfor ement learning approa h outperforms a number of naturalstandard poli ies. 127\nSingh, Litman, Kearns, & Walker6.4 The Goodness of Our MDPFinally, we an ask whether our estimate of state was a tually a good estimate or whether wehave simply been fortunate | that is, whether our MDP might have a tually been a ratherpoor predi tor of the value of a tions, but that we happened to have nevertheless hosen agood poli y by han e. As some losing eviden e against this view, we o er the results ofa simple experiment in whi h we randomly generated many (deterministi ) poli ies in ourpoli y lass. For ea h su h poli y , we used the training dialogues onsistent with to ompute an unbiased Monte Carlo estimate R̂ of the expe ted (binary ompletion) returnof (exa tly as was done for the hand-pi ked \\expert\" poli ies in Table 2). This estimatewas then paired with the value R of (for the start state) in the learned MDP. If the MDPwere a perfe t model of the user population's responses to system a tions, then the MonteCarlo estimate R̂ would simply be a (noisy) estimate of R , the orrelation between thesetwo quantities would be signi ant (but of ourse dependent on the number of samples inthe Monte Carlo estimate), and the best- t linear relationship would be simply R̂ = R +Z(slope 1 and inter ept 0), where Z is a normally distributed noise variable with adjustablemean and varian e de reasing as the number of onsistent traje tories in reases. At theother extreme, if our MDP had no relation to the user population's responses to systema tions, then R̂ and R would be un orrelated, and the best we ould do in terms of alinear t would be R̂ = Z (slope and inter ept 0) | that is, we ignore R and simplymodel R̂ as noise. The results summarized in Table 3 indi ate that we are mu h loserto the former ase than the latter. Over the 1000 random poli ies that we generated,the orrelation between R̂ and R was positive and reje ted the null hypothesis that thevariables are un orrelated well below the 0:01 level of signi an e; furthermore, the leastsquares linear t gave a slope oeÆ ient lose to 1:0 and a y-inter ept lose to 0, as predi tedby the idealized ase above.7. Dis ussionIn this paper we presented a pra ti al methodology for applying reinfor ement learning tothe problem of optimizing dialogue poli y design in spoken dialogue systems. Our method-ology takes a relatively small number of exploratory dialogues, and dire tly omputes theapparent optimal poli y within a spa e of perhaps thousands of poli ies, instead of perform-ing a sequen e of implementations of only a handful of parti ular poli ies. We have usedthis method to onstru t a training version of the NJFun spoken dialogue system, and haveempiri ally demonstrated improved performan e in NJFun after optimization. In a on-trolled experiment with human users using NJFun, we veri ed signi ant improvements inthe reward measure for whi h the optimization was performed. We also showed that therewere signi ant improvements for several other obje tive reward measures (even though thetest poli y was not optimized for these measures), but no improvements for a set of sub-je tive measures (despite an interesting hange in their distributions). Finally, we showedthat the learned poli y is not only better than the non-deterministi EIC poli y lass, butalso better than other xed hoi es proposed in the literature. Our results demonstratethat the appli ation of reinfor ement learning allows one to empiri ally optimize a system'sdialogue poli y by sear hing through a mu h larger sear h spa e than an be explored withmore traditional methods. 128\nOptimizing Dialogue Management# of Trajs. # of Poli ies Corr. Coe . p-value Slope Inter.> 0 1000 0:31 0:00 0:953 0:067> 5 868 0:39 0:00 1:058 0:087> 10 369 0:5 0:00 1:11 0:11Table 3: A test of MDP a ura y. We generated 1000 deterministi poli ies randomly. Forea h poli y we omputed a pair of numbers: its estimated value a ording to theMDP, and its value based on the traje tories onsistent with it in the trainingdata. The number of onsistent traje tories varied with poli y. The rst row isfor all 1000 poli ies, the se ond row for all poli ies that had at least 5 onsis-tent traje tories, and the last row for all poli ies that had at least 10 onsistenttraje tories. The reliability of the empiri al estimate of a poli y in reases within reasing number of onsistent traje tories. The third olumn presents the or-relation oeÆ ient between the empiri al and MDP values. The fourth olumnpresents the statisti al signi an e of the orrelation oeÆ ient. The main resultis that the hypothesis that these two sets of values are un orrelated an be soundlyreje ted. Finally, the last two olumns present the slope and inter ept resultingfrom the best linear t between the two sets of values.Reinfor ement learning has been applied to dialogue systems in previous work, but ourapproa h di ers from previous work in several respe ts. Biermann and Long (1996) did nottest reinfor ement learning in an implemented system, and the experiments of Levin et al.(2000) utilized a simulated user model. Walker et al. (1998a)'smethodology is similar to thatused here, in testing reinfor ement learning with an implemented system with human users.Walker et al. (1998a) explore initiative poli ies and poli ies for information presentation in aspoken dialogue system for a essing email over the phone. However that work only exploredpoli y hoi es at 13 states in the dialogue, whi h on eivably ould have been explored withmore traditional methods (as ompared to the 42 hoi e states explored here).We also note that our learned poli y made dialogue de isions based on ASR on den ein onjun tion with other features, and also varied initiative and on rmation de isionsat a ner grain than previous work; as su h, our learned poli y is not a standard poli yinvestigated in the dialogue system literature. For example, we would not have predi tedthe omplex and interesting ba k-o poli y with respe t to initiative when reasking for anattribute.Our system and experiments have begun to address some of the hallenges spoken di-alogue systems present to the prevailing theory and appli ation of RL (e.g., balan ing the ompeting on erns of random exploration with user experien e in a elded training system;keeping the state spa e as small as possible in order to make learning data-eÆ ient, whileretaining all information ne essary for de ision-making). However, other hallenges remainto be addressed. We do not provide a general methodology for redu ing the state spa e toa manageable size. Furthermore, in our work our learned MDP model is at best an approx-imation, we may be introdu ing the problem of hidden state or partial observability into129\nSingh, Litman, Kearns, & Walkerthe problem of hoosing optimal a tions in ea h state. For situations with hidden state ari her POMDP model is often more appropriate (Kaelbling et al., 1996). Roy, Pineau, andThrun (2000) are urrently exploring whether a POMDP-style approa h an yield MDP-likespeeds in a spoken dialogue system for a robot, where state is used to represent the user'sintentions rather than the system's state.As future work, we wish to understand the aforementioned results on the subje tivemeasures, explore the potential di eren e between optimizing for expert users and novi es,automate the hoi e of state spa e and reward for dialogue systems (whi h in our methodol-ogy is assumed to be given), investigate the use of a learned reward fun tion (Walker et al.,1998a), and explore the use of more informative non-terminal rewards.A knowledgmentsThe authors thank Fan Jiang for his substantial e ort in implementing NJFun, WielandE kert, Esther Levin, Roberto Piera ini, and Mazin Rahim for their te hni al help, JuliaHirs hberg for her omments on a draft of this paper, and David M Allester, Ri hardSutton, Esther Levin and Roberto Piera ini for helpful onversations.Appendix A. Experimental Instru tionsNJFun (The New Jersey Pla e-to-go Re ommender)General Des riptionNJFun is an experimental spoken dialogue system that allows you to a ess a database ofthings to do in New Jersey via a telephone onversation. You will be asked to all NJFun todo 6 di erent tasks. You should try to do ea h task as eÆ iently as you an. Note that youwill be speaking to a di erent version of NJFun during ea h phone all, and that NJFunmight even vary its behavior within a single phone all.Instru tions for alling NJFun an be found at ea h task s enario. Please read throughthe instru tions before alling. On rare o asions, you may get an apparently dead linewhen you all. This indi ates that all lines are busy. If this o urs, hang up and all later.Also, PLEASE DO NOT USE A SPEAKER PHONE.At the end of ea h task, you will be asked to say \\good\", \\so-so\", or \\bad\" , in orderto provide feedba k on your phone all with NJFun. PLEASE DO NOT HANG UP THEPHONE BEFORE PROVIDING THIS FEEDBACK. After you hang up the phone, therewill also be a few brief questions for you to answer. Even if NJFun aborted before you ould omplete the task, PLEASE FINISH THE SURVEY and ontinue to the next task. On eyou have nished ALL of the tasks, there will also be an opportunity for you to providefurther omments.If you have any problems during the experiment, all Diane at 973-360-8314, or Satinderat 973-360-7154.Thank you for parti ipating in this experiment!130\nOptimizing Dialogue ManagementTask S enariosYou have 6 tasks to try in this experiment. You should do one task at a time, in thepres ribed order. After you nish ea h task and have provided your feedba k, hang up thephone and nish the survey for that task. On e you have nished ALL of the tasks, pleaseprovide any nal omments. Cli k here to try Task 1 Cli k here to try Task 2 Cli k here to try Task 3 Cli k here to try Task 4 Cli k here to try Task 5 Cli k here to try Task 6 Cli k here to provide nal ommentsReferen esBertsekas, D. P., & Tsitsiklis, J. N. (1996). Neuro-Dynami Programming. Athena S ienti .Biermann, A. W., & Long, P. M. (1996). The omposition of messages in spee h-graphi sintera tive systems. In Pro eedings of the 1996 International Symposium on SpokenDialogue, pp. 97{100.Chu-Carroll, J., & Brown, M. K. (1997). Tra king initiative in ollaborative dialogue intera -tions. In Pro eedings of the 35th Annual Meeting of the Asso iation for ComputationalLinguisti s, pp. 262{270.Crites, R., & Barto, A. (1996). Improving elevator performan e using reinfor ement learn-ing. In Pro eedings NIPS 8, pp. 1017{1023.Danieli, M., & Gerbino, E. (1995). Metri s for evaluating dialogue strategies in a spokenlanguage system. In Pro eedings of the 1995 AAAI Spring Symposium on Empiri alMethods in Dis ourse Interpretation and Generation, pp. 34{39.Haller, S., & M Roy, S. (1998). Spe ial issue: Computational models of mixed-initiativeintera tion (part 1). User Modeling and User-Adapted Intera tion, 8 (3-4).Haller, S., & M Roy, S. (1999). Spe ial issue: Computational models of mixed-initiativeintera tion (part 2). User Modeling and User-Adapted Intera tion, 9 (1-2).Ja k, M., Foster, J. C., & Stentiford, F. W. (1992). Intelligent dialogues in automated tele-phone servi es. In International Conferen e on Spoken Language Pro essing, ICSLP,pp. 715 { 718. 131\nSingh, Litman, Kearns, & WalkerKaelbling, L. P., Littman, M. L., & Moore, A. W. (1996). Reinfor ement learning: A survey.Journal of Arti ial Intelligen e Resear h, 4, 237{285.Kamm, C. (1995). User interfa es for voi e appli ations. In Roe, D., & Wilpon, J.(Eds.), Voi e Communi ation between Humans and Ma hines, pp. 422{442. NationalA ademy Press.Kamm, C., Litman, D., & Walker, M. A. (1998). From novi e to expert: The e e t of tutori-als on user expertise with spoken dialogue systems. In Pro eedings of the InternationalConferen e on Spoken Language Pro essing, ICSLP98.Levin, E., & Piera ini, R. (1995). Chronus, the next generation. In Pro . of 1995 ARPASpoken Language Systems Te hnology Workshop, Austin Texas.Levin, E., Piera ini, R., & E kert, W. (2000). A sto hasti model of human ma hineintera tion for learning dialog strategies. IEEE Transa tions on Spee h and AudioPro essing, 1, 11{23.Levin, E., Piera ini, R., E kert, W., Fabbrizio, G. D., & Narayanan, S. (1999). Spokenlanguage dialogue: From theory to pra ti e. In Pro . IEEE Workshop on Automati Spee h Re ognition and Understanding, ASRUU99.Litman, D. J., & Pan, S. (2000). Predi ting and adapting to poor spee h re ognition in aspoken dialogue system. In Pro . of the Seventeenth National Conferen e on Arti ialIntelligen e, AAAI-2000.Litman, D. J., Pan, S., & Walker, M. A. (1998). Evaluating Response Strategies in a Web-Based Spoken Dialogue Agent. In Pro eedings of Thirty Sixth Annual Meeting of theAsso iation of Computational Linguisti s, pp. 780{787.Litman, D. J., Walker, M. A., & Kearns, M. J. (1999). Automati dete tion of poor spee hre ognition at the dialogue level. In Pro eedings of the Thirty Seventh Annual Meetingof the Asso iation of Computational Linguisti s, pp. 309{316.Niimi, Y., & Kobayashi., Y. (1996). A dialog ontrol strategy based on the reliability ofspee h re ognition. In Pro . of the International Symposium on Spoken Dialogue, pp.157{160.Roy, N., Pineau, J., & Thrun, S. (2000). Spoken dialog management for robots. In Pro eed-ings of the 39th Annual Meeting of the Asso iation for Computational Linguisti s.Sanderman, A., Sturm, J., den Os, E., Boves, L., & Cremers, A. (1998). Evaluation ofthe dut htrain timetable information system developed in the ARISE proje t. InIntera tive Voi e Te hnology for Tele ommuni ations Appli ations, IVTTA, pp. 91{96.Singh, S., Kearns, M. S., Litman, D. J., & Walker, M. A. (1999). Reinfor ement learningfor spoken dialogue systems. In Pro . NIPS99.132\nOptimizing Dialogue ManagementSmith, R. W. (1998). An evaluation of strategies for sele tively verifying utteran e meaningsin spoken natural language dialog. International Journal of Human-Computer Studies,48, 627{647.Sproat, R., & Olive, J. (1995). An approa h to text-to-spee h synthesis. In Kleijn, W. B.,& Paliwal, K. K. (Eds.), Spee h Coding and Synthesis, pp. 611{633. Elsevier.Sutton, R. S., & Barto, A. G. (1998). Reinfor ement Learning. MIT Press.Tesauro, G. J. (1995). Temporal di eren e learning and td-gammon. Communi ations ofthe ACM, 38, 58{68.Walker, M. A. (2000). An appli ation of reinfor ement learning to dialogue strategy sele tionin a spoken dialogue system for email. Journal of Arti ial Intelligen e Resear h, 12,387{416.Walker, M. A., Fromer, J. C., & Narayanan, S. (1998a). Learning optimal dialogue strate-gies: A ase study of a spoken dialogue agent for email. In Pro eedings of the 36thAnnual Meeting of the Asso iation of Computational Linguisti s, COLING/ACL 98,pp. 1345{1352.Walker, M. A., Litman, D. J., Kamm, C. A., & Abella, A. (1998b). Evaluating spokendialogue agents with PARADISE: Two ase studies. Computer Spee h and Language,12 (3).Walker, M., Rambow, O., & Rogati, M. (2001). Spot: A trainable senten e planner. InPro eedings of the North Ameri an Meeting of the Asso iation for ComputationalLinguisti s.Walker, M. A., & Whittaker, S. (1990). Mixed initiative in dialogue: An investigation intodis ourse segmentation. In Pro . 28th Annual Meeting of the ACL, pp. 70{79.Young, S. J.(2000). Probablisti Methods in Spoken Dialogue Systems.. In Philosophi alTransa tions of the Royal So iety (Series A) 358(1769), pp. 1389{1402.Watkins, C. J. (1989). Models of Delayed Reinfor ement Learning. Ph.D. thesis, CambridgeUniversity. 133"
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2011,
    "abstractText" : null,
    "creator" : "dvipsk 5.86 p1.5b Copyright 1996-2000 ASCII Corp.(www-ptex@ascii.co.jp)"
  }
}