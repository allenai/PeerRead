{
  "name" : "1601.08188.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "LIPREADING WITH LONG SHORT-TERM MEMORY",
    "authors" : [ "Michael Wand", "Jan Koutnı́k", "Jürgen Schmidhuber" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Index Terms— Lipreading, Long Short-Term Memory, Recurrent Neural Networks, Image Recognition\n1. INTRODUCTION\nIt is well-known that humans understand speech not only by listening, but also by taking visual cues into account [1]. Hearing-impaired persons are in fact able to comprehend human speech by purely visual lipreading, i.e. by processing visual information from a speaker’s lips and face. Consequently, research on making lipreading available to electronic speech recognition and processing systems has been of interest for some decades, with pioneering work done by Petajan [2]. His PhD thesis proposed to use lipreading to augment conventional automatic speech recognition (ASR), yet later researchers started to perform purely visual speech recognition [3], which is also the goal of this study. Lipreading systems typically consist of (at least) feature extraction and classification. The feature extraction can become quite complex: Many recent lipreading systems, e.g. [4, 5], use a lip tracking system as a first stage, followed by versatile image features such as Active Appearance Models [6] or\nThis research was supported by the FP7 Marie Curie Initial Training Network PROTOTOUCH (grant #317100) and the Swiss National Science Foundation grant “Advanced Reinforcement Learning” (grant #156682).\nLocal Binary Patterns [7]. Classification is frequently done with Support Vector Machines (SVMs), e.g. [7], or Hidden Markov Models (HMMs), e.g. [4, 5, 8, 9].\nOur aim is to replace the complete visual speech recognition pipeline with a compact neural network architecture. Neural networks (NNs) have become increasingly popular in conventional speech recognition, first as feature extractors in an HMM-based architecture [10–12], more recently replacing the entire processing chain [13]. For the latter, the Long Short Term Memory (LSTM; [14]) architecture is typically used. Consequently, our approach to the lipreading problem uses a NN that chains feed-forward layers and LSTM layers, described in detail in subsection 4.2. Manual feature extraction is no longer required. The NN inputs are now the raw mouth images, as is common in modern computer vision tasks, but stands in stark contrast e.g. to [5, 7].\n2. RELATED WORK\nLipreading has been used as a complementary modality for speech recognition from noisy audio data [2, 15], as well as for purely visual speech recognition [3, 16, 17]. The latter gives rise to a Silent Speech interface, which is defined as a system “enabling speech communication to take place when an audible acoustic signal is unavailable” [18]. Silent Speech technology has a large number of applications: It allows persons with certain speech impairments (e.g. laryngectomees, whose voice box (larynx) has been removed) to communicate, as well as enabling confidential and undisturbing communication in public places [18]. Further uses of lipreading have been proposed, e.g. automatic speech extraction from surveillance videos and its interpretation for forensic purposes [5]. Lipreading has been augmented with ultrasound images of the tongue and vocal tract [19–21]. Furthermore, there are Silent Speech interfaces based on very different principles, like speech recognition from electromyography [22–25] or (electro-)magnetic articulography [26].\nNNs have been used in speech recognition as feature extractors in HMM-based speech recognizers [10, 11]. Neural networks, LSTMs in particular, started to replace larger parts of the speech processing chain previously dominated by HMMs. An end-to-end neural network system [27,28] finally outperformed HMM-based systems and achieved the best performance (16% error) on the large Switchboard Hub5’00\nar X\niv :1\n60 1.\n08 18\n8v 1\n[ cs\n.C V\n] 2\n9 Ja\nn 20\n16\nspeech recognition benchmark [29]. Massively parallel graphics processing units (GPUs) became available in the last few years. Since then, Convolutional NNs (CNNs) trained by gradient descent [30] dominate, e.g. [31], the area of image recognition, as well as related tasks like object detection and segmentation. The first CNN application in lipreading [17] uses the CNN as a preprocessor for an HMM-based sequence classifier.\n3. THE GRID DATA CORPUS\nOur experiments were performed using the GRID audiovisual corpus [32]1, consisting of video and audio recordings of 34 speakers saying 1000 sentences each. The total length of the recordings is 28 hours; two example video frames are shown in Figure 1. Each of the sentences has a fixed structure: command(4) + color(4) + preposition(4) + letter(25) + digit(10) + adverb(4), for example “Place red at J 2, please”, where the number of alternatives words is given in parentheses. A total of 51 different words are contained in the GRID corpus; the alternative words for each of the six sentence parts are distributed uniformly. The letter W is excluded because its pronunciation is vastly longer than for any other letter.\nSentences have a fixed length of 3 seconds at a frame rate of 25 frames per second. Each sentences thus spans across 75 frames. Video data bis available in “normal” and “high” quality; the normal quality video with 360×288 pixel resolution, converted to greyscale, was used. Unreadable videos, in particular for speaker number 8, were discarded. The framelevel alignments distributed with the corpus were used to obtain word level segmentations of the video, causing the training dataset to consist of 6 · 1000 = 6000 single words per speaker. The acoustic part of the GRID corpus was not used.\nA 40 × 40 pixel window containing the mouth area from each video frame was extracted using the following procedure. The face area localized using the Mathematica FindFaces[ ] function was converted into the LAB colorspace. The A component pixels were multiplied element-wise by a Gaussian matrix (with mean located at 30% of of the\n1Publicly available at http://spandh.dcs.shef.ac.uk/ gridcorpus\nimage height along the middle column) and σ = 500 pixels, and rescaled into [0, 1] interval. The center of mass of pixels that have value above 0.9 was considered as the mouth center. The face area size was inflated by factor of 1.5, scaled to 128 pixel width, and a window of 40 × 40 pixels with the mouth coordinates in the center was extracted. This patch was converted to greyscale, the contrast was maximized (i.e. all pixel values were remapped to [0, 1] interval), and all the values in the complete dataset were standardized.\nSpeakers 1–19 from the entire GRID were used: speakers 1-9 form the development set that was used to determine the parameter settings; speakers 10–19 form the evaluation set, held back until the final evaluation of the systems. All experiments are speaker-dependent, i.e. training and test data for the classifiers were always taken from the same speaker. The results reported in this paper are averaged over the speakers. The data for each speaker was randomly divided into training, validation, and test sets, where the latter two contain five sample videos of each word, i.e. a total of 51 · 5 = 255 samples each. The training data is however highly unbalanced: For example, each letter from “a” to “z” appears 30 times, whereas each color appears 240 times.\n4. METHODS"
    }, {
      "heading" : "4.1. Baseline Feature Extraction and Classification",
      "text" : "The NN-based lipreader was compared to a baseline SVM classifier using conventional features, namely Eigenlips [15], which were used as a baseline feature in [17], and Histograms of Oriented Gradients (HOG) [33] as a more complex feature which yielded good performance in preliminary experiments.\nEigenlip features are created from raw frames by computing the PCA decomposition on the training data and then transforming all the images by multiplication with the PCA matrix, retaining only a certain number of dimensions ordered by maximal variance. HOG is originally a feature extractor for object recognition [33]; it divides the image window into small spatial regions (cells) and accumulates a local 1-D histogram of gradient directions or edge orientations over the pixels in each cell. Histogram entries are normalized over larger spatial areas. The HOG features were obtained using the VLFeat library [34].\nSince SVMs are not sequence classifiers, a single feature vector has to be computed from the sequence of frames representing each word, called sequence feature vector. Sequences vary in length. For example, a typical letter “a” is pronounced in 3–4 frames, whereas a longer word like “please” can occupy more than 10 frames. In the sequence feature vector, all frames are stacked while enforcing a specified vector length: frames are repeated if a sequence is shorter than this length, neighboring frames of longer sequences are averaged."
    }, {
      "heading" : "4.2. Neural Network Lipreader",
      "text" : "Neural networks (NNs) consist of processing units (neurons) connected by trainable weighted connections. The neurons are typically organized in layers, which can be broadly distinguished as follows based on their connectivity: (1) feedforward NNs pass the input signal to the output neurons without allowing cyclic computations; (2) recurrent NNs wire the connections in a cycle which forms a temporal memory. NNs are, in the supervised case, typically trained by gradient descent, which is realized by error back-propagation through the layers followed by adjustment of the weights. In the case of recurrent NNs the error propagates along the time axis as well, referred to as back-propagation through time, implemented by unfolding the recurrent connections into a feedforward structure as deep as the length of the sequence. Such deep networks cause the gradient to explode or vanish [35], which can be fixed by replacing a single recurrent NN unit by a LSTM cell that avoids the problem by linear recurrent connection of a cell inside the LSTM unit [14]. The information flow through the LSTM cell is regulated by input, output and forget [36] gates using multiplicative connections. See [37] for detailed LSTM description, analysis and setup.\n5. EXPERIMENTS"
    }, {
      "heading" : "5.1. Experimental Setup",
      "text" : "Two feature extractions (Eigenlips and HOG), both combined with the SVM classifier, were compared to the LSTM lipreader. All parameters were optimized on the development speakers 1–9. The error on all systems is reported on the speaker-dependent test set, no training error is reported.\nIn a series of experiments on the speakers 1–9, the optimal configuration of the feature extraction for the SVM experiments was determined, as well as the best neural network structure. The best PCA cutoff for the Eigenlip features was at 100 components, the best HOG cell size was 8. Best SVM recognition results were obtained with a linear kernel at a sequence feature vector length of 6 frames. Increasing the fea-\nture vector length did not improve the accuracy, almost certainly because among the short video sequences containing letters, where most errors occur (see subsection 5.2), 6 frames already cover the entire information available. Taking this observation into account, we hypothesize that a dedicated sequence classifier like an HMM would not substantially improve the classification accuracy on this corpus.\nThe input data for the SVM can become very highdimensional, which we assume to be the reason why higherdegree polynomial SVM kernels yielded lower accuracy than linear kernels. Zhao et al. [7] report that second-degree polynomial SVM kernels perform the best, however their SVM classifier treats the sequential information differently.\nThe best LSTM lipreader consists of one feed-forward layer followed by two recurrent LSTM layers, 128 units (neurons / LSTM cells) each, and a softmax layer with 51 units that perform the word classification. The learning rate was set to 0.02, momentum was not used, and early stopping with a delay of 10 epochs was used. Weights were initialized from uniform distribution over the range [−0.05, 0.05]."
    }, {
      "heading" : "5.2. Results",
      "text" : "The word level classification accuracies are summarized in Table 1. The LSTM lipreader yields statistically significant improvement (one-tailed t-test with p = 0.05) over the conventional features combined with the SVM classifier. The Eigenlip and HOG features perform similarly, both worse than the LSTM lipreader. On the Evaluation Speakers 10– 19, the LSTM lipreader improves the accuracy by 11.6%, compared to the best conventional solution (HOG + SVM).\nFigure 2 shows a typical confusion matrix on speaker 7 data, where classification was performed with the LSTM lipreader. The rows show reference word labels, the columns show hypotheses. The confusion on letters is far higher (upper part of the matrix) than on longer words (lower part of the matrix). For this speaker and configuration the accuracy on the letters is 69.8% (at 4% chance level), the accuracy on the non-letter words is 93.4% (at 3.8% chance level). The total accuracy is 82.0%.\nThe discrepancy between letters and other words is caused by three major factors: First, the letters from ’a’ to ’z’ are highly confusing even under optimal circumstances. In particular, this applies to voiced and voiceless versions of the same letter, like ‘p’ and ‘b’. Consequently, visemes (visual units for recognition) frequently do not distinguish between such similarly-looking sounds at all (see e.g. [38] and the references therein). Second, single letters video sequences are often are very short, sometimes consisting of only 3-4 frames. This means that very little data is available and also that the letter pronunciation is highly influenced by adjacent sounds. This context does not help distinguishing different letters, since the parts of the GRID sentences are statistically independent from each other. We note that [17] report phone accuracy on a corpus which consists of whole words: here the context plays a great role in improving recognition.\nThe confusion matrices are qualitatively similar across all speakers and all experimental setups – the longer words are recognized with close to 100% accuracy, whereas confusion is highest on the letters.\n6. CONCLUSION\nThis study shows that the neural network based lipreading system applied to raw images of the mouth regions achieves significantly better word accuracy than a system based on a conventional processing pipeline utilizing feature extraction and classification. The LSTM lipreader with a single feed-forward network, which learns the features automatically together with training the LSTM sequence classifier, consistently achieved almost 80% word accuracy in speakerdependent lipreading.\nThe experiments, not described in this paper, also included the highly popular CNNs instead of the fully connected feed-forward layer, but the results did not improve.\nOne of the possible reasons is that the small, 40 × 40 pixel area already contains just enough information for the classification. Experiments with CNNs and large image sizes as well as evaluation of a speaker-independent LSTM lipreader are the subject of future experiments.\n7. REFERENCES\n[1] H. McGurk and J. MacDonald, “Hearing Lips and Seeing Voices,” Nature, vol. 264, no. 5588, pp. 746 – 748, 1976.\n[2] E. D. Petajan, “Automatic Lipreading to Enhance Speech Recognition (Speech Reading) ,” Ph.D. dissertation, University of Illinois at Urbana-Champaign, 1984.\n[3] G. I. Chiou and J.-N. Hwang, “Lipreading from Color Video,” IEEE Transactions on Image Processing, vol. 6, no. 8, pp. 1192 – 1195, 1997.\n[4] Y. Lan, R. Harvey, B.-J. Theobald, E.-J. Ong, and R. Bowden, “Comparing Visual Features for Lipreading,” in Proc. of the Int. Conference on Auditory-Visual Speech Processing, 2009, pp. 102 – 106.\n[5] R. Bowden, S. Cox, R. Harvey, Y. Lan, E.-J. Ong, G. Owen, and B.-J. Theobald, “Recent Developments in Automated Lip-reading,” in Proc. SPIE, 2013.\n[6] T. F. Cootes, G. J. Edwards, and C. J. Taylor, “Active Appearance Models,” IEEE Trans. on Pattern Anal. and Machine Intel., vol. 23, no. 6, pp. 681 – 685, 2001.\n[7] G. Zhao, M. Barnard, and M. Pietikäinen, “Lipreading With Local Spatiotemporal Descriptors,” IEEE Trans.s on Multimedia, vol. 11, no. 7, pp. 1254 – 1265, 2009.\n[8] T. Hueber, G. Chollet, B. Denby, G. Dreyfus, and M. Stone, “Continuous-Speech Phone Recognition from Ultrasound and Optical Images of the Tongue and Lips,” in Proc. Interspeech, 2007, pp. 658–661.\n[9] F. Tao and C. Busso, “Lipreading Approach for Isolated Digits Recognition Under Whisper and Neutral Speech,” in Proc. Interspeech, 2014, pp. 1154 – 1158.\n[10] H. Bourlard and N. Morgan, Connectionist Speech Recognition. A Hybrid Approach. Kluwer Academic Publishers, 1994.\n[11] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. N. Sainath, and B. Kingsbury, “Deep Neural Networks for Acoustic Modeling in Speech Recognition,” IEEE Signal Proc. Magazine, vol. 29, no. 6, pp. 82 – 97, 2012.\n[12] A. Graves, N. Jaitly, and A. Mohamed, “Hybrid Speech Recognition with Deep Bidirectional LSTM,” in Proc. ASRU, 2013, pp. 273 – 278.\n[13] A. Graves, A. Mohamed, and G. Hinton, “Speech Recognition with Deep Recurrent Neural Networks,” in Proc. ICASSP, 2013, pp. 6645 – 6649.\n[14] S. Hochreiter and J. Schmidhuber, “Long Short-Term Memory,” Neural Comp., vol. 9, pp. 1735 – 1780, 1997.\n[15] C. Bregler and Y. Konig, “‘Eigenlips’ for Robust Speech Recognition,” in Proc. ICASSP, 1994, pp. 669 – 672.\n[16] R. Bowden, S. Cox, R. Harvey, Y. Lan, E.-J. Ong, G. Owen, and B.-J. Theobald, “Is Automated Conversion of Video to Text a Reality?” in Proc. SPIE, 2012.\n[17] K. Noda, Y. Yamaguchi, K. Nakadai, H. G. Okuno, and T. Ogata, “Lipreading using Convolutional Neural Network,” in Proc. Interspeech, 2014, pp. 1149 – 1153.\n[18] B. Denby, T. Schultz, K. Honda, T. Hueber, and J. Gilbert, “Silent Speech Interfaces,” Speech Communication, vol. 52, no. 4, pp. 270 – 287, 2010.\n[19] B. Denby and M. Stone, “Speech Synthesis from Real Time Ultrasound Images of the Tongue,” in Proc. ICASSP, 2004, pp. I–685 – I–688.\n[20] T. Hueber, G. Aversano, G. Chollet, B. Denby, G. Dreyfus, Y. Oussar, P. Roussel, and M. Stone, “Eigentongue Feature Extraction for an Ultrasound-based Silent Speech Interface,” in Proc. ICASSP, 2007, pp. I– 1245 – I–1248.\n[21] T. Hueber, E.-L. Benaroya, G. Chollet, B. Denby, G. Dreyfus, and M. Stone, “Development of a Silent Speech Interface Driven by Ultrasound and Optical Images of the Tongue and Lips,” Speech Communication, vol. 52, pp. 288 – 300, 2010.\n[22] N. Sugie and K. Tsunoda, “A Speech Prosthesis Employing a Speech Synthesizer – Vowel Discrimination from Perioral Muscle Activities and Vowel Production,” IEEE Transactions on Biomedical Engineering, vol. 32, no. 7, pp. 485 – 490, 1985.\n[23] M. S. Morse, S. H. Day, B. Trull, and H. Morse, “Use of Myoelectric Signals to Recognize Speech,” in Proc. 11th Annual Conference of the IEEE Engineering in Medicine and Biology Society, 1989, pp. 1793 – 1794.\n[24] T. Schultz and M. Wand, “Modeling Coarticulation in Large Vocabulary EMG-based Speech Recognition,” Speech Comm., vol. 52, no. 4, pp. 341 – 353, 2010.\n[25] M. Wand, M. Janke, and T. Schultz, “Tackling Speaking Mode Varieties in EMG-based Speech Recognition,” IEEE Transaction on Biomedical Engineering, vol. 61, no. 10, pp. 2515 – 2526, 2014.\n[26] M. J. Fagan, S. R. Ell, J. M. Gilbert, E. Sarrazin, and P. M. Chapman, “Development of a (Silent) Speech Recognition System for Patients Following Laryngectomy,” Medical Engineering and Physics, vol. 30, pp. 419 – 425, 2008.\n[27] A. Graves, S. Fernández, F. Gomez, and J. Schmidhuber, “Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks,” in Proc. ICML, 2006, pp. 369 – 376.\n[28] A. Graves and N. Jaitly, “Towards End-To-End Speech Recognition with Recurrent Neural Networks,” in Proc. ICML, 2014, pp. 1764 – 1772.\n[29] A. Hannun, C. Case, J. Casper, B. Catanzaro, G. Diamos, E. Elsen, R. Prenger, S. Satheesh, S. Sengupta, A. Coates, and A. Y. Ng, “DeepSpeech: Scaling up endto-end speech recognition,” arXiv: 1412: 5567v1, 2014.\n[30] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackeland, “Backpropagation Applied to Handwritten Zip Code Recognition,” Neural Computation, vol. 1, pp. 541 – 551, 1989.\n[31] D. Ciresan, U. Meier, J. Masci, and J. Schmidhuber, “A Committee of Neural Networks for Traffic Sign Classification,” in Proc. IJCNN, 2011, pp. 1918 – 1921.\n[32] M. Cooke, J. Barker, S. Cunningham, and X. Shao, “An Audio-Visual Corpus for Speech Perception and Automatic Speech Recognition,” Journal of the Acoustical Soc. of America, vol. 120, no. 5, pp. 2421 – 2424, 2006.\n[33] N. Dalal and B. Triggs, “Histograms of Oriented Gradients for Human Detection,” in Proc. CVPR, vol. 1, 2005, pp. 886 – 893.\n[34] A. Vedaldi and B. Fulkerson, “VLFeat: An Open and Portable Library of Computer Vision Algorithms,” http: //www.vlfeat.org/, 2008.\n[35] S. Hochreiter, Y. Bengio, P. Frasconi, and J. Schmidhuber, A Field Guide to Dynamical Recurrent Neural Networks. IEEE Press, 2001, ch. Gradient flow in recurrent nets: the difficulty of learning long-term dependencies.\n[36] F. A. Gers, J. Schmidhuber, and F. Cummins, “Learning to forget: Continual prediction with LSTM,” Neural Computation, vol. 12, no. 10, pp. 2451–2471, 2000.\n[37] K. Greff, R. K. Srivastava, J. Koutnı́k, B. R. Steunebrink, and J. Schmidhuber, “LSTM: A search space odyssey,” CoRR, vol. abs/1503.04069, 2015. [Online]. Available: http://arxiv.org/abs/1503.04069\n[38] L. Cappelletta and N. Harte, “Viseme Definitions Comparison for Visual-only Speech Recognition,” in Proc. EUSIPCO, 2011, pp. 2109 – 2113."
    } ],
    "references" : [ {
      "title" : "Hearing Lips and Seeing Voices",
      "author" : [ "H. McGurk", "J. MacDonald" ],
      "venue" : "Nature, vol. 264, no. 5588, pp. 746 – 748, 1976.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 1976
    }, {
      "title" : "Automatic Lipreading to Enhance Speech Recognition (Speech Reading)",
      "author" : [ "E.D. Petajan" ],
      "venue" : "Ph.D. dissertation, University of Illinois at Urbana-Champaign, 1984.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 1984
    }, {
      "title" : "Lipreading from Color Video",
      "author" : [ "G.I. Chiou", "J.-N. Hwang" ],
      "venue" : "IEEE Transactions on Image Processing, vol. 6, no. 8, pp. 1192 – 1195, 1997.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Comparing Visual Features for Lipreading",
      "author" : [ "Y. Lan", "R. Harvey", "B.-J. Theobald", "E.-J. Ong", "R. Bowden" ],
      "venue" : "Proc. of the Int. Conference on Auditory-Visual Speech Processing, 2009, pp. 102 – 106.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Recent Developments in Automated Lip-reading",
      "author" : [ "R. Bowden", "S. Cox", "R. Harvey", "Y. Lan", "E.-J. Ong", "G. Owen", "B.-J. Theobald" ],
      "venue" : "Proc. SPIE, 2013.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Active Appearance Models",
      "author" : [ "T.F. Cootes", "G.J. Edwards", "C.J. Taylor" ],
      "venue" : "IEEE Trans. on Pattern Anal. and Machine Intel., vol. 23, no. 6, pp. 681 – 685, 2001.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Lipreading With Local Spatiotemporal Descriptors",
      "author" : [ "G. Zhao", "M. Barnard", "M. Pietikäinen" ],
      "venue" : "IEEE Trans.s on Multimedia, vol. 11, no. 7, pp. 1254 – 1265, 2009.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Continuous-Speech Phone Recognition from Ultrasound and Optical Images of the Tongue and Lips",
      "author" : [ "T. Hueber", "G. Chollet", "B. Denby", "G. Dreyfus", "M. Stone" ],
      "venue" : "Proc. Interspeech, 2007, pp. 658–661.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Lipreading Approach for Isolated Digits Recognition Under Whisper and Neutral Speech",
      "author" : [ "F. Tao", "C. Busso" ],
      "venue" : "Proc. Interspeech, 2014, pp. 1154 – 1158.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Connectionist Speech Recognition",
      "author" : [ "H. Bourlard", "N. Morgan" ],
      "venue" : "A Hybrid Approach. Kluwer Academic Publishers,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1994
    }, {
      "title" : "Deep Neural Networks for Acoustic Modeling in Speech Recognition",
      "author" : [ "G. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "A. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T.N. Sainath", "B. Kingsbury" ],
      "venue" : "IEEE Signal Proc. Magazine, vol. 29, no. 6, pp. 82 – 97, 2012.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Hybrid Speech Recognition with Deep Bidirectional LSTM",
      "author" : [ "A. Graves", "N. Jaitly", "A. Mohamed" ],
      "venue" : "Proc. ASRU, 2013, pp. 273 – 278.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Speech Recognition with Deep Recurrent Neural Networks",
      "author" : [ "A. Graves", "A. Mohamed", "G. Hinton" ],
      "venue" : "Proc. ICASSP, 2013, pp. 6645 – 6649.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Long Short-Term Memory",
      "author" : [ "S. Hochreiter", "J. Schmidhuber" ],
      "venue" : "Neural Comp., vol. 9, pp. 1735 – 1780, 1997.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Eigenlips’ for Robust Speech Recognition",
      "author" : [ "C. Bregler", "Y. Konig" ],
      "venue" : "Proc. ICASSP, 1994, pp. 669 – 672.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "Lipreading using Convolutional Neural Network",
      "author" : [ "K. Noda", "Y. Yamaguchi", "K. Nakadai", "H.G. Okuno", "T. Ogata" ],
      "venue" : "Proc. Interspeech, 2014, pp. 1149 – 1153.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Silent Speech Interfaces",
      "author" : [ "B. Denby", "T. Schultz", "K. Honda", "T. Hueber", "J. Gilbert" ],
      "venue" : "Speech Communication, vol. 52, no. 4, pp. 270 – 287, 2010.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Speech Synthesis from Real Time Ultrasound Images of the Tongue",
      "author" : [ "B. Denby", "M. Stone" ],
      "venue" : "Proc. ICASSP, 2004, pp. I–685 – I–688.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Eigentongue Feature Extraction for an Ultrasound-based Silent Speech Interface",
      "author" : [ "T. Hueber", "G. Aversano", "G. Chollet", "B. Denby", "G. Dreyfus", "Y. Oussar", "P. Roussel", "M. Stone" ],
      "venue" : "Proc. ICASSP, 2007, pp. I– 1245 – I–1248.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Development of a Silent Speech Interface Driven by Ultrasound and Optical Images of the Tongue and Lips",
      "author" : [ "T. Hueber", "E.-L. Benaroya", "G. Chollet", "B. Denby", "G. Dreyfus", "M. Stone" ],
      "venue" : "Speech Communication, vol. 52, pp. 288 – 300, 2010.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "A Speech Prosthesis Employing a Speech Synthesizer – Vowel Discrimination from Perioral Muscle Activities and Vowel Production",
      "author" : [ "N. Sugie", "K. Tsunoda" ],
      "venue" : "IEEE Transactions on Biomedical Engineering, vol. 32, no. 7, pp. 485 – 490, 1985.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 1985
    }, {
      "title" : "Use of Myoelectric Signals to Recognize Speech",
      "author" : [ "M.S. Morse", "S.H. Day", "B. Trull", "H. Morse" ],
      "venue" : "Proc. 11th Annual Conference of the IEEE Engineering in Medicine and Biology Society, 1989, pp. 1793 – 1794.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 1989
    }, {
      "title" : "Modeling Coarticulation in Large Vocabulary EMG-based Speech Recognition",
      "author" : [ "T. Schultz", "M. Wand" ],
      "venue" : "Speech Comm., vol. 52, no. 4, pp. 341 – 353, 2010.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Tackling Speaking Mode Varieties in EMG-based Speech Recognition",
      "author" : [ "M. Wand", "M. Janke", "T. Schultz" ],
      "venue" : "IEEE Transaction on Biomedical Engineering, vol. 61, no. 10, pp. 2515 – 2526, 2014.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Development of a (Silent) Speech Recognition System for Patients Following Laryngectomy",
      "author" : [ "M.J. Fagan", "S.R. Ell", "J.M. Gilbert", "E. Sarrazin", "P.M. Chapman" ],
      "venue" : "Medical Engineering and Physics, vol. 30, pp. 419 – 425, 2008.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks",
      "author" : [ "A. Graves", "S. Fernández", "F. Gomez", "J. Schmidhuber" ],
      "venue" : "Proc. ICML, 2006, pp. 369 – 376.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Towards End-To-End Speech Recognition with Recurrent Neural Networks",
      "author" : [ "A. Graves", "N. Jaitly" ],
      "venue" : "Proc. ICML, 2014, pp. 1764 – 1772.",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "DeepSpeech: Scaling up endto-end speech recognition",
      "author" : [ "A. Hannun", "C. Case", "J. Casper", "B. Catanzaro", "G. Diamos", "E. Elsen", "R. Prenger", "S. Satheesh", "S. Sengupta", "A. Coates", "A.Y. Ng" ],
      "venue" : "arXiv: 1412: 5567v1, 2014.",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Backpropagation Applied to Handwritten Zip Code Recognition",
      "author" : [ "Y. LeCun", "B. Boser", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackeland" ],
      "venue" : "Neural Computation, vol. 1, pp. 541 – 551, 1989.",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 1989
    }, {
      "title" : "A Committee of Neural Networks for Traffic Sign Classification",
      "author" : [ "D. Ciresan", "U. Meier", "J. Masci", "J. Schmidhuber" ],
      "venue" : "Proc. IJCNN, 2011, pp. 1918 – 1921.",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "An Audio-Visual Corpus for Speech Perception and Automatic Speech Recognition",
      "author" : [ "M. Cooke", "J. Barker", "S. Cunningham", "X. Shao" ],
      "venue" : "Journal of the Acoustical Soc. of America, vol. 120, no. 5, pp. 2421 – 2424, 2006.",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Histograms of Oriented Gradients for Human Detection",
      "author" : [ "N. Dalal", "B. Triggs" ],
      "venue" : "Proc. CVPR, vol. 1, 2005, pp. 886 – 893.",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "VLFeat: An Open and Portable Library of Computer Vision Algorithms",
      "author" : [ "A. Vedaldi", "B. Fulkerson" ],
      "venue" : "http: //www.vlfeat.org/, 2008.",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "A Field Guide to Dynamical Recurrent Neural Networks",
      "author" : [ "S. Hochreiter", "Y. Bengio", "P. Frasconi", "J. Schmidhuber" ],
      "venue" : "IEEE Press,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 2001
    }, {
      "title" : "Learning to forget: Continual prediction with LSTM",
      "author" : [ "F.A. Gers", "J. Schmidhuber", "F. Cummins" ],
      "venue" : "Neural Computation, vol. 12, no. 10, pp. 2451–2471, 2000.",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "LSTM: A search space odyssey",
      "author" : [ "K. Greff", "R.K. Srivastava", "J. Koutnı́k", "B.R. Steunebrink", "J. Schmidhuber" ],
      "venue" : "CoRR, vol. abs/1503.04069, 2015. [Online]. Available: http://arxiv.org/abs/1503.04069",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Viseme Definitions Comparison for Visual-only Speech Recognition",
      "author" : [ "L. Cappelletta", "N. Harte" ],
      "venue" : "Proc. EUSIPCO, 2011, pp. 2109 – 2113.",
      "citeRegEx" : "38",
      "shortCiteRegEx" : null,
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "It is well-known that humans understand speech not only by listening, but also by taking visual cues into account [1].",
      "startOffset" : 114,
      "endOffset" : 117
    }, {
      "referenceID" : 1,
      "context" : "Consequently, research on making lipreading available to electronic speech recognition and processing systems has been of interest for some decades, with pioneering work done by Petajan [2].",
      "startOffset" : 186,
      "endOffset" : 189
    }, {
      "referenceID" : 2,
      "context" : "His PhD thesis proposed to use lipreading to augment conventional automatic speech recognition (ASR), yet later researchers started to perform purely visual speech recognition [3], which is also the goal of this study.",
      "startOffset" : 176,
      "endOffset" : 179
    }, {
      "referenceID" : 3,
      "context" : "[4, 5], use a lip tracking system as a first stage, followed by versatile image features such as Active Appearance Models [6] or",
      "startOffset" : 0,
      "endOffset" : 6
    }, {
      "referenceID" : 4,
      "context" : "[4, 5], use a lip tracking system as a first stage, followed by versatile image features such as Active Appearance Models [6] or",
      "startOffset" : 0,
      "endOffset" : 6
    }, {
      "referenceID" : 5,
      "context" : "[4, 5], use a lip tracking system as a first stage, followed by versatile image features such as Active Appearance Models [6] or",
      "startOffset" : 122,
      "endOffset" : 125
    }, {
      "referenceID" : 6,
      "context" : "Local Binary Patterns [7].",
      "startOffset" : 22,
      "endOffset" : 25
    }, {
      "referenceID" : 6,
      "context" : "[7], or Hidden Markov Models (HMMs), e.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 3,
      "context" : "[4, 5, 8, 9].",
      "startOffset" : 0,
      "endOffset" : 12
    }, {
      "referenceID" : 4,
      "context" : "[4, 5, 8, 9].",
      "startOffset" : 0,
      "endOffset" : 12
    }, {
      "referenceID" : 7,
      "context" : "[4, 5, 8, 9].",
      "startOffset" : 0,
      "endOffset" : 12
    }, {
      "referenceID" : 8,
      "context" : "[4, 5, 8, 9].",
      "startOffset" : 0,
      "endOffset" : 12
    }, {
      "referenceID" : 9,
      "context" : "Neural networks (NNs) have become increasingly popular in conventional speech recognition, first as feature extractors in an HMM-based architecture [10–12], more recently replacing the entire processing chain [13].",
      "startOffset" : 148,
      "endOffset" : 155
    }, {
      "referenceID" : 10,
      "context" : "Neural networks (NNs) have become increasingly popular in conventional speech recognition, first as feature extractors in an HMM-based architecture [10–12], more recently replacing the entire processing chain [13].",
      "startOffset" : 148,
      "endOffset" : 155
    }, {
      "referenceID" : 11,
      "context" : "Neural networks (NNs) have become increasingly popular in conventional speech recognition, first as feature extractors in an HMM-based architecture [10–12], more recently replacing the entire processing chain [13].",
      "startOffset" : 148,
      "endOffset" : 155
    }, {
      "referenceID" : 12,
      "context" : "Neural networks (NNs) have become increasingly popular in conventional speech recognition, first as feature extractors in an HMM-based architecture [10–12], more recently replacing the entire processing chain [13].",
      "startOffset" : 209,
      "endOffset" : 213
    }, {
      "referenceID" : 13,
      "context" : "For the latter, the Long Short Term Memory (LSTM; [14]) architecture is typically used.",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 4,
      "context" : "to [5, 7].",
      "startOffset" : 3,
      "endOffset" : 9
    }, {
      "referenceID" : 6,
      "context" : "to [5, 7].",
      "startOffset" : 3,
      "endOffset" : 9
    }, {
      "referenceID" : 1,
      "context" : "Lipreading has been used as a complementary modality for speech recognition from noisy audio data [2, 15], as well as for purely visual speech recognition [3, 16, 17].",
      "startOffset" : 98,
      "endOffset" : 105
    }, {
      "referenceID" : 14,
      "context" : "Lipreading has been used as a complementary modality for speech recognition from noisy audio data [2, 15], as well as for purely visual speech recognition [3, 16, 17].",
      "startOffset" : 98,
      "endOffset" : 105
    }, {
      "referenceID" : 2,
      "context" : "Lipreading has been used as a complementary modality for speech recognition from noisy audio data [2, 15], as well as for purely visual speech recognition [3, 16, 17].",
      "startOffset" : 155,
      "endOffset" : 166
    }, {
      "referenceID" : 15,
      "context" : "Lipreading has been used as a complementary modality for speech recognition from noisy audio data [2, 15], as well as for purely visual speech recognition [3, 16, 17].",
      "startOffset" : 155,
      "endOffset" : 166
    }, {
      "referenceID" : 16,
      "context" : "The latter gives rise to a Silent Speech interface, which is defined as a system “enabling speech communication to take place when an audible acoustic signal is unavailable” [18].",
      "startOffset" : 174,
      "endOffset" : 178
    }, {
      "referenceID" : 16,
      "context" : "laryngectomees, whose voice box (larynx) has been removed) to communicate, as well as enabling confidential and undisturbing communication in public places [18].",
      "startOffset" : 156,
      "endOffset" : 160
    }, {
      "referenceID" : 4,
      "context" : "automatic speech extraction from surveillance videos and its interpretation for forensic purposes [5].",
      "startOffset" : 98,
      "endOffset" : 101
    }, {
      "referenceID" : 17,
      "context" : "Lipreading has been augmented with ultrasound images of the tongue and vocal tract [19–21].",
      "startOffset" : 83,
      "endOffset" : 90
    }, {
      "referenceID" : 18,
      "context" : "Lipreading has been augmented with ultrasound images of the tongue and vocal tract [19–21].",
      "startOffset" : 83,
      "endOffset" : 90
    }, {
      "referenceID" : 19,
      "context" : "Lipreading has been augmented with ultrasound images of the tongue and vocal tract [19–21].",
      "startOffset" : 83,
      "endOffset" : 90
    }, {
      "referenceID" : 20,
      "context" : "Furthermore, there are Silent Speech interfaces based on very different principles, like speech recognition from electromyography [22–25] or (electro-)magnetic articulography [26].",
      "startOffset" : 130,
      "endOffset" : 137
    }, {
      "referenceID" : 21,
      "context" : "Furthermore, there are Silent Speech interfaces based on very different principles, like speech recognition from electromyography [22–25] or (electro-)magnetic articulography [26].",
      "startOffset" : 130,
      "endOffset" : 137
    }, {
      "referenceID" : 22,
      "context" : "Furthermore, there are Silent Speech interfaces based on very different principles, like speech recognition from electromyography [22–25] or (electro-)magnetic articulography [26].",
      "startOffset" : 130,
      "endOffset" : 137
    }, {
      "referenceID" : 23,
      "context" : "Furthermore, there are Silent Speech interfaces based on very different principles, like speech recognition from electromyography [22–25] or (electro-)magnetic articulography [26].",
      "startOffset" : 130,
      "endOffset" : 137
    }, {
      "referenceID" : 24,
      "context" : "Furthermore, there are Silent Speech interfaces based on very different principles, like speech recognition from electromyography [22–25] or (electro-)magnetic articulography [26].",
      "startOffset" : 175,
      "endOffset" : 179
    }, {
      "referenceID" : 9,
      "context" : "NNs have been used in speech recognition as feature extractors in HMM-based speech recognizers [10, 11].",
      "startOffset" : 95,
      "endOffset" : 103
    }, {
      "referenceID" : 10,
      "context" : "NNs have been used in speech recognition as feature extractors in HMM-based speech recognizers [10, 11].",
      "startOffset" : 95,
      "endOffset" : 103
    }, {
      "referenceID" : 25,
      "context" : "An end-to-end neural network system [27,28] finally outperformed HMM-based systems and achieved the best performance (16% error) on the large Switchboard Hub5’00 ar X iv :1 60 1.",
      "startOffset" : 36,
      "endOffset" : 43
    }, {
      "referenceID" : 26,
      "context" : "An end-to-end neural network system [27,28] finally outperformed HMM-based systems and achieved the best performance (16% error) on the large Switchboard Hub5’00 ar X iv :1 60 1.",
      "startOffset" : 36,
      "endOffset" : 43
    }, {
      "referenceID" : 27,
      "context" : "speech recognition benchmark [29].",
      "startOffset" : 29,
      "endOffset" : 33
    }, {
      "referenceID" : 28,
      "context" : "Since then, Convolutional NNs (CNNs) trained by gradient descent [30] dominate, e.",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 29,
      "context" : "[31], the area of image recognition, as well as related tasks like object detection and segmentation.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 15,
      "context" : "The first CNN application in lipreading [17] uses the CNN as a preprocessor for an HMM-based sequence classifier.",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 30,
      "context" : "Our experiments were performed using the GRID audiovisual corpus [32]1, consisting of video and audio recordings of 34 speakers saying 1000 sentences each.",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 0,
      "context" : "gridcorpus image height along the middle column) and σ = 500 pixels, and rescaled into [0, 1] interval.",
      "startOffset" : 87,
      "endOffset" : 93
    }, {
      "referenceID" : 0,
      "context" : "all pixel values were remapped to [0, 1] interval), and all the values in the complete dataset were standardized.",
      "startOffset" : 34,
      "endOffset" : 40
    }, {
      "referenceID" : 14,
      "context" : "The NN-based lipreader was compared to a baseline SVM classifier using conventional features, namely Eigenlips [15], which were used as a baseline feature in [17], and Histograms of Oriented Gradients (HOG) [33] as a more complex feature which yielded good performance in preliminary experiments.",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 15,
      "context" : "The NN-based lipreader was compared to a baseline SVM classifier using conventional features, namely Eigenlips [15], which were used as a baseline feature in [17], and Histograms of Oriented Gradients (HOG) [33] as a more complex feature which yielded good performance in preliminary experiments.",
      "startOffset" : 158,
      "endOffset" : 162
    }, {
      "referenceID" : 31,
      "context" : "The NN-based lipreader was compared to a baseline SVM classifier using conventional features, namely Eigenlips [15], which were used as a baseline feature in [17], and Histograms of Oriented Gradients (HOG) [33] as a more complex feature which yielded good performance in preliminary experiments.",
      "startOffset" : 207,
      "endOffset" : 211
    }, {
      "referenceID" : 31,
      "context" : "HOG is originally a feature extractor for object recognition [33]; it divides the image window into small spatial regions (cells) and accumulates a local 1-D histogram of gradient directions or edge orientations over the pixels in each cell.",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 32,
      "context" : "The HOG features were obtained using the VLFeat library [34].",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 33,
      "context" : "Such deep networks cause the gradient to explode or vanish [35], which can be fixed by replacing a single recurrent NN unit by a LSTM cell that avoids the problem by linear recurrent connection of a cell inside the LSTM unit [14].",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 13,
      "context" : "Such deep networks cause the gradient to explode or vanish [35], which can be fixed by replacing a single recurrent NN unit by a LSTM cell that avoids the problem by linear recurrent connection of a cell inside the LSTM unit [14].",
      "startOffset" : 225,
      "endOffset" : 229
    }, {
      "referenceID" : 34,
      "context" : "The information flow through the LSTM cell is regulated by input, output and forget [36] gates using multiplicative connections.",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 35,
      "context" : "See [37] for detailed LSTM description, analysis and setup.",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 6,
      "context" : "[7] report that second-degree polynomial SVM kernels perform the best, however their SVM classifier treats the sequential information differently.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 36,
      "context" : "[38] and the references therein).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 15,
      "context" : "We note that [17] report phone accuracy on a corpus which consists of whole words: here the context plays a great role in improving recognition.",
      "startOffset" : 13,
      "endOffset" : 17
    } ],
    "year" : 2016,
    "abstractText" : "Lipreading, i.e. speech recognition from visual-only recordings of a speaker’s face, can be achieved with a processing pipeline based solely on neural networks, yielding significantly better accuracy than conventional methods. Feedforward and recurrent neural network layers (namely Long Short-Term Memory; LSTM) are stacked to form a single structure which is trained by back-propagating error gradients through all the layers. The performance of such a stacked network was experimentally evaluated and compared to a standard Support Vector Machine classifier using conventional computer vision features (Eigenlips and Histograms of Oriented Gradients). The evaluation was performed on data from 19 speakers of the publicly available GRID corpus. With 51 different words to classify, we report a best word accuracy on held-out evaluation speakers of 79.6% using the end-toend neural network-based solution (11.6% improvement over the best feature-based solution evaluated).",
    "creator" : "LaTeX with hyperref package"
  }
}