{
  "name" : "1506.06112.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "The Extreme Value Machine",
    "authors" : [ "Ethan M. Rudd", "Lalit P. Jain", "Walter J. Scheirer", "Terrance E. Boult" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "The multi-class margin is traditionally defined as the distance between the decision boundary and the “closest” training samples from a different class [1]. The concept of a maximum-margin (max-margin) classifier has become widespread in part because of theoretical support for the hard-margin linear SVM, as well as high accuracies exhibited by both hard- and soft-margin SVMs in practice.\nThe idea of using “margin distributions” is not new; multiple researchers have explored various definitions and uses of margin distributions, [2, 3, 4, 5, 6], involving techniques such as maximizing the mean or median margin, taking a weighted combination margin, or optimizing the margin mean and variance. These papers demonstrate that leveraging the margin distribution itself can provide better error bounds than those offered by a soft-margin SVM classifier, which in some cases translates into reduced experimental error. One can intuitively rationalize characterizing the margin using mean, median, and variance of the distances of all points to the margin. As recently pointed out by Zhou in [7], “These arguments, however, are all heuristics without theoretical justification.” We argue that a margin distribution definition should use only points near the decision boundary and should have strong theoretical justification. For the first time, the Statistical Extreme Value Theory (EVT) provides us with a novel theoretical underpinning for the form of the margin distribution.\nFor linear SVMs, the margin is computed in the input space, directly separates the data, and has theoretical support for seeking to minimize the chances of errors by keeping points away from the boundary. Non-linear SVMs usually use the kernel trick to achieve their non-linearity, and the margin is then measured in the kernel space. As noted by [8]: “In this setting, the margin being maximized is still the smallest Euclidean distance between the decision surface and the training points, but this time measured in some strange, sometimes infinite dimensional, kernel-induced feature space rather than the original input space. It is less clear whether maximizing the margin in this new space, is meaningful in general.” Considerable research has demonstrated that a large margin in kernel space does not necessarily translate into a large margin in the input space [9, 10]. Moreover, different kernels yield very different margins; there is often no grounded justification for kernel choice, leading to the question of which kernel/margin should be maximized. We note that choosing a mathematically\nar X\niv :1\n50 6.\n06 11\n2v 1\n[ cs\n.L G\n] 1\n9 Ju\nn 20\njustified kernel is a far more general problem than multi-class recognition. Considering Vapnik’s principle never to solve a problem which is more general than the one in which one is actually interested, choosing a non-linear SVM kernel is problematic.\nIn this paper, we consider only margins in the input (data) space1. The data samples defining the input space margin are, by definition, the samples closest to the boundary. Therefore, the margin distances are extreme values generated from a stochastic sampling process, and with some mild assumptions we show that one can invoke EVT [11] and use it to model the extreme value distribution. The first contribution of this paper is the development of a novel Extreme Value Machine (EVM). The EVM derives directly from EVT, giving strong justification for the distribution of input space distances to the estimated max-margin. With that distribution, we compute a probability of a point being within the class boundary. Computing and thresholding this probability is similar to the kernel trick, in that it lifts the data into a new dimension and slices it. The resulting Radial Basis Functions even bear a striking resemblance to Gaussian or Laplacian kernels. However, the EVM is a theoretically derived concept rather than just a convenient trick.\nThe second contribution of this paper is a means of effectively reducing the computational complexity of applying these distributions for classification by finding a near-optimal set of points to retain. We introduce a new problem formulation that leverages properties of the per-sample probability estimate. The need for only distribution tails in our formulation allows the training algorithm to leverage efficient metric-space data structures for an order of magnitude decrease in complexity, while extreme vector selection leverages the greedy approximation of Set Cover with a polynomial time approximate optimal selection of appropriate margin distributions. This approximation allows us to have a scalable non-linear classifier where the default parameters are effective for a wide range of problems. We demonstrate that this optimization increases generalization and accuracy of the classifier, extends scalability, and decreases computational complexity.\nIn contrast to the familiar problem of closed set multi-class classification, where all classes seen at classification time are known at training time, open set recognition leaves open the possibility of unknown or novel classes during classification. Scheirer et. al. [12, 13] recently formalized the concept of open set risk and its role in open set recognition. Our third contribution is the demonstration that the EVM also manages open set risk. We present experiments conducted over a wide range of both closed and open set problems. Our results demonstrate that the EVM is generally superior to or competitive with linear and optimized Gaussian RBF SVMs in the closed set regime and offers comparable or better performance relative to the leading open set W-SVM algorithm[13] for open set problems."
    }, {
      "heading" : "2 Input Margin Distributions and Probability of Sample Inclusion",
      "text" : "Rather than define the margin distribution by starting from a decision boundary and considering locations of samples, we define our input margin distribution (IMD) based on the distribution of distances to hypothesized maximum margin decision boundaries suggested by the samples. Having estimated the distribution of distances to the boundary, we then produce our Probability of Sample Inclusion (PSI or Ψ), which estimates the probability that a given point is inside the decision boundary. We first define the IMD and Ψ-model for a single positive point with multiple negative samples, then generalize to the case of multiple positive samples in a multi-class setting.\nTo formalize these distributions, let x ∈ X be training samples in a metric space X with norm ‖ · ‖. Let yi ∈ C ∈ N be the class label for xi ∈ X . Assume, for now, that there exists only a single positive instance x̂i for some class yi. Given x̂i, the maximum margin would be given by half the distance to the closest training sample from a different class. However, the closest point is just one sample and we should consider the potential maximum margins under different samplings. The IMD seeks to estimate the distribution of the margin distances given the observed data. Thus, given x̂i and xj , where ∀j, yj 6= yi, consider the margin distance to the decision boundary that would be estimated for the pair x̂i, xj if xj were the closest instance. The hypothesized boundary point would be bij = (x̂i + xj)/2, which yields a margin distance estimate mij = ‖x̂i − xj‖/2. Although all but one of the estimates are not the minimum distance for any given sampling, we can gain information\n1One need not use raw measurements, the approach presented is consistent with any metric space data. Data transformations can be done for justified reasons, e.g. normalization, dimensionality reduction, and whitening.\nabout the margin distribution by examining the extrema. Our question then becomes: What is the limiting distribution of the minimum value of this sequence of input space margin distance estimates?\nTo estimate this distribution, we turn to the Fisher-Tippet Theorem [14] also known as the statistical Extreme Value Theory (EVT)2. Just as the Central Limit Theorem dictates that the random variables generated from certain stochastic processes follow Gaussian distributions, EVT dictates that given a well-behaved initial distribution of values (e.g., a distribution that is continuous and has an inverse), the distribution of the maximum/minimum values can assume only limited forms. To find the appropriate distribution let us first recall: Theorem 1 (Fisher-Tippet Theorem [16]:). Let (v1, v2, . . .) be a sequence of i.i.d samples. Let ζn = max{v1, . . . , vn}. If a sequence of pairs of real numbers (an, bn) exists such that each an > 0 and limz→∞ P ( ζn−bn an ≤ z )\n= F (z) then if F is a non-degenerate distribution function, it belongs to the Gumbel, the Fréchet or the Reversed-Weibull family.\nThis extreme value theorem is widely used in many fields [16], such as manufacturing (e.g., estimating time to failure), natural sciences (e.g., estimating 100 or 500 year flood levels), and finance (e.g., portfolio risks). EVT has recently been (re)introduced as applying to recognition, machine learning, and computer vision[17, 13, 18]. Here we use it to formalize the per-sample input margin distribution: Theorem 2 (Input Margin Distribution Theorem). Assume we are given a positive sample xi and sufficiently many negative samples xj drawn from well behaved class distributions, yielding pairwise margin estimates mij . Assume a continuous non-degenerate margin distribution exists. Then the distribution for the minimal values of the margin distance for xi, is given by a Weibull distribution.\nProof. Since the Fisher-Tippet Theorem applies to maxima, we perform change of variables z = −x and consider the maximum set of values −mij . The assumption of sufficient samples and a welldefined set of margin distances converging to a non-degenerate margin implies that Theorem 1 applies. Let φ be the associated distribution of the maximum of ζn in Theorem 1. Combining Theorem 1 with knowledge that the data is bounded (−mij < 0), yields that φ converges to a reversed Weibull, as it is the only one of the EVT distributions that is bounded. Changing the variable back (x = −z) yields that the minimum distance to the boundary is a Weibull distribution. Q.E.D.\nNote that this theorem holds for any given point xi, with each point estimating its own distribution of distance to the margin in its frame of reference. Using Theorem 2 we directly obtain: Corollary 1 (Probability of Sample Inclusion (PSI)). Assuming the conditions for the Input Margin Distribution Theorem, the probability that x′ is included in the boundary estimated by xi is given by:\nPΨ(xi, x ′;ωi, ) = exp\n− ( ||xi−x\n′|| λi )κi (1)\nwhere ||xi − x′|| is the distance of x′ from sample xi, and where ωi = (κi, λi) are Weibull shape and scale parameters respectively obtained from fitting to the maximum values of −mij .\nProof. The Weibull cumulative distribution function ( ∫ x −∞W (z;xi, ωi)dz) provides the probability that the margin is at or below a given value, but we want the probability that the given value does not exceed the margin, thus we seek 1− ∫ x −∞W (z;xi, ωi)dz, yielding Eq. 1. Q.E.D.\nEstimating κi and λi, requires a choice of tail size (number of points for fitting the Weibull), however EVT does not specify this tail size. Prior work such as [17, 13] used a percentage of the data. We introduce a new approach based on approximately maximizing the area covered by the Ψ-model while not violating our maximum margin constraint. Let mij∗ be the margin distance to the nearest boundary estimate and Ψt be the Ψ-model computed using the t closest points. We choose tail size t∗ = min(argmaxt(Ψt(mij∗) < ρ̂), τ̂), where ρ̂ is the desired maximum probability at the boundary sample and τ̂ is a maximum tail size, which mostly impacts speeds. The default value of ρ̂ = .005 ensures that the probability given by the CDF approaches zero at the margin and also ensures a tail which does extend the area to the margin. For this work, we slightly modified the libMR library provided by the authors of [17], which uses Maximum Likelihood Estimation (MLE) to find κi, λi. If a sample fails to have a sufficient number of unique neighbors, we return a default Weibull CDF (κ = 2, λ = 1/#dim), which happens to be the default Gaussian RBF for libSVM.\nThe margin distribution paradigm we have presented provides a per-sample model. To obtain the probability that a sample is within the interior defined by a set of samples X , one can take the max\n2There are other types of extreme value theorems, e.g., the second EVT theorem (Pickands-Balkema-de Haan Theorem [15]), which addresses probabilities conditioned on the process exceeding a sufficiently high threshold.\nover a set of samples for each class giving an interior conditional probability:\nP̂ (y|x) = max{xi∈X :yi=y}PΨ(xi, x;λi, κi). (2) A slight generalization is to find the k-largest probabilities for each class, and then compute their average. Given P̂ , we compute the open-set multi-class recognition result for x′ via:\ny∗ = { argmaxy∈C P̂ (y|x′) if P̂ (y|x′) ≥ δ. “unknown” Otherwise\n(3)"
    }, {
      "heading" : "3 The Extreme Value Machine",
      "text" : "One might use the probability estimates in many ways, e.g., weights in an SVM-like algorithm. However, we choose to directly use Eq.3 and define a near optimal subset, which we call extreme vectors, using a different objective. Not only does finding a subset reduce computational cost, but reducing parameters is important in regularizing the complexity of the classifier. Although such regularization may reduce accuracy when classifying training samples, it reduces the potential for over-fitting the classifier to the training points and decreases sensitivity to noise (e.g., a mislabeled training point). In the soft margin SVM case, such regularization is performed by introducing a regularization coefficient into the optimization objective function and selecting the resulting support vectors. When classifying via Ψ, however, a different approach is required to regularize it.\nAlgorithm 1 EVM Training for i = 1 to |C| do\nCreate a VP-Tree ([19]) Ti of negatives for class yi using all xj s.t. yj 6= yi for j = 1 to |Ci| do\nNN = search(Ti,τ ,xi) {Search VP-Tree for τ nearest negatives} ts = tail size rampup(NN,xi) {Obtain the “optimal” tail size.} ωi,j= WeibullFit(xi, ts,NN) {Fit the jth Weibull via MLE with ts points }\nend for Mi = SetCover({(x1, yi, ωi) . . . (x|Ci|, y|Ci|, ω|Ci|)}) {Optimize model’s Extreme Vectors}\nend for for i = 1 to |C|m, j = i, to |C| do\nRemoveOutliers(Mi,Mj) {Optionally remove extreme vectors covered by other classes} end for\nOur second contribution (cf. Sec. 1) is the insight that among all models that use the PΨ probabilities we want to minimize the model complexity, hence we seek to minimize the number of points needed to provide high-probability labels to the training examples. Because each Ψ-model is based on distance to the nearest boundary point, which is far from most samples, there are large overlaps of Ψ-models. We say that xi covers another point xj if PΨ(xi, xj ;λi, κi) > (1− ). The core of the optimization, then, is to find the minimum number of points that label (cover) all points. We refer to this joint optimization with respect to all classes as the multi-class RBF cover optimization problem. We can bound the complexity to find an approximate solution to this problem:\nTheorem 3 (Multi-class RBF Cover Complexity Theorem). A Multi-class RBF Cover optimization with respect to N training points reduces to Set Cover, which is N.P. Hard but has a polynomial time (1 + ln(N)) – approximate greedy solution.\nProof. Consider points xi ∈ X with corresponding RBFs PΨ(xi). Define a coverage set of indices, si ≡ {xj ∈ X |PΨ(xi, xj ;λi, κi) > (1− ) (i.e., xi covers xj) }. Then the multi-class cover problem is to select the smallest set of points to cover the training samples, i.e., it seeks argmin|S| S ≡ {si| ∪ si = {1...N}}. The latter is a standard instance of the Karp’s Set Cover optimization problem, a well-known example of an NP-hard problem. Theorem 2 in [20] provides the stated complexity upper bound for the greedy algorithm for Set Cover. Q.E.D.\nThe greedy approximation to Set Cover entails selecting the si of highest cardinality at each step. This greedy algorithm is guaranteed to produce a cover no greater than (1− o(1))ln(N), where N is the cardinality of the universe of set elements. While this is the smallest error bound for any polynomial time approximation, there are also other well-known approximations. For example, Set Cover has an integer linear programming approximation as well as a strict linear programming approximation.\nNote that with the Weibull fitting, an outlier is generally “covered” by a point from another class (see Fig 1, and such outlier points are also unlikely to “cover” many other points. Thus, outliers are added to the cover set very late, if at all. This is not an ad hoc assumption; it is an outcome of the process of minimizing the number of points that cover all examples. Like the inherent softness of the margin, this is an inherent part of the regularization approach that follows from the EVT-modeling.\nPerforming the Set Cover over all classes can be expensive if the overall algorithm is non-linear. For efficiency, we compute an initial cover per class, since most most coverage occurs within the same class. This per-class cover, however, has the potential to break the outlier cover discussed earlier. Therefore, we add a second pass during which point-RBF pairs with lower coverage counts within a class are tested against other classes. These pairs are removed from the coverage set as outliers if they would have been covered by another class under global set coverage in an earlier round.\nThe EVM training algorithm is a two step process. The first step consists of fitting the margin distributions for each training sample. The second step consists of selecting the optimal combination of margin distributions and extreme vectors to cover each class. The training steps are described in Alg 1. Now that we have seen how to choose a near-optimal subset of samples and distributions, which provides the preferred form of the EVM, we demonstrate that the algorithm manages open space risk:\nTheorem 4 (EVM Open Space Risk Theorem). Any EVM using training data combined using equations 1 and 3 solves multi-class recognition while managing open-space risk.\nProof. Given that Eq.1 is monotonically decreasing, it follows that the per-sample Ψ-model is a compact abating probability (CAP) model as defined by [13] and therefore each Ψ-model manages open space risk. Furthermore, by Theorem 1 in [21], thresholding a max-combination of CAP models also manages open space risk. Combining these two theorems, it follows that an EVM with Eq. 3 predicts known classes while managing open space risk."
    }, {
      "heading" : "4 Experimental Evaluation",
      "text" : "In this section, we evaluate the EVM for both multi-class classification and multi-class open set recognition problems. Open source code and data/scripts for the experiments can be obtained at ANONYMIZED URL. While the EVM can be used for any problem, our primary focus is on multiclass problems and scalability. Therefore, we evaluated performance of the EVM on a variety of standard multi-class benchmarks including LETTER [22], which has 26 classes, 16 features and 20000 instances; MNIST [23] with 10 classes, 785 features and 70000 instances; and the largest test ALOI [24] with 1000 classes, 128 features, and 108000 instances.\nMulti-class classification (i.e., closed set). The comparison algorithms we use for multi-class classification are Gaussian RBF 1-vs-Rest SVMs [25], Gaussian RBF W-SVMs [13] and the Multiclass from Binary (MBAS) [26]. The MBAS approach models correlation and joint probability of base binary learners to improve their combination. For the first three algorithms, we compare the number of support vectors (SVs) with the number of extreme vectors (EVs), training times (in CPU seconds and excluding parameter cross validation), and evaluation time (in CPU seconds). Experiments were run on 8-core Xeon(R) CPUE5345 @ 2.33GHz with 16GB of ram or more.\nWe report classification accuracy produced by the above technique for both default (D.) parameters and, when available, competing algorithms, using their cross validated (C.V.) parameters. The default EVM parameters are τ̂ = 500 and ρ̂ = .005, while the default SVM parameters are C = 1 and γ = 1/#features. The cross validated parameters are estimated using 5-fold cross-validation over subsets of the training data. The results in Table 1 are reported for 10-fold testing on the datasets. To demonstrate the robustness of the EVM algorithm over different tail sizes, we also ran ALOI and LETTER with ρ̂ = .005, τ̂ = 75. For ALOI, this only reduced accuracy to 94.2% – a small impact on accuracy despite a large change in tail size.\nAcross all three datasets, we notice fewer extreme vectors than support vectors, in most cases by an order of magnitude, and a very large savings in training time. Testing time savings for the EVM exceeds an order of magnitude for the ALOI dataset. However, for LETTER both training and testing times are comparable, which we attribute to initialization overhead for the EVM which we have not yet optimized in our implementation. Note, from Table 1 that even with default parameters the EVM\nsignificantly outperforms 1-vs-rest SVMs and W-SVMs on most of our evaluation metrics. Prior to our evaluation, the state-of-the-art classification accuracy for any algorithm on the ALOI dataset was ≈93% [26], obtained using MBAS. We achieved 94.8% with our proposed EVM algorithm and default parameters, and still achieve 94.2% accuracy with a much smaller tailsize consisting of less than 0.1 % of the number of negative samples. The MBAS paper did not report timing information, but personal correspondence with the authors indicated that training took days, whereas EVM training takes hours.\nDespite reasonable classification accuracies, the EVM statistically underperforms optimized Gaussian RBF SVMs or optimized Gaussian RBF W-SVMs for closed set problems. In addition, while the number of EVs obtained when evaluating MNIST with an EVM is smaller than the number of SVs obtained when evaluating MNIST with an SVM, both EV and SV counts are on the same order of magnitude for this dataset. Both of these findings suggest that there is room for improving the EVM model and we will continue to explore potential explanations for the limitations. Our current (untested) hypothesis is that the non-parameterized EVM“soft-margin”, inherent in the the Weibull fitting and set-cover formulation, might need to be extended and parameterized to better address the overlap and dense data of MNIST.\nMulti-class open set recognition. The comparison algorithms we use for multi-class open set recognition are the open set-specific W-SVM, which is currently the best performing algorithm in the literature for this class of problems, as well as Nearest Neighbor (NN) classifiers, 1-vs-Rest RBF SVMs with Platt’s probability estimator [27], Pairwise RBF SVMs with Platt probability calibration, 1-vs-Rest RBF SVMs, and Logistic Regression. We use an open source libsvm extension provided by [13] for both W-SVM and 1-vs-rest SVM with Platt calibration.\nAt classification time, to reject a sample from an unknown class, a threshold over the scores produced by each algorithm is required. As an example, consider a method generating probabilistic decisions. If the probability of a test sample is greater than the threshold, then it is in one of the known classes, otherwise it is an unknown. Scheirer et al. [13] define this threshold with respect to a measure that they call “openness.” The value of openness is directly proportional to the number of unknown classes expected at test time; as the number of unknown classes increases, the value of the threshold also increases to enforce more stringent class association in a difficult recognition setting. For consistency, we use the same thresholding process, selecting a threshold of 0.5× openness. We started by replicating the multi-class open set benchmark “OLETTER” proposed by Scheirer et al. [13], then we created a new multi-class open set benchmark for ALOI dubbed “OALOI”. Following Scheirer et al. [13], we created the OLETTER multi-class open set benchmark by randomly selecting 15 distinct labels from the known classes during training and adding unknown classes by incrementally including subsets of the remaining 11 labels during testing. We repeated this process over 20 folds to calculate averages and error bars.\nFig. 2 shows the results for OLETTER for all of the evaluated techniques. In open set testing, our EVM significantly outperforms the W-SVM. While the W-SVM is a viable algorithm, it comes at a cost, requiring two trained SVM models (one 1-class and one binary) for its operation. From Table 1, we can see that the EVM requires just a small fraction of the stored vectors, training time, and testing time of the W-SVM while achieving better open set results.\nTo create the open set ALOI (OALOI) test in Fig. 3, we first trained and tested with all 1000 known classes (i.e., 0% unknown classes; closed set) over 5 folds, and calculated open set accuracy (which includes labeling unknown classes as unknown). We then repeated the process with 25%, 50%, 75%, and 90% intervals of unknown classes during training, bringing all 1000 classes back into consideration during testing. Default parameters were used in all cases because, as discussed above, the comparison approaches did not allow parameter optimization in a reasonable amount of time in a cross validation estimation regime. It can be seen that the EVM significantly outperforms all of the comparison approaches. For this larger open set test, only the W-SVM and the the best performing tradational algorithm, the SVM variant (1-vs-Rest with Platt probability), are shown for comparison. The W-SVM exhibits better performance than the EVM at high levels of unknowns because it tends to reject many of the samples; when evaluating a dataset with 75% unknown classes, an algorithm that just rejected everything would achieve 75% accuracy."
    }, {
      "heading" : "5 Discussion",
      "text" : "In this paper, we introduced a new Extreme Value Machine for multi-class classification and multiclass open set recognition problems. Rather than using a kernel definition from user specified parameters or ad hoc assumptions, the EVM derives from a new model of the distribution of potential input margin locations. The proposed EVM algorithm uses an approximate solution to Set Cover to find a near optimal set of extreme vectors. The EVM requires low computational overhead for both training and testing, and achieves accuracies higher than or competitive with popular supervised learning approaches. Similarly, compared to recent approaches for very difficult open set recognition problems with large amounts of computational overhead (e.g., the W-SVM, which requires two trained models), the EVM algorithm is also a highly scalable alternative.\nReaders may find it interesting to note that when κ = 2, the functional form of the Eq. 1 is a Gaussian RBF, and when κ = 1 it is an exponential or Laplacian RBF. While these κ values can occur, in practice, κ assumes a much broader range of values than just these two shape parameters. Alternatively, if one approximates Eq. 1 by a weighted sum of Gaussians (Laplacians) we have two different ways of viewing a Gaussian (Laplacian) RBF algorithm as an approximation to a Ψ-model. Because the Ψ-model parameters vary in shape as a function of the data set, in a Guassian approximation the number of kernel elements and/or the accuracy of approximation must vary spatially. The EVM requires the fewest points for the margin distribution and its Ψ-model.\nNote that, for the EVM, we do not make an ad hoc assumption of a kernel trick nor a post hoc assumption of a particular kernel function; the functional form of the Ψ-model is a direct result of EVT being used to model input space distance to the margin. The Weibull fitting ensures that a small number of mislabeled points or other outliers will not cause the “margin” estimated from the Weibull to be at that “closest location”. If the fitting includes more distance points, the Ψ-model will broaden in scale/shape providing a naturally derived theory for the “softness” in its margin definition. However, the overall optimization with Set Cover currently lacks a parameter to adjust the risk tradeoff between postivie and negative classes.\nExperimentally, the EVM does very well with its default parameters, but there is need for future work to explore formulations that use these concepts yet retain the accuracy of optimized RBF SVMs. Future directions of research may include directly extending the EVM by, for example, obtaining a better parameterized soft-margin during Set Cover. This could perhaps be obtained by adding weights to balance soft-margin errors and formulating the problem in terms of linear programming, incorporating the weights. Another potential extension would be to incorporate margin weights in a loss function in an SVM-style optimization algorithm.\nThis paper presents the first step toward using Extreme Value Theory to define a new approach for learning. The resulting EVM is fast, accurate, and well suited to large scale multi-class problems, especially when facing potentially unknown classes in testing."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This research was funded by NSF Research Grant IIS-1320956 (Open Vision - Tools for Open Set Computer Vision and Learning)."
    } ],
    "references" : [ {
      "title" : "Vc theory of large margin multi-category classifiers",
      "author" : [ "Yann Guermeur" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2007
    }, {
      "title" : "Margin distribution and soft margin",
      "author" : [ "John Shawe-Taylor", "Nello Cristianini" ],
      "venue" : "Advances in Large Margin Classifiers,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2000
    }, {
      "title" : "Margin distribution and learning algorithms",
      "author" : [ "Ashutosh Garg", "Dan Roth" ],
      "venue" : "In Proc. of the Fifteenth Int. Conf. on Machine Learning (ICML),",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2003
    }, {
      "title" : "How boosting the margin can also boost classifier complexity",
      "author" : [ "L. Reyzin", "R.E. Schapire" ],
      "venue" : "In Proc. of Int. Conf. on Machine learning,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2006
    }, {
      "title" : "A kernel method for the optimization of the margin distribution",
      "author" : [ "F. Aiolli", "G. Da San Martino", "A. Sperduti" ],
      "venue" : "In Artificial Neural Networks-ICANN,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2008
    }, {
      "title" : "A risk minimization principle for a class of parzen estimators",
      "author" : [ "K. Pelckmans", "J. Suykens", "B.D. Moor" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2008
    }, {
      "title" : "Large margin distribution learning",
      "author" : [ "Z.H. Zhou" ],
      "venue" : "In Artificial Neural Networks in Pattern Recognition,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2014
    }, {
      "title" : "K-local hyperplane and convex distance nearest neighbor algorithms",
      "author" : [ "Pascal Vincent", "Yoshua Bengio" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2001
    }, {
      "title" : "Svm maximizing margin in the input space",
      "author" : [ "A. Shotaro" ],
      "venue" : "In Neural Information Processing,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2002
    }, {
      "title" : "Separating hypersurfaces of svms in input spaces",
      "author" : [ "Xun Liang", "Chao Wang" ],
      "venue" : "Pattern Recognition Letters,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2009
    }, {
      "title" : "An introduction to statistical modeling of extreme values",
      "author" : [ "S. Coles" ],
      "venue" : null,
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2001
    }, {
      "title" : "Towards open set recognition",
      "author" : [ "W.J. Scheirer", "A. Rocha", "A. Sapkota", "T.E. Boult" ],
      "venue" : "IEEE Trans. on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2013
    }, {
      "title" : "Probability models for open set recognition",
      "author" : [ "W.J. Scheirer", "L.P. Jain", "T.E. Boult" ],
      "venue" : "IEEE Trans. on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2014
    }, {
      "title" : "Limiting forms of the frequency distribution of the largest or smallest member of a sample",
      "author" : [ "R.A. Fisher", "L.H. Tippett" ],
      "venue" : "In Mathematical Proc. of the Cambridge Philosophical Society,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1928
    }, {
      "title" : "Statistical Inference Using Extreme Order Statistics",
      "author" : [ "J. Pickands" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 1975
    }, {
      "title" : "Extreme Value Distributions: Theory and Applications",
      "author" : [ "S. Kotz", "S. Nadarajah" ],
      "venue" : "World Sci. Pub. Co.,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2001
    }, {
      "title" : "Meta-recognition: The theory and practice of recognition score analysis",
      "author" : [ "W.J. Scheirer", "A. Rocha", "R.J. Micheals", "T.E. Boult" ],
      "venue" : "IEEE Trans. on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2011
    }, {
      "title" : "Extreme bandits",
      "author" : [ "Alexandra Carpentier", "Michal Valko" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2014
    }, {
      "title" : "Data structures and algorithms for nearest neighbor search in general metric spaces",
      "author" : [ "Peter N Yianilos" ],
      "venue" : "In SODA,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 1993
    }, {
      "title" : "A tight analysis of the greedy algorithm for set cover",
      "author" : [ "Petr Slavı́k" ],
      "venue" : "In Proceedings of the twenty-eighth annual ACM symposium on Theory of computing,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 1996
    }, {
      "title" : "Towards open world recognition",
      "author" : [ "A. Bendale", "T.E. Boult" ],
      "venue" : "In IEEE Conf. on Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2015
    }, {
      "title" : "Letter recognition using holland-style adaptive classifiers",
      "author" : [ "P.W. Frey", "D.J. Slate" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 1991
    }, {
      "title" : "Gradient-based learning applied to document recognition",
      "author" : [ "Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner" ],
      "venue" : "Proc. of the IEEE,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 1998
    }, {
      "title" : "The amsterdam library of object images",
      "author" : [ "J.-M. Geusebroek", "G.J. Burghouts", "A.W.M. Smeulders" ],
      "venue" : "Int. Journal of Computer Vision,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2005
    }, {
      "title" : "LIBSVM: A library for support vector machines",
      "author" : [ "C.-C. Chang", "C.-J Lin" ],
      "venue" : "ACM Trans. on Intelligent Systems and Technology,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2011
    }, {
      "title" : "Multiclass from binary: Expanding one-versus-all, one-versus-one and ECOC-based approaches",
      "author" : [ "A. Rocha", "S. Goldenstein" ],
      "venue" : "IEEE Trans. on Neural Net. and Learning Sys.,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2014
    }, {
      "title" : "Probabilistic outputs for support vector machines and comparison to regularize likelihood methods",
      "author" : [ "J. Platt" ],
      "venue" : "Advances in Large Margin Classifiers,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2000
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "The multi-class margin is traditionally defined as the distance between the decision boundary and the “closest” training samples from a different class [1].",
      "startOffset" : 152,
      "endOffset" : 155
    }, {
      "referenceID" : 1,
      "context" : "The idea of using “margin distributions” is not new; multiple researchers have explored various definitions and uses of margin distributions, [2, 3, 4, 5, 6], involving techniques such as maximizing the mean or median margin, taking a weighted combination margin, or optimizing the margin mean and variance.",
      "startOffset" : 142,
      "endOffset" : 157
    }, {
      "referenceID" : 2,
      "context" : "The idea of using “margin distributions” is not new; multiple researchers have explored various definitions and uses of margin distributions, [2, 3, 4, 5, 6], involving techniques such as maximizing the mean or median margin, taking a weighted combination margin, or optimizing the margin mean and variance.",
      "startOffset" : 142,
      "endOffset" : 157
    }, {
      "referenceID" : 3,
      "context" : "The idea of using “margin distributions” is not new; multiple researchers have explored various definitions and uses of margin distributions, [2, 3, 4, 5, 6], involving techniques such as maximizing the mean or median margin, taking a weighted combination margin, or optimizing the margin mean and variance.",
      "startOffset" : 142,
      "endOffset" : 157
    }, {
      "referenceID" : 4,
      "context" : "The idea of using “margin distributions” is not new; multiple researchers have explored various definitions and uses of margin distributions, [2, 3, 4, 5, 6], involving techniques such as maximizing the mean or median margin, taking a weighted combination margin, or optimizing the margin mean and variance.",
      "startOffset" : 142,
      "endOffset" : 157
    }, {
      "referenceID" : 5,
      "context" : "The idea of using “margin distributions” is not new; multiple researchers have explored various definitions and uses of margin distributions, [2, 3, 4, 5, 6], involving techniques such as maximizing the mean or median margin, taking a weighted combination margin, or optimizing the margin mean and variance.",
      "startOffset" : 142,
      "endOffset" : 157
    }, {
      "referenceID" : 6,
      "context" : "As recently pointed out by Zhou in [7], “These arguments, however, are all heuristics without theoretical justification.",
      "startOffset" : 35,
      "endOffset" : 38
    }, {
      "referenceID" : 7,
      "context" : "As noted by [8]: “In this setting, the margin being maximized is still the smallest Euclidean distance between the decision surface and the training points, but this time measured in some strange, sometimes infinite dimensional, kernel-induced feature space rather than the original input space.",
      "startOffset" : 12,
      "endOffset" : 15
    }, {
      "referenceID" : 8,
      "context" : "” Considerable research has demonstrated that a large margin in kernel space does not necessarily translate into a large margin in the input space [9, 10].",
      "startOffset" : 147,
      "endOffset" : 154
    }, {
      "referenceID" : 9,
      "context" : "” Considerable research has demonstrated that a large margin in kernel space does not necessarily translate into a large margin in the input space [9, 10].",
      "startOffset" : 147,
      "endOffset" : 154
    }, {
      "referenceID" : 10,
      "context" : "Therefore, the margin distances are extreme values generated from a stochastic sampling process, and with some mild assumptions we show that one can invoke EVT [11] and use it to model the extreme value distribution.",
      "startOffset" : 160,
      "endOffset" : 164
    }, {
      "referenceID" : 11,
      "context" : "[12, 13] recently formalized the concept of open set risk and its role in open set recognition.",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 12,
      "context" : "[12, 13] recently formalized the concept of open set risk and its role in open set recognition.",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 12,
      "context" : "Our results demonstrate that the EVM is generally superior to or competitive with linear and optimized Gaussian RBF SVMs in the closed set regime and offers comparable or better performance relative to the leading open set W-SVM algorithm[13] for open set problems.",
      "startOffset" : 238,
      "endOffset" : 242
    }, {
      "referenceID" : 13,
      "context" : "To estimate this distribution, we turn to the Fisher-Tippet Theorem [14] also known as the statistical Extreme Value Theory (EVT)2.",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 15,
      "context" : "Theorem 1 (Fisher-Tippet Theorem [16]:).",
      "startOffset" : 33,
      "endOffset" : 37
    }, {
      "referenceID" : 15,
      "context" : "This extreme value theorem is widely used in many fields [16], such as manufacturing (e.",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 16,
      "context" : "EVT has recently been (re)introduced as applying to recognition, machine learning, and computer vision[17, 13, 18].",
      "startOffset" : 102,
      "endOffset" : 114
    }, {
      "referenceID" : 12,
      "context" : "EVT has recently been (re)introduced as applying to recognition, machine learning, and computer vision[17, 13, 18].",
      "startOffset" : 102,
      "endOffset" : 114
    }, {
      "referenceID" : 17,
      "context" : "EVT has recently been (re)introduced as applying to recognition, machine learning, and computer vision[17, 13, 18].",
      "startOffset" : 102,
      "endOffset" : 114
    }, {
      "referenceID" : 16,
      "context" : "Prior work such as [17, 13] used a percentage of the data.",
      "startOffset" : 19,
      "endOffset" : 27
    }, {
      "referenceID" : 12,
      "context" : "Prior work such as [17, 13] used a percentage of the data.",
      "startOffset" : 19,
      "endOffset" : 27
    }, {
      "referenceID" : 16,
      "context" : "For this work, we slightly modified the libMR library provided by the authors of [17], which uses Maximum Likelihood Estimation (MLE) to find κi, λi.",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 14,
      "context" : ", the second EVT theorem (Pickands-Balkema-de Haan Theorem [15]), which addresses probabilities conditioned on the process exceeding a sufficiently high threshold.",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 18,
      "context" : "for i = 1 to |C| do Create a VP-Tree ([19]) Ti of negatives for class yi using all xj s.",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 19,
      "context" : "Theorem 2 in [20] provides the stated complexity upper bound for the greedy algorithm for Set Cover.",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 12,
      "context" : "1 is monotonically decreasing, it follows that the per-sample Ψ-model is a compact abating probability (CAP) model as defined by [13] and therefore each Ψ-model manages open space risk.",
      "startOffset" : 129,
      "endOffset" : 133
    }, {
      "referenceID" : 20,
      "context" : "Furthermore, by Theorem 1 in [21], thresholding a max-combination of CAP models also manages open space risk.",
      "startOffset" : 29,
      "endOffset" : 33
    }, {
      "referenceID" : 12,
      "context" : "Figure 2: Multi-class open set recognition accuracy on OLETTER [13].",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 21,
      "context" : "Therefore, we evaluated performance of the EVM on a variety of standard multi-class benchmarks including LETTER [22], which has 26 classes, 16 features and 20000 instances; MNIST [23] with 10 classes, 785 features and 70000 instances; and the largest test ALOI [24] with 1000 classes, 128 features, and 108000 instances.",
      "startOffset" : 112,
      "endOffset" : 116
    }, {
      "referenceID" : 22,
      "context" : "Therefore, we evaluated performance of the EVM on a variety of standard multi-class benchmarks including LETTER [22], which has 26 classes, 16 features and 20000 instances; MNIST [23] with 10 classes, 785 features and 70000 instances; and the largest test ALOI [24] with 1000 classes, 128 features, and 108000 instances.",
      "startOffset" : 179,
      "endOffset" : 183
    }, {
      "referenceID" : 23,
      "context" : "Therefore, we evaluated performance of the EVM on a variety of standard multi-class benchmarks including LETTER [22], which has 26 classes, 16 features and 20000 instances; MNIST [23] with 10 classes, 785 features and 70000 instances; and the largest test ALOI [24] with 1000 classes, 128 features, and 108000 instances.",
      "startOffset" : 261,
      "endOffset" : 265
    }, {
      "referenceID" : 24,
      "context" : "The comparison algorithms we use for multi-class classification are Gaussian RBF 1-vs-Rest SVMs [25], Gaussian RBF W-SVMs [13] and the Multiclass from Binary (MBAS) [26].",
      "startOffset" : 96,
      "endOffset" : 100
    }, {
      "referenceID" : 12,
      "context" : "The comparison algorithms we use for multi-class classification are Gaussian RBF 1-vs-Rest SVMs [25], Gaussian RBF W-SVMs [13] and the Multiclass from Binary (MBAS) [26].",
      "startOffset" : 122,
      "endOffset" : 126
    }, {
      "referenceID" : 25,
      "context" : "The comparison algorithms we use for multi-class classification are Gaussian RBF 1-vs-Rest SVMs [25], Gaussian RBF W-SVMs [13] and the Multiclass from Binary (MBAS) [26].",
      "startOffset" : 165,
      "endOffset" : 169
    }, {
      "referenceID" : 25,
      "context" : "Prior to our evaluation, the state-of-the-art classification accuracy for any algorithm on the ALOI dataset was ≈93% [26], obtained using MBAS.",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 26,
      "context" : "The comparison algorithms we use for multi-class open set recognition are the open set-specific W-SVM, which is currently the best performing algorithm in the literature for this class of problems, as well as Nearest Neighbor (NN) classifiers, 1-vs-Rest RBF SVMs with Platt’s probability estimator [27], Pairwise RBF SVMs with Platt probability calibration, 1-vs-Rest RBF SVMs, and Logistic Regression.",
      "startOffset" : 298,
      "endOffset" : 302
    }, {
      "referenceID" : 12,
      "context" : "We use an open source libsvm extension provided by [13] for both W-SVM and 1-vs-rest SVM with Platt calibration.",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 12,
      "context" : "[13] define this threshold with respect to a measure that they call “openness.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 12,
      "context" : "[13], then we created a new multi-class open set benchmark for ALOI dubbed “OALOI”.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 12,
      "context" : "[13], we created the OLETTER multi-class open set benchmark by randomly selecting 15 distinct labels from the known classes during training and adding unknown classes by incrementally including subsets of the remaining 11 labels during testing.",
      "startOffset" : 0,
      "endOffset" : 4
    } ],
    "year" : 2015,
    "abstractText" : "This paper provides a novel characterization of the max-margin distribution in input space. The use of the statistical Extreme Value Theory (EVT) is introduced for modeling margin distances, allowing us to derive a scalable non-linear model called the Extreme Value Machine (EVM). Without the need for a kernel, the EVM leverages a margin model to estimate the probability of sample inclusion in each class. The EVM selects a nearoptimal subset of the training vectors to optimize the gain in terms of points covered versus parameters used. We show that the problem reduces to the NP-hard Set Cover problem which has a provable polynomial time approximation. The resulting machine has comparable closed set accuracy (i.e., when all testing classes are known at training time) to optimized RBF SVMs and exhibits far superior performance in open set recognition (i.e., when unknown classes exist at testing time). In open set recognition performance, the EVM is more accurate and more scalable than the state of the art.",
    "creator" : "LaTeX with hyperref package"
  }
}