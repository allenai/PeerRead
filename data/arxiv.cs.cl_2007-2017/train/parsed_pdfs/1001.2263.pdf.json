{
  "name" : "1001.2263.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Syllable analysis to build a dictation system in Telugu language",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Keywords-component; formatting; style; styling; insert (key words)\nI. INTRODUCTION (HEADING 1) Developing a robust dictation system to transcribe continuous speech signal into a sequence of words is a difficult task, as continuous speech does not have any natural pauses in between words. It is also difficult to make the system robust for speaker variability and the environment conditions. There are many research organizations working on speech with different approach. The conventional method of building a large vocabulary speech recognizer for any language uses a top-down approach to speech recognition (Huang & Acero 1993)[1]. What they mean by top-down is that these recognizers first hypothesize the sentence, then the words that make up the sentence and ultimately the sub-word units that make up the words. This approach requires large speech corpus with sentence or phoneme level transcription of the speech utterances (Thomas Hain et al 2005; Ravishankar 1996) [2]. The transcriptions must also include different speech order so that the recognizer can build models for all the sounds present. It also requires maintaining a dictionary with the phoneme/subword unit transcription of the words and language models to perform large vocabulary continuous speech recognition. The recognizer outputs words that exist in the dictionary. If the system is to be developed for a new language it requires building of a dictionary and extensive language models for the\nnew language. In country like India which includes 22 officials and a number of unofficial languages, building huge text and speech databases is a difficult task. There are other related works which gained good importance is listed below.\nSome methods that require manually annotated speech corpora for speech recognition are listed. A method called bootstrapping is proposed by Rabiner et al (1982)[3] which can increase the transcribed data for training the system, for speech recognition. Ljolje & Riley (1991)[4] have used an automatic approach to segmentation and labeling of speech when only the orthographic transcription of speech is available. Kemp&Waibel (1998)[5] used unsupervised training approach for speech recognition for TV broadcasts. Wessel&Ney (2001)[6] have proposed an approach in which a low-cost recognizer trained with one hour of manually transcribed speech is used to recognize 72 hours of unrestricted acoustic data. Lamel et al (2002)[7] have shown that the acoustic models can be initialized using as little as 10 minutes of manually annotated data.\nThere are also few methods that do not require any manually annotated speech corpora for speech recognition. Incremental maximum a posteriori estimation of HMMs is proposed by Gotoh & Hochberg (1995)[8]. This algorithm randomly selects a sub-set of data from the training set, updates the model using maximum a posteriori estimation and this process is iterated until it covers all possible units. Chang et al (2000)[9] proposed a method which extracts articulatoryacoustic phonetic features from each frame of speech signal and then the phone is identified using neural networks. There is an interesting approach proposed by Nagarajan & Murthy (2004)[10] for Indian languages. Their approach focuses on automatically segmenting and transcribing the continuous speech signal into syllable-like units using a group delay based segmentation algorithm without the use of manually segmented and labeled speech corpora. This approach is more appropriate for Indian languages as they are syllable centered.\nThe focus of this paper is to extract the possible syllables from the raw Telugu text corpus. Once the syllable like units is\n171 http://sites.google.com/site/ijcsis/ ISSN 1947-5500\nobtained they are analyzed to understand their frequency of coverage in words. The final list of words can be prepared considering the words having high probable syllables or identifying the words which contain the maximum syllables.\nDetails of Telugu corpus and Procedure to convert Telugu text to WX notation is introduced in section 2. Algorithm which uses Syllabification rules to syllabify the text is discussed in section 3. A study of results obtained is summarized in section 4 and conclusion and future scope presented in section 5.\nII. CONVERSION OF TELUGU TEXT TO WX NOTATION. Telugu is one of the major Scheduled languages of India. It has the second largest number of speakers mainly concentrated in South India. It is the official language of Andhra Pradesh and second widely spoken language in Tamilnadu, Karnataka. There are number of Telugu language speakers have migrated to Mauritius, South Africa, and recently to USA, UK, and Australia. Telugu is often referred as \"Italian of the East\".\nThe Primary units of Telugu alphabet are syllables, therefore it should be rightly called a syllabic language. There is good correspondence in the written and spoken form of the south Indian languages. Any analysis done on written form would closely relate to spoken form of the language.\nThe Telugu alphabet can be viewed as consisting of more commonly used inventory, a common core, and an overall pattern comprising all those symbols that are used in all domains. The overall pattern consists of 60 symbols, of which 16 are vowels, 3 vowel modifiers, and 41 consonants.\nSince Indian languages are syllable-timed languages, syllable is considered as the basic unit in this work and analysis is performed to identify the words with syllables with high frequency and words with varying coverage of syllables."
    }, {
      "heading" : "A. Telugu to English letter translation",
      "text" : "The WX notation of thirteen vowel signs\nఅ ,ఆ ,ఇ,ఈ ,ఉ ,ఊ ,ఋ ,ఎ ,ఏ ,ఐ ,ఒ ,ఓ ,ఔ is a,A,i,I,u, U,q,eV,e,E,oV,o,O occur as stand alone characters and In UNICODE Standard 3.0., each of these is assigned a hexadecimal code point 0C00-0C7F. When a vowel occurs immediately after a consonant it is represented as a dependent or secondary sign called, guNiMtaM gurtulu. The Telugu alphabet is a syllabic language in which the primary consonant always has an inherent vowel [a] / /. When a consonant is attached with another vowel other than [a] / / then secondary vowel sign is attached to the consonant after removing the inherent vowel /a/. There are exceptions where the primary vowel may be considered as secondary.\nThere are 41 consonants in the common core inventory. In Unicode Standard 3.0 they begin with 0C15 to 0C39 and 0C1A to 0C2F. The character set for consonants in Telugu is complex and peculiar in their function. These character signs have three or more than three distinct shapes depending on their occurrence\n• Base consonants or Primaries, when they are used as stand alone characters.\n• Pure consonant or hanger, when used with a vowel other than the inherent vowel /a/\n• Ottulu or Secondary consonant, when used as a constituent of a conjunct\nThe basic character set for consonants are called as primaries or stand alone characters as they occur in the alphabet. Each of which has an inherent vowel /a/ which often is explicitly indicated by sign / /. This graphic sign indicating the vowel /a/ is normally deleted and replaced with another explicit mark for a different vowel.\nList of pure consonants carrying explicit secondary vowel /a/ sign and its corresponding WX notation are క-ka గ-ga ఖ -Ka\nఘ -G ఙ -fa , చ -ca ఛ -Ca జ -ja ఝ -Ja ఞ-Fa , ట -ta ఠ -Ta డ -da ఢ -\nDa ణ -Na , త -wa థ -Wa ద -xa ధ -Xa న-na , ప -pa ఫ -Pa బ -ba భ -\nBa మ -ma య -ya ర -ra ల -la వ-va శ -Sa ష -Ra స -sa హ -ha ళ -lYa\nఱ -rY\nThe Telugu text in Unicode format is converted to WX notation. The conversion is done character by character using Unicode value of the character. If the Unicode of the character is between 0C15 and 0C39 (క to హ ), representation\ncorresponding to Pure consonant is retrieved from WX table. If the Unicode of the character is between 0C3E and 0C4C (ా to ౌ), the last letter from Pure consonant is removed and secondary vowel representation is added. If Unicode of the character is 0C4D which correspond to stress mark ్, the last letter from the WX notation is removed indicating that the next occurrence of character is secondary consonant."
    }, {
      "heading" : "B. Algorithm",
      "text" : "The algorithm for conversion is given below where englishtext is initialized to null.\n• string englishtext=null\n• read the contents and convert into character array\no for each character till end of the file do\nif Unicode of the letter is between 0C15 and 0C60\nretrieve the corresponding English character for the Unicode, add to englishtext and increment i by 1\n172 http://sites.google.com/site/ijcsis/ ISSN 1947-5500\nelse if Unicode of the letter is between 0C3E and 0C4C\nremove the last letter from the englishtext, retrieve the corresponding English character for the Unicode, add to englishtext and increment i by 1\nelse if Unicode of the letter is 0C4D\nremove last letter from the englishtext\nelse\ncopy the character into English text and increment i by 1\no end for\n• Store in temp file for Syllabification.\n• end\nIII. SYLLABIFICATION The scripts of Indian languages have originated from the ancient Brahmi script. The basic speech sounds units and basic written form has one to one correspondence. An important feature of Indian language scripts is their phonetic nature. The characters are the orthographic representation of speech sounds. A character in Indian language scripts is close to syllable and can be typically of the following form: C, V, CV, CCV and CVC, where C is a consonant and V is a vowel. There are about 35 consonants and about 15 vowels in Indian languages. The rules required to map the letters to sounds of Indian languages are almost straight forward. All Indian language scripts have common phonetic base.\nThe majority of the speech recognition systems in existence today use an observation space based on a temporal sequence of frames containing short-term spectral information. While these systems have been successful [10, 12], they rely on the incorrect assumption of statistical conditional independence between frames. These systems ignore the segment-level correlations that exist in the speech signal. The high-energy regions in the Short Term Energy function correspond to the syllable nuclei while the valleys at both ends of the nuclei determine the syllable boundaries.\nThe text segmentation is based on the linguistic rules derived from the language. Any syllable based language can be syllabified using these generic rules. To make the text segments exactly equivalent to the speech units.\nThe syllable can be defined as a vowel nucleus supported by consonants on either side, It can be generalized as a C*VC* unit where C is a consonant and V is a vowel. The linguistic rules to extract the syllables segments from a text are generated from spoken Telugu. These rules can be generalized to any syllable centric language. The text is preprocessed to remove any punctuation. The following algorithm divides the word into syllable like units."
    }, {
      "heading" : "A. Algorithm",
      "text" : "• Read from the file which has text in WX notation.\n• Label the characters as consonants and vowels using the following rules\no Any consonant except(y, H, M) followed by y is a single consonant, label it as C\no Any consonant except (y, r, l, lY, lYY) followed by r is taken as single consonant\no Consonants like(k, c, t, w, p, g, j, d, x, b, m, R, S, s) followed by l is taken as single consonant.\no Consonant like (k, c, t, w, p, g, j, d, x, b, R, S, s, r) followed by v is taken as a single consonant.\no Label the remaining as Vowel (V) or Consonant(C) depending on the set to which it belongs.\no Store the attribute of the word in terms of (C*VC*)* in temp2 file.\n• For each word in the corpus get its label attribute from temp2 file.\no If the first character is a C then the associate it to the nearest Vowel on the right.\no If the last character is a C then associate it to the nearest Vowel on the left.\no If sequences correspond to VV then break is as V-V.\no Else If sequence correspond to VCV then break it as V-CV.\no Else If sequence correspond to VCCV then break it as VC-CV.\no Else If sequence correspond to VCCCV then break it as VC-CCV.\no The strings separated by – are identified as syllable units.\n• repeat\n• Store the result in output file.\nThe following Table:1 shows the output obtained for the input in Telugu text in UNICODE\nTABLE I. OUTPUT FOR ALGORITHM 1 AND ALGORITHM 2\nS. No Input\nOutput of Algoritm 1\nOutput of Algorithm 2\n1. కం ె కంటె kaMpeVnIkaMteV kaM-peV-nI-kaM-teV\n2. ఖరుచ్కంటె KarcukaMteV Kar-cu-kaM-teV\n3. లా ాలకు lABAlaku lA-BA-la-ku\n173 http://sites.google.com/site/ijcsis/ ISSN 1947-5500"
    }, {
      "heading" : "IV. STATISTICAL ALALYSIS",
      "text" : ""
    }, {
      "heading" : "A. Phoneme Analysis:",
      "text" : "The following observations are based on a study made on CIIL Mysore Telugu text corpus of 3 million words of running texts in Telugu. This corpus is first cleaned and words are extracted. The Word frequencies are dropped in order to avoid their skewing effect on the results of character frequencies. These words are broken into syllables using the rules of the language and analysis is performed to study the distribution of phonemes and syllables."
    }, {
      "heading" : "B. Phoneme Frequency chart:",
      "text" : "On observing it is found that of Vowels cover nearly 44.98% in total text corpus. It clearly shows that the vowels are the major units in the speech utterance. The vowel modifiers coverage is 3.82% and the consonant coverage is 51.21%. The following Fig 1 gives the details of the analysis.\nC. Vowel variation chart: Vowels occur either in stand alone form or as modifiers and have total coverage of 48%. The vowels are classified based on the position of the articulator and manner of articulation. The vowel classification is given in Table 5 and the distribution of vowels is shown in the Fig 2."
    }, {
      "heading" : "D. Consonant variation chart:",
      "text" : "Consonants are characterized by significant constriction or obstruction in the pharyngeal and/or oral cavities. Some consonants are voiced and others are unvoiced. Many consonants occur in pairs, that is, they share the same configuration of articulators, and one member of the pair additionally has voicing which the other lacks. Based on the articulators involved and manner of articulation the consonants are classified as Bilabial, Dental Alveolar, Retroflex, Velar and Glottal. The distribution of consonants is shown in the following Fig. 3.\nBased on the above analysis it is clear that the vowels play a major role in the utterance of speech units. Or the basic unit of utterance is concentric at the vowel which forms the key component in the syllable. It is hence good to build the speech recognition systems considering the syllable as the basic unit."
    }, {
      "heading" : "E. Syllable Analysis :",
      "text" : "It is shown that most of the Indian languages are syllable centric. Syllable boundary in speech signal can be approximately identified it is intended to make a study of developing a speech recognition system at Syllable level.\nThe total distinct syllables observed are 12,378 and the frequency of occurrence of the syllables is plotted in the following chart. The number of Syllables with frequency less than 100 is 11057(12,378 – 1321). It is observed that nearly 4903 syllables have frequency one. This is due to loanwords from English like (Apple, coffee, strength etc.) When these words are written in Telugu it normally takes the same pronunciation. Such kind of words creates a different syllable which may not occur in the native language. Fig 4 shows the count of Syllables with the frequencies in Hundreds.\n174 http://sites.google.com/site/ijcsis/ ISSN 1947-5500\nThe following Fig 5 shows the count of Syllables that have frequency ranging from 1K to 10K.\nThe following Fig 6 shows the count of Syllables that have frequency ranging from 10K to 100K.\nIt is observed that there are nearly 71 syllables that have frequency more than 10K.\nA study is also made in terms of the words which have varying number of syllables with varying frequencies. Here in the following figures, plots are given for words which have syllables with cut-off frequency specified on X axis, Y-axis indicates number of words having the Syllable Index and above cut-off frequency and Syllable Index 0.5, 0.8 and 1.0..\nWords count, with Syllable Index 50%, 80% and 100% and cut-off frequency varying in the range from 100 to 1000 is shown in Fig.7.\nWords count, with Syllable Index 50%, 80% and 100% and cut-off frequency varying in the range from 1K to 10K is shown in Fig.8.\nWords count, with Syllable Index 50%, 80% and 100% and cut-off frequency varying in the range from 10K to 100K is shown in Fig.9.\nIt is observed from the above figures that as the frequency increases the number of words included decreases. Importance of the word depends on the Syllable Index and on the cut-of frequency. It is directly proportional to Syllable Index and cutof frequency.\nV. CONCLUSION This paper explores the details of phonemes and syllables in the text corpus. As there is one to one correspondence between the written form and spoken form of the language, detailed analysis is performed to understand the coverage of different phonemes and syllables. This analysis is useful in selecting good set of words that would cover all possible syllables in large vocabulary. Optimal selection of words depends on the selection strategy applied. Good strategy can be used to obtain limited words, which are useful during recording the speech to train the system. This ultimately improves the performance of the dictation system.\n175 http://sites.google.com/site/ijcsis/ ISSN 1947-5500\nREFERENCE: [1] Huang, Acero 1993 Spoken Language Processing. (New Jersy: Prentice\nHall) [2] Thomas Hain, et al 2005 Automatic transcription of conversational\ntelephone speech. In IEEE Trans.Speech, Audio Processing 13: 1173– 1185\n[3] Rabiner L R Rosenberg A E, Wilpon J G, Zampini T M 1982 A bootstrapping training technique for obtaining demisyllabic reference patterns. In J. Acoust. Soc. Amer. 71: 1588–1595\n[4] Ljolje A, Riley M D 1991 Automatic segmentation and labelling of speech. In Proceedings of IEEE Int. Conf. Acoust., Speech, and Signal Processing 1: 473–476\n[5] Kemp T, Waibel A 1998 Unsupervised training of a speech recognizer using tv broadcasts. In Proc.of ICSLP 98. Vol. 5 Sydney, Australia, 2207–2210\n[6] Wessel F, Ney H 2001 Unsupervised training of acoustic models for large vocabulary continuous speech recognition. In IEEE Workshop on ASRU. 307–310\n[7] Lamel L, Gauvain J-L, Adda G 2002 Unsupervised acoustic model training. In Proceedings of IEEE Int. Conf. Acoust., Speech, and Signal Processing. 877–880\n[8] Gotoh Y and Hochberg M M 1995 Incremental map estimation of hmms for efficient training and improved performance. In Proceedings of IEEE Int. Conf. Acoust., Speech, and Signal Processing 877–880\n[9] Chang S, Sastri L, Greenberg S 2000 Automatic phonetic transcription of sponta- neous speech. In Proceedings of Int. Conf. Spoken Language Processing 4: 330–333\n[10] Nagarajan T, Murthy H A 2004 Non-bootstrap approach to segmentation and labelling of continuous speech. In National Conference on Communication. 508–512\nAUTHORS PROFILE\nDr K.V.N.Sunitha did her B.Tech ECE from Nagarjuna University in 1988, M.Tech Computer Science from REC Warangal in 1993 and Ph.D from JNTU hyderabada in 2006. She has 18 years of Teaching Experience. She has been working in G.Narayanamma Institute of Technology and Science, Hyderabad as HOD CSE Dept. from the inception of the CSE Dept since 2001. She is a recipient of “Academic Excellence Award” by GNITS in 2004.She is awarded “Best computer engineering Teacher Award “ by ISTE in Second Annual Convention held in Feb 2008.She has published more than 35 papers in International & National Journals and Conferences. Authored a book on ”Programming in UNIX and Compiler Design”. She is guiding five PhDs . She is a reviewer for International Journals-JATIT,IJFCA. She is fellow of Institute of engineers &Sr member for IACSIT, International association of CSIT, life member of many technical associations like CSI, IEEE, ACM. She is academic advisory body member for ACE Engg college , Ghatkesar. She is Board of Studies member for UG ang PG programmaes, CSE at G.Pulla Reddy Engg College,Kurnool. N. Kalyani completed B.Tech civil from Osmania University in 1994, M.Tech Computer Science from JNTUH in 2001. She has working experience of 5 years as Design Engineer in R. K. Engineers, Hyderabad and 9 years of teaching for both under and post graduate students. She is presently working as Associate Professor in the Department of Computer Science Engineering, G.Narayanamma Institute of Technology and Science, Hyderabad. She is reciepient of “Academic Excellence Award” by GNITS in 2008. She has published 10 papers in International & National Journals and conferences. She is a life member of CSI & ISTE technical associations.\n176 http://sites.google.com/site/ijcsis/ ISSN 1947-5500"
    } ],
    "references" : [ {
      "title" : "Spoken Language Processing. (New Jersy",
      "author" : [ "Huang", "Acero" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1993
    }, {
      "title" : "Automatic transcription of conversational telephone speech",
      "author" : [ "Thomas Hain" ],
      "venue" : "In IEEE Trans.Speech, Audio Processing",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2005
    }, {
      "title" : "A bootstrapping training technique for obtaining demisyllabic reference patterns",
      "author" : [ "E Rabiner L R Rosenberg A", "G Wilpon J", "M Zampini T" ],
      "venue" : "In J. Acoust. Soc. Amer",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1982
    }, {
      "title" : "Automatic segmentation and labelling of speech",
      "author" : [ "A Ljolje", "D Riley M" ],
      "venue" : "In Proceedings of IEEE Int. Conf. Acoust., Speech, and Signal Processing",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1991
    }, {
      "title" : "Unsupervised training of a speech recognizer using tv broadcasts",
      "author" : [ "T Kemp", "A Waibel" ],
      "venue" : "In Proc.of ICSLP 98. Vol. 5 Sydney,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1998
    }, {
      "title" : "Unsupervised training of acoustic models for large vocabulary continuous speech recognition",
      "author" : [ "F Wessel", "H Ney" ],
      "venue" : "In IEEE Workshop on ASRU",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2001
    }, {
      "title" : "Unsupervised acoustic model training",
      "author" : [ "L Lamel", "J-L Gauvain", "G Adda" ],
      "venue" : "In Proceedings of IEEE Int. Conf. Acoust., Speech, and Signal Processing",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2002
    }, {
      "title" : "Incremental map estimation of hmms for efficient training and improved performance",
      "author" : [ "Y Gotoh", "M Hochberg M" ],
      "venue" : "In Proceedings of IEEE Int. Conf. Acoust., Speech, and Signal Processing",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1995
    }, {
      "title" : "Automatic phonetic transcription of sponta- neous speech",
      "author" : [ "S Chang", "L Sastri", "S Greenberg" ],
      "venue" : "In Proceedings of Int. Conf. Spoken Language Processing",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2000
    }, {
      "title" : "Non-bootstrap approach to segmentation and labelling of continuous speech",
      "author" : [ "T Nagarajan", "A Murthy H" ],
      "venue" : "In National Conference on Communication",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2004
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "The conventional method of building a large vocabulary speech recognizer for any language uses a top-down approach to speech recognition (Huang & Acero 1993)[1].",
      "startOffset" : 157,
      "endOffset" : 160
    }, {
      "referenceID" : 1,
      "context" : "This approach requires large speech corpus with sentence or phoneme level transcription of the speech utterances (Thomas Hain et al 2005; Ravishankar 1996) [2].",
      "startOffset" : 156,
      "endOffset" : 159
    }, {
      "referenceID" : 2,
      "context" : "A method called bootstrapping is proposed by Rabiner et al (1982)[3] which can increase the transcribed data for training the system, for speech recognition.",
      "startOffset" : 65,
      "endOffset" : 68
    }, {
      "referenceID" : 3,
      "context" : "Ljolje & Riley (1991)[4] have used an automatic approach to segmentation and labeling of speech when only the orthographic transcription of speech is available.",
      "startOffset" : 21,
      "endOffset" : 24
    }, {
      "referenceID" : 4,
      "context" : "Kemp&Waibel (1998)[5] used unsupervised training approach for speech recognition for TV broadcasts.",
      "startOffset" : 18,
      "endOffset" : 21
    }, {
      "referenceID" : 5,
      "context" : "Wessel&Ney (2001)[6] have proposed an approach in which a low-cost recognizer trained with one hour of manually transcribed speech is used to recognize 72 hours of unrestricted acoustic data.",
      "startOffset" : 17,
      "endOffset" : 20
    }, {
      "referenceID" : 6,
      "context" : "Lamel et al (2002)[7] have shown that the acoustic models can be initialized using as little as 10 minutes of manually annotated data.",
      "startOffset" : 18,
      "endOffset" : 21
    }, {
      "referenceID" : 7,
      "context" : "Incremental maximum a posteriori estimation of HMMs is proposed by Gotoh & Hochberg (1995)[8].",
      "startOffset" : 90,
      "endOffset" : 93
    }, {
      "referenceID" : 8,
      "context" : "Chang et al (2000)[9] proposed a method which extracts articulatoryacoustic phonetic features from each frame of speech signal and then the phone is identified using neural networks.",
      "startOffset" : 18,
      "endOffset" : 21
    }, {
      "referenceID" : 9,
      "context" : "There is an interesting approach proposed by Nagarajan & Murthy (2004)[10] for Indian languages.",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 9,
      "context" : "While these systems have been successful [10, 12], they rely on the incorrect assumption of statistical conditional independence between frames.",
      "startOffset" : 41,
      "endOffset" : 49
    }, {
      "referenceID" : 0,
      "context" : "[1] Huang, Acero 1993 Spoken Language Processing.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 1,
      "context" : "(New Jersy: Prentice Hall) [2] Thomas Hain, et al 2005 Automatic transcription of conversational telephone speech.",
      "startOffset" : 27,
      "endOffset" : 30
    }, {
      "referenceID" : 2,
      "context" : "Speech, Audio Processing 13: 1173– 1185 [3] Rabiner L R Rosenberg A E, Wilpon J G, Zampini T M 1982 A bootstrapping training technique for obtaining demisyllabic reference patterns.",
      "startOffset" : 40,
      "endOffset" : 43
    }, {
      "referenceID" : 3,
      "context" : "71: 1588–1595 [4] Ljolje A, Riley M D 1991 Automatic segmentation and labelling of speech.",
      "startOffset" : 14,
      "endOffset" : 17
    }, {
      "referenceID" : 4,
      "context" : ", Speech, and Signal Processing 1: 473–476 [5] Kemp T, Waibel A 1998 Unsupervised training of a speech recognizer using tv broadcasts.",
      "startOffset" : 43,
      "endOffset" : 46
    }, {
      "referenceID" : 5,
      "context" : "5 Sydney, Australia, 2207–2210 [6] Wessel F, Ney H 2001 Unsupervised training of acoustic models for large vocabulary continuous speech recognition.",
      "startOffset" : 31,
      "endOffset" : 34
    }, {
      "referenceID" : 6,
      "context" : "307–310 [7] Lamel L, Gauvain J-L, Adda G 2002 Unsupervised acoustic model training.",
      "startOffset" : 8,
      "endOffset" : 11
    }, {
      "referenceID" : 7,
      "context" : "877–880 [8] Gotoh Y and Hochberg M M 1995 Incremental map estimation of hmms for efficient training and improved performance.",
      "startOffset" : 8,
      "endOffset" : 11
    }, {
      "referenceID" : 8,
      "context" : ", Speech, and Signal Processing 877–880 [9] Chang S, Sastri L, Greenberg S 2000 Automatic phonetic transcription of sponta- neous speech.",
      "startOffset" : 40,
      "endOffset" : 43
    }, {
      "referenceID" : 9,
      "context" : "Spoken Language Processing 4: 330–333 [10] Nagarajan T, Murthy H A 2004 Non-bootstrap approach to segmentation and labelling of continuous speech.",
      "startOffset" : 38,
      "endOffset" : 42
    } ],
    "year" : 2010,
    "abstractText" : "In recent decades, Speech interactive systems gained increasing importance. To develop Dictation System like Dragon for Indian languages it is most important to adapt the system to a speaker with minimum training. In this paper we focus on the importance of creating speech database at syllable units and identifying minimum text to be considered while training any speech recognition system. There are systems developed for continuous speech recognition in English and in few Indian languages like Hindi and Tamil. This paper gives the statistical details of syllables in Telugu and its use in minimizing the search space during recognition of speech. The minimum words that cover maximum syllables are identified. This words list can be used for preparing a small text which can be used for collecting speech sample while training the dictation system. The results are plotted for frequency of syllables and the number of syllables in each word. This approach is applied on the CIIL Mysore text corpus which is of 3 million words. Keywords-component; formatting; style; styling; insert (key words)",
    "creator" : "PScript5.dll Version 5.2.2"
  }
}