{
  "name" : "1608.06134.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "MEDIAN-BASED GENERATION OF SYNTHETIC SPEECH DURATIONS USING A NON-PARAMETRIC APPROACH",
    "authors" : [ "Srikanth Ronanki", "Oliver Watts", "Simon King", "Gustav Eje Henter" ],
    "emails" : [ "srikanth.ronanki@ed.ac.uk" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Index Terms— text-to-speech, speech synthesis, duration modelling, non-parametric models, LSTMs."
    }, {
      "heading" : "1. INTRODUCTION",
      "text" : "This paper presents a new approach to the modelling and generation of speech segment durations, a characteristic of speech which contributes to the perception of its rhythm. Generating appropriate rhythm and melody is a challenging but vital step in producing natural-sounding synthetic speech, particularly in expressive or conversational scenarios. Steady improvements have been made in statistical parametric speech synthesis (SPSS), in particular through the adoption of deep and recurrent machine-learning techniques in recent years [1, 2]. As we expect that high-level, long-range dependencies are of importance for the prosodic structure of speech, recurrent models such as LSTMs are well-suited to prosodic sequence modelling problems [3].\nIn spite of the progress, however, the prosodic characteristics of synthetic speech remain one of its major shortcomings. One factor which we suppose contributes to this shortcoming is that current systems effectively model duration with a Gaussian distribution and generate predictions from the mean of that distribution. This is problematic, as the distribution of speech-segment durations is well known to be skewed, and so using the mean of a normal distribution fit to such observations will not produce predictions that are most typical of the process (that is, have high probability). Another possible weakness of current approaches towards duration generation is that they typical operate as an initial stage, separate from the generation of acoustic features [3]. We consider it desirable to have a single model whose parameters are learned to simultaneously\ngenerate both segment durations and the frames of acoustic features within those segments. A major motivation for this is that such a joint model would allow the simultaneous adaptation [4] and control [5] of rhythmic, melodic and phonetic characteristics in a stable and consistent way. However, one obvious difficulty in implementing such a model is that predictions of segment durations and of acoustic observations are conventionally made on two separate time-scales.\nWe here present an approach to duration modelling which in essence consists of predicting at each acoustic frame a probability that the model will subsequently advance to the next phone in the phonetic sequence. We show that – assuming a recurrent model is used – this approach may describe any duration distribution on the positive integers. Although it is feasible to model duration by explicitly choosing other distributions which may be more appropriate than a Gaussian (e.g., log-normal or gamma distribution), these still might not be optimal for a given conditional duration, and so the possibility of not having to commit to a predetermined distribution before model training is a powerful advantage of our approach. Furthermore, as our model moves from modelling duration at the level of the phonetic segment to the level of the acoustic frame, our approach is a necessary ingredient of our on-going work in unifying models of durations and acoustics, so that acoustic parameters a phone transition probabilities are predicted jointly for each frame."
    }, {
      "heading" : "2. BACKGROUND",
      "text" : "In this section, we give a brief overview of how speech-sound durations have been generated in synthetic speech, with a particular emphasis on the use of statistical methods for this task. This provides context for the proposed method in Section 3."
    }, {
      "heading" : "2.1. A Brief Review of Modelling and Generating Durations",
      "text" : "In early, formant-based synthesis systems, phone durations were generated by rule [6]. Rules were commonly hand-crafted, rather than learned from data, meaning that no statistical modelling was used. The concatenative synthesis approaches that emerged next did not require modelling or generating durations, since the units themselves (typically diphones) possess intrinsic durations. That said, some approaches allowed predicted durations to be incorporated into the target cost [7].\nThe rise of statistical parametric speech synthesis (SPSS) [8, 9] has introduced a new methodology for duration generation, in which a statistical model (probability distribution) is created to describe speech-sound durations. This distribution is supplemented by two things: a machine learning method to predict the properties of the statistical model from the input text (i.e, the linguistic features), and a principle for generating durations from the model distribution, such as random sampling or taking the mean of the distribution.\nar X\niv :1\n60 8.\n06 13\n4v 1\n[ cs\n.C L\n] 2\n2 A\nug 2\n01 6\nThe first SPSS systems were based on simple hidden Markov models (HMMs), which implies that state durations (commonly five states per phone) are assumed follow a memoryless geometric distribution. Decision trees (DTs) were used to predict the distribution parameter based on training data, with the mean of the predicted distribution used for generation.\nIn reality, a geometric distribution is quite far removed from how natural speech durations are distributed, as can be seen in Figure 1. Zen et al. [10] introduced the idea of using hidden semiMarkov models (HSMMs) to describe durations in the context of speech synthesis. These models track the amount of time (acoustic frames) spent in the current HMM state, and allow the frame counter to influence the probability of transitioning, making it possible to model a much wider class of duration distributions. Typically, HSMM durations would be assumed to follow a parametric distribution of some sort; the widely-used HMM-based speech synthesis system (HTS) [11] defaults to Gaussian duration distributions, for instance, although the skewness and non-negativity of speech durations mean that other choices of distribution (log-normal, gamma) might be more suitable [12, 13]. As with HMMs, decision trees would be used to predict distribution parameters per state, and the mean of the predicted distribution was used for generation.\nRecently, SPSS has seen stronger machine-learning techniques such as deep and/or recurrent neural networks replace decision trees for predicting the properties of duration distributions from the text, e.g., [3, 14]. Most often, these systems train DNNs or RNNs to minimise the mean squared prediction error, which is theoretically equivalent to using a Gaussian duration model (with a globally tied standard deviation) in conjunction with mean-based duration generation. The evolution of approaches to duration generation for TTS is summarised in Table 1.\nLooking at Table 1, we see that all canonical methods predict only a few numbers per phone; at most 5 means and 5 standard deviations for Gaussian state-level predictions, of which only the means affect output generation. This renders the methods incapable of predicting general duration distributions with more degrees of freedom. In contrast to these prior methods, we describe a fundamentally nonparametric approach in which a model is trained to predict the phone transition probability at each timestep, i.e., acoustic frame, as outlined on the final line of Table 1. This set-up can in theory describe\nany probability mass function on the positive integers. (In practice, performance may be limited by biases in the learning algorithm used.) It thus becomes possible to meaningfully represent any and all important properties of the duration distribution, for instance its skewness, which most models hitherto have ignored."
    }, {
      "heading" : "2.2. Median-Based Generation",
      "text" : "All methods in Table 1 that predict a distribution of durations automatically support a wide variety of methods for generating output durations from this distribution. While in principle, natural speech is a random sample drawn from the true duration distribution, the output naturalness of sampling methods in speech synthesis has been found to perform poorly unless highly accurate models are used [15]. As a consequence, most SPSS systems use deterministic generation methods, which in practice has come to be synonymous with generating the mean duration.\nWe here consider a scheme where durations generated at synthesis time are based on the median – rather than the conventional mean – of the predicted duration distribution. Since we are no longer assuming that the duration distribution is symmetric, the median will typically differ from the distribution mean. To our knowledge, no other paper in the statistical speech synthesis literature has considered predicting a median duration different from the mean.\nConveniently, the median can be identified from the left tail of the distribution, whereas computing the mean requires evaluating the probability mass function over its entire, infinite support. This property enables median-duration based sequential output generation with no look-ahead or other overhead, which is attractive for incremental synthesis approaches. Unlike the mean, the median is nearly always an integer.\nThe advantages of generating median durations are more than computational: For skewed distributions, including the distribution of natural speech-sound durations as graphed in Figure 1, the median is frequently closer to the peak of the distribution than the mean is; cf. [16, 17]. This implies that, in the spirit of most likely output parameter generation [18], median-based duration prediction is likely closer to the peak density – the “most typical” outcome – than the mean is. (The mean duration is often atypically long.) Perhaps most importantly, the median is a statistically robust quantity, and is not affected by the tails of the real duration distribution. Statistical robustness is compelling for speech synthesis [19], particularly for big and found datasets, as it reduces the sensitivity to errors and unexpected behaviour in the training corpus. Among other things, this could be of value with the highly expressive and variable training data used for the experiments in Section 4."
    }, {
      "heading" : "2.3. Frame-Level Duration Prediction",
      "text" : "While frame-level duration predictions are uncommon, this paper is not the first to suggest it. Watts et al. [20] described a joint model of duration and speech parameters, where a deep neural network was trained to simultaneously output acoustic parameters and a 5-dimensional (phone) state-duration vector for each frame. Synthesis with this type of network results in a chicken-and-egg problem, where state durations are both an input and an output of the sequence prediction network. This was solved by iteratively refining state-duration predictions over multiple passes, which is slow. In our proposed method, a single pass suffices for duration generation. Furthermore, despite the frame-level granularity of the approach in [20], it is not straightforward to identify a probabilistic interpretation of their scheme.\nA general advantage of duration predictions made at the frame level is that they can be unified with the (traditionally distinct) prediction of acoustic features, such as pitch and vocal tract filter MGCs, that takes place for each frame. Such joint modelling of durations with acoustic properties of speech was a major factor in motivating the set-up in [20]. It is suspected that generating durations and fundamental frequency contours that are jointly appropriate may be of importance for synthetic speech prosody; cf. [21, 22]."
    }, {
      "heading" : "3. THEORY",
      "text" : ""
    }, {
      "heading" : "3.1. Preliminaries",
      "text" : "Let p ∈ {1, . . . , P} be a phone index, and let t ∈ {1, . . . , T} be an index into frames. Let further Dp – a random variable – be the duration of phone p, and let dp ∈ Z > 0 be an outcome of Dp.1\nNatural speech phones have different duration distributions that depend on the input text. In TTS, the properties of the distribution Dp are predicted from contextual linguistic features lp extracted from the text by the synthesiser front-end. Specifically, duration modelling is the task of mapping the sequence of linguistic features (l1, . . . , lP ) to a sequence of predicted duration distributions (D1, . . . , DP ). Duration generation, meanwhile, is the task of mapping (l1, . . . , lP ) to a sequence of generated durations (d̂1, . . . , d̂P ). In SPSS, one or the other of these mappings is learned from a corpus of parallel text and speech data using a machine learning method; in this paper, deep and recurrent neural networks will be used. We will write D to denote a dataset of parallel input features l and random variable outcomes d used to train this predictor."
    }, {
      "heading" : "3.2. Conventional Duration Modelling",
      "text" : "In statistical synthesisers that generate durations on the state or phone level, the conventional approach is to assume that the durations Dp follow some parametric family fD with a parameter θ ∈ RN (N being the number of degrees of freedom), i.e.,\nP(Dp = d) = fD(d; θp). (1)\nPredicting the distribution Dp then reduces to the stochastic regression problem of predicting the distribution parameter θp from the phone-level parallel input-output dataset\nDp = ((l1, . . . , lP ), (d1, . . . , dP )). (2)\n1In many TTS systems, Dp is a vector of per-phone state durations, but we restrict ourselves to phone-level predictions in this paper; a state-based formulation is a straightforward extension.\nWe will write Lp to denote the linguistic information available to the predictor at p, which is lp for feedforward approaches and (l1, . . . , lp) for unidirectional RNNs.\nIn practice, most contemporary DNN-based synthesisers do not perform full distribution modelling, but map directly from Lp to the property of the distribution Dp that they wish to generate, such as its mean E(Dp). The dominant principle – and the only one to be considered for the baselines in this paper – is to tune the weightsW of a DNN or RNN d(L; W ) to minimise the mean squared error (MSE) on the training dataset,\nŴ (Dp) = argmin W ∑ (Lp, dp)∈D (dp − d(Lp; W ))2; (3)\nsynthesis-time durations d̂p are then generated by\nd̂(Lp) = d(Lp; Ŵ (Dp)). (4)\nThe theoretically optimal predictor d̂? that minimises the MSE is the conditional mean,\nd̂?p(Lp) = argmin d̂\nE ( (Dp − d̂)2 ∣∣∣Lp) (5) = E(Dp |Lp), (6)\nso the end result is very similar to fitting a Gaussian duration model fD and using the mean of that fitted Gaussian to generate durations."
    }, {
      "heading" : "3.3. Non-Parametric Duration Modelling",
      "text" : "We will now describe a scheme that, unlike conventional, phonelevel approaches with parametric families fD(d; θ), is able to model and predict arbitrary duration distributions for D (restricted only by the biases of the machine learning method used). The key idea is to make predictions at the frame level about when phone transitions occur.\nAssume phone durations are known up until the current frame t, and let p(t′) for 1 ≤ t′ ≤ t be a function that maps from a given frame t′ ≤ t to the phone it has been assigned to. We can then define a frame-level sequence Lt of linguistic features\nLt = (l1, . . . , lt) (7) = (lp(1), . . . , lp(t)) (8)\nup until t, which is constant for all frames within each phone. For brevity, we shall write p for the current phone p(t). Finally, we let t0 be the final frame of the previous phone, so that we can define nt = t− t0 ≥ 1, the duration of the current phone so far.\nWe now define the transition probability πt for the phone p at time t, given the linguistic features up until this point – that is,\nπt = P(Dp = nt |Dp ≥ nt, Lt). (9)\nAs long as the transition probabilities satisfy πt ∈ [0, 1] and ∞∏\nt′=t0+1\n(1− πt′) = 0 (10)\nthey induce a unique duration distribution\nP(Dp = nt |Lt) = πt t0+nt−1∏ t′=t0+1 (1− πt′) (11)\non the positive integers. We propose to build a predictor, based on training data, that estimates πt from the linguistic input features. Specifically, we will train this predictor on the frame-level dataset\nDt = (LT , (x1, . . . , xT )), (12)\nwhere xt is an indicator variable that equals one if and only if t is the final frame of the current phone, i.e.,\nxt = { 1 if t = t0 + dp 0 otherwise.\n(13)\nIn this paper, we consider a deep and unidirectional recurrent neural network x(L; W ), with weights Ŵ trained to minimise the MSE in recursively predicting the indicator variable xt – that is,\nŴ (Dt) = argmin W ∑ t (xt − x(Lt; W ))2. (14)\nThe hypothetical predictor x̂? that minimises this MSE is the conditional mean,\nx̂?t (Lt ) = argmin x̂\nE ( (Xt − x̂)2 ∣∣∣Lt) (15) = E(Xt |Lt) (16)\n= ∑\nx∈{0, 1}\nx · P(Xt = x |Lt) (17)\n= P(Xt = 1 |Lt), (18)\nwhich is mathematically equivalent to the transition probability πt. This means that, as long as the predictor of Xt is theoretically capable of generating arbitrary outputs for every frame, our approach can describe virtually any transition distribution – and thus any duration distribution – on the positive integers. LSTMs and other RNNs satisfy this requirement, due to their internal state (memory), here denoted ct, which evolves from frame to frame.\nUnlike the maximum likelihood objective function, the MSE used here is not disproportionately sensitive to large errors in probability estimates of rare events (outliers). This makes the estimated Ŵ more statistically robust to errors in data and model assumptions."
    }, {
      "heading" : "3.4. From Transitions to Median Durations",
      "text" : "Having defined a phone duration distribution in (11) and trained a model to predict this distribution from data, we must consider how predicted durations d̂ are to be generated from this distribution. As discussed in Section 2.2, sampling typically yields poor naturalness, while mean-based generation is sensitive to the tails of the distribution and unsuitable for sequential generation. On the other hand, it is straightforward to derive the right tail probability of the phone duration distribution induced by the values of πt seen thus far, as\nP(Dp > nt |Lt) = t0+nt∏\nt′=t0+1\n(1− πt′). (19)\nThis relation straightforwardly enables synthesis based on the predicted median duration: by stepping from nt = 1 and upwards, the (estimated) median duration d̂p of phone p is reached when P(D > d) first dips down to 0.5 or below,\nd̂p = min nt∈Z nt (20)\nsuch that P(Dp > nt) ≤ 0.5. (21)\nExpression (19) thus allows us to generate frames sequentially, advancing p(t+1) to the next phone p+1 when the predicted median duration is reached, with no additional overhead at generation time.\nJust as the mean squared error is minimised by the mean, the median is the theoretical minimiser of another error measure, namely the mean absolute error (MAE),\nMAE(d̂) = ∑ p∈Dp ∣∣∣dp − d̂p∣∣∣ . (22) If a method improves the MAE, we would expect its prediction to be closer to the (conditional) median of the data. Interestingly, summing absolute rather than squared errors is a common component of the mel-cepstral distortion (MCD) [23] often used to evaluate acoustic models in speech synthesis, but the same idea is less commonly seen for evaluating generated durations."
    }, {
      "heading" : "3.5. Adding External Memory",
      "text" : "In the proposal described so far, progress through the current phone is tracked solely via the internal state of the predictor used (ct for an LSTM). This is in contrast to the approach used by the hidden semi-Markov models in HTS, which achieved more general duration distributions than regular HMMs by maintaining an external counter of the number of frames spent in each state, and using it to compute the transition probability. However, nothing prevents us from adding our variable nt, which counts the frame number within the current phone, as an input to the neural network x( · ; W ) that predicts frame-level transition probabilities, in addition to the regular, linguistic features Lt. We call these augmented features l′t and\nL′t = ([l ᵀ 1 , n1] ᵀ, . . . , [lᵀt , nt] ᵀ), (23)\nso that the augmented predictor is x(L′; W ). One may surmise that having nt as an input feature could increase performance, as it provides highly relevant information to the model at all times, instead of hoping the predictor will discover the importance of duration tracking it on its own during training. To test this hypothesis, our experiments in Section 4 compare two systems that differ only in whether or not they incorporate an external counter nt as an input to the central neural network.\nAs a side note, appending nt to lt makes the inputs to the framelevel neural network x( · ; W ) different with each frame. This makes it possible for the predicted transition distributions to differ from frame to frame as well, even without using a stateful predictor such as an RNN. This should, for example, enable the prediction of arbitrary transition distributions also when x( · ; W ) is a nonrecurrent, feedforward neural network. We have, however, not explored this possibility in the current paper."
    }, {
      "heading" : "4. EXPERIMENTAL SET-UP",
      "text" : ""
    }, {
      "heading" : "4.1. Data",
      "text" : "As a first inquiry into the viability of our frame-level durationprediction approach, we conducted some experiments on a speech database created for to the 2016 Blizzard Challenge.2 The database – provided to the Challenge by Usborne Publishing Ltd. – consists of the speech and text of 50 children’s audiobooks spoken by a British female speaker. We made use of a segmentation of the audiobooks carried out by another Challenge participant3 and kindly\n2http://www.synsig.org/index.php/Blizzard Challenge 2016 3Innoetics: https://www.innoetics.com\nmade available to other participants. The total duration of the audio was approximately 4.33 hours after segmentation. For the purposes of the work presented here, 4% of the data was set aside as a test set. The test set consists of three whole short stories: Goldilocks and the Three Bears, The Boy Who Cried Wolf and The Enormous Turnip, having a total combined duration of approximately 10 minutes."
    }, {
      "heading" : "4.2. Feature extraction",
      "text" : "We obtained a state-level forced alignment of the sentencesegmented data described above using context independent HMMs. Festvox’s ehmm [24] was used to insert pauses into the annotated phone sequences based on the acoustics, to improve the accuracy of forced-aligned durations. Each phone was then characterised by a vector of 481 text-derived binary and numerical features: these features are a subset of the features used in decision-tree clustering questions from the HTS public demo [11]; numerical features queried by those questions were used directly where possible. All inputs were normalised to the range [0.01, 0.99]. The phone durations used to train conventional baselines was obtained by counting the frames in a phone. Contrary to our previous work [19], sub-phone states were not used in either DNN training or prediction. This is consistent with our longer-term goal of freeing ourselves from depending on often arbitrary HMM sub-phone alignments."
    }, {
      "heading" : "4.3. System training",
      "text" : "Four systems were trained: two phone-level predictors (which we identify as Phone-DNN and Phone-LSTM) to provide benchmarks and assess the impact of recurrent modelling on duration prediction, and two experimental systems implementing the new idea (which we identify as Frame-LSTM-I and Frame-LSTM-E)."
    }, {
      "heading" : "4.3.1. Phone-DNN and Phone-LSTM",
      "text" : "The two baselines use phone-level linguistic features as input and are optimised to predict the (mean and variance normalised) duration of the phones in the training data. Phone-DNN is a feedforward DNN with six layers of 1024 nodes each. The hidden nodes used tanh activation and the output was linear. Phone-LSTM was configured with five feed-forward layers of 1024 nodes each and a final unidirectional SLSTM [25] hidden layer consisting of 512 nodes."
    }, {
      "heading" : "4.3.2. Frame-LSTM-I and Frame-LSTM-E",
      "text" : "Both proposed systems used the same architecture as that of PhoneLSTM but – unlike the baseline systems – were trained with 1 datapoint per frame instead of per phone. The targets presented in training were 0.0 for non-phone-final frames, and 1.0 for phone-final frames. Frame-LSTM-I took only the vector representing the current phone as input; at run-time it was therefore required to rely on its internal memory (hidden state and LSTM cell state) to determine the phone transition probability at any given frame. The inputs to Frame-LSTM-I are identical for all frames in any given phone. Frame-LSTM-E implemented the idea described in Section 3.5: inputs encoding the current phone context are augmented with a frame counter indicating the number of frames that have passed since the start of the phone. Frame-LSTM-E can therefore also rely on this external memory of how much time has elapsed within the current phone when making phone transition predictions, in addition to internal memory."
    }, {
      "heading" : "4.4. Synthesis",
      "text" : "At synthesis time, ehmm phone sequences derived from the test data were used as input to each duration prediction model. This corresponds to using an oracle pausing strategy, but providing no other acoustically-derived information to the predictors. Generation from Phone-DNN and Phone-LSTM is straightforward: an (effectively mean) output is generated for each phone.\nMedian-based duration predictions were obtained from FrameLSTM-I and Frame-LSTM-E by running the models recurrently over frame-level inputs and using the technique explained in Section 3.4 and summarised in pseudocode in Algorithm 1, to decide when the median had been reached and thus when to move to the next phone. By keeping track of the time spent in each phone, we obtain medianbased predictions of phone duration either with external memory of time elapsed in current phone (Frame-LSTM-E) or without it (Frame-LSTM-I).\nAlgorithm 1 Switching criterion for Frame-LSTM-I 1: procedure SWITCHPROB(linguisticInput) 2: nof ← length of linguisticInput 3: for each phoneNum in nof do 4: remMass← 1 5: for each frame t in order do 6: ph← linguisticInput[phoneNum] 7: probSwitchAtT ← FrameLSTM(ph) 8: remMass← remMass ∗ (1− probSwitchAtT ) 9: if remMass <= 0.5 then 10: predictedDur[phoneNum]← t+ 1 11: break; 12: end if 13: end for 14: end for 15: return predictedDur 16: end procedure"
    }, {
      "heading" : "5. RESULTS",
      "text" : "Table 2 gives RMSE, mean absolute error (MAE) and Pearson correlation computed by comparing the predicted durations against the held-out reference. Silence segments were excluded from these calculations.\nFirstly, results for the benchmark systems show that PhoneLSTM outperforms Phone-DNN in every respect, confirming the importance of the LSTM’s recurrence for duration modelling. Turning to the experimental systems, it can be seen that while the mean-based predictions of the LSTM and the DNN baselines clearly outperform the proposed methods in terms of RMSE, the gap is much smaller when it comes to MAE. Median-based generation methods are expected to score better on MAE than RMSE, because – as discussed in Section 3.4 – the median is the theoretical minimiser of MAE, while the mean minimises the (R)MSE.\nThe breakdown of results by phonetic class given in Table 3 shows an interesting phenomenon: while the proposed system Frame-LSTM-E’s MAE is worse than the LSTM benchmark for vowels and slightly worse for consonants overall, for all classes of consonant except plosives the proposed method performs better.\nWhile the best-performing experimental system does not overall outperform the LSTM baseline even on MAE, the gap in performance is effectively closed (4.556 vs. 4.574). This means that\nwe have devised a system which gives similar performance in the MAE sense as our existing one, but provides greater compatibility with our acoustic models, as it too operates at the frame level. The next logical step, therefore, is to apply this idea to joint modelling of duration and acoustic features."
    }, {
      "heading" : "6. CONCLUSIONS AND FUTURE WORK",
      "text" : "We have described a new duration modeling paradigm with LSTMs which operates at frame-level for duration prediction in speech synthesis. Experiments conducted on an audiobook data were found to perform competitively when compared with a baseline phone-level LSTM system. Future work includes joint modeling of duration and acoustic features along with subjective evaluation of synthesised speech."
    }, {
      "heading" : "7. ACKNOWLEDGEMENTS",
      "text" : "This research was supported by EPSRC Programme Grant EP/I031022/1, Natural Speech Technology (NST). The NST research data collection may be accessed at http://hdl.handle. net/10283/786."
    }, {
      "heading" : "8. REFERENCES",
      "text" : "[1] Heiga Zen, Andrew Senior, and Mike Schuster, “Statistical parametric speech synthesis using deep neural networks,” in Proc. ICASSP, 2013, pp. 7962–7966.\n[2] Zhizheng Wu, Cassia Valentini-Botinhao, Oliver Watts, and Simon King, “Deep neural networks employing multi-task learning and stacked bottleneck features for speech synthesis,” in Proc. ICASSP, 2015, pp. 4460–4464.\n[3] Heiga Zen and Haşim Sak, “Unidirectional long short-term memory recurrent neural network with recurrent output layer for low-latency speech synthesis,” in Proc. ICASSP, 2015, pp. 4470–4474.\n[4] Zhizheng Wu, Pawel Swietojanski, Christophe Veaux, Steve Renals, and Simon King, “A study of speaker adaptation for DNN-based speech synthesis,” in Proc. Interspeech, 2015.\n[5] Oliver Watts, Zhizheng Wu, and Simon King, “Sentence-level control vectors for deep neural network speech synthesis,” in Proc. Interspeech, 2015, pp. 2217–2221.\n[6] Dennis H. Klatt, “Review of text-to-speech conversion for English,” J. Acoust. Soc. Am., vol. 82, no. 3, pp. 737–793, 1987.\n[7] Ivan Bulyko and Mari Ostendorf, “Joint prosody prediction and unit selection for concatenative speech synthesis,” in Proc. ICASSP, 2001, vol. 2, pp. 781–784.\n[8] Heiga Zen, Keiichi Tokuda, and Alan W. Black, “Statistical parametric speech synthesis,” Speech Commun., vol. 51, no. 11, pp. 1039–1064, 2009.\n[9] Simon King, “An introduction to statistical parametric speech synthesis,” Sadhana, vol. 36, no. 5, pp. 837–852, 2011.\n[10] Heiga Zen, Keiichi Tokuda, Takashi Masuko, Takao Kobayashi, and Tadashi Kitamura, “Hidden semi-markov model based speech synthesis,” in Proc. Interspeech, 2004, pp. 1393– 1396.\n[11] Heiga Zen, Takashi Nose, Junichi Yamagishi, Shinji Sako, Takashi Masuko, Alan Black, and Keiichi Tokuda, “The HMM-based speech synthesis system (HTS) version 2.0,” in Proc. SSW, 2007, vol. 6, pp. 294–299.\n[12] W. Nick Campbell, “Syllable-level duration determination,” in Proc. Eurospeech, 1989, pp. 2698–2701.\n[13] K. Huber, “A statistical model of duration control for speech synthesis,” in Proc. EUSIPCO, 1990, pp. 1127–1130.\n[14] Heiga Zen, Yannis Agiomyrgiannakis, Niels Egberts, Fergus Henderson, and Przemysław Szczepaniak, “Fast, compact, and high quality LSTM-RNN based statistical parametric speech synthesizers for mobile devices,” in Proc. Interspeech, 2016.\n[15] Gustav Eje Henter, Thomas Merritt, Matt Shannon, Catherine Mayo, and Simon King, “Measuring the perceptual effects of modelling assumptions in speech synthesis using stimuli constructed from repeated natural speech,” in Proc. Interspeech, 2014, pp. 1504–1508.\n[16] H. L. MacGillivray, “The mean, median, mode inequality and skewness for a class of densities,” Australian Journal of Statistics, vol. 23, no. 2, pp. 247–250, 1981.\n[17] Sanjib Basu and Anirban DasGupta, “The mean, median, and mode of unimodal distributions: a characterization,” Theory Probab. Appl., vol. 41, no. 2, pp. 210–223, 1997.\n[18] Keiichi Tokuda, Takayoshi Yoshimura, Takashi Masuko, Takao Kobayashi, and Tadashi Kitamura, “Speech parameter generation algorithms for HMM-based speech synthesis,” in Proc. ICASSP, 2000, vol. 3, pp. 1315–1318.\n[19] Gustav Eje Henter, Srikanth Ronanki, Oliver Watts, Mirjam Wester, Zhizheng Wu, and Simon King, “Robust TTS duration modelling using DNNs,” in Proc. ICASSP, 2016, vol. 41, pp. 5130–5134.\n[20] Oliver Watts, Srikanth Ronanki, Zhizheng Wu, Tuomo Raitio, and Antti Suni, “The NST–GlottHMM entry to the Blizzard Challenge 2015,” in Proc. Blizzard Challenge Workshop, 2015.\n[21] Raul Fernandez, Asaf Rendel, Bhuvana Ramabhadran, and Ron Hoory, “Prosody contour prediction with long short-term memory, bi-directional, deep recurrent neural networks,” in Proc. Interspeech, 2014, pp. 2268–2272.\n[22] Srikanth Ronanki, Gustav Eje Henter, Zhizheng Wu, and Simon King, “A template-based approach for speech synthesis intonation generation using LSTMs,” in Proc. Interspeech, 2016.\n[23] Robert F. Kubichek, “Mel-cepstral distance measure for objective speech quality assessment,” in Proc. IEEE Pac. Rim Conf. Commun. Comput. Signal Process., 1993, vol. 1, pp. 125–128.\n[24] Kishore Prahallad, Alan W. Black, and Ravishankhar Mosur, “Sub-phonetic modeling for capturing pronunciation variations for conversational speech synthesis,” in Proc. ICASSP, 2006, pp. I–853–I–856.\n[25] Zhizheng Wu and Simon King, “Investigating gated recurrent networks for speech synthesis,” in Proc. ICASSP, 2016, pp. 5140–5144."
    } ],
    "references" : [ {
      "title" : "Statistical parametric speech synthesis using deep neural networks",
      "author" : [ "Heiga Zen", "Andrew Senior", "Mike Schuster" ],
      "venue" : "Proc. ICASSP, 2013, pp. 7962–7966.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Deep neural networks employing multi-task learning and stacked bottleneck features for speech synthesis",
      "author" : [ "Zhizheng Wu", "Cassia Valentini-Botinhao", "Oliver Watts", "Simon King" ],
      "venue" : "Proc. ICASSP, 2015, pp. 4460–4464.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Unidirectional long short-term memory recurrent neural network with recurrent output layer for low-latency speech synthesis",
      "author" : [ "Heiga Zen", "Haşim Sak" ],
      "venue" : "Proc. ICASSP, 2015, pp. 4470–4474.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "A study of speaker adaptation for DNN-based speech synthesis",
      "author" : [ "Zhizheng Wu", "Pawel Swietojanski", "Christophe Veaux", "Steve Renals", "Simon King" ],
      "venue" : "Proc. Interspeech, 2015.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Sentence-level control vectors for deep neural network speech synthesis",
      "author" : [ "Oliver Watts", "Zhizheng Wu", "Simon King" ],
      "venue" : "Proc. Interspeech, 2015, pp. 2217–2221.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Review of text-to-speech conversion for English",
      "author" : [ "Dennis H. Klatt" ],
      "venue" : "J. Acoust. Soc. Am., vol. 82, no. 3, pp. 737–793, 1987.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 1987
    }, {
      "title" : "Joint prosody prediction and unit selection for concatenative speech synthesis",
      "author" : [ "Ivan Bulyko", "Mari Ostendorf" ],
      "venue" : "Proc. ICASSP, 2001, vol. 2, pp. 781–784.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Statistical parametric speech synthesis",
      "author" : [ "Heiga Zen", "Keiichi Tokuda", "Alan W. Black" ],
      "venue" : "Speech Commun., vol. 51, no. 11, pp. 1039–1064, 2009.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "An introduction to statistical parametric speech synthesis",
      "author" : [ "Simon King" ],
      "venue" : "Sadhana, vol. 36, no. 5, pp. 837–852, 2011.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Hidden semi-markov model based speech synthesis",
      "author" : [ "Heiga Zen", "Keiichi Tokuda", "Takashi Masuko", "Takao Kobayashi", "Tadashi Kitamura" ],
      "venue" : "Proc. Interspeech, 2004, pp. 1393– 1396.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "The HMM-based speech synthesis system (HTS) version 2.0",
      "author" : [ "Heiga Zen", "Takashi Nose", "Junichi Yamagishi", "Shinji Sako", "Takashi Masuko", "Alan Black", "Keiichi Tokuda" ],
      "venue" : "Proc. SSW, 2007, vol. 6, pp. 294–299.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Syllable-level duration determination",
      "author" : [ "W. Nick Campbell" ],
      "venue" : "Proc. Eurospeech, 1989, pp. 2698–2701.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 1989
    }, {
      "title" : "A statistical model of duration control for speech synthesis",
      "author" : [ "K. Huber" ],
      "venue" : "Proc. EUSIPCO, 1990, pp. 1127–1130.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 1990
    }, {
      "title" : "Fast, compact, and high quality LSTM-RNN based statistical parametric speech synthesizers for mobile devices",
      "author" : [ "Heiga Zen", "Yannis Agiomyrgiannakis", "Niels Egberts", "Fergus Henderson", "Przemysław Szczepaniak" ],
      "venue" : "Proc. Interspeech, 2016.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Measuring the perceptual effects of modelling assumptions in speech synthesis using stimuli constructed from repeated natural speech",
      "author" : [ "Gustav Eje Henter", "Thomas Merritt", "Matt Shannon", "Catherine Mayo", "Simon King" ],
      "venue" : "Proc. Interspeech, 2014, pp. 1504–1508.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "The mean, median, mode inequality and skewness for a class of densities",
      "author" : [ "H.L. MacGillivray" ],
      "venue" : "Australian Journal of Statistics, vol. 23, no. 2, pp. 247–250, 1981.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 1981
    }, {
      "title" : "The mean, median, and mode of unimodal distributions: a characterization",
      "author" : [ "Sanjib Basu", "Anirban DasGupta" ],
      "venue" : "Theory Probab. Appl., vol. 41, no. 2, pp. 210–223, 1997.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Speech parameter generation algorithms for HMM-based speech synthesis",
      "author" : [ "Keiichi Tokuda", "Takayoshi Yoshimura", "Takashi Masuko", "Takao Kobayashi", "Tadashi Kitamura" ],
      "venue" : "Proc. ICASSP, 2000, vol. 3, pp. 1315–1318.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Robust TTS duration modelling using DNNs",
      "author" : [ "Gustav Eje Henter", "Srikanth Ronanki", "Oliver Watts", "Mirjam Wester", "Zhizheng Wu", "Simon King" ],
      "venue" : "Proc. ICASSP, 2016, vol. 41, pp. 5130–5134.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "The NST–GlottHMM entry to the Blizzard Challenge 2015",
      "author" : [ "Oliver Watts", "Srikanth Ronanki", "Zhizheng Wu", "Tuomo Raitio", "Antti Suni" ],
      "venue" : "Proc. Blizzard Challenge Workshop, 2015.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Prosody contour prediction with long short-term memory, bi-directional, deep recurrent neural networks",
      "author" : [ "Raul Fernandez", "Asaf Rendel", "Bhuvana Ramabhadran", "Ron Hoory" ],
      "venue" : "Proc. Interspeech, 2014, pp. 2268–2272.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "A template-based approach for speech synthesis intonation generation using LSTMs",
      "author" : [ "Srikanth Ronanki", "Gustav Eje Henter", "Zhizheng Wu", "Simon King" ],
      "venue" : "Proc. Interspeech, 2016.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Mel-cepstral distance measure for objective speech quality assessment",
      "author" : [ "Robert F. Kubichek" ],
      "venue" : "Proc. IEEE Pac. Rim Conf. Commun. Comput. Signal Process., 1993, vol. 1, pp. 125–128.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 1993
    }, {
      "title" : "Sub-phonetic modeling for capturing pronunciation variations for conversational speech synthesis",
      "author" : [ "Kishore Prahallad", "Alan W. Black", "Ravishankhar Mosur" ],
      "venue" : "Proc. ICASSP, 2006, pp. I–853–I–856.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Investigating gated recurrent networks for speech synthesis",
      "author" : [ "Zhizheng Wu", "Simon King" ],
      "venue" : "Proc. ICASSP, 2016, pp. 5140–5144.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Steady improvements have been made in statistical parametric speech synthesis (SPSS), in particular through the adoption of deep and recurrent machine-learning techniques in recent years [1, 2].",
      "startOffset" : 187,
      "endOffset" : 193
    }, {
      "referenceID" : 1,
      "context" : "Steady improvements have been made in statistical parametric speech synthesis (SPSS), in particular through the adoption of deep and recurrent machine-learning techniques in recent years [1, 2].",
      "startOffset" : 187,
      "endOffset" : 193
    }, {
      "referenceID" : 2,
      "context" : "As we expect that high-level, long-range dependencies are of importance for the prosodic structure of speech, recurrent models such as LSTMs are well-suited to prosodic sequence modelling problems [3].",
      "startOffset" : 197,
      "endOffset" : 200
    }, {
      "referenceID" : 2,
      "context" : "Another possible weakness of current approaches towards duration generation is that they typical operate as an initial stage, separate from the generation of acoustic features [3].",
      "startOffset" : 176,
      "endOffset" : 179
    }, {
      "referenceID" : 3,
      "context" : "A major motivation for this is that such a joint model would allow the simultaneous adaptation [4] and control [5] of rhythmic, melodic and phonetic characteristics in a stable and consistent way.",
      "startOffset" : 95,
      "endOffset" : 98
    }, {
      "referenceID" : 4,
      "context" : "A major motivation for this is that such a joint model would allow the simultaneous adaptation [4] and control [5] of rhythmic, melodic and phonetic characteristics in a stable and consistent way.",
      "startOffset" : 111,
      "endOffset" : 114
    }, {
      "referenceID" : 5,
      "context" : "In early, formant-based synthesis systems, phone durations were generated by rule [6].",
      "startOffset" : 82,
      "endOffset" : 85
    }, {
      "referenceID" : 6,
      "context" : "That said, some approaches allowed predicted durations to be incorporated into the target cost [7].",
      "startOffset" : 95,
      "endOffset" : 98
    }, {
      "referenceID" : 7,
      "context" : "The rise of statistical parametric speech synthesis (SPSS) [8, 9] has introduced a new methodology for duration generation, in which a statistical model (probability distribution) is created to describe speech-sound durations.",
      "startOffset" : 59,
      "endOffset" : 65
    }, {
      "referenceID" : 8,
      "context" : "The rise of statistical parametric speech synthesis (SPSS) [8, 9] has introduced a new methodology for duration generation, in which a statistical model (probability distribution) is created to describe speech-sound durations.",
      "startOffset" : 59,
      "endOffset" : 65
    }, {
      "referenceID" : 9,
      "context" : "[10] introduced the idea of using hidden semiMarkov models (HSMMs) to describe durations in the context of speech synthesis.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 10,
      "context" : "Typically, HSMM durations would be assumed to follow a parametric distribution of some sort; the widely-used HMM-based speech synthesis system (HTS) [11] defaults to Gaussian duration distributions, for instance, although the skewness and non-negativity of speech durations mean that other choices of distribution (log-normal, gamma) might be more suitable [12, 13].",
      "startOffset" : 149,
      "endOffset" : 153
    }, {
      "referenceID" : 11,
      "context" : "Typically, HSMM durations would be assumed to follow a parametric distribution of some sort; the widely-used HMM-based speech synthesis system (HTS) [11] defaults to Gaussian duration distributions, for instance, although the skewness and non-negativity of speech durations mean that other choices of distribution (log-normal, gamma) might be more suitable [12, 13].",
      "startOffset" : 357,
      "endOffset" : 365
    }, {
      "referenceID" : 12,
      "context" : "Typically, HSMM durations would be assumed to follow a parametric distribution of some sort; the widely-used HMM-based speech synthesis system (HTS) [11] defaults to Gaussian duration distributions, for instance, although the skewness and non-negativity of speech durations mean that other choices of distribution (log-normal, gamma) might be more suitable [12, 13].",
      "startOffset" : 357,
      "endOffset" : 365
    }, {
      "referenceID" : 2,
      "context" : ", [3, 14].",
      "startOffset" : 2,
      "endOffset" : 9
    }, {
      "referenceID" : 13,
      "context" : ", [3, 14].",
      "startOffset" : 2,
      "endOffset" : 9
    }, {
      "referenceID" : 14,
      "context" : "While in principle, natural speech is a random sample drawn from the true duration distribution, the output naturalness of sampling methods in speech synthesis has been found to perform poorly unless highly accurate models are used [15].",
      "startOffset" : 232,
      "endOffset" : 236
    }, {
      "referenceID" : 15,
      "context" : "[16, 17].",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 16,
      "context" : "[16, 17].",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 17,
      "context" : "This implies that, in the spirit of most likely output parameter generation [18], median-based duration prediction is likely closer to the peak density – the “most typical” outcome – than the mean is.",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 18,
      "context" : "Statistical robustness is compelling for speech synthesis [19], particularly for big and found datasets, as it reduces the sensitivity to errors and unexpected behaviour in the training corpus.",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 19,
      "context" : "[20] described a joint model of duration and speech parameters, where a deep neural network was trained to simultaneously output acoustic parameters and a 5-dimensional (phone) state-duration vector for each frame.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 19,
      "context" : "Furthermore, despite the frame-level granularity of the approach in [20], it is not straightforward to identify a probabilistic interpretation of their scheme.",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 19,
      "context" : "Such joint modelling of durations with acoustic properties of speech was a major factor in motivating the set-up in [20].",
      "startOffset" : 116,
      "endOffset" : 120
    }, {
      "referenceID" : 20,
      "context" : "[21, 22].",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 21,
      "context" : "[21, 22].",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 0,
      "context" : "As long as the transition probabilities satisfy πt ∈ [0, 1] and",
      "startOffset" : 53,
      "endOffset" : 59
    }, {
      "referenceID" : 22,
      "context" : "Interestingly, summing absolute rather than squared errors is a common component of the mel-cepstral distortion (MCD) [23] often used to evaluate acoustic models in speech synthesis, but the same idea is less commonly seen for evaluating generated durations.",
      "startOffset" : 118,
      "endOffset" : 122
    }, {
      "referenceID" : 23,
      "context" : "Festvox’s ehmm [24] was used to insert pauses into the annotated phone sequences based on the acoustics, to improve the accuracy of forced-aligned durations.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 10,
      "context" : "Each phone was then characterised by a vector of 481 text-derived binary and numerical features: these features are a subset of the features used in decision-tree clustering questions from the HTS public demo [11]; numerical features queried by those questions were used directly where possible.",
      "startOffset" : 209,
      "endOffset" : 213
    }, {
      "referenceID" : 18,
      "context" : "Contrary to our previous work [19], sub-phone states were not used in either DNN training or prediction.",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 24,
      "context" : "Phone-LSTM was configured with five feed-forward layers of 1024 nodes each and a final unidirectional SLSTM [25] hidden layer consisting of 512 nodes.",
      "startOffset" : 108,
      "endOffset" : 112
    } ],
    "year" : 2017,
    "abstractText" : "This paper proposes a new approach to duration modelling for statistical parametric speech synthesis in which a recurrent statistical model is trained to output a phone transition probability at each timestep (acoustic frame). Unlike conventional approaches to duration modelling – which assume that duration distributions have a particular form (e.g., a Gaussian) and use the mean of that distribution for synthesis – our approach can in principle model any distribution supported on the non-negative integers. Generation from this model can be performed in many ways; here we consider output generation based on the median predicted duration. The median is more typical (more probable) than the conventional mean duration, is robust to training-data irregularities, and enables incremental generation. Furthermore, a frame-level approach to duration prediction is consistent with a longer-term goal of modelling durations and acoustic features together. Results indicate that the proposed method is competitive with baseline approaches in approximating the median duration of held-out natural speech.",
    "creator" : "LaTeX with hyperref package"
  }
}