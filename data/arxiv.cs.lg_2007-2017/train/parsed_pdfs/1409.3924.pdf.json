{
  "name" : "1409.3924.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Yuguang Wang Feilong Cao", "Yubo Yuan" ],
    "emails" : [ "feilongcao@gmail.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n40 9.\n39 24\nv1 [\ncs .N\nE ]\n1 3\nSe p\nExtreme Learning Machine (ELM), proposed by Huang et al., has been shown a promising learning algorithm for single-hidden layer feedforward neural networks (SLFNs). Nevertheless, because of the random choice of input weights and biases, the ELM algorithm sometimes makes the hidden layer output matrix H of SLFN not full column rank, which lowers the effectiveness of ELM. This paper discusses the effectiveness of ELM and proposes an improved algorithm called EELM that makes a proper selection of the input weights and bias before calculating the output weights, which ensures the full column rank of H in theory. This improves to some extend the learning rate (testing accuracy, prediction accuracy, learning time) and the robustness property of the networks. The experimental results based on both the benchmark function approximation and real-world problems including classification and regression applications show the good performances of EELM.\nKeywords: Feedforward neural networks; Extreme learning machine; Effective\nExtreme learning machine"
    }, {
      "heading" : "1 Introduction",
      "text" : "Extreme learning machine (ELM) proposed by Huang et al. shows as a useful learning method to train single-hidden layer feedforward neural networks (SLFNs) which have\n∗The research was supported by the National Natural Science Foundation of China (No. 60873206), the Natural Science Foundation of Zhejiang Province of China (No. Y7080235) and the Innovation Foundation of Post-Graduates of Zhejiang Province of China (No. YK2008066). †Corresponding author: Feilong Cao, E-mail: feilongcao@gmail.com\nbeen extensively used in many fields because of its capability of directly approximating nonlinear mappings by input data and providing models for a number of natural and artificial problems that are difficult to cope with by classical parametric techniques. So far there have been many papers addressing relative problems. We refer the reader to [12], [14]-[18], [20], [24], [27] and [28].\nIn theory, many researchers have explored the approximation ability of SLFNs. In 1989, Cybenko [7] and Funahashi [10] proved that any continuous functions can be approximated on a compact set with uniform topology by an SLFN with any continuous, sigmoidal activation function, which made a breakthrough in the artificial neural network field. Leshno [19] improved the results of Hornik [11] and proved that any continuous functions could be approximated by feedforward networks with a nonpolynomial activation function. Furthermore, some deep and systematic studies on the condition of activation function can be found in [4, 5, 6]. Recently, Cao et al. [3] constructively gave the estimation of upper bounds of approximation for continuous functions by SLFNs with the bounded, strictly monotone and odd activation function, which means that the neural networks can be constructed without training as long as the samples are given. In practical applications, for function approximation in a finite training set, Huang and Babri [13] showed that an SLFN with at most N hidden nodes and with almost any nonlinear activation function can exactly learn N distinct samples.\nIn comparison with other traditional learning methods such as BP algorithm, the ELM algorithm proves a faster learning algorithm for SLFNs. There are some advantages of the ELM algorithm: (1) easy to use and no parameters need to be tuned except predefined network architecture; (2) it is proved a faster learning algorithm compared to other conventional learning algorithms such as BP algorithm. Most training can be accomplished in seconds and minutes (for large-scale complex applications) which might not be easily obtained using other traditional learning methods; (3) it possesses similar high generalization performance as BP and SVM; (4) a wide range of activation functions including all piecewise continuous functions can be used as activation functions in ELM. Among the above four features, the most impressive one is the fast speed of training which is far superior to other conventional learning methods.\nHowever, there are also some shortcomings that the ELM algorithm cannot over-\ncome. Random choosing of input weights and biases easily causes the so called hidden layer output matrix not full column rank. This sometimes makes the linear system that is used to train output weights (linking the hidden layer to the output layer) unsolvable. It also lowers the predicting accuracy. Bartlett [1] pointed out that for feedforward neural networks the smaller the norm of weights and training error are, the better generalization performance the networks tend to have. Therefore, it is necessary to develop a more effective learning method that can overcome this shortcoming and approximate as fast as the ELM algorithm.\nThis paper tries to design such a learning machine. To achieve the ends, we first properly train the input weights and biases such that the hidden layer output matrix full column rank. Then, a new learning algorithm called effective extreme learning machine (EELM) is proposed, where the strictly diagonally dominant criterion for determining a matrix nonsingular is used to choose the proper input weights and biases. In the first phase of the EELM algorithm the samples are sorted by affine transformation. Due to the assumption of the constructive algorithm, the activation function of the network used in our algorithm should be a Gaussian radial basis-type function. With sorted samples, the Gaussian radial basis-type activation function helps distinguish diagonal elements of the matrix from other non-diagonal ones such that the diagonal elements is larger than the sum of all absolutes of non-diagonal elements. Having chosen input weights and biases properly such that the hidden layer output matrix is full column rank, simple generalized inverse operation gives the output weights.\nThe difference between the new proposed EELM algorithm and the ELM algorithm lies in the training of input weights and biases. And time spent in the first phase of EELM is very short compared to the second step. So EELM is actually faster than ELM. And EELM algorithm also possesses the qualities of the ELM algorithm including easy implementing, good generalization performance. Moreover, the new algorithm improved the effectiveness of learning: the full column rank property of the matrix makes the orthogonal projection method, a fast algorithm for solving generalized inverse, available. So, it is called effective extreme learning machine.\nThis paper is organized as follows. Section 2 gives two theorems that show the two steps in the first phase (training input weights and biases) of EELM algorithm\nare strictly correct and reasonable theoretically. The constructive proofs in the theorems actually provide the learning method. Section 3 proposes the new EELM learning algorithm for SLFNs. In Section 4, the complexity of the algorithm is given and performance is measured. Section 5 consists of the discussions and conclusions."
    }, {
      "heading" : "2 Linear inverse problems and Regularization Model of",
      "text" : "Neural Network\nFor n arbitrary distinct samples {(Xi, ti)|i = 1, 2, . . . , n} whereXi = (xi1, xi2, . . . , xid) T ∈ R d and ti = (ti1, ti2, . . . , tim) ∈ R m, standard SLFNs with N nodes and activation function g are mathematically modeled as\nGN (X) = N ∑\ni=1\nβig(Wi ·X + bi),\nhere βi = (βi1, βi2, . . . , βiN ) T is the output weight vector connecting the i-th nodes and output nodes, Wi = (wi1, wi2, . . . , wid) T is the input weight vector connecting the i-th hidden nodes and the input nodes, and the bi is the threshold of the i-th hidden node. Approximating the samples with zero error means the proper selection of βi, Wi and bi such that\n‖GN (Xj)− tj‖ = 0 (j = 1, 2, . . . , n)\nor\nGN (Xj) = tj (j = 1, 2, . . . , n), (1)\nthat is,\nHβ = T,\nhere\nH = H(w1, w2, . . . , wN , b1, b2, . . . , bN ,X1,X2, . . . ,Xn)\n= (hij)n×n =\n\n   \ng(W1 ·X1 + b1) · · · g(WN ·X1 + bN ) ... · · · ... g(W1 ·Xn + b1) · · · g(WN ·Xn + bN )\n\n    . (2)\nAs named in Huang et al. [13, 12], H is called the hidden layer output matrix of the neural networks.\nIn ELM algorithm, the choice of βi and Wi is random, which accelerates the rate of learning. Nevertheless, the randomly selection sometimes produces nonsingular hidden layer output matrix which causes no solution of the linear system (1). To overcome the shortcoming, an extra phase of training the input weights and biases within acceptable steps to keep H full column rank should be added to the algorithm.\nFirst, we introduce the definition of inverse lexicographical order (or inverse dictio-\nnary order) in Rd.\nDefinition 2.1. Suppose X1,X2 ∈ R d where Xi = (xi1, xi2, . . . , xid) ∈ R d (i = 1, 2) are defined as X1 <d X2 if and only if there exists j0 ∈ {1, 2, . . . , d} such that x1j0 < x2j0 and x1j = x2j for j = j0 + 1, . . . , d. j0 is called different attribute and denoted by da(X1,X2) or da(1, 2) for convenience if no confusion is produced.\nWith the concept of inverse lexicographical order, we obtain the follow theorem that gives a constructive method for sorting high-dimensional vectors via an affine transformation.\nTheorem 2.2. For n distinct vectors X1 <d X2 <d · · · <d Xn ∈ R d (d ≥ 2) and\nXi = (xi1, xi2, . . . , xid) T such that ∑d j=1 x 2 ij > 0 for each i = 1, 2, . . . , n. Calculate W ∈ Rd as follows,\nw1j = 1\nmax i=1,2,...,n\n{|xij|} > 0 (j = 1, 2, . . . , d),\nx1ij = w 1 jxij ∈ [−1, 1] (i = 1, 2, . . . , n, j = 1, 2, . . . , d),\ny1ij = ∣ ∣x1i+1,j − x 1 ij ∣ ∣ (i = 1, 2, . . . , n− 1, j = 1, 2, . . . , d),\nδ = log10 d+ log10 2,\nnj =\n[\n− log10\n(\nmin i=1,2,...,n\n{ y1ij }\n)]\n+ δ (j = 1, 2, . . . , d),\nw2j = w 1 j10\n∑j p=1 np (j = 1, 2, . . . , d),\nW = ( w21, w 2 2 , . . . , w 2 d ) .\nThen follows\nW ·X1 < W ·X2 < · · · < W ·Xn.\nProof. For each fixed i = 1, 2, . . . , n − 1, set k0 = da(i, i + 1) which means by\nDefinition 2.1 that xik0 < xi+1,k0 and xij = xi+1,j (j = k0 + 1, . . . , d). Then\nW ·Xi+1 −W ·Xi\n=\nk0−1 ∑\nk=1\n(\nx1i+1,k − x 1 ik\n) 10 ∑k p=1 np + (\nx1i+1,k0 − x 1 ik0\n) 10 ∑k0 p=1 np ,\nwhere by definition x1i,k, x 1 i+1,k ∈ [−1, 1] (k = 1, 2, . . . , d) and x 1 i,k0 < x1i+1,k0 . Noticing\n10 ∑k p=1 np = 10 ∑k0−1 p=1 np\n\n\nk0−1 ∏\np=k+1\n10np\n\n\n−1\n≤ 10\n∑k0−1 p=1 np\nd(k0−1)−k (k = 1, 2, . . . , k0 − 1)\nand\n10nk0 ≥ 10 − log10\n(\nmin i=1,2,...,n\n{\ny1 ik0\n}\n)\n+δ =\n2d\nmin i=1,2,...,n\n{\ny1ik0\n} .\nTherefore,\nW ·Xi+1 −W ·Xi\n≥ −2 k0−1 ∑\nk=1\n10 ∑k p=1 np + (\ny1ik010 nk0\n) 10 ∑k0−1 p=1 np\n≥ −(2d) k0−1 ∑\nk=1\nd−k10 ∑k0−1 p=1 np + (2d) 10 ∑k0−1 p=1 np\n= (2d)10 ∑k0−1 p=1 np\n(\n1− 1− d−(k0−1)\nd− 1\n)\n> 0.\nThis completes the proof of Theorem 2.2.\nRemark 1. For the case of d = 1, one needs not to follow the steps of Theorem 2.2 when selecting of W . In fact, in the case of d = 1, the sort operation of the samples X1,X2, . . . ,Xn in the inverse lexicographical order via affine transformation can be skipped over. In addition, we don’t need the prior sorting of the samples X1,X2, . . . ,Xn in the inverse lexicographical order. This is because the computation of W is independent of the order of the samples. Therefore, one only has to sort\nW ·X1,W ·X2, . . . ,W ·Xn and change the order of t1, t2, . . . , tn correspondingly in the linear system.\nHaving calculated the W and given an order to W ·X1,W ·X2, . . . ,W ·Xn, we are able to select input weights and biases, which ensures non-singularity of the hidden layer output matrix of the neural networks. This is stated in the following theorem. It is noteworthy that the activation function should satisfy some assumptions.\nTheorem 2.3. Let g(x) be a positive finite function on R such that lim x→−∞ g(x) =\nlim x→+∞ g(x) = 0 and g(x) is not identically equal to 0. Given n distinct samples X1 <d X2 < · · · <d Xn ∈ R d (d ≥ 2) and Xi = (xi1, xi2, . . . , xid) T such that d ∑\nj=1 x2ij > 0 for\nany i = 1, 2, . . . , n, there exist input weights Wi ∈ R d and biases bi ∈ R (i = 1, 2, . . . , n) such that the square matrix\nH = (hij)n×n =\n\n      \ng(W1 ·X1 + b1) g(W2 ·X1 + b2) . . . g(Wn ·X1 + bn) g(W1 ·X2 + b1) g(W2 ·X2 + b2) . . . g(Wn ·X2 + bn) ... ... . . . ... g(W1 ·Xn + b1) g(W2 ·Xn + b2) . . . g(Wn ·Xn + bn)\n\n      \nis nonsingular.\nProof. The proof of the theorem is actually the process of selection of input weights\nand biases. By assumptions that g(x) is a finite function on R and lim x→−∞ g(x) =\nlim x→+∞ g(x) = 0, g(x) has a maximum on R. Set M = g(x0) = max {g(x)|x ∈ R} (x0 ∈ R). For ε0 = M/n > 0, there exists a > 0 such that g(x) < M/n 2 for |x| > a and x0 ∈ (−a, a). Now choose W = 1 if d = 1 and W by Theorem 2.2 if d ≥ 2, which implies by assumptions and Theorem 2.2 that\nW ·X1 < W ·X2 < · · · < W ·Xn. (3)\nThen, select Wi and bi (i = 1, 2, . . . , n) as follows.\ndist = max{a− x0, a+ x0}, (4)\nki = 2dist\nmin {W ·Xi+1 −W ·Xi,W ·Xi −W ·Xi−1} (i = 2, 3, . . . , n− 1), (5)\nWi = kiW (i = 2, 3, . . . , n − 1), W1 = W2, Wn = Wn−1,\nbi = x0 −Wi ·Xi (i = 1, 2, . . . , n). (6)\nOne thus obtains by (6) that for i = 1, 2, . . . , n,\ng(Wi ·Xi + bi) = M.\nBy (3), (4) and (5), there holds for i = 1, 2, . . . , n,\n|(Wi ·Xj + bi)− (Wi ·Xi + bi)| ≥ ki |W ·Xj −W ·Xi| ≥ 2dist (j = 1, 2, . . . , n, j 6= i),\nand\nWi ·X1 + bi < Wi ·X2 + bi < · · ·Wi ·Xi−1 + bi < x0 − a\n< Wi ·Xi + bi = x0 < x0 + a < Wi ·Xi+1 + bi < · · · < Wi ·Xn + bi.\nTherefore,\ng(Wi ·Xj + bi) < M\nn2 .\nHence,\nhii > ∑\n(k,i) 6=(i,i) k,j=1,2,...,n\n|hkj|,\nthus, H is strictly diagonally dominant. So H is nonsingular. This completes the proof of Theorem 2.3.\nRemark 2. We summarize the steps of selection of input weights and biases as\nfollows. First, choose a ∈ R such that\ng(x) < M\nn2 (|x| > a, x0 ∈ (−a, a)),\nhere,\nM = max {g(x)|x ∈ R} = g(x0) (x0 ∈ R).\nThen, calculate as follows.\ndist = max{a− x0, a+ x0},\nki = 2dist\nmin {W ·Xi+1 −W ·Xi,W ·Xi −W ·Xi−1} (i = 2, 3, . . . , n− 1),\nWi = kiW (i = 2, 3, . . . , n− 1), W1 = W2, Wn = Wn−1,\nbi = x0 −Wi ·Xi (i = 1, 2, . . . , n).\nRemark 3. Actually, there exist activation functions meeting the conditions of Theorem 2.3. One kind of such activation functions is functions with one peak such as Gaussian radial basis function g(x) = e−x 2 .\nRemark 4. In the case that the number of rows m of H is larger than that of columns, W and b are calculated based on the square matrix which consists of the m forward rows of H."
    }, {
      "heading" : "3 Extreme learning machine using iterative method",
      "text" : "Based upon Theorem 2.2 and Theorem 2.3, a more effective method for training SLFNs is proposed in this section."
    }, {
      "heading" : "3.1 Features of extreme learning machine (ELM) algorithm",
      "text" : "The ELM algorithm proposed by Huang et al. can be summarized as follows.\nAlgorithm ELM: Given a training set N = {(Xi, ti)| ∈ R d, ti ∈ R, i = 1, 2, . . . , n}\nand activation function g, hidden node number n0.\nStep 1: Randomly assign input weight Wi and bias bi (i = 1, 2, . . . , n0). Step 2: Calculate the hidden layer output matrix H. Step 3: Calculate the output weight β by β = H†T , here H† is the Moore-\nPenrose generalized inverse of H (see [21] and [25] for further details) and T = (t1, t2, . . . , tn) T .\nThe ELM is proved in practice an extremely fast algorithm. This is because it randomly chooses the input weights Wi and biases bi of the SLFNs instead of selection. However, this big advantage makes the algorithm less effective sometimes. As mentioned in Section 1, the random choice of input weights and biases is likely to produce an unexpected result, that is, the hidden layer output matrix H is not full column rank or singular (see (2)), which causes two difficulties. First, it enlarges training error of the samples, which to some extent lowers the prediction accuracy as we can see in the following sections. Besides, the ELM cannot use the orthogonal projection method to\ncalculate Moore-Penrose generalized inverse of H due to the singularity of H, instead, it prefers singular value decomposition (SVD) which wastes more time."
    }, {
      "heading" : "3.2 Improvement for the effectiveness of extreme learning machine",
      "text" : "According to Theorem 2.2 and Theorem 2.3, one can summarize the new extreme learning machine for SLFNs as follows. We call the new algorithm effective extreme learning machine. In the algorithm, Gaussian radial basis activation function is used.\nAlgorithm EELM: Given a training data set N = {(X∗i , t ∗ i )|X ∗ i ∈ R d, t∗i ∈ R, i = 1, 2, . . . , n}, activation function of radial basis function g(x) = e−x 2 and hidden node number n0.\nStep 1: Select weights Wi and bias bi (i = 1, 2, . . . , n0). SortW ·X∗1 ,W ·X ∗ 2 , . . . ,W ·X ∗ n0 in order thatW ·X∗i1 < W ·X ∗ i2 < · · · < W ·X∗in0 (ij 6= ik for j 6= k, j, k = 1, 2, . . . , n0 and ij = 1, 2, . . . , n0) are satisfied, then correspondingly change the order of the forward n0 samples (X ∗ i , t ∗ i ) (i = 1, 2, . . . , n0). And denote the sorted data by (Xi, ti) (i = 1, 2, . . . , n) and Xi = (xi1, xi2, . . . , xid) (i = 1, 2, . . . , n). For j = 1, 2, . . . , d, make following calculations.\nw1j = 1\nmax i=1,2,...,n0\n{|xij|} > 0,\nx1ij = w 1 jxij ∈ [−1, 1],\ny1ij = ∣ ∣x1i+1,j − x 1 ij ∣ ∣ (i = 1, 2, . . . , n0 − 1),\nδ = log10 d+ log10 2,\nnj =\n[\n− log10\n(\nmin i=1,2,...,n\n{ y1ij }\n)]\n+ δ,\nw2j = w 1 j10\n∑j p=1 np .\nSet\nW = ( w21, w 2 2 , . . . , w 2 d ) .\nLet\nM = 1, x0 = 0, a = max { √ |2 ln(n0)|, 1 } + 1\nsuch that M = max {g(x)|x ∈ R} = g(x0) (x0 ∈ R) and g(x) < M/n 2 0 (|x| > a, x0 = 0 ∈ (−a, a)). Then,\ndist = max{a− x0, a+ x0},\nki = 2dist\nmin {W ·Xi+1 −W ·Xi,W ·Xi −W ·Xi−1} (i = 2, 3, . . . , n0 − 1),\nWi = kiW (i = 2, 3, . . . , n0 − 1), W1 = W2, Wn0 = Wn0−1,\nbi = x0 −Wi ·Xi (i = 1, 2, . . . , n0).\nStep 2: Calculate output weights β = (β1, β2, . . . , βn0) (i = 1, 2, . . . , n0). Let T = (t1, t2, . . . , tn) T and\nH =\n\n             \ng(W1 ·X1 + b1) g(W2 ·X1 + b2) . . . g(Wn0 ·X1 + bn0) ... ... . . . ...\ng(W1 ·Xn0 + b1) g(W2 ·Xn0 + b2) . . . g(Wn0 ·Xn0 + bn0)\ng(W1 ·Xn0+1 + b1) g(W2 ·Xn0+1 + b2) . . . g(Wn0 ·Xn0+1 + bn0) ... ... . . . ...\ng(W1 ·Xn + b1) g(W2 ·Xn + b2) . . . g(Wn0 ·Xn + bn0)\n\n             \nn×n0\n.\nThen\nβ = H†T = (HTH)−1HTT.\nRemark 5. As pointed out in Remark 3, the activation function in the EELM algorithm for SLFNs should be chosen to satisfy the assumption of Theorem 2.3. By Theorem 2.3, if the samples possess the properties that they are distinct and d ∑\nj=1 x2ij > 0\n(this is actually almost surely), then the hidden layer output matrix H is full column rank. This ensures that HTH is nonsingular and thus the fast orthogonal project method can be used in computation of Moore-Penrose generalized inverse. Moreover, when n = n0, that is, H is an invertible square matrix.\nRemark 6. In accordance with Theorem 2.3, X1 <d X2 <d · · · <d Xn0 , here the (Xi, ti) (i = 1, 2, . . . , n) denotes the sorted samples as in the algorithm above. To avoid the random error, the n0 samples being used to train input weights and biases can be randomly chosen from the original n samples."
    }, {
      "heading" : "4 Complexity and performance",
      "text" : "The proposed EELM spends more time on training samples than ELM but in fact the extra time spent on selecting input weights and biases is O(n0d). Compared with the second phase of the algorithm that calculates the output weights, it can be viewed as a constant.\nIn the rest part of this section, the performance of the proposed EELM learning algorithm is measured compared with the ELM algorithm. The simulations for ELM and EELM algorithms are carried out in the Matlab 7.0 environment running in Intel Celeron 743 CPU with the speed of 1.30 GHz and in Intel Core 2 Duo CPU. The activation function used in both algorithm is Gaussian radial basis function g(x) = e−x 2 ."
    }, {
      "heading" : "4.1 Benchmarking with a regression problem: approximation of ‘SinC’",
      "text" : "function with noise\nFirst of all, we use the ‘Sinc’ function to measure the performance of the two algorithms. The target function is as follows.\ny = f(x) =\n \n\nsin(x)/x x 6= 0, 1 x = 0.\nA training set (Xi, ti) and testing set (Xi, ti) with 200 samples are respectively created, where Xi in the training data is distributed in [−10, 10] with uniform step length. The Xi in the testing data is chosen randomly in the standard normalized distribution in [−30, 30]. The reason why the range ([−30, 30]) of testing data is longer than that of training data is because an obvious way to assess the quality of the learned model is to see on how long term the predictions given by the model are accurate. The experiment is carried out on these data as follows. There are 200 hidden nodes assigned for both the ELM and the EELM algorithms. 50 trials have been conducted for the ELM algorithm to eliminate the random error and the results shown are the average results. Results shown in Table 1 include training time, testing time, training accuracy, testing accuracy and the number of nodes of both algorithms.\nFig. 2. Outputs of EELM learning algorithm\nIt can be seen from Table 1 that the EELM algorithm spent 0.0624s CPU time obtaining testing accuracy 0.1595 with zero training error whereas it takes 0.0870s CPU time for the ELM algorithm to reach a higher error 5.6642 with training error of 3.1431× 10−6. Fig. 2 shows the expected and approximated results of EELM algorithm and Fig. 1 shows the true and the approximated results of ELM algorithm. The results show that our EELM algorithm can not only approximate the training data with zero error but also have a long term prediction accuracy. And the time consumption is not more than the ELM algorithm. Though the ELM algorithm has good performance in the interval [−10, 10], its long term prediction accuracy is not very satisfactory."
    }, {
      "heading" : "4.2 Benchmarking with real-world applications",
      "text" : "In this section, we conduct the performance comparison of the proposed EELM and the ELM algorithms for 5 real problems: 3 classification tasks including Diabetes, Glass Identification (Glass ID), Statlog (Landsat Satellite), and 2 regression tasks including Housing and Slump (Concrete Slump). All the data sets are from UCI repository of machine learning databases [2]. The speculation of each database is shown in the\nTable 2. For the databases that have only one data table, as conducted in [9, 22, 23, 26], 75% and 25% of samples in the problem are randomly chosen for training and testing respectively at each trial.\nFifty trials were conducted for the two algorithms and the results are reported in Table 3, Table 4 and Table 5, which show that in our simulation, generally speaking, ELM can reach higher testing rate for mid and large size classification problems than EELM and for the small size ones, EELM can achieve a higher rate than ELM. In the regression cases, EELM has a better robustness property than ELM. Fig. 3, Fig. 4 and Fig. 5 show that EELM is more steady than ELM for regression cases.\nIn the Diabetes case, the performance of both ELM and EELM including training accuracy, testing accuracy and learning time of two algorithms for 25 different SFLNs with 20 to 500 nodes were measured and the results are reported in Fig. 6, Fig. 7 and\nFig. 8, which show that in the simulation of the mid size classification application, ELM can reach a higher testing rate than EELM with same number of nodes. Whereas, the time spent by ELM increases much faster than that spent by EELM with the increasing of the number of nodes."
    }, {
      "heading" : "5 Discussions and conclusions",
      "text" : "This paper proposed a simple and effective algorithm for single-hidden layer feedforward neural networks (SLFNs) called effective extreme learning machine (EELM) in an attempt to overcome the shortcomings of the extreme learning machine (ELM). There are several interesting features of the proposed EELM algorithm in comparison with\nthe ELM algorithm:\n(1) The learning speed of EELM is generally faster than ELM. The main differ-\nence between EELM and ELM algorithms lie in the selection of input weights and biases. The ELM algorithm chooses them randomly which consumes little time. Our EELM algorithm selects the input weights and biases properly, which also consumes short time compared with the training time of output weights.\n(2) The proposed EELM by making proper selection of input weights and biases\nof the neural networks avoids the risk of yielding singular or not full column rank hidden layer output matrix H. This allows for use of a faster method which can calculate the Moore-Penrose generalized inverse of H much more rapidly.\n(3) Another impressive feature the EELM possesses is that it has a longer pre-\ndiction term with acceptable accuracy than the ELM algorithm. Also, EELM has better robustness property than ELM. In particular, in the regression, the performance of ELM is sometimes poor. But EELM remains steady and has a good performance. It is worthwhile pointing that in our algorithm in order to sort the samples by affine transformation X 7→ W ·X+ b, we adopt the method of decimal numeral system. However, when high-dimensional data is come across and the range of the deviation between samples |xi+1,j−xij| (the symbols here have the same meanings as in Section 3) is very big, the weights w2j = w 1 j10 ∑j p=1 np become so large that the computer will treat it as infinity. To resolve the problem, one can use algorithms of large number operation. Whether there exist better methods to sort high-dimensional data effectively and simply by an affine transformation keeps open.\nFinally, the proposed EELM algorithm is effective when the activation functions satisfy the assumptions in Theorem 2.3. Gaussian radial basis function belongs to this kind of functions. Nonetheless, the sigmoidal function is not included. This poses a new problem of designing algorithms using other kinds of activation functions, which are as effective and fast as EELM."
    } ],
    "references" : [ {
      "title" : "The sample complexity of pattern classification with neural networks: the size of the weights is more important than the size of the network",
      "author" : [ "P.L. Bartlett" ],
      "venue" : "IEEE Trans. Information Theory 44 (2) ",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "UCI repository of machine learning databases",
      "author" : [ "C. Blake", "C. Merz" ],
      "venue" : "in: http://www.ics.uci.edu/ ̃mlearn/MLRepository.html, Department of Information and Computer Sciences, University of California, Irvine, USA",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "The estimate for approximation error of neural networks: a constructive approach",
      "author" : [ "F.L. Cao", "T.F. Xie", "Z.B. Xu" ],
      "venue" : "Neurocomputing 71 ",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Approximation problems in system identification with neural networks",
      "author" : [ "T.P. Chen" ],
      "venue" : "Science in China (series A) 37 (4) ",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "Approximation capability to functions of several variables",
      "author" : [ "T.P. Chen", "H. Chen" ],
      "venue" : "nonlinear functionals, and operators by radial basis function neural networks, IEEE Trans. Neural Networks, 6 ",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "Universal approximation to nonlinear operators by neural networks with arbitrary activation functions and its application to dynamical systems",
      "author" : [ "T.P. Chen", "H. Chen" ],
      "venue" : "IEEE Trans. Neural Networks 6 ",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "Approximation by superpositions of sigmoidal function",
      "author" : [ "G. Cybenko" ],
      "venue" : "Math. Control Signals Syst. 2 ",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 1989
    }, {
      "title" : "Q",
      "author" : [ "G. Feng", "G.B. Huang" ],
      "venue" : "Lin R. Gay, Error minimized extreme learning machine with growth of hidden nodes and incremental learning, IEEE Trans. Neural Networks 20 (8) ",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Experiments with a new boosting algorithm",
      "author" : [ "Y. Freund", "R.E. Schapire" ],
      "venue" : "in: International Conference on Machine Learning",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "On the approximate realization of continuous mappings by neural networks",
      "author" : [ "K.I. Funahashi" ],
      "venue" : "Neural Networks 2 ",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 1989
    }, {
      "title" : "Approximation capabilities of multilayer feedforward networks",
      "author" : [ "K. Hornik" ],
      "venue" : "Neural Networks 4 ",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 1991
    }, {
      "title" : "Learning capability and storage capacity of two-hidden-layer feedforward networks",
      "author" : [ "G.B. Huang" ],
      "venue" : "IEEE Trans. Neural Networks 14 (2) ",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Upper bounds on the number of hidden neurons in feedforward networks with arbitrary bounded nonlinear activation functions",
      "author" : [ "G.B. Huang", "H.A. Babri" ],
      "venue" : "IEEE Trans. Neural Networks 9 (1) ",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Convex incremental extreme learning machine",
      "author" : [ "G.B. Huang", "L. Chen" ],
      "venue" : "Neurocomputing 70 ",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Enhanced random search based incremental extreme learning machine",
      "author" : [ "G.B. Huang", "L. Chen" ],
      "venue" : "Neurocomputing 71 ",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Universal approximation using incremental constructive feedforward networks with random hidden nodes",
      "author" : [ "G.B. Huang", "L. Chen", "C.K. Siew" ],
      "venue" : "IEEE Trans. Neural Networks 17 (4) ",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Optimization method based extreme learning machine for classification",
      "author" : [ "G.B. Huang", "X. Ding", "H. Zhou" ],
      "venue" : "Neurocomputing, In Press, Accepted Manuscript",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Extreme learning machine: Theory and applications",
      "author" : [ "G.B. Huang", "Q.Y. Zhu", "C.K. Siew" ],
      "venue" : "Neurocomputing 70 ",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Multilayer feedforward networks with a nonpolynomial activation function can approximat any function",
      "author" : [ "M. Leshno", "V.Y. Lin", "A. Pinkus", "S. Schocken" ],
      "venue" : "Neural Networks 6 ",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 1993
    }, {
      "title" : "Fully complex extreme learning machine",
      "author" : [ "M.B. Li", "G.B. Huang", "P. Saratchandran", "N. Sundararajan" ],
      "venue" : "Neurocomputing 68 ",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Generalized Inverse of Matrices and its Applications",
      "author" : [ "C.R. Rao", "S.K. Mitra" ],
      "venue" : "Wiley, New York",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 1972
    }, {
      "title" : "An improvement of AdaBoost to avoid overfitting",
      "author" : [ "G. Rätsch", "T. Onoda", "K.R. Müller" ],
      "venue" : "in: Proceedings of the Fifth International Conference on Neural Information Processing ",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "A new incremental method for function approximation using feed-forward neural networks",
      "author" : [ "E. Romero", "R. Alquézar" ],
      "venue" : "in: Proceedings of INNS-IEEE International Joint Conference on Neural Networks ",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Online sequential fuzzy extreme learning machine for function approximation and classification problems",
      "author" : [ "H.J. Rong", "G.B. Huang", "N. Sundararajan", "P. Saratchandran" ],
      "venue" : "IEEE Trans. Systems, Man, and Cybernetics-Part B: Cybernetics 39 (4) ",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Matrices: Theory and Applications",
      "author" : [ "D. Serre" ],
      "venue" : "Springer, New York",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Heterogeneous radial basis function networks",
      "author" : [ "D.R. Wilson", "T.R. Martinez" ],
      "venue" : "in: Proceedings of the International Conference on Neural Networks (ICNN",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 1996
    }, {
      "title" : "Multicategory classification using an extreme learning machine for microarray gene expression cancer diagnosis",
      "author" : [ "R. Zhang", "G.B. Huang", "N. Sundararajan", "P. Saratchandran" ],
      "venue" : "IEEE/ACM Trans. Computational Biology and Bioinformatics 4 (3) ",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Evolutionary extreme learning machine",
      "author" : [ "Q.Y. Zhu", "A.K. Qin", "P.N. Suganthan", "G.B. Huang" ],
      "venue" : "Pattern Recognition 38 (10) ",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2005
    } ],
    "referenceMentions" : [ {
      "referenceID" : 11,
      "context" : "We refer the reader to [12], [14]-[18], [20], [24], [27] and [28].",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 13,
      "context" : "We refer the reader to [12], [14]-[18], [20], [24], [27] and [28].",
      "startOffset" : 29,
      "endOffset" : 33
    }, {
      "referenceID" : 17,
      "context" : "We refer the reader to [12], [14]-[18], [20], [24], [27] and [28].",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 19,
      "context" : "We refer the reader to [12], [14]-[18], [20], [24], [27] and [28].",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 23,
      "context" : "We refer the reader to [12], [14]-[18], [20], [24], [27] and [28].",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 26,
      "context" : "We refer the reader to [12], [14]-[18], [20], [24], [27] and [28].",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 27,
      "context" : "We refer the reader to [12], [14]-[18], [20], [24], [27] and [28].",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 6,
      "context" : "In 1989, Cybenko [7] and Funahashi [10] proved that any continuous functions can be approximated on a compact set with uniform topology by an SLFN with any continuous, sigmoidal activation function, which made a breakthrough in the artificial neural network field.",
      "startOffset" : 17,
      "endOffset" : 20
    }, {
      "referenceID" : 9,
      "context" : "In 1989, Cybenko [7] and Funahashi [10] proved that any continuous functions can be approximated on a compact set with uniform topology by an SLFN with any continuous, sigmoidal activation function, which made a breakthrough in the artificial neural network field.",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 18,
      "context" : "Leshno [19] improved the results of Hornik [11] and proved that any continuous functions could be approximated by feedforward networks with a nonpolynomial activation function.",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 10,
      "context" : "Leshno [19] improved the results of Hornik [11] and proved that any continuous functions could be approximated by feedforward networks with a nonpolynomial activation function.",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 3,
      "context" : "Furthermore, some deep and systematic studies on the condition of activation function can be found in [4, 5, 6].",
      "startOffset" : 102,
      "endOffset" : 111
    }, {
      "referenceID" : 4,
      "context" : "Furthermore, some deep and systematic studies on the condition of activation function can be found in [4, 5, 6].",
      "startOffset" : 102,
      "endOffset" : 111
    }, {
      "referenceID" : 5,
      "context" : "Furthermore, some deep and systematic studies on the condition of activation function can be found in [4, 5, 6].",
      "startOffset" : 102,
      "endOffset" : 111
    }, {
      "referenceID" : 2,
      "context" : "[3] constructively gave the estimation of upper bounds of approximation for continuous functions by SLFNs with the bounded, strictly monotone and odd activation function, which means that the neural networks can be constructed without training as long as the samples are given.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 12,
      "context" : "In practical applications, for function approximation in a finite training set, Huang and Babri [13] showed that an SLFN with at most N hidden nodes and with almost any nonlinear activation function can exactly learn N distinct samples.",
      "startOffset" : 96,
      "endOffset" : 100
    }, {
      "referenceID" : 0,
      "context" : "Bartlett [1] pointed out that for feedforward neural networks the smaller the norm of weights and training error are, the better generalization performance the networks tend to have.",
      "startOffset" : 9,
      "endOffset" : 12
    }, {
      "referenceID" : 12,
      "context" : "[13, 12], H is called the hidden layer output matrix of the neural networks.",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 11,
      "context" : "[13, 12], H is called the hidden layer output matrix of the neural networks.",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 20,
      "context" : "Step 3: Calculate the output weight β by β = HT , here H is the MoorePenrose generalized inverse of H (see [21] and [25] for further details) and T = (t1, t2, .",
      "startOffset" : 107,
      "endOffset" : 111
    }, {
      "referenceID" : 24,
      "context" : "Step 3: Calculate the output weight β by β = HT , here H is the MoorePenrose generalized inverse of H (see [21] and [25] for further details) and T = (t1, t2, .",
      "startOffset" : 116,
      "endOffset" : 120
    }, {
      "referenceID" : 1,
      "context" : "All the data sets are from UCI repository of machine learning databases [2].",
      "startOffset" : 72,
      "endOffset" : 75
    }, {
      "referenceID" : 8,
      "context" : "For the databases that have only one data table, as conducted in [9, 22, 23, 26], 75% and 25% of samples in the problem are randomly chosen for training and testing respectively at each trial.",
      "startOffset" : 65,
      "endOffset" : 80
    }, {
      "referenceID" : 21,
      "context" : "For the databases that have only one data table, as conducted in [9, 22, 23, 26], 75% and 25% of samples in the problem are randomly chosen for training and testing respectively at each trial.",
      "startOffset" : 65,
      "endOffset" : 80
    }, {
      "referenceID" : 22,
      "context" : "For the databases that have only one data table, as conducted in [9, 22, 23, 26], 75% and 25% of samples in the problem are randomly chosen for training and testing respectively at each trial.",
      "startOffset" : 65,
      "endOffset" : 80
    }, {
      "referenceID" : 25,
      "context" : "For the databases that have only one data table, as conducted in [9, 22, 23, 26], 75% and 25% of samples in the problem are randomly chosen for training and testing respectively at each trial.",
      "startOffset" : 65,
      "endOffset" : 80
    } ],
    "year" : 2014,
    "abstractText" : "Extreme Learning Machine (ELM), proposed by Huang et al., has been shown a promising learning algorithm for single-hidden layer feedforward neural networks (SLFNs). Nevertheless, because of the random choice of input weights and biases, the ELM algorithm sometimes makes the hidden layer output matrix H of SLFN not full column rank, which lowers the effectiveness of ELM. This paper discusses the effectiveness of ELM and proposes an improved algorithm called EELM that makes a proper selection of the input weights and bias before calculating the output weights, which ensures the full column rank of H in theory. This improves to some extend the learning rate (testing accuracy, prediction accuracy, learning time) and the robustness property of the networks. The experimental results based on both the benchmark function approximation and real-world problems including classification and regression applications show the good performances of EELM.",
    "creator" : "LaTeX with hyperref package"
  }
}