{
  "name" : "1705.07219.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "GAR: An efficient and scalable Graph-based Activity Regularization for semi-supervised learning",
    "authors" : [ "Ozsel Kilinc" ],
    "emails" : [ "ozsel@mail.usf.edu", "iuysal@usf.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "The idea of utilizing an auxiliary unsupervised task to help supervised learning dates back to 90s [Suddarth and Kergosien, 1990]. As an example to one of its numerous achievements, unsupervised pretraining followed by supervised fine-tuning was the first method to succeed in the training of fully connected architectures [Hinton et al., 2006]. Although today it is known that unsupervised pretraining is not a must for successful training, this particular accomplishment played an important role enabling the current deep learning renaissance and has become a canonical example of how a learning representation for one task can be useful for another one [Goodfellow et al., 2016]. There exist a variety of approaches to combine supervised and unsupervised learning in the literature. More specifically, the term semi-supervised is commonly used to describe a particular type of learning for applications in which there exists a large number of observations, where a small subset of them has ground-truth labels. Proposed approaches aim to leverage the unlabeled data to improve the generalization of the learned model and ultimately obtain a better classification performance.\nOne approach is to introduce additional penalization to the training based on the reconstruction of the input through autoencoders or their variants [Ranzato and Szummer, 2008]. Recently, there have been significant improvements in this field following the introduction of a different generative modeling technique which uses the variational equivalent of deep autoencoders integrating stochastic latent variables into the conventional architecture [Kingma and Welling, 2013] [Rezende et al., 2014]. First, [Kingma et al., 2014] have shown that such modifications make generative approaches highly competitive for semi-supervised learning. Later, [Maaløe et al., 2016] further improved the results obtained using variational autoencoders by introducing auxiliary variables increasing the flexibility of the model. Furthermore, [Rasmus et al., 2015] have applied Ladder networks [Valpola, 2014],\nSubmitted to 31st Conference on Neural Information Processing Systems (NIPS 2017). Do not distribute.\nar X\niv :1\n70 5.\n07 21\n9v 1\n[ cs\n.L G\n] 1\n9 M\nay 2\na layer-wise denoising autoencoder with skip connections from the encoder to the decoder, for semi-supervised classification tasks.\nOn the other hand, these recent improvements have also motivated researchers to offer radically novel solutions such as virtual adversarial training [Miyato et al., 2015] motivated by Generative Adversarial Nets [Goodfellow et al., 2014] proposing a new framework corresponding to a minimax two-player game. Alternatively, [Yang et al., 2016] revisited graph-based methods with new perspectives such as invoking the embeddings to predict the context in the graph. Conventional graph-based methods aim to construct a graph propagating the label information from labeled to unlabeled observations and connecting similar ones using a graph Laplacian regularization in which the key assumption is that nearby nodes are likely to have the same labels [Zhu et al., 2003], [Belkin et al., 2006]. However, the requirement for eigenanalysis of the graph Laplacian severely limits the scalability of these approaches. In a different work, [Weston et al., 2012] have shown that the idea of combining an embedding-based regularizer with a supervised learner to perform semi-supervised learning can be generalized to deep neural networks and the resulting models can be trained by stochastic gradient descent. A possible bottleneck with this approach is that the optimization of the unsupervised part of the loss function requires precomputation of the weight matrix specifying the similarity or dissimilarity between the examples whose size grows quadratically with the number of examples. For example, a common approach to computing the similarity matrix is to use the k-nearest neighbor algorithm which is computationally very expensive for a large number of samples. Therefore, it is approximated using sampling techniques.\nIn this paper, we propose a novel framework for semi-supervised learning which can be considered a variant of graph-based approach. This framework can be described as follows.\n• Instead of a graph between examples, we consider a bipartite graph between examples and output classes. To define this graph, we use the predictions of a neural network model initialized by a supervised pretraining which uses a small subset of samples with known labels.\n• This bipartite graph also infers two disjoint graphs: One between the examples and another between the output nodes. We introduce two regularization terms for the graph between the output nodes and during the unsupervised portion of training, the predictions of the network are updated only based on these two regularizers.\n• These terms implicitly provide that the bipartite graph between the examples and the output classes becomes a biregular graph and the inferred graph between the examples becomes a disconnected graph of regular subgraphs. Ultimately, because of this observation, the predictions of the network yield the embeddings that we try to find.\nThe proposed framework is naturally inductive, where predictions can be generalized to never-seenbefore examples. More importantly, it is scalable and it can be applied to datasets regardless of the sample size or the dimensionality of the feature set. Furthermore, the entire framework operationally implements the same feedforward and backpropagation mechanisms of the state of the art deep neural networks as the proposed regularization terms are added to the loss function as naturally as adding standard L1, L2 regularizations [Ng, 2004] and similarly optimized using stochastic gradient descent [Bottou, 2010]."
    }, {
      "heading" : "2 Related work",
      "text" : "Consider a semi-supervised learning problem where out of m observations, corresponding groundtruth labels are only known for a subset of mL examples and the labels of complimentary subset of mU examples are unknown where typically mL mU . Let x1:m and y1:m denote the input feature vectors and the output predictions respectively and t1:mL denote the available output labels. The main objective is to train a classifier f : x→ y using all m observations that is more accurate than another classifier trained using only the labeled examples. Graph-based semi-supervised methods consider a connected graph G = (V, E) of which vertices V correspond to all m examples and edges E are specified by an m × m adjacency matrix A whose entries indicate the similarity between the vertices. There have been many different approaches about the estimation of the adjacency matrix A. [Zhu et al., 2003] derive A according to simple Euclidean distances between the samples while [Weston et al., 2012] precompute A using k-nearest neighbor algorithm. They also suggest\nthat, in case of a sequential data, one can presume consecutive instances are also neighbors on the graph. [Yang et al., 2016], on the other hand, consider the specific case where A is explicitly given and represents additional information. The most important common factor in all these graph-based methods is the fact that A is a fixed matrix throughout the training procedure with the key assumption that nearby samples on G, which is defined by A, are likely to have the same labels. Hence, the generic form of the loss function for these approaches can be written as:\nmL∑ i=1 L ( f(xi), ti ) + λ m∑ i,j=1 U ( f(xi), f(xj), Aij ) (1)\nwhere L(.) is the supervised loss function such as log loss, hinge loss or squared loss, U(.) is the unsupervised regularization (in which a multi-dimensional embedding g(xi) = zi can also be replaced with one-dimensional f(xi)) and λ is a weighting coefficient between supervised and unsupervised metrics of the training L(.) and U(.). One of the commonly employed embedding algorithms in semi-supervised learning is Laplacian Eigenmaps [Belkin and Niyogi, 2003] which describes the distance between the samples in terms of the Laplacian L = D −A, where D is the diagonal degree matrix such that Dii = ∑ j Aij . Then, unsupervised regularization becomes:\nm∑ i,j=1 U ( g(xi), g(xj), Aij ) = m∑ i,j=1 Aij ||g(xi)− g(xj)||2 = Tr(ZTLZ) (2)\nsubject to the balancing constraint ZTDZ = I , where Z = [z1, ...,zm]T . [Zhu et al., 2003] use this regularization together with a nearest neighbor classifier while [Belkin et al., 2006] integrate hinge loss to train an SVM. Both methods impose regularization on labels f(xi). On the other hand, [Weston et al., 2012] employ a margin-based regularization by [Hadsell et al., 2006] such that\nU ( f(xi), f(xj), Aij ) = { ||f(xi)− f(xj)||2 if Aij = 1 max(0, γ − ||f(xi)− f(xj)||2) if Aij = 0\n(3)\nto eliminate the balancing constraints and enable optimization using gradient descent. They also propose to learn multi-dimensional embeddings on neural networks such that g(xi) = f l(xi) = yli, where yli is the output of the l th hidden layer corresponding to the ith sample."
    }, {
      "heading" : "3 Proposed framework",
      "text" : ""
    }, {
      "heading" : "3.1 Bipartite graph approach",
      "text" : "Instead of estimating the adjacency matrix A using an auxiliary algorithm such as nearest neighbor or auxiliary external knowledge, we propose to use the actual predictions of a neural network model initialized by a supervised pretraining using the subset of mL labeled examples.\nSuppose that after pretraining, predictions of the network, B , for all m examples are obtained as an m × n matrix, where n is the number of output classes and Bij is the probability of the ith example belonging to jth class. We observe that these predictions, indeed, define a bipartite graph G∗ = (V∗, E∗) whose vertices V∗ are m examples together with n output nodes. However, V∗ can be divided into two disjoint setsM and N , such that G∗ = (M,N , E∗), whereM is the set of examples, N is the set of output nodes and an edge e ∈ E∗ connects an example m ∈ M with an output node n ∈ N . As there is no lateral connection between m examples and between n output nodes, (m+ n)× (m+ n) adjacency matrix A∗ of graph G∗ has the following form\nA∗ = ( 0m×m Bm×n BTn×m 0n×n ) (4)\nwhere B corresponds tom×n biadjacency matrix of graph G∗ which is by itself unique and sufficient to describe the entire E∗. In graph G∗, the examples are connected with each other by even-length walks through the output nodes whereas the same is true for the output nodes through the samples. In this case, the square of the adjacency matrix A∗ represents the collection of two-walks (walks with two edges) between the vertices. It also implements two disjoint graphs GM = (M, EM) and GN = (N , EN ) such that\nA∗ 2 = ( BBTm×m 0m×n 0n×m B TBn×n ) = ( M 0 0 N ) (5)\nwhere M = BBT and N = BTB are the adjacency matrices specifying edges EM and EN , respectively. Unlike a simple graph G considered by conventional graph-based methods, GM also involves self-loops. However, they have no effect on the graph Laplacian and thus on the embeddings. Hence, one can estimate the adjacency of examples using the predictions of the pretrained neural network, i.e. M = BBT , and then find the embeddings by applying a standard unsupervised objective such as Laplacian Eigenmap minimizing Tr(ZTLMZ) as defined in (2), where LM = DM−M . It is important to note that conventional graph-based algorithms assume a fixed adjacency matrix during loss minimization whereas in the proposed framework, we consider an adaptive adjacency which is updated throughout the unsupervised training process as described in the following section."
    }, {
      "heading" : "3.2 Adaptive adjacency",
      "text" : "As derived adjacency M depends only on B, during the unsupervised training, B needs to be well-constrained to preserve the learned latent embeddings. Otherwise, the idea of updating the adjacency throughout an unsupervised task might be catastrophic and results in offsetting the effects of the supervised pretraining.\nThere are two constraints derived from the natural expectation on the specific form of the B matrix for a classification problem: i) first, a sample is to be assigned to one class with the probability of 1, while remaining n − 1 classes have 0 association probability, and ii) second, for balanced classification tasks, each class is expected to involve approximately the same number of the examples. Let us first consider the perfect case, where B assigns m/n examples to each one of the n classes with the probability of 1. B in this particular form implies that graph G∗ becomes (1,m/n)-biregular since\ndeg(mi) = 1,∀mi ∈M and deg(ni) = m/n,∀ni ∈ N (6)\nSubsequently graph GN turns into a disconnected graph including only self-loops and its adjacency matrix N becomes a scaled identity matrix indicating mutually exclusive and uniform distribution of the examples across the output nodes. Similarly, GM also becomes a disconnected graph including n disjoint subgraphs. Each one of these subgraphs are m/n-regular, where each vertex also has an additional self-loop. Hence, the degree matrix DM becomes a scaled identity matrix and M yields that B represents the optimal embedding. As they all depend only on B, the relationships between G∗, GM and GN are biconditional, which can be written as follows:\nG∗ is (1,m/n)-biregular⇔ GM : argmin ZTDMZ=I Tr(ZTLMZ) = B ⇔ GN : N = m/nI (7)\nDepending on this relation, we propose to apply regularization during the unsupervised task in order to ensure that N becomes the identity matrix. Regularizing N enables us to devise a framework that is natural to the operation of neural networks and compatible with stochastic gradient descent. The first one of the proposed two regularization terms constrains N to be a diagonal matrix, whereas the second one forces it into becoming a scaled identity matrix by equalizing the value of the diagonal entries. The second term ultimately corresponds to constraining B to obtain a uniform distribution of samples across the output classes. Obviously, this condition is not valid for every dataset, but a balancing constraint is required to prevent collapsing onto a subspace of dimension less than n and it is analogous to the constraint of ZTDZ = I in [Belkin and Niyogi, 2003]. As the relation in (7) implies, when N becomes the identity matrix, B automatically represents the optimal embedding found using the inferred adjacency M . In other words, no additional step is necessary to find the embeddings of GM, as B already yields them."
    }, {
      "heading" : "3.3 Activity regularization",
      "text" : "Consider a neural network with L hidden layers where l denotes the individual index for each hidden layer such that l ∈ {1, ..., L}. Let Y (l) denote the output of the nodes at layer l. Y (0) = X is the input and f(X) = f (L)(X) = Y (L) = Y is the output of the entire network. W (l) and b(l) are the weights and biases of layer l, respectively. Then, the feedforward operation of the neural networks can be written as\nY (l) = f (l) ( X ) = h(l) ( Y (l−1)W (l) + b(l) ) (8)\nwhere h(l)(.) is the activation function applied at layer l.\nIn the proposed framework, instead of using the output probabilities of the softmax nodes, we use the activations at their inputs to calculate the regularization. The intuition here is that regularizing linear activations rather than nonlinear probabilities defines an easier optimization task. Since the multiplication of two negative activations yields a positive (false) adjacency in M , we rectify the activations first. Then, B becomes\nB = g ( X ) = max ( 0, ( Y (L−1)W (L) + b(L) )) (9)\nRecall that N is a n × n symmetric matrix such that N := BTB and let v be a 1 × n vector representing the diagonal entries of N such that v := [N11 N22 . . . Nnn]. Then, let us define V as a n× n symmetric matrix such that V := vTv. Then, the two proposed regularization terms can be written as\nAffinity = α ( B ) :=\nn∑ i 6=j Nij\n(n− 1) n∑ i=j Nij\n(10) Balance = β ( B ) :=\nn∑ i 6=j Vij\n(n− 1) n∑ i=j Vij\n(11)\nWhile affinity penalizes the non-zero off-diagonal entries of N , balance attempts to equalize diagonal entries. One might suggest minimizing the off-diagonal entries of N directly without normalizing, however, normalization is required to bring both regularizers within the same range for optimization and ultimately to ease hyperparameter adjustment. Unlike regularizing N to simply become a diagonal matrix, equalizing the diagonal entries is not an objective that we can reach directly by minimizing some entries of N . Hence, we propose to use (11) that takes values between 0 and 1 where 1 represents the case where all diagonal entries of N are equal. Respectively, we propose to minimize the normalized term (10) instead of the direct summation of the off-diagonal entries. However, during the optimization, denominators of these terms increase with the activations which may significantly diminish the effects of both regularizers. To prevent this phenomenon, we apply L2 norm to penalize the overall activity increase. Recall that Frobenius norm for B, ||B||F is analogous to the L2 norm of a vector. Hence, the proposed overall unsupervised regularization loss ultimately becomes\nU ( g ( X )) = U ( B ) = cαα ( B ) + cβ ( 1− β ( B )) + cF ||B||2F (12)"
    }, {
      "heading" : "3.4 Training",
      "text" : "Training of the proposed framework consists of two sequential steps: Supervised pretraining and subsequent unsupervised regularization. We adopt stochastic gradient descent in the mini-batch mode [Bottou, 2010] for optimization of both steps. Indeed, mini-batch mode is required for the unsupervised task since the proposed regularizers implicitly depend on the comparison of the examples with each other. Algorithm 1 below describes the entire training procedure. Pretraining is a typical supervised training task in which mL examples XL = [x1, , ...,xmL ]\nT are introduced to the network with the corresponding ground-truth labels tL = [t1, , ..., tmL ]\nT and the network parameters are updated to minimize the log loss L(.). After the pretraining is completed, this supervised objective is never revisited and the labels tL are never reintroduced to the network in any part of the unsupervised task. Hence, the remaining unsupervised objective is driven only by the proposed regularization loss U(.) defined in (12). The examples XL used in pretraining stage can also be batched together with the upcoming unlabeled examples XU = [xmL+1, , ...,xm]\nT in order to ensure a more stable regularization. As the network is already introduced to them, bL examples randomly chosen from XL can be used as guidance samples for the remaining bU examples of XU in that batch. Such blended batches help the unsupervised task especially when the examples in the dataset have higher variance."
    }, {
      "heading" : "4 Experimental results",
      "text" : "The models have been implemented in Python using Keras [Chollet, 2015] and Theano [Theano Development Team, 2016]. Open source code is available at\nAlgorithm 1: Model training"
    }, {
      "heading" : "Supervised pretraining:",
      "text" : "Input :XL = [x1, , ...,xmL ] T , tL = [t1, , ..., tmL ] T , batch size b repeat{ (X́1, t́1), ..., (X́mL/b, t́mL/b) } ←− (XL, tL) // Shuffle and create batch pairs\nfor i← 1 to mL/b do Take ith pair (X́i, t́i) Take a gradient step for L ( f ( X́i ) , t́i )\nuntil stopping criteria is met return model"
    }, {
      "heading" : "Unsupervised training:",
      "text" : "Input :model, XL = [x1, , ...,xmL ] T , XU = [xmL+1, , ...,xm] T , bL, bU\nrepeat{ X́1, ..., X́mU/bU , } ←−XU // Shuffle and create input batches\nfor i← 1 to mU/bU do Take ith input batch X́i Ẍ ←− random(XL, bL) // Randomly sample bL examples from XL Take a gradient step for U ( g ( [ X́ T i Ẍ T ]T ))\nuntil stopping criteria is met\nhttp://github.com/ozcell/phdwork that can be used to reproduce the experimental results obtained on the three image datasets, MNIST [LeCun et al., 1998], SVHN [Netzer et al., 2011] and NORB [LeCun et al., 2004] most commonly used by previous researchers publishing in the field of semi-supervised learning at NIPS and other similar venues.\nAll experiments have been performed on convolutional neural network (CNN) models. A 6-layer CNN was used for MNIST, whereas for SVHN and NORB, a 9-layer CNN has been used. Coefficients of the proposed regularization term have been chosen as cα = 3, cβ = 1 and cF = 0.000001 in all of the experiments. We used a batch size of 128 for both supervised pretraining and unsupervised regularization steps. For each dataset, the same strategy is applied to decide on the ratio of labeled and unlabeled data in the unsupervised regularization batches, i.e. bL/bU : bL is approximately assigned as one tenth of the number of all labeled examples mL, i.e. bL ≈ mL/10, and then bU is chosen to complement the batch size up to 128, i.e. bU = 128− bL. Each experiment has been repeated for 10 times. For each repetition, to assign XL, mL examples have been chosen randomly but distributed evenly across classes. Also, a validation set of 1000 examples has been chosen randomly among the training set examples to determine the epoch to report the test accuracy as is standard."
    }, {
      "heading" : "4.1 MNIST",
      "text" : "In MNIST, experiments have been performed using 4 different settings for the number of labeled examples, i.e. mL = {100, 600, 1000, 3000} following the literature used for comparative results. The unsupervised regularization batches have been formed choosing bL = 16 and bU = 112.\nFigure 1 visualizes the realization of the graph-based approach described in this paper using real predictions for the MNIST dataset. After the supervised pretraining, in the bipartite graph G∗ (defined by B), most of the examples are connected to multiple output nodes at the same time. In fact, the graph between the examples GM (inferred by M = BBT ) looks like a sea of edges. However, thanks to pretraining, some of these edges are actually quite close to the numerical probability value of 1. Through an easier regularization objective, which is defined on the graph between the output nodes GN (inferred by N = BTB), stronger edges are implicitly propagated in graph GM. In other words, as hypothesized, when N turns into the identity matrix, G∗ closes to be a biregular graph and GM closes to be a disconnected graph of n m/n-regular subgraphs. As implied through the relation defined in (7), we expect B to ultimately become the optimal embedding that we are looking for. Figure 2 presents the t-SNE [Maaten and Hinton, 2008] visualizations of the embedding spaces inferred by B. As clearly observed from this figure, as the unsupervised regularization exploits the unlabeled data, clusters become well-separated and simultaneously the test accuracy increases.\nTable 1 summarizes the semi-supervised test accuracies observed with four different mL settings. Results of a broad range of recent existing solutions are also presented for comparison. These solutions are grouped according to their approaches to semi-supervised learning. While [Kingma et al., 2014], [Rasmus et al., 2015], [Maaløe et al., 2016] employ autoencoder variants, [Miyato et al., 2015] resembles adversarial training, and [Weston et al., 2012] is another graph-based method. To show the baseline of the unsupervised regularization step in our framework, the performance of the network after the supervised pretraining is also given. For MNIST, GAR outperforms all the contemporary methods other than Ladder Network and AGDM, which is the current state of the art."
    }, {
      "heading" : "4.2 SVHN and NORB",
      "text" : "SVHN and NORB datasets are both used frequently in recent literature for semi-supervised classification benchmarks. Either dataset represents a significant jump in difficulty for classification when\ncompared to the MNIST dataset. Table 2 summarizes the semi-supervised test accuracies observed on SVHN and NORB. For SVHN experiments, 1000 labeled examples have been chosen among 73257 training examples. Two experiments are conducted where the SVHN extra set (an additional training set including 531131 more samples) is either omitted from the unsupervised training or not. The same batch ratio has been used in both experiments as bL = 96, bU = 32. On the other hand, for NORB experiments, 2 different settings have been used for the number of labeled examples, i.e. mL = {300, 1000} with the same batch ratio selected as bL = 32, bU = 96. Both results are included for comparative purposes. As clearly observed from this table, the proposed framework has higher accuracy results in every combination of labeled sample size and experimental setup when compared to the most recent and well-cited literature. In either case, the scalability of the framework and the adaptivity of the adjacency matrix lend itself well to a more challenging problem involving two large datasets. GAR strongly outperforms the current state of the art semi-supervised approaches applied to SVHN and NORB."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this paper, we proposed a novel graph-based framework considering adaptive adjacency of the examples, M , which is inferred using the predictions of a neural network model. When wellconstrained, adaptive adjacency approach contributes to improved accuracy results and automatically yields that the predictions of the network become the optimal embedding without requiring any additional step such as applying Laplacian Eigenmaps. We satisfied these constraints by defining a regularization over the adjacency of the output nodes, N , which is also inferred using the predictions of the network. Such regularization helped us to devise an efficient and scalable framework that is natural to the operation of neural networks. Through this framework, we set the state of the art performance within semi-supervised learning on SVHN and NORB datasets.\nPretraining (Baseline) 19.11%(±1.09) 19.11%(±1.09) 17.93%(±1.07) 10.22%(±1.00) Pretraining + GAR 8.67%(±0.65) 6.98%(±0.82) 12.19%(±1.46) 7.10%(±0.57) † This column presents the results obtained when SVHN extra set is omitted from the unsupervised training. Unless otherwise specified, reported results for other approaches are assumed to represent this scenario."
    }, {
      "heading" : "A Appendices",
      "text" : ""
    }, {
      "heading" : "A.1 Datasets used in the experiments",
      "text" : "Table 3 summarizes the properties of the datasets used in the experiments. The following preprocessing steps have been applied to these datasets.\n• MNIST: Images were normalized by 255. • SVHN: We applied sample-wise centering and normalization, i.e. we set each sample mean\nto 0 and divided each input by its standard deviation.\n• NORM: Following [Maaløe et al., 2016], images were downsampled to 32× 32. We added uniform noise between 0 and 1 to each pixel value. First, we normalized the NORB dataset by 256, then applied both sample-wise centering and normalization and feature-wise centering and normalization."
    }, {
      "heading" : "A.2 Models used in the experiments",
      "text" : "Table 4 summarizes all models used in the experiments. Reported MNIST results have been obtained using the model named 6-layer CNN, whereas SVHN results were obtained on 9-layer CNN-2 model and NORB results were obtained on 9-layer CNN model. The results obtained on different models are also presented in the following sections to show the effect of the chosen model to the test accuracy. The ReLU activation function has been used for all models. For both supervised pretraining and unsupervised regularization, models were trained using stochastic gradient descent with following settings: lr = 0.01, decay = 1e−6, momentum = 0.95 with Nesterov updates."
    }, {
      "heading" : "A.3 Effects of hyperparameters",
      "text" : "A.3.1 Effect of the chosen model\nFigure 3 presents the test accuracy curves with respect to the unsupervised training epochs obtained using different models. The proposed unsupervised regularization improves the test accuracy in all models. However, the best case depends on the chosen model specifications.\nA.3.2 Effect of bL/bU ratio and applying dropout during unsupervised regularization\nThe labeled/unlabeled data ratio of unsupervised training batches is the most critical hyperparameter of the proposed regularization. Figure 4 visualizes the effect of this ratio for MNIST and SVHN datasets. These two datasets have different characteristics. MNIST dataset has a lower variance among its samples with respect to SVHN. As a result, even when the labeled examples introduced to the network during the supervised pretraining are not blended to the unsupervised training batches, i.e. bL = 0, this does not affect the performance dramatically. However, for SVHN dataset, reducing\nthe bL proportion of the unsupervised training batches significantly affects the accuracy and further decrease of bL reduces the stability of the regularization.\nOne can also observe another phenomenon through the MNIST results in Figure 4. That is, as bL approaches to mL, the generalization of the model reduces. This effect can be better observed in Figure 5 including a further step, i.e. bL = 96 when mL = 100. Since the same examples start to dominate the batches of unsupervised regularization, overfitting occurs and ultimately test accuracy significantly reduces. Figure 5 also presents the effect of applying dropout during the unsupervised regularization. Dropping out the weights the during unsupervised training dramatically affects the accuracy. This effect is more obvious when bL is smaller. Hence, for the experiments, we removed the dropouts from the models specified in Table 4 during the unsupervised training and applied the following strategy to decide on the bL/bU ratio: bL is approximately assigned as one tenth of the number of all labeled examples mL, i.e. bL ≈ mL/10, and then bU is chosen to complement the batch size up to 128, i.e. bU = 128− bL.\nA.3.3 Effect of regularization coefficients cα, cβ and cF\nThe effects of regularization coefficients are presented in Figure 6 for MNIST dataset. Part (a) of the figure visualizes the case when cF is held constant, but the ratio of cα/cβ changes. And part (b) of the figure illustrates the case when cα/cβ is held constant, but cF changes. We can say that as long as cα ≥ cβ , the ratio of cα/cβ does not affect the accuracy significantly. Furthermore, the value of cF is not so critical (close performances both with cF = 1e−6 and cF = 1e−15) unless it is too big to distort the regularization. Therefore, we can say that the proposed unsupervised regularization term\nis considerably robust with respect to the coefficients cα, cβ and cF . This can also be seen through the fact that we have applied the same coefficients for the experiments of all three datasets."
    }, {
      "heading" : "A.4 More on activity regularization",
      "text" : "Recall that\nN := BTB =  m∑ Bi1Bi1 m∑ Bi1Bi2 . . . m∑ Bi1Bin m∑ Bi2Bi1 m∑ Bi2Bi2 . . . m∑ Bi2Bin ... ... . . . ...\nm∑ BinBi1 m∑ BinBi2 . . . m∑ BinBin\n (13)\nthen we can rewrite α(B ) in terms of B only\nα ( B ) =\nn∑ i 6=j Nij\n(n− 1) n∑ i=j Nij =\n2 n−1∑ i=1 n∑ j=i+1 Nij\n(n− 1) n∑ i=1 Nii =\n2 n−1∑ i=1 n∑ j=i+1 m∑ k=1 BkiBkj\n(n− 1) n∑ i=1 m∑ k=1 BkiBki\n(14)\nAlso recall that v represents the diagonal entries of N such that v := [N11 N22 . . . Nnn] and\nV := vTv =  N11N11 N11N22 . . . N11Nnn N22N11 N22N22 . . . N22Nnn\n... ...\n. . . ...\nNnnN11 NnnN22 . . . NnnNnn\n =  m∑ B2i1 m∑ B2i1 m∑ B2i1 m∑ B2i2 . . . m∑ B2i1 m∑ B2in m∑ B2i2 m∑ B2i1 m∑ B2i2 m∑ B2i2 . . . m∑ B2i2 m∑ B2in .\n. . . . .\n. . . . . .\nm∑ B2in m∑ B2i1 m∑ B2in m∑ B2i2 . . . m∑ B2in m∑ B2in  (15)\n1− β(B ) can be written in terms of N as follows:\n1− β ( B ) = 1−\n∑ i 6=j Vij\n(n− 1) ∑ i=j Vij =\nn−1∑ i=1 n∑ j=i+1 (Nii −Njj)2\n(n− 1) n∑ i=1 N2ii\n(16)\nIf we further replace N with B then\n1− β ( B ) =\nn−1∑ i=1 n∑ j=i+1 ( m∑ k=1 BkiBki − m∑ k=1 BkjBkj )2 (n− 1)\nn∑ i=1 m∑ k=1 (BkiBki)2 (17)\nRecall that the proposed unsupervised loss is\nU ( B ) = cαα ( B ) + cβ ( 1− β ( B )) + cF ||B||2F\nthen the overall unsupervised loss can be written in terms of B as follows:\nU ( B ) = cα  2 n−1∑i=1 n∑j=i+1 m∑k=1BkiBkj (n−1)\nn∑ i=1 m∑ k=1 B2ki\n+ cβ n−1∑i=1 n∑j=i+1 ( m∑ k=1 B2ki− m∑ k=1 B2kj )2 (n−1)\nn∑ i=1 m∑ k=1 B4ki\n+ cF ( n∑ i=1 m∑ k=1 B2ki ) (18)\nwhere m is the number of examples, n is the number of output nodes and\nB = g ( X ) = max ( 0, ( Y (L−1)W (L) + b(L) ))"
    } ],
    "references" : [ {
      "title" : "Laplacian eigenmaps for dimensionality reduction and data representation",
      "author" : [ "Belkin", "Niyogi", "M. 2003] Belkin", "P. Niyogi" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Belkin et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Belkin et al\\.",
      "year" : 2003
    }, {
      "title" : "Manifold regularization: A geometric framework for learning from labeled and unlabeled examples",
      "author" : [ "Belkin et al", "M. 2006] Belkin", "P. Niyogi", "V. Sindhwani" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "al. et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2006
    }, {
      "title" : "Generative adversarial nets",
      "author" : [ "Goodfellow et al", "I.J. 2014] Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A.C. Courville", "Y. Bengio" ],
      "venue" : "In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems",
      "citeRegEx" : "al. et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2014
    }, {
      "title" : "Dimensionality reduction by learning an invariant mapping",
      "author" : [ "Hadsell et al", "R. 2006] Hadsell", "S. Chopra", "Y. LeCun" ],
      "venue" : "IEEE Computer Society Conference on Computer Vision and Pattern Recognition",
      "citeRegEx" : "al. et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2006
    }, {
      "title" : "A fast learning algorithm for deep belief nets",
      "author" : [ "Hinton et al", "G.E. 2006] Hinton", "S. Osindero", "Teh", "Y.-W" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "al. et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2006
    }, {
      "title" : "Semisupervised learning with deep generative models",
      "author" : [ "Kingma et al", "D.P. 2014] Kingma", "S. Mohamed", "D.J. Rezende", "M. Welling" ],
      "venue" : "In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems",
      "citeRegEx" : "al. et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2014
    }, {
      "title" : "Auto-encoding variational bayes",
      "author" : [ "Kingma", "Welling", "D.P. 2013] Kingma", "M. Welling" ],
      "venue" : null,
      "citeRegEx" : "Kingma et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Kingma et al\\.",
      "year" : 2013
    }, {
      "title" : "The mnist database of handwritten digits",
      "author" : [ "LeCun et al", "Y. 1998] LeCun", "C. Cortes", "C.J. Burges" ],
      "venue" : null,
      "citeRegEx" : "al. et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 1998
    }, {
      "title" : "Learning methods for generic object recognition with invariance to pose and lighting",
      "author" : [ "LeCun et al", "Y. 2004] LeCun", "F.J. Huang", "L. Bottou" ],
      "venue" : "In Computer Vision and Pattern Recognition,",
      "citeRegEx" : "al. et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2004
    }, {
      "title" : "Auxiliary deep generative models",
      "author" : [ "Maaløe et al", "L. 2016] Maaløe", "C.K. Sønderby", "S.K. Sønderby", "O. Winther" ],
      "venue" : "In Proceedings of the 33nd International Conference on Machine Learning,",
      "citeRegEx" : "al. et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2016
    }, {
      "title" : "Visualizing data using t-sne",
      "author" : [ "Maaten", "Hinton", "2008] Maaten", "L. v. d", "G. Hinton" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Maaten et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Maaten et al\\.",
      "year" : 2008
    }, {
      "title" : "Distributional smoothing by virtual adversarial examples. CoRR, abs/1507.00677",
      "author" : [ "Miyato et al", "T. 2015] Miyato", "S. Maeda", "M. Koyama", "K. Nakae", "S. Ishii" ],
      "venue" : null,
      "citeRegEx" : "al. et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2015
    }, {
      "title" : "Reading digits in natural images with unsupervised feature learning",
      "author" : [ "Netzer et al", "Y. 2011] Netzer", "T. Wang", "A. Coates", "A. Bissacco", "B. Wu", "A.Y. Ng" ],
      "venue" : "In NIPS workshop on deep learning and unsupervised feature learning,",
      "citeRegEx" : "al. et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2011
    }, {
      "title" : "Feature selection, l 1 vs. l 2 regularization, and rotational invariance",
      "author" : [ "Y. A" ],
      "venue" : "[Ng,",
      "citeRegEx" : "A.,? \\Q2004\\E",
      "shortCiteRegEx" : "A.",
      "year" : 2004
    }, {
      "title" : "Semi-supervised learning of compact document representations with deep networks",
      "author" : [ "Ranzato", "Szummer", "M. 2008] Ranzato", "M. Szummer" ],
      "venue" : "In Machine Learning, Proceedings of the Twenty-Fifth International Conference (ICML",
      "citeRegEx" : "Ranzato et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Ranzato et al\\.",
      "year" : 2008
    }, {
      "title" : "Semi-supervised learning with ladder networks",
      "author" : [ "Rasmus et al", "A. 2015] Rasmus", "M. Berglund", "M. Honkala", "H. Valpola", "T. Raiko" ],
      "venue" : "In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems",
      "citeRegEx" : "al. et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2015
    }, {
      "title" : "Stochastic backpropagation and approximate inference in deep generative models",
      "author" : [ "Rezende et al", "D.J. 2014] Rezende", "S. Mohamed", "D. Wierstra" ],
      "venue" : "In Proceedings of the 31th International Conference on Machine Learning,",
      "citeRegEx" : "al. et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2014
    }, {
      "title" : "Rule-injection hints as a means of improving network performance and learning time",
      "author" : [ "Suddarth", "Kergosien", "S.C. 1990] Suddarth", "Y.L. Kergosien" ],
      "venue" : "In Neural Networks, EURASIP Workshop",
      "citeRegEx" : "Suddarth et al\\.,? \\Q1990\\E",
      "shortCiteRegEx" : "Suddarth et al\\.",
      "year" : 1990
    }, {
      "title" : "Deep learning via semi-supervised embedding",
      "author" : [ "Weston et al", "J. 2012] Weston", "F. Ratle", "H. Mobahi", "R. Collobert" ],
      "venue" : "In Neural Networks: Tricks of the Trade - Second Edition,",
      "citeRegEx" : "al. et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2012
    }, {
      "title" : "Revisiting semisupervised learning with graph embeddings",
      "author" : [ "Yang et al", "Z. 2016] Yang", "W.W. Cohen", "R. Salakhutdinov" ],
      "venue" : "In Proceedings of the 33nd International Conference on Machine Learning,",
      "citeRegEx" : "al. et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2016
    }, {
      "title" : "Semi-supervised learning using gaussian fields and harmonic functions",
      "author" : [ "Zhu et al", "X. 2003] Zhu", "Z. Ghahramani", "J.D. Lafferty" ],
      "venue" : "In Machine Learning, Proceedings of the Twentieth International Conference (ICML",
      "citeRegEx" : "al. et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2003
    } ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "In this paper, we propose a novel graph-based approach for semi-supervised learning problems, which considers an adaptive adjacency of the examples throughout the unsupervised portion of the training. Adjacency of the examples is inferred using the predictions of a neural network model which is first initialized by a supervised pretraining. These predictions are then updated according to a novel unsupervised objective which regularizes another adjacency, now linking the output nodes. Regularizing the adjacency of the output nodes, inferred from the predictions of the network, creates an easier optimization problem and ultimately provides that the predictions of the network turn into the optimal embedding. Ultimately, the proposed framework provides an effective and scalable graph-based solution which is natural to the operational mechanism of deep neural networks. Our results show state-of-the-art performance within semi-supervised learning with the highest accuracies reported to date in the literature for SVHN and NORB datasets.",
    "creator" : "LaTeX with hyperref package"
  }
}