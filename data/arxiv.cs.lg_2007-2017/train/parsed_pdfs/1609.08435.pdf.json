{
  "name" : "1609.08435.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Asynchronous Stochastic Proximal Optimization Algorithms with Variance Reduction",
    "authors" : [ "Qi Meng", "Wei Chen", "Jingcheng Yu", "Taifeng Wang", "Zhi-Ming Ma", "Tie-Yan Liu" ],
    "emails" : [ "qimeng13@pku.edu.cn", "tie-yan.liu}@microsoft.com", "JingchengYu.94@gmail.com", "mazm@amt.ac.cn" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "In this paper, we focus on the regularized empirical risk minimization (R-ERM) problem, whose objective is a finite sum of smooth convex loss functions fi(x) plus a non-smooth regularization term R(x), i.e.,\nmin x∈Rd\nP (x) = F (x) +R(x) = 1\nn n∑ i=1 fi(x) +R(x). (1)\nIn particular, in the context of machine learning, fi(x) and R(x) are defined as follows. Suppose we are given a collection of training data (a1, b1),...,(an, bn), where each ai ∈ Rd is an input feature vector and bi ∈ R is the output variable. The loss function fi(x) measures the fitness of the model x on training data (ai, bi). Different learning tasks may use different loss functions, such as the least square loss 12 (a T i x− bi)2\nCopyright c© 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nfor regression and the logistic loss log(1 + exp(−biaTi x)) for classification. The regularization term is used to constrain the capacity of the hypothesis space. For example, the nonsmooth L1 regularization term is widely used.\nIn order to solve the R-ERM problem, the proximal stochastic gradient descent method (ProxSGD) has been widely used, which exploits the additive nature of the empirical risk function and updates the model based on the gradient which is calculated at randomly sampled training data. However, the random sampling in ProxSGD introduces non-negligible variance, which makes that we need to use a decreasing step size (also known as learning rate) to guarantee the algorithm’s convergence, and the convergence rate is only sublinear (Langford, Li, and Zhang 2009; Rakhlin, Shamir, and Sridharan 2011). To tackle this problem, people have developed a set of new technologies. For example, in (Xiao and Zhang 2014), a variance reduction technique was introduced to improve ProxSGD and a new algorithm called ProxSVRG was proposed. It has been proven that even with a constant step size, ProxSVRG can achieve linear convergence rate.\nProximal stochastic coordinate descent (ProxSCD) is another method which is used to solve the R-ERM problem (Shalev-Shwartz and Tewari 2011). Since the variance introduced by the coordinate sampling asymptotically goes to zero, the ProxSCD attains linear convergence rate when the objective function P (x) is strongly convex (Wright 2015). However, ProxSCD still requires that all component functions in the empirical risk are accessible in each iteration, which is time consuming. In (Zhao et al. 2014), a new algorithm called ProxSVRCD (also known as MRBCD) was proposed to improve ProxSCD. This algorithm, in addition to randomly samples a block of coordinates, also randomly samples training data in each iteration and uses the variance reduction technique. It has been proven that ProxSVRCD can achieve linear convergence rate and outperform ProxSCD by a lower iteration complexity.\nWhile the aforementioned new algorithms (i.e., ProxSVRG and ProxSVRCD) have both good theoretical properties and empirical performances, the investigations on them were mainly conducted in the sequential (single-machine) setting. In this big data era, we usually need to deal with very large scale R-ERM problems. In this case, sequential algorithms usually cost too much time. To tackle the challenge, paral-\nar X\niv :1\n60 9.\n08 43\n5v 1\n[ cs\n.L G\n] 2\n7 Se\np 20\n16\nlelization of these algorithms are sorely needed. Recently literature research in parallel method tend to use asynchronous parallelization due to its high efficient in system (Dean et al. 2012; Recht et al. 2011). We are interested in asynchronous parallel implementations of the aforementioned stochastic proximal algorithms with variance reduction, which are, however, not well studied in the literature, to the best of our knowledge.\nFor asynchronous ProxSVRG (Async-ProxSVRG), we consider the consistent read setting, in which we ensure the atomic pull and push of the whole parameter for the local workers. For asynchronous ProxSVRCD (AsyncProxSVRCD), since the updates are performed over coordinate blocks, we only ensure the atomic pull and push of a coordinate block of the parameter for local workers for the sake of system efficiency. Comparing with AsyncProxSVRG setting, we name it as inconsistent read setting. We conduct theoretical analysis for Async-ProxSVRG and Async-ProxSVRCD. According to our results: (1) AsyncProxSVRG can achieve near linear speedup with respect to the number of local workers, when the input feature vectors are sparse; (2) If the data are non-sparse, ProxSVRCD can still achieve near linear speedup, when the block size is small comparing to the input dimension. The intuition of the linear speedup of the asynchronous proximal algorithms with variance reduction can be explained as follows. Asynchronous implementation updates the master parameter based on the delayed gradients. If the data are sparse for asynchronous ProxSVRG or the coordinate block size is small comparing to the input dimension for ProxSVRCD, the influence of the delayed gradients can be bounded, and the asynchronous implementations are roughly equivalent to the sequential version.\nIn addition to the theoretical analysis, we have also conducted experiments on benchmark datasets to test the performances of the asynchronous stochastic proximal algorithms with variance reduction. According to the experimental results, we have the following observations: (1) AsyncProxSVRG have good speedup, especially for sparse data; (2) Async-ProxSVRCD also have good speedup, and is more efficient than Async-ProxSVRG when the input feature vectors are relatively dense or the coordinate block size is small. (3) Async-ProxSVRG and Async-ProxSVRCD can converge faster than other asynchronous algorithms reported in literature such as Async-ProxSGD (Lian et al. 2015) and AsyncProxSCD (Liu and Wright 2015). The results are consistent across different datasets, indicating that our observations are general and the two asynchronous proximal algorithms are highly efficient and scalable for practical use.\nThis paper is organized as follows: in Section 2, we briefly introduce the stochastic proximal algorithms with variance reduction including ProxSVRG and ProxSVRCD, and then related works; in Section 3, we describe the asynchronous parallelization of these algorithms; in Section 4, we prove the convergence rates for Async-ProxSVRG and AsyncProxSVRCD; in Section 5, we report the experimental results and make discussions; finally, in the last section, we conclude the paper and present future research directions.\n2 Background In this section, we will briefly introduce proximal algorithms with variance reduction, and then review the existing convergence analysis for asynchronous parallel algorithms.\nProxSGD and ProxSCD At first, let us briefly introduce the standard stochastic proximal gradient algorithms,i.e., ProxSGD and ProxSCD. With ProxSGD, at iteration k, the solution to the R-ERM problem (i.e., Eqn (1)) is as follows:\nxk+1 = proxηkR {xk − ηk∇fBk (xk))} , (2)\nwhere ηk is the step size, Bk is a mini-batch of randomly selected training data, ∇fBk(xk) = 1|Bk| ∑ i∈Bk ∇fi(xk) and the proximal mapping is defined as proxR(y) = argminx∈Rd { 1 2‖x− y‖ 2 2 +R(x) } .\nProxSCD exploits the block separability of the regularization term R in the R-ERM problem, i.e.,R(x·) =∑m j=1Rj(x·,Cj ), where x·,Cj is the j-th coordinate block of x·. For example, for the L1-norm regularizer, {Cj ; j = 1, · · · ,m} is a partition of {1, · · · , d} with m = dblock size , and Rj(x·,Cj ) = ∑ l∈Cj |x·,j |. ProxSCD randomly selects a coordinate block and update the coordinates in that block based on their gradients while keep the value of the other coordinates unchanged, i,e.,\nxk+1,Cjk = proxηRjk { xk,Cjk − η∇CjkF (xk−1)) } ,(3)\nwhere Cjk is the coordinate block sampled at iteration k, and ∇CjF (x) = [∇F (x)]Cj .\nProximal Algorithms with Variance Reduction For ProxSGD, the step size ηk has to be decreasing in order to mitigate the variance introduced by random sampling, which usually leads to slow convergence. To tackle this problem, one of the most popular variance reduction techniques was proposed by Johnson and Zhang (Johnson and Zhang 2013). Xiao and Zhang applied this variance reduction technique to improve ProxSGD, and a new algorithm called ProxSVRG was proposed (Xiao and Zhang 2014).\nThe ProxSVRG algorithm divides the optimization process into multiple stages. At the beginning of stage s, ProxSVRG calculates the full gradient at the current solution x̃s−1, i.e., ∇F (x̃s−1). Then, at iteration k inside stage s, the solution is updated as follows:\nvk = ∇fBk (xk)−∇fBk (x̃s−1) +∇F (x̃s−1), (4) xk+1 = proxηkR {xk − ηkvk} , (5)\nwhere−∇fBk(x̃s−1)+∇F (x̃s−1) is the variance reduction regularization term.\nFor ProxSCD, since the variance introduced by the block selection asymptotically goes to zero, it attains linear convergence rate. However, it still requires that all component functions are accessible within every iteration. Zhao et.al. used variance reduction technique to improve ProxSCD with random training data sampling and a new algorithm called ProxSVRCD was proposed (Zhao et al. 2014). 1\n1In (Zhao et al. 2014), this algorithm was named MRBCD. In this paper, we call it ProxSVRCD to ease our reference.\nProxSVRCD is similar to ProxSVRG, the update formula for iteration k inside stage s takes the following form:\nvk = ∇fBk (xk)−∇fBk (x̃s−1) +∇F (x̃s−1), (6)\nxk+1,Cjk = proxηkRjk { xk,Cjk − ηkvk,Cjk } , (7)\nxk+1,\\Cjk ← xk,\\Cjk . (8)\nwhere −∇fBk (x̃s−1) +∇F (x̃s−1) is the variance reduction regularization term.\nExisting Convergence Analysis of Asynchronous Parallel Algorithms The asynchronous parallel methods have been successfully applied to accelerate many optimization algorithms including stochastic gradient descent (SGD)(Agarwal and Duchi 2011; Feyzmahdavian, Aytekin, and Johansson 2015; Recht et al. 2011; Mania et al. 2015), stochastic coordinate descent (SCD) (Liu et al. 2013; Liu and Wright 2015), stochastic dual coordinate ascent (SDCA) (Tran et al. 2015) and randomized Kaczmarz algorithm (Liu, Wright, and Sridhar 2014). However, to the best of our knowledge, the asynchronous parallel versions of ProxSVRG and ProxSVRCD are not well studied, as well as their theoretical properties.\nWe briefly review the works which are closely related to ours as follows. Reddi et.al. studied asynchronous SVRG and proved that, asynchronous SVRG can achieve near linear speedup under some sparse condition (Reddi et al. 2015). Liu and Wright analyzed the asynchronous ProxSCD. They proved that the asynchronous ProxSCD can achieve near linear speedup if the delay is bounded by O(d 14 ), where d is the input dimension (Liu and Wright 2015).\nHowever, to the best of our knowledge, there is no study on the asynchronous parallel versions of proximal algorithms with variance reduction, as well as their theoretical properties."
    }, {
      "heading" : "3 Asynchronous Proximal Algorithms with Variance Reduction",
      "text" : "In this section, we describe our Async-ProxSVRG and AsyncProxSVRCD algorithms under the following asynchronous parallel architecture. Suppose there are P local workers and one master. For local workers, each of them has full access to the training data and stores a non-overlapping partition Np (p = 1, ..., P ) of the training data. Each local worker independently communicates with the master to pull the global parameters from the master, and it computes the stochastic gradients locally and then push the gradients to the master. For the master, it maintains the global model. It updates the model parameters with the gradient pushed by local workers and sends the model parameters to local workers when it receives the pull request. Master can control the access conflict based on different granularity. In Async-ProxSVRG, the local worker will access the entire model in every update. Therefore, we let master only response to one local worker’s request at one time, which means the global model is atomic for all workers. In Async-ProxSVRCD, the local worker will only access a coordinate block in every update and different workers might work on different blocks without interfering others. In this case, master will response to multiple local\nworkers simultaneously if only they are not accessing the same coordinate block, which means the global model is atomic at coordinate block level.\nWith variance reduction technique, the optimization process is divided into multiple stages (i.e., outer loop: s = 1, · · · , S). In each stage, there are two phases: full gradient computation and solution updates (i.e., inner loop: k = 1, · · · ,K).\nFull gradient computation: the workers collectively compute the full gradient in parallel based on the entire training data. Specifically, each worker pulls the master parameter from the master, computes the gradients over one part of the training data, and pushes the sum of the gradients to the master. Then the master aggregates the gradients from the workers to obtain the full gradient, and broadcasts it to the workers.\nSolution updates: the workers compute the VR-regularized stochastic gradient in an asynchronous way and the master makes updates according to the proximal algorithms. To be specific, at iteration k, one local worker (who just finished its local computation) pulls the master parameters from the master, computes the VR-regularized stochastic gradient according to Eqn (4) for ProxSVRG or Eqn(6) for ProxSVRCD, and then pushes it to the master without any synchronization with the other workers. After the master receives the VRregularized gradient from this worker, it updates the master parameter according to Eqn (5) for ProxSVRG or Eqn (7)(8) for Prox SVRCD. Then the global clock becomes k + 1, and the next iteration begins. Corresponding details can be found in Algorithm 1.\nPlease note that, the gradient pushed by a local worker to the master could be delayed. The reason is, when the worker is working on its own local computation, other workers might finish their computations and push their gradients to the master, and the master updates the master parameter accordingly.\nAs aforementioned, for Async-ProxSVRG, the whole model is atomic to each worker’s access. When the worker 0 is working on its own local computation, worker 1 and worker 2 might finish their computations, pushed their gradients to the master, and the master updates the master parameter accordingly. Thus, when worker 0 finish its computation and push it to the master, the global clock has already plus 2. Thus, the local gradients have delay=2 for the current master parameter. We use a random variable τk to denote the delay of local gradients received by the master at global clock k. The delay equals to the number of updates that other workers have committed to the master between one particular worker pulls the parameter from the master and pushes gradients to the master. For asynchronous ProxSVRCD, multiple workers may access the master parameter simultaneously, updating different coordinate blocks. Then different coordinate blocks in the model could be inconsistent regarding to the global update clock. To be precise, at global clock k, the master makes update based on the gradients computed by a local worker, who read the first coordinate block of the master parameter at global clock k − τk. We denote the finally pulled parameter as x̂k, which can be represented as below:\nx̂k = xk−τk + ∑\nh∈J(k)\n(xh+1 − xh), (9)\nwhere J(k) ⊂ {k − τk, . . . , k − 1}. The k-th update can\nbe described as xk+1,Cjk = proxηkRjk { xk,Cjk − ηkuk,Cjk } , where uk = ∇fBk (x̂k) − ∇fBk (x̃) + ∇F (x̃). The delay τk equals to the difference between the clock at which a local worker pulls the first coordinate block from the master and the clock at which the local worker pushes the gradients to the master.\nWe conduct theoretical analysis for Async-ProxSVRG and Async-ProxSVRCD based on the above setting in the next section. Like other asynchronous parallel algorithms, the delay also plays an important role in the convergence rate of asynchronous proximal algorithms with variance reduction.\nAlgorithm 1 Async-ProxSVRG and Async-ProxSVRCD Require: initial vector x̃0, step size η, number of inner loops K, size of mini-batch B, number of coordinate blocks m. Ensure: x̃S for s = 1, 2, ..., S do x̃ = x̃s−1, x0 = x̃ For local worker p: calculate ∇Fp(x̃) = ∑ i∈Np ∇fi(x̃)\nand send it to the master. For master: calculate ∇F (x̃) = 1\nn ∑P p=1∇Fp(x̃) and\nsend it to each local worker. for k = 1, ...,K do\n1. Async-ProxSVRG: consistent read For local worker p: randomly select a mini-batch Bk with |Bk| = B. Pull current state xk−τk from the master. Compute uk = ∇fBk (xk−τk )−∇fBk (x̃) +∇F (x̃). Push uk to the master. For master: Update xk+1 = proxηR(xk − ηuk). 2. Async-ProxSVRCD: inconsistent read For local worker p: randomly select Bk with |Bk| = B, and randomly select jk ∈ [m]. Pull current state x̂k from the master. Compute uk = ∇fBk (x̂k)−∇fBk (x̃) +∇F (x̃). Push uk to the master. For master: Update xk+1,Cjk = proxηRjk { xk,Cjk − ηuk,Cjk } ;\nxk+1,\\Cjk ← xk,\\Cjk end for x̃s =\n1 K ∑K k=1 xk\nend for"
    }, {
      "heading" : "4 Convergence Analysis",
      "text" : "In this section, we prove the convergence rates of the asynchronous parallel proximal algorithms with variance reduction introduced in the previous section.\nAsync-ProxSVRG At first, we introduce the following assumptions, which are very common in the theoretical analysis for asynchronous parallel algorithms (Recht et al. 2011; Reddi et al. 2015).\nAssumption 1: (Convexity) F (x) and R(x) are convex and R(x) is block sparable. The objective function P (x) is\nµ-strongly convex, i.e., ∀x, y ∈ Rd, we have,\nP (y) ≥ P (x) + ξT (y − x) + µ 2 ‖y − x‖2, ∀ξ ∈ ∂P (x).2\nAssumption 2: (Smoothness) The components {fi(x); i ∈ [n]} of F (x) are differentiable and have Lipschitz continuous partial gradients and thus Lipschitz continuous gradients, i.e., ∃T, L > 0, such that ∀x, y ∈ Rd with xj 6= yj , we have ‖∇jfi(x)−∇jfi(y)‖ ≤ T‖xj − yj‖,∀i ∈ [n], j ∈ [d]. ‖∇fi(x)−∇fi(y)‖ ≤ L‖x− y‖, ∀i.\nAssumption 3: (Bounded and Independent Delay) The random delay variables τ1, τ2, ... in consistent read setting are independent of each other and independent of Bk, and their expectations are upper bounded by τ , i.e., Eτk ≤ τ for all k. Assumption 4: (Data Sparsity) The maximal frequency of a feature appearing in the dataset is upper bounded by ∆.\nBased on these assumptions, we prove that AsyncProxSVRG has linear convergence rate. Theorem 4.1 Suppose Assumptions 1-4 hold. If the step size η < min { 2\n5LB∆τ2 , B 16L\n} , and the inner loop size K is suffi-\nciently large so that\nρ = B ηµK(B − 8ηL) + 8ηL (B − 8ηL) < 1,\nthen Async-ProxSVRG has linear convergence rate in expectation:\nEP (x̃s)− P (x∗) ≤ ρs[P (x̃0)− P (x∗)],\nwhere x∗ = argminxP (x). Due to space limitation, we only provide the proof sketch and put the proof details into supplementary materials. Proof Sketch of Theorem 4.1:\nFirstly we introduce some notations. Let xk+1 − xk = −ηgk, vk = ∇fBk(xk) − ∇fBk(x̃) + ∇F (x̃), and uk = ∇fBk(xk−τk)−∇fBk(x̃) +∇F (x̃).\nStep 1: The key for the proof is that by the spasity condition, we have F (x) ≥ F (y)−∇F (x)(y− x)− LB∆\n2 ‖x− y‖2.\nStep 2: By using the convexity of F (x) andR(x), we have:\nP (x∗) ≥ P (xk+1) + (uk −∇F (xk−τk )) T (xk+1 − x∗)\n+η‖gk‖2 − Lη2B∆τk\n2\nk∑ h=k−τk ‖gh‖2 + gTk (x∗ − xk+1).\nStep 3: We use Lemma 3 in (Xiao and Zhang 2014) to bound the term EBk(uk−∇F (xk−τk))T (xk+1−x∗). Then by summing k from 0 to K − 1, we can get: −∑K−1k=0 EgTk (xk − x∗) + ( η − Lη2B∆τ2(1 + 2Lη) )∑K−1 k=0 E‖gk‖ 2 ≤ ( 8Lη B −\n1) ∑K−1 k=0 (P (xk+1)− P (x ∗)) + 8Lη B\n(K + 1)(P (x̃)− P (x∗)). Step 4: Under the condition η < min { 2\n5LB∆τ2 , B 16L\n} , we\nhave η−Lη2B∆τ2(1+2Lη) ≥ η2 . Then following the proof of ProxSVRG, we can get the results.\nRemark: Theorem 4.1 actually shows that, AsyncProxSVRG can achieve linear speedup when ∆ is small and τ ≤ √ 8/B2∆. For sequential ProxSVRG, with step size\n2In this paper, if there is no specification, ‖ · ‖ is the L2-norm.\nη = 0.1B/L, the inner loop size K should be in the same order of O(L/Bµ) to make ρ < 1. The computation complexity (number of gradients need to calculate) for the inner loop is in the same order of O(L/µ). For the Async-ProxSVRG, with η = min{ 25LB∆τ2 , 0.05B L }, the inner loop sizeK should be in the same order of O(L/Bµ + B∆τ2L/µ) to make ρ < 1. For the case τ < √ 8/B2∆ (i.e., 0.05BL < 2 5LB∆τ2 ), by setting η = 0.05BL , the order of inner loop size K is O(L/Bµ) and the corresponding computation complexity is O(L/µ), which is the same as the sequential ProxSVRG. Therefore, Async-ProxSVRG can achieve nearly the same performance as the sequential version, but τ times faster since we are running the algorithm asynchronously, and thus we achieve \"linear speedup\". For the case τ ≥ √ 8/B2∆, the inner loop size K should be in the same order of O(B∆τ2L/µ). Compared with the sequential ProxSVRG with K = O(L/Bµ), Async-ProxSVRG can not obtain linear speedup but still have a theoretical speedup of 1/B2∆τ if B2∆τ < 1.\nAccording to Theorem 4.1 and the above discussions, we provide the following corollary for a simple setup of the parameters in Async-ProxSVRG which can achieve near linear speedup. Corollary 4.2 Suppose Assumptions 1-4 hold. If we set B =( 1 ∆ ) 1 4 , τ ≤ √ 8/∆ 1 2 , η = 0.05∆ 1 4 L and K = 200L∆ 1 4\nµ , then Async-ProxSVRG has the following linear convergence rate:\nEP (x̃s)− P (x∗) ≤ ( 5\n6\n)s [P (x̃0)− P (x∗)],\nwhere x∗ = argminxP (x).\nAsync-ProxSVRCD In this section, we present Theorem 4.3, which states the convergence rate of Async-ProxSVRCD, as well as the conditions for them to achieve near linear speedup.\nAssumption 3′:(Bounded and Independent Delay) The random delay variables τ1, τ2, ... in inconsistent read setting in Eqn 9 are independent of each other and independent of Bk, and their expectations are upper bounded by τ . Theorem 4.3 Suppose Assumptions 1, 2, and 3′ hold. In addition, we assume that the mini-batch size B ≥ L/T , the step size η and the coordinate block number m satisfies\nη < min { 1 T m 3 2−Tτ\nm 3 2 +3mτ+τ2\n, 1 8T , µ √ m 2Tτ } , and the inner loop size\nK is sufficiently large so that\nρ =\n( m\nηµK(1− Tητ µ √ m − 4ηT )\n+ 4ηT (K + 1)\n(1− Tητ µ √ m − 4ηT )K\n) < 1,\nthen Async-ProxSVRCD has linear convergence in expectation:\nEP (x̃s)− P (x∗) ≤ ρs[P (x̃0)− P (x∗)],\nwhere x∗ = argminxP (x).\nProof Sketch of Theorem 4.3:\nStep 1: By the convexity of F (x) and R(x), we have\nP (x∗) ≥ mEjkP (xk+1)− (m− 1)P (xk) + (η − Tη2\n2 )mEjk‖gk‖\n2\n+(vk −∇F (xk))T (x̄k+1 − x∗) +mEjk (gk) T (x∗ − xk) +(∇fBk (x̂k)−∇fBk (xk)) T (x̄k+1 − x∗).\nStep 2: We decompose the term −(∇fBk (x̂k) − ∇fBk (xk)) T (x̄k+1 − x∗) by using Assumption 2 as below:\n− 1 T (∇fBk (x̂k)−∇fBk (xk)) T (x̄k+1 − x∗) ≤\nk−1∑ a=k−τ ‖xa+1 − xa‖‖x̄k+1 − xk‖+ k−1∑ a=k−τ ‖xa+1 − xa‖‖xa − x∗‖\n+ k−1∑ a=k−τ k−1∑ b=a ‖xa+1 − xa‖‖xb+1 − xb‖.\nBy taking expectation w.r.t j1, · · · , jk gradually, we can bound the three terms on the right side. This is a key step for the proof and please see the details in the supplementary materials. Thus we can get\nK∑ k=1 −E(gk)T (xk − x∗) +A(η)E‖gk‖2\n≤ K∑ k=1 − (EP (xk+1)− P (x∗))− 1 m (vk −∇F (xk))T (x̄k+1 − x∗)\n+ K∑ k=1 ( (m− 1) m + Tητ µm 3 2 )E (P (xk)− P (x∗)) ,\nwhereA(η) = ( η(1− Tτ\n2m 3 2\n)− Tη2( 12 + τ 2 √ m + τm + τ2\n2m 3 2\n) ) .\nStep 3: With the assumption η < 1 T\nm 3 2−Tτ\nm 3 2 +3mτ+τ2\n, we have\nA(η) > η 2 . Then by following the proof of ProxSVRCD, we can get the results.\nRemark: Theorem 4.3 actually shows that when m is large (or equivalent the block size is small) and τ ≤ min {√ m, 4µ √ m,m 3 2 /2T } , Async-ProxSVRCD can\nachieve linear speedup. For the sequential ProxSVRCD, Corollary 4.3 in (Zhao et al. 2014) set η = 1/16T , B = L/T and the inner loop size K in the same order of O(mT/µ) to make ρ < 1. For AsyncProxSVRCD, if m is sufficiently large so that the delay satisfies τ ≤ min {√ m, 4µ √ m,m 3 2 /2T } , we can\nset η = 1/24T which guarantees the condition η <\nmin { 1 T m 3 2−Tτ\nm 3 2 +3mτ+τ2\n, 1 8T , µ √ m 2Tτ } . Thus, the inner loop size\nK should be O(mT/µ) to make ρ < 1, which is the same as sequential ProxSVRCD. Therefore, Async-ProxSVRCD can achieve near linear speedup. If we consider the indicative case (Shamir, Srebro, and Zhang 2014) in which L/µ = √ n, L = O(1) and µ = O( √ 1/n). The condition\nfor the linear speedup can be simplified to τ ≤ 4 √ m/n.\nEven if 4 √ m/n < τ ≤ √ m, Async-ProxSVRCD still\nhave a speedup of O( √ m/n) by setting η ≤ µ √ m 2Tτ = √ m 2τ √ n , since m 3 2−Tτ\nm 3 2 +3mτ+τ2\n> √ m\n2τ √ n .\nAccording to Theorem 4.3 and the above discussions, we provide the following corollary for a simple setup of the parameters in Async-ProxSVRCD which can achieve near linear speedup. Corollary 4.4 Suppose Assumptions 1,2, and 3′ hold and the delay bound satisfies τ ≤ min {√ m, 4µ √ m,m 3 2 /2T } .\nLet η = 1/24T , B = L/T and K = 216mTµ , then AsyncProxSVRCD has the following linear convergence rate:\nEP (x̃s)− P (x∗) ≤ ( 5\n6\n)s [P (x̃0)− P (x∗)],\nwhere x∗ = argminxP (x).\nBy comparing the conditions of the linear speedup for asynchronous Proximal algorithms, we have the following findings: (1). Async-ProxSVRG relies on the data sparsity to alleviate the negative impact of communication delay τ ; (2) Async-ProxSVRCD does not rely on the sparsity condition, however, it requires the block size is small or the input dimension is large, since in this way, the block-wise updates will become frequent and can also alleviate the delay of the whole parameter vector.\nTo sum up, in this section, based on a few widely used assumptions, we have proven the convergence properties of the asynchronous parallel implementations of ProxSVRG, and ProxSVRCD, and discussed the conditions for them to achieve near linear speedups as compared to their sequential (single-machine) counterparts. In the next section, we will report the results of our experiments to verify these theoretical findings."
    }, {
      "heading" : "5 Experiments",
      "text" : "In this section, we report our experimental results on the efficiency of the asynchronous proximal algorithms with variance reduction. In particular, we conducted binary classifications on three benchmark datasets: rcv1, real-sim, news20 (Reddi et al. 2015), new20 is the densest one with a much higher dimension and rcv1 is the sparsest one. The detailed information about the three data sets is given in Table 1. We use the logistic loss function with both L1 and L2 regularizations with weight λ1 and λ2 respectively.\nFollowing the practices in (Xiao and Zhang 2014), we normalized the input vector of each data set before feeding it into the classifier, which leads to an upper bound of 0.25 for the Lipschitz constant L. The stopping criterion for all the algorithms under investigation is the optimization error\nsmaller than 10−10 (i.e., P (x̃S) − P (x∗) < 10−10). For Async-ProxSVRG, we set step size η = 0.04, the mini-batch size B = 200, and the inner loop size K = 2n, where n is the data size. For Async-ProxSVRCD, we set step size η = 0.04, the number of block partitions m = d100 , the minibatch size B = 200, and a larger inner loop size K = 2nm. We implement Async-ProxSVRG and Async-ProxSVRCD in the consistent read setting and the inconsistent read setting, respectively.\nThe speedups of Async-ProxSVRG and AsyncProxSVRCD are shown in Figures 1(a) and 1(b). From the figures, we have the following observations. (1) On all the three datasets, Async-ProxSVRG has near linear speedup compared to its sequential counterpart. The speedup on rcv1 is the largest, while that on news20 is the smallest. This observation is consistent with our theoretical findings that Async-ProxSVRG has better performance on sparser data. (2) Async-ProxSVRCD also achieves nice speedup. The speedup is more significant for news20 than that for the other two data sets. This is consistent with our theoretical discussions - the sufficient condition for the linear speedup of Async-ProxSVRCD is easier to be satisfied for high-dimensional datasets. As literature also reported other asynchronous algorithms, such as Async-ProxSGD and Async-ProxSCD, we also compare with them to test the performance of our algorithms. Our algorithms actually converge faster than those without variance reduction, which means asynchronization can work together with VR techniques smoothly and enhances the model’s convergence speed. For saving space, we put the detailed results in the supplementary materials.\nIn summary, our experimental results well validate our theoretical findings, and indicate that the asynchronous proximal algorithms with variance reduction are very efficient and could have good applications in practice.\n6 Conclusion In this paper, we have studied the asynchronous parallelization of two widely used proximal gradient algorithms with variance reduction, i.e., ProxSVRG and ProxSVRCD. We have proved their convergence rates, discussed their speedups, and verified our theoretical findings through experiments. Overall speaking, these asynchronous proximal algorithms can achieve linear speedup under certain conditions, and can be highly efficient when being used to solve large scale R-\nERM problems. As for future work, we plan to make the following explorations. First, we will extend the study in this paper to the non-convex case, both theoretically and experimentally. Second, we will study the asynchronous parallelization of more proximal algorithms."
    } ],
    "references" : [ {
      "title" : "J",
      "author" : [ "A. Agarwal", "Duchi" ],
      "venue" : "C.",
      "citeRegEx" : "Agarwal and Duchi 2011",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Q",
      "author" : [ "J. Dean", "G. Corrado", "R. Monga", "K. Chen", "M. Devin", "M. Mao", "A. Senior", "P. Tucker", "K. Yang", "Le" ],
      "venue" : "V.; et al.",
      "citeRegEx" : "Dean et al. 2012",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "H",
      "author" : [ "Feyzmahdavian" ],
      "venue" : "R.; Aytekin, A.; and Johansson, M.",
      "citeRegEx" : "Feyzmahdavian. Aytekin. and Johansson 2015",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "and Zhang",
      "author" : [ "R. Johnson" ],
      "venue" : "T.",
      "citeRegEx" : "Johnson and Zhang 2013",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Sparse online learning via truncated gradient",
      "author" : [ "Li Langford", "J. Zhang 2009] Langford", "L. Li", "T. Zhang" ],
      "venue" : null,
      "citeRegEx" : "Langford et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Langford et al\\.",
      "year" : 2009
    }, {
      "title" : "Asynchronous parallel stochastic gradient for nonconvex optimization",
      "author" : [ "Lian" ],
      "venue" : null,
      "citeRegEx" : "Lian,? \\Q2015\\E",
      "shortCiteRegEx" : "Lian",
      "year" : 2015
    }, {
      "title" : "S",
      "author" : [ "J. Liu", "Wright" ],
      "venue" : "J.",
      "citeRegEx" : "Liu and Wright 2015",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "S",
      "author" : [ "Liu, J.", "Wright" ],
      "venue" : "J.; Ré, C.; Bittorf, V.; and Sridhar, S.",
      "citeRegEx" : "Liu et al. 2013",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "S",
      "author" : [ "Liu, J.", "Wright" ],
      "venue" : "J.; and Sridhar, S.",
      "citeRegEx" : "Liu. Wright. and Sridhar 2014",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "M",
      "author" : [ "H. Mania", "X. Pan", "D. Papailiopoulos", "B. Recht", "K. Ramchandran", "Jordan" ],
      "venue" : "I.",
      "citeRegEx" : "Mania et al. 2015",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Making gradient descent optimal for strongly convex stochastic optimization. arXiv preprint arXiv:1109.5647",
      "author" : [ "Shamir Rakhlin", "A. Sridharan 2011] Rakhlin", "O. Shamir", "K. Sridharan" ],
      "venue" : null,
      "citeRegEx" : "Rakhlin et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Rakhlin et al\\.",
      "year" : 2011
    }, {
      "title" : "Hogwild: A lock-free approach to parallelizing stochastic gradient descent",
      "author" : [ "Recht" ],
      "venue" : null,
      "citeRegEx" : "Recht,? \\Q2011\\E",
      "shortCiteRegEx" : "Recht",
      "year" : 2011
    }, {
      "title" : "A",
      "author" : [ "S.J. Reddi", "A. Hefny", "S. Sra", "B. Póczos", "Smola" ],
      "venue" : "J.",
      "citeRegEx" : "Reddi et al. 2015",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "and Tewari",
      "author" : [ "S. Shalev-Shwartz" ],
      "venue" : "A.",
      "citeRegEx" : "Shalev.Shwartz and Tewari 2011",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Communication-efficient distributed optimization using an approximate newton-type method",
      "author" : [ "Srebro Shamir", "O. Zhang 2014] Shamir", "N. Srebro", "T. Zhang" ],
      "venue" : "In Proceedings of the 31st International Conference on Machine Learning",
      "citeRegEx" : "Shamir et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Shamir et al\\.",
      "year" : 2014
    }, {
      "title" : "Scaling up stochastic dual coordinate ascent",
      "author" : [ "Tran" ],
      "venue" : "In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,",
      "citeRegEx" : "Tran,? \\Q2015\\E",
      "shortCiteRegEx" : "Tran",
      "year" : 2015
    }, {
      "title" : "S",
      "author" : [ "Wright" ],
      "venue" : "J.",
      "citeRegEx" : "Wright 2015",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "and Zhang",
      "author" : [ "L. Xiao" ],
      "venue" : "T.",
      "citeRegEx" : "Xiao and Zhang 2014",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Accelerated mini-batch randomized block coordinate descent method",
      "author" : [ "Zhao" ],
      "venue" : null,
      "citeRegEx" : "Zhao,? \\Q2014\\E",
      "shortCiteRegEx" : "Zhao",
      "year" : 2014
    }, {
      "title" : "2015), where constant η0 and σ0 specify the scale and speed of decay",
      "author" : [ "t+σ0 (Reddi" ],
      "venue" : null,
      "citeRegEx" : ".Reddi,? \\Q2015\\E",
      "shortCiteRegEx" : ".Reddi",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "References [Agarwal and Duchi 2011] Agarwal, A.",
      "startOffset" : 11,
      "endOffset" : 35
    }, {
      "referenceID" : 1,
      "context" : "[Dean et al. 2012] Dean, J.",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 2,
      "context" : "[Feyzmahdavian, Aytekin, and Johansson 2015] Feyzmahdavian, H.",
      "startOffset" : 0,
      "endOffset" : 44
    }, {
      "referenceID" : 3,
      "context" : "[Johnson and Zhang 2013] Johnson, R.",
      "startOffset" : 0,
      "endOffset" : 24
    }, {
      "referenceID" : 6,
      "context" : "[Liu and Wright 2015] Liu, J.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 7,
      "context" : "[Liu et al. 2013] Liu, J.",
      "startOffset" : 0,
      "endOffset" : 17
    }, {
      "referenceID" : 8,
      "context" : "[Liu, Wright, and Sridhar 2014] Liu, J.",
      "startOffset" : 0,
      "endOffset" : 31
    }, {
      "referenceID" : 9,
      "context" : "[Mania et al. 2015] Mania, H.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 12,
      "context" : "[Reddi et al. 2015] Reddi, S.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 13,
      "context" : "[Shalev-Shwartz and Tewari 2011] Shalev-Shwartz, S.",
      "startOffset" : 0,
      "endOffset" : 32
    }, {
      "referenceID" : 16,
      "context" : "[Wright 2015] Wright, S.",
      "startOffset" : 0,
      "endOffset" : 13
    }, {
      "referenceID" : 17,
      "context" : "[Xiao and Zhang 2014] Xiao, L.",
      "startOffset" : 0,
      "endOffset" : 21
    } ],
    "year" : 2016,
    "abstractText" : "Regularized empirical risk minimization (R-ERM) is an important branch of machine learning, since it constrains the capacity of the hypothesis space and guarantees the generalization ability of the learning algorithm. Two classic proximal optimization algorithms, i.e., proximal stochastic gradient descent (ProxSGD) and proximal stochastic coordinate descent (ProxSCD) have been widely used to solve the R-ERM problem. Recently, variance reduction technique was proposed to improve ProxSGD and ProxSCD, and the corresponding ProxSVRG and ProxSVRCD have better convergence rate. These proximal algorithms with variance reduction technique have also achieved great success in applications at small and moderate scales. However, in order to solve large-scale RERM problems and make more practical impacts, the parallel version of these algorithms are sorely needed. In this paper, we propose asynchronous ProxSVRG (Async-ProxSVRG) and asynchronous ProxSVRCD (Async-ProxSVRCD) algorithms, and prove that Async-ProxSVRG can achieve near linear speedup when the training data is sparse, while AsyncProxSVRCD can achieve near linear speedup regardless of the sparse condition, as long as the number of block partitions are appropriately set. We have conducted experiments on a regularized logistic regression task. The results verified our theoretical findings and demonstrated the practical efficiency of the asynchronous stochastic proximal algorithms with variance reduction.",
    "creator" : "LaTeX with hyperref package"
  }
}