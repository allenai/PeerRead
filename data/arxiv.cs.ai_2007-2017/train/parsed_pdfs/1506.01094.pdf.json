{
  "name" : "1506.01094.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Traversing Knowledge Graphs in Vector Space",
    "authors" : [ "Kelvin Gu", "John Miller", "Percy Liang" ],
    "emails" : [ "kgu@stanford.edu", "millerjp@stanford.edu", "pliang@cs.stanford.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Broad-coverage knowledge bases such as Freebase (Bollacker et al., 2008) support a rich array of reasoning and question answering applications, but they are known to suffer from incomplete coverage (Min et al., 2013). For example, as of May 2015, Freebase has an entity Tad Lincoln (Abraham Lincoln’s son), but does not have his ethnicity. An elegant solution to incompleteness is using vector space representations: Controlling the dimensionality of the vector space forces generalization to new facts (Nickel et al., 2011; Nickel et al., 2012; Socher et al., 2013; Riedel et al., 2013; Neelakantan et al., 2015). In the example, we would hope to infer Tad’s ethnicity from the ethnicity of his parents.\nHowever, what is missing from these vector space models is the original strength of knowledge bases: the ability to support compositional queries (Ullman, 1985). For example, we might ask what the ethnicity of Abraham Lincoln’s daughter would be. This can be formulated as a path query on the knowledge graph, and we would like a method that can answer this efficiently, while generalizing over missing facts and even missing or hypothetical entities (Abraham Lincoln did not in fact have a daughter).\nIn this paper, we present a scheme to answer path queries on knowledge bases by “compositionalizing” a broad class of vector space models that have been used for knowledge base completion (see Figure 1). At a high level, we interpret the base vector space model as implementing a soft edge traversal operator. This operator can then be recursively applied to predict paths. Our interpretation suggests a new compositional training objective that encourages better modeling of\nar X\niv :1\n50 6.\n01 09\n4v 1\n[ cs\n.C L\n] 3\nJ un\n2 01\n5\npaths. Our technique is applicable to a broad class of composable models that includes the bilinear model (Nickel et al., 2011) and TransE (Bordes et al., 2013).\nWe have two key empirical findings: First, we show that compositional training enables us to answer path queries up to at least length 5 by substantially reducing cascading errors present in the base vector space model. Second, we find that somewhat surprisingly, compositional training even improves upon state-of-the-art performance for knowledge base completion, which is a special case of answering unit length path queries. Therefore, compositional training can also be seen as a new form of structural regularization for existing models."
    }, {
      "heading" : "2 Task",
      "text" : "We now give a formal definition of the task of answering path queries on a knowledge base. Let E be a set of entities, and R a set of binary relations. A knowledge graph G is defined as a set of triples of the form (s, r, t) where s, t ∈ E and r ∈ R. An example of a triple in Freebase is (tad lincoln, parents, abraham lincoln).\nA path query q consists of an initial anchor entity, s, followed by a sequence of relations to be traversed, p = (r1, . . . , rk). The answer or denotation of the query, JqK, is the set of all entities that can be reached from s by traversing p. Formally, this can be defined recursively:\nJsK def= {s}, (1)\nJq/rK def= {t : ∃s ∈ JqK, (s, r, t) ∈ G} . (2)\nFor example, tad lincoln/parents/location is a query q that asks: “Where did Tad Lincoln’s parents live?”.\nFor evaluation (see Section 5 for details), define the set of candidate answers to a query C(q) as the set of all entities that “type match”, namely those that participate in the last relation at least once; let N (q) be the incorrect answers:\nC (s/r1/ · · · /rk) def = {t | ∃e, (e, rk, t) ∈ G} (3)\nN (q) def= C (q) \\JqK. (4)\nKnowledge base completion. Knowledge base completion (KBC) is the task of predicting whether a given edge (s, r, t) belongs in the graph or not. This can be formulated as a path query q = s/r with candidate answer t."
    }, {
      "heading" : "3 Compositionalization",
      "text" : "In this section, we show how to compositionalize existing KBC models to answer path queries. We start with a motivating example in Section 3.1, then present the general technique in Section 3.2. This suggests a new compositional training objective, described in Section 3.3. Finally, we illustrate the technique for several more models in Section 3.4, which we use in our experiments."
    }, {
      "heading" : "3.1 Example",
      "text" : "A common vector space model for knowledge base completion is the bilinear model. In this model, we learn a vector xe ∈ Rd for each entity e ∈ E and a matrix Wr ∈ Rd×d, for each relation r ∈ R. Given a query s/r (asking for the set of entities connected to s via relation r), the bilinear model scores how likely t ∈ Js/rK using\nscore(s/r, t) = x>s Wrxt. (5)\nTo motivate our compositionalization technique, suppose we let d = |E| and choose Wr to be the adjacency matrix for each relation r and every entity vector xe to be an indicator vector (with a 1 in the entry corresponding to entity e). To answer a path query q = s/r1/ . . . /rk, we would then compute\nscore(q, t) = x>s Wr1 . . .Wrkxt. (6)\nIt is easy to verify that the score counts the number of unique paths between s and t, following relations r1/ . . . /rk. Hence, any t with positive score is a correct answer (JqK = {t : score(q, t) > 0}).\nLet us interpret (6) recursively. The model begins with an entity vector xs, and sequentially applies traversal operators Tri(v) = v>Wri for each ri. Each traversal operation results in a new “set vector” representing the entities reached at that point in traversal (corresponding to the nonzero entries of the set vector). Finally, it applies the membership operator M(v, xt) = v>xt to check if t ∈ Js/r1/ . . . /rkK. Writing graph traversal in this way immediately suggests a useful generalization: take d much smaller than |E| and learn the parameters Wr and xe."
    }, {
      "heading" : "3.2 General technique",
      "text" : "The strategy used to extend the bilinear model of (5) to the compositional model in (6) can be applied to any composable model: namely, one that\nhas a scoring function of the form:\nscore(s/r, t) = M(Tr(xs), xt) (7)\nfor some choice of membership operator M : Rd× Rd → R and traversal operator Tr : Rd → Rd.\nWe can now define the vector denotation of a query JqKV analogous to the definition of JqK in (1) and (2):\nJsKV def = xs, (8) Jq/rKV def = Tr (JqKV) . (9)\nThe score function for a compositionalized model is then\nscore(q, t) = M(JqKV, JtKV). (10)\nWe would like JqKV to approximately represent the set JqK in the sense that for every e ∈ JqK, M (JqKV, JeKV) is larger than the values for e 6∈ JqK. Of course it is not possible to represent all sets perfectly, but in the next section, we present a training objective that explicitly optimizes T and M to preserve path information."
    }, {
      "heading" : "3.3 Compositional training",
      "text" : "The score function in (10) naturally suggests a new compositional training objective. Let {(qi, ti)}Ni=1 denote a set of path query training examples with path lengths ranging from 1 to L. We minimize the following margin objective:\nJ(Θ) = N∑ i=1 ∑ t′∈N (qi) [ 1−margin(qi, ti, t′) ] + ,\nmargin(q, t, t′) = score(q, t)− score(q, t′),\nwhere the parameters are the membership operator, the traversal operators, and the entity vectors:\nΘ = {M} ∪ {Tr : r ∈ R} ∪ { xe ∈ Rd : e ∈ E } .\nNote that this objective explicitly encourages the notion of “set vectors”: because there are path queries of different lengths and types, the model must learn to produce an accurate set vector JqKV after any sequence of traversals. Another perspective is that each traversal operator is trained such that its transformation preserves any information in the set vector which might be needed in subsequent traversal steps.\nIn contrast, previously proposed training objectives for knowledge base completion only train on\nqueries of path length 1. We will refer to this as single-edge training.\nIn Section 5, we show that compositional training leads to substantially better results for both path query answering and knowledge base completion. In Section 6, we provide insight into why."
    }, {
      "heading" : "3.4 Other composable models",
      "text" : "There are many possible candidates for T and M. For example, T could be one’s favorite neural network mapping from Rd to Rd. Here, we focus on two composable models that were both recently shown to achieve state-of-the-art performance on knowledge base completion.\nTransE. The TransE model of Bordes et al. (2013) uses the scoring function\nscore(s/r, t) = −‖xs + wr − xt‖22. (11)\nwhere xs, wr and xt are all d-dimensional vectors. In this case, the model can be expressed using membership operator M(v, xt) = −‖v − xt‖22 and traversal operator Tr(xs) = xs + wr. Hence, TransE can handle a path query q = s/r1/r2/ · · · /rk using\nscore(q, t) = −‖xs + wr1 + · · ·+ wrk − xt‖ 2 2.\n(12)\nWe visualize the compositional TransE model in Figure 2.\nBilinear-Diag. The Bilinear-Diag model of Yang et al. (2015) is a special case of the bilinear model with the relation matrices constrained to be diagonal. Alternatively, the model can be viewed as a variant of TransE with multiplicative interactions between entity and relation vectors.\nNot all models can be compositionalized. It is important to point out that some models are not naturally composable—for example, the matrix factorization model of Riedel et al. (2013) and the neural tensor network of Socher et al. (2013)."
    }, {
      "heading" : "3.5 Implementation",
      "text" : "We use AdaGrad (Duchi et al., 2010) to optimize J(Θ). We initialize all parameters with i.i.d. Gaussians of variance 0.1 in every entry. Initialization scale, mini-batch size and step size were cross-validated for all models. We first train on path queries of length 1 until convergence and then train on all path queries until convergence. This\nguarantees that the model masters basic edges before composing them to form paths. When training on path queries, we explicitly parameterize inverse relations. For the bilinear model, we initialize Wr−1 with W>r . For TransE, we initialize wr−1 with −wr. For Bilinear-Diag, we found initializing wr−1 with the exact inverse 1/wr is numerically unstable, so we instead randomly initialize wr−1 with i.i.d Gaussians of variance 0.1 in every entry. Additionally, for the bilinear model, we found that replacing the sum over N (qi) in the objective with a max yielded slightly better performance. Our code is available at http:// anonymous."
    }, {
      "heading" : "4 Datasets",
      "text" : "In Section 4.1, we describe two standard base datasets consisting of edge queries. Then in Section 4.2, we generate path query datasets from these base datasets."
    }, {
      "heading" : "4.1 Base datasets",
      "text" : "Our experiments are conducted using the subsets of WordNet and Freebase evaluated in Socher et al. (2013). The statistics of these datasets and their splits are given in Table 1.\nThe WordNet and Freebase subsets exhibit substantial differences that can influence model performance. The Freebase subset is almost bipartite with most of the edges taking the form (person, r, t) for some person, relation r and property t. In WordNet, however, both the source and target entities are arbitrary.\nBoth WordNet and Freebase contain many relations that are almost perfectly correlated with an inverse relation. For example, WordNet contains both has part and part of, and the Freebase subset contains both parents and children. At test time, a query involving an edge (s, r, t) is easy if the inverse triple (t, r−1, s) was observed in the training set. Socher et al. (2013) accounted for this by excluding such “trivial” queries from their test set."
    }, {
      "heading" : "4.2 Path query datasets",
      "text" : "Given a base knowledge graph, we generate path queries by performing random walks on the graph. If we view compositional training as a form of regularization, this approach allows us to generate extremely large amounts of training data to prevent overfitting. The procedure is given below.\nLet Gtrain be the training graph, which consists\nonly of the edges in the training set of the base dataset. We then repeatedly generate training examples with the following procedure:\n1. Uniformly sample a path length L ∈ [1, Lmax], and uniformly sample a starting entity, s ∈ E .\n2. Perform a random walk beginning at entity s and continuing L steps.\n(a) At step i of the walk, choose a relation ri uniformly from the set of relations touching the current entity e. (b) Choose the next entity uniformly from the set of entities reachable via ri.\n3. Output a query-answer pair, (q, t), where q = s/r1/ · · · /rL and t is the final entity of the random walk.\nIn practice, we do not sample paths of length 1 and instead directly add all of the edges from Gtrain to the path query dataset.\nTo generate a path query test set, we repeat the above procedure except using the graph Gfull, which is Gtrain plus all of the test edges from the base dataset. Finally, we remove any queries from the test set that also appeared in the training set. The statistics for the path query datasets are presented in Table 1."
    }, {
      "heading" : "5 Main results",
      "text" : "We evaluate the models derived in Section 3 on two tasks: path query answering and knowledge base completion. On both tasks, we show that the compositional training strategy proposed in Section 3.3 leads to substantial performance gains over standard single-edge training. We also compare directly against the KBC results of Socher et al. (2013), demonstrating that previously inferior models now match or outperform state of the art models after compositional training.\nEvaluation metric. Numerous metrics have been used to evaluate knowledge base queries, including hits at 10 (percentage of correct answers ranked in the top 10) and mean rank. We evaluate on hits at 10, as well as a normalized version of mean rank, mean quantile, which better accounts for the total number of candidates. For a query q, the quantile of a correct answer t is the fraction of incorrect answers ranked after t:\n|{t′ ∈ N (q) : score(q, t′) < score(q, t)}| |N (q)| (13)\nThe quantile ranges from 0 to 1, with 1 being optimal. Mean quantile is then defined to be the average quantile score over all examples in the dataset. To illustrate why normalization is important, consider a set of queries regarding the relation gender. A model that predicts the incorrect gender on every query will receive a high mean rank of 2 (since there are only 2 candidate answers), whereas mean quantile is 0.\nAs a final note, several of the queries in the Freebase path dataset are “type-match trivial” in the sense that all of the type matching candidates C(q) are correct answers to the query. In this case, mean quantile is undefined and we exclude such queries from evaluation.\nOverview. The upper half of Table 2 shows that compositional training improves path querying performance across all models and metrics on both datasets, reducing error by up to 76.2%.\nThe lower half of Table 2 shows that surprisingly, compositional training also improves performance on knowledge base completion across almost all models, metrics and datasets. On WordNet, TransE benefits the most, with a 43.3% reduction in error. On Freebase, Bilinear benefits\nthe most, with a 38.8% reduction in error. In terms of mean quantile, the best overall model is TransE (COMP). In terms of hits at 10, the best model on WordNet is Bilinear (COMP), while the best model on Freebase is TransE (COMP).\nDeduction and Induction. Table 3 takes a deeper look at performance on path query answering. We divided path queries into two subsets: deduction and induction. The deduction subset consists of queries q = s/p where the source and target entities are connected via path p in the training graph Gtrain, but the specific query q was never seen during training. Such queries can be answered by performing explicit traversal on the training graph, so this subset tests a model’s ability to approximate the underlying training graph and predict the existence of a path from a collection of single edges. The induction subset consists of all other queries. This means that at least one edge was missing on all paths p from source to target in the training graph. Hence, this subset tests a model’s generalization ability and its robustness to missing edges.\nPerformance on the deduction subset of the dataset is disappointingly low for models trained with single-edge training: they struggle to answer path queries even when all edges in the path query have been seen at training time. Compositional training dramatically reduces these errors, sometimes doubling mean quantile. In Section 6, we analyze how this can be possible. After compositional training, performance on the harder induction subset is also much stronger. Even when edges are missing along a path, the models are able to infer them.\nInterpretable queries. Although our path datasets consists of random queries, both datasets contain a large number of useful, interpretable queries. Results on a few illustrative examples are shown in Table 4.\nComparison with Socher et al. (2013). Here, we measure performance on the KBC task in terms of the accuracy metric of Socher et al. (2013). This evaluation involves sampled negatives, and is hence noisier than mean quantile, but makes our results directly comparable to Socher et al. (2013). Our results show that previously inferior models can outperform state of the art models after compositional training.\nSocher et al. (2013) proposed modeling each entity vector as the average of word vectors making up the entity. Table 5 reports results when using this approach in conjunction with compositional training. We did not use this approach in any of our other experiments.\nTable 5 shows that compositionally trained models outperform the neural tensor network (NTN) on WordNet, while being only slightly behind on Freebase. When the strategy of averaging word vectors to form entity vectors is not applied (EV), compositional models are strictly better. It is worth noting that in many domains, entity names do not contain lexically useful features, so word vector averaging is not always viable."
    }, {
      "heading" : "6 Analysis",
      "text" : "In this section, we try to understand why compositional training works. For concreteness, everything is described in terms of the bilinear model. We will refer to the compositionally trained model as COMP, and the model trained with single-edge training as SINGLE."
    }, {
      "heading" : "6.1 Why does compositional training improve path query answering?",
      "text" : "It is tempting to think that if SINGLE has accurately modeled individual edges in a graph, it should accurately model the paths that result from those edges. This intuition turns out to be incorrect, as revealed by SINGLE’s relatively weak performance on the path query dataset. We hypothesize that this is due to cascading errors along the path. For a given edge (s, r, t) on the path, single-edge training encourages xt to be closer to x>s Wr than any other incorrect xt′ . However, once this is achieved\nby a margin of 1, it does not push xt any closer to x>s Wr. The remaining discrepancy is noise which gets added at each step of path traversal. This is illustrated schematically in Figure 2.\nTo observe this phenomenon empirically, we examine how well a model handles each intermediate step of a path query. We can do this by measuring the reconstruction quality (RQ) of the set vector produced after each traversal operation. Since each intermediate stage is itself a valid path query, we define RQ to be the average quantile over all entities that belong in JqK:\nRQ (q) = 1 |JqK| ∑ t∈JqK quantile (q, t) (14)\nWhen all entities in JqK are ranked above all incorrect entities, RQ is 1. In Figure 3, we illustrate how RQ changes over the course of a query."
    }, {
      "heading" : "6.2 Why does compositional training improve knowledge base completion?",
      "text" : "Table 2 reveals that COMP also performs better on the single-edge task of knowledge base completion. This is somewhat surprising, since SINGLE is trained on a training set which distributionally matches the test set, whereas COMP is not. However, COMP’s better performance on path queries suggests that there must be another factor at play. At a high level, training on paths provides structural regularization which reduces cascading errors. Paths in a knowledge graph have proven to\nbe important features for predicting the existence of single edges (Lao et al., 2011). For example, consider the following Horn clause:\nparents (x, y)∧ location (y, z)⇒ place of birth (x, z) ,\nwhich states that if x has a parent with location z, then x has place of birth z. The body of the Horn clause expresses a path from x to z. If COMP models the path better, then it is better able to use that knowledge to infer the head of the Horn clause.\nMore generally, consider Horn clauses of the form p ⇒ r, where p = r1/ . . . /rk is a path and r is the relation being predicted. We will focus on Horn clauses with high precision as defined by:\nprec(p) = |JpK ∩ JrK| |JpK| (15)\nwhere JpK is the set of entity pairs connected by p, and similarly for JrK.\nOne way for the model to implicitly learn and exploit such a Horn clause would be to meet the following two criteria:\n1. It should ensure a consistent spatial relationship between entity pairs that are related by the path p; that is, keeping x>s Wr1 . . .Wrk close to xt for all valid (s, t) pairs. 2. Its representation of the path p and relation r should capture that spatial relationship; that is, x>s Wr1 . . .Wrk ≈ xt implies x>s Wr ≈ xt, or simply Wr1 . . .Wrk ≈Wr.\nWe have already empirically seen that SINGLE does not meet condition 1, because cascading errors cause it to put incorrect entity vectors xt′ closer to x>s Wr1 . . .Wrk than the correct entity.\nWe empirically verified that COMP also does a better job of meeting condition 2. Fixing r, define the “distance” between path p and relation r, dist(p), as the angle between their corresponding matrices (treated as vectors in Rd2). This is a natural measure of similarity because x>s Wrxt computes the matrix inner product between Wr and xsx > t . Hence, any matrix with small distance from Wr will produce nearly the same scores as Wr, for the same entity pairs.\nIf COMP is better at capturing the correlation between p and r, then we expect that when prec(p) is high, compositional training should shrink dist(p) more. To confirm this hypothesis, we enumerated over all 676 possible paths of length 2 (including inverted relations), and examined the proportional reduction in dist(p) caused by compositional training,\n∆dist(p) = distCOMP(p)− distSINGLE(p)\ndistSINGLE(p) . (16)\nFigure 4 shows that higher precision paths indeed correspond to larger reductions in dist(p)."
    }, {
      "heading" : "7 Related work",
      "text" : "Knowledge base completion with vector space models. Many models have been proposed for knowledge base completion (Bordes et al., 2013; Yang et al., 2015; Socher et al., 2013), relation extraction (Riedel et al., 2013), and knowledge fusion (Dong et al., 2014). Our compositional training technique is an orthogonal improvement that could help any composable model.\nDistributional compositional semantics. Previous works have explored compositional vector space representations in the context of sentence interpretation (Socher et al., 2012; Socher et al., 2014; Grefenstette, 2013; Bowman et al., 2014). We demonstrate that compositional representations are also useful in the context of knowledge base querying and completion.\nPath modeling. Numerous methods have been proposed to incorporate path information into knowledge base completion and question answering. Nickel et al. (2014), Gardner et al. (2014), Neelakantan et al. (2015) and Bordes et al. (2014)\nincorporate paths as features. Some of these models use vector spaces to determine random walk probabilities or to embed paths. Our approach is unique in using paths as a form of structural regularization, which can be applied across different models. Perozzi et al. (2014) make use of random walks as training examples, with a different goal to classify nodes in a social network."
    }, {
      "heading" : "8 Discussion",
      "text" : "We introduced the task of answering path queries on an incomplete knowledge base, and presented a general technique for compositionalizing a broad class of vector space models. Our experiments show that compositional training leads to state-ofthe-art performance on both path query answering and knowledge base completion.\nThere are several key ideas from this paper: regularization by augmenting the dataset with paths, representing sets as low-dimensional vectors in a context-sensitive way, and performing function composition using vectors. We believe these could have greater applicability in the development of vector space models for inference."
    } ],
    "references" : [ {
      "title" : "Freebase: a collaboratively created graph database for structuring human knowledge",
      "author" : [ "K. Bollacker", "C. Evans", "P. Paritosh", "T. Sturge", "J. Taylor." ],
      "venue" : "International Conference on Management of Data (SIGMOD), pages 1247–1250.",
      "citeRegEx" : "Bollacker et al\\.,? 2008",
      "shortCiteRegEx" : "Bollacker et al\\.",
      "year" : 2008
    }, {
      "title" : "Translating embeddings for modeling multi-relational data",
      "author" : [ "A. Bordes", "N. Usunier", "A. Garcia-Duran", "J. Weston", "O. Yakhnenko." ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS), pages 2787–2795.",
      "citeRegEx" : "Bordes et al\\.,? 2013",
      "shortCiteRegEx" : "Bordes et al\\.",
      "year" : 2013
    }, {
      "title" : "Question answering with subgraph embeddings",
      "author" : [ "A. Bordes", "S. Chopra", "J. Weston." ],
      "venue" : "Empirical Methods in Natural Language Processing (EMNLP).",
      "citeRegEx" : "Bordes et al\\.,? 2014",
      "shortCiteRegEx" : "Bordes et al\\.",
      "year" : 2014
    }, {
      "title" : "Recursive neural networks can learn logical semantics",
      "author" : [ "S.R. Bowman", "C. Potts", "C.D. Manning." ],
      "venue" : "CoRR, (0).",
      "citeRegEx" : "Bowman et al\\.,? 2014",
      "shortCiteRegEx" : "Bowman et al\\.",
      "year" : 2014
    }, {
      "title" : "Knowledge vault: A web-scale approach to probabilistic knowledge fusion",
      "author" : [ "X. Dong", "E. Gabrilovich", "G. Heitz", "W. Horn", "N. Lao", "K. Murphy", "T. Strohmann", "S. Sun", "W. Zhang." ],
      "venue" : "International Conference on Knowledge Discovery and Data Min-",
      "citeRegEx" : "Dong et al\\.,? 2014",
      "shortCiteRegEx" : "Dong et al\\.",
      "year" : 2014
    }, {
      "title" : "Adaptive subgradient methods for online learning and stochastic optimization",
      "author" : [ "J. Duchi", "E. Hazan", "Y. Singer." ],
      "venue" : "Conference on Learning Theory (COLT).",
      "citeRegEx" : "Duchi et al\\.,? 2010",
      "shortCiteRegEx" : "Duchi et al\\.",
      "year" : 2010
    }, {
      "title" : "Incorporating vector space similarity in random walk inference over knowledge bases",
      "author" : [ "M. Gardner", "P. Talukdar", "J. Krishnamurthy", "T. Mitchell." ],
      "venue" : "Empirical Methods in Natural Language Processing (EMNLP).",
      "citeRegEx" : "Gardner et al\\.,? 2014",
      "shortCiteRegEx" : "Gardner et al\\.",
      "year" : 2014
    }, {
      "title" : "Towards a formal distributional semantics: Simulating logical calculi with tensors",
      "author" : [ "E. Grefenstette." ],
      "venue" : "arXiv preprint arXiv:1304.5823.",
      "citeRegEx" : "Grefenstette.,? 2013",
      "shortCiteRegEx" : "Grefenstette.",
      "year" : 2013
    }, {
      "title" : "Random walk inference and learning in a large scale knowledge base",
      "author" : [ "N. Lao", "T. Mitchell", "W.W. Cohen." ],
      "venue" : "Empirical Methods in Natural Language Processing (EMNLP), pages 529–539.",
      "citeRegEx" : "Lao et al\\.,? 2011",
      "shortCiteRegEx" : "Lao et al\\.",
      "year" : 2011
    }, {
      "title" : "Distant supervision for relation extraction with an incomplete knowledge base",
      "author" : [ "B. Min", "R. Grishman", "L. Wan", "C. Wang", "D. Gondek." ],
      "venue" : "North American Association for Computational Linguistics (NAACL), pages 777–782.",
      "citeRegEx" : "Min et al\\.,? 2013",
      "shortCiteRegEx" : "Min et al\\.",
      "year" : 2013
    }, {
      "title" : "Compositional vector space models for knowledge base completion",
      "author" : [ "A. Neelakantan", "B. Roth", "A. McCallum." ],
      "venue" : "Association for Computational Linguistics (ACL).",
      "citeRegEx" : "Neelakantan et al\\.,? 2015",
      "shortCiteRegEx" : "Neelakantan et al\\.",
      "year" : 2015
    }, {
      "title" : "A three-way model for collective learning on multirelational data",
      "author" : [ "M. Nickel", "V. Tresp", "H. Kriegel." ],
      "venue" : "International Conference on Machine Learning (ICML), pages 809–816.",
      "citeRegEx" : "Nickel et al\\.,? 2011",
      "shortCiteRegEx" : "Nickel et al\\.",
      "year" : 2011
    }, {
      "title" : "Factorizing YAGO",
      "author" : [ "M. Nickel", "V. Tresp", "H. Kriegel." ],
      "venue" : "World Wide Web (WWW).",
      "citeRegEx" : "Nickel et al\\.,? 2012",
      "shortCiteRegEx" : "Nickel et al\\.",
      "year" : 2012
    }, {
      "title" : "Reducing the rank in relational factorization models by including observable patterns",
      "author" : [ "M. Nickel", "X. Jiang", "V. Tresp." ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS), pages 1179–1187.",
      "citeRegEx" : "Nickel et al\\.,? 2014",
      "shortCiteRegEx" : "Nickel et al\\.",
      "year" : 2014
    }, {
      "title" : "Deepwalk: Online learning of social representations",
      "author" : [ "B. Perozzi", "R. Al-Rfou", "S. Skiena." ],
      "venue" : "International Conference on Knowledge Discovery and Data Mining (KDD), pages 701–710.",
      "citeRegEx" : "Perozzi et al\\.,? 2014",
      "shortCiteRegEx" : "Perozzi et al\\.",
      "year" : 2014
    }, {
      "title" : "Relation extraction with matrix factorization and universal schemas",
      "author" : [ "S. Riedel", "L. Yao", "A. McCallum." ],
      "venue" : "North American Association for Computational Linguistics (NAACL).",
      "citeRegEx" : "Riedel et al\\.,? 2013",
      "shortCiteRegEx" : "Riedel et al\\.",
      "year" : 2013
    }, {
      "title" : "Semantic compositionality through recursive matrix-vector spaces",
      "author" : [ "R. Socher", "B. Huval", "C.D. Manning", "A.Y. Ng." ],
      "venue" : "Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP/CoNLL), pages",
      "citeRegEx" : "Socher et al\\.,? 2012",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2012
    }, {
      "title" : "Reasoning with neural tensor networks for knowledge base completion",
      "author" : [ "R. Socher", "D. Chen", "C.D. Manning", "A. Ng." ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS), pages 926–934.",
      "citeRegEx" : "Socher et al\\.,? 2013",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2013
    }, {
      "title" : "Grounded compositional semantics for finding and describing images with sentences",
      "author" : [ "R. Socher", "A. Karpathy", "Q.V. Le", "C.D. Manning", "A.Y. Ng." ],
      "venue" : "Transactions of the Association for Computational Linguistics (TACL), 2:207–218.",
      "citeRegEx" : "Socher et al\\.,? 2014",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2014
    }, {
      "title" : "Implementation of logical query languages for databases",
      "author" : [ "J.D. Ullman." ],
      "venue" : "ACM Transactions on Database Systems (TODS), 10(3):289–321.",
      "citeRegEx" : "Ullman.,? 1985",
      "shortCiteRegEx" : "Ullman.",
      "year" : 1985
    }, {
      "title" : "Embedding entities and relations for learning and inference in knowledge bases",
      "author" : [ "B. Yang", "W. Yih", "X. He", "J. Gao", "L. Deng." ],
      "venue" : "arXiv preprint arXiv:1412.6575.",
      "citeRegEx" : "Yang et al\\.,? 2015",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Broad-coverage knowledge bases such as Freebase (Bollacker et al., 2008) support a rich array of reasoning and question answering applications, but they are known to suffer from incomplete coverage (Min et al.",
      "startOffset" : 48,
      "endOffset" : 72
    }, {
      "referenceID" : 9,
      "context" : ", 2008) support a rich array of reasoning and question answering applications, but they are known to suffer from incomplete coverage (Min et al., 2013).",
      "startOffset" : 133,
      "endOffset" : 151
    }, {
      "referenceID" : 11,
      "context" : "An elegant solution to incompleteness is using vector space representations: Controlling the dimensionality of the vector space forces generalization to new facts (Nickel et al., 2011; Nickel et al., 2012; Socher et al., 2013; Riedel et al., 2013; Neelakantan et al., 2015).",
      "startOffset" : 163,
      "endOffset" : 273
    }, {
      "referenceID" : 12,
      "context" : "An elegant solution to incompleteness is using vector space representations: Controlling the dimensionality of the vector space forces generalization to new facts (Nickel et al., 2011; Nickel et al., 2012; Socher et al., 2013; Riedel et al., 2013; Neelakantan et al., 2015).",
      "startOffset" : 163,
      "endOffset" : 273
    }, {
      "referenceID" : 17,
      "context" : "An elegant solution to incompleteness is using vector space representations: Controlling the dimensionality of the vector space forces generalization to new facts (Nickel et al., 2011; Nickel et al., 2012; Socher et al., 2013; Riedel et al., 2013; Neelakantan et al., 2015).",
      "startOffset" : 163,
      "endOffset" : 273
    }, {
      "referenceID" : 15,
      "context" : "An elegant solution to incompleteness is using vector space representations: Controlling the dimensionality of the vector space forces generalization to new facts (Nickel et al., 2011; Nickel et al., 2012; Socher et al., 2013; Riedel et al., 2013; Neelakantan et al., 2015).",
      "startOffset" : 163,
      "endOffset" : 273
    }, {
      "referenceID" : 10,
      "context" : "An elegant solution to incompleteness is using vector space representations: Controlling the dimensionality of the vector space forces generalization to new facts (Nickel et al., 2011; Nickel et al., 2012; Socher et al., 2013; Riedel et al., 2013; Neelakantan et al., 2015).",
      "startOffset" : 163,
      "endOffset" : 273
    }, {
      "referenceID" : 19,
      "context" : "However, what is missing from these vector space models is the original strength of knowledge bases: the ability to support compositional queries (Ullman, 1985).",
      "startOffset" : 146,
      "endOffset" : 160
    }, {
      "referenceID" : 11,
      "context" : "Our technique is applicable to a broad class of composable models that includes the bilinear model (Nickel et al., 2011) and TransE (Bordes et al.",
      "startOffset" : 99,
      "endOffset" : 120
    }, {
      "referenceID" : 1,
      "context" : ", 2011) and TransE (Bordes et al., 2013).",
      "startOffset" : 19,
      "endOffset" : 40
    }, {
      "referenceID" : 1,
      "context" : "The TransE model of Bordes et al. (2013) uses the scoring function",
      "startOffset" : 20,
      "endOffset" : 41
    }, {
      "referenceID" : 20,
      "context" : "The Bilinear-Diag model of Yang et al. (2015) is a special case of the bilinear model with the relation matrices constrained to be diagonal.",
      "startOffset" : 27,
      "endOffset" : 46
    }, {
      "referenceID" : 15,
      "context" : "It is important to point out that some models are not naturally composable—for example, the matrix factorization model of Riedel et al. (2013) and the neural tensor network of Socher et al.",
      "startOffset" : 122,
      "endOffset" : 143
    }, {
      "referenceID" : 15,
      "context" : "It is important to point out that some models are not naturally composable—for example, the matrix factorization model of Riedel et al. (2013) and the neural tensor network of Socher et al. (2013).",
      "startOffset" : 122,
      "endOffset" : 197
    }, {
      "referenceID" : 5,
      "context" : "We use AdaGrad (Duchi et al., 2010) to optimize J(Θ).",
      "startOffset" : 15,
      "endOffset" : 35
    }, {
      "referenceID" : 16,
      "context" : "Our experiments are conducted using the subsets of WordNet and Freebase evaluated in Socher et al. (2013). The statistics of these datasets and their splits are given in Table 1.",
      "startOffset" : 85,
      "endOffset" : 106
    }, {
      "referenceID" : 16,
      "context" : "Socher et al. (2013) accounted for this by excluding such “trivial” queries from their test set.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 16,
      "context" : "We also compare directly against the KBC results of Socher et al. (2013), demonstrating that previously inferior models now match or outperform state of the art models after compositional training.",
      "startOffset" : 52,
      "endOffset" : 73
    }, {
      "referenceID" : 16,
      "context" : "Comparison with Socher et al. (2013). Here, we measure performance on the KBC task in terms of the accuracy metric of Socher et al.",
      "startOffset" : 16,
      "endOffset" : 37
    }, {
      "referenceID" : 16,
      "context" : "Comparison with Socher et al. (2013). Here, we measure performance on the KBC task in terms of the accuracy metric of Socher et al. (2013). This evaluation involves sampled negatives, and is hence noisier than mean quantile, but makes our results directly comparable to Socher et al.",
      "startOffset" : 16,
      "endOffset" : 139
    }, {
      "referenceID" : 16,
      "context" : "Comparison with Socher et al. (2013). Here, we measure performance on the KBC task in terms of the accuracy metric of Socher et al. (2013). This evaluation involves sampled negatives, and is hence noisier than mean quantile, but makes our results directly comparable to Socher et al. (2013). Our results show that previously inferior models can outperform state of the art models after compositional training.",
      "startOffset" : 16,
      "endOffset" : 291
    }, {
      "referenceID" : 8,
      "context" : "be important features for predicting the existence of single edges (Lao et al., 2011).",
      "startOffset" : 67,
      "endOffset" : 85
    }, {
      "referenceID" : 1,
      "context" : "Many models have been proposed for knowledge base completion (Bordes et al., 2013; Yang et al., 2015; Socher et al., 2013), relation extraction (Riedel et al.",
      "startOffset" : 61,
      "endOffset" : 122
    }, {
      "referenceID" : 20,
      "context" : "Many models have been proposed for knowledge base completion (Bordes et al., 2013; Yang et al., 2015; Socher et al., 2013), relation extraction (Riedel et al.",
      "startOffset" : 61,
      "endOffset" : 122
    }, {
      "referenceID" : 17,
      "context" : "Many models have been proposed for knowledge base completion (Bordes et al., 2013; Yang et al., 2015; Socher et al., 2013), relation extraction (Riedel et al.",
      "startOffset" : 61,
      "endOffset" : 122
    }, {
      "referenceID" : 15,
      "context" : ", 2013), relation extraction (Riedel et al., 2013), and knowledge fusion (Dong et al.",
      "startOffset" : 29,
      "endOffset" : 50
    }, {
      "referenceID" : 4,
      "context" : ", 2013), and knowledge fusion (Dong et al., 2014).",
      "startOffset" : 30,
      "endOffset" : 49
    }, {
      "referenceID" : 16,
      "context" : "Previous works have explored compositional vector space representations in the context of sentence interpretation (Socher et al., 2012; Socher et al., 2014; Grefenstette, 2013; Bowman et al., 2014).",
      "startOffset" : 114,
      "endOffset" : 197
    }, {
      "referenceID" : 18,
      "context" : "Previous works have explored compositional vector space representations in the context of sentence interpretation (Socher et al., 2012; Socher et al., 2014; Grefenstette, 2013; Bowman et al., 2014).",
      "startOffset" : 114,
      "endOffset" : 197
    }, {
      "referenceID" : 7,
      "context" : "Previous works have explored compositional vector space representations in the context of sentence interpretation (Socher et al., 2012; Socher et al., 2014; Grefenstette, 2013; Bowman et al., 2014).",
      "startOffset" : 114,
      "endOffset" : 197
    }, {
      "referenceID" : 3,
      "context" : "Previous works have explored compositional vector space representations in the context of sentence interpretation (Socher et al., 2012; Socher et al., 2014; Grefenstette, 2013; Bowman et al., 2014).",
      "startOffset" : 114,
      "endOffset" : 197
    }, {
      "referenceID" : 7,
      "context" : "Nickel et al. (2014), Gardner et al.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 4,
      "context" : "(2014), Gardner et al. (2014), Neelakantan et al.",
      "startOffset" : 8,
      "endOffset" : 30
    }, {
      "referenceID" : 4,
      "context" : "(2014), Gardner et al. (2014), Neelakantan et al. (2015) and Bordes et al.",
      "startOffset" : 8,
      "endOffset" : 57
    }, {
      "referenceID" : 1,
      "context" : "(2015) and Bordes et al. (2014) Figure 4: We divide paths of length 2 into high precision (> 0.",
      "startOffset" : 11,
      "endOffset" : 32
    }, {
      "referenceID" : 14,
      "context" : "Perozzi et al. (2014) make use of random walks as training examples, with a different goal to classify nodes in a social network.",
      "startOffset" : 0,
      "endOffset" : 22
    } ],
    "year" : 2015,
    "abstractText" : "Path queries on a knowledge graph can be used to answer compositional questions such as “What languages are spoken by people living in Lisbon?”. However, knowledge graphs often have missing facts (edges) which disrupts path queries. Recent models for knowledge base completion impute missing facts by embedding knowledge graphs in vector spaces. We show that these models can be recursively applied to answer path queries, but that they suffer from cascading errors. This motivates a new “compositional” training objective, which dramatically improves all models’ ability to answer path queries, in some cases more than doubling accuracy. On a standard knowledge base completion task, we also demonstrate that compositional training acts as a novel form of structural regularization, reliably improving performance across all base models (reducing errors by up to 43%) and achieving new state-of-the-art results.",
    "creator" : "TeX"
  }
}