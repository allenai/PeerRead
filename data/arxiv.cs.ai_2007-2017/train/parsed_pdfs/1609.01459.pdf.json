{
  "name" : "1609.01459.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Deviant Learning Algorithm: Learning Sparse Mismatch Representations through Time and Space",
    "authors" : [ ],
    "emails" : [ "viaemeka@ust.edu.ng", "nd.osegi@sure-gp.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Predictive coding (PDC) has recently attracted attention in the neuroscience and computing community as a candidate unifying paradigm for neuronal studies and artificial neural network implementations particularly targeted at unsupervised learning systems. The Mismatch Negativity (MMN) has also recently been studied in relation to PC and found to be a useful ingredient in neural predictive coding systems. Backed by the behavior of living organisms, such networks are particularly useful in forming spatio-temporal transitions and invariant representations of the input world. However, most neural systems still do not account for large number of synapses even though this has been shown by a few machine learning researchers as an effective and very important component of any neural system if such a system is to behave properly. Our major point here is that PDC systems with the MMN effect in addition to a large number of synapses can greatly improve any neural learning system's performance and ability to make decisions in the machine world. In this paper, we propose a novel bio-mimetic computational intelligence algorithm – the Deviant Learning Algorithm, inspired by these key ideas and functional properties of recent brain-cognitive discoveries and theories. We also show by numerical experiments guided by theoretical insights, how our invented bio-mimetic algorithm can achieve competitive predictions with even with very small problem specific data.\nKeywords: Artificial Neural Networks, Deviant Learning, Integer Programming, Mismatch Negativity, Predictive Coding, and Temporal Learning\n*Corresponding Author"
    }, {
      "heading" : "1. Introduction",
      "text" : "It is well known fact that brain-like learning in machine oriented systems have significant benefits in real world and synthetic data problems. However, the most suitable brain-cognitive approach to the data learning problem still remains a primary problem and have troubled researchers world-wide. While some algorithms do very well in certain tasks, they perform poorly in others confirming the “No-Free Lunch Theorem” (NFLT). Nevertheless, competitive machine learning systems that employ brain-like algorithms have evolved over time and present a promising alternative to a wide variety of machine learning algorithms. Two algorithms stand out as major candidates for future ML systems. The Cortical Learning Algorithms (CLA) proposed in (Hawkins et al, 2011) and the Long Short-Term Memory (LSTM) in (Hochreiter and Schmidhuber, 1998) have shown promising results in recent times, however, HTM-CLA seems to model closely the intelligent behavior of cortical micro-circuits. In this paper, we introduce yet another model of brain-like learning. This model is inspired by the Mismatch Negativity (MMN) effect elicited in real humans. HTM-CLAs (HTM-CLAs) have the unique ability to predict its own activation and they offer a generative procedure for building cellular data; Cellular data is a very important component of hierarchical memory-based learning systems as they allow the unique predictive capabilities found in most cortical generative models).\nHTM-CLA do not account for MMN effect. Specifically they do not use our deviant (Real Absolute Deviation (RAD) modeling approach. As described in the Numenta white paper (Numenta, 2007), postpredictive ability with slowness through time is a very essential component of machine learning systems with intelligent abilities. Our model possess the unique ability to enforce slowness through time. Our model incorporates the ideas of predictive coding with the MMN effect in (Liedler et al, 2013) to evolve a deviant model of the world. It also encourages the use of the overlap concept as in HTM-CLA with good postpredictive abilities.\nOur primary goal here is to develop a model algorithm that builds on the strengths of CLA. Such a model should also account for the MMN effect as this can prove useful in a wide range of computational problems.\nThis paper is organized as follows:\nIn section 2 we briefly review related works that inspired this paper. Brain-like theories and ideas for deeper understanding of this paper is presented in Section 3 while Section 4 describes the Deviant Learning algorithm (DLA) based on the MMN effect. Next, we present and describe comparative results on the IRIS, HEART and Word-Similarity benchmark datasets using our DLA and the HTM-CLA. Finally, we conclude this paper with a discussion and suggest ideas that will further this work."
    }, {
      "heading" : "2. Related Works",
      "text" : "Over the years, brain-like learning in artificial neural networks (ANNs) have been described by many artificial intelligence (AI) researchers and various models proposed as the quest for a better machine intelligence close to human reasoning continues worldwide. More importantly, several AI approaches have recently captured the interest of the artificial intelligence community some of which include slow feature learning in (Wiskott and Sejnowski, 2002), the deep generative models in (Hinton, 2007) and deep recurrent models of (Schmidhuber, 1992; Schmidhuber et al 2012 and Graves et al, 2013), history compression models with a focus on unexpected inputs during learning (Schmidhuber, 1992), the spatiotemporal hierarchichal representations in (Hawkins et al, 2010) including the concept of sparse distributed representations as in (Ahmad and Hawkins, 2015). Also worth mentioning is the spiking neuron familiarity and frequency models in (Bogacz et al, 2000), predictive coding model in (Bastos et al, 2012) and neurocomputational models based on the Mismatch Negativity effect (Liedler, 2013b). While all these models have one thing in common - they all employ some form of connecting link for processing sensory information, they still fall short of real human brain requirements. One interesting feature to note is the inability of most of these models to account for large number of synapses in the neocortex, the seat of intelligence in the brain - see for instance the argument in (Hawkins and Ahmad, 2016). Nevertheless, arguments and counter-arguments have far reaching consequences and is still not sufficient to describe in algorithmic terms the primary requirements of a brain-like algorithm in software. Thus, the AI field is still at a loss at the most appropriate way to tackle the intelligent machine problem."
    }, {
      "heading" : "3. Machine Brain-like Effects and Novel Theories",
      "text" : "Brain-like theories such as discussed extensively in (Hawkins and Blakeslee, 2007) as a form of a memoryprediction framework, in (Hinton et al, 2006) as deep generative connectionist learning and as recurrent learning networks with Long Short-Term Memory in (Hochreiter and Schmidhuber, 1997; Gers et al, 1999) have had profound effect in the solution approaches used in today’s AI software systems. One interesting feature of some of these theories is the ability of machine-life algorithms to remember and forget - an analogue of the birth-death theory. We strongly believe in this behavior as artificial life machines try to respond to its world by the formation of new memories and the extinction of past memories. This recreative process have strong genetic roots and is beyond the scope of this paper to describe every detail. In this section we discuss on some very important theoretical principles behind our deviant learning algorithm while inducing some novel ideologies in the process."
    }, {
      "heading" : "3.1 MisMatch Negataivity (MMN) Effect",
      "text" : "The Mismatch negativity effect (MMN) effect first discovered in the context of auditory stimulation in (Naatanen et al, 1978) is one possible neurobiological plausible attempt to model the behaviour of real\nworld neural tissue in a statistical way. It has also been studied in the context of visual modalities (see Pazo-Alvarez et al, 2003). In particular the MMN effect seeks to validate the differential response in the brain. Some important MMN theories may be found in (Liedler et al, 2013b). However, as discussed in (Liedler et al, 2013a), MMN theories are still debatable and not entirely conclusive up to this present moment. Some of the identified theories include (Liedler et al, 2013a): the Change Detection Hypothesis (CDH), Adaptation Hypothesis (AH), Model Adjustment Hypothesis (MAH), Novelty Detection Hypothesis (NDH) and the Prediction Error Hypothesis (PEH)."
    }, {
      "heading" : "3.2 Reticular Formation and Synaptic Response",
      "text" : "Responses to sensory impulses have historically been defined by the reticular forming units which serve as some sort of familiarity discrimination threshold detector with possible hypothalamic gating abilities (Kimball, 1964). This functionality also shares some resemblance to the notion of permanence introduced in (Hawkins et al, 2010).Permanence have a depolarizing effect on the synapses leading to the state of “potential synapse” i.e. synapses with a high likelihood of being connected. Thus, when the synapse connects, the neuron or cell fires."
    }, {
      "heading" : "3.3 Incremental Learning and the Growing of Synapses",
      "text" : "During post-predictive stage it is likely that the input exceeds the peak(s) of the generated units. In this situation it becomes necessary to grow more synapses in order to improve the learned observations in the neural ecosystem. This is achieved in the DLA using the parameter referred to as the learning extent. As an organism grows in age the learning extent should correspondingly increase. Thus, there is a total annihilation of previous primitive states to begin to learn more complex tasks."
    }, {
      "heading" : "4. Deviant Learning Algorithm (DLA)",
      "text" : "The Deviant Learning Algorithm (DLA) is approached using a systematic mathematical procedure. DLA in CPU-like structure is as shown in Figure 1. It is divided into two core phases - the pre-prediction and the post-prediction phases. During pre-predictions, invariant representations are learned by performing an input mismatch through a long generative list of well-defined standards. For the purposes of this study, the standards are assumed to be integer-valued representations of the input chain. Pseudo-code for the DLA is provided in Appendix 1."
    }, {
      "heading" : "4.1 Pre-Prediction Phase",
      "text" : "Here we perform first-order and second-order mismatch operations with the assumptions that the inputs are single-dimensional (1-D) matrices or vectors. These operations are described in the following sub-sections.\n4.1.1 Mismatch:\nSuppose a permanence threshold range is defined as\n1\n:\n\n\no\no\n.\nThen, for every input deviant, we obtain a first-order (level-1) mismatch using a Real Absolute Deviation (RAD) as:\n    \n\notherwise\nnllnik\nIIk\nextdev\nlgidev\n0\n,,...,,2,1,1\n||\n1)1(\n)()1(\n (1)\n.where,\n)( lg\nI .is a long list of generative integers limited by an explicit threshold, extl and,\niI .is the source input for each exemplar at time step, t"
    }, {
      "heading" : "4.1.2 Overlap",
      "text" : "The overlap (Deviant Overlap) is described as a summation over )1(devk , as frequent patterns of ones\n(binary 1’s) - superimposed into a virtual store say oS , conditioned on )1(devk and is defined as below:\n    \n\notherwise\nk\nSS\ndev\noo jtjt\n0\n11\n1\n)1(\n),(),(\n(2)\nFrom which we obtain:\n \n \n1)1(\n)(\ndev\nt\nk\noo SS\n(3)\n.and\n))max(max( )()( tt oo SS \n(4)"
    }, {
      "heading" : "4.1.3 Winner Integer Selection:",
      "text" : "In order to identify the integer(s) responsible for the observation(s), \n)( to S is conditioned on a threshold say,\n1hT defined as a recurrent operation in the positive sense i.e. this time we select only those integers for which \n)( to S is greater than or equal to 1hT . The casual observers is described through the process of\ninhibition and by the recurrent operation:\n    \n\n\n\n0\n1\n)(\n1\nho\naoao\nTSz\nCC nn\n\n(5)"
    }, {
      "heading" : "4.1.4 Learning:",
      "text" : "We perform learning using a Hebbian-Hopfield update style conditioned on the maximum value of\n\n)( to S and a time-limit say it . We also additionally encourage diversity by adding some random noise to the\npermanence update procedure defined as:\n\n    \n\n\notherwise\nTSttS hoiom\no\n0\n)( 2lim,1\n11\nmaxmax *\n\n(6)\n.where limt is the specified time limit\n2hT , is a threshold level\n*m  , represents an optional randomized hyperbolic tangent activation function adapted from (Anireh and Osegi, 2016).\nTo assure the update procedure during learning, a Monte Carlo run is performed on this routine twice.\nWe may decide to stop learning by setting 01  , but we believe that learning is a continual process and there is no end in sight except when the system is dead."
    }, {
      "heading" : "4.1.5 Inference:",
      "text" : "Inference is achieved by first computing a second-order (level-2) mismatch - a prediction on its own activation. This process is described as follows:\n\n\nn\ndev\ncgsdev\nav\ni\nkn\nIIk\n)2(\n)()2( ||\n(7)\n.where\nicg I )( = winning integers obtained from (5) and\nsI = a new input sequence at the next time-step\nNote that the average mismatch at level-2 is defined as:\n n devdev kk av )2()2( (8)\n.from which we obtain:\n n devr kk )2( (9)\n.where rk is a rating factor that can be used to evaluate inference performance over time.\nUsing (7) and (5), a predictive interpolation is performed as:\n2 )( .\n0\n,...,2,1),min( ),( )2()2(\nsr r\ndevdevao\naor\nIP Pand\notherwise\nnnikkC iCP\n \n    \n\n\n(10)\nNote that r T r PP  for ideal matches.\nA memorization process is finally performed to extract all successful integer matches by a superimposition operation on the virtual store as:\n  \n\n\n\nni\nni\nPiS rth\n,\n...,,2,1\n),( ( \n(11)\nNote that the prediction rP is updated by a counter defined by i at each time step."
    }, {
      "heading" : "4.2 Post-Prediction Phase",
      "text" : "In post-predictive phase, we perform a third and final-order mismatch operation (level-3)\nconditioned on a new permanence threshold, say, 2 as:\n    \n\notherwise\nj\njSIk hcurrent tsdev\n0\n10,,1\n|),(|\nlimlim 222\n)3(\n\n\n(12)\nWe also compute a row-wise overlap as:\n \n  )(\n1\n)3(\n2\n)(\n\n\ndevk kS t\n(13)\nFinally, we extract the possible match from the memory store by a conditional selection procedure using a prefix search (Graves et al, 2006):\n\n  \n\n \n\n\notherwise\nTjiTSi\njSP\nhphkp\nithpostr p\n3,3\n)()(\n,,\n,\n(14)\n3hT , is computed as:\n2 3 loTh  (15)\n.where lo is the length of novel deviants.\nIn practical terms, this is typically a backward pass for the exemplar at the previous time-step."
    }, {
      "heading" : "5. Experiments and Results",
      "text" : "Experiments have been performed using three benchmark datasets for the classification and similarity estimation problems. The first two datasets are specifically a classification problem; these are the IRIS dataset which is a plant species dataset (Bache and Lichman, 2013) for categorizing the Iris plant into three different classes, the HEART dataset (Blake et al, 1998) for categorizing a heart condition as either an absence or a presence of a heart disease and the Word-Similarity dataset (Finkelstein, 2001) for estimating similarity scores.\nWe begin by defining some initial experimental parameters for the HTM-CLA and the DLA. For the HTMCLA, we build a hierarchy of cells using a Monte-Carlo simulation constrained by their corresponding permanence values. This modification is less computationally intensive than the technique of combinatorics\nproposed in (Ahmad and Hawkins, 2015). The parameters for the simulation experiments are given in Appendix 2. The classification accuracies for HTM-CLA and DLA are presented in Tables 1 and 2 respectively. We use a classification metric termed the mean absolute percentage classification accuracy (MAPCA). MAPCA is similar to the mean absolute percentage error (MAPE) used in (Cui et al, 2015). MAPCA is computed as:\n100*\n)|ˆ(| )()(    \n\n\n    \n \n \nz\nn\ni\nii\nn\ntolyy\nMAPCA (1)\n.where y = the observed data exemplars\nŷ = the model’s predictions of y\nzn = size of the observation matrix, and\ntol = a tolerance constraint.\nTable1. Classification accuracies for the DLA\nIRIS Dataset HEART Dataset Word-Similarity Dataset\nPercent accuracy (%) 77.03 75.07 79.35\nTable2. Classification accuracies for the DLA\nIRIS Dataset HEART Dataset Word-Similarity Dataset\nPercent accuracy (%) 86 70 72.5"
    }, {
      "heading" : "6. Discussions",
      "text" : "The results show a competitive performance for both the HTM-CLA and DLA using the specified parameters – see Appendix 2. While the DLA outperformed the HTM-CLA in the IRIS dataset test, the HTM-CLA outperformed the DLA on the HEART and Word-Similarity datasets. This may be attributed to variations in parameters and HTM-CLA’s highly sparse representation.\n7. Conclusion and Future Work\nA novel algorithm for cortical learning based on the MMN effect has been developed. Our model can learn long range context, classify and predict forward in time. It is possible to implement forgetting and remembering by dynamically adjusting the learning extent. This will be examined in a future version of this paper."
    }, {
      "heading" : "Acknowledgement",
      "text" : "Source codes for the HTM-CLA and DLA will be made available at the Matlab central website:\nwww.matlabcentral.com\nAppendix 1: DLA Pseudocode\nInitialize time counter, time limits ( nCounter ), memory store, and permanence threshold\n1Counter\nGenerate a long list of Integers (standards),  nniI lg ,...1, )(\nFOR each exemplar\nEvaluate 1 st order Mismatch: )1(devk // Pre-prediction Phase\nCompute Deviant-Overlap\nSelect Winner Integers\nStore Winner Integers\nUpdate permanence threshold // Learning\nEvaluate 2 nd\norder Mismatch: )2(devk\nPerform a predictive interpolation in time, t\nIf )2(devk is less than min ( )2(devk )\nExtract the causal Integers // Integers maximally responsible for the memorization process\nEvaluate 3 rd\norder Mismatch: )3(devk // Post-predictive phase\nCompute Deviant-Overlap\nExtract novel memories from store\n1CounterCounter \nUntil Counter = nCounter\nAppendix 2: Experimental Parameters\nHTM-CLA uses a different architecture from the DLA with different set of parameters. HTM-CLA parameters for the IRIS, HEART and Word Similarity Datasets is provided in Table 3 while that of the DLA is given in Table 4."
    } ],
    "references" : [ {
      "title" : "Properties of sparse distributed representations and their application to hierarchical temporal memory",
      "author" : [ "S. Ahmad", "J. Hawkins" ],
      "venue" : "arXiv preprint arXiv:1503.07469",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2015
    }, {
      "title" : "A Modified Activation Function with Improved Run-Times For Neural Networks",
      "author" : [ "V.I. Anireh", "E.N. Osegi" ],
      "venue" : "arXiv preprint arXiv:1607.01691",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2016
    }, {
      "title" : "Canonical microcircuits for predictive",
      "author" : [ "A.M. Bastos", "W.M. Usrey", "R.A. Adams", "G.R. Mangun", "P. Fries", "K.J. Friston" ],
      "venue" : "coding. Neuron,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2012
    }, {
      "title" : "Frequency-based error backpropagation in a cortical network",
      "author" : [ "R. Bogacz", "M.W. Brown", "C. Giraud-Carrier" ],
      "venue" : "In Neural Networks,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2000
    }, {
      "title" : "UCI} Repository of machine learning databases",
      "author" : [ "C. Blake", "E. Keogh", "C.J. Merz" ],
      "venue" : null,
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1998
    }, {
      "title" : "Continuous online sequence learning with an unsupervised neural network model",
      "author" : [ "Y. Cui", "C. Surpur", "S. Ahmad", "J. Hawkins" ],
      "venue" : "arXiv preprint arXiv:1512.05463",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2015
    }, {
      "title" : "Placing search in context: The concept revisited",
      "author" : [ "L. Finkelstein", "E. Gabrilovich", "Y. Matias", "E. Rivlin", "Z. Solan", "G. Wolfman", "Ruppin", "April" ],
      "venue" : "In Proceedings of the 10th international conference on World Wide Web (pp. 406-414)",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2001
    }, {
      "title" : "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks",
      "author" : [ "A. Graves", "S. Fernández", "F. Gomez", "Schmidhuber", "June" ],
      "venue" : "In Proceedings of the 23rd international conference on Machine learning (pp. 369-376)",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2006
    }, {
      "title" : "Speech recognition with deep recurrent neural networks",
      "author" : [ "A. Graves", "A.R. Mohamed", "Hinton", "May" ],
      "venue" : "IEEE international conference on acoustics, speech and signal processing (pp. 6645-6649)",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2013
    }, {
      "title" : "Why neurons have thousands of synapses, a theory of sequence memory in neocortex",
      "author" : [ "J. Hawkins", "S. Ahmad" ],
      "venue" : "Frontiers in neural circuits,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2016
    }, {
      "title" : "Learning multiple layers of representation",
      "author" : [ "G.E. Hinton" ],
      "venue" : "Trends in cognitive sciences,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2007
    }, {
      "title" : "A fast learning algorithm for deep belief nets",
      "author" : [ "G.E. Hinton", "S. Osindero", "Y.W. Teh" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2006
    }, {
      "title" : "Hierarchical temporal memory including HTM cortical learning algorithms",
      "author" : [ "J. Hawkins", "S. Ahmad", "D. Dubinsky" ],
      "venue" : "Techical report,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2010
    }, {
      "title" : "Modelling trial-bytrial changes in the mismatch negativity",
      "author" : [ "F. Lieder", "J. Daunizeau", "M.I. Garrido", "K.J. Friston", "K.E. Stephan" ],
      "venue" : "PLoS Comput Biol,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2013
    }, {
      "title" : "A neurocomputational model of the mismatch negativity",
      "author" : [ "F. Lieder", "K.E. Stephan", "J. Daunizeau", "M.I. Garrido", "K.J. Friston" ],
      "venue" : "PLoS Comput Biol,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2013
    }, {
      "title" : "Early selective-attention effect on evoked potential reinterpreted",
      "author" : [ "R. Näätänen", "A.W. Gaillard", "S. Mäntysalo" ],
      "venue" : "Acta psychologica,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 1978
    }, {
      "title" : "MMN in the visual modality: a review",
      "author" : [ "P. Pazo-Alvarez", "F. Cadaveira", "E. Amenedo" ],
      "venue" : "Biological psychology,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2003
    }, {
      "title" : "A fixed size storage O (n3) time complexity learning algorithm for fully recurrent continually running networks",
      "author" : [ "J. Schmidhuber" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 1992
    }, {
      "title" : "Learning complex, extended sequences using the principle of history compression",
      "author" : [ "J. Schmidhuber" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 1992
    }, {
      "title" : "Recurrent Neural Networks",
      "author" : [ "J. Schmidhuber", "A. Graves", "F. Gomez", "S. Hochreiter" ],
      "venue" : null,
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2012
    }, {
      "title" : "Slow feature analysis: Unsupervised learning of invariances",
      "author" : [ "L. Wiskott", "T.J. Sejnowski" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2002
    } ],
    "referenceMentions" : [ ],
    "year" : 2016,
    "abstractText" : "Predictive coding (PDC) has recently attracted attention in the neuroscience and computing community as a candidate unifying paradigm for neuronal studies and artificial neural network implementations particularly targeted at unsupervised learning systems. The Mismatch Negativity (MMN) has also recently been studied in relation to PC and found to be a useful ingredient in neural predictive coding systems. Backed by the behavior of living organisms, such networks are particularly useful in forming spatio-temporal transitions and invariant representations of the input world. However, most neural systems still do not account for large number of synapses even though this has been shown by a few machine learning researchers as an effective and very important component of any neural system if such a system is to behave properly. Our major point here is that PDC systems with the MMN effect in addition to a large number of synapses can greatly improve any neural learning system's performance and ability to make decisions in the machine world. In this paper, we propose a novel bio-mimetic computational intelligence algorithm – the Deviant Learning Algorithm, inspired by these key ideas and functional properties of recent brain-cognitive discoveries and theories. We also show by numerical experiments guided by theoretical insights, how our invented bio-mimetic algorithm can achieve competitive predictions with even with very small problem specific data.",
    "creator" : "Microsoft® Office Word 2007"
  }
}