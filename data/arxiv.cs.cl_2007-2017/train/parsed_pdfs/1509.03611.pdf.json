{
  "name" : "1509.03611.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "The Haifa Corpus of Translationese",
    "authors" : [ "Ella Rabinovich", "Shuly Wintner", "Ofek Luis Lewinsohn" ],
    "emails" : [ "ellarabi@csweb.haifa.ac.il", "shuly@cs.haifa.ac.il", "o.l.lewinsohn@gmail.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Keywords Translationese · Parallel corpora · Statistical Machine Translation"
    }, {
      "heading" : "1 Introduction",
      "text" : "Research in all areas of language and linguistics is stimulated by the unprecedented availability of data. Research in Translation Studies typically requires parallel corpora: texts paired with their translation into a target language.1\nE. Rabinovich Department of Computer Science, University of Haifa. E-mail: ellarabi@csweb.haifa.ac.il\nS. Wintner Department of Computer Science, University of Haifa. E-mail: shuly@cs.haifa.ac.il\nO.L. Lewinsohn Universität des Saarlandes. E-mail: o.l.lewinsohn@gmail.com\n1 Until recently, only a small proportion of research in translation studies used corpora (Sun and Shreve, 2013, Figure 3). Still, several corpus-based studies have been reported (Baker, 1993, 1995, 1996; Al-Shabab, 1996; Laviosa, 1998, 2002; Olohan, 2004; Becher, 2011; Zanettin, 2013) and this seems to be the standard methodology today.\nar X\niv :1\n50 9.\n03 61\n1v 1\n[ cs\n.C L\n] 1\n1 Se\np 20\n15\nParallel corpora are instrumental for exploring the unique features of translationese: the sub-language of translated texts (in any given language) that is presumably distinctly different from the language of texts originally written in the same language. Parallel corpora are also crucial resources for Statistical Machine Translation (SMT), as they provide the data from which the parameters of translation models can be estimated. We introduce in this paper a parallel corpus specifically designed for translationese research.\nA specific task that the corpus will help address is automatic identification of translationese: the task is to train classifiers that can accurately distinguish between original and translated texts (in the same language). Interest in this task has increased significantly in recent years (Baroni and Bernardini, 2006; van Halteren, 2008; Kurokawa et al., 2009; Ilisei et al., 2010; Ilisei and Inkpen, 2011; Popescu, 2011; Koppel and Ordan, 2011; Volansky et al., 2015; Avner et al., Forthcoming). The main appeal of this task is that it sheds new light on the features of translationese, and in particular, can help corroborate (or refute) several hypotheses of translation studies (Ilisei, 2013; Volansky et al., 2015). Moreover, this task also has practical applications: the ability to identify translationese can help improve the quality of SMT systems. It has been shown that language models compiled from translated texts are better than ones compiled from original texts (Lembersky et al., 2011, 2012b), and that translation models compiled from parallel texts translated in the same direction as that of the SMT task are better than parallel texts translated in the opposite direction (Ozdowska and Way, 2009; Kurokawa et al., 2009; Lembersky et al., 2012a, 2013). To identify the direction of translation of parallel texts, Eetemadi and Toutanova (2014, 2015) employed bilingual text properties. They took advantage of sentence pairs translated in both directions for training a supervised classifier to identify translationese at the sentence-level. Obviously, this specific task requires parallel corpora with a reliable indication of the translation direction.\nMost of the works cited above used a single corpus for training and evaluation. Unfortunately, much evidence shows that the subtle features of translationese are overshadowed by the much more prominent features of specific datasets, such as the domain, genre, register, or epoch of the texts. For example, Koppel and Ordan (2011) showed that while the overall accuracy of their translationese identification classifiers is near-perfect, when a classifier is trained on translations from one language, but is tested on translations from a different language, accuracy drops significantly. Popescu (2011) demonstrated that a classifier, trained to discriminate between original British literature and English literature translated from French, failed to predict translation status of English literature written by American authors and translations from German. Avner et al. (Forthcoming) reported that while they were able to identify translationese with high accuracy within a given corpus (literary texts in Hebrew and Hebrew-translated-from-English), the classification was much less accurate when either the genre of the texts was modified (popular science vs. literature) or the source language was changed (French instead of English). Similar trends were noted by Rabinovich and Wintner (2015), showing a dra-\nmatic drop in the accuracy of classification when the dataset is changed, even when other parameters (such as the domain or the source language of the translations) are held constant. These observations call for a diverse corpus, representing various genres, domains, registers and source languages, for a reliable investigation of translationese.\nA good corpus for research into the properties of translationese, especially with an eye to improving the quality of machine translation through insights from translation studies, should ideally satisfy the following desiderata:\nDiversity The corpus should ideally reflect diverse genres, registers, authors, modality (written vs. spoken) etc. Parallelism The corpus should include both the source and its translation, so that features that are revealed in the translation can be traced back to their origins in the source. As opposed to parallel, the term comparable refers to a bilingual corpus where texts typically belong to the same domain, rather than are strict translations of each other. Multilinguality Having translations from several source languages to the same target language facilitates a closer inspection of properties that are language-pair-specific vs. more “universal” features of translationese (Baker, 1993; House, 2008; Laviosa, 2008; Becher, 2010). Uniformity Whatever processing is done on the texts, it must be done uniformly. This includes sentence boundary detection, tokenization, sentenceand word-alignment, part-of-speech (POS) tagging, etc. One-to-one translations While in general it would be interesting to inspect one-to-many and many-to-one sentence translations, for the purposes of training SMT systems one-to-one sentence translations are usually preferred. Availability Finally, corpora that are used for research must be publicly available so that other researchers have the opportunity to replicate and corroborate research results.\nWe describe a cross-domain, parallel, uniform, English-French and EnglishGerman corpora that were compiled specifically for research of translationese. The entire corpus is freely available;2 some subsets have already been used in a pilot study on unsupervised identification of translationese (Rabinovich and Wintner, 2015). We describe the structure of the corpus in Section 2, explain how it was processed in Section 3, and evaluate it through replicating some state-of-the-art experiments in Section 4. We conclude with suggestions for future extensions."
    }, {
      "heading" : "2 Corpus structure",
      "text" : "The Haifa Corpus of Translationese consists of five sub-corpora: Europarl, Hansard, literature, TED and political commentary.3 All are parallel cor-\n2 All corpora are available at http://cl.haifa.ac.il/projects/translationese. 3 We use “EUR”, “HAN”, “LIT”, “TED” and “POL” to denote the five corpora in tables\nand graphs hereafter.\npora, with accurate annotation indicating the direction of the translation. The datasets are uniformly pre-processed, represented, and organized. All corpora were further filtered to contain solely one-to-one sentence-alignments, which are more useful for the SMT research. Tables 1 and 2 report some statistical data on the corpus (after tokenization).We further detail each sub-corpus in this section; Section 3 describes the processing applied to the texts, and the way in which they are represented.\n2.1 Europarl\nThe Europarl sub-corpus is extracted from the collection of the proceedings of the European Parliament, dating back to 1996, originally collected by Koehn (2005). The original Europarl corpus4 is organized as several language-pairs, each with multiple sentence-aligned files. We mainly used the English-French and English-German segments, but resorted to other segments as we presently explain. We focus below on the way we generated the English-French subcorpus; its English-German counterpart was obtained in a similar way.\nEuroparl is probably the most popular parallel corpus in natural language processing, and it was indeed used for many of the translationese tasks surveyed in Section 1. Unfortunately, it is a very problematic corpus. First, it consists of transcriptions of spoken utterances that are edited (by the speakers) after they are transcribed; only then are they translated. Consequently, there\n4 We used Version 7, from http://www.statmt.org/europarl/, retrieved August, 2015.\nare significant discrepancies between the actual speeches and their “verbatim” transcriptions (Cucchi, 2012). Second, while “Members of the European Parliament have the right to use any of the EU’s [24] official languages when speaking in Parliament”,5 many of them prefer to speak in English, which is often not their native language.6\nMainly due to its multilingual nature, however, Europarl has been used extensively in SMT (Koehn et al., 2009) and in cross-lingual research (Cartoni et al., 2013). It has even been adapted specifically for research in translation studies: Islam and Mehler (2012) compiled a customized version of Europarl, where the direction of translation is indicated. They used meta-data from the corpus, and in particular the language tag, to identify the original language in which each sentence was spoken, and removed sentence pairs for which this information was missing. A similar strategy was used by Lembersky et al. (2012b) and Cartoni and Meyer (2012). We now detail how we extracted reliable speaker identities from Europarl texts.\nThe Europarl corpus is a collection of several monolingual (parallel) corpora: the original text was uttered in one language and then translated to several other languages. In each sub-corpus, each paragraph is annotated with meta-information, in particular, the original language in which the paragraph was uttered. Unfortunately, the meta-information pertaining to the original language of Europarl utterances is frequently missing. Furthermore, in some cases this information is inconsistent: different languages are indicated as the original languages of (various translations of) the same paragraph (in the various sub-corpora). Additionally, the Europarl corpus includes several bilingual sub-corpora that are generated from the original and the translated texts, and are already sentence-aligned. These bilingual corpora include only raw sentence pairs, with no meta-information.\nTo minimize the risk of erroneous information, we processed the Europarl files as follows. First, we propagated the meta-information from the monolingual texts to the bilingual sub-corpora: each sentence pair was thus annotated with the original language in which it was uttered. We repeated this process five times, using as the source of meta-information the original monolingual corpora in five languages: English, French, German, Italian, and Spanish (note that not all monolingual corpora are identical: some are much larger than others). For the same reason, not all the English-French sentence pairs in our bilingual corpus are reflected in all five monolingual corpora, and therefore some sentence pairs have less than five annotations of the original language. We restricted the bilingual corpus to only those sentence pairs that had five annotations. Then, we filtered out all sentence pairs whose annotations were inconsistent (about 0.5%) We did leave in our corpus sentence pairs where some annotations were missing but the remaining ones were consistent. We\n5 http://europa.eu/about-eu/facts-figures/administration/index_en.htm, retrieved August, 2015.\n6 http://www.theguardian.com/education/datablog/2014/may/21/\neuropean-parliament-english-language-official-debates-data, retrieved August, 2015.\nalso removed comments (about 0.5% as well), typically written in parentheses (things like “applause”, “continuation of the previous session”, etc.) As a result, we are confident that the speaker information in the filtered corpus is highly accurate.\nFinally, we augmented the obtained French-English sentence-aligned bilingual corpus by a file of meta-data that lists, for each sentence pair, the following information:\nCOUNT – sequential line number NAME – speaker name LANGUAGE – original language in which the sentence was uttered SESSION ID – the name of the corresponding protocol source file SEQ SPEAKER ID – sequential number of the speaker within a session\nAs an example, below is the meta-data of a sample sentence pair:\n<LINE COUNT=\"693\" NAME=\"Evans, Jonathan\" LANGUAGE=\"EN\" SESSION_ID=\"ep-00-01-18\" SEQ_SPEAKER_ID=\"28\"/>\nThe resulting format of the bilingual corpus is a set of three files: two of them containing raw English and French sentences, each sentence in a line; and a file of meta-information.\n2.2 Hansard\nThe Hansard corpus is a parallel corpus consisting of transcriptions of the Canadian parliament in (Canadian) English and French from 2001–2009. We used a version that was annotated with the original language of each parallel sentence. This corpus most likely suffers from similar problems as the Europarl corpus discussed above; indeed, Mollin (2007), who investigated the British Hansard parliamentary transcripts, found that “the transcripts omit performance characteristics of spoken language, such as incomplete utterances or hesitations, as well as any type of extrafactual, contextual talk” and that “transcribers and editors also alter speakers’ lexical and grammatical choices towards more conservative and formal variants.” Still, this is the largest available source of English–French sentence pairs.\nIn addition to parliament members’ speech, the original Hansard corpus contains metadata. Various annotations were used to discriminate different line types, including the date of the session, the name of the speaker, etc. We filtered out all segments except those referring to speech, i.e., only sentence pairs annotated by Content ParaText were left in the final version. In total, about 15% of the corpus line-pairs were thus eliminated.\nWe converted the original metadata files format so as to fit it to the Europarl metadata format described in Section 2.1, as follows:\nCOUNT – sequential line number NAME – speaker name\nLANGUAGE – original language in which the sentence was uttered FILENAME – propagated from the monolingual sub-corpora\nFollowing is an example of the annotation of each line in the corpus:\n<LINE COUNT=\"525\" NAME=\"Madeleine Dalphond-Guiral\"\nLANGUAGE=\"FR\" FILENAME=\"37-1-HOUSE-100\"/>\n2.3 Literature\nThis corpus consists of literary classics written and translated in the 18th– 20th centuries by English and French writers. Most of the raw material is available from the Gutenberg project.7 The organization of this sub-corpus is different from the way both Europarl and Hansard are organized. Due to the nature of the corpus, which consists of long texts (books) in a single language, rather than dialogs in which the language can change constantly, we do not provide sentence-level meta-information. Rather, we list in a single file (literature.dat) the complete list of books, annotating by ‘S’ and ‘T’ whether the text is source or translation, respectively, e.g.:\n... S Carroll_Lewis-Alice_in_wonderland.en.aligned.tok T Carroll_Lewis-Alice_in_wonderland.fr.aligned.tok ...\nThe English-German literature corpus was generated in a similar way. We used material from the Gutenberg project, Wikisource,8 and a few more books. Both English–French and English–German datasets contain a metadata file with details about the books: title, year of publication, translator name and year of translation.\n2.4 TED Talks\nTED is a global set of conferences owned a by non-profit foundation under the slogan “Ideas Worth Spreading.” TEDx are independent TED-style events organized all over the world. While TED talks are typically given in English, TEDx events are not restricted to a specific language. The Open Translation Project is a global volunteer effort to subtitle (and translate) TED talks, thus spreading the ideas across languages and borders.\nOur TED talks parallel corpus consists of talks given in English and translated to French and vise versa. The raw material was retrieved via the TED.com API (for TED talks originally given in English), and collected from Youtube\n7 http://www.gutenberg.org 8 http://en.wikisource.org/\n(for TEDx talks originally given in French). Subtitles of talks given in French were extracted with a freely-available tool.9\nThe quality of translation in this corpus is very high: not only are translators assumed to be competent, but the common practice is that each translation passes through a review before being published. This corpus consists of talks delivered orally, but we assume that they were meticulously prepared, so the language is not spontaneous but rather planned. Compared to the other sub-corpora, the TED dataset has some unique characteristics that stem from the following reasons: (i) its size is relatively small; (ii) it exhibits stylistic disparity between the original and translated texts (the former contains more “oral” markers of a spoken language, while the latter is a written translation); and (iii) TED talks are not transcribed but are rather subtitled, so they undergo some editing and rephrasing.10\nThe vast majority of TED talks are publicly available online, which makes this corpus easily extendable for future research. This corpus contains two directories: original.english for parallel files of talks given in English and translated to French, and original.french for those given in French and translated to English.\n2.5 Political News and Commentary\nThis corpus contains articles, commentary and analysis on world affairs and international relations. English articles and their translations to German were collected from Project Syndicate.11 This is a non-profit organization that primarily relies on contributions from newspapers in developed countries. It provides original commentaries by people who are shaping the world’s economics, politics, science and culture. We collected articles categorized as Word Affairs from this project, originally written by English authors and translated to German. Original German commentaries and their translations to English were collected from the Diplomatics Magazine,12 specifically, from its International Relations section.\nThis corpus contains two directories: original.english for articles originally written in English and translated to German, and original.german for those written in German and translated to English."
    }, {
      "heading" : "3 Processing",
      "text" : "The original Europarl corpora are already sentence-aligned, using an implementation of the Gale and Church (1993) sentence-alignment algorithm. Since\n9 http://keepsubs.com/, retrieved August, 2015. 10 http://translations.ted.org/wiki/How_to_Compress_Subtitles, retrieved August,\n2015. 11 http://www.project-syndicate.org/, retrieved August, 2015. 12 http://www.diplomatisches-magazin.de/, retrieved August, 2015.\nthe alignment was done for one source paragraph at a time (typically consisting of few sentences), its quality is very high. The same also holds for the Hansard corpus, so we used the original alignments for both sub-corpora. We then filtered out any alignments that were not one-to-one; this resulted in a loss of about 3% of the alignments in Europarl, and only 2% in Hansard.\nThe literary sub-corpus required more careful attention. Books that were acquired from FarkasTranslations.com were available pre-aligned at the chapter- and paragraph-level; we therefore sentence-aligned them, one paragraph at a time, using a Python implementation (Tan and Bond, 2014) of the standard algorithm of Gale and Church (1993). For the remainder of the books, we first extracted chapters by (manually) identifying characteristic chapter titles (e.g., Roman numerals, explicit “Chapter N”, etc.) Paragraph boundaries within a chapter are typically marked by a double newline, and we used this pattern to break chapters into paragraphs. Due to the fact that the GaleChurch algorithm only utilizes text length for alignment, it can be easily refined for aligning other logical units, e.g., paragraphs (Gale and Church, 1993). Finally, we aligned sentences within paragraphs using the same algorithm.\nThe genre of the literature sub-corpus is very different (presumably due to translators taking greater liberty), hence restricting the dataset to include only one-to-one sentence-alignments resulted in loss of 10-20% of each book.\nSentence-alignment of subtitles of TED talks originally delivered in French (and translated to English) involved synchronization of subtitle frames. A typical frame in a subtitles (.srt) file is structured as follows:\n18 frame sequential number 00:00:47,497 --> 00:00:50,813 frame start and end time Cet engagement, je pense que j’ai fait le choix frame text\nFirst, we re-organized the subtitles file to contain (longer) frames that start and end on a sentence boundary; we achieved this by concatenating frames until a sentence termination punctuation symbol is reached. This procedure was conducted on both French subtitles and their corresponding English translations. Then, we paragraph-aligned the English–French parallel files by alternated concatenation of paragraphs until synchronization of frame end time (up to a δ threshold that was fixed to 500 milliseconds) on the English and French sides. The paragraph-alignment procedure pseudo-code is detailed in Algorithm 1.\nWe further aligned the paragraph-aligned TED and TEDx corpora at the sentence-level using the same sentence-alignment procedure (Gale and Church, 1993).\nTED talks tend to vary greatly in terms of sentence alignments (one-to-one, one-to-many, many-to-one, many-to-many). On average, approximately 10% of the alignments are not one-to-one; those were filtered out as well. Below are a few examples of such pairs:\nAlgorithm 1 TED subtitles paragraph-alignment algorithm Comment: l paragraphs and r paragraphs are (not necessarily equal length) arrays of text paragraphs for alignment, from the left and right sides, respectively\nδ = 500 milliseconds . threshold controlling the allowed delta in aligned frames’ end time\nsubtitles paragraph alignment(1,1) . initial invocation assuming the arrays start from 1\nprocedure subtitles paragraph alignment(l count,r count) if (l count > l paragraphs.length) and (r count > r paragraphs.length) then\nreturn end if if (l count > l paragraphs.length) then\noutput r paragraphs[r count:r paragraphs.length] . remainder of the right side return\nend if if (r count > r paragraphs.length) then\noutput l paragraphs[l count:l paragraphs.length] . remainder of the left side return\nend if\nl current = l paragraphs[l count].frame content l frame end = l paragraphs[l count].frame end r current = r paragraphs[r count].frame content r frame end = r paragraphs[r count].frame end\nwhile (abs(l frame end – r frame end) > δ and (l count < l paragraphs.length) and (r count < r paragraphs.length)) do if (l frame end > r frame end) then . advance on the right side\nr count += 1 r current += r paragraphs[r count].frame content r frame end = r paragraphs[r count].frame end\nelse . advance on the left side l count += 1 l current += l paragraphs[l count].frame content l frame end = l paragraphs[l count].frame end\nend if end while\noutput “aligned paragraph pair:”, l current, r current subtitles paragraph alignment(l count+1,r count+1) . recursive invocation\nend procedure\nEnglish: “Start from what they know. Build on what they have.” French: “Commence par ce qu’ils savent, et construis á partir de ce qu’ils ont.”\nEnglish: “This is available because a Russian space program is starving, and it’s nice for them to get 20 million here and there to take one of the seats.” French: “C’est disponible parce que le programme spatial russe meurt de faim. C’est bien pour eux d’avoir 20 millions ici et lá pour occuper un des siéges.”\nEnglish: “You can really see the details of the dataset. It’s very high-resolution, and it’s our algorithms that allow us to zoom in on all the details.” French: “Vous pouvez vraiment voir les détails de l’ensemble de données; c’est á trés haute définition. Et ce sont nos algorithmes qui nous permettent d’agrandir chaque détail.”\nFor tokenization we used the Stanford tools (Manning et al., 2014). English texts were tokenized according to the PTB-based scheme; the French tokenization scheme is a derivative of PTB tokenization, with extra rules for French elision and compounding."
    }, {
      "heading" : "4 Evaluation",
      "text" : "To validate the quality of the corpus we replicated the experiments of Volansky et al. (2015), who conducted a thorough exploration of supervised classification of translationese, using dozens of feature types. While Volansky et al. (2015) only used the Europarl corpus (in its original format) and worked on English translated from French, we extended the experiments to all the datasets described above, including also English translated from German. We show that in-domain classification (with ten-fold cross-validation evaluation) yields excellent results, implying that the corpora we report on are indeed good datasets for translationese research.\n4.1 Preprocessing and tools\nThe (tokenized) datasets were split into chunks of approximately 2000 tokens, respecting sentence boundaries and preserving punctuation. We assume that translationese features are present in the texts or speeches across author, genre or native language, thus we allow some chunks to contain linguistic information from two or more different speakers simultaneously. The frequency-based features are normalized by the number of tokens in each chunk. For part of speech tagging, we employ the Stanford implementation along with its models for French and English (Manning et al., 2014).\nWe use Platt’s sequential minimal optimization algorithm (Keerthi et al., 2001) to train a support vector machine classifier with the default linear kernel, an implementation freely available in Weka (Hall et al., 2009). In all classification experiments we use (the maximal) equal number of chunks from each class (i.e., original and translated).\n4.2 Features\nWe focused on a set of features that reflect lexical and structural properties of the text, and have been shown to be effective for supervised classification of translationese (Volansky et al., 2015). First, we used function words (FW),\nmore precisely, the same list that was used in previous works on classification of translationese (Koppel and Ordan, 2011; Volansky et al., 2015). Feature values are raw counts (further denoted by term frequency, tf ), normalized by the number of tokens in the chunk; the chunk size may slightly vary, since the chunks respect sentence boundaries.\nIn addition to function words, we experimented with character trigrams, part-of-speech (POS) trigrams, contextual function words and cohesive markers. Contextual function words are a variation of POS trigrams where a trigram can be anchored by specific function words: these are consecutive triplets 〈w1,w2,w3〉 where at least two of the elements are function words, and at most one is a POS tag. Cohesive markers are words or phrases that mark the underlying flow of discourse: they organize a composition of phrases by specifying the type, purpose or direction of upcoming ideas, and can therefore serve as evidence of the translation process. We used the list of 40 cohesive markers defined in Volansky et al. (2015).\nCharacter, POS, and contextual FW trigrams are calculated as detailed in Volansky et al. (2015), but we only considered the 1000 most frequent feature values extracted from each dataset. This subset yielded the same classification quality as the full set, but reduced computation complexity.\n4.3 Results"
    }, {
      "heading" : "4.3.1 Identification of translationese using monolingual (English) corpora",
      "text" : "We first reproduced the classification results of Volansky et al. (2015) on the English vs. English-translated-from-French Europarl dataset, using the feature sets detailed in Section 4.2. We further present results for three additional English-French datasets, as well as for the English-German corpora. Tables 3 and 4 report the ten-fold cross-validation classification accuracy. The EUR* column refers to the classification results of Europarl, as reported by Volansky et al. (2015). Although they experimented with a slightly different setup, we detail these results in Table 3 for illustrative comparison.\nIn line with previous works, the classification results are very high, yielding near-perfect accuracy with most feature sets (with a single exception of cohesive markers, which may be partially attributed to their lower dimension).\nNext we tested the classifier’s sensitivity by varying the chunk size and the number of chunks that are subject to classification. We used only function words (one of the best performing, content-independent features) in these experiments. We excluded the TED dataset from these experiments due to its small size. Figures 1 and 2 report the results.\nClassification accuracy remains stable when the number of chunks used for classification decreases (Figure 1). Evidently, as few as 200 chunks are sufficient for excellent classification;13 some fluctuations in the graphs can be attributed to the random choice of original and translated sections. Kurokawa et al. (2009) reported that the accuracy of classification deteriorated when\n13 The results for LIT are limited by the amount of available data in this dataset.\nthe size of the underlying logical units (here, chunks) decreased; we corroborated this observation, but note that reasonable accuracy (above 80%) can be obtained even with 500-token chunks (Figure 2)."
    }, {
      "heading" : "4.3.2 Identification of translationese using bilingual English-French corpus",
      "text" : "So far we demonstrated the utility of our corpus for classification of original vs. translated texts using monolingual English datasets. However, our corpora are parallel – this facilitates leveraging of additional information for the task of identification of translationese; that is, the decision on a text chunk translation status can be based on both the English side and its French (or German) counterpart.\nIn the next experiment we exploit the bilingual Europarl dataset by generating an extended feature set, based on English function word frequencies (extracted from English chunk), and frequencies of French function words14 extracted from the corresponding French chunk. Overall, for a parallel EnglishFrench text we end up with about 900 feature values. We use the same classifier as specified in Section 4.1 and evaluate it using ten-fold cross-validation.\nFigure 3 depicts the results. Classification accuracy using bilingual information systematically outperforms the classifiers based on monolingual Englishand French-only features, for all chunk sizes. This experiment demonstrates the potential optimization gains of exploitation of a bilingual dataset for identification of translationese. We only conjoined the feature values extracted from both sides; however, more sophisticated exploitation of the bilingual dataset is possible, including features based on sentence- and word-alignment properties. Initial results of using some bilingual text properties for identification of\n14 The list of French function words was obtained from https://code.google.com/p/ stop-words/source/browse/trunk/stop-words, retrieved August, 2015.\ntranslationese were mentioned by Kurokawa et al. (2009) and reported more extensively by Eetemadi and Toutanova (2014, 2015)."
    }, {
      "heading" : "5 Conclusion",
      "text" : "This work presents diverse parallel bilingual English-French and English-German corpora with accurate indication of the translation direction. To evaluate the quality of the corpus, we replicated and expanded on several previous results of supervised identification of translationese, and further extended the experiments to additional datasets, including an additional language pair, and exploiting bilingual information.\nThe potential value of these datasets stimulates further corpus-development and exploratory activities. Our future plans include the extension of the corpus with additional language pairs (in particular, English-Hebrew). We also intend to further pursue the identification of translationese, decreasing the size of the logical units (chunks) that are subject to classification, eventually considering a single sentence pair.\nIt has been shown in a series of works (Kurokawa et al., 2009; Lembersky et al., 2012a, 2013, 2011, 2012b; Twitto-Shmuel et al., 2015) that awareness to translationese has a positive effect on the quality of SMT. Parallel resources presented in this work enable exploitation of bilingual information for the task of identification of translationese. More precisely, the datasets that we compiled can be used for the task of identifying the translation direction of parallel texts; this task has recently been introduced in several works (Eetemadi and Toutanova, 2014, 2015)."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This research was supported by a grant from the Israeli Ministry of Science and Technology. We are grateful to Noam Ordan for much advice and encouragement. We also thank Sergiu Nisioi for helpful suggestions. We are grateful to Philipp Koehn for making the Europarl corpus available; to Cyril Goutte, George Foster and Pierre Isabelle for providing us with an annotated version of the Hansard corpus; to François Yvon and András Farkas15 for contributing their literary corpora; and to the TED OTP team for sharing TED talks and their translations. We thank also Raphael Salkie for sharing his diverse English-German corpus.\n15 http://farkastranslations.com, retrieved August, 2015."
    } ],
    "references" : [ {
      "title" : "Corpus-based translation studies: The challenges that lie ahead",
      "author" : [ "Mona Baker" ],
      "venue" : "research. Target,",
      "citeRegEx" : "Baker.,? \\Q1995\\E",
      "shortCiteRegEx" : "Baker.",
      "year" : 1995
    }, {
      "title" : "A new approach to the study of Translationese",
      "author" : [ "jamins", "Amsterdam", "1996. Marco Baroni", "Silvia Bernardini" ],
      "venue" : null,
      "citeRegEx" : "jamins et al\\.,? \\Q1996\\E",
      "shortCiteRegEx" : "jamins et al\\.",
      "year" : 1996
    }, {
      "title" : "Using the europarl corpus",
      "author" : [ "Bruno Cartoni", "Sandrine Zufferey", "Thomas Meyer" ],
      "venue" : null,
      "citeRegEx" : "Cartoni et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Cartoni et al\\.",
      "year" : 2012
    }, {
      "title" : "Asymmetric features of human",
      "author" : [ "Kristina Toutanova" ],
      "venue" : null,
      "citeRegEx" : "Eetemadi and Toutanova.,? \\Q2012\\E",
      "shortCiteRegEx" : "Eetemadi and Toutanova.",
      "year" : 2012
    }, {
      "title" : "Detecting translation direction: A cross-domain",
      "author" : [ "Sauleh Eetemadi", "Kristina Toutanova" ],
      "venue" : null,
      "citeRegEx" : "Eetemadi and Toutanova.,? \\Q2014\\E",
      "shortCiteRegEx" : "Eetemadi and Toutanova.",
      "year" : 2014
    }, {
      "title" : "A Machine Learning Approach to the Identification of Translational Language",
      "author" : [ ],
      "venue" : null,
      "citeRegEx" : "Ilisei.,? \\Q2008\\E",
      "shortCiteRegEx" : "Ilisei.",
      "year" : 2008
    }, {
      "title" : "Identification of translationese: A machine learning approach",
      "author" : [ "Iustina Ilisei", "Diana Inkpen", "Gloria Corpas Pastor", "Ruslan Mitkov" ],
      "venue" : "Proceedings of CICLing-2010: 11th International Conference on Computational Linguistics and Intelligent Text Processing,",
      "citeRegEx" : "Ilisei et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Ilisei et al\\.",
      "year" : 2010
    }, {
      "title" : "Customization of the Europarl corpus for translation studies",
      "author" : [ "Zahurul Islam", "Alexander Mehler" ],
      "venue" : "In Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC’12). European Language Resources Association (ELRA),",
      "citeRegEx" : "Islam and Mehler.,? \\Q2012\\E",
      "shortCiteRegEx" : "Islam and Mehler.",
      "year" : 2012
    }, {
      "title" : "Improvements to platt’s smo algorithm for svm classifier design",
      "author" : [ "S.S. Keerthi", "S.K. Shevade", "C. Bhattacharyya", "K.R.K. Murthy" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Keerthi et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Keerthi et al\\.",
      "year" : 2001
    }, {
      "title" : "Europarl: A parallel corpus for statistical machine translation",
      "author" : [ "Philipp Koehn" ],
      "venue" : "MT Summit,",
      "citeRegEx" : "Koehn.,? \\Q2005\\E",
      "shortCiteRegEx" : "Koehn.",
      "year" : 2005
    }, {
      "title" : "462 machine translation systems for Europe",
      "author" : [ "Philipp Koehn", "Alexandra Birch", "Ralf Steinberger" ],
      "venue" : "In Proceedings of the Twelfth Machine Translation Summit,",
      "citeRegEx" : "Koehn et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Koehn et al\\.",
      "year" : 2009
    }, {
      "title" : "Translationese and its dialects",
      "author" : [ "Moshe Koppel", "Noam Ordan" ],
      "venue" : "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,",
      "citeRegEx" : "Koppel and Ordan.,? \\Q2011\\E",
      "shortCiteRegEx" : "Koppel and Ordan.",
      "year" : 2011
    }, {
      "title" : "Automatic detection of translated text and its impact on machine translation",
      "author" : [ "David Kurokawa", "Cyril Goutte", "Pierre Isabelle" ],
      "venue" : "In Proceedings of MT-Summit XII,",
      "citeRegEx" : "Kurokawa et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Kurokawa et al\\.",
      "year" : 2009
    }, {
      "title" : "Core patterns of lexical use in a comparable corpus of English lexical prose. Meta",
      "author" : [ "Sara Laviosa" ],
      "venue" : null,
      "citeRegEx" : "Laviosa.,? \\Q1998\\E",
      "shortCiteRegEx" : "Laviosa.",
      "year" : 1998
    }, {
      "title" : "Corpus-based translation studies: theory, findings, applications. Approaches to translation studies",
      "author" : [ "Sara Laviosa" ],
      "venue" : "Rodopi,",
      "citeRegEx" : "Laviosa.,? \\Q2002\\E",
      "shortCiteRegEx" : "Laviosa.",
      "year" : 2002
    }, {
      "title" : "Encyclopedia of Translation Studies, 2nd Edition, pages 288–292",
      "author" : [ "Gabriela Saldanha", "editors", "Routledge" ],
      "venue" : "Routledge (Taylor and Francis),",
      "citeRegEx" : "Baker et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Baker et al\\.",
      "year" : 2008
    }, {
      "title" : "Language models for machine translation: Original vs. translated texts",
      "author" : [ "Gennadi Lembersky", "Noam Ordan", "Shuly Wintner" ],
      "venue" : "In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Lembersky et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Lembersky et al\\.",
      "year" : 2011
    }, {
      "title" : "Adapting translation models to translationese improves SMT",
      "author" : [ "Gennadi Lembersky", "Noam Ordan", "Shuly Wintner" ],
      "venue" : "In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics,",
      "citeRegEx" : "Lembersky et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Lembersky et al\\.",
      "year" : 2012
    }, {
      "title" : "Language models for machine translation: Original vs. translated texts",
      "author" : [ "Gennadi Lembersky", "Noam Ordan", "Shuly Wintner" ],
      "venue" : "Computational Linguistics,",
      "citeRegEx" : "Lembersky et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Lembersky et al\\.",
      "year" : 2012
    }, {
      "title" : "Improving statistical machine translation by adapting translation models to translationese",
      "author" : [ "Gennadi Lembersky", "Noam Ordan", "Shuly Wintner" ],
      "venue" : "Computational Linguistics,",
      "citeRegEx" : "Lembersky et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Lembersky et al\\.",
      "year" : 2013
    }, {
      "title" : "The Stanford CoreNLP natural language processing toolkit",
      "author" : [ "Christopher D. Manning", "Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven J. Bethard", "David McClosky" ],
      "venue" : "In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations,",
      "citeRegEx" : "Manning et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Manning et al\\.",
      "year" : 2014
    }, {
      "title" : "The Hansard hazard: gauging the accuracy of British parliamentary",
      "author" : [ "Sandra Mollin" ],
      "venue" : "transcripts. Corpora,",
      "citeRegEx" : "Mollin.,? \\Q2007\\E",
      "shortCiteRegEx" : "Mollin.",
      "year" : 2007
    }, {
      "title" : "Introducing Corpora in Translation Studies",
      "author" : [ "Maeve Olohan" ],
      "venue" : null,
      "citeRegEx" : "Olohan.,? \\Q2004\\E",
      "shortCiteRegEx" : "Olohan.",
      "year" : 2004
    }, {
      "title" : "Optimal bilingual data for french-english pb-smt",
      "author" : [ "Sylwia Ozdowska", "Andy Way" ],
      "venue" : null,
      "citeRegEx" : "Ozdowska and Way.,? \\Q2009\\E",
      "shortCiteRegEx" : "Ozdowska and Way.",
      "year" : 2009
    }, {
      "title" : "Studying translationese at the character level",
      "author" : [ "Marius Popescu" ],
      "venue" : "Proceedings of RANLP-2011,",
      "citeRegEx" : "Popescu.,? \\Q2011\\E",
      "shortCiteRegEx" : "Popescu.",
      "year" : 2011
    }, {
      "title" : "Unsupervised identification of translationese",
      "author" : [ "Ella Rabinovich", "Shuly Wintner" ],
      "venue" : "Transactions of the Association for Computational Linguistics,",
      "citeRegEx" : "Rabinovich and Wintner.,? \\Q2015\\E",
      "shortCiteRegEx" : "Rabinovich and Wintner.",
      "year" : 2015
    }, {
      "title" : "Reconfiguring translation studies",
      "author" : [ "Sanjun Sun", "Gregory M. Shreve" ],
      "venue" : "Unpublished manuscript,",
      "citeRegEx" : "Sun and Shreve.,? \\Q2013\\E",
      "shortCiteRegEx" : "Sun and Shreve.",
      "year" : 2013
    }, {
      "title" : "NTU-MC toolkit: Annotating a linguistically diverse corpus",
      "author" : [ "Liling Tan", "Francis Bond" ],
      "venue" : "In Proceedings of 25th International Conference on Computational Linguistics (COLING",
      "citeRegEx" : "Tan and Bond.,? \\Q2014\\E",
      "shortCiteRegEx" : "Tan and Bond.",
      "year" : 2014
    }, {
      "title" : "Statistical machine translation and automatic identification of translationese",
      "author" : [ "Naama Twitto-Shmuel", "Noam Ordan", "Shuly Wintner" ],
      "venue" : "In Proceedings of WMT-2015,",
      "citeRegEx" : "Twitto.Shmuel et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Twitto.Shmuel et al\\.",
      "year" : 2015
    }, {
      "title" : "Source language markers in EUROPARL translations",
      "author" : [ "Hans van Halteren" ],
      "venue" : "22nd International Conference on Computational Linguistics, Proceedings of the Conference,",
      "citeRegEx" : "Halteren.,? \\Q2008\\E",
      "shortCiteRegEx" : "Halteren.",
      "year" : 2008
    }, {
      "title" : "On the features of translationese",
      "author" : [ "Vered Volansky", "Noam Ordan", "Shuly Wintner" ],
      "venue" : "Digital Scholarship in the Humanities,",
      "citeRegEx" : "Volansky et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Volansky et al\\.",
      "year" : 2015
    }, {
      "title" : "Corpus methods for descriptive translation studies",
      "author" : [ "Federico Zanettin" ],
      "venue" : "Procedia – Social and Behavioral Sciences,",
      "citeRegEx" : "Zanettin.,? \\Q2013\\E",
      "shortCiteRegEx" : "Zanettin.",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 22,
      "context" : "Still, several corpus-based studies have been reported (Baker, 1993, 1995, 1996; Al-Shabab, 1996; Laviosa, 1998, 2002; Olohan, 2004; Becher, 2011; Zanettin, 2013) and this seems to be the standard methodology today.",
      "startOffset" : 55,
      "endOffset" : 162
    }, {
      "referenceID" : 31,
      "context" : "Still, several corpus-based studies have been reported (Baker, 1993, 1995, 1996; Al-Shabab, 1996; Laviosa, 1998, 2002; Olohan, 2004; Becher, 2011; Zanettin, 2013) and this seems to be the standard methodology today.",
      "startOffset" : 55,
      "endOffset" : 162
    }, {
      "referenceID" : 30,
      "context" : "The main appeal of this task is that it sheds new light on the features of translationese, and in particular, can help corroborate (or refute) several hypotheses of translation studies (Ilisei, 2013; Volansky et al., 2015).",
      "startOffset" : 185,
      "endOffset" : 222
    }, {
      "referenceID" : 23,
      "context" : ", 2011, 2012b), and that translation models compiled from parallel texts translated in the same direction as that of the SMT task are better than parallel texts translated in the opposite direction (Ozdowska and Way, 2009; Kurokawa et al., 2009; Lembersky et al., 2012a, 2013).",
      "startOffset" : 198,
      "endOffset" : 276
    }, {
      "referenceID" : 12,
      "context" : ", 2011, 2012b), and that translation models compiled from parallel texts translated in the same direction as that of the SMT task are better than parallel texts translated in the opposite direction (Ozdowska and Way, 2009; Kurokawa et al., 2009; Lembersky et al., 2012a, 2013).",
      "startOffset" : 198,
      "endOffset" : 276
    }, {
      "referenceID" : 11,
      "context" : "For example, Koppel and Ordan (2011) showed that while the overall accuracy of their translationese identification classifiers is near-perfect, when a classifier is trained on translations from one language, but is tested on translations from a different language, accuracy drops significantly.",
      "startOffset" : 13,
      "endOffset" : 37
    }, {
      "referenceID" : 11,
      "context" : "For example, Koppel and Ordan (2011) showed that while the overall accuracy of their translationese identification classifiers is near-perfect, when a classifier is trained on translations from one language, but is tested on translations from a different language, accuracy drops significantly. Popescu (2011) demonstrated that a classifier, trained to discriminate between original British literature and English literature translated from French, failed to predict translation status of English literature written by American authors and translations from German.",
      "startOffset" : 13,
      "endOffset" : 310
    }, {
      "referenceID" : 11,
      "context" : "For example, Koppel and Ordan (2011) showed that while the overall accuracy of their translationese identification classifiers is near-perfect, when a classifier is trained on translations from one language, but is tested on translations from a different language, accuracy drops significantly. Popescu (2011) demonstrated that a classifier, trained to discriminate between original British literature and English literature translated from French, failed to predict translation status of English literature written by American authors and translations from German. Avner et al. (Forthcoming) reported that while they were able to identify translationese with high accuracy within a given corpus (literary texts in Hebrew and Hebrew-translated-from-English), the classification was much less accurate when either the genre of the texts was modified (popular science vs. literature) or the source language was changed (French instead of English). Similar trends were noted by Rabinovich and Wintner (2015), showing a dra-",
      "startOffset" : 13,
      "endOffset" : 1005
    }, {
      "referenceID" : 25,
      "context" : "The entire corpus is freely available; some subsets have already been used in a pilot study on unsupervised identification of translationese (Rabinovich and Wintner, 2015).",
      "startOffset" : 141,
      "endOffset" : 171
    }, {
      "referenceID" : 9,
      "context" : "The Europarl sub-corpus is extracted from the collection of the proceedings of the European Parliament, dating back to 1996, originally collected by Koehn (2005). The original Europarl corpus is organized as several language-pairs, each with multiple sentence-aligned files.",
      "startOffset" : 149,
      "endOffset" : 162
    }, {
      "referenceID" : 10,
      "context" : "Mainly due to its multilingual nature, however, Europarl has been used extensively in SMT (Koehn et al., 2009) and in cross-lingual research (Cartoni et al.",
      "startOffset" : 90,
      "endOffset" : 110
    }, {
      "referenceID" : 2,
      "context" : ", 2009) and in cross-lingual research (Cartoni et al., 2013). It has even been adapted specifically for research in translation studies: Islam and Mehler (2012) compiled a customized version of Europarl, where the direction of translation is indicated.",
      "startOffset" : 39,
      "endOffset" : 161
    }, {
      "referenceID" : 2,
      "context" : ", 2009) and in cross-lingual research (Cartoni et al., 2013). It has even been adapted specifically for research in translation studies: Islam and Mehler (2012) compiled a customized version of Europarl, where the direction of translation is indicated. They used meta-data from the corpus, and in particular the language tag, to identify the original language in which each sentence was spoken, and removed sentence pairs for which this information was missing. A similar strategy was used by Lembersky et al. (2012b) and Cartoni and Meyer (2012).",
      "startOffset" : 39,
      "endOffset" : 518
    }, {
      "referenceID" : 2,
      "context" : ", 2009) and in cross-lingual research (Cartoni et al., 2013). It has even been adapted specifically for research in translation studies: Islam and Mehler (2012) compiled a customized version of Europarl, where the direction of translation is indicated. They used meta-data from the corpus, and in particular the language tag, to identify the original language in which each sentence was spoken, and removed sentence pairs for which this information was missing. A similar strategy was used by Lembersky et al. (2012b) and Cartoni and Meyer (2012). We now detail how we extracted reliable speaker identities from Europarl texts.",
      "startOffset" : 39,
      "endOffset" : 547
    }, {
      "referenceID" : 21,
      "context" : "This corpus most likely suffers from similar problems as the Europarl corpus discussed above; indeed, Mollin (2007), who investigated the British Hansard parliamentary transcripts, found that “the transcripts omit performance characteristics of spoken language, such as incomplete utterances or hesitations, as well as any type of extrafactual, contextual talk” and that “transcribers and editors also alter speakers’ lexical and grammatical choices towards more conservative and formal variants.",
      "startOffset" : 102,
      "endOffset" : 116
    }, {
      "referenceID" : 27,
      "context" : "com were available pre-aligned at the chapter- and paragraph-level; we therefore sentence-aligned them, one paragraph at a time, using a Python implementation (Tan and Bond, 2014) of the standard algorithm of Gale and Church (1993).",
      "startOffset" : 159,
      "endOffset" : 179
    }, {
      "referenceID" : 27,
      "context" : "com were available pre-aligned at the chapter- and paragraph-level; we therefore sentence-aligned them, one paragraph at a time, using a Python implementation (Tan and Bond, 2014) of the standard algorithm of Gale and Church (1993). For the remainder of the books, we first extracted chapters by (manually) identifying characteristic chapter titles (e.",
      "startOffset" : 160,
      "endOffset" : 232
    }, {
      "referenceID" : 20,
      "context" : "For tokenization we used the Stanford tools (Manning et al., 2014).",
      "startOffset" : 44,
      "endOffset" : 66
    }, {
      "referenceID" : 30,
      "context" : "To validate the quality of the corpus we replicated the experiments of Volansky et al. (2015), who conducted a thorough exploration of supervised classification of translationese, using dozens of feature types.",
      "startOffset" : 71,
      "endOffset" : 94
    }, {
      "referenceID" : 30,
      "context" : "To validate the quality of the corpus we replicated the experiments of Volansky et al. (2015), who conducted a thorough exploration of supervised classification of translationese, using dozens of feature types. While Volansky et al. (2015) only used the Europarl corpus (in its original format) and worked on English translated from French, we extended the experiments to all the datasets described above, including also English translated from German.",
      "startOffset" : 71,
      "endOffset" : 240
    }, {
      "referenceID" : 20,
      "context" : "For part of speech tagging, we employ the Stanford implementation along with its models for French and English (Manning et al., 2014).",
      "startOffset" : 111,
      "endOffset" : 133
    }, {
      "referenceID" : 8,
      "context" : "We use Platt’s sequential minimal optimization algorithm (Keerthi et al., 2001) to train a support vector machine classifier with the default linear kernel, an implementation freely available in Weka (Hall et al.",
      "startOffset" : 57,
      "endOffset" : 79
    }, {
      "referenceID" : 30,
      "context" : "We focused on a set of features that reflect lexical and structural properties of the text, and have been shown to be effective for supervised classification of translationese (Volansky et al., 2015).",
      "startOffset" : 176,
      "endOffset" : 199
    }, {
      "referenceID" : 11,
      "context" : "more precisely, the same list that was used in previous works on classification of translationese (Koppel and Ordan, 2011; Volansky et al., 2015).",
      "startOffset" : 98,
      "endOffset" : 145
    }, {
      "referenceID" : 30,
      "context" : "more precisely, the same list that was used in previous works on classification of translationese (Koppel and Ordan, 2011; Volansky et al., 2015).",
      "startOffset" : 98,
      "endOffset" : 145
    }, {
      "referenceID" : 30,
      "context" : "We used the list of 40 cohesive markers defined in Volansky et al. (2015).",
      "startOffset" : 51,
      "endOffset" : 74
    }, {
      "referenceID" : 30,
      "context" : "Character, POS, and contextual FW trigrams are calculated as detailed in Volansky et al. (2015), but we only considered the 1000 most frequent feature values extracted from each dataset.",
      "startOffset" : 73,
      "endOffset" : 96
    }, {
      "referenceID" : 30,
      "context" : "We first reproduced the classification results of Volansky et al. (2015) on the English vs.",
      "startOffset" : 50,
      "endOffset" : 73
    }, {
      "referenceID" : 30,
      "context" : "We first reproduced the classification results of Volansky et al. (2015) on the English vs. English-translated-from-French Europarl dataset, using the feature sets detailed in Section 4.2. We further present results for three additional English-French datasets, as well as for the English-German corpora. Tables 3 and 4 report the ten-fold cross-validation classification accuracy. The EUR column refers to the classification results of Europarl, as reported by Volansky et al. (2015). Although they experimented with a slightly different setup, we detail these results in Table 3 for illustrative comparison.",
      "startOffset" : 50,
      "endOffset" : 485
    }, {
      "referenceID" : 12,
      "context" : "Kurokawa et al. (2009) reported that the accuracy of classification deteriorated when",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 10,
      "context" : "translationese were mentioned by Kurokawa et al. (2009) and reported more extensively by Eetemadi and Toutanova (2014, 2015).",
      "startOffset" : 33,
      "endOffset" : 56
    }, {
      "referenceID" : 12,
      "context" : "It has been shown in a series of works (Kurokawa et al., 2009; Lembersky et al., 2012a, 2013, 2011, 2012b; Twitto-Shmuel et al., 2015) that awareness to translationese has a positive effect on the quality of SMT.",
      "startOffset" : 39,
      "endOffset" : 134
    }, {
      "referenceID" : 28,
      "context" : "It has been shown in a series of works (Kurokawa et al., 2009; Lembersky et al., 2012a, 2013, 2011, 2012b; Twitto-Shmuel et al., 2015) that awareness to translationese has a positive effect on the quality of SMT.",
      "startOffset" : 39,
      "endOffset" : 134
    } ],
    "year" : 2017,
    "abstractText" : "We describe bilingual English–French and English–German parallel corpora in which the direction of translation is accurately and reliably annotated. The corpus is diverse, consisting of parliamentary proceedings, literary works, transcripts of TED talks and political commentary. It will be instrumental for research of translationese and its applications to (human and machine) translation; specifically, it can be used for the task of translationese identification, a research direction that enjoys a growing interest in recent years. To evaluate the quality of the corpus, we replicate and expand on previous results of supervised identification of translationese, and further extend the experiments to additional datasets, including an additional language pair, and exploiting bilingual information.",
    "creator" : "LaTeX with hyperref package"
  }
}