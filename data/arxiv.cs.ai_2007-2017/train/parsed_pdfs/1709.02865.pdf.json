{
  "name" : "1709.02865.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Prosocial learning agents solve generalized Stag Hunts better than selfish ones",
    "authors" : [ "Alexander Peysakhovich", "Adam Lerer" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "Introduction",
      "text" : "Constructing agents which can make good decisions in complex environments is a cornerstone of modern artificial intelligence research. A string of successes and advancements have cemented deep reinforcement learning (RL) as one of the dominant paradigms for this task (Sutton and Barto 1998; Silver et al. 2016; Mnih et al. 2015). This has led to recent interest in using RL to construct agents which can interact with others (Silver et al. 2016; Lazaridou, Peysakhovich, and Baroni 2017; Das et al. 2017; Lowe et al. 2017; Foerster et al. 2017; Evtimova et al. 2017; Foerster et al. 2016; Havrylov and Titov 2017; Lewis et al. 2017; Heinrich and Silver 2016; Peng et al. 2017; Leibo et al. 2017). RL methods have been applied in this domain mostly by the method of reactive training where the learner treats other agents as part of the environment and optimizes its own reward. Reactive training works very well in zero-sum situations, where for one agent to gain the other must lose (Silver et al. 2016; Wu and Tian 2016). However, for positive sum games good outcomes are far from guaranteed (Leibo et al. 2017;\n∗Equal contribution. Author order was determined via randomization.\nLerer and Peysakhovich 2017). In this paper, we will focus on a large class of positive sum coordination games where reactive training leads to inefficient outcomes for both players. We will show that social preferences, even for one agent out of a dyad, can help ameliorate these issues.\nThe multi-agent analog of an optimum in a decision problem is a Nash equilibrium: policies for each agent such that, given what everyone else is doing, nobody wants to change their own policy. It is straightforward to see that these are the only stable points of dynamic process where both agents are learning to optimize their own reward (Fudenberg and Levine 1998; Fudenberg and Tirole 1991). While single agent decision problems have single global optima (which RL methods such as value iteration are guaranteed to converge to) multi-agent decision problems can have multiple equilibria. In zero-sum games all equilibria yield equivalent payoffs (Neumann 1928). However, when games are positive-sum these equilibria can have very different efficiency properties.1\nWhen the learning agent’s partner is also a learning agent actions that are taken today can affect the behavior of the partner later and thus change which equilibrium agents converge to.2 Thus when situations are positive sum, even if we augment our learning algorithms with high capacity representations, and variance reduction techniques to guarantee that reactive play reaches some equilibrium, we still cannot\n1A separate, but also important, issue is that policies which yield the highest payoffs may not be equilibria. This is true in simple games such as the Prisoner’s Dilemma as well as more complex ones. Games which have this property are typically called cooperation games or social dilemmas. In this paper we study coordination games where payoff dominant strategies do form Nash equilibria. Though these names are sometimes used interchangeably in the recent multi-agent RL literature, we urge precision in language because these two game types have very different properties from the point of view of learning/evolutionary dynamics, how humans act in them and in how one would construct agents to solve them in the real world (Lerer and Peysakhovich 2017; Leibo et al. 2017; Kleiman-Weiner et al. 2016; Perolat et al. 2017).\n2Non-stationarity can also create convergence issues due to increasing variance of the learning procedure. This is an important issue and there is recent interest in augmenting our RL toolbox with techniques for multi-agent problems (Lowe et al. 2017). However, this increased variance is an algorithmic issue whereas the multiple equilibria one is a completely orthogonal game theoretic one.\nar X\niv :1\n70 9.\n02 86\n5v 1\n[ cs\n.A I]\n8 S\nep 2\n01 7\nguarantee it will reach the best one. This also means that if we use reactive self-play RL to explore the space of policies in these multi-agent problems we may not find the good policies we seek.\nWe show that reactive policies often converge to bad equilibria even in simple 2 player, 2 strategy coordination games. These games have multiple equilibria and we discuss how off-equilibrium payoffs determine the size of the basin of attraction of each one.\nOur main contribution is a demonstration of a link between a dyad’s ability to coordinate on payoff-dominant equilibria and the individual agents’ prosociality. We show that social preferences (that is, receiving utility when others receive rewards) increase the size of the basin of attraction of the payoff dominant equilibrium. In particular, in any Stag Hunt-type game prosociality helps convergence to the equilibrium with the highest payoffs. In the limit of perfectly selfless preferences, this convergence is guaranteed. We show analytically that this result holds even if only a single agent in the dyad has prosocial preferences.\nWe replicate these results in Markov games beyond simple matrix games. In each of our games there are two agents that move on a grid and can choose between coordinating strategies which pay off if both agents follow them or safe strategies which guarantee a steady payoff no matter what the other agent does. We consider three games with different specific rules and show that in each of them prosocial preferences, even for a single agent, can lead to better outcomes for the learning pair.\nOur results also showcase that the key assumption behind reactive play, treating other agents as a static part of the environment, comes with a cost. In particular, when environments are non-stationary because they include other adapting agents, we can do better by using strategies that take this into account."
    }, {
      "heading" : "Risk Dominance, the Stag Hunt, and Equilibrium Selection",
      "text" : "We begin by illustrating the intuition behind our results with the simple 2 player matrix Stag Hunt. In the Stag Hunt players simultaneously decide to either take a risky option (Hunt the Stag) or a safe option (Forage). Foraging always yields a safe payoff, whereas Hunting yields a higher payoff if the other person also hunts but yields a bad payoff if the player shows up to hunt alone (because the Stag gores her). This is illustrated in the payoff matrix below (rows represent strategy choices for player 1, columns represent strategy choices for player 2, entries represent payoffs to each player as a function of strategies chosen):\nPlayer 2 Hunt Forage\nPlayer 1 Hunt (2, 2) (−g, 1) Forage (1,−g) (1, 1)\nLet A1, A2 be the action spaces of the players and let\nRi(a1, a2) be the reward players receive from a pair of actions. We consider the set of stable points in this game:\nDefinition 1 Nash equilibria are strategy pairs (a∗1, a∗2) such that for any a′1 we have\nR1(a ∗ 1, a ∗ 2) ≥ R1(a′1, a∗2)\nand for any a′2 we have\nR2(a ∗ 1, a ∗ 2) ≥ R2(a∗1, a′2).\nIn a Nash equilibrium, neither agent has incentive to deviate from their chosen strategy given what the other agent is doing. For this reason, they are possible convergence points of a learning process. There are two (non-randomized) equilibria here: both choose to Hunt or both choose Forage. Hunting yields higher payoff for both agents (is the payoffdominant equilibrium) thus one may hope that learning agents converge to Hunt. However this may not always be true.\nHow can we analyze which equilibrium is more likely to occur under learning, and, if we can shape learning, how can we nudge our agents to converge to a good one? We begin by considering the general version of the Stag Hunt with the payoff matrix below.\nPlayer 2 A B\nPlayer 1 A (a, a) (c, b) B (b, c) (d, d)\nWe consider a particular class of coordination games:\nDefinition 2 A coordination game is a generalized Stag Hunt if a > b ≥ d > c.\nThese conditions imply two things. First, that (A,A), (B,B) are the equilibria and (A,A) is payoff dominant. The second part of the condition b ≥ d implies that the coordination game has a sort of asymmetric risk, if one player chooses A but the other chooses B, only the A player loses (relative to (B,B)). This generalizes the notion of the Stag Hunt that B is a safe action that does well no matter what the other chooses whereas A is a risky action that only works if the other person chooses A. This can also be thought of as a generalization of weak link effort games where each player chooses an amount of costly effort to put in but the amount of output that is produced is proportional to the minimum of the effort levels (Van Huyck, Battalio, and Beil 1990; Camerer 2003).\nWe can now consider which equilibrium is more likely to be selected by a general class of learning dynamics. This example will give us a clear intuition for our next steps. We assume that agents play the game repeatedly (either with each other, themselves in a ‘self-play’ form or with a population of others). Agents start with beliefs p1, p2 which are the probabilities they expect their partner to choose A. Agents best respond to this belief. Having observed the choice of\ntheir partner, they update their beliefs in the correct direction (that is, if the partner chose A, p goes up). This means that agents choose A if pia+ (1− pi)c ≥ pib+ (1− pi)d. We can calculate the critical value of p∗ such that if p > p∗ the agent chooses A otherwise they choose B.\nThis tells us something about the basins of attraction of each equilibrium. If both agents have beliefs above p∗ then they will converge to (A,A) - this is because they will play A this period and only increase their beliefs and thus they will play A next period and so on. A similar argument holds if both agents start below p∗. If one agent is above and the other is below then the specifics of the learning rule come into play and limit behavior depends on which of the two attractor regions they reach first. Note that what matters for the computation of p∗ are not just a and d but also the offequilibrium payoffs b and c.3 This is illustrated in figure 1. Interestingly, in games with more than 2 strategies the basins of attraction of various equilibria can be informed even by the presence of strategies which are never played in any equilibrium.\nWith this notion of basin of attraction in mind, we ask whether modifying our agents while keeping the game constant can shift the basin of attraction. We consider a type of\n3A particularly important related concept is risk dominance (Harsanyi, Selten, and others 1988) which asks whether p∗ > .5 (in which case A is said to be risk-dominant) or not (in which case B is risk dominant). Work in evolutionary game theory shows that in large population evolutionary processes in 2×2 games with multiple equilibria it is risk dominance not payoff dominance that serves as the unique convergence point (Kandori, Mailath, and Rob 1993; Nowak 2006; Fudenberg and Levine 1998). These results typically focus specifically on low mutation rates and large populations, however their qualitative results carry over to many other learning dynamics. Indeed, another way to understand risk dominance is to think about it as the basin of attraction of each equilibrium. The risk dominant equilibrium in a symmetric game is the one with the strictly larger basin of attraction.\nagent with social preferences:\nDefinition 3 A prosocial agent’s total utility from an outcome is given by\nUi(ai, aj) = (1− α)Ri(ai, aj) + αRj(ai, aj).\nWe refer to α as the agent’s level of prosociality. α = 0 agents are perfectly selfish, α = .5 agents are fully prosocial (weigh their and their partner’s utility equally) and α = 1 agents are selfless.\nWe now show our main analytical result which motivates our experiments:\nTheorem 1 In a generalized Stag Hunt the size of the basin of attraction of (A,A) increases in both players’ level of prosociality. There exists ᾱ ∈ (0, 1) such that if either agent has α ≥ ᾱ the unique interior attractor is (A,A). Proof 1 Let p be the probability that the other agent plays A. For an agent with prosociality α to prefer A over B the following must hold:\npa+ (1−p)((1−α)c+αb) ≥ p(αc+ (1−α)b) + (1−p)d\nSolving for p∗, the minimum p that leads the agent to play A yields\np∗ = (d− c)− α(b− c)\n(a+ d− b− c) .\nBy the Stag Hunt inequalities, all three bracketed quantities must be strictly positive; therefore, p∗ decreases in α (i.e. the basin of attraction increases as α increases). Solving for p∗ = 0 determines the values α∗ which makes A a weakly dominant strategy:\nα∗ = d− c b− c ."
    }, {
      "heading" : "By the Stag Hunt inequalities, d − c < b − c, and both are",
      "text" : "positive, therefore, α∗ ∈ (0, 1).\nThe intuition for the theorem comes from fact that social preferences change the (A,B) payoff for player 1 to (1 − α)c + αb. If b > c (intuitively, if risk from coordination mostly accrues to the person who chooses A) then this increases the size of the basin of A. If b ≥ d > c then for some α this makes makes A a (weakly) dominant strategy. If A is a dominant strategy for one of the players then the unique equilibrium the dyad will converge to is (A,A). The same logic can be extended to 2 player N ×N coordination games as long as any 2× 2 subset of that game is a generalized Stag Hunt (we leave the proof to the Appendix).\nCorrolary 1 In any symmetric N ×N game where the only pure equilibria are symmetric and the payoff matrix subset to any pair of strategies is a generalized Stag Hunt there exists ᾱ such that if for either player α > ᾱ the unique convergence point is the payoff dominant equilibrium.\nTheorem 1 and its Corollary implies that in generalized Stag Hunts, any agent can increase their (expected) payoff after convergence by increasing their α. However, in games with more complex strategy spaces where some aspects of\nthe game do not obey the Stag Hunt inequality, increasing α may be counterproductive, since in those parts of the game the agent will pointlessly perform self-harming and socially inefficient actions4. Setting α ≤ 0.5 at least guarantees that the agent will only choose to hurt its own payoffs if the resulting outcome is efficient (increases the sum of the payoffs). Thus we mostly think about prosocial, but not selfless, agents in practice and mostly look at α ∈ [0, .5]."
    }, {
      "heading" : "Experiments",
      "text" : "Our theorem implies that if we endow agents with prosocial preferences then dynamics should now more reliably select the payoff dominant equilibria in 2 × 2 matrix games. The relative improvement depends on the learning rule used, so our experiments will first investigate the impact of prosociality on policy gradient trained RL agents in these simple games. Second, it is not clear whether results from the simple 2 × 2 case will generalize to Markov games where strategy spaces are much more complex. Thus, we will investigate the impact of social preferences on coordination in more complex games."
    }, {
      "heading" : "Matrix Stag Hunt",
      "text" : "We begin with the matrix Stag Hunt game discussed in the intro. We vary the prosociality of both players as well as the g payoff. We train policy gradient based agents and add persistent experimentation (5 percent chance of uniformly random action choice in each period) to maintain exploration. For optimization we use Adam (Kingma and Ba 2014) with a learning rate of .01 as well as standard random initialization. We train each pair of agents for 2000 rounds and look at average behavior in the last 100 rounds. We train 100 replicates and average them together.\nWe see from figure 2 (left) that both g and social preferences affect convergence to the payoff dominant equilibrium. There are situations where we control both agents (eg. in using self play to find good policies). In this case, we see that adding social preferences to both agents can help find good optimal equilibria. An important question is what to do when dyads of learners will be put in new environments but where we as agent designers don’t get to affect the agent of our partner (eg. when the other agent is a human). We ask: if we change the preferences of the agent we do control, can we affect the equilibrium that the dyad converges to?\nWe replicate the simulations above, however, now we force the α of one agent to be 0 (we call this agent the Learner) and vary the prosociality of the other agent (we refer to this agent as the Teacher). Both agents are initialized randomly and play the Stag Hunt game with varying g and policy gradient RL. Figure 2 (right) shows the results. We see that even when a single agent is made prosocial we are more likely to converge to the payoff dominant equilibrium than when both are trained via selfish reactive RL.\n4Also, even in a true Stag Hunt, prosocial behavior relies on the partner being sufficiently adaptive to be able to learn to coordinate."
    }, {
      "heading" : "Markov Games",
      "text" : "We now consider a set of more complex Stag Hunt-like games. We represent them using the framework of Markov games:\nDefinition 4 ((Shapley 1953)) A (2-player) Markov game consists of\n• A set of states S = {s1, . . . , sn} • A set of actions for each playerA1 = {a11, . . . , a1k},A2 = {a21, . . . , a2k} • A transition function τ : S × A1 × A2 → ∆(S) which tells us the probability distribution on the next state as a function of current state and actions\n• A reward function for each playerRi : S×A1×A2 → R which tells us the utility that player gains from a state, action tuple\nThe choice variables for the agents are policies, which we denote by π which are maps from states to actions. To learn good policies we use a policy gradient with state representation constructed using a deep convolutional network with inputs as the raw board pixels. We put the details of the model architecture/training in the Appendix.\nWe now consider 3 different Markov games: Markov Stag Hunt, Harvest and Escalation (Figure 3). In each of these games two agents move on a 5×5 grid and can move any of the 4 cardinal directions. In the Markov Stag Hunt the grid\nis populated with a Stag and 2 plants. Moving over a plant gives either agent 1 point and causes the plant to disappear and re-appear in another part of the board. Moving over the Stag causes an agent to lose g points but if both agents move over the stag simultaneously they each gain 5 points and the stag disappears and re-appears randomly elsewhere on the board. In each time period the stag moves towards the closest agent to it.\nLike in the standard Stag Hunt, there are two equilibria here - either both agents try to get the stag or they both try to stay away from it and pick up plants. There is asymmetric risk, if one chooses to try to hunt while the other avoids, the hunter will suffer the consequences.\nIn the Harvest game at each time step a plant can appear randomly somewhere on the board (up to 4 plants can be on the board at a time). Each plant is born ‘young’, then every time step turns ‘mature’ with probability rmature. While a plant is mature it can die on each time step with probability rdeath. The probabilities are always selected such that each plant lives for 20 time steps in expectation.\nPlayers can move over plants to pick them up. Players receive 1 point if they up a young plant, however waiting until each plant becomes mature and picking it up yields 2 points to both players. Thus the coordination question is whether to wait or rush for the plants. Choosing to wait is an equilibrium that yields high payoffs for both players. However, just like stag hunting it incurs a risk that one’s partner loses their nerve and grabs the young plant.\nIn the Coordinated Escalation game a special marker appears on one of the squares. If the agents step on the square together, they both receive one point, at which point an adjacent square lights up. If the agents step together onto the next square, they receive 1 point. If at any time an agent breaks\nthe streak (eg. by stepping off the path), the other agent receives a penalty of some multiplier times the current length of the streak and the game ends. This game has many equilibria, where both agents play to keep streaks of size T but no more with risk escalating at each time step (as the reward from further escalation is always 1 but the cost from one’s partner failing to continue the pattern increases linearly).\nIn our experiments for each of the games we vary two parameters: the prosociality of our players and the risk (in Stag Hunt it is the penalty for hunting alone, in Harvest it is the amount of time the plant stays young, in Escalation the penalty for unilaterally escalating). We also replicate the Teacher/Learner setup from the experiment above. Both agents enter the game as blank slates and use RL to learn to play, the Learner is always completely selfish but the Teacher can either be selfish or prosocial.\nFigure 4 shows that the intuition from the coordination game replicates in this more complex environment. Giving agents social preferences can help lead to coordination on payoff-dominant strategies. Both the risk and whether one or both agents have social preferences plays an important role. Note that we see across all the Markov games that adding prosocial preferences to one of the agents increases that agent’s rewards. That is, due to the dynamic nature of Markov games, even if one only cares about their agent’s payoff, giving the agent prosocial preferences helps the agent achieve higher rewards.\nThe different structures of our Markov games allow us to further see that the basin of attraction for equilibria in a game with more than 2 strategies are influenced not only by the payoffs of the two equilibria, but also the relative size of the policy set that implements various outcomes. In the Harvest game the fraction of time in the young stage does not\naffect the payoff to either equilibrium but it does change the proportion of policies that implement payoff-dominant coordination. This further underscores a point made by (Leibo et al. 2017) that the ‘difficulty’ of discovering various strategies can affect equilibrium selection and furthermore that this may be dependent not only on the underlying game itself but on the function approximation used in our RL algorithm."
    }, {
      "heading" : "Conclusion",
      "text" : "We have shown that game theoretic properties of multi-agent systems can hamper the ability of RL methods to converge\nto Pareto optimal policies even when such policies are equilibria. We have also shown a simple method that can help ameliorate this problem in a particular class of games. Our results showcase the importance of game theory for the practical development of AI.\nWe focused on off-equilibrium payoffs as an important criterion for determining the basin of attraction of multiagent optimization. However, an important thing that we have left out is function approximation. Much modern RL does not work on the state space of a problem directly but rather approximates that state space using neural network\ntechniques (Silver et al. 2016; Mnih et al. 2015). The architectures of these networks can affect the relative complexity and thus relative likelihood of different policies. This, in turn, can affect the basins of attraction for various equilibria (Leibo et al. 2017; Tamar et al. 2016). Good choices of priors (ie. model architectures) have allowed AI to make great strides in fields like computer vision and prosocial important direction in multi-agent systems is further exploring good architectures for learning in these conditions.\nOur results relate to other solutions to the multiple optima problem discussed in past work on multi-agent systems in particular on lenient learning (Panait, Tuyls, and Luke 2008; Matignon, Laurent, and Le Fort-Piat 2012) or Frequency Maximum Q-learning (Kapetanakis and Kudenko 2002). Both of these methods solve coordination problems by having agents keep ‘optimistic’ values of each action. However, applying these methods straight out of the box requires full knowledge of the payoff matrix (for lenient learning) or large amounts of exploration as the number of choice variables (policies) becomes large (FMQ). Integrating ideas from this literature into deep RL is an interesting future direction for designing coordinating agents.\nWe close with a practical question: how can we construct agents that can coordinate productively with humans? It has been shown that humans often fail to coordinate on payoffdominant equilibria (Van Huyck, Battalio, and Beil 1990; Camerer 2003) but that artificial agents can be added to help groups coordinate better (Shirado and Christakis 2017). We have shown that learning algorithms must take into account the learning behavior of other agents if they are to learn to coordinate in multi-agent environments. Understanding how to design human-machine hybrid systems in ways that properly account for human psychology (Crandall et al. 2017; Kleiman-Weiner et al. 2016; Rand et al. 2014; Hauser et al. 2014; Ouss and Peysakhovich 2015; Arechar et al. 2016; Mao et al. 2017) and also human learning strategies (Erev and Roth 1998; Fudenberg and Peysakhovich 2016) is an important practical question to extending this work."
    }, {
      "heading" : "Appendix",
      "text" : "Proof 2 (Corollary 1) Consider an N × N payoff matrix U for agent 1; we assume symmetry, i.e. U (2) = UT for simplicity. We can order the strategies such that for all i < j, Uii ≥ Ujj without loss of generality. Then every subspace of U is a stag hunt if for all i < j,\nUii > Uji ≥ Ujj > Uij ."
    }, {
      "heading" : "If agent 1 has prosociality α, then its payoff matrix is Uα = (1− α)U + αUT i.e. a linear interpolation between U and",
      "text" : ""
    }, {
      "heading" : "UT , so there exists α < 1 for which",
      "text" : "Uαii > Uij ≥ Uαjj > Uαji for all i < j. Thus for agent 1, i weakly dominates j for all i < j. If agent 1 plays 0, then 0 is a dominant strategy for agent 2 (for any α)."
    }, {
      "heading" : "Details of Markov Game Training",
      "text" : "Our policy is modeled by a multi-layer convolutional neural network that directly computes π. For a given board size,\nthe model has dlog(2)e+ 1 repeated layers, each consisting of a 2D convolution with kernel size 3, followed by batch normalization and ReLU. The first layer has stride 1, while the successive layers each have stride 2, which decreases the width and height from k to dk/2ewhile doubling the number of channels.\nWe perform reinforcement learning via policy gradient using REINFORCE (Williams 1992) and RMSProp for optimization. The model is updated episodically in batches of 64 episodes, with a discount rate of 0.99. Markov stag hunt and Harvest are trained for 200,000 episodes with episode length drawn from exp(250), while Escalation is trained for 500,000 episodes of length at most 50 (since Escalation only extends for the duration of coordination). For Markov Stag Hunt, we add the entropy of the policy π to the loss to avoid convergence to suboptimal deterministic policies (Mnih et al. 2016)."
    } ],
    "references" : [ {
      "title" : "D",
      "author" : [ "A.A. Arechar", "A. Dreber", "D. Fudenberg", "Rand" ],
      "venue" : "G.",
      "citeRegEx" : "Arechar et al. 2016",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "M",
      "author" : [ "J.W. Crandall", "M. Oudah", "F. IshowoOloko", "S. Abdallah", "J.-F. Bonnefon", "M. Cebrian", "A. Shariff", "Goodrich" ],
      "venue" : "A.; Rahwan, I.; et al.",
      "citeRegEx" : "Crandall et al. 2017",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "J",
      "author" : [ "A. Das", "S. Kottur", "Moura" ],
      "venue" : "M.; Lee, S.; and Batra, D.",
      "citeRegEx" : "Das et al. 2017",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "A",
      "author" : [ "I. Erev", "Roth" ],
      "venue" : "E.",
      "citeRegEx" : "Erev and Roth 1998",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Emergent language in a multi-modal, multi-step referential game. arXiv preprint arXiv:1705.10369",
      "author" : [ "Evtimova" ],
      "venue" : null,
      "citeRegEx" : "Evtimova,? \\Q2017\\E",
      "shortCiteRegEx" : "Evtimova",
      "year" : 2017
    }, {
      "title" : "Y",
      "author" : [ "Foerster, J.", "Assael" ],
      "venue" : "M.; de Freitas, N.; and Whiteson, S.",
      "citeRegEx" : "Foerster et al. 2016",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Counterfactual multiagent policy gradients. arXiv preprint arXiv:1705.08926",
      "author" : [ "Foerster" ],
      "venue" : null,
      "citeRegEx" : "Foerster,? \\Q2017\\E",
      "shortCiteRegEx" : "Foerster",
      "year" : 2017
    }, {
      "title" : "D",
      "author" : [ "D. Fudenberg", "Levine" ],
      "venue" : "K.",
      "citeRegEx" : "Fudenberg and Levine 1998",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "and Peysakhovich",
      "author" : [ "D. Fudenberg" ],
      "venue" : "A.",
      "citeRegEx" : "Fudenberg and Peysakhovich 2016",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "and Tirole",
      "author" : [ "D. Fudenberg" ],
      "venue" : "J.",
      "citeRegEx" : "Fudenberg and Tirole 1991",
      "shortCiteRegEx" : null,
      "year" : 1991
    }, {
      "title" : "J",
      "author" : [ "Harsanyi" ],
      "venue" : "C.; Selten, R.; et al.",
      "citeRegEx" : "Harsanyi. Selten. and others 1988",
      "shortCiteRegEx" : null,
      "year" : 1988
    }, {
      "title" : "M",
      "author" : [ "O.P. Hauser", "D.G. Rand", "A. Peysakhovich", "Nowak" ],
      "venue" : "A.",
      "citeRegEx" : "Hauser et al. 2014",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "and Titov",
      "author" : [ "S. Havrylov" ],
      "venue" : "I.",
      "citeRegEx" : "Havrylov and Titov 2017",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "and Silver",
      "author" : [ "J. Heinrich" ],
      "venue" : "D.",
      "citeRegEx" : "Heinrich and Silver 2016",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "G",
      "author" : [ "Kandori, M.", "Mailath" ],
      "venue" : "J.; and Rob, R.",
      "citeRegEx" : "Kandori. Mailath. and Rob 1993",
      "shortCiteRegEx" : null,
      "year" : 1993
    }, {
      "title" : "and Kudenko",
      "author" : [ "S. Kapetanakis" ],
      "venue" : "D.",
      "citeRegEx" : "Kapetanakis and Kudenko 2002",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "and Ba",
      "author" : [ "D. Kingma" ],
      "venue" : "J.",
      "citeRegEx" : "Kingma and Ba 2014",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "J",
      "author" : [ "M. Kleiman-Weiner", "M. Ho", "J. Austerweil", "M.L. Littman", "Tenenbaum" ],
      "venue" : "B.",
      "citeRegEx" : "Kleiman.Weiner et al. 2016",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Multi-agent cooperation and the emergence of (natural) language",
      "author" : [ "Peysakhovich Lazaridou", "A. Baroni 2017] Lazaridou", "A. Peysakhovich", "M. Baroni" ],
      "venue" : "In International Conference on Learning Representations",
      "citeRegEx" : "Lazaridou et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Lazaridou et al\\.",
      "year" : 2017
    }, {
      "title" : "J",
      "author" : [ "Leibo" ],
      "venue" : "Z.; Zambaldi, V.; Lanctot, M.; Marecki, J.; and Graepel, T.",
      "citeRegEx" : "Leibo et al. 2017",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "and Peysakhovich",
      "author" : [ "A. Lerer" ],
      "venue" : "A.",
      "citeRegEx" : "Lerer and Peysakhovich 2017",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "Y",
      "author" : [ "M. Lewis", "D. Yarats", "Dauphin" ],
      "venue" : "N.; Parikh, D.; and Batra, D.",
      "citeRegEx" : "Lewis et al. 2017",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "Multi-agent actor-critic for mixed cooperative-competitive environments",
      "author" : [ "Lowe" ],
      "venue" : "arXiv preprint arXiv:1706.02275",
      "citeRegEx" : "Lowe,? \\Q2017\\E",
      "shortCiteRegEx" : "Lowe",
      "year" : 2017
    }, {
      "title" : "D",
      "author" : [ "A. Mao", "L. Dworkin", "S. Suri", "Watts" ],
      "venue" : "J.",
      "citeRegEx" : "Mao et al. 2017",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "G",
      "author" : [ "Matignon, L.", "Laurent" ],
      "venue" : "J.; and Le Fort-Piat, N.",
      "citeRegEx" : "Matignon. Laurent. and Le Fort.Piat 2012",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "A",
      "author" : [ "V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "Fidjeland" ],
      "venue" : "K.; Ostrovski, G.; et al.",
      "citeRegEx" : "Mnih et al. 2015",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "A",
      "author" : [ "Mnih, V.", "Badia" ],
      "venue" : "P.; Mirza, M.; Graves, A.; Lillicrap, T.; Harley, T.; Silver, D.; and Kavukcuoglu, K.",
      "citeRegEx" : "Mnih et al. 2016",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "J",
      "author" : [ "Neumann" ],
      "venue" : "v.",
      "citeRegEx" : "Neumann 1928",
      "shortCiteRegEx" : null,
      "year" : 1928
    }, {
      "title" : "M",
      "author" : [ "Nowak" ],
      "venue" : "A.",
      "citeRegEx" : "Nowak 2006",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "and Peysakhovich",
      "author" : [ "A. Ouss" ],
      "venue" : "A.",
      "citeRegEx" : "Ouss and Peysakhovich 2015",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Theoretical advantages of lenient learners: An evolutionary game theoretic perspective",
      "author" : [ "Tuyls Panait", "L. Luke 2008] Panait", "K. Tuyls", "S. Luke" ],
      "venue" : "Journal of Machine Learning Research 9(Mar):423–457",
      "citeRegEx" : "Panait et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Panait et al\\.",
      "year" : 2008
    }, {
      "title" : "Multiagent bidirectionally-coordinated nets for learning to play starcraft combat games. arXiv preprint arXiv:1703.10069",
      "author" : [ "Peng" ],
      "venue" : null,
      "citeRegEx" : "Peng,? \\Q2017\\E",
      "shortCiteRegEx" : "Peng",
      "year" : 2017
    }, {
      "title" : "J",
      "author" : [ "Perolat, J.", "Leibo" ],
      "venue" : "Z.; Zambaldi, V.; Beattie, C.; Tuyls, K.; and Graepel, T.",
      "citeRegEx" : "Perolat et al. 2017",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "J",
      "author" : [ "D.G. Rand", "A. Peysakhovich", "G.T. KraftTodd", "G.E. Newman", "O. Wurzbacher", "M.A. Nowak", "Greene" ],
      "venue" : "D.",
      "citeRegEx" : "Rand et al. 2014",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "1953",
      "author" : [ "Shapley", "L. S" ],
      "venue" : "Stochastic games. Proceedings of the national academy of sciences 39(10):1095–",
      "citeRegEx" : "Shapley 1953",
      "shortCiteRegEx" : null,
      "year" : 1100
    }, {
      "title" : "N",
      "author" : [ "H. Shirado", "Christakis" ],
      "venue" : "A.",
      "citeRegEx" : "Shirado and Christakis 2017",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "C",
      "author" : [ "D. Silver", "A. Huang", "Maddison" ],
      "venue" : "J.; Guez, A.; Sifre, L.; Van Den Driessche, G.; Schrittwieser, J.; Antonoglou, I.; Panneershelvam, V.; Lanctot, M.; et al.",
      "citeRegEx" : "Silver et al. 2016",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "A",
      "author" : [ "R.S. Sutton", "Barto" ],
      "venue" : "G.",
      "citeRegEx" : "Sutton and Barto 1998",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Value iteration networks",
      "author" : [ "Tamar" ],
      "venue" : null,
      "citeRegEx" : "Tamar,? \\Q2016\\E",
      "shortCiteRegEx" : "Tamar",
      "year" : 2016
    }, {
      "title" : "R",
      "author" : [ "J.B. Van Huyck", "R.C. Battalio", "Beil" ],
      "venue" : "O.",
      "citeRegEx" : "Van Huyck. Battalio. and Beil 1990",
      "shortCiteRegEx" : null,
      "year" : 1990
    }, {
      "title" : "R",
      "author" : [ "Williams" ],
      "venue" : "J.",
      "citeRegEx" : "Williams 1992",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "and Tian",
      "author" : [ "Y. Wu" ],
      "venue" : "Y.",
      "citeRegEx" : "Wu and Tian 2016",
      "shortCiteRegEx" : null,
      "year" : 2016
    } ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "There is much interest in applying reinforcement learning methods to multi-agent systems. A popular way to do so is the method of reactive training – ie. treating other agents as if they are a stationary part of the learner’s environment. Dyads of such learners, if they converge, will converge to Nash equilibria of the game. However, there is an important game theoretic issue here: positive-sum games can have multiple equilibria which differ in their payoffs. We show that even in simple coordination games reactive reinforcement learning agents will often coordinate on equilibria with suboptimal payoffs for both agents. We also show that receiving utility from rewards other agents receive ie. having prosocial preferences leads agents to converging to better equilibria in a class of generalized Stag Hunt games. We show this analytically for matrix games and experimentally for more complex Markov versions. Importantly, this is true even if only one of the agents has social preferences. This implies that even if an agent designer only controls a single agent out of a dyad and only cares about their agent’s payoff, it can still be better for the designer to make the agent prosocial rather than selfish.",
    "creator" : "LaTeX with hyperref package"
  }
}