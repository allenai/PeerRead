{
  "name" : "1509.06928.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Automatic Dialect Detection in Arabic Broadcast Speech",
    "authors" : [ "Ahmed Ali", "Najim Dehak", "Patrick Cardinal", "Sameer Khurana", "Sree Harsha Yella", "James Glass", "Peter Bell", "Steve Renals" ],
    "emails" : [ "amali@qf.org.qa,", "skhurana@qf.org.qa,", "najim@jhu.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n50 9.\n06 92\n8v 2\n[ cs\n.C L\n] 1\n1 A\nug 2\n01 6"
    }, {
      "heading" : "1. Introduction",
      "text" : "The task of Dialect Identification (DID) is a special case of the more general problem of Language Identification (LID). LID refers to the process of automatically identifying the language class for given speech segment or text document. DID is arguably a more challenging problem than LID, since it consists of identifying the different dialects within the same language class. The importance of addressing DID can be gauged from its growing interest in the Automatic Speech Recognition (ASR) community [1]. A good DID system can facilitate the identification of dialectal segments from an untranscribed mixed-speech dataset. This process can help reduce the ASR word error rate (WER) for dialectal data by training ASR systems for each dialect, or by adapting the ASR models to a particular dialect.\nThe natural language processing (NLP) community has aggregated dialectal Arabic into five regional language groups: Egyptian (EGY), North African or Maghrebi (NOR), Gulf or Arabian Peninsula (GLF), Levantine (LAV), and Modern Standard Arabic (MSA). An objective comparison of the varieties of Arabic dialects could potentially lead to the conclusion that Arabic dialects are historically related, but not synchronically, and are mutually unintelligible languages like English and Dutch. Normal vernacular can be difficult to understand\nacross different Arabic dialects [?]. Arabic dialects are thus sufficiently distinctive, and it is reasonable to regard the DID task in Arabic as similar to the LID task in other languages. Table 1 shows two phrases across the different dialects, it is clear from this example that there are lexical variations across the different dialects which motivates us to consider it.\nTwo broad LID approaches have been investigated in the literature: low-level acoustic features, and high-level phonetic and lexical features. In the lexical area, words, roots, morphology, and grammars [2, 3] have been studied. Acoustic features such as shifted delta cepstral coefficients [17] and prosodic features [5] using Gaussian mixture models (GMMs), i-vector representations and support vector machine (SVM) classifiers [17] have been shown to be effective for LID. More recent work explored the use of frame-by-frame phone posteriors (PLLRs) [6] as new features for LID. New subspace approaches based on non-negative factor analysis (NFA) for GMM weight decomposition and adaptation [7] were also applied to both LID and DID tasks. GMM weight adaptation subspaces seem to provide complementary information to the classical i-vector framework. Finally, phoneme sequence modeling and its n-gram subspace have been studied for both Arabic DID [8] and LID [9].\nIn this paper we investigate three Vector Subspace Models (VSMs) for Arabic DID based on 1) lexical, 2) phonetic, and 3) i-vectors. We conduct a thorough feature selection study of these models to better understand their interaction. A further contribution of this work is the release of an Arabic DID system so others can extend and improve DID performance on this task.1"
    }, {
      "heading" : "2. Vector Space Models",
      "text" : ""
    }, {
      "heading" : "2.1. Senone based Utterance VSM",
      "text" : "Senone refers to an n-gram phone sequence. In our case n ≤ 4. VSM construction takes place in two steps: first, a phoneme recognizer is used to extract the senone [10] sequence for a given\n1https://github.com/Qatar-Computing-Research-Institute/dialectID\nspeech utterance. The phoneme sequence is obtained by automatic vowelization of the training text, followed by vowelization to phonetization (V2P). The 36 chosen phonemes cover all the dialectal Arabic sounds. Further details about the speech recognition pipeline, training data, and phoneme set is given in [11]. For the phoneme sequence, we process the phoneme lattice, and obtain the one-best transcription, ignoring silences as well as noisy silences. Each speech utterance (u) is then represented as a high dimensional sparse vector ( #»u ):\n#»u = (A(f(u, s1)), A(f(u, s2)), . . . , A(f(u, sd))) , (1)\nwhere f(u, si) is the number of times a senone si occurs in the speech utterance u, and A is the scaling function. We experiment with both an identity scaling function and tf.idf scaling function, commonly used in the field of Natural Language Processing [12] to downweight the contribution of the words (in our case senones) that occur in almost all documents (in our case utterances), as these words (senones) do not provide any discriminative information about the documents (utterances).\nThe vector space is then represented by the matrix, Us ∈ R\nd×N (see Fig 1). This approach and the notation used to define a VSM is directly inspired by the seminal works in the area of VSM of Natural Language in [13, 14, 15] and in LID [?].\nUs =\n\n   \nu1 u2 ... uN\ns1 A(f(s1, u1) A(f(s1, u2) . . . A(f(s1, uN ) s2 A(f(s2, u1) A(f(s2, u2) . . . A(f(s2, uN ) ... ... ... . . . ...\nsd A(f(sd, u1) A(f(sd, u2) . . . A(f(sd, uN )\n\n   \nFigure 1: Senone-based utterance VSM. Column vectors of the matrix correspond to the speech utterance vector representation formed using equation 1. d is the size of the senone dictionary, and N is the total number of speech utterances in the dialectal speech database."
    }, {
      "heading" : "2.2. Word based Utterance VSM",
      "text" : "The word-based utterance VSM (Uw) is constructed in two steps in a manner similar to the senone features: An ASR system is used to extract the word sequence for each utterance in the speech database. Details about the ASR system can be found in [11]. Each speech utterance (u) is then represented as a high-dimensional sparse vector ( #»u ):\n#»u = (A(f(u,w1)), A(f(u,w2)), . . . , A(f(u,wd′))) , (2)\nwhere f(u, wi) is the number of times a word wi occurs in the speech utterance u and A is the scaling function which has the same interpretation as for Us (above). Vocabulary size was 55k. The tri-gram dictionary size was 580k which we used to construct the word based VSM"
    }, {
      "heading" : "2.3. i-vector-based Utterance VSM",
      "text" : ""
    }, {
      "heading" : "2.3.1. Bottleneck Features (BN)",
      "text" : "Recently, bottleneck features extracted from an ASR DNNbased model were applied successfully to language identification [29, 30, 28]. In this paper, we used a similar bottleneck features configurations as in our previous ASR-DNN system for MSA speech recognition [27]. This system is based on two\nsuccessive DNN models. Both DNNs use the same setup of 5 hidden sigmoid layers and 1 linear BN layer, and they were both based on tied-states as target outputs. The senone labels of dimension 3040 are generated by a forced alignment from an HMM-GMM baseline trained on 60 hours of manually transcribed Al-Jazeera MSA news recordings [11]. The input to the first DNN consists of 23 critical-band energies that are obtained from Mel filter-bank. Pitch and voicing probability are then added. 11 consecutive frames are then stacked together. The second DNN is used for correcting the posterior outputs of the first DNN. In this architecture, the input features of the second DNN are the outputs of the BN layer from the first DNN. Context expansion is achieved by concatenating frames with time offsets of -10, -5, 0, 5, and 10. Thus, the overall time context seen by the second DNN is 31 frames."
    }, {
      "heading" : "2.3.2. Modeling",
      "text" : "An effective and well-studied method in language and dialect recognition is the i-vector approach [7, 16, 17]. The i-vector involves modeling speech using a universal background model (UBM) – typically a large GMM – trained on a large amount of data to represent general feature characteristics, which plays a role of a prior on how all dialects look like. The i-vector approach is a powerful technique that summarizes all the updates happening during the adaptation of the UBM mean components to a given utterance. All this information is modeled in a low dimensional subspace referred to as the total variability space. In the i-vector framework, each speech utterance can be represented by a GMM supervector, which is assumed to be generated as follows:\nM = u+ Tv\nWhere u is the channel and dialect independent supervector (which can be taken to be the UBM supervector), T spans a low-dimensional subspace and v are the factors that best describe the utterance-dependent mean offset. The vector v is treated as a latent variable with the i-vector being its maximuma-posteriori (MAP) point estimate. The subspace matrix T is estimated using maximum likelihood on large training dataset. An efficient procedure for training and for MAP adaptation of ivector can be found in [18]. In this approach, the i-vector is the low-dimensional representation of an audio recording that can be used for classification and estimation purposes. In our experiments, the UBM was a GMM with 2048 components, BN features were used, and the i-vectors were 400-dimensional.\nIn order to maximize the discrimination between the different dialect classes in the i-vector space, we combine Linear Discriminant Analysis (LDA) and Within Class Co-variance Normalization [17]. This intersession compensation method has been used with both SVM [17] and cosine scoring [7]."
    }, {
      "heading" : "3. Dataset",
      "text" : ""
    }, {
      "heading" : "3.1. Train Data",
      "text" : "The training corpus was collected from the Broadcast News domain in four Arabic dialects (EGY, LAV, GLF, and NOR) as well as MSA. Data recordings were carried out at 16Khz. The recordings were segmented to avoid speaker overlap, removing any non-speech parts such as music and background noise. More details about the training data can be found in [7]. Although the test database came from the same broadcast domain, the recording setup is different. The test data was downloaded directly from the high quality video server for Aljazeera (bright-\ncove) over the period of July 2104 until January 2015, as part of QCRI Advanced Transcription Service (QATS) [19]."
    }, {
      "heading" : "3.2. Test Data",
      "text" : "The test set was labeled using the crowdsource platform CrowdFlower, with the criteria to have a minimum of three judges per file and up to nine judges, or 75% inter-annotator agreement (whichever comes first). More details about the test set and crowdsourcing experiment can be found in [20]. The test set used in this paper differs from that used in [7] for two reasons: First, the crowdsourced data is available to reproduce the results, and thus can be used as a standard test set for Arabic DID; second, the new test set has been collected using different channels, and recording setup compared to the training data, which makes our experiments less sensitive to channel/speaker characteristics.\nThe train and test data can be found on the QCRI web portal2. Table 2 and Table 3 present some statistics about the train and the test data."
    }, {
      "heading" : "4. Experiments",
      "text" : ""
    }, {
      "heading" : "4.1. Choosing the Best Classifier",
      "text" : "We first studied the best classification approach for the DID task from a set of two generative models: n-gram language model [21] and Naive Bayes [22], and two discriminative classifiers: linear SVM [23] and Maximum Entropy [24]. We measured the performance of each model on the DID task, in the word or lexical-based utterance vector space, which is constructed using the approach mentioned in section 2, using identity scaling function A, and performing no dimensionality reduction. Hence, the dimensionality of an utterance vector, #»u , is the same as the size of the lexicon, which in our case was 55k. Results can be seen in table 4. As the linear SVM performs the best, it is our choice of classifier for the rest of the experiments."
    }, {
      "heading" : "4.2. Feature Selection Study",
      "text" : "Here we examine the dialect information captured by the three utterance VSMs explained in section 2. We also explore the concatenation of the utterance vector representations, and report the results in Tables 5 and 6. Details about the terms in the results table are given below:\n• Uiw: Refers to the utterance VSM in which each utterance is represented by a vector given by equation 2, where A is chosen to be the identity function. The bases\n2http://alt.qcri.org/resources/ArabicDialectIDCorpus/\nModel ACC PRC RCL\nn-gram Language Model 40.4% 40.2% 41.3%\nNaive Bayes 37.9% 37.5% 50.2%\nMax Ent 40% 40% 40.6%\nSVM 45.2% 44.8% 45.4%"
    }, {
      "heading" : "4.3. One Vs All classification (Sanity Check)",
      "text" : "We constructed a senone-based utterance VSM (section 2.1) based on 20 hours of speech; 10 hours English (which we got from [?]) and 10 hours Arabic (randomly sampled from our training data, section 3). Binary classification (English vs Arabic) using an SVM classifier, was then performed and it yielded 100% accuracy on the 1.5 hour test set. The reason to choose the senone-based feature space and not the i-vector-based feature space for classification is to avoid channel mismatch, as the English data came from a different source domain. We did a similar experiment to classify MSA versus all dialectal Arabic and again obtained 100% classification accuracy."
    }, {
      "heading" : "4.4. System Output Combination",
      "text" : "We fused the scores of the best senone system and the SVMbased i-vector system. In the fusion steps, the original scores of each system were normalized and combined using the same fusion weights for both systems. This approach yielded a final accuracy of 60.2%, which is the best performance we achieved. One explanation for this gain is that the error patterns for the two feature spaces are quite different, and we were able to confirm that by analyzing the confusion matrix for each system."
    }, {
      "heading" : "5. Discussion",
      "text" : "We infer from the confusion matrix in Table 7 that GLF and LAV are the most confusable dialect pair. We believe that this is related to the greater lexical similarity between these two dialects (see Table 1). Note, the confusion matrix is from the best DID system. We borrowed Table 8 from previous work [20] on the test set, which shows the amount of time the same speakers switch between dialect and another (mainly MSA, and their own native dialect). For example, in the second row of Table 8, there are 200 samples from potential Gulf speakers. After manually labeling, there were 106(53%) segments labeled as MSA,\n82(41%) validated as GLF, 8(4%) as LAV, and 4 segments were not identified with enough confidence to be considered. This means more than 50% of the random GLF speakers data is infact MSA speech segments. This is strong evidence for the amount of code-switching between one dialect and MSA from the same speaker."
    }, {
      "heading" : "6. Conclusions",
      "text" : "This paper presents our efforts on automatic dialect identification for Arabic broadcast speech. We have demonstrated a dialect classifier with an accuracy of 60.2% using system combination. We also achieved 100% accuracy on two binary classification tasks; MSA vs Dialectal Arabic and English vs Arabic. We studied the potential code-switching pattern in our classifier and its correlation with the manual annotation. Further work for this research is to study the code-switch between MSA and dialectal Arabic without considering speaker diarization or silence between speech segments in what can be called dialect diarization. We shall also study deep neural network approaches of classification to learn a more complex non-linear decision boundary."
    }, {
      "heading" : "7. References",
      "text" : "[1] G. Liu, Y. Lei, and J. H. Hansen, “Dialect identification: Impact of\ndifferences between read versus spontaneous speech,” EUSIPCO2010: European Signal Processing Conference, Aalborg, Denmark, 2010.\n[2] D. A. Reynolds, W. M. Campbell, W. Shen, and E. Singer, “Automatic language recognition via spectral and token based approaches,” in Springer Handbook of Speech Processing, J. Benesty, M. M. Sondhi, and Y. Huang, Eds. Springer, 2008.\n[3] E. Ambikairajah, H. Li, L. Wang, B. Yin, and V. Sethu, “Language identification: A tutorial,” Circuits and Systems Magazine, IEEE, vol. 11, no. 2, pp. 82–108.\n[4] M. Zissman, “Comparison of four approaches to automatic language identification of telephone speech,” IEEE Transactions on Speech and Audio Processing, vol. 4, no. 1, pp. 31–44, 1996.\n[5] D. Martı́nez, L. Burget, L. Ferrer, and N. Scheffer, “ivector-based prosodic system for language identification,” in ICASSP, 2012, pp. 4861–4864.\n[6] O. Plchot, M. Diez, M. Soufifar, and L. Burget, “Pllr features in language recognition system for rats,” in Fifteenth Annual Conference of the International Speech Communication Association, 2014.\n[7] M. H. Bahari, N. Dehak, L. Burget, A. Ali, J. Glass et al., “Nonnegative factor analysis for gmm weight adaptation,” IEEE Transactions on Audio Speech and Language Processing, 2014.\n[8] H. Soltau, L. Mangu, and F. Biadsy, “From modern standard arabic to levantine asr: Leveraging gale for dialects,” in ASRU, 2011, pp. 266–271.\n[9] M. Soufifar, S. Cumani, L. Burget, and J. Černocky, “Discriminative classifiers for phonotactic language recognition with ivectors,” in ICASSP, 2012, pp. 4853–4856.\n[10] M.-y. Hwang, “Subphonetic Modeling for Speech Recognition,” pp. 174–179.\n[11] A. Ali, Y. Zhang, P. Cardinal, N. Dahak, S. Vogel, and J. Glass, “A complete kaldi recipe for building arabic speech recognition systems,” in SLT, 2014.\n[12] J. Ramos, J. Eden, and R. Edu, “Using TF-IDF to Determine Word Relevance in Document Queries.”\n[13] G. Salton, a. Wong, and C. S. Yang, “A Vector Space Model for Automatic Indexing,” Magazine Communications of the ACM, vol. 18, no. 11, pp. 613–620, 1975.\n[14] W. Lowe, S. Mcdonald, W. Lowe, and S. Mcdonald, “Division of Informatics , University of Edinburgh Institute for Adaptive and Neural Computation The Direct Route : Mediated Priming in Semantic Space by The Direct Route : Mediated Priming in Semantic Space,” no. April, 2000.\n[15] S. Padó and M. Lapata, “Dependency-Based Construction of Semantic Space Models,” Computational Linguistics, vol. 33, no. December 2004, pp. 161–199, 2007.\n[16] N. Dehak, P. Kenny, R. Dehak, P. Dumouchel, and P. Ouellet, “Front-end factor analysis for speaker verification,” Audio, Speech, and Language Processing, IEEE Transactions on, pp. 788–798, 2011.\n[17] N. Dehak, P. A. Torres-Carrasquillo, D. A. Reynolds, and R. Dehak, “Language recognition via i-vectors and dimensionality reduction,” in INTERSPEECH, 2011, pp. 857–860.\n[18] P. Kenny, P. Ouellet, N. Dehak, V. Gupta, and P. Dumouchel, “A study of interspeaker variability in speaker verification,” Audio, Speech, and Language Processing, IEEE Transactions on, pp. 980–988, 2008.\n[19] A. Ali, Y. Zhang, and S. Vogel, “QCRI advanced transcription ssystem (QATS),” in SLT, 2014.\n[20] S. Wray and A. Ali, “Crowdsource a little to label a lot: Labeling a speech corpus of dialectal arabic,” in INTERSPEECH, 2015.\n[21] M. Collins, “Language Modeling.”\n[22] A. Ng, “CS229 Lecture notes Generative Learning algorithms,” no. 0, pp. 1–14.\n[23] H. Drucker, D. Wu, and V. N. Vapnik, “Support vector machines for spam categorization,” Neural Networks, IEEE Transactions on, vol. 10, no. 5, pp. 1048–1054, 1999.\n[24] K. Nigam, J. Lafferty, and A. Mccallum, “Using Maximum Entropy for Text Classification.”\n[25] S. Meignier and T. Merlin, “Lium spkdiarization: an open source toolkit for diarization,” in CMU SPUD Workshop, 2010.\n[26] C.-h. Wu, H.-p. Shen, and C.-s. Hsu, “Code-Switching Event Detection by Using a Latent Language Space Model and the DeltaBayesian,” vol. 23, no. 11, pp. 1892–1903, 2015.\n[27] P. Cardinal, N. Dehak, Y. Zhang, and J. Glass, “Speaker Adaptation Using the I-Vector Technique for Bottleneck Features,” in INTERSPEECH, 2015.\n[28] F. Richardson, D. Reynolds, and N. Dehak, “A Unified Deep Neural Network for Speaker and Language Recognition,” in INTERSPEECH, 2015.\n[29] Y. Song, B. Jiang, Y. Bao, S. Wei, and L. R. Dai, “I-vector representation based on bottleneck features for language identification,” IEEE Electronics Letters, 2013.\n[30] P. Matejka, L. Zhang, T. Ng, H. S. Mallidi, O. Glembek, J. Ma, and B. Zhang, “Neural network bottleneck features for language identification,” in Proc. of Odyssey, 2014.\nThis figure \"DialectDetectionSpace.png\" is available in \"png\" format from:\nhttp://arxiv.org/ps/1509.06928v2\nThis figure \"bnf.png\" is available in \"png\" format from:\nhttp://arxiv.org/ps/1509.06928v2"
    } ],
    "references" : [ {
      "title" : "Dialect identification: Impact of differences between read versus spontaneous speech",
      "author" : [ "G. Liu", "Y. Lei", "J.H. Hansen" ],
      "venue" : "EUSIPCO- 2010: European Signal Processing Conference, Aalborg, Denmark, 2010.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Automatic language recognition via spectral and token based approaches",
      "author" : [ "D.A. Reynolds", "W.M. Campbell", "W. Shen", "E. Singer" ],
      "venue" : "Springer Handbook of Speech Processing, J. Benesty, M. M. Sondhi, and Y. Huang, Eds. Springer, 2008.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Language identification: A tutorial",
      "author" : [ "E. Ambikairajah", "H. Li", "L. Wang", "B. Yin", "V. Sethu" ],
      "venue" : "Circuits and Systems Magazine, IEEE, vol. 11, no. 2, pp. 82–108.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 0
    }, {
      "title" : "Comparison of four approaches to automatic language identification of telephone speech",
      "author" : [ "M. Zissman" ],
      "venue" : "IEEE Transactions on Speech and Audio Processing, vol. 4, no. 1, pp. 31–44, 1996.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "ivector-based prosodic system for language identification",
      "author" : [ "D. Martı́nez", "L. Burget", "L. Ferrer", "N. Scheffer" ],
      "venue" : "ICASSP, 2012, pp. 4861–4864.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Pllr features in language recognition system for rats",
      "author" : [ "O. Plchot", "M. Diez", "M. Soufifar", "L. Burget" ],
      "venue" : "Fifteenth Annual Conference of the International Speech Communication Association, 2014.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Nonnegative factor analysis for gmm weight adaptation",
      "author" : [ "M.H. Bahari", "N. Dehak", "L. Burget", "A. Ali", "J. Glass" ],
      "venue" : "IEEE Transactions on Audio Speech and Language Processing, 2014.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "From modern standard arabic to levantine asr: Leveraging gale for dialects",
      "author" : [ "H. Soltau", "L. Mangu", "F. Biadsy" ],
      "venue" : "ASRU, 2011, pp. 266–271.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Discriminative classifiers for phonotactic language recognition with ivectors",
      "author" : [ "M. Soufifar", "S. Cumani", "L. Burget", "J. Černocky" ],
      "venue" : "ICASSP, 2012, pp. 4853–4856.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Subphonetic Modeling for Speech Recognition",
      "author" : [ "M.-y. Hwang" ],
      "venue" : "pp. 174–179.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 0
    }, {
      "title" : "A complete kaldi recipe for building arabic speech recognition systems",
      "author" : [ "A. Ali", "Y. Zhang", "P. Cardinal", "N. Dahak", "S. Vogel", "J. Glass" ],
      "venue" : "SLT, 2014.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "A Vector Space Model for Automatic Indexing",
      "author" : [ "G. Salton", "a. Wong", "C.S. Yang" ],
      "venue" : "Magazine Communications of the ACM, vol. 18, no. 11, pp. 613–620, 1975.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 1975
    }, {
      "title" : "Division of Informatics , University of Edinburgh Institute for Adaptive and Neural Computation The Direct Route : Mediated Priming in Semantic Space by The Direct Route : Mediated Priming in Semantic Space",
      "author" : [ "W. Lowe", "S. Mcdonald", "W. Lowe", "S. Mcdonald" ],
      "venue" : "no. April, 2000.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Dependency-Based Construction of Semantic Space Models",
      "author" : [ "S. Padó", "M. Lapata" ],
      "venue" : "Computational Linguistics, vol. 33, no. December 2004, pp. 161–199, 2007.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Front-end factor analysis for speaker verification",
      "author" : [ "N. Dehak", "P. Kenny", "R. Dehak", "P. Dumouchel", "P. Ouellet" ],
      "venue" : "Audio, Speech, and Language Processing, IEEE Transactions on, pp. 788–798, 2011.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Language recognition via i-vectors and dimensionality reduction",
      "author" : [ "N. Dehak", "P.A. Torres-Carrasquillo", "D.A. Reynolds", "R. Dehak" ],
      "venue" : "INTERSPEECH, 2011, pp. 857–860.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "A study of interspeaker variability in speaker verification",
      "author" : [ "P. Kenny", "P. Ouellet", "N. Dehak", "V. Gupta", "P. Dumouchel" ],
      "venue" : "Audio, Speech, and Language Processing, IEEE Transactions on, pp. 980–988, 2008.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "QCRI advanced transcription ssystem (QATS)",
      "author" : [ "A. Ali", "Y. Zhang", "S. Vogel" ],
      "venue" : "SLT, 2014.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Crowdsource a little to label a lot: Labeling a speech corpus of dialectal arabic",
      "author" : [ "S. Wray", "A. Ali" ],
      "venue" : "INTERSPEECH, 2015.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "CS229 Lecture notes Generative Learning algorithms",
      "author" : [ "A. Ng" ],
      "venue" : "no. 0, pp. 1–14.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 0
    }, {
      "title" : "Support vector machines for spam categorization",
      "author" : [ "H. Drucker", "D. Wu", "V.N. Vapnik" ],
      "venue" : "Neural Networks, IEEE Transactions on, vol. 10, no. 5, pp. 1048–1054, 1999.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Lium spkdiarization: an open source toolkit for diarization",
      "author" : [ "S. Meignier", "T. Merlin" ],
      "venue" : "CMU SPUD Workshop, 2010.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Code-Switching Event Detection by Using a Latent Language Space Model and the Delta- Bayesian",
      "author" : [ "C.-h. Wu", "H.-p. Shen", "C.-s. Hsu" ],
      "venue" : "vol. 23, no. 11, pp. 1892–1903, 2015.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 1892
    }, {
      "title" : "Speaker Adaptation Using the I-Vector Technique for Bottleneck Features",
      "author" : [ "P. Cardinal", "N. Dehak", "Y. Zhang", "J. Glass" ],
      "venue" : "INTERSPEECH, 2015.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "A Unified Deep Neural Network for Speaker and Language Recognition",
      "author" : [ "F. Richardson", "D. Reynolds", "N. Dehak" ],
      "venue" : "INTER- SPEECH, 2015.",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "I-vector representation based on bottleneck features for language identification",
      "author" : [ "Y. Song", "B. Jiang", "Y. Bao", "S. Wei", "L.R. Dai" ],
      "venue" : "IEEE Electronics Letters, 2013.",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Neural network bottleneck features for language identification",
      "author" : [ "P. Matejka", "L. Zhang", "T. Ng", "H.S. Mallidi", "O. Glembek", "J. Ma", "B. Zhang" ],
      "venue" : "Proc. of Odyssey, 2014. This figure \"DialectDetectionSpace.png\" is available in \"png\" format from: http://arxiv.org/ps/1509.06928v2 This figure \"bnf.png\" is available in \"png\" format from: http://arxiv.org/ps/1509.06928v2",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "The importance of addressing DID can be gauged from its growing interest in the Automatic Speech Recognition (ASR) community [1].",
      "startOffset" : 125,
      "endOffset" : 128
    }, {
      "referenceID" : 1,
      "context" : "In the lexical area, words, roots, morphology, and grammars [2, 3] have been studied.",
      "startOffset" : 60,
      "endOffset" : 66
    }, {
      "referenceID" : 2,
      "context" : "In the lexical area, words, roots, morphology, and grammars [2, 3] have been studied.",
      "startOffset" : 60,
      "endOffset" : 66
    }, {
      "referenceID" : 15,
      "context" : "Acoustic features such as shifted delta cepstral coefficients [17] and prosodic features [5] using Gaussian mixture models (GMMs), i-vector representations and support vector machine (SVM) classifiers [17] have been shown to be effective for LID.",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 4,
      "context" : "Acoustic features such as shifted delta cepstral coefficients [17] and prosodic features [5] using Gaussian mixture models (GMMs), i-vector representations and support vector machine (SVM) classifiers [17] have been shown to be effective for LID.",
      "startOffset" : 89,
      "endOffset" : 92
    }, {
      "referenceID" : 15,
      "context" : "Acoustic features such as shifted delta cepstral coefficients [17] and prosodic features [5] using Gaussian mixture models (GMMs), i-vector representations and support vector machine (SVM) classifiers [17] have been shown to be effective for LID.",
      "startOffset" : 201,
      "endOffset" : 205
    }, {
      "referenceID" : 5,
      "context" : "More recent work explored the use of frame-by-frame phone posteriors (PLLRs) [6] as new features for LID.",
      "startOffset" : 77,
      "endOffset" : 80
    }, {
      "referenceID" : 6,
      "context" : "New subspace approaches based on non-negative factor analysis (NFA) for GMM weight decomposition and adaptation [7] were also applied to both LID and DID tasks.",
      "startOffset" : 112,
      "endOffset" : 115
    }, {
      "referenceID" : 7,
      "context" : "Finally, phoneme sequence modeling and its n-gram subspace have been studied for both Arabic DID [8] and LID [9].",
      "startOffset" : 97,
      "endOffset" : 100
    }, {
      "referenceID" : 8,
      "context" : "Finally, phoneme sequence modeling and its n-gram subspace have been studied for both Arabic DID [8] and LID [9].",
      "startOffset" : 109,
      "endOffset" : 112
    }, {
      "referenceID" : 9,
      "context" : "VSM construction takes place in two steps: first, a phoneme recognizer is used to extract the senone [10] sequence for a given",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 10,
      "context" : "Further details about the speech recognition pipeline, training data, and phoneme set is given in [11].",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 11,
      "context" : "This approach and the notation used to define a VSM is directly inspired by the seminal works in the area of VSM of Natural Language in [13, 14, 15] and in LID [?].",
      "startOffset" : 136,
      "endOffset" : 148
    }, {
      "referenceID" : 12,
      "context" : "This approach and the notation used to define a VSM is directly inspired by the seminal works in the area of VSM of Natural Language in [13, 14, 15] and in LID [?].",
      "startOffset" : 136,
      "endOffset" : 148
    }, {
      "referenceID" : 13,
      "context" : "This approach and the notation used to define a VSM is directly inspired by the seminal works in the area of VSM of Natural Language in [13, 14, 15] and in LID [?].",
      "startOffset" : 136,
      "endOffset" : 148
    }, {
      "referenceID" : 10,
      "context" : "Details about the ASR system can be found in [11].",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 25,
      "context" : "Recently, bottleneck features extracted from an ASR DNNbased model were applied successfully to language identification [29, 30, 28].",
      "startOffset" : 120,
      "endOffset" : 132
    }, {
      "referenceID" : 26,
      "context" : "Recently, bottleneck features extracted from an ASR DNNbased model were applied successfully to language identification [29, 30, 28].",
      "startOffset" : 120,
      "endOffset" : 132
    }, {
      "referenceID" : 24,
      "context" : "Recently, bottleneck features extracted from an ASR DNNbased model were applied successfully to language identification [29, 30, 28].",
      "startOffset" : 120,
      "endOffset" : 132
    }, {
      "referenceID" : 23,
      "context" : "In this paper, we used a similar bottleneck features configurations as in our previous ASR-DNN system for MSA speech recognition [27].",
      "startOffset" : 129,
      "endOffset" : 133
    }, {
      "referenceID" : 10,
      "context" : "The senone labels of dimension 3040 are generated by a forced alignment from an HMM-GMM baseline trained on 60 hours of manually transcribed Al-Jazeera MSA news recordings [11].",
      "startOffset" : 172,
      "endOffset" : 176
    }, {
      "referenceID" : 6,
      "context" : "An effective and well-studied method in language and dialect recognition is the i-vector approach [7, 16, 17].",
      "startOffset" : 98,
      "endOffset" : 109
    }, {
      "referenceID" : 14,
      "context" : "An effective and well-studied method in language and dialect recognition is the i-vector approach [7, 16, 17].",
      "startOffset" : 98,
      "endOffset" : 109
    }, {
      "referenceID" : 15,
      "context" : "An effective and well-studied method in language and dialect recognition is the i-vector approach [7, 16, 17].",
      "startOffset" : 98,
      "endOffset" : 109
    }, {
      "referenceID" : 16,
      "context" : "An efficient procedure for training and for MAP adaptation of ivector can be found in [18].",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 15,
      "context" : "In order to maximize the discrimination between the different dialect classes in the i-vector space, we combine Linear Discriminant Analysis (LDA) and Within Class Co-variance Normalization [17].",
      "startOffset" : 190,
      "endOffset" : 194
    }, {
      "referenceID" : 15,
      "context" : "This intersession compensation method has been used with both SVM [17] and cosine scoring [7].",
      "startOffset" : 66,
      "endOffset" : 70
    }, {
      "referenceID" : 6,
      "context" : "This intersession compensation method has been used with both SVM [17] and cosine scoring [7].",
      "startOffset" : 90,
      "endOffset" : 93
    }, {
      "referenceID" : 6,
      "context" : "More details about the training data can be found in [7].",
      "startOffset" : 53,
      "endOffset" : 56
    }, {
      "referenceID" : 17,
      "context" : "cove) over the period of July 2104 until January 2015, as part of QCRI Advanced Transcription Service (QATS) [19].",
      "startOffset" : 109,
      "endOffset" : 113
    }, {
      "referenceID" : 18,
      "context" : "More details about the test set and crowdsourcing experiment can be found in [20].",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 6,
      "context" : "The test set used in this paper differs from that used in [7] for two reasons: First, the crowdsourced data is available to reproduce the results, and thus can be used as a standard test set for Arabic DID; second, the new test set has been collected using different channels, and recording setup compared to the training data, which makes our experiments less sensitive to channel/speaker characteristics.",
      "startOffset" : 58,
      "endOffset" : 61
    }, {
      "referenceID" : 19,
      "context" : "We first studied the best classification approach for the DID task from a set of two generative models: n-gram language model [21] and Naive Bayes [22], and two discriminative classifiers: linear SVM [23] and Maximum Entropy [24].",
      "startOffset" : 147,
      "endOffset" : 151
    }, {
      "referenceID" : 20,
      "context" : "We first studied the best classification approach for the DID task from a set of two generative models: n-gram language model [21] and Naive Bayes [22], and two discriminative classifiers: linear SVM [23] and Maximum Entropy [24].",
      "startOffset" : 200,
      "endOffset" : 204
    }, {
      "referenceID" : 15,
      "context" : "We do not experiment with different i-vector dimensions and take the best dimension reported in [17] for the LID task.",
      "startOffset" : 96,
      "endOffset" : 100
    }, {
      "referenceID" : 15,
      "context" : "• U iVec+LDA+WCNN: Reducing the dimensionality of the i-vector space using LDA and performing WCNN has been reported to do well in LID tasks [17] and we use the same technique and see a significant improvement in the DID results.",
      "startOffset" : 141,
      "endOffset" : 145
    }, {
      "referenceID" : 18,
      "context" : "We borrowed Table 8 from previous work [20] on the test set, which shows the amount of time the same speakers switch between dialect and another (mainly MSA, and their own native dialect).",
      "startOffset" : 39,
      "endOffset" : 43
    } ],
    "year" : 2016,
    "abstractText" : "In this paper, we investigate different approaches for dialect identification in Arabic broadcast speech. These methods are based on phonetic and lexical features obtained from a speech recognition system, and bottleneck features using the i-vector framework. We studied both generative and discriminative classifiers, and we combined these features using a multi-class Support Vector Machine (SVM). We validated our results on an Arabic/English language identification task, with an accuracy of 100%. We also evaluated these features in a binary classifier to discriminate between Modern Standard Arabic (MSA) and Dialectal Arabic, with an accuracy of 100%. We further reported results using the proposed methods to discriminate between the five most widely used dialects of Arabic: namely Egyptian, Gulf, Levantine, North African, and MSA, with an accuracy of 59.2%. We discuss dialect identification errors in the context of dialect code-switching between Dialectal Arabic and MSA, and compare the error pattern between manually labeled data, and the output from our classifier. All the data used on our experiments have been released to the public as a language identification corpus.",
    "creator" : "LaTeX with hyperref package"
  }
}