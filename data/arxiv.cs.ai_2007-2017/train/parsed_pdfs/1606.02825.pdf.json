{
  "name" : "1606.02825.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Arbitrage-Free Combinatorial Market Making via Integer Programming",
    "authors" : [ "CHRISTIAN KROER", "SIVARAMAN BALAKRISHNAN" ],
    "emails" : [ "ckroer@cs.cmu.edu;", "mdudik@microsoft.com;", "slahaie@microsoft.com;", "siva@stat." ],
    "sections" : [ {
      "heading" : null,
      "text" : "Arbitrage-Free Combinatorial Market Making via Integer Programming\nCHRISTIAN KROER, Carnegie Mellon University MIROSLAV DUDÍK, Microsoft Research SÉBASTIEN LAHAIE, Microsoft Research SIVARAMAN BALAKRISHNAN, Carnegie Mellon University\nWe present a new combinatorial market maker that operates arbitrage-free combinatorial prediction markets specified by integer programs. Although the problem of arbitrage-free pricing, while maintaining a bound on the subsidy provided by the market maker, is #P-hard in the worst case, we posit that the typical case might be amenable to modern integer programming (IP) solvers. At the crux of our method is the Frank-Wolfe (conditional gradient) algorithm which is used to implement a Bregman projection aligned with the market maker’s cost function, using an IP solver as an oracle. We demonstrate the tractability and improved accuracy of our approach on real-world prediction market data from combinatorial bets placed on the 2010 NCAA Men’s Division I Basketball Tournament, where the outcome space is of size 263. To our knowledge, this is the first implementation and empirical evaluation of an arbitrage-free combinatorial prediction market on this scale."
    }, {
      "heading" : "1. INTRODUCTION",
      "text" : "Prediction markets have been successfully used to elicit and aggregate forecasts in a variety of domains, including business [Charette 2007; Spann and Skiera 2003], politics [Berg et al. 2008], and entertainment [Pennock et al. 2002]. In a prediction market, traders buy and sell securities with values that depend on some unknown future outcome. For instance, a play-money prediction market that Yahoo! ran for the 2010 NCAA Men’s Division I Basketball Tournament included a security that paid out 1 point if the team from Duke were to win the championship and 0 points otherwise. Thus, when the price of the security was 0.15, traders who believed that Duke’s probability of winning was larger than 0.15 were incentivized to buy shares of the security, and those that believed it was lower were incentivized to sell. The market price can be interpreted as an aggregate belief and used as a forecast.\nWe study prediction markets implemented by a centralized algorithm called a costbased market maker [Abernethy et al. 2011; Chen and Pennock 2007]. All shares are bought from and sold to the market maker, rather than between traders, and the market maker uses a convex potential function to determine current security prices. Compared with an exchange, which matches buyers and sellers, a market-maker mechanism is particularly desirable in combinatorial markets, which offer securities on interrelated propositions. For instance, the NCAA 2010 market included securities on events “Duke wins more games than Cornell” and “a team from the Big East conference wins the championship” as well as many others. Because of the large number of securities in combinatorial markets, there may be no sellers interested in trading with a given buyer,\nAuthor addresses: C. Kroer, Computer Science Dept, CMU; ckroer@cs.cmu.edu; M. Dudı́k and S. Lahaie, Microsoft Research; {mdudik,slahaie}@microsoft.com; S. Balakrishnan, Dept of Statistics, CMU; siva@stat. cmu.edu. This work was done while C. Kroer and S. Balakrishnan were at Microsoft Research. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. EC’16, July 24–28, 2016, Maastricht, The Netherlands. ACM 978-1-4503-3936-0/16/07 ...$15.00. Copyright is held by the owner/author(s). Publication rights licensed to ACM. http://dx.doi.org/10.1145/2940716.2940767\nar X\niv :1\n60 6.\n02 82\n5v 2\n[ cs\n.G T\n] 1\n0 Ju\nn 20\n16\na problem known as low liquidity. In contrast, a market maker is always available to trade, thus providing liquidity and allowing incorporation of information in the market.\nDesigners of cost-based markets aim to meet several desirable properties, including boundedness of loss suffered by the market maker and absence of arbitrage, that is, risk-free profitable trades. Bounded loss is a necessity for money markets, otherwise the market operator risks bankruptcy. Lack of arbitrage is also highly desirable. First, we would like to attract traders that provide information rather than computation. Second, arbitrage-free markets produce more accurate forecasts. While Abernethy et al. [2011] provide a complete theoretical characterization of cost-based markets with bounded loss and no arbitrage, pricing in such markets is NP-hard or #P-hard for even the simplest combinatorial settings [Chen et al. 2008]. Previous solutions restrict the betting language to allow polynomial time algorithms [e.g., Chen et al. 2007, 2008; Xia and Pennock 2011] or devise approximations [Dudı́k et al. 2012].\nIn this paper we move beyond the hardness barrier. We hypothesize that while the pricing may be difficult in the worst case, a typical case is amenable to modern integer programming solvers. Guided by this hypothesis, we propose a fully general, bounded-loss, arbitrage-free market maker based on integer programming (IP) methods. Our market maker is guaranteed to maintain bounded loss, and attempts to remove arbitrage by making calls to the IP solver.\nOur mechanism begins with any bounded-loss cost function and adds two ingredients. First, we use an integer program to specify the set of valid payoff vectors, each of which enumerates the security payoffs in a single outcome. Arbitrage-free prices are exactly the convex combinations of valid payoff vectors. Second, as we run the costbased market, we periodically remove arbitrage by projecting the market state onto the set of arbitrage-free prices, using the IP solver as an oracle within the projection algorithm. The integer program for payoff vectors is typically compact in size and easy to specify based on security definitions. For instance, when securities correspond to logical propositions, outcomes correspond to truth-assignment of literals, and each valid payoff vector enumerates 0 for false and 1 for true propositions in a given outcome. Conjunctions and disjunctions are easily expressed within an IP, so we have a compact representation for all problems in NP. To implement projection, we use the Frank-Wolfe algorithm [Frank and Wolfe 1956; Jaggi 2013], also known as the conditional gradient algorithm, which is well-suited to our setting because it only accesses the target set (in our case, the set of valid payoff vectors) through the operation of linear optimization, which can be handled by an IP solver. The projection that we consider is the Bregman projection, which generalizes the Euclidean projection to arbitrary convex potentials.\nThere are two specific issues in applying the Frank-Wolfe (FW) algorithm within cost-based markets. First, while all iterates of the FW algorithm are within the convex hull of valid payoff vectors, and therefore arbitrage-free, we need to ensure that bounded loss is maintained. In Sec. 4.2, we show how to achieve this by a suitable modification of the stopping condition of FW. The second, seemingly more serious concern, is that the projection problems that arise for common cost functions, such as Hanson’s [2003; 2007] logarithmic market scoring rule (LMSR), exhibit derivatives that go to infinity at the border of the set of arbitrage-free prices, which violates the assumptions of the FW algorithm. Fortunately, we can adapt a recently developed variant of FW [Krishnan et al. 2015], designed for the case when the derivative might grow to infinity, but its growth is suitably controlled, which is the case for LMSR.\nOur approach, which we call the Frank-Wolfe market maker (FWMM), is related to Dudı́k et al.’s [2012] linearly-constrained market maker (LCMM), which also alternates trades and (partial) arbitrage removal. While FWMM uses linear constraints in the IP to define valid payoff vectors, the arbitrage removal in LCMM is driven by a set of linear constraints on the arbitrage-free prices (i.e., the convex hull of valid payoff vectors). The\nIP constraints of FWMM can be used directly in LCMM, as linear-programming relaxations, but they are usually too loose, so tighter constraints need to be derived ad hoc for each new security type, sometimes using involved combinatorial reasoning [Dudı́k et al. 2012, 2013]. Since LCMM updates are usually substantially faster than solving an IP, the arbitrage-removal steps of LCMM and FWMM can be interleaved, and the more expensive projection step of FWMM should be invoked only after LCMM cannot remove much arbitrage.\nWe evaluate the efficacy of FWMM on Yahoo!’s NCAA 2010 basketball tournament prediction market data, from which we extracted 88k trades on 5k securities in a combinatorial market with 263 outcomes. Once the projections become practically fast, FWMM achieves superior accuracy to LCMM. Our experiments also show that the initial phase of the projection algorithm, which involves calls to the IP solver to decide which securities can be logically settled given the games completed so far, is fast even for the largest problem sizes. The results from this initial phase can be propagated as a partial outcome into the cost function, which yields an improvement over LCMM even when the overall projection algorithm is too slow.\nTournaments have previously been considered by Chen et al. [2008] and Xia and Pennock [2011]. Both focus on restricted (but non-trivial) tournament betting languages that yield tractability, but cannot, for instance, handle comparisons. In contrast, our approach works for general outcome spaces that can be represented by an IP, rather than only tournaments. Our work is closely related to the applications of Frank-Wolfe and integer programming to inference in graphical models [Belanger et al. 2013; Krishnan et al. 2015], but needs to address several issues specific to incentives and information revelation in prediction markets."
    }, {
      "heading" : "2. PRELIMINARIES",
      "text" : "We begin with an overview of cost-based market making [Abernethy et al. 2011; Chen and Pennock 2007] and then provide a high-level outline of our approach. As a running example we use the NCAA 2010 Tournament: a single-elimination tournament with 64 teams playing over 6 rounds, meaning that in each round, half of the remaining teams are eliminated."
    }, {
      "heading" : "2.1. Cost-based market making",
      "text" : "Let Ω denote a finite set of outcomes, corresponding to mutually exclusive and exhaustive states of the world. We are interested in eliciting expectations of binary random variables φi : Ω → {0, 1}, indexed by i ∈ I, which model the occurrence of various events such as “Duke wins the NCAA championship.” Each variable φi is associated with a security, which is a contract that pays out φi(ω) dollars when the outcome ω occurs. Therefore, the random variable φi is also called the payoff function. Binary securities pay out $1 if the specified event occurs and $0 otherwise. The vector (φi)i∈I is denoted φ. Traders buy bundles δ ∈ RI of security shares issued by a central market maker; negative entries in δ are permitted and correspond to short-selling. A trader holding a bundle δ receives a (possibly negative) payoff δ · φ(ω) when ω ∈ Ω occurs.\nFollowing Chen and Pennock [2007] and Abernethy et al. [2011], we assume that the market maker determines security prices using a convex and differentiable potential function C : RI → R called a cost function. The state of the market is specified by a vector θ ∈ RI listing the number of shares of each security sold so far by the market maker. A trader wishing to buy a bundle δ in the market state θ must pay C(θ+δ)−C(θ) to the market maker, after which the new state becomes θ + δ. Thus, the vector of instantaneous prices in the state θ is p(θ) := ∇C(θ). Its entries can be interpreted as market estimates of E[φi]: a trader can make an expected profit by buying (at least a small amount) of the security i if she believes that E[φi] is larger than the instantaneous\nprice pi(θ) = ∂C(θ)/∂θi, and by selling if she believes that E[φi] is lower than pi(θ); therefore, risk neutral traders with sufficient budgets maximize their expected profits by moving the price vector to match their expectation of φ.\nExample 2.1. Logarithmic market-scoring rule (LMSR). Hanson’s [2003; 2007] logarithmic market scoring rule (LMSR) is a cost function for a complete market. In a complete market, I = Ω and securities are indicators of individual outcomes, φi(ω) = 1{ω = i}, where 1{·} denotes the binary indicator, equal to 1 if true and 0 if false. Thus, traders can express arbitrary probability distributions over Ω. For instance, to set up a complete market for the number of wins of Duke in the six-round NCAA tournament, we would set I = Ω = {0, 1, . . . , 6}. LMSR has the form C(θ) = log( ∑ i∈I e θi)\nand prices pi(θ) = eθi/( ∑ j∈I e θj ).\nExample 2.2. Sum of independent markets. Now consider a market with 7 securities for the number of wins of Duke and an additional 7 securities for the number of wins of Cornell. The outcome space consists of pairs of numbers between 0 and 6, but not all pairs are possible, because if Duke and Cornell win rounds 1–4, they meet in round 5 and only one advances. Thus, Ω = {(ω1, ω2) ∈ {0, . . . , 6}2 : min{ω1, ω2} ≤ 4}. Securities are indexed by pairs I = {1, 2} × {0, . . . , 6}, with the first entry indicating the school and the second the number of wins, yielding the payoff functions φj,x(ω) = 1{ωj = x}. A natural cost function is the sum of LMSRs, C(θ) = ∑2 j=1 log( ∑6 x=0 e\nθj,x), which yields prices pj,x(θ) = eθj,x/( ∑6 y=0 e\nθj,y ). Thus, prices vary independently for each school, as if we ran two separate markets."
    }, {
      "heading" : "2.2. Arbitrage, marginal polytope and Bregman projection",
      "text" : "We consider two standard desiderata for cost-based markets. The first is the bounded loss property: there should be a constant which bounds the ultimate loss of the market maker once the outcome is determined, regardless of how many shares of each security are sold. The second is the no arbitrage property: there should be no trade that guarantees a positive profit, regardless of the outcome. Following Abernethy et al. [2011], we next relate bounded loss to properties of the convex conjugate of C, and review equivalence between optimal arbitrage removal and Bregman projection.\nGiven a cost function C, let R denote its convex conjugate,\nR(µ) := sup θ′∈RI\n[ θ′ · µ− C(θ′) ] , (1)\nwhich is itself a convex function on RI , allowed to take on the value∞. If the market is in a state θ = 0 and a trader believes that E[φ] = µ, then her expected profit for the bundle θ′ is θ′ ·µ− ( C(θ′)−C(0) ) , which is maximized by Eq. (1), omitting the constant\nterm C(0). More generally, the maximum expected profit of a trader with a belief µ in a market state θ can be shown to equal the mixed Bregman divergence, defined as\nD(µ‖θ) := R(µ) + C(θ)− θ · µ .\nConvex conjugacy implies that D(µ‖θ) ≥ 0, with equality if and only if µ = p(θ), which is equivalent to θ ∈ ∂R(µ), where ∂R is the subdifferential of R.\nExample 2.3. For the LMSR, R(µ) is equal to negative entropy whenever µ is a probability distribution and∞ otherwise, i.e., R(µ) = I{µ ∈ ∆}+ ∑ i∈I µi lnµi, where ∆ is the set of probability distributions on Ω and I{·} denotes the convex indicator, equal to 0 if true and∞ if false. Bregman divergence is the Kullback-Leibler (KL) divergence, D(µ‖θ) = I{µ ∈ ∆}+ ∑ i∈I µi ln (µi/pi(θ)), which is an information-theoretic measure of the difference between two probability distributions.\nLet Z := {φ(ω) : ω ∈ Ω} denote the (finite) set of all valid payoff vectors, andM be its convex hull, called the marginal polytope. The marginal polytope is exactly the set of vectors µ that can be written as expectations E[φ] under some probability distribution over Ω, so we refer to elements ofM as coherent beliefs or coherent prices. Abernethy et al. [2011] show that a cost-based market maker has the bounded loss property if and only if maxz∈Z R(z) <∞. We assume that this is the case for the conjugate of our cost C. Note that this assumption is satisfied for LMSR, because negative entropy equals zero at the vertices of the simplex. It is also satisfied in Example 2.2, where R(µ) is the sum of negative entropies of the two markets.\nGiven a state θ, we define the Bregman projection of θ onM as the point µ? := argmin\nµ∈M D(µ‖θ) .\nThe Bregman projection is related to an optimal arbitraging trade by the following standard result (the proof is in Appendix A for completeness):\nPROPOSITION 2.4. If the market is in a state θ, the guaranteed profit of any trader is at most D(µ?‖θ) where µ? is the Bregman projection of θ onM. Furthermore, this profit is achieved by any trade δ? moving the market to a state θ? with p(θ?) = µ?.\nThis means that an arbitrage opportunity exists whenever the prices are incoherent, since p(θ) 6∈ M implies that D(µ?‖θ) > 0. After the trade δ?, we have p(θ?) = µ? ∈M and thus there is no arbitrage opportunity in the market."
    }, {
      "heading" : "2.3. The outline of Frank-Wolfe market maker (FWMM)",
      "text" : "The mechanism proposed in this paper, called Frank-Wolfe market maker, alternates between processing trades according to the cost C and removing arbitrage. In the arbitrage removal step, our goal is to find the state θ? from Proposition 2.4. We do this by solving the Bregman projection problem using the Frank-Wolfe (FW) algorithm, which reduces the Bregman projection problem to a sequence of linear programs of the form\nmin µ∈M\nc · µ ,\nfor suitably chosen vectors c. Since the optimum of a linear program occurs at a vertex, reducing the Bregman projection problem to a sequence of linear programs results in an important simplification. Instead of specifying the marginal polytope M, whose description can be exponentially large in the number of securities, it suffices to describe its vertices Z, which we show can be done via a compact set of linear inequalities together with integer constraints. More precisely, we assume that the set Z is described by a matrix A and a vector b such that\nZ = { z ∈ {0, 1}I : A>z ≥ b } . (2)\nViewed in this way, the FW algorithm solves the Bregman projection problem by solving a sequence of integer programs. We refer to the linear constraints describing the set Z as IP constraints.\nExample 2.5. We next derive IP constraints for the market for the number of wins of Duke and Cornell from Example 2.2. First, there are exclusivity and exhaustivity constraints of the form ∑6 x=0 zj,x = 1 for j ∈ {1, 2}, corresponding to the fact that in any outcome ω, for each j, exactly one of the securities φj,x(ω) will equal 1 across x ∈ {0, . . . , 6}. However, these two constraints do not capture the fact that at most one of the teams can have exactly 5 or 6 wins. Specifically, in any outcome ω, we have\nφ1,5(ω) + φ2,5(ω) + φ1,6(ω) + φ2,6(ω) ≤ 1 .\nThus, we also include the third constraint: z1,5 + z2,5 + z1,6 + z2,6 ≤ 1. Our reasoning so far shows that any valid payoff vector satisfies the three mentioned constraints. It can be verified that any vector z satisfying these constraints is valid, i.e., it corresponds to φ(ω) for some ω ∈ Ω, so these three constraints correctly specify Z."
    }, {
      "heading" : "2.4. Linearly-constrained market maker (LCMM)",
      "text" : "The FW algorithm relies on the ability to solve integer programs (IPs), which can take exponential time in the worst case. Therefore, our mechanism also incorporates fast (poly-time) partial arbitrage removal similar to Dudı́k et al.’s [2012] linearly-constrained market maker (LCMM).\nIn LCMM, arbitrage is partly removed by considering a set of linear constraints that must be satisfied by coherent prices. Namely, an LCMM takes as an input a relaxation M̃ ⊇M described by linear constraints called LCMM constraints:\nM̃ = {µ ∈ RI : Ã>µ ≥ b̃} . When any LCMM constraint is violated, there is an arbitrage opportunity in the market, with an easy-to-compute arbitraging trade. LCMM acts as an arbitrager until none of the constraints are violated. Since M̃ is a relaxation ofM, the resulting state is not necessarily arbitrage-free.\nAssuming we have a description of Z using IP constraints specified by a matrix A and a vector b, one simple strategy is to construct M̃ as a linear-program (LP) relaxation of Z, i.e.,\nM̃ = {µ ∈ RI : µi ∈ [0, 1] for all i ∈ I and A>µ ≥ b} . (3) These constraints are satisfied by all z ∈ Z and hence also by their convex combinations µ ∈ M. Generally, this relaxation is only a loose superset of M, so various ad hoc strategies are required to obtain a tighter M̃ [Dudı́k et al. 2012, 2013]. We present one example of such a strategy in Sec. 3, for the class of comparison securities."
    }, {
      "heading" : "3. MARKET DESIGN",
      "text" : "We next show how to instantiate the market design elements of Sec. 2 in real-world combinatorial markets, including the NCAA 2010 tournament evaluated in Sec. 5. Namely, we need to define: (i) the payoff functionφ, (ii) the cost functionC, (iii) the initial market state θ, (iv) the IP constraints describing Z, and (v) the LCMM constraints describing M̃. We also need to consider how the cost and market state should be updated as the true outcome is gradually revealed over time. For example, in the NCAA tournament, 63 games play out over the course of several weeks and we would like to fix prices of securities whose payoff has already been determined."
    }, {
      "heading" : "3.1. Compositional market design",
      "text" : "We use a compositional market design along the lines of Dudı́k et al. [2013], which is a generalization of the sum of LMSRs structure of Example 2.2. The market construction begins with a collection of random variables Xj : Ω → Xj , indexed by j ∈ J , whose marginal distributions we wish to elicit, such as the number of wins of Duke and Cornell in Example 2.2. Securities are indexed by i = (j, x), with j ∈ J and x ∈ Xj , and correspond to indicators of the events Xj = x, i.e.,\nφj,x(ω) = 1{Xj(ω) = x} . The cost function is the sum of LMSRs across the random variables Xj :\nC(θ) = b ∑ j∈J ln (∑ x∈Xj e θj,x/b ) , (4)\nwhere b > 0 is the liquidity parameter controlling how fast the prices change in response to trading. A smaller value of b (lower liquidity) means prices rise faster as shares are purchased; a larger value of b (higher liquidity) yields slower changes. As in Example 2.2, Eq. (4) implies that we effectively run an independent LMSR market for each Xj . Thus, in the absence of arbitrage removal steps, we say that C implements the independent markets cost function.\nInitially, our market contains no random variables and hence no securities. The market operator can create new random variables and specify their relationship to any existing variables. At the time of creation of a new variable Xj , the operator specifies (i) its domain Xj , (ii) the mapping Xj(ω), (iii) initial prices µj,x across x ∈ Xj (these prices determine the initial-state coordinates θj,x), (iv) IP constraints to restrict zj,x across x ∈ Xj , and (v) LCMM constraints to restrict µj,x across x ∈ Xj . Due to the additive structure of the cost C, new variables Xj can be added at any time during the run of the market without affecting prices of existing securities.\nBelow we specify the items (i)–(v) for different types of random variables in our market. When describing the IP constraints on z and LCMM constraints on µ, we use the notation z{Xj = x} and µ{Xj = x} for the entries zj,x and µj,x, respectively. We also allow random variables with names other than Xj , e.g., X or Gr,t, and use the notation such as z{X = x} and µ{X = x} for the corresponding entries of z and µ.\nWhen adding a new random variable X, the initial prices µ{X = x} can be chosen based on the prices of the random variables present in the market. New IP constraints always include the exclusivity and exhaustivity constraint, ∑ x∈X z{X = x} = 1, but additional constraints may be needed to correctly describe the mapping X(ω). We add LCMM constraints using the simple strategy mentioned in Sec. 2.4, as an LP relaxation of IP constraints, with an exception of one variable type (comparison variables).\nOur market contains random variables of the following types:\nAtomic tournament variables. These random variables model outcomes in a singleelimination tournament with k rounds and 2k teams. Teams are numbered 1 through 2k. In the first round, there are 2k−1 games, between teams 2i− 1 and 2i, and the resulting 2k−1 winners advance to the second round, where again teams are matched in the order of increasing indices and the winners advance to the next round etc. The team t is associated with the random variable Xt whose outcome is the total number of wins of team t, i.e., Xt = {0, . . . , k}.\nWe also have random variables corresponding to the games played, with the outcome of each variable being the winner of the corresponding game. For a team t and round r, let Gr,t denote the game that the team t will play in the r-th round if it advances to that point. We are slightly abusing notation, because Gr,t and Gr,t′ can refer to the same game (and hence the same random variable) for distinct t and t′ (see Fig. 1). For instance Gk,t ≡ Gk,t′ for all t, t′, as there is only one game (the finals) in round k. With this notation in hand, we can introduce the IP constraints relating the entries of z\nrepresenting game and team variables:\nz{Xt = r} = z{Gr,t = t} − z{Gr+1,t = t} for all t and r < k, z{Xt = k} = z{Gk,t = t} for all t.\nLCMM constraints are just LP relaxations of the above, i.e., they are the same as the IP constraints, with z{·} replaced with µ{·}. The market operator needs to specify initial prices µ{Xt = r} and µ{Gk,t = t} explicitly, based for instance on the past performance of teams.\nSums. Given a set of existing random variables X1, . . . , Xn taking on integer values with the minimum and maximum values mj := minXj and Mj := maxXj , we define a new random variable X to represent their sum,\nX(ω) := X1(ω) + · · ·+Xn(ω) , with the domain X = {m,m + 1, . . . ,M} where m = ∑n j=1mj and M = ∑n j=1Mj . The initial prices are set proportional to a discretized Gaussian distribution with the mean and variance equal to the sum of means and variances of X1 through Xn, under the distribution described by the current prices µ{Xj = x}.\nWe introduce the following IP constraint:∑ x∈X x · z{X = x} = ∑n j=1 ∑ xj∈Xj xj · z{Xj = xj} .\nAs before, the added LCMM constraint is an LP relaxation of the added IP constraint.\nComparisons. Given two existing random variables X1 and X2 taking on integer values with the minimum and maximum values mj := minXj and Mj := maxXj , we define a new random variable X with the domain {lt, eq, gt} to represent the result of their comparison:\nX(ω) :=  lt if X1(ω) < X2(ω), eq if X1(ω) = X2(ω), gt if X1(ω) > X2(ω).\nThe initialization prices are determined by first considering an integer-valued variable Y = X2 −X1, and initializing its distribution to the discrete Gaussian with the mean equal to the difference of means and the variance initialized to the sum of variances of X2 and X1 under current prices. The initial prices of X = lt, X = eq and X = gt are obtained as probabilities that Y < 0, Y = 0 and Y > 0. The variable Y is discarded and is not part of the market.\nThe IP constraints for the new entries of z are based on the following four identities:\nX1 −X2 ≥ (m1 −M2)1{X1 < X2} , X1 −X2 − 1 ≥ (m1 −M2 − 1)1{X1 ≤ X2} , X1 −X2 ≤ (M1 −m2)1{X1 > X2} , X1 −X2 + 1 ≤ (M1 −m2 + 1)1{X1 ≥ X2} .\nTo obtain IP constraints, we replace each Xj with ∑ x∈Xj x · z{Xj = x} on the left-hand side, and replace the comparison indicators on the right-hand side by z{X = lt} for 1{X1 < X2}, and z{X= lt}+ z{X= eq} for 1{X1 ≤ X2}, and similarly for X1 > X2 and X1 ≥ X2.\nLCMM constraints in this case are not simply an LP relaxation of IP constraints, but instead they yield a tighter set M̃. They are based on the following identities, which can be derived from the transitivity of the comparison and the union bound:\nP{X1 ≤ x} ≤ P{X1 < X2}+ P{X2 ≤ x} for all x ≥ m1 and x ≤M2, P{X1 ≤ x} ≤ P{X1 ≤ X2}+ P{X2 < x} for all x ≥ m1 and x ≤M2.\nFor instance, the first inequality follows because X1 ≤ x implies that either X1 < X2 or X2 ≤ x. Otherwise we would have a contradiction: X1 ≥ X2 > x. The resulting LCMM constraints are\nµ{X1 ≤ x} ≤ µ{X = lt}+ µ{X2 ≤ x} for all m1 ≤ x ≤M2, µ{X1 ≤ x} ≤ µ { X ∈ {lt, eq} } + µ{X2 < x} for all m1 ≤ x ≤M2,\nwith analogous constraints with X1 and X2 swapped (and gt swapped for lt). We use the shorthand µ{X ∈ E} for ∑ x∈E µ{X = x}."
    }, {
      "heading" : "3.2. Partial outcomes",
      "text" : "In a typical combinatorial market, outcomes are gradually revealed over time. For example, in the NCAA tournament, 63 games play out over the course of several weeks. Thus, the market evolves through a sequence of partial outcomes defined as follows:\nDefinition 3.1. A subset σ ⊆ I × {0, 1} is called a partial outcome if there exists a valid payoff vector z ∈ Z such that zi = b for all (i, b) ∈ σ.\nWe write Iσ := {i : (i, b) ∈ σ for some b} for the set of securities whose payoffs have been determined, or settled, by σ. As securities get settled, we would like to fix their prices to 0 or 1. This is not possible by simply updating the state, but instead we need to switch to a different cost function while maintaining the information state of the market. We adapt the construction of Dudı́k et al. [2014] to our setting.\nFirst, we say that a vector u ∈ RI is compatible with σ if ui = b for all (i, b) ∈ σ. We write Vσ for the set of vectors compatible with σ—note that Vσ is an axis-aligned affine space of dimension |I\\Iσ|. Given a partial outcome σ, we define the set of associated valid payoffs Zσ := Z ∩ Vσ, and the associated marginal polytopeMσ := conv(Zσ). We assume that given a partial outcome σ, the market maker uses the cost function\nCσ(θ) = supµ∈Vσ [θ · µ−R(µ)] , (5)\nwhose conjugate is, by definition, Rσ(µ) = R(µ) + I{µ ∈ Vσ}, which coincides with R onMσ. The corresponding price map and Bregman divergence are denoted pσ and Dσ. The transformation of C to Cσ maintains the loss bound of the original market maker (see Appendix B) and also maintains the information state of the market analogously to conditioning, as our next example shows.\nExample 3.2. Partially settled LMSR. Recall that in a complete market, I = Ω and payoff vectors φ(ω) have exactly one entry equal to 1: the entry corresponding to the realized outcome. Therefore, the partial outcome σ can have at most one security settled to 1. If there is such a security i? then the market is fully settled and, by Eq. (5), we obtain Cσ(θ) = θi? , pσ,i(θ) = 1{i = i?}. If σ only contains securities settled to zero, i.e., the corresponding outcomes have been excluded, the cost function obtained by Eq. (5) is an LMSR over the remaining outcomes, Cσ(θ) = log( ∑ i 6∈Iσ e\nθi). The prices are pσ,i(θ) = 0 for i ∈ Iσ and pσ,i(θ) = eθi/( ∑ j 6∈Iσ e\nθj ) for i 6∈ Iσ, so the probability distribution over Ω described by pσ(θ) corresponds to p(θ) conditioned on the event ω 6∈ Iσ."
    }, {
      "heading" : "4. FRANK-WOLFE MARKET MAKER",
      "text" : "In this section we fully describe and analyze the Frank-Wolfe market maker (FWMM) outlined in Sec. 2.3.\nAt a high level, FWMM interleaves rapid pricing according to C with arbitrage removal, while also updating the partial outcome—see Mechanism 1. There are two kinds of arbitrage removal: fast but only partial arbitrage removal via an LCMM\nMECHANISM 1: Frank-Wolfe Market Maker (FWMM) Input: cost function C, initial state θ0, initial partial outcome σ0,\nLCMM constraints specified by Ã, b̃, IP constraints specified by A, b, FW algorithm parameters α ∈ (0, 1), ε0 ∈ (0, 1), εD > 0\nInitialize the market state and partial outcome: θ ← θ0, σ ← σ0 For t = 1, . . . , T (where T is an a priori unknown number of trades):\nreceive a request for a bundle δt sell the bundle δt for the cost Cσ(θ + δt)− Cσ(θ) θ ← θ + δt σ ← σ ∪ {newly settled securities if any} perform an LCMM step:\nchoose η ≥ 0 such that Cσ(θ + Ãη)− Cσ(θ) ≤ b̃ · η θ ← θ + Ãη\nperform a projection step: (σ,θ)← ProjectFW(θ; C, σ,A,b, α, ε0, εD)\nObserve ω, consistent with σ Pay traders δ1 · φ(ω), δ2 · φ(ω), . . . , δT · φ(ω)\nstep, and a complete removal of the remaining arbitrage via Bregman projection. For LCMM steps we use the fast algorithm of Dudı́k et al. [2012]. Bregman projection is implemented via a variant of the Frank-Wolfe (FW) algorithm, which we refer to as ProjectFW and describe later in this section. ProjectFW does not only return a new state θ such that pσ(θ) is the Bregman projection of the previous state onMσ. It also extends the partial outcome to securities that can be logically settled based on all other settled securities. This permanently removes the specific arbitrage opportunities associated with such securities since their prices become fixed to 0 or 1.\nBoth arbitrage-removal steps correspond to trades that yield a non-negative profit regardless of the outcome, which means that the loss bound of the original cost C is only improved by the value of this profit. The non-negative profit of LCMM steps follows from Dudı́k et al. [2012]. For ProjectFW, which is an iterative algorithm, we guarantee non-negative profit by designing a suitable stopping condition.\nAs we mention earlier, while we hope that the IPs created during the run of the FW algorithm are easy to solve, they are NP-hard in general, and so the IP solver can get stuck in a brute-force search. Therefore, we need the ability to interrupt the projection step, for instance, when a new trade arrives. When our implementation, ProjectFW, is interrupted in early stages, it yields no update. In later stages, it returns an arbitragefree market state corresponding to a trade with a non-negative but possibly suboptimal profit. Thus, the loss bound is always maintained, even when ProjectFW is interrupted."
    }, {
      "heading" : "4.1. Fully-corrective Frank-Wolfe algorithm",
      "text" : "Recall that the FW algorithm reduces the problem of Bregman projection, i.e., a convex minimization over the setM, into a sequence of linear optimization problems over the set Z. Our version, presented as Algorithm 2, is based on the fully-corrective variant of the Frank-Wolfe algorithm [Jaggi 2013], also known as the simplicial decomposition method [Bertsekas 2015], which we overview next.\nThe FW algorithm solves problems of the form\nmin µ∈M\nF (µ) , (6)\nwhereM is a compact convex set (in our case a polytope) and F is a convex function. Over the course of iterations t = 1, 2, . . . , the algorithm maintains an active set Zt of the vertices of the polytopeM that have been discovered so far, and repeatedly:\n(1) solves the minimization over the convex hull of Zt−1 to obtain a new iterate µt := argmin\nµ∈conv(Zt−1) F (µ) ,\n(2) finds a new descent vertex zt in the direction of the (negative) gradient of F ,\nzt := argmin z∈Z\n[ ∇F (µt) · z ] ,\n(3) and adds zt to the set of active vertices, so Zt = Zt−1 ∪ {zt}. Note that while the set Z of valid payoffs can be exponentially large, the set of active\nvertices Zt grows by only one vertex per iteration (and is initialized with only a small number of vertices). Therefore, Step (1), which is a convex optimization problem of dimension |Zt|, can be solved efficiently by standard algorithms. We use accelerated projected gradient [Nesterov 2007].\nStep (2), the linear optimization over the set Z, is the computationally expensive step. As discussed in Sec. 2.3, in our case it can be implemented by a call to an IP solver. In all of our experiments, the running time of Step (2) substantially dominated the running time of Step (1).\nThe convergence of the FW algorithm is analyzed via the FW gap, defined as\ng(µ) := max z∈Z\n[ ∇F (µ) · (µ− z) ] ,\nwhich bounds the suboptimality of µ. Specifically, g(µ) ≥ F (µ)− F (µ?), where µ? is a solution to Eq. (6). Thus, we can just monitor the gap g(µt) = ∇F (µt) · (µt − zt), and return the iterate µt when the gap becomes sufficiently small. The gap converges to zero at the rate of O(Ldiam(M)/t) where L is the Lipschitz constant of ∇F under an arbitrary norm and diam(M) is the diameter ofM under the same norm [Jaggi 2013].\nTo apply the FW algorithm to the problem of Bregman projection, we set its objective to the Bregman divergence: F (µ) = D(µ‖θ) = R(µ) +C(θ)− θ · µ. One formal problem arises due to the fact that the function R is not necessarily differentiable only subdifferentiable. To overcome this, we assume existence of a differentiable extension R̄. For LMSR, this is R̄(µ) = I{µ ≥ 0} + ∑ i∈I µi lnµi, and similarly for the sum of LMSRs. The key point is that R̄ coincides with R overM, so we can optimize the (differentiable) function F (µ) = R̄(µ) + C(θ)− θ · µ. (More details in Appendix C.)\nApart from differentiability, there are two additional challenges in applying the FW algorithm within Mechanism 1. First, we need to choose a stopping condition for the FW algorithm that would yield a state update with a guaranteed profit, since such updates maintain the worst-case loss bound of the market maker. Second, even though we have achieved the differentiability of F for our case of interest (the sum of LMSRs), the resulting derivative is unbounded, so the standard convergence analysis of FW does not apply. Fortunately, the growth of the derivative at the boundary is sufficiently controlled to obtain convergence of a modified version of FW, which is what we use in Algorithm 2. (The precise statement of the controlled growth condition is in Appendix C.)\nThe modified version of FW, due to Krishnan et al. [2015], performs FW iterations over a contracted version of the polytope M, or, more precisely, over a contracted version of Mσ̂, which reflects already settled securities. The contracted polytope is defined as M′ := (1 − ε)Mσ̂ + εu, where u ∈ Mσ̂ is a coherent price vector whose coordinates are neither 0 nor 1, except for those already settled by σ̂. In other words, M′ is a version of Mσ̂ shrunk towards the point u, which we call an interior point.\nALGORITHM 2: ProjectFW. Bregman Projection via Adaptive Fully-Corrective Frank-Wolfe. Input: cost function C, state θ, partial outcome σ,\nIP constraints specified by A, b, approx. ratio α ∈ (0, 1), initial contraction ε0 ∈ (0, 1), convergence threshold εD > 0\nOutput: extended partial outcome σ̂ ⊇ σ state θ̂, whose price vector is an approx. Bregman projection of θ onMσ̂ in the sense that one of the following holds:\n1. pσ̂(θ̂) ∈Mσ̂ and moving from θ to θ̂ guarantees the profit of αDσ̂(µ?‖θ) 2. θ̂ = θ and Dσ̂(µ?‖θ) ≤ εD 3. algorithm was interrupted; moving from θ to θ̂ guarantees a non-negative profit\nwhere µ? = argminµ∈Mσ̂ Dσ̂(µ‖θ) Initialize the interior point, active vertex set, and extend the partial outcome:\n(u,Z0, σ̂)← InitFW(σ,A,b) Define the objective function: F (µ) := R̄σ̂(µ)− θ · µ+ Cσ̂(θ)\nFor t = 1, 2, . . . perform a FW iteration on the contracted polytope:\nlet Z ′ = (1− εt−1)Zt−1 + εt−1u denote the contracted active set µt ← argminµ∈conv(Z′) F (µ) θt ← ∇R̄σ̂(µt) call IP solver to find the descent vertex (note that ∇F (µt) = θt − θ): zt ← argminz∈Zσ̂ (θt − θ) · z Zt = Zt−1 ∪ {zt}\ncompute the FW gap g(µt) = (θt − θ) · (µt − zt) update the best-iterate-so-far t? ← argmaxτ≤t [ F (µτ )− g(µτ ) ] check stopping conditions:\nif g(µt) ≤ (1− α)F (µt), or F (µt) ≤ εD, or termination requested\nreturn σ̂ and θ̂ = { θt∗ if g(µt∗) ≤ F (µt∗) θ otherwise\nadapt contraction if necessary: let gu = (θt − θ) · (µt − u) if gu < 0 and g(µt)/(−4gu) < εt−1 εt ← min { g(µt)/(−4gu), εt−1/2\n} else εt ← εt−1\nSince coordinates of u are bounded away from 0 and 1, the vertices of the contracted polytopeM′ have their coordinates also bounded away from 0 and 1 (except for Iσ̂). The controlled growth property then gives a bound on the Lipschitz constant of the gradient and guarantees convergence for any fixed ε, for the problem of projecting ontoM′. To obtain the convergence to the projection ontoMσ̂, we adaptively decrease ε according to the rule of Krishnan et al. [2015]. Their analysis shows that this adaptive version of FW drives the duality gap g(µt) to zero and thus indeed solves the non-contracted problem. Two missing pieces that we describe in the remainder of this section are the stopping condition and the construction of the interior point u."
    }, {
      "heading" : "4.2. Stopping condition for the FW algorithm",
      "text" : "The stopping condition needs to ensure that moving the market from a state θ to θ̂ constitutes a trade with a non-negative profit. We start with a lower bound on the\nALGORITHM 3: InitFW. Initialization for ProjectFW. Input: partial outcome σ, IP constraints specified by A, b Output: extended partial outcome σ̂ ⊇ σ\npoint u ∈Mσ̂ such that ui ∈ (0, 1) for i 6∈ Iσ̂ non-empty set Z0 of vertices ofMσ̂\nInitialize Z0 ← ∅, σ̂ ← σ, C ← ∅ For each i ∈ I\\Iσ and each b ∈ {0, 1}\nif (i, b) 6∈ C call IP solver to find ẑ = argmaxz∈Zσ (2b− 1)zi if ẑi = b Z0 ← Z0 ∪ {ẑ} C ← C ∪ {(j, ẑj) : j ∈ I}\nelse σ̂ ← σ̂ ∪ {(i, 1− b)}\nIf Z0 = ∅ Z0 ← {the unique point compatible with σ̂} Return σ̂, Z0, and u = 1|Z0| ∑ z∈Z0 z\nguaranteed profit of any iterate of the FW algorithm, and then use it to derive the stopping condition. We omit the conditioning on σ̂ from the exposition here.\nPROPOSITION 4.1. Consider a purchase that moves the market from a state θ to a new state θ̂ = ∇R̄(µ̂). The resulting profit is guaranteed to be at least D(µ̂‖θ)− g(µ̂).\nThus, it is “safe” to move the market to θ̂ whenever D(µ̂‖θ) ≥ g(µ̂) (for proof see Appendix D). To maximize the profit guarantee, we should return the iterate that maximizes the difference D(µ̂‖θ)− g(µ̂), which is what we do in Algorithm 2.\nApart from a forced interruption (e.g., because of the arrival of a new trade or exceeding of the time limit), the stopping conditions of Algorithm 2 concern two separate cases. First, recall that the algorithm is minimizing F (µ) = D(µ‖θ) via a sequence of iterates µt ∈M that satisfy D(µt‖θ)→ D(µ?‖θ) and g(µt)→ 0 as t→∞. Therefore, if prices p(θ) are incoherent, i.e., D(µ?‖θ) > 0, eventually we will have g(µt) < D(µt‖θ). In fact, we can guarantee something stronger. Namely, given a fixed α ∈ (0, 1), we will reach an iteration when\ng(µt) ≤ (1− α)D(µt‖θ) . At this point, our profit guarantee is at least\nD(µt‖θ)− g(µt) ≥ αD(µt‖θ) ≥ αD(µ?‖θ) thanks to the optimality of µ?. This means that we are extracting at least an α-fraction of the available arbitrager profits; this covers the first stopping condition and the first output case of Algorithm 2. On the other hand, if the prices p(θ) are coherent or closeto-coherent, then D(µt‖θ) will eventually drop below our convergence threshold εD, which we can set arbitrarily small. Since D(µ?‖θ) ≤ D(µt‖θ), this covers the second stopping condition and the second output case of Algorithm 2. The final case follows directly from Proposition 4.1."
    }, {
      "heading" : "4.3. Finding the interior point",
      "text" : "The goal here is to find a point u ∈ M where coordinates corresponding to unsettled securities are strictly between 0 and 1. In the process, we also obtain the initial set of active vertices and an extended partial outcome σ̂. To construct u, Algorithm 3 iterates\nthrough coordinates i that have not been settled in the provided partial outcome σ, and calls the IP solver to find a valid vector ẑ that is consistent with σ, but also has the i-th coordinate equal to b = 0 or b = 1. If the IP solver fails to find such ẑ for either value b, it means that the i-th coordinate can be settled to 1− b. Otherwise the found ẑ is added to the set of active vertices. This guarantees that each coordinate i is either present in σ̂, or the active set contains some valid vertices with both the value 0 and 1 at the i-th coordinate. Therefore, the average of the active vertices satisfies the requirement for u. If the active set is empty, it means that all of the securities have been settled and the unique valid vector consistent with σ̂ satisfies the requirement."
    }, {
      "heading" : "5. EXPERIMENTS",
      "text" : ""
    }, {
      "heading" : "5.1. Data description",
      "text" : "Our data consists of bets made in Predictalot, a combinatorial prediction market run by Yahoo! in 2010 for the NCAA Men’s Division I Basketball Tournament, commonly known as March Madness.1 The tournament lasted from March 18th to April 5th, 2010. It consisted of 64 teams playing a single-elimination tournament over 6 rounds. In each round, half of the remaining teams were eliminated. Traders were allowed to buy securities at any point in time throughout the tournament; the first bets were placed four days prior to the tournament start and the last bets were placed towards the end of the final match. Many bets referred to groupings of teams, known as conferences, brackets or seeds (e.g., there are sixteen seed levels and four teams to each seed).\nThere were 93 036 bets placed altogether on many different securities in Predictalot. Our experiments focus on a large subset of these, which we briefly describe here. The largest group of bets (56%) can be expressed as bundles over atomic tournament variables (winners of individual games, and the number of wins of individual teams). These include bets such as “Duke wins exactly 3 games”, “Cornell exits in round 2 or later”, “a team from the Big Ten conference wins the championship”. In addition to these bets, we also supported combinatorial bets for comparisons of the number of wins of single teams, e.g., “Duke wins more games than Cornell”, and comparisons of the number of wins by teams from different conferences, e.g., “teams from Big Ten win more games than teams from Big East”. These were implemented as comparison variables derived from pairs of atoms, and pairs of sums, respectively. The two comparison types encompass 12% of original bets.\nOur resulting dataset contains 63 689 bets, constituting 68% of all bets in the original market. Combinatorial bets (comparisons) make up 17% of our final dataset. The three largest groups of bets we did not include were: “team t1 wins more games than t2, and t3 wins more games than t4” (6%); “the number of upsets in round r will be less than/equal to/greater than c” (3%); and “the sum of seeds in round r will be less than/equal to/greater than c” (3%).\nPrice initialization. Our dataset contains realized trades, but we have no other price data from the run of the market. In particular, the initial Predictalot prices were not available, so we used the following scheme to initialize atomic tournament variables Xt (the number of wins of team t) and Gr,t (the outcome of a game). We considered bets within the 6 hour time window starting at 27 hours and ending at 21 hours before the\n1 Securities in the Predictalot market were priced using the Monte Carlo method with importance sampling against a dynamic proposal distribution. One of the larger issues, which we do not expect with the optimization methods presented in this work, was substantial price volatility as the tournament progressed, due to an increasing mismatch between the market belief and the proposal distribution. In order to avoid trivial arbitrage, independent samples were drawn to form the prices quoted to the traders, and the actual prices imposed on trade executions. As a result, some trades transacted at prices significantly different than quoted. [D. Pennock, personal communication, Feb. 22, 2016]\nfirst match of the tournament. Let µ′ denote the price at which securities were sold in this window (we use last such price if multiple exist). To initialize the game variables Gr,t, we use the prices of bets on the champion of the tournament (i.e., Xt = k):\nµ{Gr,t = t} = µ′{Xt = k}∑\nt′∈T µ ′{Xt′ = k}\n,\nwhere T is the set of all teams that can reach the game Gr,t; if the denominator equals zero, we initialize prices µ{Gr,t = t′} across t′ ∈ T to a uniform distribution. To initialize securities Xt = x, we proceeded as follows. If µ′{Xt = x} is present, we use that as the initialization price, otherwise we use the difference between µ′{Gx,t = t} and µ′{Gx+1,t = t}, where we replace one or both of these terms by our already calculated prices according to µ whenever the µ′ prices are not present. The resulting prices are then normalized to sum to one for each Xt. The team and game prices are then projected on the polytope described by LCMM constraints to obtain market initialization.\nSettling outcomes. Similar to initialization prices, the times when the individual games were settled were not available, so we handcrafted a dataset consisting of all game start times2 (to the best of our knowledge, end times are not listed anywhere) and settled each game 100 minutes after the game start. The choice of 100 minutes is conservative, based on the anecdotal observation that the shortest NCAA games last about 120 minutes, including the time for commercials and timeouts."
    }, {
      "heading" : "5.2. Evaluation",
      "text" : "We compare three market treatments: independent markets (IND), the linearly contrained market maker (LCMM), and a market maker with both linear constraints and Bregman projections for arbitrage removal (FWMM). Each market maker builds upon and extends the previous one. Recall that in IND, we use LMSR to price the securities associated with each random variable, but prices for separate variables vary independently, even if the underlying events are related. LCMM enforces price relationships across random variables using linear constraints, and FWMM adds projection steps onto the marginal polytope. The market makers were implemented in Java, using Gurobi Optimizer 5.53 to solve the integer programs in the FW algorithm. We refer to our implementation as the (market) engine.\nWe evaluate the three market makers by a counterfactual replay of the trades placed in Predictalot. All the market makers depend on the liquidity parameter b (see Eq. 4). Rather than optimizing b, we used a fixed liquidity of 150 and varied each trader’s budget. (The effect is equivalent, as increasing the budget increases price responsiveness to the trade orders.) Each trade order is viewed as a new agent, so the budget is constant for each trade. We used budget levels 0.1, 1, 10, 100, and 1000.\nFor each trade, the Predictalot dataset contains the number of shares purchased and the total cost paid. By taking the average price per share p̄, we obtain a lower bound on the trader’s probability estimate when the trade was placed. From this we create a limit order for our market engine by drawing a limit price uniformly from [p̄, 1], and providing the constant budget level mentioned previously. A limit order states that the trader wishes to purchase shares until either the market price reaches the limit price, or the budget is exhausted, whichever occurs first. Any sell orders with average price p̄ were transformed into buy orders of the complementary bundle, at price 1− p̄, and then converted into limit orders. By using three different seeds for the randomization, we generated three input files for the market engine. All market makers were run on all\n2Source: espn.com, e.g., http://scores.espn.go.com/ncb/boxscore?gameId=300950150 3www.gurobi.com\nthree input files. As the results were highly consistent across the randomization seeds, we found three replicates to be sufficient.\nTo summarize, we ran the three different market makers (IND, LCMM, FWMM) at five budget levels (0.1, 1, 10, 100, 1000) over the three randomly generated input files. During a market run, the engine records summaries of security prices and prices of all purchased bundles. These summaries are generated at regular intervals, including every hour and every 100 trades. We use the log likelihood to assess the accuracy of the security prices, viewed as probability forecasts, at a given point in time. Let µ be the price vector. We consider log likelihoods associated with two different kinds of events. First is the log likelihood assigned to the final realized value x? of a variable X, which equals logµ{X = x?}. Second is the log likelihood corresponding to the bundle of the form X ∈ E , viewed as a binary variable (the event occurs or not), which is defined as\n1{x? ∈ E} logµ{X ∈ E}+ 1{x? 6∈ E} logµ{X 6∈ E} . A larger log likelihood indicates a better forecast. We report the average log likelihood over all variables, and the average log likelihood over all purchased bundles. The former can be viewed as an average accuracy of the market, the latter is weighted towards the part of the market that sees more trading.\nEffect of liquidity. We first examine the effect of varying the budget level (equivalently, liquidity) on the overall performance of the three market makers. Fig. 2 provides the average prediction accuracy of the three market makers over variables and bundles, where the average is taken over all hourly summaries. The plots show the expected trends: when budget is too low, traders cannot incorporate their information into the market, while when budget is too high, prices are too sensitive to individual trades. The optimal budget setting is 10 for IND and LCMM, and 100 for FWMM. However, both LCMM and FWMM are far less sensitive to the budget level than IND, because information propagation (via constraints) can correct wrong bets.\nThe improvement of FWMM over LCMM for variables ranges from 2.1% to 5.6%, with a median of 3.3% over all budget levels and random seeds. For bundles, the improvement ranges from 0.9% to 3.2%, with a median of 2.2%. For the time period covering the first 16 games, LCMM and FWMM are very similar (see the next section), bringing their average performance closer together; excluding these games, the median improvement increases from 3.3% to 12.4% for securities, and from 2.2% to 5.6% for bundles. Because accuracy here is averaged over all hourly summaries, it is implictly weighed by duration, which is hard to interpret. To obtain a more fine-grained view, we next consider the evolution of market accuracy over time.\nAccuracy over time. Fig. 3 plots the prediction accuracy of the three markets as time progresses. We set a time limit of 30 minutes for Bregman projection. The first time it successfully completes is only at time stamp ‘2010-03-21 13:58:50’, after 45 games are already settled. We therefore begin the plot at time stamp ‘2010-03-19 00:00:00’, corresponding to 16 settled games, as there is very little difference between LCMM and FWMM before that point. The reason FWMM still exceeds LCMM on occasion before the first projection is due to the extension of the partial outcome afforded by the IP, as explained in Sec. 4.\nEach point of the time series represents an average over all variables or bundles defined at that time, including those whose outcomes have been settled. This explains the upwards trends of the plots, culminating at accuracy 0 (a perfect score). The trend is not entirely monotonic, as we see from the bundle log likelihood in the stretch after March 22. The dotted vertical lines indicate the beginning of days on which games are played. On such days, we see that accuracy is initially stable, then sharply increases as the games take course and their outcomes are settled.\nIn Fig. 3, we see that once Bregman projections successfully complete, the improvement of FWMM over LCMM becomes sustained. The accuracy improvements from this point onwards range from 0% to 80% for variables, with a median of 38% over all hourly summaries. The improvements range from 0% to 44% for bundles, with a median of 9%."
    }, {
      "heading" : "6. DISCUSSION AND CONCLUSION",
      "text" : "In our experiments, FWMM outperformed LCMM once the outcome space was sufficiently reduced, via settled securities, to allow computing of Bregman projections within 30 minutes on a standard workstation. This time limit yielded a manageable\nexperimental turnaround, with about 5 hours to execute the trades that originally spanned 22 days. In practice, a market designer can allow longer computation and use more powerful hardware, and expect improvements for larger problem sizes.\nSeveral approaches could further speed up our framework. For instance, FW can be used to construct separating hyperplanes to tighten the outer LCMM approximation, and thereby contribute to arbitrage removal even when there is no time to compute the projection. Also, instead of solving IPs to optimality in each iteration, it may be possible to interleave IP with local search to obtain additional descent vertices. Since IP is by far the most time-consuming part of FW, this could yield substantial speedups."
    }, {
      "heading" : "A. PROOF OF PROPOSITION 2.4",
      "text" : "We first calculate the largest possible guaranteed profit:\nsup δ∈RI min z∈Z\n[ δ · z − C(θ + δ) + C(θ) ] = sup θ′∈RI min µ∈M [ (θ′ − θ) · µ− C(θ′) + C(θ) ] = min µ∈M sup θ′∈RI [ (θ′ − θ) · µ− C(θ′) + C(θ) ] (7)\n= min µ∈M\n[ R(µ)− θ · µ+ C(θ) ] (8)\n= min µ∈M\nD(µ‖θ) = D(µ?‖θ) , (9)\nwhere Eq. (7) follows by Sion’s minimax theorem and Eqs. (8) and (9) from definitions of the convex conjugate and Bregman divergence, respectively. This shows that from the state θ the guaranteed profit is at most D(µ?‖θ).\nRecall that δ? is any trade that moves the market to a state θ? such that p(θ?) = µ?. We next show that δ? is an optimal trade, i.e., that this trade gives a profit that is at least D(µ?‖θ). Let F (µ) := D(µ‖θ). Since µ? optimizes F onM, by the first order optimality, we have for any u ∈ ∂F (µ?) and z ∈ Z that u·(z−µ?) ≥ 0. Since p(θ?) = µ?, the conjugacy implies that θ? ∈ ∂R(µ?) and thus (θ? − θ) ∈ ∂F (µ?), so the first order optimality yields\n0 ≤ (θ? − θ) · (z − µ?) , which rearranges to\n(θ? − θ) · z ≥ (θ? − θ) · µ? . (10) The profit from the trade δ? given any outcome ω is therefore at least\n(θ? − θ) · φ(ω)− C(θ?) + C(θ) ≥ (θ? − θ) · µ? − C(θ?) + C(θ) = R(µ?)− θ · µ? + C(θ) = D(µ?‖θ) ,\nwhere the first line follows by substituting φ(ω) for z in Eq. (10), the second line follows from the conjugacy of R and C, and the third line from the definition of D, completing the proof."
    }, {
      "heading" : "B. BOUNDED LOSS PROPERTY UNDER GRADUAL REVELATION OF OUTCOME",
      "text" : "We show that the bound on the worst-case loss of the cost C is maintained if we update the cost function using a sequence of partial outcomes, gradually revealing the final outcome ω. We begin with the worst-case bound on the loss under cost C:\nPROPOSITION B.1. If the initial market state is θ0 then the worst-case loss of a market-maker using C is maxω∈ΩD(φ(ω)‖θ0).\nPROOF. Let θ denote the final state before the outcome ω is revealed. Then the market maker has collected C(θ)− C(θ0) as the revenue for the sold shares, and needs\nto pay out (θ − θ0) · φ(ω) as a payoff to the traders. The worst-case loss is therefore\nmax ω∈Ω sup θ\n[ (θ − θ0) · φ(ω)− ( C(θ)− C(θ0) )] = max\nω∈Ω sup θ\n[( θ · φ(ω)− C(θ) ) − θ0 · φ(ω) + C(θ0) ] = max\nω∈Ω\n[ R ( φ(ω) ) − θ0 · φ(ω) + C(θ0) ] = max\nω∈Ω D(φ(ω)‖θ0) .\nNow, we will analyze the case with partial outcomes. We assume that the initial partial outcome σ0 = ∅, and that the market goes through a sequence of partial outcomes σ0 ⊆ σ1 ⊆ · · · ⊆ σT until finally an outcome ω is revealed, consistent with σT . After the revelation of each σt, the market-maker switches to the cost function Cσt . The initial market state is denoted θ0 and the market state in which the market switches to Cσt is denoted θt.\nPROPOSITION B.2. If the initial market state is θ0 then, regardless of the sequence of partial outcomes σ1, . . . , σT , the worst-case loss of the market-maker using the sequence of costs Cσt is maxω∈ΩD(φ(ω)‖θ0), i.e., the same as that of the market-maker using C without incorporating partial outcomes.\nPROOF. Recall that the market state at the time of switch from Cσt−1 to Cσt is θt. We first show that the value of the cost at the time of switch decreases:\nCσt(θt) = sup µ∈Vσt [θ · µ−R(µ)] ≤ sup µ∈Vσt−1 [θ · µ−R(µ)] = Cσt−1(θt) (11)\nwhere the middle inequality follows because Vσt ⊆ Vσt−1 . We are now ready to prove the bound on the worst-case loss. Let Ω(σT ) denote the set of outcomes compatible with σT , and recall that σ0 = ∅, so Cσ0 ≡ C. Recall that θt for t = 1, . . . , T are the states of the market when the cost becomes Cσt . Finally, let θT+1 denote the final state. Then the worst-case loss of the market maker can be bounded as follows\nmax σ1⊆σ2⊆...⊆σT max ω∈Ω(σT ) sup θ1,...,θT+1\n[ (θT+1 − θ0) · φ(ω)−\nT∑ t=0 ( Cσt(θt+1)− Cσt(θt)\n)]\n= max σ1⊆σ2⊆...⊆σT max ω∈Ω(σT ) sup θ1,...,θT+1\n[ (θT+1 − θ0) · φ(ω)− ( CσT (θT+1)− Cσ0(θ0) ) −\nT∑ t=1 ( Cσt−1(θt)− Cσt(θt) )] (12)\n≤ max σT max ω∈Ω(σT ) sup θT+1\n[ (θT+1 − θ0) · φ(ω)− CσT (θT+1) + Cσ0(θ0) ] (13)\n= max σT max ω∈Ω(σT )\n[ R ( φ(ω) ) − θ0 · φ(ω) + C(θ0) ] (14)\n= max ω∈Ω\nD(φ(ω)‖θ0) . (15)\nEq. (12) follows by rearranging the terms. Eq. (13) follows by Eq. (11). Eq. (14) follows because the convex conjugate of CσT is RσT (µ) = I{µ ∈ VσT }+R(µ), and RσT ( φ(ω) ) =\nR ( φ(ω) ) thanks to the compatibility of ω with σT . Finally, Eq. (15) follows from the definition of Bregman divergence, completing the proof."
    }, {
      "heading" : "C. DIFFERENTIABILITY AND CONTROLLED GROWTH OF R",
      "text" : "The algorithm used by our market maker requires a differentiable objective whose gradient does not grow too fast as it approaches the boundary ofM. Note that for LMSR, the Bregman divergence is formally not even differentiable in its first argument (it is subdifferentiable). So, in addition to requiring the controlled growth of the gradient, we also need to assume that R can be extended into a differentiable function. Specifically, we say that R̄ : RI → (−∞,∞] is a convex extension of R if R̄ is convex and coincides with R wherever R < ∞. We require existence of an extension with the controlled growth property in the following sense:\nDefinition C.1. Let S ⊆ [0, 1]n be a compact convex set. We say that a convex function F exhibits controlled growth on S if it is differentiable on S ∩ (0, 1)n and if there exists a fixed p ≥ 0 and L ≥ 0 such that for any ε > 0, the gradient ∇F has a bounded Lipschitz constant Lε ≤ Lε−p over S ∩ [ε, 1− ε]n.\nAssumption C.2. R has a convex extension R̄ such that for all partial outcomes σ, when R̄ is viewed as a function on Vσ, it exhibits controlled growth onMσ. We write R̄σ for the restriction of R̄ to Vσ. Note that this restriction is formally a function defined on a space of dimension |I\\Iσ| and thus, formally, ∇R̄σ has the dimension |I\\Iσ|. We extend ∇R̄σ into a vector in RI by inserting zeros at coordinates i ∈ Iσ. A key consequence of this construction is that for any partial outcome σ and all µ ∈Mσ such that µi ∈ (0, 1) for i 6∈ Iσ, the gradient ∇R̄σ(µ) is defined, and ∇R̄σ(µ) ∈ ∂Rσ(µ). As a result, we have that θ = ∇R̄σ(µ) implies that ∇Cσ(θ) = µ (but not vice versa). Assumption C.2 can be verified for instance by upper-bounding the operator norm of the Hessian, which directly upper-bounds the Lipschitz constant of the gradient.\nExample C.3. Controlled growth for LMSR. We define the extension of negative entropy over the non-negative orthant, R̄(µ) = I{µ ≥ 0}+ ∑ i∈I µi logµi, which yields\nR̄σ(µ) = I{µi ≥ 0 for all i 6∈ Iσ}+ ∑ i 6∈Iσ µi logµi. The Hessian is a diagonal matrix with entries 1/µi, so its operator norm is maxi6∈Iσ 1/µi, and thus Lε = O(1/ε), which satisfies the controlled growth condition with p = 1."
    }, {
      "heading" : "D. PROOF OF PROPOSITION 4.1",
      "text" : "The guaranteed profit when moving from θ to θ̂ is\nmin ω∈Ω\n[ (θ̂ − θ) · φ(ω)− C(θ̂) + C(θ) ] = min µ∈M [ (θ̂ − θ) · µ− C(θ̂) + C(θ) ] (16)\n= min µ∈M\n[ (θ̂ − θ) · (µ− µ̂) + θ̂ · µ̂− C(θ̂)− θ · µ̂+ C(θ) ] = min µ∈M [ (θ̂ − θ) · (µ− µ̂) +R(µ̂)− θ · µ̂+ C(θ) ] (17)\n= D(µ̂‖θ)− g(µ̂) . (18) Eq. (16) follows because the minimized objective is linear in φ(ω). Eq. (17) follows from the definition of R. Finally, Eq. (18) follows because ∇F (µ̂) = ∇R̄(µ̂)− θ = θ̂ − θ, and hence\ng(µ̂) = max µ∈M\n[ (θ̂ − θ) · (µ̂− µ) ] ."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2016,
    "abstractText" : "We present a new combinatorial market maker that operates arbitrage-free combinatorial prediction markets specified by integer programs. Although the problem of arbitrage-free pricing, while maintaining a bound on the subsidy provided by the market maker, is #P-hard in the worst case, we posit that the typical case might be amenable to modern integer programming (IP) solvers. At the crux of our method is the Frank-Wolfe (conditional gradient) algorithm which is used to implement a Bregman projection aligned with the market maker’s cost function, using an IP solver as an oracle. We demonstrate the tractability and improved accuracy of our approach on real-world prediction market data from combinatorial bets placed on the 2010 NCAA Men’s Division I Basketball Tournament, where the outcome space is of size 263. To our knowledge, this is the first implementation and empirical evaluation of an arbitrage-free combinatorial prediction market on this scale.",
    "creator" : "LaTeX with hyperref package"
  }
}