{
  "name" : "1606.07829.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Unsupervised Topic Modeling Approaches to Decision Summarization in Spoken Meetings",
    "authors" : [ "Lu Wang", "Claire Cardie" ],
    "emails" : [ "luwang@cs.cornell.edu", "cardie@cs.cornell.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Meetings are an important way for information sharing and collaboration, where people can discuss problems and make concrete decisions. Not surprisingly, there is an increasing interest in developing methods for extractive summarization for meetings and conversations (Zechner, 2002; Maskey and Hirschberg, 2005; Galley, 2006; Lin and Chen, 2010; Murray et al., 2010a). Carenini et al. (2011) describe the specific need for focused summaries of meetings, i.e., summaries of a particular aspect of a meeting rather than of the meeting as a whole. For example, the decisions made, the action items that emerged and the problems arised are all important outcomes of meetings. In particular, decision summaries would allow participants to review decisions from previous meetings and understand the related topics quickly, which facilitates preparation for the upcoming meetings.\nMeeting conversation is intrinsically different from well-written text, as meetings may not be well organized and most utterances have low density of salient content. Therefore, multiple problems need to be addressed for speech summarization. Consider the sample dialogue snippet in Figure 1 from the AMI meeting corpus (Carletta et al., 2005). Only decision-related dialogue acts (DRDAs) — utter-\nar X\niv :1\n60 6.\n07 82\n9v 1\n[ cs\n.C L\n] 2\n4 Ju\nn 20\nances at least one decision made in the meeting1 — are listed and ordered by time. Each DRDA is labeled numerically according to the decision it supports; so the second and third utterances (in bold) support DECISION 2, as do the fifth utterance in the snippet. Manually constructed decision abstracts for each decision are shown at the bottom of the figure.\nBesides the prevalent dialogue phenomena (such as “Uh I’m kinda liking” in Figure 1), disfluencies and off-topic expressions, we notice that single utterance is usually not informative enough to form a decision. For instance, no single DRDA associated with DECISION 4 corresponds all that well with its decision abstract: “pushbuttons”, “menu button” and “Pre-set channels” are mentioned in separate DAs. As a result, extractive summarization methods that select individual utterance to form the summary will perform poorly.\nFurthermore, it is difficult to identify the core topic when multiple topics are discussed in one utterance. For example, all of the bold DRDAs supporting DECISION 2 contain the word “latex”. However, the last DA in bold also mentions “bigger impact” and “the scroll wheel”, which are not specifically relevant for DECISION 2. Though this problem can be approached by training a classifier to identify the relevant phrases and ignore the irrelevant ones or dialogue phenomena, it needs expensive human annotation and is limited to the specific domain.\nNote also that for DECISION 4, the “power button” is not specified in any of the listed DRDAs supporting it. By looking at the transcript, we find “power button” mentioned in one of the preceding, but not decision-related DAs. Consequently another challenge would be to add complementary knowledge when the DRDAs cannot provide complete information.\nTherefore, we need a summarization approach that is tolerant of dialogue phenomena, can determine the key semantic content and is easily transferable between domains. Recently, topic modeling approaches have been investigated and achieved state-of-the-art results in multi-document summarization (Haghighi and Vanderwende, 2009; Celiky-\n1These DRDAs are annotated in the AMI corpus and usually contain the decision content. They are similar, but not completely equivalent, to the decision dialogue acts (DDAs) of Bui et al. (2009), Fernández et al. (2008), Frampton et al. (2009).\nilmaz and Hakkani-Tur, 2010). Thus, topic models appear to better ref for document similarity w.r.t. semantic concepts than simple literal word matching. However, very little work has investigated its role in spoken document summarization (Chen and Chen, 2008; Hazen, 2011), and much less conducted comparisons among topic modeling approaches for focused summarization in meetings.\nIn contrast to previous work, we study the unsupervised token-level decision summarization in meetings by identifying a concise set of key words or phrases, which can either be output as a compact summary or be a starting point to generate abstractive summaries. This paper addresses problems mentioned above and make contributions as follows:\n• As a step towards creating the abstractive summaries that people prefer when dealing with spoken language (Murray et al., 2010b), we propose a token-level rather than sentence-level framework for identifying components of the summary. Experimental results show that, compared to the sentence ranking based summarization algorithms, our token-level summarization framework can better identify the summary-worthy words and remove the redundancies.\n• Rather than employing supervised learning methods that rely on costly manual annotation, we explore and evaluate topic modeling approaches of different granularities for the unsupervised decision summarization at both the token-level and dialogue act-level. We investigate three topic models — Local LDA (LocalLDA) (Brody and Elhadad, 2010), Multi-grain LDA (MG-LDA) (Titov and McDonald, 2008) and Segmented Topic Model (STM) (Du et al., 2010) — which can utilize the latent topic structure on utterance level instead of document level. Under our proposed token-level summarization framework, three finegrained models outperform the basic LDA model and two extractive baselines that select the longest and the most representative utterance for each decision, respectively. (ROUGE-SU4 F score of 14.82% for STM vs. 13.58% and 13.46% for the baselines, given the perfect clusterings of DRDAs.)\n• In line with prior research that explore the role of context for utterance-based extractive summariza-\ntion (Murray and Renals, 2007), we investigate the role of context in our token-level summarization framework. For the given clusters of DRDAs, We study two types of context information — the DAs preceding and succeeding a DRDA and DAs of high TF-IDF similarity with a DRDA. We also investigate two ways to select relevant words from the context DA. Experimental results show that two types of context have comparable effect, but selecting words from the dominant topic of the center DRDA performs better than from the dominant topic of the context DA. Moreover, by leveraging context, the recall exceeds the provided upperbound’s recall (ROUGE-1 recall: 48.10% vs. 45.05% for upperbound by using DRDA only) although the F scores decrease after adding context information. Finally, we show that when the true DRDA clusterings are not available, adding context can improve both the recall and F score."
    }, {
      "heading" : "2 Related Work",
      "text" : "Speech and dialogue summarization has become important in recent years as the number of multimedia resources containing speech has grown. A primary goal for most speech summarization systems is to account for the special characteristics of dialogue. Early work in this area investigated supervised learning methods, including maximum entropy, conditional random fields (CRFs), and support vector machines (SVMs) (Buist et al., 2004; Galley, 2006; Xie et al., 2008). For unsupervised methods, maximal marginal relevance (MMR) is investigated in (Zechner, 2002) and (Xie and Liu, 2010). Gillick et al. (2009) introduce a conceptbased global optimization framework by using integer linear programming (ILP).\nOnly in very recent works has decision summarization been addressed in (Fernández et al., 2008), (Bui et al., 2009) and (Wang and Cardie, 2011). (Fernández et al., 2008) and (Bui et al., 2009) utilize semantic parser to identify candidate phrases for decision summaries and employ SVM to rank those phrases. They also train HMM and SVM directly on a set of decision-related dialogue acts on token level and use the classifiers to identify summary-worthy words. Wang and Cardie (2011) provide an exploration on supervised and unsupervised learning for decision summarization on both\nutterance- and token- level. Our work also arises out of applying topic models to text summarization (Bhandari et al., 2008; Haghighi and Vanderwende, 2009; Celikyilmaz and Hakkani-Tur, 2010; Celikyilmaz and Hakkani-Tur, 2010). Mostly, the sentences are ranked according to importance based on latent topic structures, and top ones are selected as the summary. There are some works for applying document-level topic models to speech summarization (Kong and shan Leek, 2006; Chen and Chen, 2008; Hazen, 2011). Different from their work, we further investigate the topic models of fine granularity on sentence level and leverage context information for decision summarization task.\nMost existing approaches for speech summarization result in a selection of utterances from the dialogue, which cannot remove the redundancy within utterances. To eliminate the superfluous words, our work is also inspired by keyphrase extraction of meetings (Liu et al., 2009; Liu et al., 2011) and keyphrase based summarization (Riedhammer et al., 2010). However, a small set of keyphrases are not enough to concretely display the content. Instead of only picking up keyphrases, our work identifies all of the summary-worthy words and phrases, and removes redundancies within utterances."
    }, {
      "heading" : "3 Summarization Frameworks",
      "text" : "In this section, we first present our proposed tokenlevel decision summarization framework — DomSum — which utilizes latent topic structure in utterances to extract words from Dominant Topic (see details in Section 3.1) to form Summaries. In Section 3.2, we describe four existing sentence scoring metrics denoted as OneTopic, MultiTopic, TMMSum and KLSum which are also based on latent topic distributions. We adopt them to the utterance-level summarization for comparison in Section 6."
    }, {
      "heading" : "3.1 Token-level Summarization Framework",
      "text" : "Domsum takes as input the clusters of DRDAs (with or without additional context DAs), the topic distribution for each DA and the word distribution for each topic. The output is a set of topic-coherent summary-worthy words which can be used directly as the summary or to further generate abstractive summary. We introduce DomSum in two steps according to its input: taking clusters of DRDAs as the input and with additional context information.\nDRDAs Only. Given clusters of DRDAs, we use Algorithm 1 to produce the token-level summary for each cluster. Generally, Algorithm 1 chooses the topic with the highest probability as the dominant topic given the dialogue act (DA). Then it collects the words with a high joint probability with the dominant topic from that DA.\nInput : Cluster C = {DAi}, P (Tj |DAi), P (wk|Tj) Output: Summary\nSummary← Φ (empty set) foreach DAi in C do\nDomTopic← maxTj P (Tj |DAi) (*) Candidate← Φ foreach word wk in DAi do\nSampleTopic← maxTj P (wk|Tj)P (Tj |DAi) if DomTopic == SampleTopic then\nCandidate← Union(Candidate, wk) end\nend Summary← Union(Summary, Candidate)\nend\nAlgorithm 1: DomSum — The token-level summarization framework. DomSum takes as input the clusters of DRDAs and related probability distributions.\nLeveraging Context. For each DRDA (denoted as “center DA”), we study two types of context information (denoted as “context DAs”). One is adjacent DAs, i.e., immediately preceding and succeeding DAs, the other is the DAs having top TF-IDF similarities with the center DA. Context DAs are added into the cluster the corresponding center DA in.\nWe also study two criteria of word selection from the context DAs. For each context DA, we can take the words appearing in the dominant topic of either this context DA or its center DRDA. We will show in Section 6.1 that the latter performs better as it produces more topic-coherent summaries. Algorithm 1 can be easily modified to leverage context DAs by updating the input clusters and assigning the proper dominant topic for each DA accordingly — this changes the step (∗) in Algorithm 1."
    }, {
      "heading" : "3.2 Utterance-level Summarization Metrics",
      "text" : "We also adopt four sentence scoring metrics based on the latent topic structure for extractive summarization. Though they are developed on different topic models, given the desired topic distributions as input, they can rank the utterances according to their importance and provide utterance-level summaries for comparison.\nOneTopic and MultiTopic. In (Bhandari et al., 2008), several sentence scoring functions are introduced based on Probabilistic Latent Semantic Indexing. We adopt two metrics, which are OneTopic and MultiTopic. For OneTopic, topic T with highest probability P (T ) is picked as the central topic per cluster C. The score for DA in C is:\nP (DA|T ) = ∑ w∈DA P (T |DA,w)∑\nDA′∈C,w∈DA′ P (T |DA′, w) ,\nMultiTopic modifies OneTopic by taking all of the topics into consideration. Given a cluster C, DA in C is scored as: ∑ T P (DA|T )P (T ) = ∑ T ∑ w∈DA P (T |DA,w)∑ DA′∈C,w∈DA′ P (T |DA′, w) P (T )\nTMMSum. Chen and Chen (2008) propose a Topical Mixture Model (TMM) for speech summarization, where each dialogue act is modeled as a TMM for generating the document. TMM is shown to provide better utterance-level extractive summaries for spoken documents than other conventional unsupervised approaches, such as Vector Space Model (VSM) (Gong and Liu, 2001), Latent Semantic Analysis (LSA) (Gong and Liu, 2001) and Maximum Marginal Relevance (MMR) (Murray et al., 2005). The importance of a sentence S can be measured by its generative probability P (D|S), where D is the document S belongs to. In our experiments, one decision is made per cluster of DAs. So we adopt their scoring metric to compute the generative probability of the cluster C for each DA:\nP (C|DA) = ∏ wi∈C ∑ Tj P (wi|Tj)P (Tj |DA),\nKLSum. Kullback-Lieber (KL) divergence is explored for summarization in (Haghighi and Vanderwende, 2009) and (Lin et al., 2010), where it is used to measure the distance of distributions between the document and the summary. For a cluster C of DAs, given a length limit θ, a set of DAs S is selected as:\nS∗ = arg min S:|S|<θ KL(PC ||PS) = arg min S:|S|<θ ∑ Ti P (Ti|C)log P (Ti|C) P (Ti|S)"
    }, {
      "heading" : "4 Topic Models",
      "text" : "In this section, we briefly describe the three finegrained topic models employed to compute the latent topic distributions on utterance level in the\nmeetings. According to the input of Algorithm 1, we are interested in estimating the topic distribution for each DA P (T |DA) and the word distribution for each topic P (w|T ). For MG-LDA, P (T |DA) is computed as the expectation of local topic distributions with respect to the window distribution."
    }, {
      "heading" : "4.1 Local LDA",
      "text" : "Local LDA (LocalLDA) (Brody and Elhadad, 2010) uses almost the same probabilistic generative model as Latent Dirichlet Allocation (LDA) (Blei et al., 2003), except that it treats each sentence as a separate document2. Each DA d is generated as follows:\n1. For each topic k: (a) Choose word distribution: φk ∼ Dir(β) 2. For each DA d: (a) Choose topic distribution: θd ∼ Dir(α) (b) For each word w in DA d:\ni. Choose topic: zd,w ∼ θd ii. choose word: w ∼ φzd,w"
    }, {
      "heading" : "4.2 Multi-grain LDA",
      "text" : "Multi-grain LDA (MG-LDA) (Titov and McDonald, 2008) can model both the meeting specific topics (e.g. the design of a remote control) and various concrete aspects (e.g. the cost or the functionality). The generative process is:\n1. Choose a global topic distribution: θglm ∼ Dir(αgl) 2. For each sliding window v of size T :\n(a) Choose local topic distribution: θlocm,v ∼ Dir(αloc) (b) Choose granularity mixture: πm,v ∼ Beta(αmix)\n3. For each DA d: (a) choose window distribution: ψm,d ∼ Dir(γ) 4. For each word w in DA d of meeting m: (a) Choose sliding window: vm,w ∼ ψm,d (b) Choose granularity: rm,w ∼ πm,vm,w (c) If rm,w = gl, choose global topic: zm,w ∼ θglm (d) If rm,w = loc, choose local topic: zm,w ∼ θlocm,vm,w (e) Choose word w from the word distribution: φrm,wzm,w"
    }, {
      "heading" : "4.3 Segmented Topic Model",
      "text" : "The last model we utilize is Segmented Topic Model (STM) (Du et al., 2010), which jointly models document- and sentence-level latent topics using a two-parameter Poisson Dirichlet Process (PDP). Given parameters α, γ,Φ and PDP parameters a, b, the generative process is:\n1. Choose distribution of topics: θm ∼ Dir(α) 2. For each dialogue act d:\n2For the generative process of LDA, the DAs in the same meeting make up the document, so “each DA” is changed to “each meeting” in LocalLDA’s generative process.\n(a) Choose distribution of topics: θd ∼ PDP (θm, a, b) 3. For each word w in dialogue act d:\n(a) Choose topic: zm,w ∼ θd (b) Choose word: w ∼ φzm,w"
    }, {
      "heading" : "5 Experimental Setup",
      "text" : "The Corpus. We evaluate our approach on the AMI meeting corpus (Carletta et al., 2005) that consists of 140 multi-party meetings. The 129 scenariodriven meetings involve four participants playing different roles on a design team. A short (usually one-sentence) abstract is manually constructed to summarize each decision discussed in the meeting and used as gold-standard summaries in our experiments.\nSystem Inputs. Our summarization system requires as input a partitioning of the DRDAs according to the decision(s) that each supports (i.e., one cluster of DRDAs per decision). As mentioned earlier, we assume for all experiments that the DRDAs for each meeting have been identified. For evaluation we consider two system input settings. In the True Clusterings setting, we use the AMI annotations to create perfect partitionings of the DRDAs as the input; in the System Clusterings setting, we employ a hierarchical agglomerative clustering algorithm used for this task in previous work (Wang and Cardie, 2011). The Wang and Cardie (2011) clustering method groups DRDAs according to their LDA topic distribution similarity. As better approaches for DRDA clustering become available, they could be employed instead.\nEvaluation Metric. To evaluate the performance of various summarization approaches, we use the widely accepted ROUGE (Lin and Hovy, 2003) metrics. We use the stemming option of the ROUGE software at http://berouge.com/ and remove stopwords from both the system and gold-standard summaries, same as Riedhammer et al. (2010) do.\nInference and Hyperparameters We use the implementation from (Lu et al., 2011) for the three topic models in Section 4. The collapsed Gibbs Sampling approach (Griffiths and Steyvers, 2004) is exploited for inference. Hyperparameters are chosen according to (Brody and Elhadad, 2010), (Titov and McDonald, 2008) and (Du et al., 2010). In LDA and LocalLDA, α and β are both set to 0.1 . For MG-LDA, αgl, αloc and αmix are set to 0.1; γ is 0.1\nand the window size T is 3. And the number of local topic is set as the same number of global topic as discussed in (Titov and McDonald, 2008). In STM, α, a and b are set to 0.5, 0.1 and 1, respectively."
    }, {
      "heading" : "5.1 Baselines and Comparisons",
      "text" : "We compare our token-level summarization framework based on the fine-grained topic models to (1) two unsupervised baselines, (2) token-level summarization by LDA, (3) utterance-level summarization by Topical Mixture Model (TMM) (Chen and Chen, 2008), (4) utterance-level summarization based on the fine-grained topic models using existing metrics (Section 3.2), (5) two supervised methods, and (6) an upperbound derived from the AMI gold standard decision abstracts. (1) and (6) are described below, others will be discussed in Section 6.\nThe LONGEST DA Baseline. As in (Riedhammer et al., 2010) and (Wang and Cardie, 2011), this baseline simply selects the longest DRDA in each cluster as the summary. Thus, it performs utterance-level decision summarization. This baseline and the next allow us to determine summary quality when summaries are restricted to a single utterance.\nThe PROTOTYPE DA Baseline. Following Wang and Cardie (2011), the second baseline selects the decision cluster prototype (i.e., the DRDA with the largest TF-IDF similarity with the cluster centroid) as the summary.\nUpperbound. We also compute an upperbound that reflects the gap between the best possible extractive summaries and the human-written abstracts according to the ROUGE score: for each cluster of DRDAs, we select the words that also appear in the associated decision abstract."
    }, {
      "heading" : "6 Results and Discussion",
      "text" : "6.1 True Clusterings How do fine-grained topic models compare to basic topic models or baselines? Figure 2 demonstrates that by using the DomSum token-level summarization framework, the three fine-grained topic models uniformly outperform the two non-trivial baselines and TMM (Chen and Chen, 2008) (reimplemented by us) that generates utterance-level summaries. Moreover, the fine-grained models also beat basic LDA under the same DomSum token-level summarization framework. This shows the fine-\ngrained topic models that discover topic structures on utterance-level better identify gist information.\nCan the proposed token-level summarization framework better identify important words and remove redundancies than utterance selection methods? Figure 3 demonstrates the comparison results for our DomSum token-level summarization framework with four existing utterance scoring metrics discussed in Section 3.2, namely OneTopic, MultiTopic, TMMSum and KLSum. The utterance with highest score is extracted to form the summary. LocalLDA and STM are utilized to compute the input distributions, i.e., P (T |DA) and P (w|T ). From Figure 3, DomSum yields the best F scores which\nshows that the token-level summarization approach is more effective than utterance-level methods.\nWhich way is better for leveraging context information? We explore two types of context information. For adjacent content (Adj in Figure 4), 5 DAs immediately preceding and 5 DAs succeeding the center DRDA are selected. For TF-IDF context (TFIDF in Figure 4), 10 DAs of highest TF-IDF similarity with the center DRDA are taken. We also explore two ways to extract summary-worthy words from the context DA — selecting words from the dominant topic of either the center DA (denoted as “One” in parentheses in Figure 4) or the current context DA (denoted as “multi” in parentheses in Fig-\nure 4). Figure 4 indicates that the two types of context information do not have significant difference, while selecting the words from the dominant topic of the center DA results in better ROUGE-SU4 F scores. Notice that compared with Figure 3, the results in Figure 4 have lower F scores when using the true clusterings of DRDAs. This is because context DAs bring in relevant words as well as noisy information. We will show in Section 6.2 that when true clusterings are not available, the context information can boost both recall and F score.\nHow do the token-level summarization framework compared to utterance selection methods for leveraging context? We also compare the ability of leveraging context of DomSum to utterance scoring metrics, i.e., OneTopic and MultiTopic. 5 DAs preceding and 5 DAs succeeding the center DA are added as context information. For context DA under DomSum, we select words from the dominant topic of the center DA (denoted as “One” in parentheses in Figure 5). For OneTopic and MultiTopic, the top 3 DAs are extracted as the summary. Figure 5 demonstrates the combination of LocalLDA and STM with each of the metrics. DomSum, as a token-level summarization metrics, dominates other two metrics in leveraging context.\nHow do our approach perform when compared with supervised learning approaches? For a better comparison, we also provide summarization results by using supervised systems along with an upperbound. We use Support Vector Machines (Joachims, 1998) with RBF kernel and order1 Conditional Random Fields (Lafferty et al., 2001) — trained with the same features as (Wang and Cardie, 2011) to identify the summary-worthy tokens to include in the abstract. A three-fold cross validation is conducted for both methods. ROUGE1, ROUGE-2 and ROUGE-SU4 scores are listed in Table 1. From Table 1, our token-level summarization approaches based on LocalLDA and STM are shown to outperform the baselines and even the CRF. Meanwhile, by adding context information, both LocalLDA and STM can get better ROUGE-1 recall than the supervised methods, even higher than the provided upperbound which is computed by only using DRDAs. This shows the DomSum framework can leverage context to compensate the summaries."
    }, {
      "heading" : "6.2 System Clusterings",
      "text" : "Results using the System Clusterings (Table 2) present similar findings, though all of the system and baseline scores are lower. By adding context information, the token-level summarization approaches based on fine-grained topic models compare favor-\nably to the supervised methods in F scores, and also get the best ROUGE-1 recalls."
    }, {
      "heading" : "6.3 Sample System Summaries",
      "text" : "To better exemplify the summaries generated by different systems, sample output for each method is shown in Table 3. We see from the table that utterance-level extractive summaries (Longest DA, Prototype DA, TMM) make more coherent but still far from concise and compact abstracts. On the other hand, the supervised methods (SVM, CRF) that produce token-level extracts better identify the overall content of the decision abstract. Unfortunately, they require human annotation in the training phase. In comparison, the output of fine-grained topic models can cover the most useful information."
    }, {
      "heading" : "7 Conclusion",
      "text" : "We propose a token-level summarization framework based on topic models and show that modeling topic structure at the utterance-level is better at identifying relevant words and phrases than document-level models. The role of context is also studied and shown to be able to identify additional summaryworthy words. Acknowledgments This work was supported in part by National Science Foundation Grants IIS-0968450 and IIS-1111176, and by a gift from Google."
    } ],
    "references" : [ {
      "title" : "Generic text summarization using probabilistic latent semantic indexing",
      "author" : [ "Harendra Bhandari", "Takahiko Ito", "Masashi Shimbo", "Yuji Matsumoto." ],
      "venue" : "Proceedings of IJCNLP, pages 133–140.",
      "citeRegEx" : "Bhandari et al\\.,? 2008",
      "shortCiteRegEx" : "Bhandari et al\\.",
      "year" : 2008
    }, {
      "title" : "Latent dirichlet allocation",
      "author" : [ "David M. Blei", "Andrew Y. Ng", "Michael I. Jordan." ],
      "venue" : "J. Mach. Learn. Res., 3:993–1022, March.",
      "citeRegEx" : "Blei et al\\.,? 2003",
      "shortCiteRegEx" : "Blei et al\\.",
      "year" : 2003
    }, {
      "title" : "An unsupervised aspect-sentiment model for online reviews",
      "author" : [ "Samuel Brody", "Noemie Elhadad." ],
      "venue" : "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10,",
      "citeRegEx" : "Brody and Elhadad.,? 2010",
      "shortCiteRegEx" : "Brody and Elhadad.",
      "year" : 2010
    }, {
      "title" : "Extracting decisions from multi-party dialogue using directed graphical models and semantic similarity",
      "author" : [ "Trung H. Bui", "Matthew Frampton", "John Dowding", "Stanley Peters." ],
      "venue" : "Proceedings of the SIGDIAL 2009 Conference, pages 235–243.",
      "citeRegEx" : "Bui et al\\.,? 2009",
      "shortCiteRegEx" : "Bui et al\\.",
      "year" : 2009
    }, {
      "title" : "Automatic summarization of meeting data: A feasibility study",
      "author" : [ "Anne Hendrik Buist", "Wessel Kraaij", "Stephan Raaijmakers." ],
      "venue" : "Proc. Meeting of Computational Linguistics in the Netherlands (CLIN).",
      "citeRegEx" : "Buist et al\\.,? 2004",
      "shortCiteRegEx" : "Buist et al\\.",
      "year" : 2004
    }, {
      "title" : "Methods for Mining and Summarizing Text Conversations",
      "author" : [ "Giuseppe Carenini", "Gabriel Murray", "Raymond Ng." ],
      "venue" : "Morgan & Claypool Publishers.",
      "citeRegEx" : "Carenini et al\\.,? 2011",
      "shortCiteRegEx" : "Carenini et al\\.",
      "year" : 2011
    }, {
      "title" : "A hybrid hierarchical model for multi-document summarization",
      "author" : [ "Asli Celikyilmaz", "Dilek Hakkani-Tur." ],
      "venue" : "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10, pages 815–824, Stroudsburg, PA, USA. Associa-",
      "citeRegEx" : "Celikyilmaz and Hakkani.Tur.,? 2010",
      "shortCiteRegEx" : "Celikyilmaz and Hakkani.Tur.",
      "year" : 2010
    }, {
      "title" : "Extractive spoken document summarization for information retrieval",
      "author" : [ "Berlin Chen", "Yi-Ting Chen." ],
      "venue" : "Pattern Recogn. Lett., 29:426–437, March.",
      "citeRegEx" : "Chen and Chen.,? 2008",
      "shortCiteRegEx" : "Chen and Chen.",
      "year" : 2008
    }, {
      "title" : "A segmented topic model based on the two-parameter poisson-dirichlet process",
      "author" : [ "Lan Du", "Wray Buntine", "Huidong Jin." ],
      "venue" : "Mach. Learn., 81:5–19, October.",
      "citeRegEx" : "Du et al\\.,? 2010",
      "shortCiteRegEx" : "Du et al\\.",
      "year" : 2010
    }, {
      "title" : "Identifying relevant phrases to summarize decisions in spoken meetings",
      "author" : [ "Raquel Fernández", "Matthew Frampton", "John Dowding", "Anish Adukuzhiyil", "Patrick Ehlen", "Stanley Peters." ],
      "venue" : "INTERSPEECH-2008, pages 78–81.",
      "citeRegEx" : "Fernández et al\\.,? 2008",
      "shortCiteRegEx" : "Fernández et al\\.",
      "year" : 2008
    }, {
      "title" : "Real-time decision detection in multi-party dialogue",
      "author" : [ "Matthew Frampton", "Jia Huang", "Trung Huu Bui", "Stanley Peters." ],
      "venue" : "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 3 - Volume 3, pages 1133–1141.",
      "citeRegEx" : "Frampton et al\\.,? 2009",
      "shortCiteRegEx" : "Frampton et al\\.",
      "year" : 2009
    }, {
      "title" : "A skip-chain conditional random field for ranking meeting utterances by importance",
      "author" : [ "Michel Galley." ],
      "venue" : "Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 364– 372.",
      "citeRegEx" : "Galley.,? 2006",
      "shortCiteRegEx" : "Galley.",
      "year" : 2006
    }, {
      "title" : "A global optimization framework for meeting summarization",
      "author" : [ "Dan Gillick", "Korbinian Riedhammer", "Benoit Favre", "Dilek Hakkani-Tur." ],
      "venue" : "Proceedings of the 2009 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP",
      "citeRegEx" : "Gillick et al\\.,? 2009",
      "shortCiteRegEx" : "Gillick et al\\.",
      "year" : 2009
    }, {
      "title" : "Generic text summarization using relevance measure and latent semantic analysis",
      "author" : [ "Yihong Gong", "Xin Liu." ],
      "venue" : "Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR ’01, pages 19–",
      "citeRegEx" : "Gong and Liu.,? 2001",
      "shortCiteRegEx" : "Gong and Liu.",
      "year" : 2001
    }, {
      "title" : "Finding scientific topics",
      "author" : [ "T.L. Griffiths", "M. Steyvers." ],
      "venue" : "Proceedings of the National Academy of Sciences, 101(Suppl. 1):5228–5235, April.",
      "citeRegEx" : "Griffiths and Steyvers.,? 2004",
      "shortCiteRegEx" : "Griffiths and Steyvers.",
      "year" : 2004
    }, {
      "title" : "Exploring content models for multi-document summarization",
      "author" : [ "Aria Haghighi", "Lucy Vanderwende." ],
      "venue" : "Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,",
      "citeRegEx" : "Haghighi and Vanderwende.,? 2009",
      "shortCiteRegEx" : "Haghighi and Vanderwende.",
      "year" : 2009
    }, {
      "title" : "Latent topic modeling for audio corpus summarization",
      "author" : [ "Timothy J. Hazen." ],
      "venue" : "INTERSPEECH, pages 913–916.",
      "citeRegEx" : "Hazen.,? 2011",
      "shortCiteRegEx" : "Hazen.",
      "year" : 2011
    }, {
      "title" : "Text categorization with Support Vector Machines: Learning with many relevant features",
      "author" : [ "Thorsten Joachims." ],
      "venue" : "Claire Nédellec and Céline Rouveirol, editors, Machine Learning: ECML-98, volume 1398, chapter 19, pages 137–142. Berlin/Heidelberg.",
      "citeRegEx" : "Joachims.,? 1998",
      "shortCiteRegEx" : "Joachims.",
      "year" : 1998
    }, {
      "title" : "Improved spoken document summarization using probabilistic latent semantic analysis (plsa)",
      "author" : [ "Sheng-Yi Kong", "Lin shan Leek." ],
      "venue" : "Proceedings of the 2006 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP ’06.",
      "citeRegEx" : "Kong and Leek.,? 2006",
      "shortCiteRegEx" : "Kong and Leek.",
      "year" : 2006
    }, {
      "title" : "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
      "author" : [ "John D. Lafferty", "Andrew McCallum", "Fernando C.N. Pereira." ],
      "venue" : "Proceedings of the Eighteenth International Conference on Machine Learning, ICML ’01, pages",
      "citeRegEx" : "Lafferty et al\\.,? 2001",
      "shortCiteRegEx" : "Lafferty et al\\.",
      "year" : 2001
    }, {
      "title" : "A risk minimization framework for extractive speech summarization",
      "author" : [ "Shih-Hsiang Lin", "Berlin Chen." ],
      "venue" : "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10, pages 79–87. Association for Computational Linguis-",
      "citeRegEx" : "Lin and Chen.,? 2010",
      "shortCiteRegEx" : "Lin and Chen.",
      "year" : 2010
    }, {
      "title" : "Automatic evaluation of summaries using n-gram co-occurrence statistics",
      "author" : [ "Chin-Yew Lin", "Eduard Hovy." ],
      "venue" : "Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technol-",
      "citeRegEx" : "Lin and Hovy.,? 2003",
      "shortCiteRegEx" : "Lin and Hovy.",
      "year" : 2003
    }, {
      "title" : "Leveraging kullback-leibler divergence measures and informationrich cues for speech summarization",
      "author" : [ "S.-H. Lin", "Y.-M. Yeh", "B. Chen" ],
      "venue" : null,
      "citeRegEx" : "Lin et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2010
    }, {
      "title" : "Unsupervised approaches for automatic keyword extraction using meeting transcripts",
      "author" : [ "Feifan Liu", "Deana Pennell", "Fei Liu", "Yang Liu." ],
      "venue" : "Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the As-",
      "citeRegEx" : "Liu et al\\.,? 2009",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2009
    }, {
      "title" : "A supervised framework for keyword extraction from meeting transcripts",
      "author" : [ "Fei Liu", "Feifan Liu", "Yang Liu." ],
      "venue" : "IEEE Transactions on Audio, Speech & Language Processing, 19(3):538–548.",
      "citeRegEx" : "Liu et al\\.,? 2011",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2011
    }, {
      "title" : "Multi-aspect sentiment analysis with topic models",
      "author" : [ "Bin Lu", "Myle Ott", "Claire Cardie", "Benjamin Tsou." ],
      "venue" : "Workshop on Sentiment Elicitation from Natural Text for Information Retrieval and Extraction.",
      "citeRegEx" : "Lu et al\\.,? 2011",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2011
    }, {
      "title" : "Comparing Lexical, Acoustic/Prosodic, Structural and Discourse Features for Speech Summarization",
      "author" : [ "Sameer Maskey", "Julia Hirschberg." ],
      "venue" : "Proc. European Conference on Speech Communication and Technology (Eurospeech).",
      "citeRegEx" : "Maskey and Hirschberg.,? 2005",
      "shortCiteRegEx" : "Maskey and Hirschberg.",
      "year" : 2005
    }, {
      "title" : "Towards online speech summarization",
      "author" : [ "Gabriel Murray", "Steve Renals." ],
      "venue" : "INTERSPEECH, pages 2785–2788.",
      "citeRegEx" : "Murray and Renals.,? 2007",
      "shortCiteRegEx" : "Murray and Renals.",
      "year" : 2007
    }, {
      "title" : "Extractive summarization of meeting recordings",
      "author" : [ "Gabriel Murray", "Steve Renals", "Jean Carletta." ],
      "venue" : "in Proceedings of the 9th European Conference on Speech Communication and Technology, pages 593– 596.",
      "citeRegEx" : "Murray et al\\.,? 2005",
      "shortCiteRegEx" : "Murray et al\\.",
      "year" : 2005
    }, {
      "title" : "Interpretation and transformation for abstracting conversations",
      "author" : [ "Gabriel Murray", "Giuseppe Carenini", "Raymond Ng." ],
      "venue" : "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguis-",
      "citeRegEx" : "Murray et al\\.,? 2010a",
      "shortCiteRegEx" : "Murray et al\\.",
      "year" : 2010
    }, {
      "title" : "Generating and validating abstracts of meeting conversations: a user study",
      "author" : [ "Gabriel Murray", "Giuseppe Carenini", "Raymond T. Ng." ],
      "venue" : "INLG’10.",
      "citeRegEx" : "Murray et al\\.,? 2010b",
      "shortCiteRegEx" : "Murray et al\\.",
      "year" : 2010
    }, {
      "title" : "Long story short - global unsupervised models for keyphrase based meeting summarization",
      "author" : [ "Korbinian Riedhammer", "Benoit Favre", "Dilek Hakkani-Tür." ],
      "venue" : "Speech Commun., 52(10):801–815, October.",
      "citeRegEx" : "Riedhammer et al\\.,? 2010",
      "shortCiteRegEx" : "Riedhammer et al\\.",
      "year" : 2010
    }, {
      "title" : "Modeling online reviews with multi-grain topic models",
      "author" : [ "Ivan Titov", "Ryan McDonald." ],
      "venue" : "Proceeding of the 17th international conference on World Wide Web, WWW ’08, pages 111–120. ACM.",
      "citeRegEx" : "Titov and McDonald.,? 2008",
      "shortCiteRegEx" : "Titov and McDonald.",
      "year" : 2008
    }, {
      "title" : "Summarizing decisions in spoken meetings",
      "author" : [ "Lu Wang", "Claire Cardie." ],
      "venue" : "Proceedings of the Workshop on Automatic Summarization for Different Genres, Media, and Languages, pages 16–24, Portland, Oregon, June. Association for Computational Linguis-",
      "citeRegEx" : "Wang and Cardie.,? 2011",
      "shortCiteRegEx" : "Wang and Cardie.",
      "year" : 2011
    }, {
      "title" : "Using confusion networks for speech summarization",
      "author" : [ "Shasha Xie", "Yang Liu." ],
      "venue" : "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10, pages 46–54. Associ-",
      "citeRegEx" : "Xie and Liu.,? 2010",
      "shortCiteRegEx" : "Xie and Liu.",
      "year" : 2010
    }, {
      "title" : "Evaluating the effectiveness of features and sampling in extractive meeting summarization",
      "author" : [ "Shasha Xie", "Yang Liu", "Hui Lin." ],
      "venue" : "Proc. of IEEE Spoken Language Technology (SLT).",
      "citeRegEx" : "Xie et al\\.,? 2008",
      "shortCiteRegEx" : "Xie et al\\.",
      "year" : 2008
    }, {
      "title" : "Automatic summarization of open-domain multiparty dialogues in diverse genres",
      "author" : [ "Klaus Zechner." ],
      "venue" : "Comput. Linguist., 28:447–485, December.",
      "citeRegEx" : "Zechner.,? 2002",
      "shortCiteRegEx" : "Zechner.",
      "year" : 2002
    } ],
    "referenceMentions" : [ {
      "referenceID" : 36,
      "context" : "Not surprisingly, there is an increasing interest in developing methods for extractive summarization for meetings and conversations (Zechner, 2002; Maskey and Hirschberg, 2005; Galley, 2006; Lin and Chen, 2010; Murray et al., 2010a).",
      "startOffset" : 132,
      "endOffset" : 232
    }, {
      "referenceID" : 26,
      "context" : "Not surprisingly, there is an increasing interest in developing methods for extractive summarization for meetings and conversations (Zechner, 2002; Maskey and Hirschberg, 2005; Galley, 2006; Lin and Chen, 2010; Murray et al., 2010a).",
      "startOffset" : 132,
      "endOffset" : 232
    }, {
      "referenceID" : 11,
      "context" : "Not surprisingly, there is an increasing interest in developing methods for extractive summarization for meetings and conversations (Zechner, 2002; Maskey and Hirschberg, 2005; Galley, 2006; Lin and Chen, 2010; Murray et al., 2010a).",
      "startOffset" : 132,
      "endOffset" : 232
    }, {
      "referenceID" : 20,
      "context" : "Not surprisingly, there is an increasing interest in developing methods for extractive summarization for meetings and conversations (Zechner, 2002; Maskey and Hirschberg, 2005; Galley, 2006; Lin and Chen, 2010; Murray et al., 2010a).",
      "startOffset" : 132,
      "endOffset" : 232
    }, {
      "referenceID" : 29,
      "context" : "Not surprisingly, there is an increasing interest in developing methods for extractive summarization for meetings and conversations (Zechner, 2002; Maskey and Hirschberg, 2005; Galley, 2006; Lin and Chen, 2010; Murray et al., 2010a).",
      "startOffset" : 132,
      "endOffset" : 232
    }, {
      "referenceID" : 5,
      "context" : "Carenini et al. (2011) describe the specific need for focused summaries of meetings, i.",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 7,
      "context" : "However, very little work has investigated its role in spoken document summarization (Chen and Chen, 2008; Hazen, 2011), and much less conducted comparisons among topic modeling approaches for focused summarization in meetings.",
      "startOffset" : 85,
      "endOffset" : 119
    }, {
      "referenceID" : 16,
      "context" : "However, very little work has investigated its role in spoken document summarization (Chen and Chen, 2008; Hazen, 2011), and much less conducted comparisons among topic modeling approaches for focused summarization in meetings.",
      "startOffset" : 85,
      "endOffset" : 119
    }, {
      "referenceID" : 3,
      "context" : "They are similar, but not completely equivalent, to the decision dialogue acts (DDAs) of Bui et al. (2009), Fernández et al.",
      "startOffset" : 89,
      "endOffset" : 107
    }, {
      "referenceID" : 3,
      "context" : "They are similar, but not completely equivalent, to the decision dialogue acts (DDAs) of Bui et al. (2009), Fernández et al. (2008), Frampton et al.",
      "startOffset" : 89,
      "endOffset" : 132
    }, {
      "referenceID" : 3,
      "context" : "They are similar, but not completely equivalent, to the decision dialogue acts (DDAs) of Bui et al. (2009), Fernández et al. (2008), Frampton et al. (2009). ilmaz and Hakkani-Tur, 2010).",
      "startOffset" : 89,
      "endOffset" : 156
    }, {
      "referenceID" : 30,
      "context" : "• As a step towards creating the abstractive summaries that people prefer when dealing with spoken language (Murray et al., 2010b), we propose a token-level rather than sentence-level framework",
      "startOffset" : 108,
      "endOffset" : 130
    }, {
      "referenceID" : 2,
      "context" : "We investigate three topic models — Local LDA (LocalLDA) (Brody and Elhadad, 2010), Multi-grain LDA (MG-LDA) (Titov and McDonald, 2008) and Segmented Topic Model (STM) (Du et al.",
      "startOffset" : 57,
      "endOffset" : 82
    }, {
      "referenceID" : 32,
      "context" : "We investigate three topic models — Local LDA (LocalLDA) (Brody and Elhadad, 2010), Multi-grain LDA (MG-LDA) (Titov and McDonald, 2008) and Segmented Topic Model (STM) (Du et al.",
      "startOffset" : 109,
      "endOffset" : 135
    }, {
      "referenceID" : 8,
      "context" : "We investigate three topic models — Local LDA (LocalLDA) (Brody and Elhadad, 2010), Multi-grain LDA (MG-LDA) (Titov and McDonald, 2008) and Segmented Topic Model (STM) (Du et al., 2010) — which can utilize the latent topic structure on utterance level instead of document level.",
      "startOffset" : 168,
      "endOffset" : 185
    }, {
      "referenceID" : 27,
      "context" : "tion (Murray and Renals, 2007), we investigate the role of context in our token-level summarization framework.",
      "startOffset" : 5,
      "endOffset" : 30
    }, {
      "referenceID" : 4,
      "context" : "Early work in this area investigated supervised learning methods, including maximum entropy, conditional random fields (CRFs), and support vector machines (SVMs) (Buist et al., 2004; Galley, 2006; Xie et al., 2008).",
      "startOffset" : 162,
      "endOffset" : 214
    }, {
      "referenceID" : 11,
      "context" : "Early work in this area investigated supervised learning methods, including maximum entropy, conditional random fields (CRFs), and support vector machines (SVMs) (Buist et al., 2004; Galley, 2006; Xie et al., 2008).",
      "startOffset" : 162,
      "endOffset" : 214
    }, {
      "referenceID" : 35,
      "context" : "Early work in this area investigated supervised learning methods, including maximum entropy, conditional random fields (CRFs), and support vector machines (SVMs) (Buist et al., 2004; Galley, 2006; Xie et al., 2008).",
      "startOffset" : 162,
      "endOffset" : 214
    }, {
      "referenceID" : 36,
      "context" : "methods, maximal marginal relevance (MMR) is investigated in (Zechner, 2002) and (Xie and Liu, 2010).",
      "startOffset" : 61,
      "endOffset" : 76
    }, {
      "referenceID" : 34,
      "context" : "methods, maximal marginal relevance (MMR) is investigated in (Zechner, 2002) and (Xie and Liu, 2010).",
      "startOffset" : 81,
      "endOffset" : 100
    }, {
      "referenceID" : 12,
      "context" : "Gillick et al. (2009) introduce a conceptbased global optimization framework by using integer linear programming (ILP).",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 9,
      "context" : "Only in very recent works has decision summarization been addressed in (Fernández et al., 2008), (Bui et al.",
      "startOffset" : 71,
      "endOffset" : 95
    }, {
      "referenceID" : 3,
      "context" : ", 2008), (Bui et al., 2009) and (Wang and Cardie, 2011).",
      "startOffset" : 9,
      "endOffset" : 27
    }, {
      "referenceID" : 33,
      "context" : ", 2009) and (Wang and Cardie, 2011).",
      "startOffset" : 12,
      "endOffset" : 35
    }, {
      "referenceID" : 9,
      "context" : "(Fernández et al., 2008) and (Bui et al.",
      "startOffset" : 0,
      "endOffset" : 24
    }, {
      "referenceID" : 3,
      "context" : ", 2008) and (Bui et al., 2009) utilize semantic parser to identify candidate phrases for decision summaries and employ SVM to rank those phrases.",
      "startOffset" : 12,
      "endOffset" : 30
    }, {
      "referenceID" : 3,
      "context" : ", 2008), (Bui et al., 2009) and (Wang and Cardie, 2011). (Fernández et al., 2008) and (Bui et al., 2009) utilize semantic parser to identify candidate phrases for decision summaries and employ SVM to rank those phrases. They also train HMM and SVM directly on a set of decision-related dialogue acts on token level and use the classifiers to identify summary-worthy words. Wang and Cardie (2011) provide an exploration on supervised and unsupervised learning for decision summarization on both utterance- and token- level.",
      "startOffset" : 10,
      "endOffset" : 396
    }, {
      "referenceID" : 0,
      "context" : "Our work also arises out of applying topic models to text summarization (Bhandari et al., 2008; Haghighi and Vanderwende, 2009; Celikyilmaz and Hakkani-Tur, 2010; Celikyilmaz and Hakkani-Tur, 2010).",
      "startOffset" : 72,
      "endOffset" : 197
    }, {
      "referenceID" : 15,
      "context" : "Our work also arises out of applying topic models to text summarization (Bhandari et al., 2008; Haghighi and Vanderwende, 2009; Celikyilmaz and Hakkani-Tur, 2010; Celikyilmaz and Hakkani-Tur, 2010).",
      "startOffset" : 72,
      "endOffset" : 197
    }, {
      "referenceID" : 6,
      "context" : "Our work also arises out of applying topic models to text summarization (Bhandari et al., 2008; Haghighi and Vanderwende, 2009; Celikyilmaz and Hakkani-Tur, 2010; Celikyilmaz and Hakkani-Tur, 2010).",
      "startOffset" : 72,
      "endOffset" : 197
    }, {
      "referenceID" : 6,
      "context" : "Our work also arises out of applying topic models to text summarization (Bhandari et al., 2008; Haghighi and Vanderwende, 2009; Celikyilmaz and Hakkani-Tur, 2010; Celikyilmaz and Hakkani-Tur, 2010).",
      "startOffset" : 72,
      "endOffset" : 197
    }, {
      "referenceID" : 7,
      "context" : "There are some works for applying document-level topic models to speech summarization (Kong and shan Leek, 2006; Chen and Chen, 2008; Hazen, 2011).",
      "startOffset" : 86,
      "endOffset" : 146
    }, {
      "referenceID" : 16,
      "context" : "There are some works for applying document-level topic models to speech summarization (Kong and shan Leek, 2006; Chen and Chen, 2008; Hazen, 2011).",
      "startOffset" : 86,
      "endOffset" : 146
    }, {
      "referenceID" : 23,
      "context" : "meetings (Liu et al., 2009; Liu et al., 2011) and keyphrase based summarization (Riedhammer et al.",
      "startOffset" : 9,
      "endOffset" : 45
    }, {
      "referenceID" : 24,
      "context" : "meetings (Liu et al., 2009; Liu et al., 2011) and keyphrase based summarization (Riedhammer et al.",
      "startOffset" : 9,
      "endOffset" : 45
    }, {
      "referenceID" : 31,
      "context" : ", 2011) and keyphrase based summarization (Riedhammer et al., 2010).",
      "startOffset" : 42,
      "endOffset" : 67
    }, {
      "referenceID" : 0,
      "context" : "In (Bhandari et al., 2008), several sentence scoring functions are introduced based on Probabilistic Latent Semantic Indexing.",
      "startOffset" : 3,
      "endOffset" : 26
    }, {
      "referenceID" : 7,
      "context" : "Chen and Chen (2008) propose a Topical Mixture Model (TMM) for speech summariza-",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 13,
      "context" : "(VSM) (Gong and Liu, 2001), Latent Semantic Analysis (LSA) (Gong and Liu, 2001) and Maximum Marginal Relevance (MMR) (Murray et al.",
      "startOffset" : 6,
      "endOffset" : 26
    }, {
      "referenceID" : 13,
      "context" : "(VSM) (Gong and Liu, 2001), Latent Semantic Analysis (LSA) (Gong and Liu, 2001) and Maximum Marginal Relevance (MMR) (Murray et al.",
      "startOffset" : 59,
      "endOffset" : 79
    }, {
      "referenceID" : 28,
      "context" : "(VSM) (Gong and Liu, 2001), Latent Semantic Analysis (LSA) (Gong and Liu, 2001) and Maximum Marginal Relevance (MMR) (Murray et al., 2005).",
      "startOffset" : 117,
      "endOffset" : 138
    }, {
      "referenceID" : 15,
      "context" : "Kullback-Lieber (KL) divergence is explored for summarization in (Haghighi and Vanderwende, 2009) and (Lin et al.",
      "startOffset" : 65,
      "endOffset" : 97
    }, {
      "referenceID" : 22,
      "context" : "Kullback-Lieber (KL) divergence is explored for summarization in (Haghighi and Vanderwende, 2009) and (Lin et al., 2010), where it is used to measure the distance of distributions between the document and the summary.",
      "startOffset" : 102,
      "endOffset" : 120
    }, {
      "referenceID" : 2,
      "context" : "Local LDA (LocalLDA) (Brody and Elhadad, 2010) uses almost the same probabilistic generative model as Latent Dirichlet Allocation (LDA) (Blei et al.",
      "startOffset" : 21,
      "endOffset" : 46
    }, {
      "referenceID" : 1,
      "context" : "Local LDA (LocalLDA) (Brody and Elhadad, 2010) uses almost the same probabilistic generative model as Latent Dirichlet Allocation (LDA) (Blei et al., 2003), except that it treats each sentence as a separate document2.",
      "startOffset" : 136,
      "endOffset" : 155
    }, {
      "referenceID" : 8,
      "context" : "The last model we utilize is Segmented Topic Model (STM) (Du et al., 2010), which jointly models document- and sentence-level latent topics using a two-parameter Poisson Dirichlet Process (PDP).",
      "startOffset" : 57,
      "endOffset" : 74
    }, {
      "referenceID" : 33,
      "context" : "tions to create perfect partitionings of the DRDAs as the input; in the System Clusterings setting, we employ a hierarchical agglomerative clustering algorithm used for this task in previous work (Wang and Cardie, 2011).",
      "startOffset" : 196,
      "endOffset" : 219
    }, {
      "referenceID" : 33,
      "context" : "tions to create perfect partitionings of the DRDAs as the input; in the System Clusterings setting, we employ a hierarchical agglomerative clustering algorithm used for this task in previous work (Wang and Cardie, 2011). The Wang and Cardie (2011) cluster-",
      "startOffset" : 197,
      "endOffset" : 248
    }, {
      "referenceID" : 21,
      "context" : "To evaluate the performance of various summarization approaches, we use the widely accepted ROUGE (Lin and Hovy, 2003) metrics.",
      "startOffset" : 98,
      "endOffset" : 118
    }, {
      "referenceID" : 21,
      "context" : "To evaluate the performance of various summarization approaches, we use the widely accepted ROUGE (Lin and Hovy, 2003) metrics. We use the stemming option of the ROUGE software at http://berouge.com/ and remove stopwords from both the system and gold-standard summaries, same as Riedhammer et al. (2010) do.",
      "startOffset" : 99,
      "endOffset" : 304
    }, {
      "referenceID" : 25,
      "context" : "Inference and Hyperparameters We use the implementation from (Lu et al., 2011) for the three topic models in Section 4.",
      "startOffset" : 61,
      "endOffset" : 78
    }, {
      "referenceID" : 14,
      "context" : "The collapsed Gibbs Sampling approach (Griffiths and Steyvers, 2004) is exploited for inference.",
      "startOffset" : 38,
      "endOffset" : 68
    }, {
      "referenceID" : 2,
      "context" : "Hyperparameters are chosen according to (Brody and Elhadad, 2010), (Titov and McDonald, 2008) and (Du et al.",
      "startOffset" : 40,
      "endOffset" : 65
    }, {
      "referenceID" : 32,
      "context" : "Hyperparameters are chosen according to (Brody and Elhadad, 2010), (Titov and McDonald, 2008) and (Du et al.",
      "startOffset" : 67,
      "endOffset" : 93
    }, {
      "referenceID" : 8,
      "context" : "Hyperparameters are chosen according to (Brody and Elhadad, 2010), (Titov and McDonald, 2008) and (Du et al., 2010).",
      "startOffset" : 98,
      "endOffset" : 115
    }, {
      "referenceID" : 32,
      "context" : "And the number of local topic is set as the same number of global topic as discussed in (Titov and McDonald, 2008).",
      "startOffset" : 88,
      "endOffset" : 114
    }, {
      "referenceID" : 7,
      "context" : "1 Baselines and Comparisons We compare our token-level summarization framework based on the fine-grained topic models to (1) two unsupervised baselines, (2) token-level summarization by LDA, (3) utterance-level summarization by Topical Mixture Model (TMM) (Chen and Chen, 2008), (4) utterance-level summarization based on the fine-grained topic models using existing metrics (Section 3.",
      "startOffset" : 256,
      "endOffset" : 277
    }, {
      "referenceID" : 31,
      "context" : "As in (Riedhammer et al., 2010) and (Wang and Cardie, 2011), this baseline simply selects the longest DRDA in each cluster as the summary.",
      "startOffset" : 6,
      "endOffset" : 31
    }, {
      "referenceID" : 33,
      "context" : ", 2010) and (Wang and Cardie, 2011), this baseline simply selects the longest DRDA in each cluster as the summary.",
      "startOffset" : 12,
      "endOffset" : 35
    }, {
      "referenceID" : 33,
      "context" : "Following Wang and Cardie (2011), the second baseline selects the",
      "startOffset" : 10,
      "endOffset" : 33
    }, {
      "referenceID" : 7,
      "context" : "How do fine-grained topic models compare to basic topic models or baselines? Figure 2 demonstrates that by using the DomSum token-level summarization framework, the three fine-grained topic models uniformly outperform the two non-trivial baselines and TMM (Chen and Chen, 2008) (reimplemented by us) that generates utterance-level summaries.",
      "startOffset" : 256,
      "endOffset" : 277
    }, {
      "referenceID" : 17,
      "context" : "We use Support Vector Machines (Joachims, 1998) with RBF kernel and order1 Conditional Random Fields (Lafferty et al.",
      "startOffset" : 31,
      "endOffset" : 47
    }, {
      "referenceID" : 19,
      "context" : "We use Support Vector Machines (Joachims, 1998) with RBF kernel and order1 Conditional Random Fields (Lafferty et al., 2001) — trained with the same features as (Wang and Cardie, 2011) to identify the summary-worthy tokens to include in the abstract.",
      "startOffset" : 101,
      "endOffset" : 124
    }, {
      "referenceID" : 33,
      "context" : ", 2001) — trained with the same features as (Wang and Cardie, 2011) to identify the summary-worthy tokens to include in the abstract.",
      "startOffset" : 44,
      "endOffset" : 67
    } ],
    "year" : 2016,
    "abstractText" : "We present a token-level decision summarization framework that utilizes the latent topic structures of utterances to identify “summaryworthy” words. Concretely, a series of unsupervised topic models is explored and experimental results show that fine-grained topic models, which discover topics at the utterance-level rather than the document-level, can better identify the gist of the decisionmaking process. Moreover, our proposed token-level summarization approach, which is able to remove redundancies within utterances, outperforms existing utterance ranking based summarization methods. Finally, context information is also investigated to add additional relevant information to the summary.",
    "creator" : "TeX"
  }
}