{
  "name" : "1107.0020.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Learning to Order BDD Variables in Verification",
    "authors" : [ "Orna Grumberg", "Shlomi Livne", "Shaul Markovitch" ],
    "emails" : [ "orna@cs.technion.ac.il", "slivne@cs.technion.ac.il", "shaulm@cs.technion.ac.il" ],
    "sections" : [ {
      "heading" : null,
      "text" : "We propose an alternative approach in which the variable ordering algorithm gains “ordering experience” from training models and uses the learned knowledge for finding good orders. Our methodology is based on offline learning of pair precedence classifiers from training models, that is, learning which variable pair permutation is more likely to lead to a good order. For each training model, a number of training sequences are evaluated. Every training model variable pair permutation is then tagged based on its performance on the evaluated orders. The tagged permutations are then passed through a feature extractor and are given as examples to a classifier creation algorithm. Given a model for which an order is requested, the ordering algorithm consults each precedence classifier and constructs a pair precedence table which is used to create the order.\nOur algorithm was integrated with SMV, which is one of the most widely used verification systems. Preliminary empirical evaluation of our methodology, using real benchmark models, shows performance that is better than random ordering and is competitive with existing algorithms that use expert knowledge. We believe that in sub-domains of models (alu, caches, etc.) our system will prove even more valuable. This is because it features the ability to learn sub-domain knowledge, something that no other ordering algorithm does."
    }, {
      "heading" : "1. Introduction",
      "text" : "The size and complexity of software and hardware systems have significantly increased in the past years. As a result, it is harder to guarantee their correct behavior. Thus, formal methods, preferably computerized, are needed for this task.\nOne of the most successful methods for automated verification of finite-state systems is temporal logic model checking (Clarke, Emerson, & Sistla, 1986; Queille & Sifakis, 1981). Temporal logics are suitable formalisms for describing the behavior of a program over time. A model checking procedure receives a finite-state model of the system and a specification\nc©2003 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.\nwritten as a temporal logic formula. It returns “yes” if the model satisfies the formula (meaning that the system behaves according to the specification). Otherwise, it returns “no”, along with a counter example that demonstrates a bad behavior.\nModel checking has been very successful in finding subtle errors in various systems. It is currently recognized by the hardware industry as an important component of the development phase of new designs. However, model checking procedures often suffer from high space requirements, needed for holding the transition relation and the intermediate results.\nOne of the most promising solutions to this problem is the use of binary decision diagrams (BDDs) (Akers, 1978; Bryant, 1986) as the basic data structure in model checking. BDDs are canonical representations of boolean functions and are often very concise in size. Their conciseness also yields efficiency in computation time. Since it is straightforward to represent the transition relation and the intermediate results as boolean functions, BDDs are particularly suitable for model checking. Today, existing industrial BDD-based verifiers, such as IBM’s RuleBase (Beer, Ben-David, Eisner, & Landver, 1996) and Motorola’s Verdict (Kaufmann & Pixley, 1997) are used by many companies in their development infrastructure.\nThe size of a BDD for a given function is sensitive to the ordering of the variables in the BDD. However, finding an optimal ordering, which yields a smallest BDD for a given function, is an NP-complete problem (Bollig & Wegener, 1996). Therefore, several heuristic algorithms based on expert knowledge have been developed for variable ordering in the hope of reducing the BDD size. Unfortunately, and in spite of the resources invested, these algorithms do not produce good enough variable orders. The reason for this may be that only general rules are used and no domain-specific knowledge is exploited.\nThe goal of this research is to develop learning techniques for acquiring and using domain-specific knowledge for variable ordering. We assume the availability of one or more training models. The training models are used for off-line acquisition of ordering experience which can be used for ordering variables of a previously unseen model.\nWe first present a method for converting the ordering learning task into a concept learning problem. The concept is the set of all ordered variable pairs that are in the “right” order. The examples are ordered pairs of variables of a given training model. We show a statistical method for tagging examples based on evaluated training orders and present a set of variable-pair features. The result is a standard concept learning problem. We apply decision tree learning to generate a decision tree for each training model. When used for an unseen model, we combine the trees and generate a partial order which is used for generating the required order. We also present an extension of the algorithm which learns context-based precedence relations.\nOur algorithm was integrated with SMV (McMillan, 1993), which is the backbone of many verification systems. Empirical evaluation of our methodology, using real benchmark models of hardware designs, shows performance that is much better than random ordering and is competitive with existing algorithms that use expert knowledge.\nSection 2 contains background on model checking. Section 3 presents our main algorithm with empirical evaluation. Section 4 shows the context-based algorithm. Our conclusions are presented in Section 5."
    }, {
      "heading" : "2. Background",
      "text" : "Model checking was introduced by Clarke and Emerson (1986) and by Queille and Sifakis (1981) in the early 1980s. They presented algorithms that automatically reason about temporal properties of finite state systems by exploring the state space. The use of binary decision diagrams (BDDs) to represent finite state systems and to perform symbolic state traversal is called symbolic model checking. The use of BDDs has greatly extended the capacity of model checkers. Models with 2100 and more states are routinely being verified.\nBDDs were introduced by Akers (1978) as compact representations for boolean functions. Bryant (1986) proposed ordered binary decision diagrams (OBDDs) as canonical representations of boolean functions. He also showed algorithms for computing boolean operations efficiently on OBDDs.\nThe following subsection gives an overview of how finite state systems are represented in symbolic model checking. BDDs are then described and the variable ordering problem is defined. Existing algorithms for static variable ordering algorithms are reviewed. Finally, a brief description of machine learning algorithms used for ordering is given."
    }, {
      "heading" : "2.1 Finite State Machines in Symbolic Model Checking",
      "text" : "Finite state systems (FSM) can be described by defining the set of possible states in a system and the transition relation between these states. A state typically describes values of components (e.g., latches in digital circuits), where each component is represented by a state variable. Let V = {v0, v1, ...vn−1} be the set of variables in a system. Let Kvi be the set of possible values for variable vi. Then a state in the system can be described by assigning values to all the variables in V . The set of all possible states SA is\nSA = Kv0 ×Kv1 ....×Kvn−1 .\nA state can be written using a function that is true only in this state:\n∧n−1 i=0 (vi == cj),\nwhere cj ∈ Kvi is the value of vi in the state. A set of states can be described by a function as the disjunction of the functions that represent the states.\nFigure 1 shows a 3-bit counter. A state in the 3-bit counter can be described by a tuple which gives an assignment to the 3 variables v2, v1, v0. For example, the tuple 〈1, 0, 0〉 represents the state with v2 = 1, v1 = 0, v0 = 0. The corresponding boolean expression for the state is (v2 == 1) ∧ (v1 == 0) ∧ (v0 == 0).\nIn order to describe a system, we also need to specify its transition relation. The transition relation describes all the possible transitions of each system state. It can thus be described by pairs of states, 〈present state, next state〉, where next state is a system state after a transition from the present state. The variables in V will represent the present state variables, and for each variable vi ∈ V we will define a corresponding next state variable v′i ∈ V ′. V ′ will denote the set of next state variables.\nAn example of a valid transition for the 3-bit counter is from 〈0, 0, 0〉 to 〈0, 0, 1〉. The boolean function which represents this transition is (v2 == 0) ∧ (v1 == 0) ∧ (v0 == 0) ∧ (v′2 == 0) ∧ (v ′ 1 == 0) ∧ (v ′ 0 == 1). The transition relation can be represented by a\nboolean function which is the disjunction of the boolean functions of each of the transitions. Table 1 shows the transition relation for the 3-bit counter.\nAn alternative method for describing the transition relation is for each state variable to define its valid next states. This form is known as the partitioned transition relation. The transition relation is then described by a set of functions (instead of one), one for each variable. For variable vi, a boolean function Ti(V, v ′ i) defines the next value of vi, v ′ i, given that the current state of the system is V .\nFor synchronous systems, in which there is a simultaneous transition of all the system components, the transition relation is\n∧n−1 i=0 Ti(V, v ′ i).\nIn model checking it is common to use the partitioned transition relation form of representation, since it is usually more compact in memory requirements and thus allows the handling of larger systems. For the 3-bit counter, the next state boolean functions are given below, where ⊗ stands for the boolean operator Xor.\nT0(V, v ′ 0) : (v ′ 0 == v0)\nT1(V, v ′ 1) : (v ′ 1 == (v0 ⊗ v1))\nT2(V, v ′ 2) : (v ′ 2 == (v2 ⊗ (v0 ∧ v1)))."
    }, {
      "heading" : "2.2 Binary Decision Diagrams",
      "text" : "A binary decision diagram (BDD) is a DAG (directed acyclic graph) representation of a boolean function. A BDD is composed of two sink nodes and several non-sink nodes. The two sink nodes, labeled 0 and 1, represent the corresponding boolean values. Each non-sink node is labeled with a boolean variable v and has two outgoing edges labeled 1 (or then) and 0 (or else). Each non-sink node represents the boolean function corresponding to its 1 edge if v = 1, or the boolean function corresponding to its 0 edge if v = 0.\nAn ordered binary decision diagram (OBDD) is a BDD with the constraint that the variables are ordered, and every root-to-sink path in the OBDD visits the variables in ascending order.\nA reduced ordered binary decision diagram (ROBDD) is an OBDD where each node represents a distinct logic function. This representation is a canonical BDD representation and the most compact representation possible for a given boolean function and a variable ordering.\nFigure 2 (a),(b) shows the OBDD and ROBDD (respectively) representations of the transition relation function for the 3-bit counter. The dashed lines are the 0 edges and the solid lines are the 1 edges. ROBDDs have only two leaf nodes, one with 1 and one with 0. We drew them several times to enhance readability. ROBDDs can also use complement edges, which produces an even more compact representation. We did not use complement edges, also for reasons of readability. Figure 2 (c),(d) shows the OBDD and ROBDD representations of the partitioned transition relation of the 3-bit counter. The variable order v2, v ′ 2, v1, v ′ 1, v0, v ′ 0 was used in all the representations. Variable ordering algorithms in model checking place the next state variable v′i adjacent to the present state variable vi.\nFor the rest of this document we will refer to ROBDDs as BDDs (unless we explicitly state otherwise).\nBollig and Wegener (1996) proved that finding an optimal variable ordering is an NPcomplete problem. An order is optimal if it yields a BDD with the smallest number of nodes. Bryant (1986) pointed out that variable ordering greatly influences the size of the BDD. He showed that for a boolean function, one variable ordering may yield a BDD that is exponential in the number of variables, while a different ordering may yield a BDD of polynomial size.\nFigure 3 gives an example of the effect of variable ordering on the BDD size for the function F (v1, v2, v3, v4) = (v1 = v3) ∧ (v2 = v4). In (a) the variable ordering is v1, v3, v2, v4 and in (b) the variable ordering is v1, v2, v3, v4.\nVarious algorithms have been developed for variable ordering. Exact algorithms (Ishiura, Sawada, & Yajima, 1991; Drechsler, Drechsler, & Slobodova, 1998; Friedman & Supowit, 1987) are algorithms that find the optimal order. These algorithms use a method similar to dynamic programming with pruning to find the optimal order. Due to the complexity of the problem, exact algorithms are only practical for small cases, and one usually has to turn to other heuristic methods. These heuristic methods can be roughly divided into two groups.\n1. Static Ordering (Aziz, Tasiran, & Brayton, 1994; Butler, Ross, & Rohit Kapur, 1991; Chung, Hajj, & Patel, 1993; Fujii, Ootomo, & Hori, 1993; Jain, Adams, & Fujita, 1998; Fujita, Fujisawa, & Kawato, 1988; Malik, Wang, Brayton, & Sangiovanni-Vincentelli, 1988; Touati, Savoj, Lin, Brayton, & Sangiovanni-Vincetelli, 1990) which try to find a good ordering before constructing the BDD. Most of these algorithms are based on the topological structure of the verified system.\n2. Dynamic Ordering (Rudell, 1993; Meinel & Slobodova, 1998; Bollig, Lobbing, & Wegener, 1995; Meinel & Slobodova, 1997; Meinel, Somenzi, & Theobald, 1997; Ishiura et al., 1991; Bern, Meinel, & Slobodova, 1995; Fujita, Kukimoto, & Brayton, 1995; Mercer, Kapur, & Ross, 1992; Zhuang, Benten, & Cheung, 1996; Drechsler, Becker,\n& Gockel, 1996; Panda & Somenzi, 1995; Panda, Somenzi, & Plessier, 1994), which given a BDD with some variable order, reorder the variables in the hope of finding a smaller BDD.\nIn model checking procedures, variable ordering is a central component. At the initial phase of model checking, when the system is translated into a BDD representation, Static Ordering is used. The order built at this stage greatly influences the memory usage during the whole computation. However, since model checking keeps producing and eliminating BDDs, the variable order should be changed dynamically in order to effect the size of the current BDDs. Dynamic Ordering is used in order to achieve this goal. It is applied by the model checking procedure whenever the size of the BDDs reaches a certain threshold.\nSince our work introduces a static ordering algorithm based on machine learning, the next subsection presents a review of the existing static algorithms. Most of these algorithms were developed for combinational circuits (i.e., models whose outputs depend only on their current inputs and not on inputs of previous cycles) and were described with hardware terminology. In order to simplify the description, we will describe them with the terminology we have used so far."
    }, {
      "heading" : "2.3 Static Ordering",
      "text" : "Static ordering algorithms try to find an initial good order for the BDD. To do so, they extract topological data from the model and use this data to determine an order. All the algorithms convert the model, described by a set of next state functions, into a directed graph known as the model connectivity graph. Vertices in the graph are variables and boolean operations (gates). A variable vertex represents a variable, while a gate vertex represents a function. The edges ni → nj in the graph are between ni, which is either a variable or gate vertex, and nj, which is a gate vertex. An edge ni → nj is placed if the function represented by ni is an operand (i.e., an immediate subfunction) of the function represented by nj. We can divide the static algorithms into four groups that differ in the way they use the graph information."
    }, {
      "heading" : "2.3.1 Graph Search Algorithms",
      "text" : "The method suggested by Malik et al. (1988) assigns to each vertex a level metric and orders the variables in decreasing level value. The level of vertices with no out edges is set to be zero and the level of every other vertex (vi) is set to be level(vi) = maxvj |vi→vj (level(vj)+1). This method resembles a BFS (breadth first search) which originates in nodes that have no out edges, and progresses backwards in the model. Fujita et al. (1988) proposed executing a DFS (depth first search) from the vertices with no out edges, and progressing backwards. Variables in this algorithm are added in post order form.\nThe algorithms of Malik et al. and Fujita et al. were designed for cases where only one function should be represented in a BDD. This is hardly ever the case in model checking. Butler et al. (1991) adapted the algorithm of Fujita et al. to models with multiple starting points (that is, multiple vertices with no out edges). Their heuristic guides the algorithm to select the first vertex as the vertex that represents the function which depends on the maximum number of variables. This heuristic also guides the search to advance (backwards)\nfrom an inner vertex to the vertex that leads to the maximum number of different variables. A tie breaking heuristic (Fujita, Fujisawa, & Matsunaga, 1993) for the enhanced algorithm advises selecting (in case of a tie) the vertex with the maximum number of out edges.\nThe DFS-based methods append the variables to the variable order. Another DFS-based algorithm relies on interleaving the variables in the order (Fujii et al., 1993). The algorithm adds a variable after the variable which precedes it in the DFS order."
    }, {
      "heading" : "2.3.2 Graph Evaluation Algorithms",
      "text" : "Graph evaluation algorithms use the model graph to evaluate the model variables and to perform guided search based on their evaluation values. Minato et al. (1990) propagate values backward through the graph, starting from vertices with no out edges, whose value is set to 1. In vertices of boolean operations, the values on the out edges are summed and the value obtained is divided equally between the in edges. This is done recursively until a vertex of a variable is reached. At variable vertices the propagated values are accumulated as the variable evaluation value. The order is constructed by iteratively adding the variable with the highest value, removing it from the graph, and updating the values.\nChung et al.(1993) proposed two algorithm frameworks. The first framework is composed of two sweeps. In the first sweep each vertex is assigned a value. The values are set by a propagating algorithm that starts from variable vertices with no in edges and advances forward (by their out edges) to all the vertices in the graph. In the second sweep a guided DFS initiated from vertices with no out edges is executed. This search is executed backward in the graph and is guided by the maximal value. This means that the order of traversal among vertex ancestors is according to their assigned value. A number of heuristics to compute the values of the vertices were proposed:\n1. Level-Based sets the value of variables with no input edges to be zero. The value of the other vertices is set to be the maximal vertex value over its inputs plus one.\n2. Fanout-Based propagates two values through the graph (one for each boolean value). At a boolean operation vertex the values are not summed and passed along. Rather, they are computed according to the boolean operation at the vertex. The initial values are of variables with no input edges. Their value is set to be the number of out edges the variable has.\nIn the second framework proposed by Chung et al., the shortest distance between each pair of variables is computed. The total distance of a variable is computed as the sum of its distances to all the variables. The variable with the lowest total distance is selected as the first variable. The next variable is selected as the closest variable to the last ordered variable. Ties are broken according to the distance to previous ordered variables.\nAll the graph evaluation algorithms try to order the variables so that the variable that most influences the model’s next state functions will be first. The algorithms differ by the methodology they use to order the other variables. Some algorithms order them so that variables which substantially influence the model’s next-state functions are placed higher in the variable order (toward the beginning of the order). Other algorithms place the other variables according to their proximity to previously ordered variables."
    }, {
      "heading" : "2.3.3 Decomposition Algorithms",
      "text" : "Decomposition algorithms break down the model into parts. The algorithms then solve two different problems. The first is finding a good order for each part, and the second is finding the order of the parts. The order is constructed by combining the solutions of the two problems.\nThe algorithm of Malik et al. was extended and adapted for finite-state machines (FSM) by Toutai et al. (1990). In their algorithm, a model is decomposed to its next state functions, each of which is considered separately. Variables of each next state function are ordered according to Malik et al. The next state functions are then ordered by a cost function. They are ordered so that functions with many overlapping variables will be adjacent. The variable order is obtained by adding the variables of the next state functions according to the order of the parts, while removing variables that already exist.\nThe algorithm of Aziz et al. (1994) decomposes the model in a different way. A model is a hierarchical composition that is constructed by joining a number of internal parts that pass information among themselves. Usually, there is less communication among the parts than within them. Variables of an internal part tend to depend highly on one another. The algorithm uses a process communication graph (PCG), which models the hierarchical structure of the model and the communication between the parts. In a PCG each vertex is an internal part, and an edge i → j connects vertex i and vertex j if part j depends on a bit of part i. The PCG has parallel edges i → j, one for each bit value in i that j depends upon. Alternatively, the edges could be weighted.\nGiven an order of the parts, an upper bound on the BDD size of the model can be computed. The computation is based on the size of the parts and the amount of communication between them. Heuristics guided by the upper bound are applied in order to determine the order of the parts. The order of the variables in each part is decided by one of the previous ordering algorithms."
    }, {
      "heading" : "2.3.4 Sample-Based Algorithms",
      "text" : "Sample-based static algorithms (Jain et al., 1998) are not real static algorithms in the sense that they do not create the order based on information extracted from the model description. Sample algorithms perform tests on parts of the model (building transition relations and reachable states). For each part, a number of orders are evaluated. The good orders are then merged to create a complete order for the model. Sampling algorithms use “traditional” algorithms in order to find the candidate orders for the parts. These candidate orders are then checked by the sampling algorithm."
    }, {
      "heading" : "2.3.5 Summary",
      "text" : "A majority of the graph search algorithms and graph evaluation algorithms were developed for other problems and adapted for symbolic model checking. Some of the algorithms were developed in the context of combinational circuits, while others were developed for the simple case of one function. In symbolic model checking the models are rarely combinational (their outputs almost always depend also on inputs of previous cycles), and there is more than one function to display. Adapting the existing algorithms to conform to the needs of\nsymbolic model checking has had various degrees of success. Most of the adapted algorithms are heuristic and apply a simple rule with some logical reasoning behind it.\nThe decomposition algorithms are either heuristic or provide a theoretical upper bound. However, the bounds they use are rarely realistic; for most models we require much smaller BDDs. The algorithms are also based on decomposing the model into parts and solving the ordering of each part using graph search algorithms. Thus, they also inherit the drawbacks of these algorithms.\nDespite the efforts that have been invested and the many algorithms that have been developed for static ordering, the results are not yet satisfactory. The produced BDDs are too large to manipulate, and dynamic ordering must be applied. One problem with the above approaches is their generality: they do not utilize domain-specific knowledge. Domain-specific knowledge is essential for solving the majority of complex problems. It is also difficult to retrieve. In the next subsection we discuss machine learning methods for acquiring domain-specific knowledge for ordering tasks."
    }, {
      "heading" : "2.4 Learning to Order Elements",
      "text" : "Learning to order elements can be done by first trying to induce a partial order, which can then be used for generating a total order. In this context, a partial order is usually called a preference predicate. Preference predicate induction is based on a set of tagged pairs of elements where the binary tag identifies the preferred element. Broos and Branting (1994) present a method for inducing a preference predicate using nearest neighbor classification. The distance between an untagged pair and each tagged pair is computed as the sum of distances between the corresponding elements. The closest tagged pair is selected. The preferred element of the untagged pair is the one matching the preferred element in the tagged pair.\nUtgoff and Saxena (1987) represent a pair A,B by the concatenated feature vector 〈a1, . . . an, b1, . . . bn〉. The preference predicate is a decision tree induced from these examples.\nUtgoff and Clouse (1991) represent a preference predicate by a polynomial. Let A = 〈a1, . . . an〉 , and B = 〈b1, . . . bn〉 be a pair of elements represented by feature vectors. Let w1, . . . , wn be a set of weights. The preference predicate P is defined as follows:\nP (A,B) =\n{\n1 ∑n\ni=1wi(ai − bi) ≥ 0 0 otherwise\nEach example represents a linear constraint and the weights are found by solving the set of constraints.\nCohen, Schapire and Singer (1999) extended the above mechanism by allowing any preference function fi instead of (ai − bi) in the above expressions. They also present two methods for generating a total order based on the induced preference predicate. Both methods use the preference predicate to construct a graph where the nodes are the elements to be ordered and a directed edge is placed between two elements that have a precedence relation. Two algorithms for inferring the order from the graph are given. The first defines for each node a degree which equals the sum of the outgoing edges minus the sum of the incoming edges. The order is then constructed by selecting the node with the greatest\ndegree and removing its edges from the graph. The second algorithm constructs the order in two stages. In the first stage, all the strongly connected components of the graph are found, and they are ordered according to the dependencies between them. In the second stage the elements of each component are ordered using the first algorithm."
    }, {
      "heading" : "3. A Learning Algorithm for Static Variable Ordering",
      "text" : "Producing a good variable order requires extensive understanding of BDDs and their relation to the model they represent. Such knowledge can be manually inserted by a human expert. However, this task is too complex for large models. Therefore, it is rarely done. Existing static ordering algorithms use relatively simple heuristic rules that are based on expert knowledge. These rules look at the model structure to compose the ordering. Since the rules are to be applied to all variables in all the models, they are general and thus limited in the ability to produce good orders. Alternatively, we can try to build a program that automatically acquires more specific knowledge based on ordering experience. In this section we present such an algorithm.\nThe first step in building such a learning algorithm is deciding what knowledge we wish to acquire from the ordering experience. The existing ordering algorithms demonstrate that the precedence relation between variables is a key consideration for the order creation. The graph search algorithms and the search-based graph evaluation algorithms try to place a variable after the variables that influence its next state value. Generally, a variable order of n variables yields\n(n 2 )\nprecedence pairs. A precedence pair vi ≺ vj denotes that variable vi should precede vj in the variable order. For example, the variable order a, b, c, d yields the precedence pairs a ≺ b, a ≺ c, a ≺ d, b ≺ c, b ≺ d, c ≺ d.\nThe above task of learning precedence pairs can be transformed into a concept learning task. A concept learning task is defined by:\n• A universe X over which the concept is learned;\n• A concept C – a subset of items in X that we want to learn (usually marked by its associated boolean characteristic function fc);\n• A set of examples – pairs of the form 〈x, fc(x)〉, where x ∈ X;\n• A set of features – functions above X that allow generalization.\nFor many learning tasks it is difficult to transform the problem to the format listed above. It is already clear from the discussion above that the general concept we wish to learn is the set of variable pairs in which the first should precede the second in the variable ordering1 .\nMore precisely, we define the universe over which the concept is learned as the set of all pairs 〈(vi, vj),M〉, where (vi,vj) is an ordered variable pair comprised of vi and vj, which are variables in the model M . Since we expect that some pairs will have no preferred order, we define a ternary instead of a binary concept. The ternary concept has the following classes:\n1. In practice, we will need only a small subset of the precedence pairs for constructing a total order.\n1. C+, the class of all 〈(vi, vj),M〉 for which it is preferable to place vi prior to vj in order to get a good initial order.\n2. C−, the class of all 〈(vi, vj),M〉 for which it is preferable to place vi after vj in order to get a good initial order.\n3. C?, the class of all 〈(vi, vj),M〉 for which placing vi before vj is just as likely to lead to a good variable order as placing vi after vj.\nIn the following subsections we describe the algorithms for learning and using this concept."
    }, {
      "heading" : "3.1 Algorithm Framework",
      "text" : "We start with the description of the general framework of the learning algorithm. Our goal is to find variable orders that yield BDDs with small number of nodes. Given a training model, the algorithm first generates a set of orders of its variables. We define a utility function u over variable orders as following. Each of the orders is used as the initial order for building the BDD representation of the model2. This BDD (denoted M-BDD) includes the model’s partitioned transition relation and its set of initial states. The utility u of a generated order is then defined to be reversely proportional to the the number of nodes in the M-BDD constructed with this order.\nA subset that consists of all the variable pairs that appear together in some next-state function is selected by the example extractor from all the possible variable pairs. We call such pairs interacting variable pairs. For example, if next(x) = y ∨ z then (y, z) is an interacting variable pair. The example tagger tags each of the selected ordered pairs with one of the classes C+, C−, or C?, based on the evaluated orders. The tagged pairs are forwarded to the feature extractor which, based on the model, computes for each pair its feature vector. The learner, which is an ID3 (Quinlan, 1986) decision tree generator, uses the tagged feature vectors to create a pair precedence classifier.\nSeveral training models are used in this manner to construct different pair precedence classifiers. When solving a new unseen problem, these pair precedence classifiers are used by the ordering algorithm to create a variable order.\nThe learning framework for creating a pair precedence classifier of a training model is given in Figure 4. The complete data flow is displayed in Figure 5. The following subsections describe in greater detail the components of the framework."
    }, {
      "heading" : "3.2 The Training Sequence Generator",
      "text" : "The goal of the training sequence generator is to produce orders with high variance in quality which is exploited by the tagger (see Subsection 3.4). The simplest strategy for generating such sequences is by producing random orders. This is indeed the strategy we have used in the experiments described in this paper. One potential problem with this approach is with domains where good orders (or bad orders) are rare. In such a case, a random generator will not necessarily produce sequences with the desired diversity in quality.\n2. We use the SMV (McMillan, 1993) system for this purpose.\nAn alternative approach is to actively try producing orders that are very good and orders that are very bad, therefore creating a large diversity in quality. One way of producing a good order is by taking the orders that are the result of the dynamic ordering process.\nAnother option is by using an existing static ordering algorithm. One interesting idea is to try and bootstrap the process by using the results of the adaptive ordering algorithm as training examples thus resulting in progressively more diverse input."
    }, {
      "heading" : "3.3 The Example Extractor",
      "text" : "Given a set of n variables, we can extract n ∗ (n − 1) example ordered pairs for training. But should we actually use all these ordered pairs as examples?\nThere are two main reasons for being selective about what examples to use:\n1. Each example carries computational costs associated with tagging, feature extraction, and the added computation by the induction procedure.\n2. Noisy examples are known to have harmful effect on the induction process.\nThe process of selecting a subset of examples, to be tagged, out of a set of untagged examples is called selective sampling. There are two common ways of performing selective sampling. One is by automatic methods that use various general metrics for selecting informative examples (Lindenbaum, Markovitch, & Rusakov, 1999). The other way is by using domain specific heuristics about the potential of an example to be informative.\nIn this work we use the second approach. Consider a function f over m variables, represented within a BDD of n variables (where m ≤ n). The number of nodes used to represent f depends only on the relative order of the m variables. This means that changing the order of the other n −m variables would not influence the BDD representation of the function f .\nThe BDD representation of a model to be checked consists of the initial states of the model and the next-state functions of the variables. Since the BDD representation for the initial states is typically small, we do not take it into account. Therefore, when looking for examples, we consider only the next-state functions. Usually, each such function is defined only over a subset of all the model variables. Thus, the order of a pair of variables (vi, vj), that do not appear together in any next-state function is less likely to affect the quality of the generated order. We therefore filter out such pairs."
    }, {
      "heading" : "3.4 The Example Tagger",
      "text" : "An ordered variable pair (vi, vj) should be tagged as belonging to C+ if it is preferable to place vi before vj. Let V = {v1, . . . , vn} be the set of variables of a given model. Let O be the set of all possible orderings over V . Let Ovi≺vj be the set of all o ∈ O where vi precedes vj . The ordered variable pair (vi, vj) is defined to be preferable to (vj , vi) if and only if\nAverage{u(o)|o ∈ Ovj≺vi} ≤ Average{u(o)|o ∈ Ovi≺vj}.\nSince it is not feasible to evaluate all the possible orders, we sample the space of possible orders, evaluate them and partition the samples to two sets as above. As the averages now only estimate the real averages, we replace the term “smaller” in the above definition with “significantly smaller.” This is determined by the unpaired t-test, which tests the significance (with a given confidence) of the difference between the averages of two samples of two populations.\nMore precisely, for each variable pair vi, vj , the set of sampled orders S ⊆ O is partitioned into two subsets Svi≺vj ⊆ Ovi≺vj and Svj≺vi ⊆ Ovj≺vi . An unpaired t-test with a predetermined confidence level is used to check if the averages of the set utilities differ significantly. If they do, the ordered pair corresponding to the set with the smaller average\nis tagged with + and the other ordered pair is tagged with - (meaning that they belong to C+ and C−, respectively). Otherwise, the average difference is not significant, and both ordered pairs are tagged with ? (meaning that they belong to C?).\nA more elaborative approach could use the t-value as a weight on how important a particular order is. These weights could solve conflicts in the ordering process. Such a scheme would require, however, a method to incorporate weights into the induction algorithm. One method is by trying to induce a continues function instead of a ternary function."
    }, {
      "heading" : "3.5 The Feature Extractor",
      "text" : "If we want to generalize from training models to future unseen models, we cannot represent the pairs by the variable names. Rather, we should use a representation that can be used across models. Most induction algorithms require that the examples be represented by feature vectors.\nThe process of constructing an appropriate feature set is a crucial part of applying a learning algorithm to a problem. It is a common knowledge engineering process where a domain expert comes up with a set of features that might be relevant. It is the role of the induction algorithm, then, to find out what combination of features are relevant to the specific problem.\nWe have come up with a set of features over variable pairs. These features are extracted from the model connectivity graph. Some of these attributes are inspired by traditional static ordering algorithms. The attributes can be categorized into three groups:\n• Variable attributes are defined on a single variable and try to capture its characteristics in the model. One example is the variable-dependence attribute, which equals the number of variables on which a variable depends. This attribute was inspired by the value used by Butler et al. (1991) to guide the DFS search. A higher value indicates that a larger portion of the model’s variables are needed to determine the variable’s next-state value. Thus, a higher value may indicate that the variable location should be lower in the order. Another example is the variable-dependency, which takes the complementary view of variable-dependence. The attribute equals the number of variables that depend on a given variable. A higher value may indicate that the variable should be placed higher in the variable order.\n• Symmetric pair attributes are defined on a variable pair vi, vj . These attributes try to capture the strength of the bond between the two variables, as well as that between this pair and the other variables in the model. For example, pair-minimal-distance measures the shortest path between the variables in the model connectivity graph. A shorter path can indicate a stronger bond between the variables. The distance-based ordering framework (Chung et al., 1993) uses a similar feature to order variables. Another example is pair-mutual-dependency, which counts the number of variables whose next-state function depends on both vi and vj.\n• Non-symmetric pair attributes try to capture the relationship between the two variables. For example, the pair-dependency-ratio is the ratio between the variabledependency values of the two variables. If the ratio is relatively high or low, it may indicate the relative order of the two. pair-ns-distance evaluates the influence of one\nvariable on the next state value of the other. It does so by measuring the distance between the variables in the subgraph that represents the next-state function.\nThe complete list of attributes can be found in Appendix A."
    }, {
      "heading" : "3.6 The Induction Algorithm",
      "text" : "After the feature extraction phase, our data is represented as a set of tagged feature vectors. This type of representation can be used to produce classifiers by many induction algorithms, including decision trees (Hunt, Marin, & Stone, 1966; Friedman, 1977; Quinlan, 1979; Breiman, Frieman, Olshen, & Stone, 1984), neural networks (Widrow & Hoff, 1960; Parker, 1985; Rumelhart & McClelland, 1986) and nearest neighbor (Cover & Hart, 1967; Duda & Hart, 1973). We have decided to use decision tree classifiers because of their relatively fast learning and fast classification. Fast classification is especially important since we wish to be competitive with other ordering algorithms and the number of variable pairs we need to classify is large.\nDecision trees have been researched thoroughly in the last decade, producing many valuable extensions. One such extension enables the decision tree to give not only the classification of items but also to associate with each such classification a confidence estimation. We have used this variant to allow conflict resolution. This will be described in Section 3.7.3."
    }, {
      "heading" : "3.7 The Ordering Algorithm",
      "text" : "The outcome of the learning process described in the last four subsections is a set of decision trees, one for each training model.\nWe could also generate one tree based on the union of generated samples. One advantage of the multiple-tree approach is that we expect the examples from the same model to be more consistent, allowing generating compact trees. In contrast, a set of examples coming from different models is likely to be more noisy, yielding a large tree. In addition, the multiple-tree version allows us using a voting scheme during the ordering process, as described below.\nGiven a model M, the algorithm first extracts the interacting variable pairs. Each of the classifiers is then applied to the feature vector representations of these pairs. For each classifier, the classifications of all the pairs are gathered to form a precedence table. These tables are then merged into one table. The order creation algorithm uses the merged precedence table to construct the model’s variable order. The following subsections describe the components in greater detail. Figure 6 shows the data flow in the ordering algorithm."
    }, {
      "heading" : "3.7.1 Building the Precedence Table",
      "text" : "To build a precedence table based on a given classifier, the algorithm asks two questions for each interacting variable pair vi, vj :\n1. Should vi ≺ vj ?\n2. Should vj ≺ vi ?\nIf the two agree, the pair order is set to the agreed order. If they disagree, the order is set to unknown. Table 2 summarizes all the possible answers for the two questions and the resulting pair order."
    }, {
      "heading" : "3.7.2 The Merging Algorithm",
      "text" : "After constructing the pair precedence tables from the training model’s classifiers, we merge the tables using a voting scheme. For each variable pair vi, vj , we count the number of tables that vote vi ≺ vj and the number of tables that vote vj ≺ vi. We then decide their pair order according to the majority (ignoring the unknown votes).\nAssuming that the majority vote chooses the order vi ≺ vj, the confidence for this vote\nis computed by conf(vi≺vj)−conf(vj≺vi) vote(vi≺vj)+vote(vj≺vi) , where vote(vi ≺ vj) is the number of tables that vote\nvi ≺ vj and conf(vi ≺ vj) is the sum of the confidence values of these votes. vote(vj ≺ vi) and conf(vj ≺ vi) are defined similarly. If this value turns out to be lower than 0.1, we set it to a minimal value of 0.1."
    }, {
      "heading" : "3.7.3 Cycle Resolution",
      "text" : "In order to build a total, strict order out of the merged table, the table must not contain any cycle. However, the above algorithm does not guarantee this. We therefore have to apply a cycle resolution algorithm that makes the table cycle-free.\nThe precedence table can be seen as a directed graph in which the nodes are variables, and there is a weighted edge vi → vj if and only if vi ≺ vj . There are many possible ways to eliminate cycles in a directed graph. One reasonable bias is removing the least number of edges. This problem is known as the minimum feedback arc set and is proven to be NP-hard (Karp, 1972). Approximation algorithms for this problem exist (Even, Naor, Schieber, & Sudan, 1998), but they are too costly for our purposes.\nWe use instead a simple greedy algorithm to solve the problem. All the constraints (edges) are gathered into a list and sorted in a decreasing order according to their weights (i.e., their confidence). A graph is initialized to hold only the variable vertices. The list of edges is then traversed and each edge is added if it does not close a cycle."
    }, {
      "heading" : "3.7.4 Pair Precedence Ordering",
      "text" : "At this stage of the algorithm, we hold an acyclic merged precedence table. The last step of the ordering process is to convert the partial order represented by this table to a total order. This is done by topological ordering. At each stage, the algorithm finds all the minimal variables, i.e., variables that are not constrained to follow other unordered variable. From this set, we select a variable vadd with maximal fan-out and add it after the last ordered variable. We then add all the variables which are larger than vadd but do not appear in any constraint with an unordered variable. We do this because it is desirable to place interacting variables near each other. The pair precedence ordering (PPO) algorithm is listed in Figure 7. Figure 8 lists the selection of vadd in PPO .\nOne possible change to the ordering process is to delay the cycle resolution to the last stage. We call this version cycle resolution on demand. The modified algorithm does not perform any cycle resolution on the merged table. Instead, the algorithm works with the merged table that may contain cycles. If the table contains a cycle, the algorithm must reach a stage where not all the variables are ordered and there are no minimal variables. In this case the algorithm performs cycle resolution as before and continues the ordering process."
    }, {
      "heading" : "3.8 Experiments",
      "text" : "We performed an empirical evaluation of the PPO algorithm using models from the ISCAS89 (Brglez, Bryan, & Kozminski, 1989) benchmark. The ISCAS89 benchmark circuits have been used to empirically evaluate many algorithms that deal with various aspects of circuit design (Chamberlain, 1995; Wahba & Borrione, 1995; Nakamura, Takagi, Kimura, & Watanabe, 1998; Long, Iyer, & Abramovici, 1995; Iyer & Abramovici, 1996; Konuk & Larrabee, 1993). We discovered that some of the circuits are insensitive to the initial\nordering. This means that the entire sample of initial orders yielded model BDDs of similar sizes. We eliminated these circuits from the set. Out of the remaining circuits we selected those with a number of variables that SMV can handle. We ended up with the following five circuits: s1269 (55), s1423 (91), s1512(86), s4863 (153), s6669 (314). The numbers in parentheses stand for the number of variables in each model.\nWe began with an offline learning session where the three smaller models (s1269, s1423, s1512) are used as training models. For each of these models we generated 200 random orders and extracted examples as described in the previous section. The algorithm then induced three precedence classifiers in the form of decision trees.\nThe number 200 was selected since it proved to be sufficiently large. In real application the algorithm can be used as an anytime algorithm where training sequences are generated as long as the user is willing to wait for the offline learner. An alternative approach would keep aside a validation set that would be used for testing the system’s performance. The training could have then be stopped when the learning curve flattens.\nThe algorithm was tested on the two larger models (s4863, s6669). For each of the models, the three learned decision trees were used to generate the merged precedence table.\nOur PPO algorithm (with cycle resolution on demand) was compared to the random algorithm. In addition, we compared our results to two advanced graph search algorithms for static ordering: the DFS append algorithm of Fujita et al. (1988) and the interleave algorithm of Fujii et al. (1993). In both algorithms we used the adaptation for multiple starting points (Butler et al., 1991) and its expanded version, which includes the tie breaking rule (Fujita et al., 1993). The random results were taken based on 200 variable orders. The two other algorithms were each run 10 times on every model. The performance of the ordering algorithms is measured by the number of nodes in the model BDDs (partitioned transition relation and initial states).\nTable 3 and Figure 9 show the results obtained. The table shows that on model s6669, PPO outperformed the random order by more than 300%. On model s4863, PPO outperformed the random order by 5%.\nThe comparison of our algorithm to the two static algorithms is given in Figure 10. The results show that our learning algorithm, after training, becomes competitive with the existing ordering algorithms written by experts.\nTo evaluate the utility of the learned knowledge we would like to compare the performance of the ordering process with and without the learned knowledge. Ordering without any learned knowledge is equivalent to random ordering. The comparison of our results to the random-ordering algorithm reveals that the learner indeed induced meaningful knowledge during the learning process. Our method is also much more stable than random ordering on s6669 as indicated by comparing the standard deviation. This large variance in the results of the random ordering is indeed exploited by our tagging procedure as explained in Section 3.4. The small variance in the results obtained by random ordering on s4863 can explain why the improvement obtained by the PPO algorithm is much smaller on this circuit. A more sophisticated training sequence generator, such as those described in Section 3.2, might have been more successful with that circuit.\nThe comparison to the hand-crafted algorithms may look disappointing at first look since the results of the learning system are not better than the existing algorithms. Recall, however, that we are comparing an automated learning process to human expertise. Most of the works in empirical machine learning make comparisons between the performance of various learning algorithms. It is not common to compare the performance of a learning\nalgorithm with a human expert or an expert system since in most cases it is clear that handcrafted algorithms would outperform automated learning processes. Since there are hardly any other learning systems that were built to solve the BDD variable ordering problem we could not make the more common comparison between learning systems."
    }, {
      "heading" : "4. Learning Context-Based Precedence for Static Ordering",
      "text" : "The precedence relation is one of the key considerations used by traditional static ordering algorithms. Another key consideration is the clustering of variables and their subsequent ordering. The algorithms try to place highly interacting variables near each other.\nThe effect of the variable clustering in a BDD can be seen in the simple example given in Figure 3. In this function, switching the two variables v2 and v3 increases the BDD size by 3 nodes. For this function, all the orders in which the variables of each of the two clusters, v1, v3 and v2,, v4, are kept together yield the minimal BDD representation. Other variable orders yield a less compact BDD. Thus, in this function, the only key consideration is the compliance with clustering (precedence is not taken into account)."
    }, {
      "heading" : "4.1 Variable Distance",
      "text" : "The above discussion leads to the hypothesis that the distance between variables is an important factor when considering alternative orders. One way to obtain distance information is by learning the distance function between pairs of variables. There are, however, two problems with this approach:\n1. The target distance function is not well-defined across models. For example, if we train on small models, the absolute distance function is not likely to be applicable for large models.\n2. Information on absolute distances between variables is not sufficient to construct a good ordering. This is because the absolute distance does not uniquely define the order between the variables. In fact, it defines two possible orders, where one is the reverse of the other.\nThe example in Figure 11 demonstrates that an order and its reverse can yield BDDs that are significantly different in size. Each of the BDDs in Figure 11 represents two functions, f1(a, b, c, d, e) = (a = b = c) ∨ (c = d) and f2(a, b, c, d, e) = (a = b = c) ∨ (c = e). The absolute distance between the variables in the orders is clearly the same. However, the upper BDD is approximately double the size of the lower one.\nWe wanted to check whether in realistic examples reverse orders can yield BDDs that are significantly different in size. We tested models from the ISCAS89 benchmarks and created 5,000 variable orders for each model. For each order, we compared its quality with the quality of the reversed order. We found that in many cases one order was exceptionally good while the reversed one was exceptionally bad. Thus, learning the absolute distance is not sufficient, and more information is needed.\nWe conclude that there are problems inherent both in learning and in utilizing absolute distances. Still, clustering is a key consideration and should be pursued. We suggest,\nalternatively, learning the relative distance that determines, for variables vi, vj , and vk, which of vj, vk should be closer to vi, given that vi precedes the other two.\nThe remainder of this section describes a method for learning and utilizing context-based precedence to infer the relative distance between variables."
    }, {
      "heading" : "4.2 Context-Based Precedence",
      "text" : "A context precedence relation is a triplet vi ≺ vj ≺ vk: given that vi precedes vj and vk, the variable vj should come before the variable vk. Thus, the context precedence relation adds context to pair ordering decisions.\nAs in pair precedence learning, we define the universe to be the set of pairs 〈(vi, vj , vk),M〉 where vi, vj , vk are variables in the model M . The universe is divided into three classes, C+, C−, C?, as before. Examples for these classes are drawn in the same way. The pair precedence framework can be applied with minor changes to work with context precedence relations. These minor changes are described below."
    }, {
      "heading" : "4.3 The Example Tagger",
      "text" : "A variable triplet (vi, vj , vk) should be tagged as C+ if, given that vi precedes vj and vk, it is preferable to place vj before vk (i.e., vi ≺ vj ≺ vk). As in pair precedence learning, we use a set of evaluated variable orders for the tagging. Any set of such orders can be partitioned to three subsets, depending on which of the three variables is first. Given a partition defined by vi (for example), we can test the order of vj and vk using t-test, as described in Section\n3.4. To reduce the number of noisy examples, we use only the partition that yields the most significant t-test results."
    }, {
      "heading" : "4.4 The Feature Extractor",
      "text" : "The attributes of a triplet (vi, vj , vk) are computed based on the attributes of the two pairs vi ≺ vj and vi ≺ vk. Each attribute value is the division/subtraction of two corresponding attribute values from the two pair attributes.\nMore precisely, assume that the pair vi ≺ vj has attributes f1(vi, vj), . . . , fn(vi, vj) and the pair vi ≺ vk has attributes f1(vi, vk), . . . , fn(vi, vk). Then the triple (vi, vj , vk) has attributes f1(vi, vj)/f1(vi, vk), . . . , fn(vi, vj)/fn(vi, vk). If some of the fl(vi, vk) can be 0 then the corresponding attributes are subtracted instead of divided.\nAs an example consider an attribute fl which is pair minimal distance (see Section 3.5). If fl(vi, vj)/fl(vi, vk) is greater than 1 than the shortest path between vi and vj is larger than the shortest path between vi and vk. This attribute can indicate that vk should appear closer to vi.\nSimilarly, if fl is pair mutual dependency then fl(vi, vj)/fl(vi, vk) > 1 indicates that the number of variables whose next-state function depends on both vi and vj is greater than those depending on both vi and vk. This may indicate that it is preferable to keep vi and vj close together."
    }, {
      "heading" : "4.5 The Ordering Algorithm",
      "text" : "The outcome of the learning phase is a set of decision trees, one for each model. This is the same as in the case of context-free pairs. In this subsection we describe ways to use these trees for ordering."
    }, {
      "heading" : "4.5.1 Building the Context Precedence Table",
      "text" : "While in the case of pair precedence we had a table of size n2 (where n is the number of variables), we now produce one such table for each context variable. For each table we perform inconsistency elimination similar to that described in Section 3.7.1. Here, however, when we ask the classifier the two questions vj , vk and vk, vj , we add the context variable vi to the query."
    }, {
      "heading" : "4.5.2 Pair Precedence Ordering with Context Precedence Filtering",
      "text" : "The ordering algorithm uses the pair precedence table in the same way as the PPO algorithm. However, it was often found to be the case that the PPO algorithm had several minimal variables, even after employing the maximal fanout filter. We use the contextbased precedence table to further reduce the size of the set of minimal elements. We use the variables in the already ordered sequence as context variables and look at their associated tables. If the set of minimal elements contains a pair of variables constrained as vj ≺ vk in one of the tables, we eliminate vk from the set. Figure 12 lists the code which when added to the PPO algorithm, accepts a variable set Vadd (from which we previously selected randomly), and returns one variable. We call the new algorithm PPOCPF . Figure 13 lists the selection of vadd in PPO CPF ."
    }, {
      "heading" : "4.6 Experiments",
      "text" : "We have evaluated the performance of PPOCPF , performing off-line learning on the training models followed by ordering of the test models. The results are shown in Figure 14. For comparison we also show the performance of the PPO algorithm and the two expert algorithms.\nThe PPOCPF algorithm outperforms all the other algorithms on the two tested models. The results show that the context-based precedence relations add valuable information.\nWe have tested the effect of the resources invested in the learning phase on the performance of the algorithms. Since the learning examples are tagged based on evaluated training orders, and since the evaluation of the training orders is the most resource-consuming operation, we used the number of these orders as the resource estimator. Figure 15 shows the learning curves of our algorithms, that is, it shows how the system performance changes according to the offline resources consumed (the number of training orders evaluated).\nWithout testing any random order, our system has no knowledge on which to build the precedence classifiers, and thus its performance is equivalent to random ordering. The\ntagging based on 20 orders is too noisy. While it improves the performance of s6669, it degrades the performance of s4863. Forty orders are sufficient to generate stable tagging, which yields improved classifiers and therefore improved ordering quality."
    }, {
      "heading" : "5. Discussion",
      "text" : "The work described in this paper presents a general framework for using machine learning methods to solve the static variable ordering problem. Our method assumes the availability\nof training models. For each training model, the learning algorithm generates a set of random orders and evaluates them by building their associated BDDs. Each ordered pair of interacting variables is then tagged as a good example if it appears more frequently in highly valued orders. The ordered pairs are converted to feature-based representations and are then given, with their associated tags, to an induction algorithm. When ordering variables of a new unseen model, the resulting classifiers (one for each model) are used to determine the ordering of variable pairs. We also present an extension of this method that learns context-based ordering.\nOur algorithm was empirically tested on real models. Its performance was significantly better than random ordering, meaning that the algorithm was able to acquire useful ordering knowledge. Our results were slightly better than existing static ordering algorithms handcrafted by experts. This result is significant if we compare it to applications of learning systems to other domains. We would surely appreciate an induction algorithm that produces a classifier with performance comparable to that of an expert system built by a medical expert. A chess learning program that is able to learn an evaluation function that is equivalent in power to a function produced by an expert will be similarly appreciated. We therefore claim that the ability of or learning algorithm to achieve results that are as good as manually designed algorithms indicates strong learning capabilities.\nIn most learning algorithms, we expect to get better performance when the testing problems are similar to the training problems. In the verification domain, we expect to get good results when the testing and training models come from a family of similar models. There are several occasions in which models are similar enough to be considered a family: Models of different versions of a design under development; models which are reduced versions of a design, each with respect to a different property; models of designs with a similar functionality like ALUs, arbiters, pipelines and bus controllers. Unfortunately, due to the difficulty in obtaining suitable real models for our experiments, we ended up experimenting with training and testing models that are not related. We expect to achieve much better results for related models.\nCompared with previous work in machine learning, our precedence relations most resemble these of Utgoff and Saxena (1987). Our ordering approach, in which we construct a total order of elements by finding the precedence relation between them, is in essence the same as that of Cohen, Schapire and Singer (1999). Specifically, the second ordering algorithm of Cohen, Schapire and Singer also uses the topological ordering approach to create an order. Their algorithm initially finds in the precedence graph the connected components and, after ordering them (using topological ordering), finds the order in each connected component. However, since the quality of the final order is determined by the sum of constraints adhered to, all topological orders have theoretically the same quality. We found that in the BDD variable ordering problem not all topological orders have the same quality. Thus, we developed a topological ordering that takes into consideration those features that we recognized as true for variable orders in BDDs.\nOur work also differs from previous research in that it introduces the notion of contextbased precedence. Using this concept we were able to create an ordering algorithm that produces the best results.\nThere are several directions for extending the work described here. One problem with our current empirical evaluation is the small number of models. In spite of our extensive\nsearch efforts we were not able to find a large set of suitable examples. The majority of the known examples are very simple (compared with real industry problems), producing small model BDD representations with very little variance. We are currently in the process of approaching companies that use model checking. In this way we hope to obtain additional real models, preferably from families of the designs described above.\nThe attributes of the variable pairs were partially based on substantive research in the field of static algorithms. We could not find such information on which to base contextbased variable attributes. Thus, we also based these attributes on those of the variable pairs. Nevertheless, we believe that human experts in this field may have information that can lead to the development of better attributes. The development of such attributes should help to capture in a better way the context-based precedence concept.\nGiven our current results, an immediate question is whether the concept of precedence pairs (context and non-context) can be extended to triplets, quadruples, etc. Such precedence relations take into account a larger part of the model and thus may possess valuable information. Such an extension, however, could carry high cost during learning and, even worse, during ordering.\nOur framework for solving the static variable ordering problem was shown to be valuable in model checking. Model checking is only one field of verification in which BDDs are used. BDDs are also used in verification for simulation and equivalence checking. Our algorithm can be applied for these problems as well. We are unaware of special static variable ordering algorithms for these fields, but if such do exist, variable attributes based on these algorithms should be added.\nThe most interesting future direction is the generalization of our framework for other ordering problems. Ordering a set of objects is a very common sub-task in problem solving. The most common approach for tackling such a problem is to evaluate each object using a utility function and order the objects according to their utilities. Such an approach is taken, for example, by most heuristic search algorithms. In many problems, however, it is much easier to determine the relative order of two objects than to give each object a global utility value. Few works have applied learning to ordering techniques that are not utility based (Cohen et al., 1999). The algorithms described in Section 3 and Section 4 can be applied to any ordering problem if a method for evaluating training orders is available, and a set of meaningful pair features can be defined.\nWe believe that the research presented in this paper contributes both to the field of machine learning and to the field of formal verification. For machine learning, it presents a new methodology for learning to order elements. This methodology can be applied to various kinds of ordering problems. For formal verification, it presents new learning-based techniques for variable ordering. Finding good variable ordering techniques is one of the key problems in this field."
    }, {
      "heading" : "Appendix A. Variable Pair Attributes",
      "text" : "The following definitions and symbols will be used in the attribute description:\n• NS(vi) for the next state function of variable vi\n• vi ⊲ vj to indicate that variable vi depends on variable vj’s value (vj ∈ NS(vi))\n• vi ⊲⊳ vj to indicate that variable vi interacts with variable vj (vi ⊲ vj and/or vj ⊲ vi)\n• # variables for the number of variables in the model\nA.1 Variable Attributes\nThe attributes computed for vi are\n1. Variable-dependence: the number of variables upon which vi depends (|{vj |vi ⊲ vj}|)\n2. Variable-dependency : the number of variables that depend on vi (|{vj |vj ⊲ vi}|)\n3. Variable-dependency-size: the sum of function sizes that depend on vi ( ∑\nvj⊲vi |{vk ∈ NS(vj)}|)\n4. Variable-dependency-average-size: the average function size dependent on vi ( ∑\nvj⊲vi |{vk∈NS(vj)}|\n|{vj |vj⊲vi}|\n)\n5. Variable-dependence-dependency-ratio: the proportion between the number of vari-\nables on which vi depends and the number of variables that depend on it\n(\n|{vj |vi⊲vj}| |{vj |vj⊲vi}|\n)\n6. Variable-interaction: the number of variables interacting with vi (|{vj |vi ⊲⊳ vj}|)\n7. Variable-dependence-percentage: the percentage of model variables on which vi de-\npends\n(\n|{vj |vi⊲vj}| #variables\n)\n8. Variable-dependency-percentage: the percentage of model variables that depend on vi (\n|{vj |vj⊲vi}| #variables\n)\n9. Variable-interaction-percentage: the percentage of model variables interacting with vi (\n|{vj |vi⊲⊳vj}| #variables\n)\nA.2 Variable Pair Attributes\nThe attributes computed for 〈vi, vj〉 are\n• Symmetric attributes\n1. Pair-minimal-distance: the minimal distance between vi,vj in the model graph\n2. Pair-minimal-distance-eval : the minimal distance between vi,vj in the model graph divided by the number of times it appears\n3. Pair-minimal-dependency : the number of variables that depend on the pair with the minimal distance\n4. Pair-minimal-dependency-eval : the minimal distance between vi,vj in the model graph divided by number of variables that depend on the minimal distance\n5. Pair-minimal-connection-class: the minimal distance between the vi,vj connection class (the operators that can be applied on two variables were divided into classes and the operator that connected the two variables in the minimal distance class was extracted)\n6. Pair-minimal-maximal : the maximal sized NS(vk) connecting the pair in minimal distance\n7. Pair-minimal-maximal-eval : the minimal distance between vi,vj in the model graph divided by maximal sized NS(vk) connecting the pair in minimal distance\n8. Pair-sum-distance: the sum of distances between vi,vj in the model graph\n9. Pair-dependency-ns-size: the sum of NS(vk) sizes that are dependent on vi and vj ( ∑\nvk⊲vi & vk⊲vj |vl ∈ NS(vk)|)\n10. Pair-sum-distance-dependency-ratio: the sum of distances between vi,vj in the model graph divided by sum of NS(vk) sizes that are dependent on vi and vj\n11. Pair-mutual-dependence: the number of variables on which both vi,vj depend (|{vk|vi ⊲ vk & vj ⊲ vk}|)\n12. Pair-mutual-dependency : the number of variables that depend on vi and vj (|{vk|vk ⊲ vi& vk ⊲ vj}|)\n13. Pair-mutual-interaction: the number of variables that interact with vi and vj (|{vk|vi ⊲⊳ vk & vi ⊲⊳ vk}|)\n14. Pair-mutual-ns-dependency : vi depends on vj and vj depends on vi - (vi ⊲ vj & vj ⊲ vi)\n• Non-Symmetric attributes ( those computed for the pair 〈vi, vj〉 with relevance to vi)\n1. Pair-ns-distance: the distance between vi,vj in NS(vi)\n2. Pair-dependence-ratio: the ratio between the number of variables that vi depends\non and the number of variables that vj depends on\n(\n|{vl|vi⊲vl}| |{vm|vj⊲vm}|\n)\n3. Pair-dependency-ratio: the ratio between the number of variables that depend\non vi and the number of variable that depend on vj\n(\n|{vl|vl⊲vi}| |{vm|vm⊲vj}|\n)\n4. Pair-interaction-ratio: the ratio between the number of variables that interact\nwith vi and the number of variables that interact with vj\n(\n|{vl|vi⊲⊳vl}| |{vm|vj⊲⊳vm}|\n)\n5. Pair-dependence-flag : the number of variables that vi depends on compared to\nthe number of variables that vj depends on\n(\n|{vl|vi⊲vl}| |{vm|vj⊲vm}| >= 1.0\n)\n6. Pair-interaction-flag : the number of variables that interact with vi compared to\nthe number of variables that vj interacts with\n(\n|{vl|vi⊲⊳vl}| |{vm|vj⊲⊳vm}| >= 1.0\n)"
    } ],
    "references" : [ {
      "title" : "Binary decision diagrams",
      "author" : [ "S. Akers" ],
      "venue" : "IEEE Transactions on Computers, C-27 (6), 509–516.",
      "citeRegEx" : "Akers,? 1978",
      "shortCiteRegEx" : "Akers",
      "year" : 1978
    }, {
      "title" : "BDD variable ordering for interacting finite state machines",
      "author" : [ "A. Aziz", "S. Tasiran", "R. Brayton" ],
      "venue" : "In Proceedings of the 31st Design Automation Conference (DAC),",
      "citeRegEx" : "Aziz et al\\.,? \\Q1994\\E",
      "shortCiteRegEx" : "Aziz et al\\.",
      "year" : 1994
    }, {
      "title" : "RuleBase: An industry-oriented formal verification tool",
      "author" : [ "I. Beer", "S. Ben-David", "C. Eisner", "A. Landver" ],
      "venue" : "In Proceedings of the 33rd Design Automation Conference (DAC),",
      "citeRegEx" : "Beer et al\\.,? \\Q1996\\E",
      "shortCiteRegEx" : "Beer et al\\.",
      "year" : 1996
    }, {
      "title" : "Efficient OBDD-based boolean manipulation in CAD beyond current limits",
      "author" : [ "J. Bern", "C. Meinel", "A. Slobodova" ],
      "venue" : "In Proceedings of the 32nd Design Automation Conference (DAC),",
      "citeRegEx" : "Bern et al\\.,? \\Q1995\\E",
      "shortCiteRegEx" : "Bern et al\\.",
      "year" : 1995
    }, {
      "title" : "Simulated annealing to improve variable orderings for OBDDs",
      "author" : [ "B. Bollig", "M. Lobbing", "I. Wegener" ],
      "venue" : "In Proceedings of the International Workshop on Logic Synthesis, pp. 5b:5.1–5.10,",
      "citeRegEx" : "Bollig et al\\.,? \\Q1995\\E",
      "shortCiteRegEx" : "Bollig et al\\.",
      "year" : 1995
    }, {
      "title" : "Improving the variable ordering of OBDDs is NP-complete",
      "author" : [ "B. Bollig", "I. Wegener" ],
      "venue" : "IEEE Transactions on Computers,",
      "citeRegEx" : "Bollig and Wegener,? \\Q1996\\E",
      "shortCiteRegEx" : "Bollig and Wegener",
      "year" : 1996
    }, {
      "title" : "Classification and Regression Trees",
      "author" : [ "L. Breiman", "J.H. Frieman", "R.A. Olshen", "C.J. Stone" ],
      "venue" : null,
      "citeRegEx" : "Breiman et al\\.,? \\Q1984\\E",
      "shortCiteRegEx" : "Breiman et al\\.",
      "year" : 1984
    }, {
      "title" : "Combinational profiles of sequential benchmark circuits",
      "author" : [ "F. Brglez", "D. Bryan", "K. Kozminski" ],
      "venue" : "In Proceedings of the International Symposium on Circuits and Systems,",
      "citeRegEx" : "Brglez et al\\.,? \\Q1989\\E",
      "shortCiteRegEx" : "Brglez et al\\.",
      "year" : 1989
    }, {
      "title" : "Compositional instance-based learning",
      "author" : [ "P. Broos", "K. Branting" ],
      "venue" : "In Proceedings of the 12th National Conference on Artificial Intelligence,",
      "citeRegEx" : "Broos and Branting,? \\Q1994\\E",
      "shortCiteRegEx" : "Broos and Branting",
      "year" : 1994
    }, {
      "title" : "Graph-based algorithms for boolean function manipulation",
      "author" : [ "R. Bryant" ],
      "venue" : "IEEE Transactions on Computers, C-35 (8), 677–691.",
      "citeRegEx" : "Bryant,? 1986",
      "shortCiteRegEx" : "Bryant",
      "year" : 1986
    }, {
      "title" : "Heuristics to compute variable orderings for efficient manipulation of ordered binary decision diagrams",
      "author" : [ "K.M. Butler", "D.E. Ross", "Rohit Kapur", "a. M.R. M" ],
      "venue" : "In Proceedings of the 28th Design Automation Conference (DAC),",
      "citeRegEx" : "Butler et al\\.,? \\Q1991\\E",
      "shortCiteRegEx" : "Butler et al\\.",
      "year" : 1991
    }, {
      "title" : "Parallel logic simulation of VLSI systems",
      "author" : [ "R. Chamberlain" ],
      "venue" : "Proceedings of the 32nd Design Automation Conference (DAC), pp. 139–143, San Francisco, California.",
      "citeRegEx" : "Chamberlain,? 1995",
      "shortCiteRegEx" : "Chamberlain",
      "year" : 1995
    }, {
      "title" : "Efficient variable ordering heuristics for shared ROBDD",
      "author" : [ "P. Chung", "I. Hajj", "J. Patel" ],
      "venue" : "In Proceedings of the International Symposium on Circuits and Systems,",
      "citeRegEx" : "Chung et al\\.,? \\Q1993\\E",
      "shortCiteRegEx" : "Chung et al\\.",
      "year" : 1993
    }, {
      "title" : "Automatic verification of finite state concurrent systems using temporal logic specifications",
      "author" : [ "E.M. Clarke", "F.A. Emerson", "A.P. Sistla" ],
      "venue" : "ACM Transactions on Programming Languages and Systems,",
      "citeRegEx" : "Clarke et al\\.,? \\Q1986\\E",
      "shortCiteRegEx" : "Clarke et al\\.",
      "year" : 1986
    }, {
      "title" : "Learning to order things",
      "author" : [ "W.W. Cohen", "R.E. Schapire", "Y. Singer" ],
      "venue" : "Journal of Artificial Intelligence Research,",
      "citeRegEx" : "Cohen et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Cohen et al\\.",
      "year" : 1999
    }, {
      "title" : "Nearest neighbor pattern classification",
      "author" : [ "T.M. Cover", "P.E. Hart" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "Cover and Hart,? \\Q1967\\E",
      "shortCiteRegEx" : "Cover and Hart",
      "year" : 1967
    }, {
      "title" : "Genetic algorithm for variable ordering of OBDDs",
      "author" : [ "R. Drechsler", "B. Becker", "N. Gockel" ],
      "venue" : "IEEE Proceedings on Computers and Digital Techniques,",
      "citeRegEx" : "Drechsler et al\\.,? \\Q1996\\E",
      "shortCiteRegEx" : "Drechsler et al\\.",
      "year" : 1996
    }, {
      "title" : "Fast exact minimization of BDDs",
      "author" : [ "R. Drechsler", "N. Drechsler", "A. Slobodova" ],
      "venue" : "In Proceedings of the 35th Design Automation Conference (DAC),",
      "citeRegEx" : "Drechsler et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Drechsler et al\\.",
      "year" : 1998
    }, {
      "title" : "Pattern Classification and Scene Analysis",
      "author" : [ "R.O. Duda", "P.E. Hart" ],
      "venue" : null,
      "citeRegEx" : "Duda and Hart,? \\Q1973\\E",
      "shortCiteRegEx" : "Duda and Hart",
      "year" : 1973
    }, {
      "title" : "Approximating minimum feedback sets and multi-cuts in directed graphs",
      "author" : [ "G. Even", "J. Naor", "B. Schieber", "M. Sudan" ],
      "venue" : null,
      "citeRegEx" : "Even et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Even et al\\.",
      "year" : 1998
    }, {
      "title" : "A recursive partitioning decision rule for nonparametric classification",
      "author" : [ "J. Friedman" ],
      "venue" : "IEEE Transactions on Computers, C-26 (4), 404–408.",
      "citeRegEx" : "Friedman,? 1977",
      "shortCiteRegEx" : "Friedman",
      "year" : 1977
    }, {
      "title" : "Finding the optimal variable ordering for binary decision diagrams",
      "author" : [ "S.J. Friedman", "K.J. Supowit" ],
      "venue" : "In Proceedings of the 24th Design Automation Conference (DAC),",
      "citeRegEx" : "Friedman and Supowit,? \\Q1987\\E",
      "shortCiteRegEx" : "Friedman and Supowit",
      "year" : 1987
    }, {
      "title" : "Interleaving based variable ordering methods for ordered binary decision diagrams",
      "author" : [ "H. Fujii", "G. Ootomo", "C. Hori" ],
      "venue" : "In Proceedings of the IEEE/ACM international conference on Computer-aided design,",
      "citeRegEx" : "Fujii et al\\.,? \\Q1993\\E",
      "shortCiteRegEx" : "Fujii et al\\.",
      "year" : 1993
    }, {
      "title" : "Evaluation and improvements of boolean comparison method based on binary decision diagrams",
      "author" : [ "M. Fujita", "H. Fujisawa", "N. Kawato" ],
      "venue" : "In Proceedings of the International Conference on Computer-Aided Design,",
      "citeRegEx" : "Fujita et al\\.,? \\Q1988\\E",
      "shortCiteRegEx" : "Fujita et al\\.",
      "year" : 1988
    }, {
      "title" : "Variable ordering algorithms for ordered binary decision diagrams and their evaluation",
      "author" : [ "M. Fujita", "H. Fujisawa", "Y. Matsunaga" ],
      "venue" : "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,",
      "citeRegEx" : "Fujita et al\\.,? \\Q1993\\E",
      "shortCiteRegEx" : "Fujita et al\\.",
      "year" : 1993
    }, {
      "title" : "BDD minimization by truth table permutations",
      "author" : [ "M. Fujita", "Y. Kukimoto", "R. Brayton" ],
      "venue" : "In Proceedings of the International Workshop on Logic Synthesis,",
      "citeRegEx" : "Fujita et al\\.,? \\Q1995\\E",
      "shortCiteRegEx" : "Fujita et al\\.",
      "year" : 1995
    }, {
      "title" : "Experiments in Induction",
      "author" : [ "E. Hunt", "J. Marin", "P. Stone" ],
      "venue" : null,
      "citeRegEx" : "Hunt et al\\.,? \\Q1966\\E",
      "shortCiteRegEx" : "Hunt et al\\.",
      "year" : 1966
    }, {
      "title" : "Minimization of binary decision diagrams based on exchanges of variables",
      "author" : [ "N. Ishiura", "H. Sawada", "S. Yajima" ],
      "venue" : "In Proceedings of the International Conference on Computer-Aided Design,",
      "citeRegEx" : "Ishiura et al\\.,? \\Q1991\\E",
      "shortCiteRegEx" : "Ishiura et al\\.",
      "year" : 1991
    }, {
      "title" : "FIRE: A fault-independent combinational redundancy identification algorithm",
      "author" : [ "M. Iyer", "M. Abramovici" ],
      "venue" : "IEEE Transactions on VLSI Systems,",
      "citeRegEx" : "Iyer and Abramovici,? \\Q1996\\E",
      "shortCiteRegEx" : "Iyer and Abramovici",
      "year" : 1996
    }, {
      "title" : "Sampling schemes for computing variable orderings",
      "author" : [ "J. Jain", "W. Adams", "M. Fujita" ],
      "venue" : "In Proceedings of the International Conference on Computer-Aided Design,",
      "citeRegEx" : "Jain et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Jain et al\\.",
      "year" : 1998
    }, {
      "title" : "Reducibility among combinatorial problems",
      "author" : [ "R.M. Karp" ],
      "venue" : "Miller, R., & Thatcher, J. (Eds.), Complexity of Computer Computations, pp. 85–103, New York. Plenum Press.",
      "citeRegEx" : "Karp,? 1972",
      "shortCiteRegEx" : "Karp",
      "year" : 1972
    }, {
      "title" : "Intertwined development and formal verification of a 60x bus model",
      "author" : [ "M. Kaufmann", "C. Pixley" ],
      "venue" : "In Proceedings of the International Conference on Computer Design: VLSI in Computers and Processors (ICCD’",
      "citeRegEx" : "Kaufmann and Pixley,? \\Q1997\\E",
      "shortCiteRegEx" : "Kaufmann and Pixley",
      "year" : 1997
    }, {
      "title" : "Explorations of sequential atpg using boolean satisfiability",
      "author" : [ "H. Konuk", "R. Larrabee" ],
      "venue" : "In Proceedings of the 11th IEEE VLSI Test Symposium,",
      "citeRegEx" : "Konuk and Larrabee,? \\Q1993\\E",
      "shortCiteRegEx" : "Konuk and Larrabee",
      "year" : 1993
    }, {
      "title" : "Selective sampling for nearest neighbor classifiers",
      "author" : [ "M. Lindenbaum", "S. Markovitch", "D. Rusakov" ],
      "venue" : "In Proceedings of the Sixteenth national confernce on Artificial Intelligence,",
      "citeRegEx" : "Lindenbaum et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Lindenbaum et al\\.",
      "year" : 1999
    }, {
      "title" : "Identifying sequentially untestable faults using illegal states",
      "author" : [ "D. Long", "M. Iyer", "M. Abramovici" ],
      "venue" : "In Proceedings of the 13th IEEE VLSI Test Symposium,",
      "citeRegEx" : "Long et al\\.,? \\Q1995\\E",
      "shortCiteRegEx" : "Long et al\\.",
      "year" : 1995
    }, {
      "title" : "Logic verification using binary decision diagrams in a logic synthesis environment",
      "author" : [ "S. Malik", "A. Wang", "R. Brayton", "A. Sangiovanni-Vincentelli" ],
      "venue" : "In Proceedings of the International Conference on Computer-Aided Design,",
      "citeRegEx" : "Malik et al\\.,? \\Q1988\\E",
      "shortCiteRegEx" : "Malik et al\\.",
      "year" : 1988
    }, {
      "title" : "Symbolic Model Checking: An Approach to the State Explosion Problem",
      "author" : [ "K. McMillan" ],
      "venue" : "Kluwer Academic Publisher.",
      "citeRegEx" : "McMillan,? 1993",
      "shortCiteRegEx" : "McMillan",
      "year" : 1993
    }, {
      "title" : "Speeding up variable ordering of OBDDs",
      "author" : [ "C. Meinel", "A. Slobodova" ],
      "venue" : "In Proceedings of the International Conference on Computer-Aided Design,",
      "citeRegEx" : "Meinel and Slobodova,? \\Q1997\\E",
      "shortCiteRegEx" : "Meinel and Slobodova",
      "year" : 1997
    }, {
      "title" : "Sample method for minimization of OBDDs",
      "author" : [ "C. Meinel", "A. Slobodova" ],
      "venue" : "In Proceedings of the Conference on Current Trends in Theory and Practice of Informatics,",
      "citeRegEx" : "Meinel and Slobodova,? \\Q1998\\E",
      "shortCiteRegEx" : "Meinel and Slobodova",
      "year" : 1998
    }, {
      "title" : "Linear sifting of decison diagrams",
      "author" : [ "C. Meinel", "F. Somenzi", "T. Theobald" ],
      "venue" : "In Proceedings of the 34th Design Automation Conference (DAC),",
      "citeRegEx" : "Meinel et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Meinel et al\\.",
      "year" : 1997
    }, {
      "title" : "Functional approaches to generating orderings for efficient symbolic representations",
      "author" : [ "M.R. Mercer", "R. Kapur", "D.E. Ross" ],
      "venue" : "In Proceedings of the 29th Design Automation Conference (DAC)",
      "citeRegEx" : "Mercer et al\\.,? \\Q1992\\E",
      "shortCiteRegEx" : "Mercer et al\\.",
      "year" : 1992
    }, {
      "title" : "Shared binary decision diagrams with attributed edges for efficient boolean function manipulation",
      "author" : [ "S. Minato", "N. Ishiura", "S. Yajima" ],
      "venue" : "In Proceedings of the 27th Design Automation Conference (DAC),",
      "citeRegEx" : "Minato et al\\.,? \\Q1990\\E",
      "shortCiteRegEx" : "Minato et al\\.",
      "year" : 1990
    }, {
      "title" : "Waiting false path analysis of sequential logic circuits for performance optimization",
      "author" : [ "K. Nakamura", "K. Takagi", "S. Kimura", "K. Watanabe" ],
      "venue" : "In Proceedings of the International Conference on Computer-Aided Design,",
      "citeRegEx" : "Nakamura et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Nakamura et al\\.",
      "year" : 1998
    }, {
      "title" : "Who are the variables in your neighbourhood",
      "author" : [ "S. Panda", "F. Somenzi" ],
      "venue" : "In Proceedings of the International Conference on Computer-Aided Design,",
      "citeRegEx" : "Panda and Somenzi,? \\Q1995\\E",
      "shortCiteRegEx" : "Panda and Somenzi",
      "year" : 1995
    }, {
      "title" : "Symmetry detection and dynamic variable ordering of decision diagrams",
      "author" : [ "S. Panda", "F. Somenzi", "B.F. Plessier" ],
      "venue" : "In Proceedings of the International Conference on Computer-Aided Design,",
      "citeRegEx" : "Panda et al\\.,? \\Q1994\\E",
      "shortCiteRegEx" : "Panda et al\\.",
      "year" : 1994
    }, {
      "title" : "Learning logic",
      "author" : [ "D.B. Parker" ],
      "venue" : "Tech. rep. TR-47, Center for Computational Research in Economics and Management Science, MIT, Cambridge, MA.",
      "citeRegEx" : "Parker,? 1985",
      "shortCiteRegEx" : "Parker",
      "year" : 1985
    }, {
      "title" : "Specification and verification of concurrent systems in cesar",
      "author" : [ "J. Queille", "J. Sifakis" ],
      "venue" : "Proceedings of the 5th International Symposium on Programming,",
      "citeRegEx" : "Queille and Sifakis,? \\Q1981\\E",
      "shortCiteRegEx" : "Queille and Sifakis",
      "year" : 1981
    }, {
      "title" : "Discovering rules by induction from large collections of examples",
      "author" : [ "J.R. Quinlan" ],
      "venue" : "Expert Systems in the Micro Electronic Age, pp. 168–201. Edinburgh University Press.",
      "citeRegEx" : "Quinlan,? 1979",
      "shortCiteRegEx" : "Quinlan",
      "year" : 1979
    }, {
      "title" : "Induction of decision trees",
      "author" : [ "J.R. Quinlan" ],
      "venue" : "Machine Learning, 1 (1), 81–106.",
      "citeRegEx" : "Quinlan,? 1986",
      "shortCiteRegEx" : "Quinlan",
      "year" : 1986
    }, {
      "title" : "Dynamic variable ordering for ordered binary decision diagrams",
      "author" : [ "R. Rudell" ],
      "venue" : "Proceedings of the International Conference on Computer-Aided Design, pp. 42–47, Santa Clara, California.",
      "citeRegEx" : "Rudell,? 1993",
      "shortCiteRegEx" : "Rudell",
      "year" : 1993
    }, {
      "title" : "Parallel distibuted processing: Exploration in the microstructure of cognition",
      "author" : [ "D.E. Rumelhart", "J.L. McClelland" ],
      "venue" : null,
      "citeRegEx" : "Rumelhart and McClelland,? \\Q1986\\E",
      "shortCiteRegEx" : "Rumelhart and McClelland",
      "year" : 1986
    }, {
      "title" : "Implicit state enumeration of finite state machines using BDDs",
      "author" : [ "H. Touati", "H. Savoj", "B. Lin", "R. Brayton", "A. Sangiovanni-Vincetelli" ],
      "venue" : "In Proceedings of the International Conference on Computer-Aided Design,",
      "citeRegEx" : "Touati et al\\.,? \\Q1990\\E",
      "shortCiteRegEx" : "Touati et al\\.",
      "year" : 1990
    }, {
      "title" : "Two kinds of training information for evaluation function learning",
      "author" : [ "P. Utgoff", "J. Clouse" ],
      "venue" : "In Proceedings of the Ninth National Conference on Artificial Intelligence,",
      "citeRegEx" : "Utgoff and Clouse,? \\Q1991\\E",
      "shortCiteRegEx" : "Utgoff and Clouse",
      "year" : 1991
    }, {
      "title" : "Learning a preference predicate",
      "author" : [ "P.E. Utgoff", "S. Saxena" ],
      "venue" : "In Proceedings of the Fourth International Workshop on Machine Learning,",
      "citeRegEx" : "Utgoff and Saxena,? \\Q1987\\E",
      "shortCiteRegEx" : "Utgoff and Saxena",
      "year" : 1987
    }, {
      "title" : "Design error diagnosis in sequential circuits",
      "author" : [ "A. Wahba", "D. Borrione" ],
      "venue" : "Lecture Notes in Computer Science,",
      "citeRegEx" : "Wahba and Borrione,? \\Q1995\\E",
      "shortCiteRegEx" : "Wahba and Borrione",
      "year" : 1995
    }, {
      "title" : "Adaptive switching circuits",
      "author" : [ "B. Widrow", "M.E. Hoff" ],
      "venue" : "IRE WESCON Convention Record,",
      "citeRegEx" : "Widrow and Hoff,? \\Q1960\\E",
      "shortCiteRegEx" : "Widrow and Hoff",
      "year" : 1960
    }, {
      "title" : "Improved variable ordering of BDDs with novel genetic algorithm",
      "author" : [ "N. Zhuang", "M. Benten", "P. Cheung" ],
      "venue" : "In Proceedings of the International Symposium on Circuits and Systems.,",
      "citeRegEx" : "Zhuang et al\\.,? \\Q1996\\E",
      "shortCiteRegEx" : "Zhuang et al\\.",
      "year" : 1996
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "One of the most promising solutions to this problem is the use of binary decision diagrams (BDDs) (Akers, 1978; Bryant, 1986) as the basic data structure in model checking.",
      "startOffset" : 98,
      "endOffset" : 125
    }, {
      "referenceID" : 9,
      "context" : "One of the most promising solutions to this problem is the use of binary decision diagrams (BDDs) (Akers, 1978; Bryant, 1986) as the basic data structure in model checking.",
      "startOffset" : 98,
      "endOffset" : 125
    }, {
      "referenceID" : 36,
      "context" : "Our algorithm was integrated with SMV (McMillan, 1993), which is the backbone of many verification systems.",
      "startOffset" : 38,
      "endOffset" : 54
    }, {
      "referenceID" : 44,
      "context" : "Model checking was introduced by Clarke and Emerson (1986) and by Queille and Sifakis (1981) in the early 1980s.",
      "startOffset" : 66,
      "endOffset" : 93
    }, {
      "referenceID" : 0,
      "context" : "BDDs were introduced by Akers (1978) as compact representations for boolean functions.",
      "startOffset" : 24,
      "endOffset" : 37
    }, {
      "referenceID" : 0,
      "context" : "BDDs were introduced by Akers (1978) as compact representations for boolean functions. Bryant (1986) proposed ordered binary decision diagrams (OBDDs) as canonical representations of boolean functions.",
      "startOffset" : 24,
      "endOffset" : 101
    }, {
      "referenceID" : 5,
      "context" : "Bollig and Wegener (1996) proved that finding an optimal variable ordering is an NPcomplete problem.",
      "startOffset" : 0,
      "endOffset" : 26
    }, {
      "referenceID" : 5,
      "context" : "Bollig and Wegener (1996) proved that finding an optimal variable ordering is an NPcomplete problem. An order is optimal if it yields a BDD with the smallest number of nodes. Bryant (1986) pointed out that variable ordering greatly influences the size of the BDD.",
      "startOffset" : 0,
      "endOffset" : 189
    }, {
      "referenceID" : 31,
      "context" : "The method suggested by Malik et al. (1988) assigns to each vertex a level metric and orders the variables in decreasing level value.",
      "startOffset" : 24,
      "endOffset" : 44
    }, {
      "referenceID" : 22,
      "context" : "Fujita et al. (1988) proposed executing a DFS (depth first search) from the vertices with no out edges, and progressing backwards.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 10,
      "context" : "Butler et al. (1991) adapted the algorithm of Fujita et al.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 22,
      "context" : "Another DFS-based algorithm relies on interleaving the variables in the order (Fujii et al., 1993).",
      "startOffset" : 78,
      "endOffset" : 98
    }, {
      "referenceID" : 40,
      "context" : "Minato et al. (1990) propagate values backward through the graph, starting from vertices with no out edges, whose value is set to 1.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 12,
      "context" : "Chung et al.(1993) proposed two algorithm frameworks.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 34,
      "context" : "The algorithm of Malik et al. was extended and adapted for finite-state machines (FSM) by Toutai et al. (1990). In their algorithm, a model is decomposed to its next state functions, each of which is considered separately.",
      "startOffset" : 17,
      "endOffset" : 111
    }, {
      "referenceID" : 1,
      "context" : "The algorithm of Aziz et al. (1994) decomposes the model in a different way.",
      "startOffset" : 17,
      "endOffset" : 36
    }, {
      "referenceID" : 29,
      "context" : "Sample-based static algorithms (Jain et al., 1998) are not real static algorithms in the sense that they do not create the order based on information extracted from the model description.",
      "startOffset" : 31,
      "endOffset" : 50
    }, {
      "referenceID" : 8,
      "context" : "Broos and Branting (1994) present a method for inducing a preference predicate using nearest neighbor classification.",
      "startOffset" : 0,
      "endOffset" : 26
    }, {
      "referenceID" : 8,
      "context" : "Broos and Branting (1994) present a method for inducing a preference predicate using nearest neighbor classification. The distance between an untagged pair and each tagged pair is computed as the sum of distances between the corresponding elements. The closest tagged pair is selected. The preferred element of the untagged pair is the one matching the preferred element in the tagged pair. Utgoff and Saxena (1987) represent a pair A,B by the concatenated feature vector 〈a1, .",
      "startOffset" : 0,
      "endOffset" : 416
    }, {
      "referenceID" : 8,
      "context" : "Broos and Branting (1994) present a method for inducing a preference predicate using nearest neighbor classification. The distance between an untagged pair and each tagged pair is computed as the sum of distances between the corresponding elements. The closest tagged pair is selected. The preferred element of the untagged pair is the one matching the preferred element in the tagged pair. Utgoff and Saxena (1987) represent a pair A,B by the concatenated feature vector 〈a1, . . . an, b1, . . . bn〉. The preference predicate is a decision tree induced from these examples. Utgoff and Clouse (1991) represent a preference predicate by a polynomial.",
      "startOffset" : 0,
      "endOffset" : 600
    }, {
      "referenceID" : 48,
      "context" : "The learner, which is an ID3 (Quinlan, 1986) decision tree generator, uses the tagged feature vectors to create a pair precedence classifier.",
      "startOffset" : 29,
      "endOffset" : 44
    }, {
      "referenceID" : 36,
      "context" : "We use the SMV (McMillan, 1993) system for this purpose.",
      "startOffset" : 15,
      "endOffset" : 31
    }, {
      "referenceID" : 10,
      "context" : "This attribute was inspired by the value used by Butler et al. (1991) to guide the DFS search.",
      "startOffset" : 49,
      "endOffset" : 70
    }, {
      "referenceID" : 12,
      "context" : "The distance-based ordering framework (Chung et al., 1993) uses a similar feature to order variables.",
      "startOffset" : 38,
      "endOffset" : 58
    }, {
      "referenceID" : 20,
      "context" : "This type of representation can be used to produce classifiers by many induction algorithms, including decision trees (Hunt, Marin, & Stone, 1966; Friedman, 1977; Quinlan, 1979; Breiman, Frieman, Olshen, & Stone, 1984), neural networks (Widrow & Hoff, 1960; Parker, 1985; Rumelhart & McClelland, 1986) and nearest neighbor (Cover & Hart, 1967; Duda & Hart, 1973).",
      "startOffset" : 118,
      "endOffset" : 218
    }, {
      "referenceID" : 47,
      "context" : "This type of representation can be used to produce classifiers by many induction algorithms, including decision trees (Hunt, Marin, & Stone, 1966; Friedman, 1977; Quinlan, 1979; Breiman, Frieman, Olshen, & Stone, 1984), neural networks (Widrow & Hoff, 1960; Parker, 1985; Rumelhart & McClelland, 1986) and nearest neighbor (Cover & Hart, 1967; Duda & Hart, 1973).",
      "startOffset" : 118,
      "endOffset" : 218
    }, {
      "referenceID" : 45,
      "context" : "This type of representation can be used to produce classifiers by many induction algorithms, including decision trees (Hunt, Marin, & Stone, 1966; Friedman, 1977; Quinlan, 1979; Breiman, Frieman, Olshen, & Stone, 1984), neural networks (Widrow & Hoff, 1960; Parker, 1985; Rumelhart & McClelland, 1986) and nearest neighbor (Cover & Hart, 1967; Duda & Hart, 1973).",
      "startOffset" : 236,
      "endOffset" : 301
    }, {
      "referenceID" : 30,
      "context" : "This problem is known as the minimum feedback arc set and is proven to be NP-hard (Karp, 1972).",
      "startOffset" : 82,
      "endOffset" : 94
    }, {
      "referenceID" : 11,
      "context" : "The ISCAS89 benchmark circuits have been used to empirically evaluate many algorithms that deal with various aspects of circuit design (Chamberlain, 1995; Wahba & Borrione, 1995; Nakamura, Takagi, Kimura, & Watanabe, 1998; Long, Iyer, & Abramovici, 1995; Iyer & Abramovici, 1996; Konuk & Larrabee, 1993).",
      "startOffset" : 135,
      "endOffset" : 303
    }, {
      "referenceID" : 10,
      "context" : "In both algorithms we used the adaptation for multiple starting points (Butler et al., 1991) and its expanded version, which includes the tie breaking rule (Fujita et al.",
      "startOffset" : 71,
      "endOffset" : 92
    }, {
      "referenceID" : 24,
      "context" : ", 1991) and its expanded version, which includes the tie breaking rule (Fujita et al., 1993).",
      "startOffset" : 71,
      "endOffset" : 92
    }, {
      "referenceID" : 21,
      "context" : "In addition, we compared our results to two advanced graph search algorithms for static ordering: the DFS append algorithm of Fujita et al. (1988) and the interleave algorithm of Fujii et al.",
      "startOffset" : 126,
      "endOffset" : 147
    }, {
      "referenceID" : 21,
      "context" : "(1988) and the interleave algorithm of Fujii et al. (1993). In both algorithms we used the adaptation for multiple starting points (Butler et al.",
      "startOffset" : 39,
      "endOffset" : 59
    }, {
      "referenceID" : 53,
      "context" : "Compared with previous work in machine learning, our precedence relations most resemble these of Utgoff and Saxena (1987). Our ordering approach, in which we construct a total order of elements by finding the precedence relation between them, is in essence the same as that of Cohen, Schapire and Singer (1999).",
      "startOffset" : 97,
      "endOffset" : 122
    }, {
      "referenceID" : 53,
      "context" : "Compared with previous work in machine learning, our precedence relations most resemble these of Utgoff and Saxena (1987). Our ordering approach, in which we construct a total order of elements by finding the precedence relation between them, is in essence the same as that of Cohen, Schapire and Singer (1999). Specifically, the second ordering algorithm of Cohen, Schapire and Singer also uses the topological ordering approach to create an order.",
      "startOffset" : 97,
      "endOffset" : 311
    }, {
      "referenceID" : 14,
      "context" : "Few works have applied learning to ordering techniques that are not utility based (Cohen et al., 1999).",
      "startOffset" : 82,
      "endOffset" : 102
    } ],
    "year" : 2011,
    "abstractText" : "The size and complexity of software and hardware systems have significantly increased in the past years. As a result, it is harder to guarantee their correct behavior. One of the most successful methods for automated verification of finite-state systems is model checking. Most of the current model-checking systems use binary decision diagrams (BDDs) for the representation of the tested model and in the verification process of its properties. Generally, BDDs allow a canonical compact representation of a boolean function (given an order of its variables). The more compact the BDD is, the better performance one gets from the verifier. However, finding an optimal order for a BDD is an NP-complete problem. Therefore, several heuristic methods based on expert knowledge have been developed for variable ordering. We propose an alternative approach in which the variable ordering algorithm gains “ordering experience” from training models and uses the learned knowledge for finding good orders. Our methodology is based on offline learning of pair precedence classifiers from training models, that is, learning which variable pair permutation is more likely to lead to a good order. For each training model, a number of training sequences are evaluated. Every training model variable pair permutation is then tagged based on its performance on the evaluated orders. The tagged permutations are then passed through a feature extractor and are given as examples to a classifier creation algorithm. Given a model for which an order is requested, the ordering algorithm consults each precedence classifier and constructs a pair precedence table which is used to create the order. Our algorithm was integrated with SMV, which is one of the most widely used verification systems. Preliminary empirical evaluation of our methodology, using real benchmark models, shows performance that is better than random ordering and is competitive with existing algorithms that use expert knowledge. We believe that in sub-domains of models (alu, caches, etc.) our system will prove even more valuable. This is because it features the ability to learn sub-domain knowledge, something that no other ordering algorithm does.",
    "creator" : "dvips(k) 5.92a Copyright 2002 Radical Eye Software"
  }
}