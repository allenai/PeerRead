{
  "name" : "1511.06342.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Emilio Parisotto", "Jimmy Ba", "Ruslan Salakhutdinov" ],
    "emails" : [ "eparisotto@cs.toronto.edu", "jimmy@cs.toronto.edu", "rsalakhu@cs.toronto.edu" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "Deep Reinforcement Learning (DRL), the combination of reinforcement learning methods and deep neural network function approximators, has recently shown considerable success in highdimensional challenging tasks, such as robotic manipulation (Levine et al., 2015; Lillicrap et al., 2015) and arcade games (Mnih et al., 2015). These methods exploit the ability of deep networks to learn salient descriptions of raw state input, allowing the agent designer to essentially bypass the lengthy process of feature engineering. In addition, these automatically learnt descriptions often significantly outperform hand-crafted feature representations that require extensive domain knowledge. One such DRL approach, the Deep Q-Network (DQN) (Mnih et al., 2015), has achieved state-ofthe-art results on the Arcade Learning Environment (ALE) (Bellemare et al., 2013), a benchmark of Atari 2600 arcade games. The DQN uses a deep convolutional neural network over pixel inputs to parameterize a state-action value function. The DQN is trained using Q-learning combined with several tricks that stabilize the training of the network, such as a replay memory to store past transitions and target networks to define a more consistent temporal difference error.\nAlthough the DQN maintains the same network architecture and hyperparameters for all games, the approach is limited in the fact that each network only learns how to play a single game at a time, despite the existence of similarities between games. For example, the tennis-like game of pong and the squash-like game of breakout are both similar in that each game consists of trying to hit a moving ball with a rectangular paddle. A network learnt to play multiple games would be able to generalize its knowledge between the games, achieving a single compact state representation as the inter-task similarities are exploited by the network. Having been trained on enough source tasks, the multitask network can also exhibit transfer to new target tasks, which can speed up learning. Training DRL agents can be extremely computationally intensive and therefore reducing training time is a significant practical benefit.\nThe contribution of this paper is to develop and evaluate methods that enable multitask and transfer learning for DRL agents, using the ALE as a test environment. To first accomplish multitask learn-\nar X\niv :1\n51 1.\n06 34\n2v 1\n[ cs\n.L G\n] 1\n9 N\nov 2\ning, we design a method called “Actor-Mimic” that leverages techniques from model compression to train a single multitask network using guidance from a set of game-specific expert networks. The particular form of guidance can vary, and several different approaches are explored and tested empirically. To then achieve transfer learning, we treat a multitask network as a pretrained DQN from a set of source tasks. We show experimentally that this multitask pre-training can result in a DQN that learns a target task significantly faster than a DQN starting from a random initialization, effectively demonstrating that the source task representations generalize to the target task."
    }, {
      "heading" : "2 BACKGROUND: DEEP REINFORCEMENT LEARNING",
      "text" : "A Markov Decision Process (MDP) is defined as a tuple (S, A, T , R, γ) where S is a set of states, A is a set of actions, T (s′|s, a) is the transition probability of ending up in state s′ when executing action a in state s,R is the reward function mapping states in S to rewards in R, and γ is a discount factor. An agent’s behaviour in an MDP is represented as a policy π(a|s) which defines the probability of executing action a in state s. For a given policy, we can further define the Q-value function Qπ(s, a) = E[ ∑H t=0 γ\ntrt|s0 = s, a0 = a] where H is the step when the game ends. The Q-function represents the expected future discounted reward when starting in a state s, executing a, and then following policy π until a terminating state is reached. There always exists at least one optimal state-action value function, Q∗(s, a), such that ∀s ∈ S, a ∈ A, Q∗(s, a) = maxπ Qπ(s, a) (Sutton & Barto, 1998). The optimal Q-function can be rewritten as a Bellman equation:\nQ∗(s, a) = E s′∼T (·|s,a)\n[ r + γ ·max\na′∈A Q∗(s′, a′)\n] . (1)\nAn optimal policy can be constructed from the optimal Q-function by choosing, for a given state, the action with highest Q-value. Q-learning, a reinforcement learning algorithm, uses iterative backups of the Q-function to converge towards the optimal Q-function. Using a tabular representation of the Q-function, this is equivalent to setting Q(n+1)(s, a) = Es′∼T (·|s,a)[r + γ ·maxa′∈AQ(n)(s′, a′)] for the (n+1)th update step (Sutton & Barto, 1998). Because the state space in the ALE is too large to tractably store a tabular representation of the Q-function, the Deep Q-Network (DQN) approach uses a deep function approximator to represent the state-action value function (Mnih et al., 2015). To train a DQN on the (n+1)th step, we set the network’s loss to\nL(n+1)(θ(n+1)) = E s,a,r,s′∼M(·)\n[( r + γ ·max\na′∈A Q(s′, a′; θ(n))−Q(s, a; θ(n+1))\n)2] , (2)\nwhere M(·) is a uniform probability distribution over a replay memory, which is a set of the m previous (s, a, r, s′) transition tuples seen during play, where m is the size of the memory. The replay memory is used to reduce correlations between adjacent states and is shown to have large effect on the stability of training the network in some games."
    }, {
      "heading" : "3 ACTOR-MIMIC",
      "text" : ""
    }, {
      "heading" : "3.1 POLICY REGRESSION OBJECTIVE",
      "text" : "Given a set of source games S1, ..., SN , our first goal is to obtain a single multitask policy network that can play any source game at as near an expert level as possible. To train this multitask policy network, we use guidance from a set of expert DQN networks E1, ..., EN , where Ei is an expert specialized in source task Si. One possible definition of “guidance” would be to define a squared loss that would match Q-values between the student network and the experts. As the range of the expert value functions could vary widely between games, we found it difficult to directly distill knowledge from the expert value functions. The alternative we develop here is to instead match policies by first transforming Q-values using a softmax. Using the softmax gives us outputs which are bounded in the unit interval and so the effects of the different scales of each expert’s Q-function are diminished, achieving higher stability during learning. Intuitively, we can view using the softmax from the perspective of forcing the student to focus more on mimicking the action chosen by the\nguiding expert at each state, where the exact values of the state are less important. We call this method “Actor-Mimic” as it is an actor, i.e. policy, that mimics the decisions of a set of experts. In particular, our technique first transforms each expert DQN into a policy network by a Boltzmann distribution defined over the Q-value outputs,\nπEi(a|s) = eτ −1QEi (s,a)∑\na′∈AEi eτ −1QEi (s,a\n′) , (3)\nwhere τ is a temperature parameter and AEi is the action space used by the expert Ei, AEi ⊆ A. Given a state s from source task Si, we then define the policy objective over the multitask network as the cross-entropy between the expert network’s policy and the current multitask policy:\nLipolicy(θ) = ∑ a∈AEi πEi(a|s) log πAMN(a|s; θ), (4)\nwhere πAMN(a|s; θ) is the multitask Actor-Mimic Network (AMN) policy, parameterized by θ. In contrast to the Q-learning objective which recursively relies on itself as a target value, we now have a stable supervised training signal (the expert network output) to guide the multitask network.\nTo acquire training data, we can sample either the expert network or the AMN action outputs to generate the trajectories used in the loss. Empirically we have observed that sampling from the AMN while it is learning gives the best results. We later prove that in either case of sampling from the expert or AMN as it is learning, the AMN will converge to the expert policy using the policy regression loss, at least in the case when the AMN is a linear function approximator. We use an -greedy policy no matter which network we sample actions from, which with probability picks a random action uniformly and with probability 1− chooses an action from the network."
    }, {
      "heading" : "3.2 FEATURE REGRESSION OBJECTIVE",
      "text" : "We can obtain further guidance from the expert networks in the following way. Let hAMN(s) and hEi(s) be the hidden activations in the feature (pre-output) layer of the AMN and i’th expert network computed from the input state s, respectively. Note that the dimension of hAMN(s) does not necessarily need to be equal to hEi(s), and this is the case in some of our experiments. We define a feature regression network fi(hAMN(s)) that, for a given state s, attempts to predict the features hEi(s) from hAMN(s). The architecture of the mapping fi can be defined arbitrarily, and fi can be trained using the following feature regression loss:\nLiFeatureRegression(θ, θfi) = ‖fi(hAMN(s; θ); θfi)− hEi(s)‖ 2 2 , (5)\nwhere θ and θfi are the parameters of the AMN and i th feature regression network, respectively. When training this objective, the error is fully back-propagated from the feature regression network output through the layers of the AMN. In this way, the feature regression objective provides pressure on the AMN to compute features that can predict an expert’s features. A justification for this objective is that if we have a perfect regression from multitask to expert features, all the information in the expert features is contained in the multitask features. The use of the separate feature prediction network fi for each task enables the multitask network to have a different feature dimension than the experts as well as prevent issues with identifiability. Empirically we have found that the feature regression objective’s primary benefit is that it can increase the performance of transfer learning in some target tasks."
    }, {
      "heading" : "3.3 ACTOR-MIMIC OBJECTIVE",
      "text" : "Combining both regression objectives, the Actor-Mimic objective is thus defined as\nLiActorMimic(θ, θfi) = Lipolicy(θ) + β ∗ LiFeatureRegression(θ, θfi), (6)\nwhere β is a scaling parameter which controls the relative weighting of the two objectives. Intuitively, we can think of the policy regression objective as a teacher (expert network) telling a student (AMN) how they should act (mimic expert’s actions), while the feature regression objective is analogous to a teacher telling a student why it should act that way (mimic expert’s thinking process)."
    }, {
      "heading" : "3.4 TRANSFERING KNOWLEDGE: ACTOR-MIMIC AS PRETRAINING",
      "text" : "Now that we have a method of training a network that is an expert at all source tasks, we can proceed to the task of transferring source task knowledge to a novel but related target task. To enable transfer to a new task, we first remove the final softmax layer of the AMN. We then use the weights of AMN as an instantiation for a DQN that will be trained on the new target task. The pretrained DQN is then trained using the same training procedure as the one used with a standard DQN.\nThere are multiple interpretations of the multitask pretraining. From a reinforcement learning perspective, instantiating the state-action value function approximator is equivalent to reward shaping (Wiewiora, 2003), a method of augmenting the reward signal of an MDP so as to make it more informative. Reward shaping does not alter the optimal policy of an MDP but can make reaching the optimal policy, or at least a local maximum, easier (Ng et al., 1999). Intuitively, instantiating a DQN with a multitask network can be seen as providing the DQN an idea of what actions might have relatively higher reward, given previous training tasks. Another perspective of multitask pretraining is that it initializes the DQN with a set of features that are effective at defining policies in related tasks. If the source and target tasks share similarities, it is probable that some of these pretrained features will also be effective at the target task (perhaps after slight fine-tuning)."
    }, {
      "heading" : "4 CONVERGENCE PROPERTIES OF ACTOR-MIMIC",
      "text" : "We further study the convergence properties of the proposed Actor-Mimic under a framework similar to (Perkins & Precup, 2002). The analysis mainly focuses on policy regression. Without losing generality, the following analysis focuses on learning from a single game expert softmax policy πE . The analysis can be readily extended to consider multiple experts on multiple games by absorbing different games into the same state space. Let Dπ(s) be the stationary distribution of the Markov decision process under policy π over states s ∈ S . The policy regression objective function can be rewritten using expectation under the stationary distribution of the Markov decision process:\nmin θ E s∼DπAMN, -greedy (·)\n[ H ( πE(a|s), πAMN(a|s; θ) )] + λ‖θ‖22, (7)\nwhere H(·) is the cross-entropy measure and λ is the coefficient of weight decay that is necessary in the following analysis of the policy regression. Under Actor-Mimic, the learning agent interacts with the environment by following an -greedy strategy of some Q function. The mapping from a Q function to an -greedy policy π -greedy is denoted by an operator Γ, where π -greedy = Γ(Q). To avoid confusion onwards, we use notation p(a|s; θ) for the softmax policies in the policy regression objective.\nAssume each state in a Markov decision process is represented by a compactK-dimensional feature representation φ(s) ∈ RK . Consider a linear function approximator for Q values with parameter matrix θ ∈ RK×|A|, Q̂(s, a; θ) = φ(s)T θa, where θa is the ath column of θ. The corresponding softmax policy of the linear approximator is defined by p(a|s; θ) ∝ exp{Q̂(s, a; θ)}."
    }, {
      "heading" : "4.1 STOCHASTIC STATIONARY POLICY",
      "text" : "For any stationary policy π∗, the stationary point of the objective function Eq. (7) can be found by setting its gradient w.r.t. θ to zero. Let Pθ be a |S| × |A| matrix where its ith row jth column element is the softmax policy prediction p(aj |si; θ) from the linear approximator. Similarly, let ΠE be a |S| × |A| matrix for the softmax policy prediction from the expert model. Additionally, let Dπ be a diagonal matrix whose entries are Dπ(s). A simple gradient following algorithm on the objective function Eq. (7) has the following expected update rule:\n∆θ = −α [ ΦTDπ(Pθ −ΠE) + λθ ] . (8)\nLemma 1. Under a fixed policy π∗, the parameters θ, updated by the stochastic gradient descent learning algorithm described above, asymptotically converge to a unique solution θ∗.\nFor a fixed policy π∗, the objective function Eq. (7) is convex and is the same as a multinomial logistic regression problem. Hence there is a unique stationary point θ∗ such that ∆θ = 0. The\ngradient following algorithm converges asymptotically to the stationary point (Robbins & Monro, 1951) ."
    }, {
      "heading" : "4.2 STOCHASTIC ADAPTIVE POLICY",
      "text" : "Consider the following learning scheme to adapt the agent’s policy. The learning agent interacts with the environment and samples states by following a fixed -greedy policy π′. Given the samples and the expert prediction, the linear function approximator parameters are updated using Eq. (8) to a unique stationary point θ′. The new parameters θ′ are then used to establish a new -greedy policy π′′ = Γ(Q̂θ′) through the Γ operator over the linear function Q̂θ′ . The agent under the new policy π′′ subsequently samples a new set of states and actions from the Markov decision process to update its parameters. The learning agent therefore generates a sequence of policies {π1, π2, π3, ...}. The proof for the following theorem is given in Appendix A.\nTheorem 1. Assume the Markov decision process is irreducible and aperiodic for any policy π induced by the Γ operator and Γ is Lipschtiz continuous with a constant c , the sequence of policies and model parameters generated by the iterative algorithm above converge to a unique solution π∗ and θ∗."
    }, {
      "heading" : "5 EXPERIMENTS",
      "text" : "In the following experiments, we validate the Actor-Mimic method by demonstrating its effectiveness at both multitask and transfer learning in the Arcade Learning Environment (ALE). For our experiments, we use subsets of a collection of 20 Atari games. 19 games of this set were among the 29 games that the DQN method performed at a super-human level. We additionally chose 1 game, the game of Seaquest, on which the DQN had performed poorly when compared to a human expert. Details on the training procedure are described in Appendix B."
    }, {
      "heading" : "5.1 ACTOR-MIMIC",
      "text" : "To first evaluate the actor-mimic objective on multitask learning, we demonstrate the effectiveness of training an AMN over multiple games simultaneously. In this particular case, since our focus is on multitask learning and not transfer learning, we disregard the feature regression objective and set β to 0. Figure 1 and Table 1 show the results of an AMN trained on 8 games simultaneously with the policy regression objective, compared to an expert DQN trained separately for each game. The AMN and every individual expert DQN in this case had the exact same network architecture. We can see that the AMN quickly reaches close-to-expert performance on 7 games out of 8, only taking\naround 20 epochs or 5 million training frames to settle to a stable behaviour. This is in comparison to the expert networks, which were trained for up to 50 million frames. The ability of an AMN to learn 8 tasks simultaneously with the same architecture as a single-task expert shows that, at least for Atari games, the model complexity does not need to scale linearly with the number of source tasks. Thus it is probable that some level of generalization between games is achieved.\nWe note that the AMN often becomes more consistent in its behaviour than the expert DQN, with a lower reward variance observed in the training plots of every game except Atlantis and Pong. Another surprising result is that the AMN achieves a significantly higher mean and maximum reward in the game of Atlantis and higher mean reward in the games of Breakout and Enduro. This is despite the fact that the AMN is not being optimized to improve reward over the expert but just replicate the expert’s behaviour. This might suggest that the network has transferred its knowledge of good behaviour from the other training games to the games of Atlantis, Breakout and Enduro. We also observed this inter-source-task transfer effect again when we later on increased the AMN model complexity for the transfer experiments (see Atlantis experiments in Appendix D). The AMN had the worst performance on the game of Seaquest, which was a game on which the expert DQN itself did not do very well. It is possible that a low quality expert policy has difficulty teaching the AMN to even replicate its own (poor) behaviour. We compare the performance of our AMN against a baseline of two different multitask DQN architectures in Appendix C.\nWe have found that although a small AMN can learn how to behave at a close-to-expert level on multiple source tasks, a larger AMN can more easily transfer knowledge to target tasks after being trained on the source tasks. For the transfer experiments, we therefore significantly increased the AMN model complexity relative to that of an expert. Using a larger network architecture also allowed us to scale up to playing 13 source games at once (see Appendix D for source task performance using the larger AMNs). We additionally found that using an AMN trained for too long on the source tasks hurt transfer, as it is likely overfitting. Therefore for the transfer experiments, we train the AMN on only 4 million frames for each of the source games.\nTo evaluate the Actor-Mimic objective on transfer learning, the previously described large AMNs will be used as a weight initialization for DQNs which are each trained on a different target task. We additionally independently evaluate the benefit of the feature regression objective during transfer by having one AMN trained with only the policy regression objective (AMN-policy) and another trained using both feature and policy regression (AMN-feature). The results are then compared to the baseline of a DQN that was initialized with random weights.\nThe performance on a set of 7 target games is detailed in Table 2. We can see that the AMN pretraining provides a definite increase in learning speed for the 3 games of Breakout, Star Gunner and Video Pinball. The results in Breakout and Video Pinball demonstrate that the policy regression objective alone provides significant positive transfer in some target tasks. The reason for this large positive transfer might be due to the source game Pong having very similar mechanics to both Video Pinball and Breakout, where one must use a paddle to prevent a ball from falling off screen. The machinery used to detect the ball in Pong would likely be useful in detecting the ball for these two target tasks, given some fine-tuning. Additionally, the feature regression objective causes a significant speed-up in the game of Star Gunner compared to both the random initialization and the network trained solely with policy regression. Therefore even though the feature regression objective can slightly hurt transfer in some source games, it can provide large benefits in others. The\npositive transfer in Breakout, Star Gunner and Video Pinball saves at least up to 5 million frames of training time in each game. Processing 5 million frames with the large model is equivalent to around 4 days of compute time on a NVIDIA GTX Titan.\nOn the other hand, for the games of Krull and Road Runner (although the multitask pretraining does help learning at the start) the effect is not very pronounced. When running Krull we observed that the policy learnt by any DQN regardless of the initialization was a sort of unexpected local maximum. In Krull, the objective is to move between a set of varied minigames and complete each one. One of the minigames, where the player must traverse a spiderweb, gives extremely high reward by simply jumping quickly in a mostly random fashion. What the DQN does is it kills itself on purpose in the initial minigame, runs to the high reward spiderweb minigame, and then simply jumps in the corner of the spiderweb until it is terminated by the spider. Because it is relatively easy to get stuck in this local maximum, and very hard to get out of it (jumping in the minigame gives unproportionally high reward compared to the other minigames), transfer does not really help learning.\nFor the games of Gopher and Robotank, we can see that the multitask pretraining does not have any significant positive effect. In particular, multitask pretraining for Robotank even seems to slow down learning, providing an example of negative transfer. The task in Robotank is to control a tank turret in a 3D environment to destroy other tanks, so it’s possible that this game is so significantly different from any source task (being the only first-person 3D game) that the multitask pretraining does not provide any useful prior knowledge."
    }, {
      "heading" : "6 RELATED WORK",
      "text" : "The idea of using expert networks to guide a single mimic network has been studied in the context of supervised learning, where it is known as model compression. The goal of model compression is to reduce the computational complexity of a large model (or ensemble of large models) to a single smaller mimic network while maintaining as high an accuracy as possible. To obtain high accuracy, the mimic network is trained using rich output targets provided by the experts. These output targets\nare either the final layer logits (Ba & Caruana, 2014) or the high-temperature softmax outputs of the experts (Hinton et al., 2015). Our approach is most similar to the technique of Hinton et al. (2015) which matches the high-temperature outputs of the mimic network with that of the expert network. In contrast to these model compression techniques, our method is not concerned with decreasing test time computation but instead using experts to provide otherwise unavailable supervision to a mimic network on several distinct tasks. In addition, we define a novel objective that provides expert guidance at the feature level, instead of only at the output level. The feature regression objective could potentially also be useful in the supervised learning model compression case, but that is out of the scope of this paper.\nActor-Mimic can also be considered as part of the larger Imitation Learning class of methods, which use expert guidance to teach an agent how to act. One such method, called DAGGER (Ross et al., 2011), is similar to our approach in that it trains a policy to directly mimic an expert’s behaviour while sampling actions from the mimic agent. Actor-Mimic can be considered as an extension of this work to the multitask case. In addition, using a deep neural network to parameterize the policy provides us with several advantages over the more general Imitation Learning framework. First, we can exploit the automatic feature construction ability of deep networks to transfer knowledge to new tasks, as long as the raw data between tasks is in the same form, i.e. pixel data with the same dimensions. Second, we can define objectives which take into account intermediate representations of the state and not just the policy outputs, for example the feature regression objective. This provides a richer training signal to the mimic network than just samples of the expert’s action output, and could conceivably help prevent the mimic network from overfitting to information on the states it currently visits most frequently.\nA wide variety of methods have also been studied in the context of RL transfer learning (see Taylor & Stone (2009) for a more comprehensive review). One related approach is to use a dual state representation with a set of task-specific and task-independent features known as “problem-space” and “agent-space” descriptors, respectively. For each source task, a task-specific value function is learnt on the problem-space descriptors and then these learnt value functions are transferred to a single value function over the agent-space descriptors. Because the agent-space value function is defined over features which maintain constant semantics across all tasks, this value function can be directly transferred to new tasks. Banerjee & Stone (2007) constructed agent-space features by first generating a fixed-depth game tree of the current state, classifying each future state in the tree as either {win, lose, draw, nonterminal} and then coalescing all states which have the same class or subtree. To transfer the source tasks value functions to agent-space, they use a simple weighted average of the source task value functions, where the weight is proportional to the number of times that a specific agent-space descriptor has been seen during play in that source task. In another related method, Konidaris & Barto (2006) transfer the value function to agent-space by using regression to predict every source tasks problem-space value function from the agent-space descriptors. A drawback of these methods are that both agent-space and problem-space descriptors are either handengineered or generated from a perfect environment model, thus requiring a significant amount of domain knowledge."
    }, {
      "heading" : "7 DISCUSSION",
      "text" : "In this paper we defined Actor-Mimic, a novel method for training a single deep policy network over a set of related source tasks. We have shown that a network trained using Actor-Mimic is capable of reaching expert performance on many games simultaneously, while having the same model complexity as a single expert. In addition, using Actor-Mimic as a multitask pretraining phase can significantly improve learning speed in a set of target tasks. This demonstrates that the features learnt over the source tasks can generalize to new target tasks, given a sufficient level of similarity between source and target tasks. A direction of future work is to develop methods that can enable a targeted knowledge transfer from source tasks by identifying related source tasks for the given target task. Using targeted knowledge transfer can potentially help in cases of negative transfer observed in our experiments.\nAcknowledgments: This work was supported by Samsung and NSERC."
    }, {
      "heading" : "APPENDIX A PROOF OF THEOREM 1",
      "text" : "Lemma 2. For any two policies π1,π2, the stationary distributions over the states under the policies are bounded: ‖Dπ1 −Dπ2‖ ≤ cD‖π1 − π2‖, for some cD > 0.\nProof. Let T 1 and T 2 be the two transition matrices under the stationary distributions Dπ1 , Dπ2 . For any ij elements T 1ij , T 2 ij in the transition matrices, we have,\n‖T 1ij − T 2ij‖ = ∥∥∥∥∥∑ a p(si|a, sj) ( π1(a|sj)− π2(a|sj) )∥∥∥∥∥ (9) ≤|A|‖π1(a|sj)− π2(a|sj)‖ (10) ≤|A|‖π1 − π2‖∞. (11)\nThe above bound for any ijth elements implies the Euclidean distance of the transition matrices is also upper bounded ‖T 1−T 2‖ ≤ |S||A|‖π1−π2‖. Seneta (1991) has shown that ‖Dπ1 −Dπ2‖ ≤\n1 1−λ1 ‖T 1−T 2‖∞, where λ1 is the largest eigenvalue of T 1. Hence, there is a constant cD > 0 such that ‖Dπ1 −Dπ2‖ ≤ cD‖π1 − π2‖.\nLemma 3. For any two softmax policy Pθ1 , Pθ2 matrices from the linear function approximator, ‖Pθ1 − Pθ2‖ ≤ cJ‖Φθ1 − Φθ2‖, for some cJ ≥ 0.\nProof. Note that the ith row jth column element p(aj |si) in a softmax policy matrix P is computed by the softmax transformation on the Q function:\npij = p(aj |si) = softmax ( Q(si, aj) ) =\neQ(si,aj)∑ k eQ(si,ak) . (12)\nBecause the softmax function is a monotonically increasing element-wise function on matrices, the Euclidean distance of the softmax transformation is upper bounded by the largest Jacobian in the domain of the softmax function. Namely, for c′J = maxz∈Dom softmax ‖ ∂softmax(z) ∂z ‖,\n‖softmax(x1)− softmax(x2)‖ ≤ c′J‖x1 − x2‖,∀x1, x2 ∈ Dom softmax. (13)\nBy bounding the elements in P matrix, it gives ‖Pθ1 − Pθ2‖ ≤ cJ‖Q̂θ1 − Q̂θ2‖ = cJ‖Φθ1 − Φθ2‖.\nTheorem 1. Assume the Markov decision process is irreducible and aperiodic for any policy π induced by the Γ operator and Γ is Lipschitz continuous with constant c , the sequence of policies and model parameters generated by the iterative algorithm above converge to a unique solution π∗ and θ∗.\nProof. We follow a similar contraction argument made in Perkins & Precup (2002) , and show the iterative algorithm is a contraction process. Namely, for any two policies π1 and π2, the learning algorithm above produces new policies Γ(Q̂θ1), Γ(Q̂θ2) after one iteration, where ‖Γ(Q̂θ1)−Γ(Q̂θ2)‖ ≤ β‖π1 − π2‖. Here ‖ · ‖ is the Euclidean norm and β ∈ [0, 1). By Lipschtiz continuity,\n‖Γ(Q̂θ1)− Γ(Q̂θ2)‖ ≤c ‖Q̂θ1 − Q̂θ2‖ = c ‖Φθ1 − Φθ2‖ (14) ≤c ‖Φ‖‖θ1 − θ2‖. (15)\nFrom Lemma 1, for θ1 and θ2 are the stationary point of Eq. (7), ∆θ1 = ∆θ2 = 0, rearrange Eq. (8) gives,\n‖θ1 − θ2‖ = 1 λ ‖ΦTDπ1(Pθ1 −Πe)− ΦTDπ2(Pθ2 −Πe)‖ (16)\n= 1\nλ ‖ΦT (Dπ2 −Dπ1)Πe + ΦTDπ1Pθ1 − ΦTDπ1Pθ2 + ΦTDπ1Pθ2 − ΦTDπ2Pθ2‖\n(17)\n= 1\nλ ‖ΦT (Dπ2 −Dπ1)Πe + ΦTDπ1(Pθ1 − Pθ2) + ΦT (Dπ1 −Dπ2)Pθ2‖ (18)\n≤ 1 λ\n[ ‖ΦT ‖‖Dπ1 −Dπ2‖‖Πe‖+ ‖ΦT ‖‖Dπ1‖‖Pθ1 − Pθ2‖+ ‖ΦT ‖‖Dπ1 −Dπ2‖‖Pθ2‖ ] (19)\n≤c‖π1 − π2‖. (20)\nThe last inequality is given by Lemma 2 and 3 and the compactness of Φ. For a Lipschtiz constant c ≥ c, there exist a β such that ‖Γ(Q̂θ1)−Γ(Q̂θ2)‖ ≤ β‖π1−π2‖. Hence, the sequence of policies generated by the algorithm converges to a unique fixed point π∗ from the Contraction Mapping Theorem Bertsekas (1995). Whereas, the model parameters converge to a stationary point θ∗ under the fixed point policy π∗."
    }, {
      "heading" : "APPENDIX B AMN TRAINING DETAILS",
      "text" : "All of our Actor-Mimic Networks (AMNs) were trained using the Adam (Kingma & Ba, 2015) optimization algorithm. The AMNs have a single 18-unit output, with each output corresponding to one of the 18 possible Atari player actions. Having the full 18-action output simplifies the multitask case when each game has a different subset of valid actions. While playing a certain game, we mask out AMN action outputs that are not valid for that game and take the softmax over only the subset of valid actions. We use a replay memory for each game to reduce correlations between successive frames and stabilize network training. Because the memory requirements of having the standard replay memory size of 1,000,000 frames for each game were prohibitive when we are training over many source games, for AMNs we use a per-game 100,000 frame replay memory. AMN training was stable even with only a per-game equivalent of a tenth of the replay memory size of the DQN experts. For the transfer experiments with the feature regression objective, we set the scaling parameter β to 0.01 and the feature prediction network fi was set to a linear projection from the AMN features to the ith expert features. For the policy regression objective, we use a softmax temperature of 1 in all cases. Additionally, during training for all AMNs we use an -greedy policy with set to a constant 0.1. Annealing from 1 did not provide any noticeable benefit. During training, we choose actions based on the AMN and not the expert DQN. We do not use weight decay during AMN training as we empirically found that it did not provide any large benefits.\nFor the experiments using the DQN algorithm, we optimize the networks with RMSProp. Since the DQNs are trained on a single game their output layers only contain the player actions that are valid in the particular game that they are trained on. The experts guiding the AMNs used the same architecture, hyperparameters and training procedure as that of Mnih et al. (2015). We use the full 1,000,000 frame replay memory when training any DQN."
    }, {
      "heading" : "APPENDIX C MULTITASK DQN BASELINE RESULTS",
      "text" : "As a baseline, we trained DQN networks over 8 games simultaneously to test their performance against the Actor-Mimic method. We tried two different architectures, the first is using the basic DQN procedure on all 8 games. This network has a single 18 action output shared by all games, but when we train or test in a particular game, we mask out and ignore the action values from actions that are invalid for that particular game. This architecture is denoted the Multitask DQN (MDQN). The second architecture is a DQN but where each game has a separate fully-connected feature layer and action output. In this architecture only the convolutions are shared between games, and thus the features and action values are completely separate. This was to try to mitigate the destabilizing\neffect that the different value scales of each game had during learning. This architecture is denoted the Multitask Convolutions DQN (MCDQN).\nThe results for the MDQN and MCDQN are shown in Figures 2 and 3, respectively. From the figures, we can observe that the AMN is far more stable during training as well as being consistently higher in performance than either the MDQN or MCDQN methods. In addition, it can be seen that the MDQN and MCDQN will often focus on performing reasonably well on a small subset of the source games, such as on Boxing and Enduro, while making little to no progress in others, such as Breakout or Pong. Between the MDQN and MCDQN, we can see that the MCDQN hardly improves results even though it has significantly larger computational cost that scales linearly with the number of source games.\nFor the specific details of the architectures we tested, for the MDQN the architecture was: 8x8x4x324 1 → 4x4x32x64-2 → 3x3x64x64-1 → 512 fully-connected units → 18 actions. This is exactly the same network architecture as used for the 8 game AMN in Section 5.1. For the MCDQN, the bottom convolutional layers were the same as the MDQN, except there are 8 parallel subnetworks on top of the convolutional layers. These game-specific subnetworks had the architecture: 512 fullyconnected units → 18 actions. All layers except the action outputs were followed with a rectifier non-linearity.\n1 Here we represent convolutional layers as WxWxCxN-S, where W is the width of the (square) convolution kernel, C is the number of input images, N is the number of filter maps and S is the convolution stride."
    }, {
      "heading" : "APPENDIX D ACTOR-MIMIC NETWORK MULTITASK RESULTS FOR",
      "text" : "TRANSFER PRETRAINING\nThe network used for transfer consisted of the following architecture: 8x8x4x256-4 1 → 4x4x256x512-2→ 3x3x512x512-1→ 3x3x512x512-1→ 2048 fully-connected units→ 1024 fullyconnected units → 18 actions. All layers except the final one were followed with a rectifier nonlinearity."
    } ],
    "references" : [ {
      "title" : "Do deep nets really need to be deep",
      "author" : [ "Ba", "Jimmy", "Caruana", "Rich" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Ba et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Ba et al\\.",
      "year" : 2014
    }, {
      "title" : "General game learning using knowledge transfer",
      "author" : [ "Banerjee", "Bikramjit", "Stone", "Peter" ],
      "venue" : "In International Joint Conferences on Artificial Intelligence,",
      "citeRegEx" : "Banerjee et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Banerjee et al\\.",
      "year" : 2007
    }, {
      "title" : "The arcade learning environment: An evaluation platform for general agents",
      "author" : [ "Bellemare", "Marc G", "Naddaf", "Yavar", "Veness", "Joel", "Bowling", "Michael" ],
      "venue" : "Journal of Artificial Intelligence Research,",
      "citeRegEx" : "Bellemare et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Bellemare et al\\.",
      "year" : 2013
    }, {
      "title" : "Dynamic programming and optimal control, volume 1",
      "author" : [ "Bertsekas", "Dimitri P" ],
      "venue" : "Athena Scientific Belmont, MA,",
      "citeRegEx" : "Bertsekas and P.,? \\Q1995\\E",
      "shortCiteRegEx" : "Bertsekas and P.",
      "year" : 1995
    }, {
      "title" : "Distilling the knowledge in a neural network",
      "author" : [ "Hinton", "Geoffrey", "Vinyals", "Oriol", "Dean", "Jeff" ],
      "venue" : "arXiv preprint arXiv:1503.02531,",
      "citeRegEx" : "Hinton et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2015
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Kingma", "Diederik P", "Ba", "Jimmy" ],
      "venue" : "In International Conference on Learning Representations,",
      "citeRegEx" : "Kingma et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kingma et al\\.",
      "year" : 2015
    }, {
      "title" : "Autonomous shaping: Knowledge transfer in reinforcement learning",
      "author" : [ "Konidaris", "George", "Barto", "Andrew G" ],
      "venue" : "In Proceedings of the 23rd international conference on Machine learning,",
      "citeRegEx" : "Konidaris et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Konidaris et al\\.",
      "year" : 2006
    }, {
      "title" : "End-to-end training of deep visuomotor policies",
      "author" : [ "Levine", "Sergey", "Finn", "Chelsea", "Darrell", "Trevor", "Abbeel", "Pieter" ],
      "venue" : "CoRR, abs/1504.00702,",
      "citeRegEx" : "Levine et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Levine et al\\.",
      "year" : 2015
    }, {
      "title" : "Continuous control with deep reinforcement learning",
      "author" : [ "Lillicrap", "Timothy P", "Hunt", "Jonathan J", "Pritzel", "Alexander", "Heess", "Nicholas", "Erez", "Tom", "Tassa", "Yuval", "Silver", "David", "Wierstra", "Daan" ],
      "venue" : "CoRR, abs/1509.02971,",
      "citeRegEx" : "Lillicrap et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Lillicrap et al\\.",
      "year" : 2015
    }, {
      "title" : "Human-level control through deep reinforcement learning",
      "author" : [ "Mnih", "Volodymyr", "Kavukcuoglu", "Koray", "Silver", "David", "Rusu", "Andrei A", "Veness", "Joel", "Bellemare", "Marc G", "Graves", "Alex", "Riedmiller", "Martin", "Fidjeland", "Andreas K", "Ostrovski", "Georg", "Petersen", "Stig", "Beattie", "Charles", "Sadik", "Amir", "Antonoglou", "Ioannis", "King", "Helen", "Kumaran", "Dharshan", "Wierstra", "Daan", "Legg", "Shane", "Hassabis", "Demis" ],
      "venue" : "Nature, 518(7540):529–533,",
      "citeRegEx" : "Mnih et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Mnih et al\\.",
      "year" : 2015
    }, {
      "title" : "Policy invariance under reward transformations: Theory and application to reward shaping",
      "author" : [ "Ng", "Andrew Y", "Harada", "Daishi", "Russell", "Stuart" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Ng et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Ng et al\\.",
      "year" : 1999
    }, {
      "title" : "A convergent form of approximate policy iteration",
      "author" : [ "Perkins", "Theodore J", "Precup", "Doina" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Perkins et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Perkins et al\\.",
      "year" : 2002
    }, {
      "title" : "A stochastic approximation method",
      "author" : [ "Robbins", "Herbert", "Monro", "Sutton" ],
      "venue" : "The annals of mathematical statistics,",
      "citeRegEx" : "Robbins et al\\.,? \\Q1951\\E",
      "shortCiteRegEx" : "Robbins et al\\.",
      "year" : 1951
    }, {
      "title" : "A reduction of imitation learning and structured prediction to no-regret online learning",
      "author" : [ "Ross", "Stephane", "Gordon", "Geoffrey", "Bagnell", "Andrew" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Ross et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Ross et al\\.",
      "year" : 2011
    }, {
      "title" : "Sensitivity analysis, ergodicity coefficients, and rank-one updates for finite markov chains",
      "author" : [ "E. Seneta" ],
      "venue" : "Numerical solution of Markov chains,",
      "citeRegEx" : "Seneta,? \\Q1991\\E",
      "shortCiteRegEx" : "Seneta",
      "year" : 1991
    }, {
      "title" : "Reinforcement learning: An introduction",
      "author" : [ "Sutton", "Richard S", "Barto", "Andrew G" ],
      "venue" : null,
      "citeRegEx" : "Sutton et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Sutton et al\\.",
      "year" : 1998
    }, {
      "title" : "Transfer learning for reinforcement learning domains: A survey",
      "author" : [ "Taylor", "Matthew E", "Stone", "Peter" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Taylor et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Taylor et al\\.",
      "year" : 2009
    }, {
      "title" : "Potential-based shaping and q-value initialization are equivalent",
      "author" : [ "Wiewiora", "Eric" ],
      "venue" : "Journal of Artificial Intelligence Research,",
      "citeRegEx" : "Wiewiora and Eric.,? \\Q2003\\E",
      "shortCiteRegEx" : "Wiewiora and Eric.",
      "year" : 2003
    }, {
      "title" : "architecture, hyperparameters and training procedure",
      "author" : [ "Mnih" ],
      "venue" : null,
      "citeRegEx" : "Mnih,? \\Q2015\\E",
      "shortCiteRegEx" : "Mnih",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : "Deep Reinforcement Learning (DRL), the combination of reinforcement learning methods and deep neural network function approximators, has recently shown considerable success in highdimensional challenging tasks, such as robotic manipulation (Levine et al., 2015; Lillicrap et al., 2015) and arcade games (Mnih et al.",
      "startOffset" : 240,
      "endOffset" : 285
    }, {
      "referenceID" : 8,
      "context" : "Deep Reinforcement Learning (DRL), the combination of reinforcement learning methods and deep neural network function approximators, has recently shown considerable success in highdimensional challenging tasks, such as robotic manipulation (Levine et al., 2015; Lillicrap et al., 2015) and arcade games (Mnih et al.",
      "startOffset" : 240,
      "endOffset" : 285
    }, {
      "referenceID" : 9,
      "context" : ", 2015) and arcade games (Mnih et al., 2015).",
      "startOffset" : 25,
      "endOffset" : 44
    }, {
      "referenceID" : 9,
      "context" : "One such DRL approach, the Deep Q-Network (DQN) (Mnih et al., 2015), has achieved state-ofthe-art results on the Arcade Learning Environment (ALE) (Bellemare et al.",
      "startOffset" : 48,
      "endOffset" : 67
    }, {
      "referenceID" : 2,
      "context" : ", 2015), has achieved state-ofthe-art results on the Arcade Learning Environment (ALE) (Bellemare et al., 2013), a benchmark of Atari 2600 arcade games.",
      "startOffset" : 87,
      "endOffset" : 111
    }, {
      "referenceID" : 9,
      "context" : "Because the state space in the ALE is too large to tractably store a tabular representation of the Q-function, the Deep Q-Network (DQN) approach uses a deep function approximator to represent the state-action value function (Mnih et al., 2015).",
      "startOffset" : 224,
      "endOffset" : 243
    }, {
      "referenceID" : 10,
      "context" : "Reward shaping does not alter the optimal policy of an MDP but can make reaching the optimal policy, or at least a local maximum, easier (Ng et al., 1999).",
      "startOffset" : 137,
      "endOffset" : 154
    }, {
      "referenceID" : 4,
      "context" : "are either the final layer logits (Ba & Caruana, 2014) or the high-temperature softmax outputs of the experts (Hinton et al., 2015).",
      "startOffset" : 110,
      "endOffset" : 131
    }, {
      "referenceID" : 13,
      "context" : "One such method, called DAGGER (Ross et al., 2011), is similar to our approach in that it trains a policy to directly mimic an expert’s behaviour while sampling actions from the mimic agent.",
      "startOffset" : 31,
      "endOffset" : 50
    }, {
      "referenceID" : 4,
      "context" : "are either the final layer logits (Ba & Caruana, 2014) or the high-temperature softmax outputs of the experts (Hinton et al., 2015). Our approach is most similar to the technique of Hinton et al. (2015) which matches the high-temperature outputs of the mimic network with that of the expert network.",
      "startOffset" : 111,
      "endOffset" : 203
    }, {
      "referenceID" : 4,
      "context" : "are either the final layer logits (Ba & Caruana, 2014) or the high-temperature softmax outputs of the experts (Hinton et al., 2015). Our approach is most similar to the technique of Hinton et al. (2015) which matches the high-temperature outputs of the mimic network with that of the expert network. In contrast to these model compression techniques, our method is not concerned with decreasing test time computation but instead using experts to provide otherwise unavailable supervision to a mimic network on several distinct tasks. In addition, we define a novel objective that provides expert guidance at the feature level, instead of only at the output level. The feature regression objective could potentially also be useful in the supervised learning model compression case, but that is out of the scope of this paper. Actor-Mimic can also be considered as part of the larger Imitation Learning class of methods, which use expert guidance to teach an agent how to act. One such method, called DAGGER (Ross et al., 2011), is similar to our approach in that it trains a policy to directly mimic an expert’s behaviour while sampling actions from the mimic agent. Actor-Mimic can be considered as an extension of this work to the multitask case. In addition, using a deep neural network to parameterize the policy provides us with several advantages over the more general Imitation Learning framework. First, we can exploit the automatic feature construction ability of deep networks to transfer knowledge to new tasks, as long as the raw data between tasks is in the same form, i.e. pixel data with the same dimensions. Second, we can define objectives which take into account intermediate representations of the state and not just the policy outputs, for example the feature regression objective. This provides a richer training signal to the mimic network than just samples of the expert’s action output, and could conceivably help prevent the mimic network from overfitting to information on the states it currently visits most frequently. A wide variety of methods have also been studied in the context of RL transfer learning (see Taylor & Stone (2009) for a more comprehensive review).",
      "startOffset" : 111,
      "endOffset" : 2161
    }, {
      "referenceID" : 4,
      "context" : "are either the final layer logits (Ba & Caruana, 2014) or the high-temperature softmax outputs of the experts (Hinton et al., 2015). Our approach is most similar to the technique of Hinton et al. (2015) which matches the high-temperature outputs of the mimic network with that of the expert network. In contrast to these model compression techniques, our method is not concerned with decreasing test time computation but instead using experts to provide otherwise unavailable supervision to a mimic network on several distinct tasks. In addition, we define a novel objective that provides expert guidance at the feature level, instead of only at the output level. The feature regression objective could potentially also be useful in the supervised learning model compression case, but that is out of the scope of this paper. Actor-Mimic can also be considered as part of the larger Imitation Learning class of methods, which use expert guidance to teach an agent how to act. One such method, called DAGGER (Ross et al., 2011), is similar to our approach in that it trains a policy to directly mimic an expert’s behaviour while sampling actions from the mimic agent. Actor-Mimic can be considered as an extension of this work to the multitask case. In addition, using a deep neural network to parameterize the policy provides us with several advantages over the more general Imitation Learning framework. First, we can exploit the automatic feature construction ability of deep networks to transfer knowledge to new tasks, as long as the raw data between tasks is in the same form, i.e. pixel data with the same dimensions. Second, we can define objectives which take into account intermediate representations of the state and not just the policy outputs, for example the feature regression objective. This provides a richer training signal to the mimic network than just samples of the expert’s action output, and could conceivably help prevent the mimic network from overfitting to information on the states it currently visits most frequently. A wide variety of methods have also been studied in the context of RL transfer learning (see Taylor & Stone (2009) for a more comprehensive review). One related approach is to use a dual state representation with a set of task-specific and task-independent features known as “problem-space” and “agent-space” descriptors, respectively. For each source task, a task-specific value function is learnt on the problem-space descriptors and then these learnt value functions are transferred to a single value function over the agent-space descriptors. Because the agent-space value function is defined over features which maintain constant semantics across all tasks, this value function can be directly transferred to new tasks. Banerjee & Stone (2007) constructed agent-space features by first generating a fixed-depth game tree of the current state, classifying each future state in the tree as either {win, lose, draw, nonterminal} and then coalescing all states which have the same class or subtree.",
      "startOffset" : 111,
      "endOffset" : 2795
    }, {
      "referenceID" : 4,
      "context" : "are either the final layer logits (Ba & Caruana, 2014) or the high-temperature softmax outputs of the experts (Hinton et al., 2015). Our approach is most similar to the technique of Hinton et al. (2015) which matches the high-temperature outputs of the mimic network with that of the expert network. In contrast to these model compression techniques, our method is not concerned with decreasing test time computation but instead using experts to provide otherwise unavailable supervision to a mimic network on several distinct tasks. In addition, we define a novel objective that provides expert guidance at the feature level, instead of only at the output level. The feature regression objective could potentially also be useful in the supervised learning model compression case, but that is out of the scope of this paper. Actor-Mimic can also be considered as part of the larger Imitation Learning class of methods, which use expert guidance to teach an agent how to act. One such method, called DAGGER (Ross et al., 2011), is similar to our approach in that it trains a policy to directly mimic an expert’s behaviour while sampling actions from the mimic agent. Actor-Mimic can be considered as an extension of this work to the multitask case. In addition, using a deep neural network to parameterize the policy provides us with several advantages over the more general Imitation Learning framework. First, we can exploit the automatic feature construction ability of deep networks to transfer knowledge to new tasks, as long as the raw data between tasks is in the same form, i.e. pixel data with the same dimensions. Second, we can define objectives which take into account intermediate representations of the state and not just the policy outputs, for example the feature regression objective. This provides a richer training signal to the mimic network than just samples of the expert’s action output, and could conceivably help prevent the mimic network from overfitting to information on the states it currently visits most frequently. A wide variety of methods have also been studied in the context of RL transfer learning (see Taylor & Stone (2009) for a more comprehensive review). One related approach is to use a dual state representation with a set of task-specific and task-independent features known as “problem-space” and “agent-space” descriptors, respectively. For each source task, a task-specific value function is learnt on the problem-space descriptors and then these learnt value functions are transferred to a single value function over the agent-space descriptors. Because the agent-space value function is defined over features which maintain constant semantics across all tasks, this value function can be directly transferred to new tasks. Banerjee & Stone (2007) constructed agent-space features by first generating a fixed-depth game tree of the current state, classifying each future state in the tree as either {win, lose, draw, nonterminal} and then coalescing all states which have the same class or subtree. To transfer the source tasks value functions to agent-space, they use a simple weighted average of the source task value functions, where the weight is proportional to the number of times that a specific agent-space descriptor has been seen during play in that source task. In another related method, Konidaris & Barto (2006) transfer the value function to agent-space by using regression to predict every source tasks problem-space value function from the agent-space descriptors.",
      "startOffset" : 111,
      "endOffset" : 3372
    } ],
    "year" : 2017,
    "abstractText" : "The ability to act in multiple environments and transfer previous knowledge to new situations can be considered a critical aspect of any intelligent agent. Towards this goal, we define a novel method of multitask and transfer learning that enables an autonomous agent to learn how to behave in multiple tasks simultaneously, and then generalize its knowledge to new domains. This method, termed “ActorMimic”, exploits the use of deep reinforcement learning and model compression techniques to train a single policy network that learns how to act in a set of distinct tasks by using the guidance of several expert teachers. We then show that the representations learnt by the deep policy network are capable of generalizing to new tasks, speeding up learning in novel environments. Although our method can in general be applied to a wide range of problems, we use Atari games as a testing environment to demonstrate these methods.",
    "creator" : "LaTeX with hyperref package"
  }
}