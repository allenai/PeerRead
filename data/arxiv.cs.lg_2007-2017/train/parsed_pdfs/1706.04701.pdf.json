{
  "name" : "1706.04701.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Adversarial Example Defenses: Ensembles of Weak Defenses are not Strong",
    "authors" : [ "Warren He", "James Wei", "Xinyun Chen", "Nicholas Carlini", "Dawn Song" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Neural networks have achieved great performance on a wide range of application domains; in particular, they have demonstrated accuracies comparable or better than humans on datasets in the fields of image recognition [10] and speech recognition [29]. However, recent work shows that deep learning models are susceptible to adversarial examples: inputs that are similar to a correctly classified input, but which are misclassified [28]. Research on other applications of neural networks has also encountered adversarial examples on different tasks beyond image classification, including deep policies in reinforcement learning and generative models [15, 14, 12, 2]. Adversarial examples pose serious threat in particular in security-critical autonomous systems such as self-driving cars. Recent work has shown that adversarial examples remain even when subject to the lossy channel of being photographed [17].\nDeveloping effective defenses against adversarial examples is an important topic. Despite many attempts [27,\n7, 9, 21, 8, 6, 5, 11], there is no strong defense against adversarial examples to date.\nIn this paper, we ask the question: if we ensemble multiple defenses to adversarial examples, will the combined defense be significantly stronger than the original individual defense? If it did, then one possible approach to constructing a robust defense to adversarial examples would be to join together many defenses, each of which independently is only slightly effective, but together are strong. This is an important question for designing effective defense against adversarial examples. To the best of our knowledge, we are the first to systematically investigate this question.\nTowards answering this question, we study three instances of ensembled defenses. First, we study two recently proposed defenses, feature squeezing [30] and the specialists+1 ensemble method [1], each of which ensembles multiple defenses that compensate for each other’s weaknesses. Note that feature squeezing and the specialists+1 ensemble are explicitly designed to combine component defenses that work well together, with the intention of creating a stronger defense.\nTo study the question of ensemble defense in a broader scope, we also evaluate an ensemble of three independent, mutually compatible detection mechanisms [6, 21, 5]. This represents an approach that combines defenses which were not designed to be used together.\nThe works that introduce these defenses showed that they are effective at detecting attacks generated for the classifier models to which they are applied. However, we find that neither the components of these defenses nor the combined defenses are effective against an attacker that is aware of the defense.\nWe evaluate these defenses with new attacks, specific to the defenses. Our attacks are able to defeat all aforementioned defenses with low distortion. From this, we conclude that combining weak defenses, even ones seem to work well together, is insufficient for defending against an adaptive attacker.\nar X\niv :1\n70 6.\n04 70\n1v 1\n[ cs\n.L G\n] 1\n5 Ju\nn 20\nContributions We make the following contributions:\n• We create effective attacks on feature squeezing [30], including individual squeezing methods and the combined adversarial example detection scheme.\n• We create an effective attack on the ensemble-ofspecialists defense [1].\n• We create effective attacks on an ensemble of recently proposed detectors. We show that adversarial examples can bypass an ensemble of detectors with nearly as little distortion as needed for the strongest individual detector.\n• Our results show that ensembled defenses do not provide significantly more resilience against adversarial examples than each individual component included in the ensemble. This implies that an ensemble of weak defenses is not sufficient to provide a strong defense against adversarial examples.\n• Our evaluation demonstrates that adaptive adversarial examples transfer across several defense or detection proposals. This phenomenon may provide one reason to explain why ensembling is not an effective approach to building defense mechanisms against adversarial examples.\nThe rest of this paper is organized as follows: in Section 2, we provide the problem statement and background information; in Section 3, we describe our attacks on individual feature squeezing defense component and their composite defense; in Section 4, we describe our attack results on the ensemble-of-specialists defense; in Section 5, we describe our attack results on a composite defense that combines multiple independently proposed detection networks; and we summarize our findings in Section 6."
    }, {
      "heading" : "2 Overview",
      "text" : "We start with an overview of background information, then we define the threat models we use and the problem statement and setup for our experiments."
    }, {
      "heading" : "2.1 Background: Adversarial Examples",
      "text" : "Recent works have pointed out that deep learning models are vulnerable to adversarial examples: these models give incorrect predictions on inputs that are slightly different from those correctly predicted ones [28, 7, 23, 26].\nSpecifically, suppose we have a classifier F with model parameters θ . Let x be an input to the classifier\nwith corresponding ground truth prediction y. An adversarial example x∗ is some instance in the input space that is close to x by some distance metric d(x,x∗), but causes Fθ to produce an incorrect output. Here we only consider those x satisfying Fθ (x) = y.\nPrior work considers two classes of adversarial examples. First, an untargeted adversarial example is an instance x∗ that causes the classifier to produce any incorrect output: Fθ (x∗) 6= y. Second, a targeted adversarial example is an x∗ that causes the classifier to produce a specific incorrect output y∗: Fθ (x∗) = y∗ where y 6= y∗.\nSeveral approaches have been proposed in previous work, including the Fast Gradient Sign Method (FGSM) [7], Fast Gradient Method [19], Jacobian-based Saliency Map Approach (JSMA) [26], Deepfool [22], and optimization-based methods [28, 3, 19]."
    }, {
      "heading" : "2.2 Threat Models",
      "text" : "In this work, we assume the adversary has full knowledge of the model, including the model architecture, parameters, and the defense strategies used in the model. Prior work has shown this assumption can often be relaxed [7, 24, 25], however for simplicity we assume this stronger threat model.\nWithin these white-box attackers, we consider two capacities of adversaries.\nStatic Adversary. A static adversary is an attacker that is not aware of any defenses that may be in place to protect the model against adversarial examples. A static adversary can generate adversarial examples using existing methods but does not tailor attacks to any specific defense.\nAdaptive Adversary. An adaptive adversary is an attacker that is aware of the defense methods used in the model, and can adapt attacks accordingly. This is a strictly more powerful adversary than static adversary. In this paper, we consider this stronger adversary."
    }, {
      "heading" : "2.3 Problem Statement",
      "text" : "To improve the robustness of models against adversarial examples, prior work investigates into two directions. The first direction attempts to produce correct predictions on adversarial examples, while not compromising the accuracy on legitimate inputs [27, 7, 9]. The other (more recent) direction instead attempts to detect adversarial examples, without introducing too many false positives. In this case, the model can reject an instance and refuse to classify those that it detects as adversarial [21, 8, 30, 1].\nIn this paper, we ask the question: if we ensemble multiple defenses to adversarial examples, then will the combined defense be significantly stronger than each individual original defense? If it did, then one possible approach to constructing a robust defense to adversarial examples would be to join together many defenses, each of which independently is only slightly effective, but together are strong. This is an important question for designing effective defense against adversarial examples. To the best of our knowledge, we are the first to systematically investigate this question.\nDefenses considered. In this paper, we consider defenses that attempt to combine multiple (somewhat weaker) defenses to construct a larger strong defense. In particular, we look at three instances of ensemble defense strategies. First and second are feature squeezing [30] and the specialists+1 ensemble method [1], both of which take this approach by construction. These defenses are constructed from components that are intended to be useful together. Their authors have shown that these defenses effectively detect low-perturbation adversarial examples generated by a static adversary. Third, to study the effectiveness of ensembling defenses more broadly, we merge together many detectors that were not designed to be used in conjunction with any other detector. In particular, as an example demonstration, we ensemble three independent detection mechanisms [6, 21, 5] to build one detection mechanism.\nFor each of these defense strategies, we propose attack methods to generate adversarial examples as an adaptive adversary against the individual component defense (when applicable) as well as the composite defense strategy. We use these attack methods to evaluate each component defense and composite defense: if our method succeeds at generating adversarial examples, this means that an adaptive adversary can defeat the defense. To gauge how strong the combined defense is compared to the components, we compare the level of distortion needed to fool each (using the same optimization method)."
    }, {
      "heading" : "2.4 Experimental Setup",
      "text" : "Datasets and models. To evaluate the effectiveness of the different defense strategies, we use two standard datasets, MNIST [18] and CIFAR-10 [16] datasets.\nFor both datasets, we randomly sample 100 images in the test set, filter out examples that are not correctly classified, and generate adversarial examples based on the correctly classified images. When evaluating each defense strategy, we use the same model architectures described in their papers respectively [30, 1, 6, 21, 5].\nAdversarial examples on MNIST tend to have higher distortion than natural images.\nAdversarial example generation method. In this paper, we use an optimization-based approach to generate adversarial examples [3], which is shown to be effective on finding adversarial examples with small distortions.\nAt a high level, the attack uses an optimizer to minimize a loss function:\nloss(x′) = ‖x′− x‖22 + c · J(Fθ (x′),y) Here, Fθ is a part of the trained classifier that outputs a vector of logits, and J computes some penalty based on the logits and some label y, either a ground truth label for non-targeted attacks or a target label for targeted attacks. A constant c is a hyperparameter that adjusts the relative weighting between distortion and misclassification. We omit details of the design choice and refer the reader to the original paper [3].\nMeasurement of distortion. Unless otherwise specified, we measure the distortion between an adversarial example and the original input as the L2-norm of their distance. Formally, d(x∗,x) = √ ∑i(x∗i − xi)2. Each dimension of input images is scaled to [0,1], i.e., 0 ≤ x∗i ,xi ≤ 1."
    }, {
      "heading" : "3 Adaptive attacks on feature squeezing",
      "text" : "In this and next section, we investigate ensemble defense strategies that are intentionally constructed to have component defenses which work together to detect adversarial examples. The first defense we study is feature squeezing, proposed by Xu et al. [30, 31].\nBackground: feature squeezing defense. To perform feature squeezing, one generates a lower fidelity version of the input image through a process known as “squeezing” before passing it into the classifier. Xu et al. proposed two methods of squeezing: reducing the color depth to fewer bits, and spatially smoothing the pixels with a median filter. According to their paper, the two methods of squeezing work well together because they address two major kinds of perturbation used in adversarial examples: color depth reduction eliminates small changes to many pixels, while spatial smoothing eliminates large changes to a few pixels.\nIn order to detect adversarial examples, Xu et al. propose a system combining the two squeezing methods. First, the system runs the classifier on three different versions of the image: the original image, the reduced-colordepth version and the spatially smoothed version of the\noriginal image. Then, it compares the softmax probability vectors across these three classifier outputs. The L1 score of the input is the highest L1 distance between any pair of softmax probability vectors among the three. It flags inputs where the L1 score exceeds a threshold as adversarial.\nIn their experiments, Xu et al. show that MNIST and CIFAR-10 classifiers are accurate on squeezed inputs. On adversarial examples generated by a static adversary using FGSM [7] and JSMA [26], they show that their detector achieves 99.74% accuracy on a test set with equal portions benign and adversarial examples. They also show that squeezing the input alone prevents 84 – 100% of the adversarial examples (correctly classifying them). Recently, Xu et al. showed that a simplified detector that uses the original version of the input and the spatially-smoothed version (excluding the color-depthreduced version) achieves a 98.80% overall detection accuracy on MNIST and 87.50% on CIFAR-10 against a static adversary using a variety of Carlini and Wagner’s attacks [31].\nSummary of our approach and results. We demonstrate that feature squeezing is not an effective defense in two stages. First, we show that an adaptive attacker can construct an adversarial example that remains adversarial after it is squeezed by each method (color depth reduction and spatial smoothing. Then, we use this approach to construct adversarial examples that are classified the same way both with and without squeezing, causing the L1 score to be smaller than a given fixed threshold. Our results show that the combined detection method is not effective against an adaptive attacker."
    }, {
      "heading" : "3.1 Evading individual feature squeezing defense components",
      "text" : "In these experiments, we evaluate whether adversarial examples are robust to each individual feature squeezing defense component, i.e., whether adversarial examples remain adversarial after each individual feature squeezing process (color depth reduction and spatial smoothing) separately. These experiments attack the components of the combined feature squeezing detection scheme. Performing this attack is necessary for defeating the combined detection scheme, wherein the predicted label probabilities of squeezed images are compared against each other."
    }, {
      "heading" : "3.1.1 Evading color-depth-reduction defense",
      "text" : "The first method of squeezing an image that Xu et al. propose is color depth reduction. This method rounds each\nvalue in the input to 2b evenly spaced values spanning the same range, which we refer to as reducing to b bits.\nAttack Approach. We use the method described in Section 2.4 to generate adversarial examples that are robust to color depth reduction. After each step of the optimization procedure, an intermediate image (perturbed from the original image) is available from the optimizer. We check if a reduced-color-depth version of this intermediate image is adversarial. We run the optimization multiple times, initializing the optimization with random perturbations of the original image each time, so that it explores different optimization paths. For each original image, we keep the successful adversarial example that has the lowest L2 distance to the original image among all the generated successful adversarial examples for this original image.\nAlthough this approach successfully generated lowdistortion adversarial examples in our experiments, there is no guarantee that it should succeed in the general case. We present an alternative approach in Appendix A which produced examples with higher distortion, but which may be useful in other scenarios.\nAttack results on MNIST. We evaluate color depth reduction to 1 – 7 bits. On the strongest defense evaluated by Xu et al., which reduces color depth to 1 bit, we successfully generated adversarial examples for all original images, with an average distortion of 3.86. Figure 1 shows a sample of these adversarial examples. Table 1 summarizes our results for other bit depths. Notice that for a system without any color depth reduction (retaining the original 8 bits of depth), we find adversarial examples with an average distortion of 1.38. Reducing color depth to fewer bits makes the system less sensitive to small changes, which requires larger distortions; however, the distortions are still very small.\nAttack results on CIFAR-10. We evaluate color depth reduction to 3 bits, which Xu et al. recommend as a good\nbalance between the accuracy on adversarial inputs and accuracy on benign images for CIFAR-10. We succeeded at generating adversarial examples for all original images, with an average distortion of 0.945. Figure 2 shows a sample of these adversarial examples. For comparison, adversarial examples for a classifier without color depth reduction have an average distortion of 0.214. Although this method of squeezing increases the distortion needed for successfully generating non-targeted adversarial examples using the same optimization method, again, such a distortion is still small and imperceptible.\nSummary. An adaptive attacker can successfully generate adversarial examples with small distortions for a system that applies color depth reduction to the input image before classifying it."
    }, {
      "heading" : "3.1.2 Evading spatial smoothing",
      "text" : "Xu et al. propose a second method for feature squeezing, which applies a median filter to the input, which replaces each pixel with the median value of a neighborhood around the pixel.\nTo generate adversarial examples that are misclassified after spatial smoothing, we use the procedure from Section 2.4 with the addition of a median filter as part of the classification model. A median filter for TensorFlow was not available, so we implemented our own.\nAttack results on MNIST. We evaluate a range of median filter sizes, ranging from 1×2 to 5×5. For a 3×3 filter, with which Xu et al. achieved the best accuracy, we successfully generated adversarial examples for all original images, with an average distortion of 1.29. Figure 3 shows a sample of these adversarial examples. Table 2 summarizes our results for other filter sizes. Larger median filters did not require greater distortion. Compared to adversarial examples generated for a system without any spatial smoothing (average distortion of 1.38), the average distortion is not increased.\nAttack results on CIFAR-10 We evaluate a 2×2 median filter, which Xu et al. identify as achieving a good rejection rate of adversarial examples and accuracy on benign images on CIFAR-10. We successfully generated adversarial examples for all original images, which have\nan average distortion of 0.205. Figure 4 shows a sample of these adversarial examples. The average distortion is not higher than for a system without spatial smoothing (0.214).\nSummary Spatial smoothing alone is not an effective defense against an adaptive attacker. We have shown that an adaptive adversary can create adversarial examples for a system that applies spatial smoothing which are not more distorted than adversarial examples for a baseline system that does not apply spatial smoothing."
    }, {
      "heading" : "3.2 Evading combination of multiple squeezing techniques",
      "text" : "While the individual feature squeezing techniques are weak against an adaptive attacker, we investigate whether the detection scheme that combines them is stronger. In this case, we find that this detection scheme is not much stronger than the strongest component defense, color depth reduction.\nBackground: Composite feature squeezing defense The detection scheme combines both methods of squeezing. In particular, the detection system has three “branches,” where each one runs the classifier on a different version of the input, the original input, a reducedcolor-depth version and a spatially-smoothed version of the original input. These three branches output different softmax probability vectors, and the scheme compares the L1 distance between these vectors against a threshold to determine whether the input is adversarial.\nIn this experiment, we evaluate the effectiveness of the combined defense against an adaptive attacker, i.e., whether adversarial examples can be misclassified and bypass this detection scheme.\nAttack Approach. We use the procedure from Section 3.1.1 with modifications to support the detection mechanism. In this attack, we run two instances of the\nmodel in parallel during optimization, representing the detector branches that operate on the input and the spatially smoothed version of the input. The optimizer receives gradients from both models, equally. We do not include the reduced-color-depth branch in the gradient calculation, because the color depth reduction removes gradient information; it is, however, included when we compute the L1 score. We collect only adversarial examples that have an L1 score below a threshold of 0.3076, a level at which Xu et al. achieved the best accuracy in their experiments on MNIST.\nAttack results on MNIST We evaluate a combination of color depth reduction to 1 bit and smoothing with a 2× 2 median filter, which Xu et al. found to be accurate on adversarial examples generated by a static adversary [31]. We successfully generated adversarial examples for all original images, with an average distortion of 4.76 and L1 score of 0.209. Figure 5 shows a sample of these adversarial examples. These examples are misclassified and successfully evade detection. This distortion is 23.3% larger than for color depth reduction alone, but still very small.\nAttack results on CIFAR-10. We evaluate a combination of color depth reduction to 3 bits and smoothing with a 2×2 median filter, a combination of settings that perform well in Xu et al.’s experiments. We successfully generated adversarial examples for all original images, with an average distortion of 0.601 and L1 score of 0.168. Figure 6 shows a sample of these adversarial examples. These examples are misclassified and successfully evade detection. This distortion is even lower than that of the color depth reduction defense alone. Although Xu et al. do not prescribe a threshold specific to CIFAR-10, the average L1 score for these examples is lower (i.e., detected as less adversarial) than the average L1 score for the original images, which is 0.225.\nSummary. The detection scheme that combines two methods of squeezing is not always stronger than the strongest component, color depth reduction. The improvement is low even on MNIST, which is particularly well suited for feature squeezing, with images being black and white (little change from color depth reduction) and having large, contiguous areas of the same color (little change from spatial smoothing). On CIFAR10, the combined attack requires less distortion than the color depth reduction defense alone."
    }, {
      "heading" : "4 Evading ensemble of specialists",
      "text" : "We study a second defense that combines multiple component defenses, an ensemble of specialists, proposed by Abbasi and Gagné [1].\nBackground: ensemble of specialist defense. The defense consists of a generalist classifier (which classifies among all classes) and a collection of specialists (which classify among subsets of the classes). The specialists classify subsets of the classes as follows. Where C is the set of all K classes in the task, for each class i, let Ui be the set of classes with which i is most often confused in adversarial examples. To compute Ui, Abbasi and Gagné select the top 80% of misclassifications caused by non-\ntargeted FGSM attacks for each class i. Further, K additional subsets are defined: UK+i = C \\Ui to be the complement set of Ui. For each j = 1, ...,2K, a specialist classifier Fj is trained on a subset of the dataset containing images belonging to the classes in U j to classify input images into the classes in U j only. In addition, a generalist classifier F2K+1 is trained to classify input images into classes in U2K+1 = C. Each classifier in the ensemble may be susceptible to basic adversarial examples, but the proposed defense assumes that each specialist can detect a few specific attacks, thus the attacker cannot fool all specialists and the generalist at the same time. The defense combines them to jointly detect general adversarial examples.\nIn order to classify an input, the system first checks if, for any class i, the generalist classifier and all specialists that can classify i agree that the input belongs to class i. If such a class i exists, note that at most one class can get the generalist’s vote, it must be unique. In this case, the system outputs i as the class. Otherwise, at least one classifier has misclassified the input, and the prediction follows the majority vote from all classifiers in the ensemble.\nAbbasi and Gagné [1] find that using an ensemble constructed this way successfully reduces the system’s confidence (fraction of voting classifiers that voted for the winning class) on adversarial examples generated by a static attacker using FGSM [7], DeepFool [22], and Szegedy et al.’s approach [28]. They conclude that a classification system can use an ensemble of diverse specialists this way and detect low-confidence examples as adversarial.\nAttack approach. In this experiment, we evaluate the effectiveness of Abbasi and Gagné’s specialists+1 ensemble against an adaptive attacker. We considered a scenario where a user provides an image to a system, and the system uses a specialists+1 ensemble to classify the image or reject it as adversarial.\nWe attempt to create targeted adversarial examples, where we chose target classes randomly. For each original image, then our goal is to create an adversarial example that is classified as the target class by the generalist classifier and all applicable specialists at the same time, resulting in the maximum possible confidence that the image is not adversarial. We use the procedure from Section 2.4 to generate adversarial examples. We modified the loss function to support multiple classifiers:\nloss(x′) = ‖x′− x‖22 + c∑ j∈{1,...,2K+1};y∗∈U j J(Fj(x ′),y∗)\nWe evaluate this defense on MNIST only. While Abbasi and Gagné also propose the defense for CIFAR-10,\nthe architecture described in their experiments have low accuracy, resulting in low confidence even in benign images.\nAttack results on MNIST. We successfully generated adversarial examples for all original images, which have an average distortion of 2.50. Figure 7 shows a sample of these adversarial examples in the second row. These adversarial examples are classified as the target label by the generalist and all applicable specialists, giving them the highest confidence possible. The distortion needed is 81.2% higher than for a non-ensemble MNIST classifier.\nAlthough this defense defines the specialists to focus on common misclassifications caused by non-targeted adversarial examples, it is still weaker at detecting the common misclassifications. Among the examples, 33 targeted a class that the original image’s ground truth class was commonly confused with. The average distortion for these images is 1.86, below the average of the entire set.\nSummary. The specialists+1 ensemble does not effectively ensure low confidence on adversarial examples generated by an adaptive attacker. An adaptive attacker can successfully generate adversarial examples with small distortions, which are unanimously classified as a target class, and thus evade the detection of the specialist+1 ensemble defense."
    }, {
      "heading" : "5 Evading ensemble of detectors",
      "text" : "In the previous sections, we have investigated ensembles of defenses that are intentionally constructed to be useful together. In Xu et al.’s work, the color depth reduction is intended to remove small changes to many pixels, and the median smoothing to remove large changes to a few pixels. Similarly, Abbasi and Gagné propose using an ensemble of generalist and specialist classifiers together; without the others, this approach would not be useful.\nTo study the effectiveness of ensembling defenses more broadly, we merge together a number of recently proposed detectors that were not designed to be used in conjunction with any other detector. We consider only detectors that are applied to a fixed classification network for simplicity, and therefore study the following schemes:\n• Gong et al. propose using adversarial training to detect adversarial examples [6]. Given the original model, generate adversarial examples on the training data. Then, train a new classifier that distinguishes the original training data from the adversarial data.\n• Metzen et al. construct a similar scheme, however instead of using the original images as the input to the detector, they train on the inner convolutional layers of the network [21].\n• Feinman et al. examine the final hidden layer of a neural network and find that adversarial examples are separable from the original images by training a density estimate using Gaussian kernels [5].\nWhen using Carlini and Wagner’s attack, these approaches are known to provide only slight increases in robustness, i.e., only increase the required distortion slightly when generating the adversarial examples with the detector vs. without the detector [4]. Given this, we now examine if constructing an ensemble of these defenses provides additional robustness. To ensemble these defenses, we run each detection method and report the input as adversarial if any of the three detectors do.\nAttack approach. We perform this experiment on CIFAR-10 exclusively, as Metzen et al.’s defense is intended for CIFAR-10. We are able to construct adversarial examples for all defenses independently. To defeat all three defenses together, we construct a new classifier G(·) so that using the loss function from Section 2.4 directly can construct adversarial examples.\nWe use the same notation as [4]. Let F(·) be a classifier on N classes, and softmax(F(·))i be the probability of class i (so that F(·)i are the logits). Let {D j(x)}Jj=1 be one of J different detectors so that the probability that detector D j reports object x as adversarial is sigmoid(D j(x)) (that is, D j returns the logits). We report that an instance is an adversarial example if the probability of any detector is greater than one half. That is, if for any j, sigmoid(D j(x))> 12 , or, alternatively, D j(x)> 0.\nWhen we ensemble the three defenses, we set J = 3 and define D(x) = max j D j(x), so that D(x) reports adversarial (i.e., D(x)> 0) if any of the three detectors do.\nGiven this, we use the same G(·) construction as Carlini and Wagner’s previous work on these de-\nfenses [4]. This function G(·) returns N+1 classes (with the new class reserved for adversarial examples) so that arg maxi G(x)i = arg maxi F(x)i when x is not adversarial, and arg maxi G(x) = N +1 when x is adversarial. To do this, [4] specifically defines\nG(x)i =    F(x)i if i≤ N (D(x)+1) ·max\nj F(x) j if i = N +1\nIf for a given instance x, D j(x) > 0 (for any classifier j) then we will have arg maxi G(x)i = N + 1 since we multiply a value greater than one by the largest of the other output logits. Conversely, if arg maxi G(x)i 6=N+1 then we must have D(x) < 0 implying that all detectors report the instance is benign.\nTherefore, by constructing adversarial examples on G so that the target class is not N + 1, we can construct adversarial examples on F that are not detected by any detector.\nAttack results on CIFAR-10 The L2 distortion required to construct adversarial examples on an unsecured network is 0.11. To construct adversarial examples on this network G(·) with the three defenses increases the distortion to 0.18, an increase of 60%. However, this distortion is still imperceptible.\nTransferability of adversarial examples across different detectors. In order to understand the reason that these defenses do not significantly increase robustness when combined together, we hypothesize that the transferability property [28, 7, 24, 19] of adversarial examples is simplifying the attacker’s task. To verify this, we construct adversarial examples on each of the three defenses in isolation and check the probability that these examples also fool the other two defenses. Table 3 contains this data. From this, we can see why constructing an ensemble of these weak defenses is not significantly more secure than each independently: the adversarial examples that fool one detector also often fool the other detectors."
    }, {
      "heading" : "6 Conclusion",
      "text" : "Our goal in this paper is to examine whether multiple (possibly weak) defenses can be combined to create a strong defense. Towards this goal, we studied three such defenses that combined multiple defense components: two recently proposed defenses designed with rationale of why their components should work well together and one that combined unrelated recently proposed detectors.\nWe showed that an adaptive adversary can generate adversarial examples with low distortion that fool all of the\ndefenses and components that we evaluate. The feature squeezing detection scheme, which combines two methods of squeezing an input image, is at best marginally stronger than color depth reduction alone. The specialists+1 ensemble, which combines several specialist classifiers, increases the required distortion slightly, but again, distortion is still small. We also showed that combining a collection of recently proposed detection mechanisms is also ineffective. In particular, our results show that adversarial examples transfer across the individual detectors.\nOur work sheds light on a few important lessons when evaluating defenses against adversarial examples: 1) one should evaluate defenses using strong attacks. For example, FGSM can quickly generate adversarial examples, but may fail to generate successful attacks when other iterative optimization based methods can succeed; 2) one should evaluate defenses using adaptive adversaries. It is important to develop defenses that are secure against attackers who know the defense mechanisms being used.\nFinally, our results indicate that combining weak defenses does not significantly improve the robustness of these systems. Developing effective defenses against adversarial examples is an important topic. We hope our work sheds light for future work in this area."
    }, {
      "heading" : "7 Acknowledgements",
      "text" : "We thank Arjun Bhagoji, Chang Liu, and Richard Shin, who have provided valuable feedback. This work was supported in part by BDD (Berkeley Deep Drive); the CLTC (Center for Long-Term Cybersecurity); FORCES (Foundations Of Resilient CybEr-Physical Systems), which receives support from the National Science Foundation (NSF award numbers CNS-1238959, CNS1238962, CNS-1239054, CNS-1239166); and the National Science Foundation under Grant No. TWC1409915. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation. Cloud computing resources were provided through a Microsoft Azure for Research award."
    }, {
      "heading" : "A Gumbel-Softmax reparameterization",
      "text" : "Defenses that mask a network’s gradients by quantizing the input values pose a challenge to gradient-based optimization methods for generating adversarial examples, such as the procedure we describe in Section 2.4. A straightforward application of the approach would find zero gradients, because small changes to the input do not alter the output at all. In Section 3.1.1, we describe an approach where we run the optimizer on a substitute network without the color depth reduction step, which approximates the real network. We rely on the optimizer stumbling across images that happen to be misclassified after the color depth reduction would take place. This worked in our experiments.\nIn this section, we describe an alternative approach that allows us to use the same optimizer (Adam) to search over the space of quantized images, by using GumbelSoftmax reparameterization [13, 20].\nBackground: Gumbel-Softmax reparameterization. In short, the Gumbel-Softmax reparameterization trick changes a discrete optimization problem into a problem of optimizing distributions. In the reparameterized problem, the input image is created by sampling discretevalued samples from distributions (one per channel, per\npixel). Each distribution is represented by a set of continuous-valued parameters. During backpropagation, gradients arriving at the integer-valued samples affect the parameters representing the distribution from which the samples were drawn, pushing the distribution toward an optimal value.\nAttack approach. We set up a network to sample a discrete-valued image, acting as a color-depth-reduced input, connected to the input of a classifier. The network samples the discrete-valued image from GumbelSoftmax distributions (one distribution for each channel of each pixel). We the approach from Section 2.4, but optimizing the distributions instead of optimizing the image directly. At each step of the optimization, we check if the sampled image is misclassified, and we retain the one closest to the original image.\nAttack results on MNIST. We evaluate color depth reduction to 1 – 7 bits. For reduction to 1 bit (the strongest color depth reduction evaluated), we successfully generated adversarial examples for all original images, with an average L2 distortion of 7.48. For comparison, simply rounding the original images to 1-bit color depth results in an average L2 distortion of 2.56. Table 4 summarizes our results for other bit depths. Again, squeezing to fewer bits increases the average distortion.\nCIFAR-10 We evaluated color depth reduction to 3 bits (a good balance between accuracy on adversarial inputs and on benign images). We succeeded at generating adversarial examples for all original images, with an average L2 distortion of 2.32. For comparison, simply rounding the original images to 3-bit color results in an average distortion of 2.27.\nSummary The reparameterized optimization succeeded at generating adversarial images, but the procedure did not produce adversarial images with the lowest distance to original (unquantized) images. This is, in part, because it always results in an image that is already quantized, which is necessarily at least some distance away from a high-color-depth original image. However, this approach may be useful in attacks on other defenses that try to obfuscate gradients through quantization. We find that the distance to the unquantized images is not much larger than the minimal distance between the original image and its normally quantized version."
    } ],
    "references" : [ {
      "title" : "Robustness to adversarial examples through an ensemble of specialists",
      "author" : [ "M. ABBASI", "C. GAGNÉ" ],
      "venue" : "arXiv preprint arXiv:1702.06856",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2017
    }, {
      "title" : "Vulnerability of deep reinforcement learning to policy induction attacks",
      "author" : [ "V. BEHZADAN", "A. MUNIR" ],
      "venue" : "arXiv preprint arXiv:1701.04143",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2017
    }, {
      "title" : "Towards evaluating the robustness of neural networks",
      "author" : [ "N. CARLINI", "D. WAGNER" ],
      "venue" : "arXiv preprint arXiv:1608.04644",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2016
    }, {
      "title" : "Adversarial examples are not easily detected: Bypassing ten detection methods",
      "author" : [ "N. CARLINI", "D. WAGNER" ],
      "venue" : "arXiv preprint arXiv:1705.07263",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2017
    }, {
      "title" : "Detecting adversarial samples from artifacts",
      "author" : [ "R. FEINMAN", "R.R. CURTIN", "S. SHINTRE", "A.B. GARD- NER" ],
      "venue" : "arXiv preprint arXiv:1703.00410",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2017
    }, {
      "title" : "Adversarial and clean data are not twins",
      "author" : [ "Z. GONG", "W. WANG", "KU", "W.-S" ],
      "venue" : "arXiv preprint arXiv:1704.04960",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2017
    }, {
      "title" : "Explaining and harnessing adversarial examples",
      "author" : [ "I.J. GOODFELLOW", "J. SHLENS", "C. SZEGEDY" ],
      "venue" : "arXiv preprint arXiv:1412.6572",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2014
    }, {
      "title" : "On the (statistical) detection of adversarial examples",
      "author" : [ "K. GROSSE", "P. MANOHARAN", "N. PAPERNOT", "M. BACKES", "P. MCDANIEL" ],
      "venue" : null,
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2017
    }, {
      "title" : "Towards deep neural network architectures robust to adversarial examples",
      "author" : [ "S. GU", "L. RIGAZIO" ],
      "venue" : "arXiv preprint arXiv:1412.5068",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2014
    }, {
      "title" : "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification",
      "author" : [ "HE K", "ZHANG X", "REN S", "SUN" ],
      "venue" : "In Proceedings of the IEEE international conference on computer vision",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2015
    }, {
      "title" : "Early methods for detecting adversarial images",
      "author" : [ "D. HENDRYCKS", "K. GIMPEL" ],
      "venue" : "International Conference on Learning Representations (Workshop Track)",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2017
    }, {
      "title" : "Adversarial attacks on neural network policies",
      "author" : [ "S. HUANG", "N. PAPERNOT", "I. GOODFELLOW", "Y. DUAN", "P. ABBEEL" ],
      "venue" : "arXiv preprint arXiv:1702.02284",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2017
    }, {
      "title" : "Categorical reparameterization with gumbel-softmax",
      "author" : [ "E. JANG", "S. GU", "B. POOLE" ],
      "venue" : "arXiv preprint arXiv:1611.01144",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2016
    }, {
      "title" : "Adversarial examples for generative models",
      "author" : [ "J. KOS", "I. FISCHER", "D. SONG" ],
      "venue" : "arXiv preprint arXiv:1702.06832",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2017
    }, {
      "title" : "Delving into adversarial attacks on deep policies",
      "author" : [ "J. KOS", "D. SONG" ],
      "venue" : "5th International Conference on Learning Representations (ICLR) Workshop",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2017
    }, {
      "title" : "Learning multiple layers of features from tiny images",
      "author" : [ "A. KRIZHEVSKY" ],
      "venue" : "Tech. rep.,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2009
    }, {
      "title" : "Adversarial examples in the physical world",
      "author" : [ "A. KURAKIN", "I. GOODFELLOW", "S. BENGIO" ],
      "venue" : "arXiv preprint arXiv:1607.02533",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2016
    }, {
      "title" : "Gradient-based learning applied to document recognition",
      "author" : [ "Y. LECUN", "L. BOTTOU", "Y. BENGIO", "P. HAFFNER" ],
      "venue" : "Proceedings of the IEEE 86,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 1998
    }, {
      "title" : "Delving into transferable adversarial examples and black-box attacks",
      "author" : [ "Y. LIU", "X. CHEN", "C. LIU", "D. SONG" ],
      "venue" : "5th International Conference on Learning Representations (ICLR)",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2017
    }, {
      "title" : "The concrete distribution: A continuous relaxation of discrete random variables",
      "author" : [ "C.J. MADDISON", "A. MNIH", "TEH", "Y. W" ],
      "venue" : null,
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2016
    }, {
      "title" : "On detecting adversarial perturbations",
      "author" : [ "J.H. METZEN", "T. GENEWEIN", "V. FISCHER", "B. BISCHOFF" ],
      "venue" : "arXiv preprint arXiv:1702.04267",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2017
    }, {
      "title" : "Deepfool: a simple and accurate method to fool deep neural networks",
      "author" : [ "MOOSAVI-DEZFOOLI", "S.-M", "A. FAWZI", "P. FROSSARD" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2016
    }, {
      "title" : "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images",
      "author" : [ "A. NGUYEN", "J. YOSINSKI", "J. CLUNE" ],
      "venue" : "In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2015),",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2015
    }, {
      "title" : "Transferability in machine learning: from phenomena to black-box attacks using adversarial samples",
      "author" : [ "N. PAPERNOT", "P. MCDANIEL", "I. GOODFELLOW" ],
      "venue" : null,
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2016
    }, {
      "title" : "Practical black-box attacks against deep learning systems using adversarial examples",
      "author" : [ "N. PAPERNOT", "P. MCDANIEL", "I. GOODFELLOW", "S. JHA", "Z.B. CELIK", "A. SWAMI" ],
      "venue" : null,
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2016
    }, {
      "title" : "The limitations of deep learning in adversarial settings",
      "author" : [ "N. PAPERNOT", "P. MCDANIEL", "S. JHA", "M. FREDRIKSON", "Z.B. CELIK", "A. SWAMI" ],
      "venue" : "IEEE European Symposium on Security and Privacy (EuroS&P) (2016),",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2016
    }, {
      "title" : "Distillation as a defense to adversarial perturbations against deep neural networks",
      "author" : [ "N. PAPERNOT", "P. MCDANIEL", "X. WU", "S. JHA", "A. SWAMI" ],
      "venue" : "In Security and Privacy (SP),",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2016
    }, {
      "title" : "Intriguing properties of neural networks",
      "author" : [ "C. SZEGEDY", "W. ZAREMBA", "I. SUTSKEVER", "J. BRUNA", "D. ERHAN", "I. GOODFELLOW", "R. FERGUS" ],
      "venue" : null,
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2013
    }, {
      "title" : "Achieving human parity in conversational speech recognition",
      "author" : [ "W. XIONG", "J. DROPPO", "X. HUANG", "F. SEIDE", "M. SELTZER", "A. STOLCKE", "D. YU", "G. ZWEIG" ],
      "venue" : null,
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2016
    }, {
      "title" : "Feature squeezing: Detecting adversarial examples in deep neural networks",
      "author" : [ "XU W", "EVANS D", "QI" ],
      "venue" : "arXiv preprint arXiv:1704.01155",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2017
    } ],
    "referenceMentions" : [ {
      "referenceID" : 9,
      "context" : "Neural networks have achieved great performance on a wide range of application domains; in particular, they have demonstrated accuracies comparable or better than humans on datasets in the fields of image recognition [10] and speech recognition [29].",
      "startOffset" : 217,
      "endOffset" : 221
    }, {
      "referenceID" : 28,
      "context" : "Neural networks have achieved great performance on a wide range of application domains; in particular, they have demonstrated accuracies comparable or better than humans on datasets in the fields of image recognition [10] and speech recognition [29].",
      "startOffset" : 245,
      "endOffset" : 249
    }, {
      "referenceID" : 27,
      "context" : "However, recent work shows that deep learning models are susceptible to adversarial examples: inputs that are similar to a correctly classified input, but which are misclassified [28].",
      "startOffset" : 179,
      "endOffset" : 183
    }, {
      "referenceID" : 14,
      "context" : "Research on other applications of neural networks has also encountered adversarial examples on different tasks beyond image classification, including deep policies in reinforcement learning and generative models [15, 14, 12, 2].",
      "startOffset" : 212,
      "endOffset" : 227
    }, {
      "referenceID" : 13,
      "context" : "Research on other applications of neural networks has also encountered adversarial examples on different tasks beyond image classification, including deep policies in reinforcement learning and generative models [15, 14, 12, 2].",
      "startOffset" : 212,
      "endOffset" : 227
    }, {
      "referenceID" : 11,
      "context" : "Research on other applications of neural networks has also encountered adversarial examples on different tasks beyond image classification, including deep policies in reinforcement learning and generative models [15, 14, 12, 2].",
      "startOffset" : 212,
      "endOffset" : 227
    }, {
      "referenceID" : 1,
      "context" : "Research on other applications of neural networks has also encountered adversarial examples on different tasks beyond image classification, including deep policies in reinforcement learning and generative models [15, 14, 12, 2].",
      "startOffset" : 212,
      "endOffset" : 227
    }, {
      "referenceID" : 16,
      "context" : "Recent work has shown that adversarial examples remain even when subject to the lossy channel of being photographed [17].",
      "startOffset" : 116,
      "endOffset" : 120
    }, {
      "referenceID" : 26,
      "context" : "Despite many attempts [27, 7, 9, 21, 8, 6, 5, 11], there is no strong defense against adversarial examples to date.",
      "startOffset" : 22,
      "endOffset" : 49
    }, {
      "referenceID" : 6,
      "context" : "Despite many attempts [27, 7, 9, 21, 8, 6, 5, 11], there is no strong defense against adversarial examples to date.",
      "startOffset" : 22,
      "endOffset" : 49
    }, {
      "referenceID" : 8,
      "context" : "Despite many attempts [27, 7, 9, 21, 8, 6, 5, 11], there is no strong defense against adversarial examples to date.",
      "startOffset" : 22,
      "endOffset" : 49
    }, {
      "referenceID" : 20,
      "context" : "Despite many attempts [27, 7, 9, 21, 8, 6, 5, 11], there is no strong defense against adversarial examples to date.",
      "startOffset" : 22,
      "endOffset" : 49
    }, {
      "referenceID" : 7,
      "context" : "Despite many attempts [27, 7, 9, 21, 8, 6, 5, 11], there is no strong defense against adversarial examples to date.",
      "startOffset" : 22,
      "endOffset" : 49
    }, {
      "referenceID" : 5,
      "context" : "Despite many attempts [27, 7, 9, 21, 8, 6, 5, 11], there is no strong defense against adversarial examples to date.",
      "startOffset" : 22,
      "endOffset" : 49
    }, {
      "referenceID" : 4,
      "context" : "Despite many attempts [27, 7, 9, 21, 8, 6, 5, 11], there is no strong defense against adversarial examples to date.",
      "startOffset" : 22,
      "endOffset" : 49
    }, {
      "referenceID" : 10,
      "context" : "Despite many attempts [27, 7, 9, 21, 8, 6, 5, 11], there is no strong defense against adversarial examples to date.",
      "startOffset" : 22,
      "endOffset" : 49
    }, {
      "referenceID" : 29,
      "context" : "First, we study two recently proposed defenses, feature squeezing [30] and the specialists+1 ensemble method [1], each of which ensembles multiple defenses that compensate for each other’s weaknesses.",
      "startOffset" : 66,
      "endOffset" : 70
    }, {
      "referenceID" : 0,
      "context" : "First, we study two recently proposed defenses, feature squeezing [30] and the specialists+1 ensemble method [1], each of which ensembles multiple defenses that compensate for each other’s weaknesses.",
      "startOffset" : 109,
      "endOffset" : 112
    }, {
      "referenceID" : 5,
      "context" : "To study the question of ensemble defense in a broader scope, we also evaluate an ensemble of three independent, mutually compatible detection mechanisms [6, 21, 5].",
      "startOffset" : 154,
      "endOffset" : 164
    }, {
      "referenceID" : 20,
      "context" : "To study the question of ensemble defense in a broader scope, we also evaluate an ensemble of three independent, mutually compatible detection mechanisms [6, 21, 5].",
      "startOffset" : 154,
      "endOffset" : 164
    }, {
      "referenceID" : 4,
      "context" : "To study the question of ensemble defense in a broader scope, we also evaluate an ensemble of three independent, mutually compatible detection mechanisms [6, 21, 5].",
      "startOffset" : 154,
      "endOffset" : 164
    }, {
      "referenceID" : 29,
      "context" : "• We create effective attacks on feature squeezing [30], including individual squeezing methods and the combined adversarial example detection scheme.",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 0,
      "context" : "• We create an effective attack on the ensemble-ofspecialists defense [1].",
      "startOffset" : 70,
      "endOffset" : 73
    }, {
      "referenceID" : 27,
      "context" : "Recent works have pointed out that deep learning models are vulnerable to adversarial examples: these models give incorrect predictions on inputs that are slightly different from those correctly predicted ones [28, 7, 23, 26].",
      "startOffset" : 210,
      "endOffset" : 225
    }, {
      "referenceID" : 6,
      "context" : "Recent works have pointed out that deep learning models are vulnerable to adversarial examples: these models give incorrect predictions on inputs that are slightly different from those correctly predicted ones [28, 7, 23, 26].",
      "startOffset" : 210,
      "endOffset" : 225
    }, {
      "referenceID" : 22,
      "context" : "Recent works have pointed out that deep learning models are vulnerable to adversarial examples: these models give incorrect predictions on inputs that are slightly different from those correctly predicted ones [28, 7, 23, 26].",
      "startOffset" : 210,
      "endOffset" : 225
    }, {
      "referenceID" : 25,
      "context" : "Recent works have pointed out that deep learning models are vulnerable to adversarial examples: these models give incorrect predictions on inputs that are slightly different from those correctly predicted ones [28, 7, 23, 26].",
      "startOffset" : 210,
      "endOffset" : 225
    }, {
      "referenceID" : 6,
      "context" : "Several approaches have been proposed in previous work, including the Fast Gradient Sign Method (FGSM) [7], Fast Gradient Method [19], Jacobian-based Saliency Map Approach (JSMA) [26], Deepfool [22], and optimization-based methods [28, 3, 19].",
      "startOffset" : 103,
      "endOffset" : 106
    }, {
      "referenceID" : 18,
      "context" : "Several approaches have been proposed in previous work, including the Fast Gradient Sign Method (FGSM) [7], Fast Gradient Method [19], Jacobian-based Saliency Map Approach (JSMA) [26], Deepfool [22], and optimization-based methods [28, 3, 19].",
      "startOffset" : 129,
      "endOffset" : 133
    }, {
      "referenceID" : 25,
      "context" : "Several approaches have been proposed in previous work, including the Fast Gradient Sign Method (FGSM) [7], Fast Gradient Method [19], Jacobian-based Saliency Map Approach (JSMA) [26], Deepfool [22], and optimization-based methods [28, 3, 19].",
      "startOffset" : 179,
      "endOffset" : 183
    }, {
      "referenceID" : 21,
      "context" : "Several approaches have been proposed in previous work, including the Fast Gradient Sign Method (FGSM) [7], Fast Gradient Method [19], Jacobian-based Saliency Map Approach (JSMA) [26], Deepfool [22], and optimization-based methods [28, 3, 19].",
      "startOffset" : 194,
      "endOffset" : 198
    }, {
      "referenceID" : 27,
      "context" : "Several approaches have been proposed in previous work, including the Fast Gradient Sign Method (FGSM) [7], Fast Gradient Method [19], Jacobian-based Saliency Map Approach (JSMA) [26], Deepfool [22], and optimization-based methods [28, 3, 19].",
      "startOffset" : 231,
      "endOffset" : 242
    }, {
      "referenceID" : 2,
      "context" : "Several approaches have been proposed in previous work, including the Fast Gradient Sign Method (FGSM) [7], Fast Gradient Method [19], Jacobian-based Saliency Map Approach (JSMA) [26], Deepfool [22], and optimization-based methods [28, 3, 19].",
      "startOffset" : 231,
      "endOffset" : 242
    }, {
      "referenceID" : 18,
      "context" : "Several approaches have been proposed in previous work, including the Fast Gradient Sign Method (FGSM) [7], Fast Gradient Method [19], Jacobian-based Saliency Map Approach (JSMA) [26], Deepfool [22], and optimization-based methods [28, 3, 19].",
      "startOffset" : 231,
      "endOffset" : 242
    }, {
      "referenceID" : 6,
      "context" : "Prior work has shown this assumption can often be relaxed [7, 24, 25], however for simplicity we assume this stronger threat model.",
      "startOffset" : 58,
      "endOffset" : 69
    }, {
      "referenceID" : 23,
      "context" : "Prior work has shown this assumption can often be relaxed [7, 24, 25], however for simplicity we assume this stronger threat model.",
      "startOffset" : 58,
      "endOffset" : 69
    }, {
      "referenceID" : 24,
      "context" : "Prior work has shown this assumption can often be relaxed [7, 24, 25], however for simplicity we assume this stronger threat model.",
      "startOffset" : 58,
      "endOffset" : 69
    }, {
      "referenceID" : 26,
      "context" : "The first direction attempts to produce correct predictions on adversarial examples, while not compromising the accuracy on legitimate inputs [27, 7, 9].",
      "startOffset" : 142,
      "endOffset" : 152
    }, {
      "referenceID" : 6,
      "context" : "The first direction attempts to produce correct predictions on adversarial examples, while not compromising the accuracy on legitimate inputs [27, 7, 9].",
      "startOffset" : 142,
      "endOffset" : 152
    }, {
      "referenceID" : 8,
      "context" : "The first direction attempts to produce correct predictions on adversarial examples, while not compromising the accuracy on legitimate inputs [27, 7, 9].",
      "startOffset" : 142,
      "endOffset" : 152
    }, {
      "referenceID" : 20,
      "context" : "In this case, the model can reject an instance and refuse to classify those that it detects as adversarial [21, 8, 30, 1].",
      "startOffset" : 107,
      "endOffset" : 121
    }, {
      "referenceID" : 7,
      "context" : "In this case, the model can reject an instance and refuse to classify those that it detects as adversarial [21, 8, 30, 1].",
      "startOffset" : 107,
      "endOffset" : 121
    }, {
      "referenceID" : 29,
      "context" : "In this case, the model can reject an instance and refuse to classify those that it detects as adversarial [21, 8, 30, 1].",
      "startOffset" : 107,
      "endOffset" : 121
    }, {
      "referenceID" : 0,
      "context" : "In this case, the model can reject an instance and refuse to classify those that it detects as adversarial [21, 8, 30, 1].",
      "startOffset" : 107,
      "endOffset" : 121
    }, {
      "referenceID" : 29,
      "context" : "First and second are feature squeezing [30] and the specialists+1 ensemble method [1], both of which take this approach by construction.",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 0,
      "context" : "First and second are feature squeezing [30] and the specialists+1 ensemble method [1], both of which take this approach by construction.",
      "startOffset" : 82,
      "endOffset" : 85
    }, {
      "referenceID" : 5,
      "context" : "In particular, as an example demonstration, we ensemble three independent detection mechanisms [6, 21, 5] to build one detection mechanism.",
      "startOffset" : 95,
      "endOffset" : 105
    }, {
      "referenceID" : 20,
      "context" : "In particular, as an example demonstration, we ensemble three independent detection mechanisms [6, 21, 5] to build one detection mechanism.",
      "startOffset" : 95,
      "endOffset" : 105
    }, {
      "referenceID" : 4,
      "context" : "In particular, as an example demonstration, we ensemble three independent detection mechanisms [6, 21, 5] to build one detection mechanism.",
      "startOffset" : 95,
      "endOffset" : 105
    }, {
      "referenceID" : 17,
      "context" : "To evaluate the effectiveness of the different defense strategies, we use two standard datasets, MNIST [18] and CIFAR-10 [16] datasets.",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 15,
      "context" : "To evaluate the effectiveness of the different defense strategies, we use two standard datasets, MNIST [18] and CIFAR-10 [16] datasets.",
      "startOffset" : 121,
      "endOffset" : 125
    }, {
      "referenceID" : 29,
      "context" : "When evaluating each defense strategy, we use the same model architectures described in their papers respectively [30, 1, 6, 21, 5].",
      "startOffset" : 114,
      "endOffset" : 131
    }, {
      "referenceID" : 0,
      "context" : "When evaluating each defense strategy, we use the same model architectures described in their papers respectively [30, 1, 6, 21, 5].",
      "startOffset" : 114,
      "endOffset" : 131
    }, {
      "referenceID" : 5,
      "context" : "When evaluating each defense strategy, we use the same model architectures described in their papers respectively [30, 1, 6, 21, 5].",
      "startOffset" : 114,
      "endOffset" : 131
    }, {
      "referenceID" : 20,
      "context" : "When evaluating each defense strategy, we use the same model architectures described in their papers respectively [30, 1, 6, 21, 5].",
      "startOffset" : 114,
      "endOffset" : 131
    }, {
      "referenceID" : 4,
      "context" : "When evaluating each defense strategy, we use the same model architectures described in their papers respectively [30, 1, 6, 21, 5].",
      "startOffset" : 114,
      "endOffset" : 131
    }, {
      "referenceID" : 2,
      "context" : "In this paper, we use an optimization-based approach to generate adversarial examples [3], which is shown to be effective on finding adversarial examples with small distortions.",
      "startOffset" : 86,
      "endOffset" : 89
    }, {
      "referenceID" : 2,
      "context" : "We omit details of the design choice and refer the reader to the original paper [3].",
      "startOffset" : 80,
      "endOffset" : 83
    }, {
      "referenceID" : 0,
      "context" : "Each dimension of input images is scaled to [0,1], i.",
      "startOffset" : 44,
      "endOffset" : 49
    }, {
      "referenceID" : 29,
      "context" : "[30, 31].",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 6,
      "context" : "On adversarial examples generated by a static adversary using FGSM [7] and JSMA [26], they show that their detector achieves 99.",
      "startOffset" : 67,
      "endOffset" : 70
    }, {
      "referenceID" : 25,
      "context" : "On adversarial examples generated by a static adversary using FGSM [7] and JSMA [26], they show that their detector achieves 99.",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 0,
      "context" : "We study a second defense that combines multiple component defenses, an ensemble of specialists, proposed by Abbasi and Gagné [1].",
      "startOffset" : 126,
      "endOffset" : 129
    }, {
      "referenceID" : 0,
      "context" : "Abbasi and Gagné [1] find that using an ensemble constructed this way successfully reduces the system’s confidence (fraction of voting classifiers that voted for the winning class) on adversarial examples generated by a static attacker using FGSM [7], DeepFool [22], and Szegedy et al.",
      "startOffset" : 17,
      "endOffset" : 20
    }, {
      "referenceID" : 6,
      "context" : "Abbasi and Gagné [1] find that using an ensemble constructed this way successfully reduces the system’s confidence (fraction of voting classifiers that voted for the winning class) on adversarial examples generated by a static attacker using FGSM [7], DeepFool [22], and Szegedy et al.",
      "startOffset" : 247,
      "endOffset" : 250
    }, {
      "referenceID" : 21,
      "context" : "Abbasi and Gagné [1] find that using an ensemble constructed this way successfully reduces the system’s confidence (fraction of voting classifiers that voted for the winning class) on adversarial examples generated by a static attacker using FGSM [7], DeepFool [22], and Szegedy et al.",
      "startOffset" : 261,
      "endOffset" : 265
    }, {
      "referenceID" : 27,
      "context" : "’s approach [28].",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 5,
      "context" : "propose using adversarial training to detect adversarial examples [6].",
      "startOffset" : 66,
      "endOffset" : 69
    }, {
      "referenceID" : 20,
      "context" : "construct a similar scheme, however instead of using the original images as the input to the detector, they train on the inner convolutional layers of the network [21].",
      "startOffset" : 163,
      "endOffset" : 167
    }, {
      "referenceID" : 4,
      "context" : "examine the final hidden layer of a neural network and find that adversarial examples are separable from the original images by training a density estimate using Gaussian kernels [5].",
      "startOffset" : 179,
      "endOffset" : 182
    }, {
      "referenceID" : 3,
      "context" : "without the detector [4].",
      "startOffset" : 21,
      "endOffset" : 24
    }, {
      "referenceID" : 3,
      "context" : "We use the same notation as [4].",
      "startOffset" : 28,
      "endOffset" : 31
    }, {
      "referenceID" : 3,
      "context" : "fenses [4].",
      "startOffset" : 7,
      "endOffset" : 10
    }, {
      "referenceID" : 3,
      "context" : "To do this, [4] specifically defines",
      "startOffset" : 12,
      "endOffset" : 15
    }, {
      "referenceID" : 27,
      "context" : "In order to understand the reason that these defenses do not significantly increase robustness when combined together, we hypothesize that the transferability property [28, 7, 24, 19] of adversarial examples is simplifying the attacker’s task.",
      "startOffset" : 168,
      "endOffset" : 183
    }, {
      "referenceID" : 6,
      "context" : "In order to understand the reason that these defenses do not significantly increase robustness when combined together, we hypothesize that the transferability property [28, 7, 24, 19] of adversarial examples is simplifying the attacker’s task.",
      "startOffset" : 168,
      "endOffset" : 183
    }, {
      "referenceID" : 23,
      "context" : "In order to understand the reason that these defenses do not significantly increase robustness when combined together, we hypothesize that the transferability property [28, 7, 24, 19] of adversarial examples is simplifying the attacker’s task.",
      "startOffset" : 168,
      "endOffset" : 183
    }, {
      "referenceID" : 18,
      "context" : "In order to understand the reason that these defenses do not significantly increase robustness when combined together, we hypothesize that the transferability property [28, 7, 24, 19] of adversarial examples is simplifying the attacker’s task.",
      "startOffset" : 168,
      "endOffset" : 183
    } ],
    "year" : 2017,
    "abstractText" : "Ongoing research has proposed several methods to defend neural networks against adversarial examples, many of which researchers have shown to be ineffective. We ask whether a strong defense can be created by combining multiple (possibly weak) defenses. To answer this question, we study three defenses that follow this approach. Two of these are recently proposed defenses that intentionally combine components designed to work well together. A third defense combines three independent defenses. For all the components of these defenses and the combined defenses themselves, we show that an adaptive adversary can create adversarial examples successfully with low distortion. Thus, our work implies that ensemble of weak defenses is not sufficient to provide strong defense against adversarial examples.",
    "creator" : "LaTeX with hyperref package"
  }
}