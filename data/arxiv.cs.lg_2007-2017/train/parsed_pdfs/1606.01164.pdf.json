{
  "name" : "1606.01164.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Dense Associative Memory for Pattern Recognition",
    "authors" : [ "Dmitry Krotov", "John J Hopfield" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "A model of associative memory is studied, which stores and reliably retrieves many more patterns than the number of neurons in the network. We propose a simple duality between this dense associative memory and neural networks commonly used in deep learning. On the associative memory side of this duality, a family of models that smoothly interpolates between two limiting cases can be constructed. One limit is referred to as the feature-matching mode of pattern recognition, and the other one as the prototype regime. On the deep learning side of the duality, this family corresponds to feedforward neural networks with one hidden layer and various activation functions, which transmit the activities of the visible neurons to the hidden layer. This family of activation functions includes logistics, rectified linear units, and rectified polynomials of higher degrees. The proposed duality makes it possible to apply energy-based intuition from associative memory to analyze computational properties of neural networks with unusual activation functions – the higher rectified polynomials which until now have not been used for training neural networks. The utility of the dense memories is illustrated for two test cases: the logical gate XOR and the recognition of handwritten digits from the MNIST data set."
    }, {
      "heading" : "1 Introduction",
      "text" : "Pattern recognition and models of associative memory [1] are closely related. Consider image classification as an example of pattern recognition. In this problem, the network is presented with an image and the task is to label the image. In the case of associative memory the network stores a set of memory vectors. In a typical query the network is presented with an incomplete pattern resembling, but not identical to, one of the stored memories and the task is to recover the full memory. Pixel intensities of the image can be combined together with the label of that image into one vector, which will serve as a memory for the associative memory. Then the image itself can be thought of as a partial memory cue. The task of identifying an appropriate label is a subpart of the associative memory reconstruction. There is a limitation in using this idea to do pattern recognition. The standard model of associative memory works well in the limit when the number of stored patterns is much smaller than the number of neurons [1], or equivalently the number of pixels in an image. In order to do pattern recognition with small error rate one would need to store many more memories than the typical number of pixels in the presented images. This is a serious problem. It can be solved by modifying the standard energy function of associative memory, quadratic in interactions between the neurons, by including in it higher order interactions. By properly designing the energy function (or Hamiltonian) for these models with higher order interactions one can store and reliably retrieve many more memories than the number of neurons in the network.\nDeep neural networks have proven to be useful for a broad range of problems in machine learning including image classification, speech recognition, object detection, etc. These models\n1Simons Center for Systems Biology, Institute for Advanced Study, Princeton, NJ, 08540, USA, krotov@ias.edu 2Princeton Neuroscience Institute, Princeton University, Princeton, NJ, 08544, USA, hopfield@princeton.edu\nar X\niv :1\n60 6.\n01 16\n4v 1\n[ cs\n.N E\n] 3\nJ un\n2 01\n6\nare composed of several layers of neurons, so that the output of one layer serves as the input to the next layer. Each neuron calculates a weighted sum of the inputs and passes the result through a non-linear activation function. Traditionally, deep neural networks used activation functions such as hyperbolic tangents or logistics. Learning the weights in such networks, using a backpropagation algorithm, faced serious problems in the 1980s and 1990s. These issues were largely resolved by introducing unsupervised pre-training, which made it possible to initialize the weights in such a way that the subsequent backpropagation could only gently move boundaries between the classes without destroying the feature detectors [2, 3]. More recently, it was realized that the use of rectified linear units (ReLU) instead of the logistic functions speeds up learning and improves generalization [4, 5, 6]. Rectified linear functions are usually interpreted as firing rates of biological neurons. These rates are equal to zero if the input is below a certain threshold and linearly grow with the input if it is above the threshold. To mimic biology the output should be small or zero if the input is below the threshold, but it is much less clear what the behavior of the activation function should be for inputs exceeding the threshold. Should it grow linearly, sub-linearly, or faster than linearly? How does this choice affect the computational properties of the neural network? Are there other functions that would work even better than the rectified linear units? These are the questions that to the best of our knowledge remain open.\nThis paper examines these questions through the lens of associative memory. We start by discussing a family of models of associative memory with large capacity. These models use higher order (higher than quadratic) interactions between the neurons in the energy function. The associative memory description is then mapped onto a neural network with one hidden layer and an unusual activation function, related to the Hamiltonian. We show that by varying the power of interaction vertex in the energy function (or equivalently by changing the activation function of the neural network) one can force the model to store representations of the data either in terms of features or in terms of prototypes."
    }, {
      "heading" : "2 Associative memory with large capacity",
      "text" : "The standard model of associative memory [1] uses a system of N binary neurons, with values ±1. A configuration of all the neurons is denoted by a vector σi. The model stores K memories, denoted by ξµi , which for the moment are also assumed to be binary. The model is defined by an energy function, which is given by\nE = −1 2 N∑ i,j=1 σiTijσj, Tij = K∑ µ=1 ξµi ξ µ j , (1)\nand a dynamical update rule that decreases the energy at every update. The basic problem is the following: when presented with a new pattern the network should respond with a stored memory which most closely resembles the input.\nThere has been a large amount of work in the community of statistical physicists investigating the capacity of this model, which is the maximal number of memories that the network can store and reliably retrieve. It has been demonstrated [1, 7, 8] that in case of random memories this maximal value is of the order of Kmax ≈ 0.14N . If one tries to store more patterns, several neighboring memories in the configuration space will merge together producing a ground state of the Hamiltonian (1), which has nothing to do with any of the stored memories. By modifying the\nHamiltonian (1) in a way that removes second order correlations between the stored memories, it is possible [9] to improve the capacity to Kmax = N .\nThe mathematical reason why the model (1) gets confused when many memories are stored is that several memories produce contributions to the energy (1) which are of the same order. In other words the energy decreases too slowly as the pattern approaches a memory in the configuration space. In order to take care of this problem, consider a modification (2) of the standard energy.\nE = − K∑ µ=1 F ( ξµi σi ) (2)\nIn this formula F (x) is some smooth function (summation over index i is assumed). The computational capabilities of the model will be illustrated for two cases. First, when F (x) = xn (n is an integer number), which is referred to as a polynomial energy function. Second, when F (x) is a rectified polynomial energy function\nF (x) = { xn, x ≥ 0 0, x < 0\n(3)\nIn the case of the polynomial function with n = 2 the network reduces to the standard model of associative memory [1]. If n > 2 each term in (2) becomes sharper compared to the n = 2 case, thus more memories can be packed into the same configuration space before cross-talk intervenes.\nHaving defined the energy function one can derive an iterative update rule that leads to decrease of the energy. We use asynchronous updates flipping one unit at a time. The update rule is:\nσ (t+1) i = Sign [ K∑ µ=1 ( F ( ξµi + ∑ j 6=i ξµj σ (t) j ) − F ( − ξµi + ∑ j 6=i ξµj σ (t) j ))] , (4)\nThe argument of the sign function is the difference of two energies. One, for the configuration when all but the i-th units are clumped to the current state of the network and the i-th unit is in the “off” state. The other one for a similar configuration, but with the i-th unit in the “on” state. This rule means that the system chooses to update a unit, given the states of the rest of the network, in such a way that the energy of the entire configuration decreases. For the case of polynomial energy function a very similar family of models was considered in [10, 11, 12, 13, 14, 15]. The update rule in those models was based on the induced magnetic fields, however, and not on the difference of energies. The two are slightly different due to the presence of self-coupling terms. Throughout this paper we use energy-based update rules.\nHow many memories can model (4) store and reliably retrieve? Consider the case of random patterns, so that each element of the memories is equal to ±1 with equal probability. Imagine that the system is initialized in a state equal to one of the memories (pattern number µ). One can derive a stability criterion, i.e. the upper bound on the number of memories such that the network stays in that initial state. Define the energy difference between the initial state and the state with spin i flipped\n∆E = K∑ ν=1 ( ξνi ξ µ i + ∑ j 6=i ξνj ξ µ j )n − K∑ ν=1 ( − ξνi ξµi + ∑ j 6=i ξνj ξ µ j )n ,\nwhere the polynomial energy function is used. This quantity has a mean 〈∆E〉 = Nn−(N−2)n ≈ 2nNn−1, which comes from the term with ν = µ, and a variance (in the limit of large N)\nΣ2 = Ωn(K − 1)Nn−1, where Ωn = 4n2(2n− 3)!!\nThe i-th bit becomes unstable when the magnitude of the fluctuation exceeds the energy gap 〈∆E〉 and the sign of the fluctuation is opposite to the sign of the energy gap. Thus the probability that the state of a single neuron is unstable (in the limit when both N and K are large, so that the noise is effectively gaussian) is equal to\nPerror = ∞∫ 〈∆E〉 dx√ 2πΣ2 e− x2 2Σ2 ≈ √ (2n− 3)!! 2π K Nn−1 e− Nn−1 2K(2n−3)!!\nRequiring that this probability is less than a small value, say 0.5%, one can find the upper limit on the number of patterns that the network can store\nKmax = αnN n−1, (5)\nwhere αn is a numerical constant, which depends on the (arbitrary) threshold 0.5%. The case n = 2 corresponds to the standard model of associative memory and gives the well known result K = 0.14N . For the perfect recovery of a memory (Perror < 1/N) one obtains,\nKmaxno errors ≈ 1 2(2n− 3)!! Nn−1 ln(N) (6)\nFor higher powers n the capacity rapidly grows with N in a non-linear way, allowing the network to store and reliably retrieve many more patterns than the number of neurons that it has, in accord3 with [12, 13, 14, 15]. This non-linear scaling relationship between the capacity and the size of the network is the phenomenon that we exploit.\nThe essence of the construction is to build a model that captures higher order correlations between activities of multiple units. There are many ways to do this. The most straightforward idea would be to add higher order interaction terms to the standard quadratic energy (1), for example (summation over repeated indices)\nE = −1 2 σiTijσj + 1 3 T (3) ijkσiσjσk + ...\nAs n increases the number of parameters describing such models becomes huge, and we very quickly run into a problem that there is not enough data to learn these terms of general form. In contrast, in model (2) the higher order interactions are highly structured, so that the number of parameters specifying the model is equal to K ·N and does not grow with n.\nWe study a family of models of this kind as a function of n. At small n many terms contribute to the sum over µ in (2) approximately equally. In the limit n→∞ the dominant contribution to the sum comes from a single memory, which has the largest overlap with the input. It turns out that optimal computation occurs in the intermediate range.\n3The n-dependent coefficient in (6) depends on the exact form of the Hamiltonian and the update rule. References [12, 13, 14] do not allow repeated indices in the products over neurons in the energy function, therefore obtain a different coefficient. In [15] the Hamiltonian coincides with ours, but the update rule is different, which, however, results in exactly the same coefficient as in (6)."
    }, {
      "heading" : "3 The case of XOR",
      "text" : "The case of XOR is elementary, yet instructive. It is presented here for three reasons. First, it illustrates the construction (2) in this simplest case. Second, it shows that as n increases, the computational capabilities of the network also increase. Third, it provides the simplest example of a situation in which the number of memories is larger than the number of neurons, yet the network works reliably.\nThe basic problem is the following: given two inputs x and y produce an output z such that the truth table (Table 1) is satisfied. We will treat this task as an associative memory problem and will simply embed the four examples of the input-output triplets x, y, z in the memory. Therefore the network has N = 3 identical units: two of which will be used for the inputs and one for the output, and K = 4 memories ξµi , which are the four lines of the truth table. Thus, the energy (2) is equal to\nEn(x, y, z) = − ( − x− y − z )n − (− x+ y + z)n − (x− y + z)n − (x+ y − z)n, (7) where the energy function is chosen to be a polynomial of degree n. For odd n, energy (7) is an odd function of each of its arguments, En(x, y,−z) = −En(x, y, z). For even n, it is an even function. For n = 1 it is equal to zero. Thus, if evaluated on the corners of the cube x, y, z = ±1, it reduces to equation (8),\nEn(x, y, z) =  0, n = 1 Cn, n = 2, 4, 6, ...\nCnxyz, n = 3, 5, 7, ...\n(8)\nwhere coefficients Cn denote numerical constants. In order to solve the XOR problem one can present to the network an “incomplete pattern” of inputs (x, y) and let the output z adjust to minimize the energy of the three-spin configuration, while holding the inputs fixed. The network clearly cannot solve this problem for n = 1 and n = 2, since the energy does not depend on the spin configuration. The case n = 2 is the standard model of associative memory. It can also be thought of as a linear perceptron, and the inability to solve this problem represents the well known statement [16] that linear perceptrons cannot compute XOR without hidden neurons. The case of odd n ≥ 3 provides an interesting solution. Given two inputs, x and y, one can choose the output z that minimizes the energy. This leads to the update rule\nz = Sign [ En(x, y,−1)− En(x, y,+1) ] = Sign [ − xy ] Thus, in this simple case the network is capable of solving the problem for higher odd values of n, while it cannot do so for n = 1 and n = 2. In case of rectified polynomials, a similar construction solves the problem for any n ≥ 2. The network works well in spite of the fact that K > N ."
    }, {
      "heading" : "4 An example of a pattern recognition problem, the case",
      "text" : "of MNIST\nThe MNIST data set is a collection of handwritten digits, which has 60000 training examples and 10000 test images. The goal is to classify the digits into 10 classes. The visible neurons, one for each pixel in the image, are combined together with 10 classification neurons in one vector that defines the state of the network. The visible part of this vector is treated as an “incomplete” pattern and the associative memory is allowed to calculate a completion of that pattern, which is the label of the image.\nDense associative memory (2) is a recurrent network in which every neuron can be updated multiple times. For the purposes of digit classification, however, this model will be used in a very limited capacity, allowing it to perform only one update of the classification neurons. The network is initialized in the state when the visible units vi are clamped to the intensities of a given image and the classification neurons are in the off state xα = −1 (see Fig.1). The network is allowed to make one update of the classification neurons, while keeping the visible units clamped, to produce the output cα. The update rule is similar to (4) except that the sign is replaced by the continuous function g(x) = tanh(x)\ncα = g\n[ β K∑ µ=1 ( F ( − ξµαxα + ∑ γ 6=α ξµγxγ + N∑ i=1 ξµi vi ) − F ( ξµαxα + ∑ γ 6=α ξµγxγ + N∑ i=1 ξµi vi ))] , (9)\nwhere parameter β regulates the slope of g(x). The proposed digit class is given by the number of a classification neuron producing the maximal output. Throughout this section the rectified polynomials (3) are used as functions F . To learn effective memories for use in pattern classification,\nvi c↵\nvi x↵\nlanguage corresponds to n = 2 for the energy function. Although not currently used to train deep networks, the case n = 3 would correspond to a rectified parabola as an activation function. We start by comparing the performances of the dense memories in these two cases.\nThe performance of the network depends on n and on the remaining hyperparameters, thus the hyperparameters should be optimized for each value of n. In order to test the variability of performances for various choices of hyperparameters at a given n, a window of hyperparameters for\nwhich the network works well on the validation set (see the appendix) was determined. Then many networks were trained for various choices of the hyperparameters from this window to evaluate the performance on the test set. The test errors as training progresses are shown in Fig.2. While there is substantial variability among these samples, on average the cluster of trajectories for n = 3 achieves better results on the test set than that for n = 2. These error rates should be compared with error rates for backpropagation alone without the use of generative pretraining or various kinds of regularizations (for example dropout), which could be added to our construction if necessary. To the best of our knowledge, the best published results on pixel permutation invariant task for MNIST are all in the 1.6% range [17], see also controls in [18, 19]. This agrees with our results for n = 2. The n = 3 case does slightly better than that as is clear from Fig.2, with all the samples performing better than 1.6%.\nHigher rectified polynomials are also faster in training compared to ReLU. For the n = 2 case, the error crosses the 2% threshold for the first time during training in the range of 179-312 epochs. For the n = 3 case, this happens earlier on average, between 158- 262 epochs. For higher powers n this speed-up is larger. This is not a huge effect for a small\ndataset such as MNIST. However, this speed-up might be very helpful for training large networks on large datasets, such as ImageNet. A similar effect was reported earlier for the transition between saturating units, such as logistics or hyperbolic tangents, to ReLU [6]. In our family of models that result corresponds to moving from n = 1 to n = 2.\nFeature to prototype transition\nHow does the computation performed by the neural network change as n varies? There are two extreme classes of theories of pattern recognition: feature-matching and formation of a prototype. According to the former, an input is decomposed into a set of features, which are compared with those stored in the memory. The subset of the stored features activated by the presented input is then interpreted as an object. One object has many features; features can also appear in more than one object. The prototype theory provides an alternative approach, in which objects are recognized as a whole. The prototypes do not necessarily match the object exactly, but rather are blurred abstract representations which include all the features that an object has. We argue that the computational models proposed here describe feature-matching mode of pattern recognition for small n and the prototype regime for large n. This can be anticipated from the sharpness of contributions that each memory makes to the total energy (2). For large n the function F (x) peaks much more sharply around each memory compared to the case of small n. Thus, at large n all the\ninformation about a digit must be written in only one memory, while at small n this information can be distributed among several memories. In the case of intermediate n some learned memories behave like features while others behave like prototypes. These two classes of memories work together to model the data in an efficient way.\nThe feature to prototype transition is clearly seen in memories shown in Fig. 3. For n = 2 or 3 each memory does not look like a digit, but resembles a pattern of activity that might be useful for recognizing several different digits. For n = 20 many of the memories can be recognized as digits, which are surrounded by white margins representing elements of memories having approximately zero values. These margins describe the variability of thicknesses of lines of different training examples and mathematically mean that the energy (2) does not depend on whether the corresponding pixel is on or off. For n = 30 most of the memories represent prototypes of whole digits or large portions of digits, with a small admixture of feature memories that do not resemble any digit.\nThe feature to prototype transition can be visualized by showing the feature detectors in situations when there is a natural ordering of pixels. Such ordering exists in images, for example. In general situations, however, there is no preferred permutation of visible neurons that would reveal this\nstructure (e.g. in the case of genomic data). It is therefore useful to develop a measure that permits a distinction to be made between features and prototypes in the absence of such visual space. Towards the end of training most of the recognition connections ξµα are approximately equal to ±1. One can choose an arbitrary cutoff, and count the number of recognition connections that are in the “on” state for each memory.\nThe distribution function of this number is shown on the left histogram in Fig. 3. Intuitively, this quantity corresponds to the number of different digit classes that a particular memory votes for. At small n, most of the memories vote for three to five different digit classes, a behavior characteristic of features. As n increases, each memory specializes and votes for only a single class. In the case n = 30, for example, more than 40% of memories vote for only one class, a\nbehavior characteristic of prototypes. A second way to see the feature to prototype transition is to look at the number of memories which make large contributions to the classification decision (right histogram in Fig. 3). For each test image one can find the memory that makes the largest contribution to the energy gap, which is the sum over µ in (9). Then one can count the number of memories that contribute to the gap by more than 0.9 of this largest contribution. For small n, there are many memories that satisfy this criterion and the distribution function has a long tail. In this regime several memories are cooperating with each other to make a classification decision. For n = 30, however, more than 8000 of 10000 test images do not have a single other memory that would make a contribution comparable with the largest one. This result is not sensitive to the arbitrary choice (0.9) of the cutoff. Interestingly, the performance remains competitive even for very large n ≈ 20 (see Fig.3) in spite of the fact that these networks are doing a very different kind of computation compared with that at small n."
    }, {
      "heading" : "5 Relationship to a neural network with one hidden layer",
      "text" : "In this section we derive a simple duality between the dense model of associative memory and a feedforward neural network with one layer of hidden neurons. In other words, we show that the same computational model has two very different descriptions: one in term of associative memory,\nthe other one in terms of a network with one layer of hidden units. Using this correspondence one can transform the family of dense memories, constructed for different values of power n, to the language of models used in deep learning. The resulting neural networks are guaranteed to inherit computational properties of the dense memories such as the\nfeature to prototype transition.\nThe construction is very similar to (9) , except that the classification neurons are initialized in the state when all of them are equal to −ε, see Fig. 4. In the limit ε→ 0 one can expand the function F in (9) so that the dominant contribution comes from the term linear in ε. Then\ncα ≈ g [ β K∑ µ=1 F ′ ( N∑ i=1 ξµi vi ) (−2ξµαxα) ] = g [ K∑ µ=1 ξµα F ′(ξµi vi)] = g[ K∑ µ=1 ξµα f ( ξµi vi )] , (10)\nwhere the parameter β is set to β = 1/(2ε) (summation over the visible index i is assumed). Thus, the model of associative memory with one step update is equivalent to a conventional feedforward neural network with one hidden layer provided that the activation function from the visible layer to the hidden layer is equal to the derivative of the energy function\nf(x) = F ′(x) (11)\nThe visible part of each memory serves as an incoming weight to the hidden layer, and the recognition part of the memory serves as an outgoing weight from the hidden layer. The expansion used in (10) is justified as long as\nN∑ i=1 ξµi vi Nc∑ α=1 ξµαxα,\na condition which is satisfied for most common problems, and is simply a statement that labels contain far less information than the data itself.\nFrom the point of view of associative memory, the dominant contribution shaping the basins of attraction comes from the low energy states. Therefore mathematically it is determined by the asymptotics of the activation function f(x), or the energy function F (x), at x → ∞. Thus different activation functions having similar asymptotics at x → ∞ should fall into the same universality class and should have similar computational properties. In the table below we list some common activation functions used in models of deep learning, their associative memory counterparts and the power n which determines the asymptotic behavior of the energy function at x → ∞. The results of section 4 suggest that for not too large n the speed of learning should\nactivation function energy function n f(x) = tanh(x) F (x) = ln ( cosh(x) ) ≈ x, at x→∞ 1\nf(x) = logistic function F (x) = ln ( 1 + ex ) ≈ x, at x→∞ 1\nf(x) =ReLU F (x) ∼ x2, at x→∞ 2 f(x) = RePn−1 F (x) = RePn n\nimprove as n increases. This is consistent with the previous observation that ReLU are faster in training than hyperbolic tangents and logistics [4, 5, 6]. The last row of the table corresponds to rectified polynomials of higher degrees. To the best of our knowledge these activation functions have not been used in neural networks. Our results suggest that for some problems these higher order functions should have even better computational properties than the rectified liner units."
    }, {
      "heading" : "6 Discussion and conclusions",
      "text" : "What is the relationship between the capacity of the dense associative memory, calculated in section 2, and the neural network with one step update that is used for digit classification? Consider the limit of very large β in (9), so that the hyperbolic tangent is approximately equal to the sign function, as in (4). In the limit of sufficiently large n the network is operating in the prototype regime. The presented image places the initial state of the network close to a local minimum, which corresponds to one of the prototypes. In most cases the one step update of the classification neurons is sufficient to bring this initial state to the nearest local minimum, thus completing the memory recovery. This is true, however, only if the stored patterns are stable and have basins of attraction around them of at least the size of one neuron flip, which is exactly (in the case of random patterns) the condition given by (6). For correlated patterns the maximal number of stored memories might be different from (6), however it still rapidly increases with increase of n. The associative memory with one step update (or the feedforward neural network) is exactly equivalent to the full associative memory with multiple updates in this limit. The calculation\nwith random patterns thus theoretically justifies the expectation of a good performance in the prototype regime.\nTo summarize, this paper contains three main results. First, it is shown how to use the general framework of associative memory for pattern recognition. Second, a family of models is constructed that can learn representations of the data in terms of features or in terms of prototypes, and that smoothly interpolates between these two extreme regimes by varying the power of interaction vertex. Third, there exists a simple duality between a one step update version of the associative memory model and a feedforward neural network with one layer of hidden units and an unusual activation function mapping the inputs to the hidden layer. This duality makes it possible to propose a class of activation functions that encourages the network to find representations of the data with various proportions of features and prototypes. These activation functions can be used in models of deep learning and should be more effective than the standard choices. They allow the networks to train faster. We have also observed an improvement of generalization ability in networks trained with the rectified parabola activation function compared to the ReLU for the case of MNIST. While these ideas were illustrated using the simplest architecture of the neural network with one layer of hidden units, the proposed activation functions can also be used in multilayer architectures. We did not study various regularizations (weight decay, dropout, etc), which can be added to our construction if necessary. The performance of the model supplemented with these regularizations, as well as performance on other common benchmark tasks, will be reported elsewhere."
    }, {
      "heading" : "Appendix. Details of experiments with MNIST.",
      "text" : "The networks were trained using stochastic gradient descent with minibatches of a relatively large size, 100 digits of each class, 1000 digits in total. Training was done for 3000 epochs. Initial weights were generated from a Gaussian distribution N(−0.3, 0.3). Momentum (0.6 ≤ p ≤ 0.95) was used to smooth out oscillations of gradients coming from the individual minibatches. The learning rate was decreasing with time according to\nε(t) = ε0f t, f = 0.998, (12)\nwhere t is the number of epoch. Typical values are 0.01 ≤ ε0 ≤ 0.04. The weights (memories) were updated after each minibatch according to\nV µI (t) = pV µ I (t− 1)− 〈∂ξµI C〉\nξµI (t) = ξ µ I (t− 1) + ε\nV µI (t)\nmax J |V µJ (t)|\n, (13)\nwhere t is the number of update, I = (i, α) is an index which unites the visible and the classification units. The proposed update in (13) is normalized so that the largest update of the weights for each hidden unit (memory) is equal to ε. This normalization is equivalent to using different learning rates for each individual memory. It prevents the network from getting stuck on a plateau. All weights were constrained to stay within the −1 ≤ ξµI ≤ 1 range. Therefore, if after an update some weights exceeded 1, they were truncated to make them equal to 1 (and similarly for −1). The slope of the function g(x) in (9) is controlled by the effective temperature β = 1/T n, which is measured in “neurons” or “pixels”. For large n the temperature can be kept constant throughout\nthe entire training (500 ≤ T ≤ 700). For small n we found useful to start at a high temperature Ti, and then linearly decrease it to the final value Tf during the first 200 epochs (250 ≤ Ti ≤ 400, 30 ≤ Tf ≤ 100). The temperature stays constant after that.\nThe MNIST dataset contains 60000 training examples, which were randomly split into 50000 training cases and 10000 validation cases. For each hyperparameter a window of values was selected, such that the error on the validation set after 3000 epochs is less than a certain threshold. After that the entire set of 60000 examples was used to train the network (for 3000 epochs) for various values of the hyperparameters from this optimal window to evaluate the performance on the test set. The validation set was not used for early stopping.\nThe objective function is given by\nC = ∑\ntraining\nexamples\nNc∑ α=1 ( cα − tα )2m , (14)\nwhere tα is the target output. The case m = 1 corresponds to the standard quadratic error. For large powers m the function x2m is small for |x| < 1 and rapidly grows for |x| > 1. Therefore, higher values of m emphasize training examples which produce largest discrepancy with the target output more strongly compared to those examples which are already sufficiently close to the target output. Such emphasis encourages the network to concentrate on correcting mistakes and moving the decision boundary farther away from the barely correct examples rather than on fitting better and better the training examples which have already been easily and correctly classified. Although much of what we discuss is valid for arbitrary value of m, including m = 1, we found that higher values of m reduce overfitting and improve generalization at least in the limit of large n. For small n, we used m = 2, 3, 4. For n = 20, 30, larger values of m ≈ 30 worked better. We also tried cross-entropy objective function together with softmax output units. The results were worse and are not presented here."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank Bernard Chazelle, David Huse, Arnie Levine, Michael Mitchell, Remi Monasson, Luca Peliti, Den Raskovalov, Bingkan Xue, and all the members of the Simons Center for Systems Biology at IAS for useful discussions. We especially thank Yasser Roudi for pointing out the reference [12] to us. The work of DK is supported by Charles L. Brown membership at IAS."
    } ],
    "references" : [ {
      "title" : "Neural networks and physical systems with emergent collective computational abilities",
      "author" : [ "J.J. Hopfield" ],
      "venue" : "Proceedings of the national academy of sciences,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1982
    }, {
      "title" : "A fast learning algorithm for deep belief nets",
      "author" : [ "G.E. Hinton", "S. Osindero", "Y.W. Teh" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2006
    }, {
      "title" : "Reducing the dimensionality of data with neural networks",
      "author" : [ "G.E. Hinton", "R.R. Salakhutdinov" ],
      "venue" : "Science, 313(5786),",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2006
    }, {
      "title" : "Rectified linear units improve restricted boltzmann machines",
      "author" : [ "V. Nair", "G.E. Hinton" ],
      "venue" : "In Proceedings of the 27th International Conference on Machine Learning",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2010
    }, {
      "title" : "Deep sparse rectifier neural networks",
      "author" : [ "X. Glorot", "A. Bordes", "Y. Bengio" ],
      "venue" : "In International Conference on Artificial Intelligence and Statistics (pp. 315-323)",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2011
    }, {
      "title" : "ImageNet classification with deep convolutional neural networks. In Advances in neural information processing systems (pp. 1097-1105)",
      "author" : [ "A. Krizhevsky", "I. Sutskever", "G.E. Hinton" ],
      "venue" : null,
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2012
    }, {
      "title" : "Storing infinite numbers of patterns in a spin-glass model of neural networks",
      "author" : [ "D.J. Amit", "H. Gutfreund", "H. Sompolinsky" ],
      "venue" : "Physical Review Letters,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1985
    }, {
      "title" : "The capacity of the Hopfield associative memory",
      "author" : [ "R.J. McEliece", "E.C. Posner", "E.R. Rodemich", "S.S. Venkatesh" ],
      "venue" : "Information Theory, IEEE Transactions on,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1987
    }, {
      "title" : "Associative recall of memory without errors",
      "author" : [ "I. Kanter", "H. Sompolinsky" ],
      "venue" : "Physical Review A,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1987
    }, {
      "title" : "High order correlation model for associative memory",
      "author" : [ "H.H. Chen", "Y.C. Lee", "G.Z. Sun", "H.Y. Lee", "T. Maxwell", "C.L. Giles", "August" ],
      "venue" : "In Neural Networks for Computing (Vol. 151,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1986
    }, {
      "title" : "Nonlinear discriminant functions and associative memories",
      "author" : [ "D. Psaltis", "C.H. Park", "August" ],
      "venue" : "In Neural networks for computing (Vol. 151,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1986
    }, {
      "title" : "Number of stable points for spin-glasses and neural networks of higher orders",
      "author" : [ "P. Baldi", "S.S. Venkatesh" ],
      "venue" : "Physical Review Letters,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1987
    }, {
      "title" : "Multiconnected neural network models",
      "author" : [ "E. Gardner" ],
      "venue" : "Journal of Physics A: Mathematical and General,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 1987
    }, {
      "title" : "Storage capacity of generalized networks",
      "author" : [ "L.F. Abbott", "Y. Arian" ],
      "venue" : "Physical Review A,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1987
    }, {
      "title" : "Capacities of multiconnected memory models",
      "author" : [ "D. Horn", "M. Usher" ],
      "venue" : "Journal de Physique,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 1988
    }, {
      "title" : "Perceptron: an introduction to computational geometry",
      "author" : [ "M. Minsky", "S. Papert" ],
      "venue" : null,
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1969
    }, {
      "title" : "August. Best practices for convolutional neural networks applied to visual document analysis",
      "author" : [ "P.Y. Simard", "D. Steinkraus", "J.C. Platt" ],
      "venue" : "In null (p",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2003
    }, {
      "title" : "Dropout: A simple way to prevent neural networks from overfitting",
      "author" : [ "N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2014
    }, {
      "title" : "Regularization of neural networks using dropconnect",
      "author" : [ "L. Wan", "M. Zeiler", "S. Zhang", "Y.L. Cun", "R. Fergus" ],
      "venue" : "In Proceedings of the 30th International Conference on Machine Learning (ICML-",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Pattern recognition and models of associative memory [1] are closely related.",
      "startOffset" : 53,
      "endOffset" : 56
    }, {
      "referenceID" : 0,
      "context" : "The standard model of associative memory works well in the limit when the number of stored patterns is much smaller than the number of neurons [1], or equivalently the number of pixels in an image.",
      "startOffset" : 143,
      "endOffset" : 146
    }, {
      "referenceID" : 1,
      "context" : "These issues were largely resolved by introducing unsupervised pre-training, which made it possible to initialize the weights in such a way that the subsequent backpropagation could only gently move boundaries between the classes without destroying the feature detectors [2, 3].",
      "startOffset" : 271,
      "endOffset" : 277
    }, {
      "referenceID" : 2,
      "context" : "These issues were largely resolved by introducing unsupervised pre-training, which made it possible to initialize the weights in such a way that the subsequent backpropagation could only gently move boundaries between the classes without destroying the feature detectors [2, 3].",
      "startOffset" : 271,
      "endOffset" : 277
    }, {
      "referenceID" : 3,
      "context" : "More recently, it was realized that the use of rectified linear units (ReLU) instead of the logistic functions speeds up learning and improves generalization [4, 5, 6].",
      "startOffset" : 158,
      "endOffset" : 167
    }, {
      "referenceID" : 4,
      "context" : "More recently, it was realized that the use of rectified linear units (ReLU) instead of the logistic functions speeds up learning and improves generalization [4, 5, 6].",
      "startOffset" : 158,
      "endOffset" : 167
    }, {
      "referenceID" : 5,
      "context" : "More recently, it was realized that the use of rectified linear units (ReLU) instead of the logistic functions speeds up learning and improves generalization [4, 5, 6].",
      "startOffset" : 158,
      "endOffset" : 167
    }, {
      "referenceID" : 0,
      "context" : "The standard model of associative memory [1] uses a system of N binary neurons, with values ±1.",
      "startOffset" : 41,
      "endOffset" : 44
    }, {
      "referenceID" : 0,
      "context" : "It has been demonstrated [1, 7, 8] that in case of random memories this maximal value is of the order of K ≈ 0.",
      "startOffset" : 25,
      "endOffset" : 34
    }, {
      "referenceID" : 6,
      "context" : "It has been demonstrated [1, 7, 8] that in case of random memories this maximal value is of the order of K ≈ 0.",
      "startOffset" : 25,
      "endOffset" : 34
    }, {
      "referenceID" : 7,
      "context" : "It has been demonstrated [1, 7, 8] that in case of random memories this maximal value is of the order of K ≈ 0.",
      "startOffset" : 25,
      "endOffset" : 34
    }, {
      "referenceID" : 8,
      "context" : "Hamiltonian (1) in a way that removes second order correlations between the stored memories, it is possible [9] to improve the capacity to K = N .",
      "startOffset" : 108,
      "endOffset" : 111
    }, {
      "referenceID" : 0,
      "context" : "In the case of the polynomial function with n = 2 the network reduces to the standard model of associative memory [1].",
      "startOffset" : 114,
      "endOffset" : 117
    }, {
      "referenceID" : 9,
      "context" : "For the case of polynomial energy function a very similar family of models was considered in [10, 11, 12, 13, 14, 15].",
      "startOffset" : 93,
      "endOffset" : 117
    }, {
      "referenceID" : 10,
      "context" : "For the case of polynomial energy function a very similar family of models was considered in [10, 11, 12, 13, 14, 15].",
      "startOffset" : 93,
      "endOffset" : 117
    }, {
      "referenceID" : 11,
      "context" : "For the case of polynomial energy function a very similar family of models was considered in [10, 11, 12, 13, 14, 15].",
      "startOffset" : 93,
      "endOffset" : 117
    }, {
      "referenceID" : 12,
      "context" : "For the case of polynomial energy function a very similar family of models was considered in [10, 11, 12, 13, 14, 15].",
      "startOffset" : 93,
      "endOffset" : 117
    }, {
      "referenceID" : 13,
      "context" : "For the case of polynomial energy function a very similar family of models was considered in [10, 11, 12, 13, 14, 15].",
      "startOffset" : 93,
      "endOffset" : 117
    }, {
      "referenceID" : 14,
      "context" : "For the case of polynomial energy function a very similar family of models was considered in [10, 11, 12, 13, 14, 15].",
      "startOffset" : 93,
      "endOffset" : 117
    }, {
      "referenceID" : 11,
      "context" : "For higher powers n the capacity rapidly grows with N in a non-linear way, allowing the network to store and reliably retrieve many more patterns than the number of neurons that it has, in accord with [12, 13, 14, 15].",
      "startOffset" : 201,
      "endOffset" : 217
    }, {
      "referenceID" : 12,
      "context" : "For higher powers n the capacity rapidly grows with N in a non-linear way, allowing the network to store and reliably retrieve many more patterns than the number of neurons that it has, in accord with [12, 13, 14, 15].",
      "startOffset" : 201,
      "endOffset" : 217
    }, {
      "referenceID" : 13,
      "context" : "For higher powers n the capacity rapidly grows with N in a non-linear way, allowing the network to store and reliably retrieve many more patterns than the number of neurons that it has, in accord with [12, 13, 14, 15].",
      "startOffset" : 201,
      "endOffset" : 217
    }, {
      "referenceID" : 14,
      "context" : "For higher powers n the capacity rapidly grows with N in a non-linear way, allowing the network to store and reliably retrieve many more patterns than the number of neurons that it has, in accord with [12, 13, 14, 15].",
      "startOffset" : 201,
      "endOffset" : 217
    }, {
      "referenceID" : 11,
      "context" : "References [12, 13, 14] do not allow repeated indices in the products over neurons in the energy function, therefore obtain a different coefficient.",
      "startOffset" : 11,
      "endOffset" : 23
    }, {
      "referenceID" : 12,
      "context" : "References [12, 13, 14] do not allow repeated indices in the products over neurons in the energy function, therefore obtain a different coefficient.",
      "startOffset" : 11,
      "endOffset" : 23
    }, {
      "referenceID" : 13,
      "context" : "References [12, 13, 14] do not allow repeated indices in the products over neurons in the energy function, therefore obtain a different coefficient.",
      "startOffset" : 11,
      "endOffset" : 23
    }, {
      "referenceID" : 14,
      "context" : "In [15] the Hamiltonian coincides with ours, but the update rule is different, which, however, results in exactly the same coefficient as in (6).",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 15,
      "context" : "It can also be thought of as a linear perceptron, and the inability to solve this problem represents the well known statement [16] that linear perceptrons cannot compute XOR without hidden neurons.",
      "startOffset" : 126,
      "endOffset" : 130
    }, {
      "referenceID" : 16,
      "context" : "6% range [17], see also controls in [18, 19].",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 17,
      "context" : "6% range [17], see also controls in [18, 19].",
      "startOffset" : 36,
      "endOffset" : 44
    }, {
      "referenceID" : 18,
      "context" : "6% range [17], see also controls in [18, 19].",
      "startOffset" : 36,
      "endOffset" : 44
    }, {
      "referenceID" : 5,
      "context" : "A similar effect was reported earlier for the transition between saturating units, such as logistics or hyperbolic tangents, to ReLU [6].",
      "startOffset" : 133,
      "endOffset" : 136
    }, {
      "referenceID" : 3,
      "context" : "This is consistent with the previous observation that ReLU are faster in training than hyperbolic tangents and logistics [4, 5, 6].",
      "startOffset" : 121,
      "endOffset" : 130
    }, {
      "referenceID" : 4,
      "context" : "This is consistent with the previous observation that ReLU are faster in training than hyperbolic tangents and logistics [4, 5, 6].",
      "startOffset" : 121,
      "endOffset" : 130
    }, {
      "referenceID" : 5,
      "context" : "This is consistent with the previous observation that ReLU are faster in training than hyperbolic tangents and logistics [4, 5, 6].",
      "startOffset" : 121,
      "endOffset" : 130
    }, {
      "referenceID" : 11,
      "context" : "We especially thank Yasser Roudi for pointing out the reference [12] to us.",
      "startOffset" : 64,
      "endOffset" : 68
    } ],
    "year" : 2016,
    "abstractText" : "A model of associative memory is studied, which stores and reliably retrieves many more patterns than the number of neurons in the network. We propose a simple duality between this dense associative memory and neural networks commonly used in deep learning. On the associative memory side of this duality, a family of models that smoothly interpolates between two limiting cases can be constructed. One limit is referred to as the feature-matching mode of pattern recognition, and the other one as the prototype regime. On the deep learning side of the duality, this family corresponds to feedforward neural networks with one hidden layer and various activation functions, which transmit the activities of the visible neurons to the hidden layer. This family of activation functions includes logistics, rectified linear units, and rectified polynomials of higher degrees. The proposed duality makes it possible to apply energy-based intuition from associative memory to analyze computational properties of neural networks with unusual activation functions – the higher rectified polynomials which until now have not been used for training neural networks. The utility of the dense memories is illustrated for two test cases: the logical gate XOR and the recognition of handwritten digits from the MNIST data set.",
    "creator" : "LaTeX with hyperref package"
  }
}