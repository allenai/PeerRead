{
  "name" : "1405.5066.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "An optimization algorithm inspired by the States of Matter that improves the balance between exploration and exploitation",
    "authors" : [ "Erik Cuevas", "Alonso Echavarría", "Marte A. Ramírez-Ortegón" ],
    "emails" : [ "erik.cuevas@cucei.udg.mx" ],
    "sections" : [ {
      "heading" : null,
      "text" : "This is a preprint copy that has been accepted for publication in Applied Intelligence\nGlobal optimization [1] has delivered applications for many areas of science, engineering, economics and others, where mathematical modelling is used [2]. In general, the goal is to find a global optimum for an objective function which is defined over a given search space. Global optimization algorithms are usually broadly divided into deterministic and stochastic [3]. Since deterministic methods only provide a theoretical guarantee of locating a local minimum of the objective function, they often face great difficulties in solving global optimization problems [4]. On the other hand, evolutionary algorithms are usually faster in locating a global optimum [5]. Moreover, stochastic methods adapt easily to black-box formulations and extremely ill-behaved functions, whereas deterministic methods usually rest on at least some theoretical assumptions about the problem formulation and its analytical properties (such as Lipschitz continuity) [6]. Evolutionary algorithms, which are considered as members of the stochastic group, have been developed by a combination of rules and randomness that mimics several natural phenomena. Such phenomena include evolutionary processes such as the Evolutionary Algorithm (EA) proposed by Fogel et al. [7], De Jong [8], and Koza [9], the Genetic Algorithm (GA) proposed by Holland [10] and Goldberg [11], the Artificial Immune System proposed by De Castro et al. [12] and the Differential Evolution Algorithm\n1 Corresponding author, Tel +52 33 1378 5900, ext. 27714, E-mail: erik.cuevas@cucei.udg.mx\nThis is a preprint copy that has been accepted for publication in Applied Intelligence\n(DE) proposed by Price & Storn [13]. Some other methods which are based on physical processes include the Simulated Annealing proposed by Kirkpatrick et al. [14], the Electromagnetism-like Algorithm proposed by İlker et al. [15] and the Gravitational Search Algorithm proposed by Rashedi et al. [16]. Also, there are other methods based on the animal-behavior phenomena such as the Particle Swarm Optimization (PSO) algorithm proposed by Kennedy & Eberhart [17] and the Ant Colony Optimization (ACO) algorithm proposed by Dorigo et al. [18]. Every EA needs to address the issue of exploration-exploitation of the search space. Exploration is the process of visiting entirely new points of a search space whilst exploitation is the process of refining those points within the neighborhood of previously visited locations, in order to improve their solution quality. Pure exploration degrades the precision of the evolutionary process but increases its capacity to find new potential solutions. On the other hand, pure exploitation allows refining existent solutions but adversely driving the process to local optimal solutions. Therefore, the ability of an EA to find a global optimal solution depends on its capacity to find a good balance between the exploitation of found-so-far elements and the exploration of the search space [19]. So far, the exploration–exploitation dilemma has been an unsolved issue within the framework of EA. Although PSO, DE and GSA are considered the most popular algorithms for many optimization applications, they fail in finding a balance between exploration and exploitation [20]; in multimodal functions, they do not explore the whole region effectively and often suffers premature convergence or loss of diversity. In order to deal with this problem, several proposals have been suggested in the literature [21-46]. In most of the approaches, exploration and exploitation is modified by the proper settings of control parameters that have an influence on the algorithm´s search capabilities [47]. One common strategy is that EAs should start with exploration and then gradually change into exploitation [48]. Such a policy can be easily described with deterministic approaches where the operator that controls the individual diversity decreases along with the evolution. This is generally correct, but such a policy tends to face difficulties when solving certain problems with multimodal functions that hold many optima, since a premature takeover of exploitation over exploration occurs. Some approaches that use this strategy can be found in [21-29]. Other works [30-34] use the population size as reference to change the balance between exploration and exploitation. A larger population size implies a wider exploration while a smaller population demands a shorter search. Although this technique delivers an easier way to keep diversity, it often represents an unsatisfactory solution. An improper handling of large populations might converge to only one point, despite introducing more function evaluations. Recently, new operators have been added to several traditional evolutionary algorithms in order to improve their original explorationexploitation capability. Such operators diversify particles whenever they concentrate on a local optimum. Some methods that employ this technique are discussed in [35-46]. Either of these approaches is necessary but not sufficient to tackle the problem of the exploration– exploitation balance. Modifying the control parameters during the evolution process without the incorporation of new operators to improve the population diversity, makes the algorithm defenseless against the premature convergence and may result in poor exploratory characteristics of the algorithm [48]. On the other hand, incorporating new operators without modifying the control parameters leads to increase the computational cost, weakening the exploitation process of candidate regions [39]. Therefore, it does seem reasonable to incorporate both of these approaches into a single algorithm. In this paper, a novel nature-inspired algorithm, known as the States of Matter Search (SMS) is proposed for solving global optimization problems. The SMS algorithm is based on the simulation of the states of matter phenomenon. In SMS, individuals emulate molecules which interact to each other by using evolutionary operations based on the physical principles of the thermal-energy motion mechanism. Such operations allow the increase of the population diversity and avoid the concentration of particles within a local minimum. The proposed approach combines the use of the defined operators with a control strategy that modifies the parameter setting of each operation during the evolution process. In contrast to other approaches that enhance traditional EA algorithms by incorporating some procedures for balancing the exploration–exploitation rate, the proposed algorithm naturally delivers such property as a result of mimicking the states of matter phenomenon. The algorithm is devised by considering each state of matter at one different exploration–exploitation ratio. Thus, the evolutionary process is divided into three stages which emulate the three states of matter: gas, liquid and solid. At each state, molecules (individuals) exhibit different behaviors. Beginning from the gas state (pure exploration), the algorithm modifies the intensities of exploration and exploitation until the solid state (pure exploitation) is reached. As a result, the approach can substantially improve the balance between exploration–exploitation, yet preserving the\nThis is a preprint copy that has been accepted for publication in Applied Intelligence\ngood search capabilities of an evolutionary approach. To illustrate the proficiency and robustness of the proposed algorithm, it has been compared to other well-known evolutionary methods including recent variants that incorporate diversity preservation schemes. The comparison examines several standard benchmark functions which are usually employed within the EA field. Experimental results show that the proposed method achieves good performance over its counterparts as a consequence of its better exploration–exploitation capability. This paper is organized as follows. Section 2 introduces basic characteristics of the three states of matter. In Section 3, the novel SMS algorithm and its characteristics are both described. Section 4 presents experimental results and a comparative study. Finally, in Section 5, some conclusions are discussed."
    }, {
      "heading" : "2. States of matter",
      "text" : "In the gas phase, molecules present enough kinetic energy so that the effect of intermolecular forces is small (or zero for an ideal gas), while the typical distance between neighboring molecules is greater than the molecular size. A gas has no definite shape or volume, but occupies the entire container in which it is confined. Fig. 1a shows the movements exerted by particles in a gas state. The movement experimented by the molecules represent the maximum permissible displacement 1ρ among particles [50]. In a liquid state, intermolecular forces are more restrictive than those in the gas state. The molecules have enough energy to move relatively to each other still keeping a mobile structure. Therefore, the shape of a liquid is not definite but is determined by its container. Fig. 1b presents a particle movement 2ρ within a liquid state. Such movement is smaller than those considered by the gas state but larger than the solid state [51]. In the solid state, particles (or molecules) are packed together closely with forces among particles being strong enough so that the particles cannot move freely but only vibrate. As a result, a solid has a stable, definite shape and a definite volume. Solids can only change their shape by force, as when they are broken or cut. Fig. 1c shows a molecule configuration in a solid state. Under such conditions, particles are able to vibrate (being perturbed) considering a minimal 3ρ distance [50].\nIn this paper, a novel nature-inspired algorithm known as the States of Matter Search (SMS) is proposed for solving global optimization problems. The SMS algorithm is based on the simulation of the states of matter phenomenon that considers individuals as molecules which interact to each other by using evolutionary operations based on the physical principles of the thermal-energy motion mechanism. The algorithm is devised by considering each state of matter at one different exploration–exploitation ratio. Thus, the evolutionary process is divided into three stages which emulate the three states of matter: gas, liquid and solid. In each state, individuals exhibit different behaviors.\nThis is a preprint copy that has been accepted for publication in Applied Intelligence"
    }, {
      "heading" : "3. States of matter search (SMS)",
      "text" : ""
    }, {
      "heading" : "3.1 Definition of Operators",
      "text" : "In the approach, individuals are considered as molecules whose positions on a multidimensional space are modified as the algorithm evolves. The movement of such molecules is motivated by the analogy to the motion of thermal-energy. The velocity and direction of each molecule’s movement are determined by considering the collision, the attraction forces and the random phenomena experimented by the molecule set [52]. In our approach, such behaviors have been implemented by defining several operators such as the direction vector, the collision and the random positions operators, all of which emulate the behavior of actual physics laws. The direction vector operator assigns a direction to each molecule in order to lead the particle movement as the evolution process takes place. On the other side, the collision operator mimics those collisions that are experimented by molecules as they interact to each other. A collision is considered when the distance between two molecules is shorter than a determined proximity distance. The collision operator is thus implemented by interchanging directions of the involved molecules. In order to simulate the random behavior of molecules, the proposed algorithm generates random positions following a probabilistic criterion that considers random locations within a feasible search space. The next section presents all operators that are used in the algorithm. Although such operators are the same for all the states of matter, they are employed over a different configuration set depending on the particular state under consideration. 3.1.1 Direction vector\nThe direction vector operator mimics the way in which molecules change their positions as the evolution\nprocess develops. For each n-dimensional molecule i p from the population P, it is assigned an n-\ndimensional direction vector i d which stores the vector that controls the particle movement. Initially, all\nthe direction vectors ( 1 2{ , , , }pN=D d d dK ) are randomly chosen within the range of [-1,1].\nAs the system evolves, molecules experiment several attraction forces. In order to simulate such forces, the proposed algorithm implements the attraction phenomenon by moving each molecule towards the best so-far particle. Therefore, the new direction vector for each molecule is iteratively computed considering the following model:\n1 1 0.5 ,k ki i i k\ngen +  = ⋅ − ⋅ +    d d a\n(1)\nwhere i a represents the attraction unitary vector calculated as ( ) /best besti i i= − −a p p p p , being bestp the\nbest individual seen so-far, while i p is the molecule i of population P. k represents the iteration number whereas gen involves the total iteration number that constitutes the complete evolution process. Under this operation, each particle is moved towards a new direction which combines the past direction, which was initially computed, with the attraction vector over the best individual seen so-far. It is important to point out that the relative importance of the past direction decreases as the evolving process advances. This particular type of interaction avoids the quick concentration of information among particles and encourages each particle to search around a local candidate region in its neighborhood, rather than interacting to a particle lying at distant region of the domain. The use of this scheme has two advantages: first, it prevents the particles from moving toward the global best position in early stages of algorithm and thus makes the algorithm less susceptible to premature convergence; second, it encourages\nThis is a preprint copy that has been accepted for publication in Applied Intelligence\nparticles to explore their own neighborhood thoroughly, just before they converge towards a global best position. Therefore, it provides the algorithm with local search ability enhancing the exploitative behavior.\nIn order to calculate the new molecule position, it is necessary to compute the velocity i v of each\nmolecule by using:\ni i init v= ⋅v d\n(2)\nbeing init v the initial velocity magnitude which is calculated as follows:\n1\n( ) n high low\nj j\nj\ninit\nb b\nv β n\n= − = ⋅ ∑\n(3)\nwhere low j b and high j b are the low j parameter bound and the upper j parameter bound respectively,\nwhereas [0,1]β ∈ . Then, the new position for each molecule is updated by:\n1 , , , rand(0,1) ( ) k k high low i j i j i j j jp p v b b + = + ⋅ ⋅ ⋅ −ρ\n(4)\nwhere 0.5 1ρ≤ ≤ .\n3.1.2 Collision The collision operator mimics the collisions experimented by molecules while they interact to each other. Collisions are calculated if the distance between two molecules is shorter than a determined proximity\nvalue. Therefore, if i q r− <p p , a collision between molecules i and q is assumed; otherwise, there is no\ncollision, considering { }, 1, , pi q N∈ K such that i q≠ . If a collision occurs, the direction vector for each particle is modified by interchanging their respective direction vectors as follows:\ni q =d d and q i =d d (5)\nThe collision radius is calculated by:\n1\n( ) n high low\nj j\nj\nb b\nr α n\n= − = ⋅ ∑\n(6)\nwhere [0,1]α ∈ . Under this operator, a spatial region enclosed within the radius r is assigned to each particle. In case the particle regions collide to each other, the collision operator acts upon particles by forcing them out of the region. The radio r and the collision operator provide the ability to control diversity throughout the search\nThis is a preprint copy that has been accepted for publication in Applied Intelligence\nprocess. In other words, the rate of increase or decrease of diversity is predetermined for each stage. Unlike other diversity-guided algorithms, it is not necessary to inject diversity into the population when particles gather around a local optimum because the diversity will be preserved during the overall search process. The collision incorporation therefore enhances the exploratory behavior in the proposed approach. 3.1.3 Random positions\nIn order to simulate the random behavior of molecules, the proposed algorithm generates random positions following a probabilistic criterion within a feasible search space. For this operation, a uniform random number\nm r is generated within the range [0,1]. If m r is smaller than a\nthreshold H, a random molecule´s position is generated; otherwise, the element remains with no change. Therefore such operation can be modeled as follows:\n1 , 1\n,\nrand(0,1) ( ) with probability\nwith probability (1- )\nlow high low\nj j jk i j k\ni j\nb b b H p\np H\n+ +  + ⋅ −=  \n(7)\nwhere { }1, , pi N∈ K and { }1, ,j n∈ K . 3.1.4 Best Element Updating Despite this updating operator does not belong to State of Matter metaphor, it is used to simply store the\nbest so-far solution. In order to update the best molecule bestp seen so-far, the best found individual from\nthe current k population ,best kp is compared to the best individual , 1best k −p of the last generation. If ,best kp is\nbetter than , 1best k −p according to its fitness value, bestp is updated with ,best kp , otherwise bestp remains with\nno change. Therefore, bestp stores the best historical individual found so-far."
    }, {
      "heading" : "3.2 SMS algorithm",
      "text" : "The overall SMS algorithm is composed of three stages corresponding to the three States of Matter: the gas, the liquid and the solid state. Each stage has its own behavior. In the first stage (gas state), exploration is intensified whereas in the second one (liquid state) a mild transition between exploration and exploitation is executed. Finally, in the third phase (solid state), solutions are refined by emphasizing the exploitation process. 3.2.1 General procedure At each stage, the same operations are implemented. However, depending on which state is referred, they are employed considering a different parameter configuration. The general procedure in each state is shown as pseudo-code in Algorithm 1. Such procedure is composed by five steps and maps the current population kP to a new population 1k +P . The algorithm receives as input the current population kP and the configuration parameters ρ , β , α , and H, whereas it yields the new population 1k +P .\nStep 1: Find the best element of the population P { } { }1 2( ) max ( ), ( ), , ( )pbest best Nf f f f∈ =p P p p p pK Step 2: Calculate\ninit v and r\n1\n( ) n high low\nj j\nj\ninit\nb b\nv β n\n= − = ⋅ ∑\n1 ( )\nn high low\nj j\nj\nb b\nr α n\n= − = ⋅ ∑\nThis is a preprint copy that has been accepted for publication in Applied Intelligence\nStep 3: Compute the new molecules by using the Direction vector operator 3.1.1 for (i=1; i<\np N +1; i++)\n( ) /best besti i i= − −a p p p p\nfor (j=1; j<n+1; j++)\n1 , , ,1 0.5 k k i j i j i j\nk d d a\ngen +  = ⋅ − ⋅ +   \n1 , , k i j i j init v d v += ⋅\n1 , , , rand(0,1) ( ) k k high low i j i j i j j jp p v b b + = + ⋅ ⋅ ⋅ −ρ\nend for end for\nStep 4: Solve collisions by using the Collision operator 3.1.2 for (i=1; i<\np N +1; i++)\nfor (j=1; j< p N +1; j++)\nif (( i j r− <p p ) and ( i j≠ ))\ni =t d\ni j\n=d d\nj =d t\nend if end for end for\nStep 5: Generate new random positions by using the Random positions operator 3.1.3\nfor (i=1; i< p N +1; i++)\nif ( m r < H) then; where rand(0,1) m r ∈ for (j=1; j<n+1; j++) 1\n, rand(0,1) ( ) k low high low i j j j j p b b b + = + ⋅ − end for end if end for\nAlgorithm 1. General procedure executed by all the states of matter.\n3.2.2 The complete algorithm The complete algorithm is divided into four different parts. The first corresponds to the initialization stage, whereas the last three represent the States of Matter. All the optimization process, which consists of a gen number of iterations, is organized into three different asymmetric phases, employing 50% of all iterations for the gas state (exploration), 40% for the liquid state (exploration-exploitation) and 10% for the solid state (exploitation). The overall process is graphically described by Figure 2. At each state, the same general procedure (see Algorithm 1) is iteratively used considering the particular configuration predefined for each State of Matter. Figure 3 shows the data flow for the complete SMS algorithm.\nThis is a preprint copy that has been accepted for publication in Applied Intelligence\nInitialization The algorithm begins by initializing a set P of\np N molecules ( 1 2{ , , , }pN=P p p pK ). Each molecule\nposition i p is a n-dimensional vector containing the parameter values to be optimized. Such values are randomly and uniformly distributed between the pre-specified lower initial parameter bound low j b and the\nupper initial parameter bound high j b , just as it is described by the following expressions:\n0 , rand(0,1) ( ) low high low i j j j j p b b b= + ⋅ −\n1, 2, , ; 1, 2, , , p\nj n i N= =K K (8)\nwhere j and i, are the parameter and molecule index respectively whereas zero indicates the initial population. Hence, j\ni p is the j-th parameter of the i-th molecule.\nGas state In the gas state, molecules experiment severe displacements and collisions. Such state is characterized by random movements produced by non-modeled molecule phenomena [52]. Therefore, the ρ value from the direction vector operator is set to a value near to one so that the molecules can travel longer distances. Similarly, the H value representing the random positions operator is also configured to a value around one, in order to allow the random generation for other molecule positions. The gas state is the first phase and lasts for the 50% of all iterations which compose the complete optimization process. The computational procedure for the gas state can be summarized as follows: Step 1: Set the parameters [0.8,1]ρ ∈ , 0.8β = , 0.8α = and H=0.9 being consistent with the gas\nstate. Step 2: Apply the general procedure which is illustrated in Algorithm 1. Step 3: If the 50% of the total iteration number is completed (1 0.5 )k gen≤ ≤ ⋅ , then the process\nThis is a preprint copy that has been accepted for publication in Applied Intelligence\ncontinues to the liquid state procedure; otherwise go back to step 2. Liquid state Although molecules currently at the liquid state exhibit restricted motion in comparison to the gas state, they still show a higher flexibility with respect to the solid state. Furthermore, the generation of random positions which are produced by non-modeled molecule phenomena is scarce [53]. For this reason, the ρ value from the direction vector operator is bounded to a value between 0.3 and 0.6. Similarly, the random position operator H is configured to a value near to cero in order to allow the random generation of fewer molecule positions. In the liquid state, collisions are also less common than in gas state, so the collision radius, that is controlled byα , is set to a smaller value in comparison to the gas state. The liquid state is the second phase and lasts the 40% of all iterations which compose the complete optimization process. The computational procedure for the liquid state can be summarized as follows: Step 4: Set the parameters [0.3,0.6]ρ ∈ , 0.4β = , 0.2α = and H=0.2 being consistent with the\nliquid state. Step 5: Apply the general procedure that is defined in Algorithm 1. Step 6: If the 90% (50% from the gas state and 40% from the liquid state) of the total iteration\nnumber is completed (0.5 0.9 )gen k gen⋅ < ≤ ⋅ , then the process continues to the solid state procedure; otherwise go back to step 5.\nSolid state In the solid state, forces among particles are stronger so that particles cannot move freely but only vibrate. As a result, effects such as collision and generation of random positions are not considered [52]. Therefore, the ρ value of the direction vector operator is set to a value near to zero indicating that the molecules can only vibrate around their original positions. The solid state is the third phase and lasts for the 10% of all iterations which compose the complete optimization process. The computational procedure for the solid state can be summarized as follows: Step 7: Set the parameters [0.0,0.1]ρ ∈ and 0.1β = , 0α = and H=0 being consistent with the\nsolid state. Step 8: Apply the general procedure that is defined in Algorithm 1. Step 9: If the 100% of the total iteration number is completed (0.9 )gen k gen⋅ < ≤ , the process is\nfinished; otherwise go back to step 8. It is important to clarify that the use of this particular configuration ( 0α = and H=0) disables the collision and generation of random positions operators which have been illustrated in the general procedure."
    }, {
      "heading" : "4. Experimental results",
      "text" : "A comprehensive set of 24 functions, collected from Refs. [54-61], has been used to test the performance of the proposed approach. Tables A1–A4 in the Appendix A present the benchmark functions used in our experimental study. Such functions are classified into four different categories: Unimodal test functions (Table A1), multimodal test functions (Table A2), multimodal test functions with fixed dimensions (Table A3) and functions proposed for the GECCO contest (Table A4). In such tables, n indicates the dimension\nof the function, opt\nf the optimum value of the function and S the subset of nR . The function optimum\nposition ( opt x ) for 1f , 2f , 4f , 6f , 7f 10f , 11f and 14f is at [ ]0 nopt =x , for 3f , 8f and 9f is at [ ]1 nopt =x , for 5f is at [ ]420.96 nopt =x , for 18f is at [ ]0 nopt =x , for 12f is at [ ]0.0003075 nopt =x and for 13f is at\n[ ]3.32 nopt = −x . In case of functions contained in Table A4, the optx and optf values have been set to\nThis is a preprint copy that has been accepted for publication in Applied Intelligence\ndefault values which have been obtained from the Matlab© implementation for GECCO competitions, as it is provided in [59]. A detailed description of optimum locations is given in Appendix A."
    }, {
      "heading" : "4.1 Performance comparison to other meta-heuristic algorithms",
      "text" : "We have applied the SMS algorithm to 24 functions whose results have been compared to those produced by the Gravitational Search Algorithm (GSA) [16], the Particle Swarm Optimization (PSO) method [17] and the Differential Evolution (DE) algorithm [13]. These are considered as the most popular algorithms in many optimization applications. In order to enhance the performance analysis, the PSO algorithm with a territorial diversity-preserving scheme (TPSO) [39] has also been added into the comparisons. TPSO is considered a recent PSO variant that incorporates a diversity preservation scheme in order to improve the balance between exploration and exploitation. In all comparisons, the population has been set to 50. The maximum iteration number for functions in Tables A1, A2 and A4 has been set to 1000 and for functions in Table A3 has been set to 500. Such stop criterion has been selected to maintain compatibility to similar works reported in the literature [4,16]. The parameter setting for each algorithm in the comparison is described as follows:\n1. GSA [16]: The parameters are set to 100 o G = and 20α = ; the total number of iterations is set to 1000 for functions 1f to 11f and 500 for functions 12f to 14f . The total number of individuals is\nset to 50. Such values are the best parameter set for this algorithm according to [16].\n2. PSO [17]: The parameters are set to 1 2c = and 2 2c = ; besides, the weight factor decreases linearly from 0.9 to 0.2.\n3. DE [13]: The DE/Rand/1 scheme is employed. The crossover probability is set to 0.9CR = and the weight factor is set to 0.8F = .\n4. TPSO [39]: The parameterα has been set to 0.5. Such value is found to be the best configuration according to [39]. The algorithm has been tuned according to the set of values which have been originally proposed by its own reference.\nThe experimental comparison between metaheuristic algorithms with respect to SMS has been developed according to the function-type classification as follows:\n1. Unimodal test functions (Table A1). 2. Multimodal test functions (Table A2). 3. Multimodal test functions with fixed dimension (Table A3). 4. Test functions from the GECCO contest (Table A4).\nUnimodal test functions This experiment is performed over the functions presented in Table A1. The test compares the SMS to other algorithms such as GSA, PSO, DE and TPSO. The results for 30 runs are reported in Table 1 considering the following performance indexes: the Average Best-so-far (AB) solution, the Median Bestso-far (MB) and the Standard Deviation (SD) of best-so-far solution. The best outcome for each function is boldfaced. According to this table, SMS delivers better results than GSA, PSO, DE and TPSO for all functions. In particular, the test remarks the largest difference in performance which is directly related to a better trade-off between exploration and exploitation. Just as it is illustrated by Figure 4, SMS, DE and GSA have similar convergence rates at finding the optimal minimal, yet faster than PSO and TPSO. A non-parametric statistical significance proof known as the Wilcoxon’s rank sum test for independent samples [62,63] has been conducted over the “average best-so-far” (AB) data of Table 1, with an 5% significance level. Table 2 reports the p-values produced by Wilcoxon’s test for the pair-wise comparison of the “average best so-far” of four groups. Such groups are formed by SMS vs. GSA, SMS vs. PSO, SMS vs. DE and SMS vs. TPSO. As a null hypothesis, it is assumed that there is no significant difference between mean values of the two algorithms. The alternative hypothesis considers a significant difference\nThis is a preprint copy that has been accepted for publication in Applied Intelligence\nbetween the “average best-so-far” values of both approaches. All p-values reported in Table 2 are less than 0.05 (5% significance level) which is a strong evidence against the null hypothesis. Therefore, such evidence indicates that SMS results are statistically significant and that it has not occurred by coincidence (i.e. due to common noise contained in the process).\nMultimodal test functions Multimodal functions represent a good optimization challenge as they possess many local minima (Table A2). In the case of multimodal functions, final results are very important since they reflect the algorithm’s ability to escape from poor local optima and to locate a near-global optimum. Experiments using 5f to\n11f are quite relevant as the number of local minima for such functions increases exponentially as their dimensions increase. The dimension of such functions is set to 30. The results are averaged over 30 runs, reporting the performance index for each function in Table 3 as follows: the Average Best-so-far (AB)\nThis is a preprint copy that has been accepted for publication in Applied Intelligence\nsolution, the Median Best-so-far (MB) and the Standard Deviation (SD) best-so-far (the best result for each function is highlighted). Likewise, p-values of the Wilcoxon signed-rank test of 30 independent runs are listed in Table 4.\nIn the case of functions 8f , 9f , 10f and 11f , SMS yields much better solutions than other methods.\nHowever, for functions 5f , 6f and 7f , SMS produces similar results to GSA and TPSO. The Wilcoxon rank test results, which are presented in Table 4, demonstrate that SMS performed better than GSA, PSO, DE and TPSO considering four functions 8 11f f− , whereas, from a statistical viewpoint, there is no difference between results from SMS, GSA and TPSO for 5f , 6f and 7f . The progress of the “average\nbest-so-far” solution over 30 runs for functions 5f and 11f is shown by Fig. 5.\nThis is a preprint copy that has been accepted for publication in Applied Intelligence\nconsidering n=30. Multimodal test functions with fixed dimensions In the following experiments, the SMS algorithm is compared to GSA, PSO, DE and TPSO over a set of multidimensional functions with fixed dimensions which are widely used in the meta-heuristic literature. The functions used for the experiments are 12f , 13f and 14f which are presented in Table A3. The results in Table 5 show that SMS, GSA, PSO, DE and TPSO have similar values in their performance. The evidence shows how meta-heuristic algorithms maintain a similar average performance when they face low-dimensional functions [54]. Fig. 6 presents the convergence rate for the GSA, PSO, DE, SMS and TPSO algorithms considering functions 12f to 13f .\nTest functions from the GECCO contest\nThe experimental set in Table A4 includes several representative functions that are used in the GECCO contest. Using such functions, the SMS algorithm is compared to GSA, PSO, DE and TPSO. The results have been averaged over 30 runs, reporting the performance indexes for each algorithm in Table 6.\nThis is a preprint copy that has been accepted for publication in Applied Intelligence\nLikewise, p-values of the Wilcoxon signed-rank test of 30 independent executions are listed in Table 7. According to results of Table 6, it is evident that SMS yields much better solutions than other methods. The Wilcoxon test results in Table 7 provide information to statistically demonstrate that SMS has performed better than PSO, GSA, DE and TPSO. Figure 7 presents the convergence rate for the GSA, PSO, DE, SMS and TPSO algorithms, considering functions 17f to 24f .\nThis is a preprint copy that has been accepted for publication in Applied Intelligence"
    }, {
      "heading" : "5. Conclusions",
      "text" : "In this paper, a novel nature-inspired algorithm called as the States of Matter Search (SMS) has been introduced. The SMS algorithm is based on the simulation of the State of Matter phenomenon. In SMS, individuals emulate molecules which interact to each other by using evolutionary operations that are based on physical principles of the thermal-energy motion mechanism. The algorithm is devised by considering each state of matter at one different exploration–exploitation ratio. The evolutionary process is divided into three phases which emulate the three states of matter: gas, liquid and solid. At each state, molecules (individuals) exhibit different movement capacities. Beginning from the gas state (pure exploration), the algorithm modifies the intensities of exploration and exploitation until the solid state (pure exploitation) is reached. As a result, the approach can substantially improve the balance between exploration–exploitation, yet preserving the good search capabilities of an EA approach. SMS has been experimentally tested considering a suite of 24 benchmark functions. The performance of SMS has been also compared to the following evolutionary algorithms: the Particle Swarm Optimization method (PSO) [17], the Gravitational Search Algorithm (GSA) [16], the Differential Evolution (DE) algorithm [13] and the PSO algorithm with a territorial diversity-preserving scheme (TPSO) [39]. Results have confirmed a high performance of the proposed method in terms of the solution quality for solving most of benchmark functions. The SMS’s remarkable performance is associated with two different reasons: (i) the defined operators allow a better particle distribution in the search space, increasing the algorithm’s ability to find the global optima; and (ii) the division of the evolution process at different stages, provides different rates between exploration and exploitation during the evolution process. At the beginning, pure exploration is favored at the gas state, then a mild transition between exploration and exploitation features during liquid state. Finally, pure exploitation is performed during the solid state."
    }, {
      "heading" : "Appendix A. List of benchmark functions",
      "text" : "Test function\nS\nopt f\nn\n( ) 21 1 n ii f x = = ∑x [ ]100,100 n− 0 30\n( ) { }2 max ,1if x i n= ≤ ≤x [ ]100,100 n− 0 30\n( ) ( ) ( )21 223 11 100 1 n i i ii f x x x − +=  = − + −  ∑x [ ]30,30 n− 0 30\n( ) 44 1 (0,1) n ii f ix rand = = +∑x [ ]1.28,1.28 n− 0 30\nThis is a preprint copy that has been accepted for publication in Applied Intelligence\nTable A1. Unimodal test functions.\nTest function\nS\nopt f\nn\n( ) ( )( )5 1418.9829 sinn i iif n x x== + −∑x [ ]500,500 n− 0 30 ( ) ( )( )50 26 1 10cos 2 10i iif x xπ== − +∑x [ ]5.12,5.12 n− 0 30\n( ) 27 1 1 1\ncos 1 4000\nnn i ii i x f x i= =  = − +    ∑ ∏x [ ]600,600 n− 0 30 { }1 2 2 28 1 11 1( ) 10sin( ) ( 1) 1 10sin ( ) ( 1) ( ,10,100,4)n ni i n ii if y y y y u xn π π π− += = = + − + + − + ∑ ∑x\n( )1 1\n4 i i\nx y + = + ( )\n( )\n( ) , , , 0\nm\ni i\ni i\nm i\ni\nk x a x a\nu x a k m a x a\nx ak x a  − > = − ≤ ≤  <− −\n[ ]50,50 n− 0 30\n{ }2 2 2 2 29 1 1( ) 0.1 sin (3 ) ( 1) 1 sin (3 1) ( 1) 1 sin (2 )n i i n nif x x x x xπ π π=    = + − + + + − +   ∑x\n1 ( ,5,100,4)\nn ii u x = +∑ where ( ), , ,iu x a k m is the same as 8f\n[ ]50,50 n− 0 30\n( ) ( ) ( )2 4210 1 1 10.5 0.5n n ni i ii i if x ix ix= = == + +∑ ∑ ∑x [ ]10,10 n− 0 30\n( ) ( )11 1 cos 2 0.1f x xπ= − +x where 21 n ji x x = = ∑ [ ]100,100 n− 0 30\nTable A2. Multimodal test functions.\nTest function\nS\nopt f\nn\n( ) ( ) 2 2 11 2\n12 21 3 4\ni i i\nii i i\nx b b x f a\nb b x x=\n +  = −\n+ +   ∑x\n[ ]0.1957,0.1947,0.1735,0.1600,0.0844,0.0627,0.456,0.0342,0.0323,0.0235,0.0246=a\n[ ]0.25,0.5,1,2,4,6,8,10,12,14,16=b\n[ ]5,5 n− 0.00030 4\n( ) ( )( )24 313 1 1expi ij j iji jf c A x P= == − −∑ ∑x 10 3 17 3.5 1.7 8 0.05 10 17 0.1 8 14\n3 3.5 17 10 17 8\n17 8 0.05 10 0.1 14      =       A\n[ ]1,1.2,3,3.2=c\n0.131 0.169 0.556 0.012 0.828 0.588\n0.232 0.413 0.830 0.373 0.100 0.999\n0.234 0.141 0.352 0.288 0.304 0.665\n0.404 0.882 0.873 0.574 0.109 0.038      =       P\n[ ]0,1 n\n-3.32 6\n( ) ( )( ) ( )( ) ( )( )2 2 214 1 2 1 2 1 21.5 1 2.25 1 2.625 1f x x x x x x= − − + − − + − −x\n[ ]4.5,4.5 n−\n0 2\nTable A3. Multimodal test functions with fixed dimensions\nThis is a preprint copy that has been accepted for publication in Applied Intelligence\nTest function\nS\nn\nGECCO\nclassification\n6 2 15 1\n2\n( ) 10 n\ni opt\ni f z z f =\n= ⋅ + +∑x\n( )optoszT= −z x x : ,n noszT → for any positive integer n, it maps element-wise. ( ),oszT=a h { }1 2, , , na a a=a K , { }1 2, , , nh h h=h K ( ) ( ) ( )( )( )1 2=sign exp 0.049 sin sin ,i ia h K c K c K+ +\nwhere\nlog( ) if 0 , 0 otherwise i ih h K ≠ =  \n1 if 0\nsign( ) 0 if 0,\n1 if 0\ni\ni i\ni\nh\nh h\nh − < = =  >\n1\n10 if 0 5.5 otherwise ih c > =   and 2 7.9 if 0 3.1 otherwise ih c > =  \n[ ]5,5 n− 30 GECCO2010 Discus function 11( )f x\n1 2 4 1\n16 1\n( ) n i\nn i opt\ni\nf z f −+ −\n=\n= +∑x\nopt= −z x x\n[ ]5,5 n−\n30\nGECCO2010 Different Powers\nfunction\n14 ( )f x\n( )17 1 1 ( ) sin 4.189828872724339 100\n100\nn\ni i pen opt\ni f z z f f n =  = − + + +    ∑ z x ˆ 2 +−= × ⊗x 1 x ( )1 1 1 1ˆ ˆ ˆˆ ˆ, 0.25 opti i i iz x z x x x+ += = + − for 1,..., 1i n= − ( )( )10 ˆ100 opt opt= Λ − +z z x x\n: ,npenf →\n( )pena f= h , { }1 2, , , nh h h=h K\n( )21100 max 0, 5 n ii a h = = −∑\n+ −1 is a n-dimensional vector with elements of -1 or 1 computed considering equal probability.\n[ ]5,5 n− 30 GECCO2010 Schwefel function\n20( )f x\n2 18\n1\n( ) 450 n\ni\ni f z =\n= −∑x\nopt= −z x x\n[ ]100,100 n−\n30 GECCO2005 Shifted Sphere\nFunction\n1( )f x\n2\n19 1 1\n( ) 450 n i\nj\ni j f z = =\n  = − \n  ∑ ∑x\nopt= −z x x\n[ ]100,100 n−\n30 GECCO2005 Shifted\nSchwefel’s Problem\n2 ( )f x\n( )( ) 2\n20 1 1\n( ) 1 0.4 0,1 450 n i\nj\ni j f z N = =     = ⋅ + −      ∑ ∑x\nopt= −z x x\n[ ]100,100 n−\n30 GECCO2005 Shifted\nSchwefel’s Problem 1.2 with Noise in Fitness\n4 ( )f x\n{ }21( ) max 310if = − −x Ax b A is a n n× matrix, ,i ja are integer random numbers in the [ ]100,100 n−\n30 GECCO2005 Schwefel’s\nProblem 2.6 with\nThis is a preprint copy that has been accepted for publication in Applied Intelligence\nrange [ 500,500]− , det( ) 0≠A .\ni i= ⋅b A o iA is the i-th row of A whereas o is a 1n × vector whose elements are random numbers in the range [-100 100].\nGlobal Optimum on Bounds\n5 ( )f x\n( ) ( )( )2 2222 1 1 ( ) 100 1 390 n i i i i f z z z+ = = − + − +∑x opt= −z x x\n[ ]100,100 n−\n30 GECCO2005 Shifted\nRosenbrock’s Function 6( )f x\n( )( ) 2\n23 1\n( ) 460 n\ni i\ni f A B =\n= − −∑x x\n( ), , 1\nsin cos n\ni i j j i j j\nj A a bα α = = +∑ ( ) ( ), , 1\nsin cos n\ni i j j i j j\ni B a x b x =\n= +∑x\nFor 1,...,i n= .\n,i ja and ,i jb are integer random numbers in the range [-100,100],\n[ ]1 2, ,..., nα α α=α , jα are random numbers in the range [ ],π π− .\n[ ], nπ π−\n30 GECCO2005 Schwefel’s\nProblem 2.13\n12( )f x\n10\n24 1\nˆ( ) ( ) /opt i i i\ni f F λ = = −∑x x x ( )1 2F − =x Ackley’s function\n( ) ( )2 1 1\n1 1 20exp 0.2 exp cos 2 20\nn n\ni i i\ni i\nF x x D D π = =    = − − − +        ∑ ∑x\n( )3 4F − =x Rastringin’s function\n( ) ( )( )2 1\n10cos 2 10 n\ni i i\ni F x xπ = = − +∑x ( )5 6F − =x Sphere function\n( ) 2 1\nn\ni i\ni F x =\n= ∑x\n( )7 8F − =x Weierstrass function\n( ) ( )( ) ( )( ) max max\n1 0 0\ncos 2 0.5 cos 2 0.5 k kn k k k k\ni i i\ni k k F x a b x n a b xπ π = = =\n     = + − ⋅    \n  ∑ ∑ ∑\n( )9 10F − =x Griewank’s function\n( ) 2\n1 1\ncos 1 4000\nnn i i\ni\ni i\nx x F\ni= =  = − +    ∑ ∏x maxˆ ( ) ( ) /\ni i i F F F=z z . max i F is the maximum value of the particular\nfunction i.\n10 5 10 5 10 5 , ,2,1, , ,20,10, , 32 32 100 100 60 60  =     λ\n[ ]5,5 n−\n30 GECCO2005 Rotated Version\nof Hybrid Composition\nFunction\n16( )f x\nThe optx and optf values have been set to default values which have been obtained from the Matlab©\nimplementation for GECCO competitions, as it is provided in [51].\nTable A4. Set of representative GECCO functions."
    } ],
    "references" : [ {
      "title" : "Dynamic group-based differential evolution using a self-adaptive strategy for global optimization problems, Applied Intelligence, DOI 10.1007/s10489-012-0393-5",
      "author" : [ "Ming-Feng Han", "Shih-Hui Liao", "Jyh-Yeong Chang", "Chin-Teng Lin" ],
      "venue" : "Applied Intelligence,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2014
    }, {
      "title" : "Recent developments and trends in global optimization",
      "author" : [ "M. Pardalos Panos", "H. Romeijn Edwin", "Hoang Tuy" ],
      "venue" : "Journal of Computational and Applied Mathematics",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2000
    }, {
      "title" : "Global optimization in the 21st century: Advances and challenges",
      "author" : [ "C. Floudas", "I. Akrotirianakis", "S. Caratzoulas", "C. Meyer", "J. Kallrath" ],
      "venue" : "Computers & Chemical Engineering,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2005
    }, {
      "title" : "A deterministic global optimization algorithm",
      "author" : [ "J. Ying", "Z. Ke-Cun", "Q. Shao-Jian" ],
      "venue" : "Applied Mathematics and Computation,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2007
    }, {
      "title" : "Global optimization based on novel heuristics, low-discrepancy sequences and genetic algorithms",
      "author" : [ "A. Georgieva", "I. Jordanov" ],
      "venue" : "European Journal of Operational Research,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2009
    }, {
      "title" : "Lipschitz and Hölder global optimization using space-filling curves",
      "author" : [ "D. Lera", "Sergeyev", "Ya" ],
      "venue" : "Applied Numerical Mathematics,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2010
    }, {
      "title" : "Artificial Intelligence through Simulated Evolution",
      "author" : [ "L.J. Fogel", "A.J. Owens", "M.J. Walsh" ],
      "venue" : null,
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1966
    }, {
      "title" : "Analysis of the behavior of a class of genetic adaptive systems, Ph.D. Thesis, University of Michigan",
      "author" : [ "K. De Jong" ],
      "venue" : "Ann Arbor, MI,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1975
    }, {
      "title" : "Genetic programming: a paradigm for genetically breeding populations of computer programs to solve problems",
      "author" : [ "J.R. Koza" ],
      "venue" : null,
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1990
    }, {
      "title" : "Adaptation in Natural and Artificial Systems, University of Michigan Press",
      "author" : [ "J.H. Holland" ],
      "venue" : "Ann Arbor, MI,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1975
    }, {
      "title" : "Genetic Algorithms in Search, Optimization and Machine",
      "author" : [ "D.E. Goldberg" ],
      "venue" : null,
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1989
    }, {
      "title" : "Artificial immune systems: Part I – basic theory and applications",
      "author" : [ "de Castro LN", "Von Zuben FJ" ],
      "venue" : "Technical report,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1999
    }, {
      "title" : "Differential evolution-a simple and efficient adaptive scheme for global optimisation over continuous spaces",
      "author" : [ "R. Storn", "K. Price" ],
      "venue" : "Tech. Rep. TR-95–012,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 1995
    }, {
      "title" : "Optimization by simulated annealing, Science",
      "author" : [ "S. Kirkpatrick", "C. Gelatt", "M. Vecchi" ],
      "venue" : null,
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1983
    }, {
      "title" : "An Electromagnetism-like Mechanism for Global Optimization",
      "author" : [ "B. İlker", "S. Birbil", "F. Shu-Cherng" ],
      "venue" : "Journal of Global Optimization,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2003
    }, {
      "title" : "Filter modeling using Gravitational Search Algorithm",
      "author" : [ "E. Rashedia", "H. Nezamabadi-pour", "S. Saryazdi" ],
      "venue" : "Engineering Applications of Artificial Intelligence,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2011
    }, {
      "title" : "Particle swarm optimization, in Proceedings",
      "author" : [ "J. Kennedy", "R. Eberhart" ],
      "venue" : "IEEE International Conference on Neural Networks,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 1995
    }, {
      "title" : "Positive feedback as a search strategy",
      "author" : [ "M. Dorigo", "V. Maniezzo", "A. Colorni" ],
      "venue" : "Technical Report No",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 1991
    }, {
      "title" : "Balancing exploration and exploitation with adaptive variation for evolutionary multi-objective optimization",
      "author" : [ "K.C. Tan", "S.C. Chiam", "A.A. Mamun", "C.K. Goh" ],
      "venue" : "European Journal of Operational Research",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2009
    }, {
      "title" : "Preserving and exploiting genetic diversity in evolutionary programming algorithms",
      "author" : [ "G. Chen", "C.P. Low", "Z. Yang" ],
      "venue" : "IEEE Transactions on Evolutionary Computation",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2009
    }, {
      "title" : "To explore or to exploit: An entropy-driven approach for evolutionary algorithms. International Journal of Knowledge-based and Intelligent Engineering Systems",
      "author" : [ "Liu", "S.-H", "M. Mernik", "B. Bryant" ],
      "venue" : null,
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2009
    }, {
      "title" : "The exploration/exploitation tradeoff in dynamic cellular genetic algorithms",
      "author" : [ "E. Alba", "B. Dorronsoro" ],
      "venue" : "IEEE Transactions on Evolutionary Computation",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2005
    }, {
      "title" : "A hybrid self-adaptive evolutionary algorithm for marker optimization in the clothing industry",
      "author" : [ "I. Fister", "M. Mernik", "B. Filipič" ],
      "venue" : "Applied Soft Computing",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2010
    }, {
      "title" : "Enhancing the performance of differential evolution using orthogonal design method",
      "author" : [ "W. Gong", "Z. Cai", "L. Jiang" ],
      "venue" : "Applied Mathematics and Computation",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2008
    }, {
      "title" : "Parameter tuning of pbil and chc evolutionary algorithms applied to solve the root identification problem",
      "author" : [ "R. Joan-Arinyo", "M.V. Luzon", "E. Yeguas" ],
      "venue" : "Applied Soft Computing",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2011
    }, {
      "title" : "Differential evolution algorithm with ensemble of parameters and mutation strategies",
      "author" : [ "R. Mallipeddi", "P.N. Suganthan", "Q.K. Pan", "M.F. Tasgetiren" ],
      "venue" : "Applied Soft Computing",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2011
    }, {
      "title" : "An Intelligent Tuned Harmony Search algorithm for optimization",
      "author" : [ "Parikshit Yadav", "Rajesh Kumar", "S.K. Panda", "C.S. Chang" ],
      "venue" : "Information Sciences,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2012
    }, {
      "title" : "A modified gravitational search algorithm for slope stability analysis",
      "author" : [ "Mohammad Khajehzadeh", "Mohd Raihan Taha", "Ahmed El-Shafie", "Mahdiyeh Eslami" ],
      "venue" : "Engineering Applications of Artificial Intelligence,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2012
    }, {
      "title" : "A saw-tooth genetic algorithm combining the effects of variable population size and reinitialization to enhance performance",
      "author" : [ "V. Koumousis", "C.P. Katsaras" ],
      "venue" : "IEEE Transactions on Evolutionary Computation",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2006
    }, {
      "title" : "Dynamic group-based differential evolution using a self-adaptive strategy for global optimization problems",
      "author" : [ "Ming-Feng Han", "Shih-Hui Liao", "Jyh-Yeong Chang", "Chin-Teng Lin" ],
      "venue" : "Applied Intelligence,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2012
    }, {
      "title" : "Population size reduction for the differential evolution algorithm",
      "author" : [ "Janez Brest", "Mirjam Sepesy Maučec" ],
      "venue" : "Applied Intelligence,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2008
    }, {
      "title" : "Multi-population co-genetic algorithm with double chain-like agents structure for parallel global numerical optimization",
      "author" : [ "Yongming Li", "Xiaoping Zeng" ],
      "venue" : "Applied Intelligence,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2010
    }, {
      "title" : "Balancing population- and individual-level adaptation in changing environments",
      "author" : [ "I. Paenke", "Y. Jin", "J. Branke" ],
      "venue" : "Adaptive Behavior",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2009
    }, {
      "title" : "Diversity through multiculturality: Assessing migrant choice policies in an island model",
      "author" : [ "Araujo", "J.J.L. Merelo" ],
      "venue" : "IEEE Transactions on Evolutionary Computation",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 2011
    }, {
      "title" : "Particle swarm algorithm with hybrid mutation strategy",
      "author" : [ "Gao", "W.H. Xu" ],
      "venue" : "Applied Soft Computing",
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 2011
    }, {
      "title" : "An effective memetic differential evolution algorithm based on chaotic local search",
      "author" : [ "D. Jia", "G. Zheng", "M.K. KHAN" ],
      "venue" : "Information Sciences 181,",
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 2011
    }, {
      "title" : "Replacement strategies to preserve useful diversity in steadystate genetic algorithms. Information Sciences",
      "author" : [ "M. Lozano", "F. Herrera", "J.R. Cano" ],
      "venue" : "Applied Intelligence,",
      "citeRegEx" : "38",
      "shortCiteRegEx" : "38",
      "year" : 2008
    }, {
      "title" : "Optimizing the modified fuzzy antminer for efficient medical diagnosis",
      "author" : [ "Thannob Aribarg", "Siriporn Supratid", "Chidchanok Lursinsap" ],
      "venue" : "Applied Intelligence,",
      "citeRegEx" : "42",
      "shortCiteRegEx" : "42",
      "year" : 2012
    }, {
      "title" : "A novel method for coevolving PS-optimizing negotiation strategies using improved diversity controlling EDAs",
      "author" : [ "Jeonghwan Gwak", "Kwang Mong Sim" ],
      "venue" : "Applied Intelligence,",
      "citeRegEx" : "44",
      "shortCiteRegEx" : "44",
      "year" : 2013
    }, {
      "title" : "Effective local evolutionary searches distributed on an island model solving bi-objective optimization problems",
      "author" : [ "Hossein Rajabalipour Cheshmehgaz", "Mohammad Ishak Desa", "Antoni Wibowo" ],
      "venue" : "Applied Intelligence,",
      "citeRegEx" : "45",
      "shortCiteRegEx" : "45",
      "year" : 2013
    }, {
      "title" : "Diversity management in evolutionary many-objective optimization",
      "author" : [ "S.F. Adra", "P.J. Fleming" ],
      "venue" : "IEEE Transactions on Evolutionary Computation",
      "citeRegEx" : "47",
      "shortCiteRegEx" : "47",
      "year" : 2011
    }, {
      "title" : "Exploration and Exploitation in Evolutionary Algorithms: A Survey",
      "author" : [ "M. Črepineš", "S.H. Liu", "M. Mernik" ],
      "venue" : "ACM Computing Surveys,",
      "citeRegEx" : "48",
      "shortCiteRegEx" : "48",
      "year" : 2011
    }, {
      "title" : "Infodynamics: Analogical analysis of states of matter and information",
      "author" : [ "G. Ceruti", "H. Rubin" ],
      "venue" : "Information Sciences",
      "citeRegEx" : "49",
      "shortCiteRegEx" : "49",
      "year" : 2007
    }, {
      "title" : "Principles of equilibrium statistical mechanics, 1° edition 2000, Wiley-VCH",
      "author" : [ "Debashish Chowdhury", "Dietrich Stauffer" ],
      "venue" : null,
      "citeRegEx" : "50",
      "shortCiteRegEx" : "50",
      "year" : 2000
    }, {
      "title" : "Differential Evolution algorithm with Separated Groups for multi-dimensional optimization problems,",
      "author" : [ "A.P. Piotrowski", "J.J. Napiorkowski", "A. Kiczko" ],
      "venue" : "European Journal of Operational Research vol. 216,",
      "citeRegEx" : "54",
      "shortCiteRegEx" : "54",
      "year" : 2012
    }, {
      "title" : "A hybrid shuffled complex evolution approach based on differential evolution for unconstrained optimization",
      "author" : [ "V. Cocco Mariani", "L.G. Justi Luvizotto", "F. Alessandro Guerra", "Leandro dos Santos Coelho" ],
      "venue" : "Applied Mathematics and Computation 217 Vol. 217, no. 12, pp. 5822–5829, 2011.",
      "citeRegEx" : "55",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Evolutionary programming made faster",
      "author" : [ "X. Yao", "Y. Liu", "G. Lin" ],
      "venue" : "IEEE Transactions on Evolutionary Computation, vol. 3, no. 2, pp. 82–102, 1999.  Please cite this article as: Cuevas, E., Echavarría, A., Ramírez-Ortegón, M.A. An optimization algorithm inspired by the States of Matter that improves the balance between exploration and exploitation, Applied Intelligence, 40(2) , (2014), 256-272. This is a preprint copy that has been accepted for publication in Applied Intelligence  22",
      "citeRegEx" : "56",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Testing unconstrained optimization software",
      "author" : [ "J.J. Mor ́e", "B.S. Garbow", "K.E. Hillstrom" ],
      "venue" : "Association for Computing Machinery. Transactions on Mathematical Software, vol. 7, no. 1, pp. 17–41, 1981.",
      "citeRegEx" : "57",
      "shortCiteRegEx" : null,
      "year" : 1981
    }, {
      "title" : "Modifications of real code genetic algorithm for global optimization",
      "author" : [ "Ioannis G. Tsoulos" ],
      "venue" : "Applied Mathematics and Computation, vol. 203, no. 2, pp. 598–607, 2008.",
      "citeRegEx" : "58",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Tabu search with multi-level neighborhood structures for high dimensional problems",
      "author" : [ "Abdel-Rahman Hedar", "Ahmed Fouad Ali" ],
      "venue" : "Applied Intelligence,",
      "citeRegEx" : "60",
      "shortCiteRegEx" : "60",
      "year" : 2012
    }, {
      "title" : "CLA-DE: a hybrid model based on cellular learning automata for numerical optimization",
      "author" : [ "R. Vafashoar", "M.R. Meybodi", "A.H. Momeni Azandaryani" ],
      "venue" : "Applied Intelligence,",
      "citeRegEx" : "61",
      "shortCiteRegEx" : "61",
      "year" : 2012
    }, {
      "title" : "A study on the use of non-parametric tests for analyzing the evolutionary algorithms’ behaviour: a case study on the CEC’2005 Special session on real parameter optimization",
      "author" : [ "S Garcia", "D Molina", "M Lozano", "F Herrera" ],
      "venue" : "J Heurist. doi:10.1007/s10732-008-9080-4",
      "citeRegEx" : "62",
      "shortCiteRegEx" : "62",
      "year" : 2008
    }, {
      "title" : "A general framework for statistical performance comparison of evolutionary computation algorithms",
      "author" : [ "D. Shilane", "J. Martikainen", "S. Dudoit", "S.. Ovaska" ],
      "venue" : "Information Sciences",
      "citeRegEx" : "63",
      "shortCiteRegEx" : "63",
      "year" : 2008
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Global optimization [1] has delivered applications for many areas of science, engineering, economics and others, where mathematical modelling is used [2].",
      "startOffset" : 20,
      "endOffset" : 23
    }, {
      "referenceID" : 1,
      "context" : "Global optimization [1] has delivered applications for many areas of science, engineering, economics and others, where mathematical modelling is used [2].",
      "startOffset" : 150,
      "endOffset" : 153
    }, {
      "referenceID" : 2,
      "context" : "Global optimization algorithms are usually broadly divided into deterministic and stochastic [3].",
      "startOffset" : 93,
      "endOffset" : 96
    }, {
      "referenceID" : 3,
      "context" : "Since deterministic methods only provide a theoretical guarantee of locating a local minimum of the objective function, they often face great difficulties in solving global optimization problems [4].",
      "startOffset" : 195,
      "endOffset" : 198
    }, {
      "referenceID" : 4,
      "context" : "On the other hand, evolutionary algorithms are usually faster in locating a global optimum [5].",
      "startOffset" : 91,
      "endOffset" : 94
    }, {
      "referenceID" : 5,
      "context" : "Moreover, stochastic methods adapt easily to black-box formulations and extremely ill-behaved functions, whereas deterministic methods usually rest on at least some theoretical assumptions about the problem formulation and its analytical properties (such as Lipschitz continuity) [6].",
      "startOffset" : 280,
      "endOffset" : 283
    }, {
      "referenceID" : 6,
      "context" : "[7], De Jong [8], and Koza [9], the Genetic Algorithm (GA) proposed by Holland [10] and Goldberg [11], the Artificial Immune System proposed by De Castro et al.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 7,
      "context" : "[7], De Jong [8], and Koza [9], the Genetic Algorithm (GA) proposed by Holland [10] and Goldberg [11], the Artificial Immune System proposed by De Castro et al.",
      "startOffset" : 13,
      "endOffset" : 16
    }, {
      "referenceID" : 8,
      "context" : "[7], De Jong [8], and Koza [9], the Genetic Algorithm (GA) proposed by Holland [10] and Goldberg [11], the Artificial Immune System proposed by De Castro et al.",
      "startOffset" : 27,
      "endOffset" : 30
    }, {
      "referenceID" : 9,
      "context" : "[7], De Jong [8], and Koza [9], the Genetic Algorithm (GA) proposed by Holland [10] and Goldberg [11], the Artificial Immune System proposed by De Castro et al.",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 10,
      "context" : "[7], De Jong [8], and Koza [9], the Genetic Algorithm (GA) proposed by Holland [10] and Goldberg [11], the Artificial Immune System proposed by De Castro et al.",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 11,
      "context" : "[12] and the Differential Evolution Algorithm",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 12,
      "context" : "This is a preprint copy that has been accepted for publication in Applied Intelligence 2 (DE) proposed by Price & Storn [13].",
      "startOffset" : 120,
      "endOffset" : 124
    }, {
      "referenceID" : 13,
      "context" : "[14], the Electromagnetism-like Algorithm proposed by İlker et al.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 14,
      "context" : "[15] and the Gravitational Search Algorithm proposed by Rashedi et al.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 15,
      "context" : "[16].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 16,
      "context" : "Also, there are other methods based on the animal-behavior phenomena such as the Particle Swarm Optimization (PSO) algorithm proposed by Kennedy & Eberhart [17] and the Ant Colony Optimization (ACO) algorithm proposed by Dorigo et al.",
      "startOffset" : 156,
      "endOffset" : 160
    }, {
      "referenceID" : 17,
      "context" : "[18].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 18,
      "context" : "Therefore, the ability of an EA to find a global optimal solution depends on its capacity to find a good balance between the exploitation of found-so-far elements and the exploration of the search space [19].",
      "startOffset" : 203,
      "endOffset" : 207
    }, {
      "referenceID" : 19,
      "context" : "Although PSO, DE and GSA are considered the most popular algorithms for many optimization applications, they fail in finding a balance between exploration and exploitation [20]; in multimodal functions, they do not explore the whole region effectively and often suffers premature convergence or loss of diversity.",
      "startOffset" : 172,
      "endOffset" : 176
    }, {
      "referenceID" : 20,
      "context" : "In order to deal with this problem, several proposals have been suggested in the literature [21-46].",
      "startOffset" : 92,
      "endOffset" : 99
    }, {
      "referenceID" : 21,
      "context" : "In order to deal with this problem, several proposals have been suggested in the literature [21-46].",
      "startOffset" : 92,
      "endOffset" : 99
    }, {
      "referenceID" : 22,
      "context" : "In order to deal with this problem, several proposals have been suggested in the literature [21-46].",
      "startOffset" : 92,
      "endOffset" : 99
    }, {
      "referenceID" : 23,
      "context" : "In order to deal with this problem, several proposals have been suggested in the literature [21-46].",
      "startOffset" : 92,
      "endOffset" : 99
    }, {
      "referenceID" : 24,
      "context" : "In order to deal with this problem, several proposals have been suggested in the literature [21-46].",
      "startOffset" : 92,
      "endOffset" : 99
    }, {
      "referenceID" : 25,
      "context" : "In order to deal with this problem, several proposals have been suggested in the literature [21-46].",
      "startOffset" : 92,
      "endOffset" : 99
    }, {
      "referenceID" : 26,
      "context" : "In order to deal with this problem, several proposals have been suggested in the literature [21-46].",
      "startOffset" : 92,
      "endOffset" : 99
    }, {
      "referenceID" : 27,
      "context" : "In order to deal with this problem, several proposals have been suggested in the literature [21-46].",
      "startOffset" : 92,
      "endOffset" : 99
    }, {
      "referenceID" : 28,
      "context" : "In order to deal with this problem, several proposals have been suggested in the literature [21-46].",
      "startOffset" : 92,
      "endOffset" : 99
    }, {
      "referenceID" : 29,
      "context" : "In order to deal with this problem, several proposals have been suggested in the literature [21-46].",
      "startOffset" : 92,
      "endOffset" : 99
    }, {
      "referenceID" : 30,
      "context" : "In order to deal with this problem, several proposals have been suggested in the literature [21-46].",
      "startOffset" : 92,
      "endOffset" : 99
    }, {
      "referenceID" : 31,
      "context" : "In order to deal with this problem, several proposals have been suggested in the literature [21-46].",
      "startOffset" : 92,
      "endOffset" : 99
    }, {
      "referenceID" : 32,
      "context" : "In order to deal with this problem, several proposals have been suggested in the literature [21-46].",
      "startOffset" : 92,
      "endOffset" : 99
    }, {
      "referenceID" : 33,
      "context" : "In order to deal with this problem, several proposals have been suggested in the literature [21-46].",
      "startOffset" : 92,
      "endOffset" : 99
    }, {
      "referenceID" : 34,
      "context" : "In order to deal with this problem, several proposals have been suggested in the literature [21-46].",
      "startOffset" : 92,
      "endOffset" : 99
    }, {
      "referenceID" : 35,
      "context" : "In order to deal with this problem, several proposals have been suggested in the literature [21-46].",
      "startOffset" : 92,
      "endOffset" : 99
    }, {
      "referenceID" : 36,
      "context" : "In order to deal with this problem, several proposals have been suggested in the literature [21-46].",
      "startOffset" : 92,
      "endOffset" : 99
    }, {
      "referenceID" : 37,
      "context" : "In order to deal with this problem, several proposals have been suggested in the literature [21-46].",
      "startOffset" : 92,
      "endOffset" : 99
    }, {
      "referenceID" : 38,
      "context" : "In order to deal with this problem, several proposals have been suggested in the literature [21-46].",
      "startOffset" : 92,
      "endOffset" : 99
    }, {
      "referenceID" : 39,
      "context" : "In order to deal with this problem, several proposals have been suggested in the literature [21-46].",
      "startOffset" : 92,
      "endOffset" : 99
    }, {
      "referenceID" : 40,
      "context" : "In most of the approaches, exploration and exploitation is modified by the proper settings of control parameters that have an influence on the algorithm ́s search capabilities [47].",
      "startOffset" : 176,
      "endOffset" : 180
    }, {
      "referenceID" : 41,
      "context" : "One common strategy is that EAs should start with exploration and then gradually change into exploitation [48].",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 20,
      "context" : "Some approaches that use this strategy can be found in [21-29].",
      "startOffset" : 55,
      "endOffset" : 62
    }, {
      "referenceID" : 21,
      "context" : "Some approaches that use this strategy can be found in [21-29].",
      "startOffset" : 55,
      "endOffset" : 62
    }, {
      "referenceID" : 22,
      "context" : "Some approaches that use this strategy can be found in [21-29].",
      "startOffset" : 55,
      "endOffset" : 62
    }, {
      "referenceID" : 23,
      "context" : "Some approaches that use this strategy can be found in [21-29].",
      "startOffset" : 55,
      "endOffset" : 62
    }, {
      "referenceID" : 24,
      "context" : "Some approaches that use this strategy can be found in [21-29].",
      "startOffset" : 55,
      "endOffset" : 62
    }, {
      "referenceID" : 25,
      "context" : "Some approaches that use this strategy can be found in [21-29].",
      "startOffset" : 55,
      "endOffset" : 62
    }, {
      "referenceID" : 26,
      "context" : "Some approaches that use this strategy can be found in [21-29].",
      "startOffset" : 55,
      "endOffset" : 62
    }, {
      "referenceID" : 27,
      "context" : "Some approaches that use this strategy can be found in [21-29].",
      "startOffset" : 55,
      "endOffset" : 62
    }, {
      "referenceID" : 28,
      "context" : "Other works [30-34] use the population size as reference to change the balance between exploration and exploitation.",
      "startOffset" : 12,
      "endOffset" : 19
    }, {
      "referenceID" : 29,
      "context" : "Other works [30-34] use the population size as reference to change the balance between exploration and exploitation.",
      "startOffset" : 12,
      "endOffset" : 19
    }, {
      "referenceID" : 30,
      "context" : "Other works [30-34] use the population size as reference to change the balance between exploration and exploitation.",
      "startOffset" : 12,
      "endOffset" : 19
    }, {
      "referenceID" : 31,
      "context" : "Other works [30-34] use the population size as reference to change the balance between exploration and exploitation.",
      "startOffset" : 12,
      "endOffset" : 19
    }, {
      "referenceID" : 32,
      "context" : "Other works [30-34] use the population size as reference to change the balance between exploration and exploitation.",
      "startOffset" : 12,
      "endOffset" : 19
    }, {
      "referenceID" : 33,
      "context" : "Some methods that employ this technique are discussed in [35-46].",
      "startOffset" : 57,
      "endOffset" : 64
    }, {
      "referenceID" : 34,
      "context" : "Some methods that employ this technique are discussed in [35-46].",
      "startOffset" : 57,
      "endOffset" : 64
    }, {
      "referenceID" : 35,
      "context" : "Some methods that employ this technique are discussed in [35-46].",
      "startOffset" : 57,
      "endOffset" : 64
    }, {
      "referenceID" : 36,
      "context" : "Some methods that employ this technique are discussed in [35-46].",
      "startOffset" : 57,
      "endOffset" : 64
    }, {
      "referenceID" : 37,
      "context" : "Some methods that employ this technique are discussed in [35-46].",
      "startOffset" : 57,
      "endOffset" : 64
    }, {
      "referenceID" : 38,
      "context" : "Some methods that employ this technique are discussed in [35-46].",
      "startOffset" : 57,
      "endOffset" : 64
    }, {
      "referenceID" : 39,
      "context" : "Some methods that employ this technique are discussed in [35-46].",
      "startOffset" : 57,
      "endOffset" : 64
    }, {
      "referenceID" : 41,
      "context" : "Modifying the control parameters during the evolution process without the incorporation of new operators to improve the population diversity, makes the algorithm defenseless against the premature convergence and may result in poor exploratory characteristics of the algorithm [48].",
      "startOffset" : 276,
      "endOffset" : 280
    }, {
      "referenceID" : 42,
      "context" : "The differences among such states are based on forces which are exerted among particles composing a material [49].",
      "startOffset" : 109,
      "endOffset" : 113
    }, {
      "referenceID" : 43,
      "context" : "The movement experimented by the molecules represent the maximum permissible displacement 1 ρ among particles [50].",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 43,
      "context" : "Under such conditions, particles are able to vibrate (being perturbed) considering a minimal 3 ρ distance [50].",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 0,
      "context" : "Initially, all the direction vectors ( 1 2 { , , , } p N = D d d d K ) are randomly chosen within the range of [-1,1].",
      "startOffset" : 111,
      "endOffset" : 117
    }, {
      "referenceID" : 0,
      "context" : "where low j b and high j b are the low j parameter bound and the upper j parameter bound respectively, whereas [0,1] β ∈ .",
      "startOffset" : 111,
      "endOffset" : 116
    }, {
      "referenceID" : 0,
      "context" : "where [0,1] α ∈ .",
      "startOffset" : 6,
      "endOffset" : 11
    }, {
      "referenceID" : 0,
      "context" : "For this operation, a uniform random number m r is generated within the range [0,1].",
      "startOffset" : 78,
      "endOffset" : 83
    }, {
      "referenceID" : 44,
      "context" : "[54-61], has been used to test the performance of the proposed approach.",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 45,
      "context" : "[54-61], has been used to test the performance of the proposed approach.",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 46,
      "context" : "[54-61], has been used to test the performance of the proposed approach.",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 47,
      "context" : "[54-61], has been used to test the performance of the proposed approach.",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 48,
      "context" : "[54-61], has been used to test the performance of the proposed approach.",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 49,
      "context" : "[54-61], has been used to test the performance of the proposed approach.",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 50,
      "context" : "[54-61], has been used to test the performance of the proposed approach.",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 15,
      "context" : "We have applied the SMS algorithm to 24 functions whose results have been compared to those produced by the Gravitational Search Algorithm (GSA) [16], the Particle Swarm Optimization (PSO) method [17] and the Differential Evolution (DE) algorithm [13].",
      "startOffset" : 145,
      "endOffset" : 149
    }, {
      "referenceID" : 16,
      "context" : "We have applied the SMS algorithm to 24 functions whose results have been compared to those produced by the Gravitational Search Algorithm (GSA) [16], the Particle Swarm Optimization (PSO) method [17] and the Differential Evolution (DE) algorithm [13].",
      "startOffset" : 196,
      "endOffset" : 200
    }, {
      "referenceID" : 12,
      "context" : "We have applied the SMS algorithm to 24 functions whose results have been compared to those produced by the Gravitational Search Algorithm (GSA) [16], the Particle Swarm Optimization (PSO) method [17] and the Differential Evolution (DE) algorithm [13].",
      "startOffset" : 247,
      "endOffset" : 251
    }, {
      "referenceID" : 3,
      "context" : "Such stop criterion has been selected to maintain compatibility to similar works reported in the literature [4,16].",
      "startOffset" : 108,
      "endOffset" : 114
    }, {
      "referenceID" : 15,
      "context" : "Such stop criterion has been selected to maintain compatibility to similar works reported in the literature [4,16].",
      "startOffset" : 108,
      "endOffset" : 114
    }, {
      "referenceID" : 15,
      "context" : "GSA [16]: The parameters are set to 100 o G = and 20 α = ; the total number of iterations is set to 1000 for functions 1 f to 11 f and 500 for functions 12 f to 14 f .",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 15,
      "context" : "Such values are the best parameter set for this algorithm according to [16].",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 16,
      "context" : "PSO [17]: The parameters are set to 1 2 c = and 2 2 c = ; besides, the weight factor decreases linearly from 0.",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 12,
      "context" : "DE [13]: The DE/Rand/1 scheme is employed.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 51,
      "context" : "A non-parametric statistical significance proof known as the Wilcoxon’s rank sum test for independent samples [62,63] has been conducted over the “average best-so-far” (AB) data of Table 1, with an 5% significance level.",
      "startOffset" : 110,
      "endOffset" : 117
    }, {
      "referenceID" : 52,
      "context" : "A non-parametric statistical significance proof known as the Wilcoxon’s rank sum test for independent samples [62,63] has been conducted over the “average best-so-far” (AB) data of Table 1, with an 5% significance level.",
      "startOffset" : 110,
      "endOffset" : 117
    }, {
      "referenceID" : 44,
      "context" : "The evidence shows how meta-heuristic algorithms maintain a similar average performance when they face low-dimensional functions [54].",
      "startOffset" : 129,
      "endOffset" : 133
    }, {
      "referenceID" : 16,
      "context" : "The performance of SMS has been also compared to the following evolutionary algorithms: the Particle Swarm Optimization method (PSO) [17], the Gravitational Search Algorithm (GSA) [16], the Differential Evolution (DE) algorithm [13] and the PSO algorithm with a territorial diversity-preserving scheme (TPSO) [39].",
      "startOffset" : 133,
      "endOffset" : 137
    }, {
      "referenceID" : 15,
      "context" : "The performance of SMS has been also compared to the following evolutionary algorithms: the Particle Swarm Optimization method (PSO) [17], the Gravitational Search Algorithm (GSA) [16], the Differential Evolution (DE) algorithm [13] and the PSO algorithm with a territorial diversity-preserving scheme (TPSO) [39].",
      "startOffset" : 180,
      "endOffset" : 184
    }, {
      "referenceID" : 12,
      "context" : "The performance of SMS has been also compared to the following evolutionary algorithms: the Particle Swarm Optimization method (PSO) [17], the Gravitational Search Algorithm (GSA) [16], the Differential Evolution (DE) algorithm [13] and the PSO algorithm with a territorial diversity-preserving scheme (TPSO) [39].",
      "startOffset" : 228,
      "endOffset" : 232
    } ],
    "year" : 2014,
    "abstractText" : "The ability of an Evolutionary Algorithm (EA) to find a global optimal solution depends on its capacity to find a good rate between exploitation of found-so-far elements and exploration of the search space. Inspired by natural phenomena, researchers have developed many successful evolutionary algorithms which, at original versions, define operators that mimic the way nature solves complex problems, with no actual consideration of the explorationexploitation balance. In this paper, a novel nature-inspired algorithm called the States of Matter Search (SMS) is introduced. The SMS algorithm is based on the simulation of the states of matter phenomenon. In SMS, individuals emulate molecules which interact to each other by using evolutionary operations which are based on the physical principles of the thermal-energy motion mechanism. The algorithm is devised by considering each state of matter at one different exploration–exploitation ratio. The evolutionary process is divided into three phases which emulate the three states of matter: gas, liquid and solid. In each state, molecules (individuals) exhibit different movement capacities. Beginning from the gas state (pure exploration), the algorithm modifies the intensities of exploration and exploitation until the solid state (pure exploitation) is reached. As a result, the approach can substantially improve the balance between exploration–exploitation, yet preserving the good search capabilities of an evolutionary approach. To illustrate the proficiency and robustness of the proposed algorithm, it is compared to other well-known evolutionary methods including novel variants that incorporate diversity preservation schemes. The comparison examines several standard benchmark functions which are commonly considered within the EA field. Experimental results show that the proposed method achieves a good performance in comparison to its counterparts as a consequence of its better exploration–exploitation balance.",
    "creator" : "PDFCreator Version 1.2.0"
  }
}