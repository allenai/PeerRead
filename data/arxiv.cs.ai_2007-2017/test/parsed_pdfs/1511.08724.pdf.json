{
  "name" : "1511.08724.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Convergence in Navigational Reinforcement Learning",
    "authors" : [ "Tom J. Ameloot", "Jan Van den Bussche" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Contents"
    }, {
      "heading" : "1 Introduction 2",
      "text" : ""
    }, {
      "heading" : "2 Related Work 5",
      "text" : ""
    }, {
      "heading" : "3 Navigational Reinforcement Learning 7",
      "text" : "3.1 Tasks and Reducibility . . . . . . . . . . . . . . . . . . . . . . . . 7 3.2 Navigational Learning . . . . . . . . . . . . . . . . . . . . . . . . 8 3.3 Convergence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 3.4 Fairness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n∗T.J. Ameloot is a Postdoctoral Fellow of the Research Foundation – Flanders (FWO).\nar X\niv :1\n51 1.\n08 72\n4v 1\n[ cs\n.L G\n] 2\n7 N\nov 2\n01 5"
    }, {
      "heading" : "4 Results 12",
      "text" : "4.1 Sufficient Condition for Convergence . . . . . . . . . . . . . . . . 13 4.2 Detecting the Final Policy . . . . . . . . . . . . . . . . . . . . . . 18 4.3 Necessary Conditions for Convergence . . . . . . . . . . . . . . . 24"
    }, {
      "heading" : "5 Examples 27",
      "text" : "5.1 Grid Navigation Tasks . . . . . . . . . . . . . . . . . . . . . . . . 28 5.2 Chain Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31"
    }, {
      "heading" : "6 Conclusion and Further Work 33",
      "text" : ""
    }, {
      "heading" : "1 Introduction",
      "text" : "Reinforcement Learning Biological organisms can learn to perform tasks in environments, such as navigating from their nest to foraging areas and back again (Geva-Sagiv et al., 2015). In the field of reinforcement learning, task performance is measured as the amount of reward received (Sutton and Barto, 1998). For example, in a food foraging task, reward would be related to finding food or returning home. For each starting state of the task, the organism could learn which sequences of actions lead to reward. Concretely, the organism learns a so-called policy, which is a mapping from encountered states to immediate actions. When the organism encounters a state, the policy proposes an immediate action for that state, after which again a new state is encountered, for which the policy should again propose an action, and so on. The policy should direct the organism to reward: at each application of an action to a state, there is the possibility of obtaining an amount of reward, e.g., related to survival.\nIn many animal brains, the connections from states to actions is established with a global dopamine signal (Schultz, 2013, 2015). There would be neurons encoding (the perception of) the currently encountered state and there would be neurons for triggering actions. One could assume that connections are initially random between these types of neuron, meaning that initially there is no clear preference of actions for states (Frémaux et al., 2013). But dopamine is released when the organism receives a reward that is higher than expected, and dopamine reinforces the connections between recently active state neurons and output neurons.1 This way, actions that have apparently caused reward are more strongly attached to the states that appeared just before that reward. Conversely, when the organism receives a reward that is lower than expected, dopamine levels decrease below a baseline; connections between neurons are not reinforced and might even be weakened. So, sometimes previous state-action connections are again weakened if it later turns out that they did not reliably lead to reward.\nWeak connections from states to actions leads to random exploration of the task if actions may be triggered by spontaneous noise. But when connections from states to actions grow stronger under reward reinforcement, a policy emerges that exploits the previously gained experience to guide the organism to reward. The eventual stability of the policy is called convergence, which means\n1The expectation of reward could for example be represented by a third group of neurons, as in the actor-critic framework (Sutton and Barto, 1998; Potjans et al., 2011; Frémaux et al., 2013).\nthat after a while states will be associated to only one or a few actions, i.e., the random exploration is gone or at least strongly diminished. Convergence is the notion of eventual success that we would like to better understand.\nUnderstanding Convergence with Paths The formed policy strongly depends on the task and on the order in which actions are tried. It seems fascinating to understand how task properties and learning algorithms interact with each other, and to identify what causes convergence or what could prevent convergence. In this viewpoint, intelligence is not an intrinsic property of an organism, but rather an implicit property that emerges from the interaction of the task and the learning algorithm. The main aim in this article is to better understand how convergence arises from the cooperation between task properties on the one hand and simple learning algorithms on the other hand.\nIn standard reinforcement learning (Sutton and Barto, 1998), reward is represented by a numeric value on each transition of the task, where larger positive numbers represent higher rewards. Convergence would then be formalized as numeric stability of a so-called value function that maps each state to the expected average reward for the future encountered states.\nTo provide a complementary viewpoint, we believe that convergence of learning can also be fundamentally understood with a simple non-numeric framework. Indeed, the essential intuition from learning as explained above is that an organism should seek sequences of actions that lead from starting states to reward, i.e., the organism seeks a path in the state space of the task. The collection of all transitions in the state space constitutes a graph structure, and we do not have to assign numeric reward signals to the transitions to reason about the paths in that graph. Such state spaces could be part of a physical world, representing a map of physical places, or they can be more abstract with more than three dimensions (Frémaux et al., 2013). In any case, paths appear tangible and intuitive to imagine. Therefore, understanding convergence as the eventual stability of such paths might be very insightful.\nTo the best of our knowledge, it would be a novel contribution to theoretically investigate the path intuition in a formal and non-numeric framework of reinforcement learning, focusing mostly on the graph structure of tasks. Such a framework brings reinforcement learning theory closer to many areas of computer science that have a more discrete mathematics approach, notably the area of model checking (Baier and Katoen, 2008). Indeed, we study a property for convergence, called reducibility, that has an intuitive interpretation in graphs. This notion is discussed below. The graph interpretation of properties that lead to convergence exemplifies the kind of insights that we seek to better understand learning, and in particular how task properties interact with learning algorithms.\nSymbolic Framework and Reducibility In the symbolic framework, we formalize tasks as nondeterministic transition systems, where the transition function δ maps each pair (q, a) of a state q and an action a to a subset of states representing the possible successor states. The pair (q, a) represents the application of action a to state q. While performing the task, each application of an action to a state randomly chooses one state from the possible successor states. We include this nondeterminism to represent noise, as typically occurs\nin real-world applications (Thrun et al., 2005). In this initial study, we omit transition probabilities on the successor states, so in some sense all successor states are equally likely. To formalize reward, we flag some transitions as being immediately rewarding. This flag is just a boolean property, i.e., true or false. In the resulting formalization, characteristic properties of tasks are (i) the form of the transition graph and (ii) the choice of rewarding transitions. The states from which there is an immediately rewarding transition are called goal states.\nIn our framework, we have a learning algorithm that says at each time step whether we keep the old action associated to the currently encountered state or if instead a new action should be randomly generated; this is detailed a bit later below. The lastly tried action is stored in the policy. By applying actions stored in the policy or by generating new actions, the learning process moves from one state to the next, and a path is followed in the graph structure of the task. The learning process, also called run, is always infinite and is divided in so-called trials: once a reward transition is taken, the current trial ends and we return again to a start state of the task. We say that a run converges if all trials end with reward and eventually we stop generating new actions for the encountered states, i.e., the policy will become stable. In general, there are multiple runs because trials can choose their start states and because the applications of actions to states nondeterministically choose among the possible successor states. We say that a task is learnable if all runs of that task converge.\nConcretely, the learning algorithm detects whether a state is repeated in the same trial. Whenever a revisit to a state q occurs in the same trial, we generate a new random action for q. The motivation for this algorithm stems from the biologically plausible dopamine algorithm mentioned above: when we revisit a state in the same trial, the organism might be disappointed because it was expecting to have already reached reward instead of walking around in a cycle; this disappointment leads to further random exploration to avoid revisiting states again. The intention is that the run initially modifies the policy with random exploration until we eventually obtain a policy that will never again revisit a state in the same trial. But whether such a policy exists depends on the task at hand, which leads us to the following notion of reducibility.\nIf we would fix one action for each state q then we make a selection of possible transitions, i.e., we select a subgraph of the full transition graph. Now, a task appears solvable if we can assign an action to each state such that the resulting subgraph is acyclic and leads from start states to goal states: this means that there is a reliable strategy to reward. In that case, we say that the task is reducible. We would like to emphasize that reducibility is a notion of progress in the task transition graph, but it is not the same as determinism because each action application, i.e., transition, remains inherently nondeterministic. Instead, one may think of reducibility as onion layers in the state space: the core of the onion consists of the goal states, where immediate reward may be obtained, and, for states in outer layers there is an action that leads one step down to an inner layer, closer to reward. When traveling from an outer layer to an inner layer, the nondeterminism manifests itself as unpredictability on the exact state that is reached in the inner layer.\nResults Our first result is that, on reducible tasks, all runs converge. Reducibility is therefore a sufficient property for tasks to be learnable. Intuitively,\nif a strategy to reward exists, by reducibility, then the random exploration induced by the learning algorithm can eventually discover such a strategy. For some practical applications, this result might give understanding about why certain learning processes converge. We note that this result requires natural fairness assumptions, to guarantee that the learning process has the opportunity to explore better policies.\nIn our second result, we characterize the form of the final policy in converging runs. In particular, we characterize the final policy as a policy in which the reachable states (in the selected subgraph) are all reduced to reward by that same policy. So, the final policy will not steer the organism to task states from which that same policy will not reliably obtain reward. Besides providing a better insight in the form of the final policy, the characterization also allows the detection of convergence in simulations.\nAs a third and smaller result, we make a first step towards characterizing the tasks that are learnable, by showing necessary properties of those tasks: if a task is learnable then (i) all states reachable from the start states should have a path to reward, and (ii) all start states are reducible to reward. Condition (i) is subtle because the path to reward might diverge from reward at each nondeterministic transition on the path. We also briefly discuss unstable sets as a concept that appears related to the gap between the sufficient and necessary properties for tasks to be learnable.\nOutline This article is organized as follows. We discuss related work in Section 2. We formalize important concepts in Section 3. We present and prove our results in Section 4. We discuss examples in Section 5, and we conclude in Section 6."
    }, {
      "heading" : "2 Related Work",
      "text" : "Frémaux et al. (2013) and Potjans et al. (2011) elaborate the actor-critic framework of reinforcement learning in the context of neurons. In particular, Frémaux et al. (2013) study both physical and more abstract state spaces. As an example of a physical state space, they consider a navigation task in which a simulated mouse has to swim to a hidden platform where it can rest, where resting corresponds to reward; each state contains only the x and y coordinate. As an example of an abstract state space, they consider an acrobatic swinging task where reward is given when the tip of a double pendulum reaches a certain height; this space is abstract because each state contains two angles and two angular velocities, i.e., there are four dimensions. Conceptually it does not matter how many dimensions a state space has, because essentially the agent is always just seeking paths in the graph structure of the task.\nAs remarked by Sutton and Barto (1998, p. 104), a repeated theme in reinforcement learning is to update the policy and value estimation of the states while the agent visits them. This theme is also strongly present in the current article, because for each visited state the policy always remembers the lastly tried action for that state. The final aim for convergence, as studied in this article, is to eventually not choose any new actions anymore for the encountered state, and thus eventually only follow one policy.\nMany previous works in numerical reinforcement learning try to find optimal policies, and their related optimal value functions (Sutton, 1988; Watkins and Dayan, 1992; Dayan, 1992; Dayan and Sejnowski, 1994; Jaakkola et al., 1994; Tsitsiklis, 1994); an optimal policy gives the highest reward in the long run. This has motivated the design of numerical learning techniques. But the proof techniques for numerical convergence are usually very technical, and do not illuminate well how properties of the task state space interplay with a particular learning algorithm. With the framework introduced in this article, we hope to shed more light on properties of the task state space, in particular on the way that paths could be formed in the graph structure of the task. Although the graph-oriented framework has a different viewpoint compared to standard numerical reinforcement learning, we believe that our Theorem 4.1 and its proof contribute to making the fascinating idea of reinforcement learning more easily accessible to a wider audience.\nThe numerical reinforcement learning technique of temporal difference (TD) learning (Sutton, 1988; Sutton and Barto, 1998) has been associated to the way biological organisms learn. In particular, the global dopamine signal in the brain might encode the predication error that is key to temporal difference learning (Schultz, 2015). However, TD-learning is not perfectly matched to the dopamine signal because the negative TD-prediction error can only be represented by dopamine to a limited extent (Potjans et al., 2011; Schultz, 2013). One might therefore be motivated to further explore other forms of reinforcement learning that do not demand notions like positivity or negativity of numeric values. For example, a non-numeric framework could put the emphasis on the interaction among discrete components, such as cells and molecules, as they are observed in living organisms. This also drives a quest for simpler learning algorithms to operate on such discrete components. The learning algorithm discussed in this article, that detects cycles in the paths through the state space of the task, is arguably one of the simplest learning algorithms.\nPerhaps in contrast to the notion of optimality often used in the previous work on numerical reinforcement learning (Sutton and Barto, 1998), where the aim is to find a policy that gives the highest expected reward, we are only studying a very local form of optimality, where convergence is formalized as the eventual avoidance of cycles in the paths through the state space. The paths need not be the shortest.2 We believe that this viewpoint on convergence aligns well with animal and human learning, where an organism learns just some path to reward, heavily dependent on the experience (Frémaux et al., 2013). Perhaps animals do not detect cycles explicitly, but the detection of cycles might nonetheless be related to a mechanism that detects disappointment or boredom when the animal is taking too long to reach reward. In particular, disappointment could be related to the expectation of reward, and can be represented by the suppression of dopamine (Schultz, 2013).\nOn a more technical note, our Theorem 4.1, showing that convergence always occurs on reducible tasks (with our definition of convergence), is related in intent to previous results showing that certain numerical learning algorithms converge with probability 1 (Watkins and Dayan, 1992; Dayan, 1992; Dayan and Sejnowski, 1994; Tsitsiklis, 1994).\n2We do not associate costs to the transitions in the task state space, so paths can only be compared in length.\nAlso, the notion of reducibility discussed in this article is related to the principles of dynamic programming explained by Sutton and Barto (1998). Indeed, in reducibility, we defer the responsibility of obtaining reward from a given state to one of the successor states under a chosen action. This resembles the way in dynamic programming that reward prediction values for a given state can be estimated by looking at the reward prediction values of the successor states. In settings of standard numeric reinforcement learning, dynamic programming finds an optimal policy in a time that is worst-case polynomial in the number of states and actions; this time complexity is also applicable to our iterative reducibility procedure given in Section 3.1."
    }, {
      "heading" : "3 Navigational Reinforcement Learning",
      "text" : "We formalize tasks and the notion of reducibility in Section 3.1. Next, in Section 3.2, we use an operational semantics to formalize the interaction between a task and our learning algorithm that detects cycles. In Section 3.3, we define convergence as the eventual stability of the policy. Lastly, in Section 3.4, we impose certain fairness restrictions on the operational semantics."
    }, {
      "heading" : "3.1 Tasks and Reducibility",
      "text" : "Tasks To formalize tasks, we use nondeterministic transition systems where some transitions are labeled as being immediately rewarding, where reward is only an on-off flag. Formally, a task is a five-tuple\nT = (Q,Q0, A, rewards, δ)\nwhere Q, Q0, and A are nonempty finite sets; Q0 ⊆ Q; rewards is a nonempty subset of Q×A; and, δ is a function that maps each (q, a) ∈ Q×A to a nonempty subset of Q. The elements of Q, Q0, and A are called respectively states, start states, and actions. The set rewards tells us which pairs of states and actions give immediate reward. Function δ describes the possible successor states of applying actions to states.\nRemark 3.1. Our formalization of tasks keeps only the graph structure of models previously studied in reinforcement learning; essentially, compared to finite Markov decision processes (Sutton and Barto, 1998), we omit transition probabilities and we simplify the numerical reward signals to boolean flags. We also do not study negative feedback signals, so performed actions give either reward or no reward, i.e., the feedback is either positive or neutral. In this article we also do not consider the topic of generalization (Sutton and Barto, 1998), where an agent only sees concepts that are abstractions of states, and the agent learns to assign actions to concepts instead of directly to states; one concept could represent many states. We mention negative feedback signals and generalization as topics for further work in Section 6.\nReducibility Let T be a task as above. We define the set\ngoals(T ) = {q ∈ Q | ∃a ∈ A with (q, a) ∈ rewards}.\nWe refer to the elements of goals(T ) as goal states. Intuitively, for a goal state there is an action that reliably gives immediate reward. Each task has at least one goal state because the set rewards is always nonempty. The agent could learn a strategy to reduce all encountered states to goal states and then perform a rewarding action. This intuition is formalized next.\nLet V ⊆ Q. We formalize how states can be reduced to V . Let N0 denote the set of natural numbers without zero. First, we define the infinite sequence L1, L2, . . . , of sets where L1 = V , and for each i ≥ 2,"
    }, {
      "heading" : "Li = Li−1 ∪ {q ∈ Q | ∃a ∈ A with δ(q, a) ⊆ Li−1}.",
      "text" : "We call L1, L2, etc, the (reducibility) layers. We define reduce(T, V ) = ⋃ i∈N0 Li. Note that reduce(T, V ) ⊆ Q. Because Q is finite, there is an index n ∈ N0 for which Ln = Ln+1, i.e., Ln is a fixpoint. Letting V\n′ ⊆ Q, we say that V ′ is reducible to V if V ′ ⊆ reduce(T, V ). Intuitively, each state in V ′ can choose an action to come closer to V . We also say that a single state q ∈ Q is reducible to V if q ∈ reduce(T, V ).\nNow, we say that task T is reducible (to reward) if the state set Q is reducible to goals(T ). We use the abbreviation reduce(T ) = reduce(T, V ) where V = goals(T ). Reducibility formalizes a sense of solvability of tasks.\nWe illustrate the notion of reducibility with the following example.\nExample 3.2. We consider the task T = (Q,Q0, A, rewards, δ) defined as follows: Q = {1, 2, 3}; Q0 = {1}; A = {a, b}; rewards = {(3, a), (3, b)}; and, regarding δ, we define\nδ(1, a) = {1, 3}, δ(1, b) = {2}, δ(2, a) = {1, 3}, δ(2, b) = {3},\nδ(3, a) = δ(3, b) = {3}.\nTask T is visualized in Figure 3.1. Note that the task is reducible, by assigning the action b to both state 1 and state 2. The reducibility layers up to and including the fixpoint, are:\nL1 = goals(T ) = {3}, L2 = {3, 2}, L3 = {3, 2, 1}.\nFor simplicity, the assignments 1 7→ b and 2 7→ b form a deterministic strategy to reward. But we could easily extend task T to a task in which the strategy to reward is always subjected to nondeterminism, by adding a new state 4 with the new mappings δ(1, b) = {2, 4}, δ(4, a) = δ(4, b) = {3}."
    }, {
      "heading" : "3.2 Navigational Learning",
      "text" : "We now formalize a learning algorithm that operates on tasks, by means of an operational semantics that describes the steps taken over time. The inspiration\nfor the learning algorithm comes from biologically plausible learning algorithms based on the global dopamine signal in brains (Potjans et al., 2011; Frémaux et al., 2013; Schultz, 2013, 2015). When an organism achieves unexpected physical reward, dopamine is released to strengthen synaptic connections between recently activated neurons. In absence of strong synaptic connections from input neurons to output neurons, random search behaviors might be exhibited. In the framework of this article, and similar to other theoretical models in standard reinforcement learning (Sutton and Barto, 1998), the input neurons are abstractly represented by task states and output neurons are abstractly represented by actions. When a state is revisited in the same trial, this represents a form of disappointment because the agent was hoping to have already reached reward. If we continually mark each encountered state with a value visited, then we can detect that a state becomes revisited if that state is already marked as visited upon entry. The value visited can therefore represent weak connections from input neurons to output neurons, causing random search behavior. Conversely, stronger connections are represented by states still marked with the opposite value unvisited.\nIn the following, let T = (Q,Q0, A, rewards, δ) be a task.\nConfigurations A configuration of T is a triple c = (q, π, w), where q ∈ Q; π maps each q ∈ Q to an element of A; and w is a function that maps each q ∈ Q to either the value unvisited or the value visited. The functions π and w are called the policy and working memory, respectively. We refer to q as the current state in the configuration, and we also say that c contains the state q. Note that there are only a finite number of possible configurations. The goal of the learning algorithm is to refine the policy during trials, as we formalize below.\nRemark 3.3. In numeric reinforcement learning, there is a value function that usually converges together with the policy (Sutton and Barto, 1998). The working memory function in our framework can not directly be compared with such standard value functions, because it does not converge; rather, it is reset after each trial (see below).\nTransitions We formalize how to go from one configuration to another, to represent the steps of the running algorithm. Let c = (q, π, w) be a configuration. We say c is branching if w(q) = visited; this means that configuration c represents a revisit to state q, and that we want to generate a new action for the current state q. Next, we define the set opt(c) as follows: letting A′ = A if c is branching and A′ = {π(q)} otherwise, we define\nopt(c) = {(a, q′) | a ∈ A′ and q′ ∈ δ(q, a)}.\nIntuitively, opt(c) contains the options of actions and successor states that may be chosen directly after c. If c is branching then all actions may be chosen, and otherwise we must restrict attention to the action stored in the policy for the current state. Note that the successor state depends on the chosen action.\nNext, for a configuration c1 = (q1, π1, w1) and a pair (a, q ′) ∈ opt(c1), we define the successor configuration c2 = (q2, π2, w2) that results from the application of (a, q′) to c1, as follows:\n• q2 = q′;\n• π2(q1) = a and π2(r) = π1(r) for all r ∈ Q \\ {q1}; and,\n• w2(q1) = visited and w2(r) = w1(r) for all r ∈ Q \\ {q1}.\nWe emphasize that only the action and visited-flag of the state q1 is modified, where q1 is the state that is departed from. We denote the successor configuration as apply(c1, a, q ′).\nA transition t is a four-tuple (c1, a, q ′, c2), also denoted as c1 a, q′−−→ c2, where c1 = (q1, π1, w1) and c2 = (q2, π2, w2) are two configurations, (a, q\n′) ∈ opt(c1), and c2 = apply(c1, a, q\n′). We refer to c1 and c2 as the source configuration and target configuration, respectively. We say that t is a reward transition if (q1, a) ∈ rewards. Note that there are only a finite number of possible transitions because there are only a finite number of possible configurations.\nTrials and Runs A chain C is a sequence of transitions where for each pair (t1, t2) of subsequent transitions, the target configuration of t1 is the source configuration of t2. Chains could be finite or infinite.\nA trial is a chain C where either (i) the chain is infinite and contains no reward transitions; or, (ii) the chain is finite, ends with a reward transition, and contains no other reward transitions. To rephrase, if a trial is finite then it ends at the first occurrence of reward; and, if there is no reward transition than the trial must be infinite.\nIn a trial, we say that an occurrence of a configuration is terminal if that occurrence is the last configuration of the trial, i.e., the occurrence is the target configuration of the only reward transition. Note that an infinite trial contains no terminal configurations.\nA start configuration is any configuration c = (q, π, w) where q ∈ Q0 and w(r) = unvisited for each r ∈ Q; no constraints are imposed on the policy π.\nNow, a run R on the task T is a sequence of trials, where\n1. the run is either an infinite sequence of finite trials, or the run consists of a finite prefix of finite trials followed by one infinite trial;\n2. the first configuration of the first trial is a start configuration;\n3. whenever one (finite) trial ends with a configuration c1 = (q1, π1, w1) and the next trial starts with a configuration c2 = (q2, π2, w2), we have (i) q2 ∈ Q0; (ii) π2 = π1; and, (iii) w2(r) = unvisited for each r ∈ Q;3 and,\n4. if the run contains infinitely many trials then each state q ∈ Q0 is used at the beginning of infinitely many trials.\nWe put condition (3) in words: when one trial ends, we start the next trial with a start state, we reuse the policy, and we again reset the working memory by marking each state as unvisited. The policy is the essential product of a trial. Condition (4), saying that each start state is used at the beginning of infinitely many trials, expresses that we want to learn the whole task, with all possible start states.\nTo refer to a precise occurrence of a trial in a run, we use the ordinal of that occurrence, which is a nonzero natural number.\nRemark 3.4. The framework studied in this article may be called episodic (Sutton and Barto, 1998) because the agent should continuously navigate from start states to goal states; and, after obtaining the immediate reward at a goal state, the agent’s location is again reset to a start state."
    }, {
      "heading" : "3.3 Convergence",
      "text" : "We now define a convergence property to formalize when learning has stopped in a run. Consider a task T = (Q,Q0, A, rewards, δ). Let R be a run on T . Definition 3.5. We say that a state q ∈ Q (eventually) becomes stable in R if there are only finitely many non-terminal occurrences of branching configurations containing q.\nAn equivalent definition is to say that after a while there are no more branching configurations at non-terminal positions containing q. Intuitively, eventual stability of q means that after a while there is no risk anymore that q is paired with new actions, so q will definitely stay connected to the same action.4 Note that states appearing only a finite number of times in R always become stable under this definition.\nWe say that the run R converges if (i) all trials terminate (with reward), and (ii) eventually all states become stable. We say that the task T is learnable if all runs on T converge.\nRemark 3.6. In a run that converges, note that the policy will eventually become fixed because the only way to change the policy is through branching configurations at non-terminal positions. The lastly formed policy in a run is called the final policy, which is studied in more detail in Section 4.2. We emphasize that a converging run never stops, because runs are defined as being infinite; the final policy remains in use indefinitely, it is just not updated anymore.\nWe would also like to emphasize that in a converging run, eventually, the trials contain no cycles before reaching reward: the only moment in a trial\n3Note that c2 satisfies the definition of start configuration. 4If a branching configuration c is terminal in a trial, c can not influence the action of its\ncontained state because there is no subsequent transition anymore.\nwhere a state could be revisited, is in the terminal configuration, i.e., in the target configuration of the reward transition."
    }, {
      "heading" : "3.4 Fairness",
      "text" : "There are two choice points in each transition of the operational semantics:\n• if the source configuration of the transition is branching, i.e., the current state is revisited, then we choose a new random action for the current state; and,\n• whenever we apply an action a to a state q, we can in general choose among several possible successor states in δ(q, a).\nFairness assumptions are needed to give the algorithm sufficient opportunities to detect problems and try better policies (Francez, 1986). Intuitively, in both choice points, the choice should be independent of what the policy and working memory say about states other than the current state. This intuition is related to the Markov assumption, or independence of path assumption (Sutton and Barto, 1998). Below, we formalize this intuition as a fairness notion for the operational semantics of Section 3.2.\nWe say that a run R is fair if for each configuration c that occurs infinitely often at non-terminal positions, for each (a, q′) ∈ opt(c), the following transition occurs infinitely often:\nc a, q′−−→ apply(c, a, q′).\nWe say that a task T is learnable under fairness if all fair runs of T converge.\nRemark 3.7. There is always a fair run for any task, as follows. For each possible configuration c, we could conceptually order the set opt(c). During a run, we could also keep track for each occurrence i ∈ N0 of a configuration c how many times we have already seen configuration c in the run, excluding the current occurrence; we denote this number as count(i).\nWe begin the first trial with a random start configuration c1, i.e., we choose a random start state and a random policy. We next choose the option (a1, q2) ∈ opt(c1) with the first ordinal in the now ordered set opt(c1). Now, for all the subsequent occurrences i of a configuration c in the run, we choose the option with ordinal j = (count(i) mod |opt(c)|) + 1 in the set opt(c). So, if a configuration occurs infinitely often at non-terminal positions then we continually rotate through all its options. Naturally, trials end at the first occurrence of reward, and then we choose another start state; taking care to use all start states infinitely often."
    }, {
      "heading" : "4 Results",
      "text" : "The simple learning algorithm formalized in Section 3.2 continually marks the encountered states as visited. At the end of trials, i.e., after obtaining reward, each state is again marked as unvisited. If the algorithm encounters a state q that is already visited within the same trial, the algorithm proposes to generate a new action for q. Intuitively, if the same state q is encountered in the same trial, the agent might be running around in cycles and some new action should\nbe tried for q to escape from the cycle. It is important to avoid cycles if we want to achieve an eventual upper bound on the length of a trial, i.e., an upper bound on the time it takes to reach reward from a given start state.\nRepeatedly trying a new action for revisited states might eventually lead to reward, and thereby terminate the trial. In this learning process, the nondeterminism of the task can be both helpful and hindering: nondeterminism is helpful if transitions choose successor states that are closer to reward, but nondeterminism is hindering if transitions choose successor states that are further from reward or might lead to a cycle. Still, on some suitable tasks, the actions that are randomly tried upon revisits might eventually globally form a policy that will never get trapped in a cycle ever again (see Theorem 4.1 below).\nThe outline of this section is as follows. In Section 4.1, we present a sufficient condition for tasks to be learnable under fairness. In Section 4.2 we discuss how a simulator could detect that convergence has occurred in a fair run. In Section 4.3 we present necessary conditions for tasks to be learnable under fairness."
    }, {
      "heading" : "4.1 Sufficient Condition for Convergence",
      "text" : "Intuitively, if a task is reducible then we might be able to obtain a policy that on each start state leads to reward without revisiting states in the same trial. As long as revisits occur, we keep searching for the acyclic flow of states that is implied by reducibility. We can imagine that states near the goal states, i.e., near immediate reward, tend to more quickly settle on an action that leads to reward. Subsequently, states that are farther removed from immediate reward can be reduced to states near goal states, and this growth process propagates through the entire state space. This intuition is confirmed by the following convergence result:\nTheorem 4.1. Reducible tasks are learnable under fairness.\nProof. Let T = (Q,Q0, A, rewards, δ) be a reducible task. Let R be a fair run on T . We show convergence of R. In Part 1 of the proof, we show that all trials in R terminate (with reward). In Part 2, we show that eventually all states become stable in R.\nPart 1: Trials terminate. Let\nL1, L2, . . .\nbe the reducibility layers for T as defined in Section 3.1, where L1 = goals(T ). Let C be a trial in R. To show finiteness of C, and thus termination of C, we show by induction on i = 1, 2, . . . that the states in Li occur only finitely many times in C. Because T is reducible, there is an index n ∈ N0 for which Ln = Ln+1 = Q, and therefore our inductive proof shows that every state only occurs a finite number of times in the trial C; hence, C is finite.\nBefore we continue, we recall that a state q is marked as visited after its first occurrence in the trial; any occurrence of q after its first occurrence is therefore in a branching configuration.\nBase case. Let q ∈ L1 = goals(T ). Towards a contradiction, suppose q occurs infinitely often in trial C, making C infinite. Because there are only a finite number of possible configurations, there is a configuration c containing q\nthat occurs infinitely often in C at non-terminal positions (because the trial is now infinite). Configuration c is branching because it occurs more than once.5 By definition of q ∈ goals(T ), there is an action a ∈ A such that (q, a) ∈ rewards. Since always δ(q, a) 6= ∅, we can choose some q′ ∈ δ(q, a). We have (a, q′) ∈ opt(c) because c is branching. By fairness, the following transition must occur infinitely often in the trial:\nc a, q′−−→ apply(c, a, q′).\nBut this transition is a reward transition, so the trial would have already ended at the first occurrence of this transition. Hence q can not occur infinitely many times; this is the desired contradiction.\nInductive step. Let i ≥ 2, and let us assume that states in Li−1 occur only finitely many times in C. Let q ∈ Li \\ Li−1. By definition of Li, there is some action a ∈ A such that δ(q, a) ⊆ Li−1. Towards a contradiction, suppose q occurs infinitely often in C, making C infinite. Like in the base case, there must be a branching configuration c containing q that occurs infinitely often in the trial (at non-terminal positions). Since always δ(q, a) 6= ∅, we can choose some q′ ∈ δ(q, a) ⊆ Li−1. We have (a, q′) ∈ opt(c) because c is branching. By fairness, the following transition must occur infinitely often in the trial:\nc a, q′−−→ apply(c, a, q′).\nBut then q′ would appear infinitely often in trial C. This is the desired contradiction, because the induction hypothesis says that all states in Li−1 (including q′) occur only finitely many times in C.\nPart 2: Stability of states. We now show that all states eventually become stable in the fair R. Let\nL1, L2, . . .\nagain be the reducibility layers for T as above, where L1 = goals(T ). We show by induction on i = 1, 2, . . . that states in Li become stable in R. Since T is reducible, there is an index n ∈ N0 such that Ln = Ln+1 = Q, so our inductive proof shows that all states eventually become stable.\nBefore we continue, we recall that Part 1 of the proof has shown that all trials are finite. So, whenever we say that a configuration occurs infinitely often in the run, this means that the configuration occurs in infinitely many trials. Similarly, if a transition occurs infinitely often in the run, this means that the transition occurs in infinitely many trials.\nBase case. Let q ∈ L1 = goals(T ). Towards a contradiction, suppose q would not become stable. This means that there are infinitely many nonterminal occurrences of branching configurations containing q.6 Because there are only finitely many possible configurations, there must be a branching configuration c containing q that occurs infinitely often at non-terminal positions.\n5To see this, take for instance the second occurrence of c in the trial. That occurrence represents a revisit to q, so q is assigned the value visited in c.\n6For completeness, we recall that if q would occur only a finite number of times in the run then we can immediately see in the definition of stability that q becomes stable.\nBy definition of q ∈ goals(T ), there is an action a ∈ A such that (q, a) ∈ rewards. Since always δ(q, a) 6= ∅, we can choose some q′ ∈ δ(q, a). We have (a, q′) ∈ opt(c) because c is branching. By fairness, the following transition t must occur infinitely often in the run:\nc a, q′−−→ apply(c, a, q′).\nTransition t is a reward transition because (q, a) ∈ rewards. Let j be the index of a trial containing transition t; this implies that t is the last transition of trial j. We now show that any non-terminal occurrences of q after trial j must be in a non-branching configuration. Hence, q becomes stable; this is the desired contradiction.\nConsider the first trial index k after j in which q occurs again at a nonterminal position. Let configuration c1 = (q1, π1, w1) with q1 = q be the first occurrence of q in trial k (which is at a non-terminal position). Note that π1(q) = a because (i) trial j ends with the assignment of action a to q (through transition t), and (ii) the trials between j and k could not have modified the action of q. Further, configuration c1 is not branching because q is not yet flagged as visited at its first occurrence in trial k. This means that at any occurrence of c1, trial k must select an option (a, q\n′′) ∈ opt(c1), with action a and q′′ ∈ δ(q, a), and perform the corresponding transition t′:\nc1 a, q′′−−−→ apply(c1, a, q′′).\nAgain, since (q, a) ∈ rewards, trial k ends directly after transition t′; no branching configuration containing q can occur in trial k at a non-terminal position.7 This reasoning can now be repeated for all following trials to see that there are no more non-terminal occurrences of branching configurations containing q.\nInductive step. Let i ≥ 2. We assume for each q′ ∈ Li−1 that q′ eventually becomes stable. Now, let q ∈ Li \\ Li−1. By definition of Li, there is an action a ∈ A such that δ(q, a) ⊆ Li−1. Towards a contradiction, suppose that q does not become stable. Our aim is to show that now also at least one q′ ∈ δ(q, a) does not become stable, which would contradict the induction hypothesis.\nRegarding terminology, we say that a chain is a (q, a)-chain if (i) the chain contains only non-reward transitions and (ii) the chain has the following desired form:\nc1 a1, q2−−−→ c2 a2, q3−−−→ . . . an−1, qn−−−−−→ cn, denoting cj = (qj , πj , wj) for each j ∈ {1, . . . , n}, where q1 = qn = q and a1 = a. Note that such a chain starts and ends with an occurrence of q, so q is revisited in the chain. Moreover, the first transition performs the action a from above. Next, we say that a trial is a (q, a)-trial if the trial contains a (q, a)-chain. In principle, each (q, a)-trial could embed a different (q, a)-chain.\nTo see that there are infinitely many occurrences of (q, a)-trials in R, we distinguish between the following two cases.\n7Although it is possible that q is directly revisited from itself, it does not matter whether the terminal configuration of the trial is branching or not.\n• Suppose that in R there are infinitely many occurrences of trials that end with a policy π where π(q) = a, i.e., action a is assigned to q. Let j be the index of such a trial occurrence. Because by assumption q does not become stable, we can consider the first trial index k after j in which q occurs in a branching configuration at a non-terminal position. Note that trials between trial j and trial k do not modify the action of q. Now, the first occurrence of q in trial k is always non-branching, and thus we perform action a there. The subsequence in trial k starting at the first occurrence of q and ending at some branching configuration of q at a non-terminal position, is a (q, a)-chain: the chain starts and ends with q, its first transition performs action a, and it contains only non-reward transitions because it ends at a non-terminal position. Hence, trial k is a (q, a)-trial.\n• Conversely, suppose that in R there are only finitely many occurrences of trials that end with a policy π where π(q) = a. Let R′ be an (infinite) suffix of R in which no trial ends with action a assigned to q. Because by assumption q does not become stable, and because the number of possible configurations is finite, there is a branching configuration c containing q that occurs infinitely often at non-terminal positions in R′. Choose some q′ ∈ δ(q, a). We have (a, q′) ∈ opt(c) because c is branching. By fairness, the following transition t occurs infinitely often in R′:\nc a, q′−−→ apply(c, a, q′).\nLet j be the index of a trial occurrence in R′ that contains transition t; there are infinitely many such indexes because all trials in R are finite (see Part 1 of the proof). Since transition t attaches action a to q, we know by definition of R′ that any occurrence of t in trial j is followed by at least one other transition from state q that attaches an action b to q with b 6= a; this implies that after each occurrence of transition t in trial j there is a branching configuration of q at a non-terminal position. In trial j, a subsequence starting at any occurrence of t and ending with the first subsequent branching configuration of q at a non-terminal position, is a (q, a)-chain: the chain starts and ends with q, its first transition performs action a, and the chain contains only non-reward transitions because it ends at a non-terminal position. Hence, trial j is a (q, a)-trial.\nWe have seen above that there are infinitely many occurrences of (q, a)-trials in R. Because there are only a finite number of possible configurations, there is a configuration c containing q that is used in infinitely many occurrences of (q, a)-trials as the last configuration of a (q, a)-chain. Note that c occurs infinitely often at non-terminal positions since (q, a)-chains contain no reward transitions.\nNext, we can choose from some occurrence of a (q, a)-trial in the run some (q, a)-chain C where in particular the last configuration of C is the configuration c. Formally, we write C as\nc1 a1, q2−−−→ c2 a2, q3−−−→ . . . an−1, qn−−−−−→ cn,\nwhere cn = c, and denoting cj = (qj , πj , wj) for each j ∈ {1, . . . , n}, where q1 =\nqn = q and a1 = a. We recall that all transitions of C are non-reward transitions. Note that 2 < n: we have q 6= q2 because q /∈ Li−1 and q2 ∈ δ(q, a) ⊆ Li−1.\nIn chain C, we have certainly marked state q as visited after its first occurrence, causing configuration cn to be branching. This implies (a, q2) ∈ opt(cn), where (a, q2) is the same option as taken by the first transition of C, since a1 = a. Also, since 2 < n, we have certainly marked state q2 as visited after its first occurrence in C; this implies wn(q2) = visited. Next, since the configuration cn = c occurs infinitely often at non-terminal positions (see above), the following transition also occurs infinitely often by fairness:\ncn a, q2−−−→ cn+1,\nwhere cn+1 = (qn+1, πn+1, wn+1) = apply(cn, a, q2). Since qn+1 = q2 and wn+1(q2) = wn(q2) = visited, configuration cn+1 is branching. Moreover, we know that (q, a) /∈ rewards because no transition of C is a reward transition, including the first transition. So, the branching configuration cn+1 occurs infinitely often at non-terminal positions. Hence, q2 would not become stable. Yet, q2 ∈ δ(q, a) ⊆ Li−1, and the induction hypothesis on Li−1 says that q2 does become stable; this is the desired contradiction.\nRemark 4.2. By Theorem 4.1, the trials in a fair run on a reducible task eventually contain a number of non-terminal configurations that is at most the number of states; otherwise at least one state would never become stable.8 So, we get a relatively good eventual upper bound on trial length. However, Theorem 4.1 provides no information on the waiting time before that upper bound will emerge, because that waiting time strongly depends on the choices made by the run regarding start states of trials, tried actions, and successor states (see also Section 6).\nBecause we seek a policy that avoids revisits to states in the same trial, an important intuition implied by Theorem 4.1 is that eventually the trials of a run follow paths without cycles through the state space. The followed paths are still influenced by nondeterminism, but they never contain a cycle. Also, a path followed in a trial is not necessarily the shortest possible path to reward, because the discovery of paths depends on experience, i.e., on the order in which actions were tried during the learning process. The experience dependence was experimentally observed by Frémaux et al. (2013).\nRemark 4.3. The order in which states become stable in a fair run does not necessarily have to follow the order of the reducibility layers of Section 3.1. In general, it seems possible that some states that are farther removed from goal states could become stable faster than some states nearer to goal states; but, to become stable, the farther removed states probably should first have some stable strategy to the goal states.\nThe following example illustrates the necessity of the fairness assumption in Theorem 4.1.\n8If there would be infinitely many trials that contain more non-terminal configurations than states, then in infinitely many trials there is a revisit to a state (in a branching configuration) on a non-terminal position. Since there are only finitely many states, there would be at least one state q that in infinitely many trials occurs in a branching configuration on a non-terminal position; this state q does not become stable by definition.\nExample 4.4. Consider again the task T from Example 3.2, that is also visualized in Figure 3.1. In the following, for ease of notation, we will denote configurations as triples (x, y, Z), where x is the current state; y is the action assigned by the policy to the specific state 1, with action a assigned to all other states; and, Z is the set of states marked with the value visited, with all other states marked with value unvisited.\nConsider now the following trial Ca where the initial policy has assigned action a to all states, including the start state 1:\n(1, a, { }) a,1−−→ (1, a, {1}) b,2−−→ (2, b, {1}) a,3−−→ (3, b, {1, 2}) a,3−−→ (3, b, {1, 2, 3}).\nThis is indeed a valid trial because the last transition is a reward transition. Note also that a revisit to state 1 occurs in the first transition. The configuration (1, a, {1}) is thus branching, which implies that the option (b, 2) may be chosen there. At the end of trial Ca, action b is assigned to state 1 and action a is assigned to the other states.\nConsider also the following trial Cb where the initial policy has assigned action b to state 1 and a to all other states:\n(1, b, { }) b,2−−→ (2, b, {1}) a,1−−→ (1, b, {1, 2}) a,3−−→ (3, a, {1, 2}) a,3−−→ (3, a, {1, 2, 3}).\nThe last transition is again a reward transition. Note that a revisit occurs to state 1 in the second transition. The configuration (1, b, {1, 2}) is therefore branching, which implies that the option (a, 3) may be chosen there. At the end of trial Cb, action a is assigned to all states, including state 1.\nNow, let R be the run that alternates between trials Ca and Cb and that starts with trial Ca. The state 1 never becomes stable in R because we assign action a and action b to state 1 in an alternating fashion. So, run R does not converge because there are infinitely many non-terminal occurrences of branching configurations containing state 1.\nAlthough run R satisfies all requirements of a valid run, R is not fair. For example, although the configuration (1, b, {1, 2}) occurs infinitely often (due to repeating trial Cb), this configuration is never extended with the valid option (b, 2) that could propagate revisits of state 1 to revisits of state 2 in the same trial; revisits to state 2 could force state 2 to use the other action b, which in turn could aid state 1 in becoming stable.\nIn conclusion, because task T is reducible and yet the valid (but unfair) run R does not converge, we see that Theorem 4.1 does not hold in absence of fairness."
    }, {
      "heading" : "4.2 Detecting the Final Policy",
      "text" : "For simulations, it is useful to detect when convergence has occurred in a run and the policy will no longer change. We refer to the lastly formed policy of a run as the final policy. In this subsection, we syntactically characterize the final policy. In general, verifying the syntactical property of the final policy requires access to the entire set of task states. Outside simulations, the syntactical property gives a clarifying insight in the form of the final policy. In this subsection, we do not require that tasks are reducible.\nWe first introduce the two key parts of the syntactical characterization, namely, the so-called forward and backward sets of states induced by a policy.\nAs we will see below, the syntactical property says that the forward set should be contained in the backward set.\nForward and Backward Let T = (Q,Q0, A, rewards, δ) be a task that is learnable under fairness. To make the notations below easier to read, we omit the symbol T from them. It will always be clear from the context which task is meant.\nLet π : Q → A be a policy, i.e., each q ∈ Q is assigned an action from A. First, we define\nground(π) = {q ∈ goals(T ) | (q, π(q)) ∈ rewards};\nthis is the set of all goal states that are assigned a rewarding action by the policy. Next, we define two sets forward(π) ⊆ Q and backward(π) ⊆ Q, as follows. For the set forward(π), we consider the infinite sequence F1, F2, . . . of sets, where F1 = Q0 and for each i ≥ 2,\nFi = Fi−1 ∪ ⋃\nq ∈ Fi−1 \\ ground(π) δ(q, π(q)).\nWe define forward(π) = ⋃ i∈N0 Fi. Note that forward(π) ⊆ Q. Intuitively, forward(π) is the set of all states that are reachable from the start states by following the policy. In the definition of Fi with i ≥ 2, we remove ground(π) from the extending states because we only want to add states to forward(π) that can occur at non-terminal positions of trials.9\nFor the set backward(π), we consider the infinite sequence B1, B2, . . . of sets, where B1 = ground(π) and for each i ≥ 2,\nBi = Bi−1 ∪ {q ∈ Q | δ(q, π(q)) ⊆ Bi−1}.\nWe define backward(π) = ⋃ i∈N0 Bi. Note that backward(π) ⊆ Q. Intuitively, backward(π) is the set of all states that are reduced to the goal states in ground(π) by the policy.\nFor completeness, we remark that the infinite sequences F1, F2, . . . , and B1, B2, . . . , each have a fixpoint because Q is finite.\nFinal Policy We formalize the final policy. Let T be a task that is learnable under fairness. Let R be a fair run on T , which implies that R converges. We define the convergence-trial of R as the smallest trial index i for which the following holds: trial i terminates and after trial i there are no more branching configurations at non-terminal positions.10 This implies that after trial i the policy can not change anymore, because to change the action assigned to a state q, the state q would have to occur again in branching configuration at a nonterminal position. We define the final policy of R to be the policy at the end of the convergence-trial. In principle, different converging runs can have different final policies.\n9Possibly, some states directly reachable from ground(π) are still in forward(π) because those states are also reachable from states outside ground(π).\n10With this definition of convergence-trial, a run converges if and only if the run contains a convergence-trial.\nNow, we can recognize the final policy with the following property, that intuitively says that any states reachable by the policy are also safely reduced by the policy to reward:\nTheorem 4.5. Let T be a task that is learnable under fairness.11 Let R be a converging fair run of the setting. A policy π occurring in run R at the end of a trial is the final policy of R if and only if\nforward(π) ⊆ backward(π).\nProof. We show in two separate parts that forward(π) ⊆ backward(π) is (i) a sufficient and (ii) a necessary condition for π to be the final policy of run R.\nPart 1: Sufficient Condition. Let π be a policy occurring in run R at the end of a trial. Assume that forward(π) ⊆ backward(π). We show that π is the final policy of R.\nConcretely, we show that any trial starting with policy π will (i) use π in all its configurations, including the terminal configuration; and, (ii), does not contain branching configurations at non-terminal positions. This implies that the first trial ending with π is the convergence-trial, so π is the final policy.\nLet C be a trial in R that begins with policy π. We explicitly denote trial C as the following finite chain of transitions:\nc1 a1, q2−−−→ . . . an−1, qn−−−−−→ cn.\nFor each i ∈ {1, . . . , n}, we denote ci = (qi, πi, wi). Let F1, F2, . . . be the infinite sequence of sets previously defined for forward(π). We show by induction on i = 1, . . . , n− 1 that (a) πi = π;\n(b) qi ∈ Fi; (c) ci is non-branching.\nAt the end of the induction, we can also see that πn = π: first, we have πn = πn−1 because configuration cn−1 is non-branching by property (c);\n12 second, πn−1 = π by property (a).\nBase case. Let i = 1. For property (a), we have π1 = π because the trial starts with policy π. For property (b), we see that q1 ∈ Q0 = F1. For property (c), we know that c1 is non-branching because the first occurrence of a state in a trial always has the value unvisited.\nInductive step. Let i ≥ 2, with i ≤ n− 1. Assume that the induction properties are satisfied for the configurations c1, . . . , ci−1. We now show that the properties are also satisfied for ci.\nProperty (a) By applying the induction hypothesis for property (c) to ci−1, namely that ci−1 is non-branching, we know πi = πi−1. By subsequently applying the induction hypothesis for property (a) to ci−1, namely πi−1 = π, we know πi = π.\n11In contrast to Theorem 4.1, we do not require that T is reducible. 12Because configuration cn−1 is non-branching, we have πn(qn−1) = an−1 = πn−1(qn−1),\nwhich, combined with πn(r) = πn−1(r) for each r ∈ Q \\ {qn−1}, gives πn = πn−1.\nProperty (b) To start, we note that qi ∈ δ(qi−1, πi−1(qi−1)) because ci−1 is non-branching by the induction hypothesis for property (c). By subsequently applying the induction hypothesis for property (a) to ci−1, namely πi−1 = π, we know qi ∈ δ(qi−1, π(qi−1)). Moreover, since i−1 < i ≤ n−1, the transition ci−1\nai−1, qi−−−−−→ ci, where ai−1 = π(qi−1), is a non-reward transition. Hence, (qi−1, π(qi−1)) /∈ rewards and thus qi−1 /∈ ground(π). Lastly, by applying the induction hypothesis for property (b) to ci−1, we overall obtain that qi−1 ∈ Fi−1 \\ ground(π). Combined with qi ∈ δ(qi−1, π(qi−1)), we see that qi ∈ Fi.\nProperty (c) Towards a contradiction, suppose that configuration ci is branching. This means that state qi is revisited in ci.\n13 Let V = {q1, . . . , qi−1}. Note that qi ∈ V , which implies V 6= ∅. By applying the induction hypothesis for property (b) to configurations c1, . . . , ci−1, we know that V ⊆ forward(π). We now show that V ∩ backward(π) = ∅, which would imply forward(π) 6⊆ backward(π); this is the desired contradiction. Let B1, B2, . . . be the infinite sequence of sets defined for backward(π) above. We show by induction on j = 1, 2, . . . that V ∩Bj = ∅, which then overall implies V ∩ backward(π) = ∅.\n• Base case: j = 1. By definition, B1 = ground(π). Let q ∈ V . Let k ∈ {1, . . . , i − 1} be the smallest index for which qk = q, i.e., configuration ck represents the first occurrence of q in the trial. By applying the outer induction hypothesis for properties (a) and (c) to ck, we know that ak = πk(qk) = π(q). But since k ≤ i−1 < i ≤ n−1, we know that transition ck\nak, qk+1−−−−−→ ck+1 is not a reward transition, implying (q, π(q)) /∈ rewards. Hence, q /∈ ground(π), and overall V ∩ ground(π) = ∅.\n• Inductive step. Let j ≥ 2. Assume V ∩ Bj−1 = ∅. Towards a contradiction, suppose V ∩ Bj 6= ∅. Take some q ∈ V ∩ Bj . If q ∈ Bj−1 then we would immediately have a contradiction with the induction hypothesis. Henceforth, suppose q ∈ Bj \\Bj−1, which, by definition of Bj , means δ(q, π(q)) ⊆ Bj−1. We will now show that V ∩δ(q, π(q)) 6= ∅, which would give V ∩Bj−1 6= ∅; this is the desired contradiction.\nSince q ∈ V , there is some smallest k ∈ {1, . . . , i−1} such that qk = q. Using a similar reasoning as in the base case (j = 1), by applying the outer induction hypothesis for properties (a) and (c) to configuration ck, we can see that ak = π(qk). This implies qk+1 ∈ δ(qk, π(qk)). As a last step, we show that qk+1 ∈ V , which gives V ∩ δ(qk, π(qk)) 6= ∅. We distinguish between the following cases:\n– If k ≤ i− 2 then k+ 1 ≤ i− 1, and surely qk+1 ∈ V by definition of V . – If k = i − 1 then we know qk+1 = qi ∈ V because configuration ci revisits state qi (see above).\n13Recall that, by definition of branching configuration, we have wi(qi) = visited.\nPart 2: Necessary Condition. Let π be the final policy of R. We show that forward(π) ⊆ backward(π). By definition of final policy, π is the policy at the end of the convergence-trial, whose trial index we denote as i. By definition of convergence-trial, after trial i there are no more branching configurations at non-terminal positions. Note in particular that the policy no longer changes after trial i.\nTowards a contradiction, suppose that forward(π) 6⊆ backward(π). Let V = forward(π) \\ backward(π). Note that V 6= ∅. We show that there is a state q ∈ V that occurs at least once in a branching configuration at a non-terminal position after the trial i; this would be the desired contradiction.\nWe provide an outline of the rest of the proof. The reasoning proceeds in two steps. First, we show for each q ∈ V that δ(q, π(q)) ∩ V 6= ∅. This means that if we are inside set V , we have the option to stay longer inside V if we follow the policy π. Now, the second step of the reasoning is to show that we can stay arbitrarily long inside V even after the convergence-trial i, causing at least one state of V to occur in a branching configuration at a non-terminal position after trial i.\nStep 1. Let q ∈ V . We show δ(q, π(q))∩V 6= ∅. Towards a contradiction, suppose that δ(q, π(q)) ∩ V = ∅. Our strategy is to show that δ(q, π(q)) ⊆ backward(π), which, by definition of backward(π), implies that there is some index j ∈ N0 such that δ(q, π(q)) ⊆ Bj . Therefore q ∈ Bj+1 ⊆ backward(π). But that is false because q ∈ V = forward(π) \\ backward(π); this is the desired contradiction.\nWe are left to show that δ(q, π(q)) ⊆ backward(π). First, we show that δ(q, π(q)) ⊆ forward(π). By definition, forward(π) = ⋃j∈N0 Fj . Since q ∈ V ⊆ forward(π), there is some index j ∈ N0 such that q ∈ Fj . Moreover, since q /∈ backward(π) and ground(π) ⊆ backward(π), we have q /∈ ground(π). Overall, q ∈ Fj \\ground(π), which implies that δ(q, π(q)) ⊆ Fj+1 ⊆ forward(π).\nNow, we can complete the reasoning by combining δ(q, π(q)) ⊆ forward(π) with our assumption δ(q, π(q)) ∩ V = ∅, to see the following:\nδ(q, π(q)) ⊆ forward(π) \\ V = forward(π) \\ (forward(π) \\ backward(π)) = forward(π) ∩ backward(π) ⊆ backward(π).\nStep 2. We now show that after convergence-trial i there is at least one occurrence of a branching configuration at a non-terminal position.\nWe first show that each q ∈ forward(π) occurs infinitely often at nonterminal positions after trial i. Recall that forward(π) = ⋃ j∈N0 Fj . We show by induction on j = 1, 2, . . . that states in Fj occur infinitely often at non-terminal positions after trial i.\n• Base case: j = 1. By definition, F1 = Q0. Because R is a valid run, each state of Q0 is used in infinitely many trials as the start state, also after trial i (see Section 3.2).14 Moreover, the first configuration of a trial is\n14We also recall here that R is assumed to converge, which, by definition of convergence, implies that R is an infinite sequence of finite trials.\nalways at a non-terminal position because each trial contains at least one transition.\n• Inductive step. Let j ≥ 2. Assume that each state in Fj−1 occurs infinitely often at non-terminal positions after trial i. Let q ∈ Fj\\Fj−1. This implies that there is some qj−1 ∈ Fj−1 \\ ground(π) for which q ∈ δ(qj−1, π(qj−1)). By applying the induction hypothesis, we know that qj−1 occurs infinitely often at non-terminal positions after trial i. Because there are only a finite number of possible configurations, there is a configuration c containing qj−1 that occurs infinitely often at non-terminal positions after trial i. Because trial i is the convergence-trial, we can make two observations about configuration c: first, c contains the final policy π because the policy no longer changes after trial i; and, second, c is non-branching because no branching configurations occur at non-terminal positions after trial i. These observations imply (π(qj−1), q) ∈ opt(c). Now, since c occurs infinitely often at non-terminal positions after trial i, the following transition t occurs infinitely often after trial i by fairness:\nc π(qj−1), q−−−−−−→ apply(c, π(qj−1), q).\nLastly, we know that (qj−1, π(qj−1)) /∈ rewards because qj−1 /∈ ground(π), so transition t is a non-reward transition. Therefore state q occurs infinitely often at non-terminal positions after trial i.\nNow take some q1 ∈ V . Since V ⊆ forward(π), we know from above that q1 occurs infinitely often at non-terminal positions after trial i. Because there are only finitely many possible configurations, there is a configuration c1 containing q1 that occurs infinitely often at non-terminal positions after trial i. After trial i, the policy no longer changes and only non-branching configurations may occur at non-terminal positions. So c1 contains the final policy π and is nonbranching. Moreover, since q1 ∈ V , we know from Step 1 above that there is some q2 ∈ δ(q1, π(q1)) ∩ V . Overall, we can see that (π(q1), q2) ∈ opt(c1). By fairness, the following transition t1 occurs infinitely often after trial i:\nc1 π(q1), q2−−−−−→ c2,\nwhere c2 = apply(c1, π(q1), q2). Because q1 ∈ V we have q1 /∈ ground(π), so transition t1 is a non-reward transition.\n15 Therefore configuration c2 occurs infinitely often at non-terminal positions after trial i. Denoting c2 = (q2, π2, w2), note that w2(q1) = visited.\nWe now make a similar reasoning for c2 as we did for c1. Since q2 ∈ V , we know again from Step 1 that there is some q3 ∈ δ(q2, π(q2)) ∩ V . Configuration c2 contains the final policy π because c2 occurs after trial i, and c2 is also nonbranching because it occurs after trial i at a non-terminal position. Therefore (π(q2), q3) ∈ opt(c2). Now, since c2 occurs infinitely often at non-terminal positions after trial i (see above), the following transition occurs infinitely often after trial i by fairness:\nc2 π(q2), q3−−−−−→ c3,\n15If q1 ∈ ground(π) then q1 ∈ backward(π), which is false since q1 ∈ V .\nwhere c3 = apply(c2, π(q2), q3). This transition is also non-rewarding because q2 ∈ V implies q2 /∈ ground(π). Denoting c3 = (q3, π3, w3), note that w3(q1) = w3(q2) = visited. Note that more states of V are now marked as visited.\nWe can now complete the reasoning. By following the final policy π from a state in V , we can always stay inside V without reaching reward. So, the above procedure can be repeated |V | times in total, to show the existence of a configuration c = (q, π, w) with q ∈ V that occurs infinitely often at nonterminal positions after trial i, and where w(q) = visited. Configuration c is therefore branching, and thus the existence of c gives the desired contradiction, as explained at the beginning of Part 2 of the proof.16"
    }, {
      "heading" : "4.3 Necessary Conditions for Convergence",
      "text" : "In Section 4.1, we have seen that reducibility is a sufficient property for tasks to be learnable under fairness (Theorem 4.1). In this subsection, we also show necessary properties for tasks to be learnable under fairness. This provides a first step towards characterizing the tasks that are learnable under fairness. Thinking about such a characterization is useful because it allows us to better understand the tasks that are not learnable under fairness.\nLet T = (Q,Q0, A, rewards, δ) be a task. We say that there is a (simple) path from a state q to a state q′ if there is a sequence of actions a1, . . . , an, with possibly n = 0, and a sequence of states q1, . . . , qn+1 such that\n• q1 = q; • qn+1 = q′; • qi+1 ∈ δ(qi, ai) for each i ∈ {1, . . . , n}; • (qi, ai) /∈ rewards for each i ∈ {1, . . . , n}; and, • qi 6= qj for each i, j ∈ {1, . . . , n+ 1} with i 6= j.\nWe emphasize that the path does not contain reward transitions and does not repeat states. We also denote a path as\nq1 a1−→ . . . an−−→ qn+1.\nNote that there is always a path from a state to itself, namely, the empty path where n = 0.\nWe say that a state q is reachable if there is a path from a start state q0 ∈ Q0 to q. Next, letting V ⊆ Q, we say that a state q has a path to V if there is a path from q to a state q′ ∈ V . Note that if q has a path to V , it is not guaranteed that the action sequence of that path always ends in V because the transition function δ is nondeterministic.\nWe can now note the following necessary properties for tasks to be learnable under fairness:\nProposition 4.6. Tasks T that are learnable under fairness satisfy the following properties:\n16We note the following for completeness. By repeatedly trying to stay inside V , we are not guaranteed to see all of V , but we still know that after at most |V | transitions we have to revisit a state of V at a non-terminal position (after trial i); that first revisit forms the desired contradiction.\n(a) The reachable states have a path to goals(T ).\n(b) The start states are reducible to goals(T ).\nProof. We show the two properties separately. Denote T = (Q,Q0, A, rewards, δ).\nProperty (a). Towards a contradiction, suppose that there is a reachable state q that has no path to goals(T ). Note that q /∈ goals(T ) because otherwise the empty path from q to itself would be a path to goals(T ). Because q is reachable, there is a path\nq1 a1−→ . . . an−−→ qn+1,\nwhere q1 ∈ Q0 and qn+1 = q. We can consider a policy π where, for each i ∈ {1, . . . , n}, we set π(qi) = ai; the other state-action mappings may be arbitrary.17 Below we will consider a fair run R whose first trial is given q1 as start state and π as the initial policy. First, we consider the following chain C:\nc1 a1, q2−−−→ . . . an, qn+1−−−−−→ cn+1,\nwhere for each i ∈ {1, . . . , n + 1} we define ci = (qi, π, wi) where wi(r) = visited for each r ∈ {q1, . . . , qi−1} and wi(r) = unvisited for each r ∈ Q\\{q1, . . . , qi−1}.18 Note that this chain is indeed valid: for each i ∈ {1, . . . , n}, configuration ci is (constructed to be) non-branching and therefore (π(qi), qi+1) = (ai, qi+1) ∈ opt(ci); this means that we do not modify the policy during the transition ci\nai, qi+1−−−−−→ apply(ci, ai, qi+1), but we only mark qi as visited, which gives ci+1 = apply(ci, ai, qi+1). Note that configuration cn+1 contains the state q.\nConsider a fair run R whose first trial starts with the chain C. We show that the first trial never terminates; this is the desired contradiction because we had assumed that task T is learnable under fairness. For the first trial to terminate, the trial must extend chain C to a chain C′ that terminates with a reward transition. But then the existence of C′ would imply that there is a path from q to a state q′ ∈ goals(T ), which is false.\nProperty (b). We show that the start states are reducible to goals(T ). Let R be a fair run on T , which implies that R converges. Because R converges, there is a final policy π, as defined in Section 4.2. Then by Theorem 4.5, we know that forward(π) ⊆ backward(π). We have Q0 ⊆ backward(π) because Q0 ⊆ forward(π) by definition of forward(π). Letting reduce(T ) be the set of states that are reducible to goals(T ), we show below that backward(π) ⊆ reduce(T ); this implies Q0 ⊆ reduce(T ), as desired.\nIn Section 3.1, we have defined reduce(T ) = ⋃ i∈N0 Li where L1 = goals(T )\nand for each i ≥ 2,"
    }, {
      "heading" : "Li = Li−1 ∪ {q ∈ Q | ∃a ∈ A with δ(q, a) ⊆ Li−1}.",
      "text" : "Also recall the definition backward(π) = ⋃ i∈N0 Bi from Section 4.2. We show by induction on i = 1, 2, . . . that Bi ⊆ Li. 17Note that there are no conflicting assignments of actions to states because, by definition of path, each state qi with i ∈ {1, . . . , n+ 1} occurs only once on the path. 18In this notation, we interpret {q1, . . . , q0} as ∅.\n• Base case: i = 1. By definition, B1 = ground(π) = {q ∈ goals(T ) | (q, π(q)) ∈ rewards}. Hence, B1 ⊆ goals(T ) = L1.\n• Inductive step. Let i ≥ 2, and let us assume that Bi−1 ⊆ Li−1. Let q ∈ Bi \\ Bi−1, which, by definition of Bi, implies δ(q, π(q)) ⊆ Bi−1. By applying the induction hypothesis, we see δ(q, π(q)) ⊆ Li−1. This implies q ∈ Li, and, overall, Bi ⊆ Li.\nThis completes the proof.\nNote that reducibility implies the necessary properties in Proposition 4.6 for tasks to be learnable under fairness. Also recall from Theorem 4.1 that reducibility is a sufficient property for tasks to be learnable under fairness. We now discuss the gap between these necessary and sufficient properties. Let T = (Q,Q0, A, rewards, δ) be a task. Letting reduce(T ) be the set of states that can be reduced to goals(T ) (as in Section 3.1), we define\nunstable(T ) = Q \\ reduce(T ).\nWe call unstable(T ) the unstable set of T . Note that for each state q ∈ unstable(T ), for each action a ∈ A, we always have δ(q, a) ∩ unstable(T ) 6= ∅.19 So, once we are inside unstable(T ), we can never reliably escape unstable(T ): escaping unstable(T ) depends on the nondeterministic choices regarding successor states. This intuition has also appeared in Part 2 of the proof of Theorem 4.5, but in that proof we were focusing on just a single fixed action for each state, as assigned by the final policy at hand.\nThe following example illustrates how a nonempty unstable set could prevent convergence. In particular, the example illustrates that the necessary properties of Proposition 4.6 are not sufficient for a task to be learnable under fairness.\nExample 4.7. Consider the task T = (Q,Q0, A, rewards, δ) defined as follows: Q = {1, 2, 3}; Q0 = {1}; A = {a, b}; rewards = {(3, a), (3, b)}; and, regarding δ, we define\nδ(1, a) = {2}, δ(1, b) = {3},\nδ(2, a) = δ(2, b) = {2, 3}, δ(3, a) = δ(3, b) = {3}.\nThe task T is visualized in Figure 4.1. Note that reduce(T ) = {1, 3}, giving unstable(T ) = {2}. Note in particular that δ(2, a)∩unstable(T ) 6= ∅ and δ(2, b)∩ unstable(T ) 6= ∅.\nNote that task T satisfies the necessary properties of Proposition 4.6: (i) the reachable states, which are all states in this case, have a path to goals(T ), and (ii) the start state 1 is reducible to goals(T ). However, task T is not learnable under fairness, as we now illustrate. Consider a trial C of the following form: starting with an initial policy that assigns action a to state 1, we first go from state 1 to state 2; next, we stay at least two consecutive times in state 2; and, lastly, we proceed to state 3 and obtain reward there. Because there are no\n19Indeed, if δ(q, a) ∩ unstable(T ) = ∅ then δ(q, a) ⊆ reduce(T ) and subsequently q ∈ reduce(T ), which is false.\nrevisits to state 1 in trial C, state 1 remains connected to action a. We now see that we can make a fair run R by repeating trials of the form of C: state 1 is never revisited and stays connected to action a, and we keep revisiting state 2. This way, there are infinitely many branching configurations containing state 2 at non-terminal positions. So, run R does not converge.\nBy looking at Example 4.7, it seems that runs would somehow have to learn to avoid entering the set unstable(T ). First, let\nborder(T ) = {q ∈ reduce(T ) | ∃a ∈ A with δ(q, a) ∩ unstable(T ) 6= ∅}.\nThe set border(T ) contains those states that could enter unstable(T ); we call such states border states. Let q ∈ border(T ), and let V be the subset of unstable(T ) that is reachable from q. Now, one idea could be to demand for each q1 ∈ V that there is some (q2, a) ∈ V × A such that q2 ∈ V is reachable from q1 and q ∈ δ(q2, a), i.e., there is some escape option to return from V back to border state q. This way, we can revisit that precise border state q in the same trial, so that under fairness we can choose a new action for q to avoid a future entrance into unstable(T ). The possibility to revisit border states in the same trial is exactly what is missing from Example 4.7. The characterization of tasks that are learnable under fairness might be a class of tasks that satisfy the necessary properties of Proposition 4.6 and that additionally specify assumptions on the transition function to ensure the possibility of revisits to border states. Illuminating the role of unstable sets in learning is an interesting avenue for further work (see Section 6)."
    }, {
      "heading" : "5 Examples",
      "text" : "Theorem 4.1 tells us that all reducible tasks are learnable under fairness, and Theorem 4.5 allows us to detect when the final policy has been formed. To illustrate these theorems, we now consider two examples of tasks that are reducible, in Section 5.1 and Section 5.2, respectively. Our aim is not to show practical efficiency of the learning algorithm, but rather to illustrate the theoretical insights. Indeed, because the considered examples are reducible, they are learnable under fairness by Theorem 4.1. Next, aided by Theorem 4.5, we can experimentally measure how long it takes for the learning process to convergence. In Section 6, for further work, we identify aspects where the learning algorithm could be improved to become more suitable for practice."
    }, {
      "heading" : "5.1 Grid Navigation Tasks",
      "text" : "Our first example is a navigation task in a grid world (Sutton and Barto, 1998; Potjans et al., 2011). In such a task, it is intuitive to imagine how paths are formed and what they mean. Below we formalize a possible version of such a grid task.\nThe grid is represented along two axes, the X- and Y -axis. At any time, the agent is inside only one grid cell (x, y). We let the state set Q be a subset of N × N. Let Goals ⊆ Q be a subset of cells, called goal cells. The agent could apply the following actions to each grid cell: finish, left, right, up, down, left-up, left-down, right-up, and right-down. Let Agrid denote the set containing these actions. The finish action is a non-movement action that gives immediate reward when applied to a goal cell. The finish action intuitively says that the agent believes it has reached a goal cell and claims to be finished. Activating the finish action in any non-goal cell will just leave the agent in that cell without reward. The actions other than finish will be referred to as movement actions.\nFor every movement action a, there is noise from the environment. We formalize this with a noise offset function that maps each movement action to the possible relative movements that it can cause. For example, offset(left) = {(−1, 0), (−1,−1), (−1, 1)}, and offset(left-up) = {(−1, 1), (−1, 0), (0, 1)}. Intuitively, noise adds one left-rotating option and one right-rotating option to the main intended direction. The offsets of the other movement actions can be similarly defined (see Appendix A). For uniformity we define offset(finish) = {(0, 0)}.\nFor a cell (x, y) ∈ N×N and an action a ∈ Agrid we define the set move((x, y), a) of child-cells that result from the application of the offsets of a to (x, y). Formally, we define\nmove((x, y), a) = {(x+ u, y + v) | (u, v) ∈ offset(a)}.\nNow, we say that a task T = (Q,Q0, A, rewards, δ) is a grid navigation task if Q ⊆ N×N, Q0 ⊆ Q, A = Agrid, there exists some nonempty subset Goals ⊆ Q such that rewards = {(q,finish) | q ∈ Goals}, and for each (q, a) ∈ Q×A we have\nδ(q, a) = { move(q, a) if move(q, a) ⊆ Q {q} otherwise.\nNote that we only perform a movement action if the set of child-cells is fully contained in the set of possible grid cells; otherwise the agent remains stationary.20\nThe assumption of reducibility can additionally be imposed on grid navigation tasks. In Figure 5.1, we visualize a grid navigation task that, for illustrative purposes, is only partially reducible.\nWe now discuss two simulation experiments that we have performed on such grid navigation tasks. Some details of the experiments can be found in Appendix A. Let N0 denote the set of natural numbers without zero.\n20This restriction results in policies that have sufficiently intuitive visualizations; see Figures 5.1, 5.2, and 5.4.\nConvergence-Trial Index Our first experiment measures the convergencetrial index. First we discuss the general setup of the experiment. We recall from Section 4.2 that the convergence-trial index of a fair run is the first trial index where the final policy occurs at the end. For a given task T , we can simulate fair runs, and we stop each run when the final policy is detected through the characterization of Theorem 4.5; we remember the convergence-trial index. Each run is started with a random policy, where each state is assigned a random action by uniformly sampling the available actions. Fairness depends on the mechanism for choosing among successor states, which is also based on a uniform distribution (see Appendix A). Interestingly, we do not have to simulate infinite runs because we always eventually detect the final policy; when we stop the simulation after finite time, the finite run may be thought of as a prefix of an infinite fair run.21\nIn principle, there is no upper bound on the convergence-trial index in the simulation. Fortunately, our experiments demonstrate that for the studied reducible tasks, there appears to be a number i ∈ N0 such that the large majority of simulated runs has a convergence-trial index below i. Possibly, there are outliers with a very large convergence-trial index, although such outliers are relatively few in number. We can exclude outliers from a list of numbers by considering a p-quantile with 0 < p < 1.22\nWe wanted to see if the convergence-trial index depends on the distance between the start cells and the goal cells. For this purpose, we have considered grid tasks with the form of a corridor, as shown in Figure 5.2. There is only one start cell. The parameter that we can change is the distance from the start cell to the patch of goal cells. All other aspects remain fixed, including the width\n21Of course, it is impossible to simulate an infinite run in practice. 22In Appendix A, we give the precise definition of p-quantile that we have used in our\nanalysis.\nof the corridor and the number and the location of the goal cells. The arrows in Figure 5.2 show reducibility of this kind of task. For a number l ∈ N0, we define the l-corridor as the corridor navigation task where the distance between the start cell and the goal cells is equal to l.\nNow, for some lengths l, we have simulated runs on the l-corridor. For each l-corridor separately, we have simulated 400 runs and we have computed the 0.9-quantile for the measured convergence-trial indexes; this gives an empirical upper bound on the convergence-trial index for the l-corridor. Figure 5.3a shows the quantiles plotted against the corridor lengths. We observe that longer corridors require more time to learn. This is probably because at each application of a movement action to a cell, there could be multiple successor cells due to nondeterminism. Intuitively, the nondeterminism causes a drift away from any straight path to the goal cells. The policy has to learn a suitable action for each of the cells encountered due to drift. So, when the corridor becomes longer, more cells are encountered due to nondeterminism, and therefore the learning process takes more time to learn a suitable action for each of the encountered cells. Figure 5.3a suggests an almost linear relationship between corridor length and the empirical upper bound on the convergence-trial index based on the 0.9- quantile. In Section 5.2, we will see another example, where the relationship is not linear.\nTrial Length For a fixed grid navigation task, we also wanted to test if trial length decreases as the run progresses. A decreasing trial length would demonstrate that the learning algorithm is gradually refining the policy to go more directly to reward from the start states, avoiding cycles in the state space.\nFor the fixed corridor length of l = 10, we have simulated the first 2000 trials of 1000 runs; and, for each trial we have measured its length as the number of transitions. This gives a data matrix where each cell (i, j), with run index i and trial index j, contains the length of trial j in the simulated run i. For each trial index we have computed the 0.9-quantile of its length measurements in all runs. By plotting the resulting empirical upper bound on trial length against the trial index, we arrive at Figure 5.3b. We can see that trial length generally\ndecreases as the run progresses. A similar experimental result is also reported by Potjans et al. (2011), in the context of neurons learning a grid navigation task.\nVisualizing Forward and Backward We recall from Theorem 4.5 that, within the context of a specific task, the final policy π in a converging run satisfies the inclusion forward(π) ⊆ backward(π). For the corridor in Figure 5.2, for one simulated run, we visualize the forward state set and the backward state set of the final policy in Figure 5.4. Note that the forward state set is indeed included in the backward state set. Interestingly, in this case, the simulated run has initially learned to perform the finish action in some goal cells, as witnessed by the backward state set, but eventually some of those goal cells are no longer reached from the start state, as witnessed by the forward state set."
    }, {
      "heading" : "5.2 Chain Tasks",
      "text" : "In addition to the concrete grid tasks of Section 5.1, we have also considered a slightly more abstract form of task, that we call chain task. The general form of a chain task is shown in Figure 5.5. The parameter n tells us how long the chain is; the states of the chain are, in order, 1, . . . , n and one final state n+ 1. To obtain reward from the start state, we should in the worst case perform all actions a1, . . . , an, in sequence, finished by one arbitrary action. For each i ∈ {1, . . . , n}, the action ai should be applied to state i. But there is forward nondeterminism that, for each pair (i, ai) could send us to an arbitrary state later in the chain, closer to state n+ 1. Also, there are backward deterministic transitions that take us back to the start state whenever we apply the wrong action to a state. Formally, for a fixed value of n ∈ N0, we obtain a graph Tn = (Q,Q0, A, rewards, δ), defined as follows: Q = {1, . . . , n, n+ 1}, Q0 = {1}, A = {a1, . . . , an}, rewards = {(n+ 1, a) | a ∈ A}, and regarding δ, we define\n• for each i ∈ Q,\nδ(i, ai) = {i+ 1, . . . , n+ 1}, δ(i, b) = {1} for each b ∈ A \\ {ai}; and,\n• for the goal state n+ 1, we define δ(n+ 1, a) = {n+ 1} for each a ∈ A.\nNote that for each n ∈ N0, the task Tn is reducible: conceptually, the reducibility iterations first assign action an to state n, then action an−1 to state n− 1, and so on, until state 1 is assigned action a1.\nOn chain tasks, we have performed simulation experiments that are similar to the experiments on the grid navigation tasks in Section 5.1. These experiments are discussed next.\nConvergence-trial Index Using the same experimental procedure as for grid tasks, we have simulated runs for some chain lengths. For each chain length separately, we have simulated 400 runs, and we have computed the 0.9-quantile of the measured convergence-trial indexes. By plotting the resulting empirical upper bound on the convergence-trial index against the chain length, we arrive at Figure 5.6a. We see that the convergence-trial index rises faster than linear in terms of the chain length; this is in contrast to Figure 5.3a for grid corridors. One possible explanation, is that the forward nondeterminism on the chain causes each state to be visited less frequently in a simulated run. For a fixed length n, the effect is that a state q ∈ {2, . . . , n} could stay connected longer to a bad action, leading back to the start state 1. Of course, at the end of each trial, the start state should be connected to action a1, because otherwise no progress could have been made; but the other states could in principle have any action. So, we might not yet encounter the final policy for a long time, as recognizable through the syntactic characterization of Theorem 4.5.\nTrial Length Using the same experimental procedure as for grid tasks, for the fixed chain length of n = 10, we have simulated the first 2000 trials of 1000 runs, and we have computed the 0.9-quantile on trial length as explained for the grid corridor experiment. By plotting the resulting empirical upper bound on trial length against the trial index, we arrive at Figure 5.6b. Again, this figure suggests that the learning algorithm is able to gradually improve the policy over trials.\nForm of the Final Policy We describe the form of the final policy, rather than visualizing it, because the form is very restricted. Consider the final policy π in a simulated run of the chain task with length n ∈ N0. In the simulation, we know that each trial should have ended with action a1 assigned to state 1 because otherwise the state n + 1 could not have been reached. This property also applies to the convergence-trial, so π(1) = a1. Hence, forward(π) is the set of all states due to the forward nondeterminism along the chain. We recall from Theorem 4.5 that the final policy satisfies forward(π) ⊆ backward(π). Therefore, backward(π) is also the set of all states. We can now see that the final policy π has to satisfy π(i) = ai for all i ∈ {1, . . . , n}. Towards a contradiction, let i be the largest state number for which π(i) 6= ai. Then, using the iterations for computing backward(π), we can see that {i + 1, . . . , n + 1} ⊆ backward(π) but {1, . . . , i} ∩ backward(π) = ∅.23 This would be the desired contradiction, because backward(π) is the set of all states."
    }, {
      "heading" : "6 Conclusion and Further Work",
      "text" : "We have studied the fascinating idea of reinforcement learning in a non-numeric framework, where the focus lies on the interaction between the graph structure of the task and a learning algorithm. We have studied the graph property of reducibility, that implies the existence of a policy that makes steady progress towards reward despite nondeterminism in the task. Interestingly, reducibility, combined with a natural fairness assumption, enables our simple learning\n23Here, the computation of backward(π) is started with the set ground(π) = {n+ 1}.\nalgorithm to learn the task. We have also characterized the final policy for converging runs, which allows the precise detection of the convergence-trial in simulations. We now discuss some avenues for further work.\nCharacterizing Tasks We have seen a sufficient property (Theorem 4.1) and necessary properties (Proposition 4.6) for tasks to be learnable under fairness. The gap between the sufficient and necessary properties seems strongly related to the unstable set of a task (see Section 4.3). Perhaps it is possible to characterize the tasks that are learnable under fairness by imposing some additional constraints on the way the unstable set is connected to the reducible states, in addition to the already identified necessary properties.\nTime Before Convergence Related to Remark 4.2 and to the simulations in Section 5, one could try to theoretically provide an upper bound on the convergence-trial index, for some class of tasks. We could also make assumptions regarding the probability distributions underlying the random actions proposed for a state and the choice of successor states when applying an action. For a given task, the result could be a probability distribution on the convergence-trial index, or on the total number of transitions before convergence.\nFading Eligibility Traces In some models of biologically plausible learning, the activation between any pair of connected neurons is represented by an eligibility trace (Sutton and Barto, 1998; Frémaux et al., 2013; Gerstner et al., 2014). When obtaining reward, the value of this trace is applied to the synaptic weight between the neurons. In realistic scenarios, the traces fade, with the advantage that a simulation or practical setup does not have to keep remembering all the information of past states before obtaining reward. The current article may be viewed as studying eligibility traces that are non-fading, because the value “visited” that the learning algorithm assigns to states is effectively stored until the end of the trial. It appears interesting to let symbolic values\ndegrade in a stepwise fashion, giving the learning algorithm in some sense a more limited memory. This idea might be incorporated into the framework by resetting the flag visited to unvisited after some limited timeout, already before the end of the trial.\nNegative Feedback In this article, any path through the state space from a given start state to reward is good. However, in some applications we want to avoid certain states. For example, in a navigation task, the organism might want to reach its nest from a certain starting location, but on the way to the nest the organism should avoid hazardous locations like pits or swamps. It appears interesting to formally investigate such cases. Information about hazards could be incorporated by extending the framework of this article with an explicit set hazards in tasks, which contains the state-action pairs which should be avoided; this is the opposite of the set rewards. The framework could further be extended to differentiate between trials that terminate with reward and trials that terminate with a hazard. A run could be said to convergence if eventually all trials terminate with reward, and all states eventually become stable.\nIncomplete Information and Generalization The framework studied in this article provides complete information to the learning algorithm because each individual state can be mapped to its own action. In real-world applications, such as robot navigation (Thrun et al., 2005), the agent can only work with limited sensory information available in each time step. In that case, the agent should first build concepts for states in order to differentiate them. These concepts should be made by remembering sensory information over time, and in general multiple states will remain grouped together under the same concept because sensory information is not sufficiently accurate to differentiate between all states. This issue was also raised as an item for future work by Frémaux et al. (2013), who initially also have considered a framework in which complete information is available to the learning agent.\nBuilding concepts is related to the problem of generalization (Sutton and Barto, 1998) because in real-world tasks there might be too many states to store in the policy. It would be useful to collect states in conceptual groups and then assign an action to each group.\nIt appears interesting to formalize incomplete information and generalization in an extended version of the current framework, and to investigate sufficient and necessary properties for convergence."
    }, {
      "heading" : "A Examples",
      "text" : "This Section contains additional details for the example tasks discussed in Section 5.\nA.1 Grid Action Offsets\nFor our formalization of grid tasks, Table A.1 gives the offsets for each action.\nA.2 Quantiles\nWe compute the quantiles with the statistics package R. For completeness, we recall here the definition of quantiles that we have used in our analysis; this definition is called Definition 1 by Hyndman and Fan (1996).\nLet L be a nonempty list of numbers, possibly containing duplicates. Let |L| denote the length of L. For an index i ∈ {1, . . . , |L|}, we write L[i] to denote the number at index i in L. We write order(L) to denote the ordered version of L, where the numbers are sorted in ascending fashion; we keep duplicates, so |order(L)| = |L|.\nLet p ∈ R with 0 < p < 1. The p-quantile of a nonempty list L of numbers, denoted Q(p, L) is defined as follows: denoting j = bp · nc where n = |L|,\nQ(p, L) = { order(L)[j + 1] if p · n− j > 0 order(L)[j] if p · n− j = 0.\nIntuitively, using the index j = bp · nc in the ordered list order(L) is a good attempt at finding a number v such that a fraction p of L is smaller than or equal to v. The assumptions 0 < p and p < 1 ensure that we only apply valid indexes in the range {1, . . . , |L|} to the list order(L).\nA.3 Implementation Notes\nThe simulation was written with Java Development Kit 8. In our experimental results, we have measured only the number of transitions and the number trials in runs. Since no wall-clock time was needed, the exact running time of the simulation in seconds was not measured.\nDuring any transition, we have used a uniform sampling from the possible successor states. Given an array of successor states, we have used the function Math.random() to generate random indexes in this array, as follows: the double precision number returned by Math.random() can be converted to an integer by multiplying with the array length and subsequently truncating the resulting number with the Math.floor() function."
    } ],
    "references" : [ {
      "title" : "Principles of Model Checking (Representation",
      "author" : [ "C. Baier", "J. Katoen" ],
      "venue" : null,
      "citeRegEx" : "Baier and Katoen,? \\Q2008\\E",
      "shortCiteRegEx" : "Baier and Katoen",
      "year" : 2008
    }, {
      "title" : "TD(λ) converges with probability 1",
      "author" : [ "P. Dayan", "T. Sejnowski" ],
      "venue" : null,
      "citeRegEx" : "Dayan and Sejnowski,? \\Q1994\\E",
      "shortCiteRegEx" : "Dayan and Sejnowski",
      "year" : 1994
    }, {
      "title" : "Reinforcement learning using a continuous time actor-critic framework with spiking neurons",
      "author" : [ "N. Frémaux", "H. Sprekeler", "W. Gerstner" ],
      "venue" : "PLoS Computational Biology, 9(4):e1003024.",
      "citeRegEx" : "Frémaux et al\\.,? 2013",
      "shortCiteRegEx" : "Frémaux et al\\.",
      "year" : 2013
    }, {
      "title" : "Neuronal Dynamics: From Single Neurons to Networks and Models of Cognition",
      "author" : [ "W. Gerstner", "W. Kistler", "R. Naud", "L. Paninski" ],
      "venue" : "Cambridge University Press.",
      "citeRegEx" : "Gerstner et al\\.,? 2014",
      "shortCiteRegEx" : "Gerstner et al\\.",
      "year" : 2014
    }, {
      "title" : "Spatial cognition in bats and rats: from sensory acquisition to multiscale maps and navigation",
      "author" : [ "M. Geva-Sagiv", "L. Las", "Y. Yovel", "N. Ulanovsky" ],
      "venue" : "Nature Reviews Neuroscience, 16(4):94–108.",
      "citeRegEx" : "Geva.Sagiv et al\\.,? 2015",
      "shortCiteRegEx" : "Geva.Sagiv et al\\.",
      "year" : 2015
    }, {
      "title" : "Sample quantiles in statistical packages",
      "author" : [ "R. Hyndman", "Y. Fan" ],
      "venue" : "The American Statistician, 50(4):361–365.",
      "citeRegEx" : "Hyndman and Fan,? 1996",
      "shortCiteRegEx" : "Hyndman and Fan",
      "year" : 1996
    }, {
      "title" : "On the convergence of stochastic iterative dynamic programming algorithms",
      "author" : [ "T. Jaakkola", "M. Jordan", "S. Singh" ],
      "venue" : "Neural Computation, 6(6).",
      "citeRegEx" : "Jaakkola et al\\.,? 1994",
      "shortCiteRegEx" : "Jaakkola et al\\.",
      "year" : 1994
    }, {
      "title" : "An imperfect dopaminergic error signal can drive temporal-difference learning",
      "author" : [ "W. Potjans", "M. Diesmann", "A. Morrison" ],
      "venue" : "PLoS Computational Biology, 7(5):e1001133.",
      "citeRegEx" : "Potjans et al\\.,? 2011",
      "shortCiteRegEx" : "Potjans et al\\.",
      "year" : 2011
    }, {
      "title" : "Updating dopamine reward signals",
      "author" : [ "W. Schultz" ],
      "venue" : "Current Opinion in Neurobiology, 23(2):229 – 238.",
      "citeRegEx" : "Schultz,? 2013",
      "shortCiteRegEx" : "Schultz",
      "year" : 2013
    }, {
      "title" : "Neuronal reward and decision signals: From theories to data",
      "author" : [ "W. Schultz" ],
      "venue" : "Physiological Reviews, 95(3):853–951.",
      "citeRegEx" : "Schultz,? 2015",
      "shortCiteRegEx" : "Schultz",
      "year" : 2015
    }, {
      "title" : "Learning to predict by the methods of temporal differences",
      "author" : [ "R. Sutton" ],
      "venue" : "Machine Learning, 3(1):9–44.",
      "citeRegEx" : "Sutton,? 1988",
      "shortCiteRegEx" : "Sutton",
      "year" : 1988
    }, {
      "title" : "Reinforcement Learning, An Introduction",
      "author" : [ "R. Sutton", "A. Barto" ],
      "venue" : "The MIT Press.",
      "citeRegEx" : "Sutton and Barto,? 1998",
      "shortCiteRegEx" : "Sutton and Barto",
      "year" : 1998
    }, {
      "title" : "Probabilistic Robotics",
      "author" : [ "S. Thrun", "W. Burgard", "D. Fox" ],
      "venue" : "The MIT Press.",
      "citeRegEx" : "Thrun et al\\.,? 2005",
      "shortCiteRegEx" : "Thrun et al\\.",
      "year" : 2005
    }, {
      "title" : "Asynchronous stochastic approximation and Q-learning",
      "author" : [ "J. Tsitsiklis" ],
      "venue" : "Machine Learning, 16(3):185–202.",
      "citeRegEx" : "Tsitsiklis,? 1994",
      "shortCiteRegEx" : "Tsitsiklis",
      "year" : 1994
    }, {
      "title" : "Q-learning",
      "author" : [ "C. Watkins", "P. Dayan" ],
      "venue" : "Machine Learning, 8(3–4):279– 292.",
      "citeRegEx" : "Watkins and Dayan,? 1992",
      "shortCiteRegEx" : "Watkins and Dayan",
      "year" : 1992
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "Reinforcement Learning Biological organisms can learn to perform tasks in environments, such as navigating from their nest to foraging areas and back again (Geva-Sagiv et al., 2015).",
      "startOffset" : 156,
      "endOffset" : 181
    }, {
      "referenceID" : 11,
      "context" : "In the field of reinforcement learning, task performance is measured as the amount of reward received (Sutton and Barto, 1998).",
      "startOffset" : 102,
      "endOffset" : 126
    }, {
      "referenceID" : 2,
      "context" : "One could assume that connections are initially random between these types of neuron, meaning that initially there is no clear preference of actions for states (Frémaux et al., 2013).",
      "startOffset" : 160,
      "endOffset" : 182
    }, {
      "referenceID" : 11,
      "context" : "1The expectation of reward could for example be represented by a third group of neurons, as in the actor-critic framework (Sutton and Barto, 1998; Potjans et al., 2011; Frémaux et al., 2013).",
      "startOffset" : 122,
      "endOffset" : 190
    }, {
      "referenceID" : 7,
      "context" : "1The expectation of reward could for example be represented by a third group of neurons, as in the actor-critic framework (Sutton and Barto, 1998; Potjans et al., 2011; Frémaux et al., 2013).",
      "startOffset" : 122,
      "endOffset" : 190
    }, {
      "referenceID" : 2,
      "context" : "1The expectation of reward could for example be represented by a third group of neurons, as in the actor-critic framework (Sutton and Barto, 1998; Potjans et al., 2011; Frémaux et al., 2013).",
      "startOffset" : 122,
      "endOffset" : 190
    }, {
      "referenceID" : 11,
      "context" : "In standard reinforcement learning (Sutton and Barto, 1998), reward is represented by a numeric value on each transition of the task, where larger positive numbers represent higher rewards.",
      "startOffset" : 35,
      "endOffset" : 59
    }, {
      "referenceID" : 2,
      "context" : "Such state spaces could be part of a physical world, representing a map of physical places, or they can be more abstract with more than three dimensions (Frémaux et al., 2013).",
      "startOffset" : 153,
      "endOffset" : 175
    }, {
      "referenceID" : 0,
      "context" : "Such a framework brings reinforcement learning theory closer to many areas of computer science that have a more discrete mathematics approach, notably the area of model checking (Baier and Katoen, 2008).",
      "startOffset" : 178,
      "endOffset" : 202
    }, {
      "referenceID" : 12,
      "context" : "in real-world applications (Thrun et al., 2005).",
      "startOffset" : 27,
      "endOffset" : 47
    }, {
      "referenceID" : 10,
      "context" : "Many previous works in numerical reinforcement learning try to find optimal policies, and their related optimal value functions (Sutton, 1988; Watkins and Dayan, 1992; Dayan, 1992; Dayan and Sejnowski, 1994; Jaakkola et al., 1994; Tsitsiklis, 1994); an optimal policy gives the highest reward in the long run.",
      "startOffset" : 128,
      "endOffset" : 248
    }, {
      "referenceID" : 14,
      "context" : "Many previous works in numerical reinforcement learning try to find optimal policies, and their related optimal value functions (Sutton, 1988; Watkins and Dayan, 1992; Dayan, 1992; Dayan and Sejnowski, 1994; Jaakkola et al., 1994; Tsitsiklis, 1994); an optimal policy gives the highest reward in the long run.",
      "startOffset" : 128,
      "endOffset" : 248
    }, {
      "referenceID" : 1,
      "context" : "Many previous works in numerical reinforcement learning try to find optimal policies, and their related optimal value functions (Sutton, 1988; Watkins and Dayan, 1992; Dayan, 1992; Dayan and Sejnowski, 1994; Jaakkola et al., 1994; Tsitsiklis, 1994); an optimal policy gives the highest reward in the long run.",
      "startOffset" : 128,
      "endOffset" : 248
    }, {
      "referenceID" : 6,
      "context" : "Many previous works in numerical reinforcement learning try to find optimal policies, and their related optimal value functions (Sutton, 1988; Watkins and Dayan, 1992; Dayan, 1992; Dayan and Sejnowski, 1994; Jaakkola et al., 1994; Tsitsiklis, 1994); an optimal policy gives the highest reward in the long run.",
      "startOffset" : 128,
      "endOffset" : 248
    }, {
      "referenceID" : 13,
      "context" : "Many previous works in numerical reinforcement learning try to find optimal policies, and their related optimal value functions (Sutton, 1988; Watkins and Dayan, 1992; Dayan, 1992; Dayan and Sejnowski, 1994; Jaakkola et al., 1994; Tsitsiklis, 1994); an optimal policy gives the highest reward in the long run.",
      "startOffset" : 128,
      "endOffset" : 248
    }, {
      "referenceID" : 10,
      "context" : "The numerical reinforcement learning technique of temporal difference (TD) learning (Sutton, 1988; Sutton and Barto, 1998) has been associated to the way biological organisms learn.",
      "startOffset" : 84,
      "endOffset" : 122
    }, {
      "referenceID" : 11,
      "context" : "The numerical reinforcement learning technique of temporal difference (TD) learning (Sutton, 1988; Sutton and Barto, 1998) has been associated to the way biological organisms learn.",
      "startOffset" : 84,
      "endOffset" : 122
    }, {
      "referenceID" : 9,
      "context" : "In particular, the global dopamine signal in the brain might encode the predication error that is key to temporal difference learning (Schultz, 2015).",
      "startOffset" : 134,
      "endOffset" : 149
    }, {
      "referenceID" : 7,
      "context" : "However, TD-learning is not perfectly matched to the dopamine signal because the negative TD-prediction error can only be represented by dopamine to a limited extent (Potjans et al., 2011; Schultz, 2013).",
      "startOffset" : 166,
      "endOffset" : 203
    }, {
      "referenceID" : 8,
      "context" : "However, TD-learning is not perfectly matched to the dopamine signal because the negative TD-prediction error can only be represented by dopamine to a limited extent (Potjans et al., 2011; Schultz, 2013).",
      "startOffset" : 166,
      "endOffset" : 203
    }, {
      "referenceID" : 11,
      "context" : "Perhaps in contrast to the notion of optimality often used in the previous work on numerical reinforcement learning (Sutton and Barto, 1998), where the aim is to find a policy that gives the highest expected reward, we are only studying a very local form of optimality, where convergence is formalized as the eventual avoidance of cycles in the paths through the state space.",
      "startOffset" : 116,
      "endOffset" : 140
    }, {
      "referenceID" : 2,
      "context" : "We believe that this viewpoint on convergence aligns well with animal and human learning, where an organism learns just some path to reward, heavily dependent on the experience (Frémaux et al., 2013).",
      "startOffset" : 177,
      "endOffset" : 199
    }, {
      "referenceID" : 8,
      "context" : "In particular, disappointment could be related to the expectation of reward, and can be represented by the suppression of dopamine (Schultz, 2013).",
      "startOffset" : 131,
      "endOffset" : 146
    }, {
      "referenceID" : 14,
      "context" : "1, showing that convergence always occurs on reducible tasks (with our definition of convergence), is related in intent to previous results showing that certain numerical learning algorithms converge with probability 1 (Watkins and Dayan, 1992; Dayan, 1992; Dayan and Sejnowski, 1994; Tsitsiklis, 1994).",
      "startOffset" : 219,
      "endOffset" : 302
    }, {
      "referenceID" : 1,
      "context" : "1, showing that convergence always occurs on reducible tasks (with our definition of convergence), is related in intent to previous results showing that certain numerical learning algorithms converge with probability 1 (Watkins and Dayan, 1992; Dayan, 1992; Dayan and Sejnowski, 1994; Tsitsiklis, 1994).",
      "startOffset" : 219,
      "endOffset" : 302
    }, {
      "referenceID" : 13,
      "context" : "1, showing that convergence always occurs on reducible tasks (with our definition of convergence), is related in intent to previous results showing that certain numerical learning algorithms converge with probability 1 (Watkins and Dayan, 1992; Dayan, 1992; Dayan and Sejnowski, 1994; Tsitsiklis, 1994).",
      "startOffset" : 219,
      "endOffset" : 302
    }, {
      "referenceID" : 10,
      "context" : "Also, the notion of reducibility discussed in this article is related to the principles of dynamic programming explained by Sutton and Barto (1998). Indeed, in reducibility, we defer the responsibility of obtaining reward from a given state to one of the successor states under a chosen action.",
      "startOffset" : 124,
      "endOffset" : 148
    }, {
      "referenceID" : 11,
      "context" : "Our formalization of tasks keeps only the graph structure of models previously studied in reinforcement learning; essentially, compared to finite Markov decision processes (Sutton and Barto, 1998), we omit transition probabilities and we simplify the numerical reward signals to boolean flags.",
      "startOffset" : 172,
      "endOffset" : 196
    }, {
      "referenceID" : 11,
      "context" : "In this article we also do not consider the topic of generalization (Sutton and Barto, 1998), where an agent only sees concepts that are abstractions of states, and the agent learns to assign actions to concepts instead of directly to states; one concept could represent many states.",
      "startOffset" : 68,
      "endOffset" : 92
    }, {
      "referenceID" : 7,
      "context" : "for the learning algorithm comes from biologically plausible learning algorithms based on the global dopamine signal in brains (Potjans et al., 2011; Frémaux et al., 2013; Schultz, 2013, 2015).",
      "startOffset" : 127,
      "endOffset" : 192
    }, {
      "referenceID" : 2,
      "context" : "for the learning algorithm comes from biologically plausible learning algorithms based on the global dopamine signal in brains (Potjans et al., 2011; Frémaux et al., 2013; Schultz, 2013, 2015).",
      "startOffset" : 127,
      "endOffset" : 192
    }, {
      "referenceID" : 11,
      "context" : "In the framework of this article, and similar to other theoretical models in standard reinforcement learning (Sutton and Barto, 1998), the input neurons are abstractly represented by task states and output neurons are abstractly represented by actions.",
      "startOffset" : 109,
      "endOffset" : 133
    }, {
      "referenceID" : 11,
      "context" : "In numeric reinforcement learning, there is a value function that usually converges together with the policy (Sutton and Barto, 1998).",
      "startOffset" : 109,
      "endOffset" : 133
    }, {
      "referenceID" : 11,
      "context" : "The framework studied in this article may be called episodic (Sutton and Barto, 1998) because the agent should continuously navigate from start states to goal states; and, after obtaining the immediate reward at a goal state, the agent’s location is again reset to a start state.",
      "startOffset" : 61,
      "endOffset" : 85
    }, {
      "referenceID" : 11,
      "context" : "This intuition is related to the Markov assumption, or independence of path assumption (Sutton and Barto, 1998).",
      "startOffset" : 87,
      "endOffset" : 111
    }, {
      "referenceID" : 2,
      "context" : "The experience dependence was experimentally observed by Frémaux et al. (2013).",
      "startOffset" : 57,
      "endOffset" : 79
    }, {
      "referenceID" : 11,
      "context" : "Our first example is a navigation task in a grid world (Sutton and Barto, 1998; Potjans et al., 2011).",
      "startOffset" : 55,
      "endOffset" : 101
    }, {
      "referenceID" : 7,
      "context" : "Our first example is a navigation task in a grid world (Sutton and Barto, 1998; Potjans et al., 2011).",
      "startOffset" : 55,
      "endOffset" : 101
    }, {
      "referenceID" : 7,
      "context" : "A similar experimental result is also reported by Potjans et al. (2011), in the context of neurons learning a grid navigation task.",
      "startOffset" : 50,
      "endOffset" : 72
    }, {
      "referenceID" : 11,
      "context" : "Fading Eligibility Traces In some models of biologically plausible learning, the activation between any pair of connected neurons is represented by an eligibility trace (Sutton and Barto, 1998; Frémaux et al., 2013; Gerstner et al., 2014).",
      "startOffset" : 169,
      "endOffset" : 238
    }, {
      "referenceID" : 2,
      "context" : "Fading Eligibility Traces In some models of biologically plausible learning, the activation between any pair of connected neurons is represented by an eligibility trace (Sutton and Barto, 1998; Frémaux et al., 2013; Gerstner et al., 2014).",
      "startOffset" : 169,
      "endOffset" : 238
    }, {
      "referenceID" : 3,
      "context" : "Fading Eligibility Traces In some models of biologically plausible learning, the activation between any pair of connected neurons is represented by an eligibility trace (Sutton and Barto, 1998; Frémaux et al., 2013; Gerstner et al., 2014).",
      "startOffset" : 169,
      "endOffset" : 238
    }, {
      "referenceID" : 12,
      "context" : "In real-world applications, such as robot navigation (Thrun et al., 2005), the agent can only work with limited sensory information available in each time step.",
      "startOffset" : 53,
      "endOffset" : 73
    }, {
      "referenceID" : 11,
      "context" : "Building concepts is related to the problem of generalization (Sutton and Barto, 1998) because in real-world tasks there might be too many states to store in the policy.",
      "startOffset" : 62,
      "endOffset" : 86
    }, {
      "referenceID" : 2,
      "context" : "This issue was also raised as an item for future work by Frémaux et al. (2013), who initially also have considered a framework in which complete information is available to the learning agent.",
      "startOffset" : 57,
      "endOffset" : 79
    } ],
    "year" : 2017,
    "abstractText" : "Reinforcement learning is a formal framework for modeling agents that learn to solve tasks. For example, one important task for animals is to navigate in an environment to find food or to return to their nest. In such tasks, the agent has to learn a path through the environment from start states to goal states, by visiting a sequence of intermediate states. The agent receives reward on a goal state. Concretely, we need to learn a policy that maps each encountered state to an immediate action that leads to a next state, eventually leading to a goal state. We say that a learning process has converged if eventually the policy will no longer change, i.e., the policy stabilizes. The intuition of paths and navigation policies can be applied generally. Indeed, in this article, we study navigation tasks formalized as a graph structure that, for each application of an action to a state, describes the possible successor states that could result from that application. In contrast to standard reinforcement learning, we essentially simplify numeric reward signals to boolean flags on the transitions in the graph. The resulting framework enables a clear theoretical study of how properties of the graph structure can cause convergence of the learning process. In particular, we formally study a learning process that detects revisits to states in the graph, i.e., we detect cycles, and the process keeps adjusting the policy until no more cycles are made. So, eventually, the agent goes straight to reward from each start state. We identify reducibility of the task graph as a sufficient condition for this learning process to converge. We also syntactically characterize the form of the final policy, which can be used to detect convergence in a simulation.",
    "creator" : "LaTeX with hyperref package"
  }
}