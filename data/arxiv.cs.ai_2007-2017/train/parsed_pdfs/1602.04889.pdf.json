{
  "name" : "1602.04889.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Multi-Source Domain Adaptation Using Approximate Label Matching",
    "authors" : [ "Jordan T. Ash", "Robert E. Schapire" ],
    "emails" : [ "JORDANTASH@GMAIL.COM", "SCAHPIRE@MICROSOFT.EDU" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Intuitively, truly intelligent agents should be able to improve on one task after having learned a similar kind of task. A human, for example, might be more capable of speaking Italian after having learned Spanish, or of programming a computer in Java after having learned C++. The goal of domain adaptation is to endow machine learning with this same sort of behavior.\nIn the usual classification or regression framework, we assume that training and testing data are generated by the same underlying distribution. When this assumption does not hold, testing performance can be significantly worse than training performance.\nThis general problem, where training and testing datasets come from different distributions, comes up frequently in many areas of machine learning, especially Natural Language Processing (NLP) (Jiang & Zhai, 2007; Glorot et al., 2011) and computer vision (Beijbom, 2012; Patel et al.,\n2015).\nOur work was motivated by a problem arising in the classification of magnetoencephalography (MEG) data. MEG machines use magnetic fields to measure the brain’s electrical activity, and are frequently used for experiments in neuroscience and psychology. In the specific problem we consider, which was originally part of an online competition, data came from sixteen different people who were shown either pictures of human faces or non-faces (images with no discernible face-like structures), and had their brain activities recorded using MEG. The goal of this challenge was to classify instances where the subject was looking at a face from those where the subject was not. Figure 1 shows this data embedded into two dimensions using t-distributed stochastic neighbor embedding (t-SNE) (der Maaten & Hinton, 2008).\nIn Figure 1, each cluster of points, or domain, corresponds to data generated by a different subject. This is a clear domain adaptation problem because, while each of the training subject datasets appear easily classifiable, none of their hypotheses would obviously transfer well to the held-out data.\nStill, each subject’s data appears to be rotated and translated with respect to most other subject’s data. If we want to transfer a classification hypothesis from one subject’s data, which we will refer to as a source, to a held-out subject, which we will refer to as a target, it would be sufficient to find a transformation that aligns these two datasets. The goal of our technique, and others like it, is to uncover such a transformation. Because we are not provided in our training set with any labeled samples that are generated by the distribution that generates the target data, the problem is called unsupervised.\nIn this work we present a new method for unsupervised domain adaptation. Our approach assigns an approximate, better-than-guessing labeling to the target dataset, then uses this labeling to uncover a transformation that matches the target data with the data from a particular source.\nar X\niv :1\n60 2.\n04 88\n9v 1\n[ cs\n.L G\n] 1\n6 Fe\nb 20\n16\nAfter describing the technique in detail, we will provide illustrative and practical examples demonstrating its performance in several spheres."
    }, {
      "heading" : "1.1. Related Work",
      "text" : "As described in (Mohri, 2015), domain adaptation has historically been approached in mainly two ways.\nIn one, training samples are re-weighted to make the resulting hypothesis better suited to classification on the testing set. Kernel Mean Matching (KMM) is a well-known domain adaptation technique that falls into this category (Gretton et al., 2009). This algorithm re-weights source points in an effort to make the means of the source and target dataset as close as possible in a reproducing kernel Hilbert space. An advantage of KMM is that it circumvents the need to approximate any distributions.\nIn the alternative approach, the source and/or target datasets are transformed into a different space where they better match each other. Transfer Component Analysis (TCA) operates in this way (Pan et al., 2011), attempting to find a set of “transfer components” such that source and target datasets appear to have the same underlying distribution when projected into this new space.\nBecause we attempt to transform the target data to make it look like the source data, our method falls into the latter category."
    }, {
      "heading" : "2. Our Method",
      "text" : ""
    }, {
      "heading" : "2.1. Problem Setup",
      "text" : "In our problem, the learner has been given k source datasets (Xs1 , Y s 1 ), ..., (X s k, Y s k ) from which it tries to transfer knowledge, and one target sequence Xt = 〈xt1, ..., xtn〉 to which it tries to transfer knowledge. Each source dataset (Xsi , Y s i ) is composed of sequences X s i = 〈xs1,i, ..., xsmi,i〉 and Y si = 〈ys1,i, ..., ysmi,i〉. Every y s j ∈ {−1, 1} corresponds to a label on xsj ∈ Rd. For each xtj ∈ Rd in the target sequence, the goal of the classifier is to assign a label yti ∈ {−1, 1} for each target example xti.\nWe assume that within each source i, each labeled example (xsi,j , y s i,j) is generated according to the same distribution, and likewise for the target. However, the distributions associated with the various sources and the target will generally all be different, although we think of them as being similar enough for learning to be transfered, to some degree, from one to another.\nA sentiment classification model trained on a corpus of book reviews, for example, may generalize poorly when reviews are considered from the domain of kitchen appliances. Similarly, a classifier that is trained on MEG data from one person may generalize poorly when used on data generated by another person that might have a different skull geometry, apparatus alignment, or neurological physiology.\nOne natural solution to this classification problem is to combine all the source datasets (Xsi , Y s i ), so that we can a model, which we call g, on all available sources simultaneously. Of course, because the testing data Xt is generated from a distribution that may be unlike any of those composing the training set, we expect this hypothesis might not work very well. In many cases though, g might not be terrible, and so we assume that it is, at a minimum, better than random guessing on the target data.\nAnother natural solution might be to treat each Xsi as a separate training set. In this framework, we would have k separate “local” classifiers, which we refer to as fi. Again, as Figure 1 shows, because Xt is generated from a different distribution from any of the Xsi , we cannot expect any particular fi to perform as well on samples from Xt as it does on samples from Xsi . Once each fi has been trained, the learner has access to k different labelings on each of the samples in Xt, and a classification for a target sample xtj ∈ Xt can be returned as sign (∑k i=1 fi(x t j) ) .\nAgain, in Figure 1, every domain appears to be rotated and translated with respect to every other domain. Motivated by such situations, we suppose that each source domain (Xsi , Y s i ) can be transformed to “look like” a target domain via a transformation φi. Even though each fi probably will\nAlgorithm 1 Approximate Label Matching Input: (Xs1 , Y s1 ), ..., (Xsk, Y sk ) and Xt Xs ← concat(Xs1 , ..., Xsk) Y s ← concat(Y s1 , ..., Y sk ) g ← train(Xs, Y s) for i = 1, 2, ..., k do fi ← train(Xsi , Y si ) φi ← argmin\nφi\n{∑n j=1 ( g(xtj)− fi(φi(xtj)) )2} end for return sign{ ∑k i=1 fi(φi(x t j))}\nnot transfer well on average to a fixed sample sample xtj , it hopefully will transfer well to φi(xtj). So, if we can find an appropriate φi for each available source, we can assign a reasonable final hypothesis for each target sample as\nsign ( k∑ i=1 fi(φi(x t j)) ) . (1)\nIn order to uncover these transformations, we leverage our assumption that g performs better than guessing on the target data. We use the output of g on the samples in Xt as an approximate labeling that allows us to solve for φi. Specifically, we are interested in the φi that minimizes the difference between g(xtj), our rough labeling, and fi(φi(x t j)) on average. We can attempt to find such a transformation φ by minimizing the squared error between these two different labelings of xtj , that is, by minimizing:\nn∑ j=1 ( g(xtj)− fi(φi(xtj)) )2 (2)\nThe intuitive idea is that fi is the correct decision boundary shape, and we only need to find a way to align Xt to Xis in order for fi to transfer well. Because g(xtj) produces a labeling that is better than guessing, the best φi hopefully occurs when g(Xtj) and fi(φi(x t j)) agree most.\nThis classification procedure as a whole is summarized by Algorithm 1. Here, train(X,Y ) trains a model on dataset (X,Y ) and concat(X1, ..., Xk) concatenates sequences X1 through Xk. To find φi for a fixed source i, we first train classifiers g and fi, and then solve for φi by minimizing Equation (2). Once we have done this procedure for all available sources, we return a consensus labeling according to Equation (1)."
    }, {
      "heading" : "2.2. Implementation",
      "text" : "These ideas can be implemented using a neural network. Specifically, we make each fi a neural network of arbitrary\narchitecture, and then use it to learn φi. As a reminder, by the time we are solving for φi, fi and g have already been trained and are fixed. To find φi for source i, we simply take the already-trained fi neural network and add one or more additional layers before its existing input layer.\nThese additional layers are connected to fi by linear neurons and represent φi. When training this aggregate network, we use g(xtj) as labels on x t j for back-propagation, and we do not update any of the weights in the layers corresponding to fi. This implementation is convenient because it allows us to easily optimize (2) even though φi is nested in fi.\nIt is worth mentioning that this same basic method can be used even when we are interested in transformations of a particular class. For example, to constrain φi to the space of rotation matrices, we have been able to leverage a solution to Wahba’s problem (Markley, 1988) at every iteration of back propagation."
    }, {
      "heading" : "2.3. An Illustration",
      "text" : "Now we show a synthetic example to provide further insight into our algorithm. In this illustration, we generate a sine wave as the correct dichotomizer between positives and negatives. We then create five sources centered at different random locations along the sine wave. Each source consists of a thousand points, which are labeled as positive if they fall above the sine wave and negative otherwise. Figure 3 shows an example of this scenario.\nFor this illustration g is logistic regression and each fi is a neural network with a single hidden layer composed of three hidden nodes. A randomly-chosen domain is held out as a target, and we attempt to assign labels to it by transferring knowledge from the four remaining labeled domains.\nBy looking at Figure 3, it is clear that g, trained on four\nsources simultaneously, will be able to classify any heldout domain fairly well. However, because the correct decision boundary is sinusoidal and curved, the linear hypotheses learned by g will be less than optimal.\nSimilarly, fi, which is trained on only one source, may or may not classify a held out target well. Instead of applying fi or g to classify the target directly, we use them to learn a transformation φi that makes our target data align to a particular source. By following this process for all sources we could finally output a classification according to Equation (1).\nAfter running this experiment a hundred times, we find that g(xtj), fi(x t j), and fi(φi(x t j)) have average error fractions of .16, .08, and .07 on held-out domains. These particular numbers are not very important, and certainly vary with experimental parameters that define the sine wave and source distributions. Still, it is notable that the average fi(φi(xtj)) is able to outperform the two naive methods of classification."
    }, {
      "heading" : "3. Experiments",
      "text" : ""
    }, {
      "heading" : "3.1. Experimental Setting",
      "text" : "This section describes three separate experiments that were conducted with our technique. For each experiment, we add one additional layer to fi when solving for φ, restricting φ to a linear transformation. In each subsection, we report different error rates for each available domain. This is done by holding out a fixed domain as a target, then using every other domain as a labeled source. For a comparison, we also include error rates for the two natural hypotheses described earlier, g(xtj) and sign( ∑k i=1 fi(x t j)), as well as those resulting from TCA and KMM.\nThe TCA (Pan et al., 2011) algorithm takes a source and target as inputs and returns a canonical feature representation. Once in this space, a neural network with the same architecture as g and each fi is used for classification.\nKMM (Gretton et al., 2009) also takes a source and target as inputs, and produces a set of weights corresponding to the importance of classifying each source instance correctly. These weights were then normalized, and a training set was constructed by sampling instances of the source\ndataset with replacement from the resulting distribution. Again, this training set was used to train a neural network with the same architecture as g and fi.\nTCA and KMM both allow the user to choose a kernel. In these experiments, we found that both worked best when used with a radial basis function.\nBecause TCA and KMM are both able to transfer knowledge from a single source to a single target, the reported errors are for labelings that result from aggregating the confidence-weighted vote of the classification scores produced from each individual source.\nWe also report the results of a “cheating” experiment, in which a learner is asked to classify some samples from the target data after being given labeled samples from the same domain. This classifier is again a neural network with the same architecture as fi and these errors are computed using ten-fold cross validation. The purpose of this is to show for comparison how well the learner can do in a setting in which training and testing examples come from the same distribution"
    }, {
      "heading" : "3.2. MEG data",
      "text" : "The MEG dataset was provided by Kaggle, a data science competition website, in a challenge sponsored by several companies, including Elekta Oy, MEG International Services, Fondazione, and Besa. The features of this data are derived by estimating each subject’s covariance matrix (Barachant & Congedo, 2014) and then computing a vector according to (Barachant et al., 2013). This representation, when projected down into two dimensions, is what is shown in Figure 1. As stated earlier, labels correspond to whether or not the subject was looking at a face at the time the MEG data was recorded.\nFor this challenge, g and fi are both neural networks with a single hidden layer composed of ten nodes. There are 16 different subjects available in this dataset, each containing an average of 588 samples, and errors corresponding to experiments holding out each are outlined in Table 1.\nFigure 4 shows our method in action. For visualization purposes, we project dimensionality to two by using t-SNE on the source and target separately before learning φi. Before the transformation, the two datasets together seem chaotic and randomly labeled, but after transforming the target, the datasets appear orderly and easily classifiable."
    }, {
      "heading" : "3.3. Sentiment Classification",
      "text" : "In the sentiment dataset, we are provided with product reviews from Amazon.com (Blitzer et al., 2007). Each review comes from either Amazon’s book, DVD, electronics, or kitchen department, and each of these domains contains\n2,000 samples. Reviews are supplied as term frequencies, which we converted into a TF-IDF representation. We then performed Principal Components Analysis (PCA) on the entire dataset, capturing 95% of its total variance and dramatically reducing its dimensionality.\nIn this experiment g and fi have the same architecture as in the MEG experiment, but with a hundred hidden nodes each instead of ten. As in the previous experiment, Table 2 shows a breakdown of percent errors. Our approach achieves lower classification errors than any of the competing methods used."
    }, {
      "heading" : "3.4. Remote Sensing",
      "text" : "The remote sensing data is created from samples produced when trying to detect landmines. Here, samples are ninedimensional representations of radar signals. This dataset\nhas 29 different domains, where each is affiliated with different geographical conditions. The labels represent whether or not a particular sample corresponds to the existence of a landmine. Each domain contains about 511 samples on average and, unlike in the previous two experiments where classes were balanced, there are about 15 negative samples for every positive sample. More information about this data can be found in (Xue et al., 2007).\nIn this experiment, g and fi both have a single hidden layer with five neurons. Detailed results can be found in 3. In this experiment, our method performs a bit worse on average than TCA."
    }, {
      "heading" : "4. Discussion",
      "text" : "While our technique appears to work well in each of the experiments, it particularly excels, compared to other approaches, on the MEG and sentiment classification tasks. In the MEG experiment, our approach outperforms other classification algorithms in 12 of the 16 domains. It also performs significantly better on average than both g and fi.\nIn the sentiment classification problem, our approach produces hypotheses that are superior to other approaches across all domains.\nThe method is less successful on the landmine dataset. This is probably because KMM and TCA only consider samples from the source and target, rather than using both samples and labels. While this appears to be a disadvantage in the other two experiments, where classes are well-balanced, it seems to be an advantage in this case because there are far more negative samples than positive samples."
    } ],
    "references" : [ {
      "title" : "A plug & play P300 BCI using information geometry",
      "author" : [ "A. Barachant", "M. Congedo" ],
      "venue" : "arXiv preprint arXiv:1409.0107,",
      "citeRegEx" : "Barachant and Congedo,? \\Q2014\\E",
      "shortCiteRegEx" : "Barachant and Congedo",
      "year" : 2014
    }, {
      "title" : "Classification of covariance matrices using a Riemannian-based kernel for BCI applications",
      "author" : [ "A. Barachant", "S. Bonnet", "M. Congedo", "C. Jutten" ],
      "venue" : null,
      "citeRegEx" : "Barachant et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Barachant et al\\.",
      "year" : 2013
    }, {
      "title" : "Instance weighting for domain adaptation in NLP",
      "author" : [ "J. Jiang", "C. Zhai" ],
      "venue" : "In ACL,",
      "citeRegEx" : "Jiang and Zhai,? \\Q2007\\E",
      "shortCiteRegEx" : "Jiang and Zhai",
      "year" : 2007
    }, {
      "title" : "Attitude determination using vector observations and the singular value decomposition",
      "author" : [ "F.L. Markley" ],
      "venue" : "The Journal of the Astronautical Sciences,",
      "citeRegEx" : "Markley,? \\Q1988\\E",
      "shortCiteRegEx" : "Markley",
      "year" : 1988
    }, {
      "title" : "Adaptation based on generalized discrepancy",
      "author" : [ "M. Mohri" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Mohri,? \\Q2015\\E",
      "shortCiteRegEx" : "Mohri",
      "year" : 2015
    }, {
      "title" : "Domain adaptation via transfer component analysis",
      "author" : [ "S.J. Pan", "I.W. Tsang", "J.T. Kwok", "Q. Yang" ],
      "venue" : "Neural Networks, IEEE Transactions on,",
      "citeRegEx" : "Pan et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Pan et al\\.",
      "year" : 2011
    }, {
      "title" : "Visual domain adaptation: A survey of recent advances",
      "author" : [ "V.M. Patel", "R. Gopalan", "R. Li", "R. Chellappa" ],
      "venue" : "Signal Processing Magazine, IEEE,",
      "citeRegEx" : "Patel et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Patel et al\\.",
      "year" : 2015
    }, {
      "title" : "Multitask learning for classification with Dirichlet process priors",
      "author" : [ "Y. Xue", "X. Liao", "L. Carin", "B. Krishnapuram" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Xue et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Xue et al\\.",
      "year" : 2007
    } ],
    "referenceMentions" : [ {
      "referenceID" : 6,
      "context" : ", 2011) and computer vision (Beijbom, 2012; Patel et al., 2015).",
      "startOffset" : 28,
      "endOffset" : 63
    }, {
      "referenceID" : 4,
      "context" : "As described in (Mohri, 2015), domain adaptation has historically been approached in mainly two ways.",
      "startOffset" : 16,
      "endOffset" : 29
    }, {
      "referenceID" : 5,
      "context" : "Transfer Component Analysis (TCA) operates in this way (Pan et al., 2011), attempting to find a set of “transfer components” such that source and target datasets appear to have the same underlying distribution when projected into this new space.",
      "startOffset" : 55,
      "endOffset" : 73
    }, {
      "referenceID" : 3,
      "context" : "For example, to constrain φi to the space of rotation matrices, we have been able to leverage a solution to Wahba’s problem (Markley, 1988) at every iteration of back propagation.",
      "startOffset" : 124,
      "endOffset" : 139
    }, {
      "referenceID" : 5,
      "context" : "The TCA (Pan et al., 2011) algorithm takes a source and target as inputs and returns a canonical feature representation.",
      "startOffset" : 8,
      "endOffset" : 26
    }, {
      "referenceID" : 1,
      "context" : "The features of this data are derived by estimating each subject’s covariance matrix (Barachant & Congedo, 2014) and then computing a vector according to (Barachant et al., 2013).",
      "startOffset" : 154,
      "endOffset" : 178
    }, {
      "referenceID" : 7,
      "context" : "More information about this data can be found in (Xue et al., 2007).",
      "startOffset" : 49,
      "endOffset" : 67
    } ],
    "year" : 2017,
    "abstractText" : "Domain adaptation, and transfer learning more generally, seeks to remedy the problem created when training and testing datasets are generated by different distributions. In this work, we introduce a new unsupervised domain adaptation algorithm for when there are multiple sources available to a learner. Our technique assigns a rough labeling on the target samples, then uses it to learn a transformation that aligns the two datasets before final classification. In this article we give a convenient implementation of our method, show several experiments using it, and compare it to other methods commonly used in the field.",
    "creator" : "LaTeX with hyperref package"
  }
}