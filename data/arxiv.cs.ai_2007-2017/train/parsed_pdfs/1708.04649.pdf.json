{
  "name" : "1708.04649.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Machine Learning for Survival Analysis: A Survey",
    "authors" : [ "PING WANG", "Virginia Tech", "YAN LI", "Ann Arbor", "CHANDAN K. REDDY" ],
    "emails" : [ "ping@vt.edu;" ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 Machine Learning for Survival Analysis: A Survey\nPING WANG, Virginia Tech YAN LI, University of Michigan, Ann Arbor CHANDAN K. REDDY, Virginia Tech\nSurvival analysis is a subfield of statistics where the goal is to analyze and model the data where the outcome is the time until the occurrence of an event of interest. One of the main challenges in this context is the presence of instances whose event outcomes become unobservable after a certain time point or when some instances do not experience any event during the monitoring period. Such a phenomenon is called censoring which can be effectively handled using survival analysis techniques. Traditionally, statistical approaches have been widely developed in the literature to overcome this censoring issue. In addition, many machine learning algorithms are adapted to effectively handle survival data and tackle other challenging problems that arise in real-world data. In this survey, we provide a comprehensive and structured review of the representative statistical methods along with the machine learning techniques used in survival analysis and provide a detailed taxonomy of the existing methods. We also discuss several topics that are closely related to survival analysis and illustrate several successful applications in various real-world application domains. We hope that this paper will provide a more thorough understanding of the recent advances in survival analysis and offer some guidelines on applying these approaches to solve new problems that arise in applications with censored data.\nCCS Concepts: •Mathematics of computing → Survival analysis; •Computing methodologies → Machine learning; •Information systems→ Data mining;\nAdditional Key Words and Phrases: Survival data; censoring; survival analysis; regression; hazard rate; Cox model; Concordance index."
    }, {
      "heading" : "1. INTRODUCTION",
      "text" : "Due to the development of various data acquisition and big data technologies, the ability to collect a wide variety of data and monitor the observation over long-term periods have been attained in different disciplines. For most of the real-world applications, the primary objective of monitoring these observations is to obtain a better estimate of the time of occurrence of a particular event of interest. One of the main challenges for such time-to-event data is that usually there exist censored instances, i.e., the event of interests is not observed for these instances due to either the time limitation of the study period or losing track during the observation period. More precisely, certain instances have experienced event (or labeled as event) and the information about the outcome variable for the remaining instances is only available until a specific time point in the study. Therefore, it is not suitable to directly apply predictive algorithms using the standard statistical and machine learning approaches to analyze the sur-\nThis material is based upon work supported by, or in part by, the U.S. National Science Foundation grants IIS-1707498, IIS-1619028 and IIS-1646881. Author’s addresses: P. Wang is with the Department of Computer Science, Virginia Tech, 900 N. Glebe Road, Arlington, VA, 22203. E-mail: ping@vt.edu; Y. Li is with the Department of Computational Medicine and Bioinformatics, University of Michigan, 100 Washtenaw Avenue, Ann Arbor, MI, 48109. E-mail: yanliwl@umich.edu; C. K. Reddy is with the Department of Computer Science, Virginia Tech, 900 N. Glebe Road, Arlington, VA, 22203. E-mail: reddy@cs.vt.edu. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. c© 2017 ACM. 0360-0300/2017/08-ART1 $15.00 DOI: 0000001.0000001\nXXX, Vol. X, No. X, Article 1, Publication date: August 2017.\nar X\niv :1\n70 8.\n04 64\n9v 1\n[ cs\n.A I]\n1 5\nA ug\n2 01\n7\nvival data. Survival analysis, which is an important subfield of statistics, provides various mechanisms to handle such censored data problems that arise in modeling such complex data (also referred to as time-to-event data when modeling a particular event of interest is the main objective of the problem) which occurs ubiquitously in various real-world application domains.\nIn addition to the difficulty in handling the censored data, there are also several unique challenges to perform the predictive modeling with such survival data and hence several researchers have, more recently, developed new computational algorithms for effectively handling such complex challenges. To tackle such practical concerns, some related works have adapted several machine learning methods to solve the survival analysis problems and machine learning researchers have developed more sophisticated and effective algorithms which either complement or compete with the traditional statistical methods. In spite of the importance of these problems and relevance to various real-world applications, this research topic is scattered across different disciplines. Moreover, there are only few surveys that are available in the literature on this topic and, to the best of our knowledge, there is no comprehensive review paper about survival analysis and its recent developments from a machine learning perspective. Almost all of these existing survey articles describe solely statistical methods and either completely ignore or barely mention the machine learning advancements in this research field. One of the earliest surveys may be found in [Chung et al. 1991], which gives an overview of the statistical survival analysis methods and describes its applications in criminology by predicting the time until recidivism. Most of the existing books about survival analysis [Kleinbaum and Klein 2006; Lee and Wang 2003; Allison 2010] focus on introducing this topic from the traditional statistical perspective instead of explaining from the machine learning standpoint. Recently, the authors in [Cruz and Wishart 2006] and [Kourou et al. 2015] discussed the applications in cancer prediction and provided a comparison of several machine learning techniques.\nThe primary purpose of this survey article is to provide a comprehensive and structured overview of various machine learning methods for survival analysis along with the traditional statistical methods. We demonstrate the commonly used evaluation metrics and advanced related formulations that are commonly investigated in this research topic. We will discuss a detailed taxonomy of all the survival analysis methods that were developed in the traditional statistics as well as more recently in the machine learning community. We will also provide links to various implementations and sources codes which will enable the readers to further dwell into the methods discussed in this article. Finally, we will discuss various applications of survival analysis.\nThe rest of this paper is organized as follows. We will give a brief review of the basic concepts, notations and definitions that are necessary to comprehend the survival analysis algorithms and provide the formal problem statement for survival analysis problem in Section 2. A taxonomy of the existing survival analysis methods, including both statistical and machine learning methods will also be provided to elucidate the holistic view of the existing works in the area of survival analysis. We will then review the well-studied representative conventional statistical methods including nonparametric, semi-parametric, and parametric models in Section 3. Section 4 describes several basic machine learning approaches, including survival trees, Bayesian methods, support vector machines and neural networks developed for survival analysis. Different kinds of advanced machine learning algorithms such as ensemble learning, transfer learning, multi-task learning and active learning for handling survival data will also be discussed. Section 5 demonstrates the evaluation metrics for survival models. In addition to the survival analysis algorithms, some interesting topics related to this topic have received considerable attention in various fields. In Section 6, several related concepts such as early prediction and complex events will be discussed. Var-\nXXX, Vol. X, No. X, Article 1, Publication date: August 2017.\nious data transformation techniques such as uncensoring and calibration which are typically used in conjunction with existing predictive methods will also be mentioned briefly. A discussion about topics in complex event analysis such as competing risks and recurrent events will also be provided. In Section 7, various real-world applications of survival analysis methods will be briefly explained and more insights into these application domains will be provided. In Section 8, the details about the implementations and software packages of the survival analysis methods are discussed. Finally, Section 9 concludes our discussion."
    }, {
      "heading" : "2. DEFINITION OF SURVIVAL ANALYSIS",
      "text" : "In this section, we will first provide the basic notations and terminologies used in this paper. We will then give an illustrative example which explains the structure of the survival data and give a more formal problem statement for survival analysis. At last, we also give a complete taxonomy of the existing survival analysis methods that are available in the literature, including both the conventional statistical methods and the machine learning approaches. It provides a holistic view of the field of survival analysis and will aid the readers to gain the basic knowledge about the methods used in this field before getting into the detailed algorithms."
    }, {
      "heading" : "2.1. Survival Data and Censoring",
      "text" : "During the study of a survival analysis problem, it is possible that the events of interest are not observed for some instances; this scenario occurs because of the limited observation time window or missing traces caused by other uninterested events. This concept is known as censoring [Klein and Moeschberger 2005]. We can broadly categorize censoring into three groups based on the occurrence of the censoring [Lee and Wang 2003], (i) right-censoring, for which the observed survival time is less than or equal to the true survival time; (ii) left-censoring, for which the observed survival time is greater than or equal to the true survival time; and (iii) interval censoring, for which we only know that the event occurs during a given time interval. It should be noted that the true event occurrence time is unknown in all the three cases. Among\nXXX, Vol. X, No. X, Article 1, Publication date: August 2017.\nthem, right-censoring is the most common scenario that arises in many practical problems [Marubini and Valsecchi 2004], thus, the survival data with right-censoring information will be mainly analyzed in this paper.\nFor a survival problem, the time to the event of interest (T ) is known precisely only for those instances who have the event occurred during the study period. For the remaining instances, since we may lose track of them during the observation time or their time to event is greater than the observation time, we can only have the censored time (C) which may be the time of withdrawn, lost or the end of the observation. They are considered to be censored instances in the context of survival analysis. In other words, here, we can only observe either survival time (Ti) or censored time (Ci) but not both, for any given instance i. If and only if yi = min(Ti, Ci) can be observed during the study, the dataset is said to be right-censored. In the survival problem with right-censored instances, the censoring time is also a random variable since the instances enter the study randomly and the randomness in the censoring time of the instances. Thus, in this paper, we assume that the censoring occurs randomly in the survival problems. For the sake of brevity, we will refer to the randomly occurred rightcensoring as censoring hereafter in the paper.\nIn Figure 1, an illustrative example is given for a better understanding of the definition of censoring and the structure of survival data. Six instances are observed in this study for 12 months and the event occurrence information during this time period is recorded. From Figure 1, we can find that only subjects S4 and S6 have experienced the event (marked by ‘X’) during the follow-up time and the observed time for them is the event time. While the event did not occur within the 12 months period for subjects S1, S2, S3 and S5, which are considered to be censored and marked by red dots in the figure. More specifically, subjects S2 and S5 are censored since there was no event occurred during the study period, while subjects S1 and S3 are censored due to the withdrawal or being lost to follow-up within the study time period.\nProblem Statement: For a given instance i, represented by a triplet (Xi, yi, δi), where Xi ∈ R1×P is the feature vector; δi is the binary event indicator, i.e., δi = 1 for an uncensored instance and δi = 0 for a censored instance; and yi denotes the observed time and is equal to the survival time Ti for an uncensored instance and Ci for a censored instance, i.e.,\nyi = { Ti if δi = 1 Ci if δi = 0\n(1)\nIt should be noted that Ti is a latent value for censored instances since these instances did not experience any event during the observation time period.\nXXX, Vol. X, No. X, Article 1, Publication date: August 2017.\nThe goal of survival analysis is to estimate the time to the event of interest Tj for a new instance j with feature predictors denoted by Xj . It should be noted that, in survival analysis problem, the value of Tj will be both non-negative and continuous."
    }, {
      "heading" : "2.2. Survival and Hazard Function",
      "text" : "The survival function, which is used to represent the probability that the time to the event of interest is not earlier than a specified time t [Lee and Wang 2003; Klein and Moeschberger 2005], is one of the primary goals in survival analysis. Conventionally, survival function is represented by S, which is given as follows:\nS(t) = Pr(T ≥ t). (2) The survival function monotonically decreases with t, and the initial value is 1 when t = 0, which represents the fact that, in the beginning of the observation, 100% of the observed subjects survive; in other words, none of the events of interest have occurred.\nOn the contrary, the cumulative death distribution function F (t), which represents the probability that the event of interest occurs earlier than t, is defined as F (t) = 1− S(t), and death density function can be obtained as f(t) = ddtF (t) for continuous cases, and f(t) = [F (t+ ∆t)− F (t)]/∆t, where ∆t denotes a small time interval, for discrete cases. Figure 2 shows the relationship among these functions.\nIn survival analysis, another commonly used function is the hazard function (h(t)), which is also called the force of mortality, the instantaneous death rate or the conditional failure rate [Dunn and Clark 2009]. The hazard function does not indicate the chance or probability of the event of interest, but instead it is the rate of event at time t given that no event occurred before time t. Mathematically, the hazard function is defined as:\nh(t) = lim ∆t→0 Pr(t ≤ T < t+ ∆t | T ≥ t) ∆t = lim ∆t→0 F (t+ ∆t)− F (t) ∆t · S(t) = f(t) S(t) (3)\nSimilar to S(t), h(t) is also a non-negative function. While all the survival functions, S(t), decrease over time, the hazard function can have a variety of shapes. Consider the definition of f(t), which can also be expressed as f(t) = − ddtS(t), so the hazard function can be represented as:\nh(t) = f(t) S(t) = − d dt S(t) · 1 S(t) = − d dt [lnS(t)]. (4)\nXXX, Vol. X, No. X, Article 1, Publication date: August 2017.\nThus, the survival function defined in Eq. (2) can be rewritten as\nS(t) = exp(−H(t)) (5) where H(t) = ∫ t\n0 h(u)du represents the cumulative hazard function (CHF) [Lee and\nWang 2003]."
    }, {
      "heading" : "2.3. Taxonomy of Survival Analysis methods",
      "text" : "Broadly speaking, the survival analysis methods can be classified into two main categories: statistical methods and machine learning based methods. Statistical methods share the common goal with machine learning methods to make predictions of the survival time and estimate the survival probability at the estimated survival time. However, they focus more on characterizing both the distributions of the event times and the statistical properties of the parameter estimation by estimating the survival curves, while machine learning methods focus more on the prediction of event occurrence at a given time point by incorporating the traditional survival analysis methods with various machine learning techniques. Machine learning methods are usually applied to the high-dimensional problems, while statistical methods are generally developed for the low-dimensional data. In addition, machine learning methods for survival analysis offer more effective algorithms by incorporating survival problems with both statistical methods and machine learning methods and taking advantages of the recent developments in machine learning and optimization to learn the dependencies between covariates and survival times in different ways.\nBased on the assumptions and the usage of the parameters used in the model, the traditional statistical methods can be subdivided into three categories: (i) nonparametric models, (ii) semi-parametric models and (iii) parametric models. Machine learning algorithms, such as survival trees, Bayesian methods, neural networks and support vector machines, which have become more popular in the recent years are included under a separate branch. Several advanced machine learning methods, including ensemble learning, active learning, transfer learning and multi-task learning methods, are also included. The overall taxonomy also includes some of the research topics that are related to survival analysis such as complex events, data transformation and early prediction. A complete taxonomy of these survival analysis methods is shown in Figure 3."
    }, {
      "heading" : "3. TRADITIONAL STATISTICAL METHODS",
      "text" : "In this section, we will introduce three different types of statistical methods to estimate the survival/hazard functions: non-parametric, semi-parametric and parametric methods. Table II shows both the advantages and disadvantages of each type of methods based on theoretical and experimental analysis and lists the specific methods in each type.\nNon-parametric methods are more efficient when there is no underlying distribution for the event time or the proportional hazard assumption does not hold. In nonparametric methods, an empirical estimate of the survival function is obtained using Kaplan-Meier (KM) method, Nelson-Aalen estimator (NA) or Life-Table (LT) method. More generally, any KM estimator for the survival probability at the specified survival time is a product of the same estimate up to the previous time and the observed survival rate for that given time. Thus, KM method is also referred to as a product-limit method [Kaplan and Meier 1958; Lee and Wang 2003]. NA method is an estimator based on modern counting process techniques [Andersen et al. 2012]. LT [Cutler and Ederer 1958] is the application of the KM method to the interval grouped survival data.\nXXX, Vol. X, No. X, Article 1, Publication date: August 2017.\nXXX, Vol. X, No. X, Article 1, Publication date: August 2017.\nUnder the semi-parametric category, Cox model is the most commonly used regression analysis approach for survival data and it differs significantly from other methods since it is built on the proportional hazards assumption and employs partial likelihood for parameter estimation. Cox regression method is described as semi-parametric method since the distribution of the outcome remains unknown even if it is based on a parametric regression model. In addition, several useful variants of the basic Cox model, such as penalized Cox models, CoxBoost algorithm and Time-Dependent Cox model (TD-Cox), are also proposed in the literature.\nParametric methods are more efficient and accurate for estimation when the time to event of interest follows a particular distribution specified in terms of certain parameters. It is relatively easy to estimate the times to the event of interest with parametric models, but it becomes awkward or even impossible to do so with the Cox model [Allison 2010]. Linear regression method is one of the main parametric survival methods, while the Tobit model, Buckley-James regression model and the penalized regression are the most commonly used linear models for survival analysis. In addition, other parametric models, such as accelerated failure time (AFT) which models the survival time as a function of covariates [Kleinbaum and Klein 2006], are also widely used. We will now describe these three types of statistical survival methods in this section."
    }, {
      "heading" : "3.1. Non-parametric Models",
      "text" : "Among all functions, the survival function or its graphical presentation is the most widely used one. In 1958, Kaplan and Meier [Kaplan and Meier 1958] developed the Kaplan-Meier (KM) Curve or the product-limit (PL) estimator to estimate the survival function using the actual length of the observed time. This method is the most widely used one for estimating survival function. Let T1 < T2 < · · · < TK be a set of distinct ordered event times observed forN(K ≤ N) instances. In addition to these event times, there are also censoring times for instances whose event times are not observed. For a specific event time Tj (j = 1, 2, · · ·,K), the number of observed events is dj ≥ 1, and rj instances will be considered to be “at risk” since their event time or censored time is greater than or equal to Tj . It should be noted that we cannot simply consider rj as the difference between rj−1 and dj−1 due to the censoring. The correct way to obtain rj is rj = rj−1 − dj−1 − cj−1, where cj−1 is the number of censored instances during the time period between Tj−1 and Tj . Then the conditional probability of surviving beyond time Tj can be defined as:\np(Tj) = rj − dj rj\n(6)\nBased on this conditional probability, the product-limit estimate of survival function S(t) = P (T ≥ t) is given as follows:\nŜ(t) = ∏\nj:Tj<t\np(Tj) = ∏\nj:Tj<t\n(1− dj rj ) (7)\nHowever, if the subjects in the data are grouped into some interval periods according to the time, or if the number of subjects is very large, or when the study is for a large population, the Life Table (LT) analysis [Cutler and Ederer 1958] will be a more convenient method. Different from KM and LT method, Nelson-Aalen estimator [Nelson 1972; Aalen 1978] is a method to estimate the cumulative hazard function for censored data based on counting process approach. It should be noted that when the time to event of interest follows a specific distribution, nonparametric methods are less efficient compared to the parametric methods."
    }, {
      "heading" : "3.2. Semi-Parametric Models",
      "text" : "As a hybrid of the parametric and non-parametric approaches, semi-parametric models can obtain a more consistent estimator under a broader range of conditions compared to the parametric models, and a more precise estimator than the non-parametric methods [Powell 1994]. Cox model [David 1972] is the most commonly used survival analysis method in this category. Unlike parametric methods, the knowledge of the underlying distribution of time to event of interest is not required, but the attributes are assumed to have an exponential influence on the outcome. We will now discuss the details of Cox model more elaborately and then describe different variants and extensions of the basic Cox model such as regularized Cox models, CoxBoost and TimeDependent Cox.\n3.2.1. The Basic Cox Model. For a given instance i, represented by a triplet (Xi, yi, δi), the hazard function h(t,Xi) in the Cox model follows the proportional hazards assumption given by\nh(t,Xi) = h0(t)exp(Xiβ), (8)\nfor i = 1, 2, · · · , N , where the baseline hazard function, h0(t), can be an arbitrary nonnegative function of time, Xi = (xi1, xi2, · · · , xiP ) is the corresponding covariate vector for instance i, and βT = (β1, β2, · · · , βP ) is the coefficient vector. The Cox model is a semi-parametric algorithm since the baseline hazard function, h0(t), is unspecified. For any two instances X1 and X2, the hazard ratio is given by\nh(t,X1) h(t,X2) = h0(t)exp(X1β) h0(t)exp(X2β) = exp[(X1 −X2)β]. (9)\nwhich means that the hazard ratio is independent of the baseline hazard function. Cox model is a proportional hazards model since the hazard ratio is a constant and all the subjects share the same baseline hazard function. Based on this assumption, the survival function can be computed as follows:\nS(t) = exp(−H0(t)exp(Xβ)) = S0(t)exp(Xβ) (10)\nwhere H0(t) is the cumulative baseline hazard function, and S0(t) = exp(−H0(t)) represents the baseline survival function. The Breslow’s estimator [Breslow 1972] is the most widely used method to estimate H0(t), which is given by\nĤ0(t) = ∑ ti≤t ĥ0(ti) (11)\nXXX, Vol. X, No. X, Article 1, Publication date: August 2017.\nwhere ĥ0(ti) = 1/ ∑ j∈Ri e\nXjβ if ti is an event time, otherwise ĥ0(ti) = 0. Here, Ri represents the set of subjects who are at risk at time ti.\nBecause the baseline hazard function h0(t) in Cox model is not specified, it is not possible to fit the model using the standard likelihood function. In other words, the hazard function h0(t) is a nuisance function, while the coefficients β are the parameters of interest in the model. To estimate the coefficients, Cox proposed a partial likelihood [David 1972; David 1975] which depends only on the parameter of interest β and is free of the nuisance parameters. The hazard function refers to the probability that an instance with covariate X fails at time t on the condition that it survives until time t can be expressed by h(t,X)dt with dt → 0. Let J (J ≤ N) be the total number of events of interest that occurred during the observation period for N instances, and T1 < T2 < · · · < TJ is the distinct ordered time to event of interest. Without considering the ties, let Xj be the corresponding covariate vector for the subject who fails at Tj , and Rj be the set of risk subjects at Tj . Thus, conditional on the fact that the event occurs at Tj , the individual probability corresponding to covariate Xj can be formulated as follows:\nh(Tj , Xj)dt∑ i∈Rj h(Tj , Xi)dt\n(12)\nand the partial likelihood is the product of the probability of each subject; referring to the Cox assumption and the presence of the censoring, the partial likelihood is defined as follows:\nL(β) = N∏ j=1 [ exp(Xjβ)∑ i∈Rj exp(Xiβ) ]δj (13)\nIt should be noted that here j = 1, 2, · · · , N ; if δj = 1, the jth term in the product is the conditional probability; otherwise, when δj = 0, the corresponding term is 1, which means that the term will not have any effect on the final product. The coefficient vector β̂ is estimated by maximizing this partial likelihood, or equivalently, minimizing the negative log-partial likelihood for improving efficiency.\nLL(β) = − N∑ j=1 δj{Xjβ − log[ ∑ i∈Rj exp(Xiβ)]}. (14)\nThe maximum partial likelihood estimator (MPLE) [David 1972; Lee and Wang 2003] can be used along with the numerical Newton-Raphson method [Kelley 1999] to iteratively find an estimator β̂ which minimizes LL(β) with time complexity O(NP 2).\n3.2.2. Regularized Cox models. With the development of data collection and detection techniques, most real-world domains tend to encounter high-dimensional data. In some cases, the number of variables (P ) in the given data is almost equal to or even exceeds the number of instances (N ). It is challenging to build the prediction model with all the features and the model might provide inaccurate results because of the overfitting problem [van Houwelingen and Putter 2011]. This motivates using sparsity norms to select vital features in high-dimension under the assumption that most of the features are not significant [Friedman et al. 2001]. For the purpose of identifying the most relevant features to the outcome variable among tens of thousands of features, different penalty functions, including lasso, group lasso, fused lasso and graph lasso, are also used to develop the prediction models using the sparse learning methods. The family of `-norm penalty functions `γ : Rγ → R, with the form of\nXXX, Vol. X, No. X, Article 1, Publication date: August 2017.\n`γ(β) =‖ β ‖γ= ( ∑P i=1 ‖βi‖γ) 1 γ , γ > 0 are the commonly used penalty functions. The smaller the value of γ, the sparser the solution, but when 0 ≤ γ < 1, the penalty is non-convex, which makes the optimization problem more challenging to solve. Here, we will introduce the commonly used regularized Cox models, whose regularizers are summarized in Table III.\nLasso-Cox: Lasso [Tibshirani 1996] is a `1-norm regularizer which is good at performing feature selection and estimating the regression coefficients simultaneously. In [Tibshirani 1997], the `1-norm penalty was incorporated into the log-partial likelihood shown in Eq. (14) to obtain the Lasso-Cox algorithm, which inherits the properties of `1-norm in feature selection.\nThere are also some extensions of Lasso-Cox method. Adaptive Lasso-Cox [Zhang and Lu 2007] is based on a penalized partial likelihood with adaptively weighted `1 penalties λ ∑P j=1 τj |βj | on regression coefficients, with small weights τj for large coefficients and large weights for small coefficients. In fused Lasso-Cox [Tibshirani et al. 2005], the coefficients and their successive differences are penalized using the `1-norm. In graphical Lasso-Cox [Friedman et al. 2008], the sparse graphs are estimated using coordinate descent method by applying a `1-penalty to the inverse covariance matrix. These extensions solve the survival problems in a similar way as the regular Lasso-Cox model by incorporating different `1 penalties.\nRidge-Cox: Ridge regression was originally proposed by Hoerl and Kennard [Hoerl and Kennard 1970] and was successfully used in the context of Cox regression by Verweij et al. [Verweij and Van Houwelingen 1994]. It incorporates a `2-norm regularizer to select the correlated features and shrink their values towards each other.\nFeature-based regularized Cox method (FEAR-Cox) [Vinzamuri and Reddy 2013] uses feature-based non-negative valued regularizer R(β) = |β|TM |β| for the modified least squares formulation of Cox regression and the cyclic coordinate descent method is used to solve this optimization problem, where M ∈ RP×P (P is the number of features) is a positive semi-definite matrix. Ridge-Cox is a special case of FEAR-Cox when M is the identity matrix.\nEN-Cox: Elastic net (EN), which combines the `1 and squared `2 penalties, has the potential to perform the feature selection and deal with the correlation between the features simultaneously [Zou and Hastie 2005]. The EN-Cox method was proposed by Noah Simon et al. [Simon et al. 2011] where the Elastic Net penalty term shown in Table III with 0 ≤ α ≤ 1 and introduced into the log-partial likelihood function in Eq. (14). Different from Lasso-Cox, EN-Cox can select more than N features if N ≤ P .\nKernel Elastic Net (KEN) algorithm [Vinzamuri and Reddy 2013], which uses the concept of kernels, compensates for the drawbacks of the EN-Cox which is partially effective at dealing with the correlated features in survival data. In KEN-Cox, it\nXXX, Vol. X, No. X, Article 1, Publication date: August 2017.\nbuilds a kernel similarity matrix for the feature space in order to incorporate the pairwise feature similarity into the Cox model. The regularizer used in KEN-Cox is defined as λα||β||1 + λ(1 − α)βTKβ, where K is a symmetric kernel matrix with Kij = exp(− ‖ xi − xj ‖22/2σ2)(i, j = 1, · · · , P ) as its entries. We can see that the equation for KEN-Cox method includes both smooth and non-smooth `1 terms.\nOSCAR-Cox: The modified graph Octagonal Shrinkage and Clustering Algorithm for Regression (OSCAR) [Yang et al. 2012; Ye and Liu 2012] regularizer is incorporated in the basic Cox model as the OSCAR-Cox algorithm [Vinzamuri and Reddy 2013], which can perform the variable selection for highly correlated features in regression problem. The main advantage of OSCAR regularizer is that it tends to have equal coefficients for the features which relate to the outcome in similar ways. In addition, it can simultaneously obtain the advantages of the individual sparsity because of the `1 norm and the group sparsity due to the `∞ norm. The regularizer used in the formulation of the OSCAR-Cox is given in Table III, where T is the sparse symmetric edge set matrix generated by building a graph structure which considers each feature as an individual node. By using this way, a pairwise feature regularizer can be incorporated into the basic Cox regression framework.\nAmong the regularizers shown in Table III, the parameters λ ≥ 0 can be tuned to adjust the influence introduced by the regularizer term. The performance of these penalized estimators significantly depend on λ, and the optimal λopt can be chosen via cross-validation. The time complexity of both Lasso-Cox and EN-Cox method is O(NP ).\n3.2.3. CoxBoost. While there exists several algorithms (such as the penalized parameter estimation) which can be applied to fit the sparse survival models on the highdimensional data, none of them are applicable in the situation that some mandatory covariates should be taken into consideration explicitly in the models. CoxBoost [Binder and Schumacher 2008] approach is proposed to incorporate the mandatory covariates into the final model. The CoxBoost method also aims at estimating the coefficients β in Eq. (8) as in the Cox model. It considers a flexible set of candidate variables for updating in each boosting step by employing the offset-based gradient boosting approach. This is the key difference from the regular gradient boosting approach, which either updates only one component of β in component-wise boosting or fits the gradient by using all covariates in each step.\n3.2.4. Time-dependent (TD) Cox Model. Cox regression model is also effectively adapted to handle time-dependent covariates, which refer to the variables whose values may change with time t for a given instance. Typically, the time-dependent variable can be classified into three categories [Kleinbaum and Klein 2006]: internal time-dependent variable, ancillary time-dependent variable and defined time-dependent variable. The reason for a change in the internal time-dependent variable depends on the internal characteristics or behavior that is specific to the individual. In contrast, a variable is called an ancillary time-dependent variable if its value changes primarily due to the environment that may affect several individuals simultaneously. Defined variable, with the form of the product of a time-independent variable multiplied by a function of time, is used to analyze a time independent predictor not satisfying the PH assumption in the Cox model. The commonly used layout of the dataset in time-dependent Cox model is in the form of counting process (CP) [Kleinbaum and Klein 2006].\nGiven a survival analysis problem which involves both time-dependent and time-independent features, we can denote the variables at time t as X(t) = (X·1(t), X·2(t), ..., X·P1(t), X·1, X·2, ..., X·P2), where P1 and P2 represent the number of time-dependent and time-independent variables, respectively. And X·j(t) and X·i rep-\nXXX, Vol. X, No. X, Article 1, Publication date: August 2017.\nresent the jth time-dependent feature and the ith time-independent feature, respectively. Then, by involving the time-dependent features into the basic Cox model given in Eq. (8), the time-dependent Cox model can be formulated as:\nh(t,X(t)) = h0(t)exp [ P1∑ j=1 δjX·j(t) + P2∑ i=1 βiX·i ] (15)\nwhere δj and βi represent the coefficients corresponding to the jth time-dependent variable and the ith time-independent variable, respectively. For the two sets of predictors at time t: X(t) = (X·1(t), X·2(t), ..., X·P1(t), X·1, X·2, ..., X·P2) and X∗(t) = (X∗·1(t), X ∗ ·2(t), ..., X ∗ ·P1(t), X ∗ ·1, X ∗ ·2, ..., X ∗ ·P2), the hazard ratio for the time-dependent Cox model can be computed as follows:\nĤR(t) = ĥ(t,X∗(t))\nĥ(t,X(t)) = exp [ P1∑ j=1 δj [X ∗ ·j(t)−X·j(t)] + P2∑ i=1 βi[X ∗ ·i −X·i] ] (16)\nSince the first component in the exponent of Eq. (16) is time-dependent, we can consider the hazard ratio in the TD-Cox model as a function of time t. This means that it does not satisfy the PH assumption mentioned in the standard Cox model. It should be noted that the coefficient δj is in itself not time-dependent and it represents the overall effect of the jth time-dependent variable at various survival time points. The likelihood function of time-dependent Cox model can be constructed in the same manner and optimized with the same time complexity as done in the Cox model."
    }, {
      "heading" : "3.3. Parametric Models",
      "text" : "The parametric censored regression models assume that the survival times or the logarithm of the survival times of all instances in the data follow a particular theoretical distribution [Lee and Wang 2003]. These models are important alternatives to the Cox-based semi-parametric models and are also widely used in many application domains. It is simple, efficient and effective in predicting the time to event of interest using parametric methods. The parametric survival models tend to obtain the survival estimates that are consistent with a theoretical survival distribution. The commonly used distributions in parametric censored regression models are: normal, exponential, weibull, logistic, log-logistic and log-normal. If the survival times of all instances in the data follow these distributions, the model is referred as linear regression model. If the logarithm of the survival times of all instances follow these distributions, the problem can be analyzed using the accelerated failure time model, in which we assume that the variable can affect the time to the event of interest of an instance by some constant factor [Lee and Wang 2003]. It should be noted that if no suitable theoretical distribution is known, nonparametric methods are more efficient.\nThe maximum-likelihood estimation (MLE) method [Lee and Wang 2003] can be used to estimate the parameters for these models. Let us assume that the number of instances is N with c censored observations and (N − c) uncensored observations, and use β = (β1, β2, · · · , βP )T as a general notation to denote the set of all parameters [Li et al. 2016e]. Then the death density function f(t) and the survival function S(t) of the survival time can be represented as f(t, β) and S(t, β), respectively. For a given instance i, if it is censored, the actual survival time will not be available. However, we can conclude that the instance i did not experience the event of interest before the censoring time Ci, so the value of the survival function S(Ci, β) will be a probability closed to 1. In contrast, if the event occurs for instance i at Ti, then the death density function f(Ti, β) will have a high probability value. Thus, we can denote\n∏ δi=1 f(Ti, β)\nXXX, Vol. X, No. X, Article 1, Publication date: August 2017.\nTable IV: Density, Survival and Hazard functions for the distributions commonly used in the parametric methods in survival analysis.\nDistribution PDF f(t) Survival S(t) Hazard h(t)\nExponential λexp(−λt) exp(−λt) λ\nWeibull λktk−1exp(−λtk) exp(−λtk) λktk−1\nLogistic e −(t−µ)/σ σ(1+e−(t−µ)/σ)2 e−(t−µ)/σ 1+e−(t−µ)/σ 1 σ(1+e−(t−µ)/σ)\nLog-logistic λkt k−1\n(1+λtk)2 1 1+λtk λktk−1 1+λtk\nNormal 1√ 2πσ\nexp(− (t−µ) 2 2σ2 ) 1− Φ( t−µ) σ ) 1√ 2πσ(1−Φ((t−µ)/σ)) exp(− (t−µ) 2 2σ2 )\nLog-normal 1√ 2πσt\nexp(− (log(t)−µ) 2 2σ2 ) 1− Φ( log(t)−µ σ )\n1√ 2πσt exp(−(log(t)−µ)2/2σ2)\n1−Φ( log(t)−µ σ ) as the joint probability of all the uncensored observations and ∏ δi=0 S(Ti, β) to represent the joint probability of the c censored observations [Li et al. 2016e]. Therefore, we can estimate the parameters β by optimizing the likelihood function of all N instances in the form of\nL(β) = ∏ δi=1 f(Ti, β) ∏ δi=0 S(Ti, β) (17)\nTable IV shows the death density function f(t) and its corresponding survival function S(t) and hazard function h(t) for these commonly used distributions. Now we will discuss more details about these distributions.\nExponential Distribution: Among the parametric models in survival analysis, exponential model is the simplest and prominent one since it is characterized by a constant hazard rate, λ, which is the only parameter. In this case, the failure or the death is assumed to be a random event independent of time. A larger value of λ indicates a higher risk and a shorter survival time period. Based on the survival function shown in Table IV, we can have logS(t) = −λt , in which the relationship between the logarithm of survival function and time t is linear with λ as the slope. Thus, it is easy to determine whether the time follows an exponential distribution by plotting logŜ(t) against time t [Lee and Wang 2003].\nWeibull Distribution: The Weibull model, which is characterized by two parameters λ > 0 and k > 0, is the most widely used parametric distribution for survival problem. The shape of the hazard function is determined using the shape parameter k, which provides more flexibility compared to the exponential model. If k = 1, the hazard will be a constant, and in this case, the Weibull model will become an exponential model. If k < 1, the hazard function will be decreasing over time. The scaling of the hazard function is determined by the scaling parameter λ.\nLogistic and Log-logistic Distribution: In contrast to Weibull model, the hazard functions of both logistic and log-logistic models allow for non-monotonic behavior in the hazard function, which is shown in Table IV. The survival time T and the logarithm of survival time log(T ) will follow the logistic distribution in logistic and log-logistic models, respectively. For logistic model, µ is the parameter to determine the location of the function, while σ is the scale parameter. For log-logistic model, the parameter k > 0 is the shape parameter. If k ≤ 1, the hazard function is decreasing over time. However, if k > 1, the hazard function will increase over time to the maximum value\nXXX, Vol. X, No. X, Article 1, Publication date: August 2017.\nfirst and then decrease, which means that the hazard function is unimodal if k > 1. Thus, the log-logistic distribution may be used to describe a monotonically decreasing hazard or a first increasing and then decreasing hazard [Lee and Wang 2003].\nNormal and Log-normal Distribution: If the survival time T satisfies the condition that T or log(T ) is normally distributed with mean µ and variance σ2, then T is normally or log-normally distributed. This is suitable for the survival patterns with an initially increasing and then decreasing hazard rate.\nBased on the framework given in Eq. (17), we will discuss these commonly used parametric methods.\n3.3.1. Linear regression models. In data analysis, the linear regression model, together with the least squares estimation method, is one of the most commonly used approach. We cannot apply it directly to solve survival analysis problems since the actual event times are missing for censored instances. Some linear models [Miller and Halpern 1982; Koul et al. 1981; Buckley and James 1979; Wang et al. 2008; Li et al. 2016e] including Tobit regression and Buckley-James (BJ) regression were proposed to handle censored instances in survival analysis. Strictly speaking, linear regression is a specific parametric censored regression, however, this method is fundamental in data analysis, and hence we discuss the linear regression methods for censored data separately here.\nTobit Regression: The Tobit model [Tobin 1958] is one of the earliest attempts to extend linear regression with the Gaussian distribution for data analysis with censored observations. In this model, a latent variable y∗ is introduced and the assumption made here is that it linearly depends on X via the parameter β as y∗ = Xβ+ , ∼ N(0, σ2), where is a normally distributed error term. Then, for the ith instance, the observable variable yi will be y∗i if y∗i > 0, otherwise it will be 0. This means that if the latent variable is above zero, the observed variable equals to the latent variable and zero otherwise. Based on the latent variable, the parameters in the model can be estimated with maximum likelihood estimation (MLE) method with time complexity O(NP 2).\nBuckley-James Regression: The Buckley-James (BJ) regression [Buckley and James 1979] estimates the survival time of the censored instances as the response value based on Kaplan-Meier (KM) estimation method, and then fits a linear (AFT) model by considering the survival times of uncensored instances and the approximated survival times of the censored instances at the same time. To handle high-dimensional survival data, Wang et al. [Wang et al. 2008] applied the elastic net regularizer in the BJ regression (EN-BJ).\nPenalized Regression: Penalized regression methods [Kyung et al. 2010] are wellknown for their nice properties of simultaneous variable selection and coefficient estimation. The penalized regression method can provide better prediction results in the presence of either multi-collinearity of the covariates or high-dimensionality. Recently, these methods have received a great attention in survival analysis. The weighted linear regression model with different regularizers for high-dimensional censored data is an efficient method to handle the censored data by giving different weights to different instances [Li et al. 2016e]. In addition, the structured regularization based linear regression algorithm [Bach et al. 2012; Vinzamuri et al. 2017] for right censored data has a good ability to infer the underlying structure of the survival data.\n— Weighted Regression: Weighted regression method [Li et al. 2016b] can be used when the constant variance assumption about the errors in the ordinary least squares regression methods is violated (which is called heteroscedasticity), which is\nXXX, Vol. X, No. X, Article 1, Publication date: August 2017.\ndifferent from the constant variance in the errors (which is called homoscedasticity) in ordinary least squares regression methods. Instead of minimizing the residual sum of squares, the weighted regression method minimizes the weighted sum of squares∑n i=1 wi(yi −Xiβ)2. The ordinary least squares is a special case of this where all the weights wi = 1. Weighted regression method can be solved in the same manner as the ordinary linear least squares problem with the time complexity of O(NP ). In addition, using the weighted regression method, we can assign higher weights to the instances that we want to emphasize or ones where mistakes are especially costly. If we give the samples high weights, the model will be pulled towards matching the data. This will be very helpful for survival analysis to put more emphasis on the instances whose information may contribute more to the model.\n— Structured Regularization: The ability to effectively infer latent knowledge through tree-based hierarchies and graph-based relationships is extremely crucial in survival analysis. This is also supported by the effectiveness of structured sparsity based regularization methods in regression [Bach et al. 2012]. Structured regularization based LInear REgression algorithm for right Censored data (SLIREC) in [Vinzamuri et al. 2017] infers the underlying structure of the survival data directly using sparse inverse covariance estimation (SICE) method and uses the structural knowledge to guide the base linear regression model. The structured approach is more robust compared to the standard statistical and Cox based methods since it can automatically adapt to different distributions of events and censored instances.\n3.3.2. Accelerated Failure Time (AFT) Model. In the parametric censored regression methods discussed previously, we assume that the survival time of all instances in the given data follows a specific distribution and that the relationship between either the survival time or the logarithm of the survival time and the features is linear. Specially, if the relationship between the logarithm of survival time T and the covariates is linear in nature, it is also termed as Accelerated failure time (AFT) model [Kalbfleisch and Prentice 2011]. Thus, we consider these regression methods as the generalized linear models.\nIn the AFT model, it assumes that the relationship of the logarithm of survival time T and the covariates is linear and can be written in the following form.\nln(T ) = Xβ + σ (18)\nwhere X is the covariate matrix, β represents the coefficient vector, σ(σ > 0) denotes an unknown scale parameter, and is an error variable which follows a similar distribution to ln(T ). Typically, we make a parametric assumption on which can follow any of the distributions given in Table IV. In this case, the survival is dependent on both the covariate and the underlying distribution. Then, the only distinction of an AFT model compared to regular linear methods would be the inclusion of censored information in the survival analysis problem. The AFT model is additive with respect to ln(T ), while multiplicative with respect to T , and is written in the form of T = eXβeσ .\nThus, AFT model assumes that the features have the multiplicative effect on the survival time. In order to demonstrate the basic function of this assumption on a specific feature, let us compare the survival functions S1(t) and S2(t) for two groups, where the instances in the same group have the same values on this feature but different values if they belong to different groups. Then, the assumption of AFT model is in the form of\nS2(t) = S1(γt) (19)\nwhere t ≥ 0 and γ represents a constant which is named as an acceleration factor for comparison of the survival time of the two groups. For the linear regression method, we can parameterize γ as exp(α), where α can be estimated using the given data.\nXXX, Vol. X, No. X, Article 1, Publication date: August 2017.\nThen, the assumption in AFT method will be updated to S2(t) = S1(exp(α)t). The acceleration factor which is the key measure of the relationships in the AFT method is used to evaluate the effect of features on the survival time. The time complexity of AFT models is also O(NP 2)."
    }, {
      "heading" : "4. MACHINE LEARNING METHODS",
      "text" : "In the past several years, due to the advantages of machine learning techniques, such as its ability to model the non-linear relationships and the quality of their overall predictions made, they have achieved significant success in various practical domains. In survival analysis, the main challenge of machine learning methods is the difficulty to appropriately deal with censored information and the time estimation of the model. Machine learning is effective when there are a large number of instances in a reasonable dimensional feature space, but this is not the case for certain problems in survival analysis [Zupan et al. 2000]. In this section, we will do a comprehensive review of commonly used machine learning methods in survival analysis."
    }, {
      "heading" : "4.1. Survival Trees",
      "text" : "Survival trees are one form of classification and regression trees which are tailored to handle censored data. The basic intuition behind the tree models is to recursively partition the data based on a particular splitting criterion, and the objects that are similar to each other based on the event of interest will be placed in the same node. The earliest attempt at using a tree structure for survival data was made in [Ciampi et al. 1981]. However, [Gordon and Olshen 1985] is the first paper which discussed the creation of survival trees.\nThe primary difference between a survival tree and the standard decision tree is in the choice of splitting criterion. The decision tree method performs recursive partitioning on the data by setting a threshold for each feature, however, it can neither consider the interactions between the features nor the censored information in the model [Safavian and Landgrebe 1991]. The splitting criteria used for survival trees can be grouped into two categories: (i) maximizing between-node heterogeneity and (ii) minimizing within-node homogeneity. The first class of approaches minimizes the loss function using the within-node homogeneity criterion. The authors in [Gordon and Olshen 1985] measured the homogeneity and Hellinger distances between the estimated distribution functions using the Wasserstein metric . An exponential loglikelihood function was employed in [Davis and Anderson 1989] for recursive partitioning based on the sum of residuals from the Cox model. Leblanc and Crowley [LeBlanc and Crowley 1992] measured the node deviance based on the first step of a full likelihood estimation procedure. In the second class of splitting criteria, Ciampi et al. [Ciampi et al. 1986] employed log-rank test statistics for between-node heterogeneity measures. Later, Ciampi et al. [Ciampi et al. 1987] proposed a likelihood ratio statistic to measure the dissimilarity between two nodes. Based on the Tarone-Ware class of two-sample statistics, Segal [Segal 1988] introduced a procedure to measure the between-node dissimilarity. The main improvement of a survival tree over the standard decision tree is its ability to handle the censored data using the tree structure.\nAnother important aspect of building a survival tree is the selection of the final tree. Procedures such as backward selection or forward selection can be followed for choosing the optimal tree [Bou-Hamad et al. 2011]. However, an ensemble of trees (described in Section 4.5) can avoid the problem of final tree selection with better performance compared to a single tree.\nXXX, Vol. X, No. X, Article 1, Publication date: August 2017."
    }, {
      "heading" : "4.2. Bayesian Methods",
      "text" : "Bayes theorem is one of the most fundamental principles in probability theory and mathematical statistics; it provides a link between the posterior probability and the prior probability, so that one can see the changes in probability values before and after accounting for a certain event. Using the Bayes theorem, there are two models, namely, Naı̈ve Bayes (NB) and Bayesian network (BN) [Friedman et al. 1997]. Both of these approaches, which provide the probability of the event of interests as their outputs, are commonly studied in the context of clinical prediction [Kononenko 1993; Pepe 2003; Zupan et al. 2000]. The experimental results of using Bayesian methods on survival data show that Bayesian methods have good properties of both interpretability and uncertainty reasoning [Raftery et al. 1995].\nNaı̈ve Bayes, a well-known probabilistic method in machine learning, is one of the simplest yet effective prediction algorithms. In [Bellazzi and Zupan 2008], the authors build a naı̈ve Bayesian classifier to make predictions in clinical medicine by estimating various probabilities from the data. Recently, the authors in [Fard et al. 2016] effectively integrate Bayesian methods with an AFT model by extrapolating the prior event probability to implement early stage prediction on survival data for the future time points. One drawback of Naı̈ve Bayes method is that it makes the independence assumption between all the features, which may not be true for many problems in survival analysis.\nA Bayesian network, in which the features can be related to each other at various levels, can graphically represent a theoretical distribution over a set of variables. Bayesian networks can visually represent all the relationships between the variables which makes it interpretable for the end user. It can acquire knowledge information by using procedures of estimating the network structures and parameters from a given dataset. In [Lisboa et al. 2003], the authors proposed a Bayesian neural network framework to perform model selection for survival data using automatic relevance determination [MacKay 1995]. In [Raftery 1995], a Bayesian model averaging for Cox proportional hazards models is proposed and also used to evaluate the Bayes factors in the problem. More recently, in [Fard et al. 2016], the authors proposed a novel framework which combines the power of Bayesian network representation with the AFT model by extrapolating the prior probabilities to future time points. The time complexity of these Bayesian approaches mainly depends on the types of Bayesian method used in the models."
    }, {
      "heading" : "4.3. Artificial Neural Networks",
      "text" : "Inspired by biological neural systems, in 1958, Frank Rosenblatt published the first paper [Rosenblatt 1958] about artificial neural network (ANN). In this approach, the simple artificial nodes denoted by “neurons” are connected based on a weighted link to form a network which simulates a biological neural network. A neuron in this context is a computing element which consists of sets of adaptive weights and generates the output based on a certain kind of activation function. Artificial neural network (ANN) has been widely used in survival analysis. Three kinds of methods are proposed in the literature which employ the neural network method to solve the survival analysis problems.\n(1) The neural network survival analysis has been employed to predict the survival time of a subject directly from the given inputs. (2) The authors in [Faraggi and Simon 1995] extended the Cox PH model to the nonlinear ANN predictor and suggested to fit the neural network which has a linear output layer and a single logistic hidden layer. The authors in [Mariani et al. 1997] used both the standard Cox model and the neural network method proposed\nXXX, Vol. X, No. X, Article 1, Publication date: August 2017.\nin [Faraggi and Simon 1995] to assess the prognostic factors for the recurrence of breast cancer. Although these extensions for Cox model allowed for preserving most of the advantages of a typical PH model, they were still not the optimal way to model the baseline variation [Baesens et al. 2005]. (3) Many approaches [Liestbl et al. 1994; Biganzoli et al. 1998; Brown et al. 1997; Ravdin and Clark 1992; Lisboa et al. 2003] take the survival status of a subject, which can be represented by the survival or hazard probability, as the output of the neural network. The authors in [Biganzoli et al. 1998] apply the partial logistic artificial neural network (PLANN) method to analyze the relationship between the features and the survival times in order to obtain a better predictability of the model. Recently, feed-forward neural networks are used to obtain a more flexible non-linear model by considering the censored information in the data using a generalization of both continuous and discrete time models [Biganzoli et al. 1998]. In [Lisboa et al. 2003], the PLANN was extended to a Bayesian neural framework with covariate-specific regularization to carry model selection using automatic relevance determination [MacKay 1995]."
    }, {
      "heading" : "4.4. Support Vector Machines",
      "text" : "Support Vector Machines (SVM), a very successful supervised learning approach, is used mostly for classification and can also be modified for regression problems [Smola and Schölkopf 2004]. It has also been successfully adapted to in survival analysis problems.\nA naive way is to consider only those instances which have events in support vector regression (SVR), in which the -insensitive loss function, f(Xi) = max(0, |f(Xi)−yi|− ), is minimized with a regularizer [Smola and Schölkopf 1998]. However, the main disadvantage of this approach is that the order information included in the censored instances will be completely ignored [Shivaswamy et al. 2007]. Another possible approach to handle the censored data is to use support vector classification using the constraint classification approach [Har-Peled et al. 2002] which imposes constraints in the SVM formulation for two comparable instances in order to maintain the required order. However, the computational complexity for this algorithm is quadratic with respect to the number of instances. In addition, it only focuses on the ordering among the instances, and ignores the actual values of the output.\nThe authors in [Khan and Zubek 2008] proposed support vector regression for censored data (SVRc), which takes advantage of the standard SVR and also adapts it for censored cases by using an updated asymmetric loss function. In this case, it considers both the uncensored and censored instances in the model. The work in [Van et al. 2007] studies a learning machine designed for predictive modeling of independently right censored survival data by introducing a health index which serves as a proxy between the instance’s covariates and the outcome. The authors in [Van et al. 2011] introduces a SVR based approach which combines the ranking and regression methods in the context of survival analysis. In average, the time complexity of these methods is O(N3) which follows the time complexity in the standard SVM.\nRelevance Vector Machine (RVM) [Widodo and Yang 2011; Kiaee et al. 2016], which obtains the parsimonious estimations for regression and probabilistic problems using Bayesian inference, has the same formulation as SVM but provides probabilistic classification. RVM adopts a Bayesian approach by considering the prior over the weights controlled by some parameters. Each of these parameters corresponds to a weight, the most probable value of which can be estimated iteratively using the data. The Bayesian representation of the RVM can avoid these parameters in SVM (the optimization methods based on cross-validation are usually used.). However, it is possible that RVMs converge to the local minimum since EM algorithm is used to learn the\nXXX, Vol. X, No. X, Article 1, Publication date: August 2017.\nparameters. This is different from the regular sequential minimal optimization (SMO) algorithm used in SVM, which can guarantee the convergence to a global minimum."
    }, {
      "heading" : "4.5. Advanced Machine Learning Approaches",
      "text" : "Over the past few years, more advanced machine learning methods have been developed to deal with and predict from censored data. These methods have various unique advantages on survival data compared to the other methods described so far.\n4.5.1. Ensemble Learning. Ensemble learning methods [Dietterich 2000] generate a committee of classifiers and then predict the class labels for the new coming data points by taking a weighted vote among the prediction results from all these classifiers. It is often possible to construct good ensembles and obtain a better approximation of the unknown function by varying the initial points, especially in the presence of insufficient data. To overcome the instability of a single method, bagging [Breiman 1996] and random forests [Breiman 2001], proposed by Breiman, are commonly used to perform the ensemble based model building. Such ensemble models have been successfully adapted to survival analysis whose time complexity mainly follows that of the base-learners.\nBagging Survival Trees: Bagging is one of the oldest and most commonly used ensemble method which typically reduces the variance of the base models that are used. In bagging survival trees, the aggregated survival function can be calculated by averaging the predictions made by a single survival tree instead of taking a majority vote [Hothorn et al. 2004]. There are mainly three steps in this method: (i) Draw B booststrap samples from the given data. (ii) For each bootstrap sample, build a survival tree and ensure that, for all the terminal nodes, the number of events is greater than or equal to the threshold d. (iii) By averaging the leaf nodes’ predictions, calculate the bootstrap aggregated survival function. For each leaf node the survival function is estimated using the KM estimator, and all the individuals within the same node are assumed to have the same survival function.\nRandom Survival Forests: Random forest is an ensemble method specifically proposed to make predictions using the tree structured models [Breiman 2001]. It is based on a framework similar to Bagging; the main difference between random forest and bagging is that, at a certain node, rather than using all the attributes, random forest only uses a random subset of the residual attributes to select the attributes based on the splitting criterion. It is shown that randomization can reduce the correlation among the trees and thus improve the prediction performance.\nRandom survival forest (RSF) [Ishwaran et al. 2008] extended Breiman’s random forest method by using a forest of survival trees for prediction. There are mainly four steps in RSF: (i) Draw B bootstrap samples randomly from the given dataset. This is also called out-of-bag (OOB) data because around 37% of the data is excluded in each sample. (ii) For each sample, build a survival tree by randomly selecting features and split the node using the candidate feature which can maximize the survival difference between the child nodes. (iii) Build the tree to the full size with a constraint that the terminal node has greater than or equal to a specific unique deaths. (iv) Using the non-parametric Nelson-Aalen estimator, calculate the ensemble cumulative hazard function (CHF) of OOB data by taking the average of the CHF of each tree. In addition, the authors in [Ishwaran et al. 2011] provide an effective way to apply RSF for highdimensional survival analysis problems by regularizing forests.\nBoosting: Boosting algorithm is one of the widely used ensemble methods designed to combine base learners into a weighted sum that represents the final output of the strong learner. It iteratively fits the appropriately defined residuals based on the gra-\nXXX, Vol. X, No. X, Article 1, Publication date: August 2017.\ndient descent algorithm [Hothorn et al. 2006; Bühlmann and Hothorn 2007]. The authors in [Hothorn et al. 2006] extend the gradient boosting algorithm to minimize the\nweighted risk function β̂Ũ,X = arg minβ N∑ i=1 wi(Ũi − h(Xi|β)), where Ũ is a pseudoresponse variable with Ũi = −∂L(yi,φ)∂φ |φ=f̂m(Xi); β is a vector of parameters; h(·|βU,X) is the prediction made by regressing U using a base learner. Then the steps to optimize this problem are as follows: (i) Initialize Ũi = yi (i = 1, · · · , N), m = 0 and f̂0(·|β̂Ũ,X); fix the number of iterations M(M > 1). (ii) Fit h(·|β̂U,X) after updating residuals Ũi (i = 1, · · · , N). (iii) Iteratively update f̂m+1(·) = f̂m(·) + vh(·|β̂U,X), where 0 < v ≤ 1 represents the step size. (iv) Repeat the procedures in steps (ii) and (iii) until m = M .\n4.5.2. Active Learning. Active learning based on the data with censored observations can be very helpful for survival analysis since the opinions of an expert in the domain can be incorporated into the models. Active learning mechanism allows the survival model to select a subset of subjects by learning from a limited set of labeled subjects first and then query the expert to get the label of survival status before considering it in the training set. The feedback from the expert is particularly useful for improving the model in many real-world application domains [Vinzamuri et al. 2014]. The goal of active learning for survival analysis problems is to build a survival regression model by utilizing the censored instances completely without deleting or modifying the instance. In [Vinzamuri et al. 2014], the active regularized Cox regression (ARC) algorithm based on a discriminative gradient sampling strategy is proposed by integrating the active learning method with the Cox model. The ARC framework is an iteration based algorithm with three main steps: (i) Build a regularized Cox regression using the training data, (ii) Apply the model obtained in (i) to all the instances in the unlabeled pool, (iii) Update the training data and the unlabeled pool, select the instance whose influence on the model is the highest and label it before running the next iteration. One of the main advantages of the ARC framework is that it can identify the instances and get the feedback about event labeling from the domain expert. The time complexity of the ARC algorithm is O(NPK), where K represents the number of unique time points in the survival problem.\n4.5.3. Transfer Learning. Collecting labeled information in survival problems is very time consuming, i.e., one has to wait for the event occurrence from a sufficient number of training instances to build robust models. A naive solution for this insufficient data problem is to merely integrate the data from related tasks into a consolidated form and build prediction models on such integrated data. However, such approaches often do not perform well because the target task (for which the predictions are to be made) will be overwhelmed by auxiliary data with different distributions. In such scenarios, knowledge transfer between related tasks will usually produce much better results compared to a data integration approach. Transfer learning method has been extensively studied to solve standard regression and classification problems [Pan and Yang 2010]. Recently, in [Li et al. 2016c], a regularized Cox PH model named Transfer-Cox, is proposed to improve the prediction performance of the Cox model in the target domain through knowledge transfer from the source domain in the context of survival models built on multiple high-dimensional datasets. The Transfer-Cox model employs `2,1-norm to penalize the sum of the loss functions (negative partial log-likelihood) for both source and target domains. Thus, the model, with time complexity O(NP ), will not only select important features but will also learn a shared representation across source and target domains to improve the model performance on the target task.\nXXX, Vol. X, No. X, Article 1, Publication date: August 2017.\n4.5.4. Multi-task Learning. In [Li et al. 2016d], the survival time prediction problem is reformulated as a multi-task learning problem. In survival data, the outcome labeling matrix is incomplete since the event label of each censored instance is unavailable after its corresponding censoring time; therefore, it is not suitable to handle the censored information using the standard multi-task learning methods. To solve this problem, the multi-task learning model for survival analysis (MTLSA) translates the original event labels into a N × K indicator matrix I, where K = max(yi) (∀i = 1, · · · , N) is the maximum follow-up time of all the instances in the dataset. The element Iij (i = 1, · · · , N ; j = 1, · · · ,K) of the indicator matrix will be 1 if the event occurred before time yj for instance i, otherwise it will be 0. One of the primary advantages of the MTLSA approach is that it can capture the dependency between the outcomes at various time points by using a shared representation across the related tasks in the transformation, which will reduce the prediction error on each task. In addition, the model can simultaneously learn from both uncensored and censored instances based on the indicator matrix. One important characteristic of non-recurring events, i.e., once the event occurs it will not occur again, is encoded via the non-negative non-increasing list structure constraint. In the MTLSA algorithm, the `2,1-norm penalty is employed to learn a shared representation, with time complexity O(NPK), across related tasks and hence compute the relatedness between the individual models built for various unique event time points."
    }, {
      "heading" : "5. PERFORMANCE EVALUATION METRICS",
      "text" : "Due to the presence of the censoring in survival data, the standard evaluation metrics for regression such as root of mean squared error andR2 are not suitable for measuring the performance in survival analysis [Heagerty and Zheng 2005]. Instead, the prediction performance in survival analysis needs to be measured using more specialized evaluation metrics."
    }, {
      "heading" : "5.1. C-index",
      "text" : "In survival analysis, a common way to evaluate a model is to consider the relative risk of an event for different instance instead of the absolute survival times for each instance. This can be done by computing the concordance probability or the concordance index (C-index) [Harrell et al. 1984; Harrell et al. 1982; Pencina and D’Agostino 2004]. The survival times of two instances can be ordered for two scenarios: (1) both of them are uncensored; (2) the observed event time of the uncensored instance is smaller than the censoring time of the censored instance [Steck et al. 2008]. This can be visualized by the ordered graph given in Figure 4. Figure 4(a) and Figure 4(b) are used to illustrate the possible ranking comparisons (denoted by edges between instances) for the survival data without and with censored instances, respectively. There are ( 5 2 ) = 10 possible pairwise comparisons for the five instances in the survival data without censored cases shown in Figure 4(a). Due to the presence of censored instances (represented by red circles) in Figure 4(b), only 6 out of the 10 comparisons are feasible. It should be noted that, for a censored instance, only an earlier uncensored instance (for example y2&y1) can be compared with. However, any censored instance cannot be compared with both censored and uncensored instances after its censored time (for example, y2&y3 and y2&y4) since its actual event time is unknown.\nConsider both the observations and prediction values of two instances, (y1, ŷ1) and (y2, ŷ2), where yi and ŷi represent the actual observation time and the predicted value, respectively. The concordance probability between them can be computed as\nc = Pr(ŷ1 > ŷ2|y1 ≥ y2) (20)\nXXX, Vol. X, No. X, Article 1, Publication date: August 2017.\nBy this definition, for the binary prediction problem, C-index will have a similar meaning to the regular area under the ROC curve (AUC), and if yi is binary, then the C-index is the AUC [Li et al. 2016d]. As the definition above is not straightforward, in practice, there are multiple ways of calculating the C-index.\n(1) When the output of the model is a hazard ratio (such as the outcome obtained by Cox based models), C-index can be computed using\nĉ = 1\nnum ∑ i:δi=1 ∑ j:yi<yj I[Xiβ̂ > Xj β̂] (21)\nwhere i, j ∈ {1, · · · , N}, num denotes the number of all comparable pairs, I[·] is the indicator function and β̂ is the estimated parameters from the Cox based models. (2) For the survival methods which aim at directly learning the survival time, the C-index should be calculated as:\nĉ = 1\nnum ∑ i:δi=1 ∑ j:yi<yj I[S(ŷj |Xj) > S(ŷi|Xi)] (22)\nwhere S(·) is the estimated survival probabilities.\nIn order to evaluate the performance during a follow-up period, Heagerty and Zheng defined the C-index for a fixed follow-up time period (0, t∗) as the weighted average of AUC values at all possible observation time points [Heagerty and Zheng 2005]. The time-dependent AUC for any specific survival time t can be calculated as\nAUC(t) = P (ŷi < ŷj |yi < t, yj > t) = 1\nnum(t) ∑ i:yi<t ∑ j:yj>t I(ŷi < ŷj) (23)\nwhere t ∈ Ts which is the set of all possible survival times and num(t) represents the number of comparable pairs for the time point t. Then the C-index during the time period (0, t∗), which is the weighted average of the time-dependent AUC obtained by Eq. (23), is computed as\nct∗ = 1\nnum ∑ i:δi=1 ∑ j:yi<yj I(ŷi < ŷj) = ∑ t∈Ts AUC(t) · num(t) num\n(24)\nThus ct∗ is the probability that the predictions are concordant with their outcomes for a given data during the time period (0, t∗).\nXXX, Vol. X, No. X, Article 1, Publication date: August 2017."
    }, {
      "heading" : "5.2. Brier Score",
      "text" : "Named after the inventor Glenn W. Brier, the Brier score (BS) [Brier 1950] is developed to predict the inaccuracy of probabilistic weather forecasts. It can only evaluate the prediction models which have probabilistic outcomes; that is, the outcome must remain within the range [0,1], and the sum of all the possible outcomes for a certain individual should be 1. When we consider the binary outcome prediction with a sample of N instances and for each Xi (i = 1, 2, ..., N), the predicted outcome at t is ŷi(t), and the actual outcome is yi(t); then, the empirical definition of the Brier score at the specific time t can be given by\nBS(t) = 1\nN N∑ i=1 [ŷi(t)− yi(t)]2 (25)\nwhere the actual outcome yi(t) for each instance can only be 1 or 0. Brier score was extended in [Graf et al. 1999] to be a performance measure for survival problems with censored information to evaluate the prediction models where the outcome to be predicted is either binary or categorical in nature. When incorporating the censoring information in the dataset, the individual contributions to the empirical Brier score are reweighted according to the censored information. Then, the Brier score can be updated as follows:\nBS(t) = 1\nN N∑ i=1 wi(t)[ŷi(t)− yi(t)]2 (26)\nIn Eq.(26), wi(t), given in Eq. (27), denotes the weight for the ith instance and it is estimated by incorporating the Kaplan-Meier estimator of the censoring distribution G obtained on the given dataset (Xi, yi, 1− δi), i = 1, · · · , N .\nwi(t) = { δi/G(yi) if yi ≤ t 1/G(yi) if yi > t\n(27)\nWith this weight distribution, the weights for the instances that are censored before t will be 0. However, they contribute indirectly to the calculation of the Brier score since they are used for calculating G. The weights for the instances that are uncensored at t are greater than 1, so that they contribute their estimated survival probability to the calculation of the Brier score."
    }, {
      "heading" : "5.3. Mean Absolute Error",
      "text" : "For survival analysis problems, the mean absolute error (MAE) can be defined as an average of the differences between the predicted time values and the actual observation time values. It is calculated as follows:\nMAE = 1\nN N∑ i=1 (δi|yi − ŷi|) (28)\nwhere yi (i = 1, · · · , N) represents the actual observation times, and ŷi (i = 1, · · · , N) denotes the predicted times. It should be noted that only the samples for which the event occurs are being considered in this metric since if δi = 0, the corresponding term will become zero. MAE can only be used for the evaluation of survival models which can provide the event time as the predicted target value such as AFT models."
    }, {
      "heading" : "6. RELATED TOPICS",
      "text" : "Besides the machine learning methods introduced in Section 4 and the traditional statistical survival methods discussed in Section 3, there are few other topics that are closely related to survival analysis and we will summarize them now.\nXXX, Vol. X, No. X, Article 1, Publication date: August 2017."
    }, {
      "heading" : "6.1. Early Prediction",
      "text" : "One of the primary challenges in the context of survival analysis, and in general longitudinal studies, is that a sufficient number of events in the training data can be collected only by waiting for a long period. This is the most significant difference from the regular supervised learning problems, in which the labels for each instance can be given by a domain expert in a reasonable time period. Therefore, a good survival model should have the ability to forecast the event occurrence at future time by using only a limited event occurrence information at the early stage of a survival analysis problem.\nThere are many real-world applications which motivate the need for new prediction models which can work using only the data collected at the early stage of the studies. For example, in the healthcare domain, it is critical to study the effect of a new treatment in order to understand the treatment or drug efficacy, which should be estimated as early as possible. In this case, the patients will be monitored over a certain time period and the event of interest will be the patient admission to the hospital due to the treatment failure. This scenario clearly indicates the need for algorithms which can predict the event occurrence effectively using only a few events.\nTo solve this problem, an Early Stage Prediction (ESP) approach trained at early stages of survival analysis studies to predict the time-to-event is proposed in [Fard et al. 2016]. Two algorithms based on Naı̈ve Bayes and Bayesian Networks are developed by estimating the posterior probability of event occurrence based on different extrapolation techniques using Weibull, Log-logistic and Log-normal distributions discussed in Section 3. The ESP framework is a two-stage algorithm: (1) Estimate the conditional probability distribution based on the training data collected until the early stage time point (tc) of the study; (2) Extrapolating the prior probability of the event for the future time (tf ) using AFT model with different distributions. According to the experimental results in these works, the ESP framework can provide more accurate predictions when the prior probability at the future time is appropriately estimated using the current information of event occurrence."
    }, {
      "heading" : "6.2. Data Transformation",
      "text" : "In this section, we will discuss two data transformation techniques that will be useful for data pre-processing in survival analysis. Both of these approaches transform the data to a more conducive form so that other survival-based (or sometimes even the standard algorithms) can be applied effectively.\n6.2.1. Uncensoring approach. In survival data, the incompleteness in the event (outcome) information makes it difficult for standard machine learning methods to learn from such data. The censored observations in survival data might look similar to unlabeled samples in classification or unknown response in regression problem in the sense that status or time-to-event is not known for some of the observations. However, different from unlabeled samples where the labeling information is completely missing, the censored instances actually have partial informative labeling information which provides the possible range of the corresponding true response (survival time). Such censored data have to be handled with special care within any machine learning method in order to make good predictions. Also, in survival analysis problems, only the information before a certain time point (before censoring occurs) is available for the censored instances and this information should be integrated into the prediction algorithm to obtain the most optimal result.\nTypically, there are two naive ways of handling such censored data. One is to delete the censored instances, and it performs well if the number of the samples are large enough and the censoring instances are not censored randomly. However, it will provide a sub-optimal model because of neglecting the available information in those cen-\nXXX, Vol. X, No. X, Article 1, Publication date: August 2017.\nsored instances [Delen et al. 2005; Burke et al. 1997]. Treating censoring as event-free is another naive and simple choice. This method performs well for data with only a few censored instances, but it underestimates the true performance of the model. Although these methods are simple for handling the censored data, they loose useful information available in the data. Here, we list two other approaches proposed in the literature to handle censored data.\n(1) Group the instances in the given data into three categorizes [Zupan et al. 2000]: (i) instances which experience the event of interest during the observation will be labeled as event; (ii) instances whose censored time is later than a predefined time point are labeled as event-free; (iii) for instances whose censored time is earlier than a predefined time point, a copy of these instances will be labeled as event and another copy of the same instances will be labeled as event-free, respectively, and all these instances will be weighted by a marginal probability of event occurrence estimated by the Kaplan-Meier method. (2) For each censored instance, estimate the probability of event and probability of being censored (considering censoring as a new event) using Kaplan-Meier estimator and give a new class label based on these probability values [Fard et al. 2016]. For each instance in the data, when the probability of event exceeds the probability of being censored, then it is labeled as event; otherwise, it will be labeled as event-free which indicates that even if there is complete follow-up information for that instance, there is extremely low chance of event occurrence by the end of the observation time period.\n6.2.2. Calibration. Censoring causes missing time-to-event labels, and this effect is compounded when dealing with datasets which have high amounts of censored instances. Instead of using the uncensoring approach, calibration methods for survival analysis can also be used to solve this problem by learning more optimal time-to-event labels for the censored instances. Generally, there are mainly two reasons which motivate calibration. First, the survival analysis model is built using the given dataset where the missing time-to-events for the censored instances are assigned to a value such as the duration of the study or last known follow up time. However, this approach is not suitable for handling data with many censored instances. In other words, for such data, these inappropriately labeled censored instances cannot provide much information to the survival algorithm. Calibration method can be used to overcome this missing time-to-events problem in survival analysis. Secondly, dependent censoring in the data, where censoring is dependent on the covariates, may lead to some bias in standard survival estimators, such as KM method. This motivates an imputed censoring approach which calibrates the time-to-event attribute to decrease the bias of the survival estimators.\nIn [Vinzamuri et al. 2017], a calibration survival analysis method which uses a regularized inverse covariance based imputation is proposed to overcome the problems mentioned above. It has the ability to capture correlations between censored instances and correlations between similar features. In calibrated survival analysis, through imputing an appropriate label value for each censored instance, a new representation of the original survival data can be learned effectively. This approach fills the gap in the current literature by estimating the calibrated time-to-event values for these censored instances by exploiting row-wise and column-wise correlations among censored instances in order to effectively impute them."
    }, {
      "heading" : "6.3. Complex Events",
      "text" : "Until now, the discussion in this paper has been primarily focused on survival problems in which each instance can experience only a single event of interest. However, in many\nXXX, Vol. X, No. X, Article 1, Publication date: August 2017.\nreal-world domains, each instance may experience different types of events and each event may occur more than once during the observation time period. For example, in the healthcare domain, one patient may be hospitalized multiple times due to different medical conditions. Since this scenario is more complex than the survival problems we discussed before, we consider them to be complex events. In this section, we will discuss two techniques, namely, competing risks and recurrent events, to tackle such complex events.\n6.3.1. Competing Risks. In the survival problem, if several different types of events are considered, but only one of them can occur for each instance over the follow-up period, then the competing risks will be defined as the probabilities of different events. In other words, the competing risks will only exist in survival problems with more than one possible event of interest, but only one event will occur at any given time. For example, in healthcare domain, a patient may have both heart attack and lung cancer before his death, but the reason of his death can be either lung cancer or heart attack, but not both. In this case, competing risks are the events that prevent an event of interest from occurring which is different from censoring. It should be noted that in the case of censoring, the event of interest still occurs at a later time, while the event of interest is impeded.\nTo solve this problem, the standard way is to analyze each of these events separately using the survival analysis approach by considering other competing events as censored [Kleinbaum and Klein 2006]. However, there are two primary drawbacks with such an approach. One problem is that this method assumes that the competing risks are independent of each other. In addition, it would be difficult to interpret the survival probability estimated for each event separately by performing the survival analysis for each event of interest in the competing risks.\nTo overcome these drawbacks, two methods are developed in the survival analysis literature: Cumulative Incidence Curve (CIC) Approach and Lunn-McNeil (LM) Approach.\nCumulative Incidence Curve (CIC) Approach: To avoid the questionable interpretation problem, the cumulative incidence curve [Putter et al. 2007] is one of the main approaches for competing risks which estimates the marginal probability of each event q. The CIC is defined as\nCICq(t) = ∑ j:tj≤t Ŝ(tj−1)ĥq(tj) = ∑ j:tj≤t Ŝ(tj−1) nqj nj\n(29)\nwhere ĥq(tj) represents the estimated hazard at time tj for event q (q = 1, · · · , Q), nqj is the number of events for the event q at tj , nj denotes the number of instances who are at the risk of experiencing events at tj , and S(tj−1) denotes the survival probability at last time point tj−1.\nLunn-McNeil (LM) Approach [Lunn and McNeil 1995]: It is an alternative approach to analyze the competing risks in the survival problems and it also allows the flexibility to conduct statistical inference from the features in the competing risk models. It fits a single Cox PH model which considers all the events in competing risks rather than separate models for each event [Kleinbaum and Klein 2006]. It should be noted that the LM approach is implemented using an augmented data, in which a dummy variable is created for each event to distinguish different competing risks.\n6.3.2. Recurrent Events. In many application domains, the event of interest in survival problems may occur several times during the observation time period. This is significantly different from the death of the patients in healthcare domain. In such cases, the\nXXX, Vol. X, No. X, Article 1, Publication date: August 2017.\noutcome event can occur for each instance more than once during the observation time period. In survival analysis, we refer to such events which occur more than once as recurrent events, which contrasts with the competing risks discussed above. Typically, if all the recurring events for each instance are of the same type, the counting process (CP) algorithm [Andersen et al. 2012] can be used to tackle this problem. If there are different types of events or the order of the events is the main goal, other methods using stratified Cox (SC) approaches can be used [Ata and Sözer 2007]. These methods include stratified CP, Gap Time and Marginal approach. These approaches differ not only in the way they determine the risk set but also in the data format.\nCounting Process: In Counting Process method, the data processing procedure is as follows: (i) For each instance, identify the time interval for each recurrent event and add one record to the data. It should be noted that an additional record for the eventfree time interval should also be included for each instance. (ii) For each instance, each record of data should be labeled by a starting time and ending time of the corresponding time interval. These properties of the data format distinguish the counting process method from other methods. They are significantly different from the regular survival data format for non-recurrent event problems, which provides only the ending time and contain only one record for each instance in the dataset.\nThe key idea to analyze the survival data with recurrent events is to treat the different time intervals for each instance as independent records from different instances. The basic Cox model is used to perform the counting process approach. Each instance will not be removed from the risk set until the last time interval during the observation period. In other words, for the survival problem with recurrent events, the partial likelihood function formula is different from that in the non-recurrent event survival problems [Kleinbaum and Klein 2006].\nStratified Cox: Stratified CP [Prentice et al. 1981], Marginal [Wei et al. 1989] and Gap Time [Prentice et al. 1981] are three approaches using stratified Cox method to differentiate the event occurrence order. (1) In Stratified CP approach, the data format is exactly the same as that used in the CP approach, and the risk set for the future events is affected by the time of the first event. (2) In Marginal approach, it uses the same data format as the non-recurrent event survival data. This method considers the length of the survival time from the starting time of the follow-up until the time of a specific event occurrence and it assumes that each event is independent of other events. For the kth event (k = 1, 2, · · · ) in this method, the risk set contains those instances which are at the risk of experience the corresponding event after their entry into the observation. (3) In the Gap Time approach, the data format (start, stop) is used, but the starting time for each data record is 0 and the ending time is the length of the interval from the previous experienced event. In this method, the risk set for the future events will not be affected by the time of the first event."
    }, {
      "heading" : "7. APPLICATION DOMAINS",
      "text" : "In this section, we will demonstrate the applications of survival analysis in various real-world domains. Table V summarizes the events of interest, expected goal and the features that are typically used in each specific application described in this section."
    }, {
      "heading" : "7.1. Healthcare",
      "text" : "In the healthcare domain, the starting point of the observation is usually a particular medical intervention such as a hospitalization admission, the beginning of taking a certain medication or a diagnosis of a given disease [Klein and Moeschberger 2005; Miller Jr 2011]. The event of interest might be death, hospital readmission, discharge from the hospitalization or any other interesting incident that can happen during the\nXXX, Vol. X, No. X, Article 1, Publication date: August 2017.\nobservation period. The missing trace of the observation is also an important characteristic of the data collected in this domain. For example, during a given hospitalization, some patients may be moved to another hospital and in such cases, that patient\nXXX, Vol. X, No. X, Article 1, Publication date: August 2017.\nwill become unobserved from the study with respect to the first hospital after that time point. In healthcare applications, survival prediction models primarily aim at estimating the failure time distribution and the prognostic evaluation of different features, including histological, biochemical and clinical characteristics [Marubini and Valsecchi 2004]."
    }, {
      "heading" : "7.2. Reliability",
      "text" : "In the field of reliability, it is a common practice to collect data over a period of time and record the interesting events that occur within this period. Reliability prediction focuses on developing methods which are good at accurately estimating the reliability of the new products [Modarres et al. 2009; Lyu 1996]. The event of interest here corresponds to the time taken for a device to fail. In such applications, it is desirable to be able to estimate which devices will fail and if they do, when they will fail. Survival analysis methods can help in building such prediction models using the available information about these devices. These models can provide early warnings about potential failures, which is significantly important to either prevent or reduce the likelihood of failures and to identify and correct the causes of device failures."
    }, {
      "heading" : "7.3. Crowdfunding",
      "text" : "In recent years, the topic of crowdfunding has gained a lot of attention. Although the crowdfunding platforms have been successful, the percentage of the projects which achieved their desired goal amount is less than 50% [Rakesh et al. 2015]. Moreover, many of the prominent crowdfunding platforms follow the “all-or-nothing” policy. In other words, if the goal is achieved before the pre-determined time period, the pledged funding can be collected. Therefore, in the crowdfunding domain, one of the most important challenges is to estimate the success probability of each project. The need to estimate the project success probability motivates the development of new prediction approaches which can integrate the advantages of both regression (for estimating the time for success) and classification (for considering both successful and failed projects simultaneously in the model) [Li et al. 2016a]. For the successful projects, the time to the success can be collected easily. However, for the projects that failed, it is not possible to collect the information about the length of the time for project success. The only information that can be collected is the funding amount that they raised until the pre-determined project end date. The authors in [Li et al. 2016a] consider both the failed and successful projects simultaneously by using censored regression methods. It fits the probability of project success with log-logistic and logistic distributions and predicts the time taken for a project to become potentially successful."
    }, {
      "heading" : "7.4. Bioinformatics",
      "text" : "One of the most popular applications of survival analysis in the domain of bioinformatics is gene expression. Gene expression is the process of synthesizing a functional gene product from the gene information and can be quantified by measuring either message RNA (mRNA) or proteins. Gene expression profiling is developed as a powerful technique to study the cell transcriptome. In recent years, multiple studies [Li et al. 2016d; Beer et al. 2002] have correlated gene expression with survival outcomes in cancer applications in a genome-wide scale. Survival analysis methods are helpful in assessing the effect of single gene on survival prognosis and then identifying the most relevant genes as biomarkers for patients. In this scenario, the event of interest is the specific type of cancer (or any disease), and the goal is to estimate the likelihood of cancer using the gene expression measurements values. Generally, the survival prediction based on gene expression data is a high-dimensional problem since each cell contains tens of thousands of mRNA molecules. The authors in [Antonov et al. 2014] developed\nXXX, Vol. X, No. X, Article 1, Publication date: August 2017.\na statistical tool for biomedical researchers to define the clinical relevance of genes under investigation via their effect on the patient survival outcome. Survival analysis methods have shown to be effective in predicting the gene expression for different cancer data with the censored information."
    }, {
      "heading" : "7.5. Student Retention",
      "text" : "In higher education, student retention rate can be evaluated by the percentage of students who return to the same university for the following semester after completing a semester of study. In the U.S. and around the world, one of the long-term goals of a university is to improve the student retention. Higher the student retention rate, more probable for the university to be positioned higher, secure more government funds, and have an easier path to program accreditations. In view of these reasons, directors and administrators in higher education constantly try to implement new strategies to increase student retention. Survival analysis has success in student retention problem [Murtaugh et al. 1999; Ameri et al. 2016]. The goal of survival analysis is to estimate the time of event occurrence, which is critical in student retention problems because both correctly identifying whether a student will dropout and estimating when the dropout will happen are important. In such cases, it will be helpful if one can reliably estimate the dropout risk at the early stage of student education using both pre-enrollment and post-enrollment information."
    }, {
      "heading" : "7.6. Customer Lifetime Value",
      "text" : "Customer lifetime value (LTV) [Berger and Nasr 1998; Zeithaml et al. 2001] of a customer refers to the profit that the customer brings to the store based on the purchase history. In the marketing domain, the customer LTV is used to evaluate the relationships between the customers and the store. It is important for a store to improve the LTV in order to maintain or increase its profits in the long term since it is often quite expensive to acquire new customers. In this case, the main goal of this problem is to identify purchase patterns of the customers who have a high LTV and provide recommendations for a relatively new user who has similar interest. Identifying loyal customers using LTV estimation has been studied by various researchers [Rosset et al. 2003; Mani et al. 1999] using survival analysis methods and data mining approaches which are helpful in identifying the purchase patterns. Then, LTV will be defined using a survival function, which can be used to estimate the time of purchase for every customer from a given store, using the store information and also the available customer demographic information in the store database, such as the gender, income and age."
    }, {
      "heading" : "7.7. Click-Through Rate",
      "text" : "Nowadays, many free web services, including online news portals, search engines and social networks present users with advertisements [Barbieri et al. 2016]. Both the topics and the display orders of the ads will affect the user clicking probability [Richardson et al. 2007]. Studies have mostly focused on predicting the click-through rate (CTR) which indicates the percentage of the users who click on a given ad. It can be calculated as the ratio of the clicking times and the corresponding presentation times (no. of ad impressions). The CTR value indicates the attraction effect of the ad to the users [Barbieri et al. 2016]. The goal is to predict how likely the user will click the ads based on the available information about the website, users and ads. The time taken to click the ad is considered to be the event time. Those users who did not click on the ads are considered to be censored observations.\nXXX, Vol. X, No. X, Article 1, Publication date: August 2017."
    }, {
      "heading" : "7.8. Duration Modeling in Economics",
      "text" : "Traditionally, duration data which measures how long individuals remain in a certain state is analyzed in biometrics and medical statistics using survival analysis methods. Actually, duration data also appears in a wide variety of situations in economics such as unemployment duration, marital instability and time-to-transaction in the stock market. Among them, the unemployment duration problem, which is the most widely studied one, analyzes the time people spend without a job [Gamerman and West 1987]. Generally, in the domain of economics, the time of being unemployed is extremely important since the length of unemployment of people plays a critical role in economics theories of job search [Kiefer 1988]. For this problem, the data contains information on the time duration of unemployment for each individual in the sample. The event of interest here is getting a new job for each person and the objective is to predict the likelihood of getting a new job within a specific time period. It is desirable to understand how the re-employment probability changes over the period of the spell and to know more about the effect of the unemployment benefits on these probabilities."
    }, {
      "heading" : "8. RESOURCES",
      "text" : "This section provides a list of software implementations developed in various statistical methods and machine-learning algorithms for survival analysis. Table VI summarizes the basic information of the software packages for each survival method. We can find that most of the existing survival analysis methods can be implemented in R.\n(1) Non-parametric methods: All the three non-parametric survival analysis methods can be implemented by employing the function coxph and survfit in the survival package in R. (2) Semi-parametric methods: Both the basic Cox model and the time-dependent Cox model can be trained using coxph function in the survival package in R. The LassoCox, Ridge-Cox and EN-Cox in the regularized Cox methods can be trained using the cocktail function in the fastcox package. The RegCox package can be used to implement OSCAR-Cox method. The CoxBoost function in the CoxBoost package can fit a Cox model by likelihood based boosting algorithm. (3) Parametric methods: The Tobit regression can be trained using the survreg function in the survival package. Buckley-James Regression can be fitted using the bujar package. The parametric AFT models can be trained using the survreg function with various distributions. (4) Machine learning methods: The BMA package can be used to train a Bayesian model by averaging for Cox models. Bagging survival tree methods can be implemented using the bagging function in the R package ipred. Random survival forest is implemented in the rfsrc function in the package randomForestSRC. The mboost function in the package mboost can be used to implement the boosting algorithm. The arc function in RegCox package can be used to train the active learning survival model. Transfer-Cox model is written in C++ language. The multi-task leaning survival method is implemented using MTLSA package in MATLAB. (5) Related topics: The ESP package which performs the early stage prediction for survival analysis problem also incorporates the uncensoring functions in the data pre-processing part. The survutils package in R can be used to implement the calibration for the survival datasets. The survfit function in survival package can also be used to train the model for competing risks. In addition, the function Survr in package survrec can be used to train the survival analysis model for recurrent event data.\nXXX, Vol. X, No. X, Article 1, Publication date: August 2017."
    }, {
      "heading" : "9. CONCLUSION",
      "text" : "The primary goal of survival analysis is to predict the occurrence of specific events of interest at future time points. Due to the widespread availability of survival data from various real-world domains combined with the recent developments in various machine learning methods, there is an increasing demand for understanding and improving methods for effectively handling survival data. In this survey article, we provided a comprehensive review of the conventional survival analysis methods and various machine learning methods for survival analysis, and described other related topics along with the evaluation metrics. We first introduced the basic notations and concepts in survival analysis, including the structure of survival data and the common functions\nXXX, Vol. X, No. X, Article 1, Publication date: August 2017.\nused in survival analysis. Then, we introduced the well-studied statistical survival methods and the representative machine learning based survival methods. Furthermore, the related topics in survival analysis, including data transformation, early prediction and complex events, were also discussed. We also provided the implementation details of these survival methods and described the commonly used performance evaluation metrics for these models. Besides the traditional applications in healthcare and biomedicine, survival analysis was also successfully applied in various real-world problems, such as reliability, student retention and user behavior modeling."
    } ],
    "references" : [ {
      "title" : "Nonparametric inference for a family of counting processes",
      "author" : [ "Odd Aalen." ],
      "venue" : "The Annals of Statistics 6, 4 (1978), 701–726.",
      "citeRegEx" : "Aalen.,? 1978",
      "shortCiteRegEx" : "Aalen.",
      "year" : 1978
    }, {
      "title" : "Survival analysis using SAS: a practical guide",
      "author" : [ "Paul D Allison." ],
      "venue" : "Sas Institute.",
      "citeRegEx" : "Allison.,? 2010",
      "shortCiteRegEx" : "Allison.",
      "year" : 2010
    }, {
      "title" : "Survival analysis based framework for early prediction of student dropouts",
      "author" : [ "Sattar Ameri", "Mahtab J Fard", "Ratna B Chinnam", "Chandan K Reddy." ],
      "venue" : "Proceedings of ACM International Conference on Conference on Information and Knowledge Management. ACM, 903–912.",
      "citeRegEx" : "Ameri et al\\.,? 2016",
      "shortCiteRegEx" : "Ameri et al\\.",
      "year" : 2016
    }, {
      "title" : "Statistical models based on counting processes",
      "author" : [ "Per Kragh Andersen", "Ornulf Borgan", "Richard D Gill", "Niels Keiding." ],
      "venue" : "Springer Science & Business Media.",
      "citeRegEx" : "Andersen et al\\.,? 2012",
      "shortCiteRegEx" : "Andersen et al\\.",
      "year" : 2012
    }, {
      "title" : "PPISURV: a novel bioinformatics tool for uncovering the hidden role of specific genes in cancer survival outcome",
      "author" : [ "A V Antonov", "M Krestyaninova", "R A Knight", "I Rodchenkov", "G Melino", "NA Barlev." ],
      "venue" : "Oncogene 33, 13 (2014), 1621–1628.",
      "citeRegEx" : "Antonov et al\\.,? 2014",
      "shortCiteRegEx" : "Antonov et al\\.",
      "year" : 2014
    }, {
      "title" : "Cox regression models with nonproportional hazards applied to lung cancer survival data",
      "author" : [ "Nihal Ata", "M Tekin Sözer." ],
      "venue" : "Hacettepe Journal of Mathematics and Statistics 36, 2 (2007), 157–167.",
      "citeRegEx" : "Ata and Sözer.,? 2007",
      "shortCiteRegEx" : "Ata and Sözer.",
      "year" : 2007
    }, {
      "title" : "Structured sparsity through convex optimization",
      "author" : [ "Francis Bach", "Rodolphe Jenatton", "Julien Mairal", "Guillaume Obozinski." ],
      "venue" : "Statist. Sci. 27, 4 (2012), 450–468.",
      "citeRegEx" : "Bach et al\\.,? 2012",
      "shortCiteRegEx" : "Bach et al\\.",
      "year" : 2012
    }, {
      "title" : "Neural network survival analysis for personal loan data",
      "author" : [ "Bart Baesens", "Tony Van Gestel", "Maria Stepanova", "Dirk Van den Poel", "Jan Vanthienen." ],
      "venue" : "Journal of the Operational Research Society 56, 9 (2005), 1089–1098.",
      "citeRegEx" : "Baesens et al\\.,? 2005",
      "shortCiteRegEx" : "Baesens et al\\.",
      "year" : 2005
    }, {
      "title" : "Improving post-click user engagement on native ads via survival analysis",
      "author" : [ "Nicola Barbieri", "Fabrizio Silvestri", "Mounia Lalmas." ],
      "venue" : "Proceedings of the 25th International Conference on World Wide Web. 761–770.",
      "citeRegEx" : "Barbieri et al\\.,? 2016",
      "shortCiteRegEx" : "Barbieri et al\\.",
      "year" : 2016
    }, {
      "title" : "Predictive data mining in clinical medicine: current issues and guidelines",
      "author" : [ "Riccardo Bellazzi", "Blaz Zupan." ],
      "venue" : "International journal of medical informatics 77, 2 (2008), 81–97.",
      "citeRegEx" : "Bellazzi and Zupan.,? 2008",
      "shortCiteRegEx" : "Bellazzi and Zupan.",
      "year" : 2008
    }, {
      "title" : "Customer lifetime value: Marketing models and applications",
      "author" : [ "Paul D Berger", "Nada I Nasr." ],
      "venue" : "Journal of interactive marketing 12, 1 (1998), 17–30.",
      "citeRegEx" : "Berger and Nasr.,? 1998",
      "shortCiteRegEx" : "Berger and Nasr.",
      "year" : 1998
    }, {
      "title" : "Feed forward neural networks for the analysis of censored survival data: a partial logistic regression approach",
      "author" : [ "Elia Biganzoli", "Patrizia Boracchi", "Luigi Mariani", "Ettore Marubini." ],
      "venue" : "Statistics in medicine 17, 10 (1998), 1169–1186.",
      "citeRegEx" : "Biganzoli et al\\.,? 1998",
      "shortCiteRegEx" : "Biganzoli et al\\.",
      "year" : 1998
    }, {
      "title" : "Allowing for mandatory covariates in boosting estimation of sparse high-dimensional survival models",
      "author" : [ "Harald Binder", "Martin Schumacher." ],
      "venue" : "BMC bioinformatics 9, 1 (2008), 1–10.",
      "citeRegEx" : "Binder and Schumacher.,? 2008",
      "shortCiteRegEx" : "Binder and Schumacher.",
      "year" : 2008
    }, {
      "title" : "Bagging predictors",
      "author" : [ "Leo Breiman." ],
      "venue" : "Machine learning 24, 2 (1996), 123–140.",
      "citeRegEx" : "Breiman.,? 1996",
      "shortCiteRegEx" : "Breiman.",
      "year" : 1996
    }, {
      "title" : "Random forests",
      "author" : [ "Leo Breiman." ],
      "venue" : "Machine learning 45, 1 (2001), 5–32.",
      "citeRegEx" : "Breiman.,? 2001",
      "shortCiteRegEx" : "Breiman.",
      "year" : 2001
    }, {
      "title" : "Discussion of the paper by D.R. Cox",
      "author" : [ "Norman E Breslow" ],
      "venue" : "J R Statist Soc B",
      "citeRegEx" : "Breslow.,? \\Q1972\\E",
      "shortCiteRegEx" : "Breslow.",
      "year" : 1972
    }, {
      "title" : "Verification of forecasts expressed in terms of probability",
      "author" : [ "Glenn W Brier." ],
      "venue" : "Monthly weather review 78, 1 (1950), 1–3.",
      "citeRegEx" : "Brier.,? 1950",
      "shortCiteRegEx" : "Brier.",
      "year" : 1950
    }, {
      "title" : "On the use of artificial neural networks for the analysis of survival data",
      "author" : [ "Stephen F Brown", "Alan J Branford", "William Moran." ],
      "venue" : "Neural Networks, IEEE Transactions on 8, 5 (1997), 1071–1077.",
      "citeRegEx" : "Brown et al\\.,? 1997",
      "shortCiteRegEx" : "Brown et al\\.",
      "year" : 1997
    }, {
      "title" : "Linear regression with censored data",
      "author" : [ "Jonathan Buckley", "Ian James." ],
      "venue" : "Biometrika 66, 3 (1979), 429–436.",
      "citeRegEx" : "Buckley and James.,? 1979",
      "shortCiteRegEx" : "Buckley and James.",
      "year" : 1979
    }, {
      "title" : "Boosting algorithms: Regularization, prediction and model fitting",
      "author" : [ "Peter Bühlmann", "Torsten Hothorn." ],
      "venue" : "Statist. Sci. 22, 4 (2007), 477–505.",
      "citeRegEx" : "Bühlmann and Hothorn.,? 2007",
      "shortCiteRegEx" : "Bühlmann and Hothorn.",
      "year" : 2007
    }, {
      "title" : "Artificial neural networks improve the accuracy of cancer survival prediction",
      "author" : [ "Harry B Burke", "Philip H Goodman", "David B Rosen", "Donald E Henson", "John N Weinstein", "Frank E Harrell", "Jeffrey R Marks", "David P Winchester", "David G Bostwick." ],
      "venue" : "Cancer 79, 4 (1997), 857–862.",
      "citeRegEx" : "Burke et al\\.,? 1997",
      "shortCiteRegEx" : "Burke et al\\.",
      "year" : 1997
    }, {
      "title" : "Survival analysis: A survey",
      "author" : [ "Ching-Fan Chung", "Peter Schmidt", "Ana D Witte." ],
      "venue" : "Journal of Quantitative Criminology 7, 1 (1991), 59–98.",
      "citeRegEx" : "Chung et al\\.,? 1991",
      "shortCiteRegEx" : "Chung et al\\.",
      "year" : 1991
    }, {
      "title" : "An approach to classifying prognostic factors related to survival experience for non-Hodgkin’s lymphoma patients: Based on a series of 982 patients: 1967–1975",
      "author" : [ "A. Ciampi", "R.S. Bush", "M. Gospodarowicz", "J.E. Till." ],
      "venue" : "Cancer 47, 3 (1981), 621–627.",
      "citeRegEx" : "Ciampi et al\\.,? 1981",
      "shortCiteRegEx" : "Ciampi et al\\.",
      "year" : 1981
    }, {
      "title" : "Recursive partition: a versatile method for exploratory-data analysis in biostatistics",
      "author" : [ "A. Ciampi", "C-H Chang", "S. Hogg", "S. McKinney." ],
      "venue" : "Biostatistics. Springer, 23–50.",
      "citeRegEx" : "Ciampi et al\\.,? 1987",
      "shortCiteRegEx" : "Ciampi et al\\.",
      "year" : 1987
    }, {
      "title" : "Stratification by stepwise regression, correspondence analysis and recursive partition: a comparison of three methods of analysis for survival data with covariates",
      "author" : [ "Antonio Ciampi", "Johanne Thiffault", "Jean-Pierre Nakache", "Bernard Asselain." ],
      "venue" : "Computational statistics & data analysis 4, 3 (1986), 185– 204.",
      "citeRegEx" : "Ciampi et al\\.,? 1986",
      "shortCiteRegEx" : "Ciampi et al\\.",
      "year" : 1986
    }, {
      "title" : "Applications of machine learning in cancer prediction and prognosis",
      "author" : [ "Joseph A Cruz", "David S Wishart." ],
      "venue" : "Cancer informatics 2 (2006).",
      "citeRegEx" : "Cruz and Wishart.,? 2006",
      "shortCiteRegEx" : "Cruz and Wishart.",
      "year" : 2006
    }, {
      "title" : "Maximum utilization of the life table method in analyzing survival",
      "author" : [ "Sidney J Cutler", "Fred Ederer." ],
      "venue" : "Journal of chronic diseases 8, 6 (1958), 699–712.",
      "citeRegEx" : "Cutler and Ederer.,? 1958",
      "shortCiteRegEx" : "Cutler and Ederer.",
      "year" : 1958
    }, {
      "title" : "Regression models and life tables",
      "author" : [ "Cox R David." ],
      "venue" : "Journal of the Royal Statistical Society 34, 2 (1972), 187–220.",
      "citeRegEx" : "David.,? 1972",
      "shortCiteRegEx" : "David.",
      "year" : 1972
    }, {
      "title" : "Partial likelihood",
      "author" : [ "Cox R David." ],
      "venue" : "Biometrika 62, 2 (1975), 269–276.",
      "citeRegEx" : "David.,? 1975",
      "shortCiteRegEx" : "David.",
      "year" : 1975
    }, {
      "title" : "Exponential survival trees",
      "author" : [ "Roger B Davis", "James R Anderson." ],
      "venue" : "Statistics in Medicine 8, 8 (1989), 947–961.",
      "citeRegEx" : "Davis and Anderson.,? 1989",
      "shortCiteRegEx" : "Davis and Anderson.",
      "year" : 1989
    }, {
      "title" : "Predicting breast cancer survivability: a comparison of three data mining methods",
      "author" : [ "Dursun Delen", "Glenn Walker", "Amit Kadam." ],
      "venue" : "Artificial intelligence in medicine 34, 2 (2005), 113–127.",
      "citeRegEx" : "Delen et al\\.,? 2005",
      "shortCiteRegEx" : "Delen et al\\.",
      "year" : 2005
    }, {
      "title" : "Ensemble methods in machine learning",
      "author" : [ "Thomas G Dietterich." ],
      "venue" : "International workshop on multiple classifier systems. Springer, 1–15.",
      "citeRegEx" : "Dietterich.,? 2000",
      "shortCiteRegEx" : "Dietterich.",
      "year" : 2000
    }, {
      "title" : "Basic statistics: a primer for the biomedical sciences",
      "author" : [ "Olive J Dunn", "Virginia A Clark." ],
      "venue" : "John Wiley & Sons.",
      "citeRegEx" : "Dunn and Clark.,? 2009",
      "shortCiteRegEx" : "Dunn and Clark.",
      "year" : 2009
    }, {
      "title" : "A neural network model for survival data",
      "author" : [ "David Faraggi", "Richard Simon." ],
      "venue" : "Statistics in medicine 14, 1 (1995), 73–82.",
      "citeRegEx" : "Faraggi and Simon.,? 1995",
      "shortCiteRegEx" : "Faraggi and Simon.",
      "year" : 1995
    }, {
      "title" : "A bayesian perspective on early stage event prediction in longitudinal data",
      "author" : [ "Mahtab J Fard", "Ping Wang", "Sanjay Chawla", "Chandan K Reddy." ],
      "venue" : "IEEE Transactions on Knowledge and Data Engineering 28, 12 (2016), 3126–3139.",
      "citeRegEx" : "Fard et al\\.,? 2016",
      "shortCiteRegEx" : "Fard et al\\.",
      "year" : 2016
    }, {
      "title" : "The elements of statistical learning",
      "author" : [ "Jerome Friedman", "Trevor Hastie", "Robert Tibshirani." ],
      "venue" : "Vol. 1. Springer series in statistics Springer.",
      "citeRegEx" : "Friedman et al\\.,? 2001",
      "shortCiteRegEx" : "Friedman et al\\.",
      "year" : 2001
    }, {
      "title" : "Sparse inverse covariance estimation with the graphical lasso",
      "author" : [ "Jerome Friedman", "Trevor Hastie", "Robert Tibshirani." ],
      "venue" : "Biostatistics 9, 3 (2008), 432–441.",
      "citeRegEx" : "Friedman et al\\.,? 2008",
      "shortCiteRegEx" : "Friedman et al\\.",
      "year" : 2008
    }, {
      "title" : "Bayesian network classifiers",
      "author" : [ "Nir Friedman", "Dan Geiger", "Moises Goldszmidt." ],
      "venue" : "Machine learning 29, 2 (1997), 131–163.",
      "citeRegEx" : "Friedman et al\\.,? 1997",
      "shortCiteRegEx" : "Friedman et al\\.",
      "year" : 1997
    }, {
      "title" : "An application of dynamic survival models in unemployment studies",
      "author" : [ "Dani Gamerman", "Mike West." ],
      "venue" : "The Statistician (1987), 269–274.",
      "citeRegEx" : "Gamerman and West.,? 1987",
      "shortCiteRegEx" : "Gamerman and West.",
      "year" : 1987
    }, {
      "title" : "Tree-structured survival analysis",
      "author" : [ "Louis Gordon", "Richard A Olshen." ],
      "venue" : "Cancer treatment reports 69, 10 (1985), 1065–1069.",
      "citeRegEx" : "Gordon and Olshen.,? 1985",
      "shortCiteRegEx" : "Gordon and Olshen.",
      "year" : 1985
    }, {
      "title" : "Assessment and comparison of prognostic classification schemes for survival data",
      "author" : [ "Erika Graf", "Claudia Schmoor", "Willi Sauerbrei", "Martin Schumacher." ],
      "venue" : "Statistics in medicine 18, 17-18 (1999), 2529–2545.",
      "citeRegEx" : "Graf et al\\.,? 1999",
      "shortCiteRegEx" : "Graf et al\\.",
      "year" : 1999
    }, {
      "title" : "Constraint classification: A new approach to multiclass classification",
      "author" : [ "Sariel Har-Peled", "Dan Roth", "Dav Zimak." ],
      "venue" : "Algorithmic Learning Theory. Springer, 365–379.",
      "citeRegEx" : "Har.Peled et al\\.,? 2002",
      "shortCiteRegEx" : "Har.Peled et al\\.",
      "year" : 2002
    }, {
      "title" : "Evaluating the yield of medical tests",
      "author" : [ "Frank E Harrell", "Robert M Califf", "David B Pryor", "Kerry L Lee", "Robert A Rosati." ],
      "venue" : "Journal of the American Medical Association 247, 18 (1982), 2543–2546.",
      "citeRegEx" : "Harrell et al\\.,? 1982",
      "shortCiteRegEx" : "Harrell et al\\.",
      "year" : 1982
    }, {
      "title" : "Regression modelling strategies for improved prognostic prediction",
      "author" : [ "Frank E Harrell", "Kerry L Lee", "Robert M Califf", "David B Pryor", "Robert A Rosati." ],
      "venue" : "Statistics in medicine 3, 2 (1984), 143–152.",
      "citeRegEx" : "Harrell et al\\.,? 1984",
      "shortCiteRegEx" : "Harrell et al\\.",
      "year" : 1984
    }, {
      "title" : "Survival model predictive accuracy and ROC curves",
      "author" : [ "Patrick J Heagerty", "Yingye Zheng." ],
      "venue" : "Biometrics 61, 1 (2005), 92–105.",
      "citeRegEx" : "Heagerty and Zheng.,? 2005",
      "shortCiteRegEx" : "Heagerty and Zheng.",
      "year" : 2005
    }, {
      "title" : "Ridge regression: Biased estimation for nonorthogonal problems",
      "author" : [ "Arthur E Hoerl", "Robert W Kennard." ],
      "venue" : "Technometrics 12, 1 (1970), 55–67.",
      "citeRegEx" : "Hoerl and Kennard.,? 1970",
      "shortCiteRegEx" : "Hoerl and Kennard.",
      "year" : 1970
    }, {
      "title" : "Survival ensembles",
      "author" : [ "Torsten Hothorn", "Peter Bühlmann", "Sandrine Dudoit", "Annette Molinaro", "Mark J Van Der Laan." ],
      "venue" : "Biostatistics 7, 3 (2006), 355–373.",
      "citeRegEx" : "Hothorn et al\\.,? 2006",
      "shortCiteRegEx" : "Hothorn et al\\.",
      "year" : 2006
    }, {
      "title" : "Bagging survival trees",
      "author" : [ "Torsten Hothorn", "Berthold Lausen", "Axel Benner", "Martin Radespiel-Tröger." ],
      "venue" : "Statistics in medicine 23, 1 (2004), 77–91.",
      "citeRegEx" : "Hothorn et al\\.,? 2004",
      "shortCiteRegEx" : "Hothorn et al\\.",
      "year" : 2004
    }, {
      "title" : "Random survival forests",
      "author" : [ "Hemant Ishwaran", "Udaya B Kogalur", "Eugene H Blackstone", "Michael S Lauer." ],
      "venue" : "The annals of applied statistics 2, 3 (2008), 841–860.",
      "citeRegEx" : "Ishwaran et al\\.,? 2008",
      "shortCiteRegEx" : "Ishwaran et al\\.",
      "year" : 2008
    }, {
      "title" : "Random survival forests for highdimensional data",
      "author" : [ "Hemant Ishwaran", "Udaya B Kogalur", "Xi Chen", "Andy J Minn." ],
      "venue" : "Statistical analysis and data mining 4, 1 (2011), 115–132.",
      "citeRegEx" : "Ishwaran et al\\.,? 2011",
      "shortCiteRegEx" : "Ishwaran et al\\.",
      "year" : 2011
    }, {
      "title" : "The statistical analysis of failure time data",
      "author" : [ "John D Kalbfleisch", "Ross L Prentice." ],
      "venue" : "Vol. 360. John Wiley & Sons.",
      "citeRegEx" : "Kalbfleisch and Prentice.,? 2011",
      "shortCiteRegEx" : "Kalbfleisch and Prentice.",
      "year" : 2011
    }, {
      "title" : "Nonparametric estimation from incomplete observations",
      "author" : [ "Edward L Kaplan", "Paul Meier." ],
      "venue" : "Journal of the American statistical association 53, 282 (1958), 457–481.",
      "citeRegEx" : "Kaplan and Meier.,? 1958",
      "shortCiteRegEx" : "Kaplan and Meier.",
      "year" : 1958
    }, {
      "title" : "Iterative methods for optimization",
      "author" : [ "Carl T Kelley." ],
      "venue" : "Vol. 18. SIAM.",
      "citeRegEx" : "Kelley.,? 1999",
      "shortCiteRegEx" : "Kelley.",
      "year" : 1999
    }, {
      "title" : "Support vector regression for censored data (SVRc): a novel tool for survival analysis",
      "author" : [ "Faisal M Khan", "Valentina B Zubek." ],
      "venue" : "Proceedings of the IEEE International Conference on Data Mining (ICDM). IEEE, 863–868.",
      "citeRegEx" : "Khan and Zubek.,? 2008",
      "shortCiteRegEx" : "Khan and Zubek.",
      "year" : 2008
    }, {
      "title" : "Relevance vector machine for survival analysis",
      "author" : [ "Farkhondeh Kiaee", "Hamid Sheikhzadeh", "Samaneh Eftekhari Mahabadi." ],
      "venue" : "IEEE transactions on neural networks and learning systems 27, 3 (2016), 648–660.",
      "citeRegEx" : "Kiaee et al\\.,? 2016",
      "shortCiteRegEx" : "Kiaee et al\\.",
      "year" : 2016
    }, {
      "title" : "Economic duration data and hazard functions",
      "author" : [ "Nicholas M Kiefer." ],
      "venue" : "Journal of economic literature 26, 2 (1988), 646–679.",
      "citeRegEx" : "Kiefer.,? 1988",
      "shortCiteRegEx" : "Kiefer.",
      "year" : 1988
    }, {
      "title" : "Survival analysis: techniques for censored and truncated data",
      "author" : [ "John P Klein", "Melvin L Moeschberger." ],
      "venue" : "Springer Science & Business Media.",
      "citeRegEx" : "Klein and Moeschberger.,? 2005",
      "shortCiteRegEx" : "Klein and Moeschberger.",
      "year" : 2005
    }, {
      "title" : "Survival analysis: a self-learning text",
      "author" : [ "David G Kleinbaum", "Mitchel Klein." ],
      "venue" : "Springer Science & Business Media.",
      "citeRegEx" : "Kleinbaum and Klein.,? 2006",
      "shortCiteRegEx" : "Kleinbaum and Klein.",
      "year" : 2006
    }, {
      "title" : "Inductive and Bayesian learning in medical diagnosis",
      "author" : [ "Igor Kononenko." ],
      "venue" : "Applied Artificial Intelligence an International Journal 7, 4 (1993), 317–337.",
      "citeRegEx" : "Kononenko.,? 1993",
      "shortCiteRegEx" : "Kononenko.",
      "year" : 1993
    }, {
      "title" : "Regression analysis with randomly right-censored data",
      "author" : [ "H Koul", "V Susarla", "J Van Ryzin." ],
      "venue" : "The Annals of Statistics 9, 6 (1981), 1276–1288.",
      "citeRegEx" : "Koul et al\\.,? 1981",
      "shortCiteRegEx" : "Koul et al\\.",
      "year" : 1981
    }, {
      "title" : "Machine learning applications in cancer prognosis and prediction",
      "author" : [ "Konstantina Kourou", "Themis P Exarchos", "Konstantinos P Exarchos", "Michalis V Karamouzis", "Dimitrios I Fotiadis." ],
      "venue" : "Computational and structural biotechnology journal 13 (2015), 8–17.",
      "citeRegEx" : "Kourou et al\\.,? 2015",
      "shortCiteRegEx" : "Kourou et al\\.",
      "year" : 2015
    }, {
      "title" : "Relative risk trees for censored survival data",
      "author" : [ "Michael LeBlanc", "John Crowley." ],
      "venue" : "Biometrics 48, 2 (1992), 411–425.",
      "citeRegEx" : "LeBlanc and Crowley.,? 1992",
      "shortCiteRegEx" : "LeBlanc and Crowley.",
      "year" : 1992
    }, {
      "title" : "Statistical methods for survival data analysis",
      "author" : [ "Elisa T Lee", "John Wang." ],
      "venue" : "Vol. 476. John Wiley & Sons.",
      "citeRegEx" : "Lee and Wang.,? 2003",
      "shortCiteRegEx" : "Lee and Wang.",
      "year" : 2003
    }, {
      "title" : "Project success prediction in crowdfunding environments",
      "author" : [ "Yan Li", "Vineeth Rakesh", "Chandan K Reddy." ],
      "venue" : "Proceedings of the Ninth ACM International Conference on Web Search and Data Mining. ACM, 247–256.",
      "citeRegEx" : "Li et al\\.,? 2016a",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2016
    }, {
      "title" : "Regularized weighted linear regression for high-dimensional censored data",
      "author" : [ "Yan Li", "Bhanukiran Vinzamuri", "Chandan K Reddy." ],
      "venue" : "Proceedings of SIAM International Conference on Data Mining. SIAM. 45–53.",
      "citeRegEx" : "Li et al\\.,? 2016b",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2016
    }, {
      "title" : "A multi-task learning formulation for survival analysis",
      "author" : [ "Yan Li", "Jie Wang", "Jieping Ye", "Chandan K Reddy." ],
      "venue" : "Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM. 1715–1724.",
      "citeRegEx" : "Li et al\\.,? 2016d",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2016
    }, {
      "title" : "Transfer learning for survival analysis via efficient L2,1-norm regularized Cox regression",
      "author" : [ "Yan Li", "Lu Wang", "Jie Wang", "Jieping Ye", "Chandan K Reddy." ],
      "venue" : "Proceedings of the IEEE International Conference on Data Mining (ICDM). IEEE, 231–240.",
      "citeRegEx" : "Li et al\\.,? 2016c",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2016
    }, {
      "title" : "Regularized parametric regression for high-dimensional survival analysis",
      "author" : [ "Yan Li", "Kevin S Xu", "Chandan K Reddy." ],
      "venue" : "Proceedings of the 2016 SIAM International Conference on Data Mining. SIAM, 765–773.",
      "citeRegEx" : "Li et al\\.,? 2016e",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2016
    }, {
      "title" : "Survival analysis and neural nets",
      "author" : [ "Knut Liestbl", "Per Kragh Andersen", "Ulrich Andersen." ],
      "venue" : "Statistics in medicine 13, 12 (1994), 1189–1200.",
      "citeRegEx" : "Liestbl et al\\.,? 1994",
      "shortCiteRegEx" : "Liestbl et al\\.",
      "year" : 1994
    }, {
      "title" : "A bayesian neural network approach for modelling censored data with an application to prognosis after surgery for breast cancer",
      "author" : [ "Paulo JG Lisboa", "H Wong", "P Harris", "Ric Swindell." ],
      "venue" : "Artificial intelligence in medicine 28, 1 (2003), 1–25.",
      "citeRegEx" : "Lisboa et al\\.,? 2003",
      "shortCiteRegEx" : "Lisboa et al\\.",
      "year" : 2003
    }, {
      "title" : "Applying Cox regression to competing risks",
      "author" : [ "Mary Lunn", "Don McNeil." ],
      "venue" : "Biometrics (1995), 524–532.",
      "citeRegEx" : "Lunn and McNeil.,? 1995",
      "shortCiteRegEx" : "Lunn and McNeil.",
      "year" : 1995
    }, {
      "title" : "Handbook of software reliability engineering",
      "author" : [ "Michael R Lyu." ],
      "venue" : "Vol. 222. IEEE computer society press CA.",
      "citeRegEx" : "Lyu.,? 1996",
      "shortCiteRegEx" : "Lyu.",
      "year" : 1996
    }, {
      "title" : "Probable networks and plausible predictions-a review of practical bayesian methods for supervised neural networks",
      "author" : [ "David JC MacKay." ],
      "venue" : "Network: Computation in Neural Systems 6, 3 (1995), 469–505.",
      "citeRegEx" : "MacKay.,? 1995",
      "shortCiteRegEx" : "MacKay.",
      "year" : 1995
    }, {
      "title" : "Statistics and data mining techniques for lifetime value modeling",
      "author" : [ "DR Mani", "James Drew", "Andrew Betz", "Piew Datta." ],
      "venue" : "Proceedings of the fifth ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 94–103.",
      "citeRegEx" : "Mani et al\\.,? 1999",
      "shortCiteRegEx" : "Mani et al\\.",
      "year" : 1999
    }, {
      "title" : "Analysing survival data from clinical trials and observational studies",
      "author" : [ "Ettore Marubini", "Maria Grazia Valsecchi." ],
      "venue" : "Vol. 15. John Wiley & Sons.",
      "citeRegEx" : "Marubini and Valsecchi.,? 2004",
      "shortCiteRegEx" : "Marubini and Valsecchi.",
      "year" : 2004
    }, {
      "title" : "Regression with censored data",
      "author" : [ "Rupert Miller", "Jerry Halpern." ],
      "venue" : "Biometrika 69, 3 (1982), 521–531.",
      "citeRegEx" : "Miller and Halpern.,? 1982",
      "shortCiteRegEx" : "Miller and Halpern.",
      "year" : 1982
    }, {
      "title" : "Survival analysis",
      "author" : [ "Rupert G Miller Jr." ],
      "venue" : "Vol. 66. John Wiley & Sons.",
      "citeRegEx" : "Jr.,? 2011",
      "shortCiteRegEx" : "Jr.",
      "year" : 2011
    }, {
      "title" : "Reliability engineering and risk analysis: a practical guide",
      "author" : [ "Mohammad Modarres", "Mark P Kaminskiy", "Vasiliy Krivtsov." ],
      "venue" : "CRC press.",
      "citeRegEx" : "Modarres et al\\.,? 2009",
      "shortCiteRegEx" : "Modarres et al\\.",
      "year" : 2009
    }, {
      "title" : "Predicting the retention of university students",
      "author" : [ "Paul A Murtaugh", "Leslie D Burns", "Jill Schuster." ],
      "venue" : "Research in higher education 40, 3 (1999), 355–371.",
      "citeRegEx" : "Murtaugh et al\\.,? 1999",
      "shortCiteRegEx" : "Murtaugh et al\\.",
      "year" : 1999
    }, {
      "title" : "Theory and applications of hazard plotting for censored failure data",
      "author" : [ "Wayne Nelson." ],
      "venue" : "Technometrics 14, 4 (1972), 945–966.",
      "citeRegEx" : "Nelson.,? 1972",
      "shortCiteRegEx" : "Nelson.",
      "year" : 1972
    }, {
      "title" : "A survey on transfer learning",
      "author" : [ "Sinno J Pan", "Qiang Yang." ],
      "venue" : "IEEE Transactions on knowledge and data engineering 22, 10 (2010), 1345–1359.",
      "citeRegEx" : "Pan and Yang.,? 2010",
      "shortCiteRegEx" : "Pan and Yang.",
      "year" : 2010
    }, {
      "title" : "Overall C as a measure of discrimination in survival analysis: model specific population value and confidence interval estimation",
      "author" : [ "Michael J Pencina", "Ralph B D’Agostino" ],
      "venue" : "Statistics in medicine 23,",
      "citeRegEx" : "Pencina and D.Agostino.,? \\Q2004\\E",
      "shortCiteRegEx" : "Pencina and D.Agostino.",
      "year" : 2004
    }, {
      "title" : "The statistical evaluation of medical tests for classification and prediction",
      "author" : [ "Margaret S Pepe." ],
      "venue" : "Oxford University Press, USA.",
      "citeRegEx" : "Pepe.,? 2003",
      "shortCiteRegEx" : "Pepe.",
      "year" : 2003
    }, {
      "title" : "Estimation of semiparametric models",
      "author" : [ "James L Powell." ],
      "venue" : "Handbook of econometrics 4 (1994), 2443–2521.",
      "citeRegEx" : "Powell.,? 1994",
      "shortCiteRegEx" : "Powell.",
      "year" : 1994
    }, {
      "title" : "On the regression analysis of multivariate failure time data",
      "author" : [ "Ross L Prentice", "Benjamin J Williams", "Arthur V Peterson." ],
      "venue" : "Biometrika 68, 2 (1981), 373–379.",
      "citeRegEx" : "Prentice et al\\.,? 1981",
      "shortCiteRegEx" : "Prentice et al\\.",
      "year" : 1981
    }, {
      "title" : "Tutorial in biostatistics: competing risks and multi-state models",
      "author" : [ "Hein Putter", "M Fiocco", "R.B. Geskus." ],
      "venue" : "Statistics in medicine 26, 11 (2007), 2389–2430.",
      "citeRegEx" : "Putter et al\\.,? 2007",
      "shortCiteRegEx" : "Putter et al\\.",
      "year" : 2007
    }, {
      "title" : "Accounting for model uncertainty in survival analysis improves predictive performance",
      "author" : [ "Adrian Raftery", "David Madigan", "Chris T. Volinsky." ],
      "venue" : "Bayesian Statistics 5 (1995), 323–349.",
      "citeRegEx" : "Raftery et al\\.,? 1995",
      "shortCiteRegEx" : "Raftery et al\\.",
      "year" : 1995
    }, {
      "title" : "Bayesian model selection in social research",
      "author" : [ "Adrian E Raftery." ],
      "venue" : "Sociological methodology 25 (1995), 111–163.",
      "citeRegEx" : "Raftery.,? 1995",
      "shortCiteRegEx" : "Raftery.",
      "year" : 1995
    }, {
      "title" : "Project recommendation using heterogeneous traits in crowdfunding",
      "author" : [ "Vineeth Rakesh", "Jaegul Choo", "Chandan K Reddy." ],
      "venue" : "Ninth International AAAI Conference on Web and Social Media. 337–346.",
      "citeRegEx" : "Rakesh et al\\.,? 2015",
      "shortCiteRegEx" : "Rakesh et al\\.",
      "year" : 2015
    }, {
      "title" : "Probabilistic group recommendation model for crowdfunding domains",
      "author" : [ "Vineeth Rakesh", "Wang-Chien Lee", "Chandan K Reddy." ],
      "venue" : "Proceedings of the Ninth ACM International Conference on Web Search and Data Mining. ACM, 257–266.",
      "citeRegEx" : "Rakesh et al\\.,? 2016",
      "shortCiteRegEx" : "Rakesh et al\\.",
      "year" : 2016
    }, {
      "title" : "A practical application of neural network analysis for predicting outcome of individual breast cancer patients",
      "author" : [ "Peter M Ravdin", "Gary M Clark." ],
      "venue" : "Breast Cancer Research and Treatment 22, 3 (1992), 285– 293.",
      "citeRegEx" : "Ravdin and Clark.,? 1992",
      "shortCiteRegEx" : "Ravdin and Clark.",
      "year" : 1992
    }, {
      "title" : "A review of clinical prediction models",
      "author" : [ "Chandan K Reddy", "Yan Li." ],
      "venue" : "Healthcare Data Analytics 36 (2015), 343–378.",
      "citeRegEx" : "Reddy and Li.,? 2015",
      "shortCiteRegEx" : "Reddy and Li.",
      "year" : 2015
    }, {
      "title" : "Predicting clicks: estimating the clickthrough rate for new ads",
      "author" : [ "Matthew Richardson", "Ewa Dominowska", "Robert Ragno." ],
      "venue" : "Proceedings of the 16th international conference on World Wide Web. ACM, 521–530.",
      "citeRegEx" : "Richardson et al\\.,? 2007",
      "shortCiteRegEx" : "Richardson et al\\.",
      "year" : 2007
    }, {
      "title" : "The perceptron: a probabilistic model for information storage and organization in the brain",
      "author" : [ "Frank Rosenblatt." ],
      "venue" : "Psychological review 65, 6 (1958), 386–408.",
      "citeRegEx" : "Rosenblatt.,? 1958",
      "shortCiteRegEx" : "Rosenblatt.",
      "year" : 1958
    }, {
      "title" : "Customer lifetime value models for decision support",
      "author" : [ "Saharon Rosset", "Einat Neumann", "Uri Eick", "Nurit Vatnik." ],
      "venue" : "Data mining and knowledge discovery 7, 3 (2003), 321–339.",
      "citeRegEx" : "Rosset et al\\.,? 2003",
      "shortCiteRegEx" : "Rosset et al\\.",
      "year" : 2003
    }, {
      "title" : "A survey of decision tree classifier methodology",
      "author" : [ "S Rasoul Safavian", "David Landgrebe." ],
      "venue" : "IEEE transactions on systems, man, and cybernetics 21, 3 (1991), 660–674.",
      "citeRegEx" : "Safavian and Landgrebe.,? 1991",
      "shortCiteRegEx" : "Safavian and Landgrebe.",
      "year" : 1991
    }, {
      "title" : "Regression trees for censored data",
      "author" : [ "Mark R Segal." ],
      "venue" : "Biometrics 44, 1 (1988), 35–47.",
      "citeRegEx" : "Segal.,? 1988",
      "shortCiteRegEx" : "Segal.",
      "year" : 1988
    }, {
      "title" : "A support vector approach to censored targets",
      "author" : [ "Pannagadatta K Shivaswamy", "Wei Chu", "Martin Jansche." ],
      "venue" : "Proceedings of the IEEE International Conference on Data Mining (ICDM). IEEE, 655–660.",
      "citeRegEx" : "Shivaswamy et al\\.,? 2007",
      "shortCiteRegEx" : "Shivaswamy et al\\.",
      "year" : 2007
    }, {
      "title" : "Learning with kernels",
      "author" : [ "Alex J Smola", "Bernhard Schölkopf." ],
      "venue" : "https://mitpress.mit.edu/books/ learning-kernels.",
      "citeRegEx" : "Smola and Schölkopf.,? 1998",
      "shortCiteRegEx" : "Smola and Schölkopf.",
      "year" : 1998
    }, {
      "title" : "A tutorial on support vector regression",
      "author" : [ "Alex J Smola", "Bernhard Schölkopf." ],
      "venue" : "Statistics and computing 14, 3 (2004), 199–222.",
      "citeRegEx" : "Smola and Schölkopf.,? 2004",
      "shortCiteRegEx" : "Smola and Schölkopf.",
      "year" : 2004
    }, {
      "title" : "On ranking in survival analysis: Bounds on the concordance index",
      "author" : [ "Harald Steck", "Balaji Krishnapuram", "Cary Dehing-oberije", "Philippe Lambin", "Vikas C Raykar." ],
      "venue" : "Advances in neural information processing systems. 1209–1216.",
      "citeRegEx" : "Steck et al\\.,? 2008",
      "shortCiteRegEx" : "Steck et al\\.",
      "year" : 2008
    }, {
      "title" : "Regression shrinkage and selection via the lasso",
      "author" : [ "Robert Tibshirani." ],
      "venue" : "Journal of the Royal Statistical Society. Series B (Methodological) 58, 1 (1996), 267–288.",
      "citeRegEx" : "Tibshirani.,? 1996",
      "shortCiteRegEx" : "Tibshirani.",
      "year" : 1996
    }, {
      "title" : "The lasso method for variable selection in the Cox model",
      "author" : [ "Robert Tibshirani." ],
      "venue" : "Statistics in medicine 16, 4 (1997), 385–395.",
      "citeRegEx" : "Tibshirani.,? 1997",
      "shortCiteRegEx" : "Tibshirani.",
      "year" : 1997
    }, {
      "title" : "Sparsity and smoothness via the fused lasso",
      "author" : [ "Robert Tibshirani", "Michael Saunders", "Saharon Rosset", "Ji Zhu", "Keith Knight." ],
      "venue" : "Journal of the Royal Statistical Society: Series B (Statistical Methodology) 67, 1 (2005), 91–108.",
      "citeRegEx" : "Tibshirani et al\\.,? 2005",
      "shortCiteRegEx" : "Tibshirani et al\\.",
      "year" : 2005
    }, {
      "title" : "Estimation of relationships for limited dependent variables",
      "author" : [ "James Tobin." ],
      "venue" : "Econometrica: journal of the Econometric Society 26, 1 (1958), 24–36.",
      "citeRegEx" : "Tobin.,? 1958",
      "shortCiteRegEx" : "Tobin.",
      "year" : 1958
    }, {
      "title" : "Support vector methods for survival analysis: a comparison between ranking and regression approaches",
      "author" : [ "Belle Van", "Kristiaan Pelckmans", "Huffel S Van", "Johan AK Suykens." ],
      "venue" : "Artificial intelligence in medicine 53, 2 (2011), 107–118.",
      "citeRegEx" : "Van et al\\.,? 2011",
      "shortCiteRegEx" : "Van et al\\.",
      "year" : 2011
    }, {
      "title" : "Support vector machines for survival analysis",
      "author" : [ "Belle V Van", "Kristiaan Pelckmans", "Johan AK Suykens", "S Van Huffel." ],
      "venue" : "Proceedings of the Third International Conference on Computational Intelligence in Medicine and Healthcare (CIMED2007). 1–8.",
      "citeRegEx" : "Van et al\\.,? 2007",
      "shortCiteRegEx" : "Van et al\\.",
      "year" : 2007
    }, {
      "title" : "Dynamic prediction in clinical survival analysis",
      "author" : [ "Hans C. van Houwelingen", "Hein Putter." ],
      "venue" : "CRC Press.",
      "citeRegEx" : "Houwelingen and Putter.,? 2011",
      "shortCiteRegEx" : "Houwelingen and Putter.",
      "year" : 2011
    }, {
      "title" : "Penalized likelihood in Cox regression",
      "author" : [ "Pierre JM Verweij", "Hans C Van Houwelingen." ],
      "venue" : "Statistics in medicine 13, 23-24 (1994), 2427–2436.",
      "citeRegEx" : "Verweij and Houwelingen.,? 1994",
      "shortCiteRegEx" : "Verweij and Houwelingen.",
      "year" : 1994
    }, {
      "title" : "Active learning based survival regression for censored data",
      "author" : [ "Bhanukiran Vinzamuri", "Yan Li", "Chandan K Reddy." ],
      "venue" : "Proceedings of the 23rd ACM international conference on conference on information and knowledge management. ACM, 241–250.",
      "citeRegEx" : "Vinzamuri et al\\.,? 2014",
      "shortCiteRegEx" : "Vinzamuri et al\\.",
      "year" : 2014
    }, {
      "title" : "Pre-processing Censored Survival Data Using Inverse Covariance Matrix Based Calibration",
      "author" : [ "Bhanukiran Vinzamuri", "Yan Li", "Chandan K Reddy." ],
      "venue" : "Transactions on Knowledge and Data Engineering (2017).",
      "citeRegEx" : "Vinzamuri et al\\.,? 2017",
      "shortCiteRegEx" : "Vinzamuri et al\\.",
      "year" : 2017
    }, {
      "title" : "Cox regression with correlation based regularization for electronic health records",
      "author" : [ "Bhanukiran Vinzamuri", "Chandan K Reddy." ],
      "venue" : "Proceedings of the IEEE International Conference on Data Mining (ICDM). IEEE, 757–766.",
      "citeRegEx" : "Vinzamuri and Reddy.,? 2013",
      "shortCiteRegEx" : "Vinzamuri and Reddy.",
      "year" : 2013
    }, {
      "title" : "Doubly penalized Buckley–James method for survival data with high-dimensional covariates",
      "author" : [ "Sijian Wang", "Bin Nan", "Ji Zhu", "David G Beer." ],
      "venue" : "Biometrics 64, 1 (2008), 132–140.",
      "citeRegEx" : "Wang et al\\.,? 2008",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2008
    }, {
      "title" : "Regression analysis of multivariate incomplete failure time data by modeling marginal distributions",
      "author" : [ "Lee-Jen Wei", "Danyu Y Lin", "Lisa Weissfeld." ],
      "venue" : "Journal of the American statistical association 84, 408 (1989), 1065–1073.",
      "citeRegEx" : "Wei et al\\.,? 1989",
      "shortCiteRegEx" : "Wei et al\\.",
      "year" : 1989
    }, {
      "title" : "Application of relevance vector machine and survival probability to machine degradation assessment",
      "author" : [ "Achmad Widodo", "Bo-Suk Yang." ],
      "venue" : "Expert Systems with Applications 38, 3 (2011), 2592–2599.",
      "citeRegEx" : "Widodo and Yang.,? 2011",
      "shortCiteRegEx" : "Widodo and Yang.",
      "year" : 2011
    }, {
      "title" : "Feature grouping and selection over an undirected graph",
      "author" : [ "Sen Yang", "Lei Yuan", "Ying-Cheng Lai", "Xiaotong Shen", "Peter Wonka", "Jieping Ye." ],
      "venue" : "Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 922–930.",
      "citeRegEx" : "Yang et al\\.,? 2012",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2012
    }, {
      "title" : "Sparse methods for biomedical data",
      "author" : [ "Jieping Ye", "Jun Liu." ],
      "venue" : "ACM SIGKDD Explorations Newsletter 14, 1 (2012), 4–15.",
      "citeRegEx" : "Ye and Liu.,? 2012",
      "shortCiteRegEx" : "Ye and Liu.",
      "year" : 2012
    }, {
      "title" : "Silence is also evidence: interpreting dwell time for recommendation from psychological perspective",
      "author" : [ "Peifeng Yin", "Ping Luo", "Wang-Chien Lee", "Min Wang." ],
      "venue" : "Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 989–997.",
      "citeRegEx" : "Yin et al\\.,? 2013",
      "shortCiteRegEx" : "Yin et al\\.",
      "year" : 2013
    }, {
      "title" : "Driving customer equity: How customer lifetime value is reshaping corporate strategy",
      "author" : [ "Valarie A Zeithaml", "Katherine N Lemon", "Roland T Rust." ],
      "venue" : "Simon and Schuster.",
      "citeRegEx" : "Zeithaml et al\\.,? 2001",
      "shortCiteRegEx" : "Zeithaml et al\\.",
      "year" : 2001
    }, {
      "title" : "Adaptive Lasso for Cox’s proportional hazards model",
      "author" : [ "Hao H Zhang", "Wenbin Lu." ],
      "venue" : "Biometrika 94, 3 (2007), 691–703.",
      "citeRegEx" : "Zhang and Lu.,? 2007",
      "shortCiteRegEx" : "Zhang and Lu.",
      "year" : 2007
    }, {
      "title" : "Regularization and variable selection via the elastic net",
      "author" : [ "Hui Zou", "Trevor Hastie." ],
      "venue" : "Journal of the Royal Statistical Society: Series B (Statistical Methodology) 67, 2 (2005), 301–320.",
      "citeRegEx" : "Zou and Hastie.,? 2005",
      "shortCiteRegEx" : "Zou and Hastie.",
      "year" : 2005
    }, {
      "title" : "Machine learning for survival analysis: a case study on recurrence of prostate cancer",
      "author" : [ "Blaž Zupan", "Janez DemšAr", "Michael W Kattan", "Robert J Beck", "Ivan Bratko." ],
      "venue" : "Artificial intelligence in medicine 20, 1 (2000), 59–75.",
      "citeRegEx" : "Zupan et al\\.,? 2000",
      "shortCiteRegEx" : "Zupan et al\\.",
      "year" : 2000
    } ],
    "referenceMentions" : [ {
      "referenceID" : 21,
      "context" : "One of the earliest surveys may be found in [Chung et al. 1991], which gives an overview of the statistical survival analysis methods and describes its applications in criminology by predicting the time until recidivism.",
      "startOffset" : 44,
      "endOffset" : 63
    }, {
      "referenceID" : 60,
      "context" : "Recently, the authors in [Cruz and Wishart 2006] and [Kourou et al. 2015] discussed the applications in cancer prediction and provided a comparison of several machine learning techniques.",
      "startOffset" : 53,
      "endOffset" : 73
    }, {
      "referenceID" : 3,
      "context" : "NA method is an estimator based on modern counting process techniques [Andersen et al. 2012].",
      "startOffset" : 70,
      "endOffset" : 92
    }, {
      "referenceID" : 35,
      "context" : "This motivates using sparsity norms to select vital features in high-dimension under the assumption that most of the features are not significant [Friedman et al. 2001].",
      "startOffset" : 146,
      "endOffset" : 168
    }, {
      "referenceID" : 103,
      "context" : "In fused Lasso-Cox [Tibshirani et al. 2005], the coefficients and their successive differences are penalized using the `1-norm.",
      "startOffset" : 19,
      "endOffset" : 43
    }, {
      "referenceID" : 36,
      "context" : "In graphical Lasso-Cox [Friedman et al. 2008], the sparse graphs are estimated using coordinate descent method by applying a `1-penalty to the inverse covariance matrix.",
      "startOffset" : 23,
      "endOffset" : 45
    }, {
      "referenceID" : 115,
      "context" : "OSCAR-Cox: The modified graph Octagonal Shrinkage and Clustering Algorithm for Regression (OSCAR) [Yang et al. 2012; Ye and Liu 2012] regularizer is incorporated in the basic Cox model as the OSCAR-Cox algorithm [Vinzamuri and Reddy 2013], which can perform the variable selection for highly correlated features in regression problem.",
      "startOffset" : 98,
      "endOffset" : 133
    }, {
      "referenceID" : 67,
      "context" : "Let us assume that the number of instances is N with c censored observations and (N − c) uncensored observations, and use β = (β1, β2, · · · , βP ) as a general notation to denote the set of all parameters [Li et al. 2016e].",
      "startOffset" : 206,
      "endOffset" : 223
    }, {
      "referenceID" : 67,
      "context" : "the joint probability of the c censored observations [Li et al. 2016e].",
      "startOffset" : 53,
      "endOffset" : 70
    }, {
      "referenceID" : 59,
      "context" : "Some linear models [Miller and Halpern 1982; Koul et al. 1981; Buckley and James 1979; Wang et al. 2008; Li et al. 2016e] including Tobit regression and Buckley-James (BJ) regression were proposed to handle censored instances in survival analysis.",
      "startOffset" : 19,
      "endOffset" : 121
    }, {
      "referenceID" : 112,
      "context" : "Some linear models [Miller and Halpern 1982; Koul et al. 1981; Buckley and James 1979; Wang et al. 2008; Li et al. 2016e] including Tobit regression and Buckley-James (BJ) regression were proposed to handle censored instances in survival analysis.",
      "startOffset" : 19,
      "endOffset" : 121
    }, {
      "referenceID" : 67,
      "context" : "Some linear models [Miller and Halpern 1982; Koul et al. 1981; Buckley and James 1979; Wang et al. 2008; Li et al. 2016e] including Tobit regression and Buckley-James (BJ) regression were proposed to handle censored instances in survival analysis.",
      "startOffset" : 19,
      "endOffset" : 121
    }, {
      "referenceID" : 112,
      "context" : "[Wang et al. 2008] applied the elastic net regularizer in the BJ regression (EN-BJ).",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 67,
      "context" : "The weighted linear regression model with different regularizers for high-dimensional censored data is an efficient method to handle the censored data by giving different weights to different instances [Li et al. 2016e].",
      "startOffset" : 202,
      "endOffset" : 219
    }, {
      "referenceID" : 6,
      "context" : "In addition, the structured regularization based linear regression algorithm [Bach et al. 2012; Vinzamuri et al. 2017] for right censored data has a good ability to infer the underlying structure of the survival data.",
      "startOffset" : 77,
      "endOffset" : 118
    }, {
      "referenceID" : 110,
      "context" : "In addition, the structured regularization based linear regression algorithm [Bach et al. 2012; Vinzamuri et al. 2017] for right censored data has a good ability to infer the underlying structure of the survival data.",
      "startOffset" : 77,
      "endOffset" : 118
    }, {
      "referenceID" : 64,
      "context" : "— Weighted Regression: Weighted regression method [Li et al. 2016b] can be used when the constant variance assumption about the errors in the ordinary least squares regression methods is violated (which is called heteroscedasticity), which is",
      "startOffset" : 50,
      "endOffset" : 67
    }, {
      "referenceID" : 6,
      "context" : "This is also supported by the effectiveness of structured sparsity based regularization methods in regression [Bach et al. 2012].",
      "startOffset" : 110,
      "endOffset" : 128
    }, {
      "referenceID" : 110,
      "context" : "Structured regularization based LInear REgression algorithm for right Censored data (SLIREC) in [Vinzamuri et al. 2017] infers the underlying structure of the survival data directly using sparse inverse covariance estimation (SICE) method and uses the structural knowledge to guide the base linear regression model.",
      "startOffset" : 96,
      "endOffset" : 119
    }, {
      "referenceID" : 121,
      "context" : "Machine learning is effective when there are a large number of instances in a reasonable dimensional feature space, but this is not the case for certain problems in survival analysis [Zupan et al. 2000].",
      "startOffset" : 183,
      "endOffset" : 202
    }, {
      "referenceID" : 22,
      "context" : "The earliest attempt at using a tree structure for survival data was made in [Ciampi et al. 1981].",
      "startOffset" : 77,
      "endOffset" : 97
    }, {
      "referenceID" : 24,
      "context" : "[Ciampi et al. 1986] employed log-rank test statistics for between-node heterogeneity measures.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 23,
      "context" : "[Ciampi et al. 1987] proposed a likelihood ratio statistic to measure the dissimilarity between two nodes.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 37,
      "context" : "Using the Bayes theorem, there are two models, namely, Naı̈ve Bayes (NB) and Bayesian network (BN) [Friedman et al. 1997].",
      "startOffset" : 99,
      "endOffset" : 121
    }, {
      "referenceID" : 121,
      "context" : "Both of these approaches, which provide the probability of the event of interests as their outputs, are commonly studied in the context of clinical prediction [Kononenko 1993; Pepe 2003; Zupan et al. 2000].",
      "startOffset" : 159,
      "endOffset" : 205
    }, {
      "referenceID" : 86,
      "context" : "The experimental results of using Bayesian methods on survival data show that Bayesian methods have good properties of both interpretability and uncertainty reasoning [Raftery et al. 1995].",
      "startOffset" : 167,
      "endOffset" : 188
    }, {
      "referenceID" : 34,
      "context" : "Recently, the authors in [Fard et al. 2016] effectively integrate Bayesian methods with an AFT model by extrapolating the prior event probability to implement early stage prediction on survival data for the future time points.",
      "startOffset" : 25,
      "endOffset" : 43
    }, {
      "referenceID" : 69,
      "context" : "In [Lisboa et al. 2003], the authors proposed a Bayesian neural network framework to perform model selection for survival data using automatic relevance determination [MacKay 1995].",
      "startOffset" : 3,
      "endOffset" : 23
    }, {
      "referenceID" : 34,
      "context" : "More recently, in [Fard et al. 2016], the authors proposed a novel framework which combines the power of Bayesian network representation with the AFT model by extrapolating the prior probabilities to future time points.",
      "startOffset" : 18,
      "endOffset" : 36
    }, {
      "referenceID" : 7,
      "context" : "Although these extensions for Cox model allowed for preserving most of the advantages of a typical PH model, they were still not the optimal way to model the baseline variation [Baesens et al. 2005].",
      "startOffset" : 177,
      "endOffset" : 198
    }, {
      "referenceID" : 68,
      "context" : "(3) Many approaches [Liestbl et al. 1994; Biganzoli et al. 1998; Brown et al. 1997; Ravdin and Clark 1992; Lisboa et al. 2003] take the survival status of a subject, which can be represented by the survival or hazard probability, as the output of the neural network.",
      "startOffset" : 20,
      "endOffset" : 126
    }, {
      "referenceID" : 11,
      "context" : "(3) Many approaches [Liestbl et al. 1994; Biganzoli et al. 1998; Brown et al. 1997; Ravdin and Clark 1992; Lisboa et al. 2003] take the survival status of a subject, which can be represented by the survival or hazard probability, as the output of the neural network.",
      "startOffset" : 20,
      "endOffset" : 126
    }, {
      "referenceID" : 17,
      "context" : "(3) Many approaches [Liestbl et al. 1994; Biganzoli et al. 1998; Brown et al. 1997; Ravdin and Clark 1992; Lisboa et al. 2003] take the survival status of a subject, which can be represented by the survival or hazard probability, as the output of the neural network.",
      "startOffset" : 20,
      "endOffset" : 126
    }, {
      "referenceID" : 69,
      "context" : "(3) Many approaches [Liestbl et al. 1994; Biganzoli et al. 1998; Brown et al. 1997; Ravdin and Clark 1992; Lisboa et al. 2003] take the survival status of a subject, which can be represented by the survival or hazard probability, as the output of the neural network.",
      "startOffset" : 20,
      "endOffset" : 126
    }, {
      "referenceID" : 11,
      "context" : "The authors in [Biganzoli et al. 1998] apply the partial logistic artificial neural network (PLANN) method to analyze the relationship between the features and the survival times in order to obtain a better predictability of the model.",
      "startOffset" : 15,
      "endOffset" : 38
    }, {
      "referenceID" : 11,
      "context" : "Recently, feed-forward neural networks are used to obtain a more flexible non-linear model by considering the censored information in the data using a generalization of both continuous and discrete time models [Biganzoli et al. 1998].",
      "startOffset" : 210,
      "endOffset" : 233
    }, {
      "referenceID" : 69,
      "context" : "In [Lisboa et al. 2003], the PLANN was extended to a Bayesian neural framework with covariate-specific regularization to carry model selection using automatic relevance determination [MacKay 1995].",
      "startOffset" : 3,
      "endOffset" : 23
    }, {
      "referenceID" : 97,
      "context" : "However, the main disadvantage of this approach is that the order information included in the censored instances will be completely ignored [Shivaswamy et al. 2007].",
      "startOffset" : 140,
      "endOffset" : 164
    }, {
      "referenceID" : 41,
      "context" : "Another possible approach to handle the censored data is to use support vector classification using the constraint classification approach [Har-Peled et al. 2002] which imposes constraints in the SVM formulation for two comparable instances in order to maintain the required order.",
      "startOffset" : 139,
      "endOffset" : 162
    }, {
      "referenceID" : 106,
      "context" : "The work in [Van et al. 2007] studies a learning machine designed for predictive modeling of independently right censored survival data by introducing a health index which serves as a proxy between the instance’s covariates and the outcome.",
      "startOffset" : 12,
      "endOffset" : 29
    }, {
      "referenceID" : 105,
      "context" : "The authors in [Van et al. 2011] introduces a SVR based approach which combines the ranking and regression methods in the context of survival analysis.",
      "startOffset" : 15,
      "endOffset" : 32
    }, {
      "referenceID" : 54,
      "context" : "Relevance Vector Machine (RVM) [Widodo and Yang 2011; Kiaee et al. 2016], which obtains the parsimonious estimations for regression and probabilistic problems using Bayesian inference, has the same formulation as SVM but provides probabilistic classification.",
      "startOffset" : 31,
      "endOffset" : 72
    }, {
      "referenceID" : 47,
      "context" : "In bagging survival trees, the aggregated survival function can be calculated by averaging the predictions made by a single survival tree instead of taking a majority vote [Hothorn et al. 2004].",
      "startOffset" : 172,
      "endOffset" : 193
    }, {
      "referenceID" : 48,
      "context" : "Random survival forest (RSF) [Ishwaran et al. 2008] extended Breiman’s random forest method by using a forest of survival trees for prediction.",
      "startOffset" : 29,
      "endOffset" : 51
    }, {
      "referenceID" : 49,
      "context" : "In addition, the authors in [Ishwaran et al. 2011] provide an effective way to apply RSF for highdimensional survival analysis problems by regularizing forests.",
      "startOffset" : 28,
      "endOffset" : 50
    }, {
      "referenceID" : 46,
      "context" : "dient descent algorithm [Hothorn et al. 2006; Bühlmann and Hothorn 2007].",
      "startOffset" : 24,
      "endOffset" : 72
    }, {
      "referenceID" : 46,
      "context" : "The authors in [Hothorn et al. 2006] extend the gradient boosting algorithm to minimize the",
      "startOffset" : 15,
      "endOffset" : 36
    }, {
      "referenceID" : 109,
      "context" : "The feedback from the expert is particularly useful for improving the model in many real-world application domains [Vinzamuri et al. 2014].",
      "startOffset" : 115,
      "endOffset" : 138
    }, {
      "referenceID" : 109,
      "context" : "In [Vinzamuri et al. 2014], the active regularized Cox regression (ARC) algorithm based on a discriminative gradient sampling strategy is proposed by integrating the active learning method with the Cox model.",
      "startOffset" : 3,
      "endOffset" : 26
    }, {
      "referenceID" : 66,
      "context" : "Recently, in [Li et al. 2016c], a regularized Cox PH model named Transfer-Cox, is proposed to improve the prediction performance of the Cox model in the target domain through knowledge transfer from the source domain in the context of survival models built on multiple high-dimensional datasets.",
      "startOffset" : 13,
      "endOffset" : 30
    }, {
      "referenceID" : 65,
      "context" : "In [Li et al. 2016d], the survival time prediction problem is reformulated as a multi-task learning problem.",
      "startOffset" : 3,
      "endOffset" : 20
    }, {
      "referenceID" : 43,
      "context" : "This can be done by computing the concordance probability or the concordance index (C-index) [Harrell et al. 1984; Harrell et al. 1982; Pencina and D’Agostino 2004].",
      "startOffset" : 93,
      "endOffset" : 164
    }, {
      "referenceID" : 42,
      "context" : "This can be done by computing the concordance probability or the concordance index (C-index) [Harrell et al. 1984; Harrell et al. 1982; Pencina and D’Agostino 2004].",
      "startOffset" : 93,
      "endOffset" : 164
    }, {
      "referenceID" : 100,
      "context" : "The survival times of two instances can be ordered for two scenarios: (1) both of them are uncensored; (2) the observed event time of the uncensored instance is smaller than the censoring time of the censored instance [Steck et al. 2008].",
      "startOffset" : 218,
      "endOffset" : 237
    }, {
      "referenceID" : 65,
      "context" : "By this definition, for the binary prediction problem, C-index will have a similar meaning to the regular area under the ROC curve (AUC), and if yi is binary, then the C-index is the AUC [Li et al. 2016d].",
      "startOffset" : 187,
      "endOffset" : 204
    }, {
      "referenceID" : 40,
      "context" : "Brier score was extended in [Graf et al. 1999] to be a performance measure for survival problems with censored information to evaluate the prediction models where the outcome to be predicted is either binary or categorical in nature.",
      "startOffset" : 28,
      "endOffset" : 46
    }, {
      "referenceID" : 34,
      "context" : "To solve this problem, an Early Stage Prediction (ESP) approach trained at early stages of survival analysis studies to predict the time-to-event is proposed in [Fard et al. 2016].",
      "startOffset" : 161,
      "endOffset" : 179
    }, {
      "referenceID" : 30,
      "context" : "sored instances [Delen et al. 2005; Burke et al. 1997].",
      "startOffset" : 16,
      "endOffset" : 54
    }, {
      "referenceID" : 20,
      "context" : "sored instances [Delen et al. 2005; Burke et al. 1997].",
      "startOffset" : 16,
      "endOffset" : 54
    }, {
      "referenceID" : 121,
      "context" : "(1) Group the instances in the given data into three categorizes [Zupan et al. 2000]: (i) instances which experience the event of interest during the observation will be labeled as event; (ii) instances whose censored time is later than a predefined time point are labeled as event-free; (iii) for instances whose censored time is earlier than a predefined time point, a copy of these instances will be labeled as event and another copy of the same instances will be labeled as event-free, respectively, and all these instances will be weighted by a marginal probability of event occurrence estimated by the Kaplan-Meier method.",
      "startOffset" : 65,
      "endOffset" : 84
    }, {
      "referenceID" : 34,
      "context" : "(2) For each censored instance, estimate the probability of event and probability of being censored (considering censoring as a new event) using Kaplan-Meier estimator and give a new class label based on these probability values [Fard et al. 2016].",
      "startOffset" : 229,
      "endOffset" : 247
    }, {
      "referenceID" : 110,
      "context" : "In [Vinzamuri et al. 2017], a calibration survival analysis method which uses a regularized inverse covariance based imputation is proposed to overcome the problems mentioned above.",
      "startOffset" : 3,
      "endOffset" : 26
    }, {
      "referenceID" : 85,
      "context" : "Cumulative Incidence Curve (CIC) Approach: To avoid the questionable interpretation problem, the cumulative incidence curve [Putter et al. 2007] is one of the main approaches for competing risks which estimates the marginal probability of each event q.",
      "startOffset" : 124,
      "endOffset" : 144
    }, {
      "referenceID" : 3,
      "context" : "Typically, if all the recurring events for each instance are of the same type, the counting process (CP) algorithm [Andersen et al. 2012] can be used to tackle this problem.",
      "startOffset" : 115,
      "endOffset" : 137
    }, {
      "referenceID" : 84,
      "context" : "Stratified Cox: Stratified CP [Prentice et al. 1981], Marginal [Wei et al.",
      "startOffset" : 30,
      "endOffset" : 52
    }, {
      "referenceID" : 113,
      "context" : "1981], Marginal [Wei et al. 1989] and Gap Time [Prentice et al.",
      "startOffset" : 16,
      "endOffset" : 33
    }, {
      "referenceID" : 84,
      "context" : "1989] and Gap Time [Prentice et al. 1981] are three approaches using stratified Cox method to differentiate the event occurrence order.",
      "startOffset" : 19,
      "endOffset" : 41
    }, {
      "referenceID" : 77,
      "context" : "[Lyu 1996] [Modarres et al. 2009] Device failure Likelihood of a device being failed within t days.",
      "startOffset" : 11,
      "endOffset" : 33
    }, {
      "referenceID" : 89,
      "context" : "[Rakesh et al. 2016] [Li et al.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 63,
      "context" : "2016] [Li et al. 2016a] Project success Likelihood of a project being successful within t days.",
      "startOffset" : 6,
      "endOffset" : 23
    }, {
      "referenceID" : 65,
      "context" : "[Li et al. 2016d] [Beer et al.",
      "startOffset" : 0,
      "endOffset" : 17
    }, {
      "referenceID" : 78,
      "context" : "[Murtaugh et al. 1999] [Ameri et al.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 2,
      "context" : "1999] [Ameri et al. 2016] Student dropout Likelihood of a student being dropout within t days.",
      "startOffset" : 6,
      "endOffset" : 25
    }, {
      "referenceID" : 118,
      "context" : "[Zeithaml et al. 2001] [Berger and Nasr 1998] Purchase behavior Likelihood of a customer purchasing from a given service supplier within t days.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 117,
      "context" : "[Yin et al. 2013] [Barbieri et al.",
      "startOffset" : 0,
      "endOffset" : 17
    }, {
      "referenceID" : 8,
      "context" : "2013] [Barbieri et al. 2016] User clicking Likelihood of a user clicking the advertisement within time t.",
      "startOffset" : 6,
      "endOffset" : 28
    }, {
      "referenceID" : 77,
      "context" : "Reliability prediction focuses on developing methods which are good at accurately estimating the reliability of the new products [Modarres et al. 2009; Lyu 1996].",
      "startOffset" : 129,
      "endOffset" : 161
    }, {
      "referenceID" : 88,
      "context" : "Although the crowdfunding platforms have been successful, the percentage of the projects which achieved their desired goal amount is less than 50% [Rakesh et al. 2015].",
      "startOffset" : 147,
      "endOffset" : 167
    }, {
      "referenceID" : 63,
      "context" : "The need to estimate the project success probability motivates the development of new prediction approaches which can integrate the advantages of both regression (for estimating the time for success) and classification (for considering both successful and failed projects simultaneously in the model) [Li et al. 2016a].",
      "startOffset" : 301,
      "endOffset" : 318
    }, {
      "referenceID" : 63,
      "context" : "The authors in [Li et al. 2016a] consider both the failed and successful projects simultaneously by using censored regression methods.",
      "startOffset" : 15,
      "endOffset" : 32
    }, {
      "referenceID" : 65,
      "context" : "In recent years, multiple studies [Li et al. 2016d; Beer et al. 2002] have correlated gene expression with survival outcomes in cancer applications in a genome-wide scale.",
      "startOffset" : 34,
      "endOffset" : 69
    }, {
      "referenceID" : 4,
      "context" : "The authors in [Antonov et al. 2014] developed",
      "startOffset" : 15,
      "endOffset" : 36
    }, {
      "referenceID" : 78,
      "context" : "Survival analysis has success in student retention problem [Murtaugh et al. 1999; Ameri et al. 2016].",
      "startOffset" : 59,
      "endOffset" : 100
    }, {
      "referenceID" : 2,
      "context" : "Survival analysis has success in student retention problem [Murtaugh et al. 1999; Ameri et al. 2016].",
      "startOffset" : 59,
      "endOffset" : 100
    }, {
      "referenceID" : 118,
      "context" : "Customer lifetime value (LTV) [Berger and Nasr 1998; Zeithaml et al. 2001] of a customer refers to the profit that the customer brings to the store based on the purchase history.",
      "startOffset" : 30,
      "endOffset" : 74
    }, {
      "referenceID" : 94,
      "context" : "Identifying loyal customers using LTV estimation has been studied by various researchers [Rosset et al. 2003; Mani et al. 1999] using survival analysis methods and data mining approaches which are helpful in identifying the purchase patterns.",
      "startOffset" : 89,
      "endOffset" : 127
    }, {
      "referenceID" : 73,
      "context" : "Identifying loyal customers using LTV estimation has been studied by various researchers [Rosset et al. 2003; Mani et al. 1999] using survival analysis methods and data mining approaches which are helpful in identifying the purchase patterns.",
      "startOffset" : 89,
      "endOffset" : 127
    }, {
      "referenceID" : 8,
      "context" : "Nowadays, many free web services, including online news portals, search engines and social networks present users with advertisements [Barbieri et al. 2016].",
      "startOffset" : 134,
      "endOffset" : 156
    }, {
      "referenceID" : 92,
      "context" : "Both the topics and the display orders of the ads will affect the user clicking probability [Richardson et al. 2007].",
      "startOffset" : 92,
      "endOffset" : 116
    }, {
      "referenceID" : 8,
      "context" : "The CTR value indicates the attraction effect of the ad to the users [Barbieri et al. 2016].",
      "startOffset" : 69,
      "endOffset" : 91
    } ],
    "year" : 2017,
    "abstractText" : "Survival analysis is a subfield of statistics where the goal is to analyze and model the data where the outcome is the time until the occurrence of an event of interest. One of the main challenges in this context is the presence of instances whose event outcomes become unobservable after a certain time point or when some instances do not experience any event during the monitoring period. Such a phenomenon is called censoring which can be effectively handled using survival analysis techniques. Traditionally, statistical approaches have been widely developed in the literature to overcome this censoring issue. In addition, many machine learning algorithms are adapted to effectively handle survival data and tackle other challenging problems that arise in real-world data. In this survey, we provide a comprehensive and structured review of the representative statistical methods along with the machine learning techniques used in survival analysis and provide a detailed taxonomy of the existing methods. We also discuss several topics that are closely related to survival analysis and illustrate several successful applications in various real-world application domains. We hope that this paper will provide a more thorough understanding of the recent advances in survival analysis and offer some guidelines on applying these approaches to solve new problems that arise in applications with censored data.",
    "creator" : "LaTeX with hyperref package"
  }
}