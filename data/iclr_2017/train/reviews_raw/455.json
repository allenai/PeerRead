{"conference": "ICLR 2017 conference submission", "title": "Energy-based Generative Adversarial Networks", "abstract": "We introduce the \"Energy-based Generative Adversarial Network\" model (EBGAN) which views the discriminator as an energy function that attributes low energies to the regions near the data manifold and higher energies to other regions. Similar to the probabilistic GANs, a generator is seen as being trained to produce contrastive samples with minimal energies, while the discriminator is trained to assign high energies to these generated samples. Viewing the discriminator as an energy function allows to use a wide variety of architectures and loss functionals in addition to the usual binary classifier with logistic output. Among them, we show one instantiation of EBGAN framework as using an auto-encoder architecture, with the energy being the reconstruction error, in place of the discriminator. We show that this form of EBGAN exhibits more stable behavior than regular GANs during training. We also show that a single-scale architecture can be trained to generate high-resolution images.", "reviews": [{"is_meta_review": true, "comments": "This paper is a parallel work to Improving Generative Adversarial Networks with Denoising Feature Matching. The main solution of both papers is introducing autoencoder into discriminator to improve the stability and quality of GAN. Different to Denoising Feature Matching, EBGAN uses encoder-decoder instead of denoising only, and use hingle loss to replace original loss function.\n\nThe theoretical results are good, and empirical result of high resolution image is unique among all recent GAN advantages.\n\nI suggest to introduce Improving Generative Adversarial Networks with Denoising Feature Matching as related work.", "IS_META_REVIEW": true}, {"TITLE": "ICLR committee final decision", "OTHER_KEYS": "ICLR 2017 pcs", "comments": "The authors have proposed an energy-based rendition of probabilistic GANs, with the addition of auto-encoder and hinge loss to improve stability. Theoretical results involving the Nash equilibrium are also given. Solid paper, well-written. Novel contribution with good empirical and theoretical justification.", "IS_META_REVIEW": false, "DATE": "06 Feb 2017", "is_meta_review": false}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "No Title", "is_meta_review": false, "comments": "This paper is a parallel work to Improving Generative Adversarial Networks with Denoising Feature Matching. The main solution of both papers is introducing autoencoder into discriminator to improve the stability and quality of GAN. Different to Denoising Feature Matching, EBGAN uses encoder-decoder instead of denoising only, and use hingle loss to replace original loss function.\n\nThe theoretical results are good, and empirical result of high resolution image is unique among all recent GAN advantages.\n\nI suggest to introduce Improving Generative Adversarial Networks with Denoising Feature Matching as related work.", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "20 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Review", "is_meta_review": false, "comments": "This paper introduces an energy-based Generative Adversarial Network (GAN) and provides theoretical and empirical results modeling a number of image datasets (including large-scale versions of categories of ImageNet). As far as I know energy-based GANs (EBGAN) were introduced in Kim and Bengio (2016), but the proposed version makes a number of different design choices. \n\nFirst, it does away with the entropy regularization term that Kim and Bengio (2016) introduced to ensure that the GAN discriminator converged to an energy function proportional to the log density of the data (at optimum). This implies that the discriminator in the proposed scheme will become uniform at convergence as discussed in the theoretical section of the paper, however the introductory text seems to imply otherwise -- that one could recover a meaningful score function from the trained energy-function (discriminator). This should be clarified. \n\nSecond, this version of the EBGAN setting includes two innovations: (1) the introduction of the hinge loss in the value function, and (2) the use of an auto-encoder parametrization for the energy function. These innovations are not empirically justified in any way - this is disappointing, as it would be really good to see empirical results supporting the arguments made in support of their introduction. \n\nThe two significant contributions of this paper are the theoretical analysis of the energy-baesd GAN formalism (showing that the optimum corresponds to a Nash equilibrium) and the impressive empirical results on large images that set a new standard in what straight GAN-style models can achieve.\n\nThe theoretical results seem solid to me and make a nice contribution.\n\nRegarding the quantitative results in Table 2, it seems not appropriate to bold the EBGAN line when it seems to be statistically indistinguishable form the Rasmus et al (2015) results. Though it is not mentioned here, the use of bold typically indicates the state-of-the-art. \n\nI think this paper could be be much stronger if the two novel contributions to the energy-based GAN setting were more thoroughly explored with ablation experiments. That being said, I think this paper has already become a contribution other are building on (including at least two other ICLR submissions) and so I think it should be accepted for publication at ICLR. \n", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"TITLE": "Questions", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "", "IS_META_REVIEW": false, "DATE": "17 Dec 2016", "is_meta_review": false}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Review", "is_meta_review": false, "comments": "This paper proposes a novel extension of generative adversarial networks that replaces the traditional binary classifier discriminator with one that assigns a scalar energy to each point in the generator's output domain. The discriminator minimizes a hinge loss while the generator attempts to generate samples with low energy under the discriminator. The authors show that a Nash equilibrium under these conditions yields a generator that matches the data distribution (assuming infinite capacity). Experiments are conducted with the discriminator taking the form of an autoencoder, optionally including a regularizer that penalizes generated samples having a high cosine similarity to other samples in the minibatch.\n\nPros:\n* The paper is well-written.\n* The topic will be of interest to many because it sets the stage for the exploration of a wider variety of discriminators than currently used for training GANs.\n* The theorems regarding optimality of the Nash equilibrium appear to be correct.\n* Thorough exploration of hyperparameters in the MNIST experiments.\n* Semi-supervised results show that contrastive samples from the generator improve classification performance.\n\nCons:\n* The relationship to other works that broaden the scope of the discriminator (e.g. [1]) or use a generative network to provide contrastive samples to an energy-based model ([2]) is not made clear in the paper.\n* From visual inspection alone it is difficult to conclude whether EB-GANs produce better samples than DC-GANs on the LSUN and CelebA datasets.\n* It is difficult to assess the effect of the PT regularizer beyond visual inspection as the Inception score results are computed with the vanilla EB-GAN.\n\nSpecific Comments\n* Sec 2.3: It is unclear to me why a reconstruction loss will necessarily produce very different gradient directions.\n* Sec 2.4: It is confusing that \"pulling-away\" is abbreviated as \"PT\".\n* Sec 4.1: It seems strange that the Inception model (trained on natural images) is being used to compute KL scores for MNIST. Using an MNIST-trained CNN to compute Inception-style scores seems to be more appropriate here.\n* Figure 3: There is little variation across the histograms, so this figure is not very enlightening.\n* Appendix A: In the proof of theorem 2, it is unclear to me why a Nash equilibrium of the system exists.\n\nTypos / Minor Comments\n* Abstract: \"probabilistic GANs\" should probably be \"traditional\" or \"classical\" GANs.\n* Theorem 2: \"A Nash equilibrium ... exists\"\n* Sec 3: Should be \"Several papers were presented\"\n\nOverall, I have some concerns with the related work and experimental evaluation sections, but I feel the model is novel enough and is well-justified by the optimality proofs and the quality of the generated samples.\n\n[1] Springenberg, Jost Tobias. \"Unsupervised and Semi-supervised Learning with Categorical Generative Adversarial Networks.\" arXiv preprint arXiv:1511.06390 (2015).\n[2] Kim, Taesup, and Yoshua Bengio. \"Deep Directed Generative Models with Energy-Based Probability Estimation.\" arXiv preprint arXiv:1606.03439 (2016).", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Preview question", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "", "IS_META_REVIEW": false, "DATE": "03 Dec 2016", "is_meta_review": false}, {"TITLE": "Pre-review questions", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "", "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "is_meta_review": false}, {"TITLE": "Motivation of Using Hinge Loss", "OTHER_KEYS": "Yuchen Lu", "comments": "Hi Junbo,\n\nThis is an interesting paper with appealing samples and thorough comparisons with normal GAN, but I am not sure what is the motivation behind using hinge loss. It seems to be an arbitrary choice since I don't see any explanation in the paper. Does the experimental and theoretical result still hold for other penalty functions?\n\nThanks,\nYuchen", "IS_META_REVIEW": false, "DATE": "30 Nov 2016", "is_meta_review": false}], "SCORE": 8, "authors": "Junbo Zhao, Michael Mathieu, Yann LeCun", "KEYWORDS": "We introduce the \"Energy-based Generative Adversarial Network\" (EBGAN) model.", "accepted": true, "id": ""}
