{
  "name" : "1706.04317.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Schema Networks: Zero-shot Transfer with a Generative Causal Model of Intuitive Physics",
    "authors" : [ "Ken Kansky", "Tom Silver", "David A. Mély", "Mohamed Eldawy", "Miguel Lázaro-Gredilla", "Xinghua Lou", "Nimrod Dorfman", "Szymon Sidor", "Scott Phoenix", "Dileep George" ],
    "emails" : [ "<ken@vicarious.com>,", "<tom@vicarious.com>." ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "A longstanding ambition of research in artificial intelligence is to efficiently generalize experience in one scenario to other similar scenarios. Such generalization is essential for an embodied agent working to accomplish a variety of goals in a changing world. Despite remarkable progress on individual tasks like Atari 2600 games (Mnih et al., 2015; Van Hasselt et al., 2016; Mnih et al., 2016) and Go (Silver et al., 2016a), the ability of state-of-the-art models to transfer learning from one environment to the next remains lim-\nAll authors affiliated with Vicarious AI, California, USA. Correspondence to: Ken Kansky <ken@vicarious.com>, Tom Silver <tom@vicarious.com>.\nCopyright 2017 by the author(s).\nFigure 1. Variations of Breakout. From top left: standard version, middle wall, half negative bricks, offset paddle, random target, and juggling. After training on the standard version, Schema Networks are able to generalize to the other variations without any additional training.\nited. For instance, consider the variations of Breakout illustrated in Fig. 1. In these environments the positions of objects are perturbed, but the object movements and sources of reward remain the same. While humans have no trouble generalizing experience from the basic Breakout to its variations, deep neural network-based models are easily fooled (Taylor & Stone, 2009; Rusu et al., 2016).\nThe model-free approach of deep reinforcement learning (Deep RL) such as the Deep-Q Network and its descendants is inherently hindered by the same feature that makes it desirable for single-scenario tasks: it makes no assumptions about the structure of the domain. Recent work has suggested how to overcome this deficiency by utilizing object-based representations (Diuk et al., 2008; Usunier et al., 2016). Such a representation is motivated by the\nar X\niv :1\n70 6.\n04 31\n7v 2\n[ cs\n.A I]\n1 7\nA ug\n2 01\n7\nwell-acknowledged Gestalt principle, which states that the ability to perceive objects as a bounded figure in front of an unbounded background is fundamental to all perception (Weiten, 2012). Battaglia et al. (2016) and Chang et al. (2016) go further, learning relations to predict object interactions.\nWhile object-based and relational representations have shown great promise alone, they stop short of modeling causality – the ability to reason about previous observations and explain away alternative causes. A causal model is essential for regression planning, in which an agent works backward from a desired future state to produce a plan (Anderson, 1990). Reasoning backward and allowing for multiple causation requires a framework like Probabilistic Graphical Models (PGMs), which can natively support explaining away (Koller & Friedman, 2009).\nHere we introduce Schema Networks – a generative model for object-oriented reinforcement learning and planning1. Schema Networks incorporate key desiderata for the flexible and compositional transfer of learned prior knowledge to new settings. 1) Knowledge is represented with “schemas” – local cause-effect relationships involving one or more object entities; 2) In a new setting, these causeeffect relationships are traversed to guide action selection; and 3) The representation deals with uncertainty, multiplecausation, and explaining away in a principled way. We first describe the representational framework and learning algorithms and then demonstrate how action policies can be generated by treating planning as inference in a factor graph. We evaluate the end-to-end system on Breakout variations and compare against Asynchronous Advantage Actor-Critic (A3C) (Mnih et al., 2016) and Progressive Networks (PNs) (Rusu et al., 2016), the latter of which extends A3C explicitly to handle transfer. We show that the structure of the Schema Network enables efficient and robust generalization beyond these Deep RL models."
    }, {
      "heading" : "2. Related Work",
      "text" : "The field of reinforcement learning has witnessed significant progress with the recent adaptation of deep learning methods to traditional frameworks like Q-learning. Since the introduction of the Deep Q-network (DQN) (Mnih et al., 2015), which uses experience replay to achieve human-level performance on a set of Atari 2600 games, several innovations have enabled faster convergence and better performance with less memory. The asynchronous methods introduced by Mnih et al. (2016) exploit multiple agents acting in copies of the same environment, combining their experiences into one model. As the Asyn-\n1We borrow the term “schema” from Drescher (1991), whose schema mechanism inspired the early development of our model.\nchronous Advantage Actor-Critic (A3C) is the best among these methods, we use it as our primary comparison.\nModel-free Deep RL models like A3C are unable to substantially generalize beyond their training experience (Jaderberg et al., 2016; Rusu et al., 2016). To address this limitation, recent work has attempted to introduce more structure into neural network-based models. The Interaction Network (Battaglia et al., 2016) (IN) and the Neural Physics Engine (NPE) (Chang et al., 2016) use object-level and pairwise relational representations to learn models of intuitive physics. The primary advantage of these models is their amenability to gradient-based methods, though such techniques might be applied to Schema Networks as well. Schema Networks offer two key advantages: latent physical properties and relations need not be hardcoded, and planning can make use of backward search, since the model can distinguish different causes. Furthermore, neither INs nor NPEs have been applied in RL domains. Progress in model-based Deep RL has thus far been limited, though methods like Embed to Control (Watter et al., 2015), Value Iteration Networks (Tamar et al., 2016), and the Predictron (Silver et al., 2016b) demonstrate the promise of this direction. However, these approaches do not exploit the objectrelational representation of INs or NPEs, nor do they incorporate a backward model for regression planning.\nSchema Networks build upon the ideas of the ObjectOriented Markov Decision Process (OO-MDP) introduced by Diuk et al. (2008) (see also Scholz et al. (2014)). Related frameworks include relational and first-order logical MDPs (Guestrin et al., 2003a). These formalisms, which harken back to classical AI’s roots in symbolic reasoning, are designed to enable robust generalization. Recent work by Garnelo et al. (2016) on “deep symbolic reinforcement learning” makes this connection explicit, marrying firstorder logic with deep RL. This effort is similar in spirit to our work with Schema Networks, but like INs and NPEs, it lacks a mechanism to learn disentangled causes of the same effect and cannot perform regression planning.\nSchema Networks transfer experience from one scenario to other similar scenarios that exhibit repeatable structure and sub-structure (Taylor & Stone, 2009). Rusu et al. (2016) show how A3C can be augmented to similarly exploit common structure between tasks via Progressive Networks (PNs). A PN is constructed by successively training copies of A3C on each task of interest. With each new task, the existing network is frozen, another copy of A3C is added, and lateral connections between the frozen network and the new copy are established to facilitate transfer of features learned during previous tasks. One obvious limitation of PNs is that the number of network parameters must grow quadratically with the number of tasks. However, even if this growth rate was improved, the PN would\nstill be unable to generalize from biased training data without continuing to learn on the test environment. In contrast, Schema Networks exhibit zero-shot transfer.\nSchema Networks are implemented as probabilistic graphical models (PGMs), which provide practical inference and structure learning techniques. Additionally, inference with uncertainty and explaining away are naturally supported by PGMs. We direct the readers to (Koller & Friedman, 2009) and (Jordan, 1998) for a thorough overview of PGMs. In particular, early work on factored MDPs has demonstrated how PGMs can be applied in RL and planning settings (Guestrin et al., 2003b)."
    }, {
      "heading" : "3. Schema Networks",
      "text" : ""
    }, {
      "heading" : "3.1. MDPs and Notation",
      "text" : "The traditional formalism for the Reinforcement Learning problem is the Markov Decision Process (MDP). An MDP M is a five-tuple (S,A, T,R, γ), where S is a set of states, A is a set of actions, T (s(t+1)|s(t), a(t)) is the probability of transitioning from state s(t) ∈ S to s(t+1) ∈ S after action a(t) ∈ A, R(r(t+1)|s(t), a(t)) is the probability of receiving reward r(t+1) ∈ R after executing action a(t) while in state s(t), and γ ∈ [0, 1] is the rate at which future rewards are exponentially discounted."
    }, {
      "heading" : "3.2. Model Definition",
      "text" : "A Schema Network is a structured generative model of an MDP. We first describe the architecture of the model informally. An image input is parsed into a list of entities, which may be thought of as instances of objects in the sense of OO-MDPs (Diuk et al., 2008). All entities share the same collection of attributes. We refer to a specific attribute of a specific entity as an entity-attribute, which is represented as a binary variable to indicate the presence of that attribute for an entity. An entity state is an assignment of states to all attributes of the entity, and the complete model state is the set of all entity states.\nA grounded schema is a binary variable associated with a particular entity-attribute in the next timestep, whose value depends on the present values of a set of binary entity-attributes. The event that one of these present entityattributes assumes the value 1 is called a precondition of the grounded schema. When all preconditions of a grounded schema are satisfied, we say that the schema is active, and it predicts the activation of its associated entity-attribute. Grounded schemas may also predict rewards and may be conditioned on actions, both of which are represented as binary variables. For instance, a grounded schema might define a distribution over Entity 1’s “position” attribute at time 5, conditioned on Entity 2’s “position” attribute at time 4 and the action “UP” at time 4. Grounded schemas\nare instantiated from ungrounded schemas, which behave like templates for grounded schemas to be instantiated at different times and in different combinations of entities. For example, an ungrounded schema could predict the “position” attribute of Entity x at time t + 1 conditioned on the “position” of Entity y at time t and the action “UP” at time t; this ungrounded schema could be instantiated at time t = 4 with x = 1 and y = 2 to create the grounded schema described above. In the case of attributes like “position” that are inherently continuous or categorical, several binary variables may be used to discretely approximate the distribution (see the smaller nodes in Figure 2). A Schema Network is a factor graph that contains all grounded instantiations of a set of ungrounded schemas over some window of time, illustrated in Figure 2.\nWe now formalize the Schema Network factor graph. For simplicity, suppose the number of entities and the number of attributes are fixed at N and M respectively. Let Ei refer to the ith entity and let α (t) i,j refer to the j\nth attribute value of the ith entity at time t. We use the notation E\n(t) i = (α (t) i,1, ..., α (t) i,M ) to refer to the state of the i th en-\ntity at time t. The complete state of the MDP modeled by the network at time t is then s(t) = (E(t)1 , ..., E (t) N ). Actions and rewards are also represented with sets of binary variables, denoted a(t) and r(t+1) respectively. A Schema Network for time t will contain the variables in s(t), a(t), s(t+1), and r(t+1).\nLet φk denote the variable for grounded schema k. φk is bound to a specific entity-attribute αi,j and activates it when the schema is active. Multiple grounded schemas can predict the same attribute, and these predictions are combined through an OR gate. For binary variables v1, ..., vn, let AND(v1, ..., vn) = ∏n i=1 P (vi = 1),\nand OR(v1, ..., vn) = 1 − ∏n\ni=1(1 − P (vi = 1)). A grounded schema is connected to its precondition entity-attributes with an AND factor, written as φk = AND(αi1,j1 , ..., αiH ,jH , a) forH entity-attribute preconditions and an optional action a. There is no restriction on how many entities or attributes from a single entity can be preconditions of a grounded schema.\nAn ungrounded schema (or template) is represented as Φl(Ex1 , ..., ExH ) = AND(αx1,y1 , αx1,y2 ..., αxH ,yH ), where xh determines the relative entity index of the h-th precondition and yh determines which attribute variable is the precondition. The ungrounded schema is a template that can be bound to multiple specific entities and locations to generate grounded schemas.\nA subset of attributes corresponds to discrete positions. These attributes are treated differently from all others, whose semantic meanings are unknown to the model. When a schema predicts a movement to a new position, we must inform the previously active position attribute to be inactive unless there is another schema that predicts it to remain active. We introduce a self-transition variable to represent the probability that a position attribute will remain active in the next time step when no schema predicts a change from that position. We compute the self-transition variable as Λi,j = AND(¬φ1, ...,¬φk, si,j) for entity i and position attribute j, where the set φ1...φk includes all schemas that predict the future position of the same entity i and include si,j as a precondition.\nWith these terms defined, we may now compute the transition function, which can be factorized as T (s(t+1)|s(t), a(t)) = ∏N i=1 ∏M j=1 Ti,j(s (t+1) i,j |s(t), a(t)). An entity-attribute is active at the next time step if either a schema predicts it to be active or if its self-transition variable is active: Ti,j(s (t+1) i,j |s(t)) = OR(φk1 , ..., φkQ ,Λi,j), where k1...kQ are the indices of all grounded schemas that predict si,j ."
    }, {
      "heading" : "3.3. Construction of Entities and Attributes",
      "text" : "In practice we assume that a vision system is responsible for detecting and tracking entities in an image. It is therefore largely up to the vision system to determine what constitutes an entity. Essentially any trackable image feature could be an entity, which most typically includes objects, their boundaries, and their surfaces. Recent work has demonstrated one possible method for unsupervised entity construction using autoencoders (Garnelo et al., 2016). Depending on the task, Schema Networks could learn to reason flexibly at different levels of representation. For example, using entities from surfaces might be most relevant for predicting collisions, while using one entity per object might be most relevant for predicting whether it can be controlled by an action. The experiments in this paper utilize surface entities, described further in Section 5.\nSimilarly, entity attributes can be provided by the vision system, and these attributes typically include: color/appearance, surface/edge orientation, object category, or part-of an object category (e.g. front-left tire). For simplicity we here restrict the entities to have fully observable attributes, but in general they could have latent attributes such as “bounciness” or “magnetism.”"
    }, {
      "heading" : "3.4. Connections to Existing Models",
      "text" : "Schema Networks are closely related to Object-Oriented MDPs (OO-MDPs) (Diuk et al., 2008) and Relational MDPs (R-MDPs) (Guestrin et al., 2003a). However, neither OO-MDPs nor R-MDPs define a transition function with an explicit OR of possible causes, and traditionally transition functions have not been learned in these models. In contrast, Schema Networks provide an explicit OR to reason about multiple causation, which enables regression planning. Additionally, the structure of Schema Networks is amenable to efficient learning.\nSchema Networks are also related to the recently proposed Interaction Network (IN) (Battaglia et al., 2016) and Neural Physics Engine (NPE) (Chang et al., 2016). At a high level, INs, NPEs, and Schema Networks are much alike – objects are to entities as relations are to schemas. However, neither INs nor NPEs are generative and hence do not support regression planning from a goal through causal chains. Because Schema Networks are generative models, they support more flexible inference and search strategies for planning. Additionally, the learned structures in Schema Networks are amenable to human interpretation, explicitly factorizing different causes, making prediction errors easier to relate to the learned model parameters."
    }, {
      "heading" : "4. Learning and Planning in Schema Networks",
      "text" : "In this section we describe how to train Schema Networks (i.e., learn its structure) from interactions with an environment, as well as how they can be used to perform planning. Planning is not only necessary at test time to maximize reward, but also can be used to improve exploration during the training procedure."
    }, {
      "heading" : "4.1. Training Procedure",
      "text" : "Given a series of actions, rewards and images, we represent each possible action and reward with a binary variable, and we convert each image into a set of entity states. The number of entities is allowed to vary between adjacent frames, accounting for objects appearing or moving out of view.\nGiven a dataset of entity states over time, we preprocess the entity states into a representation that is more convenient for learning. For N entities observed over T timesteps, we wish to predict α(t)i,j on the basis of the attribute values of the ith entity and its spatial neighbors at time t − 1 (for 1 ≤ i ≤ N and 2 ≤ t ≤ T ). The attribute values of E\n(t−1) i and its neighbors can be represented as a row vector of length MR, where M is the number of attributes and R − 1 is the number of neighbor positions of each entity, determined by a fixed radius. Let X ∈ {0, 1}D×D′ be the arrangement of all such vectors into a binary matrix, with D = NT and D′ = MR. Let y ∈ {0, 1}D be a binary vector such that if row r in X refers to E(t−1)i , then yr = α (t) i,j . Schemas are then learned to predict y from X using the method described in Section 4.2.\nWhile gathering data, actions are chosen by planning using the schemas that have been learned so far. This planning algorithm is described in Section 4.3. We use an ε-greedy approach to encourage exploration, taking a random action at each timestep with small probability. We found no need to perform any additional policy learning, and after convergence predictions were accurate enough to allow for successful planning. As shown in Section 5, since learning only involves understanding the dynamics of the game, transfer learning is simplified and there is no need for policy adaptation."
    }, {
      "heading" : "4.2. Schema Learning",
      "text" : "Structure learning in graphical models is a well studied topic in machine learning (Koller & Friedman, 2009; Jordan, 1998). To learn the structure of the Schema Network, we cast the problem as a supervised learning problem over a discrete space of parameterizations (the schemas), and then apply a greedy algorithm that solves a sequence of LP relaxations. See Jaakkola et al. (2010) for further work on\napplying LP relaxations to structure learning.\nWithX and y defined above, the learning problem is to find a mapping\ny = fW (X) = XW~1\nwhere all the involved variables are binary and operations follow Boolean logic: addition corresponds to ORing, and overlining to negation. W ∈ {0, 1}D′×L is a binary matrix, with each column representing one (ungrounded) schema for at most L schemas. The elements set to 1 in each schema represent an existing connection between that schema and an input condition (see Fig. 2). The outputs of each individual schema are ORed to produce the final prediction.\nWe would like to minimize the prediction error of Schema Networks while keeping them as simple as possible. A suitable objective function is\nmin W∈{0,1}D′×L\n1 D |y − fW (X)|1 + C|W |1, (1)\nwhere the first term computes the prediction error, the second term estimates the complexity and parameter C controls the trade-off between both. This is an NP-hard problem for which we cannot hope to find an exact solution, except for very small environments.\nWe consider a greedy solution in which linear programming (LP) relaxations are used to find each new schema. Starting from the empty set, we greedily add schemas (columns to W ) that have perfect precision and increase recall for the prediction of y (See Algorithm 1 in the Supplementary). In each successive iteration, only the input-output pairs for which the current schema network is predicting an output of zero are passed. This procedure monotonically decreases the prediction error of the overall schema network, while increasing its complexity. The process stops when we hit some predefined complexity limit. In our implementation, the greedy schema selection produces very sparse schemas, and we simply set a limit to the number of schemas to add. For this algorithm to work, no contradictions can exist in the input data (such as the same input appearing twice with different labels). Such contradictions might appear in stochastic environments and would not be artifacts in real environments, so we preprocess the input data to remove them."
    }, {
      "heading" : "4.3. Planning as Probabilistic Inference",
      "text" : "The full Schema Network graph (Fig. 2) provides a probabilistic model for the set of rewards that will be achieved by a sequence of actions. Finding the sequence of actions that will result in a given set of rewards becomes then a MAP\ninference problem. This problem can be addressed approximately using max-product belief propagation (MPBP) (Attias, 2003). Another option is variational inference. Cheng et al. (2013) use variational inference for planning but resort to MPBP to optimize the variational free energy functional. We will follow the first approach.\nWithout loss of generality, we will consider the present time step to be t = 0. The state, action and reward variables for t ≤ 0 are observed, and we will consider inference over the unobserved variables in a look-ahead window of size2 T , {s(t), a(t), r(t)}T−1t=0 . Since the Schema Network is built exclusively of compatibility factors that can take values 0 or 1, any variable assignment is either impossible or equally probable under the joint distribution of the graph. Thus, if we want to know if there exists any global assignment that activates a binary variable (say, variable r(t)(+) signaling positive reward at some future time t > 0), we should look at the max-marginal p̃(r(t)(+) = 1). It will be 0 if no global assignment compatible with both the SN and existing observations can lead to activate the reward, or 1 if it is feasible. Similarly, we will be interested in the max-marginal p̃(r\n(t) (−) = 0), i.e., whether it is feasible to find a configura-\ntion that avoids a negative reward.\nAt a high-level, planning proceeds as follows: Identify feasible desirable states (activating positive rewards and deactivating negative rewards), clamp their value to our desires by adding a unary potential to the factor graph, and then find the MAP configuration of the resulting graph. The MAP configuration contains the values of the action variables that are required to reach our goal of activating/deactivating a variable. We can also look at S to see how the model “imagines” the evolution of the entities until they reach their goal state. Then we perform the actions found by the planner and repeat. We now explain each of these stages in more detail.\nPotential feasibility analysis First we run a feasibility analysis. To this end, a forward pass MPBP from time 0 to time T is performed. This provides a (coarse) approximation to the desired max-marginals for every variable. Because the SN graph is loopy, MPBP is not exact and the forward pass can be too optimistic, announcing the feasibility of states that are unfeasible3. Actual feasibility will be verified later, at the backtracking stage.\n2In contrast with MDPs, the reward is discounted with a rolling square window instead of an exponentially weighted one.\n3To illustrate the problem, consider the case in which it is feasible for an entity to move at time t to position A or position B (but obviously not both) and then some reward is conditioned on that type of entity being in both positions: A single forward pass will not handle the entanglement properly and will incorrectly report that such reward is also feasible.\nChoosing a positive reward goal state We will choose the potentially feasible positive reward that happens soonest within our explored window, clamp its state to 1, and backtrack (see below) to find the set of actions that lead to it. If backtracking fails, we will repeat for the remaining potentially feasible positive rewards.\nAvoiding negative rewards Keeping the selected positive reward variable clamped to 1 (if it was found in the previous step), we now repeat the same procedure on the negative rewards. Among the negative rewards that have been found as potentially feasible to turn off, we clamp to zero as many negative rewards as we can find a jointly satisfying backtrack. If no positive reward was feasible, we backtrack from the earliest predicted negative reward.\nBacktracking This step is akin to Viterbi backtracking, a message passing backward pass that finds a satisfying configuration. Unlike the HMM for which the Viterbi algorithm was designed, our model is loopy, so a standard backward pass is not enough to find a satisfying configuration (although can help to find good candidates). We combine the standard backward pass with a depth-first search algorithm to find a satisfying configuration."
    }, {
      "heading" : "5. Experiments",
      "text" : "We compared the performance of Schema Networks, A3C, and PNs (Progressive Networks) on several variations of the game Breakout. The chosen variations all share similar dynamics, but the layouts change, requiring different policies to achieve high scores. A diverse set of concepts must be learned to correctly predict object movements and rewards. For example, when predicting why rewards occur, the model must disentangle possible causes to discover that reward depends on the color of a brick but is independent of the ball’s velocity and position where it was hit. While these causal relationships are straightforward for humans to recover, we have yet to see any existing approach for learning a generative model that can recover all of these dynamics without supervision and transfer them effectively.\nSchema Networks rely on an input of entity states instead of raw images, and we provided the same information to A3C and PNs by augmenting the three color channels of the image with 34 additional channels. Four of these channels indicated the shape to which each pixel belongs, including shapes for bricks, balls, and walls. Another 30 channels indicated the positions of parts of the paddle, where each part consisted of a single pixel. To reduce training time, we did not provide A3C and PN with part channels for objects other than the paddle, since these are not required to learn the dynamics or predict scores. Removing irrelevant inputs could only give A3C and PN an advantage, since\nthe input to Schema Networks did not treat any object differently. Schema Networks were provided separate entities for each part (pixel) of each object, and each entity contained 53 attributes corresponding to the available part labels (21 for bricks, 30 for the paddle, 1 for walls, and 1 for the ball). Only one of these part attributes was active per entity. Schema Networks had to learn that some attributes, like parts of bricks, were irrelevant for prediction."
    }, {
      "heading" : "5.1. Transfer Learning",
      "text" : "This experiment examines how effectively Schema Networks and PNs are able to learn a new Breakout variation after pretraining, which examines how well the two models can transfer existing knowledge to a new task. Fig. 3a shows the learning rates during 100k frames of training on Mini Breakout. In a second experiment, we pretrained on Large Breakout for 100k frames and continued training on the Middle Wall variation, shown in Fig. 1b. Fig. 3b shows that PNs require significant time to learn in this new environment, while Schema Networks do not learn anything new because the dynamics are the same."
    }, {
      "heading" : "5.2. Zero-Shot Generalization",
      "text" : "Many Breakout variations can be constructed that all involve the same dynamics. If a model correctly learns the dynamics from one variation, in theory the others could be played perfectly by planning using the learned model.\nRather than comparing transfer with additional training using PNs, in these variations we can compare zero-shot generalization by training A3C only on Standard Breakout. Fig. 1b-e shows some of these variations with the following modifications from the training environment:\n• Offset Paddle (Fig. 1d): The paddle is shifted upward by a few pixels.\n• Middle Wall (Fig. 1b): A wall is placed in the middle of the screen, requiring the agent to aim around it to hit the bricks.\n• Random Target (Fig. 1e): A group of bricks is destoyed when the ball hits any of them and then reappears in a new random position, requiring the agent to delibarately aim at the group.\n• Juggling (Fig. 1f, enlarged from actual environment to see the balls): Without any bricks, three balls are launched in such a way that a perfect policy could juggle them without dropping any.\nTable 1 shows the average scores per episode in each Breakout variation. These results show that A3C has failed to recognize the common dynamics and adapt its policy accordingly. This comes as no surprise, as the policy it has learned for Standard Breakout is no longer applicable in these variations. Simply adding an offset to the paddle is\nsufficient to confuse A3C, which has not learned the causal nature of controlling the paddle with actions and controlling the ball with the paddle. The Middle Wall and Random Target variations illustrate that Schema Networks are aiming to deliberately cause positive rewards from ball-brick collisions, while A3C struggles to adapt its policy accordingly. The Juggling variation is particularly challenging, since it is not clear which ball to respond to unless the model understands that the lowest downward-moving ball is the most imminent cause of a negative reward. By learning and transferring the correct causal dynamics, Schema Networks outperform A3C in all variations."
    }, {
      "heading" : "5.3. Testing for Learned Causes",
      "text" : "To better evaluate whether these models are truly learning the causes of rewards, we designed one more zero-shot generalization experiment. We trained both Schema Networks and A3C on a Mini Breakout variation in which the color of a brick determines whether a positive or negative reward is received when it is destroyed. Six colors of bricks provide +1 reward, and two colors provide -1 reward. Negative bricks occurred in random positions 33% of the time during training. Then during testing, the bricks were arranged into two halves, with all positive colored bricks on one half and negative colored bricks on the other (see Fig. 1c). If the causes of rewards have been correctly learned, the agent should prefer to aim for the positive half whenever possible. As Table 1 shows, Schema Networks have correctly learned\nfrom random arrangements which brick colors cause which rewards, preferring to aim for the positive half during testing, while A3C demonstrates no preference for one half or the other, achieving an average score near zero."
    }, {
      "heading" : "6. Discussion and Conclusion",
      "text" : "In this work we have demonstrated the promise of Schema Networks with strong performance on a suite of Breakout variations. Instead of learning policies to maximize rewards, the learning objective for Schema Networks is designed to understand causality within these environments. The fact that Schema Networks are able to achieve rewards more efficiently than state-of-the-art model-free methods like A3C is all the more notable, since high scores are a byproduct of learning an accurate model of the game.\nThe success of Schema Networks is derived in part from the entity-based representation of state. Our results suggest that providing Deep RL models like A3C with such a representation as input can improve both training efficiency and generalization. This finding corroborates recent attempts (Usunier et al., 2016; Garnelo et al., 2016; Chang et al., 2016; Battaglia et al., 2016) to incorporate object and relational structure into neural network-based models.\nThe environments considered in this work are conceptually diverse but also simplified in a number of ways with respect to the real world: states, actions, and rewards are all discretized as binary random variables; the dynamics of the environments are deterministic; and there is no uncertainty in the observed entity states. In future work we plan to address each of these limitations, adapting Schema Networks to continuous, stochastic domains.\nSchema Networks have shown promise toward multi-task transfer where Deep RL struggles. This transfer is enabled by explicit causal structures, which in turn allow for planning in novel tasks. As progress in RL and planning continues, robust generalization from limited experience will be vital for future intelligent systems."
    }, {
      "heading" : "Acknowledgements",
      "text" : "Special thanks to Eric Purdy and Ramki Gummadi for useful insights and discussions during the preparation of this work."
    }, {
      "heading" : "A. Breakout playing visualizations",
      "text" : "See https://vimeopro.com/user45297729/ schema-networks for visualizations of Schema Networks playing different variations of Breakout after training only on basic Breakout.\nFigure 4 shows typical gameplay for one variation."
    }, {
      "heading" : "B. LP-based Greedy Schema Learning",
      "text" : "The details of the LP-based learning of Section 4.2 are provided here.\nAlgorithm 1 LP-based greedy schema learning Input: Input vectors {xn} for which fW (xn) = 0 (the\ncurrent schema network predicts 0), and the corresponding output scalars yn 1: Find a cluster of input samples that can be solved with a single (relaxed) schema while keeping perfect precision (no false alarms). Select an input sample and put it in the set “solved”, then solve the LP\nmin w∈[0,1]D ∑ n:yn=1 (1− xn)w\ns.t. (1− xn)w > 1 ∀n:yn=0 (1− xn)w = 0 ∀n∈solved\n2: Simplify the resulting schema. Put all the input samples for which (1−xn)w = 0 in the set “solved”. Simplify the just found schema w by making it as sparse as possible while keeping the same precision and recall:\nmin w∈[0,1]D\nwT~1\ns.t. (1− xn)w > 1 ∀n:yn=0 (1− xn)w = 0 ∀n∈solved\n3: Binarize the schema. In practice, the found w is binary most of the time. If it is not, repeat the previous minimization using binary programming, but optimize only over the elements of w that were found to be nonzero. Keep the rest clamped to zero.\nOutput: New schema w to add to the network"
    } ],
    "references" : [ {
      "title" : "Cognitive psychology and its implications",
      "author" : [ "Anderson", "John R" ],
      "venue" : "WH Freeman/Times Books/Henry Holt & Co,",
      "citeRegEx" : "Anderson and R.,? \\Q1990\\E",
      "shortCiteRegEx" : "Anderson and R.",
      "year" : 1990
    }, {
      "title" : "Planning by probabilistic inference",
      "author" : [ "Attias", "Hagai" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "Attias and Hagai.,? \\Q2003\\E",
      "shortCiteRegEx" : "Attias and Hagai.",
      "year" : 2003
    }, {
      "title" : "Interaction networks for learning about objects, relations and physics",
      "author" : [ "Battaglia", "Peter", "Pascanu", "Razvan", "Lai", "Matthew", "Rezende", "Danilo Jimenez" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Battaglia et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Battaglia et al\\.",
      "year" : 2016
    }, {
      "title" : "A compositional object-based approach to learning physical dynamics",
      "author" : [ "Chang", "Michael B", "Ullman", "Tomer", "Torralba", "Antonio", "Tenenbaum", "Joshua B" ],
      "venue" : "arXiv preprint arXiv:1612.00341,",
      "citeRegEx" : "Chang et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Chang et al\\.",
      "year" : 2016
    }, {
      "title" : "Variational planning for graph-based mdps",
      "author" : [ "Cheng", "Qiang", "Liu", "Chen", "Feng", "Ihler", "Alexander T" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Cheng et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Cheng et al\\.",
      "year" : 2013
    }, {
      "title" : "An object-oriented representation for efficient reinforcement learning",
      "author" : [ "Diuk", "Carlos", "Cohen", "Andre", "Littman", "Michael L" ],
      "venue" : "In Proceedings of the 25th international conference on Machine learning,",
      "citeRegEx" : "Diuk et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Diuk et al\\.",
      "year" : 2008
    }, {
      "title" : "Made-up minds: a constructivist approach to artificial intelligence",
      "author" : [ "Drescher", "Gary L" ],
      "venue" : "MIT press,",
      "citeRegEx" : "Drescher and L.,? \\Q1991\\E",
      "shortCiteRegEx" : "Drescher and L.",
      "year" : 1991
    }, {
      "title" : "Towards deep symbolic reinforcement learning",
      "author" : [ "Garnelo", "Marta", "Arulkumaran", "Kai", "Shanahan", "Murray" ],
      "venue" : "arXiv preprint arXiv:1609.05518,",
      "citeRegEx" : "Garnelo et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Garnelo et al\\.",
      "year" : 2016
    }, {
      "title" : "Generalizing plans to new environments in relational mdps",
      "author" : [ "Guestrin", "Carlos", "Koller", "Daphne", "Gearhart", "Chris", "Kanodia", "Neal" ],
      "venue" : "In Proceedings of the 18th international joint conference on Artificial intelligence,",
      "citeRegEx" : "Guestrin et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Guestrin et al\\.",
      "year" : 2003
    }, {
      "title" : "Efficient solution algorithms for factored mdps",
      "author" : [ "Guestrin", "Carlos", "Koller", "Daphne", "Parr", "Ronald", "Venkataraman", "Shobha" ],
      "venue" : "Journal of Artificial Intelligence Research,",
      "citeRegEx" : "Guestrin et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Guestrin et al\\.",
      "year" : 2003
    }, {
      "title" : "Learning bayesian network structure using lp relaxations",
      "author" : [ "Jaakkola", "Tommi S", "Sontag", "David", "Globerson", "Amir", "Meila", "Marina" ],
      "venue" : "In AISTATS, pp",
      "citeRegEx" : "Jaakkola et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Jaakkola et al\\.",
      "year" : 2010
    }, {
      "title" : "Reinforcement learning with unsupervised auxiliary tasks",
      "author" : [ "Jaderberg", "Max", "Mnih", "Volodymyr", "Czarnecki", "Wojciech Marian", "Schaul", "Tom", "Leibo", "Joel Z", "Silver", "David", "Kavukcuoglu", "Koray" ],
      "venue" : "arXiv preprint arXiv:1611.05397,",
      "citeRegEx" : "Jaderberg et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Jaderberg et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning in graphical models, volume 89",
      "author" : [ "Jordan", "Michael Irwin" ],
      "venue" : "Springer Science & Business Media,",
      "citeRegEx" : "Jordan and Irwin.,? \\Q1998\\E",
      "shortCiteRegEx" : "Jordan and Irwin.",
      "year" : 1998
    }, {
      "title" : "Probabilistic graphical models: principles and techniques",
      "author" : [ "Koller", "Daphne", "Friedman", "Nir" ],
      "venue" : "MIT press,",
      "citeRegEx" : "Koller et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Koller et al\\.",
      "year" : 2009
    }, {
      "title" : "A physics-based model prior for objectoriented mdps",
      "author" : [ "Scholz", "Jonathan", "Levihn", "Martin", "Isbell", "Charles", "Wingate", "David" ],
      "venue" : "In Proceedings of the 31st International Conference on Machine Learning",
      "citeRegEx" : "Scholz et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Scholz et al\\.",
      "year" : 2014
    }, {
      "title" : "The predictron: End-to-end learning and planning",
      "author" : [ "Silver", "David", "van Hasselt", "Hado", "Hessel", "Matteo", "Schaul", "Tom", "Guez", "Arthur", "Harley", "Tim", "Dulac-Arnold", "Gabriel", "Reichert", "Rabinowitz", "Neil", "Barreto", "Andre" ],
      "venue" : "arXiv preprint arXiv:1612.08810,",
      "citeRegEx" : "Silver et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Silver et al\\.",
      "year" : 2016
    }, {
      "title" : "Value iteration networks",
      "author" : [ "Tamar", "Aviv", "Levine", "Sergey", "Abbeel", "WU Pieter", "YI", "Thomas", "Garrett" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Tamar et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Tamar et al\\.",
      "year" : 2016
    }, {
      "title" : "Transfer learning for reinforcement learning domains: A survey",
      "author" : [ "Taylor", "Matthew E", "Stone", "Peter" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Taylor et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Taylor et al\\.",
      "year" : 2009
    }, {
      "title" : "Episodic exploration for deep deterministic policies: An application to starcraft micromanagement tasks",
      "author" : [ "Usunier", "Nicolas", "Synnaeve", "Gabriel", "Lin", "Zeming", "Chintala", "Soumith" ],
      "venue" : "arXiv preprint arXiv:1609.02993,",
      "citeRegEx" : "Usunier et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Usunier et al\\.",
      "year" : 2016
    }, {
      "title" : "Deep reinforcement learning with double q-learning",
      "author" : [ "Van Hasselt", "Hado", "Guez", "Arthur", "Silver", "David" ],
      "venue" : "In AAAI,",
      "citeRegEx" : "Hasselt et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Hasselt et al\\.",
      "year" : 2016
    }, {
      "title" : "Embed to control: A locally linear latent dynamics model for control from raw images",
      "author" : [ "Watter", "Manuel", "Springenberg", "Jost", "Boedecker", "Joschka", "Riedmiller", "Martin" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Watter et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Watter et al\\.",
      "year" : 2015
    }, {
      "title" : "Psychology: Themes and Variations",
      "author" : [ "W. Weiten" ],
      "venue" : "PSY 113 General Psychology Series. Cengage Learning,",
      "citeRegEx" : "Weiten,? \\Q2012\\E",
      "shortCiteRegEx" : "Weiten",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : "Recent work has suggested how to overcome this deficiency by utilizing object-based representations (Diuk et al., 2008; Usunier et al., 2016).",
      "startOffset" : 100,
      "endOffset" : 141
    }, {
      "referenceID" : 18,
      "context" : "Recent work has suggested how to overcome this deficiency by utilizing object-based representations (Diuk et al., 2008; Usunier et al., 2016).",
      "startOffset" : 100,
      "endOffset" : 141
    }, {
      "referenceID" : 21,
      "context" : "well-acknowledged Gestalt principle, which states that the ability to perceive objects as a bounded figure in front of an unbounded background is fundamental to all perception (Weiten, 2012).",
      "startOffset" : 176,
      "endOffset" : 190
    }, {
      "referenceID" : 2,
      "context" : "Battaglia et al. (2016) and Chang et al.",
      "startOffset" : 0,
      "endOffset" : 24
    }, {
      "referenceID" : 2,
      "context" : "Battaglia et al. (2016) and Chang et al. (2016) go further, learning relations to predict object interactions.",
      "startOffset" : 0,
      "endOffset" : 48
    }, {
      "referenceID" : 11,
      "context" : "Model-free Deep RL models like A3C are unable to substantially generalize beyond their training experience (Jaderberg et al., 2016; Rusu et al., 2016).",
      "startOffset" : 107,
      "endOffset" : 150
    }, {
      "referenceID" : 2,
      "context" : "The Interaction Network (Battaglia et al., 2016) (IN) and the Neural Physics Engine (NPE) (Chang et al.",
      "startOffset" : 24,
      "endOffset" : 48
    }, {
      "referenceID" : 3,
      "context" : ", 2016) (IN) and the Neural Physics Engine (NPE) (Chang et al., 2016) use object-level and pairwise relational representations to learn models of intuitive physics.",
      "startOffset" : 49,
      "endOffset" : 69
    }, {
      "referenceID" : 20,
      "context" : "Progress in model-based Deep RL has thus far been limited, though methods like Embed to Control (Watter et al., 2015), Value Iteration Networks (Tamar et al.",
      "startOffset" : 96,
      "endOffset" : 117
    }, {
      "referenceID" : 16,
      "context" : ", 2015), Value Iteration Networks (Tamar et al., 2016), and the Predictron (Silver et al.",
      "startOffset" : 34,
      "endOffset" : 54
    }, {
      "referenceID" : 5,
      "context" : "Schema Networks build upon the ideas of the ObjectOriented Markov Decision Process (OO-MDP) introduced by Diuk et al. (2008) (see also Scholz et al.",
      "startOffset" : 106,
      "endOffset" : 125
    }, {
      "referenceID" : 5,
      "context" : "Schema Networks build upon the ideas of the ObjectOriented Markov Decision Process (OO-MDP) introduced by Diuk et al. (2008) (see also Scholz et al. (2014)).",
      "startOffset" : 106,
      "endOffset" : 156
    }, {
      "referenceID" : 5,
      "context" : "Schema Networks build upon the ideas of the ObjectOriented Markov Decision Process (OO-MDP) introduced by Diuk et al. (2008) (see also Scholz et al. (2014)). Related frameworks include relational and first-order logical MDPs (Guestrin et al., 2003a). These formalisms, which harken back to classical AI’s roots in symbolic reasoning, are designed to enable robust generalization. Recent work by Garnelo et al. (2016) on “deep symbolic reinforcement learning” makes this connection explicit, marrying firstorder logic with deep RL.",
      "startOffset" : 106,
      "endOffset" : 417
    }, {
      "referenceID" : 5,
      "context" : "An image input is parsed into a list of entities, which may be thought of as instances of objects in the sense of OO-MDPs (Diuk et al., 2008).",
      "startOffset" : 122,
      "endOffset" : 141
    }, {
      "referenceID" : 7,
      "context" : "Recent work has demonstrated one possible method for unsupervised entity construction using autoencoders (Garnelo et al., 2016).",
      "startOffset" : 105,
      "endOffset" : 127
    }, {
      "referenceID" : 5,
      "context" : "Schema Networks are closely related to Object-Oriented MDPs (OO-MDPs) (Diuk et al., 2008) and Relational MDPs (R-MDPs) (Guestrin et al.",
      "startOffset" : 70,
      "endOffset" : 89
    }, {
      "referenceID" : 2,
      "context" : "Schema Networks are also related to the recently proposed Interaction Network (IN) (Battaglia et al., 2016) and Neural Physics Engine (NPE) (Chang et al.",
      "startOffset" : 83,
      "endOffset" : 107
    }, {
      "referenceID" : 3,
      "context" : ", 2016) and Neural Physics Engine (NPE) (Chang et al., 2016).",
      "startOffset" : 40,
      "endOffset" : 60
    }, {
      "referenceID" : 10,
      "context" : "See Jaakkola et al. (2010) for further work on applying LP relaxations to structure learning.",
      "startOffset" : 4,
      "endOffset" : 27
    }, {
      "referenceID" : 4,
      "context" : "Cheng et al. (2013) use variational inference for planning but resort to MPBP to optimize the variational free energy functional.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 18,
      "context" : "This finding corroborates recent attempts (Usunier et al., 2016; Garnelo et al., 2016; Chang et al., 2016; Battaglia et al., 2016) to incorporate object and relational structure into neural network-based models.",
      "startOffset" : 42,
      "endOffset" : 130
    }, {
      "referenceID" : 7,
      "context" : "This finding corroborates recent attempts (Usunier et al., 2016; Garnelo et al., 2016; Chang et al., 2016; Battaglia et al., 2016) to incorporate object and relational structure into neural network-based models.",
      "startOffset" : 42,
      "endOffset" : 130
    }, {
      "referenceID" : 3,
      "context" : "This finding corroborates recent attempts (Usunier et al., 2016; Garnelo et al., 2016; Chang et al., 2016; Battaglia et al., 2016) to incorporate object and relational structure into neural network-based models.",
      "startOffset" : 42,
      "endOffset" : 130
    }, {
      "referenceID" : 2,
      "context" : "This finding corroborates recent attempts (Usunier et al., 2016; Garnelo et al., 2016; Chang et al., 2016; Battaglia et al., 2016) to incorporate object and relational structure into neural network-based models.",
      "startOffset" : 42,
      "endOffset" : 130
    } ],
    "year" : 2017,
    "abstractText" : "The recent adaptation of deep neural networkbased methods to reinforcement learning and planning domains has yielded remarkable progress on individual tasks. Nonetheless, progress on task-to-task transfer remains limited. In pursuit of efficient and robust generalization, we introduce the Schema Network, an objectoriented generative physics simulator capable of disentangling multiple causes of events and reasoning backward through causes to achieve goals. The richly structured architecture of the Schema Network can learn the dynamics of an environment directly from data. We compare Schema Networks with Asynchronous Advantage Actor-Critic and Progressive Networks on a suite of Breakout variations, reporting results on training efficiency and zero-shot generalization, consistently demonstrating faster, more robust learning and better transfer. We argue that generalizing from limited data and learning causal relationships are essential abilities on the path toward generally intelligent systems.",
    "creator" : "LaTeX with hyperref package"
  }
}