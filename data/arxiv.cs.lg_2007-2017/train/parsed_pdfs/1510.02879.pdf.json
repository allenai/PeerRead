{
  "name" : "1510.02879.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "ADAAPT: A Deep Architecture for Adaptive Policy Transfer from Multiple Sources",
    "authors" : [ "Janarthanan Rajendran", "Prasanna P", "Balaraman Ravindran", "Mitesh M Khapra" ],
    "emails" : [ "rsdjjana@gmail.com", "pp1403@gmail.com", "ravi@cse.iitm.ac.in", "mikhapra@in.ibm.com" ],
    "sections" : [ {
      "heading" : "Introduction",
      "text" : "One of the goals of Artificial Intelligence (AI) is to build autonomous agents that can learn and adapt to new environments. Reinforcement Learning (RL) is a key technique for achieving such adaptability. RL looks at the problem of intelligent decision making as one of stochastic sequential control (Sutton and Barto 1998). The goal of RL algorithms is to learn an optimal policy for choosing actions that maximises some notion of long term performance. This is typically achieved by maintaining a utility or value function over states and actions. One of the chief drawbacks of RL is that learning on new tasks from scratch takes a long time since the agent initially performs random exploration to discover details of the task. Much of the research in RL has been focused on cutting down this initial exploration. One of the key idea used is that of transfer learning.\nTransfer is by no means limited to RL. The notion of transfer is to use knowledge gained from solving related instances of a problem (source tasks) to solve a new instance (target task) better - either in terms of speeding up\nthe learning process or in terms of achieving a better solution, among other performance measures. When applied to RL, transfer could be accomplished in many ways (see (Taylor and Stone 2009; Taylor and Stone 2011) for a very good survey of the field). One could think of transferring the value function from a source task and use that as the initial estimate of the value function in the target task to cut down on the initial exploration (Sorg and Singh 2009). Another method to achieve transfer is to reuse policies derived in the source task(s) in the target task. This can take one of two forms - (i) the derived policies can be used as initial explorative trajectories (Atkeson and Schaal 1997; Niekum et al. 2013) in the target task, thereby cutting down on random exploration; and (ii) the derived policy could be used to define macro actions which may then be used by the agent in solving the target task (Mannor et al. 2004; Brunskill and Li 2014). A third method to achieve transfer is to transfer knowledge of the model of the domain from the source tasks to the target task. Such an approach assumes that the model of the target task is sufficiently close to the source task and prior knowledge of the model allows the agent to eliminate frivolous exploration.\nWhile transfer in RL has been much explored, there are two crucial issues that have not received much attention. The first is negative transfer - when transfer from a source task degrades the performance of the agent on the target task. This is widely recognised as a serious problem in the transfer learning literature. In the context of RL too this severely limits the applicability of transfer to cases when some measure of relatedness between source and target tasks can be guaranteed. One work that explicitly addresses the question of negative transfer is that of (Brunskill and Li 2014). In that work they assume that they have access to source tasks to sufficiently cover the space of problems from which the target task is drawn and further the safe exploration that they use as part of the learning process ensures no negative transfer happens. In our work we take a more general approach. We maintain a copy of the policy that is learned from scratch on the target task. If there is evidence of negative transfer happening we will fall back to this base policy.\nThe second problem with transfer is that of identifying an appropriate source task from which to transfer. This is especially problematic if we are trying to transfer whole solutions or value functions to the target task. One way of miti-\nar X\niv :1\n51 0.\n02 87\n9v 1\n[ cs\n.A I]\n1 0\nO ct\n2 01\n5\ngating this is to learn macro actions and transfer policy fragments - the learning agent decides if policy fragments are appropriate in the target task. Another way of approaching this, is to select different and multiple source tasks to transfer from at different points in the target task. We call this selective transfer. In fact ours is the first work that explicitly looks at blending policies from different source tasks for transfer to a single target task and for different parts of its state space. Earlier multiple task transfer settings have either formulated it as a multi-task learning problem, or one of selecting a specific source task for a given target task. In our framework the agent can pick and choose portions of policies from different and multiple source tasks while solving a single target tasks. This allows us to treat all the prior policies as a partial basis from which the target policy is created.\nIn this work we propose ADAAPT, A Deep Architecture for Adaptive Policy Transfer, a transfer learning framework that avoids negative transfer while performing selective transfer from multiple source tasks. One key difficulty in selective transfer is that learning a selection function that blends the different policies together is a very challenging problem. One of the distinguishing features of our approach is the use of a deep neural network that leverages ideas from recent work on learning attention (Bahdanau, Cho, and Bengio 2014) to learn complex selection functions without worrying about representation issues apriori. Since our approach needs an explicit representation of the policies of the source tasks, we present the approach using specific choices for the reinforcement learning architecture (REINFORCE, ActorCritic (Williams 1992; Konda and Tsitsiklis 2000)), for ease of exposition. The ideas presented here extend to other architectures as well.\nThe main features of ADAAPT are: 1. It avoids negative transfer as is empirically demonstrated\nby transferring from carefully constructed bad initial policies.\n2. It achieves selective policy transfer from multiple source tasks to a target task.\n3. It uses a deep neural network architecture that enables the learning of the selective transfer in a natural way, and enables the deployment of the architecture in large domains."
    }, {
      "heading" : "Related Work",
      "text" : "As mentioned earlier, transfer learning approaches could deal with transferring representations, policies or value functions. For example, (Banerjee and Stone 2007) describe a method for transferring value functions by constructing a Game tree. Similarly, (Sorg and Singh 2009) explore the idea of transferring the value function from a source task and use that as the initial estimate of the value function in the target task to cut down on the initial exploration. Another method to achieve transfer is to reuse policies derived in the source task(s) in the target task. Probabilistic Policy Reuse as discussed in (Fernández and Veloso 2006) provides a useful way for transferring policies. This method maintains a library of policies and selects a policy based on a similarity metric, or a random policy, or a max-policy from the knowledge obtained. The policy selection happens a priori\nto each episode. (Atkeson and Schaal 1997; Niekum et al. 2013) propose a method to use the learned source policies as initial explorative trajectories (Atkeson and Schaal 1997; Niekum et al. 2013) in the target task instead of relying solely on random exploration. (Lazaric and Restelli 2011) addresses the issue of negative transfer in transferring samples for a related task in a multi-task setting. Representation transfer is done using Proto Value Functions as discussed in (Ferguson and Mahadevan 2006). (Konidaris, Scheidwasser, and Barto 2012) discusses the idea of exploiting shared common features across related tasks. They learn a shaping function that can be used in later tasks.\nIn contrast to previous work, our work explicitly focuses on the ability to selectively transfer, using multiple source tasks while avoiding negative transfer. We formally define these two challenges in the next section and then propose a model to address them.\nChallenges of Transfer Learning addressed in this work\nNegative Transfer Consider a performance measure ρ, as discussed in (Taylor and Stone 2009), where ρ could be, for example, jump start (the initial performance of the agent in the target task) or time to threshold (time to reach a predefined performance level). Let π be the policy learnt from scratch in the target without any transfer and πT be the policy learnt in the target using transfer from source tasks. If following πT gives a performance, measured by ρ, worse than that got through following π, then we say it is a negative transfer.\nSelective Transfer Let there be N policies, π1, π2, . . . , πN . When the agent learns to solve a new target task, it should be able to learn policies of the form, π(s) = f(π1(s), . . . , πN (s)) ∀ s ∈ S, the set of all states in the MDP.\nProposed Model We propose a model for policy transfer from multiple source MDPs with the same structure and different model parameters. Let there be N policies, π1, π2, . . . , πN derived from solving N prior tasks. When the agent learns to solve a new target task, the agent learns policies of the form π(s) = f(π1(s), . . . , πN (s), πR(s)) where s ∈ S represents the state and πR is a policy learnt from scratch on the target task. In this work we assume that f is implemented as a convex combination of the policies and is given by f(π1, . . . , πN , πR) =\n∑N i=1 wiπi + wN+1πR, where∑N+1\ni=1 wi = 1 and wi ∈ [0, 1]. π is the policy that the agent follows. Figure 1 shows the architecture diagram of the proposed model. The key component of the model is the central network which learns the weights to be assigned to the different policies. We refer to this network as the attention network. The weights (wi, i ∈ 1, 2, . . . , N + 1) allow the network to selectively accept or reject the policies of other source tasks depending on the input state. This allows the model to achieve both its stated goals, viz., (i) avoid negative transfer from policy πi by setting wi to a very low value\nand (ii) selectively transfer the knowledge from the source tasks for certain states by setting weights of those tasks to high values for those states.\nDepending on the feedback obtained from the environment upon following π, the attention network’s (which produces wi) parameters are updated to improve performance. Even though the agent follows the policy π we update the parameters of the network that produces πR, the randomly initialised policy network, as if the action taken by the agent was based only on πR. The networks which produce the polices of the source tasks, π1, . . . , πN remain fixed.\nAlternately, we could also update the parameters of the networks that produce π1, . . . , πN . However, doing so has two major drawbacks. First, if we update the parameters of all the source networks, there could be a significant amount of unlearning in the source networks before the attention network identifies the utility of the source tasks for the target task. This could result in a weaker transfer than actually possible. Secondly, since the number of parameters of the model would increase linearly with the number of source tasks, it could lead to problems when we have a large number of source tasks. We verified this empirically and hence for all experiments reported in this paper we only update the parameters of the network that produces πR and the attention network which produces wi.\nIf there is a source task whose policy πj is useful for the target task in some parts of its state space, then over time, πR would start replicating that source task policy πj in those parts of the state spaces. Note that the agent could follow πj even before πR attains its replication in the corresponding parts of the state space. Since the attention is soft, our model has the flexibility to combine multiple source task policies. After the learning is done, πR, can be used as the policy of the target task for future endeavors. None of the other networks are required to be stored to represent the learnt policy of the target task.\nFollowing the recent success of Deep Neural Networks in a variety of Machine Learning tasks we made a de-\nsign choice to use deep neural networks in our model (and hence the name ADAAPT). This should potentially allow the model to work even for large, complex Reinforcement Learning problems. Using deep neural networks that leverages ideas from recent work on learning attention allows the agent to learn complex selection functions without worrying about representation issues apriori.\nInstantiations of ADAAPT ADAAPT is a generic framework which can be used alongside any algorithm where there is an explicit representation of the policy. Here we describe two instantiations of ADAAPT, one for direct policy search using REINFORCE algorithm and another in the Actor-Critic setup.\nADAAPTive REINFORCE REINFORCE algorithm (Williams 1992) can be used for direct policy search by making weight adjustments in a direction that lies along the gradient of the expected reinforcement. In ADAAPTive REINFORCE, ADAAPT is used directly to do policy search and its parameters are updated using REINFORCE. Let ψ represent the attention network. ψ outputs w and is parameterised by u. The update of u is given by,\n∆u = αu(r − b) ∂ ∑L t=1 log(π(s, at))\n∂u\nu← u+ ∆u where, αu is a non-negative factor, r is the current reinforcement, which is the return at the end of an episode in our case, b is the reinforcement baseline,L is the total number of steps in the episode, at is the action taken at step t from state s following π and π(s, at) is the probability of taking action at in state s as given by π. Note that π is the policy which the agent follows.\nLet φ represent the randomly initialised policy network which is learnt from scratch for the target task. φ outputs πR and is parameterised by v. The update for v is given by,\n∆v = αv(r − b) ∂ ∑L t=1 log(πR(s, at))\n∂v\nv ← v + ∆v where, at is the action taken by the agent at step t from state s following π and πR(s, at) is the probability of action at given by πR. As mentioned earlier, though the agent follows π, the parameters of the network representing πR are updated as if the action was taken by following πR.\nADAAPTive Actor-Critic Actor-Critic methods (Konda and Tsitsiklis 2000) are Temporal Difference (TD) methods that have two separate components, viz., an actor and a critic. The actor proposes a policy whereas the critic estimates the value function of the policy and criticizes the actions of the actor. The updates to the actor happens through TD-error which is the one step estimation error that helps in reinforcing an agent’s behaviour. This TD-error is a scalar value and is referred to as the critic.\nADAAPTive Actor-Critic is an Actor-Critic model, where the actor uses ADAAPT. We have a deep adaptive actor\nwhich utilises the knowledge of the learned source task while avoiding negative transfer and performing selective transfer wherever appropriate. The critic here learns the state values from scratch. In other words, the actor is always aware of all the previous learnt tasks and tries to use that knowledge for its benefit. The critic evaluates this selection as well as the action solely depending on the target task (i.e., it does not care about the source tasks).\nLet st be the state in which the agent is at time step t. st+1 is the state the agent reaches with a reward of rt+1 upon taking action at at time step t from state s following the policy π. Let V (s) represent the value of state s. Then, the update equations for parameters u of the attention network ψ and the parameters v of the randomly initialised policy’s (πR’s) network φ are as follows.\nδt = rt+1 + γV (st+1)− V (st) where, γ is the discount factor.\n∆u = αuδt ∂ log π(st,at) ∂u∣∣∣∂ log π(st,at)∂u ∣∣∣\nu← u+ ∆u where, α is a non-negative factor and π(st, at) is the probability of taking action at from state st given by the policy π.\n∆v = αvδt ∂ log πR(st,at) ∂v∣∣∣∂ log πR(st,at)∂v ∣∣∣\nv ← v + ∆v where, πR(st, at) is the probability of taking action at from state st given by the policy πR."
    }, {
      "heading" : "Experiments and Discussion",
      "text" : "We evaluate the performance of ADAAPT using two simulated worlds, viz., chain world and puddle world as described below. Chain world: Figure 2a shows the chain world where the goal of the agent is to go from one point in the chain (starting state) to another point (goal state) in the least number of steps. At each state the agent can choose from two actions, to either move one position to the left or to the right. After reaching the goal state the agent gets a reward which is inversely proportional to the number of steps taken to reach the goal. Puddle worlds: Figure 2b shows the discrete version of the standard puddle world which is widely used in Reinforcement Learning literature. In this world, the goal of the agent is to go from a specified start position to the goal position maximising its return. At each state the agent can choose one of these four actions: move one position to the north, south, east or west. With 0.9 probability the agent moves in the chosen direction and with 0.1 probability it moves in a random direction. On reaching the goal state, the agent gets a reward of +10. On reaching other parts of the grid the agent gets different penalties as mentioned in the legend of Figure\n2b. Figures 2c to 2f show different variants of the puddle world which we constructed to evaluate different features of ADAAPT as described below.\nExperiment 1: Ability to avoid negative transfer We first consider the case when only one learned source task is available such that it can hamper the learning process of the new target task. We refer to such a source task as an unfavorable source task. In such a scenario, the attention network shown in Figure 1 should learn to assign a very low weight to the action probabilities output by the policy network of this unfavorable source task and assign a higher weight to the randomly initialized policy network. We now define an experiment using the puddle world from Figure 2b to show that ADAAPT indeed does so. The target task in our experiment is to maximize the return in reaching the goal state G1 starting from any one of the states S1, S2, S3, S4. We artificially construct an unfavorable source task by first learning to solve the above task and then negating the weights of the topmost layer of the actor network. Given such an unfavorable task, Figure 3a compares the performance of the following methods:\n• R: In this case, there is no learned source task and the new task simply starts with a randomly initialized actor network and learns the weights of this network over time/episodes.\n• B: In this case, the new task simply starts with the actor\nnetwork learned for the unfavorable task and adjusts the weights of this network over time/episodes.\n• ADAAPT RB: In this case, the actor uses ADAAPT. Specifically, it is provided a randomly initialized policy network as well as the policy network of the unfavorable task.\nAs is evident from Figure 3a, ADAAPT does not get hampered by the unfavorable source task. It learns to ignore the unfavorable task and does as good as the case when such an unfavorable source task is not available and only a randomly initialized network is available (R).\nExperiment 2: Ability to transfer from a favorable source task Now, consider the case when a favorable source task is available which can help the learning process of the target task. In such a scenario, the attention network shown in Figure 1 should learn to assign a very high weight to the action probabilities output by the policy network of this favorable source task and assign a lower weight to the randomly initialized policy network. To show that ADAAPT indeed does so we use the same target task as used in Experiment 1. We artificially construct a favorable source task simply by learning to solve the target task and using the learned actor network. Figure 3b compares the following methods:\n• R: This is same as described in Experiment 1. • G: Here, the target task simply starts with the actor\nnetwork learned for the favorable task and adjusts the weights of this network over time/episodes if needed.\n• ADAAPT RG: Here, the actor uses ADAAPT. Specifically, it is provided a randomly initialized policy network as well as the policy network of the favorable task.\n• ADAAPT RGB: Here again, ADAAPT is used but in addition to the randomly initialized policy network and the trained policy network of a favorable task, the trained policy network of an unfavorable source task is also available.\nAs is evident from Figure 3b, when a favorable source task is available, ADAAPT is able to exploit it and improve the learning speed of the new task. Further, ADAAPT is not affected by the presence of an unfavorable task (as the performance of ADAAPT RGB and ADAAPT RG are almost the same).\nExperiment 3: Ability to selectively transfer from multiple source tasks In this section, we consider the case when multiple partially favorable source tasks are available such that each of them can assist the learning process for different parts of the state space of the target task. We illustrate this first using the simple chain world shown in (Fig. 2a). Consider that the target task LT is to start in A or B with uniform probability and reach C in the least number of steps. Now consider that two learned source tasks, viz., L1 and L2, are available. L1 is the source task where the agent has learned to reach the left end (A) starting from the right end (B). In contrast, L2 is the\nsource task where the agent has learned to reach the right end (B) starting from the left end (A). Intuitively, it should be clear that the target task should benefit from the policies learnt for task L1 when it is trying to move from B to C and similarly it should benefit from the policies learnt for task L2 when in is trying to move from A to C. We learn the task LT using ADAAPTive REINFORCE with the following policies (i) policies learned forL1 (i) policies learned for L2 and (iii) a randomly initialized policy network. Figure 4 shows the weights given by the attention network to the different source policies for different parts of the state space at the end of learning. We observe that the attention network has learned to ignore L1 for the left half of the state space of the target task. Similarly it has learned to ignore L2 for the right half of the state space of the target task. Since the randomly initialised actor network becomes the good policy over time, it has a high weight throughout the state space of the target task.\nWe repeat the same experiment in a relatively more complex puddle world as shown in Figure 2c. In this case, L1 is the task of moving from S1 to G1 and L2 is the task of moving from S2 to G1. In the target task LT , the agent has to learn to move to G1 starting from either S1 or S2 chosen with uniform probability. We learn the task LT using ADAAPTive Actor-Critic method where the following are available (i) learned policy networks for L1 (ii) learned policy network for L2 and (iii) a randomly initialized policy network. Figure 3c compares the performance of the following methods. • R: This is same as described in Experiment 1. • ADAAPT L1RR: In this case, ADAAPT is provided two\nrandomly initialized policy networks as well as the learned actor network of L1.\n• ADAAPT L2RR: In this case, ADAAPT is provided two randomly initialized policy networks as well as the learned actor network of L2.\n• ADAAPT L1L2R: In this case, ADAAPT is provided one randomly initialized policy network as well as the learned actor networks of both L1 and L2.\nWe use two random networks in ADAAPT L1RR ADAAPT L2RR so that the number of parameters in this setup are comparable to ADAAPT L1L2R. We observe that ADAAPT L1L2R is able to perform better than the other configurations. It is able exploit the policies learned\nfor L1 and L2 and performs better than R which effectively tries to learn the target task from scratch without any knowledge of existing source tasks.\nFinally, we move to an even more challenging task involving three variants of the puddle world. Specifically, L1 is the task shown in Figure 2d, L2 is the task shown in Figure 2e and LT is the task shown in Figure 2f. In all these worlds, the agent can start from either S1, S2, S3 or S4 with uniform probability and has to reach the goal state G1. The position and shape of the puddles as well as the position of the goal state G1 are different in each of the three worlds. Figure 3d compares the following methods:\n• R: In this case, there is no learned source task and the new task simply starts with a randomly initialized actor network and learns the weights of this network over time/episodes.\n• ADAAPT L1L2R: In this case, ADAAPT is provided one randomly initialized actor network as well as the learned actor networks of both L1 and L2.\nDespite clear differences between the source tasks and the\ntarget task, ADAAPT is able to do some meaningful transfer.\nConclusion and Future Work In this paper we present a deep neural network architecture for transfer learning that avoids negative transfer while enabling selective transfer from multiple source tasks. We empirically evaluate the performance of the proposed model using a variety of simulated worlds and show that it indeed achieves its stated goals. While in this work we focused on transfer between tasks that share the same state and action spaces, the use of deep networks opens up the possibility of going beyond this setting. For example, a deep neural network can be used to learn common representations (Wernsdorfer and Schmid 2014) for multiple tasks thereby enabling transfer between related tasks that could possibly have different state and action parameterisation. Further, the use of deep networks provides a straightforward way of applying these ideas in a continuous domain. Over all we believe that ADAAPT is a novel way to approach transfer learning that opens up many new avenues of research in this area."
    } ],
    "references" : [ {
      "title" : "and Schaal",
      "author" : [ "C.G. Atkeson" ],
      "venue" : "S.",
      "citeRegEx" : "Atkeson and Schaal 1997",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Cho Bahdanau", "D. Bengio 2014] Bahdanau", "K. Cho", "Y. Bengio" ],
      "venue" : "arXiv preprint arXiv:1409.0473",
      "citeRegEx" : "Bahdanau et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2014
    }, {
      "title" : "and Stone",
      "author" : [ "B. Banerjee" ],
      "venue" : "P.",
      "citeRegEx" : "Banerjee and Stone 2007",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "and Li",
      "author" : [ "E. Brunskill" ],
      "venue" : "L.",
      "citeRegEx" : "Brunskill and Li 2014",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "and Mahadevan",
      "author" : [ "K. Ferguson" ],
      "venue" : "S.",
      "citeRegEx" : "Ferguson and Mahadevan 2006",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "and Veloso",
      "author" : [ "F. Fernández" ],
      "venue" : "M.",
      "citeRegEx" : "Fernández and Veloso 2006",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "and Tsitsiklis",
      "author" : [ "V. Konda" ],
      "venue" : "J.",
      "citeRegEx" : "Konda and Tsitsiklis 2000",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "A",
      "author" : [ "G. Konidaris", "I. Scheidwasser", "Barto" ],
      "venue" : "G.",
      "citeRegEx" : "Konidaris. Scheidwasser. and Barto 2012",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "and Restelli",
      "author" : [ "A. Lazaric" ],
      "venue" : "M.",
      "citeRegEx" : "Lazaric and Restelli 2011",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Dynamic abstraction in reinforcement learning via clustering",
      "author" : [ "Mannor" ],
      "venue" : "In Proceedings of the twenty-first international conference on Machine learning,",
      "citeRegEx" : "Mannor,? \\Q2004\\E",
      "shortCiteRegEx" : "Mannor",
      "year" : 2004
    }, {
      "title" : "A",
      "author" : [ "S. Niekum", "S. Chitta", "Barto" ],
      "venue" : "G.; Marthi, B.; and Osentoski, S.",
      "citeRegEx" : "Niekum et al. 2013",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "and Singh",
      "author" : [ "J. Sorg" ],
      "venue" : "S.",
      "citeRegEx" : "Sorg and Singh 2009",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "A",
      "author" : [ "R.S. Sutton", "Barto" ],
      "venue" : "G.",
      "citeRegEx" : "Sutton and Barto 1998",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "P",
      "author" : [ "M.E. Taylor", "Stone" ],
      "venue" : "2009. Transfer learning for reinforcement learning domains: A survey. The Journal of Machine Learning Research 10:1633–",
      "citeRegEx" : "Taylor and Stone 2009",
      "shortCiteRegEx" : null,
      "year" : 1685
    }, {
      "title" : "and Stone",
      "author" : [ "M.E. Taylor" ],
      "venue" : "P.",
      "citeRegEx" : "Taylor and Stone 2011",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "and Schmid",
      "author" : [ "M. Wernsdorfer" ],
      "venue" : "U.",
      "citeRegEx" : "Wernsdorfer and Schmid 2014",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "R",
      "author" : [ "Williams" ],
      "venue" : "J.",
      "citeRegEx" : "Williams 1992",
      "shortCiteRegEx" : null,
      "year" : 1992
    } ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "The ability to transfer knowledge from learnt source tasks to a new target task can be very useful in speeding up the learning process of a Reinforcement Learning agent. This has been receiving a lot of attention, but the application of transfer poses two serious challenges which have not been adequately addressed in the past. First, the agent should be able to avoid negative transfer, which happens when the transfer hampers or slows down the learning instead of speeding it up. Secondly, the agent should be able to do selective transfer which is the ability to select and transfer from different and multiple source tasks for different parts of the state space of the target task. We propose ADAAPT: A Deep Architecture for Adaptive Policy Transfer, which addresses these challenges. We test ADAAPT using two different instantiations: One as ADAAPTive REINFORCE algorithm for direct policy search and another as ADAAPTive Actor-Critic where the actor uses ADAAPT. Empirical evaluations on simulated domains show that ADAAPT can be effectively used for policy transfer from multiple source MDPs sharing the same state and action space.",
    "creator" : "LaTeX with hyperref package"
  }
}