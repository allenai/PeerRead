{
  "name" : "1206.6406.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Bayesian Optimal Active Search and Surveying",
    "authors" : [ "Roman Garnett", "Yamuna Krishnamurthy", "Xuehan Xiong", "Jeff Schneider" ],
    "emails" : [ "rgarnett@cs.cmu.edu", "ykrishna@andrew.cmu.edu", "xxiong@andrew.cmu.edu", "jschneide@cs.cmu.edu", "rmann@math.uu.se" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "In many real-world classification scenarios, it can be much easier to collect input data than to observe associated labels, which could require relatively expensive\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nhuman action. For this reason, considerable research in semi-supervised and active learning has considered how to construct models exploiting unlabeled data and also how to intelligently request unknown labels to achieve a given goal as cheaply as possible.\nThe bulk of active classification research has considered obtaining labels to maximize some measure of predictive power or model accuracy. Here, we consider two distinctly different problems. In the first, which we call active search, the members of one particular class are deemed important and are to be located as quickly as possible. Many real-world problems are of this form; fraud detection, drug discovery, and product recommendation are just a few examples. In the second task, which we call active surveying, we seek to determine the portion of a dataset belonging to a particular class. Targeted opinion polling is an important and natural real-world problem of this type.\nTypical model-based active classification strategies are not appropriate for either of these problems. The consequence of catching a fraudster, discovering a new cancer drug, or selling a product can be measured in monetary terms. Learning an accurate model, on the other hand, is only useful if it can help us locate more items. Indeed we could make observations that give very high performance on either task and nonetheless produce a model that is uncertain or even completely inaccurate on large swathes of the domain.\nRather than proposing heuristics to adapt typical active learning algorithms to these problems, we will instead begin “from the beginning” and analyze these problems using Bayesian decision theory. We will first define natural utility functions for each problem and then derive the optimal policies.\nThe active search problem has been previously described (Garnett et al., 2011); here we extend that preliminary work with two contributions. First, we\nwill prove that a myopic approximation to the optimal active search policy can perform arbitrary worse than an even slightly less-myopic approximation. Our second contribution is more practical. In the general case, the optimal search policy requires time that grows exponentially in the number of unlabeled points. Here we show how for a certain class of classifiers (including k-nn), we may identify and discard points that cannot possibly be optimal with trivial extra computation. In practice, this can increase the efficiency of the algorithm by orders of magnitude and allow us to use policies that might not otherwise be possible.\nThe rest of this paper is arranged as follows. In Sections 2 and 3, we formally describe the problems at hand. In Section 4, we provide and discuss the Bayesian optimal policies for these problems. We proceed by proving a result about the potential benefit of using the optimal active search policy with increasingly long horizons. In Section 6, we discuss a branch-and-bound technique to limit the search space required for the optimal search policy. Finally, we evaluate our methods empirically."
    }, {
      "heading" : "2. Problem Definition",
      "text" : "Suppose we have a finite set of elements X , {xi} and an identified subset R ⊂ X , the members of which we will call targets. We consider the following problem. Suppose we do not know which members of X belong to R a priori, but can successively request binary observations y , χ(x ∈ X ), for an unlabeled element x ∈ X . We wish to actively select a sequence of queries to maximize a given utility function.\nFor the active search problem, we define the utility of a set of observations D , { (xi, yi) } to be the number of targets found:\nu(D) , ∑ yi.\nThis simple expression naturally captures the spirit of the problem as defined above. For the active surveying problem, we define the utility of a set of observations to be the variance in our induced probability distribution over the cardinality of R:\nu(D) , − var[cardR | D].\nAgain this expression encapsulates the goal of surveying: polls with smaller margins of error are to be preferred."
    }, {
      "heading" : "3. Related Work",
      "text" : "Active learning is a mature field with a large associated body of literature (Settles, 2010). In the active binaryclassification problem, the chosen objective is usually\nrelated to properties of the associated probabilistic model. Examples include generalization error (Zhu et al., 2003) and optimality criteria related to the Fisher information, such as A-optimality (Schein & Ungar, 2007). One of the simplest active learning techniques for binary classification is uncertainty sampling (Lewis & Gale, 1994), which successively requests the label for the point x∗ with the greatest posterior variance: x∗ , arg minx\n∣∣Pr(y = 1 | x,D)− 1/2∣∣. Both objectives considered in this paper are unusual in an active-learning context as far as the authors know. A problem similar to active search that has been considered is the active discovery of previously unseen classes (He & Carbonell, 2008). Weitzman (1979) considers an active search problem where there is no dependence between outcomes and derives the optimal policy. The problem as defined there can also be seen as a Bayesian multi-armed bandit, and the optimal policy can also be recovered via a Gittins index (Gittins et al., 2011). Here we consider the case where the “arms” of the bandits are correlated, which is a so-called restless bandit problem.\nThere is a long history of statistical research investigating the selection of respondants when conducting a survey. A particular focus of such research is identifying and correcting selection bias, where certain people are more likely to be selected for a survey than others (Berger, 2005). Here we take a completely different approach: we actively and intentionally bias our selection of points to query, choosing those that we believe will increase our understanding of the class proportion as much as possible. In the context of polling a social network, we can reasonably expect that opinions are in some way correlated under a notion of “similarity” or “closeness” in the network. For this reason, polling people with many connections throughout the network might be more fruitful than polling people who are relatively isolated. We embrace and leverage this notion of correlated opinions and influence in our design."
    }, {
      "heading" : "4. The Optimal Bayesian Policy",
      "text" : "As mentioned previously, our approach to the active search and surveying problems will be motivated by Bayesian decision theory. This will require selecting a classification model that provides the posterior probability of a point x belonging to R conditioned on previously observed data D, Pr(y = 1 | x,D). We will assume that this model is given a priori; the decision theoretic analysis does not depend on its nature.\nWithout loss of generality, we will assume that at the onset we will be allowed a fixed number of queries t.\nIn applications where the cost of obtaining a label is high, it is the total cost of the queries that limits their number, rather than the quantity of unlabeled points.\nWe now derive the policy for deciding the locations of our queries, which will entail successively calculating the expected utility of each of the remaining unlabeled points then observing the label for the point that with maximal expected utility. At time i, then, we will observe the label for the point\nx∗i , arg max xi∈X\\Di−1\nE [ u(Dt) | xi,Di−1 ] .\nWe begin by considering the case when we are allowed to make exactly one more query and will then address the general case. Suppose that we have already made t−1 observations Dt−1. To select our final observation, we calculate the expected utility of a candidate point xt, marginalizing out the unknown value of yt.\nFor active search, the expected utility is E [ u(Dt) | xt,Dt−1 ] = ∑ y u(Dt) Pr(yt = y | xt,Dt−1)\n= u(Dt−1) + Pr(yt = 1 | xt,Dt−1).\nBecause u(Dt−1) does not depend on xt, the optimal decision x∗t is therefore the point with the largest posterior probability of being a target. This makes intuitive sense: with only one evaluation remaining, there is no possible benefit to explore, and we might as well make a purely greedy last try.\nFor active surveying, the expected utility is\n∑ y u(Dt) Pr(yt = y | xt,Dt−1)\n= Eyt [ − var[cardR | Dt] | xt,Dt−1 ] .\nThe optimal decision x∗t is therefore the point with the smallest expected variance of p(cardR | Dt). This is a bit more opaque than the active search expression above, but is still intuitively reasonable.\nGiven the optimal policy for selecting xt, we now consider the problem of choosing the location of the secondto-last point xt−1. When making our decision in this case (as well as with any other xi with i < t), the problem becomes more difficult because we must now contemplate the possible consequences of our choices and how they will impact our future decisions. The mechanical manifestation of this remark is that during the calculation of the expected utility for the two-step lookahead case, we must integrate out the unknown\nlocation of the final observation xt, as well as its label: E [ u(Dt) | xt−1,Dt−2 ] =∫∫∫\nu(Dt) Pr(yt−1 | xt−1,Dt−2)p(xt | Dt−1)· · ·\n· · ·Pr(yt | xt,Dt−1) dyt−1 dxt dyt. (1)\nNote, however, that the integral over xt can be evaluated trivially because p(xt | Dt−1) is simply δ(xt−x∗t ),1 where δ is the Dirac delta function—that is, given the value of yt−1, the location of the last choice xt is deterministic and known from our discussion above.\nTo evaluate the two-step expected utility at a point xt−1, we therefore sample over the unknown value yt−1 ∈ {0, 1}; for each possible value of yt−1, we find the optimal last observation x∗t given that fictitious observation as described above. Note that sampling over y∗t is not required in the search case.\nWe may repeat the procedure described above recursively to calculate the expected `-step lookahead utility of choosing a point for any ` ≤ t, allowing us to operate on any horizon. We note that some authors would equivalently discuss the preceding analysis in terms of Bellman’s equation and Markov decision processes (mdps); our choice of presentation is purely stylistic.2\nAs noted in (Garnett et al., 2011), the optimal policies for both of these problems in general requires running time O ( (2 cardX )` ) . For lookahead more than a few steps into the future, this procedure can become daunting due to the sampling required. This is a common issue in sequential Bayesian decision problems. One typical way to address this problem is to approximate exact inference by shortening our horizon (Jones et al., 1998; Osborne et al., 2009). For timestep t−m with m > `, we myopically pretend that there are only ` observations remaining and choose xt−m by maximizing the `-step lookahead expected utility. We will address this issue further in Section 6 and show how in some reasonable cases we can restrict the exponential search space required to find the `-step optimal decision."
    }, {
      "heading" : "5. Potential Gain from Looking Ahead",
      "text" : "In this section we will discuss the behavior of the `- step optimal search policy versus the m-step policy, for ` < m. Let us first consider the behavior of the twostep policy versus the simple greedy one-step policy. Notice that the two-step policy allows us to make decisions that do not maximize the posterior probability\n1Note that x∗t depends on the unknown value of yt−1. 2In the mdp form, the potential for computational savings via dynamic programming is more apparent; however, the state space is still exponential in t.\nof observing a target at the current step. Instead, we might choose to explore a region where the probability of immediate reward is lower, but where there is a chance of discovering more targets overall during the next two evaluations. We will give a very simple example that demonstrates the effect of this tradeoff in the active search case. Figure 1 shows a three-point space. The two connected points have the same label; they are known to either both be targets or both be nontargets, with marginal probability of their being targets ε. The point on the right is independent of the others and has probability of being a target δ > ε. Consider being allowed two label queries with the goal of locating as many targets as possible. We may calculate the expected performance of the one- and two-step policies directly. The one-step policy will always choose the right point first and then will be compelled to choose either of the left points, with expected final utility ε+δ. The expected two-step utility of the left points is each 2ε+ (1− ε)δ, and the expected two-step utility of the right point is ε+δ. The difference in two-step expected utility between either left point and the right point is ε(1− δ) > 0; therefore one of the left points will always be chosen, and the two-step policy will outperform the one-step greedy policy on average for any value of δ.\nThis example demonstrates the sort of nontrivial decisions that the optimal policy can make—it can be better to explore a region where labels are expected to be highly correlated, even when the probability of being “lucky” and finding many targets is much smaller than the current most likely single point. A welcome side effect of this behavior is that when such a decision is made, we learn about the labels of the points in the chosen region even if we do not observe a target. Such evaluations can therefore be advantageous by our having improved the overall quality of our probabilistic model, despite this goal not having been specified at any step during our derivation of the optimal policy.\nWe can extend the ideas in the above example to prove that, in the case of active search, increasing our horizon can always improve performance by any arbitrary degree. Let P , (Ω, 2Ω,Pr) be a (discrete) probability space on Ω. Given P , we will denote the expected utility of the `-step-lookahead policy after t evaluations with ED [ u(D) | `, t,P ] . We may prove the following.\nTheorem 1. Let `,m ∈ N+, ` < m. For any q > 0, there exists a P and t such that\nED [ u(D) | m, t,P ] ED [ u(D) | `, t,P\n] > q; that is, the m-step active-search policy can outperform the `-step policy by any arbitrary degree.\nProof. As in the simple example above, the key to the argument is that `-step lookahead cannot differentiate between a “clump” of correlated points of size ` and one of size greater than `. To formalize this concept, define a (k, ε)-clump, denoted k ε, to be a collection of k discrete points that all have the same label, with marginal probability of being all targets ε.\nConsider applying the m-step lookahead policy for querying t labels on the space\nP , t⊔ i=1 t ε.\nThe policy is easy to analyze in this case. After no evaluations, every point in the domain has the same expected m-step utility by symmetry. After observing that point, either a clump of targets will have been discovered (with probability ε), or a clump of nontargets. In the former case, the remaining t − 1 evaluations will all be spent querying the remaining points in the selected clump, because they will all have maximal expected utility for all horizons. In the latter case, a point in another unobserved clump will be chosen, and the response to the outcome will be the same. Given this, we may calculate:\nED [ u(D) | m, t,P ] = t∑ i=1 ε(1− ε)i−1(t− i+ 1). (2)\nWe now augment P with t copies of ` δ, with δ > ε:\nP , ( t⊔ i=1 t ε ) ∪ ( t⊔ i=1 ` δ ) .\nIt is trivial to show that the `-step utility of a point in a ` δ is greater than the `-step utility of a point in a\nt ε. The form for each is of the same form as (2), and their difference may be calculated directly; it is\n(1−ε)`−(1−δ)`+δ−1 ( (1−δ)`−1 ) −ε−1 ( (1−ε)`−1 ) > 0.\nDespite the fact that the t-clump has more potential targets, the `-step lookahead policy greedily chooses the more immediately fruitful `-clump.\nWith this, we may find an upper bound for ED [ u(D) |\n`, t,P ] . Consider build a string S (initially empty) as follows. Sample r from U(0, 1). If r < δ, append ` 1s to S; otherwise, append a 0. Repeat a k times. At termination, the probability of a character in S being a 1 does not depend on k; it is δ /̀δ(`−1)+1. We can simulate the `-step method by stopping when length(S) ≥ t and taking the first t characters. When we stop, the expected number of 1s in the first t can obviously not be greater than the expected number in S, which has length at most (t+ `− 1):\nED [ u(D) | `, t,P ] <\n(t+ `)δ` δ(`− 1) + 1 . (3)\nThe final component of the proof is showing that even if ε < δ, the m-step lookahead expected utility of a point in one of the t-clumps is greater than the mstep lookahead expected utility of a point in one of the `-clumps. One can in fact prove the following, which generalizes the situation in Figure 1 (consider as → δ−).\nLemma 1. Let `,m, k ∈ N+, ` < m ≤ k, and δ ∈ (0, 1) be given. Then there is a ε < δ such that the expected m-step utility of a point in an unobserved k ε is greater than that of a point in an unobserved ` δ.\nSet ε < δ such that the m-step policy selects the t ε clumps. Notice that the m-step policy will behave identically as before, except that it can now switch to the ` δ clumps with fewer than m evaluations remaining in “unlucky” cases. The right-hand side of (2) therefore still serves as a lower bound on its performance in this new space.\nFinally, combining the lower bound in (2) and the upper bound in (3), we have\nED [ u(D) | m, t,P ] ED [ u(D) | `, t,P\n] > ( (1− ε)t+1 + εt+ ε− 1) )( δ(`− 1) + 1 ) εδ`(t+ `) ,\nwhich may be made arbitrarily large by taking small enough δ and large enough t."
    }, {
      "heading" : "6. Bounding the Active Search Space",
      "text" : "We will now discuss how we may, in certain situations, reduce the O ( (2 cardX )` ) search space required by the `-step optimal active search policy. Our approach will entail a “branch and bound”–style strategy, where we will leverage relatively inexpensive-to-calculate inequalities to prune suboptimal branches of the search space from consideration. This will require establishing two inequalities. First, we find a lower bound on the maximal `-step active search expected utility among the unlabeled points. Next we find an upper bound on the `-step expected utility of a given unlabeled point, as a function of its current probability. Combining these bounds together will ultimately provide us with a threshold θ such that any point x with Pr(y = 1 | x,D) < θ cannot possibly be the optimal `-step action. In the below we will assume we start at timestep 1 and progress to timestep `, beginning with an arbitrary starting set D1. 6.1. A lower bound on maxE [ u(D`) | x1,D1\n] We will first establish a trivial lower bound on the maximal `-step expected utility. Let\n(x′, p′) , (arg) max x∈X\\D1 Pr(y = 1 | x,D1)\nbe the point with the highest posterior probability of being a target at the first timestep, along with its probability. Let\nu′ , E [ u(D`) | x1 = x′,D1 ] . (4)\nClearly then u′ is a trivial bound on the maximal expected `-step utility. 6.2. An upper bound on E [ u(D`) | x1,D1\n] We now find an upper bound on the `-step expected utility for any arbitrary point x1. Our approach will require the chosen classification model to meet two conditions. First, we must have that conditioning on a new nontarget observation cannot raise the target probability for any unlabeled point. Second, we must be able to bound the maximum target probability among unlabeled points after conditioning on a given number of additional targets; that is, we assume there is a function p∗(n,D) such that\np∗(n,D) ≥ max x∈X\\D\nPr ( y = 1 | x,D∪D′, ∑ y′∈D′ y ′ ≤ n ) .\n(5) With the p∗ function in hand, we will define a function u∗(`, n,D) that represents a bound on the maximum `-step utility among any unlabeled point after n addi-\ntional target observations. For ` = 1, we define\nu∗(` = 1, n,D) , p∗(n,D).\nThat this bound is valid follows immediately from the analysis of the simple one-step active search case. For ` > 1, we may build u∗ recursively:\nu∗(`, n,D) , p∗(n,D) ( u∗(`− 1, n+ 1,D) + 1 ) +(\n1− p∗(n,D) ) u∗(`− 1, n,D).\nWith u∗ now defined, for a given point x, we have\nE [ u(D`) | x1 = x,D1 ] ≤\nPr(y = 1 | x,D) ( u∗(`− 1, 1,D) + 1) +(\n1− Pr(y = 1 | x,D) ) u∗(`− 1, 0,D). (6)\nCombining (4) and (6), we may eliminate any point such that the right-hand side of (6) is less than u′, because it cannot possibly be the optimal action. Of course, we may apply this pruning technique at all depths of the search tree, allowing for deeper suboptimal subtrees to be found and eliminated as well."
    }, {
      "heading" : "7. Results",
      "text" : "We implemented the optimal active search and surveying policies in matlab, as well as uncertainty sampling. Using this implementation, we evaluated the performance of our policies on both synthetic and real data.\nIn our search experiments, we used a simple k-nearest neighbor classifier. Let nn(x) represent the k-nearest neighbors of the point x in X , and let l-nn(x) represent the subset of nn(x) for which we currently have label observations. We define\nPr(y = 1 | x,D) , γ +\n∑ x′∈l-nn(x) y ′\n1 + ∑ x′∈l-nn(x) 1 . (7)\nHere the constant γ ∈ [0, 1] serves as a “pseudocount,” which smooths the probabilities on points that have few labeled neighbors. In our experiments, we fixed γ , 1/10. This model worked well empirically, and we may also easily derive the bound on maximum probabilities in (5) required for pruning the search space as described above. If we consider a point x with current probability\nPr(y = 1 | x,D) , γ + α1 + β ,\nthen after conditioning on new observations D′ containing at most n more positive observations, we have\nPr ( y = 1 | x,D ∪D′, ∑ y′∈D′ y ′ ≤ n ) ≤ γ + α+ n1 + β + n .\nNote that this bound can be trivially modified to allow for arbitrary weight functions to be included in the model; there n could be replaced with n ( maxx′∈nn(x) w(x, x′) ) ."
    }, {
      "heading" : "7.1. Illustrative example",
      "text" : "We begin with a simple example problem that illustrates the behavior of the active search and active surveying approaches versus uncertainty sampling.\nLet I , [0, 1]2 be the unit square. We repeated the following experiment 100 times. We selected 250 points uniformly at random from I, which formed our input space X . Any point landing within Euclidean distance 1/4 of any of the points (0, 0), (0, 1), (1, 0), (1, 1) or (1/2, 1/2) (the four corners and the center point) formed the set of targets R. We picked one point uniformly at random from R and added it and its label to a training set. We then used the one-step optimal active search policy, the one-step optimal active surveying policy, and uncertainty sampling to select ten more points.\nFigure 2 shows kernel density estimates of the points selected by the algorithms across all experiments. The difference in behavior is immediate. Uncertainty sampling strongly focuses on the corners, where variance is typically the highest, the search policy strongly focuses on the learned locations of the targets, including the center, and the surveying policy strongly avoids the corners, which, despite having high variance, are not terribly informative about the space overall.\n7.2. CiteSeerx data\nFor our next experiment, we created a graph from a subset of the CiteSeerx citation network. Papers in the database were grouped based on their venue of publication (after extensive data cleaning), and papers from the 48 venues with the most associated publications were retained. The graph was defined by having these papers as its nodes (38 079 in total) and undirected citation relations as its edges. We designated all papers appearing in nips proceedings (2 198 in total, 5.2% of the dataset) as targets. Differentiating these papers is difficult; many highly related venues are also prevalent.\nFor our results presented here, we computed what Fouss et al. (2007) calls “graph principal component analysis,” which is equivalent to performing principal component analysis on cardV vectors (one corresponding to each node) embedded in R(cardV )−1 that are separated by a graph metric called commute time.3 The first 20 graph\n3The commute time between nodes v and v′ is the expected time a simple random walk beginning at v takes to hit v′ and return.\nprincipal components formed our set X , and our model was as in (7) with k , 50.\nAgain, we selected a single point at random from R to form an initial training set, then ran 500 steps of the one-, two-, and three-step active search policies with the goal of finding as many nips papers as possible. This experiment was repeated ten times.\nOn average, the one-step algorithm found 167 nips papers; the two-step algorithm found 180, and the three-step algorithm found 187. Random search would be expected to find only 29 papers given the same number of evaluations. Figure 3 shows the cumulative number of targets found by each of the methods. The three-step lookahead procedure was able to find 8.5% of the targets after scanning only 1.3% of the data, 6.5 times better than expected by random search.\nTo test active surveying, we selected 75 evaluations (starting again with a single training point from R) using three different approaches: random search, uncertainty sampling, and the one-step optimal activesurveying policy. To estimate the class proportion, we subsampled the remaining unlabeled points, selecting 5% each time, and averaged the inferred means and variances of p(cardR | D) from five such samples.\nAfter each evaluation of each method, we estimated the mean and variance of cardR given the training data collected thus far, as described above. We evaluated each method’s performance by approximating the posterior over the class proportion cardR/cardX with a beta distribution whose parameters (α, β) were selected via moment-matching to the mean and variance of the induced posterior distribution over this quantity. We then computed the likelihood of the true unknown class proportion under this beta distribution. Figure 4 shows the progression of these likelihoods for each method over the course of the experiment. After a period of time where all methods have similar performance, the optimal policy begins to significantly outperform the other two methods, which behave nearly identically. There is clear utility to our active-surveying approach, even when the number of samples taken is very small."
    }, {
      "heading" : "7.3. The effect of search-space pruning",
      "text" : "Finally, we measured the effect of our branch-andbound method described in Section 6. With the same data and experimental setup as in the CiteSeerx experiment, we measured the time required by one iteration of the optimal `-step lookahead search policy, for 2 ≤ ` ≤ 4, both with and without the advantage of our pruning method. These times were measured given 100 random starting configurations, chosen as before.\nThe results are summarized in Table 1. The effect of our pruning strategy in this case is dramatic, enabling us to extend our search horizon far beyond what the realm of possibility would have been otherwise."
    }, {
      "heading" : "8. Conclusion",
      "text" : "We have presented the Bayesian optimal policy to two atypical active-learning problems related to binary classification, which we call active search and active surveying. The former focuses on actively seeking out members of a set of identified targets as quickly as possible, and the latter focuses on predicting the portion of the dataset belonging to an identified class. Our approach was to define sensible utility functions for these problems and then to derive the optimal Bayesian policy for each of them. The optimal policy for each takes the same form, but in practice the behavior of each can be dramatically different due to the sharply contrasting underlying utility functions.\nIn addition to introducing the active surveying problem, we have extended previous preliminary work on active search in two ways. We first proved a theoretical\nresult showing that the potential advantage of farther lookahead horizons is unbounded. We then presented a branch-and-bound method for pruning the exponential search space required for active search in certain cases, which we showed can improve the computational performance of the optimal policy by orders of magnitude."
    }, {
      "heading" : "Fouss, F., Pirotte, A., Renders, J-M, and Saerens, M.",
      "text" : "Random-Walk Computation of Similarities between Nodes of a Graph with Application to Collaborative Recommendation. ieee Transactions on Knowledge and Data Engineering, 19(3):355–369, 2007."
    }, {
      "heading" : "Garnett, R., Krishnamurthy, Y., Wang, D., Schneider, J.,",
      "text" : "and Mann, R. Bayesian optimal active search on graphs. In Proceedings of the Ninth Workshop on Mining and Learning with Graphs (mlg 2011), 2011.\nGittins, J., Glazebrook, K., and Weber, R. Multi-armed Bandit Allocation Indices. John Wiley & Sons, Ltd, Chichester, uk, 2011."
    }, {
      "heading" : "He, Jingrui and Carbonell, Jaime. Rare Class Discovery",
      "text" : "Based on Active Learning. In Proceedings of the 10th International Symposium on Artificial Intelligence and Mathematics (isaim 2008), 2008."
    }, {
      "heading" : "Jones, D. R., Schonlau, M., and Welch, W. J. Efficient",
      "text" : "Global Optimization of Expensive Black-Box Functions. Journal of Global Optimization, 13(4):455–492, 1998.\nLewis, David D. and Gale, William A. A Sequential Algorithm for Training Text Classifiers. In Proceedings of the 17th Annual International acm–sigir Conference on Research and Development in Information Retrieval (sigir 1994), pp. 3–12, London, uk, 1994. Springer–Verlag.\nOsborne, M. A., Garnett, R., and Roberts, S. J. Gaussian Processes for Global Optimization. In 3rd International Conference on Learning and Intelligent Optimization (lion3), 2009."
    }, {
      "heading" : "Schein, A. I. and Ungar, L. H. Active Learning for Logistic",
      "text" : "Regression: An Evaluation. Machine Learning, 68(3): 235–265, 2007.\nSettles, Burr. Active Learning Literature Survey. Technical Report 1648, Department of Computer Sciences, University of Wisconsin–Madison, 2010. URL http://www.cs.cmu.edu/~bsettles/pub/ settles.activelearning.pdf."
    }, {
      "heading" : "Weitzman, M. L. Optimal Search for the Best Alternative.",
      "text" : "Econometrica, 47(3):641–654, 1979.\nZhu, Xiaojin, Lafferty, John, and Ghahramani, Zoubin. Combining Active Learning and Semi-Supervised Learning Using Gaussian Fields and Harmonic Functions. In Proceedings of the icml 2003 Workshop on the Continuum from Labeled to Unlabeled Data in Machine Learning and Data Mining, pp. 58–65, 2003."
    } ],
    "references" : [ {
      "title" : "Selection Bias and Covariate Imbalances in Randomized Clinical Trials",
      "author" : [ "Berger", "Vance W" ],
      "venue" : null,
      "citeRegEx" : "Berger and W.,? \\Q2005\\E",
      "shortCiteRegEx" : "Berger and W.",
      "year" : 2005
    }, {
      "title" : "Random-Walk Computation of Similarities between Nodes of a Graph with Application to Collaborative Recommendation",
      "author" : [ "F. Fouss", "A. Pirotte", "Renders", "J-M", "M. Saerens" ],
      "venue" : "ieee Transactions on Knowledge and Data Engineering,",
      "citeRegEx" : "Fouss et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Fouss et al\\.",
      "year" : 2007
    }, {
      "title" : "Bayesian optimal active search on graphs",
      "author" : [ "R. Garnett", "Y. Krishnamurthy", "D. Wang", "J. Schneider", "R. Mann" ],
      "venue" : "In Proceedings of the Ninth Workshop on Mining and Learning with Graphs (mlg",
      "citeRegEx" : "Garnett et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Garnett et al\\.",
      "year" : 2011
    }, {
      "title" : "Multi-armed Bandit Allocation Indices",
      "author" : [ "J. Gittins", "K. Glazebrook", "R. Weber" ],
      "venue" : null,
      "citeRegEx" : "Gittins et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Gittins et al\\.",
      "year" : 2011
    }, {
      "title" : "Rare Class Discovery Based on Active Learning",
      "author" : [ "He", "Jingrui", "Carbonell", "Jaime" ],
      "venue" : "In Proceedings of the 10th International Symposium on Artificial Intelligence and Mathematics (isaim",
      "citeRegEx" : "He et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2008
    }, {
      "title" : "Efficient Global Optimization of Expensive Black-Box Functions",
      "author" : [ "D.R. Jones", "M. Schonlau", "W.J. Welch" ],
      "venue" : "Journal of Global Optimization,",
      "citeRegEx" : "Jones et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Jones et al\\.",
      "year" : 1998
    }, {
      "title" : "A Sequential Algorithm for Training Text Classifiers",
      "author" : [ "Lewis", "David D", "Gale", "William A" ],
      "venue" : "In Proceedings of the 17th Annual International acm–sigir Conference on Research and Development in Information Retrieval (sigir",
      "citeRegEx" : "Lewis et al\\.,? \\Q1994\\E",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 1994
    }, {
      "title" : "Gaussian Processes for Global Optimization",
      "author" : [ "M.A. Osborne", "R. Garnett", "S.J. Roberts" ],
      "venue" : "In 3rd International Conference on Learning and Intelligent Optimization",
      "citeRegEx" : "Osborne et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Osborne et al\\.",
      "year" : 2009
    }, {
      "title" : "Active Learning for Logistic Regression: An Evaluation",
      "author" : [ "A.I. Schein", "L.H. Ungar" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Schein and Ungar,? \\Q2007\\E",
      "shortCiteRegEx" : "Schein and Ungar",
      "year" : 2007
    }, {
      "title" : "Active Learning Literature Survey",
      "author" : [ "Settles", "Burr" ],
      "venue" : "Technical Report 1648,",
      "citeRegEx" : "Settles and Burr.,? \\Q2010\\E",
      "shortCiteRegEx" : "Settles and Burr.",
      "year" : 2010
    }, {
      "title" : "Optimal Search for the Best Alternative",
      "author" : [ "M.L. Weitzman" ],
      "venue" : null,
      "citeRegEx" : "Weitzman,? \\Q1979\\E",
      "shortCiteRegEx" : "Weitzman",
      "year" : 1979
    }, {
      "title" : "Combining Active Learning and Semi-Supervised Learning Using Gaussian Fields and Harmonic Functions",
      "author" : [ "Zhu", "Xiaojin", "Lafferty", "John", "Ghahramani", "Zoubin" ],
      "venue" : "In Proceedings of the icml 2003 Workshop on the Continuum from Labeled to Unlabeled Data in Machine Learning",
      "citeRegEx" : "Zhu et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2003
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "The active search problem has been previously described (Garnett et al., 2011); here we extend that preliminary work with two contributions.",
      "startOffset" : 56,
      "endOffset" : 78
    }, {
      "referenceID" : 11,
      "context" : "Examples include generalization error (Zhu et al., 2003) and optimality criteria related to the Fisher information, such as A-optimality (Schein & Ungar, 2007).",
      "startOffset" : 38,
      "endOffset" : 56
    }, {
      "referenceID" : 3,
      "context" : "The problem as defined there can also be seen as a Bayesian multi-armed bandit, and the optimal policy can also be recovered via a Gittins index (Gittins et al., 2011).",
      "startOffset" : 145,
      "endOffset" : 167
    }, {
      "referenceID" : 9,
      "context" : "Weitzman (1979) considers an active search problem where there is no dependence between outcomes and derives the optimal policy.",
      "startOffset" : 0,
      "endOffset" : 16
    }, {
      "referenceID" : 2,
      "context" : "As noted in (Garnett et al., 2011), the optimal policies for both of these problems in general requires running time O ( (2 cardX ) ) .",
      "startOffset" : 12,
      "endOffset" : 34
    }, {
      "referenceID" : 5,
      "context" : "One typical way to address this problem is to approximate exact inference by shortening our horizon (Jones et al., 1998; Osborne et al., 2009).",
      "startOffset" : 100,
      "endOffset" : 142
    }, {
      "referenceID" : 7,
      "context" : "One typical way to address this problem is to approximate exact inference by shortening our horizon (Jones et al., 1998; Osborne et al., 2009).",
      "startOffset" : 100,
      "endOffset" : 142
    }, {
      "referenceID" : 1,
      "context" : "For our results presented here, we computed what Fouss et al. (2007) calls “graph principal component analysis,” which is equivalent to performing principal component analysis on cardV vectors (one corresponding to each node) embedded in R(cardV )−1 that are separated by a graph metric called commute time.",
      "startOffset" : 49,
      "endOffset" : 69
    } ],
    "year" : 2012,
    "abstractText" : "We consider two active binary-classification problems with atypical objectives. In the first, active search, our goal is to actively uncover as many members of a given class as possible. In the second, active surveying, our goal is to actively query points to ultimately predict the proportion of a given class. Numerous real-world problems can be framed in these terms, and in either case typical model-based concerns such as generalization error are only of secondary importance. We approach these problems via Bayesian decision theory; after choosing natural utility functions, we derive the optimal policies. We provide three contributions. In addition to introducing the active surveying problem, we extend previous work on active search in two ways. First, we prove a novel theoretical result, that less-myopic approximations to the optimal policy can outperform more-myopic approximations by any arbitrary degree. We then derive bounds that for certain models allow us to reduce (in practice dramatically) the exponential search space required by a naïve implementation of the optimal policy, enabling further lookahead while still ensuring that optimal decisions are always made.",
    "creator" : "LaTeX with hyperref package"
  }
}