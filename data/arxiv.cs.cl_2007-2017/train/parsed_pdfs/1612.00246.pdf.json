{
  "name" : "1612.00246.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Lahari Poddar", "Pushpak Bhattacharyya", "Manish Shrivastava", "Dhirendra Singh", "Kashiviswanatha Sarma", "Samir Janardan" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "MULTILINGUAL MULTIWORD EXPRESSIONS\nM.Tech Report\nSubmitted in partial fulfillment of the requirements\nfor the degree of\nMaster of Technology\nby\nLahari Poddar\nRoll No:113050029\nUnder the guidance of\nProf. Pushpak Bhattacharyya\nDepartment of Computer Science and Engineering\nIndian Institute of Technology, Bombay\nMumbai\nii\niii\niv\nAbstract The project aims to provide a semi-supervised approach to identify Multiword Expressions in a multilingual context consisting of English and most of the major Indian languages. Multiword expressions are a group of words which refers to some conventional or regional way of saying things. If they are literally translated from one language to another the expression will lose its inherent meaning. Example:\nEnglish: apple of an eye\nHindi: आँख का तारा (Aankhon ka taara)\nBengali: চােখর মিণ(chokher moni)\nTo automatically extract multiword expressions from a corpus, an extraction pipeline have been constructed which consist of a combination of rule based and statistical approaches. There are several types of multiword expressions which differ from each other widely by construction. We employ different methods to detect different types of multiword expressions. Given a POS tagged corpus in English or any Indian language the system initially applies some regular expression filters to narrow down the search space to certain patterns (like, reduplication, partial reduplication, compound nouns, compound verbs, conjunct verbs etc.). The word sequences matching the required pattern are subjected to a series of linguistic tests which include verb filtering, named entity filtering and hyphenation filtering test to exclude false positives. The candidates are then checked for semantic relationships among themselves (using Wordnet). In order to detect partial reduplication we make use of Wordnet as a lexical database as well as a tool for lemmatizing. We detect complex predicates by investigating the features of the constituent words. Statistical methods are applied to detect collocations. Finally, lexicographers examine the list of automatically extracted candidates to validate whether they are true multiword expressions or not and add them to the multiword dictionary accordingly. A universal web service has been developed in order to facilitate multiword expression extraction across the various research groups in India.\nv\nApart from Multiword Expressions Extraction, in this report a Common Concept Hierarchy is also proposed which is a linked structure of wordnets of 18 different Indian languages, Universal Word dictionary and the Suggested Upper Merged Ontology (SUMO). The system is encoded in Lexical Markup Framework (LMF) and we propose modifications in LMF to accommodate Universal Word Dictionary and SUMO. This standardized version of lexical knowledge base of Indian Languages can now easily be linked to similar global resources.\nvi"
    }, {
      "heading" : "Acknowledgement",
      "text" : "I would like to take this opportunity to sincerely thank Prof. Pushpak Bhattacharyya for his insights, constant support and encouragement. His guidance has been my primary source of motivation.\nI would like to thank Munish Minia for making his report and work available for reference. I am grateful to Manish Shrivastava, Dhirendra Singh, Kashiviswanatha Sarma, Samir Janardan Sohoni for their valuable insights and intellectual contribution towards the project. I am thankful to all the members of CFILT at IIT Bombay for their keen interest and contributions, direct or indirect towards the project. It is the detailed discussions and brainstorming analysis carried out at our weekly meetings that has kept me motivated and deeply interested in this topic. I would finally like to thank my parents and friends who always had faith in me and motivated me to make this possible.\nvii\nTable of Contents Abstract .................................................................................................................................... iv Acknowledgement .................................................................................................................... vi Table of Contents ..................................................................................................................... vii Chapter 1 ....................................................................................................................................x Introduction ................................................................................................................................1\n1.1. Introduction to Multiword Expressions .........................................................................1 1.1. Motivation ....................................................................................................................2 1.2. Organization of the report .............................................................................................4 Chapter 2 ....................................................................................................................................5 Background .................................................................................................................................5\n2.1. Features of Multiword Expressions ...............................................................................5 2.2. Types of MWEs ............................................................................................................6 2.3. Classification of MWEs ................................................................................................8 2.4. Necessary and Sufficient Conditions for MWE ........................................................... 14\n2.4.1 Necessary Conditions ........................................................................................... 14 2.4.2 Sufficient Conditions ........................................................................................... 14 2.5. MWE Extraction tasks ................................................................................................ 15 Chapter 3 .................................................................................................................................. 17 MWE Extraction Approaches .................................................................................................... 17\n3.1. Approaches by various researchers .............................................................................. 17 3.1.1 Rule Based Approaches ....................................................................................... 17 3.1.2 Statistical Methods for Multiwords Extraction ..................................................... 21 3.1.3 Word Association Measures ................................................................................. 29 3.1.4 Retrieving Collocations From Text : XTRACT .................................................... 31 3.1.5 Collcation Extraction By Conceptual Similarity ................................................... 34 3.1.6 Verb Phrase Idiomatic Expressions ...................................................................... 39 3.2. Study of an Ongoing Project: MWEToolkit ................................................................. 40 3.2.1 MWEToolkit System Architecture ....................................................................... 41 3.2.2 Using Web as corpora .......................................................................................... 42 Chapter 4 .................................................................................................................................. 43 Multiword Extraction Engine .................................................................................................... 43\n4.1. System Overview ........................................................................................................ 43\nviii\n4.1.1 System Features ................................................................................................... 46 4.1.2 Integrated Resources ............................................................................................ 46 4.2. MWE Extraction Engine Pipeline ................................................................................ 46 4.2.1 Regular Expression Filter ..................................................................................... 46 4.2.2 Linguistic Filter ................................................................................................... 48 4.2.3 Complex Predicate Filter ...................................................................................... 50 4.2.4 Semantic Filter ..................................................................................................... 53 4.2.5 Partial Reduplication Filter .................................................................................. 54 4.2.6 Statistical Filter .................................................................................................... 55 4.2.7 Manual Evaluation ............................................................................................... 63 4.2.8 Universal Web Service ......................................................................................... 63 Chapter 5 .................................................................................................................................. 64 Experimentations ...................................................................................................................... 64\n5.1. Using Parallel Corpora ................................................................................................ 64 5.1.1 Multilingual Aspects of Multiword Expressions ................................................... 64 5.1.2 Motivation ........................................................................................................... 65 5.1.3 Automatically adding MWEs to Wordnet ............................................................. 66 5.1.4 Case Study ........................................................................................................... 67 5.2. Investigating Sanskrit Traditions ................................................................................. 68\n5.2.1 बहु ी ह: समासः (Exocentric Compounds) ............................................................. 68\n5.2.2 अ ययीभावः समासः (Adverbial Compounds) ........................................................ 69\n5.2.3 त पु षः समासः (Determinative Compounds) ....................................................... 70\n5.2.4 व व (Coordinating compounds) ....................................................................... 71 Chapter 6 .................................................................................................................................. 73 Evaluation of MWE Extraction Engine ..................................................................................... 73\n6.1. Evaluation of MWE engine on English Corpus ........................................................... 73 6.2. Evaluation of MWE engine on Hindi Corpus .............................................................. 74 6.3. Evaluation of MWE engine on Bengali Corpus ........................................................... 76 Chapter 7 .................................................................................................................................. 77 Common Concept Hierarchy ..................................................................................................... 77\n7.1. Motivation .................................................................................................................. 77 7.2. Related Work .............................................................................................................. 78 7.3. IndoNet ....................................................................................................................... 80\nix\n7.3.1 Common Concept Hierarchy (CCH) .................................................................... 80 7.4. Observation ................................................................................................................. 83 7.5. Conclusion .................................................................................................................. 84 Chapter 8 .................................................................................................................................. 85 Generic Stemmer ...................................................................................................................... 85\n8.1. Construction of the Generic Stemmer .......................................................................... 85 8.1.1 Storing Wordnet in a Trie ..................................................................................... 86 8.1.2 Why Trie? ............................................................................................................ 86 8.1.3 Structure of the Trie ............................................................................................. 86 8.2. GUI of the Stemmer .................................................................................................... 88 8.3. Integration with Multilingual IndoWordnet Search ...................................................... 89 Chapter 9 .................................................................................................................................. 91 Conclusion and Future Work ..................................................................................................... 91\n9.1. Conclusion .................................................................................................................. 91 9.2. Future Work ................................................................................................................ 91\n9.2.1 Short Term Future Work ...................................................................................... 92 9.2.2 Medium Term Future Work ................................................................................. 92 9.2.3 Long Term Future Work ...................................................................................... 92 Building a Machine Learning model for classification ....................................................... 92\nBibliography ............................................................................................................................. 93 PUBLICATION ........................................................................................................................ 96\nx\nList of Figures\nFigure 1.1: Machine translation of MWEs ..................................................................................3 Figure 2.1: MWE extraction tasks (Pushpak Bhattacharyya, LREC 2012 ) ............................... 16 Figure 3.1: Finding Collocations: Frequency Method [19] ........................................................ 22 Figure 3.2: Finding Collocations: Mean and Variance[19] ........................................................ 23 Figure 3.3: Hypothesis Testing Of Differences [19] .................................................................. 25 Figure 3.4: Pearson's Chi-Square Test [19] ............................................................................... 25 Figure 3.5: Likelihood Ratio[19] .............................................................................................. 26 Figure 3.6: Relative Frequency Ratio [19] ................................................................................ 27 Figure 3.7 : Producing concordances for “the Dow Jones Industrial Average”[20].................... 33 Figure 3.8: Producing the “NYSE's composite index of all its listed common stocks “ [20] ...... 33 Figure 3.9: Collocational Information for 'baggage' and 'luggage'[14] ....................................... 36 Figure 3.10: Intersection Of Concept Sets for information and data .......................................... 37 Figure 3.11: Collocational Preference ....................................................................................... 38 Figure 4.1: MWE Engine Pipeline ............................................................................................ 45 Figure 7.1 : An Example of Indonet Structure........................................................................... 80 Figure 7.2 : LMF representation for Universal Word Dictionary ............................................... 82 Figure 7.3: LMF representation for Common Concept Hierarchy ............................................. 83 Figure 8.1: Example of Hindi wordnet representation in trie ..................................................... 87 Figure 8.2: GUI of generic stemmer ......................................................................................... 89 Figure 8.3: Multilingual IndoWordnet Search ........................................................................... 90\n1\nChapter 1\nIntroduction Lexemes or tokens are basic units of natural language. Following the syntactic structure of a natural language they come together and interact with each other to form a meaningful sentence. Sometimes they convey a meaning as a single unit and sometimes multiple simple words function as a single lexical unit to convey a meaning."
    }, {
      "heading" : "1.1. Introduction to Multiword Expressions",
      "text" : "Multi Word Expressions are defined as an expression crossing word boundaries that refer to some conventional or regional way of saying things[19]. They are arbitrary word combinations that are very frequent in natural language hence they are also termed as collocations. In Wordnet 1.7 around 41% of the entries are multiword. Informally, multiword refers to a group of words, if literally translated from one language to another will lose their inherent meaning.\nAs an example of collocation, let us consider the term strong coffee. We always use the adjective strong to describe coffee or tea but not its other synonyms (say, powerful). Whereas in case of ‘drugs’, ‘powerful drug’ is more frequently used than ‘strong drug’. There is no specific reason as to why one representation or interpretation should be chosen over the other. It is just the way it is. Multiwords are random, arbitrary and idiosyncratic. They are completely language dependent and are transparent only to a native speaker of the language.\nExamples: 1. Pitter patter drops the rain\n[Hindi] टप टप पानी बरस रहा है (Tip tip paani baras raha hain)\n[Bengali] টাপুর টুপুর বৃি পের (tapur tupur bristi pore)\n2. He is working day and night\n[Hindi] वोह दन रात काम कर रहा ह (woh din raat kam kar raha hain)\n2\n[Bengali] ও সারা িদন রাত কাজ করেছ (o sara din raat kaj korche)\n3. [Hindi] घर घर म द प जले (ghar ghar mein deep jale)\n[Bengali] ঘের ঘের দীপ েল (ghore ghore deep jale)\nGloss: home home candle lit Translation: In every home candle is lit"
    }, {
      "heading" : "4. Down to earth person",
      "text" : "[Bengali] মা র মানুষ (matir manush)"
    }, {
      "heading" : "1.1. Motivation",
      "text" : "The motivation behind extracting collocations from text is that this collocational information will be useful in a number of Natural Language Processing tasks. • Machine Translation: Collocations differ from language to language. So a multiword\nexpression cannot directly be translated from one language to another conserving its inherent idiosyncrasy or metaphoric meaning.\nFor example: In Hindi ‘aankhon ka taara’ means someone's favorite person. The same meaning is conveyed by the English idiom ‘apple of an eye’. But they are not direct literal translations of one another. As another example, we have the expression red handed in English which means to catch someone while in the act of committing a crime has its Hindi counterpart as ‘range haath’ (not ‘laal haath’, the literal translation).\nFollowing is a snapshot from Google translator as an evidence of the fact that an automatic machine translation system needs to have the knowledge of multiword expressions of a language. As we can see in Figure 1.1 the translations done by a machine translation system from English to Hindi is erroneous due to the ignorance of multiword expressions in the sentence.\n3\n• Natural Language Generation: Natural Language Generation refers to the act of generating\ntext in natural language from a logical symbolic form. This task requires appreciating the nuances of the language. Collocation is one of them. If collocations are not accounted for while generating the text then some word combinations may accidentally occur in the text which have some inline meaning. This ignorance will affect the linguistic quality of the generated text. • Automatic Simplification of Text: Some of the text editors allow automatic simplification\nor modification text by providing synonyms of a particular word or expression. If it doesn't have the knowledge of collocations then it might suggest synonyms which are inappropriate or while simplifying an expression it might lose its metaphorical meaning. So such tools need to be aware of the collocational constraints of the language. • Enhancing natural language lexical resources: Multiword expressions give crucial\nknowledge about a language. They are highly prevalent and very irregular in nature. Hence MWEs must be stored in lexicons of natural language processing applications like Wordnet and disambiguated universal word dictionaries. For example:\n4\n उसके दादाजी ने दम तोडा\n Transliteration: uske dadaji ne dam toda\n Gloss: His grandfather died.\n Translation: His grandfather kicked the bucket.\nIn the above example kick the bucket and दम तोडा means to die. So these multiword expressions\nshould enter the corresponding synsets in wordnet as well as universal word dictionary.\nMotivated by the importance of multiword expressions in natural language processing tasks we, at CFILT IIT Bombay chose to address the problem of extracting multiword expressions from a given text corpora. We have developed a pipeline for extracting multiword expressions and making a repository of these expressions so that other fields of NLP can benefit from it. The current work contributes to the Cross Lingual Information Access (CLIA) and Indian Language Machine Translation efforts being undertaken by consortia of academic institutions across India."
    }, {
      "heading" : "1.2. Organization of the report",
      "text" : "In this chapter, we have introduced the concept of MWEs and explained the various fields of Natural Language Processing that needs to be concerned about the knowledge of Multiword Expressions. In Chapter 2, at first the MWEs are formally defined. We also present a classification of MWEs according to different criteria and the necessary and sufficient condition for an expression to be classified as MWE. In Chapter 3 we describe the different methods to extract MWEs from a corpus and class them as statistical method and knowledge-based method. In this chapter, another contemporary ongoing project in this research field for extracting MWEs is described. Chapter 4 describes my contribution in the development of MWE Engine in IIT Bombay. Chapter 5 describes some experiments that I have performed and the ongoing research activity regarding the project at IIT Bombay. Chapter 6 gives the evaluation of the performance of the system. In Chapter 7 we present another contribution of ours, a Common Concept Hierarchy which is built by merging wordnet, universal word dictionary and an upper ontology. Chapter 8 describes the construction of a generic stemmer, its functionalities and applications. Finally, Chapter 9 concludes this report with our perspectives and listing our future goals.\n5\nChapter 2\nBackground\nIn this section we will describe the formal definition Multiword Expression along with the necessary and sufficient conditions for an expression to be termed as MWE. The different types and characteristics possessed by such expressions are elaborated. We will also specify the necessary and sufficient conditions for an expression to be classified as an MWE. Finally we will look at the different tasks to be done for extracting MWEs from a text.\nVarious researchers have defined multiword expressions differently during their research. We’ll present some of the definitions here and it can be observed that all of them primarily refer to a single central concept.\n• A collocation is an expression consisting of two or more words that correspond to some\nconventional way of saying things.[19]\n• Idiosyncratic interpretations that cross word boundaries (or spaces) [18] • Recurrent combinations of words that co-occur more frequently than chance, often with\nnon-compositional meaning[20]\n• A pair of words is considered to be a collocation if one of the words significantly prefers a\nparticular lexical realization of the concept the other represents[14]"
    }, {
      "heading" : "2.1. Features of Multiword Expressions",
      "text" : "There are certain features that a group of words must have in order to be treated as collocation. The principal features are:\n• Non-Compositionality: The meaning of a complete multiword expression can't\ncompletely be determined from the meaning of its constituent words.\nThe meaning of the expression might be completely different from its constituents (the idiom kick the bucket means to die) or there might be some added element or inline meaning to it that cannot be predicted from the parts(the phrase back to square one means to reach back to the place from where one had started).\n6\n• Non-Substitutability: The components of a multiword expression cannot be substituted by\none of its synonyms without distorting the meaning of the expression even though they refer to the same concept.\nFor example, in the expression bread and butter the component words cannot be replaced by their synonym keeping the meaning(to earn one's daily living) intact.\n• Non-Modifiability: Many collocations cannot be freely modified by grammatical\ntransformations (like, change of tense, change in number, addition of adjective etc.). These collocations are frozen expressions, they cannot be modified in any way.\nFor example, the idiom let the cat out of the bag cannot be modified to *let the big cat out of the bag or something similar."
    }, {
      "heading" : "2.2. Types of MWEs",
      "text" : "Collocations or Multiword Expressions can be classified into different classes according to their lexical and semantic characteristics. The classification as described in [18] is given below. 1) Lexicalized Phrases: This type of phrases have some form of idiosyncratic or added\nmeaning to the structure. They are either syntactically idiosyncratic or semantically nondecomposable. Lexicalized phrases can be classified into 3 parts. a) Fixed Expressions: This is the class of expressions that defy the general conventions of\ngrammar and compositional interpretations. These expressions are completely frozen and do not undergo any modifications at all.\nExample: in short, of course, ad hoc, by and large\nb) Semi-Fixed Expressions: This type of expressions have restrictions on word order and the\nstructure of the phrase but they might undergo some form of lexical variations. Semi-Fixed expressions can be further classified into 3 subtypes:\n7\n(1) Non-Decomposable Idioms: Depending on their semantic composition, idioms can be\nclassified into two types: Decomposable and Non-Decomposable.\nFor decomposable idioms each component of the idiom can be assigned a meaning related to the overall meaning of the expression. For the idiom spill the beans, 'spill' can be assigned the sense of 'reveal' and 'beans' can denote the sense of 'secret'. But in case of Non-Decomposable idioms no such analysis is possible.\nFor the idiom kick the bucket none of its components can be assigned a sense such that the overall idiom means 'to die'.\nIt is these Non-Decomposable idioms which are semi-fixed. Due to their opaque meaning they do not undergo any syntactic variations but might allow some minor lexical modification (kick the bucket -> kicked the bucket).\n(2) Compound Nominals: Compound nominals also do not undergo syntactic\nmodifications but allow lexical inflections for number i.e. they can be changed to their singular or plural form.\nExample: car park, part of speech, railway station\n(3) Named Entities: These are syntactically highly idiosyncratic. These entities are formed\nbased on generally a place or a person.\nExample: the cricket team names in IPL are formed based on the region. In a proper context the team names are often mentioned without the name of the place, like '(Kolkata) Knight Riders', 'Royal Challengers (Bangalore)' etc. When the team name occurs as a modifier in some compound noun a modifier is added ('the Kolkata Knight Riders player...' )\nc) Syntactically-Flexible Expressions: As opposed to the strict word order constraint of Semi-\nFixed expressions, Syntactically-Flexible expressions allow a wide variety of syntactic variations. They can be classified into 3 types:\n8\n(1) Verb-Particle Construction: Verb-Particle constructions or phrasal verbs consist of a\nmain verb and a particle. Transitive verb-particle constructions are a good example of non adjacent collocations as they can take an NP argument in between (like, call him up).\nExample: call off, write up, eat up etc.\n(2) Decomposable idioms: Decomposable idioms are syntactically flexible and behave\nlike semantically linked parts. But it's difficult to predict exactly what type of syntactic variations they undergo.\nExample: spill the beans, let the cat out of the bag\n(3) Light-Verb Constructions: Verbs with little semantic content (make, take, do) are\ncalled light verbs as they can form highly idiosyncratic constructions with some nouns.\nExample: make a decision , do a favor, take a picture etc are light-verb constructions as there is no particular reason why do me a favor should be preferred over *make me a favor and so on.\n2) Institutionalized Phrases: These phrases are completely compositional (both syntactically\nand semantically) but are statistically idiosyncratic. These are just fixed terms which do not have any alternate representations.\nExample: traffic light, fresh air, many thanks, strong coffee etc."
    }, {
      "heading" : "2.3. Classification of MWEs",
      "text" : "Multiword Expressions can be classified into the following groups:\n1. Reduplication: Reduplication is a morphological process by which the root or stem of a word, or part of it, is repeated[29]\ni. Onomatopoeic Reduplication: The constituent words do not have any dictionary\nmeaning; rather they imitate a sound or an action along with the sound.\n9\nExample:\n1. knock knock\nGloss: The sound of knocking at door\n2. टक टक (Hindi)\nTransliteration: tik tik Gloss: The sound of a clock Translation: tik tik\n3. खट खट (Hindi)\nTransliteration: khat khat Gloss: The sound of knocking at door Translation: knock knock\n4. ছম ছম (Bengali)\nTransliteration: chham chham Gloss: the sound of anklets\n5. பட பட (Tamil)\nTransliteration: pada pada Translation: fluttering of wings\nii. Non-Onomatopoeic Reduplication: The constituent words are meaningful and they are\nrepeated to convey some particular sense. Example:\n1. slowly slowly\n2. अ ह ता अ ह ता (Hindi)\nTransliteration: ahista ahista Gloss: slowly slowly\n3. চলেত চলেত (Bengali)\nTransliteration: cholte cholte Gloss: walking walking Translation: while walking\n10\n4. வழ வழ (Tamil)\nTransliteration: vazha vazha Gloss: smooth smooth Translation: Very smooth texture\n5. வ வ (Tamil)\nTransliteration: veetukku veedu Gloss: to house house Translation: Every house\n2. Partial Reduplication: Only one of the words among the constituent words is meaningful\nwhile the second word is constructed by partially reduplicating the first word. Example:\n1. আেবাল তােবাল (Bengali)\nTransliteration: abol tabol Translation: gibberish\n2. বাকা সাকা (Bengali)\nTransliteration: boka soka Translation: foolish\n3. पानी वाणी (Hindi)\nTransliteration: pani vani Translation: water\n4. ேகாவ ள (Tamil)\nTransliteration: kovil kulam Gloss: temple pond Translation: temple\n3. Semantic Relationship: Sometimes the paired words have some semantic relationship\namong themselves Example: Synonym\n11\n1. धन दौलत (Hindi)\nTransliteration: dhan daulat Translation: wealth\n2. সুখ শাি (Bengali)\nTransliteration: sukh shanty\nGloss: Happiness and peace\nAntonym\n3. दन रात (Hindi)\nTransliteration: din raat Gloss: day and night Translation: round the clock\n4. জীবন মরণ (Bengali)\nTransliteration: jibon moron Gloss: life and death\n5. ரா தி பகலா (Tamil)\nTransliteration: rathiri pagala Translation: all the time Gloss: night and day\nSister Words\n6. चाये पानी (Hindi)\nTransliteration: chaye pani Gloss: tea water Translation: tea and snacks\n7. লখা পড়া (Bengali)\nTransliteration: lekha poda Gloss: writing reading Translation: study\n12\n4. Collocations: Collocations are statistical idiosyncrasies of a language. They do not have any\nsyntactical peculiarity or share any semantic relationship. These are just fixed expressions which appear very frequently in natural language without undergoing any modifications. Example: traffic light, bus driver, fresh air 5. Light Verb Constructs: Light verb constructs are formed by the combination of Noun +\nVerb where the verb has lost its meaning partially but the noun is used in its original sense.[25]\nExample: have a seat, make a decision, do a favor, take a picture\nIn the above examples as we can see in the noun verb constructs the verbs do not carry much meaning i.e. there is no reason why we should say ‘have a seat’ instead of *‘take a seat’. Due to this eccentricity we need to store such constructs but not all Noun + Verb combinations are light verb constructs. Consider the examples:\n ज हाई लेना (Hindi)\nTransliteration: jamhai lena Gloss: yawn take Translation: To yawn\n चाय लेना (Hindi)\nTransliteration: chaye lena Gloss: tea take Translation: To take tea\nIn the above examples the first one is a light verb construct (since the verb doesn’t convey much meaning) whereas the second one is completely compositional. So it is necessary to detect true light verb constructs and store them instead of storing all Noun+Verb combinations. 6. Compound Verb: A compound verb or complex predicate is a multi-word compound that\nacts as a single verb. One component of the compound is a light verb or vector, which carries any inflections, indicating tense, mood, or aspect, but provides only fine shades of meaning.\n13\nThe other, \"primary\", component is a verb which carries most of the semantics of the compound, and determines its arguments.[28]\nCompound verbs are very common in Indian languages. It is represented by Verb + Verb combination where the first verb is called ‘polar verb’ and the second verb is called ‘vector verb’. Example:\n राम घर से नकल गया (Hindi)\nTransliteration: Ram ghar se nikal gaya Gloss: Ram home of out went Translation: Ram went out of home\n আিম এই কাজটা কের দব (Bengali)\nTransliteration: ami ei kajta kore debo Gloss: I this work do will Translation: I will do this work\n ெச வ (Tamil)\nTransliteration: Senru vidu Translation: Go away\nCompound verbs i.e. this Verb + Verb combinations are very rare in English but this similar fact can be observed in English with Verb + Preposition combinations.\nExample:\n English: Finish off the work fast\n Bengali: কাজটা তাড়াতািড় কের ফেলা (kajta taratari kore felo)\n Hindi: य़ेह काम ज द ख़तम कर दो (yeh kaam jaldi khatam kar do)\n7. Compound Noun: Compound noun refers to the phenomena where two or more nouns\ncombine to form a new compound. This type of compounds is highly generative, i.e. new compounds get added to the language very frequently as the language evolves. Noun compounds belong to the most frequent classes of Multiword Expressions.\n14\nExample:\nScience fiction writer, railway station\n எதி க சி (Tamil)\nTransliteration: ethir katchi Gloss: opposition party\n க ைம (Tamil)\nTransliteration: kan mai Gloss: kajal"
    }, {
      "heading" : "2.4. Necessary and Sufficient Conditions for MWE",
      "text" : "In this section we describe the standard that has been agreed upon during Kashmir Multiword Workshop 2011. The necessary and sufficient conditions for an expression to be classified as MWE is as follow:"
    }, {
      "heading" : "2.4.1 Necessary Conditions",
      "text" : "For a word sequence to be a MWE, it has to be separated by space/delimiter.\nExample:\n இ திய கி ெக அண (Tamil)\nTransliteration: Indhiya kirikket ani Gloss: India cricket team Translation: Indian Cricket Team"
    }, {
      "heading" : "2.4.2 Sufficient Conditions",
      "text" : "The sufficient conditions to be an expression to be classified as MWE are:\n1. The non-compositionality of meaning of the MWE, i.e. meaning of a MWE\ncannot be derived from its constituents. Examples:\n ెట ◌ి◌ం ీ డర (Telugu)\nTransliteration: cevttu kimda plidaru\n15\nGloss: a lawyer sitting under the tree Translation: an idle person\n2. The fixity of expression, i.e. the constituents of MWE cannot be replaced by its\nsynonyms or other words. Examples:\n Correct: life imprisonment\n*Incorrect: lifelong imprisonment\n Correct: Many thanks\n*Incorrect: Plenty thanks"
    }, {
      "heading" : "2.5. MWE Extraction tasks",
      "text" : "Armed with the background knowledge of the definition and features of Mutiword Expressions we tried to classify them into different classes according to the most suitable extraction approach. Figure 2.1 shows the classification.\nAs we can see, both statistical and rule based approaches are necessary to solve this problem. We can also see from the following figure the stack of NLP tools that need to be deployed in order to identify different classes of Multiword Expressions.\nNLP\nML\nString + Morphology\nPOS tagging POS tagging\n+ Wordnet\nPOS tagging + List\nChunking Parsing\nRules Onomaetopic\nReduplication (tik tik, chham chham)\nNonOnomaetopic Reduplication (ghar ghar)\nSemantic relation (Synonym, Antonym, Hypernym) (raat din, dhan doulat, chaye paani)\nNoncontiguous something\n16\nStatistical Colloctions or\nfixed expressions (many thanks)\nConjunct verb (verbalizer list), Compund verb (verctor verb list) (salaha dena, has uthama)\nNoncontiguous Complex Predicate\nFigure 2.1: MWE extraction tasks (Pushpak Bhattacharyya, LREC 2012 )\n17\nChapter 3\nMWE Extraction Approaches"
    }, {
      "heading" : "3.1. Approaches by various researchers",
      "text" : "In this section we are going to present a survey of the different approaches tried out by different researchers over the years in order to extract multiword expressions from a text. The methods vary widely from one another. Some of them have taken a Linguistic approach, some have used statistical techniques and some have taken help of the open source resources available to us to solve the problem."
    }, {
      "heading" : "3.1.1 Rule Based Approaches",
      "text" : "There have been quite a few approaches which try to detect multiwords by leveraging the rules forming them in the first place."
    }, {
      "heading" : "3.1.1.1. Identification of Reduplication in Bengali",
      "text" : "Reduplication is a subtype of Multiword Expressions and a method for identifying reduplications and then classifying them has been reported by the authors. Reduplications have been categorized into 2 levels in [5], namely Expression Level and Sense Level. They can be further subcategorized as: Expression Level:\na) Onomatopoeic expressions: The constituent words imitate a sound or a sound of an\naction. Generally in this case the words are repeated twice with the same ‘matra’.\n ঝম ঝম (Bengali)\nTransliteration: jham jham Translation: the sound of rain\n টপ টপ (Bengali)\nTransliteration: top top Translation: the sound of dropping water\n18\nb) Complete Reduplication: The constituent words are meaningful and they are repeated to convey some particular sense.  চলেত চলেত (Bengali)\nTransliteration: chalte chalte Gloss: walking walking Translation: while walking\n বার বার (Bengali)\nTransliteration: bar bar Gloss: time time Gloss: time and again/ repeatedly\nc) Partial Reduplication: In partial reduplication generally three cases are possible\n(i) change of the first vowel or the matra attached with first consonant (ii) change of consonant itself in first position (iii)change of both matra and consonant\n বাকা সাকা (Bengali)\nTransliteration: boka soka Translation: Foolish\n চাল চুেলা (Bengali)\nTransliteration: chal chulo\nTranslation: belongings\nd) Semantic Reduplication: A dictionary based approach was followed to identify\nconsecutive occurrences of synonyms and antonyms.\n িদন রাত (Bengali)\nTransliteration: din-raat Gloss: day and night Translation: round the clock/ all the time\n পাপ পুণ (Bengali)\nTransliteration: paap-punyo Gloss: sin and virtue\n19\nSense Level Classification:\na) Sense of repetition:\n রাজ রাজ (Bengali)\nTransliteration: roj roj Gloss: day day Translation: everyday\n বছর বছর (Bengali)\nTransliteration: bachor bachor Gloss: year year Translation: every year\nb) Sense of plurality:\n ছােটা ছােটা (Bengali)\nTransliteration: choto choto Gloss: small small Translation: small\nc) Sense of Emphatic :\n সু র সু র (Bengali)\nTransliteration: sundor sundor Gloss: beautiful beautiful Translation: beautiful\n লাল লাল (Bengali)\nTransliteration: laal laal Gloss: red red Translation: red\nd) Sense of completion :\n খেয় দেয় (Bengali)\nTransliteration: kheye deye Translation: after finishing meal\ne) Sense of incompleteness :\n20\n বলেত বলেত (Bengali)\nTransliteration: bolte bolte Gloss: talking talking Translation: while talking\n চলেত চলেত (Bengali)\nTransliteration: cholte cholte Gloss: walking walking Translation: while walking\nSome collected articles of Rabindranath Tagore have been used as a corpus. The system developed by them reportedly achieved 92% precision and a recall of 91%. There exists some combination of words which have a semantic relationship between them but are not exactly synonyms or antonyms of each other (for eg: ‘slow and steady’). The system was unable to detect such type of reduplications using only a dictionary."
    }, {
      "heading" : "3.1.1.2. Detecting noun compounds and light verb constructions",
      "text" : "The authors have described some rule based methods to detect noun compounds and light verb constructions in running texts [25].\nNoun compounds are productive, i.e. new nominal compounds are being formed in language use all the time, which yields that they cannot be listed exhaustively in a dictionary (eg. World wide Web, Multiword Expressions). Whereas Light verb constructions are semi-productive, i.e. new light verb constructions might enter the language following some patterns (e.g. ‘give a Skype call’ on the basis of ‘give a call’).\nLight Verb compounds are syntactically very flexible. They can manifest in various forms: the verb can be inflected, the noun can occur in its plural form and the noun can be modified. The nominal and the verbal component may not even be contiguous (eg. ‘He gave me a very helpful advice’).\nMethods of MWE identification\n21\n1. Lowercase n-grams which occurred as links were collected from Wikipedia articles and\nthe list was automatically filtered in order to delete non-English terms, named entities and non-nominal compounds etc. 2. Match: A noun compound is taken into consideration if it belongs to the list or it is\ncomposed of two or more noun compounds from the list.\n3. POS rules: A noun compound candidate was marked if it occurred in the list and its POS-\ntag sequence matched one of the predefined patterns.\n4. Suffix rule: The ‘Suffix’ method exploited the fact that many nominal components in\nlight verb constructions are derived from verbs. Thus, in this case only constructions that contained nouns ending in certain derivational suffixes were allowed and for nominal compounds the last noun had to have this ending. 5. Most frequent method: This routine relied on the fact that the most common verbs\nfunction typically as light verbs (e.g. do, make, take, have etc.). Thus, the 15 most frequent verbs typical of light verb constructions were collected and constructions where the stem of the verbal component was among those of the most frequent ones were accepted. 6. Stem rule: In the case of light verb constructions, the nominal component is typically one\nthat is derived from a verbal stem (make a decision) or coincides with a verb (have a walk). 7. Syntactic Information: Generally the syntactic relation between the verb and the nominal\ncomponent in a light verb construction is verb-object."
    }, {
      "heading" : "3.1.2 Statistical Methods for Multiwords Extraction",
      "text" : "A number of basic statistical methods can be used for extracting collocations from a given corpus [7] [19]. The corpus used for carrying out the experiments was a collection of The New York Times newswire for four months that consisted of 14 million words. Let us look at these methods and their corresponding applications for extracting multiwords.\n3.1.2.3. Frequency This is the simplest method for extracting collocations as it just retrieves the most frequent bigrams in the corpora. But this naive approach produced a lot of insignificant bigrams which are very frequent (of-the,in-the etc.) This difficulty can be easily overcome by applying a simple\n22\nheuristic - pass the candidate phrases through a POS tagger and take only those combinations into considerations that have the probability of being phrases. The POStag structures that were taken into account were: AN, NN, AAN, ANN, NAN, NNN, NPN.\nAs we can see in Figure 3.1 even though it is a very simple method the results produced by this method was quite impressive.\n3.1.2.4. Mean And Variance The above method for frequency works only for fixed phrases but there are words which stand in a flexible or variable length relationship length from one another. These are the words that appear with each other very frequently but can take any number of words in between. Example: knock...door,this is a proper collocation even though there might be any number of words between knock and door depending on the structure of the sentence but knock is generally the verb associated with door.\nIn this method we calculate the mean and variance of the distance between two words. The variance is defined as:\n1\n)( =\n2\n1=2\n  n dd s i\nn\ni\n23\nWhere 'n' is the number of times the two words co-occur, id is the offset for co-occurrence 'i' ,\nand d is the sample mean of the offsets. If the offsets are same for most occurences the variance will be low and if the offsets differ highly for the occurences then the variance will be very high.\n3.1.2.5. Hypothesis Testing The basic problem that we want to solve for collocation extraction is determining whether two words occur together more often than chance. Hypothesis testing is a classic approach in\nstatistics for this type of problems. A null hypothesis 0H is formed for this stating that the two words occur merely by chance. Now the probability of occurence of the two words given that\n0H is true is calculated,and then depending on this value of probability the null hypothesis is\naccepted or rejected."
    }, {
      "heading" : "3.1.2.5.1. The t-test",
      "text" : "The t-test looks at the mean and variance of a sample, where the null hypothesis is that the sample is drawn from a distribution with mean  . The test computes the difference between the\nobserved and expected means, scaled by the variance of the data, and tells us how likely it is to get a sample of that mean and variance (or a more extreme mean and variance) assuming that the sample follows normal distribution.\nns xt / = 2 \n24\nWhere 2s is the sample variance, N is the sample size,  is the mean of the distribution. If the t\nstatistic is large enough we can reject the null hypothesis stating that the words are associated. For example,in the corpus, new occurs 15,828 times, companies 4,675 times, and there are 14,307,668 tokens overall.\nnew companies occurs 8 times among the 14,307,668 bigrams\n)()(=)(:0 companiesPnewPesnewcompaniPH\n14307668 4675* 14307668 15828= 710*3.675 \nThe observed frequency of occurence of new companies is 8 in the corpus.\n14307668 8=x\nNow applying the t-test:\nns xt / = 2 \n14307668 10*5.591\n10*3.67510*5.591 7\n77\n\n  \n.999932\nThis t value of 0.999932 is not larger than 2.576, the critical value for 0.005= . So we cannot reject the null hypothesis that new and companies occur independently and do not form a collocation."
    }, {
      "heading" : "3.1.2.5.2. Hypothesis Testing of Differences",
      "text" : "A variation of the basic t-test can be used to find words whose co-occurences best distinguish the subtle difference between two near synonyms. Figure 3.3 shows the words that occur significantly more often with powerful (the first ten words) and strong (the last ten words). The formula of the basic t-test is modified as\n2 2 21 2 1\n21\n// = nsns xxt  \n25\nThe application for this form of the t test is lexicography. Such data is useful to a lexicographer wanting to write precise dictionary entries that bring out the difference between strong and powerful.\n3.1.2.5.3. Pearson's Chi-Square Test The t-test assumes that the probabilities of occurence are approximately normally distributed, which is not true in general. It is an alternative test that doesnot depend on the normality assumption. The essence of the test is to compare the observed frequencies with the frequencies expected for independence. If the difference between observed and expected frequencies is large, then we can reject the null hypothesis of independence.\nFigure 3.4 shows the observed frquency values for new and companies. On these values the test is applied. If the difference between observed and expected frequencies is large, then we can reject the null hypothesis of independence.\nThe 2 statistic sums the differences between observed and expected frequencies,scaled by the\nmagnitude of the expected values:\n26\nij\nijij ji E EO 2 , 2 =  \nWhere i ranges over rows of the table, j ranges over columns, ijO is the observed value for cell\nand ijE is the expected value."
    }, {
      "heading" : "3.1.2.5.4. Likelihood Ratio",
      "text" : "This test produces simply a number that tells us how much more likely one hypothesis is than the other. So it more interpretable than any other forms of hypothesis testing. Moreover, likelihood ratios are more appropriate for sparse data than the Chi-Square test.\nFor applying likelihood testing, let us consider the following two hypothesis:\n)|(==)|(:2\n)|(==)|(:1\n12 21 12\n1212\nwwPppwwPHypothesis\nwwPpwwPHypothesis\n\n\nHypothesis1 is a formalization of independence whereas Hypothesis2 is a formalization of dependence. We calculate the log likelihood ratio as:\n27\nThe Figure 3.5 shows the top bigrams consisting of powerful when ranked according to likelihood ratio.\nThis approach is most useful for the discovery of subject-specific collocations. It can be used to compare a general text with a domain-specific text.\n3.1.2.6. Mutual Information This is a method derived from information theory measures where we can find out how much information does the presence of one word gives about another word in the context. Informally, it is a measure of the company that a word keeps.\nMutual information (for two words, x and y) can be defined as:\n)()( )(=),( ''\n''\n2 yPxP yxPlogyxI\n)( )|(= '\n''\n2 xP yxPlog\n)( )|(= '\n''\n2 yP xyPlog\nNone of the statistical methods work very well for sparse data but Mutual Information works particularly badly in sparse environments because of the structure of the equation.\nFor perfect dependence (i.e. whenever they occur,they occur together):\n28\n)()( )(=),( ''\n''\n2 yPxP yxPlogyxI\n)()( )(= ''\n'\n2 yPxP xPlog\n)( 1= '2 yP log\nThe value of mutual information score gets inversely proportional to the frequency value of the bigram. So the bigrams that are rare in the corpus gets an artificially inflated mutual information score.\nFor perfect independence (i.e. their occurence together is completely by chance):\n)()( )(=),( ''\n''\n2 yPxP yxPlogyxI\n)()( )()(= ''\n''\n2 yPxP yPxPlog\n1= 2log 0=\nIt can be inferred that Mutual Information is a good measure of independence between two words but it is a bad measure for deciding the dependence between a bigram.\n3.1.2.7. Comparative Analysis We would like to present a comparative analysis in this section highlighting which method will be useful for what type of collocation.\n• Frequency based method is simple and easy to implement hence it will be very useful for\nlightweight computations (Eg: Information Retrieval through search engines).\n• Mean and Variance method can be used for terminological extraction and Natural\nLanguage Generation as it works well for variable length phrases.\n• t-Test is most useful for ranking collocations and not so much for classifying whether a\nbigram is a collocation or not.\n• Hypothesis Testing Of Differences is most useful for choosing between alternatives while\ngenerating text.\n29\n• Pearson's 2 test is useful for identification of translation pairs among aligned corpora and\nmeasuring corpus similarity.\n• Likelihood Ratios are more appropriate for sparse data than any other statistical method."
    }, {
      "heading" : "3.1.3 Word Association Measures",
      "text" : "This is one of the very early attempts at collocation extraction by Kenneth Church and Pattrick Hanks (1990) [6]. They have generalized the idea of collocation to include co-occurrence. Two words are said to co-occur if they appear in the same documents very frequently.\nFor example: doctor and nurse or doctor and hospital are highly associated with each other as they occur together very frequently in a text.\nThe information theoretic measure, mutual information was used for measuring the word association norms from a corpus and then the collocations were produced."
    }, {
      "heading" : "3.1.3.1. Word Association And Psycholinguistics",
      "text" : "Word association norms are an important factor in psycholinguistic research. Informally speaking, a person responds quicker to a word hospital when he has encountered a highly associated word doctor before. In a psycholinguistic experiment a few thousand people were asked to write down a word that comes to their mind after each of the 200 words that were given to them. This was an empirical way of measuring word associations."
    }, {
      "heading" : "3.1.3.2. Information Theoretic Measure",
      "text" : "Mutual Information: If two words(x, y) have their probability of occurrence as P(x) and P(y) then their mutual information is defined as:\n)()( ),(=),( 2 yPxP yxPlogyxI\nInformally, mutual information compares the probability of x and y appearing together to, the probability of x and y occuring independent of each other. If there is some association between x and y then the mutual probability P(x,y) will be much greater than their independent probability P(x).P(y) and hence I(x,y)>>0. On the other hand,if there is no association between x and y then\n)().(),( yPxPyxP  , hence 0),( yxI .\n30\nThe word probabilities P(x) and P(y) are estimated by counting the number of observations of x and y in a corpus (normalized by N,the size of the corpus).\nMutual probabilities, P(x,y) is estimated by counting the number of times x is followed by y in a\nwindow of w words, ),( yxfw (normalized by N,the size of the corpus). The window size allows us to look for different kinds of associations. Smaller window size identifies the fixed expressions whereas larger window size enables us to understand semantic concepts.\nThe association ratio is technically different from mutual information since in case of mutual information ),(=),( xyfyxf but that is not the case for association ratio because here linear\nprecedence is taken into account."
    }, {
      "heading" : "3.1.3.3. Lexico-Syntactic Regularities",
      "text" : "The association ratio is also useful to find out important lexico-syntactic relationships between verbs and their arguments or adjuncts. For example, consider the phrasal verb set off.\nUsing Sinclair's estimates\n)10*70/(7.3),( 10*556)(,10*250)( 6\n66\n\n  offsetP offPsetP\nThe mutual information for set off is:\n6.1 )()( ),(=);( 2 offPsetP offsetPlogoffsetI\nFrom the above value we can infer that the association between set and off is quite large ( 62 i.e. 64 times larger than chance).\n3.1.3.4. Importance Of Word Association This was a pioneering approach towards extracting word associations. It extended the psycholinguistic notion of word association norm towards an information theoritic measure of mutual information. Informally,it helped us predict what word to look for if we have encountered some word. A lot can be predicted about a word by looking at the company that it keeps.\n31"
    }, {
      "heading" : "3.1.4 Retrieving Collocations From Text : XTRACT",
      "text" : "Frank Smadja has implemented a set of statistical techniques and developed a lexicographic tool, Xtract to retrieve collocations from text [20]. As already stated, the definiton of collocation varies from one author to another.\nAccording to the author, collocations have the following features:\n• Arbitrary : They cannot be directly translated from one language to another as they are\ndifficult to produce from a logical perspective.\n• Domain-dependent : There are expressions which make sense only in a specific\ndomain. These collocations will be unknown to someone not familiar with the domain.\n• Recurrent : Collocations are not exceptional or chance co-occurences of words, rather\nthey occur very frequently in a given context\n• Cohesive lexical clusters : Encountering one word or one part of a collocation often\nsuggests the probability of encountering the rest of the collocation as well.\nThe author has also classified collocations into three types:\n• Predicative Relations : Two words are said to form a predicative relation if they occur\nvery frequently in a similar syntactic structure (like, Adjective-Noun, Noun-Verb etc) For example : make-decision , hostile-takeover\n• Rigid Noun Phrases : This involves uninterrupted, fixed sequences of words\nFor example : stock exchange,railway station\n• Phrasal Templates : Phrasal templates consist of idiomatic phrases consisting of one or\nmore or no empty slots. These are generally used for language generation. For example : Temperatures indicate yesterday's highest and lowest readings is how generally a weather report begins."
    }, {
      "heading" : "3.1.4.1. Xtract: The lexicographic tool for collocation extraction",
      "text" : "Xtract does a three stage analysis to locate interesting word associations in the context and make statistical observation to identify collocations. The three stages of analysis are:\n• First Stage: statistical measures are used to retrieve from a corpus pair wise lexical\nrelations.\n• Second Stage: uses the output bigrams (of 1st stage) to produce collocations of n-grams.\n32\n• Third Stage: adds syntactic information to collocations retrieved at the first stage and\nfilters out inappropriate ones.\nThe experiments were carried out on a 10million word corpus of stock market news reports.\n3.1.4.1.1. Xtract: Stage One Two words are said to co-occur if they are in a single sentence and if there are fewer than five words between them.\nThe words form a collocation if:\n• They appear together significantly more often than expected by chance. • Because of syntactic constraints they appear in a rigid way.\nThe algorithm used for extracting the bigrams forming collocations is:\n1. Given a tagged corpus output all sentences containing a word w\n2. Produce a list of words iw with frequency information on how w and iw co-occur.\niFreq (the frequency of appearance of iw with w in the corpus), POStag of iw , 0)5,5(  jjPij (frequency of occuring iw with w such that they are j words apart).\n3. Analyze the statistical distribution and select interesting word pairs.\nStrength (w, iw ) = ik =  ffreqi \nf and  are the average frequency and standard deviation of all the collocates of a word w\nSpread ( iU ) = 10\n)( 210 1= i j ij pp \nIf iU is small then the histogram will be flat implying that iw can be used at any position around\nw. Whereas if iU is large then the histogram will have sharp peaks implying that iw can be used only in some specific positions around w.\nAt the end of this stage a lexical relation corresponding to w is produced as output. It is of the\nform of a tuple ( iw ,distance,strength,spread,j) verifying the following inequalities:\nStrength= 0k ffreqi \n\n33\nSpread 0U\n)*( 1 ii i j Ukpp \nWhere 010 ,, Ukk are thresholds set manually.\n3.1.4.1.2. Xtract: Stage Two The second stage of Xtract produces collocations consisting of more than two words and also filters out some pairwise relations. The algorithm followed in stage two is given below.\n1. Produce Concordances : Given a pair of words and the distance of the two words,\nproduce all the sentences containing them in the specific position.\n2. Compile and Sort : compute the frequency of appearance of each of the collocates of w 3. Analyze and Filter : a word or a POS is kept in the final n-gram at position if and only if\nTwiwordp )=][( 0\nwhere T is a threshold set manually while performing the experiment\nSome of the results after stage two are shown below:\nTuesday the Dow Jones industrial average rose 26.28 points to 2304.69\nThe NYSE composite index of all its listed common stocks fell 1.76 to 164.13\n34\nIn stage two of Xtract:\n• Phrasal templates are also produced in addition to rigid noun phrases • Produces the biggest possible n-gram • Relatively simpler way of producing n-grams"
    }, {
      "heading" : "3.1.4.1.3. Xtract: Stage Three",
      "text" : "In stage three of Xtract the collocations produced in stage one are analyzed and the syntactic relationship between them is established otherwise they are rejected.\n1. Produce Concordances : Given a pair of words and the distance of the two words,\nproduce all the sentences containing them in the specific position.\n2. Parse : For each sentence produce set of syntactic labels 3. Label and Filter : count the frequencies of each possible label identified for the bigram\n(w,wi) and accept if and only if\nTtilabelp )=][(\nWhere T is a threshold defined manually while performing the experiment\nFor example: If after the first two stages of Xtract the collocation make-decision is produced then in the third stage it is identified as a verb-object collocation. If no such relationship can be established then such collocations are rejected."
    }, {
      "heading" : "3.1.4.2. Analysis Of Xtract",
      "text" : "The precision and recall value of Xtract are 80% and 94% respectively. An observation that can be made from the results of Xtract is that the extracted collocations are domain dependent. Hence the domain and size of the corpus has heavy influence on the type of collocations extracted from it. This work showed a nice method of extracting 'n-grams' and by adding syntax to the collocations it could explain the syntactic relationships between the colloactes as well.\n3.1.5 Collcation Extraction By Conceptual Similarity This is a method suggetsed in [14] where the author uses Wordnet to find out the conceptual similarity between different words. It is observed that in spite of the similarity between words due to the arbitrary nature of collocations only one of the many possible synonyms of a word a\n35\ncandidate phrase prefers one word over another. From this point of view collocation can be redefined as:\nA pair of words is considered to be a collocation if one of the words significantly prefers a particular lexical realization of the concept the other represents. Consider the following examples:\nFor example, coffee significantly prefers strong over powerful and similarly the other examples. In this new outlook there's an inherent directionality as each candidate phrase prefers one synonym over another. So this is termed as collocation preference.\nThe authors studied the usages of two similar words, baggage and luggage:\n1. 2 million parsed sentences of BNC were searched for occurrences of the synonyms\nbaggage and luggage. If the difference of their occurrence for a particular word was greater than 2 then that bigram was taken into account. 2. For each such bigram obtained in step1, Alta Vista search was used to find occurrences of\nit in the world wide web.\n3. Details of collocation according to CIDE(Cambridge International Dictionary Of\nEnglish) was used as standard of judgment.\nFigure 3.9 shows the difference in usage for the two synonyms baggage and luggage.\n36\n3.1.5.1. Collocation Graph Collocation graphs are diagrammatic representation of the different senses represented by a word and the arcs are used to denote colloactional preferences described as follows.\n3.1.5.1.1. Concept Set A collocation graph consists of two or more concept nodes that represent the senses that a word has according to the Wordnet. For a word w the concept set C(w) is defined as:\n}:{=)( ii SwSwC  For example, the word information has five meanings according to the Wordnet. So its concept node will have five entries, one for each of the meanings.\n3.1.5.1.2. Intersection Of Concept Sets If two words are synonyms in some sense i.e. they share a sense in common then their concept nodes will have an intersection and that sense (common to both of them will be present in the intersection).\n37\nFigure 3.10 shows the intersection of the concept sets of information and data.\n3.1.5.1.3. Collocation Preference Concept nodes in a concept graph are connected by collocation arcs to show the preference that is being exhibited due to the property of collocations. The direction of the arc represents which word is expressing preference for which word."
    }, {
      "heading" : "3.1.5.1.4. Intersection Graphs",
      "text" : "While trying to determine significant collocations the concept nodes for the synonyms are drawn. They have one or more senses in common. A candidate phrase is said to exhibit collocational preference if it is expressing more preference for one word than the other for representing the same sense. This is denoted by a directed preference arc in the collocation graph and the arc passes through the preffered word first. This is shown as an example for emotional baggage and emotional luggage in Figure 3.11.\n38\n3.1.5.2. Poly-Collocations It might be possible for a word to express preference for another word in more than one of its synsets. These are termed as poly-collocations. Depending on whether sense information is available or not a variety of configurations are possible for the collocation graph.\n3.1.5.3. Anti-collocations A synonym set with respect to a particular target phrase can be classified into three disjoint sets :\n• The words which are frequently used with the target word (Collocations). • The words which are generally not used with the target word but do not lead to unnatural\nreading.\n• The words which are never used with the target word (Anti Collocation).\nThe knowledge of anti-collocations will be very much helpful for natural language generation and foreign language learners.\nExample: *strong drugs, *powerful coffee"
    }, {
      "heading" : "3.1.5.4. Formalization",
      "text" : "The algorithm takes a sequence of bigrams 21, pp ... Np as input.\n• The occurence count for each such pair is defined as :\n),=((=),( 1=\n bapbaC i n\ni \nwhere, (x) = 1,if x is true and is 0, if x is false.\n• The co-occurence set of a word w is defined as:\n39\n0}>),(:{=)( vwcvwcs • Wordnet is defined as a set of synsets,W. Candidate collocation synset of a word w is\ndefined as: 2}|>)(:|{=)( wcsSWSwCCS  So each candidate collocation synset S,(for a word w) consistes of atleast two elements whose co-occurence count is non-zero.\n• Most frequently co-occurring element of a synset and its frequency are defined as:\n),(=),(= '' vwargmaxcfvwargmaxcw\n• Collocation strength is defined as ''  ff where 'f is the second highest frequency in\nthe synset."
    }, {
      "heading" : "3.1.5.5. Analysis",
      "text" : "The idea presented in the paper looks promising and since the work is at the semantic level it is more intuitive and easy to connect to how a human mind works in reality.\nThe future work needs to focus on improving the basic algorithm in particular aspects :\n• The idea of synonym set can be extended to concept set. • Experiments need to be conducted for synsets other than Nouns. • Morphological processing need to be done. • Some thesaurus can be used along with Wordnet."
    }, {
      "heading" : "3.1.6 Verb Phrase Idiomatic Expressions",
      "text" : "An idiom can be defined as a speech form or an expression of a given language that is peculiar to itself grammatically or cannot be understood from the individual meanings of its elements.\nFor example: by and large, spill the beans, shoot the breeze, break the ice etc.\nThese are very typical to a language and evolve over time. Even within a language they vary from one dialect to another.\nIdioms don't follow some general conventions among its class. Like,some of them might allow some form of verbal inflection (shot the breeze) whereas some might be completely fixed (now\n40\nand then). The idioms that are perfectly grammatical are difficult to be identified as an idiom having idiosyncratic meaning as opposed to its similar structures (shoot the breeze and shoot the bird).\nThe authors have looked into two closely related problems confronting the appropriate treatment of Verb-Noun Idiomatic Combinations(where the noun is the direct object of the verb) [8]:\n• The problem of determining their degree of flexibility • The problem of determining their level of idiomaticity"
    }, {
      "heading" : "3.1.6.1. Recognizing VNICs",
      "text" : "Even though VNICs vary in their degree of flexibility on the whole, they contrast with compositional phrases (which are more lexically productive and appear in a wider range of syntactic forms). Hence the degree of lexical and syntactic flexibility of a given verb+noun combination can be used to determine the level of idiomaticity of the expression. The authors have tried to measure the lexical and syntactic fixedness of an expression by a statistical approach to determine whther it is an idiom or not."
    }, {
      "heading" : "3.1.6.2. Analysis",
      "text" : "Idioms form a very interesting part of natural language but due to its pecularity and arbitrary nature it has been side-stepped by the NLP researchers for long. The authors have tried to provide an effective mechanism for the treatment of a broadly documented and crosslinguistically frequent class of idioms, i.e., VNICs. They have done a deep examination of several linguistic properties of VNICs that distinguish them from similar literal expressions. Novel techniques for translating such characteristics into measures that predict the idiomaticity level of verb+noun combinations have also been proposed."
    }, {
      "heading" : "3.2. Study of an Ongoing Project: MWEToolkit",
      "text" : "Multiword Expression Toolkit (mwetoolkit) is developed for type and language-independent MWE identification [4]. It is a hybrid system for detecting multiwords from a corpus using rule based as well statistical association measures. The toolkit is a open source software can be downloaded from sf.net/projects/.\n41"
    }, {
      "heading" : "3.2.1 MWEToolkit System Architecture",
      "text" : "Given a text corpora the toolkit filters out the MWE candidates from the corpora. The different phases present in the toolkit to achieve this goal are:\n1. Preprocessing the corpus: Preprocess the corpus for lowercase conversion, lemmatization\nand POS tagging (using Tree tagger).\n2. Extract ‘ngrams’ depending on the predefined POS patterns. 3. For each of these bigrams take into account their corpus count as well as the web count\n(number of pages in which the particular bigram is present) using Google and Yahoo\n4. Apply some Association Measures (statistical) to filter out the candidates.\ni. The corpus containing the N word tokens is indexed and from that index\nthe counts of the tokens are estimated. Using the index, individual word counts, c(w1), c(w2)……c(wn) and the overall ngram count c(w1w2…wn) is computed.\nii. The expected N gram is computed if words occurred just by chance\n≈ ( ). ( ). ( ) … … ( )\niii. Using the above information four Association Measures are computed\n Maximum Likelihood Estimator\n= ( … … )\n Dice’s coefficient\n= ∗ ( … … )\n∑ ( )\n Pointwise Mutual Information\n= log ( … … ) ( … … )\n Students’ t-score\n− = ( … … ) − ( … … )\n( … … )\n5. Once each candidate has a set of associated features, an existing machine learning model\ncan be applied to distinguish true and false positives or a new model can be designed by assigning a class to the new candidate set.\n42"
    }, {
      "heading" : "3.2.2 Using Web as corpora",
      "text" : "Another novel aspect of the system is, it uses web count of MWEs as a feature for their Machine Learning model. Let us look a bit more closely and analyze the advantages and disadvantages of using web as a corpus. Issues:\n Web counts are “estimated” or “approximated” as page counts, whereas standard corpus\ncounts are the exact number of occurrences of the n-gram.\n In the web count, the occurrences of an n-gram are not precisely calculated in relation to\nthe occurrences of the (n − 1)-grams composing it. For instance, the n-gram “the man” may appear in 200,000 pages, while the words “the “ and “man” appear in respectively 1,000,000 and 200,000 pages, implying that the word “man” occurs with no other word than “the”.  Unlike the size of a standard corpus, which can be easily computed, it is very difficult to\nestimate how many pages exist on the web and especially because this number is always increasing.\nAdvantage:\n In spite of the issues, the biggest advantage of the web is its availability, even for\nresource-poor languages and domains. It is a free, expanding and easily accessible resource that is representative of language use, in the sense that it contains a great variability of writing styles, text genres, language levels and knowledge domains.\n The web can minimize the problem of sparse data. Most of the statistical methods suffer\ndue to the sparsely distributed data in the corpus. Web can lend a hand for dealing with this problem. Due to the sheer volume of data present on the web, it can assist us to distinguish rare occurrences from invalid cases.\n43\nChapter 4\nMultiword Extraction Engine In this section we are going to describe the Multiword Extraction Engine that has been developed in IIT Bombay to extract multiword expressions from English as well as all major Indian languages. This is a hybrid system employing both linguistic rules and statistical methods for MWE extraction."
    }, {
      "heading" : "4.1. System Overview",
      "text" : "The overview of the MWE extraction pipeline is shown in Figure 4.1. POS tagged corpus is fed to the system.\n Regular Expression (RegEx) filter, filters out the MWE candidates on the basis of pre-\nspecified regular expression patterns.\n Partial Reduplication filter is applied to detect occurrences of both meaningful and non-\nmeaningful partial reduplication.\n The filtered out candidates are passed to the Linguistic Filter, which itself is composed of\nthree filters namely Vector Verb & Verbalizer Filter, Named Entity Filter and Hyphenation Filter.\no In case the candidate belongs to Verb + Verb category or Noun + Verb category,\ncandidate is passed to the very first filter of the Linguistic Filter module.\no To filter out the noise created by the Named Entity, candidates are then passed to\nthe Named Entity Filter.\no The Hyphenation filter filters in the candidates which have hyphen (“-”) in\nbetween them, as they are most likely to be MWE.\n Complex Predicate Filter is applied to computationally detect conjunct verbs using some\nheuristics.\n After the Linguistic Filter, the candidates are checked for any semantic relationship\nbetween themselves. At this step it is checked if the constituents of a bigram are synonyms or antonyms of each other or they belong to the same class of concepts using Wordnet.\n44\n The candidates are then ranked using the Statistical measures including Point-wise\nMutual Information (PMI), Dice coefficient and Log-Likelihood Algorithms. At the end of this step, a combined ranked list is generated by the engine.  Finally the ranked list of the candidates is analyzed by a lexicographer. This is the\nmanual filtering step, which provides the user options to browse the list and analyze the candidates to ascertain whether they are true MWEs or not. They also have the option to detect false positives and false negatives. After this step a Gold Standard MWE List is produced.\n45\n46"
    }, {
      "heading" : "4.1.1 System Features",
      "text" : "1. This is a completely multilingual system that requires only raw text as input and can\nextract the multiword expressions from the text.\n2. It is highly scalable for large size of data without much memory overhead because it uses\nLucene’s index as data structure.\n3. A multilingual web-based service has been developed which will be hosted at IIT\nBombay in order to facilitate the use of this engine by all research groups across India."
    }, {
      "heading" : "4.1.2 Integrated Resources",
      "text" : " Linguistic Resources: Lists of Vector verbs, Verbalizer and Named Entities are required\nfor each language to apply the linguistic filter.\n Wordnet: The system is integrated with Princeton Wordnet (English) and Indo Wordnet\n(Indian Languages) to detect semantic relationship between the constituents of a candidate."
    }, {
      "heading" : "4.2. MWE Extraction Engine Pipeline",
      "text" : "In this section different stages of the pipeline are explained in detail. The required input for each stage, the output produced at the end of that stage and their functionalities are analyzed."
    }, {
      "heading" : "4.2.1 Regular Expression Filter",
      "text" : "This is the first filter in MWE extraction pipeline. It takes the POS tagged data as input and searches for some predefined patterns. Since multiword expressions are very heterogeneous in nature the objective of this step is to generate as many candidates as possible. Since this is the first filter of the engine the goal here is to reduce the false negatives as much as possible. We take bigrams to pentagrams into account and have defined patterns for them separately.\nFor Bigrams:"
    }, {
      "heading" : "4.2.1.1. Reduplication",
      "text" : "Here we check for repetition of a word irrespective of the tags because most of the POS taggers fail to tag reduplications properly.\n47\nPattern: word1_POS1 word2_POS2 where, word1=word2 Example:\nKnock_VB knock_VB\nGloss: The sound of knocking at door"
    }, {
      "heading" : "4.2.1.2. Compound Noun",
      "text" : "For compound nouns we take all types of Noun-Noun compounds into account. Pattern: word1_Noun word2_Noun where Noun= NN|NNP|NNC|NNPC Example:\nrailway_NN station_NN\nGloss: railway station\nशव_NN मं दर _NN (Hindi)\nTransliteration: Shiva mandir\nGloss: Shiva temple\nTranslation: Temple of Shiva"
    }, {
      "heading" : "4.2.1.3. Compound Verb",
      "text" : "Compound verbs are formed when two verbs appear consecutively in a sentence. Pattern: word1_Verb word2_Verb where Verb= VB|VBD|VBG|VBP|VBN|VBZ|VM Example:\nचला_VB गया_VM (Hindi)\nTransliteration: chala gaya\nGloss: has gone"
    }, {
      "heading" : "4.2.1.4. Conjunct Verb",
      "text" : "Syntactically conjunct verbs are Noun+Verb combinations. Pattern: word1_Noun word2_Verb Example:\nसलाह_NN देना_VM\n48\nTransliteration: salaah dena\nGloss: advice give\nTranslation: to give advice\nFor Trigrams, Quadra grams, Pentagrams:"
    }, {
      "heading" : "4.2.1.5. Noun Compounds",
      "text" : "Noun compounds can merge together and form a new larger noun compound. Hence all nounnoun combinations were taken into account. Example:\nScience fiction writer"
    }, {
      "heading" : "4.2.1.6. Adjective + Noun Compounds",
      "text" : "It has been observed that a noun compound preceded by an adjective is very frequently a terminology or a collocation Example:\nRed blood corpuscle"
    }, {
      "heading" : "4.2.2 Linguistic Filter",
      "text" : "Linguistic Filter exploits linguistic knowledge to filter out the MWE candidates. These filters are created on the basis of observations on the languages and MWE Analysis hence it is language dependent. The filer contains three sub-filters."
    }, {
      "heading" : "4.2.2.1. Vector Verb + Verbalizer Verb Filter",
      "text" : "Linguistic composition of Compound Verb and Conjunct Verb is\nCompound Verb = Verb1 + Verb2 = Polar Verb + Vector Verb\n हस उठना :(Polar Verb) + (Vector Verb)\nTransliteration: has uthna Translation: laugh out Gloss: laugh up\nConjunct Verb = Noun + Verb = Noun + Verbalizer Verb\n49\n सलाह देना :(Noun) + (Verbalizer)\nTransliteration: salah dena Translation: advice Gloss: advice give\nBoth of these, Vector verb and Verbalizer verb, are limited in number for Indian Languages. For a Compound Verb and Conjunct Verb candidate to be a MWE, its vector verb or verbalizer verb has to be from the list of vector verb or verbalizer verb list defined in the Language. Given a list of such verbs for a language verb compounds containing verbs from that list only are considered for further evaluation. The purpose of this filter is to reject the false positive compound verb and conjunct verb Bigram candidates"
    }, {
      "heading" : "4.2.2.2. Named Entity Filter",
      "text" : "Named Entities are very important part of the MWEs. As we go from Bi-gram to Pent-gram in Indian languages, the probability of MWE candidate being a Named Entity increases and most of the Quad-gram and Pent-gram candidates being extracted are Named Entities.\nExample: Indira Gandhi International Airport\nIn Bi-grams and Tri-grams these Named Entity candidates are causing lots of false positive candidates to occur. As Named Entity candidates are Noun + Noun combination, which combines with the other noun terms, coming just after them or just before them, creates false positive candidate for MWE.\nNamed Entity filters filter out the candidates which has a part of a Named Entity in them. A lower score is given to these words while ranking. The purpose of Named Entity filter is to filter out false positive candidates caused by Named Entities and give them low weight-age while ranking."
    }, {
      "heading" : "4.2.2.3. Hyphenation Filter",
      "text" : "In Indian Languages when hyphen, “-”, occurs in-between words, most of the times they are Multi-word Expressions.\n50\nExample:\n चतुर-चालाक (Hindi)\nTransliteration: Chatur-Chalak Gloss: smart-clever Translation: smart\nThese words are considered as single word by the RegEx filter, but are mainly MWE's, which when combine with the other words create false positive candidates.\nThe motivation of this filter is to filter in the words with hyphen in them, so as to decrease false negatives, as most of the words with hyphen are the MWEs. A higher weight-age is given to those words while ranking.\nAll these three filters combine to form the Linguistic filter, which decreases the number of false positive MWE candidates and provides the MWE candidates which have high probability of being a MWE."
    }, {
      "heading" : "4.2.3 Complex Predicate Filter",
      "text" : "Complex predicate is a noun, a verb, an adjective or an adverb followed by a light verb that behaves as a single unit of verb. Complex predicates (CPs) are abundantly used in Hindi and other languages of Indo Aryan family. Detecting and interpreting CPs constitute an important and a somewhat difficult task. Most of the work that has been presented in the literature for detecting CPs are language dependent and uses a list of light verbs for the specific language.\nWe use a completely automated, language independent methodology, based on a lexical resource (IndoWordnet) for detecting conjunct verbs. Complex Predicates are abundant in Indo Aryan languages and their characteristics are also similar. The method described in this section is generic and is suitable for all such languages.\nA conjunct verb is a multi-word expression (MWE) where a noun is followed by a light verb (LV) and the MWE behaves as a single unit of verb. The CP in a sentence syntactically acts as a single lexical unit of verb that has a meaning distinct from that of the LV i.e. N+V combination behaves as a ‘single semantic unit’ where the verb loses its individual meaning. For eg:\n51\n तस ल देना (Hindi)\nTransliteration: tasalli dena Gloss: assurance give Translation: assure\n न कष नकलना (Hindi)\nTransliteration: nishkarsh nikalna Gloss: summary extract Translation: summarize\n यार होना(Hindi)\nTransliteration: pyar hona Gloss: love happen Translation: love\n गव होना(Hindi)\nTransliteration: garv hona Gloss: pride happen Translation: proud\nWe use Wordnet and ontology to study the characteristics of the nouns and verbs to formulate when does such a combination makes complex predicate. In a conjunct verb, the meaning of the\nlight verb is lost i.e. in ‘तस ल देना’ none of the senses of the verb ‘देना’ is used. According to\nontology, there can be three types of verbs:\ni. Verb of Action (VOA)\nii. Verb of State (VOS)\niii. Verb of Occur (VOO)\n‘देना’ is a Verb of Action. VOA generally requires an ‘object’ whereas ‘तस ल ’ is an abstract\nnoun and not an object. Hence the ‘selectional preference’ of the verb is not met and it is not used in its usual sense, therefore forming a conjunct. We look into the ontological categories of Nouns and Verbs and check the combinations which form conjuncts. Our findings are:\n52\n53"
    }, {
      "heading" : "4.2.4 Semantic Filter",
      "text" : "This module is used to check for semantic relationship between the constituents of a bigram. Wordnet is used as the primary resource for this.\nFor a bigram we search the Wordnet of that language to check for all synsets which contain the words. After retrieving the synsets, we check if there exists any synonymous, antonymous relationship between them. To check whether the words belong to the same class or not we check if they are sister words i.e. they share a common direct hypernym."
    }, {
      "heading" : "4.2.4.1. Synonym",
      "text" : " र ते नाते (Hindi)\nTransliteration: rishte naate Gloss: relationship connection Translation: relationships"
    }, {
      "heading" : "4.2.4.2. Antonym",
      "text" : " जीना मरना (Hindi)\nTransliteration: jeena marna Gloss: living dead Translation: everything\n আকাশ পাতাল (Bengali)\nTransliteration: akash patal Gloss: sky underground Translation: a lot"
    }, {
      "heading" : "4.2.4.3. Sister Words",
      "text" : " भाई बहेन (Hindi)\nTransliteration: bhai bahen Gloss: brother sister Translation: everyone\n54"
    }, {
      "heading" : "4.2.5 Partial Reduplication Filter",
      "text" : "This is the case where a word is partially duplicated. It can further be classified in two categories.\n Meaningful Partial Reduplication: Both the components are meaningful and form a rhyming pattern.\n जाना माना (Hindi)\nTransliteration: jaana maana Translation: known acknowledged Gloss: renowned\n चलते फरते (Hindi)\nTransliteration: chalte firte Translation: while walking\nWe use Wordnet for detecting such patterns. We check if both the words have a matching suffix (implying that they form a rhyming pair) and both the words are meaningful, hence exist in a lexical database (like, wordnet).\nIdeally, words forming a meaningful partial reduplication should have the same POS tag but it is difficult for a POS tagger to tag such expressions correctly. When we incorporated the restriction on POS tags, it reduced the recall due to errors from the POS tagger. Constituent words should also have the same number of phonemes in order to make a rhyming pattern. A point to note here is that, for Indian languages the number of phonemes and the number of characters in the word are not the same. Since this is a generic system, if we want to match the number of phonemes, we’ll have to keep specific modules for each language.\nAnother challenge is, most of the Indian languages have rich morphology and inflected forms are not present in Wordnet. In order to search an inflected word in wordnet, we need to do morphological analysis. To the best of our knowledge morphological analyzer does not exist for all Indian languages at present. We solved this problem by developing a generic lemmatizer using wordnet, the details of which are described in Chapter 8. We use this lemmatizer to identify the root form of the word and determine whether it is meaningful or not. For eg:\n55\n Input Combination: चलते फरते (chalte firte)\n Stems: चल फर (chal fir)\n Lemmatized Forms: चलना फरना (chalna firna)\nWe first take the input and each word is given to the lemmatizer. The lemmatizer finds the root by maximal string matching with words from wordnet stored in a trie. Once we get the root forms we extract all the lemmas formed from that root and check if any of these lemmas for 1st word and 2nd word share the same suffix.  Non-meaningful Partial Reduplication: In non-meaningful partial reduplication\nthe first word is meaningful whereas the second word is a rhyming variation of the first word. For the construction of the second word generally three cases are possible\ni. Change of the first vowel or the matra attached with first consonant\nii. Change of consonant itself in first position\niii. Change of both matra and consonant\nIf the two words differ by only the first character and the second word is not present in a lexical database (we use Wordnet) then we consider it to be a case of Non-meaningful partial reduplication.\n चाय वाय (Hindi)\nTransliteration: chaye vaye Translation: tea\n मलता जुलता (Hindi)\nTransliteration: milta julta Translation: similar"
    }, {
      "heading" : "4.2.6 Statistical Filter",
      "text" : "We employ statistical methods for detecting collocations. Collocations are the type of multiword expressions which are statistically idiosyncratic for a language and are not governed by any linguistic rule. We use three statistical methods for detecting collocations, namely,\n Normalized Point-wise Mutual Information\n Bidirectional Log-Likelihood Ratio\n56\n Dice Coefficient"
    }, {
      "heading" : "4.2.6.1. Normalized Point-wise Mutual Information",
      "text" : "Point-wise Mutual Information is being used to statistically rank the MWE candidates on the basis of shared information score being calculated using the PMI formulae as mentioned below:\n( , ) = ( )\n( ) ∗ ( )\nWhere, I(w1,w2) is the amount mutual information shared between words w1 and w2,\np(w1w2) is the probability of w1 and w2 occurring together, p(w1)*p(w2) is the probability of the words occurring in the corpus independently.\nPMI is roughly a measure of how much one word tells us about the other and gives an idea of the ‘dependence’ between the words. If we look more closely at the formula, we’ll see that when the two words are completely dependent on each other i.e. w1 occurs only when it is followed by w2, the PMI formula reduces to,\n( , ) = ( ) ( ) ∗ ( ) = ( ) ( ) ∗ ( ) = 1 ( )\nAs the word combination gets rarer its PMI score gets higher. Bigrams composed of lowfrequency words will receive a higher score than bigram composed of high-frequency words but it is not necessarily true that low frequency occurrences are of higher importance.\nWe normalize the original Point-wise Mutual Information formula to give importance to the frequency of occurrence. We also extend the PMI formula, which is for Bi-grams, to apply it to Tri-gram, Quad-gram and Pent-gram.\n4.2.6.1.1. Bi-gram NPMI\n= ( )\n( ) ∗ ( )\nWhere, I(w1,w2) is the amount mutual information shared between words w1 and w2,\np(w1w2) is the probability of w1 and w2 occurring together, p(w1)*p(w2) is the probability of the words occurring in the corpus independently.\nWe expand the formulas of (n-1) gram to find out PMI score for n-gram.\n57\n4.2.6.1.2. Tri-gram NPMI\n= ( )\n( ) ∗ ( )\nWhere, Itri is the amount mutual information shared between w1, w2 and w3,\np(w1w2 w3) is the probability of w1, w2 and w3 occurring together, p(w1w2) *p(w2w3) is the probability of occurrence of the bigram w1w2 independent of the\nprobability of the occurrence of the bigram w2w3 in the corpus.\n4.2.6.1.3. Quad-gram NPMI\n= ( )\n( ) ∗ ( )\nWhere, Iquad is the amount mutual information shared between w1, w2, w3 and w4,\np(w1w2w3w4) is the probability of w1, w2, w3 and w4 occurring together, p(w1w2 w3) *p(w2w3w4) is the probability of occurrence of the trigram w1w2w3\nindependent of the probability of the occurrence of the trigram w2w3w4 in the corpus.\n4.2.6.1.4. Pent-gram NPMI\n= ( )\n( ) ∗ ( )\nWhere, Ipent is the amount mutual information shared between and w1, w2, w3 , w4 and w5\np(w1w2 w3 w4 w5) is the probability of w1, w2, w3 , w4 and w5 occurring together p(w1w2w3w4) *p(w2w3w4w5) is the probability of occurrence of the quad-gram w1w2w3w4\nindependent of the probability of the occurrence of the quad-gram w2w3w4w5 in the corpus."
    }, {
      "heading" : "4.2.6.1.5. Observation Results for NPMI",
      "text" : "English Corpus Hindi Corpus Bengali Corpus\nhelter skelter उथल पुथल ােভল এেজি েত\ncintayatyeva mayi ओत ोत সারভাইভ াল িনং\nfait accompli ह े क े আসপারগার িসে াম\nkirayat Swertia ततर बतर লাভ সংবরণ\n58\nTable 4.2 shows the top 10 candidates extracted by Normalized PMI method for English, Hindi and Bengali corpus."
    }, {
      "heading" : "4.2.6.2. Bi-Directional Log-Likelihood Algorithm",
      "text" : "Log-Likelihood Ratio (LLR) is a hypothesis testing used to test whether the constituent words of an expression are dependent or independent of one another. Two hypotheses (of independence and dependence) for the occurrence frequency of a bigram (w1, w2) are considered:\nHypothesis 1: P(w2|w1) = p = P(w2|w1)\nHypothesis 2: P(w2|w1) = p1  p2 = P(w2|w1)\nHypothesis 1 gives the likelihood of the occurrence of w2 being independent of w1, whereas hypothesis 2 measures the likelihood of the occurrence of w2 being dependent on w1. The ratio of these two likelihoods tells us, which hypothesis is more likely.\n)( )(=)(\n2\n1 22 HL\nHLloglog \nThe issue with the state-of-the-art LLR is that it checks only for the dependence of w2 on w1 and does not consider the dependence (or independence) of w1 on w2. We modify the original formula to take into consideration dependence from both directions."
    }, {
      "heading" : "4.2.6.2.1. Bi-gram Bi-directional Log-Likelihood Ratio",
      "text" : "Let w1w2 be the Bi-gram, the probabilities p1, p2, p3 and p4 are calculated as:\n= ( | )\n59\n= ( |~ )\n= ( | )\n= ( |~ )\nHypothesis 1: p1 = p2 Hypothesis 2: p1 ≠ p2 Hypothesis 3: p3 = p4 Hypothesis 4: p3 ≠ p4 The Log-Likelihood score is calculated as:\n= ( ( ) ( ) +\n( ) ( ) )/2\nWe expand the formulas of (n-1) gram to find out the score for n-gram."
    }, {
      "heading" : "4.2.6.2.2. Tri-gram Bi-directional Log-Likelihood Ratio",
      "text" : "Let w1w2 w3 is the Tri-gram, the probabilities p1, p2, p3 and p4 are calculated as:\n= ( | )\n= ( |~( ))\n= ( | )\n= ( |~( ))\nThe Log-Likelihood score is calculated as:\n= ( ( ) ( ) +\n( ) ( ) )/2"
    }, {
      "heading" : "4.2.6.2.3. Quad-gram Bi-directional Log-Likelihood Ratio",
      "text" : "Let w1w2 w3 w4 is the Quad-gram, the probabilities are calculated as:\n= ( | )\n= ( |~( ))\n= ( | )\n= ( |~( ))\nSo the Log-Likelihood score is calculated as:\n= ( ( ) ( ) +\n( ) ( ) )/2"
    }, {
      "heading" : "4.2.6.2.4. Pent-gram Bi-directional Log-Likelihood Ratio",
      "text" : "Let w1w2 w3 w4 w5 is the Pentagram, the probabilities p1 and p2 are calculated as:\n60\n= ( | )\n= ( |~( ))\n= ( | )\n= ( |~( ))\nThe Log-Likelihood score is calculated as:\n= ( ( ) ( ) +\n( ) ( ) )/2"
    }, {
      "heading" : "4.2.6.2.5. Observation Results for Bi-directional Log-Likelihood Ratio",
      "text" : "Table 4.3 shows the top 10 candidates extracted by Bi-directional LLR method from English, Hindi, Bengali corpus."
    }, {
      "heading" : "4.2.6.3. Dice Coefficient",
      "text" : "The Sørensen–Dice index or Dice coefficient is a statistic used for comparing the similarity of two samples. We use dice coefficient for the detection of collocations in the following way,\n61\n= ( )\n( ) + ( )\nWhere, c(w1w2) gives the count (frequency) of the bigram w1w2 in the corpus\nc(w1) is the count of w1 and c(w2) is the count of w2 in the corpus"
    }, {
      "heading" : "4.2.6.3.1. Dice Coefficient for Bigrams",
      "text" : "For two words w1 and w2 in the corpus, their dice coefficient score is calculated as:\n= ( )\n( ) + ( )\nWhere, c(w1w2) gives the count (frequency) of the bigram w1w2 in the corpus c(w1) is the count of w1 and c(w2) is the count of w2 in the corpus"
    }, {
      "heading" : "4.2.6.3.2. Dice Coefficient for Tri-grams",
      "text" : "For three words w1, w2 and w3 in the corpus, their dice coefficient score is calculated as:\n= ( )\n( ) + ( )\nWhere, c(w1w2 w3) gives the count (frequency) of the trigram w1w2w3 in the corpus c(w1 w2) is the count of bigram w1 w2 and c(w2 w3) is the count of bigram w2w3 in the corpus"
    }, {
      "heading" : "4.2.6.3.3. Dice Coefficient for Quad-grams",
      "text" : "For four words w1, w2, w3 and w4 in the corpus, their dice coefficient score is calculated as:\n= ( )\n( ) + ( )\nWhere, c(w1w2 w3 w4) gives the count (frequency) of the quad-gram w1w2w3w4 in the corpus c(w1 w2 w3) is the count of trigram w1 w2 w3 and c(w2 w3 w4) is the count of trigram w2w3 w4 in the corpus"
    }, {
      "heading" : "4.2.6.3.4. Dice Coefficient for Pent-grams",
      "text" : "For five words w1, w2, w3, w4 and w5 in the corpus, their dice coefficient score is calculated as:\n= ( )\n( ) + ( )\nWhere, c(w1w2 w3 w4 w5) gives the count (frequency) of the pent-gram w1w2w3w4 w5 in the corpus\n62\nc(w1 w2 w3 w4) is the count of quad-gram w1 w2 w3 w4 and c(w2 w3 w4 w5) is the count of\nquad-gram w2w3 w4 w5 in the corpus"
    }, {
      "heading" : "4.2.6.3.5. Observation Results for Dice Coefficient",
      "text" : "Table 4.4 shows the top 10 candidates extracted by Dice coefficient method from English, Hindi, Bengali corpus."
    }, {
      "heading" : "4.2.6.4. Combining Different Filter’s Scores",
      "text" : "To combine the scores of PMI, Log-Likelihood and Dice Coefficient, their independent scores are initially normalized. Normalization is done in the interval zero to one, by dividing score of each candidate by the maximum score obtained in the respective algorithm. After the normalization, all the three scores are added for each MWE candidate. After combining, the list is sorted to generate a combined ranked list.\n63"
    }, {
      "heading" : "4.2.7 Manual Evaluation",
      "text" : "This is the final stage of the pipeline. The generated ranked list is evaluated by lexicographers to determine whether a candidate is truly MWE or not. The false positives are discarded by the lexicographers and the true MWEs are added to the dictionary. There is also an option for lexicographers to detect false negatives and add them to the dictionary.\nThe MWEs that are added to the dictionary are thus all validated by lexicographers and hence can serve as a gold standard."
    }, {
      "heading" : "4.2.8 Universal Web Service",
      "text" : "Previously, the MWE engine is an offline Java application that needed to be installed locally and used. A multilingual web service has been developed in collaboration with Goa University to release an online interface to use the engine which will be hosted at IIT Bombay. This web service will be available to all research groups across India. Different language groups can upload their corpus and query the MWE extraction engine to extract MWE candidates from it. The results produced by the MWE extraction engine can later be validated by them.\nIn future, there will also be provision for the lexicographers to enter the true meaning of a noncompositional MWE so that these MWEs can be stored in lexical databases. Depending on the true meaning of an MWE the expression should be associated with the corresponding synset of Wordnet. By attaching gold standard MWEs to lexical resources we will not only be enhancing the resource but other applications will also be able to benefit from the knowledge of MWEs.\n64\nChapter 5\nExperimentations"
    }, {
      "heading" : "5.1. Using Parallel Corpora",
      "text" : "Multiword Expressions are idiosyncrasies of a language, or some conventional way of expressing things in a particular language. It is quite intriguing to see how multiword expressions behave across languages. Whether a multiword expression gets translated to a single word in another language, or does it have a literal translation of its constituents, or does it translate to its gloss in absence of such a concept in the other language or its corresponding expression in the other language also an MWE!"
    }, {
      "heading" : "5.1.1 Multilingual Aspects of Multiword Expressions",
      "text" : "We studied an English-Hindi parallel corpus and found the following types of transformations that multiwords undertake across languages:\nEnglish Expression Hindi Expression Transliteration Gloss\nTransliteration\nsocial services सोशल स वसेज social services an organized activity to improve the condition of\ndisadvantaged people in society\nService record स वस रेकॉड service record\nExpansion\nday care पुरे दन क\nदेखभाल\npure din ki dekhbhal\nchildcare during the day while parents work\nMWE -> Single Word\ncertificate माण प praman patra a document attesting to the truth of certain stated facts function काम काज kaam kaj the actions and activities assigned to or required or\n65"
    }, {
      "heading" : "5.1.2 Motivation",
      "text" : "Multiword expressions are abundant and typical usages of a language, which makes it very important to store such expressions in lexical resources. In order to store an expression in a lexical database we need both the expression and its meaning. Adding all the multiword expressions of a language manually to a resource is both expensive and time consuming. However extracting such expressions from corpus is challenging and ‘automatically understanding’ their meaning is fairly nontrivial.\nThe motivation behind using parallel corpora is to extract multiword expressions of a language with the help of another language and also retrieve the meaning of the expression from to parallel corpora. The idea is to use word alignment to detect corresponding expression for the mwe in other language and retrieve the meaning from.\nThe underlying assumption of alignment-based approaches to MWE extraction is that MWEs are aligned across languages in a way that differs from compositional expressions. Word alignment can be used in many ways for detecting multiword expressions:\n Sequences of length 2 or more in the source language that are aligned with sequences of\nlength 1 or more in the target. The intuition comes from the fact that non –compositional multiword expressions are generally not translated literally across languages.\n Focus on misalignments: trust the quality of 1:1 alignments and search for MWEs exactly\nin the areas that word alignment failed to properly align. The reason for trusting misalignments is that, it is generally hard for an automatic word aligner to align multiword expressions, especially idioms.\n66"
    }, {
      "heading" : "5.1.3 Automatically adding MWEs to Wordnet",
      "text" : "Our primary motivation behind using word alignment is to retrieve the meaning of the expression automatically and store them appropriately in a lexical resource. Consider the following example:\n दम तोड़ा (Hindi)\n Transliteration: dam toda\n Gloss: breath break\n Translation: die\n‘दम तोड़ा’ is an idiom in Hindi, which means ‘to die’. If we want to store this expression to\nwordnet we should store it in the synset of ‘मरना (marna)’ which also means ‘to die’. In the\nparallel corpora if we have a pair of sentences, like,\n His grandfather died\n उसके दादाजी ने दम तोड़ा (uske dadaji ne dam toda)\nThrough word alignment if we are able to map ‘दम तोड़ा’ to ‘died’ in the sentence, then we can\nfind the synset corresponding to ‘die’ in English Wordnet. There exists links between English and Hindi Wordnet synsets. The synset containing ‘die’ should be linked to the synset containing\n‘मरना (marna)’ in Hindi Wordnet. Following this link we should be able to establish the\nassociation between ‘मरना (marna)’ and ‘दम तोड़ा (dam toda)’ and put them in the same synset.\nEven though theoretically the above idea looks very intuitive, it is hard to achieve this computationally;\n Difficulty in alignment: We used GIZA++ word aligner to align the English-Hindi\nparallel corpora. The alignment quality was not satisfactory. Due to data sparsity problem in the corpus the aligner was producing a lot of misalignments.\n Difficulty in detecting idioms: Detecting idioms automatically by any extraction\napproach (statistical or linguistic) is not easy. Due to the poor performance of the automatic word aligner the word alignment methodology also could not be trusted to detect idioms.\n67\nHowever, in spite of the computational difficulties the above mentioned idea is novel and intuitive. With bigger corpus we might be able to overcome the data sparsity issue faced by word aligner and implement the algorithm for automatically adding mwe to lexical resources."
    }, {
      "heading" : "5.1.4 Case Study",
      "text" : "In this section we present a case study that we performed during our investigation of the multilingual aspects of mwe. We present the steps that we followed, our observation and the inference that we drew from the experiment.\n1) Consider a Hindi-English parallel corpora 2) Run MWE extraction engine (described in Chapter 4) on one side 3) Run automatic word aligner to word align parallel sentences 4) Retrieve the translations (marked by the word aligner) of the multiword expressions\n(extracted by MWE extraction engine)\nWe performed the above experiment by running MWE extraction engine on English side of the corpora and studied the Hindi translation equivalents of English ‘collocations’. Following are the results from top 100 collocations in English:\nEnglish Collocation Hindi Expression Transliteration\nCollocation -> Single Word (20%)\nsouth east आ नेय agneya\nfolk songs लोकगीत lokgeet\nbullock cart बैलगाड़ी byalgadi\nCollocation -> Translation/ Transliteration (80%)\ninformation technology सूचना तकनीक suchana taknik\nstate government रा य सरकार rajya sarkar\namendment act संशोधन अ ध नयम sanshodhan adhiniyam\nworld war व व यु vishwa yuddha\ntext books पा य पुस ् तक pathya pustak\n68\nAs shown in Table 5.2, among the top ranked English collocations 20% translate to a single word in Hindi and all of the rest 80% collocations are either transliterated or their components are literally translated; resulting in collocations in Hindi.\nDoes the above observation mean, collocations in one language are collocations in another language, i.e. collocations are language independent!\nIf the above postulate is true then mining collocations from one language can help us save the cost of extraction from other languages just by doing translations or transliteration when the concept is not lexicalized in the other language. We need to gather more multilingual evidences to establish this fact."
    }, {
      "heading" : "5.2. Investigating Sanskrit Traditions",
      "text" : "Sanskrit being a very ancient and grammatically enriched language there exists many rules as to when and why words join together in order to form a new expression.\nThere exists a concept of ‘samasa’ where under certain conditions words join together and the resulting expression achieves an exocentric meaning i.e. a meaning completely different from its constituent words. We will discuss a few types of ‘samasa’ in this section where we can witness such rules in action."
    }, {
      "heading" : "5.2.1 बहु ी ह: समासः (Exocentric Compounds)",
      "text" : "बहु ी ह: समासः (Bahubrihi Samasa) refers to a class of ‘samasa’ where compounds are formed\nby joining two separate words. This is similar to the case of non-compositional multiwords due to the fact that the meaning of the resulting expression lies outside the meaning of the constituent\nwords. This is also termed as Exocentric compounds. Syntactically, बहु ी ह: समासः (Bahubrihi\nSamasa) are of two types:\n69\n समाना धकरणबहु ी ह (samanadhikarana bahubrihi)\nThis category is also known as सामा य (samanya) meaning ‘regular’. Constituents of the\ncompound belonging to this category undergo same case inflection in व हवा यम ्\n(Vigrahbakyam).\no द तपश ु(Sanskrit)\nTransliteration: dattapashu Gloss: the person who has received an animal\n या धकरणबहु ी ह: (vyadhikarana bahubrihi)\nThe compounds belonging to this subtype of Bahubrihi samasa have constituent words which undergo different inflectional variations while forming the compound.\no च पाणी (Sanskrit)\nTransliteration: chakrapani Gloss: The person who is having chakra in his hand\n5.2.2 अ ययीभावः समासः (Adverbial Compounds)\nअ ययीभावः समासः (Avyaibhav samasa) refers to the type of compound where most of the times\nthe first member of the compound is an adverb and the whole compound functions as an adverb as well. Generally, compound belonging to this type do not have exocentric meaning.\nSyntactically, अ ययीभावः समासः (Avyaibhav samasa) are of three types:\n अ ययपूवपदः (Avyaya Purbapada) : The first constituent of the compound is Avyaya\nExample:\no उपकृ णम ्(Sanskrit)\nTransliteration: upakrishnam Gloss: near Krishna\n70\n अ ययो तरपदः (Avyaya Uttarpada): Avyaya appears as the second constituent of the\ncompound. Example:\no शाक त (Sanskrit)\nTransliteration: shakprati Gloss: very small quantity of vegetable\n अ ययपदर हतः (Avyaya Padarahit): None of the constituents in the compound is an\navyay, yet the compound behaves like one. ( त ठ ग ु ती न P2.1.17)\nExample:\no त ठ ग ु(Sanskrit)\nTransliteration: tishthatgu Gloss: The time when the cow gives milk"
    }, {
      "heading" : "5.2.3 त पु षः समासः (Determinative Compounds)",
      "text" : "In a त पु षः समासः (tatpuruṣa samasa), the first component is in a case relationship with\nanother and the meaning of the compound is mostly governed by the latter member. Meaning of the compound is endocentric i.e. compositional and can be inferred from the meaning of the\nconstituents. The members of the compounds need to be in the same inflectional case i.e. थमा\n(Prathama). Example:\no नीलमेघ: (Sanskrit)\nTransliteration: nilmegh Gloss: blue cloud\no व याधनम (Sanskrit)\nTransliteration: vidyadhanam Gloss: knowledge is wealth\n71"
    }, {
      "heading" : "5.2.4 व व (Coordinating compounds)",
      "text" : "A व व (dvandva) 'pair' or twin or Siamese compound refers to some concepts that could be\nconnected in sense by the conjunction. Apart from Sanskrit, Dvandvas are common in some languages such as Chinese, Japanese, and some Modern Indic languages such as Hindi and Urdu, but less common in English[30].\nव व (dvandva) compounds in Sanskrit can mostly be classifies into the following types:\n इतरेतर व व (simple)\nइतरेतर व व (Itaretara dvandva), is an enumerative compound word, the meaning of\nwhich refers to all its constituent members. The last member governs the gender and the inflections on the whole compound.\nExample:\n रामल मणौ (Sanskrit)\nTransliteration: Rama-Lakshmanau Gloss: Rama and Lakshmana\n आचाय श यौ (Sanskrit)\nTransliteration: acharya-sishya Gloss: teacher and student\n समाहार व व (collectives)\nसमाहार व व (Samahar dvandva) is a collective compound word. The meaning of the\ncompound refers to the collection of its constituent members. The resultant compound word is in the singular number and is always neuter in gender. Example:\n पा णपादमः (Sanskrit)\nTransliteration: Pani-padam Gloss: hands and legs\n72\nWe are exploring these different types of ‘samasa’ in order to relate them to Multiword Expressions in present day languages. Since Sanskrit is the mother of most of the Indian languages, the grammatical rules for many of them are derived from Sanskrit. We are trying to gain insight into why and how multiwords are formed. This insight should help us understand the problem more deeply and will also enable us to formulate some rules to solve it.\n73\nChapter 6\nEvaluation of MWE Extraction Engine In this section we present the evaluation of the performance of different filters in detecting multiword expressions of varied types and characteristics. We have carried out our evaluation for the following languages,\n English  Hindi\n Bengali\nThe details of the corpora are given in Table 6.1."
    }, {
      "heading" : "6.1. Evaluation of MWE engine on English Corpus",
      "text" : "The size of the corpus is 4401696 words. We initially applied the Regular Expression filter on the English corpus to narrow down these words by only selecting patterns which can form multiword expressions. We skip the filters of reduplication, partial reduplication and complex predicates since these phenomenon are specific to Indian languages and are not observed in English. We apply the semantic filter based on Princeton Wordnet to detect semantic relationship and then statistical filters to detect collocations.\n74\nTable 6.2 shows the precision values of Semantic, Hyphenation and Statistical Filters. The statistical methods produce ranked lists of collocations. We have evaluated top 200 candidates for each method and have shown the average precision. A comparative evaluation of the three different statistical measures has been produced in Table 6.3, showing the variation of precision values as the candidate size increases. As we can see that Bi-directional log-likelihood ratio performs considerably better than other two methods for Bigrams, whereas Dice Coefficient looks more promising for higher n-grams."
    }, {
      "heading" : "6.2. Evaluation of MWE engine on Hindi Corpus",
      "text" : "The Hindi corpus used for evaluation is parallel to the English corpus, whose evaluation was presented in the previous subsection, thus belong to the same domain and has 4605343 words. Initially a regular expression filter is applied to narrow down the candidates. Reduplication filter and Partial Reduplication Filter is applied followed by the Hyphenation filter and Semantic filter. Complex Predicate filter is applied to detect conjunct verbs. Finally, statistical measures are applied for detecting collocations.\n75\nTable 6.4 shows the precision values of Semantic, Hyphenation and Statistical Filters. The statistical methods produce ranked lists of collocations. We have evaluated top 200 candidates for each method and have shown the average precision. A comparative evaluation of the three different statistical measures has been produced in Table 6.5, showing the variation of precision values as the candidate size increases. Table 6.6 shows the accuracy of identifying complex predicates with respect to different verbs.\nAs we can see, there are quite a few verbs which do not appear in the Hindi Verbalizer list but actually form conjuncts. These expressions would have gone undetected (false negatives) if we had only used the Verbalizer list. Among the statistical measures, we can observe that Dice Coefficient performs considerably better than other two methods.\n76"
    }, {
      "heading" : "6.3. Evaluation of MWE engine on Bengali Corpus",
      "text" : "The Bengali corpus used for evaluation has 181891 words. Initially a regular expression filter is applied to narrow down the candidates. Reduplication filter and Partial Reduplication Filter is applied followed by the Hyphenation filter and Semantic filter. Finally, statistical measures are applied for detecting collocations.\nTable 6.7 shows the precision values of Reduplication, Partial Reduplication, Semantic, Hyphenation and Statistical Filters. The statistical methods produce ranked lists of collocations. We have evaluated top 200 candidates for each method and have shown the average precision. A comparative evaluation of the three different statistical measures has been produced in Table 6.8, showing the variation of precision values as the candidate size increases.\nAs we can see that Bi-directional log-likelihood ratio performs marginally better than other two methods for Bigrams, whereas Dice Coefficient looks more promising for higher n-grams.\n77\nChapter 7\nCommon Concept Hierarchy We present IndoNet, a multilingual lexical knowledge base for Indian languages. It is a linked structure of wordnets of 18 different Indian languages, Universal Word dictionary and the Suggested Upper Merged Ontology (SUMO). This is a huge knowledge base and can serve as a very useful resource for multiword as well as other natural language processing tasks. The network also provides the necessary abstraction and standardization for storing multiword expressions. We discuss various benefits of the network and challenges involved in the development. The system is encoded in Lexical Markup Framework (LMF) and we propose modifications in LMF to accommodate Universal Word Dictionary and SUMO. This standardized version of lexical knowledge base of Indian Languages can now easily be linked to similar global resources."
    }, {
      "heading" : "7.1. Motivation",
      "text" : "Lexical resources play an important role in natural language processing tasks. Past couple of decades has shown an immense growth in the development of lexical resources such as wordnet, Wikipedia, ontologies etc. These resources vary significantly in structure and representation formalism.\nIn order to develop applications that can make use of different resources, it is essential to link these heterogeneous resources and develop a common representation framework. However, the differences in encoding of knowledge and multilinguality are the major road blocks in development of such a framework. Particularly, in a multilingual country like India, information is available in many different languages. In order to exchange information across cultures and languages, it is essential to create architecture to share various lexical resources across languages.\n78\nIn this report we present IndoNet1, a lexical resource created by merging wordnets of 18 different Indian languages. These languages cover 3 different language families, Indo Aryan, SinoTebetian and Dravidian. Universal Word Dictionary[24] and an upper ontology, SUMO[15]\nUniversal Word (UW), defined by a headword and a set of restrictions which give an unambiguous representation of the concept, forms the vocabulary of Universal Networking Language. Suggested Upper Merged Ontology (SUMO) is the largest freely available ontology which is linked to the entire English WordNet [13]. Though UNL is a graph based representation and SUMO is a formal ontology, both provide language independent conceptualization. This makes 1them suitable candidates for interlingua. IndoNet is encoded in Lexical Markup Framework (LMF), an ISO standard (ISO-24613) for encoding lexical resources[10].\nThe contribution of this work is twofold,\n We propose an architecture to link lexical resources of Indian languages.\n We propose modifications in Lexical Markup Framework to create a linked structure of\nmultilingual lexical resources and ontology.\nThough the architecture currently contains only Indian languages, it can easily be extended to support other languages."
    }, {
      "heading" : "7.2. Related Work",
      "text" : "Over the years wordnet has emerged as the most widely used lexical resource. Though most of the wordnets are built by following the standards laid by English Wordnet [9], their conceptualizations differ because of the differences in lexicalization of concepts across languages. `Not only that, there exist “lexical gaps “where a word in one language has no correspondence in another language, but there are differences in the ways languages structure their words and concepts'. [16]\n1 Wordnets for Indian languages are developed in IndoWordNet project. Wordnets are available in following Indian languages: Assamese, Bodo, Bengali, English, Gujarati, Hindi, Kashmiri, Konkani, Kannada, Malayalam, Manipuri, Marathi, Nepali, Punjabi, Sanskrit, Tamil, Telugu and Urdu. These languages covers 3 different language families, Indo Aryan, Sino-Tebetian and Dravidian. http://www.cfilt.iitb.ac.in/indowordnet\n79\nThe challenge of constructing a unified multilingual resource was first addressed in EuroWordNet [27]. EuroWordNet linked wordnets of 8 different European languages through a common interlingual index (ILI). ILI consists of English synsets and serves as a pivot to link other wordnets. While ILI allows each language wordnet to preserve its semantic structure, it has two basic drawbacks as described in [26].\n An ILI tied to one specific language clearly reflects only the inventory of the language it\nis based on, and gaps show up when lexicons of different languages are mapped to it.\n The semantic space covered by a word in one language often overlaps only partially with\na similar word in another language, resulting in less than perfect mappings.\nSubsequently in KYOTO project, ontologies are preferred over ILI for linking of concepts of different languages. Ontologies provide language indpendent conceptualization, hence the linking remains unbiased to a particular language. Top level ontology SUMO is used to link common base concepts across languages. Because of the small size of the top level ontology, only a few wordnet synsets can be linked directly to the ontological concept and most of the synsets get linked through subsumption relation. This leads to a significant amount of information loss.\nKYOTO project used Lexical Markup Framework (LMF) [10] as a representation language. `LMF provides a common model for the creation and use of lexical resources, to manage the exchange of data among these resources, and to enable the merging of a large number of individual electronic resources to form extensive global electronic resources[10].\nWordNet-LMF was proposed to represent wordnets in LMF format [21]. It has further been modified to accommodate lexical relations[11]. LMF also provides extensions for multilingual lexicons and for linking external resources, such as ontology. However, LMF does not explicitly define standards to share a common ontology among multilingual lexicons.\nOur work falls in line with EuroWordNet and Kyoto except for the following key differences,\n Instead of using ILI, we use a `common concept hierarchy' as a backbone to link lexicons\nof different languages.\n In addition to an upper ontology, a concept in common concept hierarchy is also linked to\nUniversal Word Dictionary. Universal Word dictionary provides additional semantic\n80\ninformation regarding argument types of verbs, that can be used to provide clues for selectional preference of a verb.\n We refine LMF to link external resources (e.g. ontologies) with multilingual lexicon and\nto represent Universal Word Dictionary."
    }, {
      "heading" : "7.3. IndoNet",
      "text" : "IndoNet uses a common concept hierarchy to link various heterogeneous lexical resources. As shown in Figure 7.1, concepts of different wordnets, Universal Word Dictionary and Upper Ontology are merged to form the common concept hierarchy. Figure 7.1 shows how concepts\nof English WordNet (EWN), Hindi Wordnet (HWN), upper ontology (SUMO) and Universal\nWord Dictionary (UWD) are linked through common concept hierarchy (CCH). This section provides details of Common Concept Hierarchy and LMF encoding for different resources."
    }, {
      "heading" : "7.3.1 Common Concept Hierarchy (CCH)",
      "text" : "The common concept hierarchy is an abstract pivot index to link lexical resources of all languages. An element of a common concept hierarchy is defined as <sinid_1,sinid_2,..., uwid, sumoid> where, sinid_i is synset id of ith wordnet, uw_id is universal word id, and sumo_id is SUMO term id of the concept. Unlike ILI, the hypernymy-hyponymy relations from different\n81\nWordnets are merged to construct the concept hierarchy. Each synset of wordnet is directly linked to a concept in `common concept hierarchy'."
    }, {
      "heading" : "7.3.1.1. LMF for Wordnet",
      "text" : "We have adapted the Wordnet-LMF, as specified in [21]. However IndoWordnet encodes more lexical relations compared to EuroWordnet. We enhanced the Wordnet-LMF to accommodate the following relations: antonym, gradation, hypernymy, meronym, troponymy, entailment and cross part of speech links for ability and capability."
    }, {
      "heading" : "7.3.1.2. LMF for Universal Word Dictionary",
      "text" : "A Universal Word is composed of a headword and a list of restrictions, which provide unique meaning of the UW. In our architecture we allow each sense of a headword to have more than one set of restrictions (defined by different UW dictionaries) and are linked to lemmas of multiple languages with a confidence score. This allows us to merge multiple UW dictionaries and represent it in LMF format. We introduce four new LMF classes; Restrictions, Restriction, Lemmas and Lemma and add new attributes, headword and mapping score to existing LMF classes.\nFigure 7.2 shows an example of LMF representation of UW Dictionary. At present, the dictionary is created by merging two dictionaries, UW++ [3] and CFILT Hin-UW. Lemmas from different languages are mapped to universal words and stored under the Lemmas class.\n82"
    }, {
      "heading" : "7.3.1.3. LMF to link ontology with Common Concept Hierarchy",
      "text" : "Figure 7.3 shows an example LMF representation of CCH. The interlingual pivot is represented through SenseAxis. Concepts in different resources are linked to the SenseAxis in such a way that concepts linked to same SenseAxis convey the same Sense.\nUsing LMF class MonolingualExternalRefs, ontology can be integrated with a monolingual lexicon. In order to share an ontology among multilingual resources, we modify the original core package of LMF.\nAs shown in Figure 7.3, a SUMO term is shared across multiple lexicons via the SenseAxis. SUMO is linked with concept hierarchy using the following relations: antonym, hypernym, instance and equivalent. In order to support these relations, Reltype attribute is added to the interlingual Sense class.\n83"
    }, {
      "heading" : "7.4. Observation",
      "text" : "Table 7.1 shows part of speech wise status of linked concepts. The concept hierarchy contains 53848 concepts which are shared among wordnets of Indian languages, SUMO and Universal Word Dictionary. Out of the total 53848 concepts, 21984 are linked to SUMO, 34114 are linked to HWN and 44119 are linked to UW. Among these, 12,254 are common between UW and SUMO and 21984 are common between wordnet and SUMO.\nPOS HWN UW SUMO CCH Adjective 5532 2865 3140 5193 Adverb 380 2697 249 2813 Noun 25721 32831 16889 39620 Verb 2481 5726 1706 6222 Total 34114 44119 21984 53848\nTable 7.1: Statistics of Concept Linkages\n84\nThis creates a multilingual semantic lexicon that captures semantic relations between concepts of different languages. Figure 7.1 demonstrates this with an example of `kinship relation'. As shown in Figure 7.1, `uncle’ is an English language concept defined as `the brother of your father or mother'. Hindi has no concept equivalent to `uncle' but there are two more specific concepts `kaka', `brother of father.' and `mama', `brother of mother.'\nThe lexical gap is captured when these concepts are linked to CCH. Through CCH, these concepts are linked to SUMO term `FamilyRelation' which shows relation between these concepts. Universal Word Dictionary captures exact relation between these concept by applying restrictions [chacha] uncle(icl>brother (mod>father)) and [mama] uncle(icl>brother (mod>mother)). This makes it possible to link concepts across languages."
    }, {
      "heading" : "7.5. Conclusion",
      "text" : "We have presented a multilingual lexical resource for Indian languages. The proposed architecture handles the `lexical gap' and `structural divergence' among languages, by building a common concept hierarchy. In order to encode this resource in LMF, we developed standards to represent UW in LMF.\nIndoNet is emerging as the largest multilingual resource covering 18 languages of 3 different language families and it is possible to link or merge other standardized lexical resources with it. Since Universal Word dictionary is an integral part of the system, it can be used for UNL based Machine Translation tasks. Ontological structure of the system can be used for multilingual information retrieval and extraction.\nIn future, we aim to address ontological issues of the common concept hierarchy and integrate domain ontologies with the system. We are also aiming to develop standards to evaluate such multilingual resources and to validate axiomatic foundation of the same.\n85\nChapter 8\nGeneric Stemmer Stemmer is a basic building block for most of the natural language processing applications. Due to the morphological richness of Indian languages and lack of resources, stemmers are not available for many Indian languages. MWE extraction task required searching wordnet database for words read from a corpus, which might be in a morphed form. Wordnet doesn’t store morphological variations of a word and hence we need to lemmatize it before searching wordnet. In this report, we present a generic stemmer that can be used for all major Indian languages based on IndoWordnet. The stemming algorithm is based on trie data structure and uses the lemmas from Wordnet to find out lexeme for a given word."
    }, {
      "heading" : "8.1. Construction of the Generic Stemmer",
      "text" : "In this section we discuss the construction of the stemmer. IndoWordnet is a lexical database for 18 Indian languages. We use the words from wordnet as a dictionary to search for a lemma.\nGiven an input word we follow the following steps to search for its stem and lemma:\ni) Read the input word and input language ii) Search the word in wordnet of the appropriate language iii) If the word is found in the wordnet\na. Output the word as the lemma as well as stem b. Exit\niv) Find the string with maximal matching in Wordnet (maxMatch)\na. Output maxMatch as the stem b. Find all words which have maxMatch as a prefix and output them as lemmas\nv) Ask the user if the output is correct\na. If yes, then exit b. Else backtrack, remove the last character of maxMatch and go to step iv a.\n86"
    }, {
      "heading" : "8.1.1 Storing Wordnet in a Trie",
      "text" : "Searching for words in Wordnet, requires an in-memory storage of the database for efficient performance. Since Wordnet is a huge lexical database containing thousands of words it’ll be very inefficient to read the database files multiple times for each individual word. There can be multiple options for the data structure to store wordnet in-memory, we chose Trie."
    }, {
      "heading" : "8.1.2 Why Trie?",
      "text" : "In computer science, a trie, also called digital tree or prefix tree, is an ordered tree data structure that is used to store a dynamic set where the keys are usually strings [23]. Unlike a binary search tree, no node in the tree stores the key associated with that node; instead, its position in the tree defines the key with which it is associated. All the descendants of a node have a common prefix of the string associated with that node, and the root is associated with the empty string. Values are normally not associated with every node, only with leaves and some inner nodes that correspond to keys of interest.\nThe term trie comes from retrieval. Trie is a very useful data structure for string retrieval. Looking up data in a trie is faster in the worst case, O(m) time (where m is the length of a search string), compared to an imperfect hash table. An imperfect hash table can have key collisions (a key collision is the hash function mapping of different keys to the same position in a hash table).\nA common application of a trie is storing a predictive text or autocomplete dictionary. Such applications take advantage of a trie's ability to quickly search for, insert, and delete entries. Tries are also well suited for implementing approximate matching algorithms, including those used in spell checking and hyphenation software.\nFor our purpose, we need to store the lexical knowledge present in wordnet in a in-memory data structure which should provide prefix-matching and fast lookup. Hence trie was the obvious choice."
    }, {
      "heading" : "8.1.3 Structure of the Trie",
      "text" : "The words from wordnet are stored in a trie data structure along with their part-of-speech tag and synset id.\n87\nFigure 8.1 shows an example to demonstrate how the words are stored in wordnet. Each leaf node contains a Boolean marker to denote ‘end of word’. A non-leaf node might also be an ‘end of word’ as shown in figure. The nodes which have the EOW flag set to ‘true’, also stores the part-of-speech tag and synset-id(s) of the word.\nFor example, consider the input word to be चलती (chalti). We start searching from the root of\ntrie and try to match one character in each level. Consider the snippet of trie shown in Figure 8.1,\nwe match the character ‘च’ and follow the pointer to the node ‘ल’. After this node we can’t find\na node matching the next character, so this is the node of maximal matching. We store parent pointers at each node to trace back the path, hence after reaching this node; we trace the parent\npointers back to the root node and return the string ‘चल (chal)’ as stem. In order to find the\nlemmas, we search all the children nodes of the maximal matching node, traverse each path till the leaf nodes and print the words whenever ‘EOW’ flag is encountered. For the input word\n‘चलती’ if the algorithm is applied on the trie snippet shown in Figure 8.1 , ‘चलना’ and\n88\n‘चल च ’ will be returned as lemmas.\nThe problem with this approach is, it might return a lot of completely unrelated lemmas which just share the prefix with the input word. Some intelligent methodology can be applied to determine the ranking of the lemmas. We have applied a simple heuristic which ranks the lemmas according to its increasing difference of length with the root word.\nThe system provides a feedback mechanism for the user to convey whether the correct stem and lemma have been found. If it has not been found, the algorithm backtracks by going to the parent node of the maximal matching node and then producing stem and lemma from that node."
    }, {
      "heading" : "8.2. GUI of the Stemmer",
      "text" : "In order to facilitate the easy use of stemmer it is provided with a graphical user interface developed using Java Swing API.\nThe system is integrated with IndoWordnet and hence can act as a generic stemmer catering to 18 Indian languages. The user interface allows a user to select the input language from a dropdown menu of all languages and enter an input word.\nThe system displays the stem and lemmas for the input word in different textboxes and asks the user whether the correct stem and lemma have been found. If the user thinks that the output is incorrect, he can provide his feedback through the radio buttons and the system backtracks and produces a new set of results by going one level up in the depth of the trie. This is a totally user driven system and the backtracking mechanism may continue till the system reaches root of the trie.\nFigure 8.2 shows a screenshot of the GUI of the generic stemmer. The input language is Hindi\nand input word is ‘घुमते (ghumte)’. The system correctly finds the stem as ‘घुम (ghum)’ and\nidentifies the lemma ‘घुमना (ghumna)’.\n89"
    }, {
      "heading" : "8.3. Integration with Multilingual IndoWordnet Search",
      "text" : "The generic stemmer that has been developed can not only serve as a stand-alone system, but can also be integrated with a number of applications to increase their efficiency and convenience of use. Since the whole wordnet is stored in a trie data structure, this library can serve as a spell checker or auto-completion tool for wordnet search very efficiently.\nIndian languages are rich in morphology but morphological variations are not stored in wordnet. It is not be possible to store all the morphological variations of every word in wordnet but many applications may require searching a morphed word (for eg: an application that reads a corpus and uses wordnet for some analysis of the words).\n90\nThere exists a multilingual web service for searching Indo Wordnet. We have integrated our system to the web service to provide automatic Word Suggestions. When a word is entered for searching, it is searched in the trie. If the word is present in Wordnet, a complete match of it will be found in trie and the page automatically redirects to show its synsets. However, if the word is not found, it searches for all the lemmas and provides them as ‘Suggested Words’ for the user to select. The backtracking option is provided with a link named ‘Show More’. A screenshot of this is provided in Figure 8.3.\n91\nChapter 9\nConclusion and Future Work"
    }, {
      "heading" : "9.1. Conclusion",
      "text" : "Through this report we have established the notion of Multiword Expressions and the importance of them in all fields of Natural Language Processing.\nWe have presented a thorough literature survey in this paper. We have discussed various definitions of MWEs given by different researchers and their types and characteristics. We have also touched upon some of the distinguished MWE extraction approaches carried out by various researchers. There exist some ongoing project in research labs for MWE extraction, we have described one such system in detail.\nWe have presented the MWE extraction engine developed at IIT Bombay. It has a pipeline consisting of various filters to detect MWEs from a corpus. A web service is developed for MWE extraction with the engine operating at backend. The web service is on a completely multilingual platform where various language research groups in India can upload their corpus and extract MWEs. It will also help us to build Gold Standard MWE data which can be added to lexical resources.\nWe have also described a common concept hierarchy which can serve as a standardized knowledge network for multiword and many natural language processing tasks.\nA language independent generic stemmer is developed as a byproduct of the multiword research, which also has wide applications. The construction of the stemmer as well as its integration with a multilingual wordnet search web service has been discussed."
    }, {
      "heading" : "9.2. Future Work",
      "text" : "Future work of the project has been divided in three subsections depending on their complexity and importance to the project.\n92"
    }, {
      "heading" : "9.2.1 Short Term Future Work",
      "text" : "Adding MWEs to lexical resources As mentioned earlier, Multiword expressions need to be stored in lexical resources. For noncompositional multi-words it is extremely difficult (if not impossible) to detect the meaning automatically. While manual validation we need to store the meaning of such expressions, so that we can associate them with the corresponding synsets of Wordnet."
    }, {
      "heading" : "9.2.2 Medium Term Future Work",
      "text" : "Associating a chunker The definition of MWs does not bind them to be contiguous hence a multiword expression can constitute of two words which are separated by other words in between. In order to recognize such pattern we need some syntactic information such as phrase boundaries in the sentence. We plan to integrate a chunker along with the existing model to detect non-contiguous multiword expressions."
    }, {
      "heading" : "9.2.3 Long Term Future Work",
      "text" : "Building a Machine Learning model for classification Detecting MWEs can be looked upon as a classification problem solvable by a standard Machine Learning model, given a feature set and training data. In order to train such model we need some high quality Gold standard training data which is not available yet. We hope that with the web service we will be able to collect manually annotated data from all across the nation and hence build a Machine Learning model to associate with this pipeline.\n93"
    } ],
    "references" : [ {
      "title" : "July) LinkedData",
      "author" : [ "Tim Berners-Lee" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2007
    }, {
      "title" : "IndoWordNet",
      "author" : [ "Pushpak Bhattacharyya" ],
      "venue" : "LREC, 2010.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Using Wordnet for Building an Interlingual Dictionary",
      "author" : [ "I Boguslavsky", "J Bekios", "J Cardenosa", "C Gallardo" ],
      "venue" : "Fifth International Conference Information Research and Applications, 2007.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "mwetoolkit: a Framework for Multiword Expression Identification",
      "author" : [ "Ramisch Carlos", "Aline Villavicencio", "Christian Boitet" ],
      "venue" : "In Proc. of the Seventh LREC (LREC 2010), 2010.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Identification of Reduplication in Bengali Corpus and their Semantic Analysis : A Rule Based Approach",
      "author" : [ "Tanmoy Chakraborty", "Sivaji Bandyopadhyay" ],
      "venue" : "Proceedings of the Multiword Expressions: From Theory to Applications, 2010.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Word association norms,mutual information, and lexicography",
      "author" : [ "K. Church", "P. Hanks" ],
      "venue" : "Computational Linguistics, 1990.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 1990
    }, {
      "title" : "Accurate Methods for the Statistics of Surprise and Coincidence",
      "author" : [ "Ted Dunning" ],
      "venue" : "Computational Linguistics, 1993.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 1993
    }, {
      "title" : "Automatically Constructing a Lexicon of Verb Phrase Idiomatic Combinations",
      "author" : [ "Afsaneh Fazly", "Suzanne Stevenson" ],
      "venue" : "EACL, 2006.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "WordNet: An Electronic Lexical Database.",
      "author" : [ "Christiane Fellbaum" ],
      "venue" : "Bradford Books,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1998
    }, {
      "title" : "Multilingual resources for NLP in the lexical markup framework (LMF)",
      "author" : [ "Gil Francopoulo" ],
      "venue" : "Language Resources and Evaluation, 2009.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Standardizing wordnets in the ISO standard LMF: Wordnet-LMF for GermaNet",
      "author" : [ "Verena Henrich", "Erhard Hinrichs" ],
      "venue" : "Proceedings of the 23rd International Conference on Computational Linguistics, COLING, 2010.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Multi-lingual Multiword Expression",
      "author" : [ "Munish Minia" ],
      "venue" : "DDP Thesis 2012.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Linking Lexicons and Ontologies: MappingWordNet to the Suggested Upper Merged Ontology",
      "author" : [ "Ian Niles", "Adam Pease" ],
      "venue" : "Proceedings Of The 2003 International Conference  94  On Information And Knowledge Engineering, 2003.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Using conceptual similarity for collocation extraction",
      "author" : [ "Darren Pearce" ],
      "venue" : "Proceedings of the Fourth annual CLUK colloquium, 2001.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Towards a standard upper ontology",
      "author" : [ "Ian Niles", "Adam Pease" ],
      "venue" : "Proceedings of the international conference on Formal Ontology in Information Systems, 2001.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Formal ontology as interlingua: The SUMO and WordNet linking project and global wordnet",
      "author" : [ "Adam Pease", "Christiane Fellbaum" ],
      "venue" : "Ontology and Lexicon, A Natural Language Processing perspective, 2010.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Multiword Expressions in the wild? mwetoolkit comes in handy",
      "author" : [ "Carlos Ramischy", "Aline Villavicencio", "Christian Boitet" ],
      "venue" : "COLING, 2010.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Multiword Expressions:A pain in the neck for NLP",
      "author" : [ "Ivan Sag", "Timothy Baldwin", "Francis Bond", "Ann Copestake", "Dan Flickinger" ],
      "venue" : "CICLing. Springer, 2002.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Retrieving collocations from text: Xtract",
      "author" : [ "Frank Smadja" ],
      "venue" : "Computational Linguistics, 1993.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 1993
    }, {
      "title" : "Wordnet-LMF: fleshing out a standardized format for wordnet interoperability",
      "author" : [ "Claudia Soria", "Monica Monachini", "Piek Vossen" ],
      "venue" : "Proceedings of the 2009 international workshop on Intercultural collaboration, 2009.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "YAGO: A Core of Semantic Knowledge Unifying Wordnet and Wikipedia",
      "author" : [ "Fabian M. Suchanek", "Gjergji Kasneci", "Gerhard Weikum" ],
      "venue" : "ACM, 2007.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "The UNL- a Gift for the Millenium",
      "author" : [ "H. Uchida", "M. Zhu", "T. Della Senta" ],
      "venue" : "United Nations University Press, 1999.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Detecting noun compounds and light verb constructions: a contrastive study",
      "author" : [ "Veronika Vincze", "Istvan Nagy T", "Gabor Berend" ],
      "venue" : "Proceedings of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World (MWE 2011), 2011.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Challenges for a multilingual wordnet",
      "author" : [ "Christiane Fellbaum", "Piek Vossen" ],
      "venue" : "Language Resources and Evaluation, 2012.  95",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "EuroWordNet: a multilingual database with lexical semantic networks",
      "author" : [ "Piek Vossen" ],
      "venue" : "1998.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "IndoNet: A Multilingual Lexical Knowledge Network for Indian Languages, Association for Computational Linguistics",
      "author" : [ "PUBLICATION Brijesh Bhatt", "Lahari Poddar", "Pushpak Bhattacharyya" ],
      "venue" : null,
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 18,
      "context" : "7 : Producing concordances for “the Dow Jones Industrial Average”[20].",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 18,
      "context" : "8: Producing the “NYSE's composite index of all its listed common stocks “ [20] .",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 13,
      "context" : "9: Collocational Information for 'baggage' and 'luggage'[14] .",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 17,
      "context" : "[19] • Idiosyncratic interpretations that cross word boundaries (or spaces) [18] • Recurrent combinations of words that co-occur more frequently than chance, often with non-compositional meaning[20] • A pair of words is considered to be a collocation if one of the words significantly prefers a particular lexical realization of the concept the other represents[14]",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 18,
      "context" : "[19] • Idiosyncratic interpretations that cross word boundaries (or spaces) [18] • Recurrent combinations of words that co-occur more frequently than chance, often with non-compositional meaning[20] • A pair of words is considered to be a collocation if one of the words significantly prefers a particular lexical realization of the concept the other represents[14]",
      "startOffset" : 194,
      "endOffset" : 198
    }, {
      "referenceID" : 13,
      "context" : "[19] • Idiosyncratic interpretations that cross word boundaries (or spaces) [18] • Recurrent combinations of words that co-occur more frequently than chance, often with non-compositional meaning[20] • A pair of words is considered to be a collocation if one of the words significantly prefers a particular lexical realization of the concept the other represents[14]",
      "startOffset" : 361,
      "endOffset" : 365
    }, {
      "referenceID" : 17,
      "context" : "The classification as described in [18] is given below.",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 22,
      "context" : "[25]",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 4,
      "context" : "Reduplications have been categorized into 2 levels in [5], namely Expression Level and Sense Level.",
      "startOffset" : 54,
      "endOffset" : 57
    }, {
      "referenceID" : 22,
      "context" : "Detecting noun compounds and light verb constructions The authors have described some rule based methods to detect noun compounds and light verb constructions in running texts [25].",
      "startOffset" : 176,
      "endOffset" : 180
    }, {
      "referenceID" : 6,
      "context" : "2 Statistical Methods for Multiwords Extraction A number of basic statistical methods can be used for extracting collocations from a given corpus [7] [19].",
      "startOffset" : 146,
      "endOffset" : 149
    }, {
      "referenceID" : 5,
      "context" : "3 Word Association Measures This is one of the very early attempts at collocation extraction by Kenneth Church and Pattrick Hanks (1990) [6].",
      "startOffset" : 137,
      "endOffset" : 140
    }, {
      "referenceID" : 18,
      "context" : "4 Retrieving Collocations From Text : XTRACT Frank Smadja has implemented a set of statistical techniques and developed a lexicographic tool, Xtract to retrieve collocations from text [20].",
      "startOffset" : 184,
      "endOffset" : 188
    }, {
      "referenceID" : 18,
      "context" : "7 : Producing concordances for “the Dow Jones Industrial Average”[20]",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 18,
      "context" : "8: Producing the “NYSE's composite index of all its listed common stocks “ [20]",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 13,
      "context" : "5 Collcation Extraction By Conceptual Similarity This is a method suggetsed in [14] where the author uses Wordnet to find out the conceptual similarity between different words.",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 13,
      "context" : "9: Collocational Information for 'baggage' and 'luggage'[14]",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 7,
      "context" : "The authors have looked into two closely related problems confronting the appropriate treatment of Verb-Noun Idiomatic Combinations(where the noun is the direct object of the verb) [8]: • The problem of determining their degree of flexibility • The problem of determining their level of idiomaticity",
      "startOffset" : 181,
      "endOffset" : 184
    }, {
      "referenceID" : 3,
      "context" : "Study of an Ongoing Project: MWEToolkit Multiword Expression Toolkit (mwetoolkit) is developed for type and language-independent MWE identification [4].",
      "startOffset" : 148,
      "endOffset" : 151
    }, {
      "referenceID" : 21,
      "context" : "Universal Word Dictionary[24] and an upper ontology, SUMO[15]",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 14,
      "context" : "Universal Word Dictionary[24] and an upper ontology, SUMO[15]",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 12,
      "context" : "Suggested Upper Merged Ontology (SUMO) is the largest freely available ontology which is linked to the entire English WordNet [13].",
      "startOffset" : 126,
      "endOffset" : 130
    }, {
      "referenceID" : 9,
      "context" : "IndoNet is encoded in Lexical Markup Framework (LMF), an ISO standard (ISO-24613) for encoding lexical resources[10].",
      "startOffset" : 112,
      "endOffset" : 116
    }, {
      "referenceID" : 8,
      "context" : "Though most of the wordnets are built by following the standards laid by English Wordnet [9], their conceptualizations differ because of the differences in lexicalization of concepts across languages.",
      "startOffset" : 89,
      "endOffset" : 92
    }, {
      "referenceID" : 15,
      "context" : "[16]",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 24,
      "context" : "79 The challenge of constructing a unified multilingual resource was first addressed in EuroWordNet [27].",
      "startOffset" : 100,
      "endOffset" : 104
    }, {
      "referenceID" : 23,
      "context" : "While ILI allows each language wordnet to preserve its semantic structure, it has two basic drawbacks as described in [26].",
      "startOffset" : 118,
      "endOffset" : 122
    }, {
      "referenceID" : 9,
      "context" : "KYOTO project used Lexical Markup Framework (LMF) [10] as a representation language.",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 9,
      "context" : "`LMF provides a common model for the creation and use of lexical resources, to manage the exchange of data among these resources, and to enable the merging of a large number of individual electronic resources to form extensive global electronic resources[10].",
      "startOffset" : 254,
      "endOffset" : 258
    }, {
      "referenceID" : 19,
      "context" : "WordNet-LMF was proposed to represent wordnets in LMF format [21].",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 10,
      "context" : "It has further been modified to accommodate lexical relations[11].",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 19,
      "context" : "LMF for Wordnet We have adapted the Wordnet-LMF, as specified in [21].",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 2,
      "context" : "At present, the dictionary is created by merging two dictionaries, UW++ [3] and CFILT Hin-UW.",
      "startOffset" : 72,
      "endOffset" : 75
    } ],
    "year" : 2013,
    "abstractText" : "Noun • Action • Psychological Feature VOS Conjunct Verb",
    "creator" : null
  }
}