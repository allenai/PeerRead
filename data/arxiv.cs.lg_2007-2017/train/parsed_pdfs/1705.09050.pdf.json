{
  "name" : "1705.09050.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Clustering-based Consistency Adaptation Strategy for Distributed SDN Controllers",
    "authors" : [ "Mohamed Aslan", "Ashraf Matrawy" ],
    "emails" : [ "maslan@sce.carleton.ca", "ashraf.matrawy@carleton.ca" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n70 5.\n09 05\n0v 1\n[ cs\n.N I]\n2 5\nM ay\n2 01\nIndex Terms—SDN; Adaptive; Distributed; Controllers; Performance\nI. INTRODUCTION\nRecent research [2], [3], [4], [5], [6] in Software-Defined Networking (SDN) employs multiple distributed controllers for scalability and reliability reasons. Using distributed controllers allows the network to scale-out without introducing bottlenecks or single point of failure. It also provides the network with redundancy and fault-tolerance.\nDistributed SDN controllers need to communicate (via east/west interfaces) in order to synchronize their state information (we call this process controller state distribution). Hence, they are subjected to issues similar to those affecting distributed datastores [7]. A major issue is the trade-off between consistency and availability in the case of network partitioning, which was identified by Eric Brewer in the CAP (Consistency, Availability and Partitioning) conjecture [8], [9]. The CAP conjecture states that in the case of network partitioning, a distributed system will have to choose between the consistency of the data or the availability of the system. Systems that prefer consistency over availability are labeled\nas strongly-consistent systems. While, systems that have the ability to change their behavior (degree of consistency) are known as tunably-consistent systems [10], [11].\nIn SDN, the consistency level of state information exchanged among the distributed controllers can negatively affect the network application performance [4], [12], [13], depending on the performance indicators being considered.\nThere exist a multitude of SDN applications, having different performance indicators. As such, some of these applications can tolerate the state information inconsistency for the sake of higher availability. Therefore, applications could be built on-top of tunably-consistent distributed controllers which could be tuned differently for each application. Earlier [1], we proposed the use of adaptive controllers [1] running ontop of tunably-consistent controllers in order to autonomously handle setting the parameters of the tunably-consistent distributed controllers based on application-specific performance indicators.\nIn this paper, we investigate the feasibility of using adaptive controllers running on-top of tunable consistency models similar to that of Apache Cassandra [14], [15] or Amazon DB [16]. We present a controller adaptation strategy that - given an application-specific indicator (χ) - autonomously tunes the consistency level (Φ) of the distributed controllers in order to maintain a certain value for such applicationspecific indicator. In presenting such strategy, we make the following contributions: (1) we show how to quantize the level of consistency (subsection IV-B) and how to use it in selecting appropriate values for the tunable consistency model parameters, and (2) we show how online clustering techniques can be employed (subsection IV-C) in order to map the application-specific performance indicators (χ) into various consistency levels (Φ).\nThe rest of this paper is organized as follows: In §II, we discuss the need for adaptive controllers in distributed SDN deployments. We provide an overview on the topics of eventual and tunable consistency models in §III. §IV is the proposed realization of adaptive SDN controllers. The evaluation is presented in §V. Finally §VI will be our conclusion and an outline for possible foreseeable work."
    }, {
      "heading" : "II. THE CASE FOR SDN ADAPTIVE CONTROLLERS",
      "text" : "As aforementioned, the use of distributed controllers in large-scale SDN deployments is crucial. First, it can reduce the control delays as the control load is now handled by multiple controllers opposed to a single one. Second, it extricates the network from having a single point of failure embodied by the controller, hence increases the reliability and fault-tolerance of the network. Finally, employing distributed controllers allows the network to scale-out (horizontally) by adding more controllers. Dixit et al. [2], suggested dynamically growing and shrinking the pool of controllers based on the traffic conditions, and to get rid of the controller/switch static mapping which can led to uneven distribution of the control load.\nManaging distributed controllers in large-scale SDN environments can be a daunting task. First, those controllers are subjected to issues that affect distributed datastores [7] including the trade-off between consistency and availability of data during network partitioning. Next, there is a great number of SDN applications different in their requirements and employ different performance indicators. As such, some SDN applications may prefer different consistency and availability configurations [5]. Finally, two or more applications with different requirements (could be conflicting) might be running on the controllers at the same time.\nAn example for an application that might prefer to lower its consistency for higher availability - as long as it is maintaining a certain level of performance - would be a load-balancer. The load-balancer would need to maintain information about the current load distribution in the network. However, as long as it is not creating routing loops (more in [12]), it can tolerate some inconsistency in order to achieve a higher degree of availability. On the other hand, a firewall might represent an application that would not tolerate inconsistency and would prefer to be strongly consistent at the expense of being available.\nWe believe that distributed controllers that employ a tunable consistency model (similar to that of Apache Cassandra; see subsection III-B) are more suitable for large-scale SDN deployments that simultaneously run myriad of heterogeneous network applications. Onix [5] lets the applications make their own trade-off between consistency and availability by providing them with two data-stores: (1) a strongly consistent transactional data-store, and (2) an eventually consistent (more in the next section) in-memory distribute hash table (DHT).\nFurthermore, we believe that an effective strategy to handle the case of heterogeneous applications, is by extending tunable consistency with an adaptive mode. In such mode, the controllers given a per-application performance indicator will monitor the network behavior and adapt to the current conditions by autonomously tuning their consistency levels [1]. Adaptive distributed controllers can reduce the SDN applications development and maintenance cost by shifting the complexity of handling distribution issues out of the applications, reducing the application complexity. In addition, they can reduce the overhead of state distribution among the\ncontrollers."
    }, {
      "heading" : "III. BACKGROUND ON CONSISTENCY",
      "text" : "In this section, we explain the consistency model used in a number of modern data-stores such as Apache Cassandra [14], [15] and Amazon DynamoDB [16]."
    }, {
      "heading" : "A. Notations",
      "text" : "In distributed controllers, data are copied and stored at different controllers, such copies are known as replicas. In this paper, we assume that no more than a copy of a certain data item will be stored at the same controller. We also use the term replicas when referring to the machines storing the data copies. Table I shows the notations used throughout this paper."
    }, {
      "heading" : "B. The Tunable Consistency Model",
      "text" : "The consistency model employed by Apache Cassandra is both an eventual and a tunable consistency model. Eventual consistency [17] is a consistency model where all replicas eventually receive the most up-to-date values after sometime if no further updated occurred. With tunable consistency, we refer to a property of a consistency model where the level of consistency can manually be tuned. Cassandra allows the application to select between a number of predefined consistency levels, the most releavant ones are: (1) ONE, (2) QUORUM, and (3) ALL [10]. The first level ‘ONE’ indicates that an operation is considered successful if one replica (R = 1) returned the most recent version in case of a read operation, or a confirmation is received from one replica (W = 1) in case of a write operation. This level provides a low latency and a high availability. The second level ‘QUORUM’ (R+W > N ) indicates that an operation is considered a success if a quorum of replicas returned the most up-to-date version in case of a read operation, or a confirmation is received from quorum of replicas in case of a write operation. This level ensures strong consistency. Finally, the ‘ALL’ level indicates that an operation is considered a success if all of the replicas (R = N ) responded and the most up-to-date version is calculated in case of a read operation, or a confirmation is received from all of the replicas (W = N ) in case of a write operation. This level provides highest possible consistency level but the lowest availability.\nFor example (Fig. 1), for a write (or update) operation to be succeed it must be written successfully on W different nodes,\nand for a following read operation to succeed R nodes must respond and return some value. In the first case (Fig. 1a), N = 5, W = 3, and R = 3. At t1, a write operation was requested and confirmed by three nodes (at random): c1, c2, and c3, while the operation might have failed at c4 and c5, the overall operation is marked a success (recall W = 3). At t2, a read operation was initiated and only three nodes (at random): c3, c4, and c5 returned an answer. And since R+W > N then for sure one node from those that answered the read operation will hold the most up-to-date value, in this example it is node c3. On the other hand, in the second case (Fig. 1b), N = 5, W = 3, and R = 2. At t2, only two nodes (at random): c4, and c5 returned an answer to the read operation, yet the overall read operation is marked a success (recall R = 2). Those nodes may not have the most up-to-date value. Thus, there is no guarantee if R+W ≤ N that a read operation will return the most up-to-date value. However, after sometime, if no further updates occurred, all nodes will eventually receive the most up-to-date values.\nAs aforesaid, controllers might simultaneously be running multiple network applications, each having its own requirements. The number of replicas (N ) could also be applicationspecific, e.g., an application dealing with more important information would choose a higher number of replicas whereas an application dealing with less important information would choose a less value for N . Even though the number of replicas (N ) is application-specific, the nodes (controllers) themselves that are responsible for maintaining such replicas are decided by a consistent hashing function [15]."
    }, {
      "heading" : "IV. PROPOSED ADAPTATION STRATEGY",
      "text" : "The adaptation strategy requires the collaboration of different modules of an adaptive controller (proposed in [1]).\nIn this section, we describe some of the modules needed for realizing the adaptive controllers architecture, namely: (1) the stored procedure compiler module, (2) the tunable consistency module, and (3) the adaptation module."
    }, {
      "heading" : "A. Stored Procedure Compiler Module",
      "text" : "Applications often require different performance indicators (χ). This module is needed to allow applications to instruct the controllers how to calculate their performance indicators (e.g., standard deviation between the loads in case of a loadbalancing application). Moreover, in case that the applications are to run on physically separate machines from the controllers, the task of calculating the performance indicators is shifted to the controllers to reduce delays caused by the applications-controllers communication. An application installs a stored procedure similar to that used in Database systems [18] at the controller which can be executed by the stored procedure compiler module in order to calculate the value of the application-specific performance indicator (χ), when needed. We assume that security measures are taken to prevent exploiting the use of the stored procedure compiler module and to ensure safe execution of the stored procedures at the controllers. The security aspects of the controllers are outside the scope of this paper."
    }, {
      "heading" : "B. Tunable Consistency Module",
      "text" : "This module provides the adaptation module with a configurable consistency level parameter (Φ) that can be tuned in order to change the level of consistency.\nConsistency Level Parameter. As aforesaid, the adaptation module requires a parameter that can be tuned in order to change the consistency level. In the proposed strategy, we adopt the tunable consistency model discussed in section III-B as a base for our tunable consistency module. Such model provides R, W , and N as configurable parameters. However, mapping those parameters to a performance indicator (χ) could be complex for the adaptation module. R, W , and N are specific parameters to this particular consistency model (Cassandra-like). Therefore, exposing R, W , and N to the adaptation module would lower the modularity of the system i.e., it will be harder to replace the tunable consistency module with another one without having to modify the adaptation module. Hence, the tunable consistency module provides the adaptation module with a single tunable parameter (Φ) that directly relates to the consistency level, and the tunable consistency module is responsible for mapping that parameter (Φ) into its internal specific parameters (e.g., R, W , and N ).\nMeasuring the Consistency Level. We chose the probability that a read returns the most recent version as the consistency level indicator (Φ) (shown in (2)). In case of strong consistency (R+W ≥ N ), Φ = 1 otherwise Φ = 1−ps where ps (shown in (1)) is the probability that the read quorum does not include the last up-to-date version [17]. Figure 2, shows Φ versus R and W in case of N = 20. R, W and N are positive integer values (∈ Z+), hence Φ(R,W,R) is a discrete function.\nps =\n(\nN −W\nR\n)\n(\nN\nR\n) [17] (1)\nΦ(R,W,N) =\n{\n1− ps R+W ≤ N 1 R+W > N (2)\nControlling the Consistency Level. Once the adaptation module chooses a certain value (φ) for (Φ) that supposedly satisfies the application-specific performance indicator (χ), the tunable consistency module needs to find values for R, W and N that gives (φ ′ = Φ(R,W,N)), where (φ ′ ) is as close as possible to the given φ (recall that {R, W , N} ∈ Z+, and Φ(R,W,N) is a discrete function). We assume N is constant per-application and is set as a system-wide policy by the network administrator. In (3), we prove that swapping the values of R and W yields the same value for Φ. This property helps in reducing the search space.\nΦ(R,W,N) = 1−\n(N−W )! (N−W−R)!×R!\nN ! (N−R)!×R!\n(Case:R+W≤N)\n= 1− (N −W )!× (N −R)!\n(N −W −R)!×N !\nΦ(W,R,N) = 1− (N −R)!× (N −W )!\n(N −R−W )!×N !\n= Φ(R,W,N) (3)\nThe values for R and W that gives the nearest value to a certain value φ for Φ(R,W,N) could be found using (4). A simple algorithm is shown in Algorithm 1 for finding the values of R and W .\n< R,W >= argmin i,j ‖Φ(i, j, N)− φ‖ (4)\nAlgorithm 1: Given a certain value (φ) for (Φ) find appropriate values for R and W in O(2).\nData: N , number of replicas begin\nmin ← ∞ for i ∈ [1, N) do\nfor j ∈ [i, N − i] do dist ← ‖Φ(i, j,N) − φ‖ if dist ¡ min then\nmin ← dist R ← i W ← j"
    }, {
      "heading" : "C. Adaptation Module",
      "text" : "The adaptation module is responsible for selecting an appropriate configuration (i.e., consistency level (Φ)) for the tunable consistency module given a certain performance level (χ) which is calculated with the help of the stored procedure compiler module. In this section, we show how clustering can be used by the adaptation module in order to map a certain performance level (χ) into a corresponding consistency level (Φ).\nMonitoring. In order for the adaptation module to function properly, it needs to continuously collect sample data about the application performance and configuration of the tunable consistency module. In particular, it collects different values for the consistency level indicator (Φ) and notes the corresponding performance level (χ), then uses these values to update the clustering technique.\nClustering. We use clustering in order to find a mapping between the application performance indicator (χ) and the consistency level (Φ). First, the collected data is clustered by the application performance indicator (χ) and each center will be a consistency level (Φ). Next, when a specific level of application performance is needed, the nearest cluster to the required performance level is located and the value of the associated consistency level (Φ) will be used to select appropriate values for R andW . We opt for online incremental clustering techniques [19], [20]. Although such techniques can yield less accurate results compared to those offline techniques but they scale better in terms of storage and they do not require re-clustering with every new measurement. We tested two online techniques: (1) Sequential K-means, and (2) Incremental K-means. Re-Clustering. In oftentimes the adaptation module may need to recalculate the cluster heads. This is needed when there is a change in the network that affects the accuracy of the adaptation module in finding the closest configuration for a given performance level.\nSequential K-means Clustering. The first technique that we tested was the sequential K-means clustering (shown in Algorithm 2). Algorithm 2 is our adoptation of the “sequential K-means” algorithm presented in [21]. This technique requires the number of clusters to be initially specified. The first n-\nmeasurement will be assigned to the n-clusters, and then every new measurement will be assigned to the nearest cluster, and finally the cluster’s mean will be updated.\nAlgorithm 2: Using Sequential K-means Clustering at the Adaptation Module.\nData: χk, k th application’s specific performance indicator Data: Φk, k th consistency level indicator Data: Nc, number of clusters Data: Np, number of data points per cluster Data: Nt, total number of data points begin\nNt ← ∅ if Nt < Nc then\nCk.χ ← χk Ck.Φ ← Φk Ck.Np ← Ck.Np + 1\nelse ic = nearest (χk, C) Cic .χ ← (Cic .χ * Cic .Np) + χk Cic .Φ ← (Cic .Φ * Cic .Np) + Φk Cic .Np ← Cic .Np + 1 Cic .χ ← Cic .χ / Cic .Np Cic .Φ ← Cic .Φ / Cic .Np Nt ← Nt + 1 Function nearest\nData: d, datapoint Data: CN , set of N clusters begin\nidx ← ∅; min ← ∞ for i ∈ N do\ndist ← ‖d.χ− Ci.χ‖ if dist < min then\nmin ← dist idx ← i\nreturn idx\nIncremental K-means Clustering. The second technique that we tested was the incremental K-means clustering (shown in Algorithm 3). We adopt the “incremental clustering” algorithm presented in [22] as a base for Algorithm 3. This technique does not require the number of clusters to be specified as it uses a dynamic number of clusters. Every new measurement will be assigned to the nearest cluster if it is close enough (based on a threshold). If none was found, then a new cluster will be added that includes this measurement. The threshold depends on the performance indicator χ, thus we use the relative error as the distance measure to allow the use of a single threshold value for different performance indicators. Latency. In some cases, a given performance level can be satisfied by a set of different R and W pairs. Even though selecting any of them has no impact on the performance, it can have an impact on the latency. The tunable consistency module will monitor the frequency of reads and writes for each application and given the property (Φ(W,R,N) = Φ(R,W,N)) proved in (3). If the application tends to do more reads then the tunable consistency module will set R to be the smallest value in order to reduce the read latency, while if the application\nAlgorithm 3: Using Incremental K-means Clustering at the Adaptation Module.\nData: χk, k th application’s specific performance indicator Data: Φk , k th consistency level indicator Data: Nc, number of clusters Data: Np, number of data points per cluster Data: τ , threshold begin\nif Nc > 0 then ic = nearest (χk, C) if ‖ Cic .χ - χk ‖ / Cic.χ < τ\nthen Cic .χ ← (Cic .χ * Cic .Np) + χk Cic .Φ ← (Cic .Φ * Cic .Np) + Φk Cic .Np ← Cic .Np + 1 Cic .χ ← Cic .χ / Cic .Np Cic .Φ ← Cic .Φ / Cic .Np\nelse C.create new cluster(χk, Φk)\nelse C.create new cluster(χk, Φk)\nFunction nearest Data: d, datapoint Data: CN , set of N clusters begin\nidx ← ∅; min ← ∞ for i ∈ N do\ndist ← ‖d.χ − Ci.χ‖ if dist ¡ min then\nmin ← dist idx ← i\nreturn idx\ntends to do more writes then the tunable consistency module will set W to be the smallest value in order to reduce the write latency."
    }, {
      "heading" : "D. Application-Controller Interaction",
      "text" : "Figure 3 shows a sequence diagram for the proposed adaptation strategy. It shows the interaction between the application (App) and the various controller modules: stored-procedure compiler module (SPCM), adaptation module (AM), and the tunable consistency module (TCM). Initially, the application creates a stored procedure (procχ) that is responsible for calculating the application-specific performance indicator (χ), and then sends that procedure to the controller where it gets executed by the stored-procedure compiler module when needed. Next, the controller monitors and gathers samples (x and φ) for the application-specific performance indicator (χ) and the corresponding consistency level (Φ), respectively. Then for each sample, the adaptation module invokes the clustering algorithm (learn(x, φ)). Finally, when the application notifies (request(χ̄)) the controller with a desired value (χ̄) for the performance indicator (χ), the adaptation module uses the clustering algorithm to find an estimate (lookup(χ̄)) for a corresponding value (φ) for the consistency level indicator (Φ). Then, the adaptation module notifies (tune(φ)) the tunable\nconsistency module with this value, which in-turn calculates (calc(φ)) the module internal parameters (R and W ) and applies such configuration."
    }, {
      "heading" : "V. EVALUATION",
      "text" : "In order to evaluate the validity of the proposed adaptation strategy, we evaluated the effectiveness of the clustering techniques (sequential and incremental) in mapping performance indicators (χ) to consistency levels (Φ). In our evaluation, we assumed that the relationship between the application-specific performance indicator (χ) and the consistency level indicator (Φ) is one the following relations: (1) linear (χ = AΦ + C), (2) quadratic (χ = AΦ2 + BΦ + C), (3) cubic (χ = AΦ3 + BΦ2 + CΦ +D), or (4) logarithmic (χ = A.log10(Φ) + C). A, B, C, and D are constants. We used a sample of 1000 uniform random numbers to bootstrap the algorithms. Then, we chose 100 arbitrary uniform random test values for χ and let the adaptation module figure out appropriate values for Φ that satisfies the given values for χ, and calculate the RMSE between the given χ values and the ones calculated using values of Φ returned by the adaptation strategy. Figure 4 shows the RMSE of the sequential K-means technique (Algorithm 2) versus the number of clusters. The results show, in the cases we tested, that with a reasonable number of clusters (≥ 50) a plausible mapping (low RMSE) could be\nestimated between the application performance indicators (χ) which we tested and the consistency level indicator (Φ). Figure 5 shows the RMSE of the incremental K-means technique (Algorithm 3) versus the threshold. The results show, in the cases we tested, that a plausible mapping (low RMSE) could be estimated using a reasonable number of clusters (≥ 50) by using a relatively small threshold (≃ 0.01). The results also indicate that even though online clustering techniques can yield less accurate results compared to offline techniques however in the cases we tested the online clustering techniques were sufficient with a reasonable number of clusters being used."
    }, {
      "heading" : "VI. CONCLUSION AND FUTURE WORK",
      "text" : "In this paper, we examined the feasibility of using adaptive controllers that are built on-top of tunable consistency models similar to that of Apache Cassandra. We presented an adaptation strategy that selects feasible values for the consistency level indicator (Φ) that satisfies a given application performance indicator (χ). We employed two online clustering techniques (sequential and incremental K-means) in order to find suitable mapping between χ and Φ. In the cases that we tested, our results showed that in the case of sequential K-means, with a reasonable number of clusters (≥ 50), a plausible mapping (low RMSE) could be estimated between the application performance indicators (χ) and the consistency\nlevel indicator (Φ). In the case of incremental K-means, the results also showed that a plausible mapping (low RMSE) could be estimated using a similar number of clusters (≥ 50) by using a small threshold (≃ 0.01). In the future, we plan to evaluate the validity and effectiveness of the proposed consistency adaptation strategy using an implementation of an SDN application running on top of a cluster of a distributed controllers."
    }, {
      "heading" : "VII. ACKNOWLEDGMENTS",
      "text" : "The second author acknowledge support from the Natural Sciences and Engineering Research Council of Canada (NSERC) through the NSERC Discovery Grant program."
    } ],
    "references" : [ {
      "title" : "Adaptive Consistency for Distributed SDN Controllers",
      "author" : [ "M. Aslan", "A. Matrawy" ],
      "venue" : "Proceedings of the 17th International Network Strategy and Planning Symposium (Networks 2016), 2016, http://www.sce.carleton.ca/∼maslan/files/sdn-adaptive.pdf.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Towards an elastic distributed sdn controller",
      "author" : [ "A. Dixit", "F. Hao", "S. Mukherjee", "T. Lakshman", "R. Kompella" ],
      "venue" : "Proceedings of the second ACM SIGCOMM workshop on Hot topics in software defined networking. ACM, 2013, pp. 7–12.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Hyperflow: A distributed control plane for openflow",
      "author" : [ "A. Tootoonchian", "Y. Ganjali" ],
      "venue" : "Proceedings of the 2010 internet network management conference on Research on enterprise networking. USENIX Association, 2010, pp. 3–3.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Logically centralized?: state distribution trade-offs in software defined networks",
      "author" : [ "D. Levin", "A. Wundsam", "B. Heller", "N. Handigol", "A. Feldmann" ],
      "venue" : "Proc. of the first workshop on Hot topics in software defined networks. ACM, 2012, pp. 1–6.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Onix: A distributed control platform for large-scale production networks.",
      "author" : [ "T. Koponen", "M. Casado", "N. Gude", "J. Stribling", "L. Poutievski", "M. Zhu", "R. Ramanathan", "Y. Iwata", "H. Inoue", "T. Hama" ],
      "venue" : "in OSDI,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2010
    }, {
      "title" : "Onos: towards an open, distributed sdn os",
      "author" : [ "P. Berde", "M. Gerola", "J. Hart", "Y. Higuchi", "M. Kobayashi", "T. Koide", "B. Lantz", "B. O’Connor", "P. Radoslavov", "W. Snow" ],
      "venue" : "Proceedings of the third workshop on Hot topics in software defined networking. ACM, 2014, pp. 1–6.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Cap for networks",
      "author" : [ "A. Panda", "C. Scott", "A. Ghodsi", "T. Koponen", "S. Shenker" ],
      "venue" : "Proc. of the second ACM SIGCOMM workshop on Hot topics in software defined networking. ACM, 2013, pp. 91–96.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Towards robust distributed systems",
      "author" : [ "E. Brewer" ],
      "venue" : "PODC, 2000, p. 7.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Cap twelve years later: How the “rules” have changed",
      "author" : [ "——" ],
      "venue" : "Computer, vol. 45, no. 2, pp. 23–29, 2012.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Building replicated internet services using tact: A toolkit for tunable availability and consistency tradeoffs",
      "author" : [ "H. Yu", "A. Vahdat" ],
      "venue" : "Advanced Issues of E-Commerce and Web-Based Information Systems, 2000. WECWIS 2000. Second International Workshop on. IEEE, 2000, pp. 75–84.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Improving the performance of load balancing in software-defined networks through load variance-based synchronization",
      "author" : [ "Z. Guo", "M. Su", "Y. Xu", "Z. Duan", "L. Wang", "S. Hui", "H.J. Chao" ],
      "venue" : "Computer Networks, vol. 68, no. 0, pp. 95 – 109, 2014, communications and Networking in the Cloud.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "On the impact of network state collection on the performance of sdn applications",
      "author" : [ "M. Aslan", "A. Matrawy" ],
      "venue" : "IEEE Communications Letters, vol. 20, no. 1, pp. 5–8, 2016.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Cassandra: structured storage system on a p2p network",
      "author" : [ "A. Lakshman", "P. Malik" ],
      "venue" : "Proceedings of the 28th ACM symposium on Principles of distributed computing. ACM, 2009, pp. 5–5.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Cassandra: a decentralized structured storage system",
      "author" : [ "——" ],
      "venue" : "ACM SIGOPS Operating Systems Review, vol. 44, no. 2, pp. 35–40, 2010.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Amazon dynamodb: a seamlessly scalable nonrelational database service",
      "author" : [ "S. Sivasubramanian" ],
      "venue" : "Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data. ACM, 2012, pp. 729–730.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Probabilistically bounded staleness for practical partial quorums",
      "author" : [ "P. Bailis", "S. Venkataraman", "M.J. Franklin", "J.M. Hellerstein", "I. Stoica" ],
      "venue" : "Proceedings of the VLDB Endowment, vol. 5, no. 8, pp. 776–787, 2012.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Online clustering of parallel data streams",
      "author" : [ "J. Beringer", "E. Hüllermeier" ],
      "venue" : "Data & Knowledge Engineering, vol. 58, no. 2, pp. 180–204, 2006.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Scalable techniques for clustering the web",
      "author" : [ "T. Haveliwala", "A. Gionis", "P. Indyk" ],
      "venue" : "2000.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Incremental clustering: The case for extra clusters",
      "author" : [ "M. Ackerman", "S. Dasgupta" ],
      "venue" : "Advances in Neural Information Processing Systems, 2014, pp. 307–315.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Clustering methods",
      "author" : [ "L. Rokach", "O. Maimon" ],
      "venue" : "Data mining and knowledge discovery handbook. Springer, 2005, pp. 321–352.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2005
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Earlier, we suggested [1] the use of adaptivelyconsistent controllers that can autonomously tune their consistency parameters in order to meet the performance requirements of a certain application.",
      "startOffset" : 22,
      "endOffset" : 25
    }, {
      "referenceID" : 1,
      "context" : "Recent research [2], [3], [4], [5], [6] in Software-Defined Networking (SDN) employs multiple distributed controllers for scalability and reliability reasons.",
      "startOffset" : 16,
      "endOffset" : 19
    }, {
      "referenceID" : 2,
      "context" : "Recent research [2], [3], [4], [5], [6] in Software-Defined Networking (SDN) employs multiple distributed controllers for scalability and reliability reasons.",
      "startOffset" : 21,
      "endOffset" : 24
    }, {
      "referenceID" : 3,
      "context" : "Recent research [2], [3], [4], [5], [6] in Software-Defined Networking (SDN) employs multiple distributed controllers for scalability and reliability reasons.",
      "startOffset" : 26,
      "endOffset" : 29
    }, {
      "referenceID" : 4,
      "context" : "Recent research [2], [3], [4], [5], [6] in Software-Defined Networking (SDN) employs multiple distributed controllers for scalability and reliability reasons.",
      "startOffset" : 31,
      "endOffset" : 34
    }, {
      "referenceID" : 5,
      "context" : "Recent research [2], [3], [4], [5], [6] in Software-Defined Networking (SDN) employs multiple distributed controllers for scalability and reliability reasons.",
      "startOffset" : 36,
      "endOffset" : 39
    }, {
      "referenceID" : 6,
      "context" : "Hence, they are subjected to issues similar to those affecting distributed datastores [7].",
      "startOffset" : 86,
      "endOffset" : 89
    }, {
      "referenceID" : 7,
      "context" : "A major issue is the trade-off between consistency and availability in the case of network partitioning, which was identified by Eric Brewer in the CAP (Consistency, Availability and Partitioning) conjecture [8], [9].",
      "startOffset" : 208,
      "endOffset" : 211
    }, {
      "referenceID" : 8,
      "context" : "A major issue is the trade-off between consistency and availability in the case of network partitioning, which was identified by Eric Brewer in the CAP (Consistency, Availability and Partitioning) conjecture [8], [9].",
      "startOffset" : 213,
      "endOffset" : 216
    }, {
      "referenceID" : 9,
      "context" : "While, systems that have the ability to change their behavior (degree of consistency) are known as tunably-consistent systems [10], [11].",
      "startOffset" : 132,
      "endOffset" : 136
    }, {
      "referenceID" : 3,
      "context" : "In SDN, the consistency level of state information exchanged among the distributed controllers can negatively affect the network application performance [4], [12], [13], depending on the performance indicators being considered.",
      "startOffset" : 153,
      "endOffset" : 156
    }, {
      "referenceID" : 10,
      "context" : "In SDN, the consistency level of state information exchanged among the distributed controllers can negatively affect the network application performance [4], [12], [13], depending on the performance indicators being considered.",
      "startOffset" : 158,
      "endOffset" : 162
    }, {
      "referenceID" : 11,
      "context" : "In SDN, the consistency level of state information exchanged among the distributed controllers can negatively affect the network application performance [4], [12], [13], depending on the performance indicators being considered.",
      "startOffset" : 164,
      "endOffset" : 168
    }, {
      "referenceID" : 0,
      "context" : "Earlier [1], we proposed the use of adaptive controllers [1] running ontop of tunably-consistent controllers in order to autonomously handle setting the parameters of the tunably-consistent distributed controllers based on application-specific performance indicators.",
      "startOffset" : 8,
      "endOffset" : 11
    }, {
      "referenceID" : 0,
      "context" : "Earlier [1], we proposed the use of adaptive controllers [1] running ontop of tunably-consistent controllers in order to autonomously handle setting the parameters of the tunably-consistent distributed controllers based on application-specific performance indicators.",
      "startOffset" : 57,
      "endOffset" : 60
    }, {
      "referenceID" : 12,
      "context" : "In this paper, we investigate the feasibility of using adaptive controllers running on-top of tunable consistency models similar to that of Apache Cassandra [14], [15] or Amazon DB [16].",
      "startOffset" : 157,
      "endOffset" : 161
    }, {
      "referenceID" : 13,
      "context" : "In this paper, we investigate the feasibility of using adaptive controllers running on-top of tunable consistency models similar to that of Apache Cassandra [14], [15] or Amazon DB [16].",
      "startOffset" : 163,
      "endOffset" : 167
    }, {
      "referenceID" : 14,
      "context" : "In this paper, we investigate the feasibility of using adaptive controllers running on-top of tunable consistency models similar to that of Apache Cassandra [14], [15] or Amazon DB [16].",
      "startOffset" : 181,
      "endOffset" : 185
    }, {
      "referenceID" : 1,
      "context" : "[2], suggested dynamically growing and shrinking the pool of controllers based on the traffic conditions, and to get rid of the controller/switch static mapping which can led to uneven distribution of the control load.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 6,
      "context" : "First, those controllers are subjected to issues that affect distributed datastores [7] including the trade-off between consistency and availability of data during network partitioning.",
      "startOffset" : 84,
      "endOffset" : 87
    }, {
      "referenceID" : 4,
      "context" : "As such, some SDN applications may prefer different consistency and availability configurations [5].",
      "startOffset" : 96,
      "endOffset" : 99
    }, {
      "referenceID" : 10,
      "context" : "However, as long as it is not creating routing loops (more in [12]), it can tolerate some inconsistency in order to achieve a higher degree of availability.",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 4,
      "context" : "Onix [5] lets the applications make their own trade-off between consistency and availability by providing them with two data-stores: (1) a strongly consistent transactional data-store, and (2) an eventually consistent (more in the next section) in-memory distribute hash table (DHT).",
      "startOffset" : 5,
      "endOffset" : 8
    }, {
      "referenceID" : 0,
      "context" : "In such mode, the controllers given a per-application performance indicator will monitor the network behavior and adapt to the current conditions by autonomously tuning their consistency levels [1].",
      "startOffset" : 194,
      "endOffset" : 197
    }, {
      "referenceID" : 12,
      "context" : "In this section, we explain the consistency model used in a number of modern data-stores such as Apache Cassandra [14], [15] and Amazon DynamoDB [16].",
      "startOffset" : 114,
      "endOffset" : 118
    }, {
      "referenceID" : 13,
      "context" : "In this section, we explain the consistency model used in a number of modern data-stores such as Apache Cassandra [14], [15] and Amazon DynamoDB [16].",
      "startOffset" : 120,
      "endOffset" : 124
    }, {
      "referenceID" : 14,
      "context" : "In this section, we explain the consistency model used in a number of modern data-stores such as Apache Cassandra [14], [15] and Amazon DynamoDB [16].",
      "startOffset" : 145,
      "endOffset" : 149
    }, {
      "referenceID" : 15,
      "context" : "Eventual consistency [17] is a consistency model where all replicas eventually receive the most up-to-date values after sometime if no further updated occurred.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 13,
      "context" : "Even though the number of replicas (N ) is application-specific, the nodes (controllers) themselves that are responsible for maintaining such replicas are decided by a consistent hashing function [15].",
      "startOffset" : 196,
      "endOffset" : 200
    }, {
      "referenceID" : 0,
      "context" : "The adaptation strategy requires the collaboration of different modules of an adaptive controller (proposed in [1]).",
      "startOffset" : 111,
      "endOffset" : 114
    }, {
      "referenceID" : 15,
      "context" : "In case of strong consistency (R+W ≥ N ), Φ = 1 otherwise Φ = 1−ps where ps (shown in (1)) is the probability that the read quorum does not include the last up-to-date version [17].",
      "startOffset" : 176,
      "endOffset" : 180
    }, {
      "referenceID" : 15,
      "context" : "R ) [17] (1)",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 16,
      "context" : "We opt for online incremental clustering techniques [19], [20].",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 17,
      "context" : "We opt for online incremental clustering techniques [19], [20].",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 18,
      "context" : "Algorithm 2 is our adoptation of the “sequential K-means” algorithm presented in [21].",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 19,
      "context" : "We adopt the “incremental clustering” algorithm presented in [22] as a base for Algorithm 3.",
      "startOffset" : 61,
      "endOffset" : 65
    } ],
    "year" : 2017,
    "abstractText" : "Distributed controllers are oftentimes used in largescale SDN deployments where they run a myriad of network applications simultaneously. Such applications could have different consistency and availability preferences. These controllers need to communicate via east/west interfaces in order to synchronize their state information. The consistency and the availability of the distributed state information are governed by an underlying consistency model. Earlier, we suggested [1] the use of adaptivelyconsistent controllers that can autonomously tune their consistency parameters in order to meet the performance requirements of a certain application. In this paper, we examine the feasibility of employing adaptive controllers that are built on-top of tunable consistency models similar to that of Apache Cassandra. We present an adaptation strategy that uses clustering techniques (sequential k-means and incremental k-means) in order to map a given application performance indicator (χ) into a feasible consistency level (Φ) that can be used with the underlying tunable consistency model. In the cases that we modeled and tested, our results show that in the case of sequential k-means, with a reasonable number of clusters (≥ 50), a plausible mapping (low RMSE) could be estimated between the application performance indicators (χ) and the consistency level indicator (Φ). In the case of incremental k-means, the results also showed that a plausible mapping (low RMSE) could be estimated using a similar number of clusters (≥ 50) by using a small threshold (≃ 0.01).",
    "creator" : "LaTeX with hyperref package"
  }
}