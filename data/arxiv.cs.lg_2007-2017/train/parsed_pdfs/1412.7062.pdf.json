{
  "name" : "1412.7062.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "FULLY CONNECTED CRFS", "Liang-Chieh Chen", "George Papandreou" ],
    "emails" : [ "lcchen@cs.ucla.edu", "gpapan@google.com", "iasonas.kokkinos@ecp.fr", "kpmurphy@google.com", "yuille@stat.ucla.edu" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "Deep Convolutional Neural Networks (DCNNs) had been the method of choice for document recognition since LeCun et al. (1998), but have only recently become the mainstream of high-level vision research. Over the past two years DCNNs have pushed the performance of computer vision systems to soaring heights on a broad array of high-level problems, including image classification (Krizhevsky et al., 2013; Sermanet et al., 2013; Simonyan & Zisserman, 2014; Szegedy et al., 2014;\n∗Work performed while G.P. was with the Toyota Technological Institute at Chicago. The first two authors contributed equally to this work.\nar X\niv :1\n41 2.\n70 62\nv1 [\nPapandreou et al., 2014), object detection (Girshick et al., 2014), fine-grained categorization (Zhang et al., 2014) and pose estimation (Chen & Yuille, 2014; Tompson et al., 2014), among others. A common theme in these works is that DCNNs trained in an end-to-end manner deliver strikingly better results than systems relying on carefully engineered features, such as SIFT or HOG features. This success can be partially attributed to the built-in invariance of DCNNs to local image transformations, which underpins their ability to learn hierarchical abstractions of data (Zeiler & Fergus, 2014). While this invariance is clearly desirable for high-level vision tasks, it can hamper low-level tasks, such as semantic segmentation - where we want precise localization, rather than abstraction of spatial details.\nThere are two technical hurdles in the application of DCNNs to image labeling tasks: signal downsampling, and spatial ‘insensitivity’ (invariance). The first problem relates to the reduction of signal resolution incurred by the repeated combination of max-pooling and downsampling (‘striding’) performed at every layer of standard DCNNs (Krizhevsky et al., 2013; Simonyan & Zisserman, 2014; Szegedy et al., 2014). Instead, as in Papandreou et al. (2014), we employ the ‘atrous’ (with holes) algorithm originally developed for efficiently computing the undecimated discrete wavelet transform (Mallat, 1999). This allows efficient dense computation of DCNN responses in a scheme substantially simpler than earlier solutions to this problem (Giusti et al., 2013; Sermanet et al., 2013).\nThe second problem relates to the fact that obtaining object-centric decisions from a classifier inherently requires invariance to spatial transformations. For instance, we cannot expect a ‘bicycle’ classifier to respond differently to pixels that are placed on the bicycle’s support and to pixels that are placed in the interior of its holes, e.g. within the bicycle’s chassis, or rays. We partly alleviate this problem by re-purposing an Imagenet pretrained network to have a smaller receptive field, also accelerating its speed as a by-product. More importantly, we boost our model’s ability to capture fine details by employing a fully-connected Conditional Random Field (CRF). Conditional Random Fields (CRFs) have been broadly used in semantic segmentation to combine class scores computed by multi-way classifiers with the low-level information captured by the local interactions of pixels and edges (Rother et al., 2004; Shotton et al., 2009) or superpixels (Lucchi et al., 2011). Even though works of increased sophistication have been proposed to model the hierarchical dependency (He et al., 2004; Ladicky et al., 2009; Lempitsky et al., 2011) and/or high-order dependencies of segments (Delong et al., 2012; Gonfaus et al., 2010; Kohli et al., 2009; Chen et al., 2013), we use the fully connected pairwise CRF proposed by Krähenbühl & Koltun (2011) for its efficient computation, and ability to capture fine edge details while also catering for long range dependencies. That model was shown in Krähenbühl & Koltun (2011) to largely improve the performance of a boosting-based pixel-level classifier, and in our work we demonstrate that it leads to state-of-the-art results when coupled with a DCNN-based pixel-level classifier.\nThe three main advantages of our “DeepLab” system are (i) speed: by virtue of the ‘atrous’ algorithm and the reduced receptive field size, our dense DCNN operates at 8 fps, while Mean Field Inference for the fully-connected CRF requires 0.5 second, (ii) accuracy: we obtain state-of-the-art results on the PASCAL semantic segmentation challenge, outperforming the second-best approach of Mostajabi et al. (2014) by a margin of 2% and (iii) simplicity: our system is composed of a cascade of two fairly well-established modules, DCNNs and CRFs.\nOur system works directly on the pixel representation, similarly to Long et al. (2014). This is in contrast to the two-stage approaches that are now most common in semantic segmentation with DCNNs: such techniques typically use a cascade of bottom-up image segmentation and DCNN-based region classification, which makes the system commit to potential errors of the front-end segmentation system. For instance, the bounding box proposals and masked regions delivered by (Arbeláez et al., 2014; Uijlings et al., 2013) are used in Girshick et al. (2014) and (Hariharan et al., 2014b) as inputs to a DCNN to introduce shape information into the classification process. Similarly, the authors of Mostajabi et al. (2014) rely on a superpixel representation. A celebrated non-DCNN precursor to these works is the second order pooling method of (Carreira et al., 2012) which also assigns labels to the regions proposals delivered by (Carreira & Sminchisescu, 2012). Understanding the perils of committing to a single segmentation, the authors of Cogswell et al. (2014) build on (Yadollahpour et al., 2013) to explore a diverse set of CRF-based segmentation proposals, computed also by (Carreira & Sminchisescu, 2012). These segmentation proposals are then re-ranked according to a DCNN trained in particular for this reranking task. Even though this approach explicitly tries to handle the temperamental nature of a front-end segmentation algorithm, there is still no explicit ex-\nploitation of the DCNN scores in the CRF-based segmentation algorithm: the DCNN is only applied post-hoc, while it would make sense to directly try to use its results during segmentation.\nMoving towards works that lie closer to our approach, several other researchers have considered the use of convolutionally computed DCNN features for dense image labeling. Among the first have been Farabet et al. (2013) who apply DCNNs at multiple image resolutions and then employ a segmentation tree to smooth the prediction results; more recently, Hariharan et al. (2014a) propose to concatenate the computed inter-mediate feature maps within the DCNNs for pixel classification, and Dai et al. (2014) propose to pool the inter-mediate feature maps by region proposals. Even though these works still employ segmentation algorithms that are decoupled from the DCNN classifier’s results, we believe it is advantageous that segmentation is only used at a later stage, avoiding the commitment to premature decisions.\nMore recently, the segmentation-free techniques of (Long et al., 2014; Eigen & Fergus, 2014) directly apply DCNNs to the whole image in a sliding window fashion, replacing the last fully connected layers of a DCNN by convolutional layers. In order to deal with the spatial localization issues outlined in the beginning of the introduction, Long et al. (2014) upsample and concatenate the scores from inter-mediate feature maps, while Eigen & Fergus (2014) refine the prediction result from coarse to fine by propagating the coarse results to another DCNN.\nThe main difference between our model and other state-of-the-art models is the combination of pixel-level CRFs and DCNN-based ‘unary terms’. Focusing on the closest works in this direction, Cogswell et al. (2014) use CRFs as a proposal mechanism for a DCNN-based reranking system, while Farabet et al. (2013) treat superpixels as nodes for a local pairwise CRF and use graph-cuts for discrete inference; as such their results can be limited by errors in superpixel computations, while ignoring long-range superpixel dependencies. Our approach instead treats every pixel as a CRF node, exploits long-range dependencies, and uses CRF inference to directly optimize a DCNNdriven cost function."
    }, {
      "heading" : "2 CONVOLUTIONAL NEURAL NETWORKS FOR DENSE IMAGE LABELING",
      "text" : "Herein we describe how we have re-purposed and finetuned the publicly available Imagenetpretrained state-of-art 16-layer classification network of (Simonyan & Zisserman, 2014) (VGG-16) into an efficient and effective dense feature extractor for our dense semantic image segmentation system."
    }, {
      "heading" : "2.1 EFFICIENT DENSE SLIDING WINDOW FEATURE EXTRACTION WITH THE HOLE ALGORITHM",
      "text" : "Dense spatial score evaluation is instrumental in the success of our dense CNN feature extractor. As a first step to implement this, we convert the fully-connected layers of VGG-16 into convolutional ones and run the network in a convolutional fashion on the image at its original resolution. However this is not enough as it yields very sparsely computed detection scores (with a stride of 32 pixels). To compute scores more densely at our target stride of 8 pixels, we develop a variation of the method previously employed by Giusti et al. (2013); Sermanet et al. (2013). We skip subsampling after the last two max-pooling layers in the network of Simonyan & Zisserman (2014) and modify the convolutional filters in the layers that follow them by introducing zeros to increase their length (2× in the last three convolutional layers and 4× in the first fully connected layer). We can implement this more efficiently by keeping the filters intact and instead sparsely sample the feature maps on which they are applied on using a stride of 2 or 4 pixels, respectively. This approach is known as the ‘hole algorithm’ (‘atrous algorithm’) and has been developed before for efficient computation of the undecimated wavelet transform (Mallat, 1999). We have implemented this within the Caffe framework (Jia et al., 2014) by adding to the im2col function (it converts multi-channel feature maps to vectorized patches) the option to sparsely sample the underlying feature map. This approach is generally applicable and allows us to efficiently compute dense CNN feature maps at any target subsampling rate without introducing any approximations.\nWe finetune the model weights of the Imagenet-pretrained VGG-16 network to adapt it to the image classification task in a straightforward fashion, following the procedure of Long et al. (2014). We replace the 1000-way Imagenet classifier in the last layer of VGG-16 with a 21-way one. Our\nloss function is the sum of cross-entropy terms for each spatial position in the CNN output map (subsampled by 8 compared to the original image). All positions and labels are equally weighted in the overall loss function. Our targets are the ground truth labels (subsampled by 8). We optimize the objective function with respect to the weights at all network layers by the standard SGD procedure of Krizhevsky et al. (2013).\nDuring testing, we need class score maps at the original image resolution. As illustrated in Figure 1 and further elaborated in Section 3.1, the class score maps (corresponding to log-probabilities) are quite smooth, which allows us to use simple bilinear interpolation to increase their resolution by a factor of 8 at a negligible computational cost. Note that the method of Long et al. (2014) does not use the hole algorithm and produces very coarse scores (subsampled by a factor of 32) at the CNN output. This forced them to use learned upsampling layers, significantly increasing the complexity and training time of their system: Fine-tuning our network on PASCAL VOC 2012 takes about 10 hours, while they report a training time of several days (both timings on a modern GPU)."
    }, {
      "heading" : "2.2 SHRINKING THE RECEPTIVE FIELD AND ACCELERATING DENSE COMPUTATION WITH CONVOLUTIONAL NETS",
      "text" : "Another key ingredient in re-purposing our network for dense score computation is explicitly controlling the network’s receptive field size. Most recent DCNN-based image recognition methods rely on networks pre-trained on the Imagenet large-scale classification task. These networks typically have large receptive field size: in the case of the VGG-16 net we consider, its receptive field is 224×224 (with zero-padding) and 404×404 pixels if the net is applied convolutionally. We consider this receptive field size to be too large to allow good localization accuracy (unless one uses heavily zoomed-in versions of the image). Moreover, after converting the network to a fully convolutional one, the first fully connected layer has 4,096 filters of large 7×7 spatial size and becomes the computational bottleneck in our dense score map computation.\nWe have addressed both of these serious practical problems by spatially subsampling the first FC layer to 4×4 spatial size. This has reduced the receptive field of the network down to 128×128 (with zero-padding) or 308×308) (in convolutional mode) and has reduced computation time for the first FC layer by 3 times. Using our Caffe-based implementation and a Titan GPU, the resulting VGG-derived network is very efficient: Given a 306×306 input image, it produces 39×39 dense raw feature scores at the top of the network at a rate of about 8 frames/sec during testing. The speed during training is 3 frames/sec. Using smaller networks such as Krizhevsky et al. (2013) could allow video-rate test-time dense feature computation even on light-weight GPUs."
    }, {
      "heading" : "3 DETAILED BOUNDARY RECOVERY: FULLY-CONNECTED CONDITIONAL RANDOM FIELDS",
      "text" : ""
    }, {
      "heading" : "3.1 DEEP CONVOLUTIONAL NETWORKS AND THE LOCALIZATION CHALLENGE",
      "text" : "As illustrated in Figure 1, DCNN score maps can reliably predict the presense and rough position of objects in an image but are less well suited for pin-pointing their exact outline. There is a natural trade-off between classification accuracy and localization accuracy with convolutional networks: Deeper moders with multiple max-pooling layers have proven most successful in classification tasks, however their increased invariance and large receptive fields make the problem of inferring position from the scores at their top output levels more challenging.\nThe reduced receptive field size in our network architecture can ameliorate but not fully resolve this problem. Recent work has pursued two directions to address this localization challenge. The first approach is to harness information from multiple layers in the convolutional network in order to better estimate the object boundaries (Long et al., 2014; Eigen & Fergus, 2014). The second approach is to employ a super-pixel representation, essentially delegating the localization task to a low-level segmentation method. This route is followed by the very successful recent method of Mostajabi et al. (2014).\nIn Section 3.2, we pursue a novel alternative direction based on coupling the recognition capacity of DCNNs and the fine-grained localization accuracy of fully connected CRFs and show that it is remarkably successful in addressing the localication challenge, producing accurate semantic seg-\nmentation results and recovering object boundaries at a level of detail that is well beyond the reach of existing methods."
    }, {
      "heading" : "3.2 FULLY-CONNECTED CONDITIONAL RANDOM FIELDS FOR ACCURATE LOCALIZATION",
      "text" : "Traditionally, conditional random fields (CRFs) have been employed to smooth noisy segmentation maps (Rother et al., 2004; Kohli et al., 2009). Typically these models contain energy terms that couple neighboring nodes, favoring same-label assignments to spatially proximal pixels. Qualitatively, the primary function of these short-range CRFs has been to clean up the spurious predictions of weak classifiers built on top of local hand-engineered features.\nCompared to these weaker classifiers, modern DCNN architectures such as the one we use in this work produce score maps and semantic label predictions which are qualitatively different. As illustrated in Figure 1, the score maps are typically quite smooth and produce homogeneous classification results. In this regime, using short-range CRFs can be detrimental, as our goal should be to recover detailed local structure rather than further smooth it. Using contrast-sensitive potentials (Rother et al., 2004) in conjunction to local-range CRFs can potentially improve localization but still miss thin-structures and typically requires solving an expensive discrete optimization problem.\nTo overcome these limitations of short-range CRFs, we integrate into our system the fully connected CRF model of Krähenbühl & Koltun (2011). The model employs the energy function\nE(x) = ∑ i θi(xi) + ∑ ij θij(xi, xj) (1)\nwhere x is the label assignment for pixels. We use as unary potential θi(xi) = − logP (xi), where P (xi) is the label assignment probability at pixel i as computed by DCNN. The pairwise potential is θij(xi, xj) = µ(xi, xj) ∑K m=1 wm · km(f i,f j), where µ(xi, xj) = 1 if xi 6= xj , and zero otherwise (i.e., Potts Model). There is one pairwise term for each pair of pixels i and j in the image no matter how far from each other they lie, i.e. the model’s factor graph is fully connected. Each km is the Gaussian kernel depends on features (denoted as f ) extracted for pixel i and j and is weighted by parameter wm. We adopt bilateral position and color terms, specifically, the kernels are\nw1 exp ( − ||pi − pj || 2\n2σ2α − ||Ii − Ij || 2 2σ2β\n) + w2 exp ( − ||pi − pj || 2\n2σ2γ\n) (2)\nwhere the first kernel depends on both pixel positions (denoted as p) and pixel color intensities (denoted as I), and the second kernel only depends on pixel positions. The hyper parameters σα, σβ and σγ control the “scale” of the Gaussian kernels.\nCrucially, this model is amenable to efficient approximate probabilistic inference (Krähenbühl & Koltun, 2011). The message passing updates under a fully decomposable mean field approximation b(x) = ∏ i bi(xi) can be expressed as convolutions with a Gaussian kernel in feature space. High-dimensional filtering algorihtms (Adams et al., 2010) significantly speed-up this computation resulting in an algorithm that is very fast in practice, less that 0.5 sec on average for Pascal VOC images using the publicly available implementation of (Krähenbühl & Koltun, 2011)."
    }, {
      "heading" : "4 EXPERIMENTAL EVALUATION",
      "text" : "Dataset We test our DeepLab model on the PASCAL VOC 2012 segmentation benchmark (Everingham et al., 2014), consisting of 20 foreground object classes and one background class. The original dataset contains 1, 464, 1, 449, and 1, 456 images for training, validation, and testing, respectively. The dataset is augmented by the extra annotations provided by Hariharan et al. (2011), resulting in 10, 582 training images. The performance is measured in terms of pixel intersectionover-union (IOU) averaged across the 21 classes.\nTraining We adopt the simplest form of piecewise training, decoupling the DCNN and CRF training stages, assuming the unary terms provided by the DCNN are fixed during CRF training.\nFor DCNN training we employ the VGG-16 netwok which has been pre-trained on ImageNet. We fine-tuned the VGG-16 network on the VOC 21-way classification task by stochastic gradient descent on the cross-entropy loss function, as described in Section 2.1. We use a mini-batch of 20 images and initial learning rate of 0.001 (0.01 for the final classifier layer), multiplying the learning rate by 0.1 at every 2000 iterations. We use momentum of 0.9 and a weight decay of 0.0005.\nAfter the DCNN has been fine-tuned, we cross-validate the parameters of the fully connected CRF model in Eq. (2) along the lines of Krähenbühl & Koltun (2011); to avoid overfitting the validation set, the parameters w2 and σγ are fixed to be 3 and the best values of w1, σα, and σβ are crossvalidated on a small subset of the validation set (we use 200 images). We use 10 iterations for the efficient mean field inference algorithm (Krähenbühl & Koltun, 2011).\nEvaluation on Validation set We conduct the majority of our evaluations on the PASCAL ‘val’ set, training our model on the augmented PASCAL ‘train’ set. As shown in Tab. 1, incorporating the fully connected CRF to our model (denoted by DeepLab-CRF) yields a substantial performance boost, about 4% improvement. We note that the original work of Krähenbühl & Koltun (2011) improved the 27.6% result of TextonBoost (Shotton et al., 2009) to 29.1%, which makes the improvement we report here (from 59.8% to 63.7%) all the more impressive.\nTurning to qualitative results, we provide visual comparisons between DeepLab and DeepLab-CRF in Fig. 5. Employing a fully connected CRF significantly improves the results, allowing the model to accurately capture intricate object boundaries.\nMean Pixel IOU along Object Boundaries To quantify the accuracy of the proposed model near object boundaries, we evaluate the segmentation accuracy with an experiment similar to Kohli et al. (2009); Krähenbühl & Koltun (2011). Specifically, we use the ‘void’ label annotated in val set, which usually occurs around object boundaries. We compute the mean IOU for those pixels that are located within a narrow band (called trimap) of ‘void’ labels. As shown in Fig. 4, refining the segmentation results by a fully connected CRF significantly improves the results around object boundaries.\nComparison with State-of-art In Fig. 3, we qualitatively compare our proposed model, DeepLabCRF, with two state-of-art models: FCN-8s (Long et al., 2014) and TTI-Zoomout-16 (Mostajabi et al., 2014) on the ‘val’ set (the results are extracted from their papers). Our model is able to capture the intricate object boundaries.\nTest set results Having set our model choices on the validation set, we evaluate our best model variant, DeepLab-CRF, on the PASCAL VOC 2012 official ’test’ set. As shown in Tab. 2, our model achieves performance of 66.4% mean IOU1, outperforming all the other state-of-the-art models (specifically, TTI-Zoomout-16 (Mostajabi et al., 2014), FCN-8s (Long et al., 2014), and MSRACFM (Dai et al., 2014)).\n1http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php? challengeid=11&compid=6"
    }, {
      "heading" : "5 DISCUSSION",
      "text" : "Our work combines ideas from deep convolutional neural networks and fully-connected conditional random fields, yielding a novel method able to produce semantically accurate predictions and detailed segmentation maps, while being computationally efficient. Our experimental results show that the proposed method significantly advances the state-of-art in the challenging PASCAL VOC 2012 semantic image segmentation task.\nThere are multiple aspects in our model that we intend to refine, such as fully integrating its two main components (CNN and CRF) and train the whole system in an end-to-end fashion, similar to Krähenbühl & Koltun (2013); Chen et al. (2014). We also plan to experiment with more datasets and apply our method to other sources of data such as depth maps or videos.\nAt a higher level, our work lies in the intersection of convolutional neural networks and probabilistic graphical models. We plan to further investigate the interplay of these two powerful classes of methods and explore their synergistic potential for solving challenging computer vision tasks."
    } ],
    "references" : [ {
      "title" : "Fast high-dimensional filtering using the permutohedral lattice",
      "author" : [ "A. Adams", "J. Baek", "M.A. Davis" ],
      "venue" : "In Computer Graphics Forum,",
      "citeRegEx" : "Adams et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Adams et al\\.",
      "year" : 2010
    }, {
      "title" : "Multiscale combinatorial grouping",
      "author" : [ "P. Arbeláez", "J. Pont-Tuset", "J.T. Barron", "F. Marques", "J. Malik" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "Arbeláez et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Arbeláez et al\\.",
      "year" : 2014
    }, {
      "title" : "Cpmc: Automatic object segmentation using constrained parametric min-cuts",
      "author" : [ "J. Carreira", "C. Sminchisescu" ],
      "venue" : null,
      "citeRegEx" : "Carreira and Sminchisescu,? \\Q2012\\E",
      "shortCiteRegEx" : "Carreira and Sminchisescu",
      "year" : 2012
    }, {
      "title" : "Semantic segmentation with second-order pooling",
      "author" : [ "J. Carreira", "R. Caseiro", "J. Batista", "C. Sminchisescu" ],
      "venue" : "In ECCV,",
      "citeRegEx" : "Carreira et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Carreira et al\\.",
      "year" : 2012
    }, {
      "title" : "Learning a dictionary of shape epitomes with applications to image labeling",
      "author" : [ "Chen", "L.-C", "G. Papandreou", "A. Yuille" ],
      "venue" : "In ICCV,",
      "citeRegEx" : "Chen et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2013
    }, {
      "title" : "Articulated pose estimation by a graphical model with image dependent pairwise relations",
      "author" : [ "X. Chen", "A.L. Yuille" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Chen and Yuille,? \\Q2014\\E",
      "shortCiteRegEx" : "Chen and Yuille",
      "year" : 2014
    }, {
      "title" : "Combining the best of graphical models and convnets for semantic segmentation",
      "author" : [ "M. Cogswell", "X. Lin", "S. Purushwalkam", "D. Batra" ],
      "venue" : null,
      "citeRegEx" : "Cogswell et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Cogswell et al\\.",
      "year" : 2014
    }, {
      "title" : "Convolutional feature masking for joint object and stuff segmentation",
      "author" : [ "J. Dai", "K. He", "J. Sun" ],
      "venue" : null,
      "citeRegEx" : "Dai et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Dai et al\\.",
      "year" : 2014
    }, {
      "title" : "Fast approximate energy minimization with label costs",
      "author" : [ "A. Delong", "A. Osokin", "H.N. Isack", "Y. Boykov" ],
      "venue" : null,
      "citeRegEx" : "Delong et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Delong et al\\.",
      "year" : 2012
    }, {
      "title" : "Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture",
      "author" : [ "D. Eigen", "R. Fergus" ],
      "venue" : null,
      "citeRegEx" : "Eigen and Fergus,? \\Q2014\\E",
      "shortCiteRegEx" : "Eigen and Fergus",
      "year" : 2014
    }, {
      "title" : "The pascal visual object classes challenge a retrospective",
      "author" : [ "M. Everingham", "S.M.A. Eslami", "L.V. Gool", "C.K.I. Williams", "J. Winn", "A. Zisserma" ],
      "venue" : null,
      "citeRegEx" : "Everingham et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Everingham et al\\.",
      "year" : 2014
    }, {
      "title" : "Learning hierarchical features for scene labeling",
      "author" : [ "C. Farabet", "C. Couprie", "L. Najman", "Y. LeCun" ],
      "venue" : null,
      "citeRegEx" : "Farabet et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Farabet et al\\.",
      "year" : 2013
    }, {
      "title" : "Rich feature hierarchies for accurate object detection and semantic segmentation",
      "author" : [ "R. Girshick", "J. Donahue", "T. Darrell", "J. Malik" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "Girshick et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Girshick et al\\.",
      "year" : 2014
    }, {
      "title" : "Fast image scanning with deep max-pooling convolutional neural networks",
      "author" : [ "A. Giusti", "D. Ciresan", "J. Masci", "L. Gambardella", "J. Schmidhuber" ],
      "venue" : "In ICIP,",
      "citeRegEx" : "Giusti et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Giusti et al\\.",
      "year" : 2013
    }, {
      "title" : "Harmony potentials for joint classification and segmentation",
      "author" : [ "J.M. Gonfaus", "X. Boix", "J. Van de Weijer", "A.D. Bagdanov", "J. Serrat", "J. Gonzalez" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "Gonfaus et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Gonfaus et al\\.",
      "year" : 2010
    }, {
      "title" : "Semantic contours from inverse detectors",
      "author" : [ "B. Hariharan", "P. Arbeláez", "L. Bourdev", "S. Maji", "J. Malik" ],
      "venue" : "In ICCV,",
      "citeRegEx" : "Hariharan et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Hariharan et al\\.",
      "year" : 2011
    }, {
      "title" : "Hypercolumns for object segmentation and fine-grained localization",
      "author" : [ "B. Hariharan", "P. Arbeláez", "R. Girshick", "J. Malik" ],
      "venue" : null,
      "citeRegEx" : "Hariharan et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Hariharan et al\\.",
      "year" : 2014
    }, {
      "title" : "Simultaneous detection and segmentation",
      "author" : [ "B. Hariharan", "P. Arbeláez", "R. Girshick", "J. Malik" ],
      "venue" : "In ECCV,",
      "citeRegEx" : "Hariharan et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Hariharan et al\\.",
      "year" : 2014
    }, {
      "title" : "Multiscale conditional random fields for image labeling",
      "author" : [ "X. He", "R.S. Zemel", "M. Carreira-Perpindn" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "He et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2004
    }, {
      "title" : "Caffe: Convolutional architecture for fast feature embedding",
      "author" : [ "Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell" ],
      "venue" : null,
      "citeRegEx" : "Jia et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Jia et al\\.",
      "year" : 2014
    }, {
      "title" : "Robust higher order potentials for enforcing label consistency",
      "author" : [ "P. Kohli", "L. Ladicky", "P.H. Torr" ],
      "venue" : null,
      "citeRegEx" : "Kohli et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Kohli et al\\.",
      "year" : 2009
    }, {
      "title" : "Efficient inference in fully connected crfs with gaussian edge potentials",
      "author" : [ "P. Krähenbühl", "V. Koltun" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Krähenbühl and Koltun,? \\Q2011\\E",
      "shortCiteRegEx" : "Krähenbühl and Koltun",
      "year" : 2011
    }, {
      "title" : "Parameter learning and convergent inference for dense random fields",
      "author" : [ "P. Krähenbühl", "V. Koltun" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Krähenbühl and Koltun,? \\Q2013\\E",
      "shortCiteRegEx" : "Krähenbühl and Koltun",
      "year" : 2013
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "A. Krizhevsky", "I. Sutskever", "G.E. Hinton" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Krizhevsky et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Krizhevsky et al\\.",
      "year" : 2013
    }, {
      "title" : "Associative hierarchical crfs for object class image segmentation",
      "author" : [ "L. Ladicky", "C. Russell", "P. Kohli", "P.H. Torr" ],
      "venue" : "In ICCV,",
      "citeRegEx" : "Ladicky et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Ladicky et al\\.",
      "year" : 2009
    }, {
      "title" : "Gradient-based learning applied to document recognition",
      "author" : [ "Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner" ],
      "venue" : "In Proc. IEEE,",
      "citeRegEx" : "LeCun et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "LeCun et al\\.",
      "year" : 1998
    }, {
      "title" : "Pylon model for semantic segmentation",
      "author" : [ "V. Lempitsky", "A. Vedaldi", "A. Zisserman" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Lempitsky et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Lempitsky et al\\.",
      "year" : 2011
    }, {
      "title" : "Fully convolutional networks for semantic segmentation",
      "author" : [ "J. Long", "E. Shelhamer", "T. Darrell" ],
      "venue" : null,
      "citeRegEx" : "Long et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Long et al\\.",
      "year" : 2014
    }, {
      "title" : "Are spatial and global constraints really necessary for segmentation",
      "author" : [ "A. Lucchi", "Y. Li", "X. Boix", "K. Smith", "P. Fua" ],
      "venue" : "In ICCV,",
      "citeRegEx" : "Lucchi et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Lucchi et al\\.",
      "year" : 2011
    }, {
      "title" : "A Wavelet Tour of Signal Processing",
      "author" : [ "S. Mallat" ],
      "venue" : null,
      "citeRegEx" : "Mallat,? \\Q1999\\E",
      "shortCiteRegEx" : "Mallat",
      "year" : 1999
    }, {
      "title" : "Feedforward semantic segmentation with zoom-out features",
      "author" : [ "M. Mostajabi", "P. Yadollahpour", "G. Shakhnarovich" ],
      "venue" : null,
      "citeRegEx" : "Mostajabi et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Mostajabi et al\\.",
      "year" : 2014
    }, {
      "title" : "Untangling local and global deformations in deep convolutional networks for image classification and sliding window detection",
      "author" : [ "G. Papandreou", "I. Kokkinos", "Savalle", "P.-A" ],
      "venue" : null,
      "citeRegEx" : "Papandreou et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Papandreou et al\\.",
      "year" : 2014
    }, {
      "title" : "Grabcut: Interactive foreground extraction using iterated graph cuts",
      "author" : [ "C. Rother", "V. Kolmogorov", "A. Blake" ],
      "venue" : "In SIGGRAPH,",
      "citeRegEx" : "Rother et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Rother et al\\.",
      "year" : 2004
    }, {
      "title" : "Overfeat: Integrated recognition, localization and detection using convolutional networks",
      "author" : [ "P. Sermanet", "D. Eigen", "X. Zhang", "M. Mathieu", "R. Fergus", "Y. LeCun" ],
      "venue" : null,
      "citeRegEx" : "Sermanet et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Sermanet et al\\.",
      "year" : 2013
    }, {
      "title" : "Textonboost for image understanding: Multiclass object recognition and segmentation by jointly modeling texture",
      "author" : [ "J. Shotton", "J. Winn", "C. Rother", "A. Criminisi" ],
      "venue" : "layout, and context. IJCV,",
      "citeRegEx" : "Shotton et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Shotton et al\\.",
      "year" : 2009
    }, {
      "title" : "Very deep convolutional networks for large-scale image recognition",
      "author" : [ "K. Simonyan", "A. Zisserman" ],
      "venue" : null,
      "citeRegEx" : "Simonyan and Zisserman,? \\Q2014\\E",
      "shortCiteRegEx" : "Simonyan and Zisserman",
      "year" : 2014
    }, {
      "title" : "Joint Training of a Convolutional Network and a Graphical Model for Human Pose Estimation",
      "author" : [ "J. Tompson", "A. Jain", "Y. LeCun", "C. Bregler" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Tompson et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Tompson et al\\.",
      "year" : 2014
    }, {
      "title" : "Selective search for object recognition",
      "author" : [ "J. Uijlings", "K. van de Sande", "T. Gevers", "A. Smeulders" ],
      "venue" : null,
      "citeRegEx" : "Uijlings et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Uijlings et al\\.",
      "year" : 2013
    }, {
      "title" : "Discriminative re-ranking of diverse segmentations",
      "author" : [ "P. Yadollahpour", "D. Batra", "G. Shakhnarovich" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "Yadollahpour et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Yadollahpour et al\\.",
      "year" : 2013
    }, {
      "title" : "Visualizing and understanding convolutional networks",
      "author" : [ "M.D. Zeiler", "R. Fergus" ],
      "venue" : "In ECCV,",
      "citeRegEx" : "Zeiler and Fergus,? \\Q2014\\E",
      "shortCiteRegEx" : "Zeiler and Fergus",
      "year" : 2014
    }, {
      "title" : "Part-based r-cnns for fine-grained category detection",
      "author" : [ "N. Zhang", "J. Donahue", "R. Girshick", "T. Darrell" ],
      "venue" : "In ECCV,",
      "citeRegEx" : "Zhang et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 24,
      "context" : "Deep Convolutional Neural Networks (DCNNs) had been the method of choice for document recognition since LeCun et al. (1998), but have only recently become the mainstream of high-level vision research.",
      "startOffset" : 104,
      "endOffset" : 124
    }, {
      "referenceID" : 12,
      "context" : ", 2014), object detection (Girshick et al., 2014), fine-grained categorization (Zhang et al.",
      "startOffset" : 26,
      "endOffset" : 49
    }, {
      "referenceID" : 40,
      "context" : ", 2014), fine-grained categorization (Zhang et al., 2014) and pose estimation (Chen & Yuille, 2014; Tompson et al.",
      "startOffset" : 37,
      "endOffset" : 57
    }, {
      "referenceID" : 36,
      "context" : ", 2014) and pose estimation (Chen & Yuille, 2014; Tompson et al., 2014), among others.",
      "startOffset" : 28,
      "endOffset" : 71
    }, {
      "referenceID" : 23,
      "context" : "The first problem relates to the reduction of signal resolution incurred by the repeated combination of max-pooling and downsampling (‘striding’) performed at every layer of standard DCNNs (Krizhevsky et al., 2013; Simonyan & Zisserman, 2014; Szegedy et al., 2014).",
      "startOffset" : 189,
      "endOffset" : 264
    }, {
      "referenceID" : 29,
      "context" : "(2014), we employ the ‘atrous’ (with holes) algorithm originally developed for efficiently computing the undecimated discrete wavelet transform (Mallat, 1999).",
      "startOffset" : 144,
      "endOffset" : 158
    }, {
      "referenceID" : 13,
      "context" : "This allows efficient dense computation of DCNN responses in a scheme substantially simpler than earlier solutions to this problem (Giusti et al., 2013; Sermanet et al., 2013).",
      "startOffset" : 131,
      "endOffset" : 175
    }, {
      "referenceID" : 33,
      "context" : "This allows efficient dense computation of DCNN responses in a scheme substantially simpler than earlier solutions to this problem (Giusti et al., 2013; Sermanet et al., 2013).",
      "startOffset" : 131,
      "endOffset" : 175
    }, {
      "referenceID" : 32,
      "context" : "Conditional Random Fields (CRFs) have been broadly used in semantic segmentation to combine class scores computed by multi-way classifiers with the low-level information captured by the local interactions of pixels and edges (Rother et al., 2004; Shotton et al., 2009) or superpixels (Lucchi et al.",
      "startOffset" : 225,
      "endOffset" : 268
    }, {
      "referenceID" : 34,
      "context" : "Conditional Random Fields (CRFs) have been broadly used in semantic segmentation to combine class scores computed by multi-way classifiers with the low-level information captured by the local interactions of pixels and edges (Rother et al., 2004; Shotton et al., 2009) or superpixels (Lucchi et al.",
      "startOffset" : 225,
      "endOffset" : 268
    }, {
      "referenceID" : 28,
      "context" : ", 2009) or superpixels (Lucchi et al., 2011).",
      "startOffset" : 23,
      "endOffset" : 44
    }, {
      "referenceID" : 18,
      "context" : "Even though works of increased sophistication have been proposed to model the hierarchical dependency (He et al., 2004; Ladicky et al., 2009; Lempitsky et al., 2011) and/or high-order dependencies of segments (Delong et al.",
      "startOffset" : 102,
      "endOffset" : 165
    }, {
      "referenceID" : 24,
      "context" : "Even though works of increased sophistication have been proposed to model the hierarchical dependency (He et al., 2004; Ladicky et al., 2009; Lempitsky et al., 2011) and/or high-order dependencies of segments (Delong et al.",
      "startOffset" : 102,
      "endOffset" : 165
    }, {
      "referenceID" : 26,
      "context" : "Even though works of increased sophistication have been proposed to model the hierarchical dependency (He et al., 2004; Ladicky et al., 2009; Lempitsky et al., 2011) and/or high-order dependencies of segments (Delong et al.",
      "startOffset" : 102,
      "endOffset" : 165
    }, {
      "referenceID" : 8,
      "context" : ", 2011) and/or high-order dependencies of segments (Delong et al., 2012; Gonfaus et al., 2010; Kohli et al., 2009; Chen et al., 2013), we use the fully connected pairwise CRF proposed by Krähenbühl & Koltun (2011) for its efficient computation, and ability to capture fine edge details while also catering for long range dependencies.",
      "startOffset" : 51,
      "endOffset" : 133
    }, {
      "referenceID" : 14,
      "context" : ", 2011) and/or high-order dependencies of segments (Delong et al., 2012; Gonfaus et al., 2010; Kohli et al., 2009; Chen et al., 2013), we use the fully connected pairwise CRF proposed by Krähenbühl & Koltun (2011) for its efficient computation, and ability to capture fine edge details while also catering for long range dependencies.",
      "startOffset" : 51,
      "endOffset" : 133
    }, {
      "referenceID" : 20,
      "context" : ", 2011) and/or high-order dependencies of segments (Delong et al., 2012; Gonfaus et al., 2010; Kohli et al., 2009; Chen et al., 2013), we use the fully connected pairwise CRF proposed by Krähenbühl & Koltun (2011) for its efficient computation, and ability to capture fine edge details while also catering for long range dependencies.",
      "startOffset" : 51,
      "endOffset" : 133
    }, {
      "referenceID" : 4,
      "context" : ", 2011) and/or high-order dependencies of segments (Delong et al., 2012; Gonfaus et al., 2010; Kohli et al., 2009; Chen et al., 2013), we use the fully connected pairwise CRF proposed by Krähenbühl & Koltun (2011) for its efficient computation, and ability to capture fine edge details while also catering for long range dependencies.",
      "startOffset" : 51,
      "endOffset" : 133
    }, {
      "referenceID" : 1,
      "context" : "For instance, the bounding box proposals and masked regions delivered by (Arbeláez et al., 2014; Uijlings et al., 2013) are used in Girshick et al.",
      "startOffset" : 73,
      "endOffset" : 119
    }, {
      "referenceID" : 37,
      "context" : "For instance, the bounding box proposals and masked regions delivered by (Arbeláez et al., 2014; Uijlings et al., 2013) are used in Girshick et al.",
      "startOffset" : 73,
      "endOffset" : 119
    }, {
      "referenceID" : 3,
      "context" : "A celebrated non-DCNN precursor to these works is the second order pooling method of (Carreira et al., 2012) which also assigns labels to the regions proposals delivered by (Carreira & Sminchisescu, 2012).",
      "startOffset" : 85,
      "endOffset" : 108
    }, {
      "referenceID" : 38,
      "context" : "(2014) build on (Yadollahpour et al., 2013) to explore a diverse set of CRF-based segmentation proposals, computed also by (Carreira & Sminchisescu, 2012).",
      "startOffset" : 16,
      "endOffset" : 43
    }, {
      "referenceID" : 7,
      "context" : ", 2014), object detection (Girshick et al., 2014), fine-grained categorization (Zhang et al., 2014) and pose estimation (Chen & Yuille, 2014; Tompson et al., 2014), among others. A common theme in these works is that DCNNs trained in an end-to-end manner deliver strikingly better results than systems relying on carefully engineered features, such as SIFT or HOG features. This success can be partially attributed to the built-in invariance of DCNNs to local image transformations, which underpins their ability to learn hierarchical abstractions of data (Zeiler & Fergus, 2014). While this invariance is clearly desirable for high-level vision tasks, it can hamper low-level tasks, such as semantic segmentation - where we want precise localization, rather than abstraction of spatial details. There are two technical hurdles in the application of DCNNs to image labeling tasks: signal downsampling, and spatial ‘insensitivity’ (invariance). The first problem relates to the reduction of signal resolution incurred by the repeated combination of max-pooling and downsampling (‘striding’) performed at every layer of standard DCNNs (Krizhevsky et al., 2013; Simonyan & Zisserman, 2014; Szegedy et al., 2014). Instead, as in Papandreou et al. (2014), we employ the ‘atrous’ (with holes) algorithm originally developed for efficiently computing the undecimated discrete wavelet transform (Mallat, 1999).",
      "startOffset" : 27,
      "endOffset" : 1250
    }, {
      "referenceID" : 2,
      "context" : ", 2009; Chen et al., 2013), we use the fully connected pairwise CRF proposed by Krähenbühl & Koltun (2011) for its efficient computation, and ability to capture fine edge details while also catering for long range dependencies.",
      "startOffset" : 8,
      "endOffset" : 107
    }, {
      "referenceID" : 2,
      "context" : ", 2009; Chen et al., 2013), we use the fully connected pairwise CRF proposed by Krähenbühl & Koltun (2011) for its efficient computation, and ability to capture fine edge details while also catering for long range dependencies. That model was shown in Krähenbühl & Koltun (2011) to largely improve the performance of a boosting-based pixel-level classifier, and in our work we demonstrate that it leads to state-of-the-art results when coupled with a DCNN-based pixel-level classifier.",
      "startOffset" : 8,
      "endOffset" : 279
    }, {
      "referenceID" : 2,
      "context" : ", 2009; Chen et al., 2013), we use the fully connected pairwise CRF proposed by Krähenbühl & Koltun (2011) for its efficient computation, and ability to capture fine edge details while also catering for long range dependencies. That model was shown in Krähenbühl & Koltun (2011) to largely improve the performance of a boosting-based pixel-level classifier, and in our work we demonstrate that it leads to state-of-the-art results when coupled with a DCNN-based pixel-level classifier. The three main advantages of our “DeepLab” system are (i) speed: by virtue of the ‘atrous’ algorithm and the reduced receptive field size, our dense DCNN operates at 8 fps, while Mean Field Inference for the fully-connected CRF requires 0.5 second, (ii) accuracy: we obtain state-of-the-art results on the PASCAL semantic segmentation challenge, outperforming the second-best approach of Mostajabi et al. (2014) by a margin of 2% and (iii) simplicity: our system is composed of a cascade of two fairly well-established modules, DCNNs and CRFs.",
      "startOffset" : 8,
      "endOffset" : 898
    }, {
      "referenceID" : 2,
      "context" : ", 2009; Chen et al., 2013), we use the fully connected pairwise CRF proposed by Krähenbühl & Koltun (2011) for its efficient computation, and ability to capture fine edge details while also catering for long range dependencies. That model was shown in Krähenbühl & Koltun (2011) to largely improve the performance of a boosting-based pixel-level classifier, and in our work we demonstrate that it leads to state-of-the-art results when coupled with a DCNN-based pixel-level classifier. The three main advantages of our “DeepLab” system are (i) speed: by virtue of the ‘atrous’ algorithm and the reduced receptive field size, our dense DCNN operates at 8 fps, while Mean Field Inference for the fully-connected CRF requires 0.5 second, (ii) accuracy: we obtain state-of-the-art results on the PASCAL semantic segmentation challenge, outperforming the second-best approach of Mostajabi et al. (2014) by a margin of 2% and (iii) simplicity: our system is composed of a cascade of two fairly well-established modules, DCNNs and CRFs. Our system works directly on the pixel representation, similarly to Long et al. (2014). This is in contrast to the two-stage approaches that are now most common in semantic segmentation with DCNNs: such techniques typically use a cascade of bottom-up image segmentation and DCNN-based region classification, which makes the system commit to potential errors of the front-end segmentation system.",
      "startOffset" : 8,
      "endOffset" : 1117
    }, {
      "referenceID" : 1,
      "context" : "For instance, the bounding box proposals and masked regions delivered by (Arbeláez et al., 2014; Uijlings et al., 2013) are used in Girshick et al. (2014) and (Hariharan et al.",
      "startOffset" : 74,
      "endOffset" : 155
    }, {
      "referenceID" : 1,
      "context" : "For instance, the bounding box proposals and masked regions delivered by (Arbeláez et al., 2014; Uijlings et al., 2013) are used in Girshick et al. (2014) and (Hariharan et al., 2014b) as inputs to a DCNN to introduce shape information into the classification process. Similarly, the authors of Mostajabi et al. (2014) rely on a superpixel representation.",
      "startOffset" : 74,
      "endOffset" : 319
    }, {
      "referenceID" : 1,
      "context" : "For instance, the bounding box proposals and masked regions delivered by (Arbeláez et al., 2014; Uijlings et al., 2013) are used in Girshick et al. (2014) and (Hariharan et al., 2014b) as inputs to a DCNN to introduce shape information into the classification process. Similarly, the authors of Mostajabi et al. (2014) rely on a superpixel representation. A celebrated non-DCNN precursor to these works is the second order pooling method of (Carreira et al., 2012) which also assigns labels to the regions proposals delivered by (Carreira & Sminchisescu, 2012). Understanding the perils of committing to a single segmentation, the authors of Cogswell et al. (2014) build on (Yadollahpour et al.",
      "startOffset" : 74,
      "endOffset" : 665
    }, {
      "referenceID" : 27,
      "context" : "More recently, the segmentation-free techniques of (Long et al., 2014; Eigen & Fergus, 2014) directly apply DCNNs to the whole image in a sliding window fashion, replacing the last fully connected layers of a DCNN by convolutional layers.",
      "startOffset" : 51,
      "endOffset" : 92
    }, {
      "referenceID" : 9,
      "context" : "Among the first have been Farabet et al. (2013) who apply DCNNs at multiple image resolutions and then employ a segmentation tree to smooth the prediction results; more recently, Hariharan et al.",
      "startOffset" : 26,
      "endOffset" : 48
    }, {
      "referenceID" : 9,
      "context" : "Among the first have been Farabet et al. (2013) who apply DCNNs at multiple image resolutions and then employ a segmentation tree to smooth the prediction results; more recently, Hariharan et al. (2014a) propose to concatenate the computed inter-mediate feature maps within the DCNNs for pixel classification, and Dai et al.",
      "startOffset" : 26,
      "endOffset" : 204
    }, {
      "referenceID" : 6,
      "context" : "(2014a) propose to concatenate the computed inter-mediate feature maps within the DCNNs for pixel classification, and Dai et al. (2014) propose to pool the inter-mediate feature maps by region proposals.",
      "startOffset" : 118,
      "endOffset" : 136
    }, {
      "referenceID" : 6,
      "context" : "(2014a) propose to concatenate the computed inter-mediate feature maps within the DCNNs for pixel classification, and Dai et al. (2014) propose to pool the inter-mediate feature maps by region proposals. Even though these works still employ segmentation algorithms that are decoupled from the DCNN classifier’s results, we believe it is advantageous that segmentation is only used at a later stage, avoiding the commitment to premature decisions. More recently, the segmentation-free techniques of (Long et al., 2014; Eigen & Fergus, 2014) directly apply DCNNs to the whole image in a sliding window fashion, replacing the last fully connected layers of a DCNN by convolutional layers. In order to deal with the spatial localization issues outlined in the beginning of the introduction, Long et al. (2014) upsample and concatenate the scores from inter-mediate feature maps, while Eigen & Fergus (2014) refine the prediction result from coarse to fine by propagating the coarse results to another DCNN.",
      "startOffset" : 118,
      "endOffset" : 806
    }, {
      "referenceID" : 6,
      "context" : "(2014a) propose to concatenate the computed inter-mediate feature maps within the DCNNs for pixel classification, and Dai et al. (2014) propose to pool the inter-mediate feature maps by region proposals. Even though these works still employ segmentation algorithms that are decoupled from the DCNN classifier’s results, we believe it is advantageous that segmentation is only used at a later stage, avoiding the commitment to premature decisions. More recently, the segmentation-free techniques of (Long et al., 2014; Eigen & Fergus, 2014) directly apply DCNNs to the whole image in a sliding window fashion, replacing the last fully connected layers of a DCNN by convolutional layers. In order to deal with the spatial localization issues outlined in the beginning of the introduction, Long et al. (2014) upsample and concatenate the scores from inter-mediate feature maps, while Eigen & Fergus (2014) refine the prediction result from coarse to fine by propagating the coarse results to another DCNN.",
      "startOffset" : 118,
      "endOffset" : 903
    }, {
      "referenceID" : 6,
      "context" : "Focusing on the closest works in this direction, Cogswell et al. (2014) use CRFs as a proposal mechanism for a DCNN-based reranking system, while Farabet et al.",
      "startOffset" : 49,
      "endOffset" : 72
    }, {
      "referenceID" : 6,
      "context" : "Focusing on the closest works in this direction, Cogswell et al. (2014) use CRFs as a proposal mechanism for a DCNN-based reranking system, while Farabet et al. (2013) treat superpixels as nodes for a local pairwise CRF and use graph-cuts for discrete inference; as such their results can be limited by errors in superpixel computations, while ignoring long-range superpixel dependencies.",
      "startOffset" : 49,
      "endOffset" : 168
    }, {
      "referenceID" : 29,
      "context" : "This approach is known as the ‘hole algorithm’ (‘atrous algorithm’) and has been developed before for efficient computation of the undecimated wavelet transform (Mallat, 1999).",
      "startOffset" : 161,
      "endOffset" : 175
    }, {
      "referenceID" : 19,
      "context" : "We have implemented this within the Caffe framework (Jia et al., 2014) by adding to the im2col function (it converts multi-channel feature maps to vectorized patches) the option to sparsely sample the underlying feature map.",
      "startOffset" : 52,
      "endOffset" : 70
    }, {
      "referenceID" : 13,
      "context" : "To compute scores more densely at our target stride of 8 pixels, we develop a variation of the method previously employed by Giusti et al. (2013); Sermanet et al.",
      "startOffset" : 125,
      "endOffset" : 146
    }, {
      "referenceID" : 13,
      "context" : "To compute scores more densely at our target stride of 8 pixels, we develop a variation of the method previously employed by Giusti et al. (2013); Sermanet et al. (2013). We skip subsampling after the last two max-pooling layers in the network of Simonyan & Zisserman (2014) and modify the convolutional filters in the layers that follow them by introducing zeros to increase their length (2× in the last three convolutional layers and 4× in the first fully connected layer).",
      "startOffset" : 125,
      "endOffset" : 170
    }, {
      "referenceID" : 13,
      "context" : "To compute scores more densely at our target stride of 8 pixels, we develop a variation of the method previously employed by Giusti et al. (2013); Sermanet et al. (2013). We skip subsampling after the last two max-pooling layers in the network of Simonyan & Zisserman (2014) and modify the convolutional filters in the layers that follow them by introducing zeros to increase their length (2× in the last three convolutional layers and 4× in the first fully connected layer).",
      "startOffset" : 125,
      "endOffset" : 275
    }, {
      "referenceID" : 13,
      "context" : "To compute scores more densely at our target stride of 8 pixels, we develop a variation of the method previously employed by Giusti et al. (2013); Sermanet et al. (2013). We skip subsampling after the last two max-pooling layers in the network of Simonyan & Zisserman (2014) and modify the convolutional filters in the layers that follow them by introducing zeros to increase their length (2× in the last three convolutional layers and 4× in the first fully connected layer). We can implement this more efficiently by keeping the filters intact and instead sparsely sample the feature maps on which they are applied on using a stride of 2 or 4 pixels, respectively. This approach is known as the ‘hole algorithm’ (‘atrous algorithm’) and has been developed before for efficient computation of the undecimated wavelet transform (Mallat, 1999). We have implemented this within the Caffe framework (Jia et al., 2014) by adding to the im2col function (it converts multi-channel feature maps to vectorized patches) the option to sparsely sample the underlying feature map. This approach is generally applicable and allows us to efficiently compute dense CNN feature maps at any target subsampling rate without introducing any approximations. We finetune the model weights of the Imagenet-pretrained VGG-16 network to adapt it to the image classification task in a straightforward fashion, following the procedure of Long et al. (2014). We replace the 1000-way Imagenet classifier in the last layer of VGG-16 with a 21-way one.",
      "startOffset" : 125,
      "endOffset" : 1430
    }, {
      "referenceID" : 23,
      "context" : "We optimize the objective function with respect to the weights at all network layers by the standard SGD procedure of Krizhevsky et al. (2013). During testing, we need class score maps at the original image resolution.",
      "startOffset" : 118,
      "endOffset" : 143
    }, {
      "referenceID" : 23,
      "context" : "We optimize the objective function with respect to the weights at all network layers by the standard SGD procedure of Krizhevsky et al. (2013). During testing, we need class score maps at the original image resolution. As illustrated in Figure 1 and further elaborated in Section 3.1, the class score maps (corresponding to log-probabilities) are quite smooth, which allows us to use simple bilinear interpolation to increase their resolution by a factor of 8 at a negligible computational cost. Note that the method of Long et al. (2014) does not use the hole algorithm and produces very coarse scores (subsampled by a factor of 32) at the CNN output.",
      "startOffset" : 118,
      "endOffset" : 539
    }, {
      "referenceID" : 23,
      "context" : "Using smaller networks such as Krizhevsky et al. (2013) could allow video-rate test-time dense feature computation even on light-weight GPUs.",
      "startOffset" : 31,
      "endOffset" : 56
    }, {
      "referenceID" : 27,
      "context" : "The first approach is to harness information from multiple layers in the convolutional network in order to better estimate the object boundaries (Long et al., 2014; Eigen & Fergus, 2014).",
      "startOffset" : 145,
      "endOffset" : 186
    }, {
      "referenceID" : 27,
      "context" : "The first approach is to harness information from multiple layers in the convolutional network in order to better estimate the object boundaries (Long et al., 2014; Eigen & Fergus, 2014). The second approach is to employ a super-pixel representation, essentially delegating the localization task to a low-level segmentation method. This route is followed by the very successful recent method of Mostajabi et al. (2014). In Section 3.",
      "startOffset" : 146,
      "endOffset" : 419
    }, {
      "referenceID" : 32,
      "context" : "Traditionally, conditional random fields (CRFs) have been employed to smooth noisy segmentation maps (Rother et al., 2004; Kohli et al., 2009).",
      "startOffset" : 101,
      "endOffset" : 142
    }, {
      "referenceID" : 20,
      "context" : "Traditionally, conditional random fields (CRFs) have been employed to smooth noisy segmentation maps (Rother et al., 2004; Kohli et al., 2009).",
      "startOffset" : 101,
      "endOffset" : 142
    }, {
      "referenceID" : 32,
      "context" : "Using contrast-sensitive potentials (Rother et al., 2004) in conjunction to local-range CRFs can potentially improve localization but still miss thin-structures and typically requires solving an expensive discrete optimization problem.",
      "startOffset" : 36,
      "endOffset" : 57
    }, {
      "referenceID" : 20,
      "context" : ", 2004; Kohli et al., 2009). Typically these models contain energy terms that couple neighboring nodes, favoring same-label assignments to spatially proximal pixels. Qualitatively, the primary function of these short-range CRFs has been to clean up the spurious predictions of weak classifiers built on top of local hand-engineered features. Compared to these weaker classifiers, modern DCNN architectures such as the one we use in this work produce score maps and semantic label predictions which are qualitatively different. As illustrated in Figure 1, the score maps are typically quite smooth and produce homogeneous classification results. In this regime, using short-range CRFs can be detrimental, as our goal should be to recover detailed local structure rather than further smooth it. Using contrast-sensitive potentials (Rother et al., 2004) in conjunction to local-range CRFs can potentially improve localization but still miss thin-structures and typically requires solving an expensive discrete optimization problem. To overcome these limitations of short-range CRFs, we integrate into our system the fully connected CRF model of Krähenbühl & Koltun (2011). The model employs the energy function",
      "startOffset" : 8,
      "endOffset" : 1169
    }, {
      "referenceID" : 0,
      "context" : "High-dimensional filtering algorihtms (Adams et al., 2010) significantly speed-up this computation resulting in an algorithm that is very fast in practice, less that 0.",
      "startOffset" : 38,
      "endOffset" : 58
    }, {
      "referenceID" : 10,
      "context" : "Dataset We test our DeepLab model on the PASCAL VOC 2012 segmentation benchmark (Everingham et al., 2014), consisting of 20 foreground object classes and one background class.",
      "startOffset" : 80,
      "endOffset" : 105
    }, {
      "referenceID" : 10,
      "context" : "Dataset We test our DeepLab model on the PASCAL VOC 2012 segmentation benchmark (Everingham et al., 2014), consisting of 20 foreground object classes and one background class. The original dataset contains 1, 464, 1, 449, and 1, 456 images for training, validation, and testing, respectively. The dataset is augmented by the extra annotations provided by Hariharan et al. (2011), resulting in 10, 582 training images.",
      "startOffset" : 81,
      "endOffset" : 379
    }, {
      "referenceID" : 34,
      "context" : "6% result of TextonBoost (Shotton et al., 2009) to 29.",
      "startOffset" : 25,
      "endOffset" : 47
    }, {
      "referenceID" : 20,
      "context" : "Mean Pixel IOU along Object Boundaries To quantify the accuracy of the proposed model near object boundaries, we evaluate the segmentation accuracy with an experiment similar to Kohli et al. (2009); Krähenbühl & Koltun (2011).",
      "startOffset" : 178,
      "endOffset" : 198
    }, {
      "referenceID" : 20,
      "context" : "Mean Pixel IOU along Object Boundaries To quantify the accuracy of the proposed model near object boundaries, we evaluate the segmentation accuracy with an experiment similar to Kohli et al. (2009); Krähenbühl & Koltun (2011). Specifically, we use the ‘void’ label annotated in val set, which usually occurs around object boundaries.",
      "startOffset" : 178,
      "endOffset" : 226
    }, {
      "referenceID" : 27,
      "context" : "3, we qualitatively compare our proposed model, DeepLabCRF, with two state-of-art models: FCN-8s (Long et al., 2014) and TTI-Zoomout-16 (Mostajabi et al.",
      "startOffset" : 97,
      "endOffset" : 116
    }, {
      "referenceID" : 30,
      "context" : ", 2014) and TTI-Zoomout-16 (Mostajabi et al., 2014) on the ‘val’ set (the results are extracted from their papers).",
      "startOffset" : 27,
      "endOffset" : 51
    }, {
      "referenceID" : 30,
      "context" : "4% mean IOU1, outperforming all the other state-of-the-art models (specifically, TTI-Zoomout-16 (Mostajabi et al., 2014), FCN-8s (Long et al.",
      "startOffset" : 96,
      "endOffset" : 120
    }, {
      "referenceID" : 27,
      "context" : ", 2014), FCN-8s (Long et al., 2014), and MSRACFM (Dai et al.",
      "startOffset" : 16,
      "endOffset" : 35
    }, {
      "referenceID" : 7,
      "context" : ", 2014), and MSRACFM (Dai et al., 2014)).",
      "startOffset" : 21,
      "endOffset" : 39
    } ],
    "year" : 2014,
    "abstractText" : "Deep Convolutional Neural Networks (DCNNs) have recently shown state of the art performance in high level vision tasks, such as image classification and object detection. This work brings together methods from DCNNs and probabilistic graphical models for addressing the task of pixel-level classification (also called ”semantic image segmentation”). We show that responses at the final layer of DCNNs are not sufficiently localized for accurate object segmentation. This is due to the very invariance properties that make DCNNs good for high level tasks. We overcome this poor localization property of deep networks by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF). Qualitatively, our “DeepLab” system is able to localize segment boundaries at a level of accuracy which is beyond previous methods. Quantitatively, our method sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 66.4% IOU accuracy in the test set. We show how these results can be obtained efficiently: Careful network re-purposing and a novel application of the ’hole’ algorithm from the wavelet community allow dense computation of neural net responses at 8 frames per second on a modern GPU.",
    "creator" : "LaTeX with hyperref package"
  }
}