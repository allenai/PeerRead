{
  "name" : "1704.07187.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Reinforcement Learning Based Dynamic Selection of Auxiliary Objectives with Preserving of the Best Found Solution",
    "authors" : [ "Irina Petrova", "Arina Buzdalova" ],
    "emails" : [ "irenepetrova@yandex.com,", "abuzdalova@gmail.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Reinforcement Learning Based Dynamic Selection of Auxiliary Objectives with Preserving of the Best Found Solution\nIrina Petrova, Arina Buzdalova ITMO University\n49 Kronverkskiy av. Saint-Petersburg, Russia, 197101\nEmail: irenepetrova@yandex.com, abuzdalova@gmail.com\nAbstract—Efficiency of single-objective optimization can be improved by introducing some auxiliary objectives. Ideally, auxiliary objectives should be helpful. However, in practice, objectives may be efficient on some optimization stages but obstructive on others. In this paper we propose a modification of the EA+RL method which dynamically selects optimized objectives using reinforcement learning. The proposed modification prevents from losing the best found solution.\nWe analysed the proposed modification and compared it with the EA+RL method and Random Local Search on XdivK, Generalized OneMax and LeadingOnes problems. The proposed modification outperforms the EA+RL method on all problem instances. It also outperforms the single objective approach on the most problem instances. We also provide detailed analysis of how different components of the considered algorithms influence efficiency of optimization. In addition, we present theoretical analysis of the proposed modification on the XdivK problem.\nI. INTRODUCTION Consider single-objective optimization of a target objective by an evolutionary algorithm (EA). Commonly, efficiency of EA is measured in one of two ways. In the first one efficiency is defined as the number of fitness function evaluations needed to reach the optimum. In the second one efficiency of EA is computed as the maximum target objective value obtained within the fixed number of evaluations. In this work we use the first way.\nEfficiency of the target objective optimization can be increased by introducing some auxiliary objectives [1]–[5]. Ideally, auxiliary objectives should be helpful [2]. However, in practice, objectives can be generated automatically and may be efficient on some optimization stages but obstructive on others [6], [7]. We call such objectives non-stationary. One of the approaches to deal with such objectives is dynamic selection of the best objective at the current stage of optimization. The objectives may be selected randomly [8]. The better method is EA+RL which uses reinforcement learning (RL) [9], [10].\nIt was theoretically shown for a number of optimization problems that EA+RL efficiently works with stationary objectives [11], [12]. However, theoretical analysis of EA+RL with non-stationary objectives showed that EA+RL does not ignore obstructive objective on the XdivK problem [13]. Selection of an inefficient auxiliary objective causes losing of the best\nfound solution and the algorithm needs a lot of steps to find a good solution again. Also EA+RL can stuck in local optima while solving the Generalized OneMax problem with obstructive objectives [14].\nIn this paper we propose a modified version of EA+RL and analyse it theoretically and experimentally on XdivK, Generalized OneMax and LeadingOnes problems. The rest of the paper is organized as follows. First, the EA+RL and model problems with non-stationary objectives are described. Second, a modification of the EA+RL method is proposed. Then we experimentally analyse the considered methods. Finally, we provide discussion and theoretical explanation of the achieved results."
    }, {
      "heading" : "II. PRELIMINARIES",
      "text" : "In this section we describe the EA+RL method. Also we define model problems and non-stationary objectives used in this study."
    }, {
      "heading" : "A. EA+RL method",
      "text" : "In reinforcement learning (RL) an agent applies an action to an environment. Then the environment returns a numerical reward and a representation of its state and the process repeats. The goal of the agent is to maximize the total reward [9].\nIn the EA+RL method, EA is treated as an environment, selection of an objective to be optimized corresponds to an action. The agent selects an objective, EA generates new population using this objective and returns some reward to the agent. The reward depends on difference of the best target objective value in two subsequent generations. We consider maximization problems. So the higher is the newly obtained target value, the higher is the reward.\nIn recent theoretical analysis of EA+RL with non-stationary objectives, random local search (RLS) is used instead of EA, population consists of a single individual [13]. Individuals are represented as bit strings, flip-one-bit mutation is used. If values of the selected objective computed on the new individual and on the current one are equal, the new individual is accepted. The used reinforcement learning algorithm is Qlearning. Therefore, the EA+RL algorithm in this case is called\nar X\niv :1\n70 4.\n07 18\n7v 1\n[ cs\n.N E\n] 2\n4 A\npr 2\n01 7\nRLS + Q-learning. The pseudocode of RLS + Q-learning is presented in Algorithm 1. The current state s is defined as the target objective value of the current individual. The reward is calculated as difference of the target objective values in two subsequent generations. In Q-learning, the efficiency of selecting an objective h in a state s is measured by the value Q(s, a), which is updated dynamically after each selection as shown in line 13 of the pseudocode, where α and γ are the learning rate and the discount factor correspondingly.\nAlgorithm 1 RLS + Q-learning Algorithm 1: Individual y ← a random bit string 2: Construct set H of auxiliary objectives and target objective 3: Q(s, h) ← 0 for each state s and objective h ∈ H 4: while (Optimum of the target objective t is not found) do 5: Current state s ← t(y) 6: Individual y′ ← mutate y (flip a random bit) 7: Objective h: Q(s, h) = maxh′∈H Q(s, h′) . If\nQ-values are equal, objectives are selected equiprobably 8: if h(y′) ≥ h(y) then 9: y ← y′\n10: end if 11: New state s′ ← t(y) 12: Reward r ← s′ − s 13: Q(s, h) ← Q(s, h) +α(r+ max\nh′∈H Q(s′, h′)−Q(s, h))\n14: end while"
    }, {
      "heading" : "B. Model problems",
      "text" : "In this paper we consider three model problems which were used in studies of EA+RL [12]–[14]. In all considered problems, an individual is a bit string of length n. Let x be the number of bits in an individual which are set to one. Then the objective ONEMAX is equal to x and the objective ZEROMAX is equal to n− x.\nOne of the considered problems is GENERALIZED ONEMAX, denoted as OMd. The target objective of this problem called OMd is calculated as the number of bits in an individual of length n that matches a given bit mask. The bit mask has d 0-bits and n− d 1-bits.\nAnother problem is XDIVK. The target objective is calculated as bxk c, where x is the number of ones, k is a constant, k < n and k divides n.\nThe last considered problem is LEADINGONES. The target objective of this problem is equal to the length of the maximal prefix consisting of bits set to one."
    }, {
      "heading" : "C. Non-stationary objectives",
      "text" : "We used the two following non-stationary auxiliary objectives for all the considered problems. These auxiliary objectives can be both ONEMAX or ZEROMAX at different stages of optimization. More precisely, consider the auxiliary objectives h1 and h2 defined in (1). The parameter p is called a switch point, because at this point auxiliary objectives change\ntheir properties. ONEMAX and ZEROMAX are denoted as OM and ZM correspondingly.\nh1(x) = { OM, x ≤ p ZM, p < x ≤ n h2(x) = { ZM, x ≤ p OM, p < x ≤ n (1)\nIn the XDIVK problem, an objective which is currently equal to ONEMAX allows to distinguish individuals with the same value of the target objective and give preference to the individual with a higher x value. Such an individual is more likely to produce a descendant with a higher target objective value. In the LEADINGONES problem, ONEMAX is helpful because it has the same optimum but running time of optimizing ONEMAX is lower [15]. Therefore, in LEADINGONES and XDIVK problems the objective which is equal to ONEMAX at the current stage of optimization is helpful. In both these problems, the goal is to obtain individual of all ones, so ZEROMAX is obstructive.\nIn the OMd problem, both objectives may be obstructive or neutral. For example, if for some i the i-th bit of the OMd bit mask is set to 1, and the mutation operator flips the i-th bit of an individual from 0 to 1, the ZEROMAX objective is obstructive, because it would not accept this individual. In the inverse case, ONEMAX is obstructive."
    }, {
      "heading" : "III. MODIFIED EA+RL",
      "text" : "In this section we propose a modification of the EA+RL method which prevents EA+RL from losing the best found solution. In the EA+RL method, if the newly generated individual is better than the existing one according to the selected objective, the new individual is accepted. However, if the selected objective is obstructive, the new individual may be worse than the existing individual in terms of the target objective. In this case EA loses the individual with the best target objective value.\nIn the modified EA+RL, if the newly generated individual is better than the existing one according to the selected objective, but is worse according to the target objective, the new individual is rejected. As in the recent theoretical works, we use RLS as optimization problem solver and apply Qlearning to select objectives. The pseudocode of the modified RLS + Q-learning is presented in Algorithm 2.\nTo motivate the approach of reward calculation in the new method, we need to describe how the agent learns which objective should be selected in the EA+RL method. If the agent selects an obstructive objective and EA loses individual with the best target value, the best target value in the new generation is decreased. So the agent achieves a negative reward for this objective and will not select this objective in the same state later. However, when properties of auxiliary objectives are changed, the obstructive objective may become helpful. If the properties are changed within one RL state, the objective which became helpful will not be selected, because the agent previously achieved negative reward for it. And inversely, the objective which became obstructive could be selected because it was helpful earlier. And, as it was shown in [13], the EA+RL needs a lot of steps to get out of this\nAlgorithm 2 Modified RLS + Q-learning Algorithm 1: Individual y ← a random bit string 2: Construct set H of auxiliary objectives and target objective 3: Q(s, h) ← 0 for each state s and objective h ∈ H 4: while (Optimum of the target objective t is not found) do 5: Calculate current state s 6: Save target fitness: f ← t(y) 7: Individual y′ ← mutate y (flip a random bit) 8: Objective h: Q(s, h) = maxh′∈H Q(s, h′) . If\nQ-values are equal, objectives are selected equiprobably 9: if h(y′) ≥ h(y) and t(y′) ≥ t(y) then\n10: y ← y′ 11: end if 12: Calculate new state s′ 13: Calculate reward r 14: Q(s, h) ← Q(s, h) + α(r+ max\nh′∈H Q(s′, h′)−Q(s, h))\n15: end while\ntrap. For this reason, in the new method we consider two versions of the modified EA+RL with different ways of reward calculation.\nIn the first version of the modified EA+RL, if the new individual is better than the current one according to the selected objective, and its target objective value is lower, the new individual is rejected but the agent achieves negative reward as if the new individual was accepted. So the agent learns as in EA+RL. We call this algorithm the modification of EA+RL with learning on mistakes. In this case in line 13 of Algorithm 2 reward is calculated as presented in Algorithm 3. In the second version the agent in the same situation achieves zero reward because the individual in the population is not changed. Thereby, agent does not learn if the action was inefficient and learns only if the target objective value was increased. We call this algorithm the modification of EA+RL without learning on mistakes. In this case in line 13 of Algorithm 2 reward is calculated as t(y)− f .\nAlgorithm 3 Reward calculation in the modified EA+RL with learning on mistakes (Algorithm 2, line 13)\n1: r ← t(y)− f 2: if h(y′) ≥ h(y) then 3: r ← t(y′)− f 4: end if\nIn the existing theoretical works on EA+RL, RL state is defined as the target objective value [11], [14]. Denote it as target state. However, if the individual with the best target objective value is preserved, the algorithm will never return to the state where it achieved positive reward. So the agent never knows which objective is helpful. It only can learn that an objective is obstructive if the agent achieved negative reward for it. Therefore, in the present work we consider two definitions of a state. The first one is the target state. The second one is the single state which is the same in the\nwhole optimization process. This state is used to investigate efficiency of the proposed method when the agent has learned which objective is good."
    }, {
      "heading" : "IV. THEORETICAL ANALYSIS",
      "text" : "Previously, it was shown that the EA+RL method gets stuck in local optima on XdivK with non-stationary objectives [13]. Below we present theoretical analysis of the running time of the proposed EA+RL modification without learning on mistakes on this problem. The target state is used. To compute the expected running time of the algorithm, we construct the Markov chain that represents the corresponding optimization process [11], [13]. Recall that RL states are determined by the target objective value. Markov states correspond to the number of 1-bits in an individual. Therefore, an RL state includes k Markov states with different number of 1-bits.\nThe Markov chain for the XDIVK problem is shown in the Fig. 1. The labels on transitions have the format 〈F, M〉, where F is a fitness function that can be chosen for this transition, M is the corresponding effect of mutation. A transition probability is computed as the sum for all f ∈ F of product of probabilities of selection f and the corresponding mutation m.\nFurther we describe each Markov state and explain why the transitions have such labels.\ndk+1\ndk\ndk+2\nh1, 0 1 h2, 1 0\nt, 1 0 h1, 1 0\nh1, 1 0 h2, 0 1 h2, 1 0 t, 1 0\ndk+k\n...\n...\nRL state (d-1)\nRL state d\nRL state (d+1)\nh1 = ZeroMax h2 = OneMax\nh1 = OneMax h2 = ZeroMax\np = dk\ndk-1\nt, 0 1 h2, 0 1\nt, 0 1 h1, 0 1\nFig. 1. Markov chain for the EA+RL modification without learning on mistakes on XDIVK\nConsider the number of ones equal to dk, where d is a constant. So the agent is in the RL state d and the Markov\nstate is dk. Since the agent has no experience in the state d, the objectives are selected equiprobably. If the agent selects the target objective or an objective which is currently equal to ONEMAX, and the mutation operator inverts 1-bit, the new individual has dk − 1 1-bits and is worse than the current individual according to the selected objective. So the new individual will not be accepted by EA. The same situation occurs, if the selected objective is equal to ZEROMAX and the mutation operator inverts 0-bit. In the case of selection of the ZEROMAX objective and inversion of 1-bit, the new individual is better than the current one according to the selected objective. However, the target objective value of the new individual is less than the target objective value of the current individual. Therefore, the new individual is not selected for the next generation. If ONEMAX or the target objective is selected and 0-bit is inverted, the new individual is accepted.\nThe transitions in Markov states dk and dk+ 1 differ from each other when 1-bit is inverted and the agent selects the target objective or the objective which is equal to ZEROMAX. In this case, the new individual is equal to or better than the current one according to the selected objective. So the new individual is accepted. However, the new individual contains less 1-bits than the current one, so the algorithm moves to the dk Markov state. Transitions in the states dk+2, . . . , dk+k−1 are the same as transitions in the state dk+1. From the Markov chain of the algorithm, we can see, that transitions and, as a consequence, performance of the algorithm do not depend on the number and positions of switch points.\nTo analyse the running time of the considered algorithm, we also need to construct Markov chain for RLS without auxiliary objectives (see Fig. 2). This Markov chain is constructed analogically to the Markov chain described above.\nThe expected running time of the EA+RL modification without learning on mistakes for XDIVK with non-stationary objectives is equal to the number of fitness function evaluations needed to get from the Markov state 0 to the Markov state n. Each transition in the Markov chain corresponds to one fitness function evaluation of the mutated individual. So the expected running time is equal to the number of transitions in the Markov chain. Denote the expected running time of the algorithm as T (n):\nT (n) = n−1∑ i=0 E(i→ i+ 1), (2)\nwhere E(i → i + 1) is the expected number of transitions needed to reach the Markov state i+ 1 from the state i.\nConsider two cases for the state i. The first is i = dk, where d is a constant. The expected number of transitions needed to reach the state dk + 1 from the state dk is evaluated as zdk = E(dk → dk + 1):\nzdk = 2 3 · (n− dk) n · 1 + (2 3 · dk n + 1 3 ) · (1 + zdk) (3)\nFrom (3) we obtain that zdk is evaluated as:\nzdk = 3n\n2(n− dk) (4)\nThe second case is i = dk + t, where 1 ≤ t ≤ k − 1. The expected number of transitions needed to reach the state dk + t + 1 from the state dk + t is evaluated as zdk+t = E(dk + t→ dk + t+ 1):\nzdk+t = 2(n− dk − t)\n3n +\n2(dk + t)\n3n · (1 + zdk+t−1 + zdk+t)+\n+( dk + t 3n + n− dk − t 3n ) · (1 + zdk+t) (5)\nFrom (5) we obtain that zdk+t is evaluated as:\nzdk+t = zdk+t−1 · dk + t\nn− dk − t +\n3n\n2(n− dk − t) (6)\nTo estimate the efficiency of the analysed EA+RL modification, let us calculate the expected running time of RLS without auxiliary objectives. The evaluation approach is similar to the one presented above for the EA+RL modification. The total running time is calculated by (2). Analogically, we consider two cases: i = dk and i = dk + t. The expected number of transitions needed to reach the state dk+ 1 from the state dk is evaluated as adk = E(dk → dk + 1):\nadk = (n− dk) n · 1 + dk n · (1 + adk) (7)\nFrom (7) we obtain that adk is evaluated as:\nadk = n\n(n− dk) (8)\nThe expected value of transitions needed to reach the state dk + t + 1 from the state dk + t is evaluated as adk+t = E(dk + t→ dk + t+ 1):\nadk+t = (n− dk − t) n ·1+ dk + t n ·(1+adk+t−1+adk+t) (9)\nFrom (9) we obtain that adk+t is evaluated as:\nadk+t = adk+t−1 · dk + t\nn− dk − t +\nn\n(n− dk − t) (10)\nFrom (4) and (8) we obtain that zdk = 32adk. From the equations (2), (6), (10) using mathematical induction we obtain that the running time of EA+RL modification without learning on mistakes for XDIVK with non-stationary objectives is 1.5 times greater than the running time of RLS without auxiliary objective. Therefore, the EA+RL modification without learning on mistakes has asymptotically the same running time as RLS, which is bounded by Ω(nk) and O(nk+1) [11]. This shows that the EA+RL modification without learning on mistakes can deal with non-stationary auxiliary objectives unlike EA+RL does."
    }, {
      "heading" : "V. EXPERIMENTAL ANALYSIS OF MODIFIED EA+RL",
      "text" : "In this section we experimentally evaluate efficiency of the both proposed EA+RL modifications on the optimization problems defined in Section II-B."
    }, {
      "heading" : "A. Description of experiments",
      "text" : "We analysed the two versions of the proposed modification of EA+RL and the EA+RL method on OMd, XDIVK and LEADINGONES. The non-stationary objectives described in (1) were used for all the problems. For XDIVK we analysed two cases of the switch point position. The first case is the worst case [13], when the switch point is in the end of optimization, p = n−k+1. In the second case, the switch point is in the middle of optimization, p = n/2. For each algorithm we analysed two state definitions: the single state and the target state. Also we studied applying of ε-greedy strategy. In this strategy the agent selects the objective with the maximum expected reward with probability 1 − ε and with probability ε the agent selects a random objective. This strategy gives the agent an opportunity to select the objective which was inefficient but became efficient after the switch point.\nThe obtained numbers of fitness function evaluations needed to reach the optimum were averaged by 1000 runs. We used Q-learning algorithm with the same parameters as in [13]. The learning rate was set to α = 0.5 and the discount factor was equal to γ = 0.5. The ε-greedy strategy was used with ε = 0.1."
    }, {
      "heading" : "B. Discussion of experiment results",
      "text" : "Table I presents the results of the experiments. The first column contains parameter values for the considered problems. The next column contains results of RLS without auxiliary objectives. The next three columns correspond to results of EA+RL modification with learning on mistakes (modified EA+RL, learning). The following three columns correspond to\nthe results of EA+RL modification without learning on mistakes (modified EA+RL, no learning). The last three columns correspond to the results of the EA+RL method (EA+RL). Each algorithm was analysed on the single state (ss, ε = 0 and ss, ε = 0.1) and the target state (ts, ε = 0 and ts, ε = 0.1). None of the algorithms reached the optimum using the single state and ε = 0. So these results are not presented. When the optimum was not reached within 109 iterations the corresponding result is marked as ”inf”.\nWe can see from Table I that the modification of EA+RL with learning on mistakes using the single state and ε = 0.1 is the most efficient algorithm on LEADINGONES, OMd and XDIVK with switch point in the middle. On LeadingOnes and XdivK with switch point in the middle this algorithm ignores an inefficient objective and selects efficient one, so the achieved results are better than the results of RLS without objectives. On OMd this modification ignores obstructive objectives and achieves the same results as RLS. On XdivK with switch point in the end, the best results are achieved using the modification of EA+RL without learning on mistakes. For each problem, we picked the best configuration of each algorithm and compared them by Mann-Whitney test with Bonferroni correction. The algorithms were statistically distinguishable with p-value less than 0.05. Below we analyse how different components of the considered methods influence optimization performance. More precisely, we consider influence of the best individual preservation, learning which objective is obstructive, state definition and ε-greedy strategy.\n1) Influence of learning which objective is obstructive: We can see from the results that the modification of EA+RL without learning is outperformed by the version with learning on LEADINGONES and XDIVK with switch point in the middle. Therefore, learning on mistakes is useful because it allows the agent to remember that the objective is obstructive and not to select it further. However, on XdivK problem with switch point in the end, the best results are achieved using modification of EA+RL without learning on mistakes. Below we explain why learning on mistakes is not always efficient.\nIf some objective becomes helpful, the agent will not select this objective in the same state because it obtained a negative reward for it. So there are two ways for the agent to learn that the obstructive objective became helpful. The first way is to select this objective with ε-probability and achieve a positive reward. The second way is to move to the new state, where the agent has not learned which objective is efficient. It is impossible when using the single state. Consider what actions agent should do to move to the new state if the target state is used. The only way to move to the new state is increasing of the target objective value. The agent could select the target objective or the objective which was efficient but became obstructive. So target objective value can be increased only if the new individual has a higher target objective value and the selected objective is the target one.\nIn some optimization problems it is not always possible to increase the target objective in one iteration of the algorithm. For example, consider XDIVK problem. Let the number of 1-\nbits be dk, so the RL state is d. To move to the state d+1, the algorithm needs to mutate k 0-bits. Let switch point p be equal to dk + l, where 0 < l < k. Then if the number of 1-bits is greater than dk+l, the algorithm can increase the number of 1- bits only if 0-bit is mutated and the target objective is selected. However, whatever bit is mutated and whatever objective is selected, the target objective value stays unchanged until an individual with dk + k 1-bits is obtained. So the agent does not recognize if its action is good or bad because the reward is equal to zero. Therefore, to increase the target objective value algorithm needs a lot of steps. In addition, if the switch point is in the end of optimization, probability to mutate a 0-bit and select the target objective at the same time is low. The worst case is when the switch point is equal to n− k + 1, because the agent needs to select the target objective and increase the number of 1-bits during k − 1 iterations. The modification\nof EA+RL without learning on mistakes does not have this drawback, because as it was shown in theoretical analysis, it does not depend on the number of switch points and their positions.\nTo conclude, the modification of EA+RL with learning performs better than the proposed modification without learning, except the case when the switch point occurs at the time when it is hard to generate good solutions (stagnation).\n2) Influence of best individual preservation: Thanks to preserving of individual with the best target value, the proposed modification of EA+RL achieves the optimum on LEADINGONES and OMd problems unlike the EA+RL method does. However, we can see that on XdivK with switch point in the middle EA+RL achieves better results despite the possibility to lose the best individual. It is explained by the fact that EA+RL learns which objective is obstructive and for this problem it is more important than preserving of the best individual. Inverse situation is observed for XdivK with switch point in the end where learning does not help to achieve the optimum faster even in case of preserving of the best individual (see section V-B1).\nTo conclude, preservation of the best individual improves the EA+RL method. However, learning which objective is efficient has a greater impact on the algorithm performance.\n3) Influence of state definition: To begin with, let us separately consider RL efficiency when using the target state and the single state. Then we will discuss how state definition influence the performance of two proposed modifications of EA+RL.\nConsider the target state. The algorithm moves to the new state if the target objective value is increased and, as follows, a positive reward is obtained. If the target objective value can not be decreased, the algorithm never returns to the state where it achieved a positive reward. Therefore, the agent never knows which action is efficient if the target state is used.\nThe single state allows to learn which objective is helpful contrary to the target state. However, when the objective which was efficient becomes obstructive, the agent continues selection of this objective. Re-learning that the objective which was helpful became obstructive when using the single state is more difficult than if the target state is used (see Section V-B1). None of the algorithms using the single state with ε = 0 reached the optimum. So without ε-greedy strategy the agent could not re-learn. Also EA+RL without preservation of the best obtained individual does not achieve the optimum using the single state.\nConsider influence of state definition in the EA+RL modification with learning. When solving LEADINGONES, the positive effect of ability to learn which objective is efficient is more important than the negative effect of difficult re-learning. We can see different influences of these negative and positive effects when solving the XdivK problem with switch point in the middle. When k becomes bigger, it is more important to select an efficient objective during k iterations where agent does not achieve reward and could not define which objective is better. So we can see that for k = 3 the single state is much\nbetter than the target state. On XdivK problem with switch point in the end, the results obtained using the single state and the target state with ε = 0.1 are the same. These results are better than the results obtained using the target state with ε = 0. Therefore, the impact of ε is greater than the impact of state definition.\nIn the EA+RL modification without learning the agent does not achieve negative reward so re-learning when using the single state is harder. Re-learning can occur only if a positive reward is obtained when a better solution is generated. In XdivK, many iterations are needed to increase the target objective value (see Section V-B1). Therefore, the results on XdivK for the target state are better than the results for the single state. In LEADINGONES, generating of an individual with a higher target objective value is not so difficult. Therefore, using the single state the algorithm obtains better results than using the target state.\nTo conclude, in the modification of EA+RL with learning on mistakes the single state is better in the most cases. In the modification of EA+RL without learning the single state is worse than the target only on XdivK where re-learning is difficult. Also we can note that EA+RL does not achieve the optimum using the single state.\n4) Influence of ε-greedy strategy: Consider influence of ε value on performance of the considered algorithms when using the target state. In this state the agent can only learn that an objective is obstructive, if a negative reward is achieved after applying this objective. So this objective will not be selected in the same state even if it will become helpful. Non-zero ε allows to select this objective. In the modification of EA+RL without learning on mistakes the agent can obtain only nonnegative reward. So the situation described above is impossible in this modification. Therefore, ε value does not influence the efficiency of this modification. In the modification of EA+RL with learning on mistakes non-zero ε is helpful only on XdivK problem with switch point in the end. On the other problems the results obtained with non-zero ε are worse than the results obtained with ε = 0.\nWhen using the single state, ε allows to select the objective which was obstructive, as if the target state was used. However, if the single state is used, the agent learns not only that an objective is inefficient, but also that an objective is efficient. Therefore, ε also allows not to select the objective which was efficient but became obstructive. This results in reaching the optimum when using non-zero ε in the single state.\nTo conclude, if the single state is used, ε value have to be non-zero. If the target state is used, non-zero ε allows to achieve better results only if re-learning is very difficult, such as in XdivK with switch point in the end."
    }, {
      "heading" : "VI. CONCLUSION",
      "text" : "We proposed a modification of the EA+RL method which preserves the best found solution. We considered two versions of the proposed modification. In the first version, called the modification of EA+RL without learning on mistakes, the RL agent learns only when the algorithm finds a better solution.\nIn the second version, called the modification of EA+RL with learning on mistakes, the RL agent also learns when the algorithm obtains an inefficient solution.\nWe considered two auxiliary objectives which change their efficiency at switch point. We experimentally analysed the two proposed modifications and the EA+RL method on OMd, LEADINGONES, XDIVK with switch point in the middle of optimization and XDIVK with switch point in the end. Two RL state definitions were considered: the single state and the target state. Also we considered how ε-greedy exploration strategy influence the performance of the algorithm.\nThe both proposed modifications reached the optimum on OMd and LEADINGONES unlike the EA+RL method did. The modification of EA+RL with learning using the single state and ε = 0.1 achieved the best results among the considered objective selection methods on all problems, except the XDIVK problem with switch point in the end. On LEADINGONES and XDIVK with switch point in the middle this algorithm was able to select an efficient objective and obtain better results than RLS. On OMd without helpful objectives, this modification ignored obstructive objectives and achieved the same results as RLS. Therefore, keeping the best individual and using εgreedy exploration in the single state seems to be the most promising reinforcement based objective selection approach.\nWe theoretically proved that the lower and upper bounds on the running time of the modification of EA+RL without learning on mistakes on the XDIVK problem are Ω(nk) and O(nk+1) correspondingly. The asymptotic of RLS on XDIVK without auxiliary objectives is the same. This means that the modification of EA+RL without learning on mistakes on the XDIVK problem ignores the objective which is currently obstructive. Also we proved that performance of this modification is independent of the number of switch points and their positions, while performance of the modification with learning\n[2] J. D. Knowles, R. A. Watson, and D. Corne, “Reducing local optima in single-objective problems by multi-objectivization,” in Proceedings of the First International Conference on Evolutionary Multi-Criterion Optimization. Springer-Verlag, 2001, pp. 269–283. [3] F. Neumann and I. Wegener, “Can single-objective optimization profit from multiobjective optimization?” in Multiobjective Problem Solving from Nature, ser. Natural Computing Series. Springer Berlin Heidelberg, 2008, pp. 115–130. [4] D. Brockhoff, T. Friedrich, N. Hebbinghaus, C. Klein, F. Neumann, and E. Zitzler, “On the effects of adding objectives to plateau functions,” IEEE Transactions on Evolutionary Computation, vol. 13, no. 3, pp. 591–603, 2009.\ndepends on these factors. Particularly, the modification without learning achieves the best results on XdivK with switch point in the end. This is an especially difficult case because increasing of the target objective value in the end of optimization needs a lot of iterations."
    }, {
      "heading" : "VII. ACKNOWLEDGMENTS",
      "text" : "This work was supported by RFBR according to the research project No. 16-31-00380 mol a."
    } ],
    "references" : [ {
      "title" : "Runtime analysis of different approaches to select conflicting auxiliary objectives in the generalized onemax problem",
      "author" : [ "C. Segura", "C.A.C. Coello", "G. Miranda", "C. Léon", "“Using multiobjective evolutionary algorithms for single-objective optimization", "” 4OR", "vol. 3", "no. 11", "pp. 201–228", "2013. [5] J. Handl", "S.C. Lovell", "J.D. Knowles", "“Multiobjectivization by decomposition of scalar cost functions", "” in Parallel Problem Solving from Nature – PPSN X", "ser. Lecture Notes in Computer Science. Springer", "2008", "no. 5199", "pp. 31–40. [6] M. Buzdalov", "A. Buzdalova", "“Adaptive selection of helper-objectives for test case generation", "” in 2013 IEEE Congress on Evolutionary Computation", "vol. 1", "2013", "pp. 2245–2250. [7] D.F. Lochtefeld", "F.W. Ciarallo", "“Helper-objective optimization strategies for the Job-Shop scheduling problem", "” Applied Soft Computing", "vol. 11", "no. 6", "pp. 4161–4174", "2011. [8] M.T. Jensen", "“Helper-objectives: Using multi-objective evolutionary algorithms for single-objective optimisation: Evolutionary computation combinatorial optimization", "” Journal of Mathematical Modelling", "Algorithms", "vol. 3", "no. 4", "pp. 323–347", "2004. [9] R.S. Sutton", "A.G. Barto", "MA Reinforcement Learning: An Introduction. Cambridge", "USA: MIT Press", "1998. [10] A. Buzdalova", "M. Buzdalov", "“Increasing efficiency of evolutionary algorithms by choosing between auxiliary fitness functions with reinforcement learning", "” in Proceedings of the International Conference on Machine Learning", "Applications", "vol. 1", "2012", "pp. 150–155. [11] M. Buzdalov", "A. Buzdalova", "“OneMax helps optimizing XdivK: Theoretical runtime analysis for RLS", "EA+RL", "” in Proceedings of Genetic", "Evolutionary Computation Conference Companion. ACM", "2014", "pp. 201–202. [12] ——", "“Can OneMax help optimizing LeadingOnes using the EA+RL method?” in Proceedings of IEEE Congress on Evolutionary Computation", "2015", "pp. 1762–1768. [13] I. Petrova", "A. Buzdalova", "G. Korneev", "“Runtime analysis of random local search with reinforcement based selection of non-stationary auxiliary objectives: initial study", "” in Proceedings of 22nd International Conference on Soft Computing MENDEL 2016", "Czech Republic", "2016", "pp. 95–102. [14] A. Buzdalova", "I. Petrova", "M. Buzdalov" ],
      "venue" : "Proceedings of IEEE Symposium Series on Computational Intelligence, 2016, pp. 280–286. [15] A. Auger and B. Doerr, Theory of Randomized Search Heuristics: Foundations and Recent Developments. River Edge, NJ, USA: World Scientific Publishing Co., Inc., 2011.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Efficiency of the target objective optimization can be increased by introducing some auxiliary objectives [1]–[5].",
      "startOffset" : 106,
      "endOffset" : 109
    } ],
    "year" : 2017,
    "abstractText" : "Efficiency of single-objective optimization can be improved by introducing some auxiliary objectives. Ideally, auxiliary objectives should be helpful. However, in practice, objectives may be efficient on some optimization stages but obstructive on others. In this paper we propose a modification of the EA+RL method which dynamically selects optimized objectives using reinforcement learning. The proposed modification prevents from losing the best found solution. We analysed the proposed modification and compared it with the EA+RL method and Random Local Search on XdivK, Generalized OneMax and LeadingOnes problems. The proposed modification outperforms the EA+RL method on all problem instances. It also outperforms the single objective approach on the most problem instances. We also provide detailed analysis of how different components of the considered algorithms influence efficiency of optimization. In addition, we present theoretical analysis of the proposed modification on the XdivK problem.",
    "creator" : "LaTeX with hyperref package"
  }
}