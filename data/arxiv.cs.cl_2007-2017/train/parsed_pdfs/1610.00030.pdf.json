{
  "name" : "1610.00030.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Modeling Language Change in Historical Corpora: The Case of Portuguese",
    "authors" : [ "Marcos Zampieri", "Shervin Malmasi", "Mark Dras" ],
    "emails" : [ "marcos.zampieri@dfki.de,", "shervin.malmasi@mq.edu.au,", "mark.dras@mq.edu.au" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Keywords: Language Change, Temporal Text Classification, Support Vector Machines, Text Categorization"
    }, {
      "heading" : "1. Introduction",
      "text" : "It is well-known that language changes over time both in spoken and in written forms. Changes in written language can be manifested in many ways such as the use of the lexicon, grammatical structures, and textual stylistics. Recent studies have shown that it is possible to use language change to predict the approximate publication date of texts in diachronic text collections (Ciobanu et al., 2013a; Popescu and Strapparava, 2015). This task is called temporal text classification and to our knowledge it has not been substantially explored in the literature as other text classification tasks. This paper contributes in this direction. In this paper we investigate the use of supervised machine learning classifiers to predict when a text was written using lexical and morphosyntactic information. The classifiers were trained and tested on a sample of a historical Portuguese corpus called Colonia (Zampieri and Becker, 2013) which contains texts spanning from the 16th to the early 20th century. The approach we propose here is language independent and it can be applied to any diachronic corpus provided that it is annotated with POS information. This study is of interest not only to scholars working in text classification and NLP but also to linguists of different branches, particularly those interested in historical linguistics, and scholars in the digital humanities who often deal with historical manuscripts whose publication date is unknown or uncertain. This paper is organized as follows: in Section 2 we present related studies that take temporal information and language change in text collections into account (not limited to text classification). In Section 3 we describe the data, the features and the computational approach we used in our experiments; in Section 4 we present the results of three sets of experiments included in this paper. In section 5 we analyze the most informative features in the classification experiments and present a linguistic analysis of important features that indicate language change in the corpus. Finally, Section 6 presents some conclusions and avenues for future work, most notably the question of representing time intervals for temporal text classification."
    }, {
      "heading" : "2. Related Work",
      "text" : "Modeling temporal information in text is a relevant task to a number of NLP applications. Information Retrieval (IR) methods, for example, often have to process temporal information in both queries and documents to deal with dynamicity of the content found in data repositories and the Web (Dakka et al., 2012; Preotiuc-Pietro, 2014; Kanhabua et al., 2015; Zhao and Hauff, 2015b; Zhao and Hauff, 2015a). Time expressions (e.g. after 2010), can help algorithms to identify the approximate publication date of texts (Chambers, 2012), but there are a number of cases in which they are not present in text and one alternative is to use features related to language change as we propose in this paper. As will be evidenced in this section, even though there were a number of attempts to approach temporal text classification, to our knowledge this task was not substantially explored as other text classification tasks. The work by de Jong et al. (2005) uses unigram language models combined with smoothing techniques and log-likelihood ratio measure (NLLR) (Kraaij, 2004) to classify documents within different time spans. The method was tested on a collection of Dutch journalistic texts published from January 1999 to February 2005. Other methods, such as Kumar et al. (2011), make use of information gain to estimate the best features in classification. In Dalli and Wilks (2006) researchers train a classifier to predict the publication date of texts within a time span of nine years. The method uses words as features and it is aided by words which increase their frequency at some point of time, particularly named entities. Another study that works under a similar assumption is the one published by Abe and Tsumoto (2010). The authors proposed the use of similarity metrics to categorize texts based on keywords calculated using tf-idf (term frequency - inverse document frequency). Garcia-Fernandez et al. (2011) presents a method to predict the publication dates of excerpts of French journalistic texts containing between 300 and 500 tokens, published between 1801 and 1944. The corpus used was provided by the organizers of the DEFT2011 challenge (Grouin et al., 2011) which was essentially a temporal text classification ar X iv :1 61 0. 00 03 0v 1 [ cs .C L ] 3\n0 Se\np 20\n16\ntask for French following a similar DEFT2010 challenge that included both diachronic and diatopic (regional) variation (Grouin et al., 2010). Garcia-Fernandez et al. (2011) report 14% accuracy in predicting the year of publication of texts and 42% accuracy in predicting the correct decade of publication. Lexical changes are regarded to be an important feature of diachronic text collections and researchers have proposed methods to track meaning change over time (Frermann and Lapata, 2016). The study by Mihalcea and Nastase (2012) investigates how word meanings change over time in three major periods in time: 1800, 1900 and 2000. Popescu and Strapparava (2013) look at significant changes in the use of words across time for the purpose of characterizing epochs using the Google N-Gram collection from 1614 to 2009. Ciobanu et al. (2013a) and Ciobanu et al. (2013b) applied SVM and Random Forest algorithms to classify texts of a historical Romanian text collection regarding their publication date. The authors concluded that the use of lexical features is the best source of information for this task. An important issue to take into account when working on temporal text classification is how to represent time. Most studies, including our own, model the task as supervised classification in which algorithms are trained to assign texts to an n number of classes. Each of these n classes represent an arbitrarily defined time interval, for example: a month, a year, or a decade. However, there have been a few attempts to approach this task without relying on predefined time spans. The study by Niculae et al. (2014) approached the task using ranking and pairwise comparisons to predict for each pair of documents which one is older and finally to produce a rank of all documents in a collection from older to newer. Another recent study to tackle the issue of time intervals is Efremova et al. (2015). In this study authors apply clustering methods to automatically obtain optimal time partitions in a dataset of historical Dutch notary acts. We return to this question in Section 6.1. of this paper. The style of texts also changes over time and it can be a good indicator to predict the publication date of a document. In Štajner and Zampieri (2013) researchers used the style of texts calculated using readability scores to predict the publication date of Portuguese texts in the Colonia corpus. Another related study is the one by Hughes et al. (2012) that investigates the evolution of the style of 537 authors of the Project Gutenberg collection by looking at the usage of grammatical words. The most recent initiative on temporal text classification is the Semeval 2015 Task 7 ‘Diachronic Text Evaluation’ (DTE).1 In this shared task the organizers proposed three sub-tasks, two of them consisted of temporal text classification, and a third one dealing with the recognition of time specific phrases. For this task, the organizers compiled and released a test set containing English journalistic texts from 1700 to 2010. Texts were labeled with their approximate publication date in coarse, medium and finegrained intervals representing six, twelve and twenty years respectively. The task proved to be a very challenging one\n1Results and methods are described in detail in the shared task report (Popescu and Strapparava, 2015).\nand the only team to participate in all three sub-tasks was the IXA team (Salaberri et al., 2015) who used external resources such as Google N-grams and Wikipedia Entity Linking to accomplish the task. The best performing system in the DTE task was the UCD team (Szymanski and Lynch, 2015) who achieved 54.2% precision in identifying the publication date of texts in an interval of 20 years (subtask 2) using Support Vector Machine (SVM)."
    }, {
      "heading" : "3. Methods",
      "text" : "Following the results obtained by supervised learning approaches at the SemEval DTE task, in this paper we approach the task using supervised single-label multi-class classification. To test our method we used a Portuguese historical corpus, the aforementioned Colonia2, and we attribute to each text in the corpus a label corresponding to the time interval in which the text was written. As features we use the lexicon arranged as bag-of-words or word n-grams, and morphosyntatic information represented by POS tags. First we present a preliminary experiment using a small sample of the data containing excerpts of around 2,000 tokens each. The same sample was previously used in another temporal text classification approach relying on stylistic and readability features (Štajner and Zampieri, 2013). In this experiment we compared the performance of two machine learning classifiers, Multinomial Naive Bayes (MNB) (Frank and Bouckaert, 2006) and SVM (Joachims, 2006). Secondly we present the main experiments of this paper using a linear SVM classifier and a larger sample of the corpus. In particular, we use the LIBLINEAR3 package (Fan et al., 2008) which has been shown to be efficient for large-scale text classification problems such as this (Malmasi and Dras, 2015). As to the sample we used in this experiment, we opted to generate artificial documents composed of mixed sentences from different texts of the same period."
    }, {
      "heading" : "3.1. Data and Features",
      "text" : "The Colonia corpus is a historical Portuguese corpus, which contains texts spanning from the 16th century to the early 20th century (Zampieri and Becker, 2013). The corpus contains 100 documents (full novels or text collections) amounting to over 5.1 million tokens. It contains sentence boundary mark-up and coarse-grained POS annotation carried out using TreeTagger (Schmid, 1994).4 Colonia is available for download and it can be accessed through different corpus processing tools such as CQPWeb (through the project’s website), Linguateca,5 and Corpuseye.6 To our knowledge, the corpus has been used to study different aspects of the evolution of Portuguese such as diachronic morphology (Nevins et al., 2015).\n2http://corporavm.uni-koeln.de/colonia/index.html 3http://www.csie.ntu.edu.tw/%7Ecjlin/liblinear/ 4The corpus description paper does not contain any evaluation regarding the performance of the tagger on the Colonia dataset. The authors addressed solely the question of unknown lemmas after annotation in a post-processing stage.\n5http://www.linguateca.pt/acesso/corpus.php?corpus=COLONIA 6http://corp.hum.sdu.dk/cqp.pt.html\nThe availability of suitable texts is a known shortcoming in the compilation of historical corpora. Although Colonia is to our knowledge the biggest Portuguese corpus of its kind, it does not contain many texts from each period. The number of documents varies between 13 from the 16th century and 38 from the 19th century. For text classification, however, the number of documents available (especially at the training stage) is very important to provide enough information to achieve high classification performance. To circumvent this limitation, in this paper we propose the use of composite documents made of sentences from various texts. Following the methodology of Malmasi and Dras (2014a), we randomly select and combine the sentences from the same class (time period) to generate artificial texts of approximately 330 tokens on average, creating a set of documents for training and testing. This methodology ensures that the texts for each class are a mix of different authorship styles and topics. It also means that all documents are similar and comparable in length making the task more challenging. Previous work in other text classification tasks has shown that longer texts can be easier to classify (Malmasi et al., 2015). In our experiments we model two dimensions of language variation across time, lexical and (morpho-)syntactical. We do so by extracting words and part-of-speech (POS) tags from the corpus and using them as features. To the best of our knowledge these features were not yet tested in multiclass temporal text classification for Portuguese. The most similar approach to use these features is the ranking approach proposed by Niculae et al. (2014)."
    }, {
      "heading" : "4. Results",
      "text" : "In this section we present the results obtained in three sets of experiments:\n1. In Section 4.1 we describe preliminary experiments using a small sample of the corpus containing 87 documents spanning from the 17th to the early 20th century. We train two algorithms (SVM and MNB) using both words and POS tags represented as bag-of-words to predict the century in which the text was published.\n2. In Section 4.2 we apply the method of Malmasi and Dras (2014a) to generate artificial composite documents for training and testing using the complete set of texts available in Colonia (from the 16th to the early 20th century). We train an SVM classifier to predict the century in which the text was published and given the substantial increase in training material we report an important increase in performance using POS tags or words represented as uni-, bi-, and trigrams.\n3. Finally, in Section 4.3 we replicate the methods used in Section 4.2 for a smaller time span of 50 years."
    }, {
      "heading" : "4.1. Preliminary Experiments",
      "text" : "As the preliminary experiments of this paper we use a bagof-words model on a sample of the data previously used in the aforementioned study by Štajner and Zampieri (2013). Each document in this sample contains up to 2,000 token. The criteria for sampling was inspired in the Brown family\ncorpora (Francis and Kucera, 1979). The distribution of the texts across centuries is presented next.\nIn Colonia there are not many texts from each class and only a few from the 16th century. For this reason, in Štajner and Zampieri (2013) and in this preliminary experiment, we disregard texts from this century and propose an experiment with four classes instead of the five represented in Colonia. In Table 2 we present accuracy results using k-fold crossvalidation, with k = 10. We considered the majority class (19th century) as the baseline performance.\nThe best results were obtained by SVM regardless of the features used. SVM achieved 74.1% accuracy in identifying the century of texts using a combination of words and POS tags. An expected outcome of these preliminary experiments is that the results using lexical and morphsyntactic features are substantially higher than the 59% accuracy reported by Štajner and Zampieri (2013) using readability/stylistic features."
    }, {
      "heading" : "4.2. Increasing the Sample",
      "text" : "The small number of texts is a known limitation of most historical corpora. We address this question by generating artificial data with the methods described in Section 3. In this experiment we used a set of 1,500 artificially generated documents each of them combining sentences of various texts to represent each class.7 Details of the final data used in this experiment are shown in Table 3. Results obtained using an SVM classifier are presented in Table 4. Word n-grams of order 1–3 and POS n-grams of order 1–3 were extracted and a single SVM classifier was trained on each of these six n-gram features. Using the same evaluation methods of the preliminary experiment, we report accuracy results under k-fold cross-validation, with k = 10. Results are compared with a random baseline of 20%.\n7This was the largest possible number that would yield an even distribution across classes.\nThe best results were obtained using word unigrams achieving 99.8% accuracy. Results are substantially higher than those obtained in the preliminary experiment, once more confirming that the amount of training material plays a crucial role in this task. We look in more detail to the most informative features in classification in Section 5.. Next we present the confusion matrix obtained by the best performing setting, word unigrams, in Figure 1. We can observe that the classification performance is perfect for almost all centuries, with only some small confusion between the 19th century and early 20th century.\nWe contend that merging segments from different texts to form artificial documents tends to decrease the importance of stylistic preferences, lexical choices and other idiosyncrasies of a particular author. A result that corroborates this hypothesis is the performance obtained by the classifier using POS tags. The method is able to predict the century of the documents relying on POS trigrams with 90.7% ac-\ncuracy. In our opinion, this is an indication that there are important structural properties that differ in each of these time spans. This is an indication that the algorithm is able to capture language variation beyond word forms, as noted by (Zampieri et al., 2013) in a similar classification experiment to study the diatopic variation of Spanish."
    }, {
      "heading" : "4.3. Smaller Time Intervals",
      "text" : "As our method obtained almost perfect performance in predicting the century of each text, we would like to evaluate its performance in predicting the publication date of texts using a shorter time interval. For this purpose We divided the documents in the corpus into time intervals of 50 years resulting in 9 classes. This also results in a substantial drop in the random baseline, compared to the previous experiment. We then used the same methodology for generating artificial documents resulting in a total of 450 documents per class,8 4,050 documents in total and 1.35 million tokens. The distribution of data along with the total token count per time interval is presented in Table 5.\nWe used the sample presented in Table 5 for automatic classification and the results are presented in Table 6. Once more, our best performing setting achieved 99.8% accuracy using word unigrams. For the other settings we observed a slight (and expected) decrease in performance, with the lowest result obtained using POS unigrams 2.2 percentage points lower than the century classification.\nIt is interesting to note that even when working with shorter time spans, the distinction based on POS distribution tends\n8Adopting the same criterion used in the previous section, we used the largest possible number of documents that would result in an even distribution across classes.\nto perform quite well which suggests that there are structural differences between centuries as well as between time spans of 50 years. In figure 2 we present the confusion matrix obtained by the best setting using POS tags as features, POS trigrams, which achieved 90.1% accuracy. The results indicate that whenever a set of time intervals poses more challenge to the classifier, the more similar they are in terms of grammatical structures. We observed some degree of confusion in the oldest part of the corpus from 1500 to 1600 and from 1600 to 1700. The most challenging set of time intervals was the period comprising the 19th and 20th century. We take a closer look at indicative features of language change in Section 5.\nAnother interesting pattern we observed is that performance variation across different sets of features is almost identical for experiments described in Sections 4.2 and 4.3 here. Word-based methods perform best when arranged as unigrams and decrease performance for bigrams and even more when arranged as trigrams. On the other hand, classification using POS tags obtain higher performance using trigrams than bigrams and unigrams. This seems intuitive, once lexical variation is often captured by looking at individual words whereas structural differences tend to be observed when looking into sequences of two or more words."
    }, {
      "heading" : "5. Indicative Features of Language Change",
      "text" : "In this section we look at the most informative features for each of the five centuries represented in Colonia and try to highlight patterns that are, or are likely to be, good indicators of language change. The most informative features were extracted using the methodology proposed in Malmasi and Dras (2014b). This works by ranking the features according to the weights assigned by the SVM model. In this manner, SVMs have been successfully applied in data mining and knowledge discovery in a wide range of tasks such as identifying discriminant cancer genes (Guyon et al., 2002).\nAs noted by Zampieri et al. (2013), in a similar text classification task involving diatopic variation, linguistically motivated features usually do not outperform word- and character-based features. Our results also corroborate this claim. Even so, the use of features represented by POS tags and/or morphological information may provide interesting insights on language variation that can be analyzed by looking at the output of classifiers. We observed that constructions including three verbs, annotated as V V V, are a highly discriminating feature for 20th century texts. This represents constructions similar to Examples9 1 and 2, most of them with two auxiliary verbs and a main verb in the past participle tense:\n(1) O major tinha razão: o Leonardo não parecia ter nascido para emendas. (EN: seems to be born)\n(2) A música como a leitura, deve ser ministrada com prudência. (EN: must be administered)\nAnother interesting finding is the overuse of adjectives in the 19th and particularly in the 20th century. As noted in Section 4.3. this is a period in which the classifier has difficulties predicting the publication date of texts. By looking at the most informative trigrams we found patterns such as ADJ NOM ADJ (adjective noun adjective), and NOM ADJ CONJ (noun adjective conjunction). We investigated the latter and discovered that the conjunctions in this pattern usually refer to coordinate conjunctions that indicate the use of a second adjective modifying the noun in the same sentence as in Example 3:\n(3) Nada sorria naquela habitação árida e velha. (EN: dry and old housing)\nThe ADJ NOM ADJ (adjective noun adjective) pattern reflects the possibility of using adjectives after or before nouns depending on stylistic preferences or on what the speaker wants to emphasize. In Portuguese it is possible to say both homem grande and grande homem. The first one is the literal meaning, big man or tall man, whereas the second is a figurative one referring to a man who possesses great qualities, as the English expression great man. An example of the ADJ NOM ADJ pattern found in the corpus is presented in 4:\n(4) Os grandes olhos azuis, meio cerrados, às vezes se abriam languidamente como para se embeberem de luz, e abaixavam de novo as pálpebras rosadas. (EN: big blue eyes)\nDue to its thematic relevance, named entities (e.g. location, person) play an important role in text classification. Linguistically, these features are often not a very relevant indication of language change as the other features we discuss in this section, but they are usually very informative for classifiers. In the 17th century names of Portuguese monarchs such as Dom Afonso and Dom João are very informative, and the pronoun Sua Majestade (EN: Your Majesty) is also a very prominent feature.\n9All examples were obtained from the Colonia corpus.\nIn the 16th century we observed the use of a number welldocument archaisms used in that period (Castro, 1991). This includes most notably the cases of per, asi, mui, mi, and despois that refer to the current forms por (EN: for), assim (EN: thus, therefore), muito (EN: much), mim (EN: me) and depois (EN: after) respectively. The case of per is particularly interesting because it represents both the lusophone archaism as well as the Latin word from which we can trace its origin. There are a number of quotes in Latin in the 16th century part of the corpus making per a highly discriminating feature for this century. This can be observed in Examples 5 and 6:\n(5) A segunda, que esse lugar esteja em sı́tio acomodado pera socorrer dele com facilidade suas conquistas, e fazer as armadas que convém; isto se prova per muitas razões.\n(6) Siquidem me fecistis, constituamos leges, per quas terra nostra sit in pace."
    }, {
      "heading" : "6. Conclusion",
      "text" : "We investigated the use of words and POS tags to model lexical and syntactic variation in historical corpora for the purpose of temporal text classification. Our work extends the common knowledge in the task by using lexical and POS information for the first time in multi-label Portuguese temporal text classification, and by using artificially generated test and training instances combining fragments of texts written in the same period. The use of the latter was to the best of our knowledge still not investigated for this task. We contend that this methodology is an interesting strategy to cope with small amount of available texts, which is a known limitation of many historical corpora. The approach proposed in this paper is able to predict the publication date of texts, in intervals of both 100 and 50 years, using word unigrams with 99.8% accuracy. The method is also able to predict the publication date of texts using solely POS tags achieving performance of 90.7% accuracy for intervals of 100 years, and 90.1% accuracy for intervals of 50 years. Finally, we used the most informative features in the classification to investigate indicators of language change. The high word unigram results could be the result of chronological topic specificity; what’s more interesting is the POS n-grams discussed above, which aren’t related to topics but are indicative of grammatical or stylistic change. We found that texts from the 19th and 20th centuries contain on average a larger number of adjectives, which reflect stylistic preferences of that period. This kind of analysis is only possible with the use of POS tags as features. We also showed that, as expected, archaisms are a very distinctive feature of texts from the 16th century. As future work, we would like to investigate the question of time intervals (see next section) as well as to create an additional test set comprising a few texts from each time period. The new test set can be use to carry out a cross-corpus evaluation, using Colonia to training and test on the new corpus, to provide insights about how the models perform on data from different genres, text types, etc."
    }, {
      "heading" : "6.1. Future Work: Finding Optimal Time Intervals",
      "text" : "To our understanding, one of the limitations of most temporal text classification experiments (including ours) is the arbitrary definition of time intervals. In the case of our dataset, time intervals of one century are too long and they often fail to capture linguistic changes that occur in a certain point in time that do not coincide with the turn of a century. On the other hand, working with too short time spans is often unfeasible for historical datasets as there are not enough data points to be split between a large number of classes. The smallest time interval we could work with using this corpus was 50 years. Even so, we contend that the definition of arbitrary time spans is valid as a proof of concept and a perfect fit for supervised classification methods that require a predefined set of classes as the SVM-based approach presented in this paper. In real-world tasks, however, one might be interested in using more fine-grained intervals that can better capture the structure of the data and predict the publication date of a document more precisely. In light of this, our efforts are now concentrated on exploring ways to better represent the linearity of time (Niculae et al., 2014) or to find optimal time intervals in historical corpora as proposed by the aforementioned study by Efremova et al. (2015). We are currently experimenting with a recently proposed clustering method for this purpose (de Amorim and Hennig, 2015). It is important to note, however, that an adaptation of the ranking-based approach by Niculae et al. (2014) competed in the SemEval 2015 ‘Diachronic Text Evaluation’ as the AMBRA team (Zampieri et al., 2015) and did not outperform supervised learning approaches such as the one by the UCD team (Szymanski and Lynch, 2015). To our understand this seems to confirm that a combination of supervised classification and other computational approaches to find optimal time intervals (e.g. clustering) is probably the best way to approach this task."
    }, {
      "heading" : "Acknowledgements",
      "text" : "We would like to thank the anonymous reviewers for providing us important feedback and constructive comments to increase the quality of this paper.\nBibliographical References Abe, H. and Tsumoto, S. (2010). Text categorization with\nconsidering temporal patterns of term usages. In Proceedings of ICDM Workshops.\nCastro, I. (1991). Curso de Historia da Lingua Portuguesa. Universidade Aberta.\nChambers, N. (2012). Labeling documents with timestamps: Learning from their time expressions. In Proceedings of ACL.\nCiobanu, A. M., Dinu, A., Dinu, L. P., Niculae, V., and Sulea, O.-M. (2013a). Temporal text classification for romanian novels set in the past. In Proceedings of RANLP.\nCiobanu, A. M., Dinu, L. P., Dinu, A., and Niculae, V. (2013b). Temporal classification for historical Romanian texts. In Proceedings of LaTeCH.\nDakka, W., Gravano, L., and Ipeirotis, P. G. (2012). Answering general time-sensitive queries. IEEE Transactions on Knowledge and Data Engineering, 24(2):220– 235.\nDalli, A. and Wilks, Y. (2006). Automatic dating of documents and temporal text classification. In Proceedings of ARTE.\nde Amorim, R. C. and Hennig, C. (2015). Recovering the number of clusters in data sets with noise features using feature rescaling factors. Information Sciences, 324:126–145.\nde Jong, F., Rode, H., and Hiemstra, D. (2005). Temporal language models for the disclosure of historical text. In Proceedings of AHC.\nEfremova, J., Garcı́a, A. M., Zhang, J., and Calders, T. (2015). Effects of evolutionary linguistics in text classification. In Proceedings of SLSP.\nFan, R.-E., Chang, K.-W., Hsieh, C.-J., Wang, X.-R., and Lin, C.-J. (2008). LIBLINEAR: A library for large linear classification. Journal of Machine Learning Research, 9:1871–1874.\nFrancis, W. N. and Kucera, H. (1979). Brown corpus manual. Brown University.\nFrank, E. and Bouckaert, R. R. (2006). Naive bayes for text classification with unbalanced classes. In Proceedings of KDD.\nFrermann, L. and Lapata, M. (2016). A bayesian model of diachronic meaning change. Transactions of the Association for Computational Linguistics, 4:31–45.\nGarcia-Fernandez, A., Ligozat, A.-L., Dinarelli, M., and Bernhard, D. (2011). When was it written? Automatically determining publication dates. In Proceedings of SPIRE.\nGrouin, C., Forest, D., Da Sylva, L., Paroubek, P., and Zweigenbaum, P. (2010). Présentation et résultats du défi fouille de texte DEFT2010 où et quand un article de presse a-t-il été écrit? Proceedings of DEFT.\nGrouin, C., Forest, D., Paroubek, P., and Zweigenbaum, P. (2011). Présentation et résultats du défi fouille de texte deft2011 quand un article de presse a t-il été écrit? à quel article scientifique correspond ce résumé? Proceedings of DEFT.\nGuyon, I., Weston, J., Barnhill, S., and Vapnik, V. (2002). Gene selection for cancer classification using support vector machines. Machine learning, 46(1-3):389–422.\nHughes, J., Foti, N., Krakauer, D., and Rockmore, D. (2012). Quantitative patterns of stylistic influence in the evolution of literature. Proceedings of the National Academy of Sciences, 109(20):7682–7686.\nJoachims, T. (2006). Training linear SVMs in linear time. In Proceedings of KDD.\nKanhabua, N., Blanco, R., Nørvåg, K., et al. (2015). Temporal Information Retrieval. now Publishers.\nKraaij, W. (2004). Variations on language modeling for information retrieval. Ph.D. thesis, University of Twente.\nKumar, A., Lease, M., and Baldridge, J. (2011). Supervised language modelling for temporal resolution of texts. In Proceedings of CIKM.\nMalmasi, S. and Dras, M. (2014a). Chinese Native Language Identification. In Proceedings of EACL. Malmasi, S. and Dras, M. (2014b). Language Transfer Hypotheses with Linear SVM Weights. In Proceedings of EMNLP. Malmasi, S. and Dras, M. (2015). Multilingual Native Language Identification. In Natural Language Engineering. Malmasi, S., Refaee, E., and Dras, M. (2015). Arabic Dialect Identification using a Parallel Multidialectal Corpus. In Proceedings of PACLING. Mihalcea, R. and Nastase, V. (2012). Word epoch disambiguation: Finding how words change over time. In Proceedings of ACL. Nevins, A., Rodrigues, C., and Tang, K. (2015). The rise and fall of the l-shaped morphome: diachronic and experimental studies. Probus, 27(1):101–155. Niculae, V., Zampieri, M., Dinu, L. P., and Ciobanu, A. M. (2014). Temporal text ranking and automatic dating of texts. In Proceedings of EACL. Popescu, O. and Strapparava, C. (2013). Behind the times: Detecting epoch changes using large corpora. In Proceedings of IJCNLP. Popescu, O. and Strapparava, C. (2015). Semeval-2015 task 7: Diachronic text evaluation. In Proceedings of SemEval. Preotiuc-Pietro, D. (2014). Temporal models of streaming social media data. Ph.D. thesis, University of Sheffield. Salaberri, H., Salaberri, I., Arregi, O., and Zapirain, B. (2015). Ixagroupehudiac: A multiple approach system towards the diachronic evaluation of texts. In Proceedings of SemEval. Schmid, H. (1994). Probabilistic part-of-speech tagging using decision trees. In Proceedings of International Conference on New Methods in Language Processing, Manchester, UK. Štajner, S. and Zampieri, M. (2013). Stylistic changes for temporal text classification. In Proceedings of TSD. Szymanski, T. and Lynch, G. (2015). UCD: Diachronic text classification with character, word, and syntactic ngrams. In Proceedings of SemEval. Zampieri, M. and Becker, M. (2013). Colonia: Corpus of historical Portuguese. ZSM Studien, Special Volume on Non-Standard Data Sources in Corpus-Based Research. Zampieri, M., Gebre, B. G., and Diwersy, S. (2013). Ngram language models and POS distribution for the identification of Spanish varieties. In Proceedings of TALN. Zampieri, M., Ciobanu, A. M., Niculae, V., and Dinu, L. P. (2015). AMBRA: A ranking approach to temporal text classification. In Proceedings of SemEval. Zhao, Y. and Hauff, C. (2015a). Sub-document timestamping of web documents. In Proceedings of SIGIR. Zhao, Y. and Hauff, C. (2015b). Temporal information retrieval revisited: a focused study on the web. In Proceedings of the FDIA Symposium."
    } ],
    "references" : [ {
      "title" : "Text categorization with considering temporal patterns of term usages",
      "author" : [ "H. Abe", "S. Tsumoto" ],
      "venue" : "Proceedings of ICDM Workshops.",
      "citeRegEx" : "Abe and Tsumoto,? 2010",
      "shortCiteRegEx" : "Abe and Tsumoto",
      "year" : 2010
    }, {
      "title" : "Curso de Historia da Lingua Portuguesa",
      "author" : [ "I. Castro" ],
      "venue" : "Universidade Aberta.",
      "citeRegEx" : "Castro,? 1991",
      "shortCiteRegEx" : "Castro",
      "year" : 1991
    }, {
      "title" : "Labeling documents with timestamps: Learning from their time expressions",
      "author" : [ "N. Chambers" ],
      "venue" : "Proceedings of ACL.",
      "citeRegEx" : "Chambers,? 2012",
      "shortCiteRegEx" : "Chambers",
      "year" : 2012
    }, {
      "title" : "Temporal text classification for romanian novels set in the past",
      "author" : [ "A.M. Ciobanu", "A. Dinu", "L.P. Dinu", "V. Niculae", "Sulea", "O.-M." ],
      "venue" : "Proceedings of RANLP.",
      "citeRegEx" : "Ciobanu et al\\.,? 2013a",
      "shortCiteRegEx" : "Ciobanu et al\\.",
      "year" : 2013
    }, {
      "title" : "Temporal classification for historical Romanian texts",
      "author" : [ "A.M. Ciobanu", "L.P. Dinu", "A. Dinu", "V. Niculae" ],
      "venue" : "Proceedings of LaTeCH.",
      "citeRegEx" : "Ciobanu et al\\.,? 2013b",
      "shortCiteRegEx" : "Ciobanu et al\\.",
      "year" : 2013
    }, {
      "title" : "Answering general time-sensitive queries",
      "author" : [ "W. Dakka", "L. Gravano", "P.G. Ipeirotis" ],
      "venue" : "IEEE Transactions on Knowledge and Data Engineering, 24(2):220– 235.",
      "citeRegEx" : "Dakka et al\\.,? 2012",
      "shortCiteRegEx" : "Dakka et al\\.",
      "year" : 2012
    }, {
      "title" : "Automatic dating of documents and temporal text classification",
      "author" : [ "A. Dalli", "Y. Wilks" ],
      "venue" : "Proceedings of ARTE.",
      "citeRegEx" : "Dalli and Wilks,? 2006",
      "shortCiteRegEx" : "Dalli and Wilks",
      "year" : 2006
    }, {
      "title" : "Recovering the number of clusters in data sets with noise features using feature rescaling factors",
      "author" : [ "R.C. de Amorim", "C. Hennig" ],
      "venue" : "Information Sciences,",
      "citeRegEx" : "Amorim and Hennig,? \\Q2015\\E",
      "shortCiteRegEx" : "Amorim and Hennig",
      "year" : 2015
    }, {
      "title" : "Temporal language models for the disclosure of historical text",
      "author" : [ "F. de Jong", "H. Rode", "D. Hiemstra" ],
      "venue" : "In Proceedings of AHC",
      "citeRegEx" : "Jong et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Jong et al\\.",
      "year" : 2005
    }, {
      "title" : "Effects of evolutionary linguistics in text classification",
      "author" : [ "J. Efremova", "A.M. Garcı́a", "J. Zhang", "T. Calders" ],
      "venue" : "In Proceedings of SLSP",
      "citeRegEx" : "Efremova et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Efremova et al\\.",
      "year" : 2015
    }, {
      "title" : "LIBLINEAR: A library for large linear classification",
      "author" : [ "Fan", "R.-E.", "Chang", "K.-W.", "Hsieh", "C.-J.", "Wang", "X.-R.", "Lin", "C.-J." ],
      "venue" : "Journal of Machine Learning Research, 9:1871–1874.",
      "citeRegEx" : "Fan et al\\.,? 2008",
      "shortCiteRegEx" : "Fan et al\\.",
      "year" : 2008
    }, {
      "title" : "Brown corpus manual",
      "author" : [ "W.N. Francis", "H. Kucera" ],
      "venue" : "Brown University.",
      "citeRegEx" : "Francis and Kucera,? 1979",
      "shortCiteRegEx" : "Francis and Kucera",
      "year" : 1979
    }, {
      "title" : "Naive bayes for text classification with unbalanced classes",
      "author" : [ "E. Frank", "R.R. Bouckaert" ],
      "venue" : "Proceedings of KDD.",
      "citeRegEx" : "Frank and Bouckaert,? 2006",
      "shortCiteRegEx" : "Frank and Bouckaert",
      "year" : 2006
    }, {
      "title" : "A bayesian model of diachronic meaning change",
      "author" : [ "L. Frermann", "M. Lapata" ],
      "venue" : "Transactions of the Association for Computational Linguistics, 4:31–45.",
      "citeRegEx" : "Frermann and Lapata,? 2016",
      "shortCiteRegEx" : "Frermann and Lapata",
      "year" : 2016
    }, {
      "title" : "When was it written? Automatically determining publication dates",
      "author" : [ "A. Garcia-Fernandez", "Ligozat", "A.-L.", "M. Dinarelli", "D. Bernhard" ],
      "venue" : "Proceedings of SPIRE.",
      "citeRegEx" : "Garcia.Fernandez et al\\.,? 2011",
      "shortCiteRegEx" : "Garcia.Fernandez et al\\.",
      "year" : 2011
    }, {
      "title" : "Présentation et résultats du défi fouille de texte DEFT2010 où et quand un article de presse a-t-il été écrit? Proceedings of DEFT",
      "author" : [ "C. Grouin", "D. Forest", "L. Da Sylva", "P. Paroubek", "P. Zweigenbaum" ],
      "venue" : null,
      "citeRegEx" : "Grouin et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Grouin et al\\.",
      "year" : 2010
    }, {
      "title" : "Présentation et résultats du défi fouille de texte deft2011 quand un article de presse a t-il été écrit? à quel article scientifique correspond ce résumé? Proceedings of DEFT",
      "author" : [ "C. Grouin", "D. Forest", "P. Paroubek", "P. Zweigenbaum" ],
      "venue" : null,
      "citeRegEx" : "Grouin et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Grouin et al\\.",
      "year" : 2011
    }, {
      "title" : "Gene selection for cancer classification using support vector machines",
      "author" : [ "I. Guyon", "J. Weston", "S. Barnhill", "V. Vapnik" ],
      "venue" : "Machine learning, 46(1-3):389–422.",
      "citeRegEx" : "Guyon et al\\.,? 2002",
      "shortCiteRegEx" : "Guyon et al\\.",
      "year" : 2002
    }, {
      "title" : "Quantitative patterns of stylistic influence in the evolution of literature",
      "author" : [ "J. Hughes", "N. Foti", "D. Krakauer", "D. Rockmore" ],
      "venue" : "Proceedings of the National Academy of Sciences, 109(20):7682–7686.",
      "citeRegEx" : "Hughes et al\\.,? 2012",
      "shortCiteRegEx" : "Hughes et al\\.",
      "year" : 2012
    }, {
      "title" : "Training linear SVMs in linear time",
      "author" : [ "T. Joachims" ],
      "venue" : "Proceedings of KDD.",
      "citeRegEx" : "Joachims,? 2006",
      "shortCiteRegEx" : "Joachims",
      "year" : 2006
    }, {
      "title" : "Temporal Information Retrieval. now Publishers",
      "author" : [ "N. Kanhabua", "R. Blanco", "K Nørvåg" ],
      "venue" : null,
      "citeRegEx" : "Kanhabua et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kanhabua et al\\.",
      "year" : 2015
    }, {
      "title" : "Variations on language modeling for information retrieval",
      "author" : [ "W. Kraaij" ],
      "venue" : "Ph.D. thesis, University of Twente.",
      "citeRegEx" : "Kraaij,? 2004",
      "shortCiteRegEx" : "Kraaij",
      "year" : 2004
    }, {
      "title" : "Supervised language modelling for temporal resolution of texts",
      "author" : [ "A. Kumar", "M. Lease", "J. Baldridge" ],
      "venue" : "Proceedings of CIKM.",
      "citeRegEx" : "Kumar et al\\.,? 2011",
      "shortCiteRegEx" : "Kumar et al\\.",
      "year" : 2011
    }, {
      "title" : "Chinese Native Language Identification",
      "author" : [ "S. Malmasi", "M. Dras" ],
      "venue" : "Proceedings of EACL.",
      "citeRegEx" : "Malmasi and Dras,? 2014a",
      "shortCiteRegEx" : "Malmasi and Dras",
      "year" : 2014
    }, {
      "title" : "Language Transfer Hypotheses with Linear SVM Weights",
      "author" : [ "S. Malmasi", "M. Dras" ],
      "venue" : "Proceedings of EMNLP.",
      "citeRegEx" : "Malmasi and Dras,? 2014b",
      "shortCiteRegEx" : "Malmasi and Dras",
      "year" : 2014
    }, {
      "title" : "Multilingual Native Language Identification",
      "author" : [ "S. Malmasi", "M. Dras" ],
      "venue" : "Natural Language Engineering.",
      "citeRegEx" : "Malmasi and Dras,? 2015",
      "shortCiteRegEx" : "Malmasi and Dras",
      "year" : 2015
    }, {
      "title" : "Arabic Dialect Identification using a Parallel Multidialectal Corpus",
      "author" : [ "S. Malmasi", "E. Refaee", "M. Dras" ],
      "venue" : "Proceedings of PACLING.",
      "citeRegEx" : "Malmasi et al\\.,? 2015",
      "shortCiteRegEx" : "Malmasi et al\\.",
      "year" : 2015
    }, {
      "title" : "Word epoch disambiguation: Finding how words change over time",
      "author" : [ "R. Mihalcea", "V. Nastase" ],
      "venue" : "Proceedings of ACL.",
      "citeRegEx" : "Mihalcea and Nastase,? 2012",
      "shortCiteRegEx" : "Mihalcea and Nastase",
      "year" : 2012
    }, {
      "title" : "The rise and fall of the l-shaped morphome: diachronic and experimental studies",
      "author" : [ "A. Nevins", "C. Rodrigues", "K. Tang" ],
      "venue" : "Probus, 27(1):101–155.",
      "citeRegEx" : "Nevins et al\\.,? 2015",
      "shortCiteRegEx" : "Nevins et al\\.",
      "year" : 2015
    }, {
      "title" : "Temporal text ranking and automatic dating of texts",
      "author" : [ "V. Niculae", "M. Zampieri", "L.P. Dinu", "A.M. Ciobanu" ],
      "venue" : "Proceedings of EACL.",
      "citeRegEx" : "Niculae et al\\.,? 2014",
      "shortCiteRegEx" : "Niculae et al\\.",
      "year" : 2014
    }, {
      "title" : "Behind the times: Detecting epoch changes using large corpora",
      "author" : [ "O. Popescu", "C. Strapparava" ],
      "venue" : "Proceedings of IJCNLP.",
      "citeRegEx" : "Popescu and Strapparava,? 2013",
      "shortCiteRegEx" : "Popescu and Strapparava",
      "year" : 2013
    }, {
      "title" : "Semeval-2015 task 7: Diachronic text evaluation",
      "author" : [ "O. Popescu", "C. Strapparava" ],
      "venue" : "Proceedings of SemEval.",
      "citeRegEx" : "Popescu and Strapparava,? 2015",
      "shortCiteRegEx" : "Popescu and Strapparava",
      "year" : 2015
    }, {
      "title" : "Temporal models of streaming social media data",
      "author" : [ "D. Preotiuc-Pietro" ],
      "venue" : "Ph.D. thesis, University of Sheffield.",
      "citeRegEx" : "Preotiuc.Pietro,? 2014",
      "shortCiteRegEx" : "Preotiuc.Pietro",
      "year" : 2014
    }, {
      "title" : "Ixagroupehudiac: A multiple approach system towards the diachronic evaluation of texts",
      "author" : [ "H. Salaberri", "I. Salaberri", "O. Arregi", "B. Zapirain" ],
      "venue" : "Proceedings of SemEval.",
      "citeRegEx" : "Salaberri et al\\.,? 2015",
      "shortCiteRegEx" : "Salaberri et al\\.",
      "year" : 2015
    }, {
      "title" : "Probabilistic part-of-speech tagging using decision trees",
      "author" : [ "H. Schmid" ],
      "venue" : "Proceedings of International Conference on New Methods in Language Processing, Manchester, UK.",
      "citeRegEx" : "Schmid,? 1994",
      "shortCiteRegEx" : "Schmid",
      "year" : 1994
    }, {
      "title" : "Stylistic changes for temporal text classification",
      "author" : [ "S. Štajner", "M. Zampieri" ],
      "venue" : "Proceedings of TSD.",
      "citeRegEx" : "Štajner and Zampieri,? 2013",
      "shortCiteRegEx" : "Štajner and Zampieri",
      "year" : 2013
    }, {
      "title" : "UCD: Diachronic text classification with character, word, and syntactic ngrams",
      "author" : [ "T. Szymanski", "G. Lynch" ],
      "venue" : "Proceedings of SemEval.",
      "citeRegEx" : "Szymanski and Lynch,? 2015",
      "shortCiteRegEx" : "Szymanski and Lynch",
      "year" : 2015
    }, {
      "title" : "Colonia: Corpus of historical Portuguese",
      "author" : [ "M. Zampieri", "M. Becker" ],
      "venue" : "ZSM Studien, Special Volume on Non-Standard Data Sources in Corpus-Based Research.",
      "citeRegEx" : "Zampieri and Becker,? 2013",
      "shortCiteRegEx" : "Zampieri and Becker",
      "year" : 2013
    }, {
      "title" : "Ngram language models and POS distribution for the identification of Spanish varieties",
      "author" : [ "M. Zampieri", "B.G. Gebre", "S. Diwersy" ],
      "venue" : "Proceedings of TALN.",
      "citeRegEx" : "Zampieri et al\\.,? 2013",
      "shortCiteRegEx" : "Zampieri et al\\.",
      "year" : 2013
    }, {
      "title" : "AMBRA: A ranking approach to temporal text classification",
      "author" : [ "M. Zampieri", "A.M. Ciobanu", "V. Niculae", "L.P. Dinu" ],
      "venue" : "Proceedings of SemEval.",
      "citeRegEx" : "Zampieri et al\\.,? 2015",
      "shortCiteRegEx" : "Zampieri et al\\.",
      "year" : 2015
    }, {
      "title" : "Sub-document timestamping of web documents",
      "author" : [ "Y. Zhao", "C. Hauff" ],
      "venue" : "Proceedings of SIGIR.",
      "citeRegEx" : "Zhao and Hauff,? 2015a",
      "shortCiteRegEx" : "Zhao and Hauff",
      "year" : 2015
    }, {
      "title" : "Temporal information retrieval revisited: a focused study on the web",
      "author" : [ "Y. Zhao", "C. Hauff" ],
      "venue" : "Proceedings of the FDIA Symposium.",
      "citeRegEx" : "Zhao and Hauff,? 2015b",
      "shortCiteRegEx" : "Zhao and Hauff",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "Recent studies have shown that it is possible to use language change to predict the approximate publication date of texts in diachronic text collections (Ciobanu et al., 2013a; Popescu and Strapparava, 2015).",
      "startOffset" : 153,
      "endOffset" : 207
    }, {
      "referenceID" : 31,
      "context" : "Recent studies have shown that it is possible to use language change to predict the approximate publication date of texts in diachronic text collections (Ciobanu et al., 2013a; Popescu and Strapparava, 2015).",
      "startOffset" : 153,
      "endOffset" : 207
    }, {
      "referenceID" : 37,
      "context" : "The classifiers were trained and tested on a sample of a historical Portuguese corpus called Colonia (Zampieri and Becker, 2013) which contains texts spanning from the 16th to the early 20th century.",
      "startOffset" : 101,
      "endOffset" : 128
    }, {
      "referenceID" : 5,
      "context" : "Information Retrieval (IR) methods, for example, often have to process temporal information in both queries and documents to deal with dynamicity of the content found in data repositories and the Web (Dakka et al., 2012; Preotiuc-Pietro, 2014; Kanhabua et al., 2015; Zhao and Hauff, 2015b; Zhao and Hauff, 2015a).",
      "startOffset" : 200,
      "endOffset" : 312
    }, {
      "referenceID" : 32,
      "context" : "Information Retrieval (IR) methods, for example, often have to process temporal information in both queries and documents to deal with dynamicity of the content found in data repositories and the Web (Dakka et al., 2012; Preotiuc-Pietro, 2014; Kanhabua et al., 2015; Zhao and Hauff, 2015b; Zhao and Hauff, 2015a).",
      "startOffset" : 200,
      "endOffset" : 312
    }, {
      "referenceID" : 20,
      "context" : "Information Retrieval (IR) methods, for example, often have to process temporal information in both queries and documents to deal with dynamicity of the content found in data repositories and the Web (Dakka et al., 2012; Preotiuc-Pietro, 2014; Kanhabua et al., 2015; Zhao and Hauff, 2015b; Zhao and Hauff, 2015a).",
      "startOffset" : 200,
      "endOffset" : 312
    }, {
      "referenceID" : 41,
      "context" : "Information Retrieval (IR) methods, for example, often have to process temporal information in both queries and documents to deal with dynamicity of the content found in data repositories and the Web (Dakka et al., 2012; Preotiuc-Pietro, 2014; Kanhabua et al., 2015; Zhao and Hauff, 2015b; Zhao and Hauff, 2015a).",
      "startOffset" : 200,
      "endOffset" : 312
    }, {
      "referenceID" : 40,
      "context" : "Information Retrieval (IR) methods, for example, often have to process temporal information in both queries and documents to deal with dynamicity of the content found in data repositories and the Web (Dakka et al., 2012; Preotiuc-Pietro, 2014; Kanhabua et al., 2015; Zhao and Hauff, 2015b; Zhao and Hauff, 2015a).",
      "startOffset" : 200,
      "endOffset" : 312
    }, {
      "referenceID" : 2,
      "context" : "after 2010), can help algorithms to identify the approximate publication date of texts (Chambers, 2012), but there are a number of cases in which they are not present in text and one alternative is to use features related to language change as we propose in this paper.",
      "startOffset" : 87,
      "endOffset" : 103
    }, {
      "referenceID" : 21,
      "context" : "(2005) uses unigram language models combined with smoothing techniques and log-likelihood ratio measure (NLLR) (Kraaij, 2004) to classify documents within different time spans.",
      "startOffset" : 111,
      "endOffset" : 125
    }, {
      "referenceID" : 6,
      "context" : "The work by de Jong et al. (2005) uses unigram language models combined with smoothing techniques and log-likelihood ratio measure (NLLR) (Kraaij, 2004) to classify documents within different time spans.",
      "startOffset" : 15,
      "endOffset" : 34
    }, {
      "referenceID" : 6,
      "context" : "The work by de Jong et al. (2005) uses unigram language models combined with smoothing techniques and log-likelihood ratio measure (NLLR) (Kraaij, 2004) to classify documents within different time spans. The method was tested on a collection of Dutch journalistic texts published from January 1999 to February 2005. Other methods, such as Kumar et al. (2011), make use of information gain to estimate the best features in classification.",
      "startOffset" : 15,
      "endOffset" : 359
    }, {
      "referenceID" : 5,
      "context" : "In Dalli and Wilks (2006) researchers train a classifier to predict the publication date of texts within a time span of nine years.",
      "startOffset" : 3,
      "endOffset" : 26
    }, {
      "referenceID" : 0,
      "context" : "Another study that works under a similar assumption is the one published by Abe and Tsumoto (2010). The authors proposed the use of similarity metrics to categorize texts based on keywords calculated using tf-idf (term frequency - inverse document frequency).",
      "startOffset" : 76,
      "endOffset" : 99
    }, {
      "referenceID" : 16,
      "context" : "The corpus used was provided by the organizers of the DEFT2011 challenge (Grouin et al., 2011) which was essentially a temporal text classification ar X iv :1 61 0.",
      "startOffset" : 73,
      "endOffset" : 94
    }, {
      "referenceID" : 15,
      "context" : "task for French following a similar DEFT2010 challenge that included both diachronic and diatopic (regional) variation (Grouin et al., 2010).",
      "startOffset" : 119,
      "endOffset" : 140
    }, {
      "referenceID" : 14,
      "context" : "Garcia-Fernandez et al. (2011) report 14% accuracy in predicting the year of publication of texts and 42% accuracy in predicting the correct decade of publication.",
      "startOffset" : 0,
      "endOffset" : 31
    }, {
      "referenceID" : 13,
      "context" : "Lexical changes are regarded to be an important feature of diachronic text collections and researchers have proposed methods to track meaning change over time (Frermann and Lapata, 2016).",
      "startOffset" : 159,
      "endOffset" : 186
    }, {
      "referenceID" : 11,
      "context" : "Lexical changes are regarded to be an important feature of diachronic text collections and researchers have proposed methods to track meaning change over time (Frermann and Lapata, 2016). The study by Mihalcea and Nastase (2012) investigates how word meanings change over time in three major periods in time: 1800, 1900 and 2000.",
      "startOffset" : 160,
      "endOffset" : 229
    }, {
      "referenceID" : 11,
      "context" : "Lexical changes are regarded to be an important feature of diachronic text collections and researchers have proposed methods to track meaning change over time (Frermann and Lapata, 2016). The study by Mihalcea and Nastase (2012) investigates how word meanings change over time in three major periods in time: 1800, 1900 and 2000. Popescu and Strapparava (2013) look at significant changes in the use of words across time for the purpose of characterizing epochs using the Google N-Gram collection from 1614 to 2009.",
      "startOffset" : 160,
      "endOffset" : 361
    }, {
      "referenceID" : 3,
      "context" : "Ciobanu et al. (2013a) and Ciobanu et al.",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 3,
      "context" : "Ciobanu et al. (2013a) and Ciobanu et al. (2013b) applied SVM and Random Forest algorithms to classify texts of a historical Romanian text collection regarding their publication date.",
      "startOffset" : 0,
      "endOffset" : 50
    }, {
      "referenceID" : 28,
      "context" : "The study by Niculae et al. (2014) approached the task using ranking and pairwise comparisons to predict for each pair of documents which one is older and finally to produce a rank of all documents in a collection from older to newer.",
      "startOffset" : 13,
      "endOffset" : 35
    }, {
      "referenceID" : 9,
      "context" : "Another recent study to tackle the issue of time intervals is Efremova et al. (2015). In this study authors apply clustering methods to automatically obtain optimal time partitions in a dataset of historical Dutch notary acts.",
      "startOffset" : 62,
      "endOffset" : 85
    }, {
      "referenceID" : 34,
      "context" : "In Štajner and Zampieri (2013) researchers used the style of texts calculated using readability scores to predict the publication date of Portuguese texts in the Colonia corpus.",
      "startOffset" : 3,
      "endOffset" : 31
    }, {
      "referenceID" : 18,
      "context" : "Another related study is the one by Hughes et al. (2012) that investigates the evolution of the style of 537 authors of the Project Gutenberg collection by looking at the usage of grammatical words.",
      "startOffset" : 36,
      "endOffset" : 57
    }, {
      "referenceID" : 31,
      "context" : "Results and methods are described in detail in the shared task report (Popescu and Strapparava, 2015).",
      "startOffset" : 70,
      "endOffset" : 101
    }, {
      "referenceID" : 33,
      "context" : "and the only team to participate in all three sub-tasks was the IXA team (Salaberri et al., 2015) who used external resources such as Google N-grams and Wikipedia Entity Linking to accomplish the task.",
      "startOffset" : 73,
      "endOffset" : 97
    }, {
      "referenceID" : 36,
      "context" : "The best performing system in the DTE task was the UCD team (Szymanski and Lynch, 2015) who achieved 54.",
      "startOffset" : 60,
      "endOffset" : 87
    }, {
      "referenceID" : 35,
      "context" : "The same sample was previously used in another temporal text classification approach relying on stylistic and readability features (Štajner and Zampieri, 2013).",
      "startOffset" : 131,
      "endOffset" : 159
    }, {
      "referenceID" : 12,
      "context" : "In this experiment we compared the performance of two machine learning classifiers, Multinomial Naive Bayes (MNB) (Frank and Bouckaert, 2006) and SVM (Joachims, 2006).",
      "startOffset" : 114,
      "endOffset" : 141
    }, {
      "referenceID" : 19,
      "context" : "In this experiment we compared the performance of two machine learning classifiers, Multinomial Naive Bayes (MNB) (Frank and Bouckaert, 2006) and SVM (Joachims, 2006).",
      "startOffset" : 150,
      "endOffset" : 166
    }, {
      "referenceID" : 10,
      "context" : "In particular, we use the LIBLINEAR3 package (Fan et al., 2008) which has been shown to be efficient for large-scale text classification problems such as this (Malmasi and Dras, 2015).",
      "startOffset" : 45,
      "endOffset" : 63
    }, {
      "referenceID" : 25,
      "context" : ", 2008) which has been shown to be efficient for large-scale text classification problems such as this (Malmasi and Dras, 2015).",
      "startOffset" : 103,
      "endOffset" : 127
    }, {
      "referenceID" : 37,
      "context" : "The Colonia corpus is a historical Portuguese corpus, which contains texts spanning from the 16th century to the early 20th century (Zampieri and Becker, 2013).",
      "startOffset" : 132,
      "endOffset" : 159
    }, {
      "referenceID" : 34,
      "context" : "It contains sentence boundary mark-up and coarse-grained POS annotation carried out using TreeTagger (Schmid, 1994).",
      "startOffset" : 101,
      "endOffset" : 115
    }, {
      "referenceID" : 28,
      "context" : "6 To our knowledge, the corpus has been used to study different aspects of the evolution of Portuguese such as diachronic morphology (Nevins et al., 2015).",
      "startOffset" : 133,
      "endOffset" : 154
    }, {
      "referenceID" : 26,
      "context" : "Previous work in other text classification tasks has shown that longer texts can be easier to classify (Malmasi et al., 2015).",
      "startOffset" : 103,
      "endOffset" : 125
    }, {
      "referenceID" : 23,
      "context" : "Following the methodology of Malmasi and Dras (2014a), we randomly select and combine the sentences from the same class (time period) to generate artificial texts of approximately 330 tokens on average, creating a set of documents for training and testing.",
      "startOffset" : 29,
      "endOffset" : 54
    }, {
      "referenceID" : 23,
      "context" : "Following the methodology of Malmasi and Dras (2014a), we randomly select and combine the sentences from the same class (time period) to generate artificial texts of approximately 330 tokens on average, creating a set of documents for training and testing. This methodology ensures that the texts for each class are a mix of different authorship styles and topics. It also means that all documents are similar and comparable in length making the task more challenging. Previous work in other text classification tasks has shown that longer texts can be easier to classify (Malmasi et al., 2015). In our experiments we model two dimensions of language variation across time, lexical and (morpho-)syntactical. We do so by extracting words and part-of-speech (POS) tags from the corpus and using them as features. To the best of our knowledge these features were not yet tested in multiclass temporal text classification for Portuguese. The most similar approach to use these features is the ranking approach proposed by Niculae et al. (2014).",
      "startOffset" : 29,
      "endOffset" : 1040
    }, {
      "referenceID" : 23,
      "context" : "2 we apply the method of Malmasi and Dras (2014a) to generate artificial composite documents for training and testing using the complete set of texts available in Colonia (from the 16th to the early 20th century).",
      "startOffset" : 25,
      "endOffset" : 50
    }, {
      "referenceID" : 11,
      "context" : "The criteria for sampling was inspired in the Brown family corpora (Francis and Kucera, 1979).",
      "startOffset" : 67,
      "endOffset" : 93
    }, {
      "referenceID" : 34,
      "context" : "As the preliminary experiments of this paper we use a bagof-words model on a sample of the data previously used in the aforementioned study by Štajner and Zampieri (2013). Each document in this sample contains up to 2,000 token.",
      "startOffset" : 143,
      "endOffset" : 171
    }, {
      "referenceID" : 35,
      "context" : "For this reason, in Štajner and Zampieri (2013) and in this preliminary experiment, we disregard texts from this century and propose an experiment with four classes instead of the five represented in Colonia.",
      "startOffset" : 20,
      "endOffset" : 48
    }, {
      "referenceID" : 35,
      "context" : "An expected outcome of these preliminary experiments is that the results using lexical and morphsyntactic features are substantially higher than the 59% accuracy reported by Štajner and Zampieri (2013) using readability/stylistic features.",
      "startOffset" : 174,
      "endOffset" : 202
    }, {
      "referenceID" : 38,
      "context" : "This is an indication that the algorithm is able to capture language variation beyond word forms, as noted by (Zampieri et al., 2013) in a similar classification experiment to study the diatopic variation of Spanish.",
      "startOffset" : 110,
      "endOffset" : 133
    }, {
      "referenceID" : 17,
      "context" : "In this manner, SVMs have been successfully applied in data mining and knowledge discovery in a wide range of tasks such as identifying discriminant cancer genes (Guyon et al., 2002).",
      "startOffset" : 162,
      "endOffset" : 182
    }, {
      "referenceID" : 22,
      "context" : "The most informative features were extracted using the methodology proposed in Malmasi and Dras (2014b). This works by ranking the features according to the weights assigned by the SVM model.",
      "startOffset" : 79,
      "endOffset" : 104
    }, {
      "referenceID" : 17,
      "context" : "In this manner, SVMs have been successfully applied in data mining and knowledge discovery in a wide range of tasks such as identifying discriminant cancer genes (Guyon et al., 2002). As noted by Zampieri et al. (2013), in a similar text classification task involving diatopic variation, linguistically motivated features usually do not outperform word- and character-based features.",
      "startOffset" : 163,
      "endOffset" : 219
    }, {
      "referenceID" : 1,
      "context" : "In the 16th century we observed the use of a number welldocument archaisms used in that period (Castro, 1991).",
      "startOffset" : 95,
      "endOffset" : 109
    }, {
      "referenceID" : 29,
      "context" : "In light of this, our efforts are now concentrated on exploring ways to better represent the linearity of time (Niculae et al., 2014) or to find optimal time intervals in historical corpora as proposed by the aforementioned study by Efremova et al.",
      "startOffset" : 111,
      "endOffset" : 133
    }, {
      "referenceID" : 39,
      "context" : "(2014) competed in the SemEval 2015 ‘Diachronic Text Evaluation’ as the AMBRA team (Zampieri et al., 2015) and did not outperform supervised learning approaches such as the one by the UCD team (Szymanski and Lynch, 2015).",
      "startOffset" : 83,
      "endOffset" : 106
    }, {
      "referenceID" : 36,
      "context" : ", 2015) and did not outperform supervised learning approaches such as the one by the UCD team (Szymanski and Lynch, 2015).",
      "startOffset" : 94,
      "endOffset" : 121
    }, {
      "referenceID" : 8,
      "context" : ", 2014) or to find optimal time intervals in historical corpora as proposed by the aforementioned study by Efremova et al. (2015). We are currently experimenting with a recently proposed clustering method for this purpose (de Amorim and Hennig, 2015).",
      "startOffset" : 107,
      "endOffset" : 130
    }, {
      "referenceID" : 7,
      "context" : "We are currently experimenting with a recently proposed clustering method for this purpose (de Amorim and Hennig, 2015). It is important to note, however, that an adaptation of the ranking-based approach by Niculae et al. (2014) competed in the SemEval 2015 ‘Diachronic Text Evaluation’ as the AMBRA team (Zampieri et al.",
      "startOffset" : 95,
      "endOffset" : 229
    } ],
    "year" : 2016,
    "abstractText" : "This paper presents a number of experiments to model changes in a historical Portuguese corpus composed of literary texts for the purpose of temporal text classification. Algorithms were trained to classify texts with respect to their publication date taking into account lexical variation represented as word n-grams, and morphosyntactic variation represented by part-of-speech (POS) distribution. We report results of 99.8% accuracy using word unigram features with a Support Vector Machines classifier to predict the publication date of documents in time intervals of both one century and half a century. A feature analysis is performed to investigate the most informative features for this task and how they are linked to language change.",
    "creator" : "LaTeX with hyperref package"
  }
}