{
  "name" : "1704.03732.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Learning from Demonstrations for Real World Reinforcement Learning",
    "authors" : [ "Todd Hester", "Olivier Pietquin", "Marc Lanctot", "Tom Schaul", "John Agapiou", "Joel Z. Leibo" ],
    "emails" : [ "TODDHESTER@GOOGLE.COM", "MATEJVECERIK@GOOGLE.COM", "PIETQUIN@GOOGLE.COM", "LANCTOT@GOOGLE.COM", "SCHAUL@GOOGLE.COM", "PIOT@GOOGLE.COM", "SENDOS@GOOGLE.COM", "GABE@SQUIRRELSOUP.NET", "IOSBAND@GOOGLE.COM", "JAGAPIOU@GOOGLE.COM", "JZL@GOOGLE.COM", "AUDRUNAS@GOOGLE.COM" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n70 4.\n03 73\n2v 1\n[ cs\n.A I]\nseveral high profile successes in difficult control problems. However, these algorithms typically require a huge amount of data before they reach reasonable performance. In fact, their performance during learning can be extremely poor. This may be acceptable for a simulator, but it severely limits the applicability of deep RL to many real-world tasks, where the agent must learn in the real environment. In this paper we study a setting where the agent may access data from previous control of the system. We present an algorithm, Deep Q-learning from Demonstrations (DQfD), that leverages this data to massively accelerate the learning process even from relatively small amounts of demonstration data. DQfD works by combining temporal difference updates with large-margin classification of the demonstrator’s actions. We show that DQfD has better initial performance than Deep Q-Networks (DQN) on 40 of 42 Atari games and it receives more average rewards than DQN on 27 of 42 Atari games. We also demonstrate that DQfD learns faster than DQN even when given poor demonstration data."
    }, {
      "heading" : "1. Introduction",
      "text" : "Over the past few years, there have been a number of successes in learning policies for sequential decisionmaking problems and control. Notable examples in-\nclude deep model-free Q-learning for general Atari gameplaying (Mnih et al., 2015), end-to-end policy search for control of robot motors (Levine et al., 2016), model predictive control with embeddings (Watter et al., 2015), and strategic policies that combined with search led to defeating a top human expert at the game of Go (Silver et al., 2016). An important part of the success of these approaches has been to leverage the recent contributions to scalability and performance of deep learning (LeCun et al., 2015). The approach taken in (Mnih et al., 2015) builds a data set of previous experience using batch RL to train large convolutional neural networks in a supervised fashion from this data. As a result, the correlation in labels or values from state distribution bias is mitigated, leading to good (in many cases, super-human) control policies.\nIt still remains difficult to apply these algorithms to real world settings such as data centers, autonomous vehicles (Hester & Stone, 2013), helicopters (Abbeel et al., 2007), or recommendation systems (Shani et al., 2005). Typically these algorithms learn good control policies only after many millions of steps of very poor performance in simulation. This situation is acceptable when there is a perfectly accurate simulator; however, many real world problems do not come with such a simulator. Instead, in these situations, the agent must learn in the real domain with real consequences for its actions, which requires that the agent have good on-line performance from the start of learning. While accurate simulators are difficult to find, most of these problems have data of the system operating under a previous controller (either human or machine) that performs reasonably well. In this work, we make use of this demonstration data to pre-train the agent so that it can perform well\nin the task from the start of learning, and then continue improving from its own self-generated data. Enabling learning in this framework opens up the possibility of applying RL to many real world problems where demonstration data is common but simulators do not exist.\nWe propose a new deep reinforcement learning algorithm, Deep Q-learning from Demonstrations (DQfD), which leverages demonstration data to massively accelerate learning. DQfD initially pre-trains solely on the demonstration data using a combination of temporal difference (TD) and supervised losses. The supervised loss enables the algorithm to learn to imitate the demonstrator while the TD loss enables it to learn a valid value function from which it can continue learning with RL. After pre-training, the agent starts interacting with the domain with its learned policy. The agent keeps the demonstration data and its new self-generated data in separate replay buffers and each mini-batch it uses to update its network has a set proportion of data from each buffer. This algorithm out-performs pure reinforcement learning using Double DQN (van Hasselt et al., 2016) in average rewards on 27 of 42 Atari games, and out-performs pure imitation learning on 31 of 42 Atari games (Bellemare et al., 2013). DQfD learns to out-perform the demonstrator on six games. Finally, in a toy domain and one selected Atari game, DQfD receives more average rewards than both DQN and imitation despite being given bad demonstration data."
    }, {
      "heading" : "2. Background",
      "text" : "We adopt the standard Markov Decision Process (MDP) formalism for this work (Sutton & Barto, 1998). An MDP is defined by a tuple 〈S,A,R, T, γ〉, which consists of a set of states S, a set of actions A, a reward function R(s, a), a transition function T (s, a, s′) = P (s′|s, a), and a discount factor γ. In each state s ∈ S, the agent takes an action a ∈ A. Upon taking this action, the agent receives a reward R(s, a) and reaches a new state s′, determined from the probability distribution P (s′|s, a). A policy π specifies for each state which action the agent will take.\nThe goal of the agent is to find the policy π mapping states to actions that maximizes the expected discounted total reward over the agent’s lifetime. The value Qπ(s, a) of a given state-action pair (s, a) is an estimate of the expected future reward that can be obtained from (s, a)when following policy π. The optimal value functionQ∗(s, a) provides maximal values in all states and is determined by solving the Bellman equation:\nQ∗(s, a) = E\n[\nR(s, a) + γ ∑\ns′\nP (s′|s, a)max a′ Q∗(s′, a′)\n]\n.\n(1)\nThe optimal policy π is then:\nπ(s) = argmax a\nQ∗(s, a). (2)\nDQN (Mnih et al., 2015) approximates the value function Q(s, a) with a deep neural network that outputs a set of action values Q(s, ·; θ) for a given state input s, where θ are the parameters of the network. There are two key components of DQN that make this work. First, it uses a separate target network that is copied every τ steps from the regular network so that the target Q-values are more stable. Second, the agent adds all of its experiences to a replay buffer Dreplay , which is then sampled uniformly to perform updates on the network.\nDouble DQN (van Hasselt et al., 2016) adds double Qlearning, where the current network is used to calculate the argmax over next state values and the target network is used to get the value of that action. The learning target becomes:\nJDQ(Q) = ( R(s, a) + γQ(st+1, a max t+1 ; θ ′)−Q(s, a; θ) )2 ,\n(3)\nwhere θ′ are the parameters of the target network, and amaxt+1 = argmaxa Q(st+1, a; θ). Separating the value functions used for these two variables reduces the upward bias that is created with regular Q-learning updates, enabling RL-compatible pre-training."
    }, {
      "heading" : "3. Deep Q-Learning from Demonstrations",
      "text" : "In many real-world settings of reinforcement learning, we have access to data of the system being operated by its previous controller, but we do not have access to an accurate simulator of the system. Therefore, we want the agent to learn as much as possible from the demonstration data before running on the real system. The goal of the pretraining phase is to learn to imitate the demonstrator with a value function that satisfies the Bellman equation so that it can be updated with TD updates once the agent starts interacting with the environment. During this pre-training phase, the agent samples mini-batches from the demonstration data and updates the network by applying three losses: the double Q-learning loss, a supervised large margin classification loss, and an L2 regularization loss on the network weights and biases. The supervised loss is used for classification of the demonstrator’s actions, while the Q-learning loss ensures that the network satisfies the Bellman equation and can be used as a starting point for TD learning.\nThe supervised loss is critical for the pre-training to have any effect. Since the demonstration data is necessarily covering a narrow part of the state space and not taking all possible actions, many state-actions have never been taken and have no data to ground them to realistic values. If we were to pre-train the network with only Q-learning updates\ntowards the max value of the next state, the network would update towards the highest of these ungrounded variables and the network would propagate these values throughout the Q function. Adding the large margin classification loss grounds the values of the unseen actions to reasonable values, and makes the greedy policy induced by the value function imitate the demonstrator (Piot et al., 2014a):\nJE(Q) = max a∈A [Q(s, a) + l(s, aE, a)]−Q(s, aE) (4)\nwhere aE is the action the expert demonstrator took in state s and l(s, aE , a) is a margin function that is 0when a = aE and positive otherwise. This loss forces the values of the other actions to be at least a margin lower than the value of the demonstrator’s action. If the algorithm pre-trained with only this supervised loss, there would be nothing constraining the values between consecutive states and the Qnetwork would not satisfy the Bellman equation, which is required to improve the policy on-line with TD learning.\nWe also add an L2 regularization loss applied to the weights and biases of the network to help prevent it from overfitting on the relatively small demonstration dataset. The overall loss used to update the network is a combination of all three losses:\nJ(Q) = JDQ(Q) + λ1JE(Q) + λ2JL2(Q). (5)\nThe λ parameters control the weighting between the losses.\nOnce the pre-training phase is completed, ideally the agent will have learned a reasonable policy that is safe to run on the real system. In the next phase, the agent starts acting on the system, collecting self-generated data, and adding it to its agent replay buffer Dreplay . Data is added to the agent replay buffer until it is full, and then the agent starts over-writing old data in that buffer. Meanwhile the demonstration data is still maintained in a separate demonstration replay buffer Ddemo, which stays constant. Each minibatch contains n samples with the portion of demonstration data defined by parameter p = n demo\nndemo+nreplay . For the self-\ngenerated data, only the double Q-learning loss is applied, while for the demonstration data, both the supervised and double Q-learning losses are applied.\nOverall, Deep Q-learning from Demonstration (DQfD) differs from DQN in five key ways (examined in Section 4.2.2):\n• Pre-training: DQfD initially trains solely on the demonstration data before starting any interaction\nwith the environment. Pre-training happens with a combination of Q-learning loss and supervised loss so that the agent imitates the demonstrator while having a value function ready for TD learning. • Supervised losses: In addition to TD losses, a large margin supervised loss is applied that pushes the value\nof the demonstrator’s actions above the other action values (Piot et al., 2014a). • L2 Regularization losses: The algorithm also adds L2 regularization losses on the network weights to pre-\nvent over-fitting on the demonstration data.\n• Separate datasets: Demonstration data is stored in Ddemo and never overwritten, while self-generated data is stored in Dreplay and overwritten as usual. • Controlled data sampling: The proportion of demonstration data versus self-generated data is controlled in\neach mini-batch, with p = n demo\nndemo+nreplay .\nPseudo-code is sketched in Algorithm 1. The behavior policy is an ǫ-greedy policy with respect to the Qθ values.\nAlgorithm 1 Deep Q-learning from Demonstrations.\n1: Inputs: Ddemo: demonstration data set, Dreplay : empty, θ: weights for initial behavior network (ran-\ndom), θ′: weights for target network (random), τ : frequency at which to update target net, k: number of pretraining gradient updates\n2: for steps t ∈ {1, 2, . . . k} do 3: Sample a mini-batch of n transitions from Ddemo 4: Calculate loss J(Q) using target network (Eq. 5) 5: Perform a gradient descent step to update θ 6: end for 7: for steps t ∈ {1, 2, . . .} do 8: Sample action from behavior policy a ∼ πǫQθ\n9: Play action a and observe (s′, r). 10: Store tuple (s, a, r, s′) intoDreplay , overwriting old-\nest if over capacity\n11: Sample a mini-batch of n transitions from Ddemo ∪ Dreplay with a fraction p of the samples fromDdemo\n12: Calculate loss J(Q) using target network (Eq. 5) 13: Perform a gradient descent step to update θ 14: if tmod τ = 0 then θ′ ← θ end if 15: s ← s′ 16: end for"
    }, {
      "heading" : "4. Experimental Results",
      "text" : "For all of our experiments, we evaluated three different algorithms, each averaged across four trials:\n• Full DQfD algorithm • Double DQN learning without any demonstration data • Supervised imitation from demonstration data without any environment interaction\nFor DQfD, we initially performed informal parameter tuning on four Atari games (Bellemare et al., 2013). DQfD was run with the following parameters:\n• Pre-training with 1,000,000 mini-batch updates.\n• Expert Sampling Ratio p = 0.1. • Supervised loss weight λ1 = 1.0. • L2 regularization weight λ2 = 10 −5. • Expert margin l(s, aE, a) = 0.8 when a 6= aE . • ǫ-greedy exploration with ǫ = 0.01, which is the same used by Double DQN (van Hasselt et al., 2016).\nDouble DQN was run with the same exploration and L2 regularization (λ2) as DQfD, but no pre-training, no expert sampling, and no supervised loss.\nFor the supervised imitation comparison, we performed supervised classification of the demonstrator’s actions using a cross-entropy loss, with the same network architecture and L2 regularization used by DQfD and DQN. The imitation algorithm did not use any TD loss."
    }, {
      "heading" : "4.1. Catch",
      "text" : "We first evaluated the agent on a simple domain called Catch where it is easy to generate optimal demonstration data. In this domain, there is a falling ball and the agent must move across the bottom of the screen to catch the ball. The state is represented by a screen of 25x10 pixels, each valued 0 or 1. Only two pixels will be 1, the ball and the agent. At the start of each episode, the ball is in the top row in a random column, and the agent is in the bottom row in the center column. The ball falls one row each step. When the ball reaches the bottom row, the episode terminates with reward +1 if the agent is in the same column as the ball, and a reward of -1 otherwise. The agent has 10 actions: move left, move right, and eight actions that do nothing. Each time step, there is a 10% chance that the agent will move left regardless of the action it takes. The added stochasticity requires generalization of the demonstration data from the transitions that were seen, and the extra no-op actions make exploration in this task more difficult. As it only takes 10 steps for the ball to fall down, some columns the ball appears in are unreachable as they are more than 10 steps away from the center column where the agent starts. All three algorithms use a feed-forward network with one layer of 50 hidden units.\nWe generated demonstration data by training DQN on the task until it learned an optimal policy and then generating 1,000 transitions from this policy. Figure 1 shows results learning from this optimal demonstration, with 200 iterations of 250 steps each. DQfD starts out at similar performance to pure imitation learning and then improves from there. Meanwhile, DQN starts out at random performance and improves from there.\nWhen we switch from using optimal demonstrations to demonstration data with 10% random actions, the performance of imitation learning drops much more than the performance of DQfD. DQfD is able to generalize the poorer\ndemonstration data better using its three losses, and starts at similar initial performance with both datasets. As data collected from real-world experiments often comes from noisy sensor readings or unreliable human laborers, it is important for learning algorithms to be robust to imperfections in the demonstrations."
    }, {
      "heading" : "4.2. Atari",
      "text" : "We next evaluated DQfD on a much more challenging domain, the Arcade Learning Environment (ALE) (Bellemare et al., 2013). ALE is a set of Atari games that are a standard benchmark for DQN and contains many games on which humans still perform better than the best learning agents. The agent plays the Atari games from a down-sampled 84x84 image of the game screen that has been converted to greyscale, and the agent stacks four of these frames together as its state. The agent must output one of 18 possible actions for each game. The agent applies a discount factor of 0.99 and all of its actions are repeated for four Atari frames. We use the same convolutional network architecture used by DQN (Mnih et al., 2015).\nWe ran experiments on a subset of 42 Atari games. We had a human player play each game between three and twelve times. Each episode was played either until the game terminated or for 20 minutes. During game play, we logged the agent’s state, actions, rewards, and terminations. Table 1 shows the total number of transitions collected from the human demonstrator in each game, which ranges from 5574 to 75472 transitions per game. DQfD learns from a very small dataset compared to other similar work, as AlphaGo (Silver et al., 2016) learns from 30 million human\ntransitions, and DQN (Mnih et al., 2015) learns from over 50 million frames. DQfD’s smaller demonstration dataset makes it more difficult to learn a good representation without over-fitting. Table 1 lists the games we selected as well as the demonstrator’s best and worst performance on each game. Our human demonstrator is much better than DQN on some games (e.g. Private Eye, Pitfall), but much worse than DQN on many games (e.g. Breakout, Pong).\nWe found that in many of the games where the human player is better than DQN, it was due to DQN being trained with all rewards clipped to 1.0 (Mnih et al., 2015). For example, in Private Eye, DQN has no reason to go for actions that reward +25,000 versus actions that reward +10. To make the reward function used by the human demonstrator and the agent more consistent, we had the agent scale the rewards to have maximum value 1.0 by dividing by the maximal absolute reward it has seen. The agent tracks the maximum reward it has seen (in demonstration or self-generated data) over time and re-scales the rewards as it samples them from the replay buffer. For DQfD, the\nhighest one-step reward usually exists in the demonstration data set, and the rewards are scaled appropriately from the start. For DQN, this will make the rewards non-stationary. (van Hasselt et al., 2016) perform a similar adaptive rescaling of targets and show that the change causes DQN to improve on some games and perform worse on others. Overall, this reward scaling makes both algorithms use the same true reward function that the demonstrator uses."
    }, {
      "heading" : "4.2.1. MAIN RESULTS",
      "text" : "In real world tasks, the agent must perform well from its very first action. Therefore, we evaluate the agent on average on-line rewards, rather than just looking at the value of its final policy. Table 2 shows the average rewards achieved by each algorithm in every game over 200 iterations of one million Atari frames each. DQfD outperforms Double DQN in average rewards on 27 of the 42 games, and outperforms imitation learning on 31 of the 42 games.\nFigure 2 shows results on Hero, which was typical of many of the games that were run. Plots showing the results across all 42 games are included in the Appendix. DQfD starts out\nwith performance near that of the imitation policy, and continues to improve from there. Meanwhile, DQN starts out at random performance and slowly improves from there. Imitation learning is always a flat line because it will not improve with interactions in the environment.\nOne of the key components of DQfD is pre-training the agent so it can perform reasonably well from its very first action, which is critical for real world tasks. DQfD starts out with better performance than DQN on the very first iteration on all but two games. In addition, on 23 games, DQfD starts out with higher performance than pure imitation learning, as the addition of the TD loss helps the agent generalize the demonstration data better. In (Piot et al., 2014b), it was shown that adding a TD loss improved imitation performance even without any rewards in the domain.\nDQfD learns to out-perform the worst demonstration episode it was pre-trained on in 15 games and it learns to play better than the best demonstration episode in six of the games: Boxing, Breakout, Crazy Climber, Pong, Road Runner, and Up N Down. Pong, shown in Figure 3, is a particularly interesting case as DQfD performs better than DQN on the first 58 iterations even though the demonstration data it was pre-trained on was poor (the demonstrator did not win a single game). DQfD converges to a better final policy than DQN on 24 of the 42 games.\nOn some of the more difficult games such as Pitfall, DQfD actually gets worse when it starts interactingwith the game. In many of these games, the Atari reward function with a discount factor of 0.99 is malformed (e.g. in Pitfall the first positive reward is seven screens away and will be discounted close to 0). In all of the games, pure imitation\nlearning is worse than the demonstrator’s performance, and in most games imitation learning is not able to classify the expert’s action perfectly even on the states in the demonstration dataset."
    }, {
      "heading" : "4.2.2. DQFD ABLATION STUDIES",
      "text" : "Next, we looked at the impact of each of the five major differences between DQfD and DQN. There are quite a few games where DQN learns quite quickly, and DQfD gets a boost in initial performance and still learns as fast as DQN (e.g. Boxing, Pong, Freeway). We investigate the impact of the expert sampling ratio p on one of these games, Freeway, in Figure 4. As the sampling ratio is decreased and the agent sees more self-generated game interactions, it learns more quickly.\nRoad Runner (Figure 5) is another interesting game, where DQN learns a score exploit which is much different from how a human would play the game. DQfD starts out with better initial performance than DQN and continues learning from there, surpassing the best performance of the human in demonstrations, but not matching DQN’s final performance. We examined the impact of pre-training on Road Runner. The agent with pre-training receives more rewards on the first iterations, showing the clear advantage of pretraining.\nFigure 6 shows comparisons of the algorithm with each of the three losses applied to demonstration data removed, on the game of Private Eye. The removal of the TD loss in pre-training has an impact initially that goes away as the agent turns its action classification into a value function. The regularization loss impacts the starting performance\nof the agent and the policy it converges to. As expected, pre-training without any supervised loss results in a network trained towards ungrounded Q-learning targets and the agent is unable to recover from this poorly trained network. These results are representative of the results across the full set of games. With all three losses, DQfD learns a policy better than the best published results on this game.\nFigure 6 also compares using a large margin and cross entropy loss for the classification of the demonstrator’s actions. (Lakshminarayanan et al., 2016) use a cross entropy loss in their approach, but Figure 6 shows that it results in worse performance than using the large margin loss. This difference is likely because the cross-entropy loss is less compatible with the Q-learning loss as it pushes the action values as far apart as possible."
    }, {
      "heading" : "5. Related Work",
      "text" : "Imitation learning is primarily concerned with matching the performance of the demonstrator. One popular algorithm, DAGGER (Ross et al., 2011), iteratively produces new policies based on polling the expert policy outside its original state space, showing that this leads to no-regret over validation data in the online learning sense. DAGGER requires the expert to be available during training to provide additional feedback to the agent. Another popular paradigm is to setup a zero-sum game where the learner chooses a policy and the adversary chooses a reward function (Syed & Schapire, 2007; Syed et al., 2008; Ho & Ermon, 2016). Demonstrations have also been used for inverse optimal control in high-dimensional, continuous robotic control problems (Finn et al., 2016). However, these approaches only do imitation learning and do not allow for learning from task rewards.\nRecently, demonstration data has been shown to help in difficult exploration problems in RL (Subramanian et al., 2016). There has also been recent interest in this combined imitation and RL problem. For example, the HAT algorithm transfers knowledge directly from human policies (Taylor et al., 2011). Follow-ups to this work showed how expert advice or demonstrations can be used to shape rewards in the RL problem (Brys et al., 2015; Suay et al., 2016). A different approach is to shape the policy that is used to sample experience (Cederborg et al., 2015), or to use policy iteration from demonstrations (Kim et al., 2013; Chemali & Lezaric, 2015).\nOur algorithm works in a scenario where rewards are given by the environment used by the demonstrator. This framework was appropriately called Reinforcement Learning with Expert Demonstrations (RLED) in (Piot et al., 2014a) and is also evaluated in (Kim et al., 2013; Chemali & Lezaric, 2015). Our setup is similar to (Piot et al., 2014a) in that we combine TD and classification losses in a batch algorithm in a model-free setting; ours differs in that our agent is pre-trained on the demonstration data initially and the batch of self-generated data grows over time and is used as experience replay to train deep Q-networks. (Piot et al., 2014b) present interesting results showing that adding a TD loss to the supervised classification loss improves imitation learning even when there are no rewards.\nWhile our algorithm works on the RLED framework, we emphasize that the method we present is not restricted by this and could be combined with inverse RL methods (Ng & Russell, 2000; Abbeel & Ng, 2004;\nBabes-Vroman et al., 2011; Piot et al., 2013) to produce reward functions that are then used in place of task rewards.\nAnother work that is similarly motivated to ours is (Schaal, 1996). This work is focused on real world learning on robots, and thus is also concerned with on-line performance. Similar to our work, they pre-train the agent with demonstration data before letting it interact with the task. However, they do not use supervised learning to pre-train their algorithm, and are only able to find one case where pre-training helps learning on Cart-Pole.\nAlphaGo (Silver et al., 2016) takes a similar approach to our work in pre-training from demonstration data before interacting with the real task. AlphaGo first trains a policy network from a dataset of 30 million expert actions, using supervised learning to predict the actions taken by experts. It then uses this as a starting point to apply policy gradient updates during self-play, combined with planning rollouts. Here, we do not have a model available for planning, so we focus on the model-free Q-learning case.\nHuman Experience Replay (Hosu & Rebedea, 2016) is an algorithm in which the agent samples from a replay buffer that is mixed between agent and demonstration data, similar to our approach. Gains were only slightly better than a random agent, and were surpassed by their alternative approach, Human Checkpoint Replay, which requires the ability to set the state of the environment. While their algorithm is similar in that it samples from two buffers, it does not pre-train the agent or use a supervised loss. Our results show higher scores over a larger variety of games, without requiring full access to the environment. (Lipton et al., 2016) show promising results with initializing the DQN\nagent’s replay buffer with demonstration data on dialog tasks, but they do not pre-train the agent for good initial performance.\nThe work that most closely relates to ours is a workshop paper (Lakshminarayanan et al., 2016). They are also combining TD and classification losses in a deep Q-learning setup. They use a trained DQN agent to generate their demonstration data, which on most games is better than human data. It also guarantees that the policy used by the demonstrator can be represented by the apprenticeship agent as they are both using the same state input and network architecture. They use a cross-entropy classification loss rather than the large margin loss DQfD uses and they do not pre-train the agent to perform well from its first interactions with the environment. Our experiments in Section 4.2.2 show that both of these differences are crucial for the agent. In particular, the cross-entropy loss does not combine well with the double Q-learning loss."
    }, {
      "heading" : "6. Discussion",
      "text" : "The learning framework that we have presented in this paper is one that is very common in real world problems such as controlling data centers, autonomous vehicles (Hester & Stone, 2013), or recommendation systems (Shani et al., 2005). In these problems, typically there is no accurate simulator available, and learning must be performed on the real system with real consequences. However, there is often data available of the system being operated by a previous controller. We have presented a new algorithm called DQfD that takes advantage of this data to accelerate learning on the real system. It first pretrains solely on demonstration data, using a combination of TD and supervised losses so that it has a reasonable policy that is a good starting point for learning in the task. Once it starts interacting with the task, it continues learning by sampling from both its self-generated data as well as the demonstration data.\nWe have shown that this algorithm has better initial performance than DQN on 40 of 42 Atari games, and outperforms it in the average on-line rewards it receives on 27 of 42 Atari games. In addition, DQfD learns to perform better than its best demonstration episode on six of the games, and outperforms both DQN and imitation learning even when given intentionally poor demonstration data. DQfD’s ability to performwell initially and continue learning from there enables RL on a wide range of real world systems for which approaches like DQN were not previously applicable because they had to learn from scratch.\nThese results may seem obvious given that DQfD has access to privileged data, but the rewards and demonstrations are mathematically dissimilar training signals, and naive\napproaches to combining them can have disastrous results. We argue that the combination of all three losses during pre-training is critical for the agent to learn a single coherent representation that is not destroyed by the switch in training signals after pre-training.\nThere are many reasons why learning from human data is difficult. In most games, imitation learning is unable to perfectly classify the demonstrator’s actions even on the demonstration dataset. The human demonstrator is playing the game in a way that is impossible for the agent to represent with its state observations. The human may also be executing a high rewarding policy that is much different from the policy DQN would learn. In future work, we plan to measure these differences between demonstration and agent data to inform approaches that derive more value from the demonstrations. Another future direction is to apply these concepts to domains with continuous actions, where the classification loss becomes a regression loss."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The authors would like to thank Keith Anderson, Chris Apps, Ben Coppin, Nando de Freitas, Chris Gamble, Thore Graepel, Georg Ostrovski, Cosmin Paduraru, Jack Rae, Amir Sadik, Jon Scholz, David Silver, Tom Stepleton, Ziyu Wang, and many others at DeepMind for insightful discussions, code contributions, and other efforts."
    } ],
    "references" : [ {
      "title" : "Apprenticeship learning via inverse reinforcement learning",
      "author" : [ "P. Abbeel", "A.Y. Ng" ],
      "venue" : "In International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Abbeel and Ng,? \\Q2004\\E",
      "shortCiteRegEx" : "Abbeel and Ng",
      "year" : 2004
    }, {
      "title" : "An application of reinforcement learning to aerobatic helicopter flight",
      "author" : [ "P. Abbeel", "A. Coates", "M. Quigley", "A.Y. Ng" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Abbeel et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Abbeel et al\\.",
      "year" : 2007
    }, {
      "title" : "Apprenticeship learning about multiple intentions",
      "author" : [ "M. Babes-Vroman", "V. Marivate", "K. Subramanian", "M. Littman" ],
      "venue" : "In International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Babes.Vroman et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Babes.Vroman et al\\.",
      "year" : 2011
    }, {
      "title" : "The arcade learning environment: An evaluation platform for general agents",
      "author" : [ "M.G. Bellemare", "Y. Naddaf", "J. Veness", "M. Bowling" ],
      "venue" : "Journal of Artifificial Intelligence Research (JAIR),",
      "citeRegEx" : "Bellemare et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Bellemare et al\\.",
      "year" : 2013
    }, {
      "title" : "Reinforcement learning from demonstration through shaping",
      "author" : [ "T. Brys", "A. Harutyunyan", "H.B. Suay", "S. Chernova", "M.E. Taylor", "A. Nowé" ],
      "venue" : "In International Joint Conference on Artificial Intelligence (IJCAI),",
      "citeRegEx" : "Brys et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Brys et al\\.",
      "year" : 2015
    }, {
      "title" : "Policy shaping with human teachers",
      "author" : [ "T. Cederborg", "I. Grover", "C.L. Isbell", "A.L. Thomaz" ],
      "venue" : "In International Joint Conference on Artificial Intelligence",
      "citeRegEx" : "Cederborg et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Cederborg et al\\.",
      "year" : 2015
    }, {
      "title" : "Direct policy iteration from demonstrations",
      "author" : [ "J. Chemali", "A. Lezaric" ],
      "venue" : "In International Joint Conference on Artificial Intelligence (IJCAI),",
      "citeRegEx" : "Chemali and Lezaric,? \\Q2015\\E",
      "shortCiteRegEx" : "Chemali and Lezaric",
      "year" : 2015
    }, {
      "title" : "Guided cost learning: Deep inverse optimal control via policy optimization",
      "author" : [ "C. Finn", "S. Levine", "P. Abbeel" ],
      "venue" : "In International Conference on Machine Learing (ICML),",
      "citeRegEx" : "Finn et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Finn et al\\.",
      "year" : 2016
    }, {
      "title" : "TEXPLORE: Real-time sampleefficient reinforcement learning for robots",
      "author" : [ "Hester", "Todd", "Stone", "Peter" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Hester et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Hester et al\\.",
      "year" : 2013
    }, {
      "title" : "Generative adversarial imitation learning",
      "author" : [ "J. Ho", "S. Ermon" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Ho and Ermon,? \\Q2016\\E",
      "shortCiteRegEx" : "Ho and Ermon",
      "year" : 2016
    }, {
      "title" : "Playing atari games with deep reinforcement learning and human checkpoint replay",
      "author" : [ "Hosu", "I.-A", "T. Rebedea" ],
      "venue" : "In ECAI Workshop on Evaluating General Purpose AI,",
      "citeRegEx" : "Hosu et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Hosu et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning from limited demonstrations",
      "author" : [ "B. Kim", "A. Farahmand", "J. Pineau", "D. Precup" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Kim et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2013
    }, {
      "title" : "Reinforcement learning with few expert demonstrations",
      "author" : [ "Lakshminarayanan", "Aravind S", "Ozair", "Sherjil", "Bengio", "Yoshua" ],
      "venue" : "In NIPS Workshop on Deep Learning for Action and Interaction,",
      "citeRegEx" : "Lakshminarayanan et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Lakshminarayanan et al\\.",
      "year" : 2016
    }, {
      "title" : "End-to-end training of deep visuomotor policies",
      "author" : [ "Levine", "Sergey", "Finn", "Chelsea", "Darrell", "Trevor", "Abbeel", "Pieter" ],
      "venue" : "Journal of Machine Learning (JMLR),",
      "citeRegEx" : "Levine et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Levine et al\\.",
      "year" : 2016
    }, {
      "title" : "Efficient exploration for dialog policy learning with deep BBQ network & replay buffer spiking",
      "author" : [ "Lipton", "Zachary C", "Gao", "Jianfeng", "Li", "Lihong", "Xiujun", "Ahmed", "Faisal", "Deng" ],
      "venue" : null,
      "citeRegEx" : "Lipton et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Lipton et al\\.",
      "year" : 2016
    }, {
      "title" : "Algorithms for inverse reinforcement learning",
      "author" : [ "A.Y. Ng", "S.J. Russell" ],
      "venue" : "In International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Ng and Russell,? \\Q2000\\E",
      "shortCiteRegEx" : "Ng and Russell",
      "year" : 2000
    }, {
      "title" : "Learning from demonstrations: Is it worth estimating a reward function",
      "author" : [ "B. Piot", "M. Geist", "O. Pietquin" ],
      "venue" : "In European Conference on Machine Learning (ECML),",
      "citeRegEx" : "Piot et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Piot et al\\.",
      "year" : 2013
    }, {
      "title" : "Boosted bellman residual minimization handling expert demonstrations",
      "author" : [ "B. Piot", "M. Geist", "O. Pietquin" ],
      "venue" : "In European Conference on Machine Learning (ECML),",
      "citeRegEx" : "Piot et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Piot et al\\.",
      "year" : 2014
    }, {
      "title" : "Boosted and Reward-regularized Classification for Apprenticeship Learning",
      "author" : [ "Piot", "Bilal", "Geist", "Matthieu", "Pietquin", "Olivier" ],
      "venue" : "In International Conference on Autonomous Agents and Multiagent Systems (AAMAS),",
      "citeRegEx" : "Piot et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Piot et al\\.",
      "year" : 2014
    }, {
      "title" : "A reduction of imitation learning and structured prediction to no-regret online learning",
      "author" : [ "S. Ross", "G.J. Gordon", "J.A. Bagnell" ],
      "venue" : "In International Conference on Artificial Intelligence and Statistics (AISTATS),",
      "citeRegEx" : "Ross et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Ross et al\\.",
      "year" : 2011
    }, {
      "title" : "Learning from demonstration",
      "author" : [ "Schaal", "Stefan" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Schaal and Stefan.,? \\Q1996\\E",
      "shortCiteRegEx" : "Schaal and Stefan.",
      "year" : 1996
    }, {
      "title" : "An mdpbased recommender system",
      "author" : [ "Shani", "Guy", "Heckerman", "David", "Brafman", "Ronen I" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Shani et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Shani et al\\.",
      "year" : 2005
    }, {
      "title" : "Mastering the game of Go with deep neural networks and tree",
      "author" : [ "Madeleine", "Kavukcuoglu", "Koray", "Graepel", "Thore", "Hassabis", "Demis" ],
      "venue" : "search. Nature,",
      "citeRegEx" : "Madeleine et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Madeleine et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning from demonstration for shaping through inverse reinforcement learning",
      "author" : [ "Suay", "Halit Bener", "Brys", "Tim", "Taylor", "Matthew E", "Chernova", "Sonia" ],
      "venue" : "In International Conference on Autonomous Agents and Multiagent Systems (AAMAS),",
      "citeRegEx" : "Suay et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Suay et al\\.",
      "year" : 2016
    }, {
      "title" : "Exploration from demonstration for interactive reinforcement learning",
      "author" : [ "K. Subramanian", "Jr.", "C.L. Isbell", "A. Thomaz" ],
      "venue" : "In International Conference on Autonomous Agents and Multiagent Systems (AAMAS),",
      "citeRegEx" : "Subramanian et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Subramanian et al\\.",
      "year" : 2016
    }, {
      "title" : "Introduction to reinforcement learning",
      "author" : [ "Sutton", "Richard S", "Barto", "Andrew G" ],
      "venue" : null,
      "citeRegEx" : "Sutton et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Sutton et al\\.",
      "year" : 1998
    }, {
      "title" : "A game-theoretic approach to apprenticeship learning",
      "author" : [ "U. Syed", "R.E. Schapire" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Syed and Schapire,? \\Q2007\\E",
      "shortCiteRegEx" : "Syed and Schapire",
      "year" : 2007
    }, {
      "title" : "Apprenticeship learning using linear programming",
      "author" : [ "U. Syed", "M. Bowling", "R.E. Schapire" ],
      "venue" : "In International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Syed et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Syed et al\\.",
      "year" : 2008
    }, {
      "title" : "Integrating reinforcement learning with human demonstrations of varying ability",
      "author" : [ "M.E. Taylor", "H.B. Suay", "S. Chernova" ],
      "venue" : "In International Conference on Autonomous Agents and Multiagent Systems (AAMAS),",
      "citeRegEx" : "Taylor et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Taylor et al\\.",
      "year" : 2011
    }, {
      "title" : "Deep reinforcement learning with double Q-learning",
      "author" : [ "van Hasselt", "Hado", "Guez", "Arthur", "Silver", "David" ],
      "venue" : "In AAAI Conference on Artificial Intelligence (AAAI),",
      "citeRegEx" : "Hasselt et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Hasselt et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning values across many orders of magnitude",
      "author" : [ "van Hasselt", "Hado P", "Guez", "Arthur", "Hessel", "Matteo", "Mnih", "Volodymyr", "Silver", "David" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Hasselt et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Hasselt et al\\.",
      "year" : 2016
    }, {
      "title" : "Embed to control: A locally linear latent dynamics model for control from raw images",
      "author" : [ "M. Watter", "J.T. Springenberg", "J. Boedecker", "M.A. Riedmiller" ],
      "venue" : "In Advances in Neural Information Processing (NIPS),",
      "citeRegEx" : "Watter et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Watter et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 13,
      "context" : ", 2015), end-to-end policy search for control of robot motors (Levine et al., 2016), model predictive control with embeddings (Watter et al.",
      "startOffset" : 62,
      "endOffset" : 83
    }, {
      "referenceID" : 31,
      "context" : ", 2016), model predictive control with embeddings (Watter et al., 2015), and strategic policies that combined with search led to defeating a top human expert at the game of Go (Silver et al.",
      "startOffset" : 50,
      "endOffset" : 71
    }, {
      "referenceID" : 1,
      "context" : "It still remains difficult to apply these algorithms to real world settings such as data centers, autonomous vehicles (Hester & Stone, 2013), helicopters (Abbeel et al., 2007), or recommendation systems (Shani et al.",
      "startOffset" : 154,
      "endOffset" : 175
    }, {
      "referenceID" : 21,
      "context" : ", 2007), or recommendation systems (Shani et al., 2005).",
      "startOffset" : 35,
      "endOffset" : 55
    }, {
      "referenceID" : 3,
      "context" : ", 2016) in average rewards on 27 of 42 Atari games, and out-performs pure imitation learning on 31 of 42 Atari games (Bellemare et al., 2013).",
      "startOffset" : 117,
      "endOffset" : 141
    }, {
      "referenceID" : 3,
      "context" : "For DQfD, we initially performed informal parameter tuning on four Atari games (Bellemare et al., 2013).",
      "startOffset" : 79,
      "endOffset" : 103
    }, {
      "referenceID" : 3,
      "context" : "We next evaluated DQfD on a much more challenging domain, the Arcade Learning Environment (ALE) (Bellemare et al., 2013).",
      "startOffset" : 96,
      "endOffset" : 120
    }, {
      "referenceID" : 12,
      "context" : "(Lakshminarayanan et al., 2016) use a cross entropy loss in their approach, but Figure 6 shows that it results in worse performance than using the large margin loss.",
      "startOffset" : 0,
      "endOffset" : 31
    }, {
      "referenceID" : 19,
      "context" : "One popular algorithm, DAGGER (Ross et al., 2011), iteratively produces new policies based on polling the expert policy outside its original state space, showing that this leads to no-regret over validation data in the online learning sense.",
      "startOffset" : 30,
      "endOffset" : 49
    }, {
      "referenceID" : 27,
      "context" : "Another popular paradigm is to setup a zero-sum game where the learner chooses a policy and the adversary chooses a reward function (Syed & Schapire, 2007; Syed et al., 2008; Ho & Ermon, 2016).",
      "startOffset" : 132,
      "endOffset" : 192
    }, {
      "referenceID" : 7,
      "context" : "Demonstrations have also been used for inverse optimal control in high-dimensional, continuous robotic control problems (Finn et al., 2016).",
      "startOffset" : 120,
      "endOffset" : 139
    }, {
      "referenceID" : 24,
      "context" : "Recently, demonstration data has been shown to help in difficult exploration problems in RL (Subramanian et al., 2016).",
      "startOffset" : 92,
      "endOffset" : 118
    }, {
      "referenceID" : 28,
      "context" : "For example, the HAT algorithm transfers knowledge directly from human policies (Taylor et al., 2011).",
      "startOffset" : 80,
      "endOffset" : 101
    }, {
      "referenceID" : 4,
      "context" : "Follow-ups to this work showed how expert advice or demonstrations can be used to shape rewards in the RL problem (Brys et al., 2015; Suay et al., 2016).",
      "startOffset" : 114,
      "endOffset" : 152
    }, {
      "referenceID" : 23,
      "context" : "Follow-ups to this work showed how expert advice or demonstrations can be used to shape rewards in the RL problem (Brys et al., 2015; Suay et al., 2016).",
      "startOffset" : 114,
      "endOffset" : 152
    }, {
      "referenceID" : 5,
      "context" : "A different approach is to shape the policy that is used to sample experience (Cederborg et al., 2015), or to use policy iteration from demonstrations (Kim et al.",
      "startOffset" : 78,
      "endOffset" : 102
    }, {
      "referenceID" : 11,
      "context" : ", 2015), or to use policy iteration from demonstrations (Kim et al., 2013; Chemali & Lezaric, 2015).",
      "startOffset" : 56,
      "endOffset" : 99
    }, {
      "referenceID" : 11,
      "context" : ", 2014a) and is also evaluated in (Kim et al., 2013; Chemali & Lezaric, 2015).",
      "startOffset" : 34,
      "endOffset" : 77
    }, {
      "referenceID" : 14,
      "context" : "(Lipton et al., 2016) show promising results with initializing the DQN agent’s replay buffer with demonstration data on dialog tasks, but they do not pre-train the agent for good initial performance.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 12,
      "context" : "The work that most closely relates to ours is a workshop paper (Lakshminarayanan et al., 2016).",
      "startOffset" : 63,
      "endOffset" : 94
    }, {
      "referenceID" : 21,
      "context" : "The learning framework that we have presented in this paper is one that is very common in real world problems such as controlling data centers, autonomous vehicles (Hester & Stone, 2013), or recommendation systems (Shani et al., 2005).",
      "startOffset" : 214,
      "endOffset" : 234
    } ],
    "year" : 2017,
    "abstractText" : "Deep reinforcement learning (RL) has achieved several high profile successes in difficult control problems. However, these algorithms typically require a huge amount of data before they reach reasonable performance. In fact, their performance during learning can be extremely poor. This may be acceptable for a simulator, but it severely limits the applicability of deep RL to many real-world tasks, where the agent must learn in the real environment. In this paper we study a setting where the agent may access data from previous control of the system. We present an algorithm, Deep Q-learning from Demonstrations (DQfD), that leverages this data to massively accelerate the learning process even from relatively small amounts of demonstration data. DQfD works by combining temporal difference updates with large-margin classification of the demonstrator’s actions. We show that DQfD has better initial performance than Deep Q-Networks (DQN) on 40 of 42 Atari games and it receives more average rewards than DQN on 27 of 42 Atari games. We also demonstrate that DQfD learns faster than DQN even when given poor demonstration data.",
    "creator" : "LaTeX with hyperref package"
  }
}