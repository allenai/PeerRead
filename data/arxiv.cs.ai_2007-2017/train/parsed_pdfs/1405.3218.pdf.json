{
  "name" : "1405.3218.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Lifted Variable Elimination for Probabilistic Logic Programming",
    "authors" : [ "ELENA BELLODI", "EVELINA LAMMA", "FABRIZIO RIGUZZI", "VITOR SANTOS COSTA", "RICCARDO ZESE" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "KEYWORDS: Probabilistic Logic Programming, Lifted Inference, Variable Elimination, Distribution Semantics, ProbLog, Statistical Relational Artificial Intelligence"
    }, {
      "heading" : "1 Introduction",
      "text" : "Over the last years, there has been increasing interest in models that combine first-order logic and probability, both for domain modeling under uncertainty, and for efficiently performing inference and learning (Getoor and Taskar 2007; De Raedt et al. 2008). Probabilistic Logic Programming (PLP) has recently received an increasing attention for its ability to incorporate probability in logic programming. Among the various proposals, the one based on the distribution semantics (Sato 1995) has gained popularity as the basis of languages such as Probabilistic Horn Abduction (Poole 1993), PRISM (Sato\nar X\niv :1\n40 5.\n32 18\nv4 [\ncs .A\n1995), Independent Choice Logic (Poole 1997), Logic Programs with Annotated Disjunctions (Vennekens et al. 2004), and ProbLog (De Raedt et al. 2007).\nNonetheless, research in Probabilistic Logic Languages has made it very clear that it is crucial to design models that can support efficient inference, while preserving intensional, and declarative modeling. Lifted inference (Poole 2003; de Salvo Braz et al. 2005; Milch et al. 2008; Van den Broeck et al. 2011) is one of the major advances in this respect. The idea is to take advantage of the regularities in structured models to decrease the number of operations. Originally, the idea was proposed as an extension of variable elimination (ve for short in the following). Work on lifting ve started with (Poole 2003). Lifted VE exploits the symmetries present in first-order probabilistic models, so that it can apply the same principles behind ve to solve a probabilistic query without grounding the model.\nMost work in probabilistic inference compute statistics from a sum of products representation, where each element is named a factor. Lifted inference generates templates, named parametric factors or parfactors, which stand for a set of similar factors found in the inference process, thus delaying as much as possible the use of fully instantiated factors.\nIn (Gomes and Costa 2012), the Prolog Factor Language (PFL, for short) was proposed as a Prolog extension to support probabilistic reasoning with parfactors. PFL exploits the state-of-art algorithm GC-FOVE (Taghipour et al. 2013), which redefines the operations described in(Milch et al. 2008) to be correct for whatever constraint representation is being used. This decoupling of the lifted inference algorithm from the constraint representation mechanism allows any constraint language that is closed under these operators to be plugged into the algorithm to obtain an inference system. In fact, the lifted ve algorithm of (Gomes and Costa 2012) represents the adaptation of GC-FOVE to the PFL constraints.\nIn this work, we move further towards exploiting efficient inference via lifted ve for PLP Languages under the distribution semantics. To support reasoning compliant with the distribution semantics, we introduce two novel operators (named heterogeneous lifted multiplication and sum) in the PFL, and modify the GC-FOVE algorithm for computing them. We name LP2 (for Lifted Probabilistic Logic Programming) the resulting system. An experimental comparison between LP2 and ProbLog2 (Fierens et al. 2014) and PITA (Riguzzi and Swift 2011) shows that inference time increases linearly with the number of individuals of the program domain for LP2, rather than exponentially as with ProbLog2 and PITA.\nThis is an exciting development towards the goal of preserving the declarativeness and\nconciseness of Probabilistic Logic Languages, while extremely gaining in performances.\nThe paper is organized as follows. Section 2 introduces preliminaries regarding ProbLog, PFL, Causal Independence Variable Elimination, and GC-FOVE. Section 3 discusses the translation of ProbLog into the extended PFL. Section 4 presents the new operators introduced in GC-FOVE. Section 5 reports the experiments performed and Section 6 concludes the paper."
    }, {
      "heading" : "2 Preliminaries",
      "text" : ""
    }, {
      "heading" : "2.1 ProbLog",
      "text" : "ProbLog (De Raedt et al. 2007) is a Probabilistic Logic Programming (PLP) language. A ProbLog program consists of a set of ground probabilistic facts plus a definite logic program, i.e. a set of rules. A ground probabilistic fact, written p :: f , is a ground fact f annotated with a number p such that 0 ≤ p ≤ 1. An atom that unifies with a ground probabilistic fact is called a probabilistic atom, while an atom that unifies with the head of some rule in the logic program is called a derived atom.\nIf a set of probabilistic facts has the same probability p, it can be defined intensionally through the syntax p :: f(X1, X2, . . . , Xn) : −B, where f(X1, X2, . . . , Xn) is the signature of the set, and B is a conjunction of non-probabilistic goals, as shown in Example 1. Such rules are range-restricted: all variables in the head of a rule should also appear in a positive literal in the body.\nExample 1 (Running example) Here we present an example inspired by the workshop attributes problem of (Milch et al. 2008). The ProbLog program models the scenario in which a workshop is being organized and a number of people have been invited. series indicates whether the workshop is successful enough to start a series of related meetings while attends(P) indicates whether person P will attend the workshop.\nseries :- s. series :- attends(P). attends(P) :- at(P,A). 0.1::s. 0.3::at(P,A) :- person(P), attribute(A).\nThe first two rules define when the workshop becomes a series: either because of its own merits or because people attend. The third rule states that whether a person attends the workshop depends on its attributes (location, date, fame of the organizers, etc).\nThe probabilistic fact s represents the merit of the workshop. The probabilistic fact at(P,A) represents whether person P attends because of attribute A. Notice that the last statement corresponds to a set of ground probabilistic facts, one for each person P and attribute A. For brevity we do not show the (non-probabilistic) facts describing person/1 and attribute/1 predicates.\nA ProbLog program specifies a probability distribution over normal logic programs. In this work, we consider the semantics in the case of no function symbols to be restricted to finite programs, and assume all worlds have a two-valued well-founded model.\nFor each ground probabilistic fact pi :: fi, an atomic choice specifies whether to include f in a world (with probability pi) or not (with probability 1−pi). A total choice C is a set of atomic choices, one for each ground probabilistic fact. These choices are assumed to be independent, hence the probability of a total choice is the product of the probabilities of the individual atomic choices, P (C) = Πipi. A total choice C also identifies a normal logic program W = F ∪R called a world, where F is the set of facts to be included according to C andR denotes the rules in the ProbLog program. LetW be the set of all possible worlds. The probability of a world is equal to the probability of its total choice. The conditional probability of a query (a ground atom) Q given a world W P (Q|W ) is 1 if the Q is true in the well-founded model of W and 0 otherwise. The probability of a query can therefore be\nobtained as P (Q) = ∑ W∈W P (Q,W ) = ∑ W∈W P (Q|W )P (W ) = ∑ W∈W:W |=Q P (W )."
    }, {
      "heading" : "2.2 The Prolog Factor Language",
      "text" : "Most graphical models provide a concise representation of a joint distribution by encoding it as a set of factors. The probability of a set of variables X taking the value x can be expressed as product of n factors if:\nP (X = x) =\n∏ i=1,...,n φi(xi)\nZ\nwhere xi is a sub-vector of x that depends on the i-th factor and Z is a normalization constant (i.e. Z = ∑\nx ∏ i=1,...,n φi(xi)). Bayesian networks are an example where there\nis a factor for each variable that is a function of the variable Xi and its parents Xj . . . Xk, such that φ(Xi, Xj . . . Xk) = P (Xi|Xj . . . Xk) and Z = 1. As progress has been made on managing large networks, it has become clear that often the same factor appears repeatedly in the network, thus suggesting the use of templates generalizing individual factors, or parametric factors (Kisynski and Poole 2009a).\nThe Prolog Factor Language (PFL) (Gomes and Costa 2012) extends Prolog to support probabilistic reasoning with parametric factors or parfactors. The PFL syntax for a factor is Type F ; φ ; C. Type refers to the type of the network over which the parfactor is defined (bayes for directed networks or markov for undirected ones); F is a sequence of Prolog terms that define sets of random variables under the constraints in C. The set of all logical variables in F is named L. C is a list of Prolog goals that impose bindings on the logical variables in L (the successful substitutions for the goals in C are the valid values for the variables in L). φ is the table defining the factor in the form of a list of real values. By default all random variables are boolean but a different domain may be defined. An example of a factor is series,attends(P);[0.51,0.49,0.49,0.51];[person(P)]: it has the Boolean random variables series and attends(P) as arguments, [0.51,0.49,0.49,0.51] as table and [person(P)] as constraints. The semantics of a PFL program is given by the set of factors obtained by grounding parfactors: each parfactor stands for the set of its grounding obtained by replacing variables of L with the values allowed by the constraints in C. The set of ground factors define a factorization of the joint probability distribution over all random variables.\nExample 2 (PFL Program) A version of the workshop attributes problem presented in Example 1 can be modeled by a PFL program such as\nbayes attends(P), at(P,A) ; [0.7, 0.3, 0.3, 0.7] ; [person(P),attribute(A)]. bayes series, attends(P) ; [0.51, 0.49, 0.49, 0.51] ; [person(P)]."
    }, {
      "heading" : "2.3 Variable Elimination and Causal Independence",
      "text" : "Quite often we want to find out the probability distribution of a set of random variables X given that we know the values, or have evidence y, on a set of variables Y, where X is often a single variable X. Variable Elimination (ve) (Zhang and Poole 1996) is an algorithm for computing this posterior probability in factorized joint probability distributions. The key idea is to eliminate the random variables from a set of factors one by one until only the query variable X remains. To do so ve eliminates a variable V by first multiplying all\nthe factors that include V into a single factor; V can then be discarded through summing it out from the newly constructed factor.\nMore formally, suppose φ1(X1 . . . Xi, Y1 . . . Yj) and φ2(Y1 . . . Yj , Z1 . . . Zk) are factors. The product (φ1×φ2)(x1 . . . xi, y1 . . . yj , z1 . . . zk) is simply φ1(x1 . . . xi, y1 . . . yj , )×φ2(y1 . . . yj , z1 . . . zk) for every value of x1 . . . xi, y1 . . . yj , z1 . . . zk. To eliminate a variable X1 from the factors φ(x1 . . . xi) one observes that the cases for X1 are mutually exclusive,\nthus φ′(x2 . . . xi) is simply ( ∑ x1 φ)(x2 . . . xi) = φ(α1, x2 . . . xi) + . . . + φ(αm, x2 . . . xi), where α1, . . . , αm are the possible values of X1.\nThe full ve algorithm takes as input a set of factors F , an elimination order ρ, a set of query variables X and a list y of observed values. First, it sets the observed variables in all factors to their corresponding observed values. Then it repeatedly selects the first variable Z from the elimination order ρ and it calls sum-out on F and Z, until ρ becomes empty. In the final step, it multiplies together the factors of F obtaining a new factor γ that is normalized as γ(x)/ ∑ x′ γ(x ′) to give the posterior probability.\nNoisy OR-Gates Bayesian networks take advantage of conditional independence between variables to reduce the size of the representation. Causal independence (Zhang and Poole 1996) goes one step further and looks at independence conditioned on values of the random variables. One important example is the noisy OR-gate, where we have a Boolean variable X with parents Y, and ideally X should be true if any of the Yi is true. In practice, each parent Yi has a noisy inhibitor that independently blocks or activates Yi, so X is true if either any of the causes Yi holds true and is not inhibited.\nA noisy OR can be expressed as a factor φ. In fact, it can be also expressed as a combination of factors by introducing intermediate variables that represent the effect of each cause given the inhibitor. For example, if X has two causes Y1 and Y2, we can introduce a variable X ′ to account for the effect of Y1 and X ′′ for Y2, and the factor φ(Y1, Y2, X) can be expressed as\nφ(y1, y2, x) = ∑\nx′∨x′′=x ψ(y1, x\n′)γ(y2, x ′′) (1)\nwhere the summation is over all values x′ and x′′ of X ′ and X ′′ whose disjunction is equal to x. The X variable is called convergent as it is where independent contributions from different sources are collected and combined. Non-convergent variables will be called regular variables. The noisy OR thus allows for a O(n) representation of a conditional probability table with n parents. Unfortunately, straightforward use of ve for inference would lead to construct O(2n) tables. A modified algorithm, called ve1 (Zhang and Poole 1996), combines factors through a new operator ⊗, that generalizes formula (1) as follows. Let φ and ψ be two factors that share convergent variables E1 . . . Ek, let A be the list of regular variables that appear in both φ and ψ, let B1 (B2) be the list of variables appearing only in φ (ψ). The combination φ⊗ ψ is given by\nφ⊗ ψ(E1 = α1, . . . , Ek = αk,A,B1,B2) =∑ α11∨α12=α1 . . . ∑ αk1∨αk2=αk φ(E1 = α11, . . . , Ek = αk1,A,B1)ψ(E1 = α12, . . . , Ek = αk2,A,B2) (2)\nFactors containing convergent variables are called heterogeneous while the remaining factors are called homogeneous. Heterogeneous factors sharing convergent variables must be combined with ⊗ that we call heterogeneous multiplication.\nAlgorithm ve1 exploits causal independence by keeping two lists of factors instead of one: a list of homogeneous factors F1 and a list of heterogeneous factors F2. Procedure sum-out is replaced by sum-out1 that takes as input F1 and F2 and a variable Z to be eliminated. First, all the factors containing Z are removed from F1 and combined with multiplication to obtain factor φ. Then all the factors containing Z are removed from F2 and combined with heterogeneous multiplication obtaining ψ. If there are no such factors\nset ψ = nil. In the latter case, sum-out1 adds the new (homogeneous) factor ∑ z φ to\nF1 otherwise it adds the new (heterogeneous) factor ∑ z φψ to F2. Procedure ve1 is the same as ve with sum-out replaced by sum-out1 and with the difference that two sets of factors are maintained instead of one.\nThe ⊗ operator assumes that the convergent variables are independent given the regular variables. This can be ensured by deputising the convergent variables: every such variable E is replaced by a new convergent variable E′ (called a deputy variable), E′ that replaces E′ in the heterogeneous factors containing E, E becomes a regular variable, and a new factor ι(E,E′) is introduced, called deputy factor, that represents the identity function between E and E′, i.e., it is defined by ι(E,E′) ff ft tf tt\n1.0 0.0 0.0 1.0\nDeputising ensures that we do not have descendents of a convergent variable in an heterogeneous factor as long as the elimination order for ve1 is such that ρ(E′) < ρ(E)."
    }, {
      "heading" : "2.4 GC-FOVE",
      "text" : "Work on lifting ve started with (Poole 2003), and the current state of the art is the algorithm GC-FOVE (Taghipour et al. 2013), which redefines the operations of C-FOVE (Milch et al. 2008). The lifted ve algorithm of (Gomes and Costa 2012) represents the adaptation of GC-FOVE to the PFL language.\nFirst-order Variable Elimination (FOVE) (Poole 2003; de Salvo Braz et al. 2005) computes the marginal probability distribution for query random variables (randvars) by repeatedly applying operators that are lifted counterparts of ve’s operators. Models are in the form of a set of parfactors that are essentially the same as in PFL. A parametrized random variable (PRV) V is of the form A|C, where A = F (X1, . . . , Xn) is a nonground atom and C is a constraint on logical variables (logvars) X = {X1, . . . , Xn}. Each PRV represents the set of randvars {F (x)|x ∈ C}, where x is the tuple of constants (x1, . . . , xn). Given a PRV V, we use RV (V) to denote the set of randvars it represents. Each ground atom is associated with one randvar, which can take any value in range(F ).\nGC-FOVE tries to eliminate all (non-query) PRVs in a particular order. To do so, GCFOVE supports several operators. It first tries Lifted Sum-Out, that excludes a PRV from a parfactor φ if the PRV only occurs in φ. Next, Lifted Multiplication, that multiplies two aligned parfactors. Matching variables must be properly aligned and the new coefficients must be computed taking into account the number of groundings in C. Third, Lifted Absorption eliminates n PRVs that have the same observed value. If the two operations cannot be applied, a chosen parfactor must be split so that some of its PRVs match another parfactor. In the worst case, when none of the lifted operators can be applied, GCFOVE resorts to propositionalization: it completely grounds the parametrized randvars and parfactors and performs inference on the ground level.\nGC-FOVE further extends PRVs with counting formulas, introduced in C-FOVE (Milch et al. 2008). A counting formula takes advantage of symmetry existing in factors that are products of independent variables. It represents a factor of the form φ(F (x1), F (x2), . . . , F (xn)), where all variables have the same domain, as φ(#X [F (X)]). The factor implements a multinomial distribution, such that its values depend on the number of variables n and domain size. The lifted counted variable is named a PCRV. PCRVs may result from summing-out, when we obtain factors with a single PRV, or through Counting Con-\nversion that searches for factors of the form φ( ∏ i(S(Xj)F (xj , yi))) and counts on the occurrences of Y . Definitions for counting formulas are reported in Appendix B.\nGC-FOVE employs a constraint-tree to represent arbitrary constraints C, whereas the PFL simply uses sets of tuples. Arbitrary constraints can capture more symmetries in the data, which potentially offers the ability to perform more operations at a lifted level."
    }, {
      "heading" : "3 Translating ProbLog into PFL",
      "text" : "In order to translate ProbLog into PFL, let us start from the conversion of a ProbLog program into a Bayesian network with noisy OR nodes. Here we adapt the conversion for Logic Programs with Annotated Disjunctions presented in (Vennekens et al. 2004; Meert et al. 2008) to the case of ProbLog. The first step is to generate the grounding of the ProbLog program. For each atom A in the Herbrand base of the program, the Bayesian network contains a Boolean random variable with the same name. Each probabilistic fact p :: A is represented by a parentless node with the conditional probability table (CPT): A f t\n1-p p\nFor each ground rule Ri = H ← B1, . . . , Bn, not(C1), . . . , not(Cm) we add to the network a random variable called Hi that has as parents B1, . . . , Bn, C1, . . . , Cm and the following CPT: Hi B1 = t, . . . , Bn = t, C1 = f, . . . , Cm = f all other columns f 0.0 1.0 t 1.0 0.0\nIn practice Hi is the result of the conjunction of random variables representing the atoms in the body. Then for each ground atom H in the Herbrand base not appearing in a probabilistic fact, we add H to the network with parents all Hi of ground rules with H in the head and with the CPT H at least one Hi = t all other columns f 0.0 1.0 t 1.0 0.0 representing the result of the disjunction of random variables Hi.\nTranslating ProbLog into PFL allows us to stay in the lifted (non-ground) program.\nExample 3 (Translation of a ProbLog program into PFL) The translation of the ProbLog program of Example 1 into PFL is\nbayes series1, s; identity ; []. bayes series2, attends(P); identity; [person(P)]. bayes series, series1, series2 ; disjunction; []. bayes attends1(P), at(P,A); identity; [person(P),attribute(A)]. bayes attends(P), attends1(P); identity; [person(P)]. bayes s; [0.9, 0.1]; []. bayes at(P,A); [0.7, 0.3] ; [person(P),attribute(A)].\nidentity([1,0,0,1]).\ndisjunction([1,0,0,0, 0,1,1,1]).\nNotice that series2 and attends1(P) can be seen as or-nodes. Thus, after grounding, factors derived from the second and the fourth parfactor should not be multiplied together but should be combined with heterogeneous multiplication, as variables series2 and attends1(P) are in fact convergent variables. To do so, we need to identify heterogeneous factors and add deputy variables and factors. We thus introduce two new types of factors to PFL, het and deputy. The first factor is such that its ground instantiations are heterogeneous factors. The convergent variables are assumed to be represented by the first atom in the factor’s list of atoms. Lifting identity is straightforward, it corresponds to two atoms and imposes an identity factor between their ground instantiations. Since the factor is fixed, it is not indicated.\nExample 4 (Extended PFL program) The PFL program of Example 3, extended with the two new factors het and deputy, becomes:\nhet series1p, s; identity ; []. het series2p, attends(P); identity; [person(P)]. deputy series2, series2p; []. deputy series1, series1p; []. bayes series, series1, series2; disjunction ; []. het attends1p(P), at(P.A); identity; [person(P),attribute(A)]. deputy attends1(P), attends1p(P); [person(P)]. bayes attends(P), attends1(P); identity; [person(P)]. bayes s; [0.9, 0.1]; []. bayes at(P,A); [0.7, 0.3] ; [person(P),attribute(A)].\nwhere series1p, series2p and attends1p(P) are the convergent deputy random variables, and series1, series2 and attends1(P) are their corresponding new regular variables. The fifth Bayesian factor represents the combination of the contribution to series of the two rules for it. Causal independence could be applied here as well since the combination is really an OR, but for simplicity we decided to concentrate only on exploiting causal independence for the convergent variables represented by the head of rules which is the hard part."
    }, {
      "heading" : "4 Heterogeneous Lifted Multiplication and Summation",
      "text" : "GC-FOVE must be modified in order to take into account heterogeneous factors and convergent variables. The ve algorithm must be replaced by ve1, i.e., two lists of factors must be maintained, one with homogeneous and the other with heterogeneous factors. When summing out a variable, first the homogeneous factors must be combined together with homogeneous lifted multiplication. Then the heterogeneous factors must be combined together with heterogeneous lifted multiplication and, finally, the two results must be combined to produce a final factor from which the random variable is eliminated.\nLifted heterogeneous multiplication is defined as Operator 1, considering the case in which the two factors share convergent random variables. We assume familiarity with set and relational algebra (e.g., join ./) while some useful definitions are reported in Appendix B. PRVs must be count-normalized, that is, the corresponding parameters must be scaled to take into account domain size and number of occurrences in the parfactor. PRVs are then aligned and the joint domain is computed as the natural join between the\nOperator het-multiply Inputs: (1) g1 = φ1(A1)|C1: a parfactor in model G with convergent variables A1 = {A11, . . . , A1k} (2) g2 = φ2(A2)|C2: a parfactor in model G with convergent variables A2 = {A21, . . . , A2k} (3) θ = {X1 → X2}: an alignment between g1 and g2 Preconditions: (1) for i = 1, 2: Yi = logvar(Ai) \\Xi is count-normalized w.r.t. Xi in Ci Output: φ(A)|C, such that (1) C = C1θ ./ C2 (2) A = A1θ ∪ A2 (3) Let A be (A1, . . . , Ak,B) with Aj = A1jθ = A2j for j = 1, . . . , k, B the set of regular variables (4) for each assignment a = (a1, . . . , ak,b) to A with b1 = πA1θ(b), b2 = πA2 (b)\nφ(a1, . . . , ak,b) =∑ a11∨a21=a1 . . . ∑ a1k∨a2k=ak φ1(a11, . . . , a1k,b1) 1/r2φ2(a21, . . . , a2k,b2) 1/r1\nwith ri = CountYi|Xi (Ci) Postcondition: G ∼ G \\ {g1, g2} ∪ {het-multiply(g1, g2, θ)}\nOperator 1. Operator het-multiply.\nset of constraints. Following standard lifted multiplication, we assume the same PRV will have a different instance in each grounded factor. We thus proceed very much as in the grounded case, and for each case (a11, . . . , a1k,b1,b2) we sum the potentials obtained by multiplying the φ1(a11, . . . , a1k,b1) and φ2(a11, . . . , a1k,b2). Note that although potentials need not be normalised to sum to 1 until the end, the relative counts of φ1 and φ2 must be weighed by considering the number of instances φ2 for each φ1.\nExample 5 Consider the heterogeneous parfactors g1 = φ1(p(X1))|C1 and g2 = φ2(p(X2), q(X2, Y2))|C2 and suppose that we want to multiply g1 and g2; p(X) is convergent in g1 and g2; {X1 → X2} is an alignment between g1 and g2; Y2 is count-normalized w.r.t. X2 in C2; r1 = CountY1|X1(C1) = 2 and CountY2|X2(C2) = 3. Then het-multiply(g1, g2, {X1 → X2}) = φ(p(X2), q(X2, Y2))|C with φ given by: φ(p(X2), q(X2, Y2))\nff φ1(f) 1/3φ2(f,f) 1/2 ft φ1(f) 1/3φ2(f,t) 1/2 tf φ1(f) 1/3φ2(t,f) 1/2 + φ1(t) 1/3φ2(f,f) 1/2 + φ1(t) 1/3φ2(t,f) 1/2 tt φ1(f) 1/3φ2(t,t) 1/2 + φ1(t) 1/3φ2(f,t) 1/2 + φ1(t) 1/3φ2(t,t) 1/2\nExample 6 Consider the heterogeneous parfactors g1 = φ1(p(X1), q(X1, Y1))|C1 and g2 = φ2(p(X2), q(X2, Y2))|C2 and suppose we want to multiply g1 and g2; all randvars are convergent; {X1 → X2, Y1 → Y2} is an alignment between g1 and g2 (so r1 = r2 = 1). Then het-multiply(g1, g2, {X1 → X2, Y1 → Y2}) = φ(p(X2), q(X2, Y2))|C, with φ given by\nφ(p(X2), q(X2, Y2)) ff φ1(f,f)φ2(f,f) ft φ1(f,f)φ2(f,t) + φ1(f,t)φ2(f,f) + φ1(f,t)φ2(f,t) tf φ1(t,f)φ2(f,f) + φ1(f,f)φ2(t,f) + φ1(t,f)φ2(t,f) tt φ1(f,f)φ2(t,t) + φ1(f,t)φ2(t,f) + φ1(f,t)φ2(t,t)+\nφ1(t,f)φ2(f,t) + φ1(t,t)φ2(f,f) + φ1(t,t)φ2(f,t)+ φ1(t,f)φ2(t,t) + φ1(t,t)φ2(t,f) + φ1(t,t)φ2(t,t)\nThe sum-out operator must be modified as well. In fact, consider the case in which a random variable must be summed out from a heterogeneous factor (i.e. a factor that\nOperator het-sum-out Inputs: (1) g = φ(A)|C: a parfactor in model G (2) let A = (A1, . . . , Ak, Ak+1,B) where A1, . . . , Ak are convergent atoms (3) Ak+1 is the atom to be summed out Preconditions: (1) For all PRVs V, other than Ak+1|C, in G: RV (V) ∩RV (Ak+1|C) = ∅ (2) Ak+1 contains all the logvars X ∈ logvar(A) for which πX(C) is not singleton (3) Xexcl = logvar(Ak+1) \\ logvar(A \\Ak+1) is count-normalized w.r.t.\nXcom = logvar(Ak+1) ∩ logvar(A \\Ak+1) in C Output: φ′(A′)|C′, such that (1) A′ = A \\Ak+1 (2) C′ = πX(C) (3) for each assignment (a′,b) = (a′1, . . . , a ′ k,b), to A ′\nφ′(a′,b) =(∑ a≤a′ ∑ ak+1∈range(Ak+1) Mul(Ak+1, ak+1)φ(a1, . . . , ak, ak+1,b) )r −\n− ∑\na<a′ φ ′(a1, . . . , ak,b) with\nr = CountXexcl|Xcom (C) Postcondition: PG\\{g}∪{het-sum-out(g,(A1,...,Ak),Ak+1)} = ∑ RV (Ak+1) PG\nOperator 2. Operator het-sum-out. The order ≤ between truth values is the obvious one and between tuples of truth values is the product order induced by ≤ between values, i.e., (a1, . . . , ak) ≤ (a′1, . . . , a′k) iff ai ≤ a′i for i = 1, . . . , k and a < a′ iff a ≤ a′ and a′ 6≤ a. Function Mul is defined in Appendix B.\ncontains a convergent variable). Consider for example the factor φ(p(X), q(X,Y ))|C with C = {x1, x2} × {y1, y2} and suppose we want to eliminate the PRV q(X,Y ). This factor stands for four ground factors of the form φ(p(xi), q(xi, yj)) for i, j = 1, 2 where p(xi) is convergent. Given an individual xi, the two factors φ(p(xi), q(xi, y1)) and φ(p(xi), q(xi, y2)) share a convergent variable and cannot be multiplied together with regular multiplication. In order to sum out q(X,Y ) however we must first combine the two factors with heterogeneous multiplication. To avoid generating first the ground factors, we have added to GC-FOVE het-sum-out (operator 2) that performs the combination and the elimination of a random variable at the same time. We provide a correctness proof for this operator in Appendix C.\nExample 7 Consider the heterogeneous parfactor g = φ(r, p(X), q(X,Y ))|C and suppose that we want to sum out q(X,Y ), that r and p(X) are convergent, that Y is count-normalized w.r.t. X and that CountY |X(C) = 2. Then het-sum-out(g, (r, p(X)), q(X,Y )) = φ′(r, p(X))|C ′ with φ′ given by φ′(r, p(X))\nff (φ(f,f,f) + φ(f,f,t))2 ft (φ(f,t,f) + φ(f,t,t) + φ(f,f,f) + φ(f,f,t))2 − φ′(f, f) tf (φ(t,f,f) + φ(t,f,t) + φ(f,f,f) + φ(f,f,t))2 − φ′(f, f) tt (φ(t,t,f) + φ(t,t,t) + φ(t,f,f) + φ(t,f,t) + φ(f,t,f) + φ(f,t,t) + φ(f,f,f) + φ(f,f,t))2-\n−φ′(t,f)− φ′(f,t)− φ′(f,f)"
    }, {
      "heading" : "5 Experiments",
      "text" : "In order to evaluate the performance of LP2 algorithm, we compare it with PITA and ProbLog2 in two problems: workshops attributes (Milch et al. 2008) and Example 7 in (Poole 2008) that we call plates. Code of all the problems can be found in Appendix A.\nMoreover, we did a scalability test on a third problem: competing workshops (Milch et al. 2008). All the tests were done on a machine with an Intel Dual Core E6550 2.33GHz processor and 4GB of main memory. The workshops attributes problem differs from Example 1 because the first clause for series is missing and the second clause contains a probabilistic atom in its body. The competing workshops problem differs from workshops attributes because it considers, instead of workshop attributes, a set of competing workshops W each one associated with a binary random variable hot(W), which indicates whether it is focusing on popular research areas. The plates problem is an artificial example which contains two sets of individuals, X and Y . The distribution is defined by 7 probabilistic facts and 9 rules.\nFigure 1(a) shows the runtime of LP2, PITA and ProbLog2 on the workshops attributes problem for the query series where we fixed the number of people to 50 and we increased the number of attributes m. As expected, LP2 is able to solve a much larger set of problems than PITA and ProbLog2. Figure 1(b) shows the time spent by LP2 with up to 105 attributes. Figure 2(a) shows the runtime of LP2, PITA and ProbLog2 on the plates problem, while figure 2(b) shows that of LP2 with up to 12× 104 Y individuals. For this test we executed the query f and we fixed the number of different X individuals to 5 and we increased the number of Y individuals.\nFinally, we used the competing workshops problem for testing the scalability of LP2. The trend was calculated performing the query series with 10 competing workshops and an increasing number n of people problems. Figure 3 shows LP2 computation time. The trend is almost linear in the number of people contained in the problem.\nAs the results show, LP2 can manage domains that are of several orders of magnitude\nlarger than the ones managed by PITA and ProbLog2 in a shorter time."
    }, {
      "heading" : "6 Conclusions",
      "text" : "We have shown that the Lifted Variable Elimination approach is very effective at resolving queries w.r.t. probabilistic logic programs containing large amount of facts. We\nhave proved that with the introduction of the two new heterogeneous operators we can compute the probability of queries following the distribution semantics in a very efficient way. Experimental evidence shows that LP2 can achieve several orders of magnitude improvements, both in runtime and in the number of facts that can be managed effectively. In the future, we plan to compare our approach with that of (Kisynski and Poole 2009b; Takikawa and D’Ambrosio 1999; Dı́ez and Galán 2003) for dealing with noisy or factors, and to compare with weighted first order model counting (Van den Broeck et al. 2014).\nAcknowledgments: VSC was partially nanced by the North Portugal Regional Operational Programme (ON.2 O Novo Norte), under the NSRF, through the ERDF and the Fundação para a Ciência e a Tecnologia within project ADE/PTDC/EIAEIA/121686/2010. This work was supported by ”National Group of Computing Science (GNCS-INDAM)”."
    }, {
      "heading" : "Appendix A Problems code",
      "text" : "In this section we present the Problog and PFL code of the testing problems."
    }, {
      "heading" : "A.1 Workshops Attributes",
      "text" : "To all programs of this section we added 50 workshops and an increasing number of attributes.\nProblog program\nseries:- person(P),attends(P),sa(P).\n0.501::sa(P):-person(P).\nattends(P):- person(P),attr(A),at(P,A).\n0.3::at(P,A):-person(P),attr(A)."
    }, {
      "heading" : "PFL program",
      "text" : "het series1,ch1(P);[1.0, 0.0, 0.0, 1.0];[person(P)].\ndeputy series,series1;[].\nbayes ch1(P),attends(P),sa(P);[1.0,1.0,1.0,0.0, 0.0,0.0,0.0,1.0];[person(P)].\nbayes sa(P);[0.499,0.501];[person(P)].\nhet attends1(P),at(P,A);[1.0, 0.0, 0.0, 1.0];[person(P),attr(A)].\ndeputy attends(P),attends1(P);[person(P)].\nbayes at(P,A);[0.7,0.3];[person(P),attr(A)]."
    }, {
      "heading" : "A.2 Competing Workshops",
      "text" : "For the competing workshops problem we report only the PFL version. For testing purpose we added to this code 10 workshops and an increasing number of people.\nPFL program\nbayes ch1(P),attends(P),sa(P);[1.0,1.0,1.0,0.0, 0.0,0.0,0.0,1.0];[person(P)].\nhet series1,ch1(P);[1.0, 0.0, 0.0, 1.0];[person(P)].\ndeputy series,series1;[].\nbayes sa(P);[0.499,0.501];[person(P)].\nhet attends1(P),ch2(P,W);[1.0, 0.0, 0.0, 1.0];[person(P),workshop(W)].\ndeputy attends(P),attends1(P);[person(P)].\nbayes ch2(P,W),hot(W),ah(P,W);[1.0,1.0,1.0,0.0, 0.0,0.0,0.0,1.0];[person(P),workshop(W)].\nbayes ah(P,W);[0.2,0.8];[person(P),workshop(W)]."
    }, {
      "heading" : "A.3 Plates",
      "text" : "For tha plates problem we added 5 individuals for X and an increasing number of individuals for Y .\nProblog program\nf:- e(Y).\ne(Y) :- d(Y),n1(Y). e(Y) :- y(Y),\\+ d(Y),n2(Y).\nd(Y):- c(X,Y).\nc(X,Y):-b(X),n3(X,Y). c(X,Y):- x(X),\\+ b(X),n4(X,Y).\nb(X):- a, n5(X). b(X):- \\+ a,n6(X).\na:- n7.\n0.1::n1(Y) :-y(Y). 0.2::n2(Y) :-y(Y). 0.3::n3(X,Y) :- x(X),y(Y). 0.4::n4(X,Y) :- x(X),y(Y). 0.5::n5(X) :-x(X). 0.6::n6(X) :-x(X). 0.7::n7."
    }, {
      "heading" : "PFL program",
      "text" : "het f1,e(Y);[1.0, 0.0, 0.0, 1.0];[y(Y)].\ndeputy f,f1;[].\nbayes e1(Y),d(Y),n1(Y);[1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0];[y(Y)].\nbayes e2(Y),d(Y),n2(Y);[1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0];[y(Y)].\nbayes e(Y),e1(Y),e2(Y);[1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0];[y(Y)].\nhet d1(Y),c(X,Y);[1.0, 0.0, 0.0, 1.0];[x(X),y(Y)].\ndeputy d(Y),d1(Y);[y(Y)].\nbayes c1(X,Y),b(X),n3(X,Y);[1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0];[x(X),y(Y)].\nbayes c2(X,Y),b(X),n4(X,Y);[1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0];[x(X),y(Y)].\nbayes c(X,Y),c1(X,Y),c2(X,Y);[1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0];[x(X),y(Y)].\nbayes b1(X),a,n5(X);[1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0];[x(X)].\nbayes b2(X),a,n6(X);[1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0];[x(X)].\nbayes b(X),b1(X),b2(X);[1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0];[x(X)].\nbayes a,n7;[1.0, 0.0, 0.0, 1.0];[].\nbayes n1(Y);[0.9, 0.1];[y(Y)]. bayes n2(Y);[0.8, 0.2];[y(Y)]. bayes n3(X,Y);[0.7, 0.3];[x(X),y(Y)]. bayes n4(X,Y);[0.6, 0.4];[x(X),y(Y)]. bayes n5(X);[0.5, 0.5];[x(X)]. bayes n6(X);[0.4, 0.6];[x(X)]. bayes n7;[0.3, 0.7];[]."
    }, {
      "heading" : "Appendix B Definitions",
      "text" : "Definition 1 (counting formula) A counting formula is a syntactic construct of the form #Xi∈C [F (X)], where Xi ∈ X is called the counted logvar.\nA grounded counting formula is a counting formula in which all arguments of the atom F (X), except for the counted logvar, are constants. It defines a counting randvar (CRV) as follows.\nDefinition 2 (counting randvar)\nA parametrized counting randvar (PCRV) is a pair (#Xi [F (X)], C). For each instantiation of X \\Xi, it creates a separate counting randvar (CRV). The value of this CRV is a histogram, and it depends deterministically on the values of F (X). Given a valuation for F (X), it counts how many different values of Xi occur for each r ∈ range(F ). The result is a histogram of the form {(r1, n1), . . . , (rk, nk)}, with ri ∈ range(F ) and ni the corresponding count.\nDefinition 3 (multiplicity) The multiplicity of a histogram h = {(r1, n1), . . . , (rk, nk)} is a multinomial coefficient, defined as\nMul(h) = n!∏k i=1 ni! .\nAs multiplicities should only be taken into account for (P)CRVs, never for regular PRVs, we define for each PRV A and for each value v ∈ range(A) : Mul(A, v) = 1 if A is a regular PRV, and Mul(A, v) = Mul(v) if A is a PCRV. This Mul function is identical to (Milch et al. 2008)’s num-assign.\nDefinition 4 (Count function) Given a constraint CX, for any Y ⊆ X and Z ⊆ X−Y, the function CountY|X : CX → N is defined as follows:\nCountY|Z(t) = |πY(σZ=πZ(t)(CX))|\nThat is, for any tuple t, this function tells us how many values for Y co-occur with t’s value for Z in the constraint. We define CountY|Z(t) = 1 when Y = ∅.\nDefinition 5 (Count-normalized constraint) For any constraint CX, Y ⊆ X and Z ⊆ X−Y, Y is count-normalized w.r.t. Z in CX if and only if\n∃n ∈ N : ∀t ∈ CX : CountY|Z(t) = n. When such an n exists, we call it the conditional count of Y given Z in CX, and denote it CountY|Z(CX).\nDefinition 6 (substitution) A substitution θ = {X1 → t1, . . . , Xn → tn} = {X→ t} maps each logvar Xi to a term ti, which can be a constant or a logvar. When all ti are constants, θ is called a grounding substitution, and when all are different logvars, a renaming substitution. Applying a substitution θ to an expression α means replacing each occurrence of Xi in α with ti; the result is denoted αθ.\nDefinition 7 (alignment) An alignment θ between two parfactors g = φ(A)|C and g′ = φ′(A′)|C ′ is a one-toone substitution {X → X′}, with X ⊆ logvar(A) and X′ ⊆ logvar(A′), such that ρ(πX′(C)) = πX′(C ′) (with ρ the attribute renaming operator).\nAn alignment tells the multiplication operator that two atoms in two different parfactors represent the same PRV, so it suffices to include it in the resulting parfactor only once."
    }, {
      "heading" : "Appendix C Correctness proof for heterogeneous multiplication",
      "text" : "Theorem 1 Given a model (F1,F2), two heterogeneous parfactors g1, g2 ∈ F2 and an alignment θ between g1 and g2, if the preconditions of the het-multiply operator are fulfilled then the postcondition\nG \\ {g1, g2} ∪ {het-multiply(g1, g2, θ)}\nholds."
    }, {
      "heading" : "Proof",
      "text" : "Immediate from the definition of heterogeneous multiplication.\nTheorem 2 Given a model (F1,F2), a heterogeneous parfactor g ∈ F2 and an atom Ak+1 to be summed out, if the preconditions of the het-sum-out operator are fulfilled then the postcondition\nPG\\{g}∪{het-sum-out(g,(A1,...,Ak),Ak+1)} = ∑\nRV (Ak+1)\nPG\nholds."
    }, {
      "heading" : "Proof",
      "text" : "We prove the formula giving phi′ in Operator 2 by double induction over r = CountXexcl|Xcom(C) and the number n of values at t in the tuple (a ′ 1, . . . , a ′ k). For simplicity we assume that the variable to be summed out, Ak+1, is not a counting variable, but the same reasoning can be applied for a counting variable. For n = 0, r = 1\nφ′(f, . . . , f,b) = φ(f, . . . , f, f,b) + φ(f, . . . , f, t,b)\nso the thesis is proved. For n = 0, r > 1, let us call φ′r(a ′ 1, . . . , a ′ k,b) the the value of φ′ for r. Let us assume that the formula holds for r − 1. For r > 1, there is an extra valuation xexcl for Xexcl given Xcom so there is an extra factor g\n′′(xexcl). Eliminating Ak+1 from g ′′ gives\nφ′1(f, . . . , f,b) = φ(f, . . . , f, f,b) + φ(f, . . . , f, t,b)\nThis must be multiplied by φ′r−1 with heterogeneous multiplication as A1, . . . , Ak are shared obtaining\nφ′r(f, . . . , f,b) = ∑\nâ1∨ǎ1=f . . . ∑ âk∨ǎk=f φ′r−1(â1, . . . , âk,b)× φ′1(ǎ1, . . . , ǎk, f,b) =\n= φ′r−1(f, . . . , f,b)× φ′1(ǎ1, . . . , ǎk, f,b) = = (φ(f, . . . , f, f,b) + φ(f, . . . , f, t,b))r−1 × (φ(f, . . . , f, f,b) + φ(f, . . . , f, t,b)) =\n= (φ(f, . . . , f, f,b) + φ(f, . . . , f, t,b))r\nso the thesis is proved. For n > 0 of values at t in the tuple a′ = (a′1, . . . , a ′ k), we assume that the formula\nholds for (n− 1, r) and (n, r − 1). For the defintiion of heterogeneous multiplication\nφ′r(a ′ 1, . . . , a ′ k,b) = ∑ â1∨ǎ1=a′1 . . . ∑ âk∨ǎk=a′k φ′r−1(â1, . . . , âk,b)φ ′ 1(ǎ1, . . . , ǎk,b)\nBy adding and removing∑ a<a′ ∑ â1∨ǎ1=a1 . . . ∑ âk∨ǎk=ak φ′r−1(â1, . . . , âk,b)φ ′ 1(ǎ1, . . . , ǎk,b)\nwe get φ′r(a ′ 1, . . . , a ′ k,b) =\n= ∑\nâ1∨ǎ1=a′1\n. . . ∑\nâk∨ǎk=a′k\nφ′r−1(â1, . . . , âk,b)φ ′ 1(ǎ1, . . . , ǎk,b) +\n+ ∑ a<a′ ∑ â1∨ǎ1=a1 . . . ∑ âk∨ǎk=ak φ′r−1(â1, . . . , âk,b)φ ′ 1(ǎ1, . . . , ǎk,b)−\n− ∑ a<a′ ∑ â1∨ǎ1=a1 . . . ∑ âk∨ǎk=ak φ′r−1(â1, . . . , âk,b)φ ′ 1(ǎ1, . . . , ǎk,b) =\n= ∑ â1≤a′1 ∑ ǎ1≤a′1 . . . ∑ âk≤a′k ∑ ǎk≤a′k φ′r−1(â1, . . . , âk,b)φ ′ 1(ǎ1, . . . , ǎk,b)−\n− ∑ a<a′ ∑ â1∨ǎ1=a1 . . . ∑ âk∨ǎk=ak φ′r−1(â1, . . . , âk,b)φ ′ 1(ǎ1, . . . , ǎk,b) =\nFor the definition of heterogeneous multiplication we obtain φ′r(a ′ 1, . . . , a ′ k,b) =\n= ∑ â≤a′ φ′r−1(â1, . . . , âk,b) × ∑ ǎ≤a′ φ′1(ǎ1, . . . , ǎk,b) −∑ a<a′ φ′r(a1, . . . , ak,b)\nBy applying the inductive hypothesis for r − 1 we get φ′r(a′1, . . . , a′k,b) =\n= ∑ â≤a′ ∑ a≤â φ(a1, . . . , ak, f,b) + φ(a1, . . . , ak, t,b) r−1 −∑ a<â φ′r−1(a1, . . . , ak,b) × ×\n∑ ǎ≤a′ φ′1(ǎ1, . . . , ǎk,b) − ∑ a<a′ φ′r(a1, . . . , ak,b) =\n= ∑ â≤a′ ∑ a≤â φ(a1, . . . , ak, f,b) + φ(a1, . . . , ak, t,b) r−1× ∑ ǎ≤a′ φ′1(ǎ1, . . . , ǎk,b) − −\n∑ â≤a′ ∑ a<â φ′r−1(a1, . . . , ak,b) × ∑ ǎ≤a′ φ′1(ǎ1, . . . , ǎk,b)  − ∑ a<a′ φ′r(a1, . . . , ak,b) =\n= ∑ a≤a′ φ(a1, . . . , ak, f,b) + φ(a1, . . . , ak, t,b) r−1 × ∑ a≤a′ φ′1(ǎ1, . . . , ǎk,b) + +\n∑ â<a′ ∑ a≤â φ(a1, . . . , ak, f,b) + φ(a1, . . . , ak, t,b) r−1× ∑ ǎ≤a′ φ′1(ǎ1, . . . , ǎk,b) − −\n∑ â≤a′ ∑ a<â φ′r−1(a1, . . . , ak,b) × ∑ ǎ≤a′ φ′1(ǎ1, . . . , ǎk,b) − − ∑ a<a′ φ′r(a1, . . . , ak,b) =\nBy applying the formula for r = 1\nφ′r(a ′ 1, . . . , a ′ k,b) =\n= ∑ a≤a′ φ(a1, . . . , ak, f,b) + φ(a1, . . . , ak, t,b) r−1 × ∑ a≤a′ φ(a1, . . . , ak, f,b) + φ(a1, . . . , ak, t,b) + +\n∑ â<a′ ∑ a≤â φ(a1, . . . , ak, f,b) + φ(a1, . . . , ak, t,b) r−1× ∑ ǎ≤a′ φ′1(ǎ1, . . . , ǎk,b) − −\n(∑ â<a′ ∑ a<â φ′r−1(a1, . . . , ak,b) ) × ∑ ǎ≤a′ φ′1(ǎ1, . . . , ǎk,b) − − ∑ a<a′ φ′r(a1, . . . , ak,b) =\n= ∑ a≤a′ φ(a1, . . . , ak, f,b) + φ(a1, . . . , ak, t,b) r −∑ a<a′ φ′r(a1, . . . , ak,b) +\n+ ∑ â<a′ ∑ a≤â φ(a1, . . . , ak, f,b) + φ(a1, . . . , ak, t,b) r−1× ∑ ǎ≤a′ φ′1(ǎ1, . . . , ǎk,b) − −\n(∑ â<a′ ∑ a<â φ′r−1(a1, . . . , ak,b) ) × ∑ ǎ≤a′ φ′1(ǎ1, . . . , ǎk,b) \nBy collecting the factor ∑\nǎ≤a′ φ ′ 1(ǎ1, . . . , ǎk,b) φ ′ r(a ′ 1, . . . , a ′ k,b) =\n= ∑ a≤a′ φ(a1, . . . , ak, f,b) + φ(a1, . . . , ak, t,b) r −∑ a<a′ φ′r(a1, . . . , ak,b) +\n+ ∑ â<a′ ∑ a≤â φ(a1, . . . , ak, f,b) + φ(a1, . . . , ak, t,b) r−1 −∑ â<a′ ∑ a≤â φ′r−1(a1, . . . , ak,b) × ×\n∑ ǎ≤a′ φ′1(ǎ1, . . . , ǎk,b)  = =\n∑ a≤a′ φ(a1, . . . , ak, f,b) + φ(a1, . . . , ak, t,b) r −∑ a<a′ φ′r(a1, . . . , ak,b) +\n+ ∑ â<a′ ∑ a≤â φ(a1, . . . , ak, f,b) + φ(a1, . . . , ak, t,b) r−1 −∑ â<a′ ∑ a<â φ′r−1(a1, . . . , ak,b)− − ∑ a<a′ φ′r−1(a1, . . . , ak,b) ) × ( ∑ a≤a′ φ′1(a1, . . . , ak,b)) =\n= ∑ a≤a′ φ(a1, . . . , ak, f,b) + φ(a1, . . . , ak, t,b) r −∑ a<a′ φ′r(a1, . . . , ak,b) +\n+ (∑ a<a′ φ′r−1(a1, . . . , ak,b)− ∑ a<a′ φ′r−1(a1, . . . , ak,b) ) × ∑ a≤a′ φ′1(a1, . . . , ak,b) =\n= ∑ a≤a′ φ(a1, . . . , ak, f,b) + φ(a1, . . . , ak, t,b) r −∑ a<a′ φ′r(a1, . . . , ak,b)"
    } ],
    "references" : [ {
      "title" : "Probabilistic Inductive Logic Programming - Theory and Applications",
      "author" : [ "L. De Raedt", "P. Frasconi", "K. Kersting", "S. Muggleton", "Eds." ],
      "venue" : "LNCS, vol. 4911. Springer.",
      "citeRegEx" : "Raedt et al\\.,? 2008",
      "shortCiteRegEx" : "Raedt et al\\.",
      "year" : 2008
    }, {
      "title" : "ProbLog: A Probabilistic Prolog and its application in link discovery",
      "author" : [ "L. De Raedt", "A. Kimmig", "H. Toivonen" ],
      "venue" : "20th International Joint Conference on Artificial Intelligence (IJCAI-2007). AAAI Press, 2462–2467.",
      "citeRegEx" : "Raedt et al\\.,? 2007",
      "shortCiteRegEx" : "Raedt et al\\.",
      "year" : 2007
    }, {
      "title" : "Lifted first-order probabilistic inference",
      "author" : [ "R. de Salvo Braz", "E. Amir", "D. Roth" ],
      "venue" : "In 19th International Joint Conference on Artificial Intelligence,",
      "citeRegEx" : "Braz et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Braz et al\\.",
      "year" : 2005
    }, {
      "title" : "Efficient computation for the noisy max",
      "author" : [ "F.J. D́ıez", "S.F. Galán" ],
      "venue" : "International Journal of Intelligent Systems,",
      "citeRegEx" : "D́ıez and Galán,? \\Q2003\\E",
      "shortCiteRegEx" : "D́ıez and Galán",
      "year" : 2003
    }, {
      "title" : "Inference and learning in probabilistic logic programs using weighted boolean formulas",
      "author" : [ "D. Fierens", "G. Van den Broeck", "J. Renkens", "D. Shterionov", "B. Gutmann", "I. Thon", "G. Janssens", "L. De Raedt" ],
      "venue" : "Theory and Practice of Logic Programming FirstView Articles.",
      "citeRegEx" : "Fierens et al\\.,? 2014",
      "shortCiteRegEx" : "Fierens et al\\.",
      "year" : 2014
    }, {
      "title" : "Introduction to Statistical Relational Learning",
      "author" : [ "L. Getoor", "B. Taskar", "Eds." ],
      "venue" : "MIT Press.",
      "citeRegEx" : "Getoor et al\\.,? 2007",
      "shortCiteRegEx" : "Getoor et al\\.",
      "year" : 2007
    }, {
      "title" : "Evaluating inference algorithms for the prolog factor language",
      "author" : [ "T. Gomes", "V.S. Costa" ],
      "venue" : "22nd International Conference on Inductive Logic Programming, F. Riguzzi and F. Zelezný, Eds. LNCS, vol. 7842. Springer, 74–85.",
      "citeRegEx" : "Gomes and Costa,? 2012",
      "shortCiteRegEx" : "Gomes and Costa",
      "year" : 2012
    }, {
      "title" : "Constraint processing in lifted probabilistic inference",
      "author" : [ "J. Kisynski", "D. Poole" ],
      "venue" : "25th Conference on Uncertainty in Artificial Intelligence, J. Bilmes and A. Y. Ng, Eds. AUAI Press, 293–302.",
      "citeRegEx" : "Kisynski and Poole,? 2009a",
      "shortCiteRegEx" : "Kisynski and Poole",
      "year" : 2009
    }, {
      "title" : "Lifted aggregation in directed first-order probabilistic models",
      "author" : [ "J. Kisynski", "D. Poole" ],
      "venue" : "24th International Joint Conference on Artificial Intelligence, C. Boutilier, Ed. 1922–1929.",
      "citeRegEx" : "Kisynski and Poole,? 2009b",
      "shortCiteRegEx" : "Kisynski and Poole",
      "year" : 2009
    }, {
      "title" : "Learning ground CP-Logic theories by leveraging Bayesian network learning techniques",
      "author" : [ "W. Meert", "J. Struyf", "H. Blockeel" ],
      "venue" : "Fundamenta Informaticae 89, 131–160.",
      "citeRegEx" : "Meert et al\\.,? 2008",
      "shortCiteRegEx" : "Meert et al\\.",
      "year" : 2008
    }, {
      "title" : "Lifted probabilistic inference with counting formulas",
      "author" : [ "B. Milch", "L.S. Zettlemoyer", "K. Kersting", "M. Haimes", "L.P. Kaelbling" ],
      "venue" : "23rd AAAI Conference on Artificial Intelligence, D. Fox and C. P. Gomes, Eds. AAAI Press, 1062–1068.",
      "citeRegEx" : "Milch et al\\.,? 2008",
      "shortCiteRegEx" : "Milch et al\\.",
      "year" : 2008
    }, {
      "title" : "Probabilistic horn abduction and Bayesian networks",
      "author" : [ "D. Poole" ],
      "venue" : "Artificial Intelligence 64, 1, 81–129.",
      "citeRegEx" : "Poole,? 1993",
      "shortCiteRegEx" : "Poole",
      "year" : 1993
    }, {
      "title" : "The Independent Choice Logic for modelling multiple agents under uncertainty",
      "author" : [ "D. Poole" ],
      "venue" : "Artificial Intelligence 94, 7–56.",
      "citeRegEx" : "Poole,? 1997",
      "shortCiteRegEx" : "Poole",
      "year" : 1997
    }, {
      "title" : "First-order probabilistic inference",
      "author" : [ "D. Poole" ],
      "venue" : "18th International Joint Conference on Artificial Intelligence, G. Gottlob and T. Walsh, Eds. Morgan Kaufmann Publishers Inc., 985–991.",
      "citeRegEx" : "Poole,? 2003",
      "shortCiteRegEx" : "Poole",
      "year" : 2003
    }, {
      "title" : "The independent choice logic and beyond",
      "author" : [ "D. Poole" ],
      "venue" : "Probabilistic Inductive Logic Programming, L. De Raedt, P. Frasconi, K. Kersting, and S. Muggleton, Eds. LNCS, vol. 4911. Springer, 222–243.",
      "citeRegEx" : "Poole,? 2008",
      "shortCiteRegEx" : "Poole",
      "year" : 2008
    }, {
      "title" : "The PITA system: Tabling and answer subsumption for reasoning under uncertainty",
      "author" : [ "F. Riguzzi", "T. Swift" ],
      "venue" : "Theory and Practice of Logic Programming, International Conference on Logic Programming (ICLP) Special Issue 11, 433–449.",
      "citeRegEx" : "Riguzzi and Swift,? 2011",
      "shortCiteRegEx" : "Riguzzi and Swift",
      "year" : 2011
    }, {
      "title" : "A statistical learning method for logic programs with distribution semantics",
      "author" : [ "T. Sato" ],
      "venue" : "12th International Conference on Logic Programming, L. Sterling, Ed. MIT Press, 715–729.",
      "citeRegEx" : "Sato,? 1995",
      "shortCiteRegEx" : "Sato",
      "year" : 1995
    }, {
      "title" : "Lifted variable elimination: Decoupling the operators from the constraint language",
      "author" : [ "N. Taghipour", "D. Fierens", "J. Davis", "H. Blockeel" ],
      "venue" : "Journal of Artificial Intelligence Research 47, 393–439.",
      "citeRegEx" : "Taghipour et al\\.,? 2013",
      "shortCiteRegEx" : "Taghipour et al\\.",
      "year" : 2013
    }, {
      "title" : "Multiplicative factorization of noisy-max",
      "author" : [ "M. Takikawa", "B. D’Ambrosio" ],
      "venue" : "In 15th Conference on Uncertainty in Artificial Intelligence",
      "citeRegEx" : "Takikawa and D.Ambrosio,? \\Q1999\\E",
      "shortCiteRegEx" : "Takikawa and D.Ambrosio",
      "year" : 1999
    }, {
      "title" : "Skolemization for weighted first-order model counting",
      "author" : [ "G. Van den Broeck", "W. Meert", "A. Darwiche" ],
      "venue" : "ArXiv e-prints 1312.5378v2. To appear in the 14th International Conference on Principles of Knowledge Representation and Reasoning.",
      "citeRegEx" : "Broeck et al\\.,? 2014",
      "shortCiteRegEx" : "Broeck et al\\.",
      "year" : 2014
    }, {
      "title" : "Lifted probabilistic inference by first-order knowledge compilation",
      "author" : [ "G. Van den Broeck", "N. Taghipour", "W. Meert", "J. Davis", "L.D. Raedt" ],
      "venue" : "21st International Joint Conference on Artificial Intelligence, T. Walsh, Ed. IJCAI/AAAI, 2178–2185.",
      "citeRegEx" : "Broeck et al\\.,? 2011",
      "shortCiteRegEx" : "Broeck et al\\.",
      "year" : 2011
    }, {
      "title" : "Logic Programs With Annotated Disjunctions",
      "author" : [ "J. Vennekens", "S. Verbaeten", "M. Bruynooghe" ],
      "venue" : "20th International Conference on Logic Programming, B. Demoen and V. Lifschitz, Eds. Springer, LNCS 3131, 195–209.",
      "citeRegEx" : "Vennekens et al\\.,? 2004",
      "shortCiteRegEx" : "Vennekens et al\\.",
      "year" : 2004
    }, {
      "title" : "Exploiting causal independence in bayesian network inference",
      "author" : [ "N.L. Zhang", "D.L. Poole" ],
      "venue" : "Journal of Artificial Intelligence Research 5, 301–328. Appendix A Problems code",
      "citeRegEx" : "Zhang and Poole,? 1996",
      "shortCiteRegEx" : "Zhang and Poole",
      "year" : 1996
    } ],
    "referenceMentions" : [ {
      "referenceID" : 16,
      "context" : "Among the various proposals, the one based on the distribution semantics (Sato 1995) has gained popularity as the basis of languages such as Probabilistic Horn Abduction (Poole 1993), PRISM (Sato ar X iv :1 40 5.",
      "startOffset" : 73,
      "endOffset" : 84
    }, {
      "referenceID" : 11,
      "context" : "Among the various proposals, the one based on the distribution semantics (Sato 1995) has gained popularity as the basis of languages such as Probabilistic Horn Abduction (Poole 1993), PRISM (Sato ar X iv :1 40 5.",
      "startOffset" : 170,
      "endOffset" : 182
    }, {
      "referenceID" : 12,
      "context" : "1995), Independent Choice Logic (Poole 1997), Logic Programs with Annotated Disjunctions (Vennekens et al.",
      "startOffset" : 32,
      "endOffset" : 44
    }, {
      "referenceID" : 21,
      "context" : "1995), Independent Choice Logic (Poole 1997), Logic Programs with Annotated Disjunctions (Vennekens et al. 2004), and ProbLog (De Raedt et al.",
      "startOffset" : 89,
      "endOffset" : 112
    }, {
      "referenceID" : 13,
      "context" : "Lifted inference (Poole 2003; de Salvo Braz et al. 2005; Milch et al. 2008; Van den Broeck et al. 2011) is one of the major advances in this respect.",
      "startOffset" : 17,
      "endOffset" : 103
    }, {
      "referenceID" : 10,
      "context" : "Lifted inference (Poole 2003; de Salvo Braz et al. 2005; Milch et al. 2008; Van den Broeck et al. 2011) is one of the major advances in this respect.",
      "startOffset" : 17,
      "endOffset" : 103
    }, {
      "referenceID" : 13,
      "context" : "Work on lifting ve started with (Poole 2003).",
      "startOffset" : 32,
      "endOffset" : 44
    }, {
      "referenceID" : 6,
      "context" : "In (Gomes and Costa 2012), the Prolog Factor Language (PFL, for short) was proposed as a Prolog extension to support probabilistic reasoning with parfactors.",
      "startOffset" : 3,
      "endOffset" : 25
    }, {
      "referenceID" : 17,
      "context" : "PFL exploits the state-of-art algorithm GC-FOVE (Taghipour et al. 2013), which redefines the operations described in(Milch et al.",
      "startOffset" : 48,
      "endOffset" : 71
    }, {
      "referenceID" : 10,
      "context" : "2013), which redefines the operations described in(Milch et al. 2008) to be correct for whatever constraint representation is being used.",
      "startOffset" : 50,
      "endOffset" : 69
    }, {
      "referenceID" : 6,
      "context" : "In fact, the lifted ve algorithm of (Gomes and Costa 2012) represents the adaptation of GC-FOVE to the PFL constraints.",
      "startOffset" : 36,
      "endOffset" : 58
    }, {
      "referenceID" : 4,
      "context" : "An experimental comparison between LP and ProbLog2 (Fierens et al. 2014) and PITA (Riguzzi and Swift 2011) shows that inference time increases linearly with the number of individuals of the program domain for LP, rather than exponentially as with ProbLog2 and PITA.",
      "startOffset" : 51,
      "endOffset" : 72
    }, {
      "referenceID" : 15,
      "context" : "2014) and PITA (Riguzzi and Swift 2011) shows that inference time increases linearly with the number of individuals of the program domain for LP, rather than exponentially as with ProbLog2 and PITA.",
      "startOffset" : 15,
      "endOffset" : 39
    }, {
      "referenceID" : 10,
      "context" : "Example 1 (Running example) Here we present an example inspired by the workshop attributes problem of (Milch et al. 2008).",
      "startOffset" : 102,
      "endOffset" : 121
    }, {
      "referenceID" : 7,
      "context" : "As progress has been made on managing large networks, it has become clear that often the same factor appears repeatedly in the network, thus suggesting the use of templates generalizing individual factors, or parametric factors (Kisynski and Poole 2009a).",
      "startOffset" : 228,
      "endOffset" : 254
    }, {
      "referenceID" : 6,
      "context" : "The Prolog Factor Language (PFL) (Gomes and Costa 2012) extends Prolog to support probabilistic reasoning with parametric factors or parfactors.",
      "startOffset" : 33,
      "endOffset" : 55
    }, {
      "referenceID" : 22,
      "context" : "Variable Elimination (ve) (Zhang and Poole 1996) is an algorithm for computing this posterior probability in factorized joint probability distributions.",
      "startOffset" : 26,
      "endOffset" : 48
    }, {
      "referenceID" : 22,
      "context" : "Causal independence (Zhang and Poole 1996) goes one step further and looks at independence conditioned on values of the random variables.",
      "startOffset" : 20,
      "endOffset" : 42
    }, {
      "referenceID" : 22,
      "context" : "A modified algorithm, called ve1 (Zhang and Poole 1996), combines factors through a new operator ⊗, that generalizes formula (1) as follows.",
      "startOffset" : 33,
      "endOffset" : 55
    }, {
      "referenceID" : 13,
      "context" : "Work on lifting ve started with (Poole 2003), and the current state of the art is the algorithm GC-FOVE (Taghipour et al.",
      "startOffset" : 32,
      "endOffset" : 44
    }, {
      "referenceID" : 17,
      "context" : "Work on lifting ve started with (Poole 2003), and the current state of the art is the algorithm GC-FOVE (Taghipour et al. 2013), which redefines the operations of C-FOVE (Milch et al.",
      "startOffset" : 104,
      "endOffset" : 127
    }, {
      "referenceID" : 10,
      "context" : "2013), which redefines the operations of C-FOVE (Milch et al. 2008).",
      "startOffset" : 48,
      "endOffset" : 67
    }, {
      "referenceID" : 6,
      "context" : "The lifted ve algorithm of (Gomes and Costa 2012) represents the adaptation of GC-FOVE to the PFL language.",
      "startOffset" : 27,
      "endOffset" : 49
    }, {
      "referenceID" : 13,
      "context" : "First-order Variable Elimination (FOVE) (Poole 2003; de Salvo Braz et al. 2005) computes the marginal probability distribution for query random variables (randvars) by repeatedly applying operators that are lifted counterparts of ve’s operators.",
      "startOffset" : 40,
      "endOffset" : 79
    }, {
      "referenceID" : 10,
      "context" : "GC-FOVE further extends PRVs with counting formulas, introduced in C-FOVE (Milch et al. 2008).",
      "startOffset" : 74,
      "endOffset" : 93
    }, {
      "referenceID" : 21,
      "context" : "Here we adapt the conversion for Logic Programs with Annotated Disjunctions presented in (Vennekens et al. 2004; Meert et al. 2008) to the case of ProbLog.",
      "startOffset" : 89,
      "endOffset" : 131
    }, {
      "referenceID" : 9,
      "context" : "Here we adapt the conversion for Logic Programs with Annotated Disjunctions presented in (Vennekens et al. 2004; Meert et al. 2008) to the case of ProbLog.",
      "startOffset" : 89,
      "endOffset" : 131
    }, {
      "referenceID" : 10,
      "context" : "5 Experiments In order to evaluate the performance of LP algorithm, we compare it with PITA and ProbLog2 in two problems: workshops attributes (Milch et al. 2008) and Example 7 in (Poole 2008) that we call plates.",
      "startOffset" : 143,
      "endOffset" : 162
    }, {
      "referenceID" : 14,
      "context" : "2008) and Example 7 in (Poole 2008) that we call plates.",
      "startOffset" : 23,
      "endOffset" : 35
    }, {
      "referenceID" : 10,
      "context" : "Moreover, we did a scalability test on a third problem: competing workshops (Milch et al. 2008).",
      "startOffset" : 76,
      "endOffset" : 95
    }, {
      "referenceID" : 8,
      "context" : "In the future, we plan to compare our approach with that of (Kisynski and Poole 2009b; Takikawa and D’Ambrosio 1999; Dı́ez and Galán 2003) for dealing with noisy or factors, and to compare with weighted first order model counting (Van den Broeck et al.",
      "startOffset" : 60,
      "endOffset" : 138
    }, {
      "referenceID" : 18,
      "context" : "In the future, we plan to compare our approach with that of (Kisynski and Poole 2009b; Takikawa and D’Ambrosio 1999; Dı́ez and Galán 2003) for dealing with noisy or factors, and to compare with weighted first order model counting (Van den Broeck et al.",
      "startOffset" : 60,
      "endOffset" : 138
    } ],
    "year" : 2014,
    "abstractText" : "Lifted inference has been proposed for various probabilistic logical frameworks in order to compute the probability of queries in a time that depends on the size of the domains of the random variables rather than the number of instances. Even if various authors have underlined its importance for probabilistic logic programming (PLP), lifted inference has been applied up to now only to relational languages outside of logic programming. In this paper we adapt Generalized Counting First Order Variable Elimination (GC-FOVE) to the problem of computing the probability of queries to probabilistic logic programs under the distribution semantics. In particular, we extend the Prolog Factor Language (PFL) to include two new types of factors that are needed for representing ProbLog programs. These factors take into account the existing causal independence relationships among random variables and are managed by the extension to variable elimination proposed by Zhang and Poole for dealing with convergent variables and heterogeneous factors. Two new operators are added to GC-FOVE for treating heterogeneous factors. The resulting algorithm, called LP for Lifted Probabilistic Logic Programming, has been implemented by modifying the PFL implementation of GC-FOVE and tested on three benchmarks for lifted inference. A comparison with PITA and ProbLog2 shows the potential of the approach.",
    "creator" : "LaTeX with hyperref package"
  }
}