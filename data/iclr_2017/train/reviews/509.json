{"conference": "ICLR 2017 conference submission", "title": "Programming With a Differentiable Forth Interpreter", "abstract": "There are families of neural networks that can learn to compute any function, provided sufficient training data. However, given that in practice training data is scarce for all but a small set of problems, a core question is how to incorporate prior knowledge into a model. Here we consider the case of prior procedural knowledge, such as knowing the overall recursive structure of a sequence transduction program or the fact that a program will likely use arithmetic operations on real numbers to solve a task. To this end we present a differentiable interpreter for the programming language Forth. Through a neural implementation of the dual stack machine that underlies Forth, programmers can write program sketches with slots that can be filled with behaviour trained from program input-output data. As the program interpreter is end-to-end differentiable, we can optimize this behaviour directly through gradient descent techniques on user specified objectives, and also integrate the program into any larger neural computation graph. We show empirically that our interpreter is able to effectively leverage different levels of prior program structure and learn complex transduction tasks such as sequence sorting or addition with substantially less data and better generalisation over problem sizes. In addition, we introduce neural program optimisations based on symbolic computation and parallel branching that lead to significant speed improvements.", "reviews": [{"is_meta_review": true, "comments": "This paper develops a differentiable interpreter for the Forth programming\nlanguage. This enables writing a program \"sketch\" (a program with parts left\nout), with a hole to be filled in based upon learning from input-output\nexamples. The main technical development is to start with an abstract machine\nfor the Forth language, and then to make all of the operations differentiable.\nThe technique for making operations differentiable is analogous to what is done\nin models like Neural Turing Machine and Stack RNN. Special syntax is developed\nfor specifying holes, which gives the pattern about what data should be read\nwhen filling in the hole, which data should be written, and what the rough\nstructure of the model that fills the hole should be. Motivation for why one\nshould want to do this is that it enables composing program sketches with other\ndifferentiable models like standard neural networks, but the experiments focus\non sorting and addition tasks with relatively small degrees of freedom for how\nto fill in the holes.\n\nExperimentally, result show that sorting and addition can be learned given\nstrong sketches.\n\nThe aim of this paper is very ambitious: convert a full programming language to\nbe differentiable, and I admire this ambition. The idea is provocative and I\nthink will inspire people in the ICLR community.\n\nThe main weakness is that the experiments are somewhat trivial and there are no\nbaselines. I believe that simply enumerating possible values to fill in the\nholes would work better, and if that is possible, then it's not clear to me what\nis practically gained from this formulation. (The authors argue that the point\nis to compose differentiable Forth sketches with neural networks sitting below,\nbut if the holes can be filled by brute force, then could the underlying neural\nnetwork not be separately trained to maximize the probability assigned to any\nfilling of the hole that produces the correct input-output behavior?)\n\nRelated, one thing that is missing, in my opinion, is a more nuanced outlook of\nwhere the authors believe this work is going. Based on the small scale of the\nexperiments and from reading other related papers in the area, I sense that it\nis hard to scale up differentiable forth to large real-world problems. It\nwould be nice to have more discussion about this, and perhaps even an experiment\nthat demonstrates a failure case. Is there a problem that is somewhat more\ncomplex than the ones that appear in the paper where the approach does not work?\nWhat has been tried to make it work? What are the failure modes? What are the\nchallenges that the authors believe need to be overcome to make this work.\n\nOverall, I think this paper deserves consideration for being provocative.\nHowever, I'm hesitant to strongly recommend acceptance because the experiments\nare weak.", "IS_META_REVIEW": true}, {"TITLE": "ICLR committee final decision", "OTHER_KEYS": "ICLR 2017 pcs", "comments": "This work is stood out for many reviewers in terms of it's clarity (\"pleasure to read\") and originality, with reviewers calling it \"very ambitious\" and \"provocative\". Reviewers find the approach novel, and to fill an interesting niche in the area. All the reviewers were interested in the results, even if they did not buy completely the motivation (what \"practically gained from this formulation\", how does this fit in with prob programming).\n \n The main quality and impact issue is the lack of experimental results and baselines. Several reviewers find that the experiments \"do not fit the claims\", and ask for any type of baselines, even just enumeration. Lacking empirical evidence, there is a desire for a future plan showing what this type of approach could be useful for, even if it cannot really scale. I recommend this paper to be submitted to the workshop track.", "IS_META_REVIEW": false, "DATE": "06 Feb 2017", "is_meta_review": false}, {"TITLE": "New revision overview", "OTHER_KEYS": "Matko Bosnjak", "comments": "Dear reviewers, thank you for your thoughtful reviews. Based on your comments we uploaded an updated version of the paper in which we:\n- added results of a Seq2Seq baseline to both sorting and adding tasks\n- added a much more detailed quantitative analysis, regarding at which sequence length the models are not able to generalise, together with a qualitative evaluation\n- expanded the addition task section with an additional sketch (choose), more text clarifying the sketches and a detailed quantitative analysis\n- added a \u201cDiscussion\u201d section clearly stating the limitations of the presented framework, and discussing failures\nexpanded the conclusion with additional future work, showing both nuanced outlook, and the long-term goals of the framework\n- fixed Figure 2 and added details to section 3.3.1", "IS_META_REVIEW": false, "DATE": "17 Jan 2017", "is_meta_review": false}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "No Title", "is_meta_review": false, "comments": "This paper presents an approach to make a programming language (Forth) interpreter differentiable such that it can learn the implementation of high-level instruction from provided examples. The paper is well-written and the research is well-motivated. Overall, I find this paper is interesting and pleasure to read.  However, the experiments only serve as proof of concept. A more detailed empirical studies can strength the paper. \n\nComments:\n\n- To my knowledge, the proposed approach is novel and nicely bridge programming by example and sketches by programmers. The proposed approach borrow some ideas from probabilistic programming and Neural Turing Machine, but it is significantly different from these methods. It also presents optimisations of the interpreter to speed-up the training. \n\n- It would be interesting to present results on different types of programming problems and see how complex of low-level code can be generated. \n", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "20 Dec 2016", "REVIEWER_CONFIDENCE": 2}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "Review", "is_meta_review": false, "comments": "This paper presents an approach to do (structured) program induction based on program sketches in Forth (a simple stack based language). They turn the overall too open problem of program induction into a slot filling problem, with a differentiable Forth interpreter, for which one can backprop through  the slots (as they are random variables). The point of having sketches/partial programs is that one can learn more complex programs than starting from scratch (with no prior information). The loss that they optimize (end to end through the program flow) is a L2 (RMSE) of the program memory (at targeted/non-masked adresses) and the desired output. They show that they can learn addition, and bubble sort, both with a Permute (3-way) sketch and with a Compare (2-way) sketch.\n\nThe idea of making a language fully differentiable to write partial programs (sketches) and have them completed was previously explored in the  probabilistic programming community and more recently with TerpreT. I think that using Forth (a very simple stack-based language) as the sketch definition language is interesting in itself, as it is between machine code (Neural Turing Machine, Stack RNN, Neural RAM approaches...) and higher level languages (Church, TerpreT, ProbLog...).\n\nSection 3.3.1 (and Figure 2) could be made clearer (explain the color code, explain the parallel between D and the input list).\n\nThe experimental section is quite sparse, even for learning to sort, there is only one experimental setting (train on length 3 and test on length 8), and .e.g no study of the length at which the generalization breaks (it seems that it possibly does not), no study of the \"relative runtime improvement\" w.r.t. the training set (in size and length of input sequences). There are no baselines (not even at least exhaustive search, one of the neural approaches would be a plus) to compare to. Similarly, the \"addition\" experiment (section 4.2) is very shortly described, and there are no baselines either (whereas this is a staple of \"neural approaches\" to program induction). Does \"The presented sketch, when trained on single-digit addition examples, successfully learns the addition, and generalises to longer sequences.\" mean that it generalizes to three digits or more?\n\nOverall, the paper is very interesting, but it seems to me like the experiments do not support the claims, nor the usefulness, enough.", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 2}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "No Title", "is_meta_review": false, "comments": "This paper develops a differentiable interpreter for the Forth programming\nlanguage. This enables writing a program \"sketch\" (a program with parts left\nout), with a hole to be filled in based upon learning from input-output\nexamples. The main technical development is to start with an abstract machine\nfor the Forth language, and then to make all of the operations differentiable.\nThe technique for making operations differentiable is analogous to what is done\nin models like Neural Turing Machine and Stack RNN. Special syntax is developed\nfor specifying holes, which gives the pattern about what data should be read\nwhen filling in the hole, which data should be written, and what the rough\nstructure of the model that fills the hole should be. Motivation for why one\nshould want to do this is that it enables composing program sketches with other\ndifferentiable models like standard neural networks, but the experiments focus\non sorting and addition tasks with relatively small degrees of freedom for how\nto fill in the holes.\n\nExperimentally, result show that sorting and addition can be learned given\nstrong sketches.\n\nThe aim of this paper is very ambitious: convert a full programming language to\nbe differentiable, and I admire this ambition. The idea is provocative and I\nthink will inspire people in the ICLR community.\n\nThe main weakness is that the experiments are somewhat trivial and there are no\nbaselines. I believe that simply enumerating possible values to fill in the\nholes would work better, and if that is possible, then it's not clear to me what\nis practically gained from this formulation. (The authors argue that the point\nis to compose differentiable Forth sketches with neural networks sitting below,\nbut if the holes can be filled by brute force, then could the underlying neural\nnetwork not be separately trained to maximize the probability assigned to any\nfilling of the hole that produces the correct input-output behavior?)\n\nRelated, one thing that is missing, in my opinion, is a more nuanced outlook of\nwhere the authors believe this work is going. Based on the small scale of the\nexperiments and from reading other related papers in the area, I sense that it\nis hard to scale up differentiable forth to large real-world problems. It\nwould be nice to have more discussion about this, and perhaps even an experiment\nthat demonstrates a failure case. Is there a problem that is somewhat more\ncomplex than the ones that appear in the paper where the approach does not work?\nWhat has been tried to make it work? What are the failure modes? What are the\nchallenges that the authors believe need to be overcome to make this work.\n\nOverall, I think this paper deserves consideration for being provocative.\nHowever, I'm hesitant to strongly recommend acceptance because the experiments\nare weak.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Questions", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "", "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "is_meta_review": false}], "SCORE": 6, "authors": "Matko Bo\u0161njak, Tim Rockt\u00e4schel, Jason Naradowsky, Sebastian Riedel", "KEYWORDS": "This paper presents the first neural implementation of an abstract machine for an actual language, allowing programmers to inject prior procedural knowledge into neural architectures in a straightforward manner.", "accepted": false, "id": ""}
