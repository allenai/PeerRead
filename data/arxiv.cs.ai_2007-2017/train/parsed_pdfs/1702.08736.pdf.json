{
  "name" : "1702.08736.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Analysing Congestion Problems in Multi-agent Reinforcement Learning",
    "authors" : [ "Roxana Rădulescu", "Peter Vrancx", "Ann Nowé" ],
    "emails" : [ "rradules@vub.ac.be", "pvrancx@vub.ac.be", "anowe@vub.ac.be" ],
    "sections" : [ {
      "heading" : null,
      "text" : "CCS Concepts •Computing methodologies → Multi-agent systems; Multi-agent reinforcement learning;\nKeywords Multi-agent reinforcement learning; Congestion problems; Resource abstraction ∗This paper expands our AAMAS 2017 extended abstract [9] with a detailed description of the RND problem domain, extensive analysis of the resource abstraction method, and additional analysis of the experimental results.\nAppears in: Proceedings of the 16th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2017), S. Das, E. Durfee, K. Larson, M. Winikoff (eds.), May 8–12, 2017, São Paulo, Brazil. Copyright c© 2017, International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved."
    }, {
      "heading" : "1. INTRODUCTION",
      "text" : "Solving congestion problems is an important research area, as they are present in a wide variety of domains such as traffic control [16], air traffic management [2], data routing [14] and sensor networks [15]. Multi-agent reinforcement learning (MARL) has proven to be a suitable framework for such problems [13, 5, 8, 3], as it allows autonomous agents to learn in a decentralized manner, by interacting within a common environment.\nWe are interested here in MARL scenarios of independent learners, where no direct communication takes place between the agents. As the task at hand is a resource selection problem where the number of agents exceeds the available capacity, the agents should coordinate in order to achieve a high system utility. We consider here two main approaches designed to achieve this goal: difference rewards [17] and resource abstraction [8].\nDifference rewards is a reward shaping approach that aims to align the interests of agents and the system, as well as to tackle the credit assignment problem in MARL, by informing each agent of its own contribution to the performance of the system. Resource abstraction is a recent approach designed to offer a more informative reward signal that improves learning speed, coordination between agents, as well as the final solution quality. In [8] the authors have shown to outperform difference rewards, however as we show in Section 5, we cannot confirm this conclusion. Our first contribution is that we provide clear insights on how resource abstraction guides the collective behaviour of the agents and we highlight the method’s limitations, assumptions and application guidelines.\nCurrent benchmark congestion problems present in the literature often make unrealistic assumptions regarding the independence between the available resources. In complex network management domains, such as smart grids and traffic networks, resources are connected and interdependent, such that using one resource impacts the load of others as well. For this purpose we introduce the Road Network Domain (RND), a problem that models the resources as a system of interconnected roads. We proceed to demonstrate the application of state-of-the-art MARL methods on this problem and analyse their capacity of capturing the newly introduced dynamics in the environment.\nThe rest of the paper is organized as follows: Section 2 presents an overview of the theoretical concepts concerning the congestion problem and describes the two main considered resource selection tasks, Section 3 offers an in depth explanation of the recently introduced resource abstraction\nar X\niv :1\n70 2.\n08 73\n6v 1\n[ cs\n.M A\n] 2\n8 Fe\nb 20\n17\nmethod, Section 4 introduces a new congestion problem and how to apply existing methods for solving it, Section 5 presents and discusses the performed experiments and results, and finally Section 6 offers some concluding remarks and future possible directions."
    }, {
      "heading" : "2. BACKGROUND",
      "text" : ""
    }, {
      "heading" : "2.1 Reinforcement Learning",
      "text" : "Reinforcement Learning (RL) [10] is a machine learning approach which allows an agent to learn how to solve a task by interacting with the environment, given feedback in the form of a reward signal. The solution consists in finding a policy, i.e., a mapping between states and actions that maximizes the received rewards. Q-learning is a common RL value-based method, in which a value function is iteratively updated to optimize the expected long-term reward. After a transition from environment state s to s′, through action a, Q-learning performs the following update on its estimated state-action value function Q, which represents the quality of actions in given states:\nQ(s, a) = Q(s, a) + α[r + γmax a′\nQ(s′, a′)−Q(s, a)]\nwhere α is the learning rate, γ is the discount factor and r is the immediate reward received from the environment. In order to address the exploration-exploitation trade-off challenge in RL, one can use the -greedy action selection method, which allows the agent to choose exploratory random actions with a probability .\nWhen transitioning to the multi-agent case, we consider the scenario of independent learners interacting in the same environment. Solving a congestion problem can then be viewed from two perspectives – agent-centred and systemcentred – that are often in conflict. Allowing selfish entities to act in their own interest in a resource-sharing system can lead to a tragedy of the commons situation [6], which is a detrimental outcome for both the system and the agents.\nIn multi-agent reinforcement learning a central concern is thus providing a reward signal that will offer a beneficial collective behaviour at the system level. A first approach is providing a local reward (L) which reflects information about the parts of the system the agent is involved in. L is individually received by each agent and encourages a selfish behaviour, as agents try to optimize their own reward. An alternative approach is the global reward (G) which reflects the global system utility and should stimulate agents to perform actions beneficial for the system. The global reward signal can incorporate a significant amount of noise, as the individual effect of an agent on the system’s utility can be overshadowed by other learners’ effect, i.e., credit assignment problem. Additionally, in large systems, aggregating at each time-step over all the components can be more costly than relying on local information for the reward computation."
    }, {
      "heading" : "2.2 Resource Selection Congestion Problems",
      "text" : "A congestion problem from a multi-agent learning perspective is defined by a set of n available resources Ψ = {ψ1, ..., ψn}. Each resource ψ is defined by three properties: ψ = 〈wψ, cψ, xψ,t〉, where wψ ≥ 0 represents the weighting of the resource, cψ > 0 is the capacity of ψ and finally xψ,t ≥ 0 is the consumption of ψ at time t. A resource ψ is\ncongested when xψ,t > cψ. The local utility of a resource ψ is defined in terms of its properties:\nL(ψ, t) = f(xψ,t, cψ, wψ) (1)\nIn this paper we consider two resource selection problems that have become benchmark problems for studying resource allocation in RL. They mainly differ with respect to their local utility schemes. The first problem is defined as the beach problem domain (BPD) [12], where all the available resources are considered beach sections with the same weight equal to 1 and the same capacity c:\nL(ψ, t) = xψ,te −xψ,t c (2)\nThe second type of problem is the traffic lane domain (TLD) [11], where the agents have to select between several available lanes, each having a different capacity and weight (reflecting the importance or desirability of the lane):\nL(ψ, t) = wψe −1 , xψ,t ≤ cψ\nwψe −xψ,t cψ , xψ,t > cψ\n(3)\nFor both problems the global utility is defined as the sum over all the local utilities at time t:\nG(t) = ∑ ψ∈Ψ L(ψ, t) (4)\nAt each time step the agents choose to move to a certain beach section or traffic lane and receive a reinforcement in accordance to the effect of their joint action on the implemented reward scheme. The main difference between the two described local utility functions is represented by the segment before the congestion point is reached. For BPD (Figure 1a) the maximum utility for a resource is achieved at optimum capacity (xψ,t = cψ), while for TLD (Figure 1b) this condition is less strict, only requiring the lanes to be under the congestion point (xψ,t ≤ cψ).\nIf the number of agents exceeds the total capacity, the configuration achieving the highest global utility for these benchmark problems is one that overcrowds one of the resources and leaves the rest at optimum capacity. For the BPD the congested resource can be any of the available beach sections, while for the TLD it should be the lane with the lowest weight and highest capacity combination [5]."
    }, {
      "heading" : "2.3 Difference Rewards",
      "text" : "Driven by the idea that the reward signals should allow the agents to deduce their individual contribution to the system, a reward signal we consider here and which was introduced in [17] is difference rewards (D). Under a global system utility G, the difference rewards for agent i is defined as:\nDi(z) = G(z)−G(z−i) (5)\nwhere z denotes a general term for either state, action or state-action pair, according to the considered domain, and G(z−i) is the global utility of a system from which the contribution of agent i is removed. The difference rewards signal was constructed following two main guidelines: (i) aligning the reward received by the agent with the global system utility – factoredness, while (ii) lowering the impact of other agents on the reward signal – learnability, thus addressing the credit assignment problem in a multi-agent setting [1,\n12]. These characteristics have proven to significantly improve learning performance [11, 7, 13], even in domains where G(z−i) cannot be directly computed and should be estimated [2, 4, 3].\nWe now take a look at how the presented credit assignment approach can be defined for these congestion models. By combining Equations 2, 4 and 5 we obtain the following formulation of the difference rewards for the BPD case:\nDi(t) = L(ψ, t)− (xψ,t − 1)e −(xψ,t−1) c (6)\nas the impact of agent i is solely limited to the chosen resource ψ, thus all the other terms in the sum composing the global reward cancel out. The same approach can be used for Equations 3 and 5 for the TLD case."
    }, {
      "heading" : "2.4 Resource Abstraction",
      "text" : "Resource abstraction [8] is an approach that aims to improve learning speed, solution quality and scalability in large MARL congestion problems. Resource abstraction provides the agents with a more informative reward, facilitating coordination in systems with up to 1000 agents [8].\nResource abstraction entails grouping a set of resources into disjoint subsets, and modifying the local reward function after the congestion point of a resource is reached, such that agents using it will get a higher penalty for overcrowding the resource.\nAn abstract group is defined by aggregating the properties of the composing resources. In the congestion model defined above, an abstract group b has the following properties: consumption Xb,t = ∑ ψ∈b xψ,t, capacity Cb = ∑ ψ∈b cψ and weight Wb = 1 |b| ∑ ψ∈b wψ. In other words, resource abstraction creates virtual resources which are agglomerations of resources. Given a resource ψ and the abstract group b to which it belongs to, the abstract reward for an agents using resource ψ at time t for the BPD is defined as:\nA(b, ψ, t) =\n{ L(ψ, t), xψ,t ≤ cψ\n−Xb,te −Xb,t Cb , xψ,t > cψ\n(7)\nThe same approach can be used for the TLD case."
    }, {
      "heading" : "3. ANALYSING RESOURCE ABSTRACTION",
      "text" : "Notice that, in order to apply resource abstraction, information is required regarding the weight, capacity and consumption of the resources as well as the system utility function, limiting the straightforward applicability on a real-world domain, where such information might not be available. Even though the work [8] presents a few guidelines and remarks on how the resource abstraction should be applied, clear insights and explanations on how to create the group abstractions are not present. We consider that a more thorough understanding of the method is beneficial to extend its usability and applicability.\nFor a better understanding of how the resource abstraction impacts the collective behaviour of the agents, we turn to Figure 1c. We plot the abstract reward function of various groups in the BPD, differing in the number of composing resources (ranging from size 1 to size 5). There are 5 available beach sections, each with weight equal to 1 and capacity equal to 6, giving a total capacity of 30 for the entire beach. We fill the resources uniformly until we reach the maximum capacity, after which we proceed to overcrowd each of the abstract groups (by overcrowding one of the composing resources).\nWe first remark that after the congestion point of a resource is reached, the abstract reward mirrors the reward function over the x-axis (this is due to the second case of the function presented in Equation 7). This causes the initial penalty for starting to overcrowd a section to be more severe (i.e., a negative reward) compared to the local reward presented in Figure 1a. However, continuing to overcrowd a section results in smaller and smaller penalties and given enough agents it will eventually converge to 0.\nA second remark can be made on the effect of the group size. The initial penalty for congesting a resource is correlated with the size of the group, thus we expect agents to prefer overcrowding resources that are part of the smallest abstract groups (e.g., in the case presented in Figure 1c starting to overcrowd a group of size 1 returns a reward of -2, while for a group of size 5 the reward is around -11). The same reasoning can be applied for the TLD.\nLastly, we note that in order to determine the best configuration for the abstract groups, one should also have knowledge on the final desired collective behaviour of the agents (e.g., for the BDP having a group of size 1 should lead to the\ndesired ”overcrowd one” behaviour). For more complex domains, finding the optimal abstracts grouping can prove to be impossible, as we show in Section 5, due to the fact that the resource abstraction approach can no longer capture the required collective behaviour."
    }, {
      "heading" : "4. ROAD NETWORK DOMAIN",
      "text" : "We propose the Road Network Domain (RND), a problem that introduces a scenario in which the resources are not independent, as using one path introduces additional load for others as well. We model this problem as a network of roads (e.g., Figure 2), where the agents have to choose between paths of the network. Figure 2 presents an example of a small network topology that can be explored, but that already serves the points we want to make. Each road segment is modelled as a resource, corresponding to the description presented in Section 2.2. The RND can be used with the utility functions of both BPD and TLD, with the former creating a more challenging task, as the maximum value of the utility is only achieved at optimum capacity. The local reward of a path P is then simply the sum over all the local rewards of the composing road segments ψ (e.g., roads AB and BD for the path ABD):\nLpath(P, t) = ∑ ψ∈P L(ψ, t) (8)\nWe compute the global system utility by summing over all the local rewards of the roads segments present in the network (see Equation 4).1\nInanimate\nAnimate\nConcrete UncountableCountable\nICU: hout steen rijst\nIAC: kleur cijfer dag\nAbstract\nICC: dak sleutel fles\nACC: hond vis hert\nIAU: weer geluk hulp\nA\nB\nC\nD\nFigure 2: Topology example for the RND problem. The agents have to travel from point A to point D and have 3 possible paths: ABD, ABCD, or ACD.\nWe consider that the RND introduces a challenge that is often present in real-world domains (e.g., electricity grids, traffic networks). Additionally, one can always increase the difficul y of the problem by creating more complex network structures and can easily translate in this model any realworld situation of interest."
    }, {
      "heading" : "4.1 Difference Rewards",
      "text" : "As the impact of agent i on the system is limited to the composing road segments of his chosen path P , we can define the difference rewards for the RND as follows, where f is a local reward function (see Equation 1):\nDi(t) = Lpath(P, t)− Lpath(P−i, t) = Lpath(P, t)− ∑ ψ∈P f(xψ,t − 1, cψ, wψ) (9)\n1We sum over road segments rather than over paths in order to avoid having segments that belong to multiple path contributing more than once to the global utility value."
    }, {
      "heading" : "4.2 Resource Abstraction",
      "text" : "We consider here two approaches for defining the abstract group construction for the resource abstraction method: over road segments or over paths of the network. As a road segment is a resource, the properties of an abstract group over a set of segments coincide with the ones defined in section 2.4. The abstract reward for each road segment ψ and its corresponding group b is defined as:\nA(b, ψ, t) = { L(ψ, t), xψ,t ≤ cψ −f(Xb,t, Cb,Wb), xψ,t > cψ\n(10)\nwith f a local reward function (see Equation 1). Finally, the abstract reward for choosing a path P at time t then becomes the sum over the abstract reward of each composing road segment:\nApath(P, t) = ∑ ψ∈P A(b, ψ, t) (11)\nWe also extend the definition for an abstract group b over a set of paths: consumption Xb,t = ∑ P∈b xP,t, capacity Cb =∑\nP∈b cP and weight Wb = 1 |b| ∑ P∈b wP , where xP,t is the number of agents that choose path P , cP = minψ∈P (cψ) and wP =\n1 |P | ∑ ψ∈P wψ. We consider a path to be congested if\nany of its composing roads is congested. We can now define the abstract reward for a selected path P at time t as:\nA(b, P, t) = { Lpath(P, t), ∀ψ ∈ P : xψ,t ≤ cψ −f(Xb,t, Cb,Wb), ∃ψ ∈ P : xψ,t > cψ\n(12)\nwhere b is the corresponding abstract group of P . Next, we present a series of experiments designed to demonstrate how to best use the resource abstraction method, but also its limitations. Additionally, we test all the presented approaches (local and global rewards, difference rewards and resource abstraction) on the RND, and evaluate which method can best model the underlying dynamics of the environment in the reward signal."
    }, {
      "heading" : "5. EXPERIMENTS",
      "text" : "Each agent uses the Q-learning algorithm with an exploration parameter = 0.05 and an exploration decay rate of 0.9999. As an important aspect of work is to understand and explore resource abstraction, the parameters for the experiment in Section 5.1 were chosen to match the setting used in the original work [8]: learning rate α = 0.1, decay rate for α is 0.9999 and the discount factor γ = 1.0. As for the RND experiments (Section 5.2), after several tests, the parameters chosen for the local reward L, global reward G and difference rewards D are: learning rate α = 0.1, no decay, and the discount factor γ = 0.9."
    }, {
      "heading" : "5.1 Beach Problem Domain",
      "text" : "Our first experiment is performed on the BPD and it aims to explain and demonstrate the use of resource abstraction. We borrow the original setting from [8], with 6 beach sections, each with capacity 6, thus a total system capacity of 36. There are 100 agents (creating a congestion scenario) and the maximum system utility is 11.04, achieved when overcrowding one of the sections with 70 agents, while keeping the other five at the optimum capacity of 6 agents. Each\nagent has three available actions: shift to the resource on the left, shift to the one on the right and maintain position and has 5 time-steps to finalize his action sequence for an episode. We run the scenario for 10 000 episodes and plot the global utility averaged over 50 trials, together with error bars representing the standard deviation at every 500 episodes. Recall that the local reward function for the BPD is the one plotted in Figure 1a.\nFigure 3 presents the results obtained for the BPD for the following reward schemes: local reward L, global reward G, difference rewards D and resource abstraction RA. There are seven different resource abstraction configurations, just like in [8], composed of either two or three abstract groups. For example RA 3 + 2 + 1 denotes that the first 3 sections form one abstract group, the next 2 another one and the last section represents an abstract group on its own.\nGiven the insights presented in the previous section on how the grouping of the resources influences the collective behaviour and knowing that the maximum utility is achieved under the ‘overcrowd one’ behaviour, we expect that the RA with abstract groups containing only one resource should attain the best performances. The results presented in Figure 3 confirm our expectations and provide the following ranking of the RA configurations: all the variations containing a group of one resource achieve the best performance, RA configurations where 2 is the smallest group size come in second, while the grouping RA 3 + 3 comes in last, but still above the G and L reward schemes schemes.\nAn important remark we make here concerns the performance of the difference rewards D. In our experiment D ranks among the best performing methods, in contrast to the results obtained in [8], where D plateaued around the value of 8. We also note that the difference rewards application to the BPD described in Equation 14 of the work [8] does not correspond to the equation we consider to be the correct one (Equation 6). 2\n2Di(t) = L(ψ, t) − xψ,te −(xψ,t−1)\nc [8], whereas the second term should be a function of (xψ,t − 1).\nFigures 4, 5, and 6 present the final distributions of agents over the resources for the configurations RA 4+2, RA 5+1, and RA 2 + 1 + 3. The results are averaged over 50 trials and each bar plot is accompanied by the standard deviation error. Notice that in each case the agents overcrowd the resources corresponding to the smallest abstract group. RA 5+1 and RA 2+1+3 present two examples of how the optimal configuration can look like. We note that we did not include the representations for RA 3+2+1 and RA 1+3+2 as they are similar to RA 2 + 1 + 3. Visualizing the distribution for configurations like RA 2 + 2 + 2 or RA 3 + 3 is not possible, as the agents can end up overcrowding any of the abstract groups."
    }, {
      "heading" : "5.2 Road Network Domain",
      "text" : "For our next experiments we test how the considered reward schemes perform on the newly introduced Road Network Domain. As previously mentioned, RND is designed to work with any of the local utility functions presented in Section 2.2. We create two types experiments, one using the local utility of BPD and another with TLD. For all the experiments we use the network topology presented in Figure 2. There are 50 agents, all starting at point A and having to reach point D. They can choose between three paths: ABD, ACD, or ABCD. Each RA configuration is expressed using a bracket notation depicting the abstract groups over either paths or road segments. For example, in the setting [ABD,ACD], [ABCD] there are two groups: one of size 2 (ABD and ACD) and one of size 1 (ABCD).\nFor the BPD utility case all roads have a capacity of 5 and weight 1. We run the first experiment, using RA over the paths of the network, for 20 000 episodes and plot the global utility averaged over 30 trials, together with error bars representing the standard deviation at every 1 000 episodes. Figure 7 exemplifies one of the possible optimum configurations. The maximum global utility for this scenario is 5.22.\nThe results are presented in Figure 8. We remark that none of the RA settings is able to capture the underlying optimal configuration, as this can no longer be expressed in terms of a pure ‘overcrowd one’ behaviour, due to the dependencies among the resources and the shape of the utility function. This is a situation for which there does not exist an abstract grouping to guide the agents towards the optimal performance, [ABD,ACD], [ABCD] even performs worse than L or G, as this setting drives agents to overcrowd the path ABCD, contributing to the congestion of the other two paths as well. On the other hand, D manages to achieve\nthe optimal performance in this scenario, demonstrating its capacity to allow agents to adapt to more difficult environment dynamics.\nWe perform a second experiment using the BPD local utility scheme, defining the RA over road segments this time, in order to verify whether at this resource granularity level we can achieve an optimum group abstraction. Notice that, in this case, the number of possible RA settings is much higher, making the decision about the abstract group creation even harder. The L, G, and D results are the same as in Fig-\nure 8. We note that additional smoothing was performed, to improve the visibility of the results obtained for the RA and each plot is accompanied by the standard error. The results of this experiment are presented in Figure 9. We remark again that none of the RA settings manages to reach the optimum solution, with the best performing one being [AB], [AC], [CD], [BC,BD]. A visual representation of this best performing grouping can be found in Figure 7: the two segments that need to be kept under the congestion point (BC,BD) form the largest group, while all the others form their own abstract group. Thus, the RND problem has allowed us to demonstrate that having disjoint abstract groupings is not a sufficient condition for being able to reach an optimum solution and that the necessity of having independent resources goes beyond having segments not belonging to the same abstract group.\nTo better understand these results, we can turn again to Figure 7. Notice that even though the capacity of the road segments is 5, the optimum configuration does not include any segments having reached this value. We conclude that we cannot express the solution as ‘overcrowd these segments and keep the rest at optimum capacity’, thus being unable to properly express the desired solution using theRA approach.\nFor the TLD utility case we use the network scenario described in Figure 10. Increasing the weights for AC and BD determines the maximum global utility to be achieved when avoiding to overcrowd these segments and their corresponding paths. Additionally, because the condition for receiving the highest utility for a road segment is less strict compared to the BPD utility scheme (maintaining the consumption under the congestion point versus reaching the optimum capacity), the maximum global utility is achieved when overcrowding the path ABCD. An example of such a configuration is presented in Figure 11. We thus expect RA over paths to display a good performance in this scenario, as it should be able to capture the desired behaviour. We run this experiment for 2 000 episodes and plot the global utility averaged over 30 trials, together with error bars representing the standard deviation at every 100 episodes. The highest global utility is 3.68.\nInanimate\nAnimate\nConcrete UncountableCountable\nICU: hout steen rijst\nIAC: kleur cijfer dag\nAbstract\nICC: dak sleutel fles\nACC: hond vis hert\nIAU: weer geluk hulp\nA\nB\nC\nD\nc = 3, w = 1 c = 7, w = 5\nc = 5, w = 1\nc = 7, w = 5 c = 3, w = 1\nFigure 10: Weights and capacities for each road segment in the RND scenario considered under the TLD local utility.\nFigure 12 presents the obtained results. We notice that in all cases the convergence takes place very fast, however the quality of the solutions vary from one setting to another. The difference rewards approach, D, manages again to achieve optimum performance. As expected, the RA configuration [ABD,ACD], [ABCD] also performs optimally, as it explici ly ncourages the congestion of the path ABCD. [ABD], [ABCD], [ACD] comes close to the optimum performance, however the agents will not always overcrowd path ABCD in this case. The next two configurations,\nInanimate\nAnimate\nConcrete UncountableCountable\nICU: hout steen rijst\nIAC: kleur cijfer dag\nAbstract\nICC: dak sleutel fles\nACC: hond vis hert\nIAU: weer geluk hulp\n0 500 1000 1500 2000 Episodes\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\nG lo\nb a l u ti\nlit y\nL G\nD [ABD,ACD],[ABCD]\n[ABD],[ABCD,ACD] [ABD],[ABCD],[ACD]\n[ABD,ABCD],[ACD]\nFigure 12: RND with TLD local utility, 50 agents.\n[ABD,ABCD], [ACD] and [ABD], [ABCD,ACD], still perform better than L and G, although they encourage a suboptimal behaviour (overcrowd path ACD or ABD respectively). Notice that in comparison to the results in Figure 8, [ABD,ACD], [ABCD] goes from being the worst performing setting to one of the best. This emphasizes how important it is to have insights on what the desired collective behaviour of the agents is, in order to provide well performing abstract groupings."
    }, {
      "heading" : "6. DISCUSSION AND CONCLUSIONS",
      "text" : "The contribution of this work is two-fold. Firstly, we introduce a new congestion problem, the Road Network Domain, in which the resources are no longer independent, and the selection of one path influences the load in other parts of the network. Secondly, we provide a thorough analysis of the resource abstraction approach for resource selection congestion problems, together with its limitations and clear guidelines on how to best create the abstract groups according to the desired outcome.\nThe Road Network Domain presents a novel challenge for resource selection congestion problems, introducing the realistic aspect of interconnected resources as we often find in real-word application such as: electricity grids or traffic networks. We note that the network topology used here is a small one, yet sufficient to illustrate the additional challenge, and that more research is necessary in order to evaluate scenarios that closely model real-world situations.\nWhile resource abstraction seems to provide a strong method of guiding agents towards an ‘overcrowd one’ behaviour, it fails when the optimal configuration can no longer be expressed in these terms. Additionally, we consider this method to have a limited applicability in real-world domains, as it requires information regarding each composing resource (capacity, weight, consumption), as well as some intuition on the problem’s utility function.\nWe have also shown that the difference rewards approach achieves a high performance in all the tested scenarios, managing to capture in the reward signal the necessary information to allow the agents to coordinate indirectly, even in more complex scenarios as the RND."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work is supported by Flanders Innovation & Entrepreneurship (VLAIO), SBO project 140047: Stable MultI-agent LEarnIng for neTworks (SMILE-IT). We are also grateful for all the helpful comments and discussions with our VUB AI Lab colleagues at each stage of this work."
    } ],
    "references" : [ {
      "title" : "Analyzing and visualizing multiagent rewards in dynamic and stochastic domains",
      "author" : [ "A.K. Agogino", "K. Tumer" ],
      "venue" : "Autonomous Agents and Multi-Agent Systems, 17(2):320–338",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "A multiagent approach to managing air traffic flow",
      "author" : [ "A.K. Agogino", "K. Tumer" ],
      "venue" : "Autonomous Agents and Multi-Agent Systems, 24(1):1–25",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Local Approximation of Difference Evaluation Functions",
      "author" : [ "M. Colby", "T. Duchow-Pressley", "J.J. Chung", "K. Tumer" ],
      "venue" : "Proceedings of the 2016 International Conference on Autonomous Agents & Multiagent Systems, pages 521–529. International Foundation for Autonomous Agents and Multiagent Systems",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Multiagent reinforcement learning in a distributed sensor network with indirect feedback",
      "author" : [ "M. Colby", "K. Tumer" ],
      "venue" : "Proceedings of the 2013 International Conference on Autonomous Agents and Multi-agent Systems, AAMAS ’13, pages 941–948, Richland, SC",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Potential-based difference rewards for multiagent reinforcement learning",
      "author" : [ "S. Devlin", "L. Yliniemi", "D. Kudenko", "K. Tumer" ],
      "venue" : "Proceedings of the 2014 international conference on Autonomous agents and multi-agent systems, pages 165–172. International Foundation for Autonomous Agents and Multiagent Systems",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "The tragedy of the commons",
      "author" : [ "G. Hardin" ],
      "venue" : "Science, 162(3859):1243–1248",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 1968
    }, {
      "title" : "Intrusion response using difference rewards for scalability and online learning",
      "author" : [ "K. Malialis", "S. Devlin", "D. Kudenko" ],
      "venue" : "Workshop on Adaptive and Learning Agents at AAMAS (ALA-14)",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Resource abstraction for reinforcement learning in multiagent congestion problems",
      "author" : [ "K. Malialis", "S. Devlin", "D. Kudenko" ],
      "venue" : "Proceedings of the 2016 International Conference on Autonomous Agents & Multiagent Systems, pages 503–511. International  Foundation for Autonomous Agents and Multiagent Systems",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Analysing Congestion Problems in Multi-agent Reinforcement Learning",
      "author" : [ "R. Rădulescu", "P. Vrancx", "A. Nowé" ],
      "venue" : "Proceedings of the 2017 International Conference on Autonomous Agents & Multiagent Systems. International Foundation for Autonomous Agents and Multiagent Systems",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "Reinforcement learning: An introduction",
      "author" : [ "R.S. Sutton", "A.G. Barto" ],
      "venue" : "MIT press Cambridge",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Traffic congestion management as a learning agent coordination problem",
      "author" : [ "K. Tumer", "A. Agogino", "Z. Welch", "A. Bazzan", "F. Kluegl" ],
      "venue" : "Multiagent Architectures for Traffic and Transportation Engineering, pages 261–279",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Coordinating actions in congestion games: impact of top–down and bottom–up utilities",
      "author" : [ "K. Tumer", "S. Proper" ],
      "venue" : "Autonomous agents and multi-agent systems, 27(3):419–443",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Aligning social welfare and agent preferences to alleviate traffic congestion",
      "author" : [ "K. Tumer", "Z.T. Welch", "A. Agogino" ],
      "venue" : "Proceedings of the 7th international joint conference on Autonomous agents and multiagent systems-Volume 2, pages 655–662. International Foundation for Autonomous Agents and Multiagent Systems",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "TCP-like congestion control for layered multicast data transfer",
      "author" : [ "L. Vicisano", "J. Crowcroft", "L. Rizzo" ],
      "venue" : "INFOCOM’98. Seventeenth Annual Joint Conference of the IEEE Computer and Communications Societies. Proceedings. IEEE, volume 3, pages 996–1003. IEEE",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Coda: Congestion detection and avoidance in sensor networks",
      "author" : [ "C.-Y. Wan", "S.B. Eisenman", "A.T. Campbell" ],
      "venue" : "Proceedings of the 1st International Conference on Embedded Networked Sensor Systems, SenSys ’03, pages 266–279, New York, NY, USA",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Multi-agent reinforcement learning for traffic light control",
      "author" : [ "M. Wiering" ],
      "venue" : "In ICML,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2000
    }, {
      "title" : "Optimal payoff functions for members of collectives",
      "author" : [ "D.H. Wolpert", "K. Tumer" ],
      "venue" : "Advances in Complex Systems, 4(2/3):265–279",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2001
    } ],
    "referenceMentions" : [ {
      "referenceID" : 8,
      "context" : "Multi-agent reinforcement learning; Congestion problems; Resource abstraction ∗This paper expands our AAMAS 2017 extended abstract [9] with a detailed description of the RND problem domain, extensive analysis of the resource abstraction method, and additional analysis of the experimental results.",
      "startOffset" : 131,
      "endOffset" : 134
    }, {
      "referenceID" : 15,
      "context" : "Solving congestion problems is an important research area, as they are present in a wide variety of domains such as traffic control [16], air traffic management [2], data routing [14] and sensor networks [15].",
      "startOffset" : 132,
      "endOffset" : 136
    }, {
      "referenceID" : 1,
      "context" : "Solving congestion problems is an important research area, as they are present in a wide variety of domains such as traffic control [16], air traffic management [2], data routing [14] and sensor networks [15].",
      "startOffset" : 161,
      "endOffset" : 164
    }, {
      "referenceID" : 13,
      "context" : "Solving congestion problems is an important research area, as they are present in a wide variety of domains such as traffic control [16], air traffic management [2], data routing [14] and sensor networks [15].",
      "startOffset" : 179,
      "endOffset" : 183
    }, {
      "referenceID" : 14,
      "context" : "Solving congestion problems is an important research area, as they are present in a wide variety of domains such as traffic control [16], air traffic management [2], data routing [14] and sensor networks [15].",
      "startOffset" : 204,
      "endOffset" : 208
    }, {
      "referenceID" : 12,
      "context" : "Multi-agent reinforcement learning (MARL) has proven to be a suitable framework for such problems [13, 5, 8, 3], as it allows autonomous agents to learn in a decentralized manner, by interacting within a common environment.",
      "startOffset" : 98,
      "endOffset" : 111
    }, {
      "referenceID" : 4,
      "context" : "Multi-agent reinforcement learning (MARL) has proven to be a suitable framework for such problems [13, 5, 8, 3], as it allows autonomous agents to learn in a decentralized manner, by interacting within a common environment.",
      "startOffset" : 98,
      "endOffset" : 111
    }, {
      "referenceID" : 7,
      "context" : "Multi-agent reinforcement learning (MARL) has proven to be a suitable framework for such problems [13, 5, 8, 3], as it allows autonomous agents to learn in a decentralized manner, by interacting within a common environment.",
      "startOffset" : 98,
      "endOffset" : 111
    }, {
      "referenceID" : 2,
      "context" : "Multi-agent reinforcement learning (MARL) has proven to be a suitable framework for such problems [13, 5, 8, 3], as it allows autonomous agents to learn in a decentralized manner, by interacting within a common environment.",
      "startOffset" : 98,
      "endOffset" : 111
    }, {
      "referenceID" : 16,
      "context" : "We consider here two main approaches designed to achieve this goal: difference rewards [17] and resource abstraction [8].",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 7,
      "context" : "We consider here two main approaches designed to achieve this goal: difference rewards [17] and resource abstraction [8].",
      "startOffset" : 117,
      "endOffset" : 120
    }, {
      "referenceID" : 7,
      "context" : "In [8] the authors have shown to outperform difference rewards, however as we show in Section 5, we cannot confirm this conclusion.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 9,
      "context" : "Reinforcement Learning (RL) [10] is a machine learning approach which allows an agent to learn how to solve a task by interacting with the environment, given feedback in the form of a reward signal.",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 5,
      "context" : "Allowing selfish entities to act in their own interest in a resource-sharing system can lead to a tragedy of the commons situation [6], which is a detrimental outcome for both the system and the agents.",
      "startOffset" : 131,
      "endOffset" : 134
    }, {
      "referenceID" : 11,
      "context" : "The first problem is defined as the beach problem domain (BPD) [12], where all the available resources are considered beach sections with the same weight equal to 1 and the same capacity c:",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 10,
      "context" : "The second type of problem is the traffic lane domain (TLD) [11], where the agents have to select between several available lanes, each having a different capacity and weight (reflecting the importance or desirability of the lane):",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 4,
      "context" : "For the BPD the congested resource can be any of the available beach sections, while for the TLD it should be the lane with the lowest weight and highest capacity combination [5].",
      "startOffset" : 175,
      "endOffset" : 178
    }, {
      "referenceID" : 16,
      "context" : "Driven by the idea that the reward signals should allow the agents to deduce their individual contribution to the system, a reward signal we consider here and which was introduced in [17] is difference rewards (D).",
      "startOffset" : 183,
      "endOffset" : 187
    }, {
      "referenceID" : 10,
      "context" : "These characteristics have proven to significantly improve learning performance [11, 7, 13], even in domains where G(z−i) cannot be directly computed and should be estimated [2, 4, 3].",
      "startOffset" : 80,
      "endOffset" : 91
    }, {
      "referenceID" : 6,
      "context" : "These characteristics have proven to significantly improve learning performance [11, 7, 13], even in domains where G(z−i) cannot be directly computed and should be estimated [2, 4, 3].",
      "startOffset" : 80,
      "endOffset" : 91
    }, {
      "referenceID" : 12,
      "context" : "These characteristics have proven to significantly improve learning performance [11, 7, 13], even in domains where G(z−i) cannot be directly computed and should be estimated [2, 4, 3].",
      "startOffset" : 80,
      "endOffset" : 91
    }, {
      "referenceID" : 1,
      "context" : "These characteristics have proven to significantly improve learning performance [11, 7, 13], even in domains where G(z−i) cannot be directly computed and should be estimated [2, 4, 3].",
      "startOffset" : 174,
      "endOffset" : 183
    }, {
      "referenceID" : 3,
      "context" : "These characteristics have proven to significantly improve learning performance [11, 7, 13], even in domains where G(z−i) cannot be directly computed and should be estimated [2, 4, 3].",
      "startOffset" : 174,
      "endOffset" : 183
    }, {
      "referenceID" : 2,
      "context" : "These characteristics have proven to significantly improve learning performance [11, 7, 13], even in domains where G(z−i) cannot be directly computed and should be estimated [2, 4, 3].",
      "startOffset" : 174,
      "endOffset" : 183
    }, {
      "referenceID" : 7,
      "context" : "Resource abstraction [8] is an approach that aims to improve learning speed, solution quality and scalability in large MARL congestion problems.",
      "startOffset" : 21,
      "endOffset" : 24
    }, {
      "referenceID" : 7,
      "context" : "Resource abstraction provides the agents with a more informative reward, facilitating coordination in systems with up to 1000 agents [8].",
      "startOffset" : 133,
      "endOffset" : 136
    }, {
      "referenceID" : 7,
      "context" : "Even though the work [8] presents a few guidelines and remarks on how the resource abstraction should be applied, clear insights and explanations on how to create the group abstractions are not present.",
      "startOffset" : 21,
      "endOffset" : 24
    }, {
      "referenceID" : 7,
      "context" : "1 were chosen to match the setting used in the original work [8]: learning rate α = 0.",
      "startOffset" : 61,
      "endOffset" : 64
    }, {
      "referenceID" : 7,
      "context" : "We borrow the original setting from [8], with 6 beach sections, each with capacity 6, thus a total system capacity of 36.",
      "startOffset" : 36,
      "endOffset" : 39
    }, {
      "referenceID" : 7,
      "context" : "There are seven different resource abstraction configurations, just like in [8], composed of either two or three abstract groups.",
      "startOffset" : 76,
      "endOffset" : 79
    }, {
      "referenceID" : 7,
      "context" : "In our experiment D ranks among the best performing methods, in contrast to the results obtained in [8], where D plateaued around the value of 8.",
      "startOffset" : 100,
      "endOffset" : 103
    }, {
      "referenceID" : 7,
      "context" : "We also note that the difference rewards application to the BPD described in Equation 14 of the work [8] does not correspond to the equation we consider to be the correct one (Equation 6).",
      "startOffset" : 101,
      "endOffset" : 104
    }, {
      "referenceID" : 7,
      "context" : "Di(t) = L(ψ, t) − xψ,te −(xψ,t−1) c [8], whereas the second term should be a function of (xψ,t − 1).",
      "startOffset" : 36,
      "endOffset" : 39
    } ],
    "year" : 2017,
    "abstractText" : "Congestion problems are omnipresent in today’s complex networks and represent a challenge in many research domains. In the context of Multi-agent Reinforcement Learning (MARL), approaches like difference rewards and resource abstraction have shown promising results in tackling such problems. Resource abstraction was shown to be an ideal candidate for solving large-scale resource allocation problems in a fully decentralized manner. However, its performance and applicability strongly depends on some, until now, undocumented assumptions. Two of the main congestion benchmark problems considered in the literature are: the Beach Problem Domain and the Traffic Lane Domain. In both settings the highest system utility is achieved when overcrowding one resource and keeping the rest at optimum capacity. We analyse how abstract grouping can promote this behaviour and how feasible it is to apply this approach in a real-world domain (i.e., what assumptions need to be satisfied and what knowledge is necessary). We introduce a new test problem, the Road Network Domain (RND), where the resources are no longer independent, but rather part of a network (e.g., road network), thus choosing one path will also impact the load on other paths having common road segments. We demonstrate the application of state-of-theart MARL methods for this new congestion model and analyse their performance. RND allows us to highlight an important limitation of resource abstraction and show that the difference rewards approach manages to better capture and inform the agents about the dynamics of the environment.",
    "creator" : "LaTeX with hyperref package"
  }
}