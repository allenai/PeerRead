{
  "name" : "1705.01209.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Lifelong Metric Learning",
    "authors" : [ "Gan Sun", "Yang Cong" ],
    "emails" : [ "sungan@sia.cn", "congyang81@gmail.com", "jliu@cs.rochester.edu", "xwxu@ualr.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Index Terms—Lifelong Learning, Metric Learning, Multi-task Learning, Low-rank Subspace.\nI. INTRODUCTION\nONLINE metric / similarity learning has received re-markable success in a variety of applications [1], [2], [3], such as data mining [4], information retrieval [5] and computer vision [6], [7], mainly due to its high efficiency and scalability to large-scale dataset. Different from conventional batch learning methods that learn metric model offline with all training samples, online learning aims to exploit one or a group of samples each time to update the metric model iteratively, and is ideally appropriate for tasks in which data arrives sequentially.\nHowever, most state-of-the art online metric learning models [8], [9], [10] can only achieve online learning from fixed predefined t (t > 0) metric tasks and cannot add the new task. In this paper, we consider the lifelong learning problem to mimic the “human learning”, i.e., how to extend the current metric to new tasks while the current functionality of the metric remains. For example, in speech recognition, different people pronouncing the same word differs greatly based on their gender, accent, nationality, or other individual characteristics, and it is highly beneficial to leverage the\nG. Sun is with the State Key Laboratory of Robotics, Shenyang Institute of Automation, Chinese Academy of Sciences, University of Chinese Academy of Sciences, China, 110016 e-mail: sungan@sia.cn\nY. Cong is with the State Key Laboratory of Robotics, Shenyang Institute of Automation, Chinese Academy of Sciences, China, 110016 e-mail: congyang81@gmail.com\nJ. Liu is with the Department of Computer Science, University of Rochester, USA, e-mail: jliu@cs.rochester.edu\nX. Xu is with Department of Information Science, University of Arkansas at Little Rock, USA e-mail: xwxu@ualr.edu\nsimilarities of datasets from different types of speakers while adapting to the specifics of each particular users. Therefore, the speech recognition library should be delivered to coming speaker’s recognition with a set of default speech recognition capabilities, and new speaker-specific metric models need to be added. Another motivating example is in image classification system: a metric learning system can identify whether an image contains an apple or banana, however the user wishes to expand this ability to a new task, e.g., detecting an orange. To achieve this goal, most state-of-the-arts [11], [12] should storage training data of all tasks and retrain their models in a time consuming way. Therefore, the key challenge lies on how to learn and accumulate knowledge continuously where early samples are not accessible in the online scenario.\nAs depicted in Fig. 1, in this paper, we propose a new framework, called lifelong metric learning (LML), which intends to learn shared metric parameters from old ones without degrading performance or accessing to the old training data of t tasks. Based on the assumption that all tasks are retained in a low-dimensional common subspace, LML learns a library called “lifelong dictionary” as a set of shared basis for all metric models, while the learned model of t tasks can be considered as a sparse combination of this discriminative lifelong dictionary. Specifically, the lifelong dictionary can be initialized by extracting efficiently from the first training task at different regions via clustering. As new t + 1-th task arrives, LML transfers knowledge through the shared base of lifelong dictionary to learn the new metric model with sparsity regularization, and refines the lifelong dictionary with firstorder information from both the new task and previous tasks. By updating the lifelong dictionary continuously, the fresh knowledge is incorporated into the existing lifelong dictionary, thereby improving the performance of previously learned t models. Therefore, model of new t+1-th task can be obtained without accessing to previous training data. To this end, we evaluate LML framework against state-of-the-art multi-task metric learning methods on several datasets. The experimental results validate encouraging performances of the proposed LML framework.\nThe contributions of this paper include: • To the best of our knowledge, this is the first work about\nonline metric learning from the perspective of lifelong learning, which adopts previous experience and knowledge of t tasks to incorporate and learn the new t+ 1-th task, and can improve the performance in classification accuracy and reduce training time accordingly. • With the support of discriminative “lifelong dictionary”, our proposed lifelong metric learning framework can model a new task via sparse combination, which can reduce the storage burden without saving the training data of previous t tasks but first-order information.\nar X\niv :1\n70 5.\n01 20\n9v 2\n[ cs\n.L G\n] 1\n2 Ju\nn 20\n17\n2\n• We conduct comparisons and experiments with several real-world datasets, which verify the lower computational cost and higher improvement created by our LML framework.\nThe rest of the paper is structured as follows. Section II gives a brief review of some related works. Section III introduces our proposed lifelong metric learning formulation. Section IV then proposes how to solve the proposed model efficiently via online passive aggressive optimization algorithm. In Section V, we report the experimental results and conclude this paper in Section VI."
    }, {
      "heading" : "II. RELATED WORKS",
      "text" : "Metric learning and its related methods have a long history. Depending on whether metric learning incorporates multi-task learning, metric learning can be roughly categorized as: Single Metric Learning and Multi-task Metric Learning."
    }, {
      "heading" : "A. Single Metric Learning",
      "text" : "To the best of our knowledge, seeking a better distance metric through learning with a training dataset is at the key issue of of most state-of-the-art single metric learning models [13], [14], [15]. For the distance metric based researches, the representative approaches can be categorized into two key issues: batch metric learning and online metric learning.\nThe batch metric learning models [16], [17], [18], [19], [20], [21] can further be divided into two categories: models based on nearest neighbors, such as [22] optimizes the expected leave-one-out error of a stochastic nearest classifier in the projection space and [1] proposes the most widelyused Mahalanohis distance learning Large Margin Nearest Neighbors (LMNN), i.e., learning a Mahalanobis distance metric for kNN classification for labeled training examples; models based on pairs/triplets, for instance, [23] searches for a clustering that puts the similar pairs into the same clusters and dissimilar pairs into different clusters; [24] promotes input sparsity by imposing a group sparsity penalty on the learned metric and a trace constraint to encourage output sparsity; [20] proposes a novel low-rank metric learning algorithm to yield\nbilinear similarity functions which can be applicable to highdimensional data domains. However, batch metric learning models which assume all training samples are available prior to the learning phase cannot be applied into many practical applications, due to the fact that only a small amount of training samples are available in the beginning and others would come sequentially. Therefore, researchers focus on the online metric learning and intend to train the classifier with the new coming data.\nFor the online metric learning, [9] designs an Online Algorithm for Scalable Image Similarity learning (OASIS), for learning pairwise similarity that is fast and scales linearly with the number of objects and the number of non-zero features. However, OASIS may suffer from over-fitting and be difficult to be applied in the case of the high dimensions. Furthermore, computational complexity of learning full-rank metric can ranging from O(d2) to O(d6.5), when metric learner lies in a high-dimensional sample space Rd and d is the dimension of the training dataset. In order to overcome over-fitting problem, OMLLR [10] proposes a novel online metric learning model with the low rank constraint, where low-rank metric enables to reduce storage of metric matrices. [25] incorporates large-scale high-dimensional dataset into sparse online metric learning, and explore its application to image retrieval. In addition, LORETA [26] describes an iterative online learning procedure, consisting of a gradient step, followed by a second-order retraction back to the manifold. To incorporate the benefits of both online learning and Mahalanobis distance, LEGO [8] using a Log-Det regularization per instance loss, is guaranteed to yield a positive semidefinite matrix. Furthermore, more details can also be found in two surveys [27] and [28]."
    }, {
      "heading" : "B. Multi-task Metric Learning",
      "text" : "Based on the assumption that the relationships and information shared among the different tasks can be taken into account, multi-task learning [29], [30], [31], [32], [33], [34], [35], [36] aims to improve generalization performance by learning multiple related tasks simultaneously. Furthermore, there are few multi-task metric learning methods designed to make metric learning benefit from training all tasks simultaneously. With the assumption that multiple tasks share a\n3 common Mahalanobis metric and each task has a task-specific metric, mtLMNN [11] adopts the LMNN formulation to the multi-task learning. However, mtLMNN is computationally more complicated, especially in the case of high dimensions. Specifically, there are (t + 1)d2 (t and d denote the task number and data dimension, respectively) parameters to be optimized. Based on low-rank based assumptions, [12] presents\ntransformation matrix to the problem of multi-task metric learning by learning a common subspace for all tasks and an individual metric for each task, where each individual metric is restricted in the common subspace. In addition, mtSCML [37] constructs a common basis set, multi-metric are regularized to be relevant across tasks (as favored by the group sparsity). However, storage and computation will become cumbersome with large scale tasks. Therefore, in order to address the situation that total number of tasks is large or the task is coming consecutively, we employ the common subspace as the lifelong dictionary, and then build a more robust lifelong metric learning framework.\nNotations: For matrix W ∈ Rm×n, let wij be the entry in the i-th row and j-th column of W . Let us define some norms, ‖W‖0 is the number of nonzero entries in W ; denote by ‖W‖1 = ∑m i=1 ∑n j=1 |wij | and ‖W‖∞ = maxi,j |wij | the `1-norm and `∞-norm of W , respectively. Let ‖W‖2,1 = ∑m i=1 ‖wi‖2; denote by sgn(W ), (W )+ and |·| the elementwise sign, positive part elementwise and absolute value of matrix W , respectively. Let be the elementwise multiplication."
    }, {
      "heading" : "III. LIFELONG METRIC LEARNING",
      "text" : ""
    }, {
      "heading" : "A. Preliminaries",
      "text" : "Assume that there are m related tasks. (Xt, Yt) denotes the training set to the t-th task with {xti ∈ Rd̂, i = 1, . . . , nt}, where d̂ and nt are the dimension and the number of the training samples of t-th task, respectively. Define n = ∑m t=1 nt to be the total number of samples, m is the total number of tasks and ft : Rd̂ × Rd̂ → R to be the similarity / distance metric of the t-th learning tasks. The ft is assumed to be defined based on a linear transformation Lt : Rd̂ → Rd (with d d̂ to obtain a low dimensional representation) as: • Similarity Function:\nfLt(xti, xtj) = x T tiL T t Ltxtj =: ft,ij(L T t Lt). (1)\n• Distance Function:\nfLt(xti, xtj) = 4xTt,ijLTt Lt4xt,ij =: ft,ij(LTt Lt), (2)\nwhere xti and xtj are feature vectors, and 4xt,ij = xti−xtj . LTt Lt ∈ Rd̂×d̂ must be positive semi-definite to satisfy the properties of a similarity / distance metric. The set of triplets Tt = {(i, j, k)|(i, j) ∈ St, (i, k) ∈ Dt} are used to define the side-information in Xt, where St and Dt denote all the similar and dissimilar pairs, respectively. For example, fLt(xti, xtj) ≤ fLt(xti, xtk) implies similar data pairs {(xti, xtj)|(i, j) ∈ St} to stay closer than dissimilar pairs {(xti, xtk)|(i, k) ∈ Dt} depending on the similarity / distance metric ft. Without specially specifying, the similarity and distance function are denoted as fLt(xti, xtj) in the following.\nFig. 2. The demonstration of formulation given by Eq. (4), where t-th task can be represented by a series of “atoms” in the lifelong dictionary, and w11 and wij are corresponding weights of the 11-th element and ij-th element, respectively."
    }, {
      "heading" : "B. The Lifelong Metric Learning Problem",
      "text" : "The original intention of multi-task metric learning is to learn an appropriate distance metric ft for t-th task utilizing all the side-information from the joint training set {(X1, Y1), (X2, Y2), . . . , (Xm, Ym)}. Suppose that the loss involved in t-th task is determined by the distance function ft (with metric Lt) and the pairs appearing in St and Dt:\n`t(L T t Lt) = `t ( ft,ij(L T t Lt) ) , ∀(i, j) ∈ St ∪ Dt, (3)\nwhere `t is an arbitrary loss function of t-th task. However, learning new metric task without accessing to the previously used training data is not considered by traditional multi-task metric learning. In the context of multi-task metric learning, a lifelong metric learning system encounters a series of metric learning tasks `1, `2, . . . , `m, where each task `t is defined by Eq. (3). For convenience, we do not assume that the learner knows any information about tasks, e.g., the total number of tasks m, the distribution of these tasks, etc. In each time step, as the lifelong system receives a batch of training data for some metric learning task t, either a new metric task or previously learning task, this system may be asked to make predictions on samples of any previous task. Its goal is to establish task models L1, . . . , Lm such that: • Classification Accuracy: each learned metric Lt should\nclassify the new samples more accurate. • Computation Efficiency: in the training period, each Lt\nshould be updated faster than traditional multi-task metric learning (i.e., joint learning models). • Lifelong Learning: new Lt’s can be added arbitrarily and efficiently when the lifelong system encounters new metric tasks."
    }, {
      "heading" : "C. Lifelong Metric Learning Framework (LML)",
      "text" : "In order to model the correlation among different metric tasks, we assume that the metric matrix ft for t-th task can be represented using a combination of the shared common subspace from a knowledge repository. Moreover, motivated by [12], Theorem 1 gives the detail mathematical description.\nTheorem 1: Let fLt(xti, xtj) denotes the similarity / distance of xti, xtj ∈ Rd̂ defined by the transformation matrix Lt as Eq. (1) or Eq. (2). For any Lt ∈ Rd×d̂ (d d̂), there exists a low dimensional subspace St spanned by orthonormal basis {pt1, . . . , ptd} with metric matrix defined by Rt ∈ Rd×d so that\nfLt(xti, xtj) = fRt(x̂ti, x̂tj),\n4 where x̂ti = PTt xti = [pti, . . . , ptd] Txti ∈ Rd is the coordinate of the projection of xti in St with respect to basis matrix Pt.\nTherefore, metric matrix Lt for t-th task in Theorem 1 can be explicitly decomposed to a low-dimensional metric part Rt and a subspace part Pt. Our Lifelong Metric Learning (LML) framework can be simply represented as to learn an individual metric Rt for each task in a common subspace PTt = L0. Furthermore, as shown in Fig. 2, parameter matrix Mt ∈ Rd̂×d̂ for metric task ft can be expressed as:\nMt = L T t Lt = L T 0 R T t RtL0 = L T 0 WtL0 = d∑ i=1 d∑ j=1 wij lilj ,\n(4) where Wt ∈ Rd×d denotes the weight matrix. Therefore, each metric task Mt can be represented as a linear combination of “lifelong dictionary” composed by lilj ,∀i, j = 1, . . . , d. Generally, since diagonal elements in Wt represents the selfcorrelation of a transformed feature while off-diagonal element represents correlation among different transformed features, diagonal elements should be more dense than those offdiagonal elements. We encourage the off-diagonal elements of Wt’s to be sparse (i.e., use few components among lifelong dictionary) in order to ensure that each learned metric model captures a maximal reusable chunk of knowledge.\nGiven the training data for each task, we optimize the metrics to minimize the loss function over all tasks while encouraging the metrics to share common knowledge in lifelong dictionary. Therefore, LML framework can be formulated as:\nmin L0,{Wt}\n1\nm { m∑ t=1 `t(L T 0 WtL0) ) + λt ‖Wt‖1,off } + γ ‖L0‖2F ,\n(5)\nwhere the ‖·‖1,off -norm of Wt defined as ∑\ni 6=j |Wt,ij | is used as a convex approximation to the true matrix sparsity, and ‖L0‖F = (tr(L0LT0 ))1/2 is the Frobenius norm of matrix L0 to avoid overfitting. The trade-off parameter λt ≥ 0 controls the regularization of ‖Wt‖1,off for all t = 1, . . . ,m. If λt → ∞, the task-specific matrices Wt’s become selfcorrelation diagonal matrices. With the definition of `t in Eq. (3), the final optimization problem of lifelong metric learning can be formulated as:\nmin L0,{Wt}\n1\nm { m∑ t=1 `t ( ft,ij(L T 0 WtL0) ) + λt ‖Wt‖1,off } + γ ‖L0‖2F , (6)\nwhere (i, j) ∈ St ∪ Dt are the side-information in the t-th metric task."
    }, {
      "heading" : "IV. MODEL OPTIMIZATION",
      "text" : "This section provides the detail procedure of how to optimize our proposed LML framework. Since the problem in Eq. (6) is not convex with respect to L0 and Wt’s jointly, the objective function can arrive at a local optimum. A common approach for computing such a local optimum for objective functions in Eq. (6) is to alternately perform two convex\noptimization steps: one in which L0 is optimized by fixing the Wt’s, and another in which the Wt’s are optimized by holding L0 fixed. However, as shown in [38], this approach is inefficient and inapplicable to lifelong learning with many tasks and data samples. This is because that in order to optimize L0, the problem in Eq. (6) has to recompute the value of each Wt’s (which will become time consumption when increasing the number of learned tasks m). To address this problem, we aim to approximate Eq. (6) by applying the online passive aggressive (PA) [39] optimization strategy, i.e.,\nmin L0,{Wt}\n1\nm { m∑ t=1 `t ( ft,ij(L T 0 WtL0) ) + 1 2η ∥∥LT0 WtL0 −Mt∥∥2F + λt ‖Wt‖1,off } + γ ‖L0‖2F ,\n(7) where η is the learning rate. After linearizing the loss function `t around LT0 WtL0 =Mt, we obtain the following new online function:\nmin L0,{Wt}\n1\nm { m∑ t=1 `t ( ft,ij(Mt) ) + 〈LT0 WtL0 −Mt, Gt〉\n+ 1\n2η ∥∥LT0 WtL0 −Mt∥∥2F + λt ‖Wt‖1,off }+ γ ‖L0‖2F , (8)\nwhere Gt is the gradient of `t. We then rewrite the optimization problem in Eq. (8) as:\nmin L0,{Wt}\n1\nm { m∑ t=1 ∥∥LT0 WtL0 − (Mt − ηGt)∥∥2F + λt ‖Wt‖1,off } + γ ‖L0‖2F .\n(9) In Eq. (9), we have suppressed the constant term of the linearize form (since it does not affect the minimum). Crucially, we have removed the dependence of the optimization problem Eq. (6) on the number of the data samples n1, . . . , nt in each task. Additionally, Eq. (9) can be reformulate as:\nmin L0,{Wt}\n1\nm { m∑ t=1 ∥∥LT0 WtL0 −M∗t ∥∥2F + λt ‖Wt‖1,off } + γ ‖L0‖2F , (10)\nwhere M∗t = Mt − ηGt can be approximated from the large samples by online learning or small samples by offline minibatch learning in the t-th task. Moreover, the optimization problem in Eq. (10) also can be roughly divided into two subproblems with alternating direction optimization strategy. After initializing the lifelong dictionary L0, the first subproblem is to compute the optimal Wt for the new coming task M∗t , and the second subproblem is to update the lifelong dictionary L0 by fixing Wt’s."
    }, {
      "heading" : "A. Lifelong Dictionary L0 Initialization",
      "text" : "An high-quality lifelong dictionary plays an important role in our model. In order to generate a set of discriminative basis vectors in L0, we first divide data into different clusters. For each clutter, we select J nearest neighbors from each class (for J = |10, 20, 50| to count for different scales), and apply Fisher\n5 Algorithm 1 Proximal Method for Solving Wt Input: W 0t , V0 ∈ Rd×d, λt ≥ 0, η0 ≥ 0, and MAX-ITER Output: Wt\n1: Initialize W 1t =W 0 t , t−1 = 0, t0 = 1 2: for i = 1, ..., MAX-ITER do 3: Vi−1 =W i t + αi(W i t −W i−1t ); 4: while true do 5: Compute Vi via Eq. (12); 6: Update ηi via backtracking rule. 7: end while 8: W i+1t = Vi, ηi+1 = ηi; 9: if Convergence criteria satisfied then\n10: Wt =W i+1 t ; 11: break; 12: end if 13: end for 14: Return Wt;\nAlgorithm 2 Lifelong Metric Learning Framework Input: Training dataset (X1, Y1), . . . , (Xt, Yt), d̂ ≥ d >\n0, {λt}, γ; Output: {Wt}, L0\n1: Initialize L0 in the first coming task; 2: while Not Converge do 3: New t-th task: (Xnew, Ynew, t) 4: if isNewTask(t) then 5: T ← T + 1 6: Xt ← Xnew, Yt ← Ynew 7: else 8: Xt ← [Xt, Xnew], Yt ← [Yt, Ynew] 9: end if\n10: Mt ← SingleTaskLearner(Xt,Yt); 11: Update Wt via Algorithm 1; 12: Update L0 via Gradient Method; 13: end while 14: Return Wt;\ndiscriminative analysis followed by eigenvalue decomposition to obtain the basis elements."
    }, {
      "heading" : "B. Solve Wt with Given L0",
      "text" : "With the initialized lifelong dictionary L0, Wt is the variable in this subproblem. The optimization function can be rewritten as:\nmin Wt f(Wt) + g(Wt), (11)\nwhere f(Wt) = 12 ∥∥LT0 WtL0 −M∗t ∥∥2F and g(Wt) = λt ‖Wt‖1,off . Due to the non-smooth nature of g(Wt), we propose the proximal gradient method (FISTA) [40] with a fast global convergence rate to solve this optimization problem. Specifically, the proximal operator of the `1,off -norm can be applied to solve this subproblem:\nW i+1t = argmin W\n1\n2 ∥∥W −W it + ηi∇f(W it )∥∥2F+λt ‖W‖1,off , (12)\nwhere ηi > 0 is the stepsize parameter, Eq. (12) can be appropriately determined by the backtracking rule. ∇f(W it ) is the gradient matrix with respect to f(W it ) can be expressed as: ∇f(W it ) = L0LT0 W itL0LT0 − L0MtLT0 . With the gradient of f(W it ), the optimal W i+1 t depends on the proximity operator of the `1,off -norm, i.e., soft thresholding operator:\npros(W,λtηi) = sgn(W ) ( |W |−λtηi(1− I) ) + , (13)\nwhere denotes the elementwise multiplication. Notice that FISTA amounts for using two sequences {W it } and {V it } in which {W it } is the approximate solution and {V it } is search points. Moreover, the proximal method by Solving for Wt is summarized in Algorithm 1."
    }, {
      "heading" : "C. Solve L0 with Given Tt’s and Wt’s",
      "text" : "In order to evaluate the lifelong dictionary L0, we modify the formulation in Eq. (7) to remove the minimization over Wt. Besides, we also remove the second term which is used to keep the new similarity / distance matrix close to the current one. Further, we accomplish this by exploiting both sideinformation Tt (generated according to the adopted base metric learning model) and Wt in the learned tasks. In the following, we try to adopt the gradient descent method to solve L0 in Eq. (7). The gradient of `t with respect to L0 is:\n1\nm m∑ t=1\n∂`t ( ft,ij(L T 0 WtL0) ) ∂L0 + ∂(γ ‖L0‖2F ) ∂L0 ,\n= 1\nm m∑ t=1 ( WTt L04t ) + γL0,\n(14)\nwhere 4t can be calculated with different SingleTaskLearner function using side-information Tt. The LML framework is summarized in Algorithm 2, where SingleTaskLearner is learned using base metric models."
    }, {
      "heading" : "D. Computational Complexity",
      "text" : "For the complexity of our proposed algorithm, the main computational cost in each update in Algorithm 2 involves two subproblems: one optimization problem lies in Eq. (11), another one is Eq. (14).\n1) For the problem in Eq. (11), each update for the LML system begins by base metric learning models to compute Mt, we assume that this step has complexity O(ξ(d̂, n)), where d̂ is the number of feature, n = ∑m t=1 nt and nt\nis the triplets number of Tt in our paper. Next, to update Wt requires solving the instance of lasso, i.e., ||W‖1,off . Each iteration in this problem begins by the computation of the gradient of Wt, and the computational complexity is O(d2d̂+2d3 +2d̂3). Therefore, the cost for achieving -accuracy is O((d2d̂+2d3+2d̂3+ξ(d̂, n))/ √ ), where is determined by the convergence property of Accelerated Gradient Method, i.e., O(1/ √ ) with\nf(Wt) + g(Wt)− f(W ∗)− g(W ∗)\n≤\n√ 2ηt ‖W0 −W ∗‖2F\n− 1.\n(15)\n6\nIn other words, the convergence rate of Algorithm 1 can achieve O(1/T 2) as shown in [40]. Moreover, the multiplication of two matrices can be further reduced with Coppersmith-Winograd algorithm. 2) The optimization algorithm for solving L0 involves the gradient of each triplet in Tt, and the computational complexity is O(d̂2d+ d̂d2).\nFinally, the overall complexity of each update in Algorithm 2 is O(d̂2d+ d̂d2 + (d2d̂+ 2d3 + 2d̂3 + ξ(d̂, n))/ √ )."
    }, {
      "heading" : "E. Discussion",
      "text" : "In this section, we briefly review one learning method that is most related to our proposed learning algorithm. Perhaps the most relevant work to ours in the context of multi-task metric learning is from [37], which frames metric learning as learning a sparse combination of locally discriminative metrics that are generated from the training data via clustering. However, the motivation for SCML and our LML are significantly different: • SCML aims to cast metric learning as learning a sparse\ncombination of basis elements taken from a basis set B = {bi}Ki=1, where the bi’s are d̂-dimensional column vectors. Instead of fixing the metric task number, our LML focuses on the transfer of knowledge from preciously learned tasks to the new metric task using the shared basis, i.e., lifelong task learning. • In SCML, the metric matrix is represented using basis matrices induced by a `1-norm constraint, and the formulation in SCML can only achieve batch learning. Our LML encourages the communication among different basis elements via a `1,off -norm constraint, and the resulting formulation can integrate online sample learning by adopting the SinlgeTaskLearner as online metric learning. Furthermore, we have also conducted extensive experiments on the effect of `1,off -norm in the experiment section. • The optimization algorithm in SCML can only find a local solution with all the training samples. The proposed algorithm for Eq. (10) can learn new metric task without accessing to historical data because only the gradient information of previous training data is adopted in the next iteration in Eq. (14)."
    }, {
      "heading" : "V. EXPERIMENTS",
      "text" : "In this section, we carry out empirical comparisons with the state-of-the-art single and multi-task metric learning models. We first give the base metric learning with our lifelong metric learning framework in Table I with two different function: sMt(xi, xj) = x T i Mtxj and dMt(xi, xj) = (xi − xj)TMt(xi − xj), where xi and xj belong to the same class, while xi and xk are from different\nclasses. The experiments are then conducted on a series of real datasets."
    }, {
      "heading" : "A. Comparison Algorithms and Evaluation",
      "text" : "In our experiments, we compare our LML framework with single metric learning models and multi-task metric learning models. The single metric learning model includes: 1) Euclidean distance (stEuc): the standard Euclidean distance in feature space; 2) OASIS (stOASIS) [9]: the classical online metric learning model which is given in Table I, and its iteration number is 2×104 in our paper; 3) LMNN (stLMNN) [1]: Large Margin Nearest Neighbor Classification, which learns a Mahalanobis distance for k-nearest neighbor classification; 4) SCML-global (stSCML) [37]: which is simply to combine the local basis elements into a higher-rank global metric; 5) LMNN-union (uLMNN): is the LMNN metric obtained on the union of the training data of all tasks (i.e., “pooling” all the training data and ignoring the multi-task aspect). 6) SCMLunion (uSCML): is the SCML metric obtained on the union of the training data of all metric tasks.\nFor the multi-task metric learning models, the comparison models include: • multi-task LMNN (mtLMNN) [11]: common metric de-\nfined by M0 picks up general trends across multiple datasets and Mt specializes the metric further for each particular task. • multi-task SCML (mtSCML) [37]: this multi-task metric learning model considers that all learned metrics can be expressed as combinations of the same basis subset B, though with different weights for each task.\nFor the classical lifelong multi-task learning, we adopt the comparison model as: • Lifelong multi-task (ELLA) [38]: whose formulation is\nrealized by the following objective function:\n1\nm m∑ t=1 min s(t) { 1 nt nt∑ i=1 L ( f(x (t) i ;Ls (t)), y (t) i ) + µ ∥∥∥s(t)∥∥∥ 1 } + λ ‖L‖2F ,\n(16) where (x(t)i , y (t) i ) is the i-th labeled training samples for t-th task, L is a known loss function. Specifically, ELLA maintains a sparsely shared basis vector for all regression or logistic task models, transfers knowledge from the basis to learn new t-th task.\nAll the models are implemented in MATLAB, and the codes are available at the supplement website. Notice that all the parameters of the models are tuned in {10, 1, 0.1, 0.01, 0.001} and selected via 5-fold cross validation. Although our model allows different weights λt for each task, throughout this paper we only adjust our parameters: γ and λ = λt > 0. All the\n7 experiments are performed on the computer with 12G RAM, Intel i7 CPU."
    }, {
      "heading" : "B. Real Datasets",
      "text" : "According to whether the label is consistent or not, we categorize the real datasets into two different scenarios: labelconsistent and label-inconsistent. In the following, we will demonstrate the effectiveness of our proposed LML framework in the different datasets.\nLabel-consistent datasets: the label set is shared by all the metric tasks, which can be roughly categorized as: same metric task and different metric tasks with same label set. Therefore, depending on whether is the same task or not, we adopt two datasets in this paper. As shown in Table IV, Sentiment [41] consists of Amazon reviews on four product types (kitchen appliances, DVDs, books and electronics). We randomly split the dataset into training (800 samples), validation (400 samples) and testing (400 samples) sets. Isolet 1 dataset, which is a popular dataset for multi-task learning consists of 5 disjoint subjects called isolet 1-5. We randomly split each task of the dataset into training (10% samples), validation (20% samples) and testing (70% samples) sets. Moreover, we set the basis number of stSCML as 100 and 500 in Sentiment and Isolet, respectively.\nThe experimental results averaged over five random repetitions are presented in Table II, and we can conclude that: • Compared with other competing methods, our proposed\nLML framework outperforms the state-of-the-arts with the average error as 20.3 and 21.7 and achieving 2.6% and 0.6% improvement in term of classification error using Sentiment and Isolet datasets, which verifies the effectiveness of our LML framework in a lifelong learning manner. Furthermore, the performance of our LML framework is also better than the existing lifelong learning model (ELLA), due to the fact that we adopt the lifelong dictionary in LML framework, which keeps on learning step-by-steps. • For the real datasets, Table II also shows that the comparison of time consumption between our LML framework and other single / multi-task metric models. Our LML framework is more efficient than most state-of-the-arts due to we do not need to retrain all the previous tasks. However, our LML framework is little slower than the multi-task metric model in Isolet and faster than LMNN. This is because we set the high dimensional transformed features. • Similarity metric function outperforms distance metric function on Sentiment dataset, which implies that similarity metric may be important for different tasks; meanwhile, distance metric outperforms similarity metric on the Isolet dataset, which implies that distance metric may be important for this metric task.\nLabel-inconsistent datasets: the label set of coming metric task is different from the learned metric tasks. USPS 2 dataset\n1http://www.cad.zju.edu.cn/home/dengcai/Data/MLData.html 2http://statweb.stanford.edu/ tibs/ElemStatLearn/data.html\nIsolet1 Isolet2 Isolet3 Isolet4 Isolet5 Avg.Error 0\n5\n10\n15\n20\n25\n30\nEr ro\nr( %\n)\nstSCML Ours\nFig. 3. The effect of the ‖·‖1,off -norm. The horizontal and vertical axes are the index of task and classification error of Isolet dataset, respectively. In addition, the red line and black line are the corresponding standard deviation of stSCML and ours, respectively.\nconsists of 7291 16×16 gray-scale images of digits 0−9 automatically. The features are 256-d grayscale values. We split all the classification problem into 4 tasks:{0, 1}, {2, 3}, {4, 5, 6} and {7, 8, 9}, respectively. Therefore, the number of classes of each task is 2, 2, 3, 3. We use randomly selected 10% of the all the samples as training samples while the remaining for test. From the presented result in Table III, we can notice that the performance of our LML framework leads to the second best one with the average error as 2.68, only 0.02 worse than the best one mtSCML and outperforms other state-of-the-arts with a big gap. This is because our LML only train the model using the data from the only one corresponding task, instead of mtSCML adopting the data of all tasks together for model training. That is why ours is more efficient than mtSCML as shown in Table II."
    }, {
      "heading" : "C. Evaluating Lifelong Metric learning Framework",
      "text" : "In this subsection, we conduct comparisons on the proposed lifelong metric learning formulation, and study how the learned task impact its generalization performance.\n1) Effect of the ‖W‖1,off -norm Regularization: In order to study how the ‖W‖1,off -norm regularization affect the performance of the single metric task, we compare the stSCML method with our proposed framework Eq. (5) on the Isolet dataset. Specifically, we remove the regularization term of L0 in Eq. (5), i.e., γ ‖L0‖2F , and employ FISTA [40] to efficiently optimize such a convex problem. We also randomly split each task of the Isolet dataset into training (10% samples), validation (20% samples) and testing (70% samples) sets, and the performance (averaged over 5 random repetitions) is presented in Fig. 3. In general, our model in Eq. (5) outperforms stSCML on all the single learned task expect for task Isolet2. This observation verities that the correlation information among different transformed features enables to improve the learning efficacy, i.e., the effectiveness of ‖W‖1,off -norm.\n2) Effect of the Dimension of Transformed Features: In this subsection, we utilize the Sentiment dataset to evaluate how\n8\nTABLE II SENTIMENT & ISOLET DATASET: CLASSIFICATION ERROR AND TRAINING TIME OF THE COMPETING METRIC LEARNING MODELS. THE REPORTED PERFORMANCE IS AVERAGED OVER FIVE RANDOM REPETITIONS, AND METHODS WITH THE BEST PERFORMANCE ARE MARKED AS BOLDED BLACK.\nDataset Task stEuc stLMNN stSCML stOASIS uLMNN uSCML mtLMNN mtSCML ELLA Ours+OASIS Ours+SCML\nSenti ment\nBooks 33.5 ±0.5 29.7±0.4 27.0±0.5 28.3±0.4 29.6±0.4 28.0±0.4 29.1±0.4 25.8±0.4 32.8±0.5 27.8±0.4 25.3 ±0.5 DVD 33.9±0.5 29.4±0.5 26.8±0.4 23.5±0.4 29.4±0.5 27.9±0.5 29.5±0.5 26.5±0.5 31.0±0.7 23.5±0.5 25.0±0.4 Electronics 26.2±0.4 23.3±0.4 21.1±0.5 20.3±0.4 25.1±0.4 22.9±0.4 22.5±0.4 20.2±0.5 19.0±0.7 18.0±0.4 18.5±0.4 Kitchen 26.2±0.6 21.2±0.5 19.0±0.4 17.3±0.4 23.5±0.3 21.9±0.5 22.1±0.5 19.0±0.4 16.1 ±0.5 12.0±0.4 15.8±0.4\nAvg. Error 30.0±0.2 25.9±0.2 23.5±0.2 22.4±0.3 26.9±0.2 25.2±0.2 25.8±0.2 22.9±0.2 24.8 ± 0.4 20.3±0.2 21.2 ±0.4 Avg. Runtime N/A 11min 12s 0.5min 9min 10s 8min 1min 0.5min 0.5min 18s\nIsolet\nIsolet1 28.9 ±0.0 23.2±0.1 19.6±0.2 24.5±0.2 23.2±0.1 55.3±0.1 21.3±0.7 19.1±0.2 N/A 21.7± 0 16.5±0.1 Isolet2 30.5±0.7 24.4±0.9 20.9±1.6 19.9±0.0 24.4±0.9 52.9±1.0 22.9±0.6 20.1±2.1 N/A 23.3±1.3 22.4±2.1 Isolet3 35.3±1.2 28.4±1.2 24.5±0.3 25.8±0.8 28.4±1.2 53.1±2.8 26.0±1.2 22.9±0.2 N/A 21.0±1.4 23.3±0.0 Isolet4 35.7±0.4 27.2±1.9 25.3±2.4 29.4±2.7 27.2±1.9 53.5±1.8 25.3±2.4 23.8±2.8 N/A 23.1±1.0 25.1±0.2 Isolet5 37.4±0.5 30.2±0.8 26.7±1.0 29.7±0.8 30.2±0.8 54.6±3.1 28.3±1.4 25.7±2.8 N/A 21.4±1.6 27.5±0.4 Avg. Error 33.5±0.3 26.7±0.7 23.4±0.9 25.9±1.1 26.7±0.7 53.9±1.7 24.7±0.7 22.3±1.5 N/A 22.1±1.2 23.0±0.5 Avg. Runtime N/A 4min 0.1min 1min 4min 1s 10min 0.5min N/A 2min 0.3min\nTABLE III USPS DATASET: CLASSIFICATION ERROR OF THE COMPETING METRIC LEARNING MODEL. THE REPORTED PERFORMANCE IS AVERAGED OVER FIVE RANDOM REPETITIONS, AND METHODS WITH THE BEST PERFORMANCE ARE MARKED AS BOLDED BLACK.\nTask #Classes stEuc stLMNN stSCML stOASIS mtLMNN mtSCML ELLA Ours+OASIS Ours+SCML 1 2 0.13 ±0.1 0.09±0.0 0.06±0.0 0.37±0.2 0.09±0.1 0.09±0.1 N/A 0.22±0.2 0.03 ±0.1 2 2 2.20±0.8 2.05±0.6 2.20±0.4 1.86±0.0 2.10±0.6 1.70±0.1 N/A 1.52±0.6 2.12±0.0 3 3 4.19±1.0 3.59±0.8 3.18±0.3 5.23±0.4 3.92±1.2 3.41±1.3 N/A 4.58±0.1 5.74±1.4 4 3 7.00±2.0 6.67±1.6 6.82±1.4 4.40±0.2 6.60±2.0 5.04±0.6 N/A 4.43±1.1 4.51±0.3\nAvg. Error N/A 3.37±0.0 3.10±0.0 3.07±0.1 2.97±0.1 3.18±0.1 2.66±0.2 N/A 2.68±0.1 3.09 ±0.3\nthe dimension of transformed features d affect the performance of our LML framework (Ours+OASIS) in term of classification error. Specifically, we also randomly split the dataset into training (800 samples), validation (400 samples), and testing (400 samples) sets. By varying the number of transformed feature d from 40 to 200, we present the performance (averaged over 5 random repetitions) as shown in Fig. 4. Notice that average classification error changes with different number of transformed features, which verifies that all the metric task should be embedded in a low-dimensional subspace, namely lifelong dictionary in our LML framework. In addition, the error of average classification is minimum when d = 120, i.e., the performance of our LML is best. After that, the classification error is decreasing with the increase of size of d. This is because that the larger size of d, the more redundant feature information can be involved in the lifelong dictionary L.\n3) Effect of the Number of Learned Tasks: In this subsection, we also adopt Sentiment dataset to study how the number of learned tasks t affect the classification performance of our LML framework. After splitting the dataset into training (800 samples), validation (400 samples) and testing (400 samples) sets, we set the sequence of learned t tasks as: Books, DVD, Electronics and Kitchen; we present the classification performance (averaged over 5 random repetitions) in Fig. 5. Obviously, as each metric task is imposed step-by-step, the error of our LML is decreased, i.e., the performance of our LML framework is improved gradually, which justifies that our\nLML framework can accumulate knowledge continuously and achieve lifelong learning like “human learning”. In addition, the performance of early learned tasks are improved more obviously than succeeding task, i.e., the early tasks can benefit from the accumulated knowledge."
    }, {
      "heading" : "VI. CONCLUSION",
      "text" : "In this paper, we study how to add metric task into original metric system without retraining the whole system in a too time consuming way as most state-of-the-art online metric learning models. Specifically, we propose lifelong metric learning (LML) framework, which learns “lifelong dictionary” as shared basis for all metric models based on the assumption that all metric tasks are retained in a low-dimensional common subspace. When new metric task\n9 1 2 3 4 Number of Learned Tasks 14 16 18 20 22 24 26 28 Er ro r ( % ) Books DVD Electronics Kitchen Avg. Error\nFig. 5. The Effect of the Number of Learned Tasks. The horizontal and vertical axes are the number of learned tasks and classification error of each task, respectively. The initial classification error of each task is achieved using sinlge metric learning. Notice that the average error is decreased when increasing the number of tasks\narrives, our LML can transfer knowledge through the shared lifelong dictionary to learn the new coming metric model with sparsity regularization, and redefine the basis metrics with knowledge from the new metric task. After converting this convex problem into two subproblems via Online Passive Aggressive optimization, we adopt proximal gradient method to solve our proposed LML framework. Through extensive experiments carried our on several multi-task datasets, we verify that our proposed framework are well suited to the lifelong learning problem, and exhibit prominent performance in both effectiveness and efficiency."
    } ],
    "references" : [ {
      "title" : "Distance metric learning for large margin nearest neighbor classification",
      "author" : [ "K.Q. Weinberger", "L.K. Saul" ],
      "venue" : "Journal of Machine Learning Research, vol. 10, no. Feb, pp. 207–244, 2009.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Robust distance metric learning with auxiliary knowledge.",
      "author" : [ "Z.-J. Zha", "T. Mei", "M. Wang", "Z. Wang", "X.-S. Hua" ],
      "venue" : "in International Joint Conference on Artificial Intelligence,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2009
    }, {
      "title" : "Semi-supervised metric learning using pairwise constraints.",
      "author" : [ "M.S. Baghshah", "S.B. Shouraki" ],
      "venue" : "in International Joint Conference on Artificial Intelligence,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2009
    }, {
      "title" : "Survey on distance metric learning and dimensionality reduction in data mining",
      "author" : [ "F. Wang", "J. Sun" ],
      "venue" : "Data Mining and Knowledge Discovery, vol. 29, no. 2, pp. 534–564, 2015.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Informationtheoretic metric learning",
      "author" : [ "J.V. Davis", "B. Kulis", "P. Jain", "S. Sra", "I.S. Dhillon" ],
      "venue" : "Proceedings of the 24th international conference on Machine learning. ACM, 2007, pp. 209–216.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Recognizing indoor scenes",
      "author" : [ "A. Quattoni", "A. Torralba" ],
      "venue" : "CVPR. IEEE, 2009, pp. 413–420.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Decomposition-based transfer distance metric learning for image classification",
      "author" : [ "Y. Luo", "T. Liu", "D. Tao", "C. Xu" ],
      "venue" : "IEEE Transactions on Image Processing, vol. 23, no. 9, pp. 3789–3801, 2014.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Online metric learning and fast similarity search",
      "author" : [ "P. Jain", "B. Kulis", "I.S. Dhillon", "K. Grauman" ],
      "venue" : "Advances in neural information processing systems, 2009, pp. 761–768.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "An online algorithm for large scale image similarity learning",
      "author" : [ "G. Chechik", "U. Shalit", "V. Sharma", "S. Bengio" ],
      "venue" : "Advances in Neural Information Processing Systems, 2009, pp. 306–314.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Self-supervised online metric learning with low rank constraint for scene categorization",
      "author" : [ "Y. Cong", "J. Liu", "J. Yuan", "J. Luo" ],
      "venue" : "IEEE Transactions on Image Processing, vol. 22, no. 8, pp. 3179–3191, 2013.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Large margin multi-task metric learning",
      "author" : [ "S. Parameswaran", "K.Q. Weinberger" ],
      "venue" : "Advances in neural information processing systems, 2010, pp. 1867–1875.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Multi-task low-rank metric learning based on common subspace",
      "author" : [ "P. Yang", "K. Huang", "C.-L. Liu" ],
      "venue" : "International Conference on Neural Information Processing. Springer, 2011, pp. 151–159.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "An overview and empirical comparison of distance metric learning methods",
      "author" : [ "P. Moutafis", "M. Leng", "I.A. Kakadiaris" ],
      "venue" : "IEEE transactions on cybernetics, vol. 47, no. 3, pp. 612–625, 2017.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "Deep multimodal distance metric learning using click constraints for image ranking",
      "author" : [ "J. Yu", "X. Yang", "F. Gao", "D. Tao" ],
      "venue" : "IEEE transactions on cybernetics, 2017.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "Person reidentification by minimum classification error-based kiss metric learning",
      "author" : [ "D. Tao", "L. Jin", "Y. Wang", "X. Li" ],
      "venue" : "IEEE transactions on Cybernetics, vol. 45, no. 2, pp. 242–252, 2015.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Distance-based image classification: Generalizing to new classes at near-zero cost",
      "author" : [ "T. Mensink", "J. Verbeek", "F. Perronnin", "G. Csurka" ],
      "venue" : "IEEE transactions on pattern analysis and machine intelligence, vol. 35, no. 11, pp. 2624–2637, 2013.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Information-theoretic semi-supervised metric learning via entropy regularization",
      "author" : [ "G. Niu", "B. Dai", "M. Yamada", "M. Sugiyama" ],
      "venue" : "Neural computation, vol. 26, no. 8, pp. 1717–1762, 2014.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Semi-supervised distance metric learning for collaborative image retrieval and clustering",
      "author" : [ "S.C. Hoi", "W. Liu", "S.-F. Chang" ],
      "venue" : "ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM), vol. 6, no. 3, p. 18, 2010.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Learning bregman distance functions and its application for semi-supervised clustering",
      "author" : [ "L. Wu", "R. Jin", "S.C. Hoi", "J. Zhu", "N. Yu" ],
      "venue" : "Advances in Neural Information Processing Systems, 2009, pp. 2089– 2097.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Low-rank similarity metric learning in high dimensions.",
      "author" : [ "W. Liu", "C. Mu", "R. Ji", "S. Ma", "J.R. Smith", "S.-F. Chang" ],
      "venue" : "Association for the Advancement of Artificial Intelligence,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2015
    }, {
      "title" : "Robust transfer metric learning for image classification",
      "author" : [ "Z. Ding", "Y. Fu" ],
      "venue" : "IEEE Transactions on Image Processing, vol. 26, no. 2, pp. 660–670, 2017.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "Neighbourhood components analysis",
      "author" : [ "J. Goldberger", "G.E. Hinton", "S.T. Roweis", "R.R. Salakhutdinov" ],
      "venue" : "Advances in neural information processing systems, 2005, pp. 513–520.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Distance metric learning with application to clustering with side-information",
      "author" : [ "E.P. Xing", "A.Y. Ng", "M.I. Jordan", "S. Russell" ],
      "venue" : "Advances in Neural Information Processing Systems, vol. 15, no. 505-512, 2002, p. 12.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Robust structural metric learning.",
      "author" : [ "D. Lim", "G.R. Lanckriet", "B. McFee" ],
      "venue" : "International Conference on Machine Learning,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2013
    }, {
      "title" : "Soml: Sparse online metric learning with application to image retrieval.",
      "author" : [ "X. Gao", "S.C. Hoi", "Y. Zhang", "J. Wan", "J. Li" ],
      "venue" : "Association for the Advancement of Artificial Intelligence,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2014
    }, {
      "title" : "Online learning in the embedded manifold of low-rank matrices",
      "author" : [ "U. Shalit", "D. Weinshall", "G. Chechik" ],
      "venue" : "Journal of Machine Learning Research, vol. 13, no. Feb, pp. 429–458, 2012.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Metric learning: A survey",
      "author" : [ "B. Kulis" ],
      "venue" : "Foundations and Trends R  © in Machine Learning, vol. 5, no. 4, pp. 287–364, 2013.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "A survey on metric learning for feature vectors and structured data",
      "author" : [ "A. Bellet", "A. Habrard", "M. Sebban" ],
      "venue" : "arXiv preprint arXiv:1306.6709, 2013.",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Multitask learning",
      "author" : [ "R. Caruana" ],
      "venue" : "Machine Learning, vol. 28, no. 1, pp. 41–75, 1997.",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Integrating low-rank and group-sparse structures for robust multi-task learning",
      "author" : [ "J. Chen", "J. Zhou", "J. Ye" ],
      "venue" : "KDD, 2011, pp. 42–50.",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Learning task grouping and overlap in multitask learning",
      "author" : [ "A. Kumar", "H.D. III" ],
      "venue" : "International Conference on Machine Learning, 2012.",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Joint covariate selection and joint subspace selection for multiple classification problems",
      "author" : [ "G. Obozinski", "B. Taskar", "M.I. Jordan" ],
      "venue" : "Statistics and Computing, vol. 20, no. 2, pp. 231–252, 2010.",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Convex multi-task feature learning",
      "author" : [ "A. Argyriou", "T. Evgeniou", "M. Pontil" ],
      "venue" : "Machine Learning, no. 73, pp. 243–272, 2008.",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "A dirty model for multi-task learning",
      "author" : [ "A. Jalali", "P.D. Ravikumar", "S. Sanghavi", "C. Ruan" ],
      "venue" : "Advances in neural information processing systems, 2010, pp. 964–972.",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "The benefit of multitask representation learning",
      "author" : [ "A. Maurer", "M. Pontil", "B. Romera-Paredes" ],
      "venue" : "Journal of Machine Learning Research, vol. 17, no. 81, pp. 1–32, 2016.",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Latent maxmargin multitask learning with skelets for 3-d action recognition",
      "author" : [ "Y. Yang", "C. Deng", "D. Tao", "S. Zhang", "W. Liu", "X. Gao" ],
      "venue" : "IEEE transactions on cybernetics, vol. 47, no. 2, pp. 439–448, 2017.",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "Sparse compositional metric learning",
      "author" : [ "Y. Shi", "A. Bellet", "F. Sha" ],
      "venue" : "arXiv preprint arXiv:1404.4105, 2014.  10",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Ella: An efficient lifelong learning algorithm.",
      "author" : [ "P. Ruvolo", "E. Eaton" ],
      "venue" : "International Conference on Machine Learning (1),",
      "citeRegEx" : "38",
      "shortCiteRegEx" : "38",
      "year" : 2013
    }, {
      "title" : "Online passive-aggressive algorithms",
      "author" : [ "K. Crammer", "O. Dekel", "J. Keshet", "S. Shalev-Shwartz", "Y. Singer" ],
      "venue" : "Journal of Machine Learning Research, vol. 7, no. Mar, pp. 551–585, 2006.",
      "citeRegEx" : "39",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "A fast iterative shrinkage-thresholding algorithm for linear inverse problems",
      "author" : [ "A. Beck", "M. Teboulle" ],
      "venue" : "SIAM journal on imaging sciences, vol. 2, no. 1, pp. 183–202, 2009.",
      "citeRegEx" : "40",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Biographies, bollywood, boomboxes and blenders: Domain adaptation for sentiment classification",
      "author" : [ "J. Blitzer", "M. Dredze", "F. Pereira" ],
      "venue" : "ACL, vol. 7, 2007, pp. 440–447.",
      "citeRegEx" : "41",
      "shortCiteRegEx" : null,
      "year" : 2007
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "ONLINE metric / similarity learning has received remarkable success in a variety of applications [1], [2], [3], such as data mining [4], information retrieval [5] and computer vision [6], [7], mainly due to its high efficiency and scalability to large-scale dataset.",
      "startOffset" : 97,
      "endOffset" : 100
    }, {
      "referenceID" : 1,
      "context" : "ONLINE metric / similarity learning has received remarkable success in a variety of applications [1], [2], [3], such as data mining [4], information retrieval [5] and computer vision [6], [7], mainly due to its high efficiency and scalability to large-scale dataset.",
      "startOffset" : 102,
      "endOffset" : 105
    }, {
      "referenceID" : 2,
      "context" : "ONLINE metric / similarity learning has received remarkable success in a variety of applications [1], [2], [3], such as data mining [4], information retrieval [5] and computer vision [6], [7], mainly due to its high efficiency and scalability to large-scale dataset.",
      "startOffset" : 107,
      "endOffset" : 110
    }, {
      "referenceID" : 3,
      "context" : "ONLINE metric / similarity learning has received remarkable success in a variety of applications [1], [2], [3], such as data mining [4], information retrieval [5] and computer vision [6], [7], mainly due to its high efficiency and scalability to large-scale dataset.",
      "startOffset" : 132,
      "endOffset" : 135
    }, {
      "referenceID" : 4,
      "context" : "ONLINE metric / similarity learning has received remarkable success in a variety of applications [1], [2], [3], such as data mining [4], information retrieval [5] and computer vision [6], [7], mainly due to its high efficiency and scalability to large-scale dataset.",
      "startOffset" : 159,
      "endOffset" : 162
    }, {
      "referenceID" : 5,
      "context" : "ONLINE metric / similarity learning has received remarkable success in a variety of applications [1], [2], [3], such as data mining [4], information retrieval [5] and computer vision [6], [7], mainly due to its high efficiency and scalability to large-scale dataset.",
      "startOffset" : 183,
      "endOffset" : 186
    }, {
      "referenceID" : 6,
      "context" : "ONLINE metric / similarity learning has received remarkable success in a variety of applications [1], [2], [3], such as data mining [4], information retrieval [5] and computer vision [6], [7], mainly due to its high efficiency and scalability to large-scale dataset.",
      "startOffset" : 188,
      "endOffset" : 191
    }, {
      "referenceID" : 7,
      "context" : "However, most state-of-the art online metric learning models [8], [9], [10] can only achieve online learning from fixed predefined t (t > 0) metric tasks and cannot add the new",
      "startOffset" : 61,
      "endOffset" : 64
    }, {
      "referenceID" : 8,
      "context" : "However, most state-of-the art online metric learning models [8], [9], [10] can only achieve online learning from fixed predefined t (t > 0) metric tasks and cannot add the new",
      "startOffset" : 66,
      "endOffset" : 69
    }, {
      "referenceID" : 9,
      "context" : "However, most state-of-the art online metric learning models [8], [9], [10] can only achieve online learning from fixed predefined t (t > 0) metric tasks and cannot add the new",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 10,
      "context" : "To achieve this goal, most state-of-the-arts [11], [12] should storage training data of all tasks and retrain their models in a time consuming way.",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 11,
      "context" : "To achieve this goal, most state-of-the-arts [11], [12] should storage training data of all tasks and retrain their models in a time consuming way.",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 12,
      "context" : "[13], [14], [15].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 13,
      "context" : "[13], [14], [15].",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 14,
      "context" : "[13], [14], [15].",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 15,
      "context" : "The batch metric learning models [16], [17], [18], [19], [20], [21] can further be divided into two categories: models based on nearest neighbors, such as [22] optimizes the expected leave-one-out error of a stochastic nearest classifier",
      "startOffset" : 33,
      "endOffset" : 37
    }, {
      "referenceID" : 16,
      "context" : "The batch metric learning models [16], [17], [18], [19], [20], [21] can further be divided into two categories: models based on nearest neighbors, such as [22] optimizes the expected leave-one-out error of a stochastic nearest classifier",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 17,
      "context" : "The batch metric learning models [16], [17], [18], [19], [20], [21] can further be divided into two categories: models based on nearest neighbors, such as [22] optimizes the expected leave-one-out error of a stochastic nearest classifier",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 18,
      "context" : "The batch metric learning models [16], [17], [18], [19], [20], [21] can further be divided into two categories: models based on nearest neighbors, such as [22] optimizes the expected leave-one-out error of a stochastic nearest classifier",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 19,
      "context" : "The batch metric learning models [16], [17], [18], [19], [20], [21] can further be divided into two categories: models based on nearest neighbors, such as [22] optimizes the expected leave-one-out error of a stochastic nearest classifier",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 20,
      "context" : "The batch metric learning models [16], [17], [18], [19], [20], [21] can further be divided into two categories: models based on nearest neighbors, such as [22] optimizes the expected leave-one-out error of a stochastic nearest classifier",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 21,
      "context" : "The batch metric learning models [16], [17], [18], [19], [20], [21] can further be divided into two categories: models based on nearest neighbors, such as [22] optimizes the expected leave-one-out error of a stochastic nearest classifier",
      "startOffset" : 155,
      "endOffset" : 159
    }, {
      "referenceID" : 0,
      "context" : "in the projection space and [1] proposes the most widelyused Mahalanohis distance learning Large Margin Nearest Neighbors (LMNN), i.",
      "startOffset" : 28,
      "endOffset" : 31
    }, {
      "referenceID" : 22,
      "context" : ", learning a Mahalanobis distance metric for kNN classification for labeled training examples; models based on pairs/triplets, for instance, [23] searches for a clustering that puts the similar pairs into the same clusters",
      "startOffset" : 141,
      "endOffset" : 145
    }, {
      "referenceID" : 23,
      "context" : "and dissimilar pairs into different clusters; [24] promotes input sparsity by imposing a group sparsity penalty on the learned metric and a trace constraint to encourage output sparsity; [20] proposes a novel low-rank metric learning algorithm to yield bilinear similarity functions which can be applicable to highdimensional data domains.",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 19,
      "context" : "and dissimilar pairs into different clusters; [24] promotes input sparsity by imposing a group sparsity penalty on the learned metric and a trace constraint to encourage output sparsity; [20] proposes a novel low-rank metric learning algorithm to yield bilinear similarity functions which can be applicable to highdimensional data domains.",
      "startOffset" : 187,
      "endOffset" : 191
    }, {
      "referenceID" : 8,
      "context" : "For the online metric learning, [9] designs an Online Algorithm for Scalable Image Similarity learning (OASIS), for learning pairwise similarity that is fast and scales linearly with the number of objects and the number of non-zero features.",
      "startOffset" : 32,
      "endOffset" : 35
    }, {
      "referenceID" : 9,
      "context" : "In order to overcome over-fitting problem, OMLLR [10] proposes a novel online metric learning model with the low rank constraint, where low-rank metric enables to reduce storage of metric matrices.",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 24,
      "context" : "[25] incorporates large-scale high-dimensional dataset into sparse online metric learning,",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 25,
      "context" : "In addition, LORETA [26] describes an iterative online learning procedure, consisting of a gradient step, followed by a second-order retraction back to the manifold.",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 7,
      "context" : "To incorporate the benefits of both online learning and Mahalanobis distance, LEGO [8] using a Log-Det regularization per instance loss, is guaranteed",
      "startOffset" : 83,
      "endOffset" : 86
    }, {
      "referenceID" : 26,
      "context" : "Furthermore, more details can also be found in two surveys [27] and [28].",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 27,
      "context" : "Furthermore, more details can also be found in two surveys [27] and [28].",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 28,
      "context" : "Based on the assumption that the relationships and information shared among the different tasks can be taken into account, multi-task learning [29], [30], [31], [32], [33], [34],",
      "startOffset" : 143,
      "endOffset" : 147
    }, {
      "referenceID" : 29,
      "context" : "Based on the assumption that the relationships and information shared among the different tasks can be taken into account, multi-task learning [29], [30], [31], [32], [33], [34],",
      "startOffset" : 149,
      "endOffset" : 153
    }, {
      "referenceID" : 30,
      "context" : "Based on the assumption that the relationships and information shared among the different tasks can be taken into account, multi-task learning [29], [30], [31], [32], [33], [34],",
      "startOffset" : 155,
      "endOffset" : 159
    }, {
      "referenceID" : 31,
      "context" : "Based on the assumption that the relationships and information shared among the different tasks can be taken into account, multi-task learning [29], [30], [31], [32], [33], [34],",
      "startOffset" : 161,
      "endOffset" : 165
    }, {
      "referenceID" : 32,
      "context" : "Based on the assumption that the relationships and information shared among the different tasks can be taken into account, multi-task learning [29], [30], [31], [32], [33], [34],",
      "startOffset" : 167,
      "endOffset" : 171
    }, {
      "referenceID" : 33,
      "context" : "Based on the assumption that the relationships and information shared among the different tasks can be taken into account, multi-task learning [29], [30], [31], [32], [33], [34],",
      "startOffset" : 173,
      "endOffset" : 177
    }, {
      "referenceID" : 34,
      "context" : "[35], [36] aims to improve generalization performance by learning multiple related tasks simultaneously.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 35,
      "context" : "[35], [36] aims to improve generalization performance by learning multiple related tasks simultaneously.",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 10,
      "context" : "common Mahalanobis metric and each task has a task-specific metric, mtLMNN [11] adopts the LMNN formulation to the multi-task learning.",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 11,
      "context" : "Based on low-rank based assumptions, [12] presents transformation matrix to the problem of multi-task metric learning by learning a common subspace for all tasks and an individual metric for each task, where each individual metric is restricted in the common subspace.",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 36,
      "context" : "In addition, mtSCML [37] constructs a common basis set, multi-metric are regularized to be relevant across tasks (as favored by the group sparsity).",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 11,
      "context" : "Moreover, motivated by [12], Theorem 1 gives the detail mathematical description.",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 37,
      "context" : "However, as shown in [38], this approach is inefficient and inapplicable to lifelong learning with many tasks and data samples.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 38,
      "context" : "(6) by applying the online passive aggressive (PA) [39] optimization strategy, i.",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 39,
      "context" : "Due to the non-smooth nature of g(Wt), we propose the proximal gradient method (FISTA) [40] with a fast global convergence rate to solve this optimization problem.",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 8,
      "context" : "(14) OASIS [9] Similarity `t(Mt) = max ( 0, 1− sMt (xi, xj) + sMt (xi, xk) ) ∑",
      "startOffset" : 11,
      "endOffset" : 14
    }, {
      "referenceID" : 36,
      "context" : "SCML [37] Distance `t(Mt) = max ( 0, 1− dMt (xi, xj) + dMt (xi, xk) ) ∑",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 39,
      "context" : "In other words, the convergence rate of Algorithm 1 can achieve O(1/T ) as shown in [40].",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 36,
      "context" : "Perhaps the most relevant work to ours in the context of multi-task metric learning is from [37], which frames metric learning as learning a sparse combination of locally discriminative metrics that are generated from the training data via clustering.",
      "startOffset" : 92,
      "endOffset" : 96
    }, {
      "referenceID" : 8,
      "context" : "The single metric learning model includes: 1) Euclidean distance (stEuc): the standard Euclidean distance in feature space; 2) OASIS (stOASIS) [9]: the classical online metric learning model which is given in Table I, and its iteration number is 2×10 in our paper; 3) LMNN (stLMNN) [1]: Large Margin Nearest Neighbor Classification, which learns a Mahalanobis distance for k-nearest neighbor classification; 4) SCML-global (stSCML) [37]: which is simply to combine the local basis elements into a higher-rank global metric; 5) LMNN-union (uLMNN): is the LMNN metric obtained on the union of the training data of all tasks (i.",
      "startOffset" : 143,
      "endOffset" : 146
    }, {
      "referenceID" : 0,
      "context" : "The single metric learning model includes: 1) Euclidean distance (stEuc): the standard Euclidean distance in feature space; 2) OASIS (stOASIS) [9]: the classical online metric learning model which is given in Table I, and its iteration number is 2×10 in our paper; 3) LMNN (stLMNN) [1]: Large Margin Nearest Neighbor Classification, which learns a Mahalanobis distance for k-nearest neighbor classification; 4) SCML-global (stSCML) [37]: which is simply to combine the local basis elements into a higher-rank global metric; 5) LMNN-union (uLMNN): is the LMNN metric obtained on the union of the training data of all tasks (i.",
      "startOffset" : 282,
      "endOffset" : 285
    }, {
      "referenceID" : 36,
      "context" : "The single metric learning model includes: 1) Euclidean distance (stEuc): the standard Euclidean distance in feature space; 2) OASIS (stOASIS) [9]: the classical online metric learning model which is given in Table I, and its iteration number is 2×10 in our paper; 3) LMNN (stLMNN) [1]: Large Margin Nearest Neighbor Classification, which learns a Mahalanobis distance for k-nearest neighbor classification; 4) SCML-global (stSCML) [37]: which is simply to combine the local basis elements into a higher-rank global metric; 5) LMNN-union (uLMNN): is the LMNN metric obtained on the union of the training data of all tasks (i.",
      "startOffset" : 432,
      "endOffset" : 436
    }, {
      "referenceID" : 10,
      "context" : "• multi-task LMNN (mtLMNN) [11]: common metric defined by M0 picks up general trends across multiple datasets and Mt specializes the metric further for each particular task.",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 36,
      "context" : "• multi-task SCML (mtSCML) [37]: this multi-task metric learning model considers that all learned metrics can be expressed as combinations of the same basis subset B, though with different weights for each task.",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 37,
      "context" : "• Lifelong multi-task (ELLA) [38]: whose formulation is realized by the following objective function:",
      "startOffset" : 29,
      "endOffset" : 33
    }, {
      "referenceID" : 40,
      "context" : "As shown in Table IV, Sentiment [41] consists of Amazon reviews on four product types (kitchen appliances, DVDs, books and electronics).",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 39,
      "context" : ", γ ‖L0‖2F , and employ FISTA [40] to efficiently optimize such a convex problem.",
      "startOffset" : 30,
      "endOffset" : 34
    } ],
    "year" : 2017,
    "abstractText" : "The state-of-the-art online learning approaches are only capable of learning the metric for predefined tasks. In this paper, we consider lifelong learning problem to mimic “human learning”, i.e., endowing a new capability to the learned metric for a new task from new online samples and incorporating previous experiences and knowledge. Therefore, we propose a new metric learning framework: lifelong metric learning (LML), which only utilizes the data of the new task to train the metric model while preserving the original capabilities. More specifically, the proposed LML maintains a common subspace for all learned metrics, named lifelong dictionary, transfers knowledge from the common subspace to each new metric task with task-specific idiosyncrasy, and redefines the common subspace over time to maximize performance across all metric tasks. For model optimization, we apply online passive aggressive optimization algorithm to solve the proposed LML framework, where the lifelong dictionary and task-specific partition are optimized alternatively and consecutively. Finally, we evaluate our approach by analyzing several multi-task metric learning datasets. Extensive experimental results demonstrate effectiveness and efficiency of the proposed framework.",
    "creator" : "LaTeX with hyperref package"
  }
}