{
  "name" : "1704.05396.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Study of Deep Learning Robustness Against Computation Failures",
    "authors" : [ "Jean-Charles Vialatte" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "I. INTRODUCTION\nDeep neural networks achieve excellent performance at various artificial intelligence tasks, such as speech recognition [1] and computer vision [2]. Clearly, the usefulness of these algorithms would be increased significantly if state-of-the-art accuracy could also be obtained when implemented on embedded systems operating with reduced energy budgets. In this context, the energy efficiency of the inference phase is the most important, since the learning phase can be performed offline.\nTo achieve the best energy efficiency, it is desirable to design specialized hardware for deep learning inference. However, whereas in the past the energy consumption of integrated circuits was decreasing steadily with each new integrated circuit technology, the energy improvements that can be expected from further shrinking of CMOS circuits is small [3]. A possible approach to continue improving the energy efficiency of CMOS circuits is to operate them in the near-threshold regime, which unfortunately drastically increases the amount of delay variations in the circuit, which can lead to functional failures [4]. There is therefore a conflict between the desire to obtain circuits that operate reliably and that are energy efficient. In other words, tolerating circuit faults without degrading performance translates into energy gains. Neural networks are interesting algorithms to study in this context, because their ability to process noisy data could also be useful to compensate for hardware failures. It is interesting to draw a parallel with another important class of algorithms that process noisy data: decoders of error-correction codes. For the case of low-density parity-check codes, it was indeed shown that a decoder can save energy by operating unreliably while preserving equivalent performance [5].\nOf course, a perhaps more straightforward way to decrease the energy consumption is to reduce the computational complexity. For example, [6] proposes an approach to decrease the number of parameters in a deep convolutional neural network (CNN) while preserving prediction accuracy, and [7, 8] propose approaches to replace floating-point operations with much simpler binary operations. The approach of this paper is the opposite. We consider increasing the number of parameters in the model to provide robustness that can then be traded for energy efficiency. The two aims are most likely complementary, and the situation is in fact similar in essence to the problem of data transmission, where it is in many practical cases asymptotically (in the size of the transmission) optimal to first compress the data to be transmitted, and then to add back some redundancy using an error-correction code.\nIn this preliminary study, we consider the ability of simple neural network models to increase their robustness to computation failures through an increase of the number of parameters. It was shown previously for an implementation of a multilayer perceptron (MLP) based on stochastic computing that it is possible to compensate for a reduced precision by increasing the size of the network [9]. The impact of hardware faults on CNNs is also considered in\nThis is an extended version of the paper with the same title published in Proc. 9th Int. Conf. on Advanced Cognitive Technologies and Applications, Feb. 2017.\nJean-Charles Vialatte and François Leduc-Primeau are with IMT Atlantique, Brest, France. Jean-Charles Vialatte is also with Cityzen Data, Guipavas, France.\nEmails: {jc.vialatte, francois.leduc-primeau}@imt-atlantique.fr.\nar X\niv :1\n70 4.\n05 39\n6v 1\n[ cs\n.N E\n] 1\n8 A\npr 2\n01 7\n[10] using a different approach that consists in adding compensation mechanisms in hardware while keeping the same network size. The deviation models that we consider in this paper attempt to represent the case in which computation circuits are not protected by any compensation mechanism. We consider a pessimistic model and a more optimistic deviation model, but in both cases the magnitude of the error is only limited by the bounded codomain of the computations. We show that despite this, increasing the size of the network can sometimes compensate for faulty computations. The results provide an idea of the reduction in energy that must be obtained by a faulty circuit in order to reduce the overall energy consumption of the system.\nThe remainder of this paper is organized as follows. Section II provides a quick summary of the neural network models used in this paper, and Section III presents the methodology used for selecting hyperparameters of the models and for training. Section IV then describes the modeling of deviations, that is of the impact of circuit faults on the computations. Section V discusses the robustness of the inference based on simulation results. Finally, Section VI concludes the paper."
    }, {
      "heading" : "II. BACKGROUND AND NAMING CONVENTIONS",
      "text" : "A neural network is a neural representation of a function f that is a composition of layer functions fi. Each layer function is usually composed of a linear operation followed by a non-linear one. A dense layer is such that its inputs and outputs are vectors. Its linear part can be written as a matrix multiplication between an input x and a weight matrix W : x 7→Wx. The number of neurons of a dense layer refers to the number of rows of W . A n-dimensional convolutional layer is such that its inputs and outputs are tensors of rank n. Its linear part can be written as a n-dimensional convolution between an input x and a weight tensor W of rank n + 2: x 7→ (∑\np Wpq ∗n xp)∀q,\nwhere p and q index slices at the last ranks and ∗n denotes the n-dimensional convolution. The output tensor slices indexed by p and q are called feature maps. A pooling operation is often added to convolutional layers to scale them down.\nAn MLP [11] is composed only of dense layers. A CNN [12] is mainly composed of convolutional layers. For both network types, to perform supervised classification, a dense output layer with as many neurons as classes is usually added. Then, the weights are trained according to an optimization algorithm based on gradient descent."
    }, {
      "heading" : "III. NEURAL NETWORK MODELS",
      "text" : "We consider two types of deep learning models and train them in the usual way, assuming reliable computations. To simplify model selection, we place some mild restrictions on the hyperparameter space, since we are more interested in the general robustness of the models, rather than in finding models with the very best accuracy. As described below, we restrict most layers to have the same number of neurons, and the same activation function. We also try only one optimisation algorithm, and consider only one type of weight initialization.\nThe first model type that we consider is an MLP network composed of L dense layers, each containing N neurons, that we denote as MLP–L–N . The activation function used in all the layers is chosen as the rectified linear unit (reLU) [13]. In fact, since a circuit implementation (and particularly, a fixed-point circuit implementation) can only represent values over a bounded range, we take this into account in the training by using a clipped-reLU activation, which adds a saturation operation on positive outputs. We note that such an activation function has been used before in a different context, in order to avoid the problem of diverging gradients in the training of recurrent networks [14]. The use of the tanh activation was also considered, but reLU was observed to yield better accuracy.\nThe second model type is a CNN network composed of L convolutional C × C layers with P × P pooling of type pool, ultimately followed by a dense layer of 200 neurons [12]. This class is denoted as CNN–L–C–P–F– pool, where F is the number of feature maps used in each convolutional layer. Clipped-reLU activations are used throughout. The type of pooling pool can be either “max” pooling or no pooling.\nFor simplicity, we opted to train the networks on the task of digit classification using the MNIST dataset [15]. In addition to the layers defined above, each model is terminated by a dense “softmax” layer used for classification, and containing one neuron per class. Initialization was done as suggested in [16]. To prevent overfitting during training, a dropout [17] of 25% and 50% of the neurons have been applied on convolutional and dense layers, respectively, except on the first layer. We used categorical crossentropy as the loss function and the “adadelta” optimizer [18]. The batch size was 128 and we trained for 15 epochs. The saturation value of the clipped-reLU activation is chosen as 1 as this provides a good balance between performance and implementation complexity. Note that as the range is increased, more bits must be used to maintain the same precision, leading to larger circuits."
    }, {
      "heading" : "IV. DEVIATION MODELS",
      "text" : "We consider that all the computations performed in the inference phase are unreliable, either because they are performed by unreliable circuits, or because the matrix or tensor W is stored in an unreliable memory. We do, however, assume that the softmax operation performed at the end of the classification layer (i.e. the output layer) is computed reliably. We assume that each layer is affected by deviations independently, and that deviations occur independently for each scalar element of a layer output. Note that in the case of a convolutional layer, it would be natural in practice to store each convolution kernel only once, even though each kernel is used to generate multiple output scalar elements. The assumption that deviations occur independently for each scalar output is therefore a simplifying assumption for the case where the tensor W of a convolutional layer is stored unreliably. However, it remains realistic for the case where deviations are caused by the processing circuits.\nWe are interested in circuits with a reasonably low amount of unreliability, and therefore it is useful to partition the outcome of a computation in terms of the occurrence or non-occurrence of a deviation event. We say that a deviation event occurs if the output of a circuit is different from the output that would be generated by a fully reliable circuit. The probability of a deviation event is denoted by p.\nObtaining a precise characterization of the output of a circuit when timing violations or other circuit faults are possible requires knowledge of the specific circuit architecture and of various implementation parameters. We will rely here on two simplified deviation models. We assume that the circuits operate on fixed-point values defined over a certain range, which is motivated by the fact that fixed-point computations are much simpler than floating-point ones, and reducing circuit complexity is the obvious first step in trying to improve energy efficiency. The first deviation model is a pessimistic model that assumes that when a deviation occurs, the output is sampled uniformly at random from the bounded output domain of that circuit. We call this deviation model conditionally uniform. Note that we assume that the circuit output is continuous within the bounded range to avoid the need to take into account the number of quantization levels. The second model is more optimistic, and assumes that the occurrence of a deviation can be detected. Therefore, when a deviation occurs, we replace the output by a “neutral” value, in this case 0. We call this deviation model the erasure model.\nIn a synchronous circuit, the deviations can be observed in the memory elements (registers) that separate the logic circuits. By changing the placement of these registers, we can in effect select the point where deviations will be taken into account. Note however that register placement cannot be arbitrary, as we also seek to have similar logic depth separating all registers. When considering the effect of the deviations on the inference, we noticed that robustness can be increased if deviations are sampled before the activation function of each layer. This is not surprising, since these activation functions act as denoising operations. All simulation results presented in this paper therefore consider that deviations are sampled before the activation function. In the case of convolutional layers, the pooling operation is also performed after the sampling of deviations."
    }, {
      "heading" : "V. RESULTS",
      "text" : "The classification performance of the MLP and CNN models was evaluated using Monte-Carlo (MC) simulations that sample deviations according to the deviation models described in Section IV. Because we wish to evaluate a large number of neural network models, each MC simulation is performed by sampling only 10 deviation realizations."
    }, {
      "heading" : "A. Effect of some hyperparameters on robustness",
      "text" : "Robustness refers to the ability to maintain good classification accuracy in the presence of deviations. A model has better robustness if it achieves a lower classification error at a given p. We investigated the impact of several hyperparameters on the robustness of the inference. We evaluated the performance using the clipped-reLU and tanh activation functions, and found robustness to be similar in both cases. For CNN models, we also evaluated the impact of the choice of pooling function. In this case, we found that using no pooling, rather than max pooling, provided a slight improvement in robustness. Finally, we considered the impact of the number of layers L. The effect of L on classification error is shown in Figure 1 for an MLP-L-N network and in Figure 2 for a CNN-L-CP -F -pool network, for the case where the inference is affected by conditionally uniform deviations. For each value of L, we select the 5 best models based on their deviation-free performance to show the variability of the model optimization. In both cases, the number of layers does not have a clear impact on robustness, but depending on the value of p, increasing the number of layers can improve robustness, even when a larger number of layers decreases\nperformance at p = 0 (reliable computation). For example, in the case of MLP models, the best performance at p = 0 is obtained by a 2-layer model, but using a larger number of layers improves performance for p ≥ 5 · 10−4."
    }, {
      "heading" : "B. Fault tolerance",
      "text" : "Since neural networks are designed to reject noise in their input, we might expect them to also be able to reject the “noise” introduced by faulty computations. We investigate this ability for networks trained with standard procedures, in order to provide a baseline for future targeted approaches.\nWe first choose an error rate target that we want the network to achieve. We then consider various deviation probability values, and for each, look for the model with the smallest number of parameters that can achieve or outperform the performance target under the deviation constraint. For MLP networks, the results are shown in Figure 3 for the case of conditionally uniform deviations, and in Figure 4 for the case of erasure deviations. Similarly, Figures 5 and 6 show the results for CNN networks, respectively for conditionally uniform and erasure deviations.\nNote that the finite set of models trained implies upper and lower bounds on the number of parameters. As a result, the lack of robust models for some performance targets (represented by single data points at p = 0) does not necessarily mean that deviations cannot be tolerated at that performance target, but merely that no sufficiently\nlarge model were found within the set of models that were trained. Similarly, the saturation to a minimum number of parameters that is observed as the performance target is decreased corresponds to the number of parameters in the smallest trained models.\nWe simulate deviation probabilities on the order of 10−3, which are already considered quite large for digital circuit design. We can see that for both network types and for both deviation models, there are indeed many performance targets at which performance can be preserved in the presence of computation failures by picking a larger model from the set of pre-trained models. As might be expected, the necessary redundancy grows more slowly for the optimistic erasure deviation model. In fact for this deviation model, CNN models are able to preserve performance even for deviation probabilities on the order of 10−2, as seen in Fig. 6.\nWe expect all the curves in Figures 3–6 to be non-decreasing, since computation noise is unlikely to improve performance in a feedforward network. The apparent inconsistency observed in Fig. 4 for the 0.016 error target is likely an artefact caused by the small number of deviations that are sampled in the MC simulation.\nIn addition to considering the absolute number of parameters required to achieve a performance target in the presence of deviations, it is interesting to compare the number of parameters required by the faulty and reliable implementations, to obtain the fault-tolerance efficiency of the network. Let M be a network model that achieves our performance target when deviations occur with probability p, and Mo be the smallest model that achieves the target under reliable computation. We define the fault-tolerance efficiency of M as n(Mo)/n(M), where n(X)\ndenotes the number of parameters used by a model X . To evaluate efficiency empirically, we take Mo to be the smallest available model that achieves the performance target, even though it is possible that a smaller model exists but was not found during the training phase.\nEfficiency curves for the MLP models are shown in Figures 7 and 8, and for the CNN models in Figures 9 and 10. Two characteristics of these results are worth pointing out. First, even if, in the case of CNN models, a significant amount of deviations can be tolerated while maintaining an efficiency close to 1, there exist clear deviation thresholds after which the efficiency starts degrading rapidly. Second, this deviation threshold becomes smaller as the error rate target is decreased. This suggests that with this naive fault-tolerance approach, we are forced to compromise on fault-tolerance efficiency to achieve a lower classification error rate."
    }, {
      "heading" : "VI. CONCLUSION",
      "text" : "In this paper, we have considered the effect of faulty computations on the performance of MLP and CNN inference, using a pessimistic and an optimistic deviation model. We showed that using standard training procedures, it is in many cases possible to find models that will compensate for computation failures, when the deviation probability is on the order of 10−3 and at the cost of an increase in the number of parameters. We also studied the efficiency of the fault-tolerance that is achieved using standard model training. Our results show that the fault-tolerance efficiency decreases as the performance target is increased. It seems reasonable to expect that networks designed for fault-\ntolerance could achieve an efficiency that only depends on the amount of deviations occurring during the inference process, and not on the performance target. These results therefore provide a baseline for future work seeking to identify systematic ways of designing robust deep neural networks, and highlight that one of the objectives of the network design should be to decouple the fault tolerance from the classification performance."
    } ],
    "references" : [ {
      "title" : "Deep speech 2: End-to-end speech recognition in english and mandarin",
      "author" : [ "D. Amodei" ],
      "venue" : "CoRR, vol. abs/1512.02595, 2015. [Online]. Available: http://arxiv.org/abs/1512.02595",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "A. Krizhevsky", "I. Sutskever", "G.E. Hinton" ],
      "venue" : "Advances in Neural Information Processing Systems, 2012, pp. 1097–1105.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Near-threshold computing: Reclaiming Moore’s law through energy efficient integrated circuits",
      "author" : [ "R. Dreslinski", "M. Wieckowski", "D. Blaauw", "D. Sylvester", "T. Mudge" ],
      "venue" : "Proc. of the IEEE, vol. 98, no. 2, pp. 253–266, Feb. 2010.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Ultra-low power VLSI circuit design demystified and explained: A tutorial",
      "author" : [ "M. Alioto" ],
      "venue" : "Circuits and Systems I: Regular Papers, IEEE Transactions on, vol. 59, no. 1, pp. 3–29, 2012.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Modeling and energy optimization of LDPC decoder circuits with timing violations",
      "author" : [ "F. Leduc-Primeau", "F.R. Kschischang", "W.J. Gross" ],
      "venue" : "CoRR, vol. abs/1503.03880, 2015. [Online]. Available: http://arxiv.org/abs/1503.03880",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <1mb model size",
      "author" : [ "F.N. Iandola", "M.W. Moskewicz", "K. Ashraf", "S. Han", "W.J. Dally", "K. Keutzer" ],
      "venue" : "CoRR, vol. abs/1602.07360, 2016. [Online]. Available: http://arxiv.org/abs/1602.07360",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Binarynet: Training deep neural networks with weights and activations constrained to +1 or -1",
      "author" : [ "M. Courbariaux", "Y. Bengio" ],
      "venue" : "CoRR, vol. abs/1602.02830, 2016. [Online]. Available: http://arxiv.org/abs/1602.02830",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "XNOR-Net: ImageNet classification using binary convolutional neural networks",
      "author" : [ "M. Rastegari", "V. Ordonez", "J. Redmon", "A. Farhadi" ],
      "venue" : "CoRR, vol. abs/1603.05279, 2016. [Online]. Available: http://arxiv.org/abs/1603.05279",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "VLSI implementation of deep neural network using integral stochastic computing",
      "author" : [ "A. Ardakani", "F. Leduc-Primeau", "N. Onizawa", "T. Hanyu", "W.J. Gross" ],
      "venue" : "IEEE Trans. on VLSI Systems, 2017, to appear.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "Variation-tolerant architectures for convolutional neural networks in the near threshold voltage regime",
      "author" : [ "Y. Lin", "S. Zhang", "N.R. Shanbhag" ],
      "venue" : "2016 IEEE International Workshop on Signal Processing Systems (SiPS), Oct 2016, pp. 17–22.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Multilayer feedforward networks are universal approximators",
      "author" : [ "K. Hornik", "M. Stinchcombe", "H. White" ],
      "venue" : "Neural networks, vol. 2, no. 5, pp. 359–366, 1989.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 1989
    }, {
      "title" : "Gradient-based learning applied to document recognition",
      "author" : [ "Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner" ],
      "venue" : "Proceedings of the IEEE, vol. 86, no. 11, pp. 2278–2324, 1998.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Deep sparse rectifier neural networks",
      "author" : [ "X. Glorot", "A. Bordes", "Y. Bengio" ],
      "venue" : "International Conference on Artificial Intelligence and Statistics, 2011, pp. 315–323.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Deep speech: Scaling up end-to-end speech recognition",
      "author" : [ "A.Y. Hannun" ],
      "venue" : "CoRR, vol. abs/1412.5567, 2014. [Online]. Available: http://arxiv.org/abs/1412.5567",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "The MNIST database of handwritten digits",
      "author" : [ "Y. LeCun", "C. Cortes", "C.J. Burges" ],
      "venue" : "1998.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Understanding the difficulty of training deep feedforward neural networks",
      "author" : [ "X. Glorot", "Y. Bengio" ],
      "venue" : "International conference on artificial intelligence and statistics, 2010, pp. 249–256.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Dropout: A simple way to prevent neural networks from overfitting",
      "author" : [ "N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov" ],
      "venue" : "The Journal of Machine Learning Research, vol. 15, no. 1, pp. 1929–1958, 2014.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 1929
    }, {
      "title" : "ADADELTA: an adaptive learning rate method",
      "author" : [ "M.D. Zeiler" ],
      "venue" : "CoRR, vol. abs/1212.5701, 2012. [Online]. Available: http://arxiv.org/abs/1212.5701",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Deep neural networks achieve excellent performance at various artificial intelligence tasks, such as speech recognition [1] and computer vision [2].",
      "startOffset" : 120,
      "endOffset" : 123
    }, {
      "referenceID" : 1,
      "context" : "Deep neural networks achieve excellent performance at various artificial intelligence tasks, such as speech recognition [1] and computer vision [2].",
      "startOffset" : 144,
      "endOffset" : 147
    }, {
      "referenceID" : 2,
      "context" : "However, whereas in the past the energy consumption of integrated circuits was decreasing steadily with each new integrated circuit technology, the energy improvements that can be expected from further shrinking of CMOS circuits is small [3].",
      "startOffset" : 238,
      "endOffset" : 241
    }, {
      "referenceID" : 3,
      "context" : "A possible approach to continue improving the energy efficiency of CMOS circuits is to operate them in the near-threshold regime, which unfortunately drastically increases the amount of delay variations in the circuit, which can lead to functional failures [4].",
      "startOffset" : 257,
      "endOffset" : 260
    }, {
      "referenceID" : 4,
      "context" : "For the case of low-density parity-check codes, it was indeed shown that a decoder can save energy by operating unreliably while preserving equivalent performance [5].",
      "startOffset" : 163,
      "endOffset" : 166
    }, {
      "referenceID" : 5,
      "context" : "For example, [6] proposes an approach to decrease the number of parameters in a deep convolutional neural network (CNN) while preserving prediction accuracy, and [7, 8] propose approaches to replace floating-point operations with much simpler binary operations.",
      "startOffset" : 13,
      "endOffset" : 16
    }, {
      "referenceID" : 6,
      "context" : "For example, [6] proposes an approach to decrease the number of parameters in a deep convolutional neural network (CNN) while preserving prediction accuracy, and [7, 8] propose approaches to replace floating-point operations with much simpler binary operations.",
      "startOffset" : 162,
      "endOffset" : 168
    }, {
      "referenceID" : 7,
      "context" : "For example, [6] proposes an approach to decrease the number of parameters in a deep convolutional neural network (CNN) while preserving prediction accuracy, and [7, 8] propose approaches to replace floating-point operations with much simpler binary operations.",
      "startOffset" : 162,
      "endOffset" : 168
    }, {
      "referenceID" : 8,
      "context" : "It was shown previously for an implementation of a multilayer perceptron (MLP) based on stochastic computing that it is possible to compensate for a reduced precision by increasing the size of the network [9].",
      "startOffset" : 205,
      "endOffset" : 208
    }, {
      "referenceID" : 9,
      "context" : "[10] using a different approach that consists in adding compensation mechanisms in hardware while keeping the same network size.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 10,
      "context" : "An MLP [11] is composed only of dense layers.",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 11,
      "context" : "A CNN [12] is mainly composed of convolutional layers.",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 12,
      "context" : "The activation function used in all the layers is chosen as the rectified linear unit (reLU) [13].",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 13,
      "context" : "We note that such an activation function has been used before in a different context, in order to avoid the problem of diverging gradients in the training of recurrent networks [14].",
      "startOffset" : 177,
      "endOffset" : 181
    }, {
      "referenceID" : 11,
      "context" : "The second model type is a CNN network composed of L convolutional C × C layers with P × P pooling of type pool, ultimately followed by a dense layer of 200 neurons [12].",
      "startOffset" : 165,
      "endOffset" : 169
    }, {
      "referenceID" : 14,
      "context" : "For simplicity, we opted to train the networks on the task of digit classification using the MNIST dataset [15].",
      "startOffset" : 107,
      "endOffset" : 111
    }, {
      "referenceID" : 15,
      "context" : "Initialization was done as suggested in [16].",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 16,
      "context" : "To prevent overfitting during training, a dropout [17] of 25% and 50% of the neurons have been applied on convolutional and dense layers, respectively, except on the first layer.",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 17,
      "context" : "We used categorical crossentropy as the loss function and the “adadelta” optimizer [18].",
      "startOffset" : 83,
      "endOffset" : 87
    } ],
    "year" : 2017,
    "abstractText" : "For many types of integrated circuits, accepting larger failure rates in computations can be used to improve energy efficiency. We study the performance of faulty implementations of certain deep neural networks based on pessimistic and optimistic models of the effect of hardware faults. After identifying the impact of hyperparameters such as the number of layers on robustness, we study the ability of the network to compensate for computational failures through an increase of the network size. We show that some networks can achieve equivalent performance under faulty implementations, and quantify the required increase in computational complexity.",
    "creator" : "LaTeX with hyperref package"
  }
}