{
  "name" : "1709.00831.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ "ndgurnan@ucsd.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n70 9.\n00 83\n1v 1\n[ cs\n.C L\n] 4\nS ep\n2 01\n7\nexact, distribution free, high-dimensional hypothesis test as an intrinsic evaluation metric for word embeddings. We show that cross-match is an effective means of measuring distributional similarity between different vector representations and of evaluating the statistical significance of different vector embedding models. Additionally, we find that cross-match can be used to provide a quantitative measure of linguistic similarity for selecting bridge languages for machine translation. We demonstrate that the results of the hypothesis test align with our expectations and note that the framework of two sample hypothesis testing is not limited to word embeddings and can be extended to all vector representations."
    }, {
      "heading" : "1 Introduction",
      "text" : "Word embeddings obtained via specialized models (Brown et al., 1992; Pennington et al., 2014; Mikolov et al., 2013a) or neural networks (Bengio et al., 2003) have been successfully used to address various natural language processing tasks (Vaswani et al., 2013; Soricut and Och, 2015). These embeddings provide a nuanced representation of words that can capture various syntactic and semantic properties of natural language (Mikolov et al., 2013b). Despite their effectiveness in downstream applications, embeddings have limited practical value as standalone items. Consequently, an intrinsic evaluation metric must provide insight on the downstream task the embeddings are designed for. In this work, we use Cross-match (Rosenbaum, 2005) - an exact, distribution free, high-dimensional\nhypothesis test to propose a novel approach for intrinsic evaluation of word embeddings, one that provides insight on tasks that depend on linguistic similarity.\nEvaluating general purpose vector representations is difficult. They are trained using simple objectives and applied to a variety of downstream tasks, thus making no single extrinsic evaluation definitive. Often, due to computational constraints, direct downstream evaluations are also impractical. In the case of word embeddings, these constraints have led to the development of dedicated evaluation tasks like similarity and analogy (Rohde et al., 2006; Levy et al., 2015) which are not directly related to training objectives or to downstream tasks. Despite their ease of interpretability, Faruqui et al. (2016) have shown that these tasks do not correlate well with downstream performance. In related work, Tsvetkov et al. (2016) propose an evaluation measure QVECCCA that is shown to correlate well with downstream semantic tasks where the objective is to quantify the linguistic content of word embeddings by maximizing the correlation with a manually annotated linguistic resource.\nIn this work, we use the Cross-match hypothesis test (Rosenbaum, 2005) to measure distributional similarity between different word vector representations. Cross-match is an adjacency based test traditionally used in clinical settings where the goal is to assess no treatment effect on a high-dimensional outcome in a randomized experiment. In our setting, we assume there exists some unknown distribution W from which our constructed word embeddings {w1, . . . ,wn} are “sampled” from. Given two sets of word embeddings, cross-match tests whether the underlying distribution from which the embeddings were “sampled” are identical or not. The test uses optimal non-bipartite matching to pair vectors from\nboth sets of embeddings based on distance (e.g. a vector will be paired with it’s nearest neighbor based on some distance metric). The cross-match test statistic C is the number of times that a vector from one set is paired with a vector from another. The null hypothesis assumes that the vectors were sampled from the same distribution and rejects for small values of C . Thus, a large number of cross-matches between two sets of word embeddings suggests that they are from the same embedding distribution.\nUsing cross-match, we propose two illustrative examples of intrinsic evaluation. First, we use pretrained word vectors (trained on Wikipedia using the skip-gram model in Bojanowski et al. (2016)) from Facebook’s fastText library for several languages to calculate the cross-match statistic for several language pairs. We hypothesize that for linguistically similar languages, a larger statistic will be observed. Secondly, we use cross-match to assess the statistical significant of word embedding models. We consider several well known models trained on the same corpus and use crossmatch to assess whether the respective word vector representations are statistically significantly different. We hypothesize that the number of crossmatches between two different embedding models is small, thus suggesting that they capture fundamentally different linguistic aspects of the corpus.\nThis paper is organized as follows: Section 2 introduces the cross-match test in detail. Experiments on embedding similarity and evaluation are described in Section 3. We discuss extensions and conclude in Section 4."
    }, {
      "heading" : "2 Cross-Match Test",
      "text" : "The cross-match test (Rosenbaum, 2005) is a nonparametric goodness-of-fit test in arbitrary dimensions. It is an exact, distribution-free, twosample hypothesis test that measures whether two distributions are equal or not. Formally, given two independent samples w1, . . . , wn ∼ W and v1, . . . , vm ∼ V , cross-match tests the null hypothesis H0 : W = V versus the alternative hypothesisH1 : W 6= V . The test has been traditionally used in clinical settings, where the goal is to assess no treatment effect on a high-dimensional outcome between control and treated subjects in a randomized experiment (Heller et al., 2010). In the case of word embeddings, the goal is to test whether two sets of word embedding vectors have\nbeen “sampled” from the same distribution."
    }, {
      "heading" : "2.1 Definition of the Cross-Match Statistic",
      "text" : "Let W,V denote two word embedding distributions (distributions of word embedding vectors over a corpus), suppose we obtain two sets of word vectors {w1, . . . ,wn} ∼ W and {v1, . . . ,vm} ∼ V . Assign the group labels 0 and 1 to indicate which sample the vectors are from such that the data are organized as follows: {(0,w1), . . . , (0,wn)} and {(1,v1), . . . (1,vm)}.\nThe cross-match statistic C, is a function of the word vectorsD = {w1, . . . ,wn,v1, . . . ,vn} and the group labels G = {0, . . . , 0, 1, . . . , 1}. If H0 : W = V is true, then all the word vectors are i.i.d. “sampled” from W and the group labels are meaningless. It’s as if the 0’s and 1’s were randomly assigned.\nThe cross-match test is performed as follows. For notational convenience ignore the group labels and treat the data as one sample {z1, . . . , zn+m} of size n+m = N (assume for simplicity thatN is even). We define aN×N symmetric distance matrix, with row k and column l giving the distance (any distance metric can be used) between zk and zl. Compute the optimal non-bipartite matching of the z′s (match the vectors into non-overlapping pairs) that minimizes the total distances between the points in each pair.\nFormally, we find a permutation σ̂ of\n{1, . . . , N} that minimizes\nMatch(σ) = N ∑\ni=1\nd(Zi, Zσ(i))\nwhere i 6= σ(i) and d is our chosen distance measure. The cross-match statistic C , is defined as the number of pairs that have group labels (0,1) or (1,0), the test rejects for small values of C .\nIf there is an odd number of word embedding vectors, then a psuedo-vector is added to the distance matrix at zero distance from everyone else. N\n2 pairs are formed as before, and the pair contain-\ning the psuedo-vector is discarded (thus the least matchable word vector is discarded)."
    }, {
      "heading" : "2.2 Null Distribution of the Cross-Match Statistic",
      "text" : "One advantage of the cross-match test is that we can compute the exact distribution of the statistic C under the null hypothesis H0. Given N\n2 paired\nvectors, let c0 denote the observed number of the pairs with group labels (0,0), let c1 denote the observed number of pairs with group labels (0,1) or (1,0) (this is our observed cross-match statistic) and finally let c2 denote the observed number of pairs with group labels (1,1). The null distribution of C in closed form is:\nf(c1) = P (C = c1) = 2c1n! (\nN n\n)\nc0!c1!c2!\nwhere N2 = c0+ c1+ c2. Having the null distribution in closed form also allows us to compute the exact p-value for our observed cross-match statistic. The resulting p-value is equal to F (c1) where\nF (c1) = P (C ≤ c1) = c1 ∑\nc′ 1 =0\nf(c′1)\nA low p-value would suggests that we have evidence to reject the null hypothesis (at a given level of significance) that the word embedding vectors were “sampled” from the same distribution."
    }, {
      "heading" : "3 Experiments",
      "text" : "In the following experiments, we demonstrate two different illustrative examples of the cross-match test. Our objective is to show the effectiveness of cross-match as a general tool for intrinsic evaluation of word embedding vectors."
    }, {
      "heading" : "3.1 Embedding Similarity",
      "text" : "A bridge language (also referred to as a pivot language), is an artificial or natural language used as an intermediary for translation between two different languages. In machine translation, a bridge language is useful in low-resource situations where a good parallel corpora is not available for the target language. In such cases, a resource rich, linguistically similar language is used as a proxy in order to perform the required NLP task. For example in Tsvetkov and Dyer (2015) the authors use Arabic, Italian and French as bridge languages to perform Swahili-English, Maltese-English and Romanian-English translations respectively.\nAssessing whether languages are linguistically similar is a reasonably difficult task and depends on the notion of similarity one uses (lexical, morphological etc.) In this experiment, we use crossmatch to provide a quantitative measure to assess linguistic similarity between languages.\nWe use pre-trained word vectors (trained on Wikipedia using the skip-gram model in Bojanowski et al. (2016)) from Facebook’s fastText library for several languages and calculate the cross-match statistic for several language pairs. Specifically, we randomly select 100,000 word vectors for each language (with the exception of Maltese and Swahili which have only 26,000 and 52,000 vectors respectively). Then for each language pair, we randomly sample 200 vectors and calculate the number of cross-matches between them using R’s crossmatch package (https://github.com/cran/crossmatch). We repeat this 500 times for each language pair and report the average cross-match statistic.\nTables 1 and 2 present the results of calculating the average number of cross-matches between several English-pair and Maltese-pair languages. We note that with a sample of 400 vectors (200 from each language) the maximum possible number of cross-matches is 200. Given that are our reported statistics are considerably lower than 200 we can safely conclude that the distributions from which the word embedding vec-\ntors were generated are different for different languages. In table 1 we note that the number of cross-matches between English and other romance languages (French, Italian, Spanish, Portuguese, Romanian) is noticeably higher than that between English and non-romance languages (Arabic, Maltese, Swahili). This corresponds with our notions of linguistic similarity between the languages, we certainly expect English to be more “similar” to French than to Maltese. We also note that in table 2, the Maltese-Italian pair has the highest crossmatch statistic, thus supporting the choice of Italian as a bridge language for Maltese."
    }, {
      "heading" : "3.2 Embedding Evaluation",
      "text" : "In this experiment, we use cross-match to assess the statistical significance of word embedding models. Despite the popularity of various different embedding models (Mikolov et al., 2013a,b; Pennington et al., 2014) it is not always clear whether one model represents a statistically significant improvement to other existing models (it maybe that all of them capture largely similar features of the text).\nWe consider four popular word embedding models: word2vec Skip-gram, word2vec CBOW, Glove and fastText all trained on the same English wikipedia corpus. Once again we take samples of size 200 from each method, caluclate the p-value between two pairs of methods using cross-match and then report the average p-value across 500 repeated iterations.\nThe results in 3 show low p-values across all pairs of word embedding methods thus suggesting that they all seem to capture different aspects of the corpus they are modeling. In other words, using cross-match we have evidence to reject the null hypothesis that the vectors derived from any pair of models come from the same word embedding distribution.\nLastly, we note that there are at present some computational constraints in performing the crossmatch test. There exists a bottleneck in the calculation of the optimal non-bipartite matching and\nthis makes performing the test for larger sample sizes currently intractable. However, we feel confident that this software issue can be easily overcome by writing custom routines (as opposed to using existing open-source code) and parallelizing the problem. As a result of our limited sample size, we not that it is possible that the power of our hypothesis test is low and thus we may be making type I errors (falsely rejecting the null). Nonetheless our initial results seem promising and are in line with our expectations."
    }, {
      "heading" : "4 Conclusion",
      "text" : "In this work we introduced the cross-match test, an exact, distribution free, high-dimensional hypothesis test as an intrinsic evaluation metric for word embeddings. We were able to demonstrate on two illustrative examples that the test performs reasonably in line with our expectations and can potentially be a useful tool in assessing bridge languages for machine translation. Despite the initially promising results, much further work remains to be done in order to confirm the efficacy of cross-match in the context of word embeddings.\nWe posit that our main contribution is the introduction of the hypothesis testing framework as a method for intrinsic evaluation of vector representations. We observe that there is nothing notable about word embeddings or the cross-match test and our experiments could be extended for other vector representations (sentence, phrase etc.) using other modern two-sample hypothesis tests such as the popular maximum mean discrepancy (Gretton et al., 2012). Given the rich literature on hypothesis testing in statistics, there is certainly much to be explored here.\nFor future work we aim to focus solely on the problem of bridge languages in machine translation. Our objective is to conduct a larger scale study that is able to definitively show a strong correlation between the results of a hypothesis test on word embedding vectors, and their subsequent performance on the downstream machine translation task."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank Ndapa Nakashole for several useful discussions helping formulate the problem and the anonymous reviewers for their feedback."
    } ],
    "references" : [ {
      "title" : "A neural probabilistic language model",
      "author" : [ "Yoshua Bengio", "Réjean Ducharme", "Pascal Vincent", "Christian Janvin." ],
      "venue" : "J. Mach. Learn. Res. 3:1137–1155.",
      "citeRegEx" : "Bengio et al\\.,? 2003",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2003
    }, {
      "title" : "Enriching word vectors with subword information",
      "author" : [ "Piotr Bojanowski", "Edouard Grave", "Armand Joulin", "Tomas Mikolov." ],
      "venue" : "arXiv preprint arXiv:1607.04606 .",
      "citeRegEx" : "Bojanowski et al\\.,? 2016",
      "shortCiteRegEx" : "Bojanowski et al\\.",
      "year" : 2016
    }, {
      "title" : "Class-based n-gram models of natural language",
      "author" : [ "Peter F. Brown", "Peter V. deSouza", "Robert L. Mercer", "Vincent J. Della Pietra", "Jenifer C. Lai." ],
      "venue" : "Comput. Linguist. 18(4):467–479.",
      "citeRegEx" : "Brown et al\\.,? 1992",
      "shortCiteRegEx" : "Brown et al\\.",
      "year" : 1992
    }, {
      "title" : "Problems with evaluation of word embeddings using word similarity tasks",
      "author" : [ "Manaal Faruqui", "Yulia Tsvetkov", "Pushpendre Rastogi", "Chris Dyer." ],
      "venue" : "CoRR abs/1605.02276.",
      "citeRegEx" : "Faruqui et al\\.,? 2016",
      "shortCiteRegEx" : "Faruqui et al\\.",
      "year" : 2016
    }, {
      "title" : "A kernel two-sample test",
      "author" : [ "Arthur Gretton", "Karsten M. Borgwardt", "Malte J. Rasch", "Bernhard Schölkopf", "Alexander Smola." ],
      "venue" : "J. Mach. Learn. Res. 13:723–773.",
      "citeRegEx" : "Gretton et al\\.,? 2012",
      "shortCiteRegEx" : "Gretton et al\\.",
      "year" : 2012
    }, {
      "title" : "Sensitivity analysis for the cross-match test, with applications in genomics",
      "author" : [ "Ruth Heller", "Shane T. Jensen", "Paul R. Rosenbaum", "Dylan S. Small." ],
      "venue" : "Journal of the American Statistical Association 105(491):1005–1013.",
      "citeRegEx" : "Heller et al\\.,? 2010",
      "shortCiteRegEx" : "Heller et al\\.",
      "year" : 2010
    }, {
      "title" : "Improving distributional similarity with lessons learned from word embeddings",
      "author" : [ "Omer Levy", "Yoav Goldberg", "Ido Dagan." ],
      "venue" : "Transactions of the Association for Computational Linguistics 3:211–225.",
      "citeRegEx" : "Levy et al\\.,? 2015",
      "shortCiteRegEx" : "Levy et al\\.",
      "year" : 2015
    }, {
      "title" : "Efficient estimation of word representations in vector space",
      "author" : [ "Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean." ],
      "venue" : "CoRR abs/1301.3781.",
      "citeRegEx" : "Mikolov et al\\.,? 2013a",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean." ],
      "venue" : "C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger, editors, Ad-",
      "citeRegEx" : "Mikolov et al\\.,? 2013b",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher D Manning." ],
      "venue" : "EMNLP. volume 14, pages 1532– 1543.",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "An improved model of semantic similarity based on lexical co-occurence",
      "author" : [ "Douglas L.T. Rohde", "Laura M. Gonnerman", "David C. Plaut." ],
      "venue" : "COMMUNICATIONS OF THE ACM 8:627–633.",
      "citeRegEx" : "Rohde et al\\.,? 2006",
      "shortCiteRegEx" : "Rohde et al\\.",
      "year" : 2006
    }, {
      "title" : "An exact distribution-free test comparing two multivariate distributions based on adjacency",
      "author" : [ "Paul R. Rosenbaum." ],
      "venue" : "Journal of the Royal Statistical Society: Series B (Statistical Methodology) 67(4):515– 530.",
      "citeRegEx" : "Rosenbaum.,? 2005",
      "shortCiteRegEx" : "Rosenbaum.",
      "year" : 2005
    }, {
      "title" : "Unsupervised morphology induction using word embeddings",
      "author" : [ "Radu Soricut", "Franz Och." ],
      "venue" : "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Asso-",
      "citeRegEx" : "Soricut and Och.,? 2015",
      "shortCiteRegEx" : "Soricut and Och.",
      "year" : 2015
    }, {
      "title" : "Cross-lingual bridges with models of lexical borrowing",
      "author" : [ "Yulia Tsvetkov", "Chris Dyer." ],
      "venue" : "J. Artif. Intell. Res. (JAIR) 55:63–93.",
      "citeRegEx" : "Tsvetkov and Dyer.,? 2015",
      "shortCiteRegEx" : "Tsvetkov and Dyer.",
      "year" : 2015
    }, {
      "title" : "Correlation-based intrinsic evaluation of word vector representations",
      "author" : [ "Yulia Tsvetkov", "Manaal Faruqui", "Chris Dyer" ],
      "venue" : "CoRR abs/1606.06710",
      "citeRegEx" : "Tsvetkov et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Tsvetkov et al\\.",
      "year" : 2016
    }, {
      "title" : "Decoding with large-scale neural language models improves translation",
      "author" : [ "Ashish Vaswani", "Yinggong Zhao", "Victoria Fossum", "David Chiang." ],
      "venue" : "EMNLP. ACL, pages 1387–1392.",
      "citeRegEx" : "Vaswani et al\\.,? 2013",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "Word embeddings obtained via specialized models (Brown et al., 1992; Pennington et al., 2014; Mikolov et al., 2013a) or neural networks (Bengio et al.",
      "startOffset" : 48,
      "endOffset" : 116
    }, {
      "referenceID" : 9,
      "context" : "Word embeddings obtained via specialized models (Brown et al., 1992; Pennington et al., 2014; Mikolov et al., 2013a) or neural networks (Bengio et al.",
      "startOffset" : 48,
      "endOffset" : 116
    }, {
      "referenceID" : 7,
      "context" : "Word embeddings obtained via specialized models (Brown et al., 1992; Pennington et al., 2014; Mikolov et al., 2013a) or neural networks (Bengio et al.",
      "startOffset" : 48,
      "endOffset" : 116
    }, {
      "referenceID" : 0,
      "context" : ", 2013a) or neural networks (Bengio et al., 2003) have been successfully used to address various natural language processing",
      "startOffset" : 28,
      "endOffset" : 49
    }, {
      "referenceID" : 15,
      "context" : "tasks (Vaswani et al., 2013; Soricut and Och, 2015).",
      "startOffset" : 6,
      "endOffset" : 51
    }, {
      "referenceID" : 12,
      "context" : "tasks (Vaswani et al., 2013; Soricut and Och, 2015).",
      "startOffset" : 6,
      "endOffset" : 51
    }, {
      "referenceID" : 8,
      "context" : "syntactic and semantic properties of natural language (Mikolov et al., 2013b).",
      "startOffset" : 54,
      "endOffset" : 77
    }, {
      "referenceID" : 11,
      "context" : "In this work, we use Cross-match (Rosenbaum, 2005) - an exact, distribution free, high-dimensional hypothesis test to propose a novel approach for intrinsic evaluation of word embeddings, one that provides insight on tasks that depend on linguistic similarity.",
      "startOffset" : 33,
      "endOffset" : 50
    }, {
      "referenceID" : 10,
      "context" : "In the case of word embeddings, these constraints have led to the development of dedicated evaluation tasks like similarity and analogy (Rohde et al., 2006; Levy et al., 2015) which are not directly related to training objectives or to downstream tasks.",
      "startOffset" : 136,
      "endOffset" : 175
    }, {
      "referenceID" : 6,
      "context" : "In the case of word embeddings, these constraints have led to the development of dedicated evaluation tasks like similarity and analogy (Rohde et al., 2006; Levy et al., 2015) which are not directly related to training objectives or to downstream tasks.",
      "startOffset" : 136,
      "endOffset" : 175
    }, {
      "referenceID" : 3,
      "context" : "Despite their ease of interpretability, Faruqui et al. (2016) have shown that these tasks do not correlate well with downstream performance.",
      "startOffset" : 40,
      "endOffset" : 62
    }, {
      "referenceID" : 3,
      "context" : "Despite their ease of interpretability, Faruqui et al. (2016) have shown that these tasks do not correlate well with downstream performance. In related work, Tsvetkov et al. (2016) propose an evaluation measure QVECCCA that is shown to correlate well with downstream semantic tasks where the objective is to",
      "startOffset" : 40,
      "endOffset" : 181
    }, {
      "referenceID" : 11,
      "context" : "In this work, we use the Cross-match hypothesis test (Rosenbaum, 2005) to measure distributional similarity between different word vec-",
      "startOffset" : 53,
      "endOffset" : 70
    }, {
      "referenceID" : 1,
      "context" : "First, we use pretrained word vectors (trained on Wikipedia using the skip-gram model in Bojanowski et al. (2016)) from Facebook’s fastText library for several languages to calculate the cross-match statistic for several language pairs.",
      "startOffset" : 89,
      "endOffset" : 114
    }, {
      "referenceID" : 11,
      "context" : "The cross-match test (Rosenbaum, 2005) is a nonparametric goodness-of-fit test in arbitrary dimensions.",
      "startOffset" : 21,
      "endOffset" : 38
    }, {
      "referenceID" : 5,
      "context" : "The test has been traditionally used in clinical settings, where the goal is to assess no treatment effect on a high-dimensional outcome between control and treated subjects in a randomized experiment (Heller et al., 2010).",
      "startOffset" : 201,
      "endOffset" : 222
    }, {
      "referenceID" : 13,
      "context" : "For example in Tsvetkov and Dyer (2015) the authors use Arabic, Italian and French as bridge languages to perform Swahili-English, Maltese-English and Romanian-English translations respectively.",
      "startOffset" : 15,
      "endOffset" : 40
    }, {
      "referenceID" : 9,
      "context" : "Despite the popularity of various different embedding models (Mikolov et al., 2013a,b; Pennington et al., 2014) it is not always clear whether one model represents a statistically significant improvement to other existing models (it maybe that all of them capture largely similar features of the text).",
      "startOffset" : 61,
      "endOffset" : 111
    }, {
      "referenceID" : 4,
      "context" : ") using other modern two-sample hypothesis tests such as the popular maximum mean discrepancy (Gretton et al., 2012).",
      "startOffset" : 94,
      "endOffset" : 116
    } ],
    "year" : 2017,
    "abstractText" : "We introduce the cross-match test an exact, distribution free, high-dimensional hypothesis test as an intrinsic evaluation metric for word embeddings. We show that cross-match is an effective means of measuring distributional similarity between different vector representations and of evaluating the statistical significance of different vector embedding models. Additionally, we find that cross-match can be used to provide a quantitative measure of linguistic similarity for selecting bridge languages for machine translation. We demonstrate that the results of the hypothesis test align with our expectations and note that the framework of two sample hypothesis testing is not limited to word embeddings and can be extended to all vector representations.",
    "creator" : "LaTeX with hyperref package"
  }
}