{
  "name" : "1609.06375.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Theory of Interactive Debugging of Knowledge Bases in Monotonic Logics",
    "authors" : [ "Patrick Rodler" ],
    "emails" : [ "patrick.rodler@aau.at" ],
    "sections" : [ {
      "heading" : null,
      "text" : ""
    }, {
      "heading" : "1 Introduction 1",
      "text" : ""
    }, {
      "heading" : "2 Preliminaries 13",
      "text" : "2.1 Assumptions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 2.2 Considered Logics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 2.3 Notational Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16"
    }, {
      "heading" : "3 Knowledge Base Debugging 19",
      "text" : "3.1 Parsimonious Knowledge Base Debugging . . . . . . . . . . . . . . . . . . . . . . . . . 25 3.2 Background Knowledge . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26"
    }, {
      "heading" : "4 Diagnosis Computation 28",
      "text" : "4.1 Conflict Sets versus Justifications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 4.2 The Relation between Conflict Sets and Diagnoses . . . . . . . . . . . . . . . . . . . . 32 4.3 Methods for Diagnosis Computation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\n4.3.1 Computation of a Minimal Conflict Set . . . . . . . . . . . . . . . . . . . . . . 36 4.3.2 Correctness of Conflict Set Computation . . . . . . . . . . . . . . . . . . . . . 41\n4.4 Hitting Set Tree Based Diagnosis Computation . . . . . . . . . . . . . . . . . . . . . . 48 4.5 Diagnosis Probability Space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56\n4.5.1 Construction of a Probability Space . . . . . . . . . . . . . . . . . . . . . . . . 58 4.5.2 Using Probabilities for Diagnosis Computation . . . . . . . . . . . . . . . . . . 63 4.5.3 Correctness of Weighted Diagnosis Computation . . . . . . . . . . . . . . . . . 66 4.5.4 Using Probabilities to Compute Minimum Cardinality Diagnoses . . . . . . . . 69\n4.6 Non-Interactive Debugging Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . 70"
    }, {
      "heading" : "5 Interactive Knowledge Base Debugging 75",
      "text" : "5.1 User Interaction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76\n5.1.1 Queries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77 5.1.2 Leading Diagnoses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78 5.1.3 Q-Partitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78\n1\nCONTENTS 2\n5.1.4 Interpretation of Q-Partitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79 5.1.5 The Relation between a Query and its Q-Partition . . . . . . . . . . . . . . . . . 80 5.1.6 Existence of Queries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81\n5.2 Query Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82 5.2.1 Generation of a Pool of Queries . . . . . . . . . . . . . . . . . . . . . . . . . . 85 5.2.2 Discussion of Query Pool Generation . . . . . . . . . . . . . . . . . . . . . . . 88 5.2.3 Minimization of Queries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94 5.2.4 Soundness of Query Minimization . . . . . . . . . . . . . . . . . . . . . . . . . 96 5.2.5 Complexity of Query Pool Generation . . . . . . . . . . . . . . . . . . . . . . . 97 5.2.6 Shortcomings of Query Pool Generation . . . . . . . . . . . . . . . . . . . . . . 98 5.2.7 Correctness of Query Pool Generation . . . . . . . . . . . . . . . . . . . . . . . 99 5.3 An Algorithm for Interactive Knowledge Base Debugging . . . . . . . . . . . . . . . . 102 5.3.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102 5.3.2 Detailed Description . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103\n5.3.2.1 Input Arguments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104 5.3.2.2 Output . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105 5.3.2.3 Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107 5.3.2.4 Algorithm Walkthrough . . . . . . . . . . . . . . . . . . . . . . . . . 109\n5.3.3 Query Selection Measures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115 5.3.4 Correctness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117"
    }, {
      "heading" : "6 Iterative Diagnosis Computation 121",
      "text" : "6.1 STATICHS: A Static Iterative Diagnosis Computation Algorithm . . . . . . . . . . . . . 121\n6.1.1 Overview and Intuition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121 6.1.2 Algorithm Walkthrough . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124 6.1.3 Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127\n6.2 DYNAMICHS: A Dynamic Iterative Diagnosis Computation Algorithm . . . . . . . . . . 137 6.2.1 Overview and Intuition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137 6.2.2 Algorithm Walkthrough . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140 6.2.3 Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147 6.3 Discussion of Iterative Diagnosis Computation . . . . . . . . . . . . . . . . . . . . . . 164"
    }, {
      "heading" : "7 Related Work 168",
      "text" : ""
    }, {
      "heading" : "8 Summary and Future Work 171",
      "text" : "Bibliography 175\nList of Figures\n1.1 The Principle of Non-Interactive KB Debugging . . . . . . . . . . . . . . . . . . . . . . 3 1.2 The Principle of Interactive KB Debugging . . . . . . . . . . . . . . . . . . . . . . . . 8\n4.1 Recursion Tree for the Computation of a Minimal Conflict Set . . . . . . . . . . . . . . 40 4.2 Non-Interactive KB Debugging Process without Fault Information . . . . . . . . . . . . 71 4.3 Non-Interactive KB Debugging Process with Fault Information . . . . . . . . . . . . . . 72\n6.1 (Example 6.1) Solving the Problem of Interactive Static KB Debugging . . . . . . . . . 133 6.2 (Example 6.2) Solving the Problem of Interactive Static KB Debugging . . . . . . . . . 134 6.3 (Example 6.2 continued) Solving the Problem of Interactive Static KB Debugging . . . . 135 6.4 (Example 6.3) Solving the Problem of Interactive Dynamic KB Debugging . . . . . . . 152 6.5 (Example 6.3 continued) Solving the Problem of Interactive Dynamic KB Debugging . . 153 6.6 (Example 6.4) Solving the Problem of Interactive Dynamic KB Debugging . . . . . . . 158 6.7 (Example 6.4 continued) Solving the Problem of Interactive Dynamic KB Debugging . . 159\ni\nList of Tables\n2.1 Symbols and Abbreviations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n4.1 Propositional Logic Example DPI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36 4.2 Description Logic Example DPI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 4.3 Description Logic Example DPI 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 4.4 Computation of Fault Probabilities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70\n5.1 First-Order Logic Example DPI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83 5.2 (Example 5.1) Computing Entailments for Query Generation . . . . . . . . . . . . . . . 87 5.3 Queries and Associated Q-Partitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96 5.4 (Example 5.7) Diagnoses Probabilities . . . . . . . . . . . . . . . . . . . . . . . . . . . 118\n6.1 (Example 6.2) Formula Fault Probabilities . . . . . . . . . . . . . . . . . . . . . . . . . 129 6.2 Comparison: STATICHS versus DYNAMICHS. . . . . . . . . . . . . . . . . . . . . . . . 163\nii\nList of Algorithms\n1 QX: Computation of a Minimal Conflict Set . . . . . . . . . . . . . . . . . . . . . . . . 39 2 HS: Computation of Minimal Diagnoses . . . . . . . . . . . . . . . . . . . . . . . . . . 57 3 Non-Interactive KB Debugging . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71 4 Generation of Queries and Q-Partitions . . . . . . . . . . . . . . . . . . . . . . . . . . 86 5 Interactive KB Debugging . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101 6 Interactive KB Debugging (continued) . . . . . . . . . . . . . . . . . . . . . . . . . . . 102 7 Iterative Construction of a Static Hitting Set Tree . . . . . . . . . . . . . . . . . . . . . 136 8 Iterative Construction of a Dynamic Hitting Set Tree 1 . . . . . . . . . . . . . . . . . . 160 9 Iterative Construction of a Dynamic Hitting Set Tree 2 . . . . . . . . . . . . . . . . . . 161 10 Iterative Construction of a Dynamic Hitting Set Tree 3 . . . . . . . . . . . . . . . . . . 162\niii\nAbstract\nMost artificial intelligence applications rely on knowledge about a relevant real-world domain that is encoded in a knowledge base (KB) by means of some logical knowledge representation language. The most essential benefit of such logical KBs is the opportunity to perform automatic reasoning to derive implicit knowledge or to answer complex queries about the modeled domain. The feasibility of meaningful reasoning requires a KB to meet some minimal quality criteria such as consistency; that is, there must not be any contradictions in the KB. Without adequate tool assistance, the task of resolving such violated quality criteria in a KB can be extremely hard even for domain experts, especially when the problematic KB includes a large number of logical formulas, comprises complicated formalisms, was developed by multiple people or in a distributed fashion or was (partially) generated by means of some automatic systems.\nNon-interactive Debugging systems published in research literature often cannot localize all possible faults (incompleteness), suggest the deletion or modification of unnecessarily large parts of the KB (nonminimality), return incorrect solutions which lead to a repaired KB not satisfying the imposed quality requirements (unsoundness) or suffer from poor scalability due to the inherent complexity of the KB debugging problem. Even if a system is complete and sound and considers only minimal solutions, there are generally exponentially many solution candidates to select one from. However, any two repaired KBs obtained from these candidates differ in their semantics in terms of entailments and non-entailments. Selection of just any of these repaired KBs might result in unexpected entailments, the loss of desired entailments or unwanted changes to the KB which in turn might cause unexpected new faults during the further development or application of the repaired KB. Also, manual inspection of a large set of solution candidates can be time-consuming (if not practically infeasible), tedious and error-prone since human beings are normally not capable of fully realizing the semantic consequences of deleting a set of formulas from a KB. Hence there is a need for adequate tools that support a user when facing a faulty KB.\nIn this work, we account for these issues and propose methods for the interactive debugging of KBs which are complete and sound and compute only minimally invasive solutions, i.e. suggest the deletion or modification of just a set-minimal subset of the formulas in the problematic KB. User interaction takes place in the form of queries asked to a person, e.g. a domain expert, about intended and non-intended entailments of the correct KB. To construct a query, only a minimal set of two solution candidates must be available. After the answer to a query is known, the search space for solutions is pruned. Iteration of this process until there is only a single solution candidate left yields a repaired KB which features exactly the semantics desired and expected by the user.\nThe novel contributions of this work are:\n• Thorough Theoretical Workup of the Topic of Interactive Debugging of Monotonic KBs: We evolve the theory of the topic by first elaborating on the theory of non-interactive KB debugging, revealing crucial shortcomings in the application of non-interactive methods and thereby motivating the development and deployment of interactive approaches in KB debugging. Then, we give some important results that guarantee the feasibility of interactive KB debugging, give some precise definitions of the problems interactive KB debugging aims to solve and present algorithms that provably solve these problems.\niv\nABSTRACT v\n• A Complete Picture of an Interactive Debugging System Is Drawn: This is the first work that deals with an entire system of algorithms that are required for the interactive debugging of monotonic KBs, considers and details all algorithms separately, proves their correctness and demonstrates how all these algorithms are orchestrated to make up a full-fledged and provably correct interactive KB debugging system.\n• Two New Algorithms for the iterative computation of candidate solutions in the scope of interactive KB debugging are proposed. The first one guarantees constant convergence towards the exact solution of the interactive KB problem by the ascertained reduction of the number of remaining solutions after any query is answered. The second one features powerful search tree pruning techniques and might thus be expected to exhibit a more time- and space-saving behavior than existing algorithms, in particular for growing problem instances.\nChapter 1\nIntroduction\nMotivation. Most artificial intelligence applications rely on knowledge that is encoded in a knowledge base (KB) by means of some logical knowledge representation language such as propositional logic [10], datalog [9], first-order logic (FOL) [10], The Web Ontology Language (OWL [56], OWL 2 [21, 50]) or description logic (DL) [3]. Experts in a variety of application domains keep developing KBs of constantly growing size. A concrete example of a repository containing biomedical KBs is the Bioportal1, which comprises vast ontologies with tens or even hundreds of thousands of terms each (e.g. the SNOMED-CT ontology with currently over 395.000 terms). Such KBs however pose a significant challenge for people as well as tools involved in their evolution, maintenance and application.\nAll these activities are based on the most essential benefit of KBs, namely the opportunity to perform automatic reasoning to derive implicit knowledge or to answer complex queries about the modeled domain. The feasibility of meaningful reasoning requires a KB to meet the minimum quality criterion consistency, i.e. there must not be any contradictions in the KB. Because any logical formula can be derived from an inconsistent KB. Further on, one might postulate further requirements to be met by a KB. For instance, one might consider faulty a FOL KB entailing ∀X ¬p(X) for some predicate symbol p occurring in the KB. Such a KB would be incoherent, i.e. it would violate the requirement coherency (which has originally been defined for DL KBs [68, 55]. Additionally, test cases can be specified giving information about desired (positive test cases) and non-desired (negative test cases) entailments a correct KB should feature. This characterization of a KB’s intended semantics is a direct analogon to the field of software debugging, where test cases are exploited as a means to verify the correct semantics of the program code.\nAs KBs are growing in size and complexity, their likeliness of violating one of these criteria increases. Faults in KBs may, for instance, arise because human reasoning is simply overstrained [24, 26]. That is, generally a person will not be capable of completely grasping or mentally processing the entire knowledge contained in a (large or complex) KB at once. In fact, a person might fully comprehend some isolated part of a the KB, but might not be able to determine or understand all implications or non-implications of this isolated part combined with other parts of a KB, i.e. when new logical formulas are added.\nAnother reason for the non-compliance with the mentioned quality criteria imposed on KBs might be that multiple (independently working) editors contribute to the development of the KB [53] which may lead to contradictory formulas. The OBO Project2 and the NCI Thesaurus3 are examples of collaborative KB development projects. Employing automatic tools, e.g. [33, 51, 32], to generate (parts of) KBs can further exacerbate the task of KB quality assurance [46, 16].\n1http://bioportal.bioontology.org 2http://obo.sourceforge.net 3http://nciterms.nci.nih.gov/ncitbrowser\n1\nCHAPTER 1. INTRODUCTION 2\nMoreover, as studies in cognitive psychology [8, 35] attest, humans make systematic errors while formulating or interpreting logical formulas. These observations are confirmed by [59, 64] which present common faults people make when developing a KB (ontology). Hence, it is essential to devise methods that can efficiently identify and correct faults in a KB.\nNon-Interactive KB Debugging. Given a set of requirements to the KB and sets of test cases, KB debugging methods [68, 38, 19, 25] can localize a (potential) fault by computing a subset D of the formulas in the KB K called a diagnosis. At least all formulas in a diagnosis must be (adequately) modified or deleted in order to obtain a KB K∗ that satisfies all postulated requirements and test cases. Such a KB K∗ constitutes the solution to the KB debugging problem. Figure 1.14 outlines such a KB debugging system. The input to the system is a diagnosis problem instance (DPI) defined by\n• some KB K formulated using some (monotonic) logical language L (every formula in K might be correct or faulty),\n• (optionally) some KB B (over L) formalizing some background knowledge relevant for the domain modeled by K (such that B and K do not share any formulas; all formulas in B are considered correct)\n• a set of requirements R to the correct KB,\n• sets of positive (P ) and negative (N ) test cases (over L) asserting desired semantic properties of the correct KB and\n• (optionally) some fault information FP, e.g. in terms of fault probabilities of logical formulas in K.\nMoreover, the system requires a sound and complete logical reasoner for deciding consistency (coherency) and calculating logical entailments of a KB formulated over the language L. Some approaches (including the ones presented in this work) use the reasoner as a black-box (e.g. [74, 23]) within the debugging system. That is, the reasoner is called as is and serves as an oracle independent from other computations during the debugging process; that is, the internals of the reasoner are irrelevant for the debugging task. On the other hand, glass-box approaches (e.g. [68, 23, 41]) attempt to exploit internal modifications of the reasoner for debugging purposes; in other words, the sources of problems (e.g. contradictory formulas) in the KB are computed as a direct consequence of reasoning [23]. The advantages of a black-box approach over a glass-box approach are the lower memory consumption and better performance [41] of the reasoner and the reasoner independence of the debugging method. The latter benefit is essential for the generality of our approaches and their applicability to various knowledge representation formalisms.\nGiven these inputs, the debugging system focuses on (a subset of) all possible fault candidates (usually the set of minimal, i.e. irreducible, diagnoses) and usually outputs the most probable one amongst these if some fault information is provided or the minimum cardinality one, otherwise. Alternatively, a debugging system might also be employed to calculate a predefined number of (most probable or minimum cardinality) minimal diagnoses or to determine all minimal diagnoses computable within a predefined time limit.\nIssues with Non-Interactive KB Debugging Systems. In real-world scenarios, debugging tools often have to cope with large numbers of minimal diagnoses where the trivial application, i.e. deletion, of any minimal diagnosis leads to a (repaired) KB with different semantics in terms of entailed and nonentailed formulas. For example, in [73] a sample study of real-world KBs revealed that the number\n4Thanks to Kostyantyn Shchekotykhin for making available to me parts of this diagram.\nCHAPTER 1. INTRODUCTION 3\nof different minimal diagnoses might exceed thousand by far (1782 minimal diagnoses for a KB with only 1300 formulas). In such situations simple visualization of all these alternative modifications of the ontology is clearly ineffective. Selecting a wrong diagnosis (in terms of its semantics, not in terms of fulfillment of test cases and requirements) can lead to unexpected entailments or non-entailments, lost desired entailments and surprising future faults when the KB is further developed. Manual inspection of a large set of (minimal) diagnoses is time-consuming (if not practically infeasible), error-prone and often computationally infeasible due to the complexity of diagnosis computation.\nMoreover, [81] has put several (non-interactive) debugging systems to the test using a test set of faulty (incoherent OWL) real-world KBs which were partly designed by humans and partly by the application of automatic systems. The result was that most of the investigated systems had serious performance problems, ran out of memory, were not able to locate all the existing faults in the KB (incompleteness), reported parts of a KB as faulty which actually were not faulty (unsoundness), produced only trivial solutions or suggested non-minimal faults (non-minimality). Often, performance problems and incompleteness of non-interactive debugging methods can be traced back to an explosion of the search tree for minimal diagnoses.\nThe Solution: Interactive KB Debugging. In this work we present algorithms for interactive KB debugging. These aim at the gradual reduction of compliant minimal diagnoses by means of user interaction, thereby seeking to prevent the search tree for minimal diagnoses from exploding in size by performing regular pruning operations. “User” in this case might refer to a single person or multiple persons, usually experts of the particular domain the faulty KB is dealing with such as biology, medicine or chemistry. Throughout an interactive debugging session, the user is asked a set of automatically chosen queries about the domain that should be modeled by a given faulty KB. A query can be created by the system after a set D of a minimum of two minimal diagnoses has been precomputed (we call D the leading diagnoses). Each query is a conjunction (i.e. a set) of logical formulas that are entailed by some correct subset of the formulas in the KB. With regard to one particular query Q, any set of minimal diagnoses for the KB, in particular the set D which has been utilized to generate Q, can be partitioned into three sets, the first one (D+) including all diagnoses in D compliant only with a positive answer to Q, the second (D−) including all diagnoses in D compliant only with a negative answer to Q, and the third (D0) including all diagnoses in D compliant with both answers. A positive answer to Q signalizes that the conjunction of formulas in Q must be entailed by the correct KB wherefore Q is added to the set of positive test cases. Likewise, if the user negates Q, this is an indication that at least one formula in Q must not be entailed by the correct KB. As a consequence, Q is added to the set of negative test cases.\nCHAPTER 1. INTRODUCTION 4\nAssignment of a query Q to either set of test cases results in a new debugging scenario. In this new scenario, all elements of D− are no longer minimal diagnoses given that Q has been classified as a positive test case. Otherwise, all diagnoses in D+ are invalidated. In this vein, the successive reply to queries generated by the system will lead the user to the single minimal solution diagnosis that perfectly reflects their intended semantics. In other words, after deletion of all formulas in the solution diagnosis from the KB and the addition of the conjunction of all formulas in the specified positive test cases to the KB, the resulting KB meets all requirements and positive as well as negative test cases. In that, the added formulas contained in the positive test cases serve to replace the desired entailments that are broken due to the deletion of the solution diagnosis from the KB.\nThence, in the interactive KB debugging scenario the user is not required to cope with the understanding of which faults (e.g. sources of inconsistency or implications of negative test cases) occur in the faulty initial KB, why they are faults (i.e. why particular entailments are given and others not) and how to repair them. All these tasks are undertaken by the interactive debugging system.\nThe proposed approaches to interactive KB debugging in this work follow the standard model-based diagnosis (MBD) technique [60, 44]. MBD has been successfully applied to a great variety of problems in various fields such as robotics [79], planning [80], debugging of software programs [87], configuration problems [17], hardware designs [20], constraint satisfaction problems and spreadsheets [1]. Given a description (model) of a system, together with an observation of the system’s behavior which conflicts with the intended behavior of the system, the task of MBD is to find those components of the system (a diagnosis) which, when assumed to be functioning abnormally, provide an explanation of the discrepancy between the intended and the observed system behavior. Translated to the setting of KB debugging, the set of “system components” comprises the formulas ax i in the given faulty KB K. The “system description” refers to the statement that the KB K along with the background KB B and the positive test cases p ∈ P must meet all predefined requirements (e.g. consistency, coherency) and must not logically entail any of the negative test cases n ∈ N , i.e.\n(i) K ∪ B ∪ ⋃\np∈P p satisfies requirement r for all r ∈ R and (ii) K ∪ B ∪ ⋃\np∈P p 6|= n for all n ∈ N .\nThe “observation which conflicts with the intended behavior of the system” corresponds to the finding that (i) or (ii) or both are violated. That is, the “system description” along with the “observation” and the assumption that all components are sound yields an inconsistency. An “explanation for the discrepancy between observed and intended system behavior” (i.e. a diagnosis) is the assumption D that all formulas in a subsetD ofK are faulty (“behave abnormally”) and all formulas inK\\D are correct (“do not behave abnormally”) such that the “system description” along with the “observation” and the assumption D is consistent. Computation of (minimal) diagnoses is accomplished with the aid of minimal conflict sets, i.e. irreducible sets of formulas in the KB K that preserve the violation of (i) or (ii) or both.\nAn MBD problem can be modeled as an abduction problem [7], i.e. finding an explanation for a set of data. It was proven in [7] that the computation of the first explanation (minimal diagnosis) is in P. However, given a set of explanations (minimal diagnoses) it is NP-complete to decide whether there is an additional explanation (minimal diagnosis). Stated differently, the detection of the first explanation can be efficiently accomplished whereas the finding of any further one is intractable (unless P = NP). When seeing the (interactive) KB debugging problem as an abduction problem, one must additionally take into account the costs for reasoning. Because, a call to a logical reasoner is required in order to decide whether or not a set of hypotheses (a subset of the KB) is an explanation (minimal diagnosis). Incorporating the necessary reasoning costs and assuming consistency a minimal requirement to the correct KB, the finding of the first explanation (minimal diagnosis) is already NP-hard even for propositional KBs [70] (since propositional satisfiability checking is NP-complete). The worst case complexity for the debugging of KBs formulated over more expressive logics such as OWL 2 (reasoning is 2-NExpTimecomplete [21, 42]) will be of course even worse. This seems quite discouraging. However, we have shown\nCHAPTER 1. INTRODUCTION 5\nin our previous works [63, 74, 76] that for many real-world KBs interactive KB debugging is feasible in reasonable time, despite high (or intractable) worst case reasoning costs and the intractable complexity of the abduction (i.e. minimal diagnosis finding) problem as such. Hence, the goal of this work is amongst others to present algorithms that work well in many practical scenarios.\nAssumptions about the Interacting User. About a user u consulting an (interactive) debugging system, we make the following plausible assumptions:\nU1 u is not able to explicitly enumerate a set of logical formulas that express the intended domain that should be modeled in a satisfactory way, i.e. without unwanted entailments or non-fulfilled requirements,\nU2 u is able to answer concrete queries about the intended domain that should be modeled, i.e. u can classify a given logical formula (or a conjunction of logical formulas) as a wanted or unwanted proposition in the intended domain (i.e. an entailment or non-entailment of the correct domain model).\nThe first assumption is obviously justified since otherwise u could have never obtained a faulty KB, i.e. a KB that violates at least one requirement or test case, and there would be no need for u to employ a debugging system.\nRegarding the second assumption, the first thing to be noted is that any KB (i.e. any model of the intended domain) either does entail a certain logical formula ax or it does not entail ax . Second, if u is assumed to bring along enough expertise in that domain, u should be able to gauge the truth of (at least) some formulas about that domain, especially if these formulas constitute logical entailments of parts of the specified knowledge in KB so far. We want to emphasize that u is not required to be capable of answering all possible queries (or formulas) about the respective domain since u might always skip a particular query in our system without any noticeable disadvantages. In such a case, the system keeps generating further queries, one at a time (usually the next-best one according to some quality measure for queries), until u is ready to answer it. As the number of possible queries is usually exponential in the number of minimal diagnoses exploited to compute it, there will be plenty of different “surrogate queries” in most scenarios.\nA Motivating Example. To get a more concrete idea of these assumptions, the reader is invited to think about whether the following first-order KB K is consistent (a similar example is discussed in [26]):\n∀X(res(X)↔ ∀Y (writes(X,Y )→ paper(Y ))) (1.1) ∀X((∃Y writes(X,Y ))→ res(X)) (1.2) ∀X(secr(X)→ gen(X)) (1.3) ∀X(gen(X)→ ¬res(X)) (1.4) secr(pam) (1.5)\nIf we assume that the predicate symbols res, secr and gen stand for ’researcher’, ’secretary’ and ’general employee’, respectively, and the constant pam stands for the person Pam, the KB says the following:\n• Formula 1.1: “Somebody is a researcher if and only if everything they write is a paper.”\n• Formula 1.2: “Everybody who writes something is a researcher.”\n• Formula 1.3: “Each secretary is a general employee.”\n• Formula 1.4: “No general employee is a researcher.”\nCHAPTER 1. INTRODUCTION 6\n• Formula 1.5: “Pam is a secretary.”\nThis KB is indeed inconsistent. The reader might agree that it is not very easy to understand why this is the case. The observations made in [26] concerning a slight modification K′ of the KB K extracted from a real-world KB confirm this assumption. Compared to K, the KB K′ included only Formulas 1.1-1.3 of K, was formulated in DL (cf. Section 2.2), and used the terms A,C, . . . instead of res, paper, . . . . Amongst others, this KB K′ was used as a sample KB in a study where participants had to find out whether a concrete given formula is or is not entailed by a concrete given KB. In the case of the KBK′, the assignment (translated to the terminology in our KBK) was to find out whether ∀X(secr(X)→ res(X)) is an entailment of formulas 1.1-1.3. Although K′ contains only three formulas, the result was that even participants with many years of experience in DL, among them also DL reasoner developers, did not realize that this is in fact the case (the reason for this entailment to hold is that formulas 1.1-1.3 imply that ∀X res(X) holds).\nSince ∀X res(X) is also necessary for the inconsistency of K, this suggests that people might also have severe difficulties in comprehending why K is inconsistent. Once the validity of this entailment is clear, it is relatively straightforward to see that K cannot have any models. For, res(pam) (due to ∀X res(X)) and ¬res(pam) (due to formulas 1.3-1.5) are implications of K.\nConsequently, we might also assume that even experienced knowledge engineers (not to mention pure domain experts) could end up with a contradictory KB like K, which substantiates our first assumption (U1) about u. Probably, the intention of those people who specified formulas 1.1-1.3 was not that ∀X res(X) should be entailed. That is, it might be already a too complex task for many people to (mentally) reason even with such a small KB like this and manually derive implicit knowledge from it.\nHowever, on the other hand, we might well assume u to be able to answer a concrete query about the intended domain they tried to model by K. For instance, one such query could be whether Q1 := {∀X res(X)} is a desired entailment of their model (i.e. “should everybody be a researcher in your intended model of the domain?”). If we assume the (seemingly obvious) case that u negates this query, i.e. asserts that this is an unwanted entailment, then an interactive debugging system (employing a logical reasoner) can derive that at least one of the formulas 1.1 and 1.2 must be faulty. This holds because the only set-minimal explanation in terms of formulas in K for the entailment ∀X res(X) is given by these two formulas. In other words, the set of formulas {1.1, 1.2} is the only minimal conflict set in K given thatQ1 is a negative test case. Hence, the deletion (or suitable modification) of any of these formulas will break this unwanted entailment.\nBefore it is known that Q1 must not be entailed by the correct KB, given consistency is the only requirement to the KB postulated by u, the complete KB K is a minimal conflict set. That is, after the assignment of a (strategically well-chosen) query to the set of positive or, in this case, negative test cases can already shift the focus of potential modifications or deletions to a subset of only two candidate formulas. We would call these two formulas the remaining minimal diagnoses after an answer to the query Q1 has been submitted.\nInitially, there are five minimal diagnoses, each formula in K is one. The meaning of a diagnosis is that its deletion from K leads to the fulfillment of all requirements and (so-far-)specified positive and negative test cases. As the reader should be easily able to see, the deletion of any formula from K yields a consistent KB; e.g. removing formula 1.5 prohibits the entailment ¬res(pam) whereas discarding formula 1.2 prohibits the entailment res(pam). The reader should notice that, as soon as the negative test case Q1 is known, removing (only) formula 1.5 does not yield a correct KB since {1.1, 1.2, 1.3, 1.4} still entails Q1 which must not be entailed.\nA second query to u could be, for example, Q2 : {∃X((∃Y writes(X,Y )) ∧ ¬res(X))} (i.e. “is there somebody who writes something, but is no researcher?”). Again, it is reasonable to suppose that u might know whether or not this should hold in their intended domain model. The (seemingly obvious) answer in this case would be positive, e.g. because u intends to model students who write homework, exams, etc., but are no researchers. This positive answer leads to the new positive test case Q2. Adding\nCHAPTER 1. INTRODUCTION 7\nthis positive test case, like a set of new formulas, to the KB K would result in Knew := K ∪ Q2. The debugging system would then figure out that formula 1.2 is the only minimal conflict set in the KBKnew. The reason for this is that the elimination of formula 1.2 breaks the entailmentQ1 (negative test case) and enables the addition of a new desired entailment Q2 (positive test case) without involving the violation of any requirements (consistency). Therefore, formula 1.2 is the only minimal diagnosis that is still compliant with the new knowledge in terms of Q1 = false and Q2 = true obtained.\nIt is important to notice that the solution KB Knew that is returned to the user as a result of the interactive debugging session includes a new logical formula Q2 that can be seen as a repair of the deleted formula 1.2. Since the knowledge after the debugging session is that ¬1.2 ≡ Q2 must be true, this new knowledge is incorporated into the KBKnew. This indicates that the fault in KB was simply that the ¬ in front of formula 1.2 had been forgotten.\nNotice however that the positive test case Q2 is not added to K as a usual KB formula, but rather as an extension of K that has already been approved by the user. Should the user at some later point in time commit the same fault again (and explicitly specify some formula x equivalent to formula 1.2), then the interactive debugging system, owing to the positive test case Q2, would immediately detect a singleton conflict comprising only formula x. As a consequence, each diagnosis considered during this later debugging session would suggest to delete or modify (at least) x.\nThis scenario should illustrate that, in spite of not being able to specify their domain knowledge in a logically consistent way, the user u might still be able to answer questions about the intended domain, which supports our second assumption made about the user u (the reader might agree that answering Q1 and Q2 is much easier than recognizing the entailment ∀X res(X) of the KB). In other words, the availability of an (efficient) debugging system could help u debug their KB, without needing to analyze which entailments hold or do not hold, why certain entailments hold or do not hold or why exactly the KB does not meet certain imposed requirements or test cases, by simply answering queries whether a certain entailment should or should not hold. These queries are automatically generated by the system in a way that they focus on the problematic parts of the KB, i.e. the minimal conflict sets, and discriminate between the possible solution candidates, i.e. the minimal diagnoses.\nBenefits of the Usage of Conflict Sets. We want to remark that the usage of minimal conflict sets “naturally” forces the system to take into consideration only the smallest relevant (faulty) parts of the problematic KB. This is owed to the property of minimal conflict sets to abstract from what all the reasons for a certain entailment or requirements violation are. Instead, only the “root” (subset-minimal) causes for such violations are examined and no computation time is wasted to extract “purely derived” causes (those which are resolved as a byproduct of fixing all root causes from which it is derived, cf. [23, 37]). For example, assuming the debugging scenario involving our example KB consisting only of formulas 1.1-1.4 which is incoherent and a requirements set including coherency. Then, there are two entailments reflecting the incoherency of this KB, first ∀X ¬secr(X) and second ∀X ¬gen(X) (these entailments hold due to ∀X res(X) which follows from formulas 1.1 and 1.2). Of these two, only the second one is a “root” problem; the first one is a “purely derived” problem. That means, the entailment ∀X ¬secr(X) only holds due to the presence of the entailment ∀X ¬gen(X). So, the cause for ∀X ¬gen(X) is given by the set of formulas {1.1, 1.2, 1.4} whereas the proper superset {1.1, 1.2, 1.3, 1.4} of this set accounts for the entailment ∀X ¬secr(X). The exploitation of minimal conflict sets (the only minimal conflict set for this KB is {1.1, 1.2, 1.4}) ascertains that such “purely derived” causes of requirements or test case violations will not be considered at all.\nThe Ability to Incorporate Background Knowledge. Another feature of the approaches described in this work is their ability to incorporate relevant additional information in terms of a background knowledge KB B (which is regarded to be correct). B is a (consistent) KB which is usually semantically related with the faulty KB, e.g. B represents knowledge about the domain modeled by K that has already been\nCHAPTER 1. INTRODUCTION 8\nsufficiently endorsed by domain experts. For instance, a doctor who wants to express their knowledge of dermatology in terms of a KB might resort to an approved background KB that specifies the human anatomy. Taking this background information into account puts the problematic KB into some context with existing knowledge and can thereby help a great deal to restrict the search space for solutions of the (interactive) KB debugging problem. This has also been found in [81]. This useful strategy of prior search space restriction is also exploited in the field of ontology matching5 where automatic systems are employed to generate an alignment, i.e. a set of correspondences between semantically related entities of two different ontologies (KBs). Here, both ontologies are considered correct and diagnoses are only allowed to include elements of the alignment [48].\nApplying a strategy like that to our example KB given above, supposing that we know that Pam is not a researcher in the world the KB should model, we might specify the background KB B := {¬res(pam)} prior to starting the interactive debugging session. This would immediately reduce the initial set of possible minimal diagnoses from five (i.e. the entire KB) to two (i.e. the first two formulas 1.1 and 1.2). Reason for this is that the entailment ∀X res(X) of formulas 1.1 and 1.2 already conflicts with the background knowledge ¬res(pam).\nOutline of an Interactive KB Debugging System. The schema of an interactive debugging system is pictured by Figure 1.26. As in the case of a non-interactive debugging system (see above), the system receives as input a diagnosis problem instance (DPI). Further on, a range of additional parameters might be provided to the system. These serve as a means to fine-tune the system’s behavior in various aspects. Hence, we call these inputs tuning parameters. These are (roughly) explained next.\nFirst, some parameters might be specified that take influence on the number of leading diagnoses used for query generation and the necessary computation time invested for leading diagnoses computation. Moreover, some parameter determining the quantity of (pre-)generated queries (of which one is selected to be asked to the user) versus the reaction time (the time it takes the system to compute the next query after the current one has been answered) of the system can be chosen. A further input argument is a query selection measure constituting a notion of query “goodness” that is employed to filter out the “best” query among the set of generated queries. To give the system a criterion specifying when a solution of the interactive KB debugging problem is “good enough”, the user is allowed to define a fault tolerance\n5http://www.ontologymatching.org/ 6Thanks to Kostyantyn Shchekotykhin for making available to me parts of this diagram.\nCHAPTER 1. INTRODUCTION 9\nparameter σ. The lower this parameter is chosen, the better the (possibly “approximate”) solution that is guaranteed to be found. In case of specifying this parameter to zero, the system will (if feasible) return the “exact” solution of the interactive KB debugging problem. Roughly, the exact solution is given in terms of a solution KB obtained by means of a single solution candidate (minimal diagnosis) that is left after a sufficient number of queries have been answered (and added to the test cases). On the contrary, an approximate solution is represented by a solution KB obtained by means of a solution candidate with sufficiently high probability (where “sufficiently high” is determined by σ) at some point where there are still multiple solution candidates available.\nFinally, the user may choose between two different modes (static or dynamic) of determining the leading diagnoses. The static diagnosis computation strategy guarantees a constant “convergence” towards the exact solution by “freezing” the set of solution candidates at the very beginning and exploiting answered queries only for the deletion of minimal diagnoses. A possible disadvantage of this approach is the lack of efficient pruning of the used search tree. On the other hand, the dynamic method of calculating leading diagnoses has a primary focus on the preservation of a search tree of small size, thereby aiming at being able to solve diagnosis problem instances which are not soluble by the static approach due to high time and (more critically) space complexity. To this end, more powerful pruning rules are applied in this case which do not permit the algorithm to consider only a fixed set of solution candidates. Rather, the set of minimal diagnoses and minimal conflict sets are generally variable in this case which means that they are subject to change after assignment of an answered query to the test cases.\nLike in the case of a non-interactive debugger, an interactive debugging system requires a sound and complete logical reasoner for deciding consistency (coherency) and calculating logical entailments of a KB formulated over the language L.\nThe workflow in interactive KB debugging illustrated by Figure 1.2 is the following:\n1. A set of leading diagnoses is computed by the diagnosis engine (by means of the fault information, if available) using the logical reasoner and passes it to the query generation module.\n2. The query generation module computes a pool of queries exploiting the set of leading diagnoses and delivers it to the query selection module.\n3. The query selection module filters out the “best query” (often by means of the fault information, if available) and shows it to the interacting user.\n4. The user submits an answer to the query.\n5. The query along with the given answer is used to formulate a new test case.\n6. This new test case is transferred back to the diagnosis engine and taken into account in prospective iterations. If the stop criterion (as per σ, see above) is not met, another iteration starts at step 1. Otherwise, the solution KB K∗ constructed from the currently most probable minimal diagnosis is output.\nContributions of this Work. The contributions of this work are the following:\n• This work evolves the theory of interactive KB debugging (for monotonic KBs) in a detailed fashion by presupposing a reader to have only some basic knowledge of propositional and first-order logic. To the best of our knowledge, this work provides the most comprehensive and detailed introduction to the field of interactive debugging of (monotonic) KBs. Our previous works on the topic [74, 73, 63, 19, 76] are more application-oriented and thus abstract from some details and omit some of the proofs in favor of comprehensive evaluations of the presented strategies.\nCHAPTER 1. INTRODUCTION 10\n• This is the first work that gives formal and precise problem statements of the problems addressed in interactive KB debugging and introduces methods that can be proven to solve these problems.\n• An in-depth discussion of query computation including computational complexity considerations together with an accentuation of potential ways of improving these methods is given. The investigated methods for query computation have been used in [74, 63, 73, 76] too, but have not been addressed in depth in these works.\n• We discuss various ways of exploiting diverse sources of meta information in the KB debugging process from which diagnosis probabilities can be extracted.\n• We give a formal proof of the soundness of an algorithm QX (based on [36]) for the detection of a minimal conflict set in a KB and we show the correctness (completeness, soundness, optimality) of a hitting set tree algorithm HS (based on [60]) for finding minimal diagnoses in a KB in bestfirst order (i.e. most probable diagnoses first) which uses QX for conflict set computation only on-demand. We are not aware of any other work that comprises such proofs.\n• We establish the theoretical relationship between the widely-used notions of a conflict set and a justification. The former is i.a. used in [44, 60, 74, 63] and the latter i.a. in [25, 26, 27, 23, 24, 29, 82, 37, 47, 67, 52]. As a consequence, empirical results concerning the one might be translated to the other. For instance, since each minimal conflict set is a subset of a justification and there is an efficient (polynomial) method for computing a minimal conflict set given a superset of a minimal conflict set, a result manifesting the efficiency of justification computation for a set of KBs (e.g. [28]) implies the efficiency of conflict set computation for the same set of KBs. Moreover, we argue that minimal conflict sets are the better choice for our system since these put the focus of the debugger only on the smallest faulty subsets of the KB whereas justifications are better suited in scenarios where exact explanations for the presence of certain entailments are sought.\n• Two new algorithms for iterative (leading) diagnosis computation in interactive KB debugging are proposed. One that is guaranteed to reduce the number of remaining solutions after a query is answered and one that features more powerful pruning techniques than our previously published algorithms [74, 63] (an evaluation that compares the overall efficiency of our previous algorithms with the ones proposed in this work must still be conducted and is part of our future research).\nOrganization of this Work. The rest of this work is organized as follows: In Chapter 2, besides introducing the notation used in this work, we describe the requirements imposed on the logic L that might be used with our approaches, namely monotonicity, idempotency as well as extensiveness. It should be noted that the postulation of these properties does not restrict the applications of our approaches very much. For instance, these might be employed to resolve over-constrained constraint satisfaction problems (CSPs) or repair faulty KBs in PL, FOL, DL, datalog or OWL. Since DL provides the logical underpinning of OWL which has recently received increasing attention due to the extensive research in the field of the Semantic Web, we will also give a short introduction to DL. For, to underline the flexibility of the presented theory in this work, we will illustrate how it can be applied to examples involving PL, FOL as well as DL KBs.\nIn Chapter 3, we first give a formal definition of a KB Debugging problem and define a diagnosis problem instance (DPI), the input of a KB debugger, and a solution KB, the output of a KB debugger. Further on, we formally characterize a diagnosis and give the notion of KB validity and what it means for a KB to be faulty. We discuss and prove relationships between these notions and specify properties a DPI must satisfy in order to be an admissible (i.e. soluble) input to a KB debugger.\nWe motivate why it makes sense to focus on set-minimal diagnoses instead of all diagnoses, i.e. to stick to “The Principle of Parsimony” [60, 7]. This results in the definition of the problem of Parsimonious\nCHAPTER 1. INTRODUCTION 11\nKB Debugging. We prove that solving this problem is equivalent to the computation of minimal diagnoses. Eventually we explain the benefits of using some background KB in (parsimonious) KB debugging.\nIn Chapter 4 we describe methods for diagnosis computation. To this end, we first introduce the notion of a (minimal) conflict set, discuss some properties of conflict sets related to the notion of KB validity and give sufficient and necessary criteria for the existence of non-trivial conflict sets w.r.t. a DPI. Subsequently, we derive the relationship between a conflict set and the notion of a justification (a minimal set of formulas necessary for a particular entailment to hold) which is well-known and frequently used, especially in the field of DL [25, 26, 27, 23, 24, 28]. Concretely, we will demonstrate that a minimal conflict set is a subset of a justification for some negative test case or for some inconsistency (entailment false) or incoherency (entailment ∀X1, . . . , Xk ¬p(X1, . . . , Xk) for some predicate symbol p of arity k) of the given KB.\nHaving deduced all relevant characteristics of (minimal) conflict sets, we proceed to give a description of a method (QX, Algorithm 1) due to [36] which was originally presented as a method for finding preferred explanations (conflicts) in over-constrained CSPs, but can also be employed for an efficient computation of a minimal conflict set w.r.t. a DPI in KB debugging. We discuss and exemplify this algorithm in detail, prove its correctness as a routine for minimal conflict set computation and give its complexity.\nHaving at our disposal a proven sound method for generation of a minimal conflict set, we continue with the delineation of a hitting set tree algorithm similar to the one originally presented in [60] which enables the computation of different minimal conflict sets by means of successive calls to QX, each time given an (adequately) modified DPI. In this manner, a hitting set tree can be constructed (breadth-first) which facilitates the computation of minimal diagnoses (minimum cardinality diagnoses first). We prove the correctness (termination, soundness, completeness, minimum-cardinality-first property) of this hitting set tree algorithm coupled with the QX method which serves to solve the problem of parsimonious KB debugging.\nIn order to be able to incorporate fault information into the diagnoses finding process, we deal with the induction of a probability space over diagnoses in Section 4.5. We discuss several ways of constructing a probability space including different sources of fault information. Hereinafter, we detail how diagnosis probabilities can be determined on the basis of some available fault information and how these can be appropriately updated after new observations (in terms of answered queries) have been made. We then outline how fault probabilities can be appropriately incorporated into the hitting set search tree in order to guarantee the discovery of minimal diagnoses in best-first order, i.e. most probable ones first. Finally we prove the correctness (termination, soundness, completeness, best-first property) of this best-first diagnosis finding algorithm for parsimonious KB debugging.\nSection 4.6 describes the non-interactive KB debugging procedure (Algorithm 3) that relies on this best-first diagnosis finding algorithm. Some illustrating examples are provided which at the same time reveal significant shortcomings present in non-interactive KB debugging. This motivates the development of interactive KB debugging algorithms.\nChapter 5 first states how disadvantages of non-interactive KB debugging procedures can be overcome by allowing a user to take part in the debugging process. We define the problem of interactive static KB debugging as well as the problem of interactive dynamic KB debugging which “naturally” arise from the fact that the DPI in interactive KB debugging is always renewed after a new test case has been specified (a new query has been answered). The former problem searches for a solution KB w.r.t. the DPI given as input such that this solution KB satisfies all test cases added during the debugging session and there is no other such solution KB. The latter problem searches for a solution KB w.r.t. the current DPI (i.e. the input DPI including all new test cases added throughout the debugging session so far) such that there is no other solution KB w.r.t. the current DPI.\nNext, in Section 5.1, the central term of a query is specified which constitutes the medium for user interaction. Queries are generated from a set of leading diagnoses which is characterized thereafter. These\nCHAPTER 1. INTRODUCTION 12\nleading diagnoses are uniquely partitioned into three subsets by each query. The tuple including these subsets is called q-partition. Subsequently, the reader is given some explanations how the q-partition can be interpreted, and how it relates to a query. In fact, we will prove that the notion of a q-partition can serve as a criterion for checking whether as set of logical formulas is a query or not. After that, we will learn that a query exists for any set of (at least two) leading diagnoses which grants that the presented algorithms will definitely be able to come up with a query without the need to impose any restrictions on which (minimal) diagnoses are computed by the diagnosis engine in each iteration.\nSection 5.2 shows a method for the generation of (a pool of) set-minimal queries (Algorithm 4) aiming at stressing the interacting user as sparsely as possible, features in-depth discussions of this method’s properties, proves its correctness, provides complexity results and gives some illustrating examples. Further on, drawbacks of this method are pointed out and possible solutions are discussed.\nSubsequently, Section 5.3 deals with the presentation of the central algorithm of this work which implements the interactive KB debugger (Algorithm 5). First, this section includes an overview of the workflow of interactive KB debugging (Section 5.3.1), followed by a more comprehensive detailed specification of the algorithm (Section 5.3.2). Finally, some query selection measures are discussed [63, 74] (Section 5.3.3) and optimization versions of the problems of interactive dynamic and static KB debugging are defined where the goal is to obtain the solution to these problems by asking the user a minimal number of queries. Section 5.3.4 proves the correctness of the interactive KB debugging algorithm and provides a discussion of its complexity.\nChapter 6 goes into detail w.r.t. the two strategies for diagnoses computation introduced in this work that might be plugged into Algorithm 5. Section 6.1 describes the static method which is a sound and complete method for the iterative computation of minimal diagnoses w.r.t. the DPI given as an input to the debugger. In this way, used as a routine for leading diagnosis computation in Algorithm 5, the static method solves the problem of interactive static KB debugging. Section 6.2 details the dynamic method which is a sound and complete method for the iterative computation of minimal diagnoses w.r.t. the current DPI, i.e. the DPI given as an input to the debugger extended by the information given by all so-far-answered queries. Employed as a routine for leading diagnosis computation in Algorithm 5, the dynamic method solves the problem of interactive dynamic KB debugging.\nIn Chapter 7 we talk about related work before the concluding Chapter 8 provides a summary of this work and a discussion of future work topics.\nChapter 2\nPreliminaries"
    }, {
      "heading" : "2.1 Assumptions",
      "text" : "The techniques described in this work are applicable for any logical knowledge representation formalism L for which the entailment relation is\n1. monotonic: is given when adding a new logical formula to a KB KL cannot invalidate any entailments of the KB, i.e. KL |= αL implies that KL ∪ {βL} |= αL,\n2. idempotent: is given when adding implicit knowledge explicitly to a KB KL does not yield new entailments of the KB, i.e. KL |= αL and KL ∪ {αL} |= βL implies KL |= βL and\n3. extensive: is given when each logical formula entails itself, i.e. {αL} |= αL for all αL,\nand for which\n4. reasoning procedures for deciding consistency and calculating logical entailments of a KB are available,\nwhere αL, βL are logical formulas and KL is a set { ax (1) L , . . . , ax (n) L } of logical formulas formulated\nover the language L. KL is to be understood as the conjunction ∧n i=1 ax (i) L . Notice that the elements of a KB are called quite differently in literature. Possible denotations are logical formula (e.g. [45]), wellformed formula (e.g. [10]), (logical) sentence or axiom (e.g. [65]) and axiom (in most of the description logic literature, e.g. [3]). We will mainly stick to the term formula (sometimes axiom) to refer to the elements of a KB. As the logic will be clear from the context in the sequel, we will omit the index L when referring to formulas or KBs over L throughout the rest of this work."
    }, {
      "heading" : "2.2 Considered Logics",
      "text" : "To underline the general character of this work, we will illustrate our approaches using example diagnosis problem instances expressed in different logical languages. In this section we give notational remarks concerning these different logics used, namely propositional logic (PL), first-order logic (FOL) as well as description logic (DL). Whereas we assume the reader to be familiar with FOL and PL (a good introduction to PL and FOL can be found in [10]), we will give a short introduction to DL.\nRemark 2.1 It is important to notice that the usage of DL as well as FOL examples throughout this work should not suggest that the Properties 1 – 4 stated above are satisfied for any DL or FOL language L. In\n13\nCHAPTER 2. PRELIMINARIES 14\nfact, it is well-known by the theorems of Church and Turing (cf. [49]; the original works are [11, 86]) that FOL is not decidable in general, i.e. property 4 above is not met. Also in the case of DL, which subsumes a range of different logical languages featuring different expressivity and thus different computational complexity of reasoning procedures, there are languages which are undecidable. For instance, a DL language allowing the formalism of equality role-value-maps which facilitates the expression of concepts like “persons whose co-workers coincide with their relatives” can be proven undecidable [3, 69].\nProperty 4 is satisfied, for example, for the DL language SROIQ which is the logical underpinning of OWL 2 [21]. However, the complexity (2-NEXPTIME-complete [42]) of logical reasoning is intractable in the worst case for this language which implies the intractability of our methods in the worst case. Nevertheless, other DL languages applied with similar systems as those described in this paper have been showing reasonable performance [76, 63, 74]. Also from the theoretical point of view, there are DL languages that allow for efficient reasoning. One example is the OWL 2 EL profile which enables polynomial time reasoning [2]. For this language, the efficient reasoning service ELK has been presented by [43]. For FOL, datalog is an example of a decidable sublanguage where reasoning is efficient [65]. Further, restricted sublanguages of FOL can often be translated to some DL language wherefore DL positive results concerning the decidability of reasoning as well as complexity results can be adopted for these restricted FOL languages [3, chapter 4] [6].\nMoreover, we want to point out that the practical efficiency of our systems depends strongly on the practical performance (which might be by far better than suggested by the worst case reasoning complexities) of the reasoning services called by our algorithms since the reasoning services are used as a black-box (as mentioned in Chapter 1).\nOntologies and The Semantic Web Ontologies are KBs that formally and explicitly represent common knowledge about a domain in the form of individuals, concepts (set of individuals) and roles (binary relationships between individuals). As, in the last decade, extensive research has been done in the area of The Semantic Web [5] making (automatic) ontology development tools and reasoning services more efficient, ontology engineering for the Semantic Web is on the upswing. The Semantic Web aims at the enrichment of unstructured information on the web by semantic meta data which should facilitate the usage of the web as structured database of knowledge of all kinds where computers are able to “understand” this structured data, establish relationships between different data sources, combine information from different data sources and (most essentially) derive new (implicit) knowledge from the structured data. At this, ontologies are the key to a common vocabulary used for the semantic meta data. Ontologies are employed to precisely define the meaning of different terms, state relationships between different terms and to introduce new terms by means of already specified ones.\nThe constantly increasing number of people creating ontologies of increasing size (examples were given in Chapter 1) results in more and more (faulty) ontologies which constitute useful application scenarios and test cases for our approaches. For that reason, we also want to use ontology engineering for The Semantic Web as a concrete use case for the presented work. The standard knowledge representation formalism for ontologies is OWL 2 [50, 21] which relies on DL. A short introduction to DL is given next.\nDescription Logic\nDescription Logic (DL) [3] is a family of knowledge representation languages with a formal logic-based semantics that are designed to represent knowledge about a domain in form of concept descriptions. The syntax of a description language L is defined by its signature and a set of constructors. The signature of L corresponds to the union of possibly disjoint sets NC , NR and NI , where NC contains all concept names (unary predicates), NR comprises all role names (binary predicates) and NI is the set of all individuals\nCHAPTER 2. PRELIMINARIES 15\n(constants) in L. Each concept and role description can be either atomic or complex. The latter ones are composed using constructors defined in the particular language L. A typical set of DL constructors for complex concepts includes conjunction A u B, disjunction A t B, negation ¬A, existential ∃r.A and value ∀r.A restrictions, where A,B are concept descriptions and r ∈ NR.\nAxioms are statements of knowledge that must be true in a domain. An ontology K is defined as a tuple (T ,A), where T (TBox) is a set of terminological axioms andA (ABox) a set of assertional axioms. Each TBox axiom is expressed by a general concept inclusion A v B, a form of logical implication, or by a definition A ≡ B, a kind of logical equivalence, where A and B are concept descriptions or role descriptions. ABox axioms are used to assert properties of individuals in terms of the vocabulary defined in the TBox, e.g. concept A(x) or role r(x, y) assertions, where A is a concept description, r a role description, and x, y ∈ NI .\nThe semantics of a description language is given in terms of interpretations I = (∆I , ·I) consisting of a non-empty domain ∆I and a function ·I that assigns to every atomic concept A ∈ NC a set AI ⊆ ∆I , to every atomic role r ∈ NR a set rI ⊆ ∆I × ∆I and to every individual x ∈ NI some value xI ∈ ∆I . The interpretation function is extended to complex concept descriptions by the following inductive definitions:\n>I = ∆I\n⊥I = ∅ (A uB)I = AI ∩BI\n(A tB)I = AI ∪BI\n(¬A)I = ∆I \\AI (∃r.A)I = { x ∈ ∆I | ∃y. (x, y) ∈ rI ∧ y ∈ AI } (∀r.A)I = { x ∈ ∆I | ∀y. (x, y) ∈ rI → y ∈ AI\n} where > and ⊥ are predefined concepts; the former is the universal concept and the latter the bottom concept.\nThe semantics of axioms is defined as follows for (1) TBox and (2) ABox axioms: (1) Interpretation I satisfies A v B iff AI ⊆ BI and it satisfies A ≡ B iff AI = BI . (2) A(x) is satisfied by I iff xI ∈ AI and r(x, y) is satisfied iff (xI , yI) ∈ rI . An interpretation I is a model of K = (T ,A) iff it satisfies all TBox axioms in T and all ABox axioms in A. An ontology K is consistent iff it has a model. A concept A (role r) is satisfiable w.r.t K iff there is a model I of K with AI 6= ∅ (rI 6= ∅). An ontology K is coherent iff all concepts and roles occurring in K are satisfiable. An axiom α is entailed by K iff α is true in all models I of K. For a set of axioms X we write K |= X as a shorthand for K |= α for all α ∈ X .\nUsually description logic systems provide sound and complete reasoning services to their users. Besides verification of coherency and consistency ofK and satisfiability checking of concepts, reasoner tasks include classification and realization. Classification determines, for each concept name A occurring in K, most specific (general) concepts that subsume (are subsumed by) A. A concept A subsumes (is subsumed by) a concept B iff K |= B v A (K |= A v B). Classification is employed to build a taxonomy of concepts in K. Realization, given an individual name x occurring in K and a given set of concepts in K (usually all concepts in K), computes the most specific concepts A1, . . . , An from the set such that K |= Ai(x) for all i = 1, . . . , n. The most specific concepts are those that are minimal w.r.t. the subsumption ordering v.\nExample 2.1 The example KB given in the Introduction (Chapter 1) can be equivalently represented in\nCHAPTER 2. PRELIMINARIES 16\nDL (cf. Remark 2.1) as follows:\nRes ≡ ∀writes.Paper (2.1) ∃writes.> v Res (2.2)\nSecr v Gen (2.3) Gen v ¬Res (2.4) Secr(pam) (2.5)\nwhere Res is the concept symbol with equivalent meaning as the predicate symbol res, the role symbol writes corresponds to the equally named binary predicate, Paper to paper, and so on. Notice that axiom 2.2 states that the domain of writes is Res."
    }, {
      "heading" : "2.3 Notational Remarks",
      "text" : "General Notational Conventions. Throughout this work, the nomenclature given by Table 2.1 is used (many of the designators in the table will be explained later in this work). We will mainly refer to an ontology by the term KB.\nIn order to make a clear distinction between scalars and functions, we denote all scalars g by g and all functions g by g(). If an ordered list occurs in a set operation, then this list is interpreted as a (nonordered) set. For example, let L := [1, 3, 4, 2] be an ordered list; then L∩{1, 2, 3} yields the set {1, 2, 3}.\nNotational Convention for PL (cf. [65]). We use uppercase letters A,B, . . . to denote atoms and the standard logical connectives to build PL formulas from atoms. The operator precedence we use is ¬, ∧, ∨,→,↔, from highest to lowest. Given a PL KBK and a PL formula ax , we call K̃ and ãx the signature of K and the signature of ax , respectively. The former comprises all atoms occurring in K and the latter all atoms occurring in ax .\nNotational Convention for FOL (cf. [9]). Variables are denoted by uppercase letters; constants and predicate symbols are denoted by strings beginning with a lowercase letter.1 Recalling the example KB given in Chapter 1, X,Y are variables, pam is a constant and res, writes, paper, secr and gen are predicate symbols. FOL formulas are built from the standard logical connectives described for PL above. The operator precedence we use for FOL formulas is the same as stated above.2 The precedence of quantifiers ∀, ∃ is such that a quantifier outside of any parenthesized expression holds over everything to the right of it; if occurring in a parenthesized expression, a quantifier holds over everything to the right of it within this expression. For example, ∀Xprof (X)→ ∃Y secr(Y ) is equivalent to (∀X(prof (X)→ (∃Y (secr(Y ))))) (i.e. “for each professor there is at least one secretary”) and not to (∀Xprof (X))→ ∃Y secr(Y ) (i.e. “if everybody is a professor, then there is at least one secretary”).\nGiven a FOL KB K and a FOL formula ax , we call K̃ and ãx the signature of K and the signature of ax , respectively. The former comprises all predicate, function and constant symbols occurring in K and the latter all predicate, function and constant symbols occurring in ax . The signature of the example KB given in Chapter 1 is {res, writes, paper, secr, gen, pam} and the signature of formula 1.2 of this KB is {writes, res}.\nRemark 2.2 By analogy with the definition of coherency in DL (see Section 2.2), we call a FOL KB K incoherent iff K |= ∀X1, . . . , Xk ¬p(X1, . . . , Xk) for some k-place predicate symbol p in the signature of K where k ≥ 1.\n1We do not use any function symbols throughout this work. 2We do not use equality = in FOL formulas throughout this work.\nCHAPTER 2. PRELIMINARIES 17\nRemark 2.3 We want to point out that whenever we will speak of entailment computation we address the invocation of a sound reasoning service that is guaranteed to terminate after finite execution time and returns a finite number of entailments for any KB given as input (cf. Remark 2.1). Similarly, when we say that all entailments of a KB are computed, we always refer to a finite set of entailments of certain types output by such a reasoning service. Examples of such entailment types regarding DL are the (a) classification and (b) realization entailments, by which we mean (a) all the subsumption relationships between concept names appearing in the KB, i.e. entailments of the form C1 v C2 for concept names C1, C2 ∈ K̃ and (b) all the concept names instantiated by a given individual for all individuals appearing in the KB, i.e. entailments of the form C(a) for concepts names C ∈ K̃ and individual names a ∈ K̃.\nCHAPTER 2. PRELIMINARIES 18\nChapter 3\nKnowledge Base Debugging\nKB debugging can be seen as a test-driven procedure comparable to test-driven software development and debugging, where test cases are specified to restrict the possible faults until the user detects the actual fault manually or there is only one (highly probable) fault remaining which is in line with the specified test cases. In this chapter, we want to study the theory of (non-interactive) KB debugging, present and discuss mechanisms that can be employed for the debugging of KBs and reveal drawbacks of such systems. In (non-interactive) KB debugging we assume test cases fixed during the debugging procedure. That is, a user might specify a set of test cases offline, run a debugging system and investigate the output solution(s). In case no satisfactory solution has been returned, some additional test cases might be defined offline before the debugger might be invoked again.\nThe inputs to a KB debugging problem can be characterized as follows: Given is a KB K and a KB B (background knowledge), both formulated over some logic L complying with the conditions 1 – 4 given in Chapter 2. All formulas in B are considered to be correct and all formulas in K are considered potentially faulty. K ∪ B does not meet postulated requirements R where {consistency} ⊆ R ⊆ {coherency, consistency} or does not feature desired semantic properties, called test cases.1 Positive test cases (aggregated in the set P ) correspond to desired entailments and negative test cases (N ) represent undesired entailments of the correct (repaired) KB (along with the background KB B). Each test case p ∈ P and n ∈ N is a set of logical formulas over L. The meaning of a positive test case p ∈ P is that the correct KB integrated with B must entail each formula (or the conjunction of formulas) in p, whereas a negative test case n ∈ N signalizes that some formula (or the conjunction of formulas) in n must not be entailed by the correct KB integrated with B.\nRemark 3.1 In the sequel, we will write K |= X for some set of formulas X to denote that K |= ax for all ax ∈ X and K 6|= X to state that K 6|= ax for some ax ∈ X .\nThe described inputs to the KB debugging problem are captured by the notion of a diagnosis problem instance:\nDefinition 3.1 (Diagnosis Problem Instance). Let\n• K be a KB over L,\n• P ,N sets including sets of formulas over L,\n• {consistency} ⊆ R ⊆ {coherency, consistency}, 1We assume consistency a minimal requirement to a solution KB provided by a debugging system, as inconsistency makes a KB\ncompletely useless from the semantic point of view.\n19\nCHAPTER 3. KNOWLEDGE BASE DEBUGGING 20\n• B be a KB over L such that K ∩ B = ∅ and B satisfies all requirements r ∈ R,\n• the cardinality of all sets K, B, P , N be finite.\nThen we call the tuple 〈K,B,P ,N 〉R a diagnosis problem instance (DPI) over L.2\nNote that, for now, we do not make any assumptions about the contents of the sets K, B, P and N that go beyond Definition 3.1. So, it might be well the case, for example, to specify a DPI according to Definition 3.1 for which there are no solutions or for which only trivial solutions exist. Later on, we will discuss properties a DPI must fulfill to guarantee existence of solutions for it.\nWe define a solution KB for a DPI as follows:\nDefinition 3.2 (Solution KB). Let 〈K,B,P ,N 〉R be a DPI. Then a KB K∗ is called solution KB w.r.t. 〈K,B,P ,N 〉R, written as K∗ ∈ Sol〈K,B,P,N 〉R , iff all the following conditions hold:\n∀ r ∈ R : K∗ ∪ B fulfills r (3.1) ∀ p ∈ P : K∗ ∪ B |= p (3.2) ∀n ∈ N : K∗ ∪ B 6|= n. (3.3)\nA solution KB K∗ w.r.t. a DPI is called maximal, written as K∗ ∈ Solmax〈K,B,P,N 〉R , iff there is no solution KB K′ such that K′ ∩ K ⊃ K∗ ∩ K.\nNow, the problem of KB debugging can be formalized:\nProblem Definition 3.1 (KB Debugging). Given a DPI 〈K,B,P ,N 〉R, find a solution KB w.r.t. 〈K,B,P ,N 〉R.\nNote that basically any KBK∗ that meets conditions (3.1) - (3.3) is a solution KB in the sense of Definition 3.2. Hence, K∗ does not even need to have a non-empty intersection with K. Only the postulation of maximality of a solution KB (as detailed later in Section 3.1) establishes a relationship to the given KB K.\nRemark 3.2 Let K′ := K ∪ B ∪ UP . Then, conditions (3.1) - (3.3) can be reduced to conditions (3.2) and (3.3) if\n• N := N ∪ {{false}} given R = {consistency} or\n• N := N ∪ {{∀X1, . . . , Xk p(X1, . . . , Xk)→ false} | p is k-place predicate symbol in K̃′, k ≥ 1} ∪ {{false}} in case R = {consistency, coherency}.\nThis holds because a KBK is inconsistent iffK |= {false} andK is incoherent iff some predicate symbol in K′ must be false for any instantiation. Notice that the latter must hold for all predicate symbols in K′ and not only in K (see Example 3.1). For PL and DL, the definitions of N are analogous (cf. Chapter 2), but for PL coherency is not defined wherefore only the first bullet is relevant for PL. In what follows we will stick to the more explicit characterization of a solution KB given by Definition 3.2.\n2 In the following we will often call a DPI over L simply a DPI for brevity and since the concrete logic will not be relevant in our theoretical analyses as long as it is compliant with the conditions 1 – 4 given in Chapter 2. Nevertheless we will mean exactly the logic over which a particular DPI is defined when we use the designator L.\nCHAPTER 3. KNOWLEDGE BASE DEBUGGING 21\nExample 3.1 Let a DL DPI be defined as\nK := {B v C} B := {A v B,C v ¬A} P := ∅ N := ∅ R := {coherency, consistency}\nThen, K̃ = {B,C}, but there is some concept A /∈ K̃, but A ∈ K̃′, which is unsatisfiable w.r.t. K ∪ B. Since we want a solution KB integrated with B to meet the conditions (3.1) - (3.3),K is not a solution KB w.r.t. 〈K,B,P ,N 〉R despite the fact that it is perfectly consistent and coherent as an isolated KB.\nWhereas the definition of a solution KB refers to the desired properties of the output of a KB debugging system, the following definition can be seen as a characterization of KBs provided as an input to a KB debugger. If a KB is valid w.r.t. the background knowledge, the requirements and the test cases, then finding a solution KB w.r.t. the DPI is trivial. Otherwise, obtaining a solution KB from it involves modification of the input KB and subsequent addition of suitable formulas. Usually, the KB K part of the DPI given as an input to a debugger is assumed to be invalid w.r.t. this DPI.\nDefinition 3.3 (Valid KB). Let 〈K,B,P ,N 〉R be a DPI. Then, we say that a KB K′ is valid w.r.t. 〈·,B,P ,N 〉R iff K′ ∪ B ∪ UP does not violate any r ∈ R and does not entail any n ∈ N . A KB is said to be invalid (or faulty) w.r.t. 〈·,B,P ,N 〉R iff it is not valid w.r.t. 〈·,B,P ,N 〉R.3\nIntuitively, if a KB K is faulty w.r.t. 〈·,B,P ,N 〉R, then there is at least one incorrect formula in K that needs to be corrected or deleted; if a KB K is valid w.r.t. 〈·,B,P ,N 〉R, a solution KB can be directly obtained by simply extending K by the set UP of all sentences comprised in positive test cases. Note, however, that K being valid w.r.t. 〈·,B,P ,N 〉R does not necessarily mean that K ∪ B entails any p ∈ P .\nProposition 3.1. Let 〈K,B,P ,N 〉R be a DPI. Then, K′ ∪ UP ∈ Sol〈K,B,P,N 〉R iff K′ is valid w.r.t. 〈·,B,P ,N 〉R.\nProof. “⇒”: If K′ ∪ UP is a solution KB, then K′ ∪ UP ∪ B meets all r ∈ R as per condition (3.1) and does not entail any n ∈ N as per condition (3.3). Hence, K′ is valid w.r.t. 〈·,B,P ,N 〉R.\n“⇐”: IfK′ is valid w.r.t. 〈·,B,P ,N 〉R, then (K′∪UP )∪B meets all r ∈ R, i.e. meets condition (3.1). Moreover, (K′ ∪UP )∪B 6|= n for all n ∈ N , i.e. (K′ ∪UP )∪B meets condition (3.3). By extensiveness of the used language L, (K′∪UP )∪B |= p for all p ∈ P , i.e. condition (3.2) is fulfilled by (K′∪UP )∪B. Thus, K′ ∪ UP is a solution KB.\nDefinition 3.4 (Extension). Let 〈K,B,P ,N 〉R be a DPI over L and K′ ⊆ K. A set of formulas E over L is called an extension w.r.t. K′ and 〈K,B,P ,N 〉R, written as E ∈ EX(K′)〈K,B,P,N 〉R , iff (K \\ K′) ∪ E is a solution KB w.r.t. 〈K,B,P ,N 〉R.\nDefinition 3.5 (Diagnosis). Let 〈K,B,P ,N 〉R be a DPI. A set of formulas D ⊆ K is called a diagnosis w.r.t. 〈K,B,P ,N 〉R, written as D ∈ aD〈K,B,P,N 〉R , iff there exists some E ∈ EX(D)〈K,B,P,N 〉R , i.e. (K \\ D) ∪ E is a solution KB w.r.t. 〈K,B,P ,N 〉R.\nA diagnosisD w.r.t. 〈K,B,P ,N 〉R is minimal, written asD ∈mD〈K,B,P,N 〉R , iff there is noD′ ⊂ D such that D′ is a diagnosis w.r.t. 〈K,B,P ,N 〉R. A diagnosis D w.r.t. 〈K,B,P ,N 〉R is a minimum cardinality diagnosis w.r.t. 〈K,B,P ,N 〉R iff there is no diagnosisD′ w.r.t. 〈K,B,P ,N 〉R such that |D′| < |D|.\n3 It would be more precise to call a KB valid w.r.t. the elements B, P , N , R of a DPI. Though, for brevity, we stick to the presented notation where the dot · in 〈·,B,P ,N 〉R signalizes the irrelevance of the first element K of a DPI 〈K,B,P ,N 〉R for determining validity of a KB K′ w.r.t. this DPI.\nCHAPTER 3. KNOWLEDGE BASE DEBUGGING 22\nProposition 3.2. Let 〈K,B,P ,N 〉R be a DPI. Then, D ∈ aD〈K,B,P,N 〉R iff K \\ D is valid w.r.t. 〈·,B,P ,N 〉R.\nProof. “⇒”: If D is a diagnosis w.r.t. 〈K,B,P , N 〉R, there is some extension E w.r.t. D and 〈K,B,P , N 〉R which implies that (K \\ D) ∪ E is a solution KB w.r.t. 〈K,B,P ,N 〉R. Now, assume that K \\ D is not valid w.r.t. 〈·,B,P ,N 〉R. By Proposition 3.1, this means that (K \\ D) ∪ UP is not a solution KB. Hence, (K \\ D) ∪ UP ∪ B violates some r ∈ R or entails some n ∈ N . As (K \\ D) ∪ E is a solution KB, we have that (K \\ D) ∪ E ∪ B |= p for all p ∈ P . So, by idempotency of L, (K \\ D) ∪ E ∪ B ≡ (K \\ D) ∪ E ∪ B ∪ UP ⊇ (K \\ D) ∪ UP ∪ B which violates some r ∈ R or entails some n ∈ N . By monotonicity of L, (K\\D)∪E ∪B also violates some r ∈ R or entails some n ∈ N whereby (K\\D)∪E is not a solution KB which is a contradiction.\n“⇐”: IfK\\D is valid w.r.t. 〈·,B,P ,N 〉R, then (K\\D)∪B∪UP does not violate any r ∈ R and does not entail any n ∈ N . Since (K\\D)∪B∪UP also entails each positive test case p ∈ P by extensiveness of L, we can conclude that (K \\ D) ∪ UP is a solution KB. By Definition 3.4, UP ∈ EX(D)〈K,B,P,N 〉R and thus D is a diagnosis w.r.t. 〈K,B,P ,N 〉R.\nIn other words, D is a diagnosis w.r.t. 〈K,B,P ,N 〉R iff (K \\ D) ∪ B meets all requirements, i.e. consistency and/or coherency, as per condition (3.1), does not entail any negative test cases as per condition (3.3), and the positive test cases p ∈ P can be added to (K \\ D) ∪ B without violating any of the conditions (3.1) or (3.3).\nFrom a given DPI 〈K,B,P ,N 〉R, a solution KB K∗ can be obtained by a deletion and an expansion step. The deletion step involves the elimination of a diagnosis D ⊆ K from K. Note that, due to monotonicity of L, only deletion (and not expansion) of the KB can effectuate a repair of inconsistencies, incoherencies and unwanted entailments. Note, if K is already valid w.r.t. 〈·,B,P ,N 〉R, then D can be set to ∅ and the deletion step can be omitted. The expansion step aims at the fulfillment of positive test cases P , i.e. condition (3.2), which is not necessarily the case after the deletion step. In fact, some new logical sentences E ∈ EX(D)〈K,B,P,N 〉R may need to be added to (K \\D) ∪ B to grant entailment of all positive test cases.\nCorollary 3.1. Let D be a diagnosis w.r.t. 〈K,B,P ,N 〉R. Then there is a set of logical sentences E ∈ EX(D)〈K,B,P,N 〉R over L such that:\n∀ r ∈ R : (K \\ D) ∪ E ∪ B fulfills r ∀ p ∈ P : (K \\ D) ∪ E ∪ B |= p ∀n ∈ N : (K \\ D) ∪ E ∪ B 6|= n.\nProof. The proposition of the corollary is a direct consequence of Definition 3.2 and Definition 3.5.\nFrom the point of view of a solution KB K∗ w.r.t. 〈K,B,P ,N 〉R, K \\ K∗ is a diagnosis w.r.t. 〈K,B,P ,N 〉R and K∗ \\ K is one possible extension w.r.t. D and 〈K,B,P ,N 〉R.\nProposition 3.3. For each solution KB K∗ w.r.t. 〈K,B,P ,N 〉R there is a diagnosis w.r.t. 〈K,B,P ,N 〉R and an extension E w.r.t. D and 〈K,B,P ,N 〉R such that K∗ = (K \\ D) ∪ E and E ∩ D = ∅.\nProof. LetK∗ be a solution KB w.r.t. 〈K,B,P ,N 〉R. ThenK∗ can be written asK∗ = (K∩K∗)∪ (K∗ \\ K) = (K \\ (K \\ K∗)) ∪ (K∗ \\ K). Let K \\ K∗ =: D and K∗ \\ K =: E , then E ∩ D = ∅. Further on, D ⊆ K holds and E is a set of logical sentences such thatK∗ = (K\\D)∪E ∈ Sol〈K,B,P,N 〉R . Therefore, D ∈ aD〈K,B,P,N 〉R and E ∈ EX(D)〈K,B,P,N 〉R .\nCorollary 3.2. The (non-)existence of a diagnosis w.r.t. 〈K,B,P ,N 〉R is equivalent to the (non-)existence of a solution KB w.r.t. 〈K,B,P ,N 〉R.\nCHAPTER 3. KNOWLEDGE BASE DEBUGGING 23\nProof. Proposition 3.3 shows that there is a diagnosis for each solution KB. By Definition 3.5, there is also a solution KB for each diagnosis.\nThe next Proposition gives sufficient and necessary criteria for the existence of a solution, i.e. a diagnosis or a solution KB, respectively, for a given DPI.\nProposition 3.4. Let 〈K,B,P ,N 〉R be a DPI. Then, a diagnosis D w.r.t. 〈K,B,P ,N 〉R exists iff\n• ∀ r ∈ R : B ∪ UP fulfills r and\n• ∀n ∈ N : B ∪ UP 6|= n .\nProof. “⇐”: Let us define D := K. Then X := (K \\ D) ∪ B ∪ UP = B ∪ UP . Consequently, X satisfies each r ∈ R as per condition (3.1), X 6|= n for each n ∈ N as per condition (3.3), and finally X |= p for each p ∈ P by extensiveness of L and thus meets condition (3.2). So, X is a solution KB w.r.t. 〈K,B,P ,N 〉R wherefore D must be a diagnosis.\n“⇒”: Let D ⊆ K be some diagnosis w.r.t. 〈K,B,P ,N 〉R. Then, by definition of a diagnosis, there is some solution KB K∗ w.r.t. 〈K,B,P ,N 〉R. Then K∗ ∪ B |= p for all p ∈ P by condition (3.2), which implies that K∗ ∪ B ∪ UP does not feature any new entailments compared to K∗ ∪ B by idempotency of L. So, K∗ ∪ B ≡ K∗ ∪ B ∪ UP holds. Now, for arbitrary n ∈ N , since K∗ ∪ B 6|= n we have that K∗ ∪ B ∪ UP 6|= n , and, by monotonicity of L, that B ∪ UP 6|= n . Analogously, for any r ∈ R, because K∗ ∪ B satisfies r, it must be true that K∗ ∪ B ∪ UP satisfies r and, by monotonicity of L, that B ∪ UP satisfies r.\nDefinition 3.6 (Admissible DPI). We call a DPI 〈K,B,P ,N 〉R admissible iff there is at least one diagnosis D ∈ aD〈K,B,P,N 〉R .\nA non-admissible DPI may arise in a situation where a user specifies test cases manually. For this procedure a similar error-proneness as for the user’s formulation of KB formulas can be assumed. And there are lots of pitfalls to escape, as Proposition 3.4 shows. In particular, the specified test cases in P and N must be “compatible” with each other, i.e. positive test cases must not contradict negative ones. For example, adding p1 := {A v C,E ≡ B} and p2 := {C v E} to P and n1 := {A v B} to N leads to a contradiction between P and N and consequently to the non-admissibility of a DPI comprising P and N . Furthermore, the background KB B which is considered as correct, must indeed be correct, at least in terms of R; and negative test cases must be specified in a way not to postulate non-entailment of knowledge specified in B. A counterexample is B := {∃r.> v A, r(x, y), A v C} and N := {{C(x)}}. And third, the union of positive test cases together with B must be in compliance with R, particularly the formulas in P must not be inconsistent or incoherent. Because the union of positive test cases UP can be viewed as an own KB since all logical sentences occurring in some p ∈ P must be true in the solution KB. So, in a setting where test cases are specified manually, faults occur as likely in UP as they do in K.\nThe debugging system presented in this work, however, guarantees by automatic test case generation that admissibility of a DPI is satisfied at any time, provided that an admissible DPI is given as an initial input to the debugging system.\nRemark 3.3 In case of a present DPI 〈K,B,P ,N 〉R which is non-admissible, the DPI must be properly modified before it can be used with our debugging system. More concretely, the sets B, P as well as N must be prepared in a way that the two conditions in Proposition 3.4 are satisfied. When supposing that B is an already approved and correct KB (which is a reasonable assumption for a KB used as background knowledge during a debugging session), then there are (at least) the following ways to obtain an admissible DPI from a given non-admissible DPI without modifying B.\n(a) One straightforward way to achieve that is the deletion of all manually specified test cases from P and N . After that, both sets are either the empty set (if no automatic test cases, e.g. from former\nCHAPTER 3. KNOWLEDGE BASE DEBUGGING 24\ndebugging sessions were included in these sets) or comprise only automatically generated test cases. The former case yields an admissible DPI independently of K by the property of B to not violate any requirements in R (see Definition 3.1). That the latter case implies the admissibility of the DPI is a property of the debugging system described in this work (as we will show later by Corollary 5.3).\n(b) Another way to resolve the non-admissibility of a DPI 〈K,B,P ,N 〉R is to first check whether 〈UP ,B, ∅,N 〉R is admissible (verification of Proposition 3.4 by means of a reasoning service). If so, it is clear that B does not conflict with N . Then, a debugger (like the one presented in this work) can be exploited to find an as small as possible subset of the set of all formulas occurring in the positive test cases, the removal of which causes the DPI to become admissible. This would be accomplished by the computation of a minimal diagnosis DP w.r.t. 〈UP ,B, ∅,N 〉R and the usage of the modified admissible DPI 〈K,B, {UP \\ DP} ,N 〉R instead of the original one. In this case, only a set-minimal set DP of formulas that were desired entailments of the user are lost. This modification is possible in polynomial time apart from the reasoning costs, i.e. by means of a polynomial number of calls to a reasoner (cf. Chapter 1).\n(c) Otherwise, i.e. if B already conflicts with the negative test cases N , then an algorithm similar to Algorithm 1 (that will be presented in Section 4.3.1) can be employed to determine a maximal subset N ′ of N w.r.t. set inclusion such that B will not be in conflict with N ′. This approach also requires only a polynomial number of calls to a reasoner (cf. Proposition 4.8). If the resulting modified DPI 〈K,B,P ,N ′〉R is not yet admissible, i.e. after adding the positive test cases UP to B there are again conflicts with N ′, method (b) must be executed in order to finally obtain an admissible DPI.\nThat is, given a non-admissible DPI, there is a transformation achievable in polynomial time which enables the establishment of admissibility involving a set-minimal number of modifications to the given test cases. Thence, in the rest of this work, we will assume that a DPI given as an input to our algorithms is admissible.\nIn general, there are multiple (minimal) diagnoses for a DPI, i.e. |aD〈K,B,P,N 〉R | ≥ |mD〈K,B,P,N 〉R | > 1, and there are multiple, in fact infinitely many, extensions E ∈ EX(D)〈K,B,P,N 〉R for a fixed diagnosis D ∈ aD〈K,B,P,N 〉R . The task addressed in this work is finding an optimal diagnosis for a given DPI, whereas the identification of an optimal extension w.r.t. that diagnosis and the DPI is not the aim. What we understand by “optimality” of a diagnosis will be addressed in more detail in Chapter 5. Instead, we will content ourselves with finding any extension that enables to formulate a solution KB given a DPI and a diagnosis for that DPI. In fact, the problem of finding a solution KB for a DPI can be reduced to finding a diagnosis for that DPI since a suitable extension can be easily formulated for any diagnosis, as the next proposition shows:\nProposition 3.5. Let 〈K,B,P ,N 〉R be a DPI and D ∈ aD〈K,B,P,N 〉R . Then UP is an extension w.r.t. D and 〈K,B,P ,N 〉R.\nProof. Let us assume that there is some D ∈ aD〈K,B,P,N 〉R and UP is not an extension w.r.t. D and 〈K,B,P ,N 〉R. By the definition of a diagnosis, this is equivalent to stating that (K \\ D) ∪ UP is not a solution KB which in turn means that at least one condition (3.1), (3.2) or (3.3) of Definition 3.2 is violated by (K \\ D) ∪ UP . However, the fact that D is a diagnosis implies the existence of some extension E ∈ EX(D)〈K,B,P,N 〉R that can be added to (K \\ D) to obtain a solution KB. This means that conditions (3.1) and (3.3) must be already valid for (K \\ D), since, by monotonicity of L, addition of logical sentences E can neither solve inconsistencies or incoherencies necessary for fulfillment of condition (3.1) nor invalidate non-desired entailments as per condition (3.3). As a consequence, condition (3.2) must be violated by (K \\ D) ∪ UP . By extensiveness of L it holds that (K \\ D) ∪ UP |= p for all p ∈ P whereby we obtain that condition (3.2) is fulfilled which yields a contradiction.\nProposition 3.5 claims that the expansion operation, i.e. identifying a concrete extension for a diagnosis, is trivial, at least for our purposes, namely formulating an extension reflecting only evident\nCHAPTER 3. KNOWLEDGE BASE DEBUGGING 25\nentailments given by the set of positive test cases P . Consequently, in order to find a solution KB for some DPI, it is sufficient to concentrate on the deletion step, i.e. on the search for diagnoses.\nNote that using UP as a canonical extension when computing diagnoses does not affect the set of identified diagnoses. In other words, exchanging E ∈ EX(D)〈K,B,P,N 〉R for UP in Definition 3.5 yields an equivalent definition. The following corollary proves this statement and summarizes the relationship between the notions diagnosis, solution KB and valid KB.\nCorollary 3.3. The following statements are equivalent:\n1. D is a diagnosis w.r.t. 〈K,B,P ,N 〉R\n2. (K \\ D) ∪ UP is a solution KB w.r.t. 〈K,B,P ,N 〉R\n3. (K \\ D) is valid w.r.t. 〈·,B,P ,N 〉R.\nProof. That (1) is equivalent to (2) follows from Definition 3.5 which states that D is a diagnosis w.r.t. 〈K,B,P ,N 〉R iff there is some set of sentences E ∈ EX(D)〈K,B,P,N 〉R such that (K \\ D) ∪ E is a solution KB, and from Proposition 3.5 which proves that UP is an extension w.r.t. any diagnosis D and 〈K,B,P ,N 〉R.\nThat (1) is equivalent to (3) follows directly from Proposition 3.2 and the equivalence of (2) and (3) has been shown in Proposition 3.1."
    }, {
      "heading" : "3.1 Parsimonious Knowledge Base Debugging",
      "text" : "Why are minimal diagnoses interesting? First, the set of minimal diagnoses w.r.t. a DPI captures all the information that explains the unwanted properties, i.e. violation of requirements or test cases, of the DPI. In other words, the minimal diagnoses represent all subset-minimal possibilities to modify a KB in a way it becomes a valid KB w.r.t. the given DPI (e.g. by simply deleting a minimal diagnosis from the KB in the trivial case). By monotonicity of the logic L, each superset of a minimal diagnosis w.r.t. a DPI is a diagnosis w.r.t. this DPI. That is, aD〈K,B,P,N 〉R can be easily reconstructed given mD〈K,B,P,N 〉R . There is however no evidence (in terms of specified requirements and test cases) in a DPI that would justify the selection of a non-minimal diagnosis. That is, if K is a KB and D ⊆ K a minimal diagnosis w.r.t. a DPI including K, K \\ D does not violate any of the postulated properties that must hold for a KB to be valid w.r.t. this DPI. For that reason, there is no evident need to delete or modify any other sentences in K except for the ones in some minimal diagnosis D.\nSecond, usually a setting can be assumed where the author of a KB specifies formulas to the best of their knowledge. Hence, the assumption that a formula is rather correct than faulty, or in other words, that the KB author wants to keep as many formulated sentences as possible in a solution KB obtained from a debugger, is practical.\nThis also motivates the importance of a certain subset of minimal diagnoses, namely minimum cardinality diagnoses, which are the solutions of choice in scenarios where no probabilistic information about the KB authors’ faults is available, e.g. in terms of statistics retrieved from log data of the used IDE (see Section 4.5 for details). In an application where such information is given, minimum cardinality diagnoses might not always be the appropriate choice (for details see Chapter 5). In this case the aim is to find a minimal diagnosis with a maximal probability of including only sentences that are actually faulty (which might not necessarily be a minimum cardinality diagnosis).\nThird, minimality of diagnoses will be a necessary condition to guarantee the possibility of discrimination between different (candidate) diagnoses to formulate a solution KB, as will be seen later in Section 5.1.\nFourth, focusing only on minimal diagnoses rather than all diagnoses can greatly reduce the search space for diagnoses and therefore greatly speed up the debugging procedure (cf. [44]).\nCHAPTER 3. KNOWLEDGE BASE DEBUGGING 26\nProjected to the task of KB debugging, namely finding a solution KB w.r.t. a given DPI, this means we are interested in minimal invasiveness, that is making as few formula-deletion-modifications to the input KB K as possible in the course of the performed debugging actions. That is, the actual goal is to find some maximal solution KB K∗ for a DPI. Compare with “The Principle of Parsimony” in [60, p. 7] [7].\nProblem Definition 3.2 (Parsimonious KB Debugging). Given a DPI 〈K,B,P ,N 〉R, the task is to find a maximal solution KB w.r.t. 〈K,B,P ,N 〉R.\nThe next proposition shows that this problem can be reduced to finding a minimal diagnosis.\nProposition 3.6. (i) K \\ K∗ is a minimal diagnosis w.r.t. 〈K,B,P ,N 〉R for each maximal solution KB K∗ w.r.t. 〈K,B,P ,N 〉R.\n(ii) If D is a minimal diagnosis w.r.t. 〈K,B,P ,N 〉R, then (K\\D)∪E is a maximal solution KB w.r.t. 〈K,B,P ,N 〉R for all extensions E ∈ EX(D)〈K,B,P,N 〉R .\nProof. Ad (i): Let K∗ be an arbitrary maximal solution KB w.r.t. 〈K,B,P ,N 〉R. The first observation is that D := K \\ K∗ is a diagnosis w.r.t. 〈K,B,P ,N 〉R since K∗ \\ K ∈ EX(D)〈K,B,P,N 〉R by the fact that K∗ = (K \\ D) ∪ (K∗ \\ K) is a solution KB by assumption. Let us assume that there is a diagnosis Dk ∈ aD〈K,B,P,N 〉R such that D ⊃ Dk. Since Dk is a diagnosis, it holds per Definition 3.5 that there is an extension E ∈ EX(Dk)〈K,B,P,N 〉R such that K∗k := (K \\ Dk) ∪ E is a solution KB. Further on, K∩K∗k = K∩((K\\Dk)∪E) = (K\\Dk)∪(K∩E). SinceK∩K∗ can be written asK\\(K\\K∗) = K\\D which is a strict subset ofK\\Dk which in turn is a subset of (K\\Dk)∪(K∩E) = K∩K∗k. Consequently, K∩K∗ ⊂ K∩K∗k holds, which is by Definition 3.2 a contradiction to the maximality of the solution KB K∗. Thus, D = K \\ K∗ is a minimal diagnosis w.r.t. 〈K,B,P ,N 〉R.\nAd (ii): Let D be a minimal diagnosis w.r.t. 〈K,B,P ,N 〉R. Then, by Definition 3.5, there is an extension E ∈ EX(D)〈K,B,P,N 〉R such that K∗ := (K \\ D) ∪ E is a solution KB. Let us assume that E ∩ D 6= ∅. We can rewrite K∗ as K∗ = (K \\ D) ∪ (E ∩ D) ∪ (E \\ D). Since ∅ ⊂ E ∩ D ⊆ D, we have that (K \\ D) ∪ (E ∩ D) ⊃ K \\ D. Thus, there is a D′ := D \\ (E ∩ D) ⊂ D and an extension E ′ ∈ EX(D′)〈K,B,P,N 〉R such that E ′ := E \\ D such that K∗ = (K \\ D′) ∪ E ′. As K∗ is a solution KB, this is a contradiction to the minimality of D. Therefore, (*) E ∩ D = ∅ for all E ∈ EX(D)〈K,B,P,N 〉R must hold.\nLet E be any extension w.r.t.D and 〈K,B,P ,N 〉R. Then we can writeK∩K∗ = K∩((K\\D)∪E) = (K\\D)∪(K∩E) and by (*) alsoK∩E = ((K\\D)∪D)∩E = ((K\\D)∩E)∪(D∩E) = (K\\D)∩E ⊆ K\\D. Consequently, (**) K ∩ K∗ = K \\ D. Now, assume that there is a solution KB K∗k with the property K∩K∗k ⊃ K∩K∗. By (**), this implies that K∩K∗k ⊃ K \\D which means that there is a Dk ⊂ D ⊆ K such that K ∩ K∗k = K \\ Dk ⊆ K∗k. Now K∗k is a solution KB w.r.t. 〈K,B,P ,N 〉R and can be written as K∗k = (K∗k ∩ K) ∪ (K∗k \\ K) = (K \\ Dk) ∪ (K∗k \\ K). By Dk ⊆ K and since there is a set of formulas E := K∗k \\ K such that (K \\ Dk) ∪ E ∈ Sol〈K,B,P,N 〉R we have that E ∈ EX(Dk)〈K,B,P,N 〉R must hold wherefore Dk is a diagnosis by Definition 3.5. This, however, is a contradiction to the minimality of D. Therefore, K∗ = (K \\ D) ∪ E must be a maximal solution KB for any E ∈ EX(D)〈K,B,P,N 〉R .\nBy claim (i), Proposition 3.6 assures that each maximal solution KB can be found by investigating all minimal diagnoses w.r.t. a DPI. Claim (ii) shows that any solution KB built from a minimal diagnosis is indeed maximal. Thus, finding a suitable minimal diagnosis solves the problem of KB debugging completely."
    }, {
      "heading" : "3.2 Background Knowledge",
      "text" : "The general debugging setting considered in this work envisions the opportunity for the user to specify some background knowledge B, i.e. a set of formulas that are known (or strongly assumed) to be correct\nCHAPTER 3. KNOWLEDGE BASE DEBUGGING 27\nin advance. Note that, in order for the debugging procedure to work soundly, before some background knowledge is incorporated into the DPI, it is necessary to verify its conformance with the postulated requirements R (cf. Definition 3.1).We can distinguish between two basic scenarios how background knowledge can be leveraged: (1) We have an initial KB Kinit and we know or want to assume that a subset of formulas in Kinit is correct, i.e. B ∩ Kinit 6= ∅, and (2) we have an initial KB Kinit and some background knowledge disjoint from Kinit, i.e. B ∩ Kinit = ∅.\nExample use cases for scenario (1) are situations where a user knows that a subset of formulas B in K is definitely sound or wants to restrict the scope of debugging to a particular part of the KB. Concretely, this may occur, for instance, when B is the result, i.e. the finally output solution KB K∗, of a former successful debugging session and K is a further development of K∗, or in a collaborative setting where many users are involved in the development of K and one of them may want to debug only formulas authored by herself and not touch foreign formulas, which are thus assumed as correct and assigned to B. In (1), Kinit ∩ B and Kinit \\ B partition the original KB Kinit into a set of correct and a set of possibly incorrect formulas, respectively. The corresponding DPI would thus be 〈Kinit \\ B,B,P ,N 〉R for some sets of test cases P and N . Note that this DPI does meet the necessary condition (cf. Definition 3.1) K∩B = ∅ as (Kinit \\ B)∩B = ∅. So, in the debugging session, onlyK := Kinit \\ B is used to search for diagnoses, which can reduce the search space substantially. Though, B is incorporated in the calculations throughout the KB debugging procedure, but no formula in B may take part in a diagnosis. The advantage of this over simply not considering the formulas in B at all is, that the semantics of formulas in B is not lost and can be exploited, e.g., to grant the desired semantic properties also in the context of existing approved knowledge or to facilitate a greater choice of queries to interact with a user, which can be exploited to ask queries with lower cardinality or involving less complex formulas (see Section 5.1 for details on queries).\nIn scenario (2), the corresponding DPI looks like 〈Kinit,B,P ,N 〉R for some sets of test cases P and N . An application of this scenario could be the reuse of an existing KB to support an increase of the fault detection rate and thus more sustainable debugging. For example, when formulating a KB Kinit about a domain, a reference KB B in that domain that is thoroughly curated by experts could be leveraged. The use of such a KB B is possible both if Kinit is correct as a standalone KB, i.e. Kinit is already a solution KB for 〈Kinit, ∅,P ,N 〉R, or not. In the first case, Kinit might still contain formulations conflicting with B. In this vein, in both cases, faults may be detected that would have been missed otherwise.\nChapter 4\nDiagnosis Computation\nThe search space for minimal diagnoses w.r.t. 〈K,B,P ,N 〉R the size of which is in general O(2|K|) (if all subsets of the KB K are investigated) can be reduced to a great extent by exploiting the notion of a conflict set [60, 44, 74].\nDefinition 4.1 (Conflict Set). Let 〈K,B,P ,N 〉R be a DPI. A set of formulas C ⊆ K is called a conflict set w.r.t. 〈K,B,P ,N 〉R, written as C ∈ aC〈K,B,P,N 〉R , iff C ∪ UP is not a solution KB w.r.t. 〈K,B,P ,N 〉R. A conflict set C is minimal, written as C ∈mC〈K,B,P,N 〉R , iff there is no C′ ⊂ C such that C′ is a conflict set.\nSimply put, a (minimal) conflict set is a (minimal) faulty KB that is a subset of K. That is, a conflict set is one source causing the faultiness of K in the context of B ∪ UP . In other words, a valid KB may not include all the formulas of any conflict set.\nCorollary 4.1. C ⊆ K is a conflict set w.r.t. 〈K,B,P ,N 〉R iff C is invalid w.r.t. 〈·,B,P ,N 〉R.\nProof. If C is a conflict set w.r.t. 〈K,B,P ,N 〉R, then C∪UP is not a solution KB, i.e. C∪B∪UP violates some r ∈ R, some p ∈ P or some n ∈ N . By extensiveness of L, C ∪ B ∪ UP |= p for all p ∈ P , so C ∪ B ∪ UP must violate some r ∈ R or entail some n ∈ N . Thus, by Definition 3.3, C is invalid w.r.t. 〈·,B,P ,N 〉R.\nIf C ⊆ K is not valid w.r.t. 〈·,B,P ,N 〉R, then C∪B∪UP violates some r ∈ R or entails some n ∈ N , wherefore C∪UP /∈ Sol〈K,B,P,N 〉R . Hence, by Definition 4.1, C is a conflict set w.r.t. 〈K,B,P ,N 〉R.\nConsequently, a conflict set C along with the background knowledge B either violates some r ∈ R, entails some n ∈ N , or yields to a violation of some r ∈ R or entailment of some n ∈ N if all formulas UP comprised by the positive test cases are added to C. Any KB K that is not valid w.r.t. 〈·,B,P ,N 〉R is itself a conflict set and includes at least one minimal conflict set.\nProposition 4.1. Let 〈K,B,P ,N 〉R be a DPI. Then, K is not valid w.r.t. 〈·,B,P ,N 〉R iff K includes at least one minimal conflict set w.r.t. 〈K,B,P ,N 〉R.\nProof. “⇒”: LetK be not valid w.r.t. 〈·,B,P ,N 〉R. ThenK∪UP is not a solution KB w.r.t. 〈K,B,P ,N 〉R, which means thatK is a conflict set w.r.t. 〈K,B,P ,N 〉R by definition 4.1. So, eitherK is a already a minimal conflict set or there must be some subset C ⊂ K which is a minimal conflict set w.r.t. 〈K,B,P ,N 〉R.\n“⇐”: Let K include at least one minimal conflict set w.r.t. 〈K,B,P ,N 〉R. Then, by Definition 4.1, there is some C ⊆ K such that C ∪ UP is not a solution KB. Hence, by the monotonicity of L, K ∪ UP cannot be a solution KB either. So, by Proposition 3.1, K is not valid w.r.t. 〈·,B,P ,N 〉R.\n28\nCHAPTER 4. DIAGNOSIS COMPUTATION 29\nAs a consequence, a complete and sound method for computing minimal conflict sets w.r.t. a DPI 〈K,B,P ,N 〉R can be used to decide validity of K w.r.t. 〈·,B,P ,N 〉R. Moreover, such a method can be used to decide whether a given DPI is admissible, i.e. has solutions. For, if a DPI is admissible and the given KB is invalid w.r.t. this DPI, then there cannot be an empty conflict set. In other words, if the empty KB is a conflict set – or, equivalently, an empty conflict set exists w.r.t. a DPI –, then the DPI is not admissible.\nProposition 4.2. Let 〈K,B,P ,N 〉R be a DPI and K be invalid w.r.t. 〈·,B,P ,N 〉R. Then, there exists a minimal conflict set C 6= ∅ w.r.t. 〈K,B,P ,N 〉R iff 〈K,B,P ,N 〉R is admissible.\nProof. SinceK is not valid w.r.t. 〈·,B,P ,N 〉R, there must be at least one conflict set w.r.t. 〈K,B,P ,N 〉R by Proposition 4.1. Assume that there exists a minimal conflict set C 6= ∅ w.r.t. 〈K,B,P ,N 〉R. This can be true iff ∅ is not a (minimal) conflict set w.r.t. 〈K,B,P ,N 〉R. By Corollary 4.1 and Definition 3.3, this is equivalent to the fact that ∅ ∪ B ∪ UP ≡ B ∪ UP does not violate any r ∈ R and does not entail any n ∈ N . By Proposition 3.4, this holds iff there exists a diagnosis w.r.t. 〈K,B,P ,N 〉R. By Definition 3.6, this is equivalent to 〈K,B,P ,N 〉R being admissible.\nThe following proposition provides information about the relationship between (minimal) conflict sets and the background knowledge as well as the positive test cases.\nProposition 4.3. Let 〈K,B,P ,N 〉R be a DPI and C a conflict set w.r.t. 〈K,B,P ,N 〉R. Then the following holds:\n1. C ∩ B = ∅.\n2. If C is a minimal conflict set w.r.t. 〈K,B,P ,N 〉R, then C ∩ UP = ∅.\nProof. 1): C ∩ B = ∅ holds since C ⊆ K (Definition 4.1) and K ∩ B = ∅ (Definition 3.1). 2): Assume that C is a minimal conflict set w.r.t. 〈K,B,P ,N 〉R and C ∩ UP 6= ∅. Since C is a conflict set, we have that C ∪ B ∪ UP violates some r ∈ R or entails some n ∈ N by Corollary 4.1 and Definition 3.3. Since (C \\ UP ) ∪ B ∪ UP = C ∪ B ∪ UP and (C \\ UP ) ⊂ C, this implies that (C \\ UP ) is a conflict set w.r.t. 〈K,B,P ,N 〉R which in turn implies that C /∈ mC〈K,B,P,N 〉R which is a contradiction."
    }, {
      "heading" : "4.1 Conflict Sets versus Justifications",
      "text" : "The notion of a conflict set is closely related to the notion of a justification [25, 26, 27, 23, 24, 28] which is frequently adopted in the field of the Semantic Web (cf. Section 2.2) in order to find minimal explanations for particular entailments in DL ontologies. Thus, the paradigm of a justification can be a useful aid in the debugging of faulty ontologies [37]. Note that sometimes justifications are referred to as MinAs (Minimal Axiom Sets) [4] or MUPS (Minimal Unsatisfiability Preserving Sub-TBoxes) [68] where the latter term is mostly used in the context of ontology debugging. The notion of a (minimal) conflict set, on the other hand, has been mainly adopted in the Diagnosis community [60, 44, 57, 87, 17]. In this section we want to establish a relationship between these two widely used instruments used for debugging. It will turn out that both terms are strongly related, but in debugging systems like the ones proposed in our work conflict sets are better suited as they automatically focus only on the minimal explanations for faults in a KB.\nFor example, the author of [37] i.a. discusses the use of justifications to aid the debugging of incoherent ontologies, i.e. ontologies that include unsatisfiable concepts (cf. Section 2.2). If there are multiple unsatisfiable concepts, then some of these might be only unsatisfiable due to the unsatisfiability of another concept. Assume, for instance, an incoherent DL KB K := {A @ B,B v E u ¬E}. In K there are two unsatisfiable concepts A and B where A’s unsatisfiability is dependent on B’s unsatisfiability. Using the\nCHAPTER 4. DIAGNOSIS COMPUTATION 30\nterminology of [37, 23], A would be called a purely derived unsatisfiable concept whereas B would be called a root unsatisfiable concept. Because the (only) justification for the unsatisfiability ofA is JA := K whereas the (only) justification for the unsatisfiability of B is JB = {B v E u ¬E} ⊂ JA. Therefore, [37] proposes to resolve root unsatisfiable concepts first since this might resolve some (purely) derived concepts as well, as in this example. However, finding out whether a concept is root or derived involves the computation of justifications for all unsatisfiable concepts in a KB. On the other hand, reliance on minimal conflict sets would implicate a direct focus on the faultiness (in this example: the incoherency) of the KB and not necessarily on the exact explanations of all unsatisfiable concepts that cause the incoherency. In this vein, no justification for a purely derived concept can be a minimal conflict set. So, the computation of minimal conflict sets involves only the determination of those justifications for faults that must necessarily be resolved. Therefore, for the given example, the only minimal conflict set is JB .\nA justification for a given formula (axiom) relative to a KB is a (subset-)minimal subset of the KB that entails the given formula.\nDefinition 4.2 (Justification for a formula). [38] Let K be a KB and α a formula, both over L. Then J ⊆ K is called a justification for α w.r.t. K, written as J ∈ Just(α,K), iff J |= α and for all J ′ ⊂ J it holds that J ′ 6|= α.\nSince we consider test cases which are sets of formulas over L, we generalize the definition of a justification as follows:\nDefinition 4.3 (Justification for a set of formulas). Let K, K′ be KBs over L. Then J ⊆ K is called a justification for K′ w.r.t. K, written as J ∈ Just(K′,K), iff J |= K′ and for all J ′ ⊂ J it holds that J ′ 6|= K′.1\nIn order to express the connection between justifications and conflict sets, we require yet another generalization of this definition. To this end, the following definition characterizes a justification for a set X of KBs relative to a KB K as a (subset-)minimal subset of K such that this subset entails some KB in X .\nDefinition 4.4 (Justification for a set of sets of formulas). Let K be a KB over L and X a set of KBs over L. Then J ⊆ K is called justification for X w.r.t. K, written as J ∈ Just(X,K), iff J |= K′ for some K′ ∈ X and for all J ′ ⊂ J it holds that J ′ 6|= K′′ for all K′′ ∈ X .\nBased on Definition 4.4, the relation between conflict sets and justifications is captured by the following Proposition 4.4. Intuitively, any conflict set w.r.t. 〈K,B,P ,N 〉R is the part of a justification for a fault that is relevant for the debugging task, where fault refers to an inconsistency (and/or incoherency) and/or a negative test case entailed by K ∪ B ∪ UP . Since debugging focuses on the deletion of KB formulas only, “relevant” in this context refers to the subset of the justification that does not contain any sentences in B and UP , but solely sentences from K. Importantly, there may be justifications, in general, the relevant subset of which is not a minimal conflict set. The reason why this case can arise in spite of the set-minimality of justifications is that the relevant part of a justification (for some set of sentencesK1, e.g. a negative test case n1 ∈ N ) may be a superset of the relevant part of another justification (for some other set of sentences K2, e.g. another negative test case n2 ∈ N ) whereas both justifications are not in a subset-relationship (i.e. contain different sentences from B and/or UP ). This circumstance is illustrated by the following example:\n1Remember that J |= K′ means that J |= ax for each ax ∈ K′ (cf. Remark 3.1).\nCHAPTER 4. DIAGNOSIS COMPUTATION 31\nExample 4.1 Let a DPI 〈K,B,P ,N 〉R be defined as\nK := {B v E,E v ∃r.G} B := {A v B} N := {{A v E} , {B v ∃r.G}} P := ∅ R := {consistency}\nWe have thatK∪B∪UP is consistent and thus no requirement in R is violated. But, the two negative test cases are both entailed byK∪B∪UP whereforeK is invalid w.r.t. 〈·,B,P ,N 〉R. The set of justifications for the violation of the first negative test case is Jn1 = {{A v B,B v E}}; for the second one it is Jn2 = {{B v E,E v ∃r.G}}. The relevant subset of the justification J1 in Jn1 is J1,rel = {B v E} (since {A v B} is in B) whereas the relevant subset of the justification J2 in Jn2 is J2,rel = {B v E, E v ∃r.G}, i.e. J1,rel ⊂ J2,rel despite that there is no set subset-relationship between J1 and J2. Hence, there are two justifications that explain the invalidity of K w.r.t. 〈·,B,P ,N 〉R, but there is only one minimal conflict set C = J1,rel w.r.t. 〈K,B,P ,N 〉R.\nSo, generally, the set of minimal conflict sets w.r.t. a DPI is a subset of the set of justifications for faults in K ∪ B ∪ UP , which is due to the focus on just the parts of justifications that are relevant for the KB debugging task.\nProposition 4.4. Let 〈K,B,P ,N 〉R be a DPI. Additionally, let\n(a) X := {{Ai v ⊥} |Ai ∈ NC} ∪ {{ri v ⊥} | ri ∈ NR} ∪ {{> v ⊥}} ∪N if R = {consistency, coherency} and\n(b) X := {{> v ⊥}} ∪N if R = {consistency}.2\nThen the following holds:\n1. If C is a minimal conflict set w.r.t. 〈K,B,P ,N 〉R, then there is some J ∈ Just(X,K ∪ B ∪ UP ) such that (J ∩ K) \\ UP = C.\n2. For all J ∈ Just(X,K ∪ B ∪ UP ) it is true that C := (J ∩ K) \\ UP is a conflict set w.r.t. 〈K,B,P ,N 〉R, but not necessarily a minimal one.\nProof. 1): Assume that C ∈mC〈K,B,P,N 〉R and for all J ∈ Just(X,K∪B ∪UP ) it holds that (J ∩K) \\ UP 6= C. There are two cases to distinguish between: (a) there is some sentence in (J ∩ K) \\ UP that is not in C and (b) there is some sentence in C that is not in (J ∩ K) \\ UP .\nLet us first assume (a), i.e. for all J ∈ Just(X,K ∪ B ∪ UP ) it holds that there is some sentence ax in (J ∩ K) \\ UP that is not in C. Additionally, assume there is a J ∈ Just(X,K ∪ B ∪ UP ) such that J ⊆ C ∪ B ∪ UP . We can write J as J = S1 ∪ S2 ∪ S3 for S1 := [(J ∩ K) \\ UP ], S2 := [J ∩ B] and S3 := [J∩UP ]. Since J = S1∪S2∪S3 ⊆ C∪B∪UP it must hold in particular that S1 ⊆ C∪B∪UP and therefore ax ∈ C ∪ B ∪UP . However, ax /∈ C by assumption, ax /∈ B since ax ∈ K and B ∩K = ∅, and ax /∈ UP since ax ∈ S1 and S1∩UP = ∅. This is a contradiction. Hence, for all J ∈ Just(X,K∪B∪UP ) it holds that J 6⊆ C ∪ B ∪ UP . Since X captures all r ∈ R and n ∈ N , we can conclude that C is not a conflict set w.r.t. 〈K,B,P ,N 〉R which is a contradiction to C ∈mC〈K,B,P,N 〉R .\nLet us now assume (b), i.e. for all J ∈ Just(X,K ∪ B ∪ UP ) it holds that there is some sentence ax in C that is not in (J ∩ K) \\ UP . Since C is a conflict set and since X captures all r ∈ R and n ∈ N ,\n2We use DL notation in this proposition since justifications, as argued, are mostly applied to DL KBs. An equivalent formulation of the proposition for FOL or PL is straightforward (cf. Example 2.1 and Remark 3.2). Note that for PL only (b) is relevant since coherency is not defined for PL. Further, recall that NC and NR are defined in Section 2.2.\nCHAPTER 4. DIAGNOSIS COMPUTATION 32\nwe have that C ∪ B ∪ UP |= K′ for some K′ ∈ X . So, there must be some J0 ∈ Just(X,K ∪ B ∪ UP ) such that J0 ⊆ C ∪B ∪UP . As C ∈mC〈K,B,P,N 〉R , there cannot be any J ∈ Just(X,K∪B ∪UP ) with J ⊆ C′ ∪B ∪UP for arbitrary C′ ⊂ C. This must hold in particular for J0 which implies that J0 ∩ C = C which is equivalent to C ⊆ J0. As (1) C ⊆ K (Definition 4.1) and, by Proposition 4.3 and by the fact that C ∈ mC〈K,B,P,N 〉R , (2) C ∩ UP = ∅, we can conclude that C ⊆ (J0 ∩ K) \\ UP which is a contradiction since there cannot be a ax in C that is not in (J0 ∩ K) \\ UP .\n2): If J ∈ Just(X,K∪B∪UP ), then, by Definition 4.4, J |= K′ for someK′ ∈ X and J ⊆ K∪B∪UP . So, [(J ∩ K) \\ UP ] ∪ B ∪ UP = (J ∩ K) ∪ B ∪ UP ⊇ J wherefore [(J ∩ K) \\ UP ] ∪ B ∪ UP |= K′ by monotonicity of L. As K′ ∈ X and X captures all the reasons why some r ∈ R or some n ∈ N may not be fulfilled (cf. discussion in Chapter 3), we have that [(J ∩ K) \\ UP ] ∪ B ∪ UP violates some r ∈ R or entails some n ∈ N . This implies that [(J ∩K) \\UP ] ∪UP /∈ Sol〈K,B,P,N 〉R . Since (J ∩K) \\UP ⊆ K is also true, (J ∩ K) \\ UP ∈ aC〈K,B,P,N 〉R by Definition 4.1.\nTo see that (J ∩ K) \\ UP /∈ mC〈K,B,P,N 〉R holds in general, reconsider Example 4.1 where (J2 ∩ K) \\ UP = J2 ⊃ C holds for the justification J2 and the minimal conflict set C."
    }, {
      "heading" : "4.2 The Relation between Conflict Sets and Diagnoses",
      "text" : "A minimal conflict set has the property that deletion of any formula in it yields a set of formulas which is correct in the context of B, P , N and R.\nProposition 4.5. If C is a minimal conflict set w.r.t. 〈K,B,P ,N 〉R, then C′ is valid w.r.t. 〈·,B,P ,N 〉R for each C′ ⊂ C.\nProof. Since C ∈ mC〈K,B,P,N 〉R , it must hold that C′ /∈ aC〈K,B,P,N 〉R . Then, by Corollary 4.1, C′ is valid w.r.t. 〈·,B,P ,N 〉R.\nHence, by deletion of at least one formula from each minimal conflict set w.r.t. 〈K,B,P ,N 〉R, a valid KB can be obtained fromK. Thus, a solution KB (K\\D)∪UP can be obtained by calculation of a hitting set D of all minimal conflict sets in mC〈K,B,P,N 〉R . The Hitting Set problem is defined as follows:\nDefinition 4.5 (Hitting Set). Let S = {S1, . . . , Sn} be a set of sets. Then, H is called a hitting set of S iff H ⊆ US and H ∩ Si 6= ∅ for all i = 1, . . . , n.\nA hitting set H of S is minimal iff there is no hitting set H ′ of S such that H ′ ⊂ H .\nProposition 4.6. [19] A (minimal) diagnosis w.r.t. the DPI 〈K,B,P ,N 〉R is a (minimal) hitting set of all minimal conflict sets w.r.t. 〈K,B,P ,N 〉R.\nNow, we want to contemplate two example DPIs and analyze them regarding the their minimal conflict sets and minimal diagnoses:\nExample 4.2 In this example, we analyze the PL DPI 〈K,B,P ,N 〉R given by Table 4.1. There are two minimal conflict sets w.r.t. 〈K,B,P ,N 〉R, i.e. mC〈K,B,P,N 〉R = {C1, C2} = {〈1, 2, 5〉 , 〈1, 2, 7〉}. 3\nWhy is C1 a conflict set w.r.t. 〈K,B,P ,N 〉R? We recall Definition 4.1 and argue as follows to deduce the entailment C1 |= n1 where n1 ∈ N (left of the colon: the formulas used in the deduction are\n3Please notice that we sometimes write i instead of ax i for brevity when it is clear what is meant. We will do so in many other examples as well.\nCHAPTER 4. DIAGNOSIS COMPUTATION 33\nunderlined; right of the colon: the relevant implications are underlined):\nax 1 : A → E ax 2 : X ∨ E → F ∧ Y ∧ Z ax 5 : Y → ¬A\nax 1, ax 2, ax 5 : A → ¬A ≡ ¬A ∨ ¬A ≡ ¬A n1 ∈ N : ¬A\nMinimality of C2 is obvious from this argumentation. i.e. we cannot deduce n1 if any one of the formulas 1, 2 or 5 is omitted, and there is no other fault except for the violation of n1.\nWhy is C2 a conflict set w.r.t. 〈K,B,P ,N 〉R? We recall Definition 4.1 and argue as follows to deduce the entailment C2 ∪ B |= n1 where n1 ∈ N (left of the colon: the formulas used in the deduction are underlined; right of the colon: the relevant implications are underlined):\nax 1 : A → E ax 2 : X ∨ E → F ∧ Y ∧ Z ax 7 : Z → G\n(G → ¬A) ∈ B : G → ¬A ax 1, ax 2, ax 7,B : A → ¬A ≡ ¬A ∨ ¬A ≡ ¬A\nn1 ∈ N : ¬A\nMinimality of C2 is obvious from this argumentation. i.e. we cannot deduce n1 if any one of the formulas 1, 2 or 7 is omitted, and there is no other fault except for the violation of n1.\nThere are no further minimal conflict sets w.r.t. 〈K,B,P ,N 〉R. This is fairly easy to see since\n• K ∪ B ∪UP = K ∪B cannot be inconsistent due to the fact that the only negative literal occurring on the righthand side of an implication is ¬A and A does not occur at the righthand side of any implication in K ∪ B,\n• there is no other way to deduce n1 than using a superset of the formulas in C1 or C2 and\n• n1 is the only negative test case in N .\nHence, the set of all minimal diagnoses mD〈K,B,P,N 〉R = {D1,D2,D3} = {[1], [2], [5, 7]} is obtained by computing all minimal hitting sets of mC〈K,B,P,N 〉R = {C1, C2} (cf. Proposition 4.6).\nExample 4.3 In this example, we analyze the DL DPI 〈K,B,P ,N 〉R given by Table 4.2. There are four minimal conflict sets w.r.t. 〈K,B,P ,N 〉R, i.e.\nmC〈K,B,P,N 〉R = {C1, C2, C3, C4} = {〈1, 2, 5〉 , 〈2, 4, 6〉 , 〈1, 3, 4〉 , 〈1, 5, 6, 8〉}\nWhy is C1 a conflict set w.r.t. 〈K,B,P ,N 〉R? We recall Definition 4.1 and argue as follows to deduce the entailment C1 |= n1 where n1 ∈ N (left of the colon: the formulas used in the deduction are underlined; right of the colon: the relevant implications are underlined):\nax 1 : A v B ax 2 : B v G ax 5 : G v K\nax 1, ax 2, ax 5 : A v K n1 ∈ N : A v K\nCHAPTER 4. DIAGNOSIS COMPUTATION 34\nMinimality of C1 is follows from this argumentation. i.e. we cannot deduce n1 if any one of the formulas 1, 2 or 5 is omitted, and from the fact that we cannot deduce an incoherency (r2), inconsistency (r1) or the entailment of any other negative test case n ∈ N for any KB C′1 ∪ B ∪ UP for any C′1 ⊂ C1.\nWhy is C2 a conflict set w.r.t. 〈K,B,P ,N 〉R? We recall Definition 4.1 and argue as follows to deduce that C2 ∪ B is incoherent and thus violates the requirement r2 ∈ R (left of the colon: the formulas used in the deduction are underlined; right of the colon: the relevant implications are underlined):\nax 2 : B v G ax 6 : G v ∃r.F\n(1) : ax 2, ax 6 : B v ∃r.F ax 4 : B v ∀r.H\n(H v ¬F ) ∈ B : H v ¬F (2) : ax 4,B : B v ∀r.¬F (1) and (2) : B v ⊥\nr1 ∈ R : B 6v ⊥\nSince we cannot deduce an incoherency (r2), inconsistency (r1) or the entailment of any negative test case n ∈ N for any KB C′2 ∪ B ∪ UP for any C′2 ⊂ C2, the minimality of C2 follows.\nWhy is C3 a conflict set w.r.t. 〈K,B,P ,N 〉R? We recall Definition 4.1 and argue as follows to deduce that C3 ∪B ∪UP is inconsistent and thus violates the requirement r1 ∈ R (left of the colon: the formulas used in the deduction are underlined; right of the colon: the relevant implications are underlined):\nA(x) ∈ B : A(x) ax 1 : A v B\n(1) : ax 1,B : B(x) (2) : p1 ∈ P : r(x, y)\nax 4 : B v ∀r.H (3) : (1) and ax 4 : H(y)\n(4) : ax 3 : ¬H(y) (3) and (4) : E\nNo inconsistency (r1) or incoherency (r2) can be derived and no negative test case n ∈ N is entailed from any C′3 ∪ B ∪ UP for C′3 ⊂ C3. Hence, C3 is a minimal conflict set w.r.t. 〈K,B,P ,N 〉R.\nWhy is C4 a conflict set w.r.t. 〈K,B,P ,N 〉R? We recall Definition 4.1 and argue as follows to deduce the entailment C4 ∪ B |= n2 where n2 ∈ N (left of the colon: the formulas used in the deduction are underlined; right of the colon: the relevant implications are underlined):\nax 8 : L v G ax 6 : G v ∃r.F\n(1) : ax 6, ax 8 : L v ∃r.F A(x) ∈ B : A(x)\n(2) : ax 1,B : B(x) (3) : ax 5 : G v K\n(1) and (2) and (3) : L v ∃r.F, B(x), G v K n1 ∈ N : L v ∃r.F, B(x), G v K\nCHAPTER 4. DIAGNOSIS COMPUTATION 35\nNo inconsistency (r1) or incoherency (r2) can be derived and no negative test case n ∈ N is entailed from any C′4 ∪ B ∪ UP for C′4 ⊂ C4. Thus, C4 is a minimal conflict set w.r.t. 〈K,B,P ,N 〉R.\nHence, the set of all minimal diagnoses mD〈K,B,P,N 〉R , obtained by computing all minimal hitting sets of mC〈K,B,P,N 〉R = {C1, C2, C3, C4} (cf. Proposition 4.6), comprises ten minimal diagnoses Di for i = 1, . . . , 10:\nD1 = [1, 2] D2 = [1, 4] D3 = [1, 6] D4 = [2, 3, 5] D5 = [2, 3, 6] D6 = [2, 3, 8] D7 = [2, 4, 6] D8 = [2, 4, 8] D9 = [3, 5, 6] D10 = [4, 5]\nAlthough the DPI 〈K,B,P ,N 〉R is very small in size, i.e. number of formulas occurring in it is very small, the reader might agree that it is not trivial on the one hand (1) to realize which subsets of this KB K are (minimal) conflict sets, (2) to see that or why a subset of this KB K along with the background knowledge B and the union of the positive test cases UP is a (minimal) conflict set (cf. [24]) and (3) to assess that there are no further minimal conflict sets w.r.t. 〈K,B,P ,N 〉R. This example gives a little bit of an impression that tool assistance in the debugging of KBs is inevitable especially for real-world KBs that are huge in size and/or complex in terms of the expressivity of the used logic or in terms of their “debugging properties”, i.e. large number and/or size of minimal conflict sets and/or minimal diagnoses.\nA means to handle problems (1) and (3) is provided by some method for the computation of a minimal conflict set (e.g. QX given by Algorithm 1 below, see Section 4.3.1) coupled with a hitting set tree algorithm (e.g. HS described by Algorithm 2 below, see Section 4.4) for the systematic computation of different minimal conflict sets, or other mechanisms such as the ALL_JUST_ALG presented in [38] which computes all justifications for some particular entailment (but, some post-processing of the justifications is necessary to obtain minimal conflict sets, cf. Section 4.1).\nProblem (2) and its complexity for humans has been studied in [24] with a focus on justifications in DL or OWL KBs. Since a minimal conflict set can be regarded as the relevant (i.e. potentially faulty) part of a justification for some undesired entailment (i.e. a violated requirement or test case) as we analyzed in Section 4.1, the cognitive complexity model proposed by [24] applies also to minimal conflict sets. Ways to facilitate the understanding of justifications for humans (that might be successfully applied also to conflict sets) have been addressed in [27, 26, 25]. Moreover, there is an ontology editing browser SWOOP [40] equipped with a strikeout feature [37] that highlights parts of justifications that are relevant for the entailment by striking out all irrelevant parts. This is more or less the automation of our analyses of the conflict sets by underlining the relevant parts of the formulas in this example and Example 4.2."
    }, {
      "heading" : "4.3 Methods for Diagnosis Computation",
      "text" : "Two common methods employed for the computation of (minimal) diagnoses [74, 63] are the QuickXPlain algorithm [36] (in short QX) and a hitting set search tree [60, 22] (in short HS). Thereby, QX serves as a deterministic method for computing one minimal conflict set w.r.t. a given DPI 〈K,B,P ,N 〉R per call. Since a diagnosis is a hitting set of all minimal conflict sets, more than one minimal conflict set is generally required to compute a diagnosis. Due to its determinism, however, QX always computes the same minimal conflict set for the same input DPI. Thus, in order to compute different (or all) minimal conflict sets, the input to QX needs to be varied accordingly. This can be done by means of HS which serves as a search tree to systematically and successively explore all minimal conflict sets w.r.t. an initially given DPI. Note that often not all minimal conflict sets w.r.t. a DPI are necessary to obtain a\nCHAPTER 4. DIAGNOSIS COMPUTATION 36\nminimal diagnosis w.r.t. this DPI. This is the case when different minimal conflict sets overlap, i.e. have a non-empty intersection. In the extreme case, when all minimal conflict sets w.r.t. a DPI share some formulas, then the computation of any single minimal conflict set can suffice to obtain a minimal diagnosis, which is actually even a minimum cardinality diagnosis.\nAnother approach for computing a minimal conflict set (or justification) is the “expand-and-shrink” algorithm presented in [38]. However, empirical evaluations and a theoretical analysis of the best and worst case complexity of the “expand-and-shrink” method compared to QX performed in [75] revealed that the latter is preferable over the former.\nAlso, alternative strategies for the computation of minimal diagnoses have been suggested. One common method is to avoid the indirection of diagnosis computation via minimal conflict sets and use algorithms that determine diagnoses directly [66], i.e. without the necessity to compute conflict sets. This approach has been applied for the non-interactive debugging of ontologies [14] and constraints [18]. In our previous work, we adopted such a direct technique for the interactive debugging of KBs [76]. The reason why we stick to the conflict-based approach in this work is that we want to present bestfirst algorithms that figure out minimal diagnoses in descending order of their probability. This is not (systematically) realizable with a direct approach."
    }, {
      "heading" : "4.3.1 Computation of a Minimal Conflict Set",
      "text" : "The QX algorithm takes a DPI 〈Korig,Borig,P ,N 〉R over some monotonic logic L as input and returns a minimal conflict set C ⊆ Korig w.r.t. 〈Korig,Borig,P ,N 〉R as output, if some conflict set exists for the DPI, and ’no conflict’ otherwise.\nMonotonic Properties. Basically, QX can be employed to find for an input set X a set-minimal subset Xmin ⊆ X that has a certain property prop for problems of completely different nature such as propositional unsatisfiability or over-constrainedness of constraint satisfaction problems. The only postulated prerequisite for QX to work correctly is that prop is a monotonic property. A property is monotonic if and only if the binary function that returns 1 if the property holds for the input set and 0 otherwise is a\nCHAPTER 4. DIAGNOSIS COMPUTATION 37\nmonotonic function.\nDefinition 4.6 (Binary monotonic function). Let X be a set and f : 2X → {0, 1} be a binary function defined for all subsets of X . Then, f is monotonic iff\n∀X ′, X ′′ ⊆ X : X ′ ⊂ X ′′ ∧ f(X ′) = 1 =⇒ f(X ′′) = 1\nSo, prop is monotonic iff, given that prop holds for some set X ′, it follows that prop also holds for any superset X ′′ of X ′. Note that, by simple logical transformation, an equivalent statement can be derived from Definition 4.6; namely that, given that prop does not hold for some set X ′′, it follows that prop does not hold for any subset X ′ of X ′′ either.\nAs inconsistency and incoherency as well as the entailment of some n ∈ N over some monotonic language L are clearly monotonic properties, the following proposition holds.\nProposition 4.7. Let 〈K,B,P ,N 〉R be a DPI. Then, the invalidity of K′ ⊆ K w.r.t. 〈·,B,P ,N 〉R (as per Definition 3.3) is a monotonic property.\nBy Corollary 4.1, a (minimal) conflict set w.r.t. 〈K,B,P ,N 〉R is a (minimal) invalid sub-KB of K w.r.t. 〈·,B,P ,N 〉R. Therefore:\nCorollary 4.2. Let 〈K,B,P ,N 〉R be a DPI. Then, being a conflict set w.r.t. 〈K,B,P ,N 〉R is a monotonic property.\nThus, QX is applicable for the problem of finding a minimal conflict set w.r.t. a DPI. As we shall see later in Section 5.2, another monotonic property will enable us to apply QX also for the minimization of queries asked to an interacting user in the interactive debugging of KBs.\nCHAPTER 4. DIAGNOSIS COMPUTATION 38\nHow QX (Algorithm 1) Works. After verifying that the trivial cases, i.e. Korig is already a valid KB w.r.t. 〈·,Borig,P ,N 〉R or Korig = ∅, are not met, a non-empty minimal conflict set w.r.t. 〈Korig,Borig, P ,N 〉R must exist. So, the algorithm enters the recursive procedure QX′(∅, 〈Korig,Borig,P ,N 〉R). Note that the parameters P ,N ,R of QX′ are used for validity tests (ISKBVALID, line 9) only and are maintained invariant during the entire recursive execution. In case Korig is not a singleton, i.e. it does not hold for sure that Korig is an element of a minimal conflict set w.r.t. 〈Korig,Borig,P ,N 〉R, the idea is to apply a divide-and-conquer strategy to reduceKorig into two subproblems and solve one subproblem first, i.e. find a minimal conflict set for this subproblem, and then the second subproblem. The union of the minimal conflict sets found for the subproblems is then a minimal conflict set for the original problem. This division into smaller problems is recursively executed for each subproblem until the trivial case, i.e. the KB of the subproblem that is analyzed includes only one element, occurs. Then this element is an element of a minimal conflict set w.r.t. the original problem.\nSimply put, one can imagine that QX takes Korig, partitions it into K1 and K2 and first considers the DPI with KB K2 and background knowledge B ∪K1 (line 16). If the latter already includes a conflict set (second condition in line 9), then K2 can be safely discarded and does not need to be further considered. Instead, K1 is further investigated, i.e. the DPI with KB K1,2 and background knowledge B∪K1,1 where K1,1 and K2,2 partition K1. Notice that, in this way, |K2| sentences can be dismissed by a single call to ISKBVALID which is the only function in Algorithm 1 that calls a reasoner.\nIf, on the other hand, B∪K1 includes no conflict set, K2 is partitioned intoK2,1 andK2,2 and the two DPIs, the first with KB K2,2 and background knowledge B∪K1∪K2,1 and the second with KB K2,1 and background knowledge B ∪K1 ∪ C2,2, are recursively analyzed where C2,2 is the result computed for the first DPI.\nThis recursion is executed until encountering a trivial case, i.e. a leaf node of the recursion tree, along each path. Then, the recursion unwinds by building the union of all leaf nodes, i.e. the union of all returned sets for subproblems where a trivial case occurred.\nThe next example illustrates one execution of QX which computes one minimal conflict set:\nExample 4.4 Let us consider the DL example DPI depicted by Table 4.3. We will now demonstrate\nCHAPTER 4. DIAGNOSIS COMPUTATION 39\nAlgorithm 1 QX: Computation of a Minimal Conflict Set Input: a DPI 〈Korig,Borig,P ,N 〉R Output: a minimal conflict set w.r.t. 〈Korig,Borig,P ,N 〉R\n1: procedure QX(〈Korig,Borig,P ,N 〉R) 2: if ISKBVALID(Korig, (Borig,P ,N ,R)) then 3: return ‘no conflict’ 4: else if Korig = ∅ then 5: return ∅ 6: else 7: return QX′(∅, 〈Korig,Borig,P ,N 〉R)\n8: procedure QX′(C, 〈K,B,P ,N 〉R) 9: if C 6= ∅ ∧ ¬ISKBVALID(B, 〈·, ∅,P ,N 〉R) then\n10: return ∅ 11: if |K| = 1 then 12: return K 13: k ← SPLIT(|K|) 14: K1 ← GET(K, 1, k) 15: K2 ← GET(K, k + 1, |K|) 16: C2 ← QX′(K1, 〈K2,B ∪ K1,P ,N 〉R) 17: C1 ← QX′(C2, 〈K1,B ∪ C2,P ,N 〉R) 18: return C1 ∪ C2\n19: procedure ISKBVALID(K, 〈·,B,P ,N 〉R) 20: K′ ← K ∪ B ∪ ⋃ p∈P p 21: if ¬VERIFYREQ(K′,R) then 22: return false 23: for n ∈ N do 24: if ENTAILS(K′,n) then 25: return false 26: return true\nhow a minimal conflict set is computed by Algorithm 1 (see Fig. 4.1). Since K is not the empty set and not a valid KB w.r.t. the DPI (conditions in lines 4 and 2 are false), QX′(∅, 〈K,B,P ,N 〉R) is called in line 7. This call is illustrated by the root node (node 1©) of the recursion tree given in Fig. 4.1 (whereas the evaluations made by QX prior to this call are not depicted in the figure). Notice that each node in the tree shows only the values of C, K and B since all other parameters P , N and R are invariant throughout the entire execution of Algorithm 1.\nDue to the fact that C = ∅ andK includes five formulas and is thus not a singleton,K = {ax 1, . . . , ax 5} is partitioned into K1 = {ax 1, ax 2, ax 3} and K2 = {ax 4, ax 5} and QX′ is recursively called in line 16 with parameters C = K1, K = K2 and B = B ∪ {ax 1, ax 2, ax 3} which is expressed in the figure by a left branch to node 2©. This call, however, returns ∅ directly since B ∪ {ax 1, ax 2, ax 3} is already invalid w.r.t. 〈·, ∅,P ,N 〉R because B ∪ {ax 1, ax 2, ax 3} ∪ UP = { A(w), A(v), s(v, w) } ∪{\nA v B,B v E,B vD u ¬∃s.C } ∪{{B(w)}} |= {¬C(w)} which is a negative test case, i.e. must not be entailed by a solution KB w.r.t. the input DPI (the parts of the formulas relevant for the entailment to hold are underlined). Returning ∅ in this case means discarding K2 = {ax 4, ax 5}.\nSo, the algorithm opens a right branch from the root to node 3© by calling QX′ (line 17) with parameters C = ∅ (result of left branch), K = K1 = {ax 1, ax 2, ax 3} and B = B. During the execution of this call K1 is partitioned into {ax 1, ax 2} (left branch to node 4©) and {ax 3} (right branch to node 5©). In\nCHAPTER 4. DIAGNOSIS COMPUTATION 40\nand are written in format C,K,B k© where k is a counter starting from 1 that indicates when the respective\ncall is made. A recursive call to QX′ (left branch = call in line 16; right branch = call in line 17) is denoted by a normal arrow whereas the return of a set is visualized by a dashed arrow.\nnode 4©, it holds that B∪{ax 1, ax 2} can be extended to a solution KB by adding UP , i.e. B∪{ax 1, ax 2} is valid. As it is already an established fact since the execution of node 2© that B ∪ {ax 1, ax 2, ax 3} is invalid, it must be the case that ax 3 is an element of a minimal conflict set w.r.t. the input DPI (as there is a conflict set w.r.t. the input DPI in {ax 1, ax 2, ax 3}, but there is none in {ax 1, ax 2}). The algorithm accounts for that by checking whether K is a singleton (line 11) in which case it is guaranteed that K is a subset of a minimal conflict set w.r.t. the input DPI. So, node 4© returns {ax 3}. This procedure is continued until each path from the root node reaches a node where a trivial case is met. Then the recursion unwinds and, when arrived at the root node, the minimal conflict set 〈ax 1, ax 3〉 is returned.\nThat C := 〈ax 1, ax 3〉 is indeed a conflict set can be recognized easily by the underlinings in the formulas given before. Minimality is given since B ∪ C ∪ UP is neither inconsistent nor incoherent and the deletion of any formula from C breaks the entailment of n1. Hence, QX has returned a sound output.\nThe complexity of Algorithm 1 in terms of the number of calls to the function ISKBVALID, which is the only place in the algorithm where a reasoning service is consulted, is captured by the following proposition.\nProposition 4.8 (Complexity of QX). [36] Let 〈K,B,P ,N 〉R be a DPI and the function SPLIT (line 13\nCHAPTER 4. DIAGNOSIS COMPUTATION 41\nof Algorithm 1) be defined as SPLIT(n) = bn2 c where n is a natural number. Then, the worst case number of calls to ISKBVALID during one call to QX(〈K,B,P ,N 〉R) is in O(|C| log |K||C| ) where C is the output of QX(〈K,B,P ,N 〉R).\nFor any other definition of the function SPLIT, the worst case number of ISKBVALID invocations gets larger."
    }, {
      "heading" : "4.3.2 Correctness of Conflict Set Computation",
      "text" : "This section is dedicated to the proof of correctness of Algorithm 1. First, we show some essential properties of QX by various Lemmata which will finally be exploited to demonstrate the overall soundness of QX.\nThe QX algorithm accepts a DPI 〈Korig,Borig,P ,N 〉R over some monotonic language L as input and returns a minimal conflict set C ⊆ Korig w.r.t. 〈Korig,Borig,P ,N 〉R as output. First, the algorithm checks whether Korig is a valid KB w.r.t. the input DPI 〈·,Borig,P ,N 〉R (line 2). If so, there is no conflict set for the DPI by Proposition 4.1 and the algorithm returns ’no conflict’. Otherwise, the test Korig = ∅ is performed (line 4). If so, then the negative outcome of the validity test executed in line 2 actually means that one of the two criteria of Proposition 3.4 is violated which, by Definition 3.6, implies that the DPI is not admissible. Invalidity of Korig w.r.t. 〈·,Borig,P ,N 〉R and non-admissiblity of 〈Korig,Borig,P ,N 〉R mean that there is only one minimal conflict set C = ∅ by Proposition 4.2. Thus, ∅ is returned in line 5.\nLemma 4.1. Let 〈K,B,P ,N 〉R be an admissible DPI and K be invalid w.r.t. 〈·,B,P ,N 〉R. Then, there is a minimal conflict set C ⊃ ∅ w.r.t. 〈K,B,P ,N 〉R.\nProof. The proposition is a direct consequence of Proposition 4.2.\nSo, if both initial tests (lines 2 and 4) are negative, then, by Lemma 4.1, there is a non-trivial minimal conflict set w.r.t. 〈Korig,Borig,P ,N 〉R wherefore the algorithm enters the recursion by a call to the procedure QX′.\nThe argumentation so far proves the following lemma.\nLemma 4.2.\n• QX(〈K,B,P ,N 〉R) returns ’no conflict’ iff there is no (minimal) conflict w.r.t. 〈K,B,P ,N 〉R.\n• QX(〈K,B,P ,N 〉R) returns ∅ iff ∅ is the only (minimal) conflict w.r.t. 〈K,B,P ,N 〉R.\n• QX(〈K,B,P ,N 〉R) returns QX′(∅, 〈K,B,P ,N 〉R) iff there is some minimal conflict C ⊃ ∅ w.r.t. 〈K,B,P ,N 〉R.\nCorollary 4.3. QX(〈K,B,P ,N 〉R) returns QX′(∅, 〈K,B,P ,N 〉R) iff 〈K,B,P ,N 〉R is an admissible DPI.\nProof. By the third proposition of Lemma 4.2 and Proposition 4.1 we have that QX(〈K,B,P ,N 〉R) returns QX′(∅, 〈K,B,P ,N 〉R) iff K is invalid w.r.t. 〈·,B,P ,N 〉R. By Proposition 4.2, we can then conclude that QX(〈K,B,P ,N 〉R) returns QX′(∅, 〈K,B,P ,N 〉R) iff 〈K,B,P ,N 〉R is an admissible DPI.\nThe input arguments (at any call) to QX′ are (a) some subset C of the original input KB Korig to QX and (b) a DPI 〈K,B,P ,N 〉R where K ⊆ Korig and B ⊇ Borig.\nThe principle of QX′ relies on the following fact.\nLemma 4.3. [36] LetK1,K2 be a partition ofK. If C2 is a minimal conflict set w.r.t. 〈K2,B∪K1,P ,N 〉R and C1 is a minimal conflict set w.r.t. 〈K1,B ∪ C2,P ,N 〉R, then C1 ∪ C2 is a minimal conflict set w.r.t. 〈K1 ∪ K2,B,P ,N 〉R = 〈K,B,P ,N 〉R.\nCHAPTER 4. DIAGNOSIS COMPUTATION 42\nProof. Since C1 is a minimal conflict set w.r.t. 〈K1,B ∪ C2,P ,N 〉R, we have that C1 is invalid w.r.t. 〈·,B ∪ C2,P ,N 〉R. From that we obtain that C1 ∪ C2 must be invalid w.r.t. 〈·,B,P ,N 〉R. Further on, by the fact that K1,K2 partition K we have that C1 ⊆ K1 ⊆ K since C1 is a minimal conflict set w.r.t. 〈K1,B ∪ C2,P ,N 〉R and C2 ⊆ K2 ⊆ K since C2 is a minimal conflict set w.r.t. 〈K2,B ∪ K1,P ,N 〉R. Consequently, C1∪C2 ⊆ Kmust be true. So, by Corollary 4.1, C1∪C2 is a conflict set w.r.t. 〈K,B,P ,N 〉R.\nTo show the minimality of C1 ∪ C2, assume that C ⊂ C1 ∪ C2 is a minimal conflict set w.r.t. 〈K, B,P ,N 〉R. Due to K1 ∩ K2 = ∅ and C1 ⊆ K1 and C2 ⊆ K2, it must hold that C1 ∩ C2 = ∅. Thus, (1) C ∩ C1 ⊂ C1 or (2) C ∩ C2 ⊂ C2.\nLet us assume (1) holds. Then, C is invalid w.r.t. 〈·,B,P ,N 〉R, i.e. C∪B∪UP = (C′1∪C2)∪B∪UP = C′1∪(B∪C2)∪UP violates some r ∈ R or some n ∈ N where C′1 ⊂ C1. This, however, is a contradiction to the minimality of the conflict set C1 w.r.t. 〈K1,B ∪ C2,P ,N 〉R.\nNow, let us assume (2) holds. Then, C is invalid w.r.t. 〈·,B,P ,N 〉R, i.e. C ∪ B ∪ UP = (C1 ∪ C′2) ∪ B ∪ UP violates some r ∈ R or some n ∈ N where C′2 ⊂ C2. By monotonicity of L and C1 ⊆ K1, this implies C′2 ∪ (K1 ∪ B) ∪ UP violates some r ∈ R or some n ∈ N , i.e. C′2 ⊂ K2 is a conflict set w.r.t. 〈K2,B ∪ K1,P ,N 〉R which is a contradiction due to C′2 ⊂ C2 and the minimality of the conflict set C2 w.r.t. 〈K2,B ∪ K1,P ,N 〉R.\nQX′(C, 〈K,B,P ,N 〉R) computes a minimal conflict set w.r.t. 〈K,B,P ,N 〉R in a divide-and-conquer fashion whereby the argument C is the set of sentences of Korig that has been added to B in the current iteration. That is, in this iteration QX′ will output either (1) ∅ if the current B (which includes C) already contains a minimal conflict set w.r.t. the original DPI 〈Korig,Borig,P ,N 〉R or (2) a minimal conflict set w.r.t. the current DPI 〈K,B,P ,N 〉R (i.e. a subset of a minimal conflict set w.r.t. the original DPI) which does not include any sentence from C.\nLemma 4.4.\n1. For each call QX′(C, 〈K,B,P ,N 〉R) within Algorithm 1 it holds that C ⊆ B.\n2. If QX′(C, 〈K,B,P ,N 〉R) is called in line 16 of Algorithm 1, C 6= ∅ holds.\n3. If QX′(C, 〈K,B,P ,N 〉R) returns ∅, then there is some non-empty minimal conflict set w.r.t. 〈C,B\\ C,P ,N 〉R.\n4. If QX′(C, 〈K,B,P ,N 〉R) returns ∅, then ∅ is the only minimal conflict set w.r.t. 〈K,B,P ,N 〉R.\n5. QX′(C, 〈K,B,P ,N 〉R) terminates.\nProof. 1): There are three situations when QX′(C, 〈K,B,P ,N 〉R) is called within Algorithm 1, namely in lines 7, 16 and 17. In line 7, C := ∅ ⊆ B holds. In line 16, C := K1 ⊆ B ∪ K1 =: B holds. In line 17, C := C2 ⊆ B ∪ C2 =: B holds.\n2): In line 16, QX′ is called with C := K1, which is always not the empty set due to the definition of the SPLIT function in line 13 that is used to extract K1 from K.\n3): The first observation is that QX′(C, 〈K,B,P ,N 〉R) cannot return ∅ if C = ∅ as in this case the first condition in line 9 is not met. Thus, in particular, QX′ cannot return ∅ if called in line 7.\nSo, ∅ can be returned by QX′(C, 〈K,B,P ,N 〉R) only if it is called (1) in line 16 or (2) in line 17. If QX′(C, 〈K,B,P ,N 〉R) returns ∅, then C 6= ∅ and B is invalid w.r.t. 〈·, ∅,P ,N 〉R (line 9), i.e. B contains a minimal conflict set w.r.t. 〈B, ∅,P ,N 〉R which is non-empty by Proposition 4.2 since 〈B, ∅,P ,N 〉R is an admissible DPI by admissibility of the input DPI and the invariance of P ,N ,R throughout QX′. Additionally, C ⊆ B holds by the first proposition of this lemma. Now, assume that there is no non-empty (minimal) conflict set w.r.t. 〈C,B \\ C,P ,N 〉R. Then, for each minimal conflict set\nCHAPTER 4. DIAGNOSIS COMPUTATION 43\nC′ (which we know is non-empty) w.r.t. 〈B, ∅,P ,N 〉R it must hold that C ∩ C′ = ∅, i.e. there is already a non-empty minimal conflict set w.r.t. 〈B \\ C, ∅,P ,N 〉R.\nCase (1): Let us assume first that the call to QX′ was made in line 16. Then, before this call to QX′, B was exactly B \\ C. By the second proposition of this lemma, C 6= ∅ as QX′ was called in line 16. Thus, before the current call to QX′, the algorithm must have already returned ∅ (both conditions in line 9 are met) in line 10 which is a contradiction to the assumption that QX′(C, 〈K,B,P ,N 〉R) was called in line 16.\nCase (2): Now, assume that the call to QX′(C2, 〈K1,B ∪ C2,P ,N 〉R) was made in line 17. Then C2 is the result of the call to QX′(K1, 〈K2,B ∪ K1,P ,N 〉R) in line 16. By the argumentation above, we have that C2 6= ∅ and there is a non-empty minimal conflict set w.r.t. 〈B ∪ C2, ∅,P ,N 〉R. Moreover, we have that there is a non-empty minimal conflict set w.r.t. 〈B, ∅,P ,N 〉R. However, as QX′(K1, 〈K2,B ∪ K1,P ,N 〉R) in line 16 did not return ∅ andK1 6= ∅ by the second proposition of this lemma, it must hold that B ∪ K1 is valid w.r.t. 〈·, ∅,P ,N 〉R, i.e. there is no (minimal) conflict set w.r.t. 〈B ∪ K1, ∅,P ,N 〉R. By monotonicity of L, this is a contradiction to the fact that there is a non-empty minimal conflict set w.r.t. 〈B, ∅,P ,N 〉R.\n4): Assume QX′(C, 〈K,B,P ,N 〉R) returns ∅ and there is some non-empty minimal conflict set w.r.t. 〈K,B,P ,N 〉R. Since ∅ is returned, both conditions in line 2 must be met, i.e. in particular B must be invalid w.r.t. 〈·, ∅,P ,N 〉R which means that 〈K,B,P ,N 〉R is not admissible. By Proposition 4.2, there cannot be a non-empty (minimal) conflict set w.r.t. 〈K,B,P ,N 〉R. This yields a contradiction.\n5): QX′(C, 〈K,B,P ,N 〉R) either returns ∅ in line 10 iff the conditions in line 9 are met or otherwise returns K in line 12 iff |K| = 1 or otherwise calls itself recursively in lines 16 and 17. However, for each recursive call QX′(C′, 〈K′,B′,P ,N 〉R) within QX′(C, 〈K,B,P ,N 〉R) it holds that K′ ⊂ K as K′ ∈ {K1,K2} and K1,K2 ⊂ K due to the definition of the SPLIT function in line 13 that is used to computeK1 andK2 fromK in lines 14 and 15. Hence, each recursive call must finally reach the stopping criterion |K| = 1 and return K if it does not reach the stopping criterion in line 9 before.\nLemma 4.5. Let 〈K,B,P ,N 〉R be an admissible DPI. If QX′(C, 〈K,B,P ,N 〉R) is called, then at least one of the immediate recursive calls of QX′ in line 16 or line 17 is given an admissible DPI as argument.\nProof. Let us assume that 〈K,B,P ,N 〉R is an admissible DPI. Within QX′(C, 〈K,B, P ,N 〉R), the immediate recursive call is QX′(K1, 〈K2,B∪K1,P ,N 〉R) in line 16 and QX′(C2, 〈K1,B∪C2,P ,N 〉R) in line 17 where K1,K2 is a partition of K and C2 is the result of QX′(K1, 〈K2,B ∪ K1,P ,N 〉R). If 〈K2,B ∪ K1,P ,N 〉R is admissible, then the proposition of the lemma is fulfilled. So, assume that that 〈K2,B ∪K1,P ,N 〉R is not admissible. Due to this non-admissibility, it must hold that B ∪K1 is invalid w.r.t. 〈·, ∅,P ,N 〉R, so the second condition in line 2 is met. As the call to QX′(K1, 〈K2,B∪K1,P ,N 〉R) was made in line 16, it must be true by Lemma 4.4, prop. 2 that K1 6= ∅ wherefore the first condition in line 2 is met as well. Thus, the result of the call of QX′ in line 16 must be ∅. So, the call of QX′ in line 17 looks like QX′(∅, 〈K1,B,P ,N 〉R). However, the DPIs 〈K1,B,P ,N 〉R and 〈K,B,P ,N 〉R are identical except for the first entries, i.e. K1 and K. We know that the latter DPI is admissible. Due to the fact that admissibility of a DPI is defined independently of the KB (the first entry of the DPI tuple), we have that 〈K1,B,P ,N 〉R must be admissible. This completes the proof.\nAs long as the algorithm goes downwards in the recursion tree (and has never gone upwards), (1) the invariant that a minimal conflict set exists for each recursive call to QX′ holds, (2) each call to QX′ that returns, returns a singleton or empty set and (3) the two calls to QX′ immediately before going upwards in the recursion tree for the first time must both return either a singleton or an empty set.\nLemma 4.6 (QX: Downwards Correctness). Let 〈K,B,P ,N 〉R be an admissible DPI and let there be a non-empty minimal conflict set w.r.t. 〈K,B,P ,N 〉R. Then, the following propositions hold:\n1. Before line 18 has ever been reached during the execution of QX′(C, 〈K, B,P , N 〉R), the following holds: If some call to QX′(C′, 〈K′,B′,P ,N 〉R) returns a set S, then S = ∅ or |S| = 1.\nCHAPTER 4. DIAGNOSIS COMPUTATION 44\n2. Before line 18 has ever been reached during the execution of QX′(C, 〈K, B,P , N 〉R), the following holds: If QX′(C′, 〈K′,B′,P ,N 〉R) is recursively called, then there is some non-empty minimal conflict set w.r.t. 〈K′ ∪ C′,B′ \\ C′,P ,N 〉R.\n3. Before line 18 has ever been reached during the execution of QX′(C, 〈K,B,P , N 〉R), the following holds: If some call to QX′(C′, 〈K′,B′,P ,N 〉R) returns a set S, then S is a minimal conflict set w.r.t. 〈K,B,P ,N 〉R.\n4. When line 18 is reached for the first time, each of the calls to QX′ immediately before in lines 16 and 17 must have returned ∅ or some K with |K| = 1.\nProof. 1): Assume the opposite, i.e. some call to QX′(C′, 〈K′,B′,P ,N 〉R) returns a set S with |S| > 1 before line 18 has ever been reached. There are three places where QX′ can return, namely in line 10, in line 12 or in line 18. However, in line 10, only ∅ and in line 12 only a singleton set can be returned. That is, S must be returned in line 18 which is a contradiction to the assumption that line 18 has not yet been reached.\n2): Induction Base: The first recursive call QX′(C′, 〈K′,B′,P ,N 〉R) can only occur at line 16 where C′ = K1, K′ = K2 and B′ = B ∪ K1 and K1,K2 is a partition of K as per the definition of the SPLIT and GET functions in lines 13-15. So, K′ ∪ C′ = K and B′ \\ C′ = B. The latter holds since C′ ⊆ K and for each DPI K ∩ B = ∅ holds by Definition 3.1. As there is a non-empty minimal conflict set w.r.t. 〈K,B,P ,N 〉R we have that there is a non-empty minimal conflict set w.r.t. 〈K′ ∪ C′,B′ \\ C′,P ,N 〉R by the fact that 〈K,B,P ,N 〉R = 〈K′ ∪ C′,B′ \\ C′,P ,N 〉R. Thus, the existence of a non-empty minimal conflict set w.r.t. 〈K′ ∪ C′,B′ \\ C′,P ,N 〉R is given during the execution of the first recursive call to QX′.\nInduction Assumption: Now, let us assume that the existence of a non-empty minimal conflict set w.r.t. 〈K ∪ C,B \\ C,P ,N 〉R is given during some call QX′(C, 〈K,B,P , N 〉R). The goal is now to show that the existence of a non-empty minimal conflict set w.r.t. 〈K′ ∪ C′,B′ \\ C′,P ,N 〉R is given during any recursive call QX′(C′, 〈K′,B′,P ,N 〉R) that is invoked during execution of QX′(C, 〈K,B,P ,N 〉R).\nInduction Step: Now, there are three cases where this recursive call to QX′ can take place, namely (1) in line 16, (2) in line 17 where the result of QX′ in line 16 is C2 = ∅ and (3) in line 17 where the result of QX′ in line 16 is some C2 with |C2| = 1. The case where some C2 with |C2| > 1 is returned by QX′ in line 16, is impossible due to the assumption that line 18 has not yet been reached and the first proposition of this lemma.\nCase (1): Let us assume that the call QX′(C′, 〈K′,B′,P ,N 〉R) is made in line 16. Since that call is made within QX′(C, 〈K,B,P ,N 〉R), it must hold that some condition in line 2 during QX′(C, 〈K,B,P , N 〉R) is violated, as otherwise a return would have taken place in line 10 which is a contradiction to the assumption that QX′(C′, 〈K′,B′,P ,N 〉R) is called in line 16.\nLet us first assume that C = ∅ holds. In this case, the first condition in line 2 is violated and, by the Induction Assumption, it is true that there is a non-empty minimal conflict set w.r.t. the DPI 〈K ∪ C,B \\ C,P ,N 〉R which is equal to the DPI 〈K,B,P ,N 〉R by C = ∅. So, an equal argumentation to the one of the Induction Base can be applied to derive that there is a non-empty minimal conflict set w.r.t. 〈K′ ∪ C′,B′ \\ C′,P ,N 〉R.\nIf C 6= ∅ holds, on the other hand, then the first condition in line 2 is satisfied wherefore the second condition in line 2 must be violated. That is, there is no conflict set w.r.t. 〈B, ∅,P ,N 〉R. As there is a non-empty minimal conflict set w.r.t. 〈K ∪ C,B \\ C,P ,N 〉R by the Induction Assumption, C ⊆ B by Lemma 4.4, prop. 1 and |K| ≥ 2 by the fact that there was no return in line 12, there must be a non-empty minimal conflict set w.r.t. 〈K,B,P ,N 〉R. Again, an equal argumentation to the one of the Induction Base can be applied to derive that there is a non-empty minimal conflict set w.r.t. 〈K′ ∪ C′,B′ \\ C′,P ,N 〉R.\nCase (2): Here, we assume that the recursive call QX′(C′, 〈K′,B′,P ,N 〉R) is made in line 17 and the result of QX′ in line 16 is C2 = ∅. So, it holds that C′ = C2 = ∅, K′ = K1 and B′ = B, i.e. the\nCHAPTER 4. DIAGNOSIS COMPUTATION 45\nrecursive call can be written as QX′(∅, 〈K1,B,P ,N 〉R). By the fact that QX′(K1, 〈K2,B∪K1,P ,N 〉R) called in line 16 returned ∅, both conditions in line 2 during QX′(K1, 〈K2,B ∪ K1,P ,N 〉R) must have been met. Thus, in particular the existence of a non-empty minimal conflict set w.r.t. 〈B ∪K1, ∅,P ,N 〉R must be given. Further on, by the Induction Assumption there is a non-empty minimal conflict set w.r.t. 〈C ∪ K,B \\ C,P ,N 〉R.\nLet us first assume C = ∅. In this case 〈C ∪ K,B \\ C,P ,N 〉R can be written as 〈K,B,P ,N 〉R and it holds that there is a non-empty minimal conflict set w.r.t. 〈K,B,P ,N 〉R, i.e. K is invalid w.r.t. 〈·,B,P ,N 〉R. By Proposition 4.2, this implies that 〈K,B,P ,N 〉R is admissible. In other words, there is no conflict set w.r.t. 〈B, ∅,P ,N 〉R. Consequently, there must be a non-empty minimal conflict set w.r.t. 〈K1,B,P ,N 〉R.\nIf C 6= ∅, on the other hand, then the second condition in line 2 during QX′(C, 〈K,B,P ,N 〉R) must be invalid, i.e. there is no conflict set w.r.t. 〈B, ∅,P ,N 〉R. Consequently, there must be a non-empty minimal conflict set w.r.t. 〈K1,B,P ,N 〉R.\nCase (3): Here, we assume that the recursive call QX′(C′, 〈K′,B′,P ,N 〉R) is made in line 17 and the result of QX′ in line 16 is C2 6= ∅. As C2 6= ∅ and line 18 has never been reached by assumption, C2 must have been returned in line 12 of QX′(K1, 〈K2,B ∪K1,P ,N 〉R) (which was called in line 16) wherefore C2 = K2 must hold. So, it holds that C′ = K2, K′ = K1 and B′ = B ∪ K2, i.e. the recursive call can be written as QX′(K2, 〈K1,B ∪ K2,P ,N 〉R). By the Induction Assumption, there is a non-empty minimal conflict set w.r.t. 〈C ∪ K,B \\ C,P ,N 〉R. Moreover, C ⊆ B by Lemma 4.4, prop. 1 and (*) there is a nonempty minimal conflict set w.r.t. the DPI 〈K,B,P ,N 〉R which is equal to the DPI 〈K1 ∪ K2,B,P ,N 〉R by the fact that K1,K2 partition K as per the definition of the SPLIT and GET functions in lines 13-15.\nWhat must still be proven, is (*): Let us first assume that C = ∅ holds. In this case, 〈C ∪ K,B \\ C,P ,N 〉R = 〈K,B,P ,N 〉R and thus there is a non-empty minimal conflict set w.r.t. 〈K,B,P ,N 〉R.\nIf C 6= ∅, on the other hand, then the second condition in line 2 during QX′(C, 〈K,B,P ,N 〉R) must be invalid as otherwise ∅ would have been returned which is a contradiction to the assumption that the recursive call QX′(C′, 〈K′,B′,P ,N 〉R) was invoked in line 17. So, there is no conflict set w.r.t. 〈B, ∅,P ,N 〉R. Consequently, there must be a non-empty minimal conflict set w.r.t. 〈K,B,P ,N 〉R due to C ⊆ B by Lemma 4.4, prop. 1.\n3): Case S 6= ∅: By S 6= ∅ and the fact that line 18 has not yet been reached, we obtain by the first proposition of this lemma that |S| = 1 must hold.\nThere are two cases that can trigger QX′(C, 〈K,B,P ,N 〉R) to return K with |K| = 1, i.e. case 1 involving C 6= ∅ and case 2 involving C = ∅.\nIn case 1, B must be valid w.r.t. 〈·, ∅,P ,N , 〉R as otherwise ∅ would be returned in line 10. So, there is no (minimal) conflict set w.r.t. 〈B, ∅,P ,N 〉R.\nAs |K| = 1 by assumption and by the fact that C ⊆ B (holds by Lemma 4.4, prop. 1) and there is some non-empty minimal conflict set w.r.t. 〈K∪C,B\\C,P ,N 〉R (holds by the second proposition of this lemma), K must include a non-empty minimal conflict set w.r.t. 〈K,B,P ,N 〉R. Since the only proper subset of K is the empty set, K must be a minimal conflict set w.r.t. 〈K,B,P ,N 〉R.\nCase 2 can arise only when QX′(C, 〈K,B,P ,N 〉R) is called in line 7 or line 17. In line 16 QX′ is called with C 6= ∅ by Lemma 4.4, prop. 2.\nIn line 7 QX′ is called with C = ∅ and, by Corollary 4.3, with an admissible DPI 〈K,B,P ,N 〉R for which a non-empty minimal conflict set exists as arguments. By the second proposition of this lemma, there is some non-empty minimal conflict set w.r.t. 〈K ∪ ∅,B \\ ∅,P ,N 〉R = 〈K,B,P ,N 〉R, and, by admissibility of 〈K,B,P ,N 〉R, there is no (minimal) conflict set w.r.t. 〈B, ∅,P ,N 〉R. By |K| = 1, K must be a minimal conflict set w.r.t. 〈K,B,P ,N 〉R.\nA necessary condition for QX′ to be called with C = ∅ in line 17 is obviously that QX′(K1, 〈K2,B ∪ K1,P ,N 〉R) called in line 16 returns ∅. By the Lemma 4.4, prop. 3, there is some non-empty minimal conflict set w.r.t. 〈K1,B,P ,N 〉R. In line 17, the call QX′(∅, 〈K1,B,P ,N 〉R) is made which, by assumption, returns K1 with |K1| = 1. That means K1 is a minimal conflict set w.r.t. 〈K1,B,P ,N 〉R.\nCHAPTER 4. DIAGNOSIS COMPUTATION 46\nCase S = ∅: Here, both conditions in line 2 must be met, i.e. in particular B is invalid w.r.t. 〈·, ∅,P ,N 〉R which implies thatK is invalid w.r.t. 〈·,B,P ,N 〉R and 〈K,B,P ,N 〉R is admissible. Therefore, by Proposition 4.2, there is no non-empty minimal conflict set w.r.t. 〈K,B,P ,N 〉R. However, since K is invalid w.r.t. 〈·,B,P ,N 〉R, there must be a conflict set w.r.t. 〈K,B,P ,N 〉R. So, there is only the empty minimal conflict set w.r.t. 〈K,B,P ,N 〉R.\n4): This proposition is an immediate consequence of the first proposition of this lemma.\nLemma 4.7. Let 〈K,B,P ,N 〉R be a non-admissible DPI. Then, ∅ is the only minimal conflict set w.r.t. 〈K,B,P ,N 〉R and QX′(C, 〈K,B,P ,N 〉R) with C 6= ∅ returns ∅ immediately in line 10.\nProof. Since 〈K,B,P ,N 〉R is non-admissible, B ∪ UP violates some r ∈ R or B ∪ UP |= n for some n ∈ N . Therefore, ∅ is invalid w.r.t. 〈·,B,P ,N 〉R, which, by Corollary 4.1, implies that ∅ is a (minimal) conflict set w.r.t. 〈K,B,P ,N 〉R.\nQX′(C, 〈K,B,P ,N 〉R) returns ∅ in line 10 as both conditions in line 9 are satisfied due to C 6= ∅ and the non-admissibility of 〈K,B,P ,N 〉R.\nLemma 4.8. Let 〈K,B,P ,N 〉R be an admissible DPI. Then QX′(C, 〈K,B,P ,N 〉R) does not return in line 10.\nProof. By Definition 3.6, B must be valid w.r.t. 〈·, ∅,P ,N 〉R. Hence, the second condition in line 9 is not satisfied wherefore a return cannot take place in line 10.\nLemma 4.9. Let 〈K,B,P ,N 〉R be an admissible DPI and let there be a non-empty minimal conflict set w.r.t. 〈K,B,P ,N 〉R. Then the following holds: When QX′(C, 〈K,B,P ,N 〉R) reaches line 18 for the first time, C1 ∪ C2 is a non-empty minimal conflict set w.r.t. 〈K,B,P ,N 〉R.\nProof. The premises of this lemma are the same as those of Lemma 4.6. By Lemma 4.6, prop. 4 we know that for C2 and C1 that are returned by the the calls to QX′ in lines 16 and 17 |C1| ≤ 1 and |C2| ≤ 1 holds. Moreover, we know by Lemma 4.3 that C1 ∪ C2 is a minimal conflict set w.r.t. 〈K,B,P ,N 〉R.\nWhat remains open is to show that C1 ∪ C2 6= ∅. To this end, we first assume that C 6= ∅. Then, by Lemma 4.7, 〈K,B,P ,N 〉R must be an admissible DPI since it does not return in line 10, but only in line 18.\nIf, on the other hand, C = ∅ holds, we can apply Lemma 4.6, prop. 2 to obtain that there is a non-empty minimal conflict set w.r.t. 〈K,B,P ,N 〉R. This implies that K is invalid w.r.t. 〈·,B,P ,N 〉R. Therefore, we can conclude by means of Proposition 4.2 that 〈K,B,P ,N 〉R is an admissible DPI.\nThus, in both cases we have that 〈K,B,P ,N 〉R is an admissible DPI. Applying Lemma 4.5 yields that at least one recursive call to QX′ in lines 16 and 17 is given an admissible DPI as argument. By Lemma 4.8, this call cannot return in line 10. So, it must return in line 12 by the assumption that line 18 has not yet been reached before, wherefore it must return a set of cardinality 1. This completes the proof.\nAs long as the algorithm goes upwards after going upwards for the first time, a non-empty minimal conflict set is propagated upwards.\nLemma 4.10 (QX: Upwards Correctness). Let 〈K,B,P ,N 〉R be an admissible DPI and let there be a non-empty minimal conflict set w.r.t. 〈K,B,P ,N 〉R. Then: After QX′(C, 〈K,B,P ,N 〉R) has reached line 18 for the first time, the following holds: As long as line 16 is not reached, each return in line 18 returns a minimal conflict set w.r.t. 〈K,B,P ,N 〉R.\nProof. The premises of this lemma are the same as those of Lemma 4.6. By Lemma 4.9 we know that a non-empty minimal conflict C set is returned at the first return that is made in line 18. As, by assumption, C is not the result C2 of a prior call to QX′ in line 16, it must be the result C1 of a prior call to QX′\nCHAPTER 4. DIAGNOSIS COMPUTATION 47\nin line 17. Since the premises of Lemma 4.6 are fulfilled, Lemma 4.6 can be applied. Since the call QX′(K1, 〈K2,B∪K1,P ,N 〉) (that returned C2) in line 16 took place before line 18 was first reached, we have that C2 is a minimal conflict set w.r.t. 〈K2,B ∪ K1,P ,N 〉 by Lemma 4.6, prop. 3. By Lemma 4.3, we have that C2 ∪ C is a minimal conflict set w.r.t. 〈K,B,P ,N 〉. As long as line 16 is not reached, the same argumentation can be used to show that a minimal conflict set is returned in line 18.\nWhen the algorithm goes downwards again after going upwards for the first time, the invariant that that a minimal conflict set exists for each recursive downwards call to QX′ holds.\nLemma 4.11 (QX: Downwards-after-upwards Correctness). Let 〈K,B,P ,N 〉R be an admissible DPI and let there be a non-empty minimal conflict set w.r.t. 〈K,B,P ,N 〉R. Then: After QX′(C, 〈K,B,P , N 〉R) has reached line 18 for the first time, the following holds: If line 16 is reached for the first time, then, if the DPI 〈K1,B∪C2,P ,N 〉R which is the argument to the immediate call QX′(C2, 〈K1,B∪C2,P ,N 〉R) in line 17 is admissible, then there is a non-empty minimal conflict set w.r.t. 〈K1,B ∪ C2,P ,N 〉R.\nProof. The premises of this lemma are the same as those of Lemma 4.6. Since line 16 is first reached after line 18 has been reached for the first time, it must hold that QX′(K1, 〈K2,B ∪ K1,P ,N 〉R) in line 16 was called before line 18 has been reached. The reason for this to hold is the fact that only returns and no new calls to QX′ can have been made between the first occurrence of line 18 and the next occurrence of line 16.\nTherefore, the result C2 of the call QX′(K1, 〈K2,B ∪K1,P ,N 〉R) in line 16 is a minimal conflict set w.r.t. 〈K2,B ∪ K1,P ,N 〉R due to Lemma 4.6, prop. 3. As a consequence, C2 ∪ B ∪ K1 ∪ UP violates some r ∈ R or some N ∈ N . As the DPI 〈K1,B ∪ C2,P ,N 〉R is admissible by assumption, it holds that C2 ∪ B ∪ UP does not violate any r ∈ R or N ∈ N . Hence, K1 must be invalid w.r.t. 〈·,B ∪ C2,P ,N 〉R which implies that there must be a non-empty minimal conflict set S w.r.t. 〈K1,B ∪ C2,P ,N 〉R.\nBy applying the argumentation of Lemmas 4.6, 4.10 and 4.11 recursively on the entire recursion tree, we can prove the correctness of QX′.\nLemma 4.12. If QX′(C, 〈Korig,Borig,P ,N 〉R) is called in line 7 by Algorithm 1, it returns a non-empty minimal conflict set w.r.t. 〈Korig,Borig,P ,N 〉R.\nProof. If QX′(C, 〈Korig,Borig,P ,N 〉R) is called in line 7 of Algorithm 1, it must be true, by Lemma 4.2, prop. 4.2 and Corollary 4.3, that 〈Korig,Borig,P ,N 〉R is an admissible DPI for which a non-empty minimal conflict set exists. As a consequence, the premises of Lemma 4.6 are met for 〈Korig,Borig,P ,N 〉R.\nThere are two cases to consider: Either (a) |Korig| ≤ 1 or (b) |Korig| > 1 for the initial call to QX′(C, 〈Korig,Borig,P ,N 〉R) in line 7. In case (a), 0 = |Korig| < 1 cannot hold as there must be a nonempty minimal conflict set C w.r.t. 〈Korig,Borig,P ,N 〉R due to Lemma 4.2, prop. 4.2. Since ∅ ⊂ C ⊆ Korig must hold for C, this would be a contradiction to |Korig| = 0.\nSo, |Korig| = 1 holds in case (a). In this case, QX′ returns Korig immediately in line 12, since C = ∅ and thus the conditions checked in line 9 cannot be met. In this case, Korig is indeed a nonempty minimal conflict set since for the DPI 〈Korig,Borig,P ,N 〉R given as argument there is a non-empty minimal conflict set by Lemma 4.2, prop. 4.2. Therefore ∅ cannot be a conflict set w.r.t. this DPI whereby Korig is the only possible minimal conflict set due to |Korig| = 1.\nCase (b): In this case, a direct return can neither take place in line 10 by C = ∅ nor in line 12 by |Korig| > 1. So, QX′ is called recursively in lines 16 and 17. Since QX′ terminates due to Lemma 4.2, prop. 5, QX′ must reach line 18. The first time some recursive call QX′(C, 〈K,B,P ,N 〉R) reaches line 18, it returns a non-empty minimal conflict set w.r.t. 〈K,B,P ,N 〉R due to Lemma 4.9.\nBy Lemma 4.10, as long as line 16 is not reached, i.e. no “left branch” (call to QX′ in line 16) but only “right branches” (calls to QX′ in line 17) return, a minimal conflict set S is returned for each call to QX′ that “wraps” (is higher in the recursion tree than) the call that was the first to reach line 18. It holds that S 6= ∅ since S is a union of sets including the non-empty set returned when line 18 was first reached.\nCHAPTER 4. DIAGNOSIS COMPUTATION 48\nWhen it comes to an execution of line 16, i.e. the left branch returns, then the algorithm will take the right branch by executing line 17, i.e. calling QX′(C2, 〈K1,B ∪ C2,P ,N 〉R), and go downwards in the recursion tree.\nNow, there are two cases. First, 〈K1,B ∪ C2,P ,N 〉R is non-admissible. Then, by Lemma 4.7, there is only one minimal conflict set w.r.t. 〈K1,B∪C2,P ,N 〉R, namely ∅, and QX′(C2, 〈K1,B∪C2,P ,N 〉R) directly returns ∅. As also the result C2 of the call to QX′(K1, 〈K2,B∪K1,P ,N 〉R) immediately before in line 16 is a minimal conflict set w.r.t. 〈K2,B∪K1,P ,N 〉R, as established above, we can apply Lemma 4.3 to derive that indeed a minimal conflict set w.r.t. 〈K,B,P ,N 〉R is returned in line 18. Thus, Lemma 4.10 can be further applied to move upwards in the recursion tree until line 16 occurs again.\nSecond, 〈K1,B ∪ C2,P ,N 〉R is admissible. Then, by Lemma 4.11, there is a non-empty minimal conflict set w.r.t. 〈K1,B ∪ C2,P ,N 〉R. Hence, Lemma 4.6 can be used again for the subtree of the recursion tree rooted at the call QX′(C2, 〈K1,B ∪ C2,P ,N 〉R). That is, it can be used to show that each call to QX′ within this subtree returns a minimal conflict set w.r.t. the DPI given as argument as long as the algorithm moves downwards in the tree. Having reached line 18 for the first time, Lemma 4.9 lets us conclude again that a non-empty conflict set w.r.t. the respective argument DPI is actually returned at this place. Subsequently, Lemma 4.10 can be applied to show that each return gives back a minimal conflict set w.r.t. the argument DPI of the respective call, as long as the algorithm moves upwards in the recursion tree.\nWhat is still open is to show that the call QX′(C2, 〈K1,B ∪ C2,P ,N 〉R) in line 17 that is made immediately after the algorithm first reached line 16 after moving upwards after reaching line 18 for the first time returns a minimal conflict set w.r.t. 〈K1,B ∪ C2,P ,N 〉R, indeed. This holds by the fact that Lemmas 4.6 and 4.10 guarantee that a left branch always returns a minimal conflict set, Lemma 4.11 guarantees that Lemmas 4.6 and 4.10 can be applied after making a single right branch. However, as QX′ terminates the recursion tree is finite and thus the case must arise where the right branch directly returns. In case the DPI 〈K,B,P ,N 〉R given as argument for this right branch is non-admissible, the only minimal conflict set ∅ is returned, as established above. If the DPI 〈K,B,P ,N 〉R given as argument for this right branch is admissible, on the other hand, then we have already shown above that there is a non-empty minimal conflict set w.r.t. this DPI. Moreover, |K| = 1 must hold due to the fact that this right branch directly returns (without entering a further recursion). Therefore, K is returned which is actually a minimal conflict set w.r.t. 〈K,B,P ,N 〉R as K is the only non-empty subset of K.\nProposition 4.9. Let 〈K,B,P ,N 〉R be a DPI. Then, QX(〈K,B,P ,N 〉R) terminates and returns\n• ’no conflict’ iff there is no conflict w.r.t. 〈K,B,P ,N 〉R (K is valid w.r.t. 〈·,B,P ,N 〉R)\n• ∅ iff ∅ is the only minimal conflict set w.r.t. 〈K,B,P ,N 〉R (DPI is non-admissible)\n• a non-empty minimal conflict set w.r.t. 〈K,B,P ,N 〉R iff there is a non-empty minimal conflict set w.r.t. 〈K,B,P ,N 〉R (DPI is admissible and K is invalid w.r.t. 〈·,B,P ,N 〉R).\nProof. The proposition is a direct consequence of Lemma 4.2 and Lemma 4.12."
    }, {
      "heading" : "4.4 Hitting Set Tree Based Diagnosis Computation",
      "text" : "One way to compute minimal diagnoses from minimal conflict sets is to use a hitting set tree algorithm which was originally proposed by Reiter [60]. In this work we describe methods for non-interactive and interactive diagnosis computation based on the ones used in [19, 73, 74] which are closely related to the\nCHAPTER 4. DIAGNOSIS COMPUTATION 49\noriginal hitting set tree algorithm. Differences of the described non-interactive algorithm to the original one of Reiter are\n1. the usage of different edge weights (probabilities) inducing an order of node generation (uniformcost) different to breadth-first and\n2. the opportunity to specify an execution time threshold t as well as a minimal (nmin) and maximal (nmax) desired number of minimal diagnoses to be computed by the algorithm.\nIn this vein, the algorithm computes at least the nmin most-probable minimal diagnoses w.r.t. the given probabilities and goes on computing further next most-probable minimal diagnoses until either overall computation time reaches the time limit t or nmax diagnoses have been computed.\nSuch a time threshold and an interval of minimal and maximal number of diagnoses is particularly relevant in settings where not all potential minimal faulty sets need to be computed, such as iterative, interactive settings where reaction time is crucial (since a user is waiting to interact with the system). Instead, in such settings only a “representative” set of minimal diagnoses is exploited to decide which question to ask a user such that the answer to that question allows the constructed partial tree to be pruned. After pruning, the tree is expanded again to compute another “representative” set of minimal diagnoses.\nInputs. The non-interactive version of the algorithm is delineated by Algorithm 2. The algorithm takes as input an admissible DPI 〈K,B,P ,N 〉R, some computation timeout t, a desired minimal (nmin) and maximal (nmax) number of minimal diagnoses to be returned, and a function p : K → (0, 0.5) that assigns to each formula ax ∈ K a weight that represents the (estimated) likeliness of ax to be faulty and thereby determines the search strategy, e.g. breadth-first or uniform-cost. Within the algorithm, p() is used to impose an order on open nodes that tells the algorithm which node to expand next. Details concerning the function p() will be discussed in Section 4.5. Until the end of the currect section (Section 4.4) we assume that p() implies a first-in-first-out sorting of open nodes, i.e. a breadth-first search strategy as described in [60].\nAlgorithm Overview and Implementation Remarks. To compute minimal diagnoses w.r.t. 〈K,B,P , N 〉R from minimal conflict sets w.r.t. 〈K,B,P ,N 〉R, the algorithm produces a labeled tree where a nonclosed node is labeled by a minimal conflict set and a closed node is labeled by either valid or closed. From a non-closed node labeled by a minimal conflict set C = {axp, . . . , ax q} there are |C| outgoing edges, each labeled by one ax ∈ C and each leading to a new node that needs to be labeled. Closed nodes are leaf nodes of the produced tree, i.e. they have no successor nodes, and correspond to non-minimal or duplicate hitting sets (label closed) or to minimal hitting sets (label valid) of all minimal conflict sets w.r.t. the input DPI 〈K,B,P ,N 〉R. Conflict sets to label nodes are computed only on-demand for time efficiency after the attempt to reuse an already computed one fails. In case an appropriate order of node labeling (e.g. breadth-first tree construction) is used, the complete tree given when all nodes in the tree are closed contains all minimal diagnoses w.r.t. the DPI 〈K,B,P ,N 〉R provided as input. In this complete tree, the set of edge labels on each path from the root node to a node labeled by valid is a minimal diagnosis.\nWhat Algorithm 2 actually does is building up a pruned HS-tree for a given DPI. So, we next provide formal definitions of a (partial) HS-tree and a (partial) pruned HS-tree based on the definitions given in [60].\nDefinition 4.7 (HS-Tree). Let 〈K,B,P ,N 〉R be an admissible DPI. An edge-labeled and node-labeled tree T is called an HS-tree w.r.t. 〈K,B,P ,N 〉R iff it is a smallest tree with the following properties:\nCHAPTER 4. DIAGNOSIS COMPUTATION 50\n1. The root of T is labeled by valid if K is valid w.r.t. 〈·,B,P ,N 〉R. Otherwise, the root is labeled by a conflict set w.r.t. 〈K,B,P ,N 〉R.\n2. If n is a node of T , define H(n) to be the set of edge labels on the path in T from the root node to n. If n is labeled by valid, it has no successor nodes in T . If n is labeled by a conflict set C w.r.t. 〈K,B,P ,N 〉R, then for each ax ∈ C, n has a successor node nax joined to n by an edge labeled by ax . The label for nax is a conflict set C′ w.r.t. 〈K,B,P ,N 〉R such that C′ ∩H(nax ) = ∅ if such a set C′ exists. Otherwise, nax is labeled by valid.\nT is called a partial HS-tree w.r.t. 〈K,B,P ,N 〉R iff T is a HS-tree w.r.t. 〈K,B,P ,N 〉R where not all nodes in T are labeled and non-labeled nodes have no successors.\nDefinition 4.8 (Pruned HS-Tree). Let 〈K,B,P ,N 〉R be an admissible DPI. An edge-labeled and nodelabeled tree T is called a pruned HS-tree (pHS-tree) w.r.t. 〈K,B,P ,N 〉R iff T is the result of constructing an HS-tree w.r.t. 〈K,B,P ,N 〉R with due regard to the following rules:\n1. Label nodes in the HS-tree in breadth-first order.\n2. Use only minimal conflict sets w.r.t. 〈K,B,P ,N 〉R to label nodes in T .\n3. Reusing node labels: If node n is labeled by C and n′ is a node such that H(n′) ∩ C = ∅, label n′ by C.\n4. Non-minimality pruning rule: If node n is labeled by valid and node n′ is such thatH(n) ⊆ H(n′), label n′ by closed.\n5. If node n is labeled by closed, it has no successors.\n6. Duplicate pruning rule: If node n is next to be labeled and there is some node n′ such that H(n′) = H(n), then label n by closed.\nT is called a partial pruned HS-tree iff T is a pruned HS-tree where not all nodes in T have been labeled yet and non-labeled nodes have no successors.\nRemark 4.1 Notice that we use a definition of a pruned HS-tree that slightly differs from the definition given in [60] in that we inherently assume that only minimal conflict sets w.r.t. the given DPI are used to label nodes in the tree. Therefore we could omit the last rule in the definition of [60]. Namely, such a situation where some node has been labeled by a subset of the label of another node cannot arise in our definition since no minimal conflict set can be a subset of another different minimal conflict set w.r.t. the same DPI.\nIn general, there are multiple different pHS-trees w.r.t. one and the same DPI [22]. Reason for this is that\n• the order of adding successor nodes (on the same tree level) to the queue Q and\n• which of generally multiple minimal conflict sets to (re)use to label a node\nis not determined by Definition 4.8.\nBy [60, Theorem 4.8] and Proposition 4.6, the following holds:\nProposition 4.10. Let 〈K,B,P ,N 〉R be an admissible DPI and T a pHS-tree w.r.t. 〈K,B,P ,N 〉R. Then, {H(n) | n is a node of T labeled by valid} = mD〈K,B,P,N 〉R , i.e. the set of all minimal diagnoses w.r.t. 〈K,B,P ,N 〉R.\nCHAPTER 4. DIAGNOSIS COMPUTATION 51\nRemark 4.2 A node nd in Algorithm 2 is defined as the set of formulas that label the edges on the path from the root node to nd. In other words, we associate a node n with H(n). In this vein, Algorithm 2 internally does not store a labeled tree, but only “relevant” sets of nodes and conflict sets. That is, it does not store any\n• non-leaf nodes,\n• labels of non-leaf nodes, i.e. it does not store which minimal conflict set labels which node,\n• edges between nodes,\n• labels of edges and\n• leaf nodes labeled by closed.\nLet T denote the (partial) pHS-tree produced by Algorithm 2 at some point during its execution (Corollary 4.4 will show that Algorithm 2 using breadth-first search in fact produces a (partial) pHS-tree). Then, Algorithm 2 only stores\n• a set of nodes Dcalc where each node corresponds to the edge labels along a path in T leading to a leaf node that has been labeled by valid (minimal diagnoses w.r.t. 〈K,B,P ,N 〉R),\n• a list of open (non-closed) nodes Q where each node in Q corresponds to the edge labels along a path in T leading from the root node to a leaf node that has been generated, but has not yet been labeled and\n• the set Ccalc of already computed minimal conflict sets w.r.t. 〈K,B,P ,N 〉R that have been used to label non-leaf nodes in T .\nWe call 〈Dcalc,Q,Ccalc〉 the relevant data of T . If T is a pHS-tree, then Q is the empty list. This internal representation of the constructed (partial) pHS-tree by its relevant data does not constrain the functionality of the algorithm. This holds as diagnoses are paths from the root, i.e. nodes in the internal representation, and the goal of a (partial) pHS-tree is to determine minimal diagnoses w.r.t. the given DPI. The node labels or edge labels along a certain path and their order along this path is completely irrelevant when it comes to finding a label for the leaf node of this path. Instead, only the set of edge labels is required for the computation of the label for a leaf node. Also, to rule out nodes corresponding to nonminimal diagnoses, it is sufficient to know the set of already found diagnoses Dcalc. No already closed nodes are needed for the correct functionality of Algorithm 2.\nInitialization. First, Algorithm 2 initializes the variable tstart with the current system time (GETTIME), the set of calculated minimal diagnoses Dcalc to the empty set and the ordered queue of open nodes Q to a list including the empty set only (i.e. only the unlabeled root node).\nThe Main Loop. Within the loop (line 5) the algorithm gets the node to be processed next, namely the first node node (GETFIRST, line 6) in the list of open nodes Q ordered by the function pnodes() and removes node from Q (DELETEFIRST, line 7). Note that pnodes() can be directly obtained from p(). As mentioned before, for the moment the reader should simply suppose that pnodes() imposes an order on Q which effectuates a breadth-first labeling of open nodes in the tree. A definition of pnodes() will be given by Definition 4.9 after a motivation and detailed explanation of pnodes() will have been given in Section 4.5.\nCHAPTER 4. DIAGNOSIS COMPUTATION 52\nComputation of Node Labels. Then, a label is computed for node in line 8. Nodes are labeled by valid, closed or a minimal conflict set w.r.t. 〈K,B,P ,N 〉R by the procedure LABEL (line 18 ff.). This procedure gets as inputs the DPI 〈K,B,P ,N 〉R, the current node node, the set of already computed minimal conflicts (Ccalc) and minimal diagnoses (Dcalc) and the queue Q of open nodes, and it returns an updated set of computed minimal conflicts Ccalc and a label for node. It works as follows:\nA node node is labeled by closed iff (a) there is an already computed minimal diagnosis D in Dcalc that is a subset of this node, i.e. D ⊆ node, which means that node cannot be a minimal diagnosis (nonminimality criterion, lines 19-21) or (b) there is some node nd in the queue of open nodes Q such that node = nd which means that one of the two tree branches with an equal set of edge labels can be closed, i.e. removed from Q (duplicate criterion, lines 22-24).\nIf none of these closed-criteria is met, the algorithm searches for some C in Ccalc, the set of already computed minimal conflict sets, such that C ∩ node = ∅ and returns the label C for node (reuse criterion, lines 25-27). This means that the path represented by node cannot be a diagnosis as there is (at least) one minimal conflict set, namely C, that is not hit by node.\nIf the reuse criterion does not apply, a call to QX(〈K \\ node,B,P ,N 〉R) is made (line 28) in order to check whether there is a not-yet-computed minimal conflict set that is not hit by node. Note that the KB K \\ node that is given to QX as part of the argument DPI ensures that only minimal conflict sets C ⊆ K\\node can be computed, i.e. ones that do not share any single formula with node (cf. Section 4.3.1).\nRemark 4.3 A minimal conflict set computed by QX(〈K \\ node,B,P ,N 〉R) is a minimal conflict set w.r.t. 〈K,B,P ,N 〉R indeed since (i) QX(〈K \\ node,B,P ,N 〉R) returning a set C means that C is a minimal conflict set w.r.t. 〈K \\ node,B,P ,N 〉R by Proposition 4.9 and (ii) the “⇒” direction of Corollary 4.1 implies that C is not valid w.r.t. 〈·,B,P ,N 〉R and (iii) the “⇐” direction of Corollary 4.1 lets us conclude that C is a minimal conflict w.r.t. 〈X,B,P ,N 〉R where X is any superset of C, in particular X := K.\nQX may then return (a) ’no conflict’, i.e. K \\ node is already valid w.r.t. 〈·,B,P ,N 〉R, or (b) a new conflict set L 6= ∅ such that L /∈ Ccalc. Note that the case of the output L = ∅ of QX cannot arise since (i) the DPI provided as input to the algorithm is assumed to be admissible, (ii) no other DPI for which QX is called can be non-admissible since admissibility is defined only by the sets B,P ,N ,R which remain unmodified throughout the execution of Algorithm 2, and (iii) as per Proposition 4.9, QX returns ∅ only if the DPI given to it as an argument is non-admissible. Further on, we point out that the conflict set L in case (b) must be a new conflict set since the reuse criterion is always checked before the call to QX and thus must be negative. That is, each C ∈ Ccalc is hit by node and L is not hit by node wherefore L 6= C must hold for all C ∈ Ccalc.\nIn each of the described cases, the LABEL procedure returns a tuple including the respective label as explained and the set Ccalc where Ccalc is equal to the input argument Ccalc in all cases except for the case where a new minimal conflict set is computed by QX. In this case, the newly computed conflict set is added to Ccalc (line 32) before the procedure returns.\nProcessing of a Node Label. Back in the main procedure, Ccalc is updated (line 9) and then the label L returned by procedure LABEL is processed as follows:\nIf L = valid, then there is no minimal conflict set w.r.t. 〈K,B,P ,N 〉R that is not hit by (i.e. has an empty intersection with) the current node node. Thus, node is added to the set of calculated minimal diagnoses Dcalc. Minimality of diagnoses added to Dcalc is guaranteed by the pruning rule (lines 19- 21) which eliminates non-minimal nodes (paths) and the way the tree is built level by level by the used breadth-first strategy. In case a uniform-cost variant of tree construction is used, certain properties of the function p() need to be postulated to preserve this minimality guarantee. We discuss these properties in Section 4.5.\nIf, on the other hand, L = closed is the returned label of the procedure LABEL, then there is either a minimal diagnosis in Dcalc that is a subset of the current node node or a duplicate of node is already\nCHAPTER 4. DIAGNOSIS COMPUTATION 53\nincluded in Q. Consequently, node must simply be removed from Q which has already been executed in line 7.\nIn the third case, if a minimal conflict set L is returned in line 8, then L is a label for node meaning that |L| successor nodes of node need to be added to Q in sorted order using the function pnodes() (INSERTSORTED, line 15), as will be explained in more detail in Section 4.5.\nRecap. To summarize, in each iteration, the node node that is the first element of the queue Q is deleted from Q and,\n1. if node is a diagnosis, it is added to the set Dcalc\n2. if there is some diagnosis in Dcalc that is a proper subset of node or node is equal to some other node in Q, no action is performed, i.e. the algorithm deletes node without substitution\n3. if there is some minimal conflict set that node does not hit, then such a conflict set C is computed and for each ax ∈ C a new node node ∪ {ax} is added to Q.\nWe call each node nd that is added to Q in the latter case a successor of the node node.\nCorrectness of Breadth-first Diagnosis Computation For the discussion of the output of Algorithm 2 we will exploit the following result saying that Algorithm 2 computes all and only minimal diagnoses, if it executes until the queue of open nodes becomes the empty set.\nProposition 4.11 (Soundness and Completeness of Algorithm 2 using Breadth-First Search). Let 〈K, B,P ,N 〉R be an admissible DPI given as input to Algorithm 2. If Algorithm 2 using a breadth-first tree construction strategy terminates due to Q = [], then the algorithm returns exactly the set of all minimal diagnoses w.r.t. 〈K,B,P ,N 〉R.\nProof. This proposition is a consequence of Proposition 4.10 and the following Lemma 4.13 which witnesses that Algorithm 2 using a breadth-first tree construction strategy produces a pHS-tree as per Definition 4.8.\nLemma 4.13. Algorithm 2 with the admissible input DPI 〈K,B,P ,N 〉R using a breadth-first tree construction strategy is a procedure for producing a pHS-tree T w.r.t. 〈K,B,P ,N 〉R.\nProof. We verify whether all rules given by Definitions 4.7 and 4.8 are satisfied by Algorithm 2.\n• Definition 4.7, rule 1: The root node ∅ which is the only element of the initial list Q is labeled by the first call to LABEL for node := ∅ in line 8. If valid is returned, then QX(〈K,B,P ,N 〉R) must have returned ’no conflict’ which is the case if K is valid w.r.t. 〈·,B,P ,N 〉R. Otherwise, if valid is not returned by LABEL, then some minimal conflict setLw.r.t. 〈K,B,P ,N 〉R must have been returned in line 33. L is a minimal conflict set w.r.t. 〈K,B,P ,N 〉R by Proposition 4.9 and since QX(〈K,B,P ,N 〉R) has not returned ’no conflict’ as otherwise valid would have been returned contradicting our assumption and since 〈K,B,P ,N 〉R is an admissible DPI by assumption. LABEL cannot have returned earlier in line 21 or line 24, since Dcalc is the empty set and Q the empty list at this time. The former holds since Dcalc is only extended in line 11 which cannot ever have been reached before the first call to LABEL has returned. The latter holds as Q initially contained only ∅ and as ∅ was deleted from Q in line 7 before the call to LABEL was made in line 8.\nCHAPTER 4. DIAGNOSIS COMPUTATION 54\n• Definition 4.7, rule 2: Suppose a node node is labeled by valid, then it is added to Dcalc in line 11. Since node can only get a label different from closed if it is the only exemplar of this node in Q due to the duplicate criterion (lines 22-24), it must be the case that node /∈ Q (line 7) after node has been labeled by valid. Only nodes that get labeled by a conflict set can have successor nodes added to Q in line 15. Only nodes in Q can get a label (cf. lines 6 and 8). For node to be added to Q at some later point in time there must be a proper subset of node that is still in Q as each node newly added to Q is a proper superset of some node in Q (cf. line 15 which is the only position in the algorithm where nodes are added to Q). This is impossible due to the breadth-first tree construction strategy which implies that all nodes of cardinality |node| − 1 have already been labeled (and thus deleted from Q in line 7) when node is being labeled. Hence, if node is labeled by valid, then it has no successors.\nIf node is labeled by some conflict set L, then Algorithm must come to line 15, where a successor node ∪ {e} is added to Q for all e ∈ L. How node nodee := node ∪ {e} must be labeled is overridden by the rules 3, 4 and 6 of Definition 4.8 (see below).\n• Definition 4.8, rule 1: This is true by our assumption about p() and pnodes().\n• Definition 4.8, rule 2: This holds since QX(〈K\\node,B,P ,N 〉R) computes only minimal conflict sets w.r.t. 〈K,B,P ,N 〉R (cf. Remark 4.3).\n• Definition 4.8, rule 3: All minimal conflict sets that have been used to label nodes so far are stored in Ccalc. Before a minimal conflict to label node might be computed by a call to QX in line 28, the reuse criterion in lines 25-27 checks whether there is a set C in Ccalc with C ∩ node. If positive, C is returned as a label for node.\n• Definition 4.8, rule 4: This is accomplished by the non-minimality criterion in lines 19-21 which checks for existence of a node already labeled by valid which is a subset of the node to be labeled right now. All nodes labeled by valid are stored in Dcalc (cf. lines 10 and 11).\n• Definition 4.8, rule 5: If some node node is labeled by closed, then no action is performed (cf. line 12). Before each node is labeled in line 8, it is deleted from Q in line 7. That node cannot be inserted into Q at some later point in time follows from the argumentation used above to demonstrate that Definition 4.7, rule 2 is met.\n• Definition 4.8, rule 6: This is achieved by the duplicate criterion in lines 22-24 where Q is browsed for some node equal to the one that is to be labeled right now. When some node node is next to be labeled, then all duplicates of node must already be in Q as reasoned above in the argumentation to show that Definition 4.7, rule 2 is satisfied. Thus, the criterion must search for duplicates in no other collections than Q. Indeed, only one (i.e. the last non-deleted) exemplar of these duplicates of node in Q can get a label other than closed due to the duplicate criterion which closes duplicates as long as there are any.\nWe conclude that Algorithm 2 is a procedure for constructing a pHS-tree.\nBy Proposition 4.11 and the fact that there is no place in Algorithm 2 where nodes are removed from Dcalc (which implies that only minimal diagnoses can be added to Dcalc), the following corollary is obvious.\nCorollary 4.4. Algorithm 2 with the admissible input DPI 〈K,B,P ,N 〉R using a breadth-first tree construction strategy stores by 〈Dcalc,Q,Ccalc〉 the relevant data of\nCHAPTER 4. DIAGNOSIS COMPUTATION 55\n• a pHS-tree w.r.t. 〈K,B,P ,N 〉R if Algorithm 2 stops due to Q = [],\n• a partial pHS-tree w.r.t. 〈K,B,P ,N 〉R otherwise.\nIf a pHS-tree is computed in breath-first order, minimal diagnoses are generated with increasing cardinality, as the following Corollary 4.5 attests. Consequently, for the generation of all minimum cardinality diagnoses, only the first level of the tree has to be generated, where a node is labeled.\nCorollary 4.5. If Algorithm 2 using breadth-first search returns a set D of cardinality n, then D is the set of diagnoses of minimum cardinality w.r.t. the DPI 〈K,B,P ,N 〉R given as input to the algorithm. That is, if D contains some diagnosis of cardinality k, then it includes all diagnoses w.r.t. 〈K,B,P ,N 〉R of cardinality lower than k.\nProof. By Proposition 4.11, it is a fact that the algorithm computes all and only minimal diagnoses w.r.t. 〈K,B,P ,N 〉R. As these are computed in breadth-first order, the first computed diagnoses must be the minimum cardinality ones. To see this, assume that D with |D| = n is returned which includes one non-minimum cardinality diagnosis D and does not comprise a minimum cardinality diagnosis D′, i.e. |D| > |D′|. By breadth-first search, nodes are labeled in ascending order of their cardinality. And, if the first node of cardinality k is labeled, no more nodes of cardinality k − 1 can be in Q (cf. proof of Lemma 4.13). So, we have that the pHS-tree obtained by further execution of the algorithm until Q = [] can never label D′ since |D| > |D′| and D has already been labeled. Hence, the algorithm would not return D′ in its final output D. Since each minimum cardinality diagnosis is a minimal diagnosis, D′ is a minimal diagnosis. Thus, we have a contradiction to the fact that the algorithm computes all minimal diagnoses.\nOutput. The repeat-loop is iterated until the stop criterion (line 16) applies. In case at least nmin minimal diagnoses w.r.t. 〈K,B,P ,N 〉R exist, there are two cases:\n• If the finding of the nmin-th minimal diagnosis happens after t′ < t time has passed since the start of Algorithm 2, then the algorithm will continue iterating and terminate only if execution time amounts to at least t time or |D| = nmax at the time line 16 is processed.\n• Otherwise, if the detection of the nmin-th minimal diagnosis takes place after processing longer than t time, then the algorithm will terminate immediately after having determined the nmin-th minimal diagnosis.\nIn both cases, the output is a set D of minimal diagnoses w.r.t. 〈K,B,P ,N 〉R such that nmin ≤ |D| ≤ nmax and D is the set of best minimal diagnoses as per p(), in this case the set of minimal diagnoses with minimum cardinality since p() is assumed to be specified as to cause a breadth-first tree construction.\nIf fewer than nmin minimal diagnoses exist w.r.t. 〈K,B,P ,N 〉R, then Q = [] will be the cause for the algorithm to terminate. In this case, the pHS-tree w.r.t. 〈K,B,P ,N 〉R has been built up and all minimal diagnoses w.r.t. 〈K,B,P ,N 〉R are stored in Dcalc. Thus, the output is the set mD〈K,B,P,N 〉R of all minimal diagnoses w.r.t. 〈K,B,P ,N 〉R.\nTermination. The next proposition shows that Algorithm 2 must yield a set of minimal diagnoses after finite time.\nProposition 4.12. Algorithm 2 always terminates.\nProof. This is due to the fact that minimal conflict sets used to label non-leaf nodes are subsets of K and that nodes in Q are subsets of K, which is a finite set by Definition 3.1. Moreover, a node in Q is either deleted without substitution from Q if valid or closed (line 7) or deleted (line 7) and replaced by proper\nCHAPTER 4. DIAGNOSIS COMPUTATION 56\nsupersets of it (INSERTSORTED in line 15). This means that the cardinality of all nodes in Q is strictly monotonically increasing. Thus each node (path) node is guaranteed to be closed (valid or closed) when node = K as in this case node must hit all possible (minimal) conflict sets Ci w.r.t. 〈K,B,P ,N 〉R since Ci ⊆ K holds by Definition 4.1. So, after finite time the queue Q definitely becomes the empty list which is a stop criterion (line 16).\nThe argumentation so far proves the following\nProposition 4.13. Let 〈K,B,P ,N 〉R be an admissible DPI, t, nmin, nmax ∈ N and p : K → (0, 0.5) defined in a way that Q is always ordered first-in-first-out. For these inputs, Algorithm 2 always terminates and returns a set D of minimal diagnoses w.r.t. 〈K,B,P ,N 〉R which is\n• the set of the |D| minimal diagnoses of minimum cardinality w.r.t. 〈K,B,P ,N 〉R (i.e. the first |D| elements in mD〈K,B,P,N 〉R if mD〈K,B,P,N 〉R is assumed to be sorted in ascending order by cardinality) such that nmin ≤ |D| ≤ nmax, if at least nmin minimal diagnoses exist w.r.t. 〈K,B,P ,N 〉R, or\n• the set of all minimal diagnoses w.r.t. 〈K,B,P ,N 〉R, otherwise."
    }, {
      "heading" : "4.5 Diagnosis Probability Space",
      "text" : "The induction of a probability space [15] over diagnoses facilitates incorporation of well-established probability theoretic methods into the process of KB debugging; for example, a Bayesian approach [74, 63, 44] for identifying the true diagnosis, i.e. the one which leads to a solution KB with the desired semantics, by repeated measurements (see Chapter 5). Let the true diagnosis be denoted as Dt in the sequel.\nThe Probability Space of All Diagnoses. From the point of view of probability theory, a diagnosis can be viewed as an atomic event in a probability space 〈Ω, E , p〉 defined as follows:\n• Ω is the sample space consisting of all possible diagnoses w.r.t. a DPI 〈K,B,P ,N 〉R, i.e. Ω = aD〈K,B,P,N 〉R ,\n• E is a sigma-algebra on Ω, in our case the powerset 2Ω of Ω, and • p is a probability measure assigning a probability to each event in E , i.e. p : E → [0, 1] such that∑ ω∈Ω p({ω}) = 1 which means ∑ D∈aD〈K,B,P,N〉R p({D}) = 1.\nSo, p({D}) for D ∈ aD〈K,B,P,N 〉R can be seen as the probability that D is the true diagnosis, i.e. the probability of the event Dt = D (or Dt ∈ {D}). Consequently, p({D}) for D ∈ aD〈K,B,P,N 〉R is the probability distribution of the random variableDt, i.e. the probability distribution of the true diagnosis. In this vein, the probability of a set {Di, . . . ,Dj} ∈ E is interpreted as the likeliness of this set to comprise the true diagnosis Dt. That is, p({Di, . . . ,Dj}) = p(Dt ∈ {Di, . . . ,Dj}) = p(Dt = Di ∨ · · · ∨ Dt = Dj) = 0.3 means that Dt is an element of {Di, . . . ,Dj} with 30% probability. Note that singletons are often written without curly braces, i.e. p({Di}) is usually written as p(Di); we will also do so in the rest of this work.\nThe elements of the sample space Ω of a probability space are often called atomic events because they must be mutually exclusive (i.e. two atomic events cannot “happen” at the same time as an outcome of the fictive experiment a probability space describes) and exhaustive (i.e. for each “execution” of the experiment the probability space describes one atomic event must “happen”). Since the true diagnosisDt\nCHAPTER 4. DIAGNOSIS COMPUTATION 57\nAlgorithm 2 HS: Computation of Minimal Diagnoses Input: an admissible DPI 〈K,B,P ,N 〉R, a desired computation timeout t, a desired minimal (nmin) and maximal\n(nmax) number of diagnoses to be returned, a function p : K → (0, 0.5) Output: a set D which is\n(a) a set of most probable (according to p()) minimal diagnoses w.r.t. 〈K,B,P ,N 〉R such that nmin ≤ |D| ≤ nmax, if at least nmin minimal diagnoses exist w.r.t. 〈K,B,P ,N 〉R, or (b) the set of all minimal diagnoses w.r.t. 〈K,B,P ,N 〉R otherwise\n1: procedure HS(〈K,B,P ,N 〉R, t, nmin, nmax, p()) 2: tstart ← GETTIME() 3: Dcalc,Ccalc ← ∅ 4: Q← [∅] 5: repeat 6: node← GETFIRST(Q) 7: Q← DELETEFIRST(Q) 8: 〈L,C〉 ← LABEL(〈K,B,P ,N 〉R, node,Ccalc,Dcalc,Q) 9: Ccalc ← C\n10: if L = valid then 11: Dcalc ← Dcalc ∪ {node} 12: else if L = closed then . do nothing 13: else . L must be a minimal conflict set 14: for e ∈ L do 15: Q← INSERTSORTED(node ∪ {e} ,Q, pnodes()) 16: until Q = [] ∨ [|Dcalc| ≥ nmin ∧ (|Dcalc| = nmax ∨ GETTIME()− tstart > t)] 17: return Dcalc\n18: procedure LABEL(〈K,B,P ,N 〉R, node,Ccalc,Dcalc,Q) 19: for nd ∈ Dcalc do 20: if node ⊇ nd then . non-minimality 21: return 〈closed,Ccalc〉 22: for nd ∈ Q do 23: if node = nd then . remove duplicates 24: return 〈closed,Ccalc〉 25: for C ∈ Ccalc do 26: if C ∩ node = ∅ then . reuse C 27: return 〈C,Ccalc〉 28: L← QX(〈K \\ node,B,P ,N 〉R) 29: if L = ’no conflict’ then . node is a diagnosis 30: return 〈valid,Ccalc〉 31: else . L is new minimal conflict set (/∈ Ccalc) 32: Ccalc ← Ccalc ∪ {L} 33: return 〈L,Ccalc〉\nCHAPTER 4. DIAGNOSIS COMPUTATION 58\nmust be a diagnosis w.r.t. 〈K,B,P ,N 〉R and Ω by definition comprises all such diagnoses, exhaustiveness is clearly fulfilled. Mutual exclusiveness is a consequence of the fact that each diagnosisD gives complete information about the correctness of each formula axk ∈ K. In other words, Dt ∈ {D} is a shorthand for the statement that all ax i ∈ D are faulty and all ax j ∈ K \\ D are correct. Thus, any two different diagnoses are mutually exclusive events, i.e. Dt = Di implies Dt 6= Dj for all Dj ∈ aD such that Di 6= Dj .\nThe probability measure p is completely defined if a probability p(D) for each diagnosis D ∈ Ω is given. Then, by the mutual exclusiveness of events Dt ∈ {Di} and Dt ∈ {Dj} for Di 6= Dj , the probability\np(E) = ∑ D∈E p(D) (4.1)\nfor each event E ∈ E .\nRestricted Probability Spaces of Diagnoses. In many cases, only a restricted set of diagnoses w.r.t. a DPI is considered relevant for the debugging task. That is, the focus is on locating the true diagnosis among a predefined subset of all diagnoses aD〈K,B,P,N 〉R . This involves an adaptation of the probability space, in particular of the set Ω. For instance, if not the set of all, but only the set of minimal diagnoses mD〈K,B,P,N 〉R w.r.t. 〈K,B,P ,N 〉R should be considered by a debugging system – as motivated in Section 3.1 – then Ω := mD〈K,B,P,N 〉R . The other properties E = 2 Ω and ∑\nω∈Ω p({ω}) = 1 remain the same for each restricted probability space, but depend on Ω. Thus, for example, a probability p(D) for D ∈mD〈K,B,P,N 〉R ⊆ aD〈K,B,P,N 〉R must be generally defined differently, i.e. assigned a higher value, when Ω = mD〈K,B,P,N 〉R instead of Ω = aD〈K,B,P,N 〉R . This is due to the condition that all probabilities of atomic events in Ω must sum up to 1. In practice, because of the computational complexity of diagnosis computation, the used probability space will usually need to be restricted even further in that Ω comprises only a set of “leading diagnoses” which is a subset of all minimal diagnoses w.r.t. a DPI (see Section 5.1)."
    }, {
      "heading" : "4.5.1 Construction of a Probability Space",
      "text" : "Since a diagnosis constitutes an assumption about the correctness of each formula in the KB, the probability of a diagnosis D (to be the true diagnosis Dt) can be computed by means of fault probabilities of formulas. In other words, computing the probability of the event D = Dt corresponds to computing the probability of the event that exactly all formulas in D are faulty and all other formulas in the KB are correct.\nEstimating Fault Probabilities of Formulas in the KB\nNext we discuss various possibilities of how the probability of an ax ∈ K might be assessed. To this end, we first make a distinction between situations where some useful empirical data is available or not and then we differentiate between different sorts of such available data and how to take advantage of it.\nEmpirical Data is Accessible. Let us first reflect on how to utilize different empirical data sources in order to compute formula probabilities. Data can be of the following kinds (enumeration may not be complete):\n(a) Regarding formulas: Change logs of formulas in the KB\n(b) Regarding the user: Data about common mistakes of the user who has formulated the KB\nCHAPTER 4. DIAGNOSIS COMPUTATION 59\nAd (a): Prerequisite for the availability of change logs of formulas in the KB is the usage of some KB engineering software with integrated logging or change management. Examples of such KB (ontology) developing environments are Protégé [54], Web Protégé [85], SWOOP [40], OntoEdit [83] or KAON24. Given a formula ax ∈ K and its change log, the fault probability p(ax ) of this formula can be estimated by counting the number of modifications accomplished for ax in the change log. The intuition is, the more often ax has been altered, the more uncertain the (set of) author(s) might be about its correctness. This method of probability computation however suffers from a cold-start problem. If a KB is completely newly created, then such information is not available at all. On the other hand, for KBs that are being developed over a long period of time, this method can be assumed to be a rather reliable way of assessing the likeliness of formulas to be faulty. Ad (b): Clearly, data about common mistakes of a user has to be related to some type of entity that is recurrent and not dependent on a particular KB. Formulas are therefore not suitable and too coarsegrained since one and the same formula will rarely occur in many KBs. More adequate entities to relate a user fault to are predicates (terms) and logical connectives – these usually (re-)appear in many different KBs. In this way, the extrapolation and reusability of collected personal fault information of a user within one KB and between different KBs is granted.\nOne way of obtaining data about common mistakes of user u on this syntactical level is, for instance, the examination of diagnoses got as a result of past debugging sessions performed on KBs authored by u. Another way is, again, to use the change logs (if available) of formulas in KBs user u has created in the past.\nGiven such a past diagnosis D, we know that all formulas ax ∈ D that had been written by u have been confirmed to be faulty by a user. So, these formulas could be analyzed for contained predicates (terms) and logical connectives and the probability of being faulty of those syntactical constructs could be raised relative to those constructs that do not occur in formulas inD. At this, the following assumptions could be made:\n• If a formula has been confirmed to be faulty by the user, then the meaning of all predicates (terms) appearing in this formula is not correct (because in the domain that should be modeled the relationship between the predicates (terms) occurring in the formula stated by the formula must not hold). So, all predicates (terms) in ax get more suspicious of being faulty in general if ax ∈ D for some past solution diagnosis D.\n• If a formula including some logical connective is part of some past solution diagnosis, then this type of logical connective gets more suspicious of being faulty in general.\nWhen exploiting change logs of formulas authored by u, the following assumptions could be made:\n• If a formula has been modified, then a user has changed the meaning of all predicates (terms) appearing in this formula. So, all predicates (terms) in ax get more suspicious of being faulty in general if ax has been edited at least once. The more often it has been altered, the more suspicious the predicates (terms) get.\n• If some logical connective in a formula is modified, i.e. deleted or added, then this type of logical connective gets more suspicious of being faulty in general.\nThe following example should give an intuition of these assumptions:\nExample 4.5 Imagine the situation where the author of formula ax := ∀X pet(X) ↔ animal(X) ∧ (∃Y hasOwner(X,Y ) ∧ person(Y )) is known to have only vague knowledge about the predicate pet and to frequently interchange ∧ and ∨ when formulating logical formulas. This could be reflected by\n4http://kaon2.semanticweb.org/\nCHAPTER 4. DIAGNOSIS COMPUTATION 60\nthe assignment of higher fault probability to the predicate pet than to the predicates animal, hasChild and person and by raising the fault probability of ∧ as well as ∨ compared to other logical connectives available in the used logic L. Then, formula ax should intuitively have a higher probability of being faulty than, e.g., formula ax ′ = ∀X animal(X) → ¬person(X) since ax ′ does not include any of the “suspicious” terms or connectives as ax does.\nA probability of 0.25 of some predicate (term) a occurring inK could then account for the observation made in the logs that, in past debugging sessions (not necessarily related to the current KB K), every fourth formula formulated by user u which includes the term a was modified at least once. Similarly, another term b could be assigned fault probability 0.5 which could reflect that formulas formulated by u including b have been altered twice as often as formulas formulated by u comprising a. Given additionally that a occurred in two formulas formulated by u of past diagnoses whereas b did not occur in any, the probability of a could be increased by some addend or factor to take account of this.\nConcerning some logical connective, say ∃, the observation that all past diagnosis formulas contained ∃ and in 80% of formulas formulated by this user including ∃ the ∃ connective has been modified at least once, the fault probability of ∃ might be assigned rather high. In comparison, the probability of some other connective, say ¬, occurring in no diagnosis and having been altered only in 10% of the formulas comprising ¬, the probability of the ¬ connective might be estimated rather low.\nA shortcoming of this approach is again a cold-start problem. If a user is new to conceptualizing knowledge in a structured logical manner or at least in the given logical language L, then no such (personalized) past diagnoses or change logs will be available. So, this issue especially concerns beginners who are usually anyhow more prone to errors than expert-users. On the positive side, utilization of such empirical data can yield to fault information that is very well tailored for the user and that can imply a significant reduction of computation time and user effort necessary for debugging of the KB at hand [74].\nNo Empirical Data is Available. If no data of the kinds (a) and (b) discussed above is available to a debugging system, then we have the following possibilities:\n(c) Common fault patterns\n(d) Subjective self-assessment of a user\n(e) Examination of structural complexity of logical formulas\n(f) Using no probabilities\nAd (c): A common fault pattern [59, 12, 39], also called anti-pattern, refers to a set of formulas that either leads to an inconsistency (logical anti-pattern) or corresponds to a potential modeling error that – alone – does not lead to a inconsistency or incoherency (non-logical anti-pattern), but still might become a source of inconsistency if merged with other formulas (cf. Section 3.2). Although most of these patterns incorporate more than one formula which makes the individual consideration of a formula in terms of fault probability calculation difficult, an idea to incorporate knowledge about anti-patterns to probability estimation of formulas could be to count for each ax ∈ K in how many different (logical or non-logical) anti-patterns it occurs. The higher this count, the more likely a formula might be involved in a conflict set and thus in the true diagnosis.\nA drawback of this method could be that most of the formulas involved in a KB might not correspond to any formula occurring in an anti-pattern. Thus, one might end up with no probability estimate for most of the formulas in a KB K. Besides that, the information provided by these anti-patterns is not personalized at all and therefore might significantly diverge from the true fault probabilities for a user and lead to a false bias in the used fault data. This justifies to basically rely on another approach to get a first estimate of a formula’s likeliness of being faulty and use this method only to make adaptations to already established probabilities.\nCHAPTER 4. DIAGNOSIS COMPUTATION 61\nAd (d): The method of a user’s self-assessment of own fault probabilities supposes a user to be able to specify fault probabilities of predicates (terms), logical connectives or complete formulas by themselves. Since users not always have a clear picture of own strengths and weaknesses, this variant must be regarded with suspicion. Furthermore, in settings where several persons are involved in the engineering of the KB, a reasonable rating of fault probabilities of terms, connectives or formulas authored by other persons might be difficult or impossible for a user. Ad (e): Here the idea is to examine “grammatical” (i.e. syntactical) aspects of formulas such as the “nesting depth” of subordinate clauses or the mere “length” of a formula. The underlying assumption can be that higher length and/or deeper nesting means higher complexity and cognitive difficulty in understanding of the formula’s semantics – as it does in natural language. For instance, it is reasonable to expect formulas like ax 1 := ∀X a(X) → (∃Y r1(X,Y ) ∧ (∀Z r2(Y,Z) → b(Z))) to tend to be more error-prone and more likely to be faulty than ax 2 := ∀X g(X) → b(X). This intuition is modeled by the maximum nesting depth as well as by the length of ax 1 in comparison to ax 2. Using the analogy to natural language, the maximum nesting depth of a formula could roughly be defined as the maximum number of encapsulated subordinate clauses that cannot be “flattened” occurring in the natural language translation of the formula. For formula ax 1, this would imply a maximum nesting depth of two; for ax 2 it would amount to zero. The reason is that ax 1 stated in natural language would sound “if somebody X is a, then there is somebody Y , who satisfies property r1 with X and for whom anybody, who satisfies property r2 with Y is b”. In this natural language formulation, there are two subordinate clauses, i.e. the clauses beginning with the word “who”; the first is at nesting depth one and the second at depth two. These subordinate clauses cannot be flattened, i.e. be brought to some lower depth, because the Z is related to the Y which in turn is related to the X . The length of formulas could be defined similarly as in [25] which provides such a definition for DL languages. In this case the length of ax 1 and ax 2 would be four (roughly: four predicates in ax 1) and two (two predicates in ax 2), respectively.\nA disadvantage of such a “grammatical” approach gets evident when most of the formulas in a KB are rather “simple”, i.e. have a low nesting depth and a short length. In such case this method will give little differentiation between different formulas and should thus be combined with another method of probability estimation in general. Ad (f): In a situation where all the aforementioned ways of gauging probabilities do not apply or are believed to have a too high risk of introducing a false bias into the debugging system, the solution is to define all formulas to be equally probably faulty. The obvious pro of this is that the system cannot get misled by unreasonable fault probabilities whereas the con is that possibly well-suited probabilistic information cannot be exploited. Moreover, experiments in our previous work [74] have manifested that fault information of only “average” quality most often leads to a better performance than no fault information. Apart from that, we have suggested a reinforcement learning “plug-in” to a debugger which could successfully mitigate the negative effect of low-quality fault information and in many cases, in spite of the low-quality fault information, even led to lower resource consumption (user, time) than a debugger without this plug-in using good fault information [63].\nCollaborative KB Development. In a collaborative development scenario involving several authors, provenance information could be additionally leveraged to refine probability estimates (cf. [39]). At this point, user skills could come into play; that is, formulas authored by more experienced authors get a lower overall fault probability as opposed to beginners concerning KB engineering or logic skills or expertise in the modeled domain. This probability adaptation can also affect syntactical elements in that one and the same predicate (term) or logical connective can get a different probability depending on in which formula it occurs and who authored that formula.\nRemark 4.4 Of course, these assumptions and methods of obtaining fault probabilities of syntactical elements and formulas are only some possible ways of doing so. For example, one might argue that the\nCHAPTER 4. DIAGNOSIS COMPUTATION 62\n“authorship” of a formula is somewhat not clearly defined. What if user u1 has originally written formula ax and then user u2 alters the formula to become ax ′? Who is the author of ax ′? u1, u2 or both? For whose fault probability computation should the renewed modification of ax ′ to ax ′′ count? Questions like this one need to be discussed and maybe evaluations using real data need to be accomplished in order to find a practical answer; or perhaps to find out that completely different approaches turn out to be reasonable. This is a topic of our future work.\nRemark 4.5 By the definition of a DPI (Definition 3.1) stating that the KB K must be disjoint with the background knowledge B and the role B has within a DPI, namely to comprise all formulas that are definitely correct, we postulate that no formula ax ∈ K must have a probability of zero. In a situation when this is not the case, a modified DPI must be used where such formulas have been moved from K to B.\nComputation of Diagnosis Probabilities. In the following, we denote by ax (K) the set of logical connectives and quantifiers occurring in a formula ax (in the KB K) and by ãx (K̃) the signature of ax (of K).\nExample 4.6 Considering the DL formula ax := Pet ≡ Animal u ∃hasOwner.Person, we have that ax = {≡,u,∃} and ãx = {Pet,Animal, hasOwner,Person}.\nWe now suppose that either a fault probability p(e) := p(“e is faulty”) of each element e ∈ K ∪ K̃ or the fault probability p(ax ) := p(“ax is faulty”) of each formula ax ∈ K is given. For estimation of these probabilities any (combination) of the methods mentioned above might be employed. In case formula probabilities are given, diagnosis probabilities can be directly computed by Formula 4.3. Otherwise, the following pre-computations must be performed.\nThe fault probability p(ax ) of ax can be calculated as the probability that at least one (occurrence of a) syntactical element in ax is faulty. So, p(ax ) is equal to 1 minus the probability that none of the syntactical elements occurring in ax is faulty. Hence, under the assumption of mutual independence of syntactical faults concerning elements e ∈ ax ∪ ãx ,\np(ax ) = 1− ∏\ne∈ax∪ãx\n(1− p(e))n(e) (4.2)\nwhere n(e) is the number of occurrences of syntactical element e in ax . If p(ax ) for all ax ∈ K is known, the fault probability p(D) of any diagnosisD ∈ Ω ⊆ aD〈K,B,P,N 〉R can be determined as the probability that each formula in D is faulty whereas each formula in K \\ D is correct, i.e. not faulty. Thence,\np(D) = ∏\naxr∈D p(ax r) ∏ axs∈K\\D (1− p(ax s)) (4.3)\nRecall that probabilities of all atomic events in a well-defined probability space must sum up to 1. As not every subset of K is a diagnosis, this is in general not the case. Therefore, diagnosis probabilities need to be normalized, i.e. each diagnosis probability p(D) must be divided by the sum of all diagnosis probabilities for diagnoses in Ω. That is, the following adjustment is necessary:\np(D) ← p(D)∑ Dk∈Ω p(Dk)\n(4.4)\nWe want to emphasize that the probability measures p(e) of syntactical elements e and p(ax ) of formulas ax are not required to satisfy any conditions except for p(e) ∈ (0, 1] and p(ax ) ∈ (0, 1] for all\nCHAPTER 4. DIAGNOSIS COMPUTATION 63\ne ∈ ax ∪ ãx and all ax ∈ K (see Remark 4.5 why the intervals (0, 1] are open). In particular, no normalization is needed. The reason for this is that “e is faulty” and “ax is faulty” are assumptions about a single logical connective and a single logical formula, respectively. “D is the true diagnosis”, to the contrary, is an assumption about each formula in the KB K. So, the probabilities of two different syntactical elements ei 6= ej are computed on the basis of two different probability spaces, namely Ωei = {“ei is faulty”, “ei is not faulty”} and Ωej = {“ej is faulty”, “ej is not faulty”} which clearly do not depend on each other at all. The same argumentation holds for probabilities of formulas.\nMore Reliable Probabilities through Observations. As we argued before, the basic fault information from which diagnosis probabilities are deduced might be rather vague. A usual way of dealing with scenarios of that kind, is to regard the initial probabilities as a first (a-priori) estimation and to gather additional information, e.g. by making measurements or observations, and exploit this information to adapt the a-priori estimation in order to obtain a more reliable a-posteriori estimation. The more additional information has been accumulated and incorporated, the more realistic is the resulting updated estimation of probabilities.\nA well-known technique enabling computation of a-posteriori probabilities from a-priori probabilities is Bayes’ Theorem. Let p(D) be the a-priori probability of some D ∈ Ω ⊆ aD〈K,B,P,N 〉R and Obs be a new observation. Then, the a-posteriori probability p(D |Obs) of D, i.e. the probability that the true diagnosis Dt = D taking into account the new information Obs, is computed according to Bayes’ Theorem as\np(D |Obs) = p(Obs | D) p(D) p(Obs)\n(4.5)\nwhere p(Obs) is the (a-priori) probability that observation Obs is made and p(Obs | D) is the (a-priori) probability that the observation Obs is made under the assumption that D is the true diagnosis, i.e. Dt = D. That is, the a-priori probability p(D), i.e. the probability that Dt = D without any additional knowledge, must be multiplied by p(Obs | D)/p(Obs) which is often referred to as the support Obs provides for D. If the support is greater than 1, then the a-posteriori probability of D is greater than its a-priori probability, otherwise the a-posteriori probability gets smaller after incorporating the new information Obs. Note that Bayes’ Theorem is only applicable to KB debugging if a suitable class of observations can be defined such that p(Obs) and p(Obs | D) can be computed for observations Obs of this class. As we shall see in Section 5.1, the assignment of test cases to either P or N is one such class of observations. For instance, ti ∈ P and tj ∈ N for sets of formulas ti, tj over L are two such observations."
    }, {
      "heading" : "4.5.2 Using Probabilities for Diagnosis Computation",
      "text" : "If available, formula fault probabilities can be exploited during construction of the pHS-tree (Algorithm 2) in that most probable instead of minimum cardinality diagnoses are calculated first. To achieve that, breadth-first construction of the tree must be replaced by uniform-cost order of node expansion by means of the function p() that assigns a fault probability to each formula ax ∈ K. Thereby, the “probability” p(nd) of a node nd = {ax s, . . . , ax t} in Algorithm 2 is defined through p(ax ), ax ∈ K as\np(nd) = ∏\nax i∈nd\np(ax i) ∏\naxj∈K\\nd\n(1− p(ax j)) (4.6)\nNotice that this formula extends the definition of Formula 4.3 to arbitrary subsets ofK, not only diagnoses. Thus, Formula 4.3 is a special case of Formula 4.6.\nFirst, note that we put “probability” of a node in quotation marks as, to be concise, each node (path) which is not yet a diagnosis, i.e. needs to be further expanded to become one, has probability zero (of\nCHAPTER 4. DIAGNOSIS COMPUTATION 64\nbeing the true diagnosis Dt). For, a probability space is defined on a set of diagnoses and not on a set of arbitrary subsets nd of the KB. However, we misuse the diagnosis probability space in this case to determine the probability of “pseudo-diagnoses” in order to impose an order on the queue of open nodes in the tree. This will guarantee the finding of the most probable diagnoses first, as we shall see below (Proposition 4.17).\nSecond, note that no normalization, i.e. application of Formula (4.4), is necessary within the scope of the non-interactive Algorithm 2 since the aim here is only the expansion of nodes nd in the order of p(nd) and the return of the most probable identified diagnoses at a certain point in time. For this, the comparison of the probability of one node nd with the probability of another node nd′ suffices. Thus, no other calculations using the properties of a probability space are performed by Algorithm 2. We shall recognize in Section 5.3 that this will not hold for the interactive Algorithm 5 where Formula (4.4) is essential.\nSo, nodes nd are inserted into Q in a way descending order of node probabilities in Q is always maintained. Consequently, nodes with highest fault probability are processed first. This is practical since a user will usually be most interested in seeing those possible faults first that have the highest (estimated) probability to be the actual fault they seek.\nHowever, one needs to be careful when using probabilities as weights in order not to lose the property of Algorithm 2 to compute minimal diagnoses only. To this end, the formula probabilities p(ax ) for all ax ∈ K must be adapted as\np(ax ) ← c p(ax ) (4.7)\nwhere the factor c is an arbitrary positive real number smaller than 0.5, e.g. c := 0.49/max{ax∈K}(p(ax )). This transformation effects that all probabilities p(ax ) become smaller than 50%. In other words, each formula must be more likely to be correct than faulty which in turn means that a minimal diagnosis is more likely than any of its supersets.\nDefinition 4.9. Let p : K → [0, 1] be some function that assigns to each ax ∈ K some p(ax ) ∈ [0, 1]. Then, we denote by pnodes : 2K → [0, 1] the function that assigns to each node nd ⊆ K some pnodes(nd) ∈ [0, 1] which is obtained by means of Formula 4.6 and p(). Lemma 4.14. Let nd, nd′ ⊆ K where nd ⊂ nd′ and p : K → (0, 0.5) a function which assigns to each ax ∈ K some probability p(ax ) ∈ (0, 0.5). Then pnodes(nd) > pnodes(nd′) holds. Proof. According to Formula 4.6 and Definition 4.9 we have that\npnodes(nd) = ∏\nax i∈nd\np(ax i) ∏\naxj∈K\\nd\n(1− p(ax j))\nThen the probability pnodes(nd′) can be computed from pnodes(nd) in that, for each formula ax in nd′ \\ nd ⊆ K \\ nd, we multiply pnodes(nd) by a factor fax := p(ax )/(1 − p(ax )) because ax “moves” from K \\ nd to nd. However, fax < 1 holds due to p(ax ) < 0.5 and thus 1− p(ax ) > 0.5.\nThis result will be a key to proving the completeness, soundness and correctness of Algorithm 2 in the next section.\nThe next definition characterizes a (partial) weighted pHS-tree, the type of hitting set tree constructed by Algorithm 2 given any function p(ax ) ∈ (0, 0.5) for all ax ∈ K as input which is not necessarily specified in a way a breadth-first tree construction is forced.\nDefinition 4.10 (Weighted Pruned HS-Tree). Let 〈K,B,P ,N 〉R be an admissible DPI and let w : K → [0, 1] be a weight function which assigns a weight to each node n ⊆ K with the property that w(n1) > w(n2) if n1 ⊂ n2. An edge-labeled and node-labeled tree T is called a weighted pruned HS-tree (wpHStree) w.r.t. 〈K,B,P ,N 〉R and w() iff T is the result of constructing an HS-tree w.r.t. 〈K,B,P ,N 〉R with due regard to the following rule\nCHAPTER 4. DIAGNOSIS COMPUTATION 65\n1. Label open nodes in the HS-tree in order of descending w(),\nand the rules 2 to 6 as per Definition 4.8. T is called a partial weighted pruned HS-tree w.r.t. 〈K,B,P ,N 〉R and w() iff T is a weighted pruned HS-tree w.r.t. 〈K,B,P ,N 〉R and w() where not all nodes in T have been labeled yet and non-labeled nodes have no successors.\nThen, we have the following relationship between a (partial) pHS-tree and a (partial) wpHS-tree. An explanation why this holds will be given in Section 4.5.4.\nProposition 4.14. A (partial) pHS-tree w.r.t. 〈K,B,P ,N 〉R is a (partial) wpHS-tree w.r.t. 〈K,B,P ,N 〉R and w() where w() is a weight function which, additionally to the property postulated in Definition 4.10, satisfies w(n1) = w(n2) if |n1| = |n2|.\nIn general, a (partial) wpHS-tree w.r.t. 〈K,B,P ,N 〉R and w() is not a (partial) pHS-tree w.r.t. 〈K,B,P ,N 〉R.\nLemma 4.15. Algorithm 2 is a procedure for producing a wpHS-tree T w.r.t. 〈K,B,P ,N 〉R and pnodes().\nProof. First, the property pnodes(n1) > pnodes(n2) if n1 ⊂ n2 postulated by Definition 4.10 holds by Lemma 4.14 and the fact that the function p given as input to Algorithm 2 satisfies p(ax ) ∈ (0, 0.5) for all ax ∈ K. Moreover, the DPI 〈K,B,P ,N 〉R provided as an input to Algorithm 2 is admissible, as postulated by Definition 4.10.\nThe compliance with rule 1 of Definition 4.7 as well as with rules 2 to 6 of Definition 4.8 is a simple consequence of Lemma 4.13. In the following we prove that rule 2 of Definition 4.7 and rule 1 of Definition 4.10 are satisfied.\n• Definition 4.7, rule 2: Suppose a node nd is labeled by valid. Then it is added to Dcalc in line 11. Since nd can only get a label different from closed if it is the only exemplar of this node in Q due to the duplicate criterion (lines 22-24), it must be the case that nd /∈ Q (line 7) after nd has been labeled by valid. Only nodes that get labeled by a conflict set can have successor nodes added to Q in line 15. Only nodes in Q can get a label (cf. lines 6 and 8). For nd to be added to Q at some later point in time there must be a proper subset of nd that is still in Q as each node newly added to Q is a proper superset of some node in Q (cf. line 15 which is the only position in the algorithm where nodes are added to Q). This is impossible since Q is ordered descending by pnodes(). Hence, each proper subset of nd must have been ranked before nd in Q and thus must have already been labeled because nd is already labeled by assumption. Hence, if nd is labeled by valid, then it has no successors.\n• Definition 4.10, rule 1: That nodes are processed and labeled in order of descending pnodes() follows from the fact that new nodes are inserted into Q only in a way that the order of Q by descending pnodes() is maintained (INSERTSORTED in line 15) and by the fact that always the first element of Q is selected to be labeled next (GETFIRST in line 6).\nThis completes the proof.\nLet the relevant data of a wpHS-tree be defined as for a pHS-tree (cf. Remark 4.2). By the correctness of Lemma 4.15, we have:\nCorollary 4.6. Algorithm 2 stores by 〈Dcalc,Q,Ccalc〉 the relevant data of\n• a wpHS-tree w.r.t. 〈K,B,P ,N 〉R and pnodes() if Algorithm 2 stops due to Q = [], and\n• a partial wpHS-tree w.r.t. 〈K,B,P ,N 〉R and pnodes() otherwise.\nCHAPTER 4. DIAGNOSIS COMPUTATION 66"
    }, {
      "heading" : "4.5.3 Correctness of Weighted Diagnosis Computation",
      "text" : "First, we show the completeness of Algorithm 2 regarding minimal diagnoses, i.e. that it computes all minimal diagnoses w.r.t. the DPI it is given as input.\nLemma 4.16. Only diagnoses w.r.t. 〈K,B,P ,N 〉R can be added to Dcalc by Algorithm 2.\nProof. A node nd can be added to Dcalc only in line 11. To reach this line, LABEL must have returned valid for nd. For this to hold, QX(〈K \\ nd,B,P ,N 〉R) must have returned ’no conflict’ which implies that nd is a diagnosis w.r.t. 〈K,B,P ,N 〉R by Propositions 4.9 and 3.2.\nLemma 4.17. Let T denote a (partial) wpHS-tree produced by Algorithm 2. Further, let Q be the queue of open nodes in T maintained by Algorithm 2 and let nd be some node which occurs only once in Q and which is a proper subset of some minimal diagnosis w.r.t. 〈K,B,P ,N 〉R. Then:\n(1) The nodes ∅ = nd1, . . . , ndk along any path from the root node ∅ to ndk in T satisfy ndi ⊂ ndi+1 and |ndi|+ 1 = |ndi+1| and ndi ⊆ K for 1 ≤ i ≤ k.\n(2) If the LABEL function is called for nd, then it yields some minimal conflict set C w.r.t. 〈K,B,P ,N 〉R with nd ∩ C = ∅.\nProof. (1): In the representation used by Algorithm 2, a node nd in the (partial) wpHS-tree T produced by Algorithm 2 is defined as the set of all edge labels on the path from the root node to nd (see Remark 4.2) and the successor of a node is defined as a node added to Q after nd has been labeled by a minimal conflict set.After the LABEL function for node nd has returned some minimal conflict set L as a label for nd, Algorithm 2 goes to line 15 since L 6= closed and L 6= valid and adds an element nd ∪ {e} to Q for each e ∈ L. Therefore, it holds that |nd∪ {e} | = |nd|+ 1 for each successor of nd. Hence, ndi ⊂ ndi+1 and |ndi|+ 1 = |ndi+1| holds for any path of nodes ∅ = nd1, . . . , ndk in T starting from the root node.\nThe argumentation why each node must be a subset of K is as follows: Suppose node ∪ {e} is added to Q in line 15 which is the only place in Algorithm 2 where nodes are added to Q. So, LABEL must have returned neither valid nor closed for node. Hence, node cannot be a diagnosis w.r.t. 〈K,B,P ,N 〉R as otherwise LABEL with argument node must have returned valid in line 30. Due to the fact that node = K is definitely a diagnosis w.r.t. 〈K,B,P ,N 〉R as it must hit all minimal conflict sets w.r.t. 〈K,B,P ,N 〉R which must all be subsets of K (Definition 4.1), node ⊂ K must hold.\n(2): Suppose the LABEL function is called for a node nd ∈ Q where nd ⊂ D for some minimal diagnosis D.\nFirst, there cannot be any nd′ ∈ Dcalc with nd′ ⊆ nd since Dcalc includes only diagnoses w.r.t. 〈K,B,P ,N 〉R and nd ⊂ D wherefore there would be a diagnosis nd′ ⊂ D, contradiction. Due to the fact that nd is present only once in Q, there cannot be some nd′ = nd in Q. Thus, closed cannot be returned for nd by LABEL.\nBy the facts that a diagnosis must hit all minimal conflict sets (Proposition 4.6) and that nd is a proper subset of a diagnosis, either the criterion checked in line 26 must be true or QX(〈K \\ nd,B,P ,N 〉R) must return a minimal conflict set L, i.e. L 6= ’no conflict’. In both cases, a minimal conflict set is returned by LABEL.\nThere are no other labels that can be returned by LABEL.\nLemma 4.18. Each minimal diagnosis w.r.t. 〈K,B,P ,N 〉R occurs as a node in Q during the execution of Algorithm 2, if the execution stops due to Q = [].\nProof. For Algorithm 2 it holds that\n(i) if nd is the last exemplar of some node in Q which is a proper subset of some minimal diagnosis w.r.t. 〈K,B,P ,N 〉R and the LABEL function is called for nd, then it yields some minimal conflict set C w.r.t. 〈K,B,P ,N 〉R with nd ∩ C = ∅ by Lemma 4.17 and\nCHAPTER 4. DIAGNOSIS COMPUTATION 67\n(ii) each node nd that has been labeled by some minimal conflict set C is deleted from Q (line 7) whereupon one successor node ndax = nd∪{ax} for each element ax ∈ C is added to Q (INSERTSORTED in line 23) and\n(iii) each minimal diagnosis w.r.t. 〈K,B,P ,N 〉R is a superset of ∅ and a subset of K (Definition 3.5) which includes one element of each minimal conflict set w.r.t. 〈K,B,P ,N 〉R and includes only elements of minimal conflict sets (Proposition 4.6).\nLet D be some minimal diagnosis w.r.t. 〈K,B,P ,N 〉R. Then, there is a path of nodes from the root node ∅ to D in the pHS-tree produced by Algorithm 2, if the execution stops due to Q = [].\nThis holds by the following argumentation: If D = ∅, then the path is 〈∅〉. Now, suppose D ⊃ ∅. Since D is a minimal diagnosis wherefore no other diagnosis can be equal to ∅, the root node n0 := ∅ of the constructed tree must be labeled by some minimal conflict set C1. Then, by (iii), there must be some ax 1 ∈ C1 that is an element of D. So, we define n1 := {ax 1}. If n1 = D, then the path is 〈∅, n1〉. Otherwise, due to D ⊃ n1 and (i), node n1 in the pHS-tree must be labeled by some minimal conflict set C2. Then, by (iii), there must be some ax 2 ∈ C2 that is an element of D. So, we define n2 := n1 ∪{ax 2}. If n2 = D, then the path is 〈∅, n1, n2〉. Otherwise, due to D ⊃ n2 and (i), node n2 in the pHS-tree must be labeled by some minimal conflict set C3. This reasoning can be continued until nk = D for some k. By (iii), D ⊆ K holds wherefore such k must exist.\nAlgorithm 2 cannot stop executing before nk has been in Q since each node ni labeled by a minimal conflict set Ci+1 involves the addition of |Ci+1| successor nodes to Q by (ii). In particular, the successor node ni ∪{ax i+1}must be added to Q. As the execution stops due to Q = [], all nodes ni for i ≤ k must be labeled before termination. Thus, D must be in Q sometime.\nProposition 4.15 (Completeness of Algorithm 2). If Algorithm 2 terminates due to Q = [], then the algorithm returns a set D including all minimal diagnoses w.r.t. 〈K,B,P ,N 〉R.\nProof. Assume some minimal diagnosis D w.r.t. 〈K,B,P ,N 〉R where D /∈ D after Algorithm 2 has returned due to Q = []. First, each minimal diagnosis will occur in Q throughout the execution of Algorithm 2 because it executes until Q = [] wherefore Lemma 4.18 applies. Any node nd in Q can only be deleted from Q if LABEL is called with the argument node nd (lines 7 and 8). There is no other point in Algorithm 2 where elements are removed from Q. Since at the end Q = [], each minimal diagnosis, in particular D, must be labeled.\nSuppose D is the last exemplar of possibly multiple duplicates of it in Q. Then, the LABEL function cannot return closed for D. This holds, on the one hand, because the duplicate criterion (lines 22-24) only removes possible duplicate nodes from Q, but never the last exemplar of a node in Q. On the other hand, D can never be closed due to the non-minimality criterion (lines 19-21) as Dcalc can only include diagnoses w.r.t. 〈K,B,P ,N 〉R by Proposition 4.16. Thus, due to the minimality of D, Dcalc cannot comprise any diagnosis D′ with D′ ⊆ D, except for some D′ which is equal to D. This would however be a contradiction to the assumption that D /∈ D.\nThe reuse criterion (lines 25-27) cannot apply forD either since a minimal diagnosis is a hitting set of all minimal conflict sets (Proposition 4.6) wherefore there cannot be a minimal conflict set in Ccalc which has an empty intersection with D. So, the algorithm will come to line 28 where QX(〈K \\ D,B,P ,N 〉R) will return ’no conflict’ (Propositions 4.9 and 3.2). Therefore, D will be labeled by valid and will be added to Dcalc in line 11.\nNext, we show the soundness of Algorithm 2 w.r.t. minimal diagnoses, i.e. that it computes only minimal diagnoses w.r.t. the DPI it is given as input.\nProposition 4.16 (Soundness of Algorithm 2). If an element D is added to the set Dcalc during the execution of Algorithm 2, D is a minimal diagnosis w.r.t. 〈K,B,P ,N 〉R.\nCHAPTER 4. DIAGNOSIS COMPUTATION 68\nProof. Assume that some element nd is added to Dcalc which is not a diagnosis w.r.t. 〈K,B,P ,N 〉R. This immediately yields a contradiction due to Lemma 4.16.\nAssume now that some element nd is added to Dcalc which is a diagnosis w.r.t. 〈K,B,P ,N 〉R, but not a minimal one. Now, since nd is a non-minimal diagnosis, there is some D ⊂ nd which is a minimal diagnosis w.r.t. 〈K,B,P ,N 〉R.\nThen, there are three cases to distinguish: (a) D is in Q and (b) D is in Dcalc and (c) D is neither in Q nor in Dcalc, i.e. the node D has not yet been generated.\nNote that these are all possible cases as D is a minimal diagnosis by assumption. So, D cannot have been ruled out, i.e. labeled by closed, by the non-minimality criterion (lines 19-21) before since only diagnoses can be added to Dcalc as argued in the first paragraph of this proof and there cannot be a diagnosis D′ ∈ Dcalc such that D′ ⊂ D. The case D′ = D is already considered by case (b). The duplicate criterion (lines 22-24) does not need to be taken into account since it deletes duplicate nodes only.\n(a): To be added to Dcalc, nd must have been the first element of the queue Q by GETFIRST in line 6. Since D ∈ Q by assumption and since Q is sorted in descending order of node probability (INSERTSORTED in line 15), we conclude that pnodes(D) ≤ pnodes(nd). However, as pnodes(X) for a node X ⊆ K is defined by means of p(ax ) where p(ax ) ∈ (0, 0.5) for all ax ∈ K as per Formula 4.6 (Definition 4.9), Lemma 4.14 applies and establishes the truth of pnodes(S1) > pnodes(S2) if S1 ⊂ S2 for S1, S2 ⊆ K. By D ⊂ nd, this implies pnodes(D) > pnodes(nd), contradiction.\n(b): Assuming case (b), we can derive a contradiction as follows. By the fact that nd is added to Dcalc, it must hold that the LABEL procedure called for nd in line 8 returned valid as part of its output in line 30. However, as D ⊂ nd is already an element of Dcalc by assumption, the LABEL procedure must have already returned in line 21 wherefore it cannot have reached line 30, contradiction.\n(c): Suppose that D has not yet been generated as a node in Q. By Lemma 4.17, the nodes ∅ = nd1, . . . , ndk along a path from the root node in the pHS-Tree produced by Algorithm 2 satisfy ndi ⊂ ndi+1 and |ndi| + 1 = |ndi+1|. So, by Lemma 4.14, the node probabilities along any path from the root node are strictly monotonically decreasing. Since pnodes(D) > pnodes(nd) holds by the same argumentation as in (a), we have that all nodes on the path from the root node to D have a higher probability than nd. As Q is sorted in descending order of node probability and in each iteration the first element in Q is processed as explained in (a), we infer that D must have already been generated at the time nd is processed, contradiction.\nNext, we argue that Algorithm 2 computes minimal diagnoses in descending order of diagnosis probability according to the parameter p() given as input to the algorithm.\nCorollary 4.7. Let the probability p(D) of a diagnosis D in Algorithm 2 be computed from the given function p(ax ), ax ∈ K as per Formula 4.3.\n1. At any point in time during the execution of Algorithm 2, Dcalc comprises the |Dcalc|most probable minimal diagnoses w.r.t. 〈K,B,P ,N 〉R.\n2. If Algorithm 2 returns a set D of cardinality n, then D is the set of the n most-probable minimal diagnoses w.r.t. 〈K,B,P ,N 〉R.\nProof. (1): By Propositions 4.15 and 4.16, it is a fact that Algorithm 2 computes all and only minimal diagnoses w.r.t. 〈K,B,P ,N 〉R. What must still be shown is that minimal diagnoses are added to Dcalc in descending order of their probability p() as per Formula 4.3. The probability p(D) of some diagnosis D is equal to pnodes(D) since a each diagnosis is a node and Formula 4.3 is a special case of Formula 4.6 by which the probability pnodes(nd) of a node nd is calculated.\nLet us denote by Dpmax the minimal diagnosis with maximum probability that has not yet been added to Dcalc and by D¬pmax an arbitrary minimal diagnosis with non-maximal probability. That is,\nCHAPTER 4. DIAGNOSIS COMPUTATION 69\npnodes(D¬pmax) < pnodes(Dpmax). So, we need to demonstrate that each node nd ⊂ Dpmax on a path from the root node to nodeDpmax is processed beforeD¬pmax is treated. By Lemma 4.17, a path from the root node in the pHS-Tree produced by Algorithm 2 is a set of nodes ∅ = nd1, . . . , ndk where ndi ⊂ ndi+1 and |ndi| + 1 = |ndi+1|. Further recall that the probability pnodes(X) of a node X ⊆ K in Algorithm 2 is defined as per Formula 4.6. So, by Lemma 4.14, the node probabilities along any path from the root node are strictly monotonically decreasing. Hence, each node nd on a path from the root node to Dpmax has a probability pnodes(nd) > pnodes(Dpmax) > pnodes(D¬pmax). By the insertion of new nodes into Q (INSERTSORTED in line 15) in a way descending order of Q as per pnodes() is always maintained, and by the selection of the first element of Q (GETFIRST in line 6) as next node to be processed, each node nd on a path toDpmax must be processed beforeD¬pmax is processed. Consequently, minimal diagnoses are added to Dcalc in descending order of their probability p() as per Formula 4.3.\n(2): This proposition follows directly from (1).\nProposition 4.17. Algorithm 2 always terminates and returns a set D of minimal diagnoses w.r.t. 〈K, B,P , N 〉R which is\n• the set of the |D|most probable (w.r.t. p() and Formula 4.3) minimal diagnoses w.r.t. 〈K,B,P ,N 〉R such that nmin ≤ |D| ≤ nmax, if at least nmin minimal diagnoses exist w.r.t. 〈K,B,P ,N 〉R, or\n• the set of all minimal diagnoses w.r.t. 〈K,B,P ,N 〉R, otherwise.\nProof. The proposition is a direct consequence of Propositions 4.12, 4.15 and 4.16 and Corollary 4.7."
    }, {
      "heading" : "4.5.4 Using Probabilities to Compute Minimum Cardinality Diagnoses",
      "text" : "The function p : K → (0, 0.5) can be defined in a way that minimum cardinality instead of maximum probability diagnoses are identified first. To this end, p() is specified as a fixpoint function that maps each formula ax ∈ K to one and the same constant value p(ax ) := c where c is an arbitrary real number such that 0 < c < 0.5, e.g. c := 0.3. That in this setting diagnoses are found in order of ascending cardinality is a simple consequence of Corollary 4.7.\nExample 4.7 Let us now study how such formula and diagnosis probabilities would be constructed for the example DPI depicted by Table 4.1. Let us suppose that the KB K in the DPI was formulated by a single user u for whom the personal fault probabilities of syntactical elements K̃ ∪ K given by the first row of Table 4.4 have been extracted from log data of the KB editing software applied by u. Then, the resulting probabilities of formulas ax ∈ K as per Formula 4.2 are as presented in the rightmost column of Table 4.4. The entries in the table from the second to the last but two column display the number of occurrences of the syntactical element given by the column label in the formula given by the row label. These values are required to compute the formula probabilities listed in the last but one column as per Formula 4.2. The final probabilities that can “safely” be incorporated into Algorithm 2 under a guarantee that only minimal diagnoses will be output are shown in the last column. These result from an application of Formula 4.7 to the probabilities given in the last but one column with an adaptation parameter c := 0.49.\nNotice that, for example, p(ax 5) is rather high since the predicates A and Y as well as the connective ¬ occurring in ax 5 have a comparably high fault probability in relation to syntactical elements appearing in other formulas. Formula ax 3, on the other hand, comprises only two predicates which should be wellunderstood by u and no connectives except for → which is not problematic for u either. Therefore, its fault probability is rather low."
    }, {
      "heading" : "4.6 Non-Interactive Debugging Algorithm",
      "text" : "Algorithm 3 describes the procedure for non-interactive debugging of KBs. The algorithm requires as input all the parameters that are required by Algorithm 2 and an additional parameter auto ∈ {true, false} indicating either automatic (true) or manual (false) mode. If auto = false , Algorithm 3 calls HS (Algorithm 2) with the parameters as provided. The set of minimal diagnoses D returned by HS is then presented to the user who can select a diagnosis manually after inspecting the diagnoses in D. Alternatively, in case of auto = true , the system calls HS with the parameters as provided, but with nmin = nmax = 1. Hence, only the most probable minimal diagnosis is computed by HS and returned as an output of Algorithm 3 to the user.\nIf a user wants the algorithm to output the set of all minimal diagnoses w.r.t. 〈K,B,P ,N 〉R, then the parameter setting auto = false and nmin = ∞ must be chosen. If, on the other hand, a fixed number n of leading diagnoses should be computed (as long as there are at least n minimal diagnoses for the DPI), then nmin := n =: nmax are the correct parameter settings. Note that in both cases the specification of t has no effect.\nOf course, the user can also apply Algorithm 3 several times with varying parameters t, nmin, nmax and p(). Or they can specify a test case, i.e. add a set of formulas X either to P (if each ax ∈ X should be entailed by the correct KB) or to N (if the conjunction of all formulas in X must not be implied by the correct KB), and rerun the algorithm with this modified DPI.\nAnyway, the user must either find the correct diagnosis (if it is an element of the output set D at all) by hand or be convinced that the returned minimum cardinality or respectively maximum probability diagnosis is indeed the one that yields a solution KB with the intended semantics. Moreover, when formulating test cases by hand, a user can be assumed to be as likely to specify something contradictory or faulty as during creation of the KB itself.\nUnsurprisingly, application of Algorithm 3 will often lead to unsatisfying solution ontologies. Remedy for this is provided by Interactive KB Debugging which on the one hand requires higher effort of one (or several) user(s), but on the other hand ensures a high quality solution in terms of its semantics to the problem of Parsimonious KB Debugging (Problem Definition 3.2).\nExample 4.8 Assume a user wants to find a maximal solution KB for the example DPI 〈K,B,P ,N 〉R provided by Table 4.1 and that no data giving information about fault probabilities of syntactical con-\nCHAPTER 4. DIAGNOSIS COMPUTATION 71\nAlgorithm 3 Non-Interactive KB Debugging Input: a tuple 〈〈K,B,P ,N 〉R, t, nmin, nmax, p(), auto〉 consisting of\nstructs or formulas in K is available. Therefore, let p(ax ) := c for some fixed c ∈ (0, 0.5) (see Section 4.5.2 for an explanation of this choice of c). The non-interactive KB debugging algorithm presented by Algorithm 3 called with 〈K,B,P ,N 〉R, the function p(), nmin = ∞ and auto = false as inputs results in the hitting set tree given by the upper picture in Figure 4.2. By nmin =∞ and auto = false , the user signalizes that inspection of all minimal diagnoses w.r.t. the input DPI is desired. Hence, the (complete) breadth-first pHS-tree as per Algorithm 2 is constructed. So, the output is the set of all minimal diagnoses mD〈K,B,P,N 〉R = {[1], [2], [5, 7]}.\nIn the shown hitting set tree, minimal diagnoses are indicated by nodes labeled by X(D) where D is a name given to this diagnosis. A node closed due to non-minimality is denoted by×(⊃D) whereD is some minimal diagnosis that is a subset of the set of edge labels along the path leading from the root node to this node. The label CC means that the minimal conflict set C has been freshly computed by a call to QX. The label CR, on the other hand, means that the minimal conflict set C has been reused from the set of already computed minimal conflict sets. In this example, both minimal conflict sets are computed by QX and no conflict sets are reused. The order of node labeling is indicated by the numbers i© starting from 1. Open nodes, i.e. generated nodes that have not yet been labeled, are indicated by a question mark.\nIn case auto = true was given as an input to the algorithm instead, the partial pHS-tree depicted by the lower picture in Figure 4.2 would be constructed and the output would be D = {D1} = {[1]} containing just the first found and thus most probable minimal diagnosis w.r.t. the input DPI. Note that D1 = [1] and D2 = [2] (which is not computed) have equal probability and whether the one or the other is computed first depends only on the ordering of equally probable (in this case: equal cardinality) nodes in Q. As already mentioned in Section 4.5.2, in this example the most probable diagnosis is equivalent to a minimum cardinality diagnosis since all formula probabilities are equal.\nPlease notice that the internal “flat” representation used by Algorithm 2 which does not store a tree but only the set of open and closed nodes differs from the standard tree representation [37, 19, 82, 60] we use to depict the hitting set tree graphically in Figure 4.2. Whereas within Algorithm 2 a node node stores the set of all the edge labels on the path leading from the root node to node, in the figure we label each node in the tree by the respective label that is computed for this node by the LABEL function, i.e. either by a minimal conflict set, by X or by ×.\nExample 4.9 Recall Example 4.7 which demonstrated how formula fault probabilities are constructed from fault probabilities of syntactical elements for the example DPI depicted by Table 4.1. Now we want to show how the non-interactive KB debugging algorithm given by Algorithm 3 works when these formula probabilities are incorporated.\nSuppose the inputs to the algorithm are the DPI 〈K,B,P ,N 〉R, the function p(ax ) for ax ∈ K displayed by the rightmost column of Table 4.4 and auto = false . Further on, let the user of the debugging\nCHAPTER 4. DIAGNOSIS COMPUTATION 73\nalgorithm be willing to wait a maximum of one second for an output and let them postulate a minimum of two most probable minimal diagnoses to be returned, e.g. to have at least a second choice if the employed formula probabilities are not perfectly suitable and the most probable diagnosis is not the desired solution. These postulations are expressed by specifying the parameters nmin = 2 and t = 1 (second). Additionally, assume the user expects the provided probabilities to be sufficiently reasonable such that the desired diagnosis will be among the best four diagnoses wherefore nmax = 4 is chosen. Moreover, let us imagine that the time for each fresh computation of a minimal conflict plus generation of the (unlabeled) successor nodes of this node is 0.4 seconds and the cost of computing any other label of a node is 0.1 seconds.\nThen the partial wpHS-tree produced by Algorithm 3 initialized in this way is illustrated by Figure 4.3. The used notation is as described in Example 4.8 with one additional attribute. Namely, each edge is not only labeled by one element of the conflict set from which it goes out, but also by a label p ∈ (0, 1) that is placed near the arrow head of the arrow that expresses the edge. This label p gives the probability as per pnodes() (cf. Definition 4.9) of the (partial) diagnosis that corresponds to the union of the edge labels along the path from the root to and including the edge that is labeled by p. For example, the label 0.06 of the edge directed at the node number 4© means that the probability of {2, 5} is 0.06. Further on, open, i.e. generated, but not yet labeled nodes, are designated by a question mark.\nAs outlined by the circled numbers i©, as a first action the root node is labeled by the newly computed minimal conflict set 〈1, 2, 5〉, the computation time of which amounts to 0.4. Then, the tree construction proceeds according to the (partial) diagnosis probabilities according to pnodes() computed from the formula probabilities p(ax ), ax ∈ K provided by the last column of Table 4.4. Therefore, the most probable edge leading away from the root node is labeled next. This already leads to the finding of the first minimal diagnosis D1 = [2] after overall computation time of 0.5 seconds. Since nmin = 2 diagnoses have not yet been computed and there are still unlabeled open nodes, namely those corresponding to paths {1} and {5}, the algorithm continues the execution by labeling the next best node {5} with a probability of 0.07 – as opposed to 0.02 for the other open node {1}. Since {5} is neither a superset of an already computed minimal diagnosis nor a duplicate of another open node nor a diagnosis itself, it must be labeled by some minimal conflict set. Because the already established minimal conflict set 〈1, 2, 5〉 is not disjoint with {5}, no reuse is possible and QX is called to determine a new minimal conflict set 〈1, 2, 7〉 w.r.t. 〈K,B,P ,N 〉R. All successor nodes of the newly labeled node 3©, i.e. the nodes corresponding to the paths {1, 5} , {2, 5} and {5, 7}, are added to the list Q of open nodes such that descending order of probabilities is maintained. The resulting queue is then Q = [{2, 5} , {5, 7} , {1} , {1, 5}]. As a next step, again the first and thus best open node {2, 5} is chosen from Q and labeled by ×(⊃D1) which means that the corresponding path is closed since it is a superset of an already found minimal diagnosis, namely D1 = [2]. At this point, the overall computation time amounts to 1 second which corresponds to the time limit t. For that reason, the algorithm will go ahead searching for minimal diagnoses only until a minimal number nmin thereof is detected. The node processed next, corresponding to the path {5, 7}, is then determined to be a minimal diagnosis by the LABEL procedure.\nThus, the output of the algorithm after 1.1 seconds execution time is the set of minimal diagnoses D = {[2], [5, 7]} which is a proper subset of all minimal diagnoses D〈K,B,P,N 〉R = {[1], [2], [5, 7]}. However, if we assume that the user’s intended KB should entail E → G, for instance, then none of the returned diagnoses can be used to compute a solution KB featuring this entailment when integrated with the background knowledge B. Hence, the true diagnosis Dt would be missed in this case.\nAlso, when computing all minimal diagnoses w.r.t. a DPI – if this is even possible in a concrete case due to the computational complexity – and showing them to the user, a user might review just the most probable ones and make a decision on which one to choose only based on these. For instance, [73] reported on one DPI where computation of all minimal diagnoses, 1782 in number, is feasible. In such a case it is hard to expect that a user will be willing or will have the time to inspect more than a small fraction of these 1782 diagnoses. The consequence will be a wrong choice of diagnosis in many cases,\nCHAPTER 4. DIAGNOSIS COMPUTATION 74\nalso because a simple view on a diagnosis will often not lead to the certainty of a user that this one is or is not the desired one. The reason for this is that usually it is too complex for a human brain to perform the necessary mental reasoning to make oneself a picture of the implications of choosing one diagnosis as opposed to another one.\nFor our example DPI, a user getting the output D = mD〈K,B,P,N 〉R = {[1], [2], [5, 7]} with the computed probabilities p([1]) = 12%, p([2]) = 60% and p([5, 7]) = 28% might decide to just inspect the diagnoses that make the most probable 80% fraction of diagnoses. In this case, either [2] or [5, 7] would be selected, which corresponds to a wrong choice in case E → G should be entailed be the resulting solution KB after integration with the background KB B.\nChapter 5\nInteractive Knowledge Base Debugging\nSo far, we have learned that the problem of (parsimonious) KB debugging as defined in Problem Definitions 3.1 and 3.2 in Chapter 3 can be solved by investigating minimal diagnoses w.r.t. a given DPI 〈K,B,P ,N 〉R. We have seen how minimal diagnoses can be computed, we have introduced a probability space over diagnoses and we have discussed how a-priori probability estimates for diagnoses can be established. Now, assume the situation where a DPI with say 100 minimal diagnoses is given, among which there is one diagnosis D with highest estimated probability p(D) = 10%. By the definitions of a diagnosis and a solution KB (Definitions 3.2 and 3.5), each of the 100 diagnoses can be used to formulate a solution KB w.r.t. the DPI 〈K,B,P ,N 〉R. So, should the system output the solution KB (K \\D) ∪UP obtained fromD as the optimal solution? Will a user be satisfied with a likeliness of 90% of being offered a suboptimal solution? What if the diagnoses probabilities are bad estimates and another diagnosis D′ should actually have a probability of 20%?\nWhy not simply apply Algorithm 3 to show all 100 minimal diagnoses to the user and let them select the preferred one by hand? First, due to the complexity of diagnosis calculation algorithms (cf. Chapter 1), pre-computation of 100 (or, generally, all) minimal diagnoses is usually not tractable within reasonable time. This makes such an approach quite unattractive in an interactive setting. Second, going through large sets of diagnoses can be time-consuming, tedious and error-prone. Third, human beings are normally not capable of (fully) realizing the semantic consequences of deleting a diagnosis from a KB, especially if the KB is large, complex and/or has been created by multiple engineers or automatic systems. Thus, applying a suboptimal diagnosis can result in unexpected entailments or unwanted changes, and thus an incorrect solution KB (incorrect in the sense of the semantics, not in the sense of violating given requirements or test cases), which might cause unexpected new faults and contradictions when augmented by new formulas. Consequently, a solution diagnosis is only acceptable if the user has sufficiently scrutinized and approved its semantic effect to the KB.\nThis leads to the definition of two types of Interactive KB Debugging problems. First, there is the problem of Interactive Dynamic KB Debugging which, given an input DPI, aims at the extension of this DPI by new test cases confirmed by a user such that there is only one minimal diagnosis left w.r.t. the extended DPI. Second, we specify the problem of Interactive Static KB Debugging which, given an input DPI, aims at the formulation of new test cases confirmed by a user such that these new test cases rule out all but one minimal diagnosis w.r.t. the input DPI.\n75\nCHAPTER 5. INTERACTIVE KNOWLEDGE BASE DEBUGGING 76\nProblem Definition 5.1 (Interactive Dynamic KB Debugging). Given a DPI 〈K,B,P ,N 〉R, the task is to find a maximal solution KB (K \\ D) ∪ UP∪P ′ w.r.t. a DPI 〈K,B,P ∪ P ′,N ∪N ′〉R such that\n• D is the only minimal diagnosis w.r.t. 〈K,B,P ∪ P ′,N ∪N ′〉R and\n• a user has confirmed that each p′ ∈ P ′ is a positive test case and that each n ′ ∈ N ′ is a negative test case.\nRemark 5.1 The solution of an Interactive Dynamic KB Debugging problem given the DPI 〈K,B, P ,N 〉R solves the problem of KB Debugging (Problem Defnition 3.1) as well as the problem of Parsimonious KB Debugging (Problem Defnition 3.2) for the DPI 〈K,B,P ∪ P ′,N ∪ N ′〉R, but in general not for the original DPI 〈K,B,P ,N 〉R. This is the reason why we term it “dynamic”, since a solution is found for a version of the initial DPI that has been extended by test cases.\nProblem Definition 5.2 (Interactive Static KB Debugging). Given a DPI 〈K,B,P ,N 〉R, the task is to find a maximal solution KB (K \\ D) ∪ UP w.r.t. 〈K,B,P ,N 〉R such that\n• there are sets of positive test cases P ′ and negative test cases N ′ where a user has confirmed that each p′ ∈ P ′ is a positive test case and that each n ′ ∈ N ′ is a negative test case, and\n• D is the only minimal diagnosis w.r.t. 〈K,B,P ,N 〉R that satisfies all positive and negative test cases P ′ and N ′, respectively.\nRemark 5.2 The solution of an Interactive Static KB Debugging problem given the DPI 〈K,B,P ,N 〉R constitutes a solution to the problem of KB Debugging (Problem Defnition 3.1) as well as to the problem of Parsimonious KB Debugging (Problem Defnition 3.2) for the original DPI 〈K,B,P ,N 〉R, therefore the term “static”.\nNow, we give a more formal definition of a true diagnosis (an informal characterization of which was given in Section 4.5). If sufficiently many new test cases are specified and added to a given DPI such that there is only one remaining minimal diagnosis w.r.t. the input DPI (the input DPI extended by the new test cases) left, then this diagnosis is referred to as the true diagnosis w.r.t. Interactive Static (Dynamic) KB Debugging.\nDefinition 5.1 (True Diagnosis). Let Dt be equal to D in Problem Definition 5.4 (5.3). Then Dt is called the true diagnosis w.r.t. Interactive Static KB Debugging (Interactive Dynamic KB Debugging)."
    }, {
      "heading" : "5.1 User Interaction",
      "text" : "The idea in interactive KB debugging is to iteratively consult a user asking them to give additional information as regards desired and undesired entailments of the correct KB. Thus, the principle of interactive KB debugging is based on that of Sequential Diagnosis which has been suggested by [44] as an iterative way to localize the faulty components (among an initially large set of possibilities) in malfunctioning digital circuits by performing repeated (most informative) measurements. We have shown in our previous works [73, 74] how sequential diagnosis can be applied to KBs (ontologies).\nIn our approach, for the selection of which question (of a pool of possible ones) to ask a user next, an active learning [71] approach is applied.1 Active Learning is an iterative supervised machine learning technique in which a learning algorithm is able to interactively query the user to obtain a label for a\n1Note that the minimal a-posteriori expected entropy of solution candidate probabilities as a means to select the best next measurement as used in [44] is only one of many possible active learning strategies [71].\nCHAPTER 5. INTERACTIVE KNOWLEDGE BASE DEBUGGING 77\ndesired unlabeled instance. In the case of a KB debugging system, an unlabeled instance is a set of logical formulas and the label is whether the conjunction of these formulas should or should not be entailed by the correct KB. Since the learner can choose the instances to be labeled, the number of consultations of an interacting user required to learn a concept (in this case the one solution KB with the desired semantics w.r.t. a given DPI) can often be much lower than the number required in a standard supervised learning setting since the risk that the algorithm must deal with lots of uninformative examples is reduced.\nWe suppose the user of an interactive KB debugger to be a single person or multiple persons, usually experts of the particular domain the faulty KB is dealing with or authors of the faulty KB. Moreover, we assume the interacting user to be able to answer concrete queries about the intended domain that should be modeled. Otherwise put, we suppose that a user can classify a given logical formula (or a conjunction of logical formulas) as a wanted or unwanted proposition in the intended domain, i.e. as an entailment or non-entailment of the correct domain model. We have already argued in Chapter 1 why this assumption is plausible."
    }, {
      "heading" : "5.1.1 Queries",
      "text" : "In interactive KB debugging, a set of logical formulas Q is presented to the user who should decide whether to assign Q to the set of positive (P ) or negative (N ) test cases w.r.t. a given DPI 〈K,B,P ,N 〉R. In other words, the system asks the user “should the KB you intend to model entail all formulas in Q?”. In that, Q is generated by the debugging algorithm in a way that any decision of the user\n1. invalidates at least one minimal diagnosis (search space restriction) and\n2. preserves validity of at least one minimal diagnosis (solution preservation).\nWe call a set of logical formulas Q with these properties a query. Successive classification of queries as entailments (all formulas in Q must be entailed) or non-entailments (at least one formula in Q must not be entailed) of the correct KB enables gradual restriction of the search space for (minimal) diagnoses. Further on, classification of sufficiently many queries guarantees the detection of a single correct solution diagnosis which can be used to determine a solution KB with the correct semantics w.r.t. a given DPI.2\nDefinition 5.2 (Query). Let 〈K,B,P ,N 〉R over L and D ⊆ mD〈K,B,P,N 〉R . Then a set of logical formulas Q 6= ∅ over L is called a query w.r.t. D iff there are diagnoses D,D′ ∈ D such that D /∈ mD〈K,B,P∪{Q},N 〉R and D′ /∈mD〈K,B,P,N∪{Q}〉R . The set of all queries w.r.t. D and 〈K,B,P ,N 〉R is denoted by QD,〈K,B,P,N 〉R .\nRemark 5.3 Although Definition 5.2 only postulates that at least one diagnosis in D is invalidated for whatever answer is given to the query, this implies that, for each answer to the query, there is also a diagnosis that remains valid after adding the corresponding test case to the DPI, as will be shown by Proposition 5.4.\nSo, w.r.t. a set of minimal diagnoses D ⊆ mD〈K,B,P,N 〉R , a query Q is a set of logical formulas that rules out at least one diagnosis in D (and therefore in mD〈K,B,P,N 〉R ) as a candidate to formulate a solution KB, regardless of whether Q is classified as a positive or negative test case.\n2Correctness of the diagnosis must not be understood as a guarantee that all formulas in the KB which are not in the diagnosis are definitely correct. Instead, correctness must be seen with regard to other diagnoses and with the “Principle of Parsimony” in mind (cf. Section 3.1). That is, all other possible diagnoses are ruled out by a present set of test cases wherefore the single remaining diagnosis is the one that is correct (in comparison with all other incorrect ones). And, there is no evidence (at the time the correct diagnosis is found) that any other formulas in the KB might be faulty. This might change however after new formulas are added to the KB.\nCHAPTER 5. INTERACTIVE KNOWLEDGE BASE DEBUGGING 78"
    }, {
      "heading" : "5.1.2 Leading Diagnoses",
      "text" : "Query generation requires a precalculated set of minimal diagnoses D ⊆ mD〈K,B,P,N 〉R that serves as a representative for all minimal diagnoses mD〈K,B,P,N 〉R . As already mentioned, computation of the entire set mD〈K,B,P,N 〉R is generally not tractable within reasonable time. Usually, D is defined as a set of most probable or minimum cardinality diagnoses (cf. Chapter 4). Therefore, D is called the set of leading diagnoses w.r.t. 〈K,B,P ,N 〉R [74].\nThe leading diagnoses D are then exploited to determine a query Q the answering of which enables a discrimination between the diagnoses in mD〈K,B,P,N 〉R . That is, a subset of mD〈K,B,P,N 〉R which is not “compatible” with the new information obtained by adding the test case Q to P or N is ruled out (see Proposition 5.3 below). For the computation of the subsequent query only a leading diagnoses set Dnew w.r.t. the minimal diagnoses still compliant with the new sets of test cases P ′ and N ′ is taken into consideration, i.e. Dnew ⊆ D〈K,B,P ′,N ′〉R .\nThe number of precomputed leading diagnoses D affects the quality of the obtained query. The higher |D|, the more representative is D w.r.t. mD〈K,B,P,N 〉R , the more options there are to specify a query in a way that a user can easily comprehend and answer it, and the higher is the chance that a query that eliminates a high rate of diagnoses w.r.t. D will also eliminate a high rate of all minimal diagnoses mD〈K,B,P,N 〉R . The selection of a lower |D| on the other hand means better timeliness regarding the interaction with a user, first because fewer leading diagnoses might be computed much faster and second because the search space for an “optimal” query is smaller.3 So, the optimal number of leading diagnoses depends on the complexity of the particular DPI considered. One way to determine a suitable |D| can be to first define an interval [nmin, nmax] that must comprise |D| where the upper bound defines the desired number of leading diagnoses and the lower bound the minimally postulated number. Second, the search for minimal diagnoses is run at least as long as it takes to compute nmin diagnoses and at the longest until nmax diagnoses have been found or a timeout t expires that is specified in a manner it enables frequent user interaction. Note that such parameters have already been taken into account in the non-interactive KB debugging Algorithm 2 (see Section 4.6)."
    }, {
      "heading" : "5.1.3 Q-Partitions",
      "text" : "Now we introduce the notion of a q-partition, a partition of the leading diagnoses set D induced by a query w.r.t. D. A q-partition will be a helpful instrument in deciding whether a set of logical formulas is a query or not. It will facilitate an estimation of the impact a query answer has in terms of invalidation of minimal diagnoses. And, given fault probabilities, it will enable us to gauge the probability of getting a positive or negative answer to a query.\nFrom now on, given a DPI 〈K,B,P ,N 〉R and some minimal diagnosis Di w.r.t. 〈K,B,P ,N 〉R, we will use the following abbreviation for the solution KB obtained by deletion of Di along with the given background knowledge B:\nK∗i := (K \\ Di) ∪ B ∪ UP (5.1)\nDefinition 5.3 (q-Partition4). Let 〈K,B,P ,N 〉R be a DPI over L, D ⊆ mD〈K,B,P,N 〉R . Further, let Q be a set of logical formulas over L and\n• D+(Q) := {Di ∈ D | K∗i |= Q}, 3Roughly, a query Q is “optimal” if the number of queries that still need to be answered to identify the desired solution KB after Q is added to the (positive or negative) test cases is minimal. “Optimality” of a query can be captured by quantitative information theoretic measures studied in the field of active learning [71] that can be used to estimate the quality of a query beforehand, i.e. before an answer to it is known. See Section 5.3.3 and [63, 73, 74] for details.\n4In existing literature, e.g. [74, 63, 73], a q-partition is often simply referred to as partition. We call it q-partition to emphasize that not each partition of D into three sets is necessarily a q-partition.\nCHAPTER 5. INTERACTIVE KNOWLEDGE BASE DEBUGGING 79\n• D−(Q) := {Di ∈ D | ∃x ∈ R ∪N : K∗i ∪Q violates x},\n• D0(Q) := D \\ (D+j ∪D − j ).\nThen 〈D+(Q),D−(Q),D0(Q)〉 is called a q-partition iff Q is a query w.r.t. D and 〈K,B,P ,N 〉R.\nRemark 5.4 The set D−(Q) contains exactly those diagnoses Di ∈ D where K \\ Di is invalid w.r.t. 〈·,B,P ∪ {Q} ,N 〉 (cf. Definition 3.3).\nProposition 5.1. For each query Q w.r.t. some D ⊆ mD〈K,B,P,N 〉R it holds that 〈D+(Q), D−(Q), D0(Q)〉 is a partition of D.\nProof. First, by definition of D0(Q), we have that D+(Q)∪D−(Q)∪D0(Q) = D, D+(Q)∩D0(Q) = ∅ and D−(Q) ∩ D0(Q) = ∅. Second, D+(Q) ∩ D−(Q) = ∅ since K∗i |= Qj and ∃x ∈ R ∪ N : (K∗i ∪Qj violates x) imply by idempotency ofL thatK∗i violates some x ∈ R∪N which is a contradiction to Di being a diagnosis w.r.t. 〈K,B,P ,N 〉R. Thus, each diagnosis in D is an element of exactly one set of D+(Q),D−(Q),D0(Q) which is equivalent to the statement of the proposition.\nRemark 5.5 In fact, Proposition 5.1 holds for any set D ⊆ aD〈K,B,P,N 〉R , i.e. for any subset of all diagnoses w.r.t. 〈K,B,P ,N 〉R. This can be easily seen from the proof of Proposition 5.1 which does not require minimality of diagnoses. That is, any set of diagnoses w.r.t. a DPI is partitioned into the three sets D+(Q), D−(Q) and D0(Q) as per Definition 5.3 by a query Q w.r.t. this DPI.\nProposition 5.2. For each query Q w.r.t. some D ⊆ mD〈K,B,P,N 〉R there is one and only one partition 〈D+(Q),D−(Q),D0(Q)〉.\nProof. The existence of a partition D+(Q),D−(Q),D0(Q) follows directly from Proposition 5.1. Assume there are two different partitions 〈D+1 (Q),D − 1 (Q),D 0 1(Q)〉 and 〈D+2 (Q),D − 2 (Q),D 0 2(Q)〉. Then, (a) D+1 (Q) 6= D + 2 (Q) or (b) D − 1 (Q) 6= D − 2 (Q) or (c) D 0 1(Q) 6= D02(Q) must hold. If (a) is true, then there is one diagnosis Di ∈ D such that K∗i |= Q and K∗i 6|= Q – a contradiction. If (b) is true, then there is one diagnosis Di ∈ D such that K∗i ∪Q violates some x ∈ R ∪ N and K∗i ∪Q does not violate any y ∈ R ∪ N – a contradiction. If (c) is true, then (D+1 (Q) ∪D − 1 (Q)) 6= (D + 2 (Q) ∪D − 2 (Q)) which implies that either (a) or (b) must be true.\nDue to the uniqueness of a q-partition 〈D+(Q),D−(Q),D0(Q)〉 for a query Q, we denote this qpartition by P(Q). As a consequence of Definition 5.3 and Proposition 5.2, a query Q is a set of common entailments of KBsK∗i , each resulting from the deletion of a single minimal diagnosisDi ∈ D+(Q) from K.\nCorollary 5.1. For each query Q ∈ QD,〈K,B,P,N 〉R there is a set of minimal diagnoses D+(Q) ⊆ mD〈K,B,P,N 〉R as defined by Definition 5.3 such that Q ⊆ {e | ∀Di ∈ D+(Q) : K∗i |= e}."
    }, {
      "heading" : "5.1.4 Interpretation of Q-Partitions",
      "text" : "Since K∗i corresponds to the solution KB (along with B) obtained under the assumption that Dt = Di, i.e. the true diagnosis (cf. Definition 5.1) corresponds to Di, the sets D+(Q) and D−(Q) can be interpreted as those leading diagnoses that predict the classification of Q as a positive and negative test case, respectively. In other words, if the true diagnosis Dt is in D+(Q), then the true solution KB K∗t entails Q by Definition 5.3. Therefore the user will answer Q positively (cf. Definition 5.1). If, conversely,Dt is in D−(Q), then the true solution KBK∗t would be invalidated ifQwas answered positively,\nCHAPTER 5. INTERACTIVE KNOWLEDGE BASE DEBUGGING 80\nsince K∗t ∪ Q = (K \\ Dt) ∪ B ∪ UP∪{Q} violates some x ∈ R ∪ N and thus K \\ Dt is invalid w.r.t. 〈·,B,P ∪ {Q} ,N 〉R, which implies that Dt is not a diagnosis w.r.t. 〈K,B,P ∪ {Q} ,N 〉R according to Proposition 3.2. Hence, the user will answer Q negatively (cf. Definition 5.1). Diagnoses in D0(Q) on the other hand neither predict Q ∈ P nor Q ∈ N . This means that we do not know how the user will answer a query Q for which the true diagnosis Dt is in D0(Q). In this case, for any answer to Q, the true diagnosis Dt is in the set of minimal diagnoses w.r.t. the new DPI including Q as a test case. To summarize: If the true diagnosis Dt is an element of D+(Q) (D−(Q)), then Q will be answered positively (negatively).\nConversely, this means that a q-partition P(Q) gives a prior indication which leading diagnoses would be invalidated by a user’s answer. Diagnoses in D+(Q) are invalidated by the classification Q ∈ N , and diagnoses in D−(Q) in case of Q ∈ P . Diagnoses in D0(Q) can never be invalidated by an answer to Q. Thus, intuitively, queries with D0(Q) = ∅ are preferable over other queries (as per the information provided by the set of leading diagnoses D) as the number of (definitely) eliminated diagnoses in mD〈K,B,P,N 〉R should be maximized.\nThe following proposition is a direct consequence of Corollary 3.3 and explicates the impact of the addition of a test case to a DPI regarding the set of minimal diagnoses for this DPI.\nProposition 5.3. Let Q be a query w.r.t. D ⊆ mD〈K,B,P,N 〉R and let the answer of a user to Q be u(Q) ∈ {true, false}.\nIf u(Q) = true , then Di ∈ mD〈K,B,P,N 〉R is a diagnosis w.r.t. 〈K,B,P ∪ {Q} ,N 〉R iff K \\ Di is valid w.r.t. 〈·,B,P ∪ {Q} ,N 〉R.\nIn other words, both of the following conditions must hold:\n∀r ∈ R : K∗i ∪Q does not violate r ∀n ∈ N : K∗i ∪Q 6|= n\nIf u(Q) = false , then Di ∈ mD〈K,B,P,N 〉R is a diagnosis w.r.t. 〈K,B,P ,N ∪ {Q}〉R iff K \\ Di is valid w.r.t. 〈·,B,P ,N ∪ {Q}〉R.\nIn other words, both of the following conditions must hold:\n∀r ∈ R : K∗i does not violate r ∀n ∈ (N ∪ {Q}) : K∗i 6|= n\nRemark 5.6 From Proposition 5.3 and Definition 5.3 it is easy to see that at least Di ∈ D−(Q) ⊂ mD〈K,B,P,N 〉R are eliminated by a positive answer to Q. Namely, D\n−(Q) comprises exactly those diagnoses Di that imply the violation of some r ∈ R or the entailment of some n ∈ N if Q is added to K∗i . On the other hand, at least Di ∈ D+(Q) ⊂ mD〈K,B,P,N 〉R are discarded if u(Q) = false as all diagnoses in D+(Q) entail Q which must not be entailed.\nNote that, in general, the addition of a query to the test cases of a DPI causes not only an invalidation of some leading minimal diagnoses in D, but also the elimination of minimal diagnoses that have not even been computed yet. On the other hand, an added test case might also introduce new minimal diagnoses, i.e. ones that were no minimal diagnoses before this test case was added. However, the newly obtained DPI after the addition of any new test case can only exhibit a reduced set of all (i.e. minimal and nonminimal) diagnoses compared with the DPI before the test case was added."
    }, {
      "heading" : "5.1.5 The Relation between a Query and its Q-Partition",
      "text" : "The following proposition shows the relationship between a query and its q-partition and provides a criterion that enables to check whether a set of logical formulas is a query w.r.t. some set of leading diagnoses or not.\nCHAPTER 5. INTERACTIVE KNOWLEDGE BASE DEBUGGING 81\nProposition 5.4. Let 〈K,B,P ,N 〉R be a DPI over L and D ⊆ mD〈K,B,P,N 〉R . Then a set of logical formulas Q 6= ∅ over L is a query w.r.t. D iff D+(Q) 6= ∅ and D−(Q) 6= ∅.\nProof. “⇐”: If D+(Q) 6= ∅ and D−(Q) 6= ∅ holds, then a non-empty set of diagnoses D−(Q) (D+(Q)) becomes invalid for positive (negative) answer to Q. So, Q is a query.\n“⇒”: If Q is a query, then there are diagnoses D,D′ ∈ D such that D /∈ mD〈K,B,P∪{Q},N 〉R and D′ /∈mD〈K,B,P,N∪{Q}〉R . Consequently,D ∈ D\\mD〈K,B,P∪{Q},N 〉R andD′ ∈ D\\mD〈K,B,P,N∪{Q}〉R holds. But, as the diagnoses in D \\mD〈K,B,P∪{Q},N 〉R are exactly the diagnoses in D that become invalid by the positive answer to Q, we obtain D ∈ D−(Q). The argumentation for D′ ∈ D+(Q) is analogous. Hence, D+(Q) 6= ∅ and D−(Q) 6= ∅.\nCorollary 5.2. Let D ⊆mD〈K,B,P,N 〉R . Then, for each q-partition P(Q) = 〈D+(Q),D−(Q),D0(Q)〉 w.r.t. D it holds that D+(Q) 6= ∅ and D−(Q) 6= ∅.\nProof. Follows from Definition 5.3 which grants the existence of a query for any q-partition and Proposition 5.4 which states that neither D+(Q) nor D−(Q) must be empty sets for any query.\nSo, by Proposition 5.4, a query not only eliminates at least one leading diagnosis, but also leaves at least one leading diagnosis valid. Therefore, an admissible DPI can never get non-admissible by adding a query to the positive or negative test cases.\nCorollary 5.3. Let 〈K,B,P ,N 〉R be an admissible DPI, D ⊆mD〈K,B,P,N 〉R andQ ∈ QD,〈K,B,P,N 〉R . Then 〈K,B,P ∪ {Q} ,N 〉R as well as 〈K,B,P ,N ∪ {Q}〉R are admissible DPIs.\nProof. Assume that 〈K,B,P ∪ {Q} ,N 〉R is non-admissible. Then there is no valid diagnosis for this DPI. Since 〈K,B,P ,N 〉R is an admissible DPI, this means that Q invalidates each diagnosis D ∈ aD〈K,B,P,N 〉R ⊇ mD〈K,B,P,N 〉R ⊃ D. By Proposition 5.4, this is a contradiction to the fact that Q is a query. The argumentation for 〈K,B,P ,N ∪ {Q}〉R is analogue.\nThis means in particular that a query can never contain a conflict set or result in a violation of some requirement r ∈ R when added to B ∪ UP (cf. Proposition 3.4)."
    }, {
      "heading" : "5.1.6 Existence of Queries",
      "text" : "For any set of at least two leading minimal diagnoses the existence of a query is guaranteed, as the next proposition and corollary show. In particular, this implies that for arbitrary two minimal diagnoses D,D′ w.r.t. a DPI there is a query Q that enables to differentiate between D and D′, i.e. exactly one of these diagnoses is invalidated by each answer to Q.\nProposition 5.5. Let D ⊆ mD〈K,B,P,N 〉R with |D| ≥ 2 and UD be the union of all diagnoses in D. Then\n(I) Q := (UD \\ Di) is a query w.r.t. D for arbitrary Di ∈ D and\n(II) P(Q) = 〈{Di} ,D \\ {Di} , ∅〉.\nProof. Ad (I): Assume thatQ is not a query. Then either (1)Q = ∅ or (2) D+(Q) = ∅ or (3) D−(Q) = ∅. In the following we prove that neither (1) nor (2) nor (3) can hold.\n(1): Q = ∅ means that Di ⊇ UD. Since any diagnosis D in D is a subset of UD, this implies that for each D ∈ D, D ⊆ Di holds. As |D| ≥ 2 is assumed, there is a Dk 6= Di ∈ D for which this property holds. This, however, is a contradiction to the minimality of diagnosis Di.\n(2): D+(Q) = ∅ cannot hold, since (K \\ Di) ⊇ (UD \\ Di) and UD \\ Di |= Q by monotonicity of description logics imply that K∗i = (K \\ Di) ∪ B ∪ UP |= Q. Hence, there is at least one diagnosis, namely Di, in D+(Q).\nCHAPTER 5. INTERACTIVE KNOWLEDGE BASE DEBUGGING 82\n(3): To prove that D−(Q) 6= ∅, we must show that there is a diagnosis D ∈ D such that Y := (K \\ D)∪B∪UP∪Q = (K\\D)∪B∪UP∪(UD\\Di) is incoherent. However, (K\\D)∪(UD\\Di) = K\\(D∩Di) by distributive and De Morgan laws which yields Y = K \\ (D ∩ Di) ∪ B ∪ UP . But, D ∩ Di ⊂ D must hold as D 6⊆ Di by the subset-minimality of Di whereby D must comprise a formula ax /∈ Di. Hence, Y ⊃ (K \\ D) ∪ B ∪ UP is incoherent by subset-minimality of D.\nAd (II): We already know that Di ∈ D+(Q) by (2). Since D ∈ D in (3) can be chosen arbitrarily, we obtain that D ∈ D−(Q) for all diagnoses D ∈ D \\ {Di}.\nWe immediately obtain a lower bound for the number of queries by Proposition 5.5:\nCorollary 5.4. Let D ⊆ mD〈K,B,P,N 〉R with |D| > 1. Then a lower bound for the number of queries w.r.t. D is |D|.\nRemark 5.7 Notice that the preceding proposition and corollary require a set of minimal diagnoses. This means that subset-minimality of diagnoses is a necessary prerequisite for guaranteeing the possibility of discrimination between diagnoses. In other words, interactive debugging by means of (some or only) non-minimal diagnoses cannot be proven to work correctly (without making any further assumptions)."
    }, {
      "heading" : "5.2 Query Generation",
      "text" : "In this section, we want to describe, discuss and prove the correctness of methods for the generation of queries which takes place at each iteration of an interactive KB debugging algorithm after a set of leading diagnoses has been determined. With Algorithm 4, similar versions of which can be found in [74, 63], we present a way to compute a pool QP of queries and associated q-partitions w.r.t. a set of leading diagnoses D and a DPI 〈K,B,P ,N 〉R. The generation of this pool QP is the first stage of the query computation function used in the interactive debugging algorithm (Algorithm 5) presented below. In a second stage, one particular query that meets certain criteria such as maximum expected information gain is selected from QP (see Section 5.3.3).\nBefore we give a description of Algorithm 4, let us have a look at some example by which we want to demonstrate the principle how a query w.r.t. some set of leading diagnoses for a DPI can be constructed. This should give the reader a first idea and an intuition of how the presented algorithm works.\nExample 5.1 Consider the example FOL DPI given by Table 5.1. The set of minimal conflict sets mC〈K,B,P,N 〉R = {C1, C2} = {〈1, 3, 4〉 , 〈1, 2, 3, 5〉} (like in previous examples, formulas ax i in Table 5.1 are sometimes referred to just by their number i if it is clear from the context what is meant). Let the set of leading diagnoses be the set of all minimal diagnoses, i.e. D = mD〈K,B,P,N 〉R = {D1,D2,D3,D4} = {[1], [3], [4, 5], [2, 4]}. To enable a better understanding of this example, we first analyze why C1 and C2 are minimal conflict sets w.r.t. 〈K,B,P ,N 〉R.\nWhy is C1 a conflict set w.r.t. 〈K,B,P ,N 〉R? In the following we underline the formulas ax i and relevant parts of these formulas used in the derivation of the conflict set. First, there is the background KB B including a1(w) and a1(u). Due to ax 1, by substitution of X by w (written as X/w), we obtain a2(w),m1(w) and m2(w) from a1(w). Likewise, we can derive a2(u),m1(u) and m2(u) from a1(u) by X/u. Substituting X by w in ax 3 yields m1(w)→ ¬a(w) ∧ b(w). Thus, we obtain ¬a(w). A substitution of X by u in ax 4 results in m2(u) → (∀Y s(u, Y ) → a(Y )) ∧ d(u). By Y/w, we have m2(u)→ (s(u,w)→ a(w)) ∧ d(u). Since m2(u) has already been deduced from the background formula a1(u) and s(u,w) is a background formula as well, we can conclude a(w) from ax 4. All in all, we have derived ¬a(w) and a(w), i.e. an inconsistency, by means of B and C1 (and UP which is the empty set) wherefore C1 is a conflict set w.r.t. 〈K,B,P ,N 〉R by Definition 4.1. The minimality of C1 can be\nCHAPTER 5. INTERACTIVE KNOWLEDGE BASE DEBUGGING 83\neasily verified by the way we derived that it is a conflict set; namely, leaving out any of the formulas ax 1, ax 3 or ax 4 does not allow to derive an inconsistency or incoherency (note that the set of negative test cases N is empty).\nWhy is C2 a conflict set w.r.t. 〈K,B,P ,N 〉R? We argue as follows to deduce the inconsistency responsible for C2 to be a conflict set (the relevant implications and used formulas are again underlined):\n(1) : a1(w) ∈ B : a1(w) (2) : X/w in ax 1 : a1(w) → a2(w) ∧m1(w) ∧m2(w) (3) : X/w in ax 3 : m1(w) → ¬a(w) ∧ b(w)\n(4) : ax 5 and X/w : b(w) → m3(w) (5) : (1)− (4) : m3(w)\n(6) : a1(u) ∈ B : a1(u) (7) : X/u in ax 1 : a1(u) → a2(u) ∧m1(u) ∧m2(u) (8) : X/u in ax 2 : a2(u) → ¬(∃Y s(u, Y ) ∧m3(Y ))\n∧ (∃Zs(u, Z) ∧m2(Z)) (9) : (6)− (8) : ¬(∃Y s(u, Y ) ∧m3(Y ))\n(10) : s(u,w) ∈ B : s(u,w) (11) : (5) and (10) : ∃Y s(u, Y ) ∧m3(Y )\n(9) and (11) : E\nMinimality of C2 can again be verified by observing that, given any formula of C2 is left out, no inconsistency or incoherency can be derived.\nNow we show how to construct a query manually. As suggested by Definition 5.3 and Proposition 5.4 and discussed in Section 5.1.5, an obvious way of generating a query w.r.t. D and 〈K,B,P ,N 〉R is\nCHAPTER 5. INTERACTIVE KNOWLEDGE BASE DEBUGGING 84\nvia the notion of a q-partition. Definition 5.3 states that Q is a set of common entailments of KBs K∗i (Formula 5.1) where Di ∈ D+(Q), a subset of D. Hence, a first step towards query computation is to choose some non-empty subset S of the leading diagnoses D which we will call the seed for query generation. For our manual construction, let S = {D3,D4} = {[4, 5], [2, 4]}. For each of the diagnoses Di in S, we assemble the KB K∗i and use a reasoning engine to obtain a set of entailments EDi of K∗i . ForD3 we obtainK∗3 := {1, 2, 3, 4, 5}\\{4, 5}∪{6, 7, 8}∪{} = {1, 2, 3, 6, 7, 8}. Similarly, we compute K∗4 = {1, 3, 5, 6, 7, 8}.\nSuppose that the reasoner invoked by the used GETENTAILMENTS function produces only entailments of the type ∀Xp1(X) → p2(X) for predicate names p1, p2 and of the type p(a) where p is a predicate name and a is a constant (cf. Remark 2.3). For this purpose, DL and OWL reasoners, respectively, such as Pellet [78], HermiT [77], FaCT++ [84] or KAON25 could be used with their classification and realization reasoning services. The reason why this is possible can be realized after a short analysis of the DPI 〈K,B,P ,N 〉R given by Table 5.1. For, this DPI can be translated to DL similarly as demonstrated in Example 2.1. All the mentioned reasoners can deal with the expressivity of the resulting DL language.\nThen, we obtain the sets ED3 and ED4 , i.e. the sets of entailments of K∗3 and K∗4 , respectively, as depicted by Table 5.2. The set of common entailments Q, i.e. Q = ED3 ∩ ED4 is then the set containing all elements in the rows of Table 5.2 that are above the dashed line.\nNotice at this point that the set {a1(w), a1(u), s(u,w)} = B does not need to be computed or, respectively, included in Q since none of these formulas can serve to discriminate between diagnoses (which is the only aim of a query). The simple reason for this is thatK∗i for eachDi ∈ D comprises these formulas and thus each K∗i entails these formulas by the extensiveness of FOL (cf. Chapter 2). Since entailed by each potential solution KBK∗i , these formulas cannot yield a violation of any requirements or test cases since none of the KBs K∗i violates any requirements or test cases (follows from Definitions 3.5 and 3.2).\nContinuing with our query construction, we know by Proposition 5.4 that Q is a query w.r.t. D and 〈K,B,P ,N 〉R iff D+(Q) 6= ∅ and D−(Q) 6= ∅. Whereas it is trivial that the former condition is met since D+(Q) contains (at least) the two diagnoses D3 and D4 that we used to compute Q (cf. Definition 5.3), we still need to verify whether the latter condition is actually satisfied for Q. To this end, as per Definition 5.3, we must simply find some diagnosis Dj in D \\ S = {D1,D2,D3,D4} \\ {D3,D4} = {D1,D2} such that K∗j ∪ Q violates some x ∈ N ∪ R, i.e. whether some negative test case is entailed or whether this KB is incoherent or inconsistent. So, we start with D1, i.e. we examine (K \\ D1) ∪ B ∪ P ∪Q = {1, 2, 3, 4, 5} \\ {1} ∪ {6, 7, 8} ∪ {} ∪Q = {2, 3, 4, 5, 6, 7, 8} ∪Q.\nAnd, indeed, we are able to prove an inconsistency for this KB. To see that, verify that by X/w in e2 ∈ Q (see Table 5.2) and a1(w) = ax 6 ∈ K∗1 we can derive m1(w) which lets us conclude ¬a(w) by the substitution of X by w in ax 3 ∈ K∗1 . On the other hand, we obtain a(w) by X/u in e3 ∈ Q, {X/u, Y/w} in ax 4 ∈ K∗1 and s(u,w) = ax 8 ∈ K∗1 as shown in the explanation for conflict set C1 above. Thus, D1 ∈ D−(Q).\nThat is, we have just proven thatQ is de facto a query w.r.t. D and 〈K,B,P ,N 〉R. And this, although we have not yet assigned each leading diagnosis to the respective set of the q-partition ofQ. In a situation where just any query shall be asked to the user, this would suffice, and the query could be presented to the interacting user.\nHowever, in case a “best” query according to some criterion shall be determined from a set of different competing queries, usually the computation of the full q-partition of each competing query is required. This is due to the fact that the q-partition provides information about several properties of queries that are considered by common query selection techniques (for details see Section 5.3.3). So, let us complete the q-partition for our query Q by investigating K∗2 ∪ Q = {1, 2, 4, 5, 6, 7, 8} ∪ Q. Also in this case we can derive an inconsistency which can be easily realized by reconsidering the argumentation why C2 is a\n5http://kaon2.semanticweb.org/\nCHAPTER 5. INTERACTIVE KNOWLEDGE BASE DEBUGGING 85\nconflict set above and by using e4 ∈ Q instead of ax 3 /∈ K∗2 ∪Q. That means, the final q-partition P(Q) for Q is given by 〈{D3,D4} , {D1,D2} , ∅〉.\nThe next question that arises directly from the proofs that D3,D4 ∈ D−(Q) is whether there is a (set-minimal) subset Qmin of Q such that Qmin preserves the discrimination properties of Q, i.e. the q-partition P(Qmin) = P(Q). In fact, the answer is yes for the query Q we computed, but also for the majority of other cases. This is a simple consequence of using the reasoning engine as a black-box which suggests a strategy we pursued in our query construction which relies on a precomputation of entailments and a final minimization part. Sticking to this black-box concept however does not allow to use some customized reasoning procedure that pointedly returns a set of common entailments Q for a set of diagnoses S ⊂ D where all formulas in Q are necessary for a requirement or test case violation, respectively, of KBs K∗j for diagnoses in D \\ S.\nWhat militates for such a black-box approach is the generality and independence of a particular logic (for which an adequate glass-box reasoner exists), the easier implementation of the debugging system and potential performance issues with a glass-box approach [41]. For a black-box algorithm to work, only a reasoner implementing a sound and complete inference procedure for the used logic L must be available.\nIn general, there is more than one minimized version of a query that preserves the q-partition. Theoretically, the number of such minimal queries w.r.t. one q-partition can be exponential in the size of the initially computed query that is provided as an input to the minimization procedure. For our query Q, for instance,\nQmin,1 = {a2(u), b(w)} = {e7, e12}, Qmin,2 = {∀Xa1(X)→ a2(X), b(w)} = {e1, e12} , Qmin,3 = {∀Xa1(X)→ a2(X),\n∀Xa1(X)→ m1(X), ∀Xm1(X)→ b(X)} = {e1, e2, e4} and\nQmin,4 = {∀Xa1(X)→ m1(X), ∀Xa1(X)→ m2(X), ∀Xm1(X)→ b(X)} = {e2, e3, e4}\nare set-minimal, q-partition preserving subqueries. Namely, each of the sets Qmin,1, Qmin,2 and Qmin,3 together with {2, 5, 6, 7, 8} implies an inconsistency since m3(w) and ¬m3(w) can be derived and {2, 5, 6, 7, 8} ⊆ K∗1 and {2, 5, 6, 7, 8} ⊆ K∗2 . {e2, e3} ⊂ Qmin,4 yields an inconsistency when added to K∗1 , i.e. a(w) and ¬a(w) are entailed, and {e4} ⊂ Qmin,4 merged with K∗2 yields an inconsistency, i.e. the derivation of m3(w) and ¬m3(w). In order not to overwhelm the user we would of course ask them such a minimized version of a query rather than the full query that contains plenty of irrelevant formulas.\nAn example of a seed S that does not lead to the discovery of a query is S = {D1,D2,D3} since the set of common entailments ED1 ∩ ED2 ∩ ED3 = ∅. Note that this holds when all EDi contain only entailments of the types we specified above. For other types of entailments, i.e. a different specification of the GETENTAILMENTS function, this might no longer hold."
    }, {
      "heading" : "5.2.1 Generation of a Pool of Queries",
      "text" : "The main function GETPOOLOFQUERIES of Algorithm 4 gets as inputs an admissible DPI 〈K,B,P ,N 〉R over L, a set of leading (minimal) diagnoses D ⊆ mD〈K,B,P,N 〉R such that |D| ≥ 2 and a parameter q ∈ N ∪ {∞} , q ≥ 1 that indicates the number of queries in QD,〈K,B,P,N 〉R the algorithm is supposed to return (where q := ∞ signalizes that a maximum number of queries should be output). The way of generating a pool of queries is guided by Proposition 5.4 which says that a non-empty set Q of formulas\nCHAPTER 5. INTERACTIVE KNOWLEDGE BASE DEBUGGING 86\nAlgorithm 4 Generation of Queries and Q-Partitions Input: an admissible DPI 〈K,B,P ,N 〉R, a set of minimal diagnoses D ⊆ mD〈K,B,P,N〉R such that |D| ≥ 2, a\ndesired number q ∈ N ∪ {∞} , q ≥ 1 of queries w.r.t. 〈K,B,P ,N 〉R to be returned Output: a set QP including tuples 〈 Q, 〈 D+(Q),D−(Q),D0(Q) 〉〉 such that: If q ≥ |QPmax|, then\n1. there are no two tuples 〈Q,P(Q)〉 , 〈Q′,P(Q′)〉 in QP such that Q = Q′ or P(Q) = P(Q′), and 2. QP includes a tuple 〈 Q, 〈 D+(Q),D−(Q),D0(Q) 〉〉 only if Q ∈ QD,〈K,B,P,N〉R , and 3. QP includes at most one tuple where D+(Q) = Y for each Y ⊂ D, and 4. for each Y ⊂ D for which a query Q w.r.t. D and 〈K,B,P ,N 〉R exists such that (a) Q includes only\nentailments computed by the used GETENTAILMENTS function and (b) P(Q) is such that D+(Q) = Y , QP includes a tuple 〈Q′,P(Q′)〉 such that D+(Q′) = Y , and\n5. QP 6= ∅. If q < |QPmax|, then QP includes q tuples satisfying (1), (2) and (3). (|QPmax| ≥ 0 is the maximum number of tuples 〈Q,P(Q)〉 that can be computed by GETPOOLOFQUERIES by the used GETENTAILMENTS function)\n1: procedure GETPOOLOFQUERIES(〈K,B,P ,N 〉R,D, q) 2: ED ← ∅ 3: for D ∈ D do 4: ED ← GETENTAILMENTS(D,K,B,P) . EDr is the set of entailments of K∗r 5: ED ← ED ∪ {〈D, ED〉} 6: for ∅ ⊂ S ⊂ D do 7: isQuery ← false 8: Q← GETCOMMONENTAILMENTS(S, ED) 9: if Q 6= ∅ then 10: for Dr ∈ D \\ S do 11: if Q ⊆ EDr then . Does K∗r |= Q ? 12: D+ ← D+ ∪ {Dr} 13: else if ¬ISKBVALID(K∗r ∪Q, 〈·, ∅, ∅,N 〉R) then . ISKBVALID (see Algorithm 1) 14: D− ← D− ∪ {Dr} 15: isQuery ← true 16: else 17: D0 ← D0 ∪ {Dr} 18: if isQuery ∧ ¬INCLQPART(QP, 〈 D+,D−,D0 〉 ) then\n19: Q′ ← MINQ(∅, Q, ∅, 〈 D+,D−,D0 〉 , 〈K,B,P ,N 〉R)\n20: QP← QP ∪ {〈 Q′, 〈 D+,D−,D0 〉〉} 21: if |QP| = q then 22: return QP 23: if |QP| = 0 then 24: QP← ADDTRIVIALQUERIES(D,QP) 25: return QP 26: procedure MINQ(X,Q,QB, 〈 D+,D−,D0 〉 , 〈K,B,P ,N 〉R)\n27: if X 6= ∅ ∧ ISQPARTCONST(QB, 〈 D+,D−,D0 〉 , 〈K,B,P ,N 〉R) then 28: return ∅ 29: if |Q| = 1 then 30: return Q 31: k ← SPLIT(|Q|) 32: Q1 ← GET(Q, 1, k) 33: Q2 ← GET(Q, k + 1, |Q|) 34: Qmin2 ← MINQ(Q1, Q2, QB ∪Q1, 〈 D+,D−,D0 〉 , 〈K,B,P ,N 〉R)\n35: Qmin1 ← MINQ(Qmin2 , Q1, QB ∪Qmin2 , 〈 D+,D−,D0 〉 , 〈K,B,P ,N 〉R) 36: return Qmin1 ∪Qmin2 37: procedure ISQPARTCONST(Q, 〈 D+,D−,D0 〉 , 〈K,B,P ,N 〉R) 38: for Dr ∈ D− do 39: if ISKBVALID(K∗r ∪Q, 〈·, ∅, ∅,N 〉R) then . ISKBVALID (see Algorithm 1) 40: return false 41: for Dr ∈ D0 do 42: if K∗r |= Q then 43: return false 44: return true\nCHAPTER 5. INTERACTIVE KNOWLEDGE BASE DEBUGGING 87\nover L is a query w.r.t. D and 〈K,B,P ,N 〉R if and only if D+(Q) as well as D−(Q) are non-empty sets of diagnoses. That is, the necessary and sufficient criteria for Q to be a query are\n(CQ1) Q 6= ∅ and\n(CQ2) D+(Q) 6= ∅ and\n(CQ3) D−(Q) 6= ∅.\nNote, since the disjoint sets of diagnoses D+(Q) ⊆ D and D−(Q) ⊆ D must not be empty, |D| ≥ 2 must be postulated in order for any queries to exist w.r.t. D and 〈K,B,P ,N 〉R (cf. Corollary 5.4).\nAs a first action (lines 3-5), the algorithm computes a set of entailments EDi for each K∗i (cf. Formula 5.1) where Di ∈ D and stores these entailments along with the respective diagnosis as a tuple 〈Di, EDi〉 in a set ED. This is accomplished by the function GETENTAILMENTS which gets a tuple 〈X,Y, Z,W 〉 of arguments where X,Y, Z are sets of formulas over some logic L and W is a set including sets of formulas over L. Then, GETENTAILMENTS computes a finite (cf. Remark 2.3) set of entailments of certain types (cf. Examples 5.1 and 5.6) of the KB (Y \\X) ∪ Z ∪ UW .\nThen, the algorithm runs through all proper non-empty subsets S of the leading diagnoses D and, for each S, it computes the set of common entailments Q of all KBs K∗i where Di ∈ S (function GETCOMMONENTAILMENTS) by means of the precomputed set ED. That is, Q := ⋂ D∈SED. If Q is non-empty, then CQ1 and CQ2 are fulfilled for Q. CQ2 is met since S 6= ∅ and thus there is a diagnosis Di ∈ D such that K∗i |= Q which implies that D+(Q) 6= ∅. So, the algorithm proceeds to verify\nCHAPTER 5. INTERACTIVE KNOWLEDGE BASE DEBUGGING 88\nCQ3 (lines 10-17) in that it assigns the remaining diagnoses in D that are not in S to the according sets D+(Q), D−(Q) or D0(Q) as per Definition 5.3. Note that the function ISKBVALID has been specified in Algorithm 1 on page 39. With the parameters given when called in line 13, ISKBVALID checks whether K∗r ∪ Q = (K \\ Dr) ∪ B ∪ UP∪{Q} does not violate any requirement in R and does not entail any test case in N . Once the call to this function returns false for one diagnosis Dr ∈ D \\S, it holds that Dr ∈ D−(Q) thus CQ3 is definitely met. Therefore, isQuery is set to true in line 15. If, on the other hand, isQuery is not set to true for any diagnosis in D \\ S, then the set D−(Q) = ∅ and thus Q is not in QD,〈K,B,P,N 〉R .\nSo far, we have proven the following proposition.\nProposition 5.6. Let a DPI 〈K,B,P ,N 〉R, a set of diagnoses D ⊆mD〈K,B,P,N 〉R and a natural number q ≥ 1 be the input to the function GETPOOLOFQUERIES. Then, a value stored in variable Q at the time GETPOOLOFQUERIES executes line 18 is a query w.r.t. D and 〈K,B,P ,N 〉R iff the variable isQuery stores the value true .\nIf the purpose was only to find queries (and not q-partitions), the algorithm could stop processing for the current Q and go to the next set S, given that isQuery is set to true for some diagnosis. However, as the q-partition provides meaningful information to assess a query, e.g. it gives the number of diagnoses invalidated for each answer or the estimated probability of each answer (cf. Section 5.1), the q-partition is a necessary input to the subsequently called function SELECTBESTQUERY (line 48 in Algorithm 6, see later in Sections 5.3.2.4 and 5.3.3) that selects a query from the pool of queries QP. For this reason, the algorithm continues until the computation of the q-partition for Q is complete.\nIn a last step (lines 18-20), given that isQuery is true and there is not yet a query with the same q-partition in QP, the algorithm computes a set-minimal subset Qmin of Q such that the q-partition of Qmin is the same as the one of Q (function MINQ). Finally, the tuple 〈 Qmin, 〈 D+,D−,D0 〉〉 including\nthe minimized query Qmin along with its q-partition 〈 D+,D−,D0 〉 is added to QP. If |QP| = q, then QP is returned; otherwise, a further iteration for another S is executed. If |QP| = q is not met until all seeds S have been processed, the set QP is checked for emptiness in line 23. If QP = ∅, then the function ADDTRIVIALQUERIES (line 24) adds |D| ≥ 2 queries as defined byQ in Proposition 5.5 to QP (cf. Corollary 5.4) and then returns QP; otherwise, QP is directly returned.\nRemark 5.8 Notice that lines 23 and 24 in Algorithm 4 aim at ensuring the non-emptiness of the pool of queries QP returned by GETPOOLOFQUERIES for any GETENTAILMENTS function (see Example 5.6 for different specifications of the GETENTAILMENTS function). This is a necessary criterion for the interactive KB debugging system (Algorithm 5) to work in a sound way since it guarantees that the CALCQUERY function (line 16 in Algorithm 5) always returns a query w.r.t. the current set of leading diagnoses D and the given DPI. Note that the |D| queries generated and added to QP by ADDTRIVIALQUERIES can be trivially obtained without the consultation of a reasoning service by extraction of the respective formulas from the KB K, as prescribed by Proposition 5.5."
    }, {
      "heading" : "5.2.2 Discussion of Query Pool Generation",
      "text" : "Multiple Equal Q-Partitions. In the general case there is more than one query w.r.t. one and the same q-partition. For that reason alone that a minimized query is a set-minimal subset of an initially computed one where multiple such subsets may exist.\nExample 5.2 An example for such a query resulting in multiple minimized subqueries with identical q-partition can be found in Example 5.1.\nHowever, note that GETPOOLOFQUERIES is designed to compute a pool QP that includes at most one query with one and the same q-partition. The idea behind this is (1) to minimize the calls to the\nCHAPTER 5. INTERACTIVE KNOWLEDGE BASE DEBUGGING 89\nexpensive function MINQ and (2) that two queries with the same q-partition have exactly the same properties w.r.t. common query selection criteria such as maximum expected information gain or maximum worst case invalidation rate of diagnoses after the query answer is known. Such criteria have been shown to often lead to a reduction of debugging effort for the interacting user (cf. [74, 63]). As the purpose of the computation of the pool of queries QP is to constitute an input to the query selection function that uses exactly such selection measures, the inclusion of only one query with a particular q-partition is reasonable, also (3) to minimize computation time of the query selection function which needs to go through all elements of QP in order to pick the “best” one in the worst case.\nOn the other hand, regarding the comprehensibility of the query, i.e. the cognitive load on the user when it comes to understanding the meaning of the query, two queries with the same q-partition may well be significantly different. This however is beyond the scope of this work and considered a topic for future research.\nThe following proposition gives evidence that the set QP returned by GETPOOLOFQUERIES is indeed duplicate-free w.r.t. the q-partitions in QP.\nProposition 5.7. Let a DPI 〈K,B,P ,N 〉R, a set of diagnoses D ⊆ mD〈K,B,P,N 〉R and q ∈ N ∪ {∞} , q ≥ 1 be the input to the function GETPOOLOFQUERIES. Then, the function GETPOOLOFQUERIES returns a set QP including tuples of the form 〈Q,P(Q)〉 where Q ∈ QD,〈K,B,P,N 〉R is a query and P(Q) = 〈 D+(Q),D−(Q),D0(Q) 〉 is the q-partition of Q such that QP does not include any two equal queries and does not include any two equal q-partitions.\nProof. The test of the criterion ¬INCLQPART tested before the call to MINQ will always return false for the q-partition 〈 D+,D−,D0 〉 if 〈 D+,D−,D0 〉 is already included in a tuple in QP. Since MINQ is q-partition-preserving, no q-partition that does not occur in a tuple in QP can become equal to some q-partition in QP by a call to MINQ. Therefore, QP cannot include any two equal q-partitions. Since two equal queries have equal q-partitions, any two different q-partitions cannot be q-partitions of equal queries. Thus, QP cannot include any two equal queries either.\nNote that, on account of the q-partition preserving property of MINQ, only such q-partitions are ruled out by the criterion in line 18 that would lead to duplicates at the time they should be added to QP in line 20.\nComputation of Entailments. Generally, the (theoretical) number of entailments of a set of formulas is not finite. However, the entailments (of a certain type) returned by a reasoner are finite. For instance, asked for entailments of {A v B u C}, a reasoner performing the classification reasoning service would give back A v B and A v C, but not entailments like A v B t C or A v C u C u C. That is, when we speak of entailments, then we mean entailments in the practical sense (cf. Remark 2.3), i.e. w.r.t. a reasoning service such as classification for DL KBs which computes all and only subsumptions X v Y such that Y is the most specific concept that subsumes X , or forward-chaining for Datalog KBs which computes all and only atoms that are entailed by the KB.\nExample 5.3 If we recall Example 5.1, we see that the number of computed entailments of K∗4 and K∗3 was 19 and 13 respectively, which are rather high numbers in the light of the small KBs, but importantly these numbers are necessarily finite. For, there cannot be more than |Pred|2 entailments of the ∀Xp1(X)→ p2(X) type and not more than |Pred| |Const| entailments of the p(a) type for a KB whose signature includes the unary predicate symbols Pred and constant symbols Const and does not include any function symbols. In case of KB K∗3 , for example, the set Pred = {a1, a2,m1,m2,m3, a, b} and Const = {u,w} which means that upper bounds for the number of entailments of the first and second type are 49 and 14, respectively.\nCHAPTER 5. INTERACTIVE KNOWLEDGE BASE DEBUGGING 90\nFurther, note that the number of existing different q-partitions and which q-partitions there are at all w.r.t. some set of leading diagnoses D and a DPI depends on the function GETENTAILMENTS, i.e. on the set of entailments calculated by it.\nExample 5.4 Recall Example 5.1 where we constructed a queryQw.r.t. the set of all minimal diagnoses for the DPI given by Table 5.1. Assume now that only entailments of the first type, i.e. those of the form ∀Xp1(X) → p2(X), and none of the second type p(a) are computed by GETENTAILMENTS and denote the set of entailments of this form of K∗i by E′Di . Then, Q ′ = E′D3 ∪ E ′ D4 = {e1, . . . , e5} (cf. Table 5.2), i.e. a subset of the queryQ computed for a GETENTAILMENTS function producing entailments of both types. The q-partition ofQ′ is the same as the q-partition ofQ, namely 〈{D3,D4} , {D1,D2} , ∅〉. However, the queries Qmin,1 and Qmin,2 are no longer obtained as minimized versions of Q′, unlike Qmin,3 and Qmin,4 which are subqueries of Q′, too.\nMinimizing the Set D0 in Q-Partitions. Recall that D0 = ∅ is a desirable property of a q-partition since a query with such q-partition may invalidate any leading diagnosis, depending on the answer to the query (cf. Section 5.1). In other words, no leading diagnosis is guaranteed to be still valid for any answer after the query is added as a test case to the DPI.\nIn general, GETPOOLOFQUERIES computes q-partitions where D0 may be a non-empty set. However, if the GETENTAILMENTS function is specified to compute certain explicit entailments of K, then D0 = ∅ can be guaranteed.\nDefinition 5.4 (Explicit entailment). Let K be a KB. Then, α is an explicit entailment of K iff α ∈ K.\nNow, if each set of entailments ED computed by GETENTAILMENTS includes all the formulas that occur in some diagnosis in D, but do not occur in D, then GETPOOLOFQUERIES definitely returns a set QP of queries and associated q-partitions where D0(Q) = ∅ holds for each tuple in QP.\nProposition 5.8. Let 〈K,B,P ,N 〉R be a DPI and D ⊆ mD〈K,B,P,N 〉R . If the set ED computed by GETENTAILMENTS meets ED ⊇ UD \\ D for all D ∈ D, then GETPOOLOFQUERIES computes only queries Q with D0(Q) = ∅.\nProof. Assume that Q is some query computed by GETPOOLOFQUERIES. As MINQ is a q-partition preserving transformation of Q, we can assume w.l.o.g. that Q is a query computed by GETPOOLOFQUERIES before MINQ is called for Q. We have to show that for an arbitrary diagnosis Di ∈ D either Di is assigned to D+(Q) or to D−(Q).\nSo, let us assume that there is a diagnosis Dk which is assigned to D0(Q) = D \\ (D+(Q)∪D−(Q)) in line 17. Then, Q 6⊆ EDk and K∗k ∪ Q does not violate any x ∈ R ∪ N must hold, otherwise Dk would have already been assigned to D+(Q) in line 12 or to D−(Q) in line 14. But Q 6⊆ EDk implies Q 6⊆ UD \\ Dk since EDk ⊇ UD \\ Dk by precondition. This in turn means that there is some formula ax in Q which is not in UD \\ Dk. Then ax ∈ Dk must hold, as otherwise for all formulas ax ′ ∈ Q it would hold that ax ′ is an entailment ofK∗k = (K\\Dk)∪B∪UP , i.e. an entailment of all formulas inK∪B∪UP except for those in Dk. However, all entailments of K∗k are stored in EDk by the implementation of the function GETENTAILMENTS. Thus Q ⊆ EDk would hold which cannot be the case as shown before. Consequently, we have derived that Q ∩ Dk 6= ∅ which means by set-minimality of diagnoses in D, in particular of Dk, that K∗k ∪ Q must violate some x ∈ R ∪ N which is a contradiction to the assumption that Dk ∈ D0(Q).\nExample 5.5 Let us come back to the example DPI given by Table 5.1. The possibility of a query Q constructed by Algorithm 4 with D0(Q) 6= ∅ is witnessed by the selection of seed S = {D1} and the assumption that entailments of the two types given in Example 5.1 are produced by GETENTAILMENTS.\nCHAPTER 5. INTERACTIVE KNOWLEDGE BASE DEBUGGING 91\nThe set of entailmentsQ = ED1 = {e4, e14, e15,∀Xm2(X)→ d(X)} (for ei cf. Table 5.2). Then,D2 as well asD3 are assigned to D−(Q) as both KBsK∗3∪Q,K∗4∪Q entailm3(w) and ¬m3(w) wherefore they are both inconsistent and thus violate r1 ∈ R. However, D4 ∈ D0(Q) since K∗i 6|= ∀Xm2(X) → d(X) and hence does not entail Q and since K∗i ∪Q does not violate consistency or coherency (recall that the set of negative test cases is empty in the DPI and thus must not be considered), i.e. does not contain a conflict set.\nApplying Proposition 5.8, we could use a modified GETENTAILMENTS function that returns a minimal set of entailments just that the precondition of the proposition is met, i.e. E′D = UD \\ D for all D ∈ D. With this function, for the seed S = {D1} we would get Q′ = E′D1 = {2, 3, 4, 5} (again, formulas in Table 5.1 are referred to just by their number). Let us now check whether D0(Q′) is indeed empty. As explicit entailments are stronger than non-explicit ones, we must still have that D2,D3 ∈ D−(Q′). For D4, we have K∗4 ∪ Q′ = {1, 3, 5, 6, 7, 8} ∪ {2, 3, 4, 5} = {1, 2, 3, 4, 5, 6, 7, 8} which corresponds to the entire KB plus background knowledge of the given DPI and includes conflict sets C1 = {1, 3, 4} and C2 = {1, 2, 3, 5} wherefore it is inconsistent. Therefore, diagnosis D4 must also be an element of D−(Q′).\nPlease note that making the entailments Q = ED1 computed by the unmodified GETENTAILMENTS function only slightly stronger would already suffice to force inclusion ofD4 in D0(Q). In fact, including ax 4 := ∀Xm2(X) → (∀Y s(X,Y ) → a(Y )) ∧ d(X) in Q instead of ∀Xm2(X) → d(X) would make Q non-disjoint with D4 as both comprise ax 4. Consequently, in line with the proof of Proposition 5.8, K∗4 ∪Q must include a conflict set ({1, 3, 4}) wherefore D4 ∈ D−(Q).\nAnother point we want to mention is that empty D0 could also be achieved by making the query slightly weaker. For our concrete query Q = ED1 , this means that leaving out ∀Xm2(X) → d(X) would lead to empty D0(Q). However, the difference to the scenario above where we made Q sightly stronger is that D4 would be an element of D+(Q) instead of D−(Q) in this case. i.e. the q-partition would be 〈{D1,D4} , {D2,D3} , ∅〉.\nA shortcoming of the strategy of making the query weaker is that it can be computationally expensive as perhaps a large number of subsets of Q might need to be considered and tested for fulfillment of D0(Q) = ∅. Each such test would involve calls to the reasoner which are usually expensive. A second drawback is that no guarantee is given to finally end up with an empty set D0(Q) since weakening of Q might also involve the “shift” of some diagnosis from D−(Q) to D0(Q). On the other hand, the strategy of computing stronger entailments is computationally more resource-saving as (trivially obtained) explicit entailments can be added to make the query stronger. Furthermore, making the query stronger – in a controlled way, by adding formulas from UD \\UD+(Q) to Q as suggested by Proposition 5.8 – can never lead to non-empty D0(Q) as Proposition 5.8 substantiates.\n(Non-)Completeness of Query Pool QP. Note that specifying q :=∞ causes GETPOOLOFQUERIES to run through all S ⊂ D and to compute a maximum number of queries. However, in general, not all theoretically possible queries are computed by GETPOOLOFQUERIES. One trivial reason for this is that only minimized, i.e. set-minimal, queries are contained in the returned set QP.\nBut, also queries Q′ with D+(Q′) = Y ⊂ D will not be included in QP if there is some query Q with D+(Q) = Y such that |D−(Q)| > |D−(Q′)| (and, equivalently, |D0(Q)| < |D0(Q′)|). As we will learn in a moment, both mentioned reasons for the incompleteness of the output of GETPOOLOFQUERIES will even be desirable for reasons of efficiency. That is, the mentioned types of queries that are not taken into account in QP are “non-preferred” as non-set-minimal queries demand a non-necessary amount of user interaction and the answering of queries Q with a non-necessarily large set D0(Q) involves a worse discrimination between leading minimal diagnoses (and, if these are “good” representatives of all minimal diagnoses, then of all minimal diagnoses) than other queries Q′ with |D0(Q′)| < |D0(Q)| and D+(Q) = D+(Q′).\nCHAPTER 5. INTERACTIVE KNOWLEDGE BASE DEBUGGING 92\nStill, GETPOOLOFQUERIES meets a completeness criterion for a subset of all queries QD,〈K,B,P,N 〉R , elements of which cannot be trivially detected to be “non-preferred”. That is, GETPOOLOFQUERIES is complete w.r.t. the set D+, as the following proposition states. In other words, for each subset X ⊂ D it detects a q-partition with D+ = X , if one exists.\nProposition 5.9. Let a DPI 〈K,B,P ,N 〉R, D ⊆ mD〈K,B,P,N 〉R such that |D| ≥ 2 and some q ∈ N ∪ {∞} , q ≥ 1 be the inputs to GETPOOLOFQUERIES and let |QPmax| ≥ 0 be the maximum number of tuples 〈Q,P(Q)〉 that can be computed by GETPOOLOFQUERIES by means of the used GETENTAILMENTS function. Further, let Y be an arbitrary subset of D. If there is some query Q ∈ QD,〈K,B,P,N 〉R that (1) includes only entailments that are computed by GETENTAILMENTS and (2) has a q-partition such that D+(Q) = Y , then GETPOOLOFQUERIES with parameter q ≥ |QPmax| returns a set QP including a query Q′ with D+(Q′) = Y . Moreover, this query Q′ is found in the iteration where the seed S = Y .\nProof. Since q ≥ |QPmax|, GETPOOLOFQUERIES will arrive at a step where it selects the seed S = Y in line 6. Now, let us assume that in this iteration no query Q with D+(Q) = Y is found. Then, either (a) no query is found at all, i.e. CQ1 or CQ2 or CQ3 are violated, or (b) a query Q with D+(Q) 6= Y is found.\n(a): Assume first that CQ1 is violated, i.e. GETCOMMONENTAILMENTS called with argument S returns ∅. This implies that the KBs K∗r for Dr ∈ Y have no common entailments, if entailments are computed by GETENTAILMENTS. This however means that there cannot be a q-partition with D+ ⊇ Y which is a contradiction to the precondition that there is some query Q ∈ QD,〈K,B,P,N 〉R that includes only entailments computed by GETENTAILMENTS and has a q-partition such that D+(Q) = Y .\nSecond, assume that CQ2 is violated, i.e. D+(Q) = ∅. If GETCOMMONENTAILMENTS with argument S returned Q 6= ∅, then D+(Q) ⊇ S ⊃ ∅ would hold. Thus, Q = ∅, i.e. CQ1 is violated. So, as shown before, this leads to a contradiction.\nIn case any of CQ1 or CQ2 is violated, we already derived a contradiction. So, we make the assumption that CQ1 and CQ2 are met. So, finally, let us assume that CQ3 is violated, i.e. that D−(Q) = ∅. That is, if Q (which must be a non-empty set by CQ1) denotes all common entailments (computable with GETENTAILMENTS) ofK∗r forDr ∈ Y , thenK∗i ∪Q does not violate any x ∈ R∪N for anyDi ∈ D\\S. Consequently, for all diagnoses Di in D we have that K∗i ∪ Q does not violate any x ∈ R ∪ N . But, as there is, by precondition, a query with D+ = Y , this query must be a subset of all possible common entailments (computable with GETENTAILMENTS) of KBs K∗i for diagnoses in Y , i.e. this query must be a subset of Q. But, by monotonicity of L, no K∗i ∪Q′ for a subset Q′ of Q can violate x ∈ R ∪ N if Q does not. Again, we have a contradiction to the precondition as above.\n(b): Here, a query Q is found with D+(Q) 6= Y and D−(Q) 6= ∅. Since Q is a query, Q 6= ∅ must hold. Since the seed S = Y , this means that Q is the set of all common entailments (computable with GETENTAILMENTS) of K∗i for Di ∈ Y , i.e. D+(Q) ⊇ Y . By D+(Q) 6= Y , we conclude that D+(Q) ⊃ Y must be true. The only way of achieving a smaller set D+(Q), namely D+(Q) = Y , is to add some formulas toQ as makingQ smaller can only increase D+(Q). This holds because postulating that, instead of Q, only a subset Q′ of Q must be entailed by K∗i , can cause a new KB K∗j for diagnosis Dj /∈ D+(Q) to entail Q′. However, as Q is the set of all entailments computable with GETENTAILMENTS of KBs K∗i for Di ∈ Y , a superset Q′′ of Q computed by GETENTAILMENTS with D+(Q′′) = Y can never be obtained. Therefore, we have a contradiction to the precondition.\nWe have now proven the following: If there exists a q-partition as described in the proposition, then this q-partition is found in the iteration where the seed S = Y .\nRemark 5.9 Regarding Proposition 5.9, note the following:\n(a) In fact, as one and the same q-partition must occur at most once in QP, GETPOOLOFQUERIES must only keep assigning diagnoses in D \\ S to the respective sets of the q-partition as long as D+ = S. Because for D+ = Z ⊃ S, we know to find a query (if one exists) for the seed S = Z.\nCHAPTER 5. INTERACTIVE KNOWLEDGE BASE DEBUGGING 93\n(b) A statement equivalent to the proposition is: If there is no query (including only entailments computed by the GETENTAILMENTS function) with D+ = Y found for seed S = Y , then such a query and q-partition, respectively, does not exist.\nThe following proposition states that if a q-partition with one and the same set D+ is found twice during the execution of GETPOOLOFQUERIES, then the queries for both q-partitions and thus both qpartitions must be equal. That is, for one set D+, there is at most one tuple in QP.\nProposition 5.10. LetQi be a query with D+(Qi) = Y in the set QP returned by GETPOOLOFQUERIES and found for seed Si = Y and let Qj be a query with D+(Qj) = Y in the set QP returned by GETPOOLOFQUERIES and found for some seed Sj ⊂ Y . Then Qi = Qj . Proof. Let Q′i, Q ′ j be the queries stored in the variable Q in line 18 for seeds Si and Sj , respectively; i.e. the supersets of the queries Qi, Qj before the minimization function MINQ is called for each of them. Q′j ⊆ Q′i holds by the fact that Q′i is the set of all common entailments computable with GETENTAILMENTS of K∗r for Dr ∈ Y and by the fact that Q′j must be a set of common entailments computed by GETENTAILMENTS of exactly these KBs, because of D+(Q′j) = Y and Definition 5.3. Q ′ j ⊇ Q′i holds by the fact thatQ′j is computed as intersection of EDr whereDr ∈ Sj andQ′i is computed as intersection of EDs where Ds ∈ Si ⊃ Sj . Thus, we can conclude that Q′i = Q′j .\nAs Q′i = Q ′ j , also P(Q ′ i) = P(Q ′ j) must hold for the q-partitions by Proposition 5.2. That the minimized versions Qi, Qj of Q′i, Q ′ j output by MINQ are equal, follows from the determinism of the MINQ function, wherefore equal inputs, i.e. (∅, Q′i, ∅,P(Q′i), 〈K,B,P ,N 〉R) = (∅, Q′j , ∅,P(Q′j), 〈K,B,P , N 〉R), must yield equal outputs.\nRemark 5.10 Proposition 5.10 hints at a possible improvement of Algorithm 4, namely to check in line 6 whether the seed S already occurs as a set D+ in some tuple in QP and only continue the execution for S if this does not hold (not shown in Algorithm 4). In this vein, time and reasoning costs (line 14) can be saved.\nAnother improvement regarding line 6 is to delete all remaining seeds S′ with the property S′ ⊃ S if Q in line 8 is the empty set (not shown in Algorithm 4). Namely, all seeds S′ must also lead to Q = ∅ since the intersection of ED for D ∈ S already returned ∅ wherefore the intersection of ED for D ∈ S′ must also return ∅.\nBy now, we know from Proposition 5.10 that, given a query with D+ exists, one and only one qpartition with D+ will be added to QP, but which one?\nW.r.t. one and the same set D+, queries with a set D− with higher cardinality are preferable over others as the cardinality of D0 should be minimized (cf. Section 5.1). So, preferable queries among those with equal set D+ are those for which D− is a set-maximal set. Exactly such a query is added to QP for each D+ for which a query exists, as the following proposition shows.\nProposition 5.11. If the set QP returned by GETPOOLOFQUERIES comprises a queryQ with D+(Q) = Y , then Q is a query with minimal |D0(Q)| among all queries Q′ with D+(Q′) = Y computable with the function GETENTAILMENTS.\nProof. Assume that GETPOOLOFQUERIES finds a query Q with D+(Q) = Y and |D0(Q)| = k and assume there is a query Q′ (consisting only of entailments computed by function GETENTAILMENTS) with D+(Q′) = Y and with |D0(Q′)| < k. This means that |D−(Q)| < |D−(Q′)|. However, as Q is computed for seed S = Y , Q is a maximal set of entailments computable with GETENTAILMENTS of K∗i for Di ∈ Y . Because Q′ is also a common entailment of K∗i for Di ∈ Y , we have that Q′ ⊆ Q must be true. Since the fact that K∗i ∪ Q does not violate any x ∈ R ∪ N , i.e. the fact that Di /∈ D−(Q), implies by monotonicity of L that K∗i ∪Q′ for the subset Q′ of Q cannot violate any x ∈ R ∪ N either, i.e. Di /∈ D−(Q′), we conclude that |D−(Q′)| ≤ |D−(Q)| must hold. This is a contradiction.\nCHAPTER 5. INTERACTIVE KNOWLEDGE BASE DEBUGGING 94"
    }, {
      "heading" : "5.2.3 Minimization of Queries",
      "text" : "MINQ. The minimization of the queryQ by MINQ (see Algorithm 4) while preserving the q-partition aims at simplifying the job of the answering user who only needs to go through a smaller set of logical formulas Qmin in order to come up with an answer to the query. Since the q-partition reflects the properties of a query w.r.t. the invalidation of (leading) diagnoses and two queries have equal such properties, then of course the one that is a subset of the other should be asked.\nThe concept of the function MINQ is similar to the one of QX (Algorithm 1). Like QX, MINQ carries out a divide-and-conquer strategy to find a set-minimal set with a monotonic property. In this case, the monotonic property is not the invalidity of a subset of the KB w.r.t. a DPI (as per Definition 3.3) as it is for the computation of minimal conflict sets using QX, but the property of some Qmin ⊂ Q having the same q-partition as Q. So, the crucial difference between QX and MINQ is the function that checks this monotonic property. For MINQ, this function – that checks a subset of a query for constant q-partition – is ISQPARTCONST.\nMINQ – Input Parameters. MINQ gets five parameters as input. The first three, namelyX,Q andQB, are relevant for the divide-and-conquer execution, whereas the last two, namely the original q-partition〈 D+,D−,D0 〉 of the query (i.e. the parameter Q) that should be minimized, and the DPI 〈K,B,P ,N 〉R are both needed as an input to the function ISQPARTCONST. Besides the latter two, another argument QB is passed to this function where QB is a subset of the original query Q. ISQPARTCONST then checks whether the q-partition for the (potential) query QB is equal to the q-partition 〈 D+,D−,D0\n〉 of the original query given as argument. The DPI is required as the parameters K,B,P ,N and R are necessary for these checks.\nMINQ – Testing Sub-Queries for Constant Q-Partition. In particular, ISQPARTCONST tests for each Dr ∈ D− whether K∗r ∪QB is valid (w.r.t. 〈·, ∅, ∅, N〉R). If so, this means that Dr /∈ D−(QB) and thus that the q-partition of QB is different to the one of Q wherefore false is immediately returned. If true for all Dr ∈ D−, it is tested for Dr ∈ D0 whether K∗r |= QB. If so, this means that Dr /∈ D0(QB) and thus that the q-partition of QB is different to the one of Q wherefore false is immediately returned. If false is not returned for any Dr ∈ D− or Dr ∈ D0, then the conclusion is that QB is a query w.r.t. to D and 〈K,B,P ,N 〉R and has the same q-partition as Q wherefore the function returns true .\nNote that, instead of calling a reasoner to answer whether K∗r |= QB, the set of precalculated entailments EDr of K∗r for each Dr ∈ D can be given as an argument to MINQ as well as to ISQPARTCONST (not shown in Algorithm 4). In this case an equivalent test is QB ⊆ EDr . Such a strategy is particularly appropriate if reasoning is expensive for the DPI at hand.\nSoundness of ISQPARTCONST is proven by the following lemma.\nLemma 5.1. Let 〈K,B,P ,N 〉R be a DPI, D ⊆ mD〈K,B,P,N 〉R , Q ∈ QD,〈K,B,P,N 〉R with q-partition P(Q) = 〈 D+(Q),D−(Q),D0(Q) 〉 . Then a non-empty set QB ⊂ Q is a query in QD,〈K,B,P,N 〉R with P(QB) = P(Q) if\n1. ∀Dr ∈ D−(Q) : K∗r ∪QB violates some r ∈ R or entails some n ∈ N and\n2. ∀Dr ∈ D0(Q) : K∗r 6|= QB.\nProof. Let Q ∈ QD,〈K,B,P,N 〉R and QB be an arbitrary proper subset of Q. If criterion 1) of this lemma is met, then we know that each diagnosis in D−(Q) is in D−(QB) as well, i.e. (I): D−(QB) ⊇ D−(Q) holds.\nAssume a minimal diagnosis Dr ∈ D0(Q). Then, K∗r ∪ Q does not violate any r ∈ R and does not entail any n ∈ N andK∗r does not entailQ. This however implies thatK∗r ∪QB cannot violate any r ∈ R and cannot entail any n ∈ N either by monotonicity of L. But it is possible that K∗r |= QB. So, validity\nCHAPTER 5. INTERACTIVE KNOWLEDGE BASE DEBUGGING 95\nof criterion 2) of this lemma is sufficient to guarantee that each diagnosis in D0(Q) is in D0(QB) as well, i.e. (II): D0(QB) ⊇ D0(Q) holds.\nAs all diagnoses in D+(Q) entail all formulas in Q by Definition 5.3, all diagnoses in D+(Q) must entailQB as well. Consequently, due to deletion of some formulas fromQ, noDr ∈ D+(Q) can “move” to any set D−(QB) or D0(QB). That is, (III): D+(QB) ⊇ D+(Q) must hold.\nSo, the overall conclusion is that, if criterion 1) and 2) are met, then (I), (II) and (III) hold. Assume that some ⊇-relation in i ∈ {(I), (II), (III)} is a ⊃-relation. This leads to a violation of some j ∈ {(I), (II), (III)} with j 6= i since 〈 D+(Q),D−(Q),D0(Q) 〉 and 〈 D+(QB),D−(QB),D0(QB) 〉 are partitions of D. Therefore, all ⊇-relations must be =-relations and we can derive that P(Q) = P(QB). Moreover, we have that QB must be a query. This is due to the facts that QB is non-empty, Q is a query and the q-partitions of Q and QB are equal. Therefore, D+(QB) = D+(Q) ≥ 1 and D−(QB) = D−(Q) ≥ 1 which lets us conclude by Proposition 5.4 that QB is a query.\nMINQ – The Divide-and-Conquer Strategy. Intuitively, MINQ partitions the given query Q in two parts Q1 and Q2 and first analyzes Q2 while Q1 is part of QB (line 34). Note that in each iteration QB is the subset of Q that is currently assumed to be part of the sought minimized query (i.e. the one query that will finally be output by MINQ). In other words, analysis of Q2 while Q1 is part of QB means that all irrelevant formulas in Q2 should be located and removed from Q2 resulting in Qmin2 ⊆ Q2. That is, Qmin2 must include only relevant formulas which means that Q min 2 along with QB is a query with an equal q-partition as Q, but the deletion of any further formula from Qmin2 changes the q-partition. After the relevant subset Qmin2 of Q2, i.e. the subset that is part of the minimized query, has been returned, Q1 is removed from QB, Qmin2 is added to QB and Q1 is analyzed for a relevant subset that is part of the minimized query (line 35). This relevant subset, Qmin1 , together with Q min 2 , then builds a set-minimal subset of the input Q that is a query and has a q-partition equal to that of Q. Note that the argument X of MINQ is the subset of Q that has most recently been added to QB.\nFor each call in line 34 or line 35, the input Q to MINQ is recursively analyzed until a trivial case arises, i.e. (a) until Q is identified to be irrelevant for the computed minimized query wherefore ∅ is returned (lines 27 and 28) or (b) until |Q| = 1 and Q is not irrelevant for the computed minimized query wherefore Q is returned (lines 29 and 30).\nExample 5.6 Let us reconsider the FOL DPI depicted by Table 5.1 on page 83. We recall that sets of minimal conflict sets and minimal diagnoses w.r.t. this DPI were given by mC〈K,B,P,N 〉R = {C1, C2} = {〈1, 3, 4〉 , 〈1, 2, 3, 5〉} as well as mD〈K,B,P,N 〉R = {D1,D2,D3,D4} = {[1], [3], [4, 5], [2, 4]}. For this DPI, a set of minimized queries computed by GETPOOLOFQUERIES is presented by Table 5.3. Note that these queries have been produced by different GETENTAILMENTS functions (as indicated by the dashed lines in Table 5.3). That is, Qi for i ∈ {1, . . . , 5} have been produced by the same GETENTAILMENTS function that is described in Example 5.1. For i ∈ {6, . . . , 9}, Qi has been computed from a GETENTAILMENTS function that outputs only explicit entailments (cf. Definition 5.4) and Q10 from a GETENTAILMENTS function that returns a finite set of entailments where each entailment is some FOL formula. This could be accomplished, for example, by some resolution-based reasoning procedure [10].\nIt is important to realize that the results regarding Algorithm 4 established so far, most of which depend on the particular used GETENTAILMENTS function, must only hold within one part of Table 5.3 (where different parts are separated by the dashed lines). For example, for Q2 and Q9 it holds that D+(Q2) = D\n+(Q9), but D−(Q2) 6= D−(Q9) and D0(Q2) 6= D0(Q9). By application of one and the same GETENTAILMENTS function, this case would be prohibited by Proposition 5.10. Furthermore, by Proposition 5.11, only Q9 would be an element of the query pool QP in this case since D0(Q9) ⊂ D0(Q2).\nMoreover, we want to remark that Q7, Q8 and Q9 can be seen as a proof that Q6 is indeed setminimal. Each Qi, i ∈ {7, 8, 9} is a result of the removal of a single formula from Q6. And, each such\nCHAPTER 5. INTERACTIVE KNOWLEDGE BASE DEBUGGING 96\nQi features a q-partition different from the one of Q6. This illustrates quite well the principle of MINQ which performs tests of exactly this kind to verify minimality of a query or detect formulas that might be deleted from it under preservation of the q-partition, respectively.\nAnother essential note is that it is guaranteed that D0(Q6) = ∅. This holds due to the construction of Q6 as UD \\ D4 = {1, 2, 3, 4, 5} \\ [2, 4] = {1, 3, 5} (recall that we use squared brackets to denote diagnoses in spite of the fact that these are sets, cf. Table 2.1). So, Q6 comprises all formulas occurring in minimal diagnoses except for the ones contained in D4. We have that for any two different minimal diagnoses Di,Dj w.r.t. one and the same DPI it must be true that Di \\ Dj 6= ∅ as well as Dj \\ Di 6= ∅ as otherwise one would be necessarily a subset of the other. From this, we can easily derive that K∗i ∪ Q6 for i ∈ {1, . . . , 3}, i.e. for all minimal diagnoses Di w.r.t. this DPI other than D4 which was used to build the query Q6, must comprise a conflict set. This must be valid by the minimality of Di and since by Q6 at least one formula of Di is readded to the KB. Note that a similar argumentation was used in the proof of Proposition 5.8."
    }, {
      "heading" : "5.2.4 Soundness of Query Minimization",
      "text" : "The following lemma shows that the function ISQPARTCONST used by MINQ is indeed a monotonic function (cf. Definition 4.6), which is a necessary prerequisite for versions of the QX algorithm to work in a sound way.\nLemma 5.2. Let 〈K,B,P ,N 〉R be a DPI, D ⊆ mD〈K,B,P,N 〉R , Q ∈ QD,〈K,B,P,N 〉R with q-partition P(Q). Further, let f : 2Q → {0, 1} be a function that maps a subset QB of Q to 1 if QB has q-partition P(QB) = P(Q), to 0 otherwise. Then, f is a monotonic function (as per Definition 4.6).\nProof. Assume a subset Q′ of Q with f(Q′) = 1, i.e. Q′ has q-partition P(Q′) = P(Q). Let Q′ ⊂ Q′′ ⊆ Q and assume that f(Q′′) = 0, i.e. Q′′ has a q-partition P(Q′′) 6= P(Q).\nAs shown in the proof of Lemma 5.1, D+(X1) ⊇ D+(X2) holds for any X1 ⊆ X2. Therefore, we have D+(Q′) ⊇ D+(Q′′) ⊇ D+(Q) and by P(Q′) = P(Q) that D+(Q′) = D+(Q) and thus that all ⊇-relations are =-relations. So, either D−(Q′′) 6= D−(Q) or D0(Q′′) 6= D0(Q) must hold.\nFirst, assume that D−(Q′′) 6= D−(Q). Then, as K∗r ∪ Q′′ ⊂ K∗r ∪ Q and by monotonicity of L, it can only be the case that for some Dr ∈ D some x ∈ R ∪ N that is violated for K∗r ∪ Q is not violated for K∗r ∪Q′′. Hence, D−(Q′′) ⊂ D−(Q) must hold. By a similar argumentation – without the\nCHAPTER 5. INTERACTIVE KNOWLEDGE BASE DEBUGGING 97\nassumption that D−(Q′) 6= D−(Q′′) holds – we have that D−(Q′) ⊆ D−(Q′′) and thus, altogether, that D−(Q′) ⊂ D−(Q) must be true. Due to P(Q′) = P(Q) we know that D−(Q′) = D−(Q) which is a contradiction.\nFinally, assume that D0(Q′′) 6= D0(Q). Since K∗r ∪ Q does not violate any x ∈ R ∪ N for Dr ∈ D0(Q),K∗r∪Q′′ cannot violate any x ∈ R∪N by monotonicity ofL. As a conclusion, the only possibility for D0(Q′′) 6= D0(Q) is that K∗r |= Q′′ for some Dr ∈ D0(Q), i.e. that Dr ∈ D+(Q′′) which implies that D0(Q′′) ⊂ D0(Q). By a similar argumentation – without the assumption that D0(Q′) 6= D0(Q′′) holds – we have that D0(Q′) ⊆ D0(Q′′) and thus, altogether, that D0(Q′) ⊂ D0(Q) must be true. Due to P(Q′) = P(Q) we know that D0(Q′) = D0(Q) which is a contradiction.\nThis completes the proof for monotonicity of the given function f .\nProposition 5.12 (Correctness of MINQ). Given a query Q ∈ QD,〈K,B,P,N 〉R as input, MINQ computes a subset Qmin ⊆ Q such that P(Qmin) = P(Q) and there is no Q′ ⊂ Qmin such that P(Q′) = P(Q).\nProof. This proposition is a consequence of the correctness of QX shown by Proposition 4.9, of the correctness of function ISQPARTCONST established by Lemma 5.1 and of the monotonicity of the property tested by the function ISQPARTCONST guaranteed by Lemma 5.2."
    }, {
      "heading" : "5.2.5 Complexity of Query Pool Generation",
      "text" : "The complexity of query minimization, i.e. one call to MINQ, in terms of calls to the ISQPARTCONST function is directly obtained from the complexity results for the standard QX algorithm given by Proposition 4.8.\nProposition 5.13 (Complexity of MINQ). Let 〈K,B,P ,N 〉R be a DPI, D ⊆ mD〈K,B,P,N 〉R , Q ∈ QD,〈K,B,P,N 〉R with P(Q) = 〈 D+(Q),D−(Q),D0(Q) 〉 and the function SPLIT (line 31 of Algorithm 4) be defined as SPLIT(n) = bn2 c where n is a natural number. Then, the worst case number of calls to ISQPARTCONST during one call to MINQ(∅, Q, ∅,P(Q), 〈K,B,P ,N 〉R) is in\nO ( |Qmin| log\n|Q| |Qmin| ) where Qmin is the output of MINQ(∅, Q, ∅,P(Q), 〈K,B,P ,N 〉R).\nFor any other definition of the function SPLIT, the worst case number of calls to ISQPARTCONST gets larger.\nThe overall complexity of GETPOOLOFQUERIES in terms of calls to functions that call the reasoner, i.e. functions GETENTAILMENTS, ISKBVALID and ISQPARTCONST, is established by the following proposition.\nProposition 5.14 (Complexity of GETPOOLOFQUERIES). Let 〈K,B,P ,N 〉R be a DPI, q a natural number and D ⊆mD〈K,B,P,N 〉R . Then, the worst case number of calls to functions that call a reasoner during one call to GETPOOLOFQUERIES(〈K,B,P ,N 〉R,D, q) is in\nO |D|+ ∣∣∣Q(max)min ∣∣∣ log ∣∣Q(max)∣∣∣∣∣Q(max)min ∣∣∣  2|D| \nwhere ∣∣Q(max)∣∣ is the maximum size of a query before minimization, i.e. the size of the set of maximum\ncardinality that is stored in variable Q in line 19 throughout all iterations, and ∣∣∣Q(max)min ∣∣∣ is the maximum size of a minimized query, i.e. the size of the set of maximum cardinality that is stored in variable Q′ in line 19 throughout all iterations.\nCHAPTER 5. INTERACTIVE KNOWLEDGE BASE DEBUGGING 98\nProof. During the execution of the for-loop over lines 3-5 the function GETENTAILMENTS is called |D| times. During the execution of the for-loop over lines 6-22 which may be executed at most 2|D|−2 times, ISKBVALID is called at most |D| − 1 times since |S| ≥ 1 and S ⊂ D and thus |D \\ S| ≤ |D| − 1 holds; furthermore, MINQ may be called once, namely if the condition tested by the if-statement in line 18 is true. During one execution of MINQ, by Proposition 5.13, at most\n|Qmin| log |Q| |Qmin|\ncalls to ISQPARTCONST are made where Qmin is the output of the call to MINQ. So, an upper bound of the number of calls to ISQPARTCONST performed by one call to MINQ among all calls to MINQ throughout the execution of GETPOOLOFQUERIES, is∣∣∣Q(max)min ∣∣∣ log\n∣∣Q(max)∣∣∣∣∣Q(max)min ∣∣∣ where\n∣∣∣Q(max)min ∣∣∣ is the set of maximum cardinality that is stored in variable Q′ in line 19 throughout all iterations and\n∣∣Q(max)∣∣ is the set of maximum cardinality that is stored in variableQ in line 19 throughout all iterations.\nSo, all in all we know that functions that call a reasoner are invoked at most\n|D|+ |D| − 1 + ∣∣∣Q(max)min ∣∣∣ log ∣∣Q(max)∣∣∣∣∣Q(max)min ∣∣∣  (2|D| − 2) times during the execution of GETPOOLOFQUERIES. Since|D|+ ∣∣∣Q(max)min ∣∣∣ log ∣∣Q(max)∣∣∣∣∣Q(max)min ∣∣∣  2|D|\nis an upper bound of this number, the proposition holds.\nNote that none of the parameters that affect the complexity of the function GETPOOLOFQUERIES grows with the size of the DPI provided as an input to the interactive KB debugging problem. Merely the costs for reasoning, where a black-box debugging approach has no influence on, are affected by a higher complexity or larger size of the input DPI. Moreover, the size of the most relevant parameter influencing the worst case complexity, namely the exponent |D|, can be specified by the user to any value greater or equal to 2. In other words, minus reasoning time, the generation of a pool of queries is a fixed parameter tractable problem [13] in the context of interactive KB debugging."
    }, {
      "heading" : "5.2.6 Shortcomings of Query Pool Generation",
      "text" : "First, the exponential time complexity regarding the parameter |D| is a problem arising from the paradigm of computing an optimal query w.r.t. a certain quantitative measure qsm() such as information gain [74, 63] by calculating a (generally exponentially large) pool QP of queries in a first stage, whereupon qsm(Q) ∈ R is evaluated for Q ∈ QP until the one Q∗ with optimal qsm(Q∗) is found and selected as the query to be asked to the user.\nA key to solving this issue is the use of a different paradigm that does not rely on the computation of the pool QP. Instead, qualitative measures can be derived from quantitative measures that have been used\nCHAPTER 5. INTERACTIVE KNOWLEDGE BASE DEBUGGING 99\nin interactive debugging scenarios [74, 63, 73]. These qualitative measures provide a way to estimate the qsm() value of partial q-partitions, i.e. ones where not all leading diagnoses have been assigned to the respective set in the q-partition yet. That way a direct search for a query with (nearly) optimal properties is possible. A similar strategy called CKK has been employed in [74] for the information gain measure (see Section 5.3.3). From such a technique we can expect to save a high number of reasoner calls. Because only a usually small subset of q-partitions included in the pool computed by GETPOOLOFQUERIES is required to find a query with desirable properties if the search is implemented by means of a heuristic that involves the exploration of seemingly favorable (potential) queries and (partial) q-partitions, respectively, first. This is a topic of future work.\nAnother shortcoming of GETPOOLOFQUERIES is the extensive use of reasoning services which may be computationally expensive (depending on the given DPI). Instead of computing a set of common entailments Q of a set of KBs K∗i first and consulting a reasoner to fill up the (q-)partition for Q in order to test whether Q is a query at all, the idea enabling a significant reduction of reasoner dependence is to compute some kind of canonical query without a reasoner and use simple set comparisons to decide whether the associated partition is a q-partition. Guided by qualitative properties mentioned before, a search for such q-partition with desirable properties can be accomplished without reasoning at all. Also, a set-minimal version of the optimal canonical query can be computed without reasoning aid. Only for the optional enrichment of the identified optimal canonical query by additional entailments and for the subsequent minimization of the enriched query, the reasoner may be employed. This is also a topic of future work.\nAnother aspect that can be improved is that only one minimized version of each query is computed by Algorithm 4. That is, per q-partition P, there might be some set-minimal queries which do not occur in the output set QP. From the point of view of how well a query might be understood by an interacting user, of course not all minimized queries can be assumed equally good in general. Hence, in order to avoid a situation where a potentially best-understood query w.r.t. P is not included in QP, the query minimization process (see Section 5.2.3) might be adapted to take into account some information about faults the interacting user is prone to. This could be exploited to estimate how well this user might be able to understand and answer a query. For instance, given that the user frequently has problems to apply ∃ in a correct manner to express what they intend to express, but has never made any mistakes in formulating implications →, then the query Q1 = {∀X p(X)→ q(X), r(a)} might be better comprehended than Q2 = {∀X∃Y s(X,Y )}. One way to achieve the finding of a well-understood query for some q-partition P is to run the query minimization MINQ more than once, each time with a modified input (using a hitting set tree to accomplish this in a systematic manner – cf. Chapter 4, where an analogue idea is used to compute different minimal conflict sets w.r.t. a DPI). In this way, different set-minimal queries for P can be identified and the process can be stopped when a suitable query is found."
    }, {
      "heading" : "5.2.7 Correctness of Query Pool Generation",
      "text" : "The following proposition confirms the correctness of Algorithm 4, i.e. of the function GETPOOLOFQUERIES. Roughly, it states that the output of QP of the function is duplicate-free, i.e. no query or q-partition occurs twice in QP, that QP includes only queries and q-partitions, that tuples in QP are unique w.r.t. the set D+ of a q-partition and that, given q > |QP|, there is no subset Y of D for which a q-partition with D+ = Y exists and for which no q-partition with D+ = Y is an element of QP.\nProposition 5.15. Let a DPI 〈K,B,P ,N 〉R, D ⊆ mD〈K,B,P,N 〉R such that |D| ≥ 2 and some q ∈ N ∪ {∞} , q ≥ 1 be the inputs to GETPOOLOFQUERIES and let |QPmax| ≥ 0 be the maximum number of tuples 〈Q,P(Q)〉 that can be computed by GETPOOLOFQUERIES by means of the used GETENTAILMENTS function. If q ≥ |QPmax| (in particular q =∞), then\n1. there are no two tuples 〈Q,P(Q)〉 , 〈Q′,P(Q′)〉 in QP such that Q = Q′ or P(Q) = P(Q′), and\nCHAPTER 5. INTERACTIVE KNOWLEDGE BASE DEBUGGING 100\n2. QP includes a tuple 〈 Q, 〈 D+(Q),D−(Q),D0(Q) 〉〉 only if Q ∈ QD,〈K,B,P,N 〉R , and\n3. QP includes at most one tuple where D+(Q) = Y for each Y ⊂ D, and\n4. for each Y ⊂ D for which a query Q w.r.t. D and 〈K,B,P ,N 〉R exists such that\n(a) Q includes only entailments computed by the used GETENTAILMENTS function and\n(b) P(Q) is such that D+(Q) = Y ,\nQP includes a tuple 〈Q′,P(Q′)〉 such that D+(Q′) = Y , and\n5. QP 6= ∅.\nIf q < |QPmax|, then QP includes q tuples satisfying (1), (2) and (3).\nProof. Statement (1) is a consequence of Proposition 5.7. Statement (2) is an implication of Proposition 5.6 and Proposition 5.12. The former says that only sets Q that are actually queries w.r.t. D and 〈K,B,P ,N 〉R can pass line 18. Thus, only queries are passed to MINQ as parameter Q. By the latter which states that MINQ is correct, i.e. outputs a query if the input is a query, statement (2) follows. Statement (3) follows from Proposition 5.10. If q ≥ |QPmax|, the truth of statement (4) is witnessed by Proposition 5.9. Statement (5) is true by lines 23 and 24 and by Proposition 5.5 as well as Corollary 5.4 and the premise that |D| ≥ 2 which guarantee that the function ADDTRIVIALQUERIES always adds at least |D| ≥ 2 > 0 queries to QP. In case q < |QPmax|, only statements (1), (2) and (3) are satisfied in general (for the same reasons as given above for the case q ≥ |QPmax|) and QP is returned in line 22 by the definition of |QPmax|. Thence, the condition |QP| = q ≥ 1 tested in line 21 must be valid for QP.\nCHAPTER 5. INTERACTIVE KNOWLEDGE BASE DEBUGGING 101\nAlgorithm 5 Interactive KB Debugging Input: a tuple 〈 〈K,B,P ,N 〉R, nmin, nmax, t, pK̃∪K, q, qsm(), σ,mode 〉 consisting of\n• an admissible DPI 〈K,B,P ,N 〉R, • leading diagnoses computation parameters, natural numbers nmin ≥ 2, nmax, t, • a function pK̃∪K : K̃ ∪ K → (0, 1], • a parameter q ∈ N ∪ {∞} , q ≥ 1 that determines the size of the computed query pool, • a function qsm(Q) ∈ R used for query selection that assigns a real number to a query Q to express the\n“goodness” of Q,\n• a maximum fault tolerance σ ∈ [0, 1] and • a mode mode ∈ {static, dynamic} that determines the used method for diagnosis computation.\nOutput: The output depends on mode and σ:\n• mode = static: a maximal solution KB w.r.t. the input DPI 〈K,B,P ,N 〉R which is – an approximation of the solution to Interactive Static KB Debugging (Problem Def. 5.2) if σ > 0. – the (exact) solution to Interactive Static KB Debugging if σ = 0.\n• mode = dynamic: a maximal solution KB w.r.t. the current DPI 〈K,B,P ∪ P ′,N ∪N ′〉R which is – an approximation of the solution to Interactive Dynamic KB Debugging (Problem Def. 5.1) if σ > 0. – the (exact) solution to Interactive Dynamic KB Debugging if σ = 0.\n(for a more formal and precise characterization of the output see Proposition 5.16 on page 105)\n1: P ′,N ′,Ccalc,DX,D×,Dout,D⊃, qData← ∅ 2: Qdup, QA← [] 3: Q← [∅] 4: answer ← false 5: pK()← GETFORMULAPROBS(K, pK̃∪K()) . application of Formulas 4.2 and 4.7 6: while true do 7: if mode = static then . see Algorithm 7 8: 〈DX,Q,Ccalc,D×〉 ← STATICHS(〈K,B,P ,N 〉R,Q, t, nmin, nmax,\nCcalc,DX,D×, pK(),P ′,N ′)\n9: else . see Algorithms 8, 9 and 10 10: 〈DX,Q,Ccalc,D×,D⊃,Qdup〉 ← DYNAMICHS(〈K,B,P ,N 〉R,Q,Qdup, t, nmin, nmax,\nCcalc,DX,D×, pK(),P ′,N ′,D⊃)\n11: pD()← GETPROBDIST(DX, pK(), 〈K,B,P ,N 〉R, QA) . see Algorithm 6 12: Dmax ← GETMODE(DX, pD()) 13: if pD(Dmax) ≥ 1− σ then . stop criterion 14: return GETSOLKB(Dmax, 〈K,B,P ∪ P ′,N ∪N ′〉R,P ′,mode) . return solution KB 15: else 16: 〈Q,P(Q)〉 ← CALCQUERY(DX, qData, pD(), pK̃∪K(), qsm(), 〈K,B,P ∪ P ′,N ∪N ′〉R, q) . see Algorithm 6 17: answer ← u(Q) . user interaction 18: QA← APPEND(〈Q, answer〉 , QA) 19: Dout ← GETINVALIDDIAGS(P(Q), answer) 20: qData← UPDATEQDATA(Dout,DX, answer) 21: DX ← DX \\Dout 22: D× ← D× ∪Dout 23: if answer = true then 24: P ′ ← P ′ ∪ {Q} 25: else 26: N ′ ← N ′ ∪ {Q}\nCHAPTER 5. INTERACTIVE KNOWLEDGE BASE DEBUGGING 102\nAlgorithm 6 Interactive KB Debugging (continued) 27: procedure GETPROBDIST(DX, pK(), 〈K,B,P ,N 〉R, QA) 28: P ′′,N ′′ ← ∅ 29: pD,prio()← GETPRIODIAGPROBS(DX, pK(), 〈K,B,P ,N 〉R) . application of Formula 4.3 30: for 〈Q, u(Q)〉 ∈ QA do . run through chronologically sorted query-answer pairs 31: if u(Q) = true then 32: for Dr ∈ DX do . function GETENTAILMENTS is defined on page 87 33: EDr ← GETENTAILMENTS(Dr,K,B,P ∪ P ′′) . EDr is a set of entailments of K∗r 34: if Q 6⊆ EDr then . Dr ∈ D0(Q) 35: pD,prio(Dr)← 12 pD,prio(Dr) 36: P ′′ ← P ′′ ∪ {Q} 37: else 38: for Dr ∈ DX do . ISKBVALID (see Algorithm 1) 39: if ISKBVALID((K \\ Dr) ∪Q, 〈·,B,P ∪ P ′′,N ∪N ′′〉R) then . Dr ∈ D\n0(Q) 40: pD,prio(Dr)← 12 pD,prio(Dr) 41: N ′′ ← N ′′ ∪ {Q} 42: sum← ∑ Dr∈DX pD,prio(Dr) 43: for Dr ∈ DX do 44: pD,prio(Dr)← 1sum pD,prio(Dr) . normalization 45: return pD,prio()\n46: procedure CALCQUERY(DX, qData, pD(), pK̃∪K(), qsm(), 〈K,B,P ∪ P ′,N ∪N ′〉R, q) 47: QP← GETPOOLOFQUERIES(〈K,B,P ∪ P ′,N ∪N ′〉R,DX, q) . see Algorithm 4 48: return SELECTBESTQUERY(QP, qData, pD(), pK̃∪K(), qsm()) . see Section 5.3.3"
    }, {
      "heading" : "5.3 An Algorithm for Interactive Knowledge Base Debugging",
      "text" : "In this section we will give a description of an algorithm for interactive KB debugging (Algorithm 5) which implements the entire functionality required by an interactive debugging system. All other algorithms presented so far will be subroutines of Algorithm 5 which are either directly or indirectly called by it. Before we explain and discuss Algorithm 5 in detail, we give the reader a rough and informal overview of the algorithm’s input, output and actions in the following section in order to make the details of the algorithm easier to digest.\nRemark 5.11 Note, in the following, when we speak of the input DPI we refer to the DPI 〈K,B,P ,N 〉R that is provided as an input to Algorithm 5, by the current DPI we mean the DPI 〈K,B,P ∪ P ′,N ∪N ′〉R where P ′ and N ′, respectively, are all positive and negative test cases added to the input DPI from the start of the algorithm’s execution until the current point in time. Further on, an intermediate (or previous) DPI denotes a DPI 〈K,B,P ∪ P ′′,N ∪N ′′〉R which is not the current DPI and where ∅ ⊆ P ′′ ⊆ P ′ and ∅ ⊆ N ′′ ⊆ N ′. Finally, the last-but-one DPI corresponds to an intermediate DPI 〈K,B,P ∪ P ′′,N ∪N ′′〉R where either |P ′| = |P ′′|+ 1 or |N ′| = |N ′′|+ 1 is true, but not both."
    }, {
      "heading" : "5.3.1 Interactive Debugging Algorithm: Overview",
      "text" : "Input:\nAn admissible DPI and some meta information where the latter consists of\n• fault probabilities of syntactical elements occurring in the KB,\nCHAPTER 5. INTERACTIVE KNOWLEDGE BASE DEBUGGING 103\n• a minimal and desired number of leading diagnoses,\n• a desired maximum reaction time (time between two successive queries presented to the user),\n• a maximum fault tolerance (roughly, the probability of being presented a non-desired solution KB as output),\n• a measure for query selection (determines which query is the best query within a given set of queries),\n• a parameter that determines the size of the computed pool of queries in each iteration and\n• a parameter specifying the way the hitting set tree for computation of leading diagnoses is constructed and updated.\nOutput: A solution KB such that the diagnosis used to formulate the solution KB has a probability (w.r.t. the current leading diagnoses) greater than or equal to 1 minus the given maximum fault tolerance.\nProcedure:\n1. Initialization: Compute the fault probability of each formula in the KB by means of the given fault probabilities.\n2. Leading Diagnoses Computation: Use a hitting set tree constructed and updated in a manner as specified in the input coupled with QX to calculate a set of leading diagnoses. In that, the cardinality and computation time of the set of leading diagnoses is determined by the corresponding input parameters specifying minimal and desired number of leading diagnoses and desired reaction time.\n3. Probability Update and Stop Criterion: Use the formula fault probabilities and the new information obtained by already specified test cases (answered queries) to compute updated (posterior) probabilities of the current leading diagnoses. If one diagnosis probability is greater than or equal to 1 minus the maximum fault tolerance, return the solution KB obtained by deletion of this diagnosis from the KB and subsequent addition of the union of all positive test cases.\n4. Query Generation and Selection: Use the set of leading diagnoses (and possibly their fault probabilities) to generate a pool of queries, the size of which depends on the respective parameter provided as input. Given the pool of queries, select the best query according to the given query selection measure.\n5. User Interaction and Incorporation of New Information: Ask the user the selected query and add it to the positive test cases in case of a positive answer and to the negative test cases otherwise.\n6. Hitting Set Tree Update: Update the hitting set tree based on the new information given by the classification of the test case resulting from the query answer. In particular, this involves the deletion of all those minimal diagnoses that conflict with the new test case.\n7. Repeat from Step 2."
    }, {
      "heading" : "5.3.2 Interactive Debugging Algorithm: Detailed Description",
      "text" : "To describe the detailed process of Algorithm 5, we first characterize the input arguments, the output and the meaning of the variables used and then provide a step-by-step textual description of the actions taken by the algorithm.\nCHAPTER 5. INTERACTIVE KNOWLEDGE BASE DEBUGGING 104"
    }, {
      "heading" : "5.3.2.1 Input Arguments",
      "text" : "The input parameters of Algorithm 5 are the following:\n• An admissible DPI 〈K,B,P ,N 〉R (cf. Definition 3.6).\n• Natural numbers nmin ≥ 2, nmax, t for leading diagnoses calculation (see description in Section 5.1 on page 78).\nRemark: The postulation nmin ≥ 2 is necessary in order for the existence of queries w.r.t. any computed set of leading minimal diagnoses D and 〈K,B,P ,N 〉R to be guaranteed (see Proposition 5.5).\n• A function pK̃∪K : K̃ ∪ K → (0, 1] that assigns a fault probability pK̃∪K(e) to each e ∈ K̃ ∪ K reflecting the degree of belief that (one occurrence of) a syntactical element e appearing in K is faulty (see Section 4.5).\nRemarks: Forbidding a probability of zero for syntactical elements assures that no formula in K can have a probability of zero (cf. Remark 4.5).\nRecall from Section 4.5.1 that K̃ refers to the signature of K (cf. Chapter 2) and K denotes the set of all logical connectives occurring inK. From probabilities of logical connectives and elements of the signature, probabilities of formulas in K and from those in turn probabilities of diagnoses w.r.t. the DPI can be derived as shown by Formulas 4.2 and 4.3.\nFurther note that in the description of the algorithms in this section, unlike in Section 4.5, we use different denotations for probabilities of syntactical elements (pK̃∪K), formulas (pK()) and diagnoses (pD()) in order to make a clear distinction between these different functions.\n• A natural number q ≥ 1 that denotes the number of queries that should be precomputed, i.e. the preferred size of the query pool QP (see Section 5.2), before the “best” tuple 〈Q∗,P(Q∗)〉 is selected from QP.\nRemark: In general, higher q implies better quality of the selected query in terms of the query selection measure qsm() (see next bullet point). The chance of locating a good query in a larger set of queries is higher. On the other hand, higher q involves a worse reaction time, i.e. time between two successive queries. The more queries are computed, the more time the function GETPOOLOFQUERIES consumes.\n• A query selection measure qsm() where qsm : QP → R is a function that assigns a real-valued number qsm(〈Q,P(Q)〉) to each tuple in QP, often called the score of 〈Q,P(Q)〉. Remark: qsm() defines what is considered the “best” query in the set QP, namely the query Q∗ in the tuple 〈Q∗,P(Q∗)〉 with best score among all tuples in the pool QP. Diverse measures that can be used as a qsm() function in this algorithm have been discussed and evaluated within the scope of interactive KB debugging in literature [74, 63] (for details see Section 5.3.3).\n• A maximum fault tolerance σ that defines the stop criterion of the algorithm. That is, for a current set of leading diagnoses, the stop criterion is satisfied iff the most probable leading diagnosis has an (updated) probability of at least 1−σ (see below for a precise definition of what “updated” means). Remark: The smaller σ is chosen, the higher is the chance that a desired diagnosis is found. Selecting σ := 0, i.e. admitting zero fault tolerance, is the safest (but also most time-consuming) way to run a debugging session with Algorithm 5, as in this case the session will stop only after all but one diagnosis have been invalidated by test cases.\n• A mode mode ∈ {static, dynamic} that determines\nCHAPTER 5. INTERACTIVE KNOWLEDGE BASE DEBUGGING 105\n(i) which type of leading diagnoses are computed, i.e. only minimal diagnoses w.r.t. the input DPI (static) or minimal diagnoses w.r.t. the current DPI (dynamic),\n(ii) the hitting set tree pruning strategy after a query has been answered, i.e. conservative pruning (static) or invasive pruning (dynamic),\n(iii) the space and time complexity of diagnosis computation, i.e. not much affected by the asked queries (static) – tree is almost monotonically growing, but cannot get larger in size than the complete non-interactive hitting set tree (the tree produced by Algorithm 2 with input nmin = ∞) – or significantly influenced by the asked queries (dynamic) – tree may shrink significantly if new test cases do not introduce “completely new” minimal conflict sets (that are in no subset-relation with an existing one), or lead to a tree that is significantly larger than the complete non-interactive hitting set tree if many “completely new” minimal conflict sets result from the addition of new test cases. For an in-depth discussion and comparison of both strategies the reader may consult Chapter 6."
    }, {
      "heading" : "5.3.2.2 Output",
      "text" : "The output of Algorithm 5 can be explained as follows by making a distinction between the two modes of the algorithm specified by input parameter mode:\nProposition 5.16. If mode = static, then Algorithm 5 returns the (exact) solution of the Interactive Static KB Debugging problem (Problem Definition 5.2) if σ = 0 and an approximate solution of the problem if σ > 0 where the likeliness of finding the (exact) solution increases with decreasing σ.\nMore concretely, a maximal solution KB K∗ = (K \\ Dmax) ∪ UP w.r.t. the input DPI 〈K,B,P ,N 〉R is returned such that\n1. Dmax ∈ D (Dmax is an element of the current set of leading diagnoses)\n2. Dmax = arg maxD∈D pD(D) (Dmax is the a-posteriori most probable leading diagnosis)\n3. pD(Dmax) ≥ 1− σ (the a-posteriori probability of Dmax exceeds the predefined threshold)\n4. D ⊆mD〈K,B,P,N 〉R∩mD〈K,B,P∪P ′,N∪N ′〉R comprises the |D|most probable minimal diagnoses w.r.t. 〈K,B,P ,N 〉R as per the diagnosis probability measure pD,prio() (the set of leading diagnoses corresponds to the a-priori most probable minimal diagnoses w.r.t. the input DPI that satisfy all specified test cases),\n5. a-priori probability measure pD,prio() is computed from pK̃∪K() as per\n(a) Formula 4.2 (computation of formula fault probabilities)\n(b) Formula 4.7 (adaptation of formula fault probabilities)\n(c) Formula 4.3 (computation of diagnoses probabilities from formula fault probabilities)\n6. the a-posteriori probability measure pD() is computed from pD,prio() as per Bayes’ Theorem (Formula 4.5, for details see below) taking into account the new information given by the set of all answered queries so far, i.e. the collected sets of positive (P ′) and negative (N ′) test cases.\nIf mode = dynamic, then Algorithm 5 returns the (exact) solution of the Interactive Dynamic KB Debugging problem (Problem Definition 5.1) if σ = 0 and an approximate solution of the problem if σ > 0 where the likeliness of finding the (exact) solution increases with decreasing σ.\nMore concretely, a maximal solution KBK∗ = (K\\Dmax)∪UP∪P ′ w.r.t. the current DPI 〈K,B,P ∪ P ′,N ∪N ′〉R is returned such that\nCHAPTER 5. INTERACTIVE KNOWLEDGE BASE DEBUGGING 106\n1. Dmax ∈ D (Dmax is an element of the current set of leading diagnoses)\n2. Dmax = arg maxD∈D pD(D) (Dmax is the a-posteriori most probable leading diagnosis)\n3. pD(Dmax) ≥ 1− σ (the a-posteriori probability of Dmax exceeds the predefined threshold)\n4. D ⊆mD〈K,B,P∪P ′,N∪N ′〉R comprises the |D| most probable minimal diagnoses w.r.t. 〈K,B,P ∪ P ′,N ∪N ′〉R as per the diagnosis probability measure pD,prio() (the set of leading diagnoses corresponds to the a-priori most probable minimal diagnoses w.r.t. the current DPI),\n5. the a-priori probability measure pD,prio() is computed from pK̃∪K() as per\n(a) Formula 4.2 (computation of formula fault probabilities)\n(b) Formula 4.7 (adaptation of formula fault probabilities)\n(c) Formula 4.3 (computation of diagnoses probabilities from formula fault probabilities)\n6. the a-posteriori probability measure pD() is computed from pD,prio() as per Bayes’ Theorem (Formula 4.5, for details see below) taking into account the new information given by the set of all answered queries so far, i.e. the collected sets of positive (P ′) and negative (N ′) test cases.\nRemark 5.12 We still need to explain what we mean by “approximate solution” of the Interactive Static (Dynamic) KB Debugging problem. Roughly, an approximate solution is one constructed from a diagnosis which is not the only remaining minimal diagnosis. More precisely, an approximate solution of\n• the Interactive Static KB Debugging problem is a maximal solution KB (K \\ D) ∪ UP such that\n– D is a minimal diagnosis w.r.t. the input DPI and w.r.t. the current DPI and – there is some D′ 6= D which is a minimal diagnosis w.r.t. the input DPI and w.r.t. the current\nDPI\n• the Interactive Dynamic KB Debugging problem is a maximal solution KB (K \\D)∪UP∪P ′ such that\n– D is a minimal diagnosis w.r.t. the current DPI and – there is some D′ 6= D which is a minimal diagnosis w.r.t. the current DPI\nwhere the input DPI is given by 〈K,B,P ,N 〉R and the currect DPI by 〈K,B,P ∪ P ′,N ∪N ′〉R. So, as long as not all but one diagnosis candidate that enables the formulation of a solution KB has been ruled out by the classification of test cases, we speak of an approximate solution. Now, the lower a value for σ is predefined, the longer Algorithm 5 will usually need to iterate and the more test cases will usually need to be specified until one diagnosis has a probability greater than or equal to 1− σ. Thence, at the time a diagnosis exceeds the probability 1 − σ there will be usually fewer minimal diagnoses left than in case of the selection a higher value for σ. Therefore, the likeliness of picking the (exact) solution will usually be the higher, the lower σ is.\nRemark 5.13 Note that granting a maximum absolute fault tolerance σ that is independent of a set of leading diagnoses is generally computationally infeasible due to the high complexity of diagnosis computation (see Chapter 1). Since, for an absolute fault tolerance to hold, all minimal diagnoses w.r.t. the current DPI have to be computed in order to determine their probability and to decide whether the most probable diagnosis has a probability greater than or equal to 1− σ.\nCHAPTER 5. INTERACTIVE KNOWLEDGE BASE DEBUGGING 107\nIn fact, the fault tolerance used by Algorithm 5 which is relative to the set of leading diagnoses, i.e. the (a-priori) most probable minimal diagnoses D w.r.t. a DPI can be interpreted as follows. Under the assumption that the true diagnosis Dt is included in D, the chance that the most probable minimal diagnosis Dmax ∈ D which satisfies the stop criterion is not equal to Dt is smaller than the predefined threshold σ (cf. Section 4.5). Thus, under this assumption, the (a-posteriori) probability of being presented a non-desired solution KB as output of Algorithm 5 is smaller than σ.\nThe a-priori diagnoses probability measure pD,prio() refers to the one that is computed directly from the fault information provided as an input to Algorithm 5 whereas the a-posteriori diagnoses probability measure pD() is the one obtained from pD,prio() after incorporating the information given by the new test cases specified so far during the debugging session. So, pD,prio() and pD() might differ in terms of the probability order of diagnoses. Incorporation of updated probabilities directly into the hitting set tree algorithms to be used for the determination of leading diagnoses in the order prescribed by an updated probability measure is only possible if there is an additional update operator (besides Bayes’ Theorem for adapting diagnoses probabilities) that can be applied to formula probabilities. For, the latter are exploited in the hitting set tree to assign probability weights to paths that are not yet diagnoses (cf. pnodes() specified by Definition 4.9 and the discussion of Formula 4.6) in order to guide the search for minimal diagnoses in best-first order. Updated diagnosis probabilities are not helpful at all for this purpose. Devising a reasonable mechanism of updating formula probabilities seems to be hard mostly due to the lack of suitable data that might be collected during the debugging session to accomplish that. What would be imaginable during the debugging session is to try to learn something about the fault probability of syntactical elements by examining the positive (all formulas are definitely correct) and singleton negative (the single formula is definitely incorrect) test cases. However, a drawback of such a strategy comes into effect when only syntactically very simple queries are used which is, for instance, the case in Example 5.1 (see the definition of the GETENTAILMENTS function there). From such queries not many useful insights concerning faulty syntactical elements might be gained. On the other hand, such queries are absolutely desirable from the point of view of how well a user might comprehend the formulas asked by the system. Hence, these two aspects seem to contradict each other. Still, it is a topic for future research to attempt to elaborate a solution for that issue.\nA way to achieve that pD() coincides with pD,prio(), at least in case mode = static, is to exclude queries Q with D0(Q) 6= ∅ (see Remark 5.18). How this might be accomplished is stated by Proposition 5.8. Please notice that ignorance of queries with non-empty D0 does not implicate any disadvantages for interactive debugging. On the contrary, it is even a desirable feature of a debugger and brings along higher computational efficacy of query generation and stronger test cases from the logical point of view (cf. Section 5.2.2). For the scenario mode = dynamic, it is not possible in general to bypass the probability update by means of such queries (see Remark 5.18)."
    }, {
      "heading" : "5.3.2.3 Variables",
      "text" : "The variables used by Algorithm 5 that are not input arguments to the algorithm are the following:\n• P ′,N ′ are the sets of positive and negative test cases, respectively, collected during the execution of Algorithm 5 so far. That is, P ′ stores all positively answered queries, whereas N ′ stores all negatively answered ones.\n• Ccalc is the set of all conflict sets computed by QX during the execution of Algorithm 5 so far. Remark: In case of static debugging (mode = static), Ccalc includes exclusively minimal conflict sets w.r.t. the input DPI, whereas, in case of dynamic debugging (mode = dynamic), Ccalc may comprise minimal conflict sets w.r.t. the current or any intermediate DPI.\nCHAPTER 5. INTERACTIVE KNOWLEDGE BASE DEBUGGING 108\n• DX is the set of leading diagnoses returned by a call of STATICHS in case of static debugging (mode = static) and by a call of DYNAMICHS in case of dynamic debugging (mode = dynamic).\nRemarks: In case of dynamic debugging, DX ⊆mD〈K,B,P∪P ′,N∪N ′〉R is the set of most probable minimal diagnoses w.r.t. the current DPI 〈K,B,P ∪P ′,N ∪N ′〉R as per the diagnosis probability measure pD,prio() computed from pK̃∪K() by Formulas 4.2, 4.7, 4.3 and 4.4 (cf. Sections 4.5 and 5.3.2.2).\nIn case of static debugging, DX ⊆mD〈K,B,P,N 〉R ∩mD〈K,B,P∪P ′,N∪N ′〉R , i.e. DX includes only diagnoses that are minimal diagnoses w.r.t. the input DPI 〈K,B,P ,N 〉R as well as w.r.t. the current DPI 〈K,B,P∪P ′,N ∪N ′〉R. Moreover, DX comprises the most probable minimal diagnoses w.r.t. the input DPI according to the diagnosis probability measure pD,prio() computed from pK̃∪K() by Formulas 4.2, 4.7, 4.3 and 4.4 (cf. Sections 4.5 and 5.3.2.2).\n• D× stores all minimal diagnoses w.r.t. the input DPI that have been invalidated by one of the collected positive and negative test cases P ′ and N ′, respectively (mode = static). D× stores the minimal diagnoses w.r.t. the last-but-one DPI that have been invalidated by the most recently added test case (mode = dynamic).\n• Dout is the subset of the set of current leading diagnoses DX that has been invalidated by the most recently added test case.\n• D⊃ stores all diagnoses that are non-minimal w.r.t. the current DPI, i.e. for each diagnosis nd ∈ D⊃ there is some nd′ ∈ DX such that nd ⊃ nd′ (mode = dynamic). Remark: D⊃ is solely needed for dynamic and not for static debugging as the latter does not need to store non-minimal diagnoses (cf. rule 4 of Definition 4.8 on page 50). Reason for this is the fact that only minimal diagnoses w.r.t. the input DPI are searched for. On the other hand, in case of dynamic debugging, non-minimal diagnoses might become minimal ones after some new test cases are specified since minimal diagnoses w.r.t. the (changing) current DPI are considered.\n• qData is an informal variable that comprehends any kind of data that might be taken into account by the query selection measure qsm() and that might need to be adapted after a query has been answered (and diagnoses have been invalidated) in order to take the obtained new information into account. One can imagine qData as a log specific to the particular function qsm() that is used which records data of prior (query answering) iterations executed by the algorithm such as certain performance measures. An example of a qsm() strategy using one such metric, namely the ratio of leading diagnoses invalidated by a test case, can be found in [63].\n• QA := [〈Q, u(Q)〉]Q∈P ′∪N ′ where u(Q) ∈ {true, false} is the chronologically ordered list of queries and user answers collected so far during the execution of Algorithm 5.\n• Q is the current queue of open nodes in the hitting set tree maintained by Algorithm 5.\n• The list Qdup roughly stores all duplicate nodes (that is, nodes for each of which there is a node in the hitting set tree that corresponds to an equal set of edge labels) computed so far during the execution of Algorithm 5.\nRemark: The list Qdup is only relevant in case mode = dynamic and not needed if mode = static. The purpose of this set is to enable the “replacement” of pruned nodes which is necessary to guarantee the completeness of DYNAMICHS in terms of not missing any minimal diagnoses (for a detailed explanation, see Section 6.2).\nCHAPTER 5. INTERACTIVE KNOWLEDGE BASE DEBUGGING 109"
    }, {
      "heading" : "5.3.2.4 Algorithm Walkthrough",
      "text" : "Initialization. In the first 4 lines, variable declarations take place. First, all variables that store sets of conflict sets, diagnoses or test cases, and qData are initialized to the empty set. Further on, Qdup and QA are initialized to an empty list. Finally, the queue Q of open nodes used for the hitting set tree construction by STATICHS (mode = static) or DYNAMICHS (mode = dynamic), respectively, is set to [∅] since it initially includes only a non-labeled root node.\nRemark 5.14 The non-labeled root node is denoted by ∅ since nodes in STATICHS are associated with the set of edge labels along the path in the hitting set tree from the root node to this node (cf. Chapter 4 and Section 6.1). Hence, the root node itself corresponds to the empty path which includes no edges.\nNotice that in case of DYNAMICHS, nodes will be (ordered) lists instead of (non-ordered) sets like in STATICHS (cf. Section 6.2). That is, to be precise, the unlabeled root node in this case corresponds to the empty list []. For the ease of representation of Algorithm 5, only one set Q is initialized to be used with either STATICHS or DYNAMICHS. Thence, by abuse of notation, we associate ∅ in this case with the empty list [].\nComputing Formula Probabilities. Then, GETFORMULAPROBS is called in line 5 with the KB K and the function pK̃∪K : K̃ ∪ K → (0, 1] as inputs. The function first applies Formula 4.2 to compute probabilities for each formula in K, then applies Formula 4.7 to these probabilities leading to the output pK : K → (0, 0.5), a function that assigns a value pK(ax ) ∈ (0, 0.5) to each ax ∈ K.\nComputing Leading Diagnoses. At this point, all input arguments required by for the hitting set tree construction are instantiated. So, the algorithm enters the while loop in line 6. As a first step within the loop, either STATICHS, if mode = static, or DYNAMICHS, otherwise, is called in order to obtain a tuple including a set of leading diagnoses along with variables that store the “state” of the (partial) hitting set tree constructed so far and facilitate the reuse of this tree in the next iteration.\nIn concrete terms, STATICHS accepts the arguments 〈K,B,P ,N 〉R, Q, t, nmin, nmax, Ccalc, DX, D×, pK(), P ′ and N ′ and returns a tuple 〈D,Q,Ccalc,D×〉 the elements of which are defined as follows:\n• D is the current set of leading diagnoses such that\n(a) D ⊆mD〈K,B,P,N 〉R ∩mD〈K,B,P∪P ′,N∪N ′〉R is the set of most probable minimal diagnoses w.r.t. 〈K,B,P ,N 〉R that satisfy all test cases P ′ and N ′ such that (i) nmin ≤ |D| ≤ nmax and\n(ii) D ⊃ DX, if such a set D exists; or\n(b) D is equal to the set of all minimal diagnoses mD〈K,B,P,N 〉R ∩mD〈K,B,P∪P ′,N∪N ′〉R , otherwise;\nwhere “most-probable” refers to the diagnosis probability measure pD,prio() obtained from pK() by application of Formulas 4.3 and 4.4.\n• Q is the current queue of open nodes of the hitting set tree.\n• Ccalc ⊆ mC〈K,B,P,N 〉R is the set of all computed minimal conflict sets w.r.t. the input DPI throughout all calls of STATICHS during the execution of Algorithm 5 so far.\n• D× comprises all computed minimal diagnoses throughout all calls of STATICHS during the execution of Algorithm 5 so far where each D ∈ D× has been invalidated by some test case in P ′ or N ′.\nCHAPTER 5. INTERACTIVE KNOWLEDGE BASE DEBUGGING 110\nSimilarly, DYNAMICHS accepts the arguments 〈K,B,P ,N 〉R, Q, Qdup, t, nmin, nmax, Ccalc, DX, D×, pK(), P ′, N ′ and D⊃ and returns a tuple 〈D,Q,Ccalc,D×,D⊃,Qdup〉 the elements of which are defined as follows:\n• D is the current set of leading diagnoses such that\n(a) D ⊆ mD〈K,B,P∪P ′,N∪N ′〉R is the set of most probable minimal diagnoses w.r.t. 〈K,B,P ∪ P ′,N ∪N ′〉R such that (i) nmin ≤ |D| ≤ nmax and\n(ii) D \\DX 6= ∅, if such a set D exists, or\n(b) D is equal to the set of all minimal diagnoses mD〈K,B,P∪P ′,N∪N ′〉R , otherwise,\nwhere “most-probable” refers to the diagnosis probability measure pD,prio() obtained from pK() by application of Formulas 4.3 and 4.4.\n• Q is the current queue of open (non-labeled) nodes of the hitting set tree,\n• Ccalc is a set of conflict sets w.r.t. the current DPI 〈K,B,P ∪ P ′,N ∪N ′〉R,\n• D× = ∅,\n• D⊃ is the set of all processed nodes so far throughout the execution of Algorithm 5 that are nonminimal diagnoses w.r.t. the current DPI 〈K,B,P ∪ P ′,N ∪N ′〉R and\n• Qdup includes all duplicate nodes found so far throughout the execution of Algorithm 5 (for a detailed explanation see Section 6.2 and Algorithm 8).\nRemark 5.15 It is very important to notice that the function pnodes() for p() := pK() as specified by Definition 4.9 on page 64 imposes the same order on a set of minimal diagnoses as the a-priori probability measure pD,prio(). That is pnodes(D) = c · pD,prio(D) for all minimal diagnoses D w.r.t. a DPI where c is a constant (which is the same for all diagnoses D). The difference between both functions is that pnodes() is defined for allX ⊆ K whereas pD,prio() is only defined for (leading) minimal diagnosesD ⊆ K. Further on pD,prio() is normalized whereas pnodes() is not which accounts for the (normalization) constant c. The function pnodes() is essential for the best-first construction of the hitting set tree in STATICHS and DYNAMICHS since it allows for the assignment of a “probability” to non-diagnoses (cf. the discussion of Formula 4.6 on page 63). Since the input argument p() (which is the same for all calls) to STATICHS as well as DYNAMICHS is equal to pK() by lines 8 and 10 in Algorithm 5, the set D returned by STATICHS (DYNAMICHS) is also the set of most probable minimal diagnoses w.r.t. 〈K,B,P ,N 〉R (〈K,B,P ∪ P ′,N ∪N ′〉R) as per the function pnodes() (cf. Propositions 6.1 and 6.2).\nRemark 5.16 Notice that the return parameter that is relevant for the main purpose of Algorithm 5, namely to compute a query and thereby obtain a new test case classified by the user, is solely the set of leading diagnoses D. The other return parameters serve as a means to store the state of the hitting set tree that is gradually built up by successive calls of STATICHS (if mode = static) and DYNAMICHS (if mode = dynamic), respectively. Whereas Q and Ccalc (and D⊃ and Qdup in case of DYNAMICHS) are never modified until the next call to STATICHS or DYNAMICHS, the sets DX and D× are only changed once, after the subset of invalidated leading diagnoses Dout is known, in lines 21 and 22.\nAt this moment, we do not go into detail regarding the way how leading diagnoses are computed by STATICHS and DYNAMICHS. We simply suppose that both functions act in a manner that the outputs\nCHAPTER 5. INTERACTIVE KNOWLEDGE BASE DEBUGGING 111\njust specified are returned for the given inputs. An in-depth delineation of both functions will be given in Sections 6.1 and 6.2 in Chapter 6. Further note that the return parameter D is stored in variable DX from line 10 on.\nComputing a Probability Distribution of Leading Diagnoses. After the set of leading diagnoses DX has been computed, the variables DX, pK(), 〈K,B,P ,N 〉R andQA are used as arguments to the function GETPROBDIST (see Algorithm 6) which computes a probability distribution of the leading diagnoses, i.e. a probability measure pD() for the probability space with sample space Ω = DX (cf. Section 4.5). As a first action to achieve this, the (a-priori) probabilities pD,prio(D) for D ∈ DX are computed from the (a-priori) probabilities pK(ax ) for formulas ax ∈ K as per Formula 4.3 (GETPRIODIAGPROBS in line 29). Application of Formula 4.4 is not necessary at this point as probabilities are anyhow normalized at the end of GETPROBDIST (line 44). Notice that the function pK() remains constant, i.e. unmodified, throughout the entire execution of Algorithm 5.\nNow, since a-priori diagnosis probabilities assigned by pD,prio() directly rely upon pK() which in turn is computed directly from the initially given fault probabilities pK̃∪K(), the probability measure pD,prio() is adapted to yield a-posteriori diagnosis probabilities pD() in order to reflect the new evidence provided by the collected test cases P ′ and N ′.\nThe a-posteriori probability of a current leading diagnosis D in DX is pD(D |QA) and can be computed by means of Bayes’ Theorem (Formula 4.5) from pD,prio() as follows.\npD(D |QA) = pD,prio(QA | D) pD,prio(D)\npD,prio(QA)\nwhere QA is the chronologically ordered list of queries and user answers collected so far during the execution of Algorithm 5 (see page 108). We point out that pD,prio(QA) is only a normalization factor that is equal for each diagnosis and thus does not need to be explicitly computed. The crucial factor is\npD,prio(QA | D) = pD,prio(∀ 〈Q, u(Q)〉 ∈ QA : Q = u(Q) | D)\nwhich describes the probability of getting exactly the answer u(Q) for each query Q ∈ P ′∪N ′ under the assumption that D corresponds to the true diagnosis Dt, i.e. Dt = D. In other words, pD,prio(QA | D) is the probability of QA under the assumption that the user answers in a way that u(Q) = true if D ∈ D+(Q) and u(Q) = false if D ∈ D−(Q).\nFor a single query Qi, the probability pD,prio(Qi = u(Qi) | D) is defined as (cf. [44])\npD,prio(Qi = u(Qi) | D) =  1, if D ∈ D+(Qi) 0, if D ∈ D−(Qi) 1 2 , if D ∈ D 0(Qi)\n(5.2)\nfor u(Qi) = true and\npD,prio(Qi = u(Qi) | D) =  1, if D ∈ D−(Qi) 0, if D ∈ D+(Qi) 1 2 , if D ∈ D 0(Qi)\n(5.3)\nfor u(Qi) = false where D+(Qi), D−(Qi) and D0(Qi) are computed w.r.t. the DPI 〈K, B,P ∪P ′′,N ∪ N ′′〉 where P ′′ and N ′′, respectively, include all test cases collected prior to Qi, i.e. P ′′ ∪ N ′′ = {Q1, . . . , Qi−1} if queries are numbered chronologically. That is, if D predicted the answer u(Qi) to Qi given by the user, the probability is 1, zero if D predicted the converse answer ¬u(Qi) and 12 if D did not predict any answer to Qi.\nCHAPTER 5. INTERACTIVE KNOWLEDGE BASE DEBUGGING 112\nSo, aside from the normalization factor (see above), pD,prio(Qi = u(Qi) | D) is the factor by which the a-priori probability pD,prio(D) must be multiplied to obtain the a-posteriori probability pD(D) of a diagnosis D after a single query Qi has been answered and added as a test case to the DPI.\nThe intuitive explanation for the update by this factor is that ifD predicted (at least) one answer u(Q) conversely as given by the user, then D is a-posteriori impossible since it has already been invalidated by the addition of test case Q. In case a diagnosis has never predicted the wrong answer, but did not predict any answer for many queries so far, then it is a-posteriori more unlikely than a diagnosis that did predict a correct answer more often. That is, our a-posteriori degree of belief that D is the correct diagnosis is the higher, the more often D had predicted answers to queries that were later actually given by the user (cf. Section 5.1.4 for an explanation what we mean by “predict”).\nThe value of pD,prio(Qi = u(Qi) | D) can be computed by use of QA and the q-partitions P(Q1), . . . , P(Qi−1) of the current set of leading diagnoses DX (for which a-posteriori probabilities are to be computed) for all queries Q1, . . . , Qi−1 answered before query Qi. Thereby, each P(Qj) where j ∈ {1, . . . , i− 1} must be computed for a DPI where only Q1, . . . , Qj−1 are incorporated as test cases.\nTaking these thoughts into account, GETPROBDIST (Algorithm 6) updates pD,prio(D) for each diagnosis D ∈ DX in that it runs through all query-answer pairs 〈Q, u(Q)〉 in QA chronologically and for each D ∈ DX it multiplies pD,prio(D) by 12 if D ∈ D\n0(Q) as per Formulas 5.2 and 5.3. For each check whether a diagnosis is in D0(Q) in lines 34 and 39 a DPI is used that already incorporates all test cases P ′′ and N ′′ that have been added chronologically before Q was asked. This is achieved by updating P ′′ and N ′′ successively (lines 36 and 41). After all elements of QA have been processed, the updated diagnosis probabilities are finally normalized (line 44, cf. Formula 4.4 on page 62) and the resulting function pD,prio() is returned.\nRemark 5.17 Note that the function GETPROBDIST exploits the fact that all diagnoses in DX are leading diagnoses w.r.t. the current DPI 〈K,B,P ∪ P ′,N ∪ N ′〉R which guarantees that none of these diagnoses has been invalidated by any of the test cases in P ′ or in N ′ added throughout the execution of Algorithm 5. Hence, it is clear that each D ∈ DX must be in D+(Q) ∪ D0(Q) if u(Q) = true and in D−(Q) ∪ D0(Q) if u(Q) = false , and it is only tested whether D /∈ D+(Q) in the prior case (line 34) and whether D /∈ D−(Q) in the latter (line 39). It must be further noted that, in case of mode = dynamic, diagnoses in DX are not necessarily minimal diagnoses w.r.t. the intermediate DPIs 〈K, B,P ∪ P ′′,N ∪N ′′〉 that are used for the probability update. However, this is not problematic since any set of (minimal and/or non-minimal) diagnoses is partitioned into the three sets D+(Q), D−(Q) and D0(Q) by a query Q (cf. Remark 5.5) wherefore P(Q) exists for any set DX. Thence, the correctness of GETPROBDIST remains unaffected by the usage of the setting mode = dynamic.\nRemark 5.18 We want to emphasize that an adaptation of pD,prio(D) is only necessary in case D ∈ D0(Qj) for some query Qj answered so far during the execution of Algorithm 5 as otherwise a multiplication by 1 is required which does not change pD,prio(D).\nFor the case of static debugging (mode = static), an immediate implication of this is the following: The restriction of asking the user only queriesQj w.r.t. a DPI with the property that no minimal diagnosis w.r.t. this DPI can be an element of D0(Qj) makes the probability update for each diagnosis in DX equivalent to a multiplication by 1 and hence obsolete. This must be the case since each diagnosis in DX which is a subset of mD〈K,B,P,N 〉R ∩ mD〈K,B,P∪P ′,N∪N ′〉R (see Section 5.3.2.2) must be a minimal diagnosis w.r.t. each intermediate DPI (which includes a superset of the test cases in the input DPI 〈K,B,P ,N 〉R and a subset of the test cases in the current DPI 〈K,B,P ∪P ′,N ∪N ′〉R). Consequently, such a scenario implicates that the order of diagnoses computed by STATICHS corresponds to the bestfirst order also w.r.t. the a-posteriori diagnosis probabilities (cf. Remark 5.13).\nThe approach of only using queries with this property is feasible, e.g. by using a GETENTAILMENTS function in conformity with Proposition 5.8 for the generation of the query pool (GETPOOLOFQUERIES).\nCHAPTER 5. INTERACTIVE KNOWLEDGE BASE DEBUGGING 113\nSuch a type of queries is also favorable from the discrimination point of view, as we pointed out in Section 5.2.2. An improvement of static debugging with this type of queries is to deactivate the probability update, i.e. replace line 11 in Algorithm 5 by line 29 of Algorithm 6. This improvement is not shown in Algorithm 5.\nIn a dynamic debugging session (mode = dynamic), on the contrary, the usage of such queries does not guarantee the triviality of the probability update. For, also if no minimal diagnosis w.r.t. the DPI (for which a query Qj is computed) can be an element of D0(Qj), there may be some non-minimal one which is. For example, for any admissible DPI 〈K,B,P ,N 〉R is holds that D := K is a diagnosis (cf. Proposition 3.4 and Definition 3.6), albeit in most cases a non-minimal one. In such a case, (K \\ D) ∪ B ∪ UP which is equal to B ∪ UP cannot entail Qj . Because, were this the case, then all minimal diagnoses Di ∈ mD〈K,B,P,N 〉R would be elements of D+(Qj) as each K∗i ⊇ B ∪ UP and thus each K∗i |= Qj by the monotonicity of L. Hence, this would be a contradiction to the fact that Qj is a query w.r.t. 〈K,B,P ,N 〉R by Corollary 5.2. On the other hand, (K\\D)∪B∪UP ∪Qj = B∪UP ∪Qj cannot violate any x ∈ N ∪R. Since, if this were the case, then addingQj to the positive test cases would lead to a non-admissible DPI 〈K,B,P ∪ {Qj} ,N 〉R. By Corollary 5.3, this would be a contradiction to the fact that Qj is a query w.r.t. 〈K,B,P ,N 〉R. Thence, D ∈ D0(Qj) must hold for the assumed non-minimal diagnosis D. From that we conclude that the probability update in dynamic debugging cannot be made obsolete in general by the usage of such a type of queries.\nStop Criterion and Output. The (a-posteriori) probability distribution pD() of leading diagnoses DX is then used in line 12 of Algorithm 5 to compute the mode of this distribution, i.e. the one diagnosis Dmax ∈ DX with maximum probability according to pD().\nIn the sequel, Dmax is used to check the stop criterion (line 13), namely whether Dmax has a probability greater than or equal to 1 − σ. If this is the case and mode = static, the function GETSOLKB computes a maximal solution KB w.r.t. the input DPI as (K \\ Dmax) ∪ UP by means of the current DPI 〈K,B,P ∪ P ′,N ∪ N ′〉R, P ′ and Dmax. Given that mode = dynamic, GETSOLKB returns a maximal solution KB w.r.t. the current DPI as (K \\ Dmax) ∪ UP∪P ′ by means of the current DPI 〈K,B,P ∪P ′,N ∪N ′〉R andDmax. This solution KB is then returned as an output of Algorithm 5. If, on the other hand, the stop criterion is not met, the algorithm continues the execution with the computation of another query.\nRemark 5.19 Notice that the returned maximal solution KB (K \\ Dmax) ∪ UP w.r.t. the input DPI in case mode = static can be easily extended to constitute a maximal solution KB w.r.t. the current DPI, namely by extending it by UP ′ . Ifmode = dynamic, then the KB output in line 14 is a maximal solution KB w.r.t. the current DPI, but possibly a non-maximal solution KB w.r.t. the input DPI.\nQuery Computation and User Interaction. In line 16, the function CALCQUERY is applied to compute a query and the associated q-partition by means of the leading diagnoses DX, (possibly) the collected data qData, the probability distribution pD() of the leading diagnoses, a query selection function qsm() (which might exploit the function pK̃∪K()), a parameter q determining the size of the computed query pool and the current DPI 〈K,B,P ∪ P ′,N ∪N ′〉R.\nAs a first step within CALCQUERY, the function GETPOOLOFQUERIES computes a query pool QP as detailed in Section 5.2 from DX, q and 〈K,B,P ∪P ′,N ∪N ′〉R. Then, the best tuple 〈Q,P(Q)〉 ∈ QP according to the function qsm() is searched for and finally returned as the output of CALCQUERY. During the query selection process, the evaluation of the query selection measure qsm(Q) ∈ R for queries Q where 〈Q,P(Q)〉 ∈ QP may require qData, the fault probabilities pD() of leading diagnoses as well as the fault probabilities pK̃∪K() of syntactical elements in K. This depends on which concrete measure qsm() is employed (see Section 5.3.3 which presents some possible measures).\nCHAPTER 5. INTERACTIVE KNOWLEDGE BASE DEBUGGING 114\nAs a next step, the query Q of the best tuple 〈Q,P(Q)〉 ∈ QP is presented to the interacting user in line 17 which is the only place in Algorithm 5 where user interaction takes place. The user is modeled as a deterministic function u : QD,〈K,B,P∪P ′,N∪N ′〉 → {true, false} that allocates a positive (true) or negative (false) answer to each query w.r.t. any set of leading diagnoses D for some current DPI 〈K,B,P ∪ P ′,N ∪N ′〉. The answer u(Q) given by the user is stored in the variable answer.\nRemark 5.20 We want to point out that the algorithm can be easily adapted to allow a user to reject queries, e.g. if they are not sure how to answer. That is, the user function might be modeled as u : QD,〈K,B,P∪P ′,N∪N ′〉 → {true, false, unknown} where u(Q) = unknown signifies the rejection of query Q. In this case, an accordingly modified version of Algorithm 5 would calculate an alternative query w.r.t. D and 〈K, B,P ∪ P ′,N ∪ N ′〉, e.g. the second best one according to the query selection measure qsm() among all tuples in QP (this potential feature is not shown in Algorithm 5). In this vein, a total of |QP| − 1 queries can be dismissed per set of leading diagnoses D.\nWe want to accentuate that the presented interactive algorithm might be easily adapted to cope with queries whose answer is unknown to the user, but a definite assumption for the algorithm to return a correct solution is a user that does not give wrong answers. In other words, the algorithm does not provide inherent mechanisms that allow for the detection of wrong answers or for the debugging of the KB debugging procedure (keyword “garbage in, garbage out”). So, we suppose the function u() to be deterministic which prohibits the situation that a user might change their mind at a later point in time. Of course, this is still a possible scenario in practice, but in case it arises, a user has to revise, i.e. delete or edit, specified test cases they disagree with by hand before a new debugging session using the modified DPI might be started.\nAnother remark at this place concerns the way a user might choose to answer the query. A “minimal” feedback of a user that we regard as an answer to a query Q is to merely say true , i.e. each formula in Q (or the conjunction of formulas in Q) must be entailed by the correct KB, or false , i.e. at least one formula in Q (or the conjunction of formulas in Q) must not be entailed by the correct KB. The presented algorithm (Algorithm 5) is designed to deal with exactly this kind of an answer. However, imagine a user being presented Q and think of how they might proceed in order to come up with an answer to Q. The first observation is that, in order to respond by true , a user must definitely scrutinize each single formula in Q because otherwise they could never decide for sure whether the conjunction of all formulas in Q is correct. Another observation is that a user might cease to go through the rest of the formulas in case they have already identified one that must not be an entailment of the desired KB. For, in this situation, the overall query Q is already false . This however indicates that at least one formula must be known to be correct or false whatever answer is given to Q. Therefore, we can usually expect a user to be able to give exactly this information, namely one formula in Q that must be incorrect, additionally to answering by false . This extra piece of information can be exploited to achieve better space and time efficiency in the context of diagnosis computation. Proposing more efficient algorithms that exploit this information is a topic for future work.\nIncorporating the New Information. The new information represented by the answer answer to Q is incorporated (lines 18-26) by updating values of all relevant parameters. First, by means of the function APPEND, the tuple consisting of the answered query Q and the corresponding answer answer given by the user is added as a last element to the chronological list of queries and answers QA that is used for the next probability update (line 11).\nThen, the subset Dout of the leading diagnoses DX that gets invalidated after addingQ to the positive or negative test cases of the DPI, respectively, is computed by the function GETINVALIDDIAGS that gets the q-partition P(Q) = 〈 D+(Q),D−(Q),D0(Q) 〉 of Q and answer as input arguments. Dout then corresponds to the set D−(Q) given that answer is true and to D+(Q) otherwise (cf. Section 5.1.4). Note that ∅ ⊂ Dout ⊂ DX holds by Proposition 5.4 and since Q is a query w.r.t. DX (since DX is given\nCHAPTER 5. INTERACTIVE KNOWLEDGE BASE DEBUGGING 115\nas an input to CALCQUERY). As a next step, the data qData is updated. As already pointed out in Section 5.3.2.3, the form of the variable qData depends on the employed query selection measure qsm() and so do the actions that are performed by UPDATEQDATA.\nIn order to communicate the impact of the answered query to the hitting set tree algorithm (either STATICHS or DYNAMICHS), the set of invalidated leading diagnoses Dout is deleted from the leading diagnoses DX and added to D×. After this update, DX includes all diagnoses that have been computed by the hitting set tree algorithm so far that are minimal diagnoses w.r.t. the current DPI.\nFinally, the new test case Q is added to the new positive test cases P ′ if answer is true and to the new negative test cases N ′ in case of answer = false ."
    }, {
      "heading" : "5.3.3 Query Selection Measures",
      "text" : "In this section, we give a brief introduction to some query selection measures qsm() that have been suggested and evaluated in literature within the scope of KB or ontology debugging [74, 63]. Such query selection measures, when used as a parameter in an interactive KB debugging algorithm such as the one described by Algorithm 5, aim at solving the following optimization problems. In Interactive Dynamic KB Debugging, the problem is defined as follows:\nProblem Definition 5.3. The task is to solve the problem specified by Problem Definition 5.1 in a way that |P ′|+ |N ′| is minimal.\nIn Interactive Static KB Debugging, the problem is defined as follows:\nProblem Definition 5.4. The task is to solve the problem specified by Problem Definition 5.2 in a way that |P ′|+ |N ′| is minimal.\nThat is, these optimization problems aim at the minimization of user effort during interactive KB debugging. In other words, the goal is the minimization of the number of queries required to be asked to a user in order to solve the Interactive Static KB Debugging or the Interactive Dynamic KB Debugging Problem, respectively.\nIn our previous work [74], we have discussed entropy-based (ENT()) and split-in-half (SPL()) query selection measures.\nEntropy-Based Query Selection. A best query QENT according to ENT() has a maximal information gain among all queriesQwhere 〈Q,P(Q)〉 ∈ QP. In other words,QENT minimizes the expected entropy of the probability distribution of the leading diagnoses DX after QENT has been added as a test case to the DPI based on the user’s answer u(QENT). As shown in [44], this leads to the definition\nENT(Q) := ∑\na∈{true,false}\np(Q = a) log p(Q = a) + p(D0(Q))\nwhere p() in the case of our algorithm corresponds to the leading diagnoses probability measure pD() computed in line 11 in Algorithm 5 and\np(Q = true) = p(D+(Q)) + 1\n2 p(D0(Q))\np(Q = false) = p(D−(Q)) + 1\n2 p(D0(Q))\nCHAPTER 5. INTERACTIVE KNOWLEDGE BASE DEBUGGING 116\n(cf. Section 5.1.4) where\np(D+(Q)) = ∑\nD∈D+(Q)\np(D)\np(D−(Q)) = ∑\nD∈D−(Q)\np(D)\np(D0(Q)) = ∑\nD∈D0(Q)\np(D)\nThen, the best query in a pool QP according to qsm() := ENT() is\nQENT = arg min {Q | 〈Q,P(Q)〉∈QP} ENT(Q)\nSo, theoretically optimal w.r.t. ENT() is a queryQwhose positive and negative answers are equally likely and for which D0(Q) is the empty set. In other words, the best query has the property that the sum of probabilities of leading diagnoses predicting the positive answer as well as the sum of probabilities of leading diagnoses predicting the negative answer is 50%.\nSplit-In-Half Query Selection. For the selection criterion qsm() := SPL(), on the other hand, the query\nQSPL = arg min {Q | 〈Q,P(Q)〉∈QP} SPL(Q)\nis preferred where\nSPL(Q) := ∣∣ |D+(Q)| − |D−(Q)| ∣∣+ |D0(Q)|\nHence, this measure is optimized by queries Q for which the number of leading diagnoses predicting the positive answer is equal to the number of leading diagnoses predicting the negative answer and for which D0(Q) is the empty set.\nRisk-Optimized Query Selection. For scenarios where a-priori probabilities are vague, we have presented another more complex query selection measure RIO() in [63] which uses a reinforcement learning strategy to constantly adapt some “risk” parameter that indicates the current amount of trust in the probabilities. Whereas ENT() and SPL() do not rely on qData, this learning strategy does so and requires the invalidation rate or “performance”, i.e. |Dout||DX| , of the previous iteration for the adaptation of the learning parameter. As long as the invalidation rate is “good”, the trust in the current (a-posteriori) probabilities – that strongly depend on the vague a-priori probabilities – is high, but it is gradually decreased after observing “worse” performance, and so on. High trust in the probabilities means usage of ENT() which can exploit high quality fault information well as demonstrated in the experiments conducted in [74], whereas low trust involves selection of queries that guarantee a higher worst case invalidation rate, i.e. have similar properties to queries SPL() would select.\nExample 5.7 Let us reconsider the queries and associated q-partitions for the example DPI of Table 5.1 that are depicted by Table 5.3 on page 96. Let us denote by Qi ≺M Qj that Qi is preferred over Qj and by Qi ≺ M Qj that Qi is equally preferable as Qj if the query selection measure qsm() := M is used. Furthermore, we make the assumption that the probability distribution pD of the (leading) diagnoses DX = {D1, . . . ,D4} is as shown in Table 5.4.\nThen, we make the following observations:\nCHAPTER 5. INTERACTIVE KNOWLEDGE BASE DEBUGGING 117\n• Q6 is the theoretically optimal query w.r.t. ENT() since pD(D+(Q6)) = 0.5, pD(D−(Q6)) = 0.5 and D0(Q6) = ∅, i.e. the positive and the negative answer have equal probabilities of 50% and thus Q6 the highest theoretically possible information gain of 1 (bit). This can be compared with one toss of a coin where the information gain of tossing the coin and checking whether it is head or tail is highest in a case where the coin is fair. For a coin that shows head with a probability of 0.95, conversely, the information gain of tossing the coin is rather small since we are already quite sure about the result in advance.\n• Q9 ≺M Q5 as well as Q9 ≺M Q2 for M ∈ {SPL(),ENT()} because both Q5 and Q2 share one set in {D+,D−} with Q9, but exhibit a non-empty set D0 whereas D0(Q9) = ∅. This shows that both split-in-half and entropy-based query selection penalize a query Q if there are leading diagnoses that are definitely not discriminated by it, i.e. D0(Q) 6= ∅. This is perfectly desirable as we discussed.\n• Q4 ≺ M Q10 for M ∈ {SPL(),ENT()} since their q-partitions differ just by commutation of the sets D+ and D−. This is what one would expect of such a measure, i.e. that it does not matter whether the positive or negative answer is more probable if the probability values are the same (in case of ENT()) and whether the number of diagnoses predicting the positive or negative answer is higher if the numbers are the same (in case of SPL()). However, notice that Q4 might be much easier to comprehend and answer for the interacting user. Therefore, Q4 might be preferred in a scenario where some second measure qsm2() comes into play to identify a best query among equally preferable queries w.r.t. some qsm1() that is used as a primary measure. For, example some “query-easiness” measure qsm2() might be employed after qsm1() ∈ {SPL(),ENT()} has filtered out an equally preferable set of queries; in this case let this set be {Q4, Q10}. The measure qsm2() could be defined to simply count the logical connectives and quantifiers occurring in a query Q and pick one for which this number is minimal. In this case, this number would be 0 for Q4 and 7 for Q10, wherefore Q4 would be decisively better than Q10 w.r.t. qsm2().\n• It holds that Q3 ≺ENT() Q10 ≺ENT() Q1, but Q3 ≺ SPL() Q10 ≺ SPL() Q1. The former holds since all three queries feature an empty set D0, but the difference between p(D+) and p(D−) is largest for Q1 (p(D+(Q1)) = 0.95), second largest for Q10 (p(D−(Q10)) = 0.85) and smallest for Q3 (p(D+(Q3)) = 0.7).\n• Q9 is the second best query among those given in Table 5.3 because both answers of it are almost equally probable (positive answer has a probability of 0.55 and negative answer a probability of 0.45).\n• Queries Q7, Q8 and Q9 are theoretically optimal w.r.t. the SPL() measure, since D0 = ∅ and |D+| = |D−| for all of them.\n• Regarding the RIO() measure, queries Q7, Q8 and Q9 are “no risk” queries since they feature the maximum possible worst case elimination rate of 50%. Q2 and Q6, for instance, have a “higher risk” as their minimal invalidation rate amounts to only 25%. That is, if Q2 (Q6) is answered positively (negatively), then only one of four leading diagnoses is invalidated."
    }, {
      "heading" : "5.3.4 Interactive Debugging Algorithm: Correctness",
      "text" : "In this section we prove the correctness of Proposition 5.16 on page 105 by using the results of Sections 6.1.2 and 6.2.2 which provide evidence for the correctness (soundness, completeness and optimality) of methods STATICHS and DYNAMICHS:\nProof of Proposition 5.16. First, we argue why Algorithm 5 must terminate. The function GETFORMULAPROBS in line 5 terminates since it applies Formulas 4.2 and 4.7 |K| times and |K| is finite by Definition 3.1. If mode = static, then STATICHS terminates due to Proposition 6.1. If mode = dynamic, then DYNAMICHS terminates due to Proposition 6.2. GETPROBDIST terminates since (1) the number of already answered queries |QA| is finite, (2) |DX| is finite since diagnoses are subsets of K and thus there is only a finite number of (minimal) diagnoses w.r.t. any DPI according to Definition 3.1 (since all sets included in the DPI are finite) and (3) reasoning (GETENTAILMENTS and ISKBVALID) is assumed to be decidable for the logic L over which the DPI is formulated as per Chapter 2. Further, GETMODE clearly terminates due to the fact that |DX| is finite and returns the mode Dmax of the diagnoses probability distribution pD() over the diagnoses in DX. Now, if the stop criterion pD(Dmax) ≥ 1 − σ is met, then GETSOLKB is called. GETSOLKB simply deletes the given diagnosis Dmax from the given KB K and adds a finite set of formulas to it, and thence terminates.\nIf the stop criterion is not met, then |DX| ≥ 2 must hold as otherwise the single diagnosis D ∈ DX would necessarily have fulfilled the stop criterion as its probability as per any probability measure over the sample space Ω := DX must be equal to 1 and thus greater than or equal to 1− σ where σ ≥ 0.\nDue to |DX| ≥ 2, Proposition 5.15 implies that GETPOOLOFQUERIES (called within CALCQUERY) terminates and yields a non-empty query pool as output. SELECTBESTQUERY (also called within CALCQUERY) terminates as well since it simply selects one query from the pool according to the measure qsm() (cf. Section 5.3.3). Since we assume the interacting user to answer to a query or to reject it within finite time, u(Q) also terminates. It is clear that APPEND terminates. GETINVALIDDIAGS simply extracts one entry of the given q-partition and thus terminates. Finally, UPDATEQDATA also terminates by assumption (no qsm() must be used for which UPDATEQDATA might not terminate). As a consequence, all functions called in Algorithm 5 terminate. What remains to be proven is that the stop criterion must be met after a finite number of iterations, i.e. after a finite number of test cases have been added to the input DPI.\nIn mode = static the stop criterion must be satisfied after a finite number of iterations due to the following argumentation:\n• There is a finite set of minimal diagnoses w.r.t. the input DPI 〈K,B,P ,N 〉R since each (minimal) diagnosis w.r.t. this DPI is a subset of K according to Definition 3.5 and since |K| is finite by Definition 3.1.\n• In each iteration, one test case is added either to P ′ or N ′.\n• Each test case added to whatever set P ′ or N ′ invalidates at least one minimal diagnosis w.r.t. the input DPI in the set DX by the definition of a query (Definition 5.2) and since each query is computed w.r.t. the leading diagnoses DX by the correctness of GETPOOLOFQUERIES (cf. Proposition 5.15).\n• DX contains only minimal diagnoses w.r.t. the input DPI by Proposition 6.1.\n• Also by Proposition 6.1, no invalidated minimal diagnosis w.r.t. the input DPI can be an element of some subsequent set of leading diagnoses DX.\n• Therefore, unless the stop criterion is met before due to a sufficiently high probability of one of multiple leading diagnoses as per pD(), Algorithm 5 inmode = staticmust arrive at a point where\nCHAPTER 5. INTERACTIVE KNOWLEDGE BASE DEBUGGING 119\n|DX| = 1 after a finite number of iterations. Note that |DX| = 0 is impossible due to the definition of a query (Definition 5.2) which ensures that each added test case leaves valid at least one minimal diagnosis in DX.\nAlgorithm 5 terminates in mode = dynamic since for any sequence QA of queries that are added to the positive or negative test cases P ′ or N ′, respectively, there is a finite number kQA such that there is no more than one minimal diagnosis w.r.t. 〈K,B,P ∪ P ′,N ∪N ′〉R for |P ′| + |N ′| = kQA wherefore the stop criterion must be met. Now, let us assume that the opposite holds. That is, there is a sequence QA∗ of queries that are added to the positive or negative test cases P ′ or N ′, respectively, and for all natural numbers k there is more than one minimal diagnosis w.r.t. 〈K,B,P ∪ P ′,N ∪N ′〉R for |P ′|+ |N ′| = k. Then we argue as follows to derive a contradiction:\n• There is a finite set of (minimal) diagnoses w.r.t. any DPI 〈K,B,P ∪ P ′,N ∪N ′〉R obtained from the input DPI by the addition of test cases. This is true since |K| is finite by Definition 3.1 and since each (minimal) diagnosis w.r.t. 〈K,B,P ∪ P ′,N ∪N ′〉R is a subset of K according to Definition 3.5.\n• In each iteration, one test case is added either to P ′ or N ′.\n• Each test case added to whatever set P ′ or N ′ invalidates at least one minimal diagnosis w.r.t. the current DPI in the set DX by the definition of a query (Definition 5.2) and since each query is computed w.r.t. the leading diagnoses DX by the correctness of GETPOOLOFQUERIES (cf. Proposition 5.15).\n• If DPI denotes the current DPI at the time DYNAMICHS is called, then the set DX returned by DYNAMICHS is a subset of or equal to mDDPI , i.e. DX contains only minimal diagnoses w.r.t. DPI by Proposition 6.2.\n• Let 〈DPI0, DPI1, . . . 〉 denote the sequence of DPIs encountered in the case of adding answered queries as test cases to the input DPI DPI0 as per QA∗. Further, let 〈aD0,aD1, . . . 〉 be the sequence such that aDi := aDDPIi , i = 0, 1, . . . , i.e. aDi is the set of all diagnoses w.r.t. DPIi. Then aDi ⊃ aDi+1 for all i ≥ 0.\n• As each query added as a test case to DPIi leaves valid at least one (minimal) diagnosis w.r.t. DPIi due to Definition 5.2, we have that aDk ⊃ ∅ for k = 0, 1, . . . .\n• Since aDi is finite, there must be some finite number k∗ such that |aDk∗ | = 1 wherefore |mDk∗ | = 1 must also be valid. This is a contradiction.\nThence, Algorithm 5 terminates in any mode mode. Now, we show that propositions (1)-(6) of Proposition 5.16 hold for (i) mode = static and (ii) mode = dynamic.\n(i): First, by the proof so far, we have that Algorithm 5 in mode = static given the input DPI 〈K,B,P ,N 〉R terminates. Since the only point where the algorithm can terminate is line 14, GETSOLKB is called with arguments 〈Dmax, 〈K,B,P ∪ P ′,N ∪N ′〉R,P ′, static〉. By the definition of GETSOLKB (see Section 5.3.2.4), we have that (K \\ Dmax) ∪ UP is returned by the algorithm.\nPropositions (1) and (2) follow from the specification of the GETMODE function which is called with arguments 〈DX, pD()〉. Proposition (3) is true since GETSOLKB can never be reached without pD(Dmax) ≥ 1−σ being fulfilled. DX ⊆mD〈K,B,P,N 〉R ∩mD〈K,B,P∪P ′,N∪N ′〉R is true due to Proposition 6.1, Remark 5.15 and the fact that DX is obtained as an output of STATICHS. Hence, Proposition (4) holds. Proposition (5) is implied by Remark 5.15 and by the specification of the GETFORMULAPROBS function which computes pK() from pK̃∪K() as per Formulas 4.2 and 4.7 in line 5. Finally, Proposition (6) is a consequence of the definition of the GETPROBDIST function which accounts for the computation of\nCHAPTER 5. INTERACTIVE KNOWLEDGE BASE DEBUGGING 120\npD() from pK(), the input DPI, DX and the chronological sequence of all queries and associated answers QA so far. Therefore, Proposition 5.16 is true for mode = static.\n(ii): First, by the proof so far, we have that Algorithm 5 in mode = dynamic given the input DPI 〈K,B,P ,N 〉R terminates. Since the only point where the algorithm can terminate is line 14, GETSOLKB is called with arguments 〈Dmax, 〈K,B,P ∪ P ′,N ∪N ′〉R,P ′, dynamic〉. By the definition of GETSOLKB (see Section 5.3.2.4), we have that (K \\ Dmax) ∪ UP∪P ′ is returned by the algorithm.\nPropositions (1) and (2) follow from the specification of the GETMODE function which is called with arguments 〈DX, pD()〉. Proposition (3) is true since GETSOLKB can never be reached without pD(Dmax) ≥ 1 − σ being fulfilled. DX ⊆ mD〈K,B,P∪P ′,N∪N ′〉R is true due to Proposition 6.2, Remark 5.15 and the fact that DX is obtained as an output of DYNAMICHS. Hence, Proposition (4) holds. Proposition (5) is implied by Remark 5.15 and by the specification of the GETFORMULAPROBS function which computes pK() from pK̃∪K() as per Formulas 4.2 and 4.7 in line 5. Finally, Proposition (6) is a consequence of the definition of the GETPROBDIST function which accounts for the computation of pD() from pK(), the input DPI, DX and the chronological sequence of all queries and associated answers QA so far. Therefore, Proposition 5.16 is true for mode = dynamic.\nNext, we show that the solution to Interactive Static KB Debugging is found for σ = 0 in case mode = static:\n(s1) DX ⊆mD〈K,B,P,N 〉R ∩mD〈K,B,P∪P ′,N∪N ′〉R holds for the output of STATICHS in each iteration by Proposition 6.1. Therefore, DX comprises only minimal diagnoses w.r.t. the input DPI that comply with all specified test cases in P ′ and N ′.\n(s2) By pK̃∪K() : K̃ ∪ K → (0, 1] we derive by Formula 4.2 that each formula in K must have a probability greater than zero. Further, by Formula 4.7, no formula in K can have a probability greater than or equal to 0.5 (i.e. in particular a probability of 1 is not possible for a formula). Hence, we have that pK : K → (0, 0.5) for the measure pK() computed by GETFORMULAPROBS in line 5 in Algorithm 5. Thence, by the definition of pnodes() in STATICHS based on p() := pK() (cf. Definition 4.9 on page 64) due to the fact that pK() is given as an input argument to STATICHS in line 8, we have that no diagnosis can have an (a-priori) probability of zero. Since the function GETPROBDIST might only perform some multiplications of a diagnosis probability by 12 , also the a-posteriori probability of each diagnosis must be greater than zero.\n(s3) Hence, due to σ = 0, it must be necessarily be true that |DX| = 1 before the algorithm terminates.\n(s4) By Problem Definition 5.2 and the specification of the GETSOLKB function, the output solution KB must be the solution to Interactive Static KB Debugging.\nThat a solution found for σ > 0 in case mode = static might be an approximate solution to Interactive Static KB Debugging is a direct consequence of the definition of approximate solution given in Remark 5.12.\nFinally, the proof that the solution to Interactive Dynamic KB Debugging is found for σ = 0 in case mode = dynamic is analogue to the one for mode = static, just\n(d1) DX ⊆ mD〈K,B,P∪P ′,N∪N ′〉R holds for the output of DYNAMICHS in each iteration by Proposition 6.2. Therefore, DX comprises only minimal diagnoses w.r.t. the current DPI.\n(d2) By (s2), (s3), Problem Definition 5.1 and the specification of the GETSOLKB function, the output solution KB must be the solution to Interactive Dynamic KB Debugging.\nThat a solution found for σ > 0 in case mode = dynamic might be an approximate solution to Interactive Dynamic KB Debugging is a direct consequence of the definition of approximate solution given in Remark 5.12.\nThis completes the proof of Proposition 5.16.\nChapter 6\nIterative Diagnosis Computation\nIn this chapter we will introduce and discuss two methods, STATICHS and DYNAMICHS, which are called in lines 8 and 10 of Algorithm 5, respectively. The former provides a method for solving the Interactive Static KB Debugging Problem (Problem Definition 5.2) whereas the latter aims at solving the Interactive Dynamic KB Debugging Problem (Problem Definition 5.1). Both are methods for iterative diagnosis computation that are employed to compute a set of leading diagnoses in each iteration of the presented interactive KB debugging algorithm (Algorithm 5). Each time a query has been answered by the interacting user and added to the respective set of test cases of the DPI, a subset of the leading diagnoses (and usually also a set of not-yet-computed minimal diagnoses) is invalidated. An iterative diagnosis computation method is then invoked to update the leading diagnoses set taking the new information into account that is given by the recently added test case. That is, the k ≤ nmax most probable ways of solving the Interactive Static (Dynamic) KB Debugging Problem in the light of the new evidence are extracted by STATICHS (DYNAMICHS) after the search space has been suitably pruned. In this vein, if there is only one solution left, the (exact) solution of Interactive Static (Dynamic) KB Debugging has been found."
    }, {
      "heading" : "6.1 STATICHS: A Static Iterative Diagnosis Computation Algorithm",
      "text" : "As the name already suggests, STATICHS (Algorithm 7) is a procedure that solves the problem of Interactive Static KB Debugging defined by Problem Definition 5.2 if used for leading diagnosis computation in Algorithm 5. STATICHS is sound, complete and optimal w.r.t. the set of solutions of the Interactive Static KB Debugging problem. Optimality refers to the best-first computation of minimal diagnoses regarding a given probability measure."
    }, {
      "heading" : "6.1.1 Overview and Intuition",
      "text" : "The STATICHS algorithm is strongly related to the non-interactive hitting set algorithm HS (see Algorithm 2) in that, at any stage during the execution of Algorithm 5, the hitting set tree produced by STATICHS corresponds to some part of the complete (non-interactive) wpHS-tree built-up by Algorithm 2. This is achieved by the strategy to use new test cases only for the invalidation of diagnoses, and not for the computation of conflict sets (and thus diagnoses). That is, all minimal conflict sets are computed w.r.t. the input DPI. Thereby, the introduction of new diagnoses, i.e. ones that are not minimal diagnoses w.r.t. the input DPI, through addition of new test cases to the DPI is prohibited (cf. Proposition 4.6).\nSo, what STATICHS as a subroutine of Algorithm 5 does is gradually building up the standard (noninteractive) wpHS-tree in multiple phases. During each phase some new (not-yet-computed) minimal\n121\nCHAPTER 6. ITERATIVE DIAGNOSIS COMPUTATION 122\ndiagnoses w.r.t. the input DPI are computed, in the order of their probability, most probable ones first. Before such a newly detected minimal diagnosis is added to the set of leading diagnoses (Dcalc ∪DX), a test is performed that verifies that this new diagnosis is consistent with all test cases added to the input DPI so far. In this vein, all answered queries so far not only serve to eliminate a subset of the set of leading diagnoses at the time when the respective query is answered, but also to eliminate incompatible minimal diagnoses w.r.t. the input DPI that are found at some later point in time. However, in order to be eliminated due to a specified test case, a minimal diagnosis must first be computed. That is, no partial diagnoses can be eliminated due to newly specified test cases.\nBetween each two phases of tree construction, a query computed on the basis of the current set of leading diagnoses is asked to the user (this is accomplished directly in Algorithm 5). After incorporating the user’s answer, some leading diagnoses are eliminated (this is granted by the definition of a query, see Definition 5.2). Moreover, the “state” of the tree is maintained during the execution of Algorithm 5 until STATICHS is again called in order to calculate further leading diagnoses. The state of the current partial wpHS-tree is stored by variables\n• Dcalc ∪ DX – computed minimal diagnoses w.r.t. the input DPI consistent with all test cases specified so far,\n• Q – the list of open, non-labeled nodes,\n• Ccalc – minimal conflict sets w.r.t. the input DPI computed so far and\n• D× – computed minimal diagnoses w.r.t. the input DPI not consistent with all test cases specified so far.\nEach time a tree construction phase, i.e. the computation of new leading diagnoses, is finished, a new diagnosis probability distribution is obtained by the diagnosis probability update as per Bayes’ Theorem described in Section 5.3.2. Once this distribution involves one highly probable diagnosis (the probability of which exceeds a predefined threshold 1 − σ) and else just highly improbable ones, the algorithm terminates. The output is a solution KB w.r.t. the input DPI built from this highly probable minimal diagnosis.\nRemark 6.1 In case σ has a predefined value of zero, the output is the (exact) solution to the problem of Interactive Static KB Debugging for the input DPI. In a scenario where some fault tolerance σ > 0 is given, the solution KB returned by Algorithm 5 is an approximation of the (exact) solution to Interactive Static KB Debugging for the input DPI where a better approximation can be expected for smaller values of σ (cf. Remark 5.12). “Better” in this context refers to the satisfaction of desired semantic properties of the KB returned by Algorithm 5, i.e. desired entailments and desired non-entailments of the KB. The intuition is that the specification of additional test cases T guarantees the output of a KB complying with these test cases, whereas accepting one – albeit highly probable – of multiple solution KBs without having incorporated T leaves open the possibility for this KB to not fulfill T .\nHowever, answering queries is effort for an interacting user. Therefore, the approach that involves the “early” termination of the algorithm after a solution KB has a sufficiently high probability (lower than 1) constitutes a trade-off between exactness of the output and the effort of the user and overall execution time of the interactive KB debugging algorithm, respectively.\nConstant “Convergence” towards the Solution. As said, each added test case is an answered query and thus eliminates at least one minimal diagnosis w.r.t. the input DPI. And, only minimal diagnoses w.r.t. the input DPI are computed by STATICHS. Hence, by the fact that a solution to Interactive Static KB Debugging can only be constructed from a minimal diagnosis w.r.t. the input DPI, it is guaranteed that the number of solutions to Interactive Static KB Debugging is strictly monotonically decreasing\nCHAPTER 6. ITERATIVE DIAGNOSIS COMPUTATION 123\nthroughout the execution of Algorithm 5. That is, the initial number of (all) minimal diagnoses (w.r.t. the input DPI) is “static” which means that no “new” minimal diagnoses can be introduced when the input DPI is extended by new test cases.\nAs a consequence of this, it is reasonable to employ STATICHS in a situation where the (complete) wpHS-tree produced by the standard (non-interactive) algorithm HS is believed to be as compact as to fit into the available system memory. In this case, STATICHS is also guaranteed to not exceed the available memory, even if an exact solution (σ = 0) is intended.\nUnfortunately, however, it will be generally the case that a complete enumeration of all minimal diagnoses is intractable, especially due to an overwhelming space complexity. In such a case, Algorithm 5 using STATICHS will definitely run out of memory (given that STATICHS is called sufficiently often). The reason is that the space consumption of STATICHS will sooner or later definitely reach the huge extent of the wpHS-tree produced by HS. Nevertheless, STATICHS might be used to (possibly) find some (approximate) solution. This might work in a scenario where the given probabilistic information in terms of pK̃∪K() provided as an input to Algorithm 5 is “reasonable” in that the desired diagnosis is assigned a rather high probability and is thus figured out early, before the available memory is exhausted.\nA possible modification of the stop criterion in STATICHS in a way that new leading diagnoses are not computed until a desired number of such is detected or a timeout is reached, but rather until a predefined maximum space is consumed, would not mitigate space complexity issues very much. An explanation for this is that stopping STATICHS on account of no more available memory implies that no further call of STATICHS will be able to execute. That is because, as mentioned before, an added test case can only invalidate already computed diagnoses, no other branches in the wpHS-tree, and each invalidated minimal diagnosis cannot be discarded, but must be stored (in D×) to avoid the usage of leading diagnoses that are non-minimal w.r.t. the input DPI (cf. lines 21-23 in Algorithm 7).\nPoor Pruning. As we explained before, the preservation of a constantly shrinking set of minimal diagnoses comes at the cost of being able to exploit new test cases only partially, i.e. only for the invalidation of already computed minimal diagnoses w.r.t. the input DPI and not for the computation of minimal conflict sets and thus minimal diagnoses. The incorporation of test cases into the DPI that is used to determine minimal conflict sets (line 30 in Algorithm 7) could, on the one hand, lead to new minimal conflict sets that are no minimal conflict sets w.r.t. the input DPI. As a consequence of this, minimal diagnoses might be determined by the algorithm which are no minimal diagnoses w.r.t. the input DPI, but w.r.t. the current DPI. Hence, the soundness of STATICHS w.r.t. the set of solutions of the Interactive Static KB Debugging problem would be violated. Furthermore, such conflict sets could lead to the missing of some minimal diagnoses w.r.t. the input DPI, a violation of the completeness of STATICHS w.r.t. the set of solutions of the Interactive Static KB Debugging problem.\nOn the other hand, the exploitation of new test cases for conflict set generation might give rise to the possibility of pre-pruning of any tree branches, not just branches that already correspond to diagnoses w.r.t. the input DPI. Such a “dynamic” strategy which exploits the new information given by a test case not just partially, but for the invalidation and computation of diagnoses and conflict sets, will be implemented be DYNAMICHS which we will detail in Section 6.2.\nPut another way, in STATICHS only the standard pruning rules for the construction of a wpHS-tree are applicable, namely the deletion of duplicate nodes and the elimination of non-minimal diagnoses (cf. Definition 4.10). Newly defined test cases only facilitate the deletion of tree branches from the leading diagnoses set Dcalc ∪ DX, but not from memory (as invalidated minimal diagnoses must be stored in D×, as pointed out before).\nTo summarize, STATICHS on the one hand makes sure to only consider relevant solutions of the problem of Interactive Static KB Debugging, but on the other hand suffers from this conservative strategy in that tree pruning cannot be designed very effectively. So, on the positive side, uncontrolled growth of the produced wpHS-tree can be avoided, but, on the negative side, consultation of an interacting user\nCHAPTER 6. ITERATIVE DIAGNOSIS COMPUTATION 124\ncannot be taken advantage of in terms of reduction of the space complexity of STATICHS compared to the construction of a wpHS-tree by a non-interactive procedure like Algorithm 2."
    }, {
      "heading" : "6.1.2 Algorithm Walkthrough",
      "text" : "Input Parameters. When STATICHS (Algorithm 7) is called for the first time in Algorithm 5, the inputs Ccalc, DX, D×, P ′ and N ′ correspond to the empty set and Q = [∅] (cf. lines 1-4 and 8 in Algorithm 5). Further on, Dcalc is defined to be the empty set at the beginning of each execution of STATICHS. That is, STATICHS starts the construction of the wpHS-tree from an initial tree consisting of a single unlabeled root node ∅ (∈ Q). And, all collections that are later returned by STATICHS, except for Q, are initially empty. Further input arguments are the DPI 〈K,B,P ,N 〉R provided as an input to Algorithm 5, the sets of positively (P ′) and negatively (N ′) answered queries since the start of Algorithm 5, the leading diagnosis computation parameters nmin, nmax, t (see description in Section 5.1 on page 78) and the probability measure p() := pK() that assigns a probability in the interval (0, 0.5) to each formula in K (cf. line 5 in Algorithm 5).\nThe Main Loop. During the repeat-loop, in each iteration the first node node in Q is processed (GETFIRST, line 5). That is, node is deleted from Q (DELETEFIRST, line 6) and the SLABEL function is called given node (i.a.) as a parameter. Notice that elements are added to Q (line 17) in a way that a sorting of Q in descending order according to pnodes() (cf. Definition 4.9) is maintained throughout the execution of STATICHS.\nComputation of a Node Label. The SLABEL function processes node as follows. First, the nonminimality criterion (lines 21-23) is checked. That is, among all nodes in D(×,X,calc) = D×∪DX∪Dcalc one is searched which is a subset of node. If such a node nd is found, then node must be a non-minimal diagnosis (nd ⊂ node) or a duplicate diagnosis (nd = node) w.r.t. 〈K,B,P ,N 〉R since all sets D×, DX and Dcalc contain only minimal diagnoses w.r.t. 〈K,B,P ,N 〉R. In this case, the branch in the wpHS-tree corresponding to node can be dismissed which is taken account of by returning the label closed for node.\nIn case the non-minimality criterion is not satisfied, the duplicate criterion (lines 24-26) is checked next. Here, Q is browsed for a node that is equal to node. If such a one is found, node can be discarded because it suffices to consider only one tree branch among multiple tree branches in the wpHS-tree featuring one and the same set of edge labels. Hence, closed is returned as a label for node. Altogether, this means that only the last processed exemplar of a node corresponding to one and the same set of edge labels is labeled, all others are discarded.\nIf the duplicate criterion is not met, the reuse criterion (lines 27-29) is checked next. That is, Ccalc is browsed for a set C (Ccalc comprises only minimal conflict sets w.r.t. 〈K,B,P ,N 〉R) such that C and node are disjoint sets. If such a C is detected, then C can be used to label node since the set of edge labels along the path in the wpHS-tree leading from the root node to node does not hit C. In this case, the label C is returned for node by SLABEL.\nGiven that the reuse criterion fails, QX is called given the DPI 〈K \\ node,B,P ,N 〉R as an argument (line 30). If the output L is equal to ’no conflict’, then we know by Proposition 4.9 that node is a diagnosis w.r.t. 〈K,B,P ,N 〉R, wherefore the label valid is returned for node. Otherwise, the output L must be a minimal conflict set w.r.t. 〈K,B,P ,N 〉R that has an empty set-intersection with node. Since the reuse criterion failed, i.e. there is no set in Ccalc that does not intersect with node, L must be a fresh minimal conflict set w.r.t. 〈K,B,P ,N 〉R in the sense that L /∈ Ccalc must hold. Therefore the label L is first added to Ccalc and then returned by SLABEL as a label for node.\nProcessing of a Node Label. Back in the main procedure, Ccalc is updated (line 8) and then the label L returned by the SLABEL function is processed as follows. If L = valid, then it is a fact that node is a\nCHAPTER 6. ITERATIVE DIAGNOSIS COMPUTATION 125\nminimal diagnosis w.r.t. 〈K,B,P ,N 〉R, but it is not certain that node also meets all positive test cases P ′ and all negative test cases N ′ that have been specified and added to 〈K,B,P ,N 〉R so far. Thus, according to Proposition 5.3, the validity of the KB K \\ node w.r.t. 〈·,B,P ∪ P ′,N ∪ N ′〉R must still be checked (line 10). If successful, node is added to the set Dcalc of calculated minimal diagnoses w.r.t. the input DPI that comply with all answered queries so far. Otherwise, node is added to the set D× of minimal diagnoses w.r.t. the input DPI that have been invalidated by some answered query.\nRoughly, the minimality of diagnoses added to Dcalc is assured by the pruning rule (lines 21-23) which eliminates non-minimal nodes and the fact that pnodes() sorts a node nd′ corresponding to a superset of some node nd behind nd in Q.\nIf, on the other hand, L = closed is the label returned by SLABEL, then node must simply be removed from Q which has already been executed in line 6. Thence, no actions are necessary (cf. line 14).\nIn the third case, if a minimal conflict set L is returned by SLABEL, then L is a label for node meaning that |L| successor nodes of node, namely a node node ∪ {e} for all elements e ∈ L, need to be added to Q in sorted order using the function pnodes() (INSERTSORTED, line 17).\nStop Criterion. The first criterion causing STATICHS to terminate is Q = [] which means that the complete wpHS-tree has been constructed and no further nodes can be labeled. In this case, Dcalc ∪DX comprises all minimal diagnoses w.r.t. 〈K,B,P ,N 〉R that are compliant with all the specified positive and negative test cases P ′ and N ′.\nIf the first criterion is not met, then the second criterion is checked. That is, a test is performed which checks whether the number of leading minimal diagnoses w.r.t. 〈K,B,P ,N 〉R in Dcalc ∪DX amounts to at least nmin and either |Dcalc ∪ DX| = nmax or more than t time has passed since the start of the execution of STATICHS. In the latter case, nmin ≤ |Dcalc ∪ DX| < nmax holds. In the former case, |Dcalc ∪DX| = nmax is satisfied.\nProcessing of the Leading Diagnoses Returned by STATICHS. When a call of STATICHS in Algorithm 5 returns 〈Dcalc ∪DX,Q,Ccalc,D×〉, the set Dcalc ∪ DX is stored in the variable DX in Algorithm 5. Between two successive calls of STATICHS in Algorithm 5, only this set DX as well as D× are modified. The list Q and the set Ccalc remain unchanged until they are used as input parameters to the next call of STATICHS in Algorithm 5.\nIn case one diagnosis Dmax of the current leading diagnoses in DX has a probability greater or equal 1 − σ as per the probability measure pD() (see Section 5.3.2), the stop criterion of interactive KB debugging is met and a solution KB w.r.t. 〈K,B,P ,N 〉R constructed from the input DPI 〈K,B,P ,N 〉R as well as from Dmax is returned to the user. Thereafter, Algorithm 5 terminates and no more calls of STATICHS take place.\nOtherwise, if no leading diagnosis satisfies the stop criterion, a query Q together with its q-partition P(Q) is computed as has been detailed in Sections 5.2 and 5.3.2. An answer u(Q) to this query is submitted by the interacting user (line 17 in Algorithm 5). Then u(Q) along with P(Q) is exploited to figure out the subset Dout of DX that does not comply with u(Q). This set Dout is then deleted from DX and added to D×. Additionally, Q is added to the positive test cases P ′ if u(Q) = true and to the negative test cases N ′ otherwise. Subsequently, STATICHS is called again given\n• the updated parameters DX, D×, P ′ and N ′ (which are modified within and outside of STATICHS during the execution of Algorithm 5),\n• the unchanged parameters Q, Ccalc (which are modified only within STATICHS during the execution of Algorithm 5) and\n• the constant parameters 〈K,B,P ,N 〉R, t, nmin, nmax and pK() (which are not modified within or outside of STATICHS during the execution of Algorithm 5).\nCHAPTER 6. ITERATIVE DIAGNOSIS COMPUTATION 126\nThe execution of this next and any subsequent call to STATICHS runs in analogue way as described.\nRemark 6.2 We want to emphasize that queries are computed w.r.t. the current DPI 〈K,B,P ∪P ′,N ∪ N ′〉R although STATICHS focuses on solutions to the problem of Interactive Static KB Debugging which involves exclusively minimal diagnoses w.r.t. the input DPI 〈K,B,P ,N 〉R. However, a minimal diagnosis w.r.t. 〈K,B,P ,N 〉R that satisfies all positive test cases P ′ as well as all negative test cases N ′ is also a minimal diagnosis w.r.t. 〈K,B,P ∪ P ′,N ∪N ′〉R. And, a minimal diagnosis w.r.t. 〈K,B,P ,N 〉R that does not satisfy all positive test cases P ′ as well as all negative test cases N ′ is not a minimal diagnosis w.r.t. 〈K,B,P ∪ P ′,N ∪N ′〉R.\nHence, it holds that\n• D is a minimal diagnosis w.r.t. 〈K,B,P ,N 〉R that satisfies P ′ ∪ {Q} as well as N ′ if and only if D is a minimal diagnosis w.r.t. 〈K,B,P ∪ P ′ ∪ {Q} ,N ∪N ′〉R and\n• D is a minimal diagnosis w.r.t. 〈K,B,P ,N 〉R that satisfies P ′ as well as N ′ ∪ {Q} if and only if D is a minimal diagnosis w.r.t. 〈K,B,P ∪ P ′,N ∪N ′ ∪ {Q}〉R.\nTherefore, each query constructed during Algorithm 5 with mode = static must be a query w.r.t. the current set of leading diagnoses DX and the current DPI 〈K,B,P ∪ P ′,N ∪ N ′〉R (cf. Equation 5.1, Definition 5.3 and Proposition 5.3 on pages 78-80).\nAs a consequence of this, no additional test is required in order to ascertain that each diagnosis in the set DX that is given as a parameter to the next call of STATICHS does in fact satisfy all answered queries so far.\nThe following proposition states the correctness of STATICHS (a proof of this proposition is beyond the scope of this work):\nProposition 6.1 (Correctness of STATICHS). Any call to STATICHS (given the inputs described in Algorithm 7) within Algorithm 5 terminates and yields an output 〈D,Q,Ccalc,D×〉 where\n(1) it holds for D that\n(a) D ⊆ mD〈K,B,P,N 〉R ∩ mD〈K,B,P∪P ′,N∪N ′〉R is the set of most probable minimal diagnoses w.r.t. 〈K,B,P ,N 〉R that satisfy all test cases P ′ and N ′ such that\n(i) nmin ≤ |D| ≤ nmax and (ii) D ⊃ DX,\nif such a set D exists, or\n(b) D is equal to the set of all minimal diagnoses mD〈K,B,P,N 〉R ∩mD〈K,B,P∪P ′,N∪N ′〉R , otherwise,\nwhere “most-probable” refers to the probability measure pnodes() (cf. Definition 4.9) obtained from the given function p();\n(2) Q is the current queue of open (non-labeled) nodes of the produced (partial) wpHS-tree,\n(3) Ccalc is the set of all minimal conflict sets w.r.t. 〈K,B,P ,N 〉R computed so far and\n(4) D× is the set of all minimal diagnoses w.r.t. 〈K,B,P ,N 〉R computed so far where each diagnosis in D× does not satisfy all test cases P ′ and N ′.\nCHAPTER 6. ITERATIVE DIAGNOSIS COMPUTATION 127"
    }, {
      "heading" : "6.1.3 STATICHS: Examples",
      "text" : "In this section we will give two examples of how interactive KB debugging using STATICHS (Algorithm 5 with parameter mode = static) works. The first one will show the similarities and differences between the usage of STATICHS (within Algorithm 5) and HS (within Algorithm 3) since it will depict the application of STATICHS on the same example DPI (see Table 4.1) that was used to show the functionality of HS in examples 4.8 and 4.9. At the same time, the first example will provide evidence that solving the problem of Interactive Static KB Debugging can be more efficient than solving the problem of Interactive Dynamic KB Debugging in terms of the number of query answers required from an interacting user. This will be discussed in more detail in Section 6.3.\nThe second example is supposed to deepen the reader’s understanding of the way STATICHS works. To this end, the example DPI provided by Table 4.2 will be used which constitutes a significantly harder (interactive) debugging task than the DPI investigated in the first example. This example will involve the construction of a relatively large hitting set tree and thereby give a presentiment of the space and time complexity problems caused by the poor tree pruning inherent in the STATICHS algorithm. In addition, this example will draw a reverse image of the first example in that it will stress the advantage of the decision to search for a solution of Interactive Dynamic KB Debugging rather than for a solution of Interactive Static KB Debugging (more on that in Section 6.3).\nExample 6.1 In this example we assume that the author (called user throughout this example) of the (admissible) DPI 〈K,B,P ,N 〉R given by Table 4.1 applies Algorithm 5 with mode = static to interactively debug 〈K,B,P ,N 〉R. Further, suppose the following user requirements:\nIn order to guarantee a fast reaction time of the system (the time between two successive queries to the user), the user wants each query to be computed from the minimally necessary number of leading diagnoses. Thus, in each iteration exactly two leading diagnoses should be computed by STATICHS (cf. Proposition 5.5). This postulation is reflected by setting nmin = nmax = 2. Notice that the time limit t is irrelevant in this case.\nMoreover, the user desires to get just any query, i.e. they do not demand any particular properties – such as optimal information gain among a pool of queries – to be satisfied by a query. This can be ensured by choosing q := 1 (cf. Section 5.2) and qsm() equal to any query selection measure described in Section 5.3.3.\nThe user is new to KB debugging and has neither an idea of faults they frequently make nor access to any kind of data that would indicate their tendency to certain types of faults. Thence, pK(ax ) := c < 0.5 for all ax ∈ K, i.e. all formula fault probabilities are specified to be equal (to some constant c). In such a case, if a formula fault probability measure pK() is given as an input to Algorithm 5, then line 5 in Algorithm 5 is omitted. Please notice that this aspect is not shown in Algorithm 5.\nFinally, the user’s intention is to get the (exact) solution to the problem of Interactive Static KB Debugging. This can be taken into account by specifying σ := 0.\nThe tree constructed and parameters computed and used by Algorithm 5 using STATICHS are visualized by Figure 6.1. We use the same notation as in Figures 4.2 and 4.3 which is described in Examples 4.8 and 4.9. The only new notational element here is the =⇒ labeled by some designator of a query. That is, X(Di) Qj =⇒ X means that Di is still a minimal diagnosis after Qj has been answered and added to the respective set of test cases of the DPI. On the other hand, X(Di) Qj\n=⇒ × signifies that the minimal diagnosis Di is invalidated through the addition of the answered query Qj to the respective set of test cases of the DPI. Please notice that =⇒ does not point at a node of the wpHS-tree. Instead, the label at which =⇒ points is to be understood as the new label of the node originally labeled by X(Di) from which the (first of possibly multiple) =⇒ goes out. This notation should help to keep track of the evolution of node labels in the wpHS-tree without needing to overload a single node by multiple different successive labels.\nCHAPTER 6. ITERATIVE DIAGNOSIS COMPUTATION 128\nIn the first iteration, i.e. during the execution of the first call of STATICHS during Algorithm 5, the root node (initially the empty set) is labeled by the minimal conflict set 〈1, 2, 5〉 w.r.t. 〈K,B,P ,N 〉R and three successor nodes, namely {1}, {2} as well as {5}, are added to the queue of open nodes Q. Since all formulas have been assigned an equal fault probability, STATICHS conducts a breadth-first tree construction (as displayed by the numbers i© that give the order of node labeling). That is, Q in this case is a first-infirst-out queue. In this vein, first [1] and then [2] are identified as minimal diagnoses w.r.t. the given DPI. Since DX∪Dcalc = ∅∪{[1], [2]} has a cardinality of nmin = nmax = 2, the stop criterion of STATICHS causes it to terminate and return 〈Dcalc ∪DX,Ccalc,Q,D×〉 = 〈{[1], [2]} , {〈1, 2, 5〉} , [{5}], ∅〉 (because DX and D× are initially empty sets), as shown in the upper right column in Figure 6.1.\nThen, in Algorithm 5, outside of the STATICHS procedure, the first query Q1 = {E → ¬A} is computed from the leading diagnoses set {[1], [2]}. The q-partition P(Q1) associated withQ1 is 〈{[1]} , {[2]}, ∅〉. The user’s answer u(Q1) to Q1 is then false . Thence, the set Dout is calculated from P(Q1) as D+(Q1) = {[1]} (due to negative answer, cf. Remark 5.6), deleted from DX := DX ∪Dcalc to yield DX = {[2]} and added to D× to yield D× = {[1]}. The set DX corresponds to the set of all already computed minimal diagnoses w.r.t. the input DPI that satisfy all queries answered so far. The set D× comprises all already computed minimal diagnoses w.r.t. the input DPI that do not satisfy all queries answered so far. These sets DX and D× along with the collections Q and Ccalc which are unmodified outside of STATICHS are used as input arguments for the second call of STATICHS. Notice that, in the figure, the resulting values of operations performed within STATICHS are given in the righthand column above the dashed line whereas values computed outside of STATICHS are given below the dashed line.\nAfter the modifications caused by the addition of the query Q1 to the negative test cases of 〈K,B,P , N 〉R have been taken into account in step 4©, the partial wpHS-tree built in iteration 1 is further constructed in iteration 2 resulting in the tree depicted by the middle picture in the lefthand column of Figure 6.1. Whereas the branches with edge labels {5, 1} and {5, 2} correspond to proper supersets of the minimal diagnoses [1] and [2], respectively, w.r.t. the input DPI 〈K,B,P ,N 〉R and are thus closed by the non-minimality criterion tested in the SLABEL function, the branch with edge labels {5, 7} is identified as a minimal diagnosis D3 := [5, 7] w.r.t. 〈K,B,P ,N 〉R. However, D3 is not directly added to the set Dcalc. In fact, the validity of the KB K \\ D3 w.r.t. the current DPI 〈K,B,P ,N ∪ {Q1}〉R is tested beforehand. As this test is successful, meaning that D3 ∈ mD〈K,B,P,N 〉R ∩mD〈K,B,P,N∪{Q1}〉R , D3 can be safely added to Dcalc implying the set of leading diagnoses DX ∪Dcalc = {D2,D3} with cardinality two. Due to nmin = nmax = 2, STATICHS terminates.\nAfter the second query Q2 has been answered negatively involving the dismissal of the leading diagnosisD2, STATICHS ends up with an empty queue Q of open nodes in iteration 3 (see the tree in the lower left column of Figure 6.1). Hence, STATICHS returns a singleton set including the leading diagnosis D3. Now, independently of the specified formula probabilities, pD(D3) = 1 ≥ 1 − σ = 1 is satisfied since the probability space considered by the probability measure pD() focuses on the sample space Ω = {D3} (cf. Sections 4.5 and 5.3.2). Thus, the stop condition of Algorithm 5 is met wherefore the solution KB Ksol := (K \\ D3) ∪ UP = (K \\ D3) ∪ ∅ = K \\ D3 is returned to the user. This solution KB Ksol is the (exact) solution to Interactive Static KB Debugging given the DPI 〈K,B,P ,N 〉R of Table 4.1 as an input because D3 is the only minimal diagnosis w.r.t. 〈K,B,P ,N 〉R that conforms with all answered queries Q1 = false and Q2 = false .\nAll in all, the execution of Algorithm 5 in this example performs\n• 2 full QX calls, i.e. calls of QX that actually return a minimal conflict set (there are two minimal conflict sets labeled by C in the picture at the bottom of the lefthand column in Figure 6.1) and\n• 6 validity checks, i.e. calls of QX that return ’no conflict’ (one check for each of the three found minimal diagnoses; notice that QX does only perform a single KB validity check by ISKBVALID in case it returns ’no conflict’, see Algorithm 1) or calls of ISKBVALID in line 10 in STATICHS (one call for each of the three found minimal diagnoses),\ncomputes\n• 3 minimal diagnoses w.r.t. the input DPI,\n• 2 minimal conflict sets w.r.t. the input DPI and\n• 2 queries and asks the user 2 logical formulas (1 per query)\nand stores\n• a maximum of 5 nodes (where node refers to the internal representation of a node in STATICHS as a set of edge labels along a path from the root node to a leaf node; there are even more nodes in the sense of tree nodes in the picture at the bottom of the lefthand column in Figure 6.1).\nExample 6.2 Let us now consider the (admissible) DPI 〈K,B,P ,N 〉R given by Table 4.2. We assume an expert (called user throughout this example) in the domain Dom modeled by K who wants to find a solution to Interactive Static KB Debugging for the given DPI 〈K,B,P ,N 〉R by means of Algorithm 5 with mode = static. Further, we suppose the following requirements:\nThe user wants each query to be computed from three leading diagnoses. Thus, after each iteration of STATICHS, the set DX ∪Dcalc should comprise exactly three elements. This postulation is reflected by setting nmin = nmax = 3. Notice that the time limit t is irrelevant in this case.\nMoreover, as in example 6.1, we assume no demand for queries satisfying special properties which is reflected by choosing q := 1 (cf. Section 5.2) and qsm() equal to any query selection measure described in Section 5.3.3.\nLet there be several documentations of past debugging sessions (e.g. in terms of formula change logs) involving KBs in the domain Dom of the author auth of K accessible to the user. Further, let the user have extracted term and logical construct probabilities pK̃∪K(ax ) ∈ [0, 1] for ax ∈ K for auth from this data. This function pK̃∪K : K̃ ∪ K → [0, 1] is then provided as an input to Algorithm 5.\nFinally, the user’s intention is to get the (exact) solution to the problem of Interactive Static KB Debugging. This can be taken into account by specifying σ := 0.\nThe tree constructed and parameters computed and used by Algorithm 5 using STATICHS are visualized by Figures 6.2 as well as 6.3. We use the same notation as in Figures 4.2, 4.3 and 6.1 which is described in Examples 4.8, 4.9 and 6.1.\nAfter the initialization of variables, Algorithm 5 calls the function GETFORMULAPROBS in line 5 which exploits pK̃∪K() to calculate the function pK() giving the fault probabilities of formulas in K (cf. Sections 4.5.1, 5.3.2 and Example 4.7). Let the resulting probabilities be as depicted by Table 6.1.\nThen, STATICHS is called for the first time, resulting in the wpHS-tree given in the first picture in\nFigure 6.2. Contrary to Example 6.1, where the tree was built up in breadth-first order, in this example the formula probabilities p() := pK() given by Table 6.1 are used to assign a probability pnodes(n) to each path n in the wpHS-tree starting from the root node (cf. Formula 4.6 and Definition 4.9). In this vein, as outlined by the numbers i© indicating when a node is labeled, after the root node has been labeled by C1 := 〈1, 2, 5〉, the node corresponding to the outgoing edge of C1 labeled by the formula with the largest fault probability among all formulas in C1 is labeled first. That is, the node {1} with pnodes({1}) = 0.41 (as opposed to the nodes {2} and {5} with 0.25 each) is labeled first. The SLABEL procedure, after\nCHAPTER 6. ITERATIVE DIAGNOSIS COMPUTATION 130\nchecking whether {1} is a non-minimal diagnosis w.r.t. 〈K,B,P ,N 〉R or a duplicate of some other node in Q (both checks negative), computes another minimal conflict set C2 := 〈2, 4, 6〉 such that {1}∩C2 = ∅ (C2 is not hit by the node {1}) to constitute a label for node {1}. The successor nodes {1, 2}, {1, 4} and {1, 6} of {1} are generated and added to the list Q in a way that the sorting of Q in descending order of pnodes() is maintained.\nSince {1, 4} (0.28) as well as {1, 6} (0.27) have a larger probability (as per pnodes()) than the nodes {2} (0.25) and {5} (0.25), Q is given by [{1, 4} , {1, 6} , {2} , {5} , {1, 2}] when it comes to the processing of the next node. Since STATICHS always treats the first node of Q next, it identifies the first minimal diagnosis D1 := [1, 4] w.r.t. 〈K,B,P ,N 〉R in step 3©. In steps 4© and 8©, two further minimal diagnoses D2 := [1, 6] and D3 := [5, 4] are detected. Altogether, the union of DX (initially the empty set) and Dcalc (comprising the three computed diagnoses) now contains 3 = nmin = nmax elements wherefore STATICHS terminates and outputs the tuple 〈Dcalc ∪DX,Ccalc,Q,D×〉 where the sets in this tuple are given under the wpHS-tree of iteration 1 in Figure 6.2.\nFrom this set of leading diagnoses DX := DX ∪ Dcalc, the probability measure pD : DX → [0, 1] is computed by the function GETPROBDIST (cf. Algorithm 6 and Section 5.3.2). The result is 〈pD(D1), pD(D2), pD(D3)〉 = 〈0.38, 0.37, 0.25〉. The modeDmax := D1 of this probability distribution is then computed by GETMODE. As σ = 0, pD(Dmax) = 0.38 6≥ 1 wherefore the stop criterion of Algorithm 5 is not satisfied.\nConsequently, Algorithm 5 proceeds to generate the first queryQ1 = {B v K} (based on the current set of leading diagnoses DX) along with its associated q-partition P(Q1) = 〈{D1,D2} , {D3} , ∅〉. The diagnosis D1 is in D+(Q1) because K∗1 = (K \\D1)∪B ∪UP (recall Formula 5.1 for a definition of K∗i ) comprises formulas 2, 3, 5, 6, 7, 8 and 9 as well as p1 (cf. Table 4.2) wherefore K∗1 |= {B v K} = Q1 (due to the set of formulas {2, 3} = {B v G,G v K}). That D2 belongs to D+(Q1) as well follows analogously. On the other hand,D3 ∈ D−(Q1) must be true sinceK∗3 ∪Q1 includes i.a. A v B (formula 1) and B v K (∈ Q1) wherefore {A v K} = n1 is an entailment of K∗3 . Thus, the negative test case n1 is violated.\nThe positive user answer u(Q1) = true is incorporated in that Q1 is appended to the set of positive test cases P yielding P ∪ {Q1} = {{r(x, y)} , {B v K}}. Step 9© shows the impact of this test case addition on the set of leading diagnoses, i.e. all diagnoses in the set Dout = D−(Q1) = {D3} (due to positive answer, cf. Remark 5.6) are re-labeled by × whereas all other leading diagnoses (D1,D2) are still labeled by X.\nIn the same fashion, further node labelings are conducted in iteration 2 until |DX ∪ Dcalc| = | {D1,D2} ∪ {[2, 1]} | = 3 = nmin = nmax holds again. These actions are displayed by the tree at the bottom of Figure 6.2.\nNotice that, after step 12©, two nodes corresponding to the same set are elements of the list Q. At step 13©, the duplicate criterion checked by SLABEL comes into play. Since the node {1, 2} (the leftmost branch in the tree) is ranked first in Q (we assume a first-in-first-out ordering of nodes corresponding to equal sets of edge labels in Q), the SLABEL procedure is called given node := {1, 2} as an argument and detects the node {2, 1} (the fourth leftmost branch in the tree) in Q. Hence, node = {1, 2} is closed as a duplicate node which finds expression in the label ×(dup). When {2, 1} (which must have the same probability as {1, 2} due to set-equality) is processed at step 14©, it is discovered to be a minimal diagnosis (D5) w.r.t. 〈K,B,P ,N 〉R.\nMoreover, we want to point out that another minimal diagnosis (D4 = [2, 4, 6]) is found in iteration 2 before D5 is detected. However, D4 is immediately ruled out and added to D× (cf. line 13 in STATICHS) due to the fact that K\\D4 is invalid w.r.t. the current DPI 〈·,B,P ∪{Q1} ,N 〉R (cf. Definition 3.3). The explanation why this holds is as follows:\nBy Definition 3.3,K\\D4 is valid w.r.t. 〈·,B,P∪{Q1} ,N 〉R iffK∗4 = (K\\D4)∪B∪U(P∪{Q1}) (recall Formula 5.1 for a definition of K∗i ) does not violate any r ∈ R = {consistency, coherency} and does not entail any n ∈ N = {n1,n2} = {{A v K} , {L v ∃r.F,B(x), G v K}}. Applying the diagnosis D4\nCHAPTER 6. ITERATIVE DIAGNOSIS COMPUTATION 131\nto K yields K \\ D4 = {1, 3, 5, 8} which includes in particular formula 1 which is equal to A v B (see Table 4.2). However, there is also the negative test case n1 indicating that A v K must not be entailed by K∗4 . That is, B v K ∈ K∗4 (due to Q1) and A v B ∈ K∗4 which implies that K∗4 |= {A v K} = n1 wherefore K∗4 is invalid w.r.t. 〈·,B,P ∪ {Q1} ,N 〉R.\nSuch a direct dismissal of a discovered diagnosisDi due to a newly added test case Qj is indicated by k©X(Di) Qj =⇒ k©×, i.e. the step number k© at the shaft of the =⇒ is equal to the step number at the head of =⇒. In case of the invalidation of a leading diagnosis (i.e. one that was utilized in the computation of Qj), on the contrary, the step number at the shaft is lower than the step number at the arrow head.\nAs shown at the top of Figure 6.3, the second query Q2 computed from the leading diagnosis set DX∪Dcalc = {D1,D2,D5} is then answered by u(Q2) = true as well, wherefore the leading diagnoses D2,D5 are ruled out and added to D×. So, the input argument DX given to the next call of STATICHS in Algorithm 5 consists of the single diagnosis D1.\nIn the third iteration (see the picture given in Figure 6.3), STATICHS again executes in order to complete the leading diagnosis set to contain three elements. However, as we can say in advance, D1 is the only minimal diagnosis w.r.t. the input DPI 〈K,B,P ,N 〉R which is also a diagnosis w.r.t. the current DPI 〈K,B,P ∪ {Q1, Q2} ,N 〉R. Nevertheless, STATICHS continues expanding the wpHS-tree until it has verified that this is the case (Q = []). This is equivalent to finishing the construction of the noninteractive wpHS-tree that is generated by HS with parameters nmin = nmax = ∞. We want to stress that the construction of the entire wpHS-tree w.r.t. 〈K,B,P ,N 〉R and p() := pK() is inevitable in a debugging scenario where the (exact) solution to the Interactive Static KB Debugging problem is sought (the probability w.r.t. pD() of a diagnosis can only be equal to 1 if there is only a single leading diagnosis returned by STATICHS).\nIn fact, there are five further diagnoses D6, . . . ,D10 w.r.t. 〈K,B,P ,N 〉R that are detected in iteration 3 and directly dismissed (added to D×) after the validity check in line 10 of STATICHS. All other tree branches are closed due to the non-minimality (label ×(⊃Di)) or duplicate criterion (label ×(dup)). Due to σ = 0 and the associated necessity to grow the wpHS-tree until all leaf nodes are labeled, the final tree (19 labeled leaf nodes) depicted in Figure 6.3 is relatively large in comparison to the small size |K| = 7.\nThis example might already give an idea of the potential explosion of the wpHS-tree produced by STATICHS in case the (exact) solution to the Interactive Static KB Debugging problem is desired. This is why it will usually make sense in practice to specify a fault tolerance σ > 0 which enables Algorithm 5 with mode = static to escape from the generally intractable complexity of the complete investigation of all minimal diagnoses w.r.t. the input DPI (full construction of the wpHS-tree). However, in this concrete example, allowing a small fault tolerance σ has no effect either. Actually, σ ≥ 0.56 is necessary to achieve a premature termination of the tree construction. This holds due to the fact that the probability distributions of leading diagnoses are 〈pD(D1), pD(D2), pD(D3)〉 = 〈0.38, 0.37, 0.25〉 (after iteration 1) and 〈pD(D1), pD(D2), pD(D5)〉 = 〈0.44, 0.42, 0.14〉 (after iteration 2). Now, given say σ := 0.6, the stop criterion of Algorithm 5 would be met after iteration 2 because pD(Dmax) = pD(D1) = 0.44 ≥ 0.4 = 1 − 0.6 = 1 − σ. Nate that, in this case, the same (exact) solution would be returned as for the setting σ := 0. The (significant) difference is just that the final tree in this case has only 14 leaf nodes, of which only 7 are labeled (the labeling of a node is in general significantly more costly than the mere generation of a node). As opposed to this, the full tree comprises 19 labeled nodes. On the other side of the coin, choosing a value of σ > 0.5, for example, means that – from the point of view of the knowledge at the time Algorithm 5 terminates – a solution to Interactive Static KB Debugging is returned by Algorithm 5 which has a higher probability of not being the (exact) solution than of being the (exact) solution.\nAll in all, the execution of Algorithm 5 in this example performs\n• 4 full QX calls, i.e. calls of QX that actually return a minimal conflict set (there are four minimal conflict sets labeled by C in the tree in Figure 6.3) and\nCHAPTER 6. ITERATIVE DIAGNOSIS COMPUTATION 132\n• 20 validity checks, i.e. calls of QX that return ’no conflict’ (one check for each of the 10 found minimal diagnoses; notice that QX does only perform a single KB validity check by ISKBVALID in case it returns ’no conflict’, see Algorithm 1) or calls of ISKBVALID in line 10 in STATICHS (one call for each of the 10 found minimal diagnoses),\ncomputes\n• 10 minimal diagnoses w.r.t. the input DPI,\n• 4 minimal conflict sets w.r.t. the input DPI and\n• 2 queries and asks the user 2 logical formulas (1 per query)\nand stores\n• a maximum of 19 nodes (where node refers to the internal representation of a node in STATICHS as a set of edge labels along a path from the root node to a leaf node; there are even more nodes in the sense of tree nodes in the picture in Figure 6.3).\nCHAPTER 6. ITERATIVE DIAGNOSIS COMPUTATION 134\n1 © 〈1 ,2 ,5 〉C\n2 © 〈2 ,4 ,6 〉C\n5 © 〈1 ,3 ,4 〉C\n6 © 〈2 ,4 ,6 〉R\n? 3 © X\n(D 1 )\n4 © X\n(D 2 )\n? ?\n7 © 〈1 ,5 ,6 ,8 〉C\n? 8 © X\n(D 3 )\n?\n? ?\n? ?\n1\n0 .4 1 tt\n2 0 .2\n5\n5 0 .2\n5 --\n2\n0 .0\n9\n4\n0 .2\n8\n6\n0 .2\n7\n1\n0 .0\n9\n3 0 .0\n7\n4 0 .1\n8 ++\n2 0 .0\n6\n4 0 .1\n8\n6\n0 .1\n7 **\n1\n0 .0\n6\n5\n0 .0\n4\n6 0 .1\n1\n8\n0 .0\n4 ''\nIt er\nat io\nn 1\n〉\nD X ∪ D\nc a lc = ∅ ∪ {D\n1 ,D\n2 ,D\n3 } = {[ 1 ,4 ], [1 ,6 ], [5 ,4 ]}\n, Q\n= [{ 5 ,6 } ,{ 2 ,4 ,6 } ,{ 1 ,2 } ,{ 2 ,1 } ,{ 2 ,3 } ,{ 5 ,2 } ,{ 2 ,4 ,1 } ,{ 2 ,4 ,5 } ,{ 2 ,4 ,8 }]\n, D × = ∅\nC c a lc = {〈 1 ,2 ,5 〉, 〈2 ,4 ,6 〉, 〈1 ,3 ,4 〉, 〈1 ,5 ,6 ,8 〉}\n,\n〈Q 1 ,P\n(Q 1 )〉\n= 〈{ B v K } ,〈 {D\n1 ,D\n2 } ,{ D\n3 } ,∅ 〉〉\n, u (Q\n1 ) =\ntr u e\n, D\nX = {D\n1 ,D\n2 },\nD o u t =\nD × = {D\n3 }\n〉\n1 © 〈1 ,2 ,5 〉C\n2 © 〈2 ,4 ,6 〉C\n5 © 〈1 ,3 ,4 〉C\n6 © 〈2 ,4 ,6 〉R\n13 © ×\n(d u p )\n3 © X\n(D 1 )\n4 © X\n(D 2 )\n14 © X\n(D 5 )\n? 7 © 〈1 ,5 ,6 ,8 〉C\n? 8 © X\n(D 3 )\n10 © 〈1 ,3 ,4 〉R\n? ?\n? 11 © X (D 4 )\n9 © ×\n9 © X\n9 © X\n? ?\n12 © ×\n(⊃ D\n3 )\n11 © ×\n1\n0 .4 1 uu\n2 0 .2\n5\n5 0 .2\n5 --\n2\n0 .0\n9\n4\n0 .2\n8\n6\n0 .2\n7\n1\n0 .0\n9\n3 0 .0\n7\n4 0 .1\n8 ++\n2 0 .0\n6\n4 0 .1\n8\n6\n0 .1 7 ))\n1\n0 .0\n6\n5\n0 .0\n4\n8\n0 .0\n4 ''\n6 0 .1\n1\nQ 1\nQ\n1\nQ\n1\n1\n0 .0\n6 3\n0 .0\n4\n4\n0 .1\n1\nQ 1\nIt er\nat io\nn 2\n〉\nFi gu\nre 6.\n2: (E\nxa m\npl e\n6. 2)\nSo lv\nin g\nth e\npr ob\nle m\nof In\nte ra\nct iv\ne St\nat ic\nK B\nD eb\nug gi\nng (P\nro bl\nem D\nefi ni\ntio n\n5. 2)\nfo rt\nhe ex\nam pl\ne D\nPI gi\nve n\nby Ta\nbl e\n4. 2\nby m\nea ns of A lg or ith m 5 an d S TA T IC H S.\nCHAPTER 6. ITERATIVE DIAGNOSIS COMPUTATION 135\nD X ∪ D\nc a lc = {D\n1 ,D\n2 } ∪ {D\n5 } = {[ 1 ,4 ], [1 ,6 ], [2 ,1 ]}\n, Q\n= [{ 2 ,4 ,6 } ,{ 2 ,3 } ,{ 5 ,2 } ,{ 2 ,4 ,1 } ,{ 5 ,6 ,1 } ,{ 2 ,4 ,5 } ,{ 5 ,6 ,3 }]\n, D × = {D\n3 ,D\n4 } = {[ 5 ,4 ], [2 ,4 ,6 ]}\nC c a lc = {〈 1 ,2 ,5 〉, 〈2 ,4 ,6 〉, 〈1 ,3 ,4 〉, 〈1 ,5 ,6 ,8 〉}\n,\n〈Q 2 ,P\n(Q 2 )〉\n= 〈{ B v ∃r .F } ,〈 {D\n1 } ,{ D\n2 ,D\n5 } ,∅ 〉〉\n, u (Q\n2 ) =\ntr u e\n, D\nX = {D\n1 },\nD o u t = {D\n2 ,D\n5 },\nD × = {D\n3 ,D\n4 ,D\n2 ,D\n5 }\n〉\n1 © 〈1 ,2 ,5 〉C\n2 © 〈2 ,4 ,6 〉C\n5 © 〈1 ,3 ,4 〉C\n6 © 〈2 ,4 ,6 〉R\n13 © ×\n(d u p )\n3 © X\n(D 1 )\n4 © X\n(D 2 )\n14 © X\n(D 5 )\n16 © 〈1 ,5 ,6 ,8 〉R\n7 © 〈1 ,5 ,6 ,8 〉C\n17 © 〈1 ,3 ,4 〉R\n8 © X\n(D 3 )\n10 © 〈1 ,3 ,4 〉R\n18 © ×\n(⊃ D\n5 )\n20 © ×\n(⊃ D\n3 )\n21 © X\n(D 6 )\n11 © X\n(D 4 )\n9 © ×\n9 © X\n9 © X\n15 © ×\n19 © ×\n(⊃ D\n2 )\n22 © X\n(D 7 )\n12 © ×\n(⊃ D\n3 )\n28 © ×\n(⊃ D\n5 )\n24 © ×\n(⊃ D\n3 )\n25 © ×\n(⊃ D\n5 )\n26 © ×\n(d u p )\n23 © X\n(D 8 )\n27 © X\n(D 9 )\n29 © X\n(D 1 0 )\n11 © ×\n22 © ×\n15 © X\n15 © ×\n21 © ×\n23 © ×\n27 © ×\n29 © ×\n(D 1 0 )\n1\n0 .4 1 uu\n2\n0 .2\n5\n5\n0 .2\n5 //\n2\n0 .0\n9\n4\n0 .2\n8\n6\n0 .2\n7\n1\n0 .0 9 uu\n3 0 .0\n7\n4 0 .1\n8 ,,\n2\n0 .0 6 tt\n4\n0 .1\n8\n6 0 .1\n7 &&\n1\n0 .0 6 uu\n5 0 .0 4\n8\n0 .0\n4 ))\n6 0 .1\n1\nQ 1\nQ 1\nQ\n1\n1 0 .0 6 xx\n3\n0 .0\n4\n4 0 .1\n1 &&\n1\n0 .0 2 xx\n5\n0 .0\n2\n6\n0 .0\n4\n8\n0 .0\n2\n4 0 .0\n4 &&\n1 0 .0 2 xx\n3\n0 .0\n2\nQ 1\nQ 2\nQ 2\nQ\n2\nQ 1\nQ 2\nQ 1\nQ 1 Q 1\nIt er\nat io\nn 3\n〉\nD X ∪ D\nc a lc = {D\n1 } ∪ ∅ = {D\n1 },\nQ =\n[] ,\nC c a lc = {〈 1 ,2 ,5 〉, 〈2 ,4 ,6 〉, 〈1 ,3 ,4 〉, 〈1 ,5 ,6 ,8 〉}\n,\nD × = {D\n3 ,D\n4 ,D\n2 ,D\n5 ,D\n6 ,D\n7 ,D\n8 ,D\n9 ,D\n1 0 } = {[ 5 ,4 ], [2 ,4 ,6 ], [1 ,6 ], [2 ,1 ], [2 ,4 ,8 ], [5 ,6 ,3 ], [2 ,3 ,6 ], [2 ,3 ,8 ], [5 ,2 ,3 ]}\n,\np D (D\n1 ) =\n1 ⇒\nre tu\nrn th\ne so\nlu tio\nn K\nB (K \\ D\n1 ) ∪ p 1\n(p 1 :c\nf. Ta\nbl e\n4. 2)\nFi gu\nre 6.\n3: (E\nxa m\npl e\n6. 2\nco nt\nin ue\nd) So\nlv in\ng th\ne pr\nob le\nm of\nIn te\nra ct\niv e\nSt at\nic K\nB D\neb ug\ngi ng\n(P ro\nbl em\nD efi\nni tio\nn 5.\n2) fo\nrt he\nex am\npl e\nD PI\ngi ve\nn by\nTa bl\ne 4.\n2 by\nm ea\nns of\nA lg\nor ith\nm 5\nan d\nS TA\nT IC\nH S.\nCHAPTER 6. ITERATIVE DIAGNOSIS COMPUTATION 136\nAlgorithm 7 Iterative Construction of a Static Hitting Set Tree Input: a tuple 〈〈K,B,P ,N 〉R,Q, t, nmin, nmax,Ccalc,DX,D×, p(),P ′,N ′〉 consisting of\n• the DPI 〈K,B,P ,N 〉R given as input to Algorithm 5, • the overall sets of positively (P ′) and negatively (N ′) answered queries added as test cases to 〈K,B,P ,N 〉R so far, • the current queue Q of open (non-labeled) nodes of a (partial) wpHS-tree, • some desired computation timeout t, • a desired minimal (nmin ≥ 2) and maximal (nmax) number of minimal diagnoses to be returned, • the set Ccalc of all minimal conflict sets w.r.t. 〈K,B,P ,N 〉R computed so far, • the set DX of all minimal diagnoses w.r.t. 〈K,B,P ,N 〉R computed so far that satisfy all test cases P ′ and N ′, • the set D× of all minimal diagnoses w.r.t. 〈K,B,P ,N 〉R computed so far that do not satisfy all test cases P ′ and N ′. • a function p : K → (0, 0.5).\nOutput: a tuple 〈D,Q,Ccalc,D×〉 where • D is the current set of leading diagnoses such that\n(a) D ⊆mD〈K,B,P,N〉R ∩mD〈K,B,P∪P′,N∪N ′〉R is the set of most probable minimal diagnoses w.r.t. 〈K,B,P ,N 〉R that satisfy all test cases P ′ and N ′ such that (i) nmin ≤ |D| ≤ nmax and\n(ii) D ⊃ DX, if such a set D exists, or\n(b) D is equal to the set of all minimal diagnoses mD〈K,B,P,N〉R ∩mD〈K,B,P∪P′,N∪N ′〉R , otherwise, where “most-probable” refers to the probability measure pnodes() (cf. Definition 4.9) obtained from the given function p();\n• Q is the current queue of open (non-labeled) nodes of the produced (partial) wpHS-tree, • Ccalc is the set of all minimal conflict sets w.r.t. 〈K,B,P ,N 〉R computed so far and • D× comprises those minimal diagnoses w.r.t. 〈K,B,P ,N 〉R computed so far that do not satisfy all test cases P ′ and N ′.\n1: procedure STATICHS(〈K,B,P ,N 〉R,Q, t, nmin, nmax,Ccalc,DX,D×, p(),P ′,N ′) 2: tstart ← GETTIME() 3: Dcalc ← ∅ 4: repeat 5: node← GETFIRST(Q) 6: Q← DELETEFIRST(Q) 7: 〈L,C〉 ← SLABEL(〈K,B,P ,N 〉R, node,Ccalc,D× ∪DX ∪Dcalc,Q) 8: Ccalc ← C 9: if L = valid then . node is minimal diagnosis w.r.t. 〈K,B,P ,N 〉R 10: if ISKBVALID(K \\ node, 〈·,B,P ∪ P ′,N ∪N ′〉R) then . ISKBVALID (see Algorithm 1) 11: Dcalc ← Dcalc ∪ {node} . node does satisfy all test cases P ′ and N ′ 12: else 13: D× ← D× ∪ {node} . node does not satisfy all test cases P ′ and N ′ 14: else if L = closed then . do nothing, no need to store non-minimal diagnoses 15: else . L must be a minimal conflict set 16: for e ∈ L do 17: Q← INSERTSORTED(node ∪ {e} ,Q, pnodes(), descending) 18: until Q = [] ∨ [|Dcalc| 6= ∅ ∧ |Dcalc ∪DX| ≥ nmin ∧ (|Dcalc ∪DX| = nmax ∨ GETTIME()− tstart > t)] 19: return 〈Dcalc ∪DX,Q,Ccalc,D×〉\n20: procedure SLABEL(〈K,B,P ,N 〉R, node,Ccalc,D(×,X,calc),Q) 21: for nd ∈ D(×,X,calc) do 22: if node ⊇ nd then . node is a non-minimal diagnosis 23: return 〈closed,Ccalc〉 24: for nd ∈ Q do 25: if node = nd then . node is a duplicate node 26: return 〈closed,Ccalc〉 27: for C ∈ Ccalc do 28: if C ∩ node = ∅ then . the minimal conflict set C can be reused to label node 29: return 〈C,Ccalc〉 30: L← QX(〈K \\ node,B,P ,N 〉R) . see Algorithm 1 (page 39) 31: if L = ’no conflict’ then . node is a diagnosis 32: return 〈valid,Ccalc〉 33: else . L is a new minimal conflict set (/∈ Ccalc) 34: Ccalc ← Ccalc ∪ {L} 35: return 〈L,Ccalc〉\nCHAPTER 6. ITERATIVE DIAGNOSIS COMPUTATION 137"
    }, {
      "heading" : "6.2 DYNAMICHS: A Dynamic Iterative Diagnosis Computation Algorithm",
      "text" : "As the name already suggests, DYNAMICHS (Algorithm 8) is a procedure that solves the problem of Interactive Dynamic KB Debugging defined by Problem Definition 5.1 if used for leading diagnosis computation in Algorithm 5. DYNAMICHS is sound, complete and optimal w.r.t. the set of solutions of the Interactive Dynamic KB Debugging problem. Optimality refers to the best-first computation of minimal diagnoses regarding a given probability measure."
    }, {
      "heading" : "6.2.1 Overview and Intuition",
      "text" : "Synoptic View of the Algorithm. DYNAMICHS (Algorithm 8) is employed as a subroutine in Algorithm 5 with mode = dynamic to build up a hitting set tree iteratively. That is, each time DYNAMICHS is called in Algorithm 5, it expands the existing tree only to a sufficient extent in order to determine a desired number of new leading diagnoses used for the generation of the next query. Then, the leading diagnoses set is returned.\nOutside of the DYNAMICHS method in Algorithm 5, a new diagnosis probability distribution is obtained by the diagnosis probability update (cf. Section 5.3.2). Once this distribution involves one diagnosis, the probability of which exceeds a predefined threshold 1 − σ, the algorithm terminates. The output is a solution KB w.r.t. the current DPI built from this highly probable minimal diagnosis.\nRemark 6.3 In case σ has a predefined value of zero, the output is the (exact) solution to the problem of Interactive Dynamic KB Debugging for the input DPI. In a scenario where some fault tolerance σ > 0 is given, the solution KB returned by Algorithm 5 is an approximation of the (exact) solution to Interactive Dynamic KB Debugging for the input DPI where a better approximation can be expected for smaller values of σ (cf. Remark 5.12). “Better” in this context refers to the satisfaction of desired semantic properties of the KB returned by Algorithm 5, i.e. desired entailments and desired non-entailments of the KB. The intuition is that specification of additional test cases T guarantees the output of a KB complying with these test cases, whereas accepting one – albeit highly probable – of multiple solution KBs without having incorporated T leaves open the possibility for this KB to not fulfill T .\nHowever, answering queries is effort for an interacting user. Therefore, the approach that involves the “early” termination of the algorithm after a solution KB has a sufficiently high probability (lower than 1) constitutes a trade-off between exactness of the output and the effort of the user and overall execution time of the interactive KB debugging algorithm, respectively.\nIn case there is no highly probable leading diagnosis, a query constructed from the current set of leading diagnoses is asked to the user. The user’s answer is incorporated into the current DPI resulting in a new DPI. Thereafter, DYNAMICHS is invoked again given this new DPI as an argument.\nStorage of the Tree. Between each two calls of DYNAMICHS in Algorithm 5, the “state” of the current hitting set tree is stored by variables\n• Dcalc – computed minimal diagnoses w.r.t. the current DPI,\n• Q – the list of open, non-labeled nodes,\n• Ccalc – (not necessarily minimal) conflict sets w.r.t. the current DPI computed so far,\n• D⊃ – non-minimal diagnoses w.r.t. the current DPI computed so far,\n• Qdup – non-labeled duplicate nodes (i.e. nodes corresponding to tree branches with the same set of edge labels as branches that are already present in the tree)\nCHAPTER 6. ITERATIVE DIAGNOSIS COMPUTATION 138\n• D× – the empty set (is filled up during Algorithm 5 between two calls of DYNAMICHS with diagnoses from Dcalc that have been invalidated by an answered query)\nwhere nodes in the tree again store (among others) the edge labels on the path from the root node to themselves.\nTree Update. It is immediately apparent from the enumeration given above that, in comparison to STATICHS, additional collections, i.e. D⊃ as well as Qdup, need to be maintained in order to “remember” the current tree while Algorithm 5 is processing outside of the method DYNAMICHS. The cause for these additional variables is the tree update necessary after each addition of a test case to a DPI. For, each iteration of DYNAMICHS considers a different DPI in terms of the test cases. And, any two different DPIs in general lead to a different hitting set tree and to different sets of minimal diagnoses and conflict sets. Hence, the idea of the tree update is the following: Reuse the partial hitting set tree T (stored by the variables described above) constructed before the new test case was added to the current DPI DPIj and perform suitable modifications to T in order to obtain a tree T ′ such that the further expansion of T ′ allows to identify all minimal diagnoses w.r.t. the new DPI DPIj+1 resulting from the addition of the new test case to DPIj . In other words, the tree update seeks to establish a tree that is equivalent to one built by execution of DYNAMICHS using the new DPI DPIj+1 starting from an empty tree.\nNode Storage. Notice that, unlike in STATICHS or HS, it is crucial to store nodes not as sets in DYNAMICHS, but as ordered lists of formulas. That is, each node nd stores a list of all the edge labels along the (directed) path in the hitting set tree from the root node to nd where the order of formulas in the list is given by the order of traversing the edge labels along this path. Additionally, DYNAMICHS stores the attribute nd.cs for each node nd which is an ordered list including the node labels, i.e. the conflict sets, along the path from the root node to nd in analogous way. Associating a node with these two lists instead of one set is necessary from the point of view of the tree update. Because this facilitates the differentiation between two nodes corresponding to an equal (partial) diagnosis. For example, there could be some node nd1 that is “redundant” after some query Q has been answered, but there is a set-equal node nd2 which is still “relevant” (set-equality refers to equal sets, not lists, of edge labels stored by two nodes). In this case, the algorithm should get rid of nd1 (in order to save time and space) while preserving node nd2 (in order to maintain completeness). Associating set-equal nodes with each other might thus either lead to unnecessary tree expansion steps (if none is deleted) or incompleteness of the algorithm concerning the consideration of all minimal diagnoses (in case both are deleted).\nAddition of a Test Case Changes Set of Solutions. Unlike the STATICHS algorithm, which is strongly related to the non-interactive hitting set algorithm HS (Algorithm 2) as outlined in Section 6.1.1, the hitting set tree produced by DYNAMICHS will usually differ significantly from the non-interactive hitting set tree produced by HS. The reason for this is that in DYNAMICHS the initial DPI DPI0 is not fixed (in that conflict sets and diagnoses are calculated only w.r.t. DPI0), but new test cases are also used for the computation of minimal conflict sets (and thus minimal diagnoses) and not only for the invalidation of diagnoses. Hence, every time a query has been answered and a respective test case has been incorporated into the DPI, the minimal conflict sets computed for the old DPIDPIj might not be minimal conflict sets w.r.t. the current DPI DPIj+1 anymore (see Examples 6.3 and 6.4). On the one hand, a minimal conflict set C w.r.t.DPIj might be a non-minimal conflict set w.r.t.DPIj+1 (since there is a new minimal conflict set C′ ⊂ C w.r.t. DPIj+1). On the other hand, there might be also “completely new” minimal conflict sets Ck w.r.t. DPIj+1 which are in no set-relationship with any minimal conflict set w.r.t. DPIj .\nDue to this changing set of minimal conflict sets, the set of minimal diagnoses is variable as well (cf. Proposition 4.6). To see this, let D be a minimal diagnosis w.r.t. DPIj . Then D hits all minimal conflict sets Ck in mCDPIj . Now, assume that D comprises (only) the element ax from Ck, but there\nCHAPTER 6. ITERATIVE DIAGNOSIS COMPUTATION 139\nis a minimal conflict set C′k in mCDPIj+1 such that C′k ⊆ Ck \\ {ax}. In this case, D is not a (minimal) hitting set of all minimal conflict sets in mCDPIj+1 (since D does not hit C′k), i.e. D is not a (minimal) diagnosis w.r.t. DPIj+1. That means, D needs to be extended (by a hitting set of all minimal conflict sets in mCDPIj+1 it does not hit) in order to become a diagnosis w.r.t. DPIj+1. After extending D, both situations might arise, either thatD is a minimal diagnosis w.r.t.DPIj+1 or thatD is a non-minimal diagnosis w.r.t. DPIj+1. When the latter case occurs, DYNAMICHS might often be able to figure out that (the tree branch corresponding to) D is simply redundant (w.r.t. the new DPI DPIj+1) and does not need to be considered during the further expansion of the hitting set tree (which searches for minimal diagnoses w.r.t. DPIj+1 and not w.r.t. DPIj). That is, such redundant tree branches are unnecessary in order to explore all minimal diagnoses w.r.t. DPIj+1.\nAs a consequence, the nice property of STATICHS that the set of minimal diagnoses that needs to be taken into account given DPIj+1 is a proper subset of the minimal diagnoses set that needed to be considered given DPIj in no longer valid for DYNAMICHS. That is, the set of remaining solution candidates in DYNAMICHS is not guaranteed to “converge” constantly towards a singleton comprising only one solution. The DPI, the minimal conflict sets as well as the minimal diagnoses are “dynamic”. What holds for both DYNAMICHS and STATICHS is the guarantee that the set of all (i.e. minimal and non-minimal) diagnoses is constantly shrinking, i.e. aDDPIj ⊃ aDDPIj+1 .\nTree Pruning. Let T be the hitting set tree produced in the j-th iteration of DYNAMICHS (i.e. T is the tree that was used to search for minimal diagnoses w.r.t. DPIj). Then, after a new test case has been added to DPIj , there are often redundant subtrees in T that can be pruned. The resulting tree T ′ can then be used in the (j + 1)-th iteration of DYNAMICHS to identify minimal diagnoses w.r.t. the new DPI DPIj+1. Using T instead of T ′ might lead to a significant time and (more severely) space overhead, due to the unnecessary expansion of redundant branches that are known to give no new information at all. Another approach could be to simply discard the entire tree T and start to construct a new one w.r.t. DPIj+1 from scratch. This strategy, however, will usually also suffer from a non-negligible time overhead since most of the tree T can be safely reused in iteration j+1 and only parts of it must be revised. In particular, this strategy would potentially involve many additional calls of QX (which internally calls an expensive reasoner) as, in the worst case (when no pruning is possible), the entire existing tree might be rebuilt.\nAs Remark 6.7 and Examples 6.3 as well as 6.4 shall indicate, the overhead in terms of (expensive) calls to a reasoner (i.e. calls of QX) due to tree pruning (compared to its impact on the tree) seems absolutely reasonable. In fact, only one call of a “fast version” of QX might already lead to the deletion of 75% of the tree branches as one can see in the first pruning step in Example 6.4.\nThe evolution of the hitting set tree produced by Algorithm 5 using DYNAMICHS is thus characterized by alternating expansion and pruning phases. Also for very complex problems, in case that expansion phases are “short enough” such that tree pruning can take place “often enough”, one might be able to keep the hitting set tree “small enough” to handle it efficiently. The extent of the expansion phase can be steered by the specification of the leading diagnosis parameters nmin, nmax and t (cf. Section 5.3.2). In the extreme case, these can be defined in a way (nmin = nmax = 2) the algorithm will allow only the computation of a single further minimal diagnosis (in the first expansion phase: two diagnoses) before DYNAMICHS (i.e. the tree expansion phase) terminates and a further pruning phase might take place.\nHowever, it is not automatically warranted that tree pruning is possible after each expansion phase. Similarly, no certainty is given that the transition from DPIj to DPIj+1 just causes the deletion of parts of the tree and no additional expansion of the tree. In fact, this depends on certain properties of the test case that is added after an expansion phase (i.e. properties of the generated query).\nTest Cases Affect Tree Pruning. Some added test case might give rise to some pruning steps as well as it might induce the construction of new subtrees (where “new” means that these would be no subtress of a\nCHAPTER 6. ITERATIVE DIAGNOSIS COMPUTATION 140\nhitting set tree w.r.t. the previous DPIDPIj). The latter situation occurs when “completely new” minimal conflict sets (see above) are introduced by the addition of a test case. If this is the only impact of a test case, then this test case has only a negative influence on the time and space complexity. In other words, none of the invalidated minimal diagnoses (and no other nodes in the tree) are redundant; but all of them must additionally hit the set of “completely new” minimal conflict sets (in order to become diagnoses w.r.t. DPIj+1). Hence, in this case, the transition from DPIj to DPIj+1 results only in monotonic growth of the tree. If possible, such “negative-impact test cases” must be avoided. On the other hand, one must strive for the usage of “positive-impact test cases”, i.e. those that only trigger tree pruning, but no tree expansion. Defining and studying properties that constitute such “positive-impact test cases” and developing specialized algorithms for extracting exactly those types of queries that enable as substantial and effective pruning as possible is a topic of future research.\nAn idea pertinent to this issue could for example be to attempt to extract a query by means of the conflict set C that labels the root node of the tree. More concretely, if any answer to a query yields a new test case that leads to the introduction of a minimal conflict set that is a proper subset of C, then it is for sure that significant pruning can take place (since entire subtrees starting from the root of the tree can be deleted). For instance, the first query Q1 in Example 6.4 features this property. Roughly, the reasons for that are that Q1 is an entailment of a proper subset Csub of C (i.e. Csub is a justification of Q1, cf. Section 4.1) and Q1 is “relevant” for this conflict set C to be a conflict set. In other words, the latter means that Q1 can be used to “replace” the part Csub of C, i.e. (C \\ Csub) ∪Q1 is invalid w.r.t. the given DPI. That is, addition of Q1 to the positive test cases asserts the correctness of one part of C, namely Csub (cf. Example 6.4), wherefore the other part must be incorrect (because some part of a conflict set must be definitely incorrect). On the other hand, assignment of Q1 to the negative test cases asserts exactly the incorrectness of Csub wherefore the formulas C \\ Csub become obsolete in the minimal conflict set C yielding the new minimal conflict set C′ := Csub. Another desirable property of Q1 is that addition of Q1 to either set of test cases does not imply the origination of any “completely new” conflict sets (see above) which result in additional growth of the tree.\nThat is, in its original form (without assuring only the usage of “positive-impact test cases”), the time and space complexity of DYNAMICHS is a function of the generated queries. There is a potential to perform significant pruning, but also the risk of significant tree growth. In case mostly “positiveimpact queries” are generated and asked to the user, the performance might be very nice and significantly superior to the one of STATICHS. In the reverse case, the performance might be also worse than the one of STATICHS. In the case of STATICHS, there is no chance for significant pruning, but also no chance for a tree growth that goes beyond the size of the non-interactive tree produced by HS.\nIn STATICHS, there are only expansion phases (in case the tree pruning described by Definition 4.8 is considered part of an expansion phase) which means that the tree constructed by STATICHS will constantly grow (apart from the deleted duplicate nodes and non-minimal diagnoses). All the user can do is hope that Algorithm 5 applying STATICHS will not run out of memory (cf. Section 6.1.1).\nThe idea is now to be able to use DYNAMICHS instead of STATICHS particularly if the latter runs out of memory soon. If the leading diagnosis parameters are specified small enough to prevent the hitting set tree produced during one expansion phase from becoming too large and test cases are not chosen unfavorably, the DYNAMICHS method should be able to outperform STATICHS significantly, as Examples 6.2 and 6.4 suggest."
    }, {
      "heading" : "6.2.2 Algorithm Walkthrough",
      "text" : "Input Parameters. When DYNAMICHS (Algorithm 8) is called for the first time in Algorithm 5, the inputs Ccalc, DX, D×, P ′ and N ′ correspond to the empty set and Q = [∅] (cf. lines 1-4 and 10 in Algorithm 5). Further on, Dcalc is defined to be the empty set at the beginning of each execution of DYNAMICHS. That is, DYNAMICHS starts the construction of the hitting set tree from an initial tree consist-\nCHAPTER 6. ITERATIVE DIAGNOSIS COMPUTATION 141\ning of a single unlabeled root node ∅ (∈ Q). And, all collections that are later returned by DYNAMICHS in line 25, except for Q, are initially empty. Further input arguments are the DPI 〈K,B,P ,N 〉R provided as an input to Algorithm 5, the sets of positively (P ′) and negatively (N ′) answered queries since the start of Algorithm 5 (both sets initially empty), the leading diagnosis computation parameters nmin, nmax, t (see description in Section 5.1 on page 78) and the probability measure p() := pK() that assigns a probability in the interval (0, 0.5) to each formula in K (see line 5 in Algorithm 5).\nTree Update during First Iteration of DYNAMICHS. Before the repeat-loop in DYNAMICHS is entered, the UPDATETREE function is called (line 4), but has no effect. This holds since UPDATETREE first iterates over all elements in D×, then over all elements in D⊃ and finally over all elements in DX where D× = D⊃ = DX = ∅, as pointed out before.\nThe Main Loop. During the repeat-loop, in each iteration the first node node in the queue Q of open (non-labeled) nodes is processed (GETFIRST, line 6). Notice that, anywhere throughout DYNAMICHS, nodes are added to Q in a way that a sorting of Q in descending order according to pnodes() (cf. Definition 4.9) is maintained (cf. INSERTSORTED in lines 17, 68, 77, 80, 100 and 103). Hence, the most probable node (according to pnodes()) is always processed next.\nSo, when node is processed, it is first deleted from Q (DELETEFIRST, line 7). Then a test is performed whether node ∈ DX, i.e. whether node is already known to be a minimal diagnosis w.r.t. the current DPI 〈K,B,P ∪ P ′,N ∪ N ′〉R. In case this test is positive, node is directly added to Dcalc, the set of leading diagnoses that will be output by the current call of DYNAMICHS. Otherwise, the DLABEL function is called given node (i.a.) as a parameter (line 11).\nComputation of a Node Label. The DLABEL function processes node as follows. First, the nonminimality criterion (lines 27-29) is checked. That is, among all nodes in Dcalc, one is searched which is a proper subset of node. If such a node nd is found, then node must be a non-minimal diagnosis w.r.t. the current DPI since, anytime throughout the execution of DYNAMICHS, Dcalc contains only minimal diagnoses w.r.t. the current DPI 〈K,B,P ∪ P ′,N ∪ N ′〉R. In this case, unlike in STATICHS, the branch in the hitting set tree corresponding to node cannot be simply discarded, but needs to be still stored (in the set D⊃). It is necessary to store non-minimal diagnoses as these might become minimal diagnoses w.r.t. the new DPI obtained after the subsequent addition of a new test case to the current DPI.\nIn case the non-minimality criterion is not satisfied, the reuse criterion (lines 30-40) is checked next. That is, the set Ccalc containing (not necessarily minimal) conflict sets w.r.t. the current DPI is browsed for a set C such that C and node are disjoint sets. If such a set C is found, there must be some set X ⊆ C which is a minimal conflict set w.r.t. the current DPI. This minimal conflict set X can then be used to label node since the set of edge labels along the path in the tree leading from the root node to node does not hit X (because it does not hit C).\nThe minimality of C is verified by a call of QX(〈C,B,P ∪ P ′,N ∪ N ′〉R) that yields X , a minimal conflict set w.r.t. the current DPI. In case X ⊂ C (line 33), before X is returned as a label for node, the following tree pruning steps are performed:\n• All the conflict sets Ci used as node labels in the hitting set tree or in duplicate tree branches so far (i.e. Ci ∈ nd.cs for a node nd ∈ Q∪D⊃∪Qdup) such thatX ⊂ Ci are replaced byX (PRUNEQDUP and PRUNE in lines 36-38),\n• any subtree is pruned if its root node is linked to a node now labeled byX (replacing some Ci ⊃ X) by an edge with label ax where ax is in Ci \\X (PRUNEQDUP and PRUNE in lines 36-38) and\nCHAPTER 6. ITERATIVE DIAGNOSIS COMPUTATION 142\n• for each pruned node nd, if there is a non-pruned node in Qdup suited to construct a node nd′ that can replace nd, nd′ is added to the collection of nodes from which nd was deleted (PRUNEQDUP and PRUNE in lines 36-38),\n• all the conflict sets Ci ∈ Ccalc that are proper supersets of X are deleted from Ccalc and X is added to Ccalc (ADDSETDELSUPSETS in line 39).\nOtherwise, C (= X) is directly returned by DLABEL without performing any tree pruning because the reused conflict set C is (still) a minimal conflict set w.r.t. the current DPI 〈K,B,P ∪P ′,N ∪N ′〉R (notice that each element of Ccalc was added to Ccalc as a minimal conflict set w.r.t. some DPI 〈K,B,P∪P ′′,N∪ N ′′〉R where P ′′ ⊆ P ′ and N ′′ ⊆ N ′ during the execution of this or a previous call of DYNAMICHS).\nRemark 6.4 During the execution of the first call of DYNAMICHS in Algorithm 5, no tree pruning can take place (neither within the scope of DLABEL nor anywhere else) since all elements of Ccalc (initially the empty set) must be minimal conflict sets w.r.t. the input DPI which is at the same time the current DPI. Pruning of the hitting set tree is only possible in case some non-leaf nodes of the tree are labeled by conflict sets that are not minimal w.r.t. the current DPI.\nGiven that the reuse criterion fails, QX is called given the current DPI 〈K\\node,B,P∪P ′,N ∪N ′〉R as an argument (line 41). If the output L is equal to ’no conflict’, then we know by Proposition 4.9 that node is a diagnosis w.r.t. the current DPI, wherefore the label valid is returned for node. Otherwise, the output L must be a minimal conflict set w.r.t. 〈K,B,P ∪P ′,N ∪N ′〉R that has an empty set-intersection with node. Since the reuse criterion failed, i.e. there is no set in Ccalc that does not intersect with node, L must be a fresh minimal conflict set w.r.t. 〈K,B,P ∪ P ′,N ∪ N ′〉R in the sense that L /∈ Ccalc must hold. Therefore the label L is first added to Ccalc and then returned by DLABEL as a label for node.\nRemark 6.5 Please notice that this call of QX to label a node is one of the key differences between STATICHS and DYNAMICHS. Whereas the former uses QX exclusively for the computation of minimal conflict sets w.r.t. the (static) input DPI exploiting just the initial sets of positive and negative test cases P and N , respectively, the latter employs QX to compute minimal conflict sets w.r.t. the (dynamic) current DPI which includes all new test cases (P ′ and N ′) resulting from answered queries in the ongoing interactive debugging session so far.\nProcessing of a Node Label. Back in the main procedure, the label L returned by the DLABEL function is processed as follows. If L = valid, then it is a fact that node is a minimal diagnosis w.r.t. the current DPI wherefore node is added to the set Dcalc. Otherwise, if nonmin is the returned label for node, node is added to the set D⊃ of non-minimal diagnoses w.r.t. the current DPI. Otherwise, i.e. if L /∈ {valid, nonmin}, then Lmust be a minimal conflict set w.r.t. the current DPI (see the description of node label computation above). In this case, |L| successor nodes of node are generated (lines 18 and 19). For each logical formula e ∈ L, a new node is computed from node (and node.cs) as nodee := ADD(node, e) and nodee.cs := ADD(node.cs, L) which means that e is appended to the end of the list node and L is appended to the end of the list node.cs.\nIf there is already a node nd ∈ Q such that nd = nodee (line 20), where ’=’ applied to these lists means that the list nd interpreted as a set is equal to the list nodee interpreted as a set, then there is already a branch in the existing tree which includes the same set of edge labels as the new node nodee. Note that the tree branch corresponding to nd will differ from the one corresponding to nodee in terms of the order of edge labels or (the order of) the node labels visited when traversed starting from the root node. As it makes no sense to expand two branches with equal sets of edge labels in a hitting set tree (cf. rule 6 in Definition 4.8) for time and space complexity reasons and the fact that the sought diagnoses are sets – and not lists – of edge labels in the tree, such a duplicate node nodee is stored in the separate list Qdup. This list Qdup is always kept sorted by ascending node-cardinality (INSERTSORTED in line 21).\nCHAPTER 6. ITERATIVE DIAGNOSIS COMPUTATION 143\nThe purpose of storing and not deleting such nodes is the possibility that the now “active” branch nd might be pruned after the addition of some test case whereas nodee might be unaffected by that pruning step. In this case, nodee, given it meets certain properties (see Algorithm 10), can be reactivated and incorporated into the tree in order to replace nd. Had nodee just been discarded instead of being stored, the completeness of Algorithm 5 with mode = dynamic would be violated in general. That is, we would not have any guarantee that all minimal diagnoses w.r.t. the current DPI are actually explored by the algorithm.\nOtherwise, if there is no node in Q that is set-equal to nodee, then nodee is added to the k-th position in Q (INSERTSORTED in line 23) if there are (exactly) k − 1 nodes in Q that have a probability as per pnodes() that is greater than or equal to pnodes(nodee).\nStop Criterion. The repeat-loop of DYNAMICHS is executed until the stop criterion in line 24 is satisfied. The first criterion causing DYNAMICHS to terminate is Q = [] which means that the complete hitting set tree has been constructed and no further nodes can be labeled. In this case, Dcalc comprises all minimal diagnoses w.r.t. the current DPI 〈K,B,P ∪ P ′,N ∪N ′〉R.\nIf the first criterion is not met, then the second criterion is checked. That is, a test is performed which checks first whether there is at least one new diagnosis w.r.t. the current DPI in Dcalc which was not returned by the last-but-one call of DYNAMICHS (i.e. which is not an element of DX). Notice that this criterion or Q = [] will be definitely met after finite execution time of DYNAMICHS since either new nodes in Q will be processed (and labeled) until there is some new diagnosis w.r.t. the current DPI identified or the Q will become empty.\nAdditionally, the second criterion involves a test that checks whether the cardinality of Dcalc amounts to at least nmin and either |Dcalc| = nmax or more than t time has passed since the start of the execution of DYNAMICHS. In the latter case, nmin ≤ |Dcalc| < nmax holds. In the former case, |Dcalc| = nmax is satisfied.\nProcessing of the Leading Diagnoses Returned by DYNAMICHS. When a call of DYNAMICHS in Algorithm 5 returns 〈Dcalc,Q,Ccalc,D×,D⊃,Qdup〉, the set Dcalc is stored in the variable DX in Algorithm 5. Between two successive calls of DYNAMICHS in Algorithm 5, only this set DX as well as D× are modified. The collections Q, Ccalc, D⊃ as well as Qdup remain unchanged until they are used as input parameters when it comes to the next call of DYNAMICHS in Algorithm 5.\nIn case one diagnosis Dmax of the current leading diagnoses in DX has a probability greater than or equal to 1−σ as per the probability measure pD() (see Section 5.3.2), the stop criterion of interactive KB debugging is met and the solution KB (K\\Dmax)∪UP∪P ′ w.r.t. the current DPI 〈K,B,P∪P ′,N ∪N ′〉R is returned to the user (GETSOLKB in line 14, cf. Section 5.3.2). Thereafter, Algorithm 5 terminates and no more calls of DYNAMICHS take place.\nOtherwise, if no leading diagnosis satisfies the stop criterion, a query Q together with its q-partition P(Q) is computed as has been detailed in Sections 5.2 and 5.3.2. An answer u(Q) to this query is submitted by the interacting user (line 17 in Algorithm 5). Then u(Q) along with P(Q) is exploited to figure out the subset Dout of DX that does not comply with u(Q). This set Dout is then deleted from DX and added to D×. Additionally, Q is added to the positive test cases P ′ if u(Q) = true and to the negative test cases N ′ otherwise. Subsequently, DYNAMICHS is called again given\n• the updated parameters DX, D×, P ′ and N ′ (which are modified within and outside of DYNAMICHS during the execution of Algorithm 5),\n• the unchanged parameters Q, Ccalc, D⊃ and Qdup (which are modified only within DYNAMICHS during the execution of Algorithm 5) and\nCHAPTER 6. ITERATIVE DIAGNOSIS COMPUTATION 144\n• the constant parameters 〈K,B,P ,N 〉R, t, nmin, nmax and pK() (which are not modified within or outside of DYNAMICHS during the execution of Algorithm 5).\nThe execution of this next and any subsequent call to DYNAMICHS runs in analogue way as described so far, except for the effect of the UPDATETREE function called at the very beginning of each execution of DYNAMICHS (recall that the execution of UPDATETREE had no effect during the first execution of DYNAMICHS). We shall now explicate how this function works in all other executions of DYNAMICHS, except for the first one.\nTree Update. Between line 48 and line 69, UPDATETREE goes through all nodes nd ∈ D× (recall that D× includes exactly these diagnoses that have been ruled out by the most recently answered query) and first performs the Quick Redundancy Check (QRC, lines 50-54) for nd. If the QRC is not successful, it additionally performs the Complete Redundancy Check (CRC, lines 56-60) for nd.\nThe QRC aims at identifying whether nd is redundant and can be pruned, i.e. it attempts to find a witness of redundancy of nd. Informally, a redundant node in (redundant subtree of) the tree is a node (subtree) such that the further expansion of the current tree without this node (subtree) still yields to the detection of all minimal diagnoses w.r.t. the current DPI. A witness of redundancy of nd is a minimal conflict set C′ w.r.t. the current DPI such that a superset C ⊃ C′ was used as a node label on the tree path nd represents (that is, there is some i ≤ |nd.cs| such that C is the i-th element of nd.cs, i.e. C = nd.cs[i]) and the label (nd[i]) of the outgoing edge of C on the path represented by nd is an element not in C′ (that is, an element in C \\ C′).\nTo this end, the QRC involves the call of QX(〈Und.cs \\ nd,B,P ∪ P ′,N ∪N ′〉R) which returns X . If X is a set (and not ’no conflict’), then X is a minimal conflict set w.r.t. the current DPI 〈K,B,P ∪ P ′,N ∪N ′〉R (as Und.cs \\ nd ⊆ K, cf. Proposition 4.9). To check if X is in fact a witness of redundancy of nd, X ⊂ C (line 52) is tested for all C ∈ nd.cs. If such a C is located, X is a witness of redundancy of nd and the QRC is successful (expressed by quickRC ← true in line 53). In this case, the execution is resumed at line 61.\nThe QRC bears its name due to the fact that it requires at most one call of QX (which internally performs expensive calls to a reasoner). Moreover, it passes to QX a (DPI including a) KB of a size that is generally significantly smaller than |K| where |K| is roughly the size of the KB used in the (more expensive) calls of QX made in the DLABEL function. Hence, the QRC will be usually very fast (cf. Proposition 4.8).\nOtherwise, since the negative outcome of the QRC (which is sound, but not complete w.r.t. the finding of a witness of redundancy of nd) does not imply the non-existence of a witness of redundancy of nd, the CRC must be performed. As the name already suggests, the CRC is sound and complete and will therefore be positive and yield a witness of redundancy if and only if there is some. The CRC involves multiple calls of QX(〈nd.cs[i] \\ {nd[i]} ,B,P ∪ P ′,N ∪N ′〉R), one for each conflict set nd.cs[i] in nd.cs. It is straightforward from the characterization of a witness of redundancy given before that, given the CRC returns a set X , X is a witness of redundancy of nd.\nIf nd is non-redundant, there cannot be any witness of redundancy of nd. Hence, the complete and sound method CRC will not find such a one. Therefore, quickRC = false and completeRC = false must hold in line 61. In this case, the for-loop in line 48 continues with the next node in D×.\nOn the other hand, if nd is redundant, due to the completeness of CRC, either quickRC = true or completeRC = true must hold when it comes to the execution of the if-statement in line 61. At this point, it is guaranteed that the variable X stores a witness of redundancy of nd.\nThe CRC, contrary to the QRC, generally requires multiple (at most |nd|) calls of QX (which internally performs expensive calls to a reasoner). But, like the QRC, it passes to QX a (DPI including a) KB of a size that is generally significantly smaller than |K|. Furthermore, at most one call of QX will involve more than one call of ISKBVALID (see Algorithm 1), i.e. the function that calls the reasoner. This must\nCHAPTER 6. ITERATIVE DIAGNOSIS COMPUTATION 145\nbe true since CRC only requires an additional call of QX if a witness of redundancy has not yet been found. And, each call of QX that does not find a witness of redundancy of nd returns ’no conflict’ which necessitates only a single invocation of ISKBVALID. Hence, each execution of the CRC will be very fast in general as well (cf. Proposition 4.8).\nWhat comes next is the pruning of all redundant nodes in the tree for which X is a witness of redundancy. Essentially, the same pruning steps are performed here as in the reuse criterion described in ’Computation of a node label’ above.\nNotice that a redundant node is guaranteed to be a redundant node in any further iteration of DYNAMICHS (using a new current DPI that incorporates new test cases). So, nodes pruned by PRUNE or PRUNEQDUP can be deleted for good and do not need to be stored any longer. Moreover, it should be noted that only redundant nodes are pruned at any pruning step in DYNAMICHS. For, as long as a node in DYNAMICHS is not known to be redundant, some successor node of this node might be a minimal diagnosis w.r.t. the current DPI. Thus, the deletion of such a node could perhaps prevent the algorithm from finding a particular minimal diagnosis which would implicate the algorithm’s incompleteness.\nRemark 6.6 Since the removal of a node from a collection S ∈ {D×,Q,Qdup,D⊃} within the scope of PRUNE or PRUNEQDUP can be followed by the re-addition to S of a suitable duplicate node constructed from a node stored in Qdup, D× might be changed both in that nodes are deleted from it and added to it during the for-loop (line 48). Therefore, the ’for nd ∈ D×’-statement must be read as ’if nd is a node in the current set D× which has not yet been processed’. For a better code readability, we abstained from using a programmatically precise representation of this issue in Algorithm 9.\nDue to the soundness and completeness of QRC paired with CRC concerning the identification of a witness of redundancy for a given node and the accomplished pruning of (at least) all nodes in D× for which a witness of redundancy has been extracted, all nodes that are in D× when the algorithm reaches line 67 are non-redundant nodes. Consequently, there is no evidence to exclude the remaining nodes in D× from the further search for minimal diagnoses. For this reason, each of these nodes is reinserted into Q by INSERTSORTED in line 68 such that the sorting of Q in descending order of pnodes() is maintained. Then these nodes are deleted from D×. Thus, D× = ∅ holds after each execution of UPDATETREE.\nSo, in DYNAMICHS, unlike in STATICHS, diagnoses (and nodes in general) are not ruled out due to the fact that they contradict an answered query, but only if they are (found to be) redundant. Nevertheless, a diagnosis that contradicts an answered query is a “hot candidate” for finding some witness of redundancy. For that reason, UPDATETREE searches for witnesses of redundancy (only) by means of D× which includes the most “suspicious” nodes. Namely, it comprises those nodes that were minimal diagnoses w.r.t. the last-but-one DPI, but have been invalidated by the most recently answered query. The two possible reasons for a diagnosis nd to be invalidated are its redundancy as defined above or that it does not hit a new minimal conflict set (which is not a subset of one in nd.cs) that has been introduced by the addition of the test case resulting from the user’s query answer. Thus, it is likely to detect witnesses of redundancy by investigating nodes in D×, as the QRC and the CRC do. Throughout the pruning steps performed in lines 62-65, witnesses of redundancy extracted from nodes in D× are exploited to remove redundant nodes in the other collections Qdup, D⊃ and Q as well.\nRemark 6.7 It should be noted that the collections Q as well as D⊃ are not necessarily cleaned from all redundant nodes after all pruning steps in UPDATETREE are finished. At this point, all those redundant nodes are still elements of these collections for which no witness of redundancy was found (there might exist one, though) throughout the redundancy checks (QRC and CRC) performed.\nAssuring the non-existence of redundant nodes in Q and D⊃ might involve extensive usage of the (expensive) reasoner. In the worst case, one call of QX for each non-leaf node along each path from the root node to a leaf node labeled by nonmin or to a leaf node that has no label would be necessary. However, the number of these non-leaf nodes is generally exponential in the maximum length of such a\nCHAPTER 6. ITERATIVE DIAGNOSIS COMPUTATION 146\npath in the tree. In comparison, the number of calls of QX for investigating all nodes in D× by QRC and CRC is polynomial (linear) in the maximum length of a tree path labeled by ×. For, the number of QX-calls cannot get larger than (nmax − 1)(|ndmax| + 1) where the constant nmax is the maximum number of desired leading diagnoses predefined by the user and |ndmax| is the maximum cardinality of some nd ∈ D×. This holds since |D×| ≤ nmax − 1 (cf. Corollary 5.3) and QRC requires at most one and CRC at most |ndmax| QX-calls.\nOther than that, the chance of locating new witnesses of redundancy by means of investigating nodes in Q and D⊃ can be assumed to be smaller than for nodes in D× since there is no indication or evidence that these nodes might be redundant. So, cleaning Q and D⊃ from all redundant nodes might be significant effort with negligible impact. Therefore, DYNAMICHS is designed to focus the search for witnesses of redundancy only on the “suspicious nodes” in D×.\nAs mentioned above, when the execution arrives at line 70, only nodes that are definitely redundant (because they were deleted due to some witness of redundancy) have been deleted from the sets Q, D×, D⊃ and Qdup.\nIn lines 70-78, each node nd ∈ D⊃ which has not been deleted throughout the pruning operations in line 65 is processed as follows: If there is no minimal diagnosis D ∈ DX such that nd ⊃ D, then nd is removed from D⊃ and reinserted into Q (lines 77 and 78) in a way the sorting of Q in descending order according to pnodes() is maintained (INSERTSORTED). This re-insertion is plausible since there is no more evidence of nd (which is a non-minimal diagnosis w.r.t. the last-but-one DPI) being a non-minimal diagnosis w.r.t. the current DPI (non-minimal diagnoses might become minimal diagnoses by the addition of test cases).\nOtherwise, nd remains an element of the set of non-minimal diagnoses D⊃ w.r.t. the current DPI as DX comprises exclusively minimal diagnoses w.r.t. the current DPI and one of these is a proper subset of nd.\nIn lines 79-80, all elements in DX, each of which is a minimal diagnosis w.r.t. the current DPI, are added to Q in a way the sorting of Q in descending order according to pnodes() is maintained.\nRemark 6.8 Please notice that the elements of DX, although they are known to be minimal diagnoses w.r.t. the current DPI, are not directly added to the set of found leading diagnoses Dcalc w.r.t. the current DPI, but to Q. The reason for this is that there might be (not-yet-found) minimal diagnoses w.r.t. the current DPI (nodes in Q or successor nodes thereof) which were not minimal diagnoses w.r.t. the lastbut-one DPI (and thus are no elements of DX) that have a higher probability as per pnodes() than elements of DX. For instance, such diagnoses might have been added to Q from the set D⊃ in line 77.\nIn this way, since always the first (and most probable) node in Q is processed next, a guarantee is given that Dcalc always comprises the |Dcalc| most probable minimal diagnoses w.r.t. the current DPI as per pnodes(). The knowledge of the validity of minimal diagnoses in DX w.r.t. the current DPI is however not forgotten, but exploited in line 12 (i.e. no call of DLABEL and QX is necessary for a node in DX to be added to Dcalc), as elucidated in ’The main loop’ above.\nThe next proposition asserts that DYNAMICHS works correctly, i.e. terminates and yields an output complying with the assertions given in Algorithm 8 (a proof of this proposition is beyond the scope of this work):\nProposition 6.2 (Correctness of DYNAMICHS). Any call to DYNAMICHS (given the inputs described in Algorithm 8) within Algorithm 5 terminates and yields an output 〈Dcalc,Q,Ccalc,D×,D⊃,Qdup〉 where\n(1) Dcalc is the current set of leading diagnoses such that\n(a) Dcalc ⊆ mD〈K,B,P∪P ′,N∪N ′〉R is the set of most probable minimal diagnoses w.r.t. 〈K,B,P ∪ P ′,N ∪N ′〉R such that\nCHAPTER 6. ITERATIVE DIAGNOSIS COMPUTATION 147\n(i) nmin ≤ |Dcalc| ≤ nmax and (ii) Dcalc \\DX 6= ∅,\nif such a set Dcalc exists; or (b) Dcalc is equal to the set of all minimal diagnoses mD〈K,B,P∪P ′,N∪N ′〉R , otherwise;\nwhere “most-probable” refers to the probability measure pnodes() given by Definition 4.9 and obtained from the function p() given as an input argument to DYNAMICHS.\n(2) Q is the current queue of open (non-labeled) nodes of the produced hitting set tree,\n(3) Ccalc is a set of conflict sets w.r.t. the current DPI 〈K,B,P ∪ P ′,N ∪N ′〉R,\n(4) D× = ∅,\n(5) D⊃ is the set of all processed nodes so far throughout the execution of Algorithm 5 that are nonminimal diagnoses w.r.t. the current DPI 〈K,B,P ∪ P ′,N ∪N ′〉R and\n(6) Qdup is the updated set of stored (duplicate) nodes nd that can be used when it comes to constructing a replacement node of a pruned node nd′ ⊇ nd after tree pruning."
    }, {
      "heading" : "6.2.3 DYNAMICHS: Examples",
      "text" : "In this section we will give two examples of how interactive KB debugging using DYNAMICHS (Algorithm 5 with parametermode = dynamic) works. The first one will show the similarities and differences between the usage of DYNAMICHS (within Algorithm 5) and HS (within Algorithm 3) since it will depict the application of STATICHS on the same example DPI (see Table 4.1) that was used to show the functionality of HS in examples 4.8 and 4.9. At the same time, the first example will provide evidence that solving the problem of Interactive Dynamic KB Debugging can be less efficient than solving the problem of Interactive Static KB Debugging in terms of the number of query answers required from an interacting user. This will be discussed in more detail in Section 6.3.\nThe second example is supposed to deepen the reader’s understanding of the way DYNAMICHS works. To this end, the example DPI provided by Table 4.2 will be used which constitutes a significantly harder (interactive) debugging task than the DPI investigated in the first example. This example will involve the construction of a relatively large hitting set tree in the first iteration of DYNAMICHS (which behaves very similarly to STATICHS as well as HS and constructs the same wpHS-tree as these methods), but will then show the power of the tree pruning that can be exploited in Interactive Dynamic KB Debugging in that the tree will shrink rapidly after the addition of test cases. Hence, this example will emphasize the advantage of the decision to search for a solution of Interactive Dynamic KB Debugging rather than for a solution of Interactive Static KB Debugging (more on that in Section 6.3).\nNotice that, in the following examples, whenever some tuple or list occurs in an expression using set operators, it is interpreted as a set.\nExample 6.3 In this example we assume that the author (called user throughout this example) of the (admissible) DPI 〈K,B,P ,N 〉R given by Table 4.1 applies Algorithm 5 with mode = dynamic to interactively debug 〈K,B,P ,N 〉R. Further, the same scenario and parameter settings as in Example 6.1 are supposed. That is, nmin = nmax = 2 (notice that the time limit t is irrelevant in this case), q := 1 (cf. Section 5.2), qsm() is equal to any query selection measure described in Section 5.3.3, pK(ax ) := c < 0.5 for all ax ∈ K, i.e. all formula fault probabilities are specified to be equal (to some constant c) and σ := 0.\nThe tree constructed and parameters computed and used by Algorithm 5 using DYNAMICHS are visualized by Figures 6.4 and 6.5. We use the same notation as in Figures 4.2, 4.3, 6.1, 6.2 and 6.3 which is described in Examples 4.8, 4.9, 6.1 and 6.2.\nCHAPTER 6. ITERATIVE DIAGNOSIS COMPUTATION 148\nIn the first iteration, i.e. during the execution of the first call of DYNAMICHS during Algorithm 5, the root node (initially the empty set) is labeled by the minimal conflict set 〈1, 2, 5〉 w.r.t. 〈K,B,P ,N 〉R and three successor nodes, namely nd1 := [1], nd2 := [2] as well as nd3 := [5] with nd1.cs = nd2.cs = nd3.cs = [〈1, 2, 5〉], are added to the queue of open nodes Q. Since all formulas have been assigned an equal fault probability, DYNAMICHS conducts a breadth-first tree construction (as displayed by the numbers i© that give the order of node labeling). That is, Q in this case is a first-in-first-out queue. In this vein, first [1] and then [2] are identified as minimal diagnoses w.r.t. the given DPI.\nSince Dcalc = {[1], [2]} has a cardinality of nmin = nmax = 2, the stop criterion of DYNAMICHS causes it to terminate and return 〈Dcalc,Q,Ccalc,Q,D×,D⊃,Qdup〉 = 〈 {[1], [2]}, [[5]], {〈1, 2, 5〉}, ∅, ∅, []〉, as shown in the upper right column in Figure 6.4.\nThen, in Algorithm 5, outside of the DYNAMICHS procedure, the first query Q1 = {E → ¬A} is computed from the leading diagnoses set {[1], [2]}. The q-partition P(Q1) associated with Q1 is 〈{[1]} , {[2]} , ∅〉. The user’s answer u(Q1) to Q1 is then false . Thence, the set Dout is calculated from P(Q1) as D+(Q1) = {[1]} (due to negative answer, cf. Remark 5.6), deleted from DX := DX ∪Dcalc to yield DX = {[2]} and added to D× to yield D× = {[1]}. Now, the set DX corresponds to the set of all computed (i.e. added to Dcalc) minimal diagnoses w.r.t. the last-but-one DPI 〈K,B,P ,N 〉R that are minimal diagnoses w.r.t. current DPI 〈K,B,P ,N ∪ {Q1}〉R, i.e. that satisfy the most recently answered queryQ1. The set D× comprises all computed (i.e. added to Dcalc) minimal diagnoses w.r.t. the last-butone DPI 〈K,B,P ,N 〉R that are not minimal diagnoses w.r.t. current DPI 〈K,B,P ,N ∪{Q1}〉R, i.e. that do not satisfy the most recently answered query Q1.\nThese sets DX and D× along with the collections Q, Qdup, D⊃ and Ccalc which are unmodified outside of DYNAMICHS are used as input arguments for the second call of DYNAMICHS. Notice that, in Figures 6.4 and 6.5, the resulting values of operations performed within DYNAMICHS are given in the righthand column above the dashed line whereas values computed outside of DYNAMICHS are given below the dashed line.\nThe execution of the second call of DYNAMICHS starts with a call of the UPDATETREE function. The purpose of this function is to transform the hitting set tree T that was constructed by the first call of DYNAMICHS into an updated hitting set tree T ′. Whereas the tree T was used to locate minimal diagnoses w.r.t. the last-but-one DPI 〈K,B,P ,N 〉R, the modified tree T ′ should serve to generate minimal diagnoses w.r.t. the current DPI 〈K,B,P ,N ∪ {Q1}〉R. The parameters DX, D×, Q, Qdup, D⊃ and Ccalc that represent the tree T (given at the top of the lefthand column in Figure 6.4), where DX ∪D× is equal to the set Dcalc produced by the first call of DYNAMICHS, are i.a. given as input arguments to the UPDATETREE function.\nAs a first step within UPDATETREE, a redundancy check is performed for each diagnosis in D×. In this case D× = {D1} since D1 is the only minimal diagnosis that has been ruled out by the most recently added negative test case Q1. The purpose of the redundancy check is to figure out whether D1 is redundant w.r.t. the current DPI and must be pruned or whether it might be extended to become a minimal diagnosis w.r.t. the current DPI.\nFirst, the Quick Redundancy Check (QRC) QX(〈{2, 5} ,B,P ,N ∪ {Q1}〉R) = 〈2, 5〉 (line 50 in DYNAMICHS) is executed for D1 which detects (line 52 in DYNAMICHS) that D1 (and possibly some further nodes) is redundant and can be pruned. This holds since the minimal conflict set 〈1, 2, 5〉 w.r.t. the last-but-one DPI 〈K,B,P ,N 〉R is not a minimal conflict set w.r.t. the current DPI 〈K,B,P ,N ∪{Q1}〉R because 〈2, 5〉 returned by QX is already a minimal conflict set w.r.t. the current DPI (cf. Proposition 4.9). We call the minimal conflict set 〈2, 5〉 a witness of redundancy for D1. Hence, all branches in the hitting set tree starting from the outgoing edge of 〈1, 2, 5〉 labeled by 1 can be safely deleted from all collections representing the new tree T ′ (warranted that all minimal diagnoses w.r.t. the current DPI can still be generated from the pruned tree T ′).\nPlease notice that the QRC involves only a single call of QX using a KB of a size (here: 2) that is generally significantly smaller than |K| (here: 7) which is roughly the size of the KB used in calls of QX\nCHAPTER 6. ITERATIVE DIAGNOSIS COMPUTATION 149\nmade in the DLABEL function. Hence, the QRC will be usually very fast. An illustration why 〈2, 5〉 “replaces” 〈1, 2, 5〉 as a minimal conflict set w.r.t. the current DPI can be given as follows: First, 〈1, 2, 5〉 is a minimal conflict set w.r.t. 〈K,B,P ,N 〉R as it is a set-minimal subset of K that entails {¬A} = n1 ∈ N , there is no other negative test case in N except for n1 and there is no proper subset C′ of 〈1, 2, 5〉 where C′ ∪ B ∪ UP violates any r ∈ R (see example 4.2 for a detailed explanation). Second, formula 2 implies in particular E → Y which, along with formula 5 (Y → ¬A), yields E → ¬A. As the negative answer to Q1 is equivalent to postulating that {E → ¬A} must not be entailed by the KB desired by the user, we have that 〈2, 5〉 is a conflict set w.r.t. 〈K,B,P ,N ∪ {Q1}〉R. As neither {2} nor {5} is a invalid KB w.r.t. 〈·,B,P ,N ∪ {Q1}〉R (cf. Corollary 4.1 and Definition 4.1), we have that 〈2, 5〉 is a minimal conflict set w.r.t. 〈K,B,P ,N ∪ {Q1}〉R.\nBecause the QRC has been successful, yielding some witness of redundancy of D1, the Complete Redundancy Check (CRC) is no more necessary and the collections Qdup, Q, D× as well as D⊃ are processed by the PRUNE and PRUNEQDUP functions, respectively, which involve the removal of all nodes in these collections that are redundant due to the witness 〈2, 5〉. In other words, all nodes are eliminated which correspond to a path in the tree that includes a node label Cold ⊃ 〈2, 5〉 and the label e of the outgoing edge of Cold on this path is an element of Cold \\ 〈2, 5〉. Moreover, all the supersets of 〈2, 5〉 in Ccalc (here, only 〈1, 2, 5〉) are replaced by 〈2, 5〉 since they are not minimal conflict sets anymore (ADDSETDELSUPSETS).\nThe pruning of nodes is expressed by dashed arrows in the pictures labeled by ’Updated Tree’ in Figures 6.4 and 6.5 where the location of cutting a branch is marked by a crossline at the shaft of a dashed arrow. Furthermore, the elements of “old” minimal conflict sets that are no more elements of known (i.e. already computed) current minimal conflict sets are crossed out. As shown by the picture ’Updated Tree’ in the righthand column of Figure 6.4, D1 is the only removed node during the pruning steps using the witness of redundancy 〈2, 5〉.\nSince D⊃ = ∅, UPDATETREE directly jumps to the last three lines where all elements of DX are readded to Q in sorted order (but at the same time remain elements of DX). In the figure, this is displayed by the Q1\n=⇒ pointing to a question mark (which stands for an open node) instead of a checkmark as in the case of the STATICHS algorithm. Notice that, although it is a fact that all elements of DX are minimal diagnoses w.r.t. the current DPI, this step is necessary in order to make sure the set Dcalc returned by any call of DYNAMICHS actually comprises the |Dcalc| most probable minimal diagnoses w.r.t. the current DPI. For, there might be, for instance, some node that is a non-minimal diagnosis w.r.t. the last-but-one DPI (and is thus not an element of DX), but becomes a minimal diagnosis w.r.t. the current DPI and has a higher probability than some node in DX. Additionally, we want to point out that no calls of the DLABEL procedure are needed for diagnoses in DX as we know their label must be valid. This is reflected by the test in line 8 in DYNAMICHS.\nIn the figure, all the updated collections D⊃, Ccalc, Q as well as Qdup, after being processed by UPDATETREE are shown at the bottom of fields labeled by UPDATETREE. We want to remark that D× is always the empty set at the end of the execution of UPDATETREE since each node in D× gets either pruned or is reinserted into Q as an open node. These updated collections represent the new pruned hitting set tree that can be further constructed in order to detect all and only minimal diagnoses w.r.t. the current DPI 〈K,B,P ,N ∪{Q1}〉R. Note that the actions carried out by UPDATETREE take place between steps 4© and 5©.\nThe expansion of this tree during the repeat-loop in DYNAMICHS is depicted by the picture named ’Iteration 2’ in Figure 6.4. Namely, first (step 5©) the node [2] is directly labeled by valid (line 8) since it is a known minimal diagnosis w.r.t. the current DPI (as explained before). In the sixth step, [5] is labeled by the minimal conflict set 〈1, 2, 7〉 w.r.t. the current DPI and three further nodes ([5, 1], [5, 2] and [5, 7], all with nd.cs = [〈2, 5〉 , 〈1, 2, 7〉]) are generated as successor nodes of [5] and are added to Q. Now, [5, 1] (first-in-first-out) is the foremost node in Q and is thus processed next and found to be a minimal diagnosis w.r.t. the current DPI. Therefore, DYNAMICHS terminates and returns i.a. the new set of leading\nCHAPTER 6. ITERATIVE DIAGNOSIS COMPUTATION 150\ndiagnoses Dcalc = {[2], [5, 1]}. Please notice the difference here to Example 6.1 where the node {5, 1} never became part of Q in STATICHS due to the existence of a minimal diagnosis [1] w.r.t. the input DPI 〈K,B,P ,N 〉R which is a proper subset of this node (and due to the fact that STATICHS must only consider minimal diagnoses w.r.t. the input DPI). In the current example, this node can only become relevant w.r.t. the current DPI if all (known) diagnoses (here, only [1]) that are proper subsets of it have already been pruned. It should now be clear to the reader why non-minimal nodes cannot be deleted for good as in STATICHS and why the set D⊃ is necessary in DYNAMICHS.\nThis leading diagnosis [5, 1] is also the reason why the second query Q2 = {E → G} is different from the second query (Y → ¬A) calculated in Example 6.1.\nThe execution of the algorithm continues in an analogue manner as explained so far. In the following, we just want to explain some interesting aspects in the rest of its execution:\n• After the queryQ3 = {Y → ¬A} (the same query as the second query in Example 6.1) is answered negatively and Q3 is added to N ′ yielding the current DPI 〈K,B,P ,N ∪ {Q1, Q2, Q3}〉R, the UPDATETREE function not only prunes [2] = D2 ∈ D× and adds [5, 7] = D4 ∈ DX to Q as we delineated above for the first query Q1, but adds [5, 2] ∈ D⊃ to Q as well. The reason for that is the deletion of the minimal diagnosis [2] w.r.t. the last-but-one DPI 〈K,B,P ,N ∪ {Q1, Q2}〉R wherefore the last evidence for the non-minimality of node [5, 2] has been deleted. Hence, the status of [5, 2] as a non-minimal diagnosis is no more justified wherefore it must be added to the queue to preserve the completeness of the algorithm w.r.t. the finding of all minimal diagnoses w.r.t. the current DPI. And, indeed, [5, 2] is identified as minimal diagnosis (D5) in iteration 4.\n• For each element of D× during each execution of UPDATETREE throughout the execution of Algorithm 5, the Quick Redundancy Check (QRC) is successful. That is, each witness of redundancy used for pruning throughout the entire runtime of the algorithm could be determined very fast. Namely, as it is easy to see from line 50 in DYNAMICHS, the KB used in the call of QX in the QRC for some node nd has a size in O((|nd| − 1)|Cmax|) where Cmax is the minimal conflict set of maximum cardinality in Ccalc. In most of the cases, |nd| |K| as well as |Cmax| |K| will hold. The (usually more expensive) Complete Redundancy Check (CRC), which requires O(|nd|) calls to QX with a KB of size O(|Cmax| − 1), is thus never employed.\n• In this example, the same minimal diagnosis [5, 7] is used to compute the finally returned solution KB as in Example 6.1. The only difference between both outputs is that the KB (K \\ [5, 7]) ∪ Q4 returned by DYNAMICHS in this example contains the new positive test case Q4 ∈ P ′. The output by STATICHS in Example 6.1 does not contain any newly specified positive test case in P ′ (cf. Remark 5.19), just the union of the “original” positive test cases in P (apart from that, there is not even a newly specified positive test case in Example 6.1).\n• In spite of finding the same solution diagnosis, STATICHS requires fewer queries than DYNAMICHS. Notably, DYNAMICHS even needs a proper superset of the queries asked by STATICHS (Q1, Q2 in Example 6.1 are equal to Q1, Q3 in our current example) in this case. Such a proposition however cannot be made in general since the queries formulated by STATICHS generally differ from those formulated by DYNAMICHS. In this vein, it might just as well be the case that it takes DYNAMICHS fewer queries to finish than it takes STATICHS, due to its advantages in tree pruning.\nAll in all, the execution of Algorithm 5 in this example performs\n• 2 full QX calls, i.e. calls of QX using the KB K \\ node for a node node that actually return a minimal conflict set (there are two minimal conflict sets labeled by C in Figures 6.4 and 6.5 which do not result from QRC, CRC or the minimality test of a conflict set in line 32 of DYNAMICHS),\nCHAPTER 6. ITERATIVE DIAGNOSIS COMPUTATION 151\n• 4 fast QX calls, i.e. executions of QX within the scope of the QRC (one call of QX each for the QRC of D1, D3, D2 and D5),\n• 5 validity checks, i.e. calls of QX that return ’no conflict’ (one check for each of the five found minimal diagnoses where the identification of diagnoses D2 at step 5©, D2 at step 9©, D4 at step\n14© and D4 at step 16© does not require any call to a reasoning service by means of DX, see line 8 in DYNAMICHS; notice that QX does only perform a single KB validity check by ISKBVALID in case it returns ’no conflict’, see Algorithm 1) and\n• 4 tree update processes involving 4 pruned nodes (1 per tree update),\ncomputes\n• 5 minimal diagnoses (D1, D2, D4 w.r.t. the input DPI and D3 and D5 w.r.t. some DPI resulting from the input DPI by addition of new test cases),\n• 6 minimal conflict sets (〈1, 2, 5〉 as well as 〈1, 2, 7〉 w.r.t. the input DPI and the subsets thereof 〈2, 5〉, 〈2, 7〉, 〈5〉 and 〈7〉 w.r.t. some DPI resulting from the input DPI by addition of new test cases) and\n• 4 queries and asks the user 4 logical formulas (1 per query)\nand stores\n• a maximum of 4 nodes (where node refers to the internal representation of a node nd in DYNAMICHS as a list of edge labels (nd) and a list of node labels (nd.cs) along a path from the root node to a leaf node).\nCHAPTER 6. ITERATIVE DIAGNOSIS COMPUTATION 154\nExample 6.4 Let us now consider the (admissible) DPI 〈K,B,P ,N 〉R given by Table 4.2. We assume an expert (called user throughout this example) in the domain Dom modeled by K who wants to find a solution to Interactive Dynamic KB Debugging for the given DPI 〈K,B,P ,N 〉R by means of Algorithm 5 with mode = dynamic. Further, the same scenario and parameter settings as in Example 6.2 are supposed. That is, nmin = nmax = 3 (notice that the time limit t is irrelevant in this case), q := 1 (cf. Section 5.2), qsm() is equal to any query selection measure described in Section 5.3.3, pK̃∪K : K̃∪K → [0, 1] is given such that pK(ax ) for ax ∈ K resulting from the application of GETAXIOMSPROBS is as given by Table 6.1 and σ := 0.\nThe tree constructed and parameters computed and used by Algorithm 5 using DYNAMICHS are visualized by Figures 6.6 and 6.7. We use the same notation as in Figures 4.2, 4.3, 6.1, 6.2, 6.3, 6.4 and 6.5 which is described in Examples 4.8, 4.9, 6.1, 6.2 and 6.3.\nAfter the initialization of variables, Algorithm 5 calls the function GETFORMULAPROBS in line 5 which exploits pK̃∪K() to calculate the function pK() giving the fault probabilities of formulas in K (cf. Sections 4.5.1, 5.3.2 and Example 4.7).\nThen, DYNAMICHS is called for the first time, resulting in the hitting set tree given in the first picture in Figure 6.6. As outlined by the numbers i© indicating at which point in time a node is labeled, the root node (initially the empty set) is labeled first by C1 := 〈1, 2, 5〉 and three successor nodes, namely nd1 := [1], nd2 := [2] as well as nd3 := [5] with nd1.cs = nd2.cs = nd3.cs = [〈1, 2, 5〉], are added to the queue of open nodes Q. Contrary to Example 6.3, where the tree was built up in breadth-first order, in this example the formula probabilities p() := pK() given by Table 6.1 are used to assign a probability pnodes(n) to each path n in the tree starting from the root node (cf. Formula 4.6 and Definition 4.9). In this vein, the node corresponding to the outgoing edge of C1 labeled by the formula with the largest fault probability among all formulas in C1 is processed next. That is, the node [1] with pnodes([1]) = 0.41 (as opposed to the nodes [2] and [5] with 0.25 each) is labeled next. The DLABEL procedure, after checking whether [1] is a non-minimal diagnosis w.r.t. 〈K,B,P ,N 〉R (check is negative), computes another minimal conflict set C2 := 〈2, 4, 6〉 such that [1] ∩ C2 = ∅ (C2 is not hit by the node [1]) to constitute a label for node [1]. The successor nodes [1, 2], [1, 4] and [1, 6] of [1] are generated and added to the list Q in a way that the sorting of Q in descending order of pnodes() is maintained.\nSince [1, 4] (0.28) as well as [1, 6] (0.27) have a larger probability (as per pnodes()) than the nodes [2] (0.25) and [5] (0.25), Q is given by [[1, 4], [1, 6], [2], [5], [1, 2]] when it comes to the processing of the next node. Since DYNAMICHS always treats the first node of Q next, it identifies the first minimal diagnoses D1 := [1, 4] andD2 := [1, 6] w.r.t. 〈K,B,P ,N 〉R at steps 3© and 4©, respectively. At step 5©, when node [2] is processed, a minimal conflict set C3 := 〈1, 3, 4〉 is computed and set as a label for [2], giving rise to the generation of three further nodes [2, 1], [2, 3] and [2, 4], all with ndi.cs = [〈1, 2, 5〉 , 〈1, 3, 4〉].\nHowever, notice that not all of these new nodes are added to Q, contrary to STATICHS (cf. Example 6.2). For, there is already a node [1, 2] corresponding to the set {1, 2} in Q. Due to the test performed in line 20, this duplicate node [2, 1] is assigned to the list Qdup which is expressed in the figure by dup. Since diagnoses are sets, not lists, [1, 2, ax 1, . . . , axk] and [2, 1, ax 1, . . . , axk] constitute one and the same diagnosis and it is irrelevant whether the one or the other is found. Hence, the nodes [1, 2] and [2, 1] are regarded as duplicates. Nevertheless, ndi := [2, 1] (with ndi.cs = [〈1, 2, 5〉 , 〈1, 3, 4〉]) must not be completely deleted as it might be the case that (some successor node of) ndj := [1, 2] (with ndj .cs = [〈1, 2, 5〉 , 〈2, 4, 6〉]) becomes redundant due to the eventual addition of some test case. For example, in case the reason for the redundancy of ndj is given (only) by a witness of redundancy that is a subset of 〈2, 4, 6〉, ndj is pruned and replaced by the node ndi which is still non-redundant.\nThence, only [2, 3] and [2, 4] are added to Q as successor nodes of the processed node [2]. Next, the minimal conflict set C2 = 〈2, 4, 6〉 is reused (lines 30-40 in DLABEL) as a label for node [5] with pnodes([5]) = 0.25 and the three new nodes [5, 2], [5, 4] as well as [5, 6] are generated and assigned to Q at step 7©. Then, the fourth minimal conflict set C4 := 〈1, 5, 6, 8〉 is computed to label the node [2, 4] with pnodes([2, 4]) = 0.18 and the four new nodes [2, 4, 1], [2, 4, 5], [2, 4, 6] as well as [2, 4, 8] are generated\nCHAPTER 6. ITERATIVE DIAGNOSIS COMPUTATION 155\nand assigned to Q st step 8©. At step 9©, the third minimal diagnosis D3 := [5, 4] w.r.t. 〈K,B,P ,N 〉R is eventually found and added to Dcalc which now has reached a cardinality of 3 = nmin = nmax wherefore DYNAMICHS stops and returns i.a. the set of leading diagnoses Dcalc = {[1, 4], [1, 6], [5, 4]}. The returned values are given in the lefthand column in Figure 6.6.\nAs in Example 6.2, where a debugging session for the same DPI using STATICHS is presented, the first query Q1 is computed as {B v K} and answered by true by the user. The assignment of Q1 to the positive test cases of the DPI 〈K,B,P ,N 〉R brings the opportunity to perform some significant pruning actions (within the function UPDATETREE called at the beginning of the second call of DYNAMICHS). These are shown in the tree with the caption ’Updated Tree’ and in the righthand column in Figure 6.6.\nAs a first step within UPDATETREE, a redundancy check is performed for each diagnosis in D×. In this case D× = {D3} = {[5, 4]} since D3 is the only minimal diagnosis that has been ruled out by the most recently added positive test case Q1. The purpose of the redundancy check is to figure out whether D3 is redundant w.r.t. the current DPI and must be pruned or whether it might be extended to become a minimal diagnosis w.r.t. the current DPI.\nFirst, the Quick Redundancy Check (QRC) QX(〈{1, 2, 6} ,B,P ∪ {Q1} ,N 〉) = 〈1〉 (line 50 in DYNAMICHS) is executed for D3 where the KB {1, 2, 6} used in this call of QX is obtained by deletion of node := D3 from the union of all conflict sets (the elements of node.cs) along the path that corresponds to D3, i.e. {1, 2, 6} = (〈1, 2, 5〉 ∪ 〈2, 4, 6〉) \\ [5, 4]. By means of the QRC it is figured out (line 52 in DYNAMICHS) that D3 (and possibly some further nodes) is redundant and can be pruned. This holds since the minimal conflict set 〈1, 2, 5〉 w.r.t. the last-but-one DPI 〈K,B,P ,N 〉R is not a minimal conflict set w.r.t. the current DPI 〈K,B,P∪{Q1} ,N 〉R because 〈1〉 returned by QX is already a minimal conflict set w.r.t. the current DPI (cf. Proposition 4.9). We call this minimal conflict set 〈1〉 a witness of redundancy for D3. Hence, all branches in the hitting set tree starting from an outgoing edge of 〈1, 2, 5〉 labeled by 2 or by 5 can be safely deleted from all collections storing nodes in DYNAMICHS.\nAn illustration why 〈1〉 “replaces” 〈1, 2, 5〉 as a minimal conflict set w.r.t. the current DPI can be given as follows: First, 〈1, 2, 5〉 is a minimal conflict set w.r.t. 〈K,B,P ,N 〉R as it is a set-minimal subset of K that entails {A v K} = n1 ∈ N and there is no proper subset C′ of 〈1, 2, 5〉 where C′ ∪ B ∪ UP violates any r ∈ R or entails any n ∈ N (see example 4.3 for a detailed explanation). Second, considering the current DPI 〈K,B,P ∪ {Q1} ,N 〉R, we have that 〈1, 2, 5〉 ∪ B ∪ UP∪{Q1} |= n1, too. However, {2, 5} = {B v G,G v K} |= {B v K} = Q1 implies that B ∪ UP∪{Q1} ⊇ Q1 can replace the subset {2, 5} of the conflict set 〈1, 2, 5〉. For, formula 1 (A v B) along with Q1 (B v K) already entails n1. Further, B ∪ UP∪{Q1} cannot violate any negative test case ni ∈ N or requirement rj ∈ R by the admissibility of the input DPI 〈K,B,P ,N 〉R, the fact that Q1 is a query, Corollary 5.3, Definition 3.6 and Proposition 3.4. Thus, by Definition 4.1, 〈1〉 is in fact a minimal conflict set w.r.t. the current DPI 〈K,B,P ∪ {Q1} ,N 〉R.\nNow, the first nice thing at this point is that 〈1〉 is not only a witness of redundancy of nodes nd where 〈1, 2, 5〉 ∈ nd.cs, but of each nd (in the tree or in the set Qdup of duplicate nodes) where nd.cs contains a conflict set that is a proper superset of 〈1〉. That is, 〈1〉 also replaces 〈1, 3, 4〉 as well as 〈1, 5, 6, 8〉. This implicates that two outgoing edges (those labeled by 2 or 5) of 〈1, 2, 5〉, two outgoing edges (those labeled by 3 or 4) of 〈1, 3, 4〉 and three outgoing edges (those labeled by 5, 6 or 8) of 〈1, 5, 6, 8〉 can be pruned.\nThe second nice thing that has an even more significant bearing on tree pruning than the first thing is that 〈1〉 is a witness of redundancy of the conflict set that labels the root node. That is, pruning can take place at the very top of the tree and two of three subtrees rooted at successor nodes of the root node can be pruned. That is, for instance, within the rightmost subtree of the root node in the picture with caption ’Updated Tree’ in Figure 6.6 no pruning is possible at all since the conflict set 〈2, 4, 6〉 labels the root node of this subtree and 〈1〉 is not a subset of 〈2, 4, 6〉. However, this subtree is still redundant since it is connected with the root node by a “redundant” edge labeled by 5. As a consequence, we can observe the pruning of a total of 9 nodes (of altogether 12 nodes in the tree) in only one execution of UPDATETREE.\nCHAPTER 6. ITERATIVE DIAGNOSIS COMPUTATION 156\nNow, to receive an impression of the power of tree pruning in DYNAMICHS, the reader is invited to compare the trees used in iterations 2 and 3 in the current example (the bottom left pictures in Figure 6.6 and Figure 6.7) with the trees used in iterations 2 and 3 in Example 6.2 (the bottom picture in\nFigure 6.2 and the picture in Figure 6.3) which deals with the debugging of the same DPI (just by means of STATICHS instead of DYNAMICHS), uses the same sets of leading diagnoses in each iteration, thus the same queries, and of course the same user (that gives the same answers in both examples).\nAfter all diagnoses of DX are added to Q as a final action within UPDATETREE, the repeat-loop of the second iteration of DYNAMICHS is entered. Here, the minimal diagnoses D1 (pnodes(D1) = 0.28, step 11©), D2 (0.27, 12©) and D4 (0.09, 13©) are found and assigned to the empty set Dcalc before DYNAMICHS terminates again. Notice that only one call of the DLABEL procedure is required in the second iteration (for node [1, 2]) due to the test in line 8 of DYNAMICHS which is positive forD1 andD2 (sinceD1,D2 ∈ DX).\nOnce the second query Q2 = {B v ∃r.F} is added to the positive test cases resulting in the DPI 〈K,B,P ∪ {Q1, Q2} ,N 〉R, the UPDATETREE function causes the pruning of two further nodes (D2 = [1, 6] and D4 = [1, 2]) leading to the continuance of only a single node (D1 = [1, 4]) in the memory of DYNAMICHS (see the picture with caption ’Updated Tree’ in Figure 6.7). The reason for this is that Q2 can “replace” the part {2, 6} = {B v G,G v ∃r.F} (which entails Q2) of the minimal conflict set 〈2, 4, 6〉 w.r.t. the last-but-one DPI 〈K,B,P ∪ {Q1} ,N 〉R such that 〈2, 4, 6〉 \\ {2, 6} = 〈4〉 is already a minimal conflict set w.r.t. the current DPI 〈K,B,P ∪ {Q1, Q2} ,N 〉R (cf. the analysis of the minimal conflict set C2 = 〈2, 4, 6〉 in Example 4.3).\nSince, by now, all minimal conflict sets 〈1, 2, 5〉, 〈2, 4, 6〉, 〈1, 5, 6, 8〉 as well as 〈1, 3, 4〉w.r.t. the input DPI 〈K,B,P ,N 〉R have “shrunk” as much as to constitute only two different set-minimal sets 〈1〉 and 〈4〉, it is clear by Proposition 4.6 that there can be only a single minimal diagnosis [1, 4] w.r.t. the current DPI 〈K,B,P ∪ {Q1, Q2} ,N 〉R. Therefore, the third iteration of DYNAMICHS terminates due to Q = [] and returns the singleton set Dcalc = {[1, 4]}. Consequently, the probability pD([1, 4]) = 1 wherefore Algorithm 5 also stops executing and returns (K \\ [1, 4]) ∪ p1 ∪ Q1 ∪ Q2 as the (exact) solution to the Interactive Dynamic KB Debugging problem for the DPI 〈K,B,P ,N 〉R.\nThe advantage of DYNAMICHS in this example over STATICHS in Example 6.2 in iterations 2 and 3 is that the pruning of nodes lets the algorithm automatically focus on the still relevant (i.e. non-redundant) parts of the tree. STATICHS, on the other hand, is doomed to spend most of the execution time for investigating nodes that turn out to be already invalidated by some specified test case(s). As already mentioned in Example 6.2, the inability of STATICHS to “early-prune” incomplete branches of the tree is especially unfavorable in the last iteration of STATICHS in case σ = 0 since all irrelevant minimal diagnoses w.r.t. the input DPI must first be computed before they can be ruled out.\nThis immense upside of DYNAMICHS over STATICHS (see the analysis in the end of Example 6.2) also finds expression in the quantitative analysis of this example given next. All in all, the execution of Algorithm 5 in this example performs\n• 4 full QX calls, i.e. calls of QX using the KB K \\ node for a node node that actually return a minimal conflict set (there are four minimal conflict sets labeled by C in Figures 6.6 and 6.7 which do not result from QRC, CRC or the minimality test of a conflict set in line 32 of DYNAMICHS),\n• 2 fast QX calls, i.e. executions of QX within the scope of the QRC (one call of QX each for the QRC of D3 and D2),\n• 4 validity checks, i.e. calls of QX that return ’no conflict’ (one check for each of the four found minimal diagnoses where the identification of diagnosesD1 at step 11©,D2 at step 12© andD1 at step 15© does not require any call to a reasoning service by means of DX, see line 8 in DYNAMICHS;\nnotice that QX does only perform a single KB validity check by ISKBVALID in case it returns ’no conflict’, see Algorithm 1) and\nCHAPTER 6. ITERATIVE DIAGNOSIS COMPUTATION 157\n• 2 tree update processes involving 11 pruned nodes (9 nodes during the first update between steps 10© and 11© and 2 nodes during the second between steps 14© and 15©),\ncomputes\n• 4 minimal diagnoses (D1, D2, D3 and D4, all w.r.t. the input DPI),\n• 6 minimal conflict sets (〈1, 2, 5〉, 〈2, 4, 6〉, 〈1, 3, 4〉 and 〈1, 5, 6, 8〉 w.r.t. the input DPI and the subsets thereof 〈1〉 and 〈4〉 w.r.t. some DPI resulting from the input DPI by addition of new test cases) and\n• 2 queries and asks the user 2 logical formulas (1 per query)\nand stores\n• a maximum of 12 nodes (where node refers to the internal representation of a node nd in DYNAMICHS as a list of edge labels (nd) and a list of node labels (nd.cs) along a path from the root node to a leaf node).\nFinally, we want to emphasize that, in all executions of UPDATETREE throughout this example, the usually very efficient QRC was successful right off and the usually more time-consuming CRC was never required.\nCHAPTER 6. ITERATIVE DIAGNOSIS COMPUTATION 158\n1©〈1, 2, 5〉C\n2©〈2, 4, 6〉C 5©〈1, 3, 4〉C 7©〈2, 4, 6〉R\n? 3©X(D1) 4©X(D2) 6© dup ? 8©〈1, 5, 6, 8〉C ? 9©X(D3) ?\n? ? ? ?\n1\n0.41 rr\n2 0.25 5 0.25 ,,\n2\n0.09ww 4 0.28 6 0.27''\n1\n0.09ww 3 0.07 4 0.18''\n2\n0.06ww 4 0.18 6 0.17''\n1\n0.06ww\n5\n0.04\n6\n0.11\n8\n0.04\nIteration 1\n〉\nDcalc = {D1,D2,D3} = {[1, 4], [1, 6], [5, 4]}\nQ = [[5, 6], [2, 4, 6], [1, 2], [2, 3], [5, 2],\n[2, 4, 1], [2, 4, 5], [2, 4, 8]]\nCcalc = {〈1, 2, 5〉 , 〈2, 4, 6〉 , 〈1, 3, 4〉 , 〈1, 5, 6, 8〉}\nD⊃ = ∅\nQdup = [[2, 1]]\n〈Q1,P(Q1)〉 = 〈{B v K} , 〈{D1,D2} , {D3} , ∅〉〉\nu(Q1) = true\nDX = {D1,D2}, D× = {D3}\n〉\nUPDATETREE:\nQRC (D3): QX(〈{1, 2, 6} ,B,P ∪ {Q1} ,N 〉) = 〈1〉\n⇒ PRUNEQDUP/PRUNE: 〈1, 2, 5〉 → 〈1〉, 〈1, 3, 4〉 → 〈1〉\n• prune all subtrees starting from nodes 〈1, 2, 5〉\nby an outgoing edge with label 2 or 5\n• prune all subtrees starting from nodes 〈1, 3, 4〉\nby an outgoing edge with label 3 or 4\n• replace by 〈1〉 all node labels in the tree\nthat are proper supersets of 〈1〉\n⇒ D⊃ = ∅, Ccalc = {〈1〉 , 〈2, 4, 6〉},\nQ = [[1, 4], [1, 6], [1, 2]], Qdup = []\n〉\n1© 〈 1, A2, A5 〉C 2©〈2, 4, 6〉C 5© 〈 1, A3, A4 〉C 7©〈2, 4, 6〉R\n? 3©X(D1) 4©X(D2) 6© dup ? 8© 〈 1, A5, A6, A8 〉C ? 9©X(D3) ?\n10©? 10©? 10©×\n? ? ? ?\n1\n0.41 rr\n_ 2 0.25\n5 0.25 ,,\n2\n0.09ww 4 0.28 6 0.27''\n1 0.09ww _ 3 0.07 4 0.18'' 2 0.06ww 4 0.18 6 0.17''\nQ1 Q1\nQ1 1\n0.06ww\n?\n5\n0.04\n_\n6\n0.11\n8\n0.04\nUpdated Tree\n〉\n1©〈1〉C\n2©〈2, 4, 6〉C\n13©X(D4) 3©X(D1) 4©X(D2)\n11©X(D1) 12©X(D2)\n1\n0.41ss\n2\n0.09 ww 4 0.28 6 0.27''\nQ1 Q1\nIteration 2\n〉 Dcalc = {D1,D2,D4} = {[1, 4], [1, 6], [1, 2]} Q = [] Ccalc = {〈1〉 , 〈2, 4, 6〉}\nD⊃ = ∅\nQdup = []\n〈Q2,P(Q2)〉 = 〈{B v ∃r.F} , 〈{D1} , {D2,D4} , ∅〉〉\nu(Q2) = true\nDX = {D1}, D× = {D2,D4}\n〉\nFigure 6.6: (Example 6.4) Solving the problem of Interactive Dynamic KB Debugging (Problem Definition 5.1) for the example DPI given by Table 4.2 by means of Algorithm 5 and DYNAMICHS.\nCHAPTER 6. ITERATIVE DIAGNOSIS COMPUTATION 160\nAlgorithm 8 Iterative Construction of a Dynamic Hitting Set Tree Input: a tuple 〈〈K,B,P ,N 〉R,Q,Qdup, t, nmin, nmax,Ccalc,DX,D×, p(),P ′,N ′,D⊃〉 consisting of\n• the DPI 〈K,B,P ,N 〉R given as input to Algorithm 5, • the overall sets of positively (P ′) and negatively (N ′) answered queries added as test cases to 〈K,B,P ,N 〉R so far, • a queue Q of open (non-labeled) nodes, • some computation timeout t, • a desired minimal (nmin ≥ 2) and maximal (nmax) number of minimal diagnoses to be returned, • a set Ccalc of conflict sets w.r.t. the current DPI 〈K,B,P ∪ P ′,N ∪N ′〉R, • a set DX of minimal diagnoses w.r.t. the current DPI 〈K,B,P ∪ P ′,N ∪N ′〉R, • a set D× of minimal diagnoses w.r.t. the last-but-one DPI that are invalidated by the most recently added test case, • a function p : K → (0, 0.5), • a set D⊃ of non-minimal diagnoses w.r.t. the last-but-one DPI and • a set Qdup of stored (duplicate) nodes nd that can be used when it comes to constructing a replacement node of a pruned\nnode nd′ ⊇ nd after tree pruning. Output: a tuple 〈Dcalc,Q,Ccalc,D×,D⊃,Qdup〉 where\n• Dcalc is the current set of leading diagnoses such that (a) Dcalc ⊆ mD〈K,B,P∪P′,N∪N ′〉R is the set of most probable minimal diagnoses w.r.t. 〈K,B,P ∪ P\n′,N ∪ N ′〉R such that (i) nmin ≤ |Dcalc| ≤ nmax and (ii) Dcalc \\DX 6= ∅, if such a set Dcalc exists, or\n(b) Dcalc is equal to the set of all minimal diagnoses mD〈K,B,P∪P′,N∪N ′〉R , otherwise,\nwhere “most-probable” refers to the probability measure pnodes() (cf. Definition 4.9) obtained from the given function p();\n• Q is the current queue of open (non-labeled) nodes of the hitting set tree, • Ccalc is a set of conflict sets w.r.t. the current DPI 〈K,B,P ∪ P ′,N ∪N ′〉R, • D× = ∅, • D⊃ is the set of all processed nodes so far throughout the execution of Algorithm 5 that are non-minimal diagnoses w.r.t. the\ncurrent DPI 〈K,B,P ∪ P ′,N ∪N ′〉R and • Qdup is the updated set of stored (duplicate) nodes nd that can be used when it comes to constructing a replacement node of\na pruned node nd′ ⊇ nd after tree pruning.\n1: procedure DYNAMICHS(〈K,B,P ,N 〉R,Q,Qdup, t, nmin, nmax,Ccalc,DX,D×, p(),P ′,N ′,D⊃) 2: tstart ← GETTIME() 3: Dcalc ← ∅ 4: 〈Q,D×,D⊃,Ccalc,Qdup〉 ← UPDATETREE(〈K,B,P ,N 〉R,D×,Q,Qdup,D⊃,DX,Ccalc, p(),P ′,N ′) 5: repeat . UPDATETREE (see Algorithm 9) 6: node← GETFIRST(Q) . node is processed 7: Q← DELETEFIRST(Q) 8: if node ∈ DX then .DX includes only minimal diagnoses w.r.t. current DPI 9: L← valid 10: else 11: 〈L,Ccalc,Qdup〉 ← DLABEL(〈K,B,P ,N 〉R, node,Ccalc,Dcalc,Q,Qdup, p(),P ′,N ′) 12: if L = valid then . DLABEL (see Algorithm 9) 13: Dcalc ← Dcalc ∪ {node} . node is a minimal diagnosis w.r.t. current DPI 14: else if L = nonmin then 15: D⊃ ← D⊃ ∪ {node} . node is a non-minimal diagnosis w.r.t. current DPI 16: else 17: for e ∈ L do . L is a minimal conflict set w.r.t. current DPI 18: nodee ← ADD(node, e) . nodee is generated 19: nodee.cs← ADD(node.cs, L) 20: if nodee ∈ Q then . nodee is a (set-equal) duplicate of a node in Q 21: Qdup ← INSERTSORTED(nodee,Qdup, cardinality, ascending) 22: else 23: Q← INSERTSORTED(nodee,Q, pnodes(), descending) 24: until Q = [] ∨ [Dcalc \\DX 6= ∅ ∧ |Dcalc| ≥ nmin ∧ (|Dcalc| = nmax ∨ GETTIME()− tstart > t)] 25: return 〈Dcalc,Q,Ccalc,D×,D⊃,Qdup〉\nCHAPTER 6. ITERATIVE DIAGNOSIS COMPUTATION 161\nAlgorithm 9 Iterative Construction of a Dynamic Hitting Set Tree (continued) 26: procedure DLABEL(〈K,B,P ,N 〉R, node,Ccalc,Dcalc,Q,Qdup, p(),P ′,N ′) 27: for nd ∈ Dcalc do 28: if node ⊃ nd then . node is a non-minimal diagnosis 29: return 〈nonmin,Ccalc,Qdup〉 30: for C ∈ Ccalc do .Ccalc includes only conflict sets w.r.t. current DPI 31: if C ∩ node = ∅ then . reuse (a subset of) C to label node 32: X ← QX(〈C,B,P ∪ P ′,N ∪N ′〉R) . Algorithm 1 (page 39) to test if C is minimal w.r.t. current DPI 33: if X = C then 34: return 〈C,Ccalc,Qdup〉 35: else . X ⊂ C 36: Qdup ← PRUNEQDUP(X,Qdup) . PRUNEQDUP (see Algorithm 10) 37: Q← PRUNE(X,Q,Qdup, pnodes()) . PRUNE (see Algorithm 10) 38: D⊃ ← PRUNE(X,D⊃,Qdup, ∅) 39: Ccalc ← ADDSETDELSUPSETS(X,Ccalc) . add X to Ccalc and delete all its supersets from Ccalc 40: return 〈X,Ccalc,Qdup〉 41: L← QX(〈K \\ node,B,P ∪ P ′,N ∪N ′〉R) . Algorithm 1 (page 39) to test if node is a diagnosis 42: if L = ’no conflict’ then . node is a diagnosis 43: return 〈valid,Ccalc,Qdup〉 44: else . L is a new minimal conflict set (/∈ Ccalc) 45: Ccalc ← Ccalc ∪ {L} 46: return 〈L,Ccalc,Qdup〉\n47: procedure UPDATETREE(〈K,B,P ,N 〉R,D×,Q,Qdup,D⊃,DX,Ccalc, p(),P ′,N ′) 48: for nd ∈ D× do 49: quickRC, completeRC ← false 50: X ← QX(〈Und.cs \\ nd,B,P ∪ P ′,N ∪N ′〉R) 51: for C ∈ nd.cs do 52: if X ⊂ C then 53: quickRC ← true 54: break 55: if quickRC = false then 56: for i← 1, . . . , |nd| do 57: X ← QX(〈nd.cs[i] \\ {nd[i]} ,B,P ∪ P ′,N ∪N ′〉R) 58: if X 6= ’no conflict’ then 59: completeRC ← true 60: break 61: if quickRC = true ∨ completeRC = true then . condition true iff nd redundant w.r.t. current DPI 62: Qdup ← PRUNEQDUP(X,Qdup) . PRUNEQDUP (see Algorithm 10) 63: Q← PRUNE(X,Q,Qdup, pnodes()) . PRUNE (see Algorithm 10) 64: D× ← PRUNE(X,D×,Qdup, ∅) 65: D⊃ ← PRUNE(X,D⊃,Qdup, ∅) 66: Ccalc ← ADDSETDELSUPSETS(X,Ccalc) . add X to Ccalc and delete all its supersets from Ccalc 67: for nd ∈ D× do . add all (non-pruned) nodes in D× to Q 68: Q← INSERTSORTED(nd,Q, pnodes(), descending) 69: D× ← D× \\ {nd} 70: for nd ∈ D⊃ do . update D⊃: add all nodes to Q which are not proper supersets of a diagnosis in DX 71: nonmin← false 72: for nd′ ∈ DX do 73: if nd ⊃ nd′ then 74: nonmin← true 75: break 76: if nonmin = false then 77: Q← INSERTSORTED(nd,Q, pnodes(), descending) 78: D⊃ ← D⊃ \\ {nd} 79: for D ∈ DX do . reinsert known minimal diagnoses to Q to find diagnoses in order of descending pnodes() 80: Q← INSERTSORTED(D,Q, pnodes(), descending) 81: return 〈Q,D×,D⊃,Ccalc,Qdup〉\nCHAPTER 6. ITERATIVE DIAGNOSIS COMPUTATION 162\nAlgorithm 10 Iterative Construction of a Dynamic Hitting Set Tree (continued) 82: procedure PRUNE(X,S,Dup, sort_measure) 83: if S is a list then 84: S′ ← [] 85: else 86: S′ ← ∅ 87: for nd ∈ S do 88: k ← 0 89: for i = 1 to |nd.cs| do 90: if nd.cs[i] ⊃ X then . check first redundancy criterion 91: if nd[i] ∈ nd.cs[i] \\X then . check second redundancy criterion 92: k ← i 93: else 94: nd.cs[i]← X . replace each superset of X in nd.cs by X 95: if k > 0 then . nd is redundant 96: for node← Dup[1], . . . , Dup[|Dup|] do 97: if |node| ≥ k ∧ nd[1..|node|] = node then 98: ndnew ← ADD(node, nd[|node|+ 1..|nd|]) . construct replacement node ndnew of nd 99: ndnew.cs← ADD(node.cs, nd.cs[|node|+ 1..|nd|]) 100: S′ ← INSERTSORTED(ndnew, S′, sort_measure, descending) 101: break 102: else . X is not a witness of redundancy of nd 103: S′ ← INSERTSORTED(nd, S′, sort_measure, descending) 104: return S′\n105: procedure PRUNEQDUP(X,Dup) 106: Dupnew ← [] 107: for i← 1 to |Dup| do 108: ndi← Dup[i] 109: k ← 0 110: for m← 1 to |ndi.cs| do 111: if ndi.cs[m] ⊃ X then . check first redundancy criterion 112: if ndi[m] ∈ ndi.cs[m] \\X then . check second redundancy criterion 113: k ← m 114: else 115: ndi.cs[m]← X . replace each superset of X in ndi.cs by X 116: if k > 0 then . ndi is redundant 117: for ndj ∈ Dupnew do 118: if |ndj| ≥ k ∧ ndi[1..|ndj|] = ndj then 119: ndinew ← ADD(ndj, ndi[|ndj|+ 1..|ndi|]) . construct replacement node ndinew of ndi 120: ndinew.cs← ADD(ndj.cs, ndi.cs[|ndj|+ 1..|ndi|]) 121: Dupnew ← INSERTSORTED(ndinew, Dupnew, cardinality, ascending) 122: break 123: else . X is not a witness of redundancy of ndi 124: Dupnew ← INSERTSORTED(ndi, Dupnew, cardinality, ascending) 125: return Dupnew\nCHAPTER 6. ITERATIVE DIAGNOSIS COMPUTATION 163\nCHAPTER 6. ITERATIVE DIAGNOSIS COMPUTATION 164"
    }, {
      "heading" : "6.3 Discussion of Iterative Diagnosis Computation",
      "text" : "In this section we want to summarize properties of and differences between STATICHS and DYNAMICHS that we already pointed out in previous sections and, additionally, we want to shed light on some further interesting aspects of these iterative diagnosis computation methods in the scope of interactive KB debugging (Algorithm 5). Table 6.2 provides an overview of what we did discuss or will discuss below.\nFirst Segment of Table 6.2 – Addressed Problem and Properties w.r.t. Solutions. The first row of the table has been proven by Proposition 5.16 on page 105. Results given by the second up to the fourth row of the table are substantiated by Propositions 6.1 (STATICHS) and 6.2 (DYNAMICHS). We have discussed in Section 6.1.1 that Algorithm 5 with mode = static can artificially fix the search space for possible solutions initially. This is an inherent property of the Interactive Static KB Debugging Problem which the algorithm aims to solve in static mode. For, a minimal diagnosis w.r.t. the input DPI which satisfies all answered queries added as test cases throughout the debugging session must be detected (see left column of category “diagnoses” in Table 6.2). Hence, the solution space is given by |mDinputDPI |. “Initially fixed search space” in this case means that, given the fault tolerance σ = 0, Algorithm 5 in static mode must compute all minimal diagnoses w.r.t. the input DPI, i.e. the entire set mDinputDPI . In case of dynamic mode, on the other hand, the solution space (i.e. minimal diagnoses w.r.t. the current DPI, see right column of Table 6.2 in category “diagnoses”) that needs to be explored by Algorithm 5 for a given value of zero for σ is not known in advance. It rather depends on which test cases are specified or, respectively, which queries the user is asked. In case of the usage of mainly “positive-impact queries”, the search space might have significantly smaller cardinality than mDinputDPI whereas it might grow significantly beyond the cardinality of mDinputDPI in a scenario where many unfavorable “negativeimpact queries” are generated (cf. Section 6.2.1). The maximum theoretically possible cardinality of the search space for DYNAMICHS is given by |aDinputDPI |.\nSecond Segment of Table 6.2 – Impact of New Test Cases and Computation Focus. The properties given in the category “computes” in Table 6.2 are confirmed by Propositions 6.1 (STATICHS) and 6.2 (DYNAMICHS). Hence, other than DYNAMICHS which analyzes the current DPI in terms of minimal conflict sets and diagnoses in each iteration, STATICHS must only consider minimal conflict sets w.r.t. the input DPI (see categories “diagnoses” and “conflict sets” in Table 6.2). This is sufficient for the exploration of all minimal diagnoses w.r.t. the input DPI by Proposition 4.6. In this vein, new test cases in static KB debugging are not taken into account in the computation of minimal conflict sets. Instead, new test cases are just exploited to invalidate already computed minimal diagnoses w.r.t. the input DPI. Thus, test cases specified during static KB debugging are treated somewhat inferior to test cases already present in the input DPI. Because, the newly gained information given by these test cases is not utilized to reveal new faults in the KB or to lay the focus on just the now relevant parts of existing faults, but only for the purpose of constraining the search space for minimal diagnoses w.r.t. the input DPI 〈K,B,P ,N 〉R. We might thus call test cases added during the execution of Algorithm 5 with mode = static pure differentiation test cases (see category “purpose of test cases” in Table 6.2).\nOf course, seen from the point of view of a current DPI, i.e. the input DPI extended by differentiation test cases, STATICHS does not guarantee completeness w.r.t. this current DPI, but only w.r.t. the initial one. This however does not mean that, after the (exact) solution K∗ := (K \\ D) ∪ UP of the Interactive Static KB Debugging problem has been localized by means of STATICHS, the differentiation test cases (P ′ and N ′) cannot be simply added to the DPI. In this case, K∗ is still a maximal solution KB w.r.t. the extended input DPI 〈K,B,P ∪ P ′,N ∪ N ′〉R. In other words, there is no conflict set (and thus no diagnosis) w.r.t. 〈K\\D,B,P ∪P ′,N ∪N ′〉R andK\\D is valid w.r.t. 〈·,B,P ∪P ′,N ∪N ′〉R. However, in spite of using the (exact) solution KB of the Interactive Static KB Debugging problem, it is not ensured that this solution is the optimal one w.r.t. the extended DPI, i.e. of the Interactive Dynamic KB Debugging\nCHAPTER 6. ITERATIVE DIAGNOSIS COMPUTATION 165\nproblem. This is because user interaction is just exploited to the extent that the best solution w.r.t. the input DPI is crystallized out. It is not used to have the solution verified by the user in the light of the extended DPI.\nOn the other hand, test cases assigned throughout dynamic KB debugging by means of Algorithm 5 with mode = dynamic are treated equally as test cases already given in the input DPI. They are used to prune the search space and to pinpoint new faults that arise from added test cases resulting from answered queries. The dynamic algorithm assists the user in filtering out a solution and verifying in a thorough manner that this solution is the desired one w.r.t. the extended DPI, among all existing solutions w.r.t. the extended DPI. Due to these aspects we might regard Algorithm 5 with mode mode = dynamic as the standard method for Interactive KB Debugging.\nW.r.t. the impact of new test cases (answered queries) added to the DPI on the set of minimal (all) diagnoses it can be shown for STATICHS that (for arbitrary iteration i of Algorithm 5) mDi ⊃ mDi+1 and aDi ⊃ aDi+1 where mDi and aDi denote the set of all minimal diagnoses and the set of all diagnoses, respectively, that are relevant (for the DPI considered) during iteration i. That is, the set of minimal as well as the set of all diagnoses (w.r.t. the input DPI) is reduced to a proper subset after a new test case has been added. For DYNAMICHS, (for arbitrary iteration i of Algorithm 5) we have that generally mDi 6⊃ mDi+1, but still aDi ⊃ aDi+1, where mDi and aDi are defined as above. That is, not only might some minimal diagnoses (w.r.t. the last-but-one DPI) be invalidated, but also some new ones (w.r.t. the current DPI) might originate from the incorporation of the information given by a query answer.\nConcerning minimal conflict sets, the set of all (or: relevant) minimal conflict sets does not change throughout a debugging session by means of STATICHS, i.e. mCi = mCi+1 (for arbitrary iteration i of Algorithm 5) where mCi is the set of minimal conflict sets relevant (for the DPI considered) during iteration i. This holds since the minimal conflict sets w.r.t. the input DPI are artificially fixed (see above). On the contrary, the assignment of a new test case using DYNAMICHS involves the reduction of some minimal conflict sets (w.r.t. the last-but-one DPI) to smaller subset conflict sets (w.r.t. the current DPI) and/or the introduction of some “completely new” minimal conflict sets (which are in no subset-relation with existing ones, cf. Section 6.2.1). These results are summarized by the categories “set of all X upon addition of a test case” in Table 6.2.\nThird Segment of Table 6.2 – Hitting Set Tree Construction, Pruning, Complexity and Query Generation. Regarding the constructed hitting set tree, we have explained that STATICHS builds a wpHStree (see Definition 4.10 on page 64) just as the HS method which is employed for diagnosis computation in the presented non-interactive KB debugging scenario (Algorithm 3). The main differences between Algorithm 5 in static mode and Algorithm 3 are, first, that the former constructs the wpHS-tree step-bystep in multiple phases. Between each two phases a query is generated and presented to the user. The latter, by contrast, finishes the tree construction (to the extent as prescribed by the given parameters nmin, nmax and t, see Section 4.6) before a single most probable automatically selected solution or a set of solutions is displayed to the user. Second, the tree constructed by the interactive static algorithm exhibits a different labeling of leaf nodes than the one built up be the non-interactive algorithm. In the former, some leaf nodes might be labeled by × indicating that the path to this node is a minimal diagnosis w.r.t. the input DPI, but one which is not in accordance with all answered queries. Notice that such invalidated diagnoses cannot be simply deleted in favor of memory savings, but must be stored in order for the non-minimality criterion (lines 21-23) to function properly which is necessary to preserve the property of STATICHS to compute only minimal diagnoses. In the non-interactive wpHS-tree, on the other hand, all minimal diagnoses w.r.t. the input DPI are labeled by X.\nWhat the interactive static and the non-interactive tree have in common is the usage of only minimal conflict sets w.r.t. the input DPI as labels of internal (i.e. non-leaf) nodes and the adherence to the “standard” pruning rules [60] as per Definition 4.8 on page 50, i.e. the immediate deletion of non-minimal\nCHAPTER 6. ITERATIVE DIAGNOSIS COMPUTATION 166\nand duplicate tree paths. Except for the standard pruning actions that take place during tree expansion, no separate pruning phases are performed by STATICHS. The reason for this is the fixation of the minimal conflict sets, i.e. the consideration of only minimal conflict sets w.r.t. the input DPI. Incorporation of new minimal conflict sets resulting from answered queries would generally negate completeness of STATICHS w.r.t. the exploration of all minimal diagnoses w.r.t. the input DPI. Integration of new conflict sets that are subsets of existing ones, however, is the key to more substantial pruning actions carried out by DYNAMICHS.\nDue to the more or less equivalent construction of both the tree built up by STATICHS and the one constructed by the HS method in the non-interactive algorithm, it is straightforward to recognize that the worst case time and space complexity of both tree computations (without taking into the account other actions performed by the interactive algorithm like probability updates and query generations) are equal. By worst case complexity we refer to the complexity of the search for the (exact) solution solution of the Interactive Static KB Debugging Problem on the one hand and the complexity of enumerating all minimal diagnoses w.r.t. the input DPI on the other hand. In particular, the complexity of tree construction in static KB debugging is independent of given parameters such as the ones for leading diagnoses computation (nmin, nmax and t) and of the test cases that are classified positively or negatively, respectively, during the debugging session.\nTo sum up, due to the artificial fixation of the solution set, there is no possibility of tree pruning in static KB debugging except for the standard pruning rules and hence no way to escape the generally immense worst case complexity for diagnosis search in case σ = 0.\nThe hitting set tree constructed by DYNAMICHS, on the other hand, might differ significantly from the wpHS-tree produced by the non-interactive algorithm. First, it uses minimal conflict sets w.r.t. the current DPI to label internal nodes in the tree during each expansion stage. Since minimal conflict sets can only “shrink” and not “grow” due to the integration of test cases into a DPI, the finding that by now a subset of a former minimal conflict set (w.r.t. some previous DPI) is already a minimal conflict set (w.r.t. the current DPI) gives rise to very powerful ways of tree pruning, as we illustrated by Example 6.4. In this vein, the evolution of the tree produced by DYNAMICHS can be characterized by alternating expansion and pruning stages. A pruning stage takes place after a test case has been added to the last-but-one DPI in order to modify the tree Ti used to search for minimal diagnoses w.r.t. the last-but-one DPI to obtain a tree Ti+1 that enables the discovery of all minimal diagnoses w.r.t. the current DPI. Concretely, both pre-pruning as well as post-pruning is possible during a pruning phase. Pre-pruning refers to the deletion of tree paths ending in an open leaf node, i.e. paths corresponding to partial diagnoses, and post-pruning refers to the deletion of tree paths ending in a closed node, i.e. paths corresponding to (minimal or nonminimal) diagnoses. Both pre- and post-pruning are not possible in STATICHS. The ability for significant tree pruning comes at the cost of not being able to exploit the standard pruning rules as STATICHS does. For, non-minimal diagnoses and duplicate tree paths must be stored to guarantee the proper working of tree pruning and in further consequence the completeness of minimal diagnoses search for each current DPI.\nAs we pointed out in Section 6.2.1, the test cases specified during the dynamic debugging session and the defined leading diagnoses computation parameters nmin, nmax and t might have a material influence on the extent of possible tree pruning on the one hand and the extent of undesired tree growth on the other. Thence, worst case time and space complexity of the tree generation by means of DYNAMICHS cannot be initially (at least theoretically) quantified as in the case of STATICHS. Consequently, significant savings as well as a substantial overhead compared to STATICHS are possible. Careful “control” of certain properties of asked queries (added test cases) might help to keep considerable unwanted tree growth within bounds, as we touched upon in Section 6.2.1 and will elaborate on in future work.\nNevertheless, we want to mention a shortcoming of STATICHS compared to DYNAMICHS. Namely, for σ = 0, STATICHS must enumerate all minimal diagnoses w.r.t. the input DPI (otherwise no diagnosis can have a probability of 1, see the proof of Proposition 5.16 in Section 5.3.4) whereas DYNAMICHS\nCHAPTER 6. ITERATIVE DIAGNOSIS COMPUTATION 167\nmight be able to obtain some extended DPI (by the addition of test cases) soon for which only one minimal diagnosis exists. This might require the computation of only a small fraction of the number of |mDinputDPI | minimal diagnoses that STATICHS must determine and therefore might be substantially more time and space saving than figuring out all minimal diagnoses w.r.t. some DPI. This is quite well illustrated by Examples 6.2 and 6.4.\nRegarding query generation, we explained in Remark 6.2 on page 126 that queries in STATICHS are computed w.r.t. the current DPI albeit only minimal diagnoses w.r.t. the input DPI (which are at the same time minimal diagnoses w.r.t. the current DPI, cf. bullet (a) on page 109) are considered and calculated by Algorithm 5 with mode = static. In the case of dynamic debugging it is clear that queries are computed w.r.t. the current DPI since only minimal diagnoses w.r.t. the current DPI are taken into account.\nChapter 7\nRelated Work\nTo the best of our knowledge no interactive KB debugging methods that ask a user automatically selected queries have been proposed to repair faulty (monotonic) KBs so far (except for our own previous works [73, 74, 63, 76]).\nNon-interactive debugging methods for KBs (ontologies) are introduced in [68, 38, 19]. Ranking of diagnoses and proposing a “best” diagnosis is presented in [39]. This method uses a number of measures such as (a) the frequency with which a formula appears in conflict sets, (b) the impact on the KB in terms of its “lost” entailments when some formula is modified or removed, (c) provenance information about the formula and (d) syntactic relevance of a formula. All these measures are evaluated for each formula in a conflict set. The scores are then combined in a rank value which is associated with the corresponding formula. These ranks are then used by a modified hitting set tree algorithm that identifies diagnoses with a minimal rank. In this work no query generation and selection strategy is proposed if the intended diagnosis cannot be determined reliably with the given a-priori knowledge. In our work additional information is acquired until the minimal diagnosis with the intended semantics can be identified with confidence. In general, the work of [39] can be combined with the approaches presented in our work as ranks of logical formulas can be taken into account together with other observations for calculating the prior probabilities of minimal diagnoses (see Section 4.5.1).\nThe idea of selecting the next query based on certain query selection measures was exploited in the generation of decisions trees [58] and for selecting measurements in the model-based diagnosis of circuits [44] (in both works, the minimal expected entropy measure was used). We extended these methods to query selection in the domain of KB debugging [73] and devised further query selection measures [74, 63].\nAn approach for the debugging of faulty aligned KBs (ontologies) was proposed by [46]. An aligned KB is the union of two KBs K1 and K2 and an alignment A1,2 (which is properly formatted as a set of logical formulas, cf. Definition 18 in [46]). A1,2 is a set of correspondences (each with an associated automatically computed confidence value) produced by an automatic system (an ontology matcher) given K1 and K2 as inputs where each correspondence represents a (possible) semantic relationship between a term occurring in K1 and a term occurring in K2. The goal of a debugging system for faulty aligned KBs is usually the determination of a subset of the alignment A′1,2 ⊂ A1,2 such that the aligned KB using A′1,2 is not faulty. In terms of our approaches, this corresponds to the setting K := A1,2 and B := K1 ∪ K2. We have already shown in [62, 72] that our systems can also be applied for fault localization in aligned KBs. The work of [46] describes approximate algorithms for computing a “local optimal diagnosis” and complete methods to discover a “global optimal diagnosis”. Optimality in this context refers to the maximum sum of confidences in the resulting repaired alignment A′1,2. In contrast to our framework, diagnoses are determined automatically without support for user interaction. Instead,\n168\nCHAPTER 7. RELATED WORK 169\n[46] demonstrates techniques for the manual revision of the alignment as a procedure independent from debugging. Another difference to our approach is the way of detecting sources of faults. We rely on a divide-and-conquer algorithm [36] for the identification of a minimal conflict set C ⊆ A1,2 (in [46] C is called a MIPS, cf. [19, 68]). In the worst case the method we use exhibits only O(|C| ∗ log(|A1,2|/|C|)) calls of some function that performs a check for faults in a KB and internally uses a reasoner (in our case ISKBVALID, see Algorithm 1). The “shrink” strategy applied in [46] (which is similar to the “expandand-shrink” method used in [38]), on the other hand, requires a worst case number of O(|A1,2|) calls to such a function. Empirical evaluations and a theoretical analysis of the best and worst case complexity of the “expand-and-shrink” method compared to the divide-and-conquer method performed in [75] revealed that the latter is preferable over the former. It should be noted that a similar divide-and-conquer method as used in our work could most probably be also plugged into the system in [46] instead of the “shrink” method.\nThere are some ontology matchers which incorporate alignment repair features: CODI [30], YAM++ [51], ASMOV [32] and KOSIMap [61], for instance, employ logic-based techniques to search for a set of predefined “anti-patterns” which must not occur in the aligned ontology, either to avoid inconsistencies or incoherencies or to eliminate unwanted or redundant entailments. In case such a pattern is revealed, it is resolved by eliminating from the alignment some correspondences responsible for its occurrence. All the techniques incorporated in these matchers are distinct from the presented approaches in that they implement incomplete or approximate methods of alignment repair, i.e. not all alternative solutions to the alignment debugging problem are taken into account. As a consequence of this, on the one hand, the final alignment produced by these systems may still trigger faults in the aligned KB. On the other hand, a suboptimal solution may be found, e.g. in terms of the user-intended semantics w.r.t. the aligned ontology or other criteria such as alignment confidence or cardinality.\nAnother ontology matcher, LogMap 2 [34], provides integrated debugging features and the opportunity for a user to interact during this process. However, the system is not really comparable with ours since it is very specialized and dedicated to the goal of producing a fault-free alignment. Concretely, there are at least two differences to our approach. First, LogMap 2 uses incomplete reasoning mechanisms in order to speed up the matching process. Hence, the output is not guaranteed to be fault-free. Second, the option for user interaction aims in fact at the revision of a set of correspondences, i.e. the sequential assessing of single correspondences as ’faulty’ or ’correct’. Our approach, on the contrary, asks the user queries (i.e. entailments of non-faulty parts of the KB).\nAn interactive technique similar to our approaches was presented in [52], where a user is successively asked single KB formulas (ontology axioms) in order to obtain a partition of a given ontology into a set of desired or correct and a set of undesired or incorrect formulas. Whereas our strategies aim at finding a parsimonious solution involving minimal change to the given faulty KB in order to repair it, the method proposed in [52] pursues a (potentially) more invasive approach to KB quality assurance, namely a (reasoner-supported) exhaustive manual inspection of (parts of) a KB. Given an inconsistent/incoherent KB, this technique starts from an empty set of desired formulas aiming at adding to this set only correct formulas of the KB which preserve consistency and coherency. Our approach, on the other hand, works its way forward the other way round in that it starts from the complete KB aiming at finding a minimal set of formulas to be deleted or modified which are responsible for the violation of the pre-specified requirements. Another difference of our approach compared to the one suggested in [52] is the type of queries asked to the user and the way these are selected. Our method allows for the generation of queries which are not explicit formulas in the KB, but implicit consequences of non-faulty parts of the KB. Besides, the set of selectable queries in our approach differs from one iteration to the next due to the changing set of leading diagnoses whereas queries (i.e. KB formulas) in [52] are known in advance and the challenge is to figure out the best ordering of formulas to be assessed by the user. Whereas we apply mostly information theoretic measures (e.g. the minimal expected entropy in the set of leading diagnoses after a query has been answered), the authors in [52] employ “impact measures” which, roughly speaking,\nCHAPTER 7. RELATED WORK 170\nindicate the number of automatically classifiable formulas in case of positive and, respectively, negative classification of a query (i.e. a particular formula).\nChapter 8\nSummary and Future Work\nSummary\nIn this work we motivated why appropriate tool assistance is a must when it comes to repairing faulty KBs. For, KBs that do not satisfy some minimal quality criteria such as logical consistency can make artificial intelligence applications relying on the domain knowledge modeled by this KB completely useless. In such a case, no meaningful reasoning or answering of queries about the domain is possible.\nNon-interactive debugging systems published in research literature often cannot localize all possible faults (incompleteness), suggest the deletion or modification of unnecessarily large parts of the KB (nonminimality), return incorrect solutions which lead to a repaired KB not satisfying the imposed quality requirements (unsoundness) or suffer from poor scalability due to the inherent complexity of the KB debugging problem [81]. Even if a system is complete and sound and considers only minimal solutions, there are generally exponentially many solution candidates to select one from. However, any two repaired KBs obtained from these candidates differ in their semantics in terms of entailments and non-entailments. Selection of just any of these repaired KBs might result in unexpected entailments, the loss of desired entailments or unwanted changes to the KB which in turn might cause unexpected new faults during the further development or application of the repaired KB. Also, manual inspection of a large set of solution candidates can be time-consuming (if not practically infeasible), tedious and error-prone since human beings are normally not capable of fully realizing the semantic consequences of deleting a set of formulas from a KB.\nTo account for this issue, we evolved a comprehensive theory on which provably complete, sound and optimal (in terms of given probability information) interactive KB debugging systems can be built which suggest only minimal changes to repair a present KB. Interaction with a user is realized by asking the user queries. That is, a conjunction of logical formulas must be classified either as an intended or a non-intended entailment of the correct KB. To construct a query, only a minimal set of two solution candidates must be available. After the answer to a query is known, the search space for solutions is pruned. Iteration of this process until there is only a single solution candidate left yields a (repaired) solution KB which features exactly the semantics desired and expected by the user.\nWe presented algorithms for the computation of minimal conflict sets, i.e. irreducible faulty subsets of the KB, and for the computation of minimal diagnoses, i.e. irreducible sets of KB formulas that must be properly modified or deleted in order to repair the KB. We combined these algorithms with methods that derive probabilities of diagnoses from meta information about faults (e.g. the outcome of a statistical analysis) to constitute a non-interactive debugging system for monotonic KBs which computes minimal diagnoses in best-first order. Building on the idea of this non-interactive method, we devised a complete and sound best-first algorithm for the interactive debugging of monotonic KBs that allows a user to take part in the debugging process in order to figure out the best solution.\n171\nCHAPTER 8. SUMMARY AND FUTURE WORK 172\nIn order to integrate the new information collected by successive consultations of the user, the diagnoses computation in an interactive system must be regularly stopped. That is, there must be alternating phases, on the one hand for the further exploration of the solution space in order to gain new evidence for query generation and on the other hand for user interaction. To this end, we proposed two new strategies for the iterative computation of minimal diagnoses that exactly serve this purpose. The first strategy, STATICHS, takes advantage of an artificial fixation of the solution set which guarantees the monotonic reduction of the solution space independently of the asked queries, the given answers or other parameters of the algorithm. In this vein, the complexity of this algorithm is initially known and the maximum overhead compared to the non-interactive algorithm is polynomially bound.1 On the downside, STATICHS cannot optimally exploit the information given by the answered queries and thus cannot employ powerful methods that enable a more efficient pruning of the solution search space. Such powerful methods can be incorporated by the second suggested strategy, DYNAMICHS, the performance of which can be orders of magnitude better than the (initially fixed) performance of STATICHS in the best case. That is, the ability to fully incorporate the information gained from user interaction might lead to a modified problem instance for which only a single (best) solution exists with only a small fraction of the time, space and user effort needed by STATICHS. Moreover, the (exact) solution located by means of an interactive debugging session applying DYNAMICHS is generally a better (verified) solution than the (exact) solution found by use of STATICHS. However, the complexity of DYNAMICHS depends to a great degree on which queries are generated and which input parameters are chosen and the worst case complexity is not initially bound as in case of STATICHS.\nWe want to point out that this work is unique in that it provides an in-depth theoretical workup of the topic of interactive (monotonic) KB debugging which (to the best of our knowledge) cannot be found in such a detailed fashion in other works. Furthermore, this is the first work that gives precise definitions of the problems addressed in interactive KB debugging. Additionally, it is unique in that it features (new) algorithms that provably solve these interactive KB debugging problems. To account for a tradeoff between solution quality and execution time, these algorithms are equipped with a feature to compute approximate solutions where the goodness of the approximation can be steered by the user. Another unique characteristic of this work is that it deals with an entire system of algorithms that are required for the interactive debugging of monotonic KBs, considers and details all algorithms separately, proves their correctness and demonstrates how all these algorithms are orchestrated to make up a full-fledged and provably correct interactive KB debugging system.\nTopics for Future Work\nThis work has given rise to several questions we will elaborate on in our future work:\nQuery Generation and Selection. Our discussions of the presented query generation methods have revealed some drawbacks (cf. Section 5.2). Albeit being a fixed-parameter tractable problem as argued, the exponential time complexity regarding the number of leading diagnoses |D| in case an optimal query is desired is clearly an aspect that should be improved. This high complexity arises from the paradigm of computing an optimal query w.r.t. some measure qsm() by calculating a (generally exponentially large) pool QP of queries in a first stage, whereupon the best query in QP according to qsm() is filtered out in a second stage.\nA key to solving this issue is the use of a different paradigm that does not rely on the computation of the pool QP. Instead, qualitative measures can be derived from quantitative measures that have been used\n1This holds under the reasonable assumption that, in practice, a debugging session will involve only a polynomial number of queries to an interacting user. Recall that a user can abort the debugging session at any time and select the currently most probable diagnosis as their solution to the debugging problem.\nCHAPTER 8. SUMMARY AND FUTURE WORK 173\nin interactive debugging scenarios [74, 63, 73]. These qualitative measures provide a way to estimate the qsm() value of partial q-partitions, i.e. ones where not all leading diagnoses have been assigned to the respective set in the q-partition yet. In this way a direct search for a query with (nearly) optimal properties is possible. A similar strategy called CKK has been employed in [74] for the information gain measure qsm() := ENT() (see Section 5.3.3). From such a technique we can expect to save a high number of reasoner calls. Because usually only a small subset of q-partitions included in a query pool (of exponential cardinality) is required to find a query with desirable properties if the search is implemented by means of a heuristic that involves the exploration of seemingly favorable (potential) queries and (partial) q-partitions, respectively, first.\nAnother shortcoming of the paradigm of query pool generation and subsequent selection of the best query is the extensive use of reasoning services which may be computationally expensive (depending on the given DPI). Instead of computing a set of common entailments Q of a set of KBs K∗i first and consulting a reasoner to fill up the (q-)partition for Q in order to test whether Q is a query at all (see Section 5.2), the idea enabling a significant reduction of reasoner dependence is to compute some kind of canonical query without a reasoner and use simple set comparisons to decide whether the associated partition is a q-partition. Guided by qualitative properties mentioned before, a search for such q-partition with desirable properties can be accomplished without reasoning at all. Also, a set-minimal version of the optimal canonical query can be computed without reasoning aid. Only for the optional enrichment of the identified optimal canonical query by additional entailments and for the subsequent minimization of the enriched query, the reasoner may be employed. We will present strategies accounting for these ideas in the near future.\nAnother aspect that can be improved is that only one minimized version of each query is computed by Algorithm 4. That is, per q-partition P, there might be some set-minimal queries which do not occur in the output set QP. From the point of view of how well a query might be understood by an interacting user, of course not all minimized queries can be assumed equally good in general. For instance, consider the minimized queries Q4 and Q10 in Table 5.3 on page 96. Both are equally good regarding their qpartitions (just the sets D+ and D− are commuted), but most people will probably agree that Q4 is much easier to comprehend from the logical point of view and thus much easier to answer.\nHence, in order to avoid a situation where a potentially best-understood query w.r.t. P is not included in QP, the query minimization process (see Section 5.2.3) might be adapted to take into account some information about faults the interacting user is prone to. This could be exploited to estimate how well this user might be able to understand and answer a query. For instance, given that the user frequently has problems to apply ∃ in a correct manner to express what they intend to express, but has never made any mistakes in formulating implications→, then the query Q1 = {∀X p(X)→ q(X), r(a)} might be better comprehended than Q2 = {∀X∃Y s(X,Y )}. One way to achieve the finding of a well-understood query for some q-partition P is to run the query minimization MINQ more than once, each time with a modified input (using a hitting set tree to accomplish this in a systematic manner – cf. Chapter 4, where an analogue idea is used to compute different minimal conflict sets w.r.t. a DPI). In this way, different set-minimal queries for P can be identified and the process can be stopped when a suitable query is found.\nIn order to come up with such a strategy, however, one must first gain insight into how well a user might understand certain logical formalisms and what properties make a query easy to comprehend from the logical perspective. It is planned to gather corresponding data about different users in the scope of a user study and to utilize the results to achieve a model of “query hardness” (by sticking to a similar overall methodology as used in [24]) in order to come up with strategies for the determination of minimal queries that are easily understood. Note that such a model could also act as a guide how to specify the initial fault probabilities of syntactical elements that are used to obtain diagnoses probabilities (see Section 4.5).\nUsage of “Positive-Impact” Queries in Combination with DYNAMICHS. As we discussed in Section 6.2.1 in the context of Algorithm 5 in dynamic mode, an added test case might give rise to some\nCHAPTER 8. SUMMARY AND FUTURE WORK 174\npruning steps as well as it might induce the construction of new subtrees (where “new” means that these would be no subtress of a hitting set tree w.r.t. the DPI not including this test case). The latter situation occurs when “completely new” minimal conflict sets (those that are in no subset-relationship with existing ones) are introduced by the addition of a test case. If this is the only impact of a test case, then this test case has only a negative influence on the time and space complexity of Algorithm 5 using DYNAMICHS. In other words, none of the invalidated minimal diagnoses (and no other nodes in the tree) are redundant, but all of them must additionally hit the set of “completely new” minimal conflict sets (in order to become diagnoses w.r.t. new DPI). Hence, in this case, the transition from one DPI to another including this test case results only in monotonic growth of the tree. If possible, such “negative-impact test cases” must be avoided. On the other hand, one must strive for the usage of “positive-impact test cases”, i.e. those that only trigger tree pruning, but no tree expansion. Defining and studying properties that constitute such “positive-impact test cases” and “negative-impact test cases”, respectively, and developing specialized algorithms for extracting exactly those types of queries that enable as substantial and effective pruning as possible in the context of DYNAMICHS is part of our already ongoing research. Note that a rough intuition of which properties make out a “positive-impact test case” is illustrated on the basis of an example in Section 6.2.1.\nFinding the Right Expert to Answer a Query in a Collaborative KB Development Setting. As we mentioned in Chapter 1, there are collaborative KB development projects such as the OBO Project2 and the NCI Thesaurus3, where many different people contribute to the specification of their knowledge in large KBs. In such a setting, it may be hard to decide who is the person that has the highest chance of being able to answer a concrete query correctly. The idea in such a scenario could be to use a combination of different measures such as educational level (e.g. professor versus PhD student) or hierarchy of contributors (e.g. senior user versus regular user), statistical information about past faults of a contributor (e.g., how many of the formulas originally authored by a person have been corrected by other persons of higher educational level) or provenance information regarding terms occurring in the query (who has authored most of the formulas in which these terms occur?) in order to learn an “expert model” and use it to devise some kind of recommender system [31] that suggests which person to ask a particular query.\nOnce established, such an expert model together with provenance information of KB formulas and other types of information discussed in Section 4.5.1 could also be exploited when it comes to the definition of the fault information provided as input to our debugging system. An example of a system which enables the remote collaborative development of KBs (ontologies) and also provides logs of interesting usage data such as formula change logs and provenance information is Web Protégé [85].\n2http://obo.sourceforge.net 3http://nciterms.nci.nih.gov/ncitbrowser"
    } ],
    "references" : [ {
      "title" : "Constraint-based Debugging of Spreadsheets",
      "author" : [ "R. Abreu", "A. Riboira", "F. Wotawa" ],
      "venue" : "In: CIbSE. pp",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2012
    }, {
      "title" : "Pushing the EL envelope",
      "author" : [ "F. Baader", "S. Brandt", "C. Lutz" ],
      "venue" : "In: IJCAI. pp",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2005
    }, {
      "title" : "eds.): The Description Logic Handbook: Theory, Implementation, and Applications",
      "author" : [ "F. Baader", "D. Calvanese", "D.L. McGuinness", "D. Nardi", "P.F. Patel-Schneider" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2007
    }, {
      "title" : "Axiom Pinpointing in General Tableaux",
      "author" : [ "F. Baader", "R. Penaloza" ],
      "venue" : "Journal of Logic and Computation",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2008
    }, {
      "title" : "On the relative expressiveness of description logics and predicate logics",
      "author" : [ "A. Borgida" ],
      "venue" : "Artificial Intelligence 82(1-2),",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1996
    }, {
      "title" : "The computational complexity of abduction",
      "author" : [ "T. Bylander", "D. Allemang", "M. Tanner", "J. Josephson" ],
      "venue" : "Artificial Intelligence",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1991
    }, {
      "title" : "Sources of error in syllogistic reasoning",
      "author" : [ "J. Ceraso", "A. Provitera" ],
      "venue" : "Cognitive Psychology 2(4),",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1971
    }, {
      "title" : "What you always wanted to know about Datalog (and never dared to ask)",
      "author" : [ "S. Ceri", "G. Gottlob", "L. Tanca" ],
      "venue" : "IEEE Transactions on Knowledge and Data Engineering",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1989
    }, {
      "title" : "R.C.T.: Symbolic Logic and Mechanical Theorem Proving",
      "author" : [ "C.L. Chang", "Lee" ],
      "venue" : null,
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1973
    }, {
      "title" : "An unsolvable problem of elementary number theory",
      "author" : [ "A. Church" ],
      "venue" : "American Journal of Mathematics pp",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1936
    }, {
      "title" : "Pattern-based OWL Ontology Debugging Guidelines",
      "author" : [ "O. Corcho", "C. Roussey", "Vilches Blázquez", "L. Manuel", "I. Pérez" ],
      "venue" : "(eds.) Workshop on Ontology Patterns (WOP 2009), collocated with the 8th International Semantic Web Conference (ISWC",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2009
    }, {
      "title" : "Fixed-parameter tractability and completeness I: Basic results",
      "author" : [ "R.G. Downey", "M.R. Fellows" ],
      "venue" : "SIAM Journal on Computing",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 1995
    }, {
      "title" : "A Decomposition-Based Approach to OWL DL Ontology Diagnosis",
      "author" : [ "J. Du", "G. Qi", "J.Z. Pan", "Y.D. Shen" ],
      "venue" : "Proceedings of 23rd IEEE International Conference on Tools with Artificial Intelligence. pp",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2011
    }, {
      "title" : "Probability: Theory and Examples, Fourth Edition",
      "author" : [ "R. Durrett" ],
      "venue" : null,
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2010
    }, {
      "title" : "Final results of the Ontology Alignment Evaluation Initiative",
      "author" : [ "J. Euzenat", "A. Ferrara", "W.R. van Hage", "L. Hollink", "C. Meilicke", "A. Nikolov", "D. Ritze", "F. Scharffe", "P. Shvaiko", "H. Stuckenschmidt", "O. Sváb-Zamazal", "C.T. dos Santos" ],
      "venue" : "Proceedings of the 6th International Workshop on Ontology Matching",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2011
    }, {
      "title" : "Consistency-based diagnosis of configuration knowledge bases",
      "author" : [ "A. Felfernig", "G. Friedrich", "D. Jannach", "M. Stumptner" ],
      "venue" : "Artificial Intelligence 152(2),",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2004
    }, {
      "title" : "An efficient diagnosis algorithm for inconsistent constraint sets. Artificial Intelligence for Engineering Design, Analysis and Manufacturing",
      "author" : [ "A. Felfernig", "M. Schubert", "C. Zehentner" ],
      "venue" : "(Jun",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2011
    }, {
      "title" : "A General Diagnosis Method for Ontologies",
      "author" : [ "G. Friedrich", "K. Shchekotykhin" ],
      "venue" : "Proceedings of the 4th International Semantic Web Conference (ISWC",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2005
    }, {
      "title" : "Model-based diagnosis of hardware designs",
      "author" : [ "G. Friedrich", "M. Stumptner", "F. Wotawa" ],
      "venue" : "Artif. Intell. 111(1-2),",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 1999
    }, {
      "title" : "OWL 2: The next step for OWL",
      "author" : [ "B.C. Grau", "I. Horrocks", "B. Motik", "B. Parsia", "P.F. Patel-Schneider", "U. Sattler" ],
      "venue" : "Web Semantics: Science, Services and Agents on the World Wide Web",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2008
    }, {
      "title" : "A correction to the algorithm in Reiter’s theory of diagnosis",
      "author" : [ "R. Greiner", "B. Smith", "R. Wilkerson" ],
      "venue" : "Artificial Intelligence 41(1),",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 1989
    }, {
      "title" : "Justification based Explanation in Ontologies",
      "author" : [ "M. Horridge" ],
      "venue" : "Ph.D. thesis,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2011
    }, {
      "title" : "The cognitive complexity of OWL justifications",
      "author" : [ "M. Horridge", "S. Bail", "B. Parsia" ],
      "venue" : "Proceedings of the 10th International Semantic Web Conference (ISWC",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2011
    }, {
      "title" : "Laconic and Precise Justifications in OWL",
      "author" : [ "M. Horridge", "B. Parsia", "U. Sattler" ],
      "venue" : "Proceedings of the 7th International Semantic Web Conference (ISWC",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2008
    }, {
      "title" : "Lemmas for Justifications in OWL. In: Proceedings of the 22nd Workshop of Description Logics DL2009",
      "author" : [ "M. Horridge", "B. Parsia", "U. Sattler" ],
      "venue" : "CEUR Workshop Proceedings",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2009
    }, {
      "title" : "Justification Oriented Proofs in OWL",
      "author" : [ "M. Horridge", "B. Parsia", "U. Sattler" ],
      "venue" : "Proceedings of the 9th International Semantic Web Conference (ISWC",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2010
    }, {
      "title" : "Extracting justifications from BioPortal ontologies",
      "author" : [ "M. Horridge", "B. Parsia", "U. Sattler" ],
      "venue" : "Proceedings of the 11th International Semantic Web Conference (ISWC",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2012
    }, {
      "title" : "Justification Masking in Ontologies",
      "author" : [ "M. Horridge", "B. Parsia", "U. Sattler" ],
      "venue" : "Thirteenth International Conference on the Principles of Knowledge Representation and Reasoning",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2012
    }, {
      "title" : "CODI: Combinatorial Optimization for Data Integration - Results for OAEI",
      "author" : [ "J. Huber", "T. Sztyler", "J. Noessner", "C. Meilicke" ],
      "venue" : "Proceedings of the 6th International Workshop on Ontology Matching",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2011
    }, {
      "title" : "Recommender Systems: An Introduction",
      "author" : [ "D. Jannach", "M. Zanker", "A. Felfernig", "G. Friedrich" ],
      "venue" : null,
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2010
    }, {
      "title" : "Ontology Matching with Semantic Verification",
      "author" : [ "Y.R. Jean-Mary", "E.P. Shironoshita", "M.R. Kabuka" ],
      "venue" : "Web Semantics: Science, Services and Agents on the World Wide Web",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2009
    }, {
      "title" : "Logmap: Logic-based and scalable ontology matching",
      "author" : [ "E. Jiménez-Ruiz", "B.C. Grau" ],
      "venue" : "Proceedings of the 10th International Semantic Web Conference (ISWC",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2011
    }, {
      "title" : "Large-scale interactive ontology matching: Algorithms and implementation",
      "author" : [ "E. Jiménez-Ruiz", "B.C. Grau", "Y. Zhou", "I. Horrocks" ],
      "venue" : "Proceedings of 20th European Conference on Artificial Intelligence",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2012
    }, {
      "title" : "Deductive reasoning",
      "author" : [ "P.N. Johnson-Laird" ],
      "venue" : "Annual review of psychology 50,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 1999
    }, {
      "title" : "QUICKXPLAIN: Preferred Explanations and Relaxations for Over-Constrained Problems",
      "author" : [ "U. Junker" ],
      "venue" : "Proceedings of the Nineteenth National Conference on Artificial Intelligence, Sixteenth Conference on Innovative Applications of Artificial Intelligence",
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 2004
    }, {
      "title" : "Debugging and Repair of OWL Ontologies",
      "author" : [ "A. Kalyanpur" ],
      "venue" : "Ph.D. thesis, University of Maryland,",
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 2006
    }, {
      "title" : "Finding all Justifications of OWL DL Entailments",
      "author" : [ "A. Kalyanpur", "B. Parsia", "M. Horridge", "E. Sirin" ],
      "venue" : "ISWC 2007 + ASWC 2007. LNCS,",
      "citeRegEx" : "38",
      "shortCiteRegEx" : "38",
      "year" : 2007
    }, {
      "title" : "Repairing Unsatisfiable Concepts in OWL Ontologies",
      "author" : [ "A. Kalyanpur", "B. Parsia", "E. Sirin", "B. Cuenca-Grau" ],
      "venue" : "The Semantic Web: Research and Applications, 3rd European Semantic Web Conference,",
      "citeRegEx" : "39",
      "shortCiteRegEx" : "39",
      "year" : 2006
    }, {
      "title" : "Swoop: A Web Ontology Editing Browser",
      "author" : [ "A. Kalyanpur", "B. Parsia", "E. Sirin", "B.C. Grau", "J. Hendler" ],
      "venue" : "J. Web Sem",
      "citeRegEx" : "40",
      "shortCiteRegEx" : "40",
      "year" : 2006
    }, {
      "title" : "Debugging Unsatisfiable Classes in OWL Ontologies",
      "author" : [ "A. Kalyanpur", "B. Parsia", "E. Sirin", "J. Hendler" ],
      "venue" : "Web Semantics: Science, Services and Agents on the World Wide Web 3(4),",
      "citeRegEx" : "41",
      "shortCiteRegEx" : "41",
      "year" : 2005
    }, {
      "title" : "SRIQ and SROIQ are harder than SHOIQ",
      "author" : [ "Y. Kazakov" ],
      "venue" : "Proceedings of the 21st Workshop of Description Logics",
      "citeRegEx" : "42",
      "shortCiteRegEx" : "42",
      "year" : 2008
    }, {
      "title" : "The incredible ELK",
      "author" : [ "Y. Kazakov", "M. Krötzsch", "F. Simančík" ],
      "venue" : "Journal of automated reasoning 53(1),",
      "citeRegEx" : "43",
      "shortCiteRegEx" : "43",
      "year" : 2014
    }, {
      "title" : "Diagnosing multiple faults",
      "author" : [ "J. de Kleer", "B.C. Williams" ],
      "venue" : "Artificial Intelligence",
      "citeRegEx" : "44",
      "shortCiteRegEx" : "44",
      "year" : 1987
    }, {
      "title" : "Logik für Informatiker",
      "author" : [ "M. Kreuzer", "S. Kühling" ],
      "venue" : "Pearson Studium, München, Germany",
      "citeRegEx" : "45",
      "shortCiteRegEx" : "45",
      "year" : 2006
    }, {
      "title" : "Alignment Incoherence in Ontology Matching",
      "author" : [ "C. Meilicke" ],
      "venue" : "Ph.D. thesis, Universität Mannheim",
      "citeRegEx" : "46",
      "shortCiteRegEx" : "46",
      "year" : 2011
    }, {
      "title" : "An Efficient Method for Computing Alignment Diagnoses",
      "author" : [ "C. Meilicke", "H. Stuckenschmidt" ],
      "venue" : "Proceedings of the 3rd International Conference on Web Reasoning and Rule Systems",
      "citeRegEx" : "47",
      "shortCiteRegEx" : "47",
      "year" : 2009
    }, {
      "title" : "Repairing Ontology Mappings",
      "author" : [ "C. Meilicke", "H. Stuckenschmidt", "A. Tamilin" ],
      "venue" : "Proceedings of the 22nd National Conference on Artificial intelligence -",
      "citeRegEx" : "48",
      "shortCiteRegEx" : "48",
      "year" : 2007
    }, {
      "title" : "Introduction to Mathematical Logic, Fifth Edition",
      "author" : [ "E. Mendelson" ],
      "venue" : null,
      "citeRegEx" : "49",
      "shortCiteRegEx" : "49",
      "year" : 2009
    }, {
      "title" : "OWL 2 Web Ontology Language Structural Specification and Functional-Style Syntax",
      "author" : [ "B. Motik", "P.F. Patel-Schneider", "B. Parsia" ],
      "venue" : "W3C recommendation pp",
      "citeRegEx" : "50",
      "shortCiteRegEx" : "50",
      "year" : 2009
    }, {
      "title" : "YAM++ - A combination of graph matching and machine learning approach to ontology alignment task",
      "author" : [ "D. Ngo", "Z. Bellahsene" ],
      "venue" : "Journal of Web Semantics - The Semantic Web Challenge",
      "citeRegEx" : "51",
      "shortCiteRegEx" : "51",
      "year" : 2011
    }, {
      "title" : "Interactive Ontology Revision",
      "author" : [ "N. Nikitina", "S. Rudolph", "B. Glimm" ],
      "venue" : "Web Semantics: Science, Services and Agents on the World Wide Web 12-13,",
      "citeRegEx" : "52",
      "shortCiteRegEx" : "52",
      "year" : 2012
    }, {
      "title" : "A framework for ontology evolution in collaborative environments",
      "author" : [ "N.F. Noy", "A. Chugh", "W. Liu", "M.A. Musen" ],
      "venue" : "Proceedings of the 5th International Semantic Web Conference",
      "citeRegEx" : "53",
      "shortCiteRegEx" : "53",
      "year" : 2006
    }, {
      "title" : "Creating Semantic Web Contents with Protégé-2000",
      "author" : [ "N.F. Noy", "M. Sintek", "S. Decker", "M. Crubézy", "R.W. Fergerson", "M.A. Musen" ],
      "venue" : "IEEE Intelligent Systems",
      "citeRegEx" : "54",
      "shortCiteRegEx" : "54",
      "year" : 2000
    }, {
      "title" : "Debugging OWL ontologies",
      "author" : [ "B. Parsia", "E. Sirin", "A. Kalyanpur" ],
      "venue" : "Proceedings of the 14th international conference on World Wide Web",
      "citeRegEx" : "55",
      "shortCiteRegEx" : "55",
      "year" : 2005
    }, {
      "title" : "OWL Web Ontology Language Semantics and Abstract Syntax",
      "author" : [ "P.F. Patel-Schneider", "P. Hayes", "I Horrocks" ],
      "venue" : "W3C recommendation",
      "citeRegEx" : "56",
      "shortCiteRegEx" : "56",
      "year" : 2004
    }, {
      "title" : "Model-Based Diagnosis or Reasoning from First Principles",
      "author" : [ "B. Peischl", "F. Wotawa" ],
      "venue" : "IEEE Intelligent Systems",
      "citeRegEx" : "57",
      "shortCiteRegEx" : "57",
      "year" : 2003
    }, {
      "title" : "Induction of Decision Trees",
      "author" : [ "J.R. Quinlan" ],
      "venue" : "Machine Learning 1(1),",
      "citeRegEx" : "58",
      "shortCiteRegEx" : "58",
      "year" : 1986
    }, {
      "title" : "OWL Pizzas: Practical Experience of Teaching OWL-DL: Common Errors & Common Patterns",
      "author" : [ "A. Rector", "N. Drummond", "M. Horridge", "J. Rogers", "H. Knublauch", "R. Stevens", "H. Wang", "C. Wroe" ],
      "venue" : "Engineering Knowledge in the Age of the SemanticWeb 14th International Conference,",
      "citeRegEx" : "59",
      "shortCiteRegEx" : "59",
      "year" : 2004
    }, {
      "title" : "A Theory of Diagnosis from First Principles",
      "author" : [ "R. Reiter" ],
      "venue" : "Artificial Intelligence 32(1),",
      "citeRegEx" : "60",
      "shortCiteRegEx" : "60",
      "year" : 1987
    }, {
      "title" : "KOSIMap: Use of Description Logic Reasoning to Align Heterogeneous Ontologies",
      "author" : [ "Q. Reul", "J.Z. Pan" ],
      "venue" : "Proceedings of the 23rd International Workshop on Description Logics DL2010",
      "citeRegEx" : "61",
      "shortCiteRegEx" : "61",
      "year" : 2010
    }, {
      "title" : "RIO: Minimizing User Interaction in Debugging of Aligned Ontologies",
      "author" : [ "P. Rodler", "K. Shchekotykhin", "P. Fleiss", "G. Friedrich" ],
      "venue" : "Proceedings of the 7th International Workshop on Ontology Matching",
      "citeRegEx" : "62",
      "shortCiteRegEx" : "62",
      "year" : 2012
    }, {
      "title" : "RIO: Minimizing User Interaction in Ontology Debugging",
      "author" : [ "P. Rodler", "K. Shchekotykhin", "P. Fleiss", "G. Friedrich" ],
      "venue" : "Web Reasoning and Rule Systems, Lecture Notes in Computer Science,",
      "citeRegEx" : "63",
      "shortCiteRegEx" : "63",
      "year" : 2013
    }, {
      "title" : "A catalogue of OWL ontology antipatterns",
      "author" : [ "C. Roussey", "O. Corcho", "L.M. Vilches-Blázquez" ],
      "venue" : "In: International Conference On Knowledge Capture. pp. 205–206",
      "citeRegEx" : "64",
      "shortCiteRegEx" : "64",
      "year" : 2009
    }, {
      "title" : "Artificial Intelligence: A Modern Approach",
      "author" : [ "S.J. Russell", "P. Norvig" ],
      "venue" : "Pearson Education,",
      "citeRegEx" : "65",
      "shortCiteRegEx" : "65",
      "year" : 2010
    }, {
      "title" : "Enumerating Minimally Revised Specifications Using Dualization",
      "author" : [ "K. Satoh", "T. Uno" ],
      "venue" : "New Frontiers in Artificial Intelligence, Lecture Notes in Computer Science,",
      "citeRegEx" : "66",
      "shortCiteRegEx" : "66",
      "year" : 2006
    }, {
      "title" : "Which Kind of Module Should I Extract",
      "author" : [ "U. Sattler", "T. Schneider", "M. Zakharyaschev" ],
      "venue" : "Proceedings of the 22nd International Workshop on Description Logics. CEUR Workshop Proceedings,",
      "citeRegEx" : "67",
      "shortCiteRegEx" : "67",
      "year" : 2009
    }, {
      "title" : "Debugging Incoherent Terminologies",
      "author" : [ "S. Schlobach", "Z. Huang", "R. Cornet", "F. Harmelen" ],
      "venue" : "Journal of Automated Reasoning 39(3),",
      "citeRegEx" : "68",
      "shortCiteRegEx" : "68",
      "year" : 2007
    }, {
      "title" : "Subsumption in KL-ONE is undecidable",
      "author" : [ "M. Schmidt-Schauß" ],
      "venue" : "Proceedings of the 1st International Conference on Principles of Knowledge Representation and Reasoning",
      "citeRegEx" : "69",
      "shortCiteRegEx" : "69",
      "year" : 1989
    }, {
      "title" : "Abductive and default reasoning: A computational core",
      "author" : [ "B. Selman", "H.L. Levesque" ],
      "venue" : "Proceedings of the 8th National Conference on Artificial Intelligence pp",
      "citeRegEx" : "70",
      "shortCiteRegEx" : "70",
      "year" : 1989
    }, {
      "title" : "Direct computation of diagnoses for ontology alignment",
      "author" : [ "K. Shchekotykhin", "P. Fleiss", "P. Rodler", "G. Friedrich" ],
      "venue" : "Proceedings of the 7th International Workshop on Ontology Matching",
      "citeRegEx" : "72",
      "shortCiteRegEx" : "72",
      "year" : 2012
    }, {
      "title" : "Query strategy for sequential ontology debugging",
      "author" : [ "K. Shchekotykhin", "G. Friedrich" ],
      "venue" : "Proceedings of the 9th International Semantic Web Conference (ISWC",
      "citeRegEx" : "73",
      "shortCiteRegEx" : "73",
      "year" : 2010
    }, {
      "title" : "Interactive Ontology Debugging: Two Query Strategies for Efficient Fault Localization",
      "author" : [ "K. Shchekotykhin", "G. Friedrich", "P. Fleiss", "P. Rodler" ],
      "venue" : "Web Semantics: Science, Services and Agents on the World Wide Web 12-13,",
      "citeRegEx" : "74",
      "shortCiteRegEx" : "74",
      "year" : 2012
    }, {
      "title" : "On Computing Minimal Conflicts for Ontology Debugging",
      "author" : [ "K. Shchekotykhin", "G. Friedrich", "D. Jannach" ],
      "venue" : "MBS 2008 - Workshop on Model-Based Systems",
      "citeRegEx" : "75",
      "shortCiteRegEx" : "75",
      "year" : 2008
    }, {
      "title" : "Sequential diagnosis of high cardinality faults in knowledge-bases by direct diagnosis generation",
      "author" : [ "K. Shchekotykhin", "G. Friedrich", "P. Rodler", "P. Fleiss" ],
      "venue" : "Proceedings of the 21st European Conference on Artificial Intelligence (ECAI",
      "citeRegEx" : "76",
      "shortCiteRegEx" : "76",
      "year" : 2014
    }, {
      "title" : "HermiT : A Highly-Efficient OWL Reasoner",
      "author" : [ "R. Shearer", "B. Motik", "I. Horrocks" ],
      "venue" : "Proc. of the 5th Int. Workshop on OWL: Experiences and Directions (OWLED 2008 EU)",
      "citeRegEx" : "77",
      "shortCiteRegEx" : "77",
      "year" : 2008
    }, {
      "title" : "Pellet: A practical OWL-DL reasoner",
      "author" : [ "E. Sirin", "B. Parsia", "B.C. Grau", "A. Kalyanpur", "Y. Katz" ],
      "venue" : "Web Semantics: Science, Services and Agents on the World Wide Web 5(2),",
      "citeRegEx" : "78",
      "shortCiteRegEx" : "78",
      "year" : 2007
    }, {
      "title" : "Detecting and locating faults in the control software of autonomous mobile robots",
      "author" : [ "G. Steinbauer", "F. Wotawa" ],
      "venue" : "IJCAI International Joint Conference on Artificial Intelligence. pp",
      "citeRegEx" : "79",
      "shortCiteRegEx" : "79",
      "year" : 2005
    }, {
      "title" : "Robust Plan Execution Using Model-Based Reasoning",
      "author" : [ "G. Steinbauer", "F. Wotawa" ],
      "venue" : "Advanced Robotics 23(10),",
      "citeRegEx" : "80",
      "shortCiteRegEx" : "80",
      "year" : 2009
    }, {
      "title" : "Debugging OWL Ontologies - A Reality Check",
      "author" : [ "H. Stuckenschmidt" ],
      "venue" : "Proceedings of the 6th International Workshop on Evaluation of Ontology-based Tools and the Semantic Web Service Challenge (EON)",
      "citeRegEx" : "81",
      "shortCiteRegEx" : "81",
      "year" : 2008
    }, {
      "title" : "A Modularization-Based Approach to Finding All Justifications for OWL DL Entailments",
      "author" : [ "B. Suntisrivaraporn", "G. Qi", "Q. Ji", "P. Haase" ],
      "venue" : "Proceedings of the 7th International Semantic Web Conference (ISWC",
      "citeRegEx" : "82",
      "shortCiteRegEx" : "82",
      "year" : 2008
    }, {
      "title" : "OntoEdit: Collaborative Ontology Development for the Semantic Web",
      "author" : [ "Y. Sure", "M. Erdmann", "J. Angele", "S. Staab", "R. Studer", "D. Wenke" ],
      "venue" : "Proceedings of the 1st International Semantic Web Conference (ISWC",
      "citeRegEx" : "83",
      "shortCiteRegEx" : "83",
      "year" : 2002
    }, {
      "title" : "FaCT++ description logic reasoner: System description",
      "author" : [ "D. Tsarkov", "I. Horrocks" ],
      "venue" : "Proc. of the Int. Joint Conf. on Automated Reasoning (IJCAR",
      "citeRegEx" : "84",
      "shortCiteRegEx" : "84",
      "year" : 2006
    }, {
      "title" : "WebProtégé: A Collaborative Ontology Editor and Knowledge Acquisition Tool for the Web",
      "author" : [ "T. Tudorache", "C. Nyulas", "N.F. Noy", "M.A. Musen" ],
      "venue" : "Semantic Web 4(1),",
      "citeRegEx" : "85",
      "shortCiteRegEx" : "85",
      "year" : 2013
    }, {
      "title" : "On Computable Numbers, with an Application to the Entscheidungsproblem",
      "author" : [ "A.M. Turing" ],
      "venue" : "Proceedings of the London Mathematical Society",
      "citeRegEx" : "86",
      "shortCiteRegEx" : "86",
      "year" : 1937
    }, {
      "title" : "Model-Based Debugging or How to Diagnose Programs Automatically",
      "author" : [ "F. Wotawa", "M. Stumptner", "W. Mayer" ],
      "venue" : "Developments in Applied Artificial Intelligence, Lecture Notes in Computer Science,",
      "citeRegEx" : "87",
      "shortCiteRegEx" : "87",
      "year" : 2002
    } ],
    "referenceMentions" : [ {
      "referenceID" : 8,
      "context" : "Most artificial intelligence applications rely on knowledge that is encoded in a knowledge base (KB) by means of some logical knowledge representation language such as propositional logic [10], datalog [9], first-order logic (FOL) [10], The Web Ontology Language (OWL [56], OWL 2 [21, 50]) or description logic (DL) [3].",
      "startOffset" : 188,
      "endOffset" : 192
    }, {
      "referenceID" : 7,
      "context" : "Most artificial intelligence applications rely on knowledge that is encoded in a knowledge base (KB) by means of some logical knowledge representation language such as propositional logic [10], datalog [9], first-order logic (FOL) [10], The Web Ontology Language (OWL [56], OWL 2 [21, 50]) or description logic (DL) [3].",
      "startOffset" : 202,
      "endOffset" : 205
    }, {
      "referenceID" : 8,
      "context" : "Most artificial intelligence applications rely on knowledge that is encoded in a knowledge base (KB) by means of some logical knowledge representation language such as propositional logic [10], datalog [9], first-order logic (FOL) [10], The Web Ontology Language (OWL [56], OWL 2 [21, 50]) or description logic (DL) [3].",
      "startOffset" : 231,
      "endOffset" : 235
    }, {
      "referenceID" : 54,
      "context" : "Most artificial intelligence applications rely on knowledge that is encoded in a knowledge base (KB) by means of some logical knowledge representation language such as propositional logic [10], datalog [9], first-order logic (FOL) [10], The Web Ontology Language (OWL [56], OWL 2 [21, 50]) or description logic (DL) [3].",
      "startOffset" : 268,
      "endOffset" : 272
    }, {
      "referenceID" : 19,
      "context" : "Most artificial intelligence applications rely on knowledge that is encoded in a knowledge base (KB) by means of some logical knowledge representation language such as propositional logic [10], datalog [9], first-order logic (FOL) [10], The Web Ontology Language (OWL [56], OWL 2 [21, 50]) or description logic (DL) [3].",
      "startOffset" : 280,
      "endOffset" : 288
    }, {
      "referenceID" : 48,
      "context" : "Most artificial intelligence applications rely on knowledge that is encoded in a knowledge base (KB) by means of some logical knowledge representation language such as propositional logic [10], datalog [9], first-order logic (FOL) [10], The Web Ontology Language (OWL [56], OWL 2 [21, 50]) or description logic (DL) [3].",
      "startOffset" : 280,
      "endOffset" : 288
    }, {
      "referenceID" : 2,
      "context" : "Most artificial intelligence applications rely on knowledge that is encoded in a knowledge base (KB) by means of some logical knowledge representation language such as propositional logic [10], datalog [9], first-order logic (FOL) [10], The Web Ontology Language (OWL [56], OWL 2 [21, 50]) or description logic (DL) [3].",
      "startOffset" : 316,
      "endOffset" : 319
    }, {
      "referenceID" : 66,
      "context" : "it would violate the requirement coherency (which has originally been defined for DL KBs [68, 55].",
      "startOffset" : 89,
      "endOffset" : 97
    }, {
      "referenceID" : 53,
      "context" : "it would violate the requirement coherency (which has originally been defined for DL KBs [68, 55].",
      "startOffset" : 89,
      "endOffset" : 97
    }, {
      "referenceID" : 22,
      "context" : "Faults in KBs may, for instance, arise because human reasoning is simply overstrained [24, 26].",
      "startOffset" : 86,
      "endOffset" : 94
    }, {
      "referenceID" : 24,
      "context" : "Faults in KBs may, for instance, arise because human reasoning is simply overstrained [24, 26].",
      "startOffset" : 86,
      "endOffset" : 94
    }, {
      "referenceID" : 51,
      "context" : "Another reason for the non-compliance with the mentioned quality criteria imposed on KBs might be that multiple (independently working) editors contribute to the development of the KB [53] which may lead to contradictory formulas.",
      "startOffset" : 184,
      "endOffset" : 188
    }, {
      "referenceID" : 31,
      "context" : "[33, 51, 32], to generate (parts of) KBs can further exacerbate the task of KB quality assurance [46, 16].",
      "startOffset" : 0,
      "endOffset" : 12
    }, {
      "referenceID" : 49,
      "context" : "[33, 51, 32], to generate (parts of) KBs can further exacerbate the task of KB quality assurance [46, 16].",
      "startOffset" : 0,
      "endOffset" : 12
    }, {
      "referenceID" : 30,
      "context" : "[33, 51, 32], to generate (parts of) KBs can further exacerbate the task of KB quality assurance [46, 16].",
      "startOffset" : 0,
      "endOffset" : 12
    }, {
      "referenceID" : 44,
      "context" : "[33, 51, 32], to generate (parts of) KBs can further exacerbate the task of KB quality assurance [46, 16].",
      "startOffset" : 97,
      "endOffset" : 105
    }, {
      "referenceID" : 14,
      "context" : "[33, 51, 32], to generate (parts of) KBs can further exacerbate the task of KB quality assurance [46, 16].",
      "startOffset" : 97,
      "endOffset" : 105
    }, {
      "referenceID" : 6,
      "context" : "Moreover, as studies in cognitive psychology [8, 35] attest, humans make systematic errors while formulating or interpreting logical formulas.",
      "startOffset" : 45,
      "endOffset" : 52
    }, {
      "referenceID" : 33,
      "context" : "Moreover, as studies in cognitive psychology [8, 35] attest, humans make systematic errors while formulating or interpreting logical formulas.",
      "startOffset" : 45,
      "endOffset" : 52
    }, {
      "referenceID" : 57,
      "context" : "These observations are confirmed by [59, 64] which present common faults people make when developing a KB (ontology).",
      "startOffset" : 36,
      "endOffset" : 44
    }, {
      "referenceID" : 62,
      "context" : "These observations are confirmed by [59, 64] which present common faults people make when developing a KB (ontology).",
      "startOffset" : 36,
      "endOffset" : 44
    }, {
      "referenceID" : 66,
      "context" : "Given a set of requirements to the KB and sets of test cases, KB debugging methods [68, 38, 19, 25] can localize a (potential) fault by computing a subset D of the formulas in the KB K called a diagnosis.",
      "startOffset" : 83,
      "endOffset" : 99
    }, {
      "referenceID" : 36,
      "context" : "Given a set of requirements to the KB and sets of test cases, KB debugging methods [68, 38, 19, 25] can localize a (potential) fault by computing a subset D of the formulas in the KB K called a diagnosis.",
      "startOffset" : 83,
      "endOffset" : 99
    }, {
      "referenceID" : 17,
      "context" : "Given a set of requirements to the KB and sets of test cases, KB debugging methods [68, 38, 19, 25] can localize a (potential) fault by computing a subset D of the formulas in the KB K called a diagnosis.",
      "startOffset" : 83,
      "endOffset" : 99
    }, {
      "referenceID" : 23,
      "context" : "Given a set of requirements to the KB and sets of test cases, KB debugging methods [68, 38, 19, 25] can localize a (potential) fault by computing a subset D of the formulas in the KB K called a diagnosis.",
      "startOffset" : 83,
      "endOffset" : 99
    }, {
      "referenceID" : 71,
      "context" : "[74, 23]) within the debugging system.",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 21,
      "context" : "[74, 23]) within the debugging system.",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 66,
      "context" : "[68, 23, 41]) attempt to exploit internal modifications of the reasoner for debugging purposes; in other words, the sources of problems (e.",
      "startOffset" : 0,
      "endOffset" : 12
    }, {
      "referenceID" : 21,
      "context" : "[68, 23, 41]) attempt to exploit internal modifications of the reasoner for debugging purposes; in other words, the sources of problems (e.",
      "startOffset" : 0,
      "endOffset" : 12
    }, {
      "referenceID" : 39,
      "context" : "[68, 23, 41]) attempt to exploit internal modifications of the reasoner for debugging purposes; in other words, the sources of problems (e.",
      "startOffset" : 0,
      "endOffset" : 12
    }, {
      "referenceID" : 21,
      "context" : "contradictory formulas) in the KB are computed as a direct consequence of reasoning [23].",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 39,
      "context" : "The advantages of a black-box approach over a glass-box approach are the lower memory consumption and better performance [41] of the reasoner and the reasoner independence of the debugging method.",
      "startOffset" : 121,
      "endOffset" : 125
    }, {
      "referenceID" : 70,
      "context" : "For example, in [73] a sample study of real-world KBs revealed that the number",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 78,
      "context" : "Moreover, [81] has put several (non-interactive) debugging systems to the test using a test set of faulty (incoherent OWL) real-world KBs which were partly designed by humans and partly by the application of automatic systems.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 58,
      "context" : "The proposed approaches to interactive KB debugging in this work follow the standard model-based diagnosis (MBD) technique [60, 44].",
      "startOffset" : 123,
      "endOffset" : 131
    }, {
      "referenceID" : 42,
      "context" : "The proposed approaches to interactive KB debugging in this work follow the standard model-based diagnosis (MBD) technique [60, 44].",
      "startOffset" : 123,
      "endOffset" : 131
    }, {
      "referenceID" : 76,
      "context" : "MBD has been successfully applied to a great variety of problems in various fields such as robotics [79], planning [80], debugging of software programs [87], configuration problems [17], hardware designs [20], constraint satisfaction problems and spreadsheets [1].",
      "startOffset" : 100,
      "endOffset" : 104
    }, {
      "referenceID" : 77,
      "context" : "MBD has been successfully applied to a great variety of problems in various fields such as robotics [79], planning [80], debugging of software programs [87], configuration problems [17], hardware designs [20], constraint satisfaction problems and spreadsheets [1].",
      "startOffset" : 115,
      "endOffset" : 119
    }, {
      "referenceID" : 84,
      "context" : "MBD has been successfully applied to a great variety of problems in various fields such as robotics [79], planning [80], debugging of software programs [87], configuration problems [17], hardware designs [20], constraint satisfaction problems and spreadsheets [1].",
      "startOffset" : 152,
      "endOffset" : 156
    }, {
      "referenceID" : 15,
      "context" : "MBD has been successfully applied to a great variety of problems in various fields such as robotics [79], planning [80], debugging of software programs [87], configuration problems [17], hardware designs [20], constraint satisfaction problems and spreadsheets [1].",
      "startOffset" : 181,
      "endOffset" : 185
    }, {
      "referenceID" : 18,
      "context" : "MBD has been successfully applied to a great variety of problems in various fields such as robotics [79], planning [80], debugging of software programs [87], configuration problems [17], hardware designs [20], constraint satisfaction problems and spreadsheets [1].",
      "startOffset" : 204,
      "endOffset" : 208
    }, {
      "referenceID" : 0,
      "context" : "MBD has been successfully applied to a great variety of problems in various fields such as robotics [79], planning [80], debugging of software programs [87], configuration problems [17], hardware designs [20], constraint satisfaction problems and spreadsheets [1].",
      "startOffset" : 260,
      "endOffset" : 263
    }, {
      "referenceID" : 5,
      "context" : "An MBD problem can be modeled as an abduction problem [7], i.",
      "startOffset" : 54,
      "endOffset" : 57
    }, {
      "referenceID" : 5,
      "context" : "It was proven in [7] that the computation of the first explanation (minimal diagnosis) is in P.",
      "startOffset" : 17,
      "endOffset" : 20
    }, {
      "referenceID" : 68,
      "context" : "Incorporating the necessary reasoning costs and assuming consistency a minimal requirement to the correct KB, the finding of the first explanation (minimal diagnosis) is already NP-hard even for propositional KBs [70] (since propositional satisfiability checking is NP-complete).",
      "startOffset" : 213,
      "endOffset" : 217
    }, {
      "referenceID" : 19,
      "context" : "The worst case complexity for the debugging of KBs formulated over more expressive logics such as OWL 2 (reasoning is 2-NExpTimecomplete [21, 42]) will be of course even worse.",
      "startOffset" : 137,
      "endOffset" : 145
    }, {
      "referenceID" : 40,
      "context" : "The worst case complexity for the debugging of KBs formulated over more expressive logics such as OWL 2 (reasoning is 2-NExpTimecomplete [21, 42]) will be of course even worse.",
      "startOffset" : 137,
      "endOffset" : 145
    }, {
      "referenceID" : 61,
      "context" : "in our previous works [63, 74, 76] that for many real-world KBs interactive KB debugging is feasible in reasonable time, despite high (or intractable) worst case reasoning costs and the intractable complexity of the abduction (i.",
      "startOffset" : 22,
      "endOffset" : 34
    }, {
      "referenceID" : 71,
      "context" : "in our previous works [63, 74, 76] that for many real-world KBs interactive KB debugging is feasible in reasonable time, despite high (or intractable) worst case reasoning costs and the intractable complexity of the abduction (i.",
      "startOffset" : 22,
      "endOffset" : 34
    }, {
      "referenceID" : 73,
      "context" : "in our previous works [63, 74, 76] that for many real-world KBs interactive KB debugging is feasible in reasonable time, despite high (or intractable) worst case reasoning costs and the intractable complexity of the abduction (i.",
      "startOffset" : 22,
      "endOffset" : 34
    }, {
      "referenceID" : 24,
      "context" : "To get a more concrete idea of these assumptions, the reader is invited to think about whether the following first-order KB K is consistent (a similar example is discussed in [26]):",
      "startOffset" : 175,
      "endOffset" : 179
    }, {
      "referenceID" : 24,
      "context" : "The observations made in [26] concerning a slight modification K′ of the KB K extracted from a real-world KB confirm this assumption.",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 21,
      "context" : "[23, 37]).",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 35,
      "context" : "[23, 37]).",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 78,
      "context" : "This has also been found in [81].",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 46,
      "context" : "Here, both ontologies are considered correct and diagnoses are only allowed to include elements of the alignment [48].",
      "startOffset" : 113,
      "endOffset" : 117
    }, {
      "referenceID" : 71,
      "context" : "Our previous works on the topic [74, 73, 63, 19, 76] are more application-oriented and thus abstract from some details and omit some of the proofs in favor of comprehensive evaluations of the presented strategies.",
      "startOffset" : 32,
      "endOffset" : 52
    }, {
      "referenceID" : 70,
      "context" : "Our previous works on the topic [74, 73, 63, 19, 76] are more application-oriented and thus abstract from some details and omit some of the proofs in favor of comprehensive evaluations of the presented strategies.",
      "startOffset" : 32,
      "endOffset" : 52
    }, {
      "referenceID" : 61,
      "context" : "Our previous works on the topic [74, 73, 63, 19, 76] are more application-oriented and thus abstract from some details and omit some of the proofs in favor of comprehensive evaluations of the presented strategies.",
      "startOffset" : 32,
      "endOffset" : 52
    }, {
      "referenceID" : 17,
      "context" : "Our previous works on the topic [74, 73, 63, 19, 76] are more application-oriented and thus abstract from some details and omit some of the proofs in favor of comprehensive evaluations of the presented strategies.",
      "startOffset" : 32,
      "endOffset" : 52
    }, {
      "referenceID" : 73,
      "context" : "Our previous works on the topic [74, 73, 63, 19, 76] are more application-oriented and thus abstract from some details and omit some of the proofs in favor of comprehensive evaluations of the presented strategies.",
      "startOffset" : 32,
      "endOffset" : 52
    }, {
      "referenceID" : 71,
      "context" : "The investigated methods for query computation have been used in [74, 63, 73, 76] too, but have not been addressed in depth in these works.",
      "startOffset" : 65,
      "endOffset" : 81
    }, {
      "referenceID" : 61,
      "context" : "The investigated methods for query computation have been used in [74, 63, 73, 76] too, but have not been addressed in depth in these works.",
      "startOffset" : 65,
      "endOffset" : 81
    }, {
      "referenceID" : 70,
      "context" : "The investigated methods for query computation have been used in [74, 63, 73, 76] too, but have not been addressed in depth in these works.",
      "startOffset" : 65,
      "endOffset" : 81
    }, {
      "referenceID" : 73,
      "context" : "The investigated methods for query computation have been used in [74, 63, 73, 76] too, but have not been addressed in depth in these works.",
      "startOffset" : 65,
      "endOffset" : 81
    }, {
      "referenceID" : 34,
      "context" : "• We give a formal proof of the soundness of an algorithm QX (based on [36]) for the detection of a minimal conflict set in a KB and we show the correctness (completeness, soundness, optimality) of a hitting set tree algorithm HS (based on [60]) for finding minimal diagnoses in a KB in bestfirst order (i.",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 58,
      "context" : "• We give a formal proof of the soundness of an algorithm QX (based on [36]) for the detection of a minimal conflict set in a KB and we show the correctness (completeness, soundness, optimality) of a hitting set tree algorithm HS (based on [60]) for finding minimal diagnoses in a KB in bestfirst order (i.",
      "startOffset" : 240,
      "endOffset" : 244
    }, {
      "referenceID" : 42,
      "context" : "used in [44, 60, 74, 63] and the latter i.",
      "startOffset" : 8,
      "endOffset" : 24
    }, {
      "referenceID" : 58,
      "context" : "used in [44, 60, 74, 63] and the latter i.",
      "startOffset" : 8,
      "endOffset" : 24
    }, {
      "referenceID" : 71,
      "context" : "used in [44, 60, 74, 63] and the latter i.",
      "startOffset" : 8,
      "endOffset" : 24
    }, {
      "referenceID" : 61,
      "context" : "used in [44, 60, 74, 63] and the latter i.",
      "startOffset" : 8,
      "endOffset" : 24
    }, {
      "referenceID" : 23,
      "context" : "in [25, 26, 27, 23, 24, 29, 82, 37, 47, 67, 52].",
      "startOffset" : 3,
      "endOffset" : 47
    }, {
      "referenceID" : 24,
      "context" : "in [25, 26, 27, 23, 24, 29, 82, 37, 47, 67, 52].",
      "startOffset" : 3,
      "endOffset" : 47
    }, {
      "referenceID" : 25,
      "context" : "in [25, 26, 27, 23, 24, 29, 82, 37, 47, 67, 52].",
      "startOffset" : 3,
      "endOffset" : 47
    }, {
      "referenceID" : 21,
      "context" : "in [25, 26, 27, 23, 24, 29, 82, 37, 47, 67, 52].",
      "startOffset" : 3,
      "endOffset" : 47
    }, {
      "referenceID" : 22,
      "context" : "in [25, 26, 27, 23, 24, 29, 82, 37, 47, 67, 52].",
      "startOffset" : 3,
      "endOffset" : 47
    }, {
      "referenceID" : 27,
      "context" : "in [25, 26, 27, 23, 24, 29, 82, 37, 47, 67, 52].",
      "startOffset" : 3,
      "endOffset" : 47
    }, {
      "referenceID" : 79,
      "context" : "in [25, 26, 27, 23, 24, 29, 82, 37, 47, 67, 52].",
      "startOffset" : 3,
      "endOffset" : 47
    }, {
      "referenceID" : 35,
      "context" : "in [25, 26, 27, 23, 24, 29, 82, 37, 47, 67, 52].",
      "startOffset" : 3,
      "endOffset" : 47
    }, {
      "referenceID" : 45,
      "context" : "in [25, 26, 27, 23, 24, 29, 82, 37, 47, 67, 52].",
      "startOffset" : 3,
      "endOffset" : 47
    }, {
      "referenceID" : 65,
      "context" : "in [25, 26, 27, 23, 24, 29, 82, 37, 47, 67, 52].",
      "startOffset" : 3,
      "endOffset" : 47
    }, {
      "referenceID" : 50,
      "context" : "in [25, 26, 27, 23, 24, 29, 82, 37, 47, 67, 52].",
      "startOffset" : 3,
      "endOffset" : 47
    }, {
      "referenceID" : 26,
      "context" : "[28]) implies the efficiency of conflict set computation for the same set of KBs.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 71,
      "context" : "One that is guaranteed to reduce the number of remaining solutions after a query is answered and one that features more powerful pruning techniques than our previously published algorithms [74, 63] (an evaluation that compares the overall efficiency of our previous algorithms with the ones proposed in this work must still be conducted and is part of our future research).",
      "startOffset" : 189,
      "endOffset" : 197
    }, {
      "referenceID" : 61,
      "context" : "One that is guaranteed to reduce the number of remaining solutions after a query is answered and one that features more powerful pruning techniques than our previously published algorithms [74, 63] (an evaluation that compares the overall efficiency of our previous algorithms with the ones proposed in this work must still be conducted and is part of our future research).",
      "startOffset" : 189,
      "endOffset" : 197
    }, {
      "referenceID" : 58,
      "context" : "to stick to “The Principle of Parsimony” [60, 7].",
      "startOffset" : 41,
      "endOffset" : 48
    }, {
      "referenceID" : 5,
      "context" : "to stick to “The Principle of Parsimony” [60, 7].",
      "startOffset" : 41,
      "endOffset" : 48
    }, {
      "referenceID" : 23,
      "context" : "Subsequently, we derive the relationship between a conflict set and the notion of a justification (a minimal set of formulas necessary for a particular entailment to hold) which is well-known and frequently used, especially in the field of DL [25, 26, 27, 23, 24, 28].",
      "startOffset" : 243,
      "endOffset" : 267
    }, {
      "referenceID" : 24,
      "context" : "Subsequently, we derive the relationship between a conflict set and the notion of a justification (a minimal set of formulas necessary for a particular entailment to hold) which is well-known and frequently used, especially in the field of DL [25, 26, 27, 23, 24, 28].",
      "startOffset" : 243,
      "endOffset" : 267
    }, {
      "referenceID" : 25,
      "context" : "Subsequently, we derive the relationship between a conflict set and the notion of a justification (a minimal set of formulas necessary for a particular entailment to hold) which is well-known and frequently used, especially in the field of DL [25, 26, 27, 23, 24, 28].",
      "startOffset" : 243,
      "endOffset" : 267
    }, {
      "referenceID" : 21,
      "context" : "Subsequently, we derive the relationship between a conflict set and the notion of a justification (a minimal set of formulas necessary for a particular entailment to hold) which is well-known and frequently used, especially in the field of DL [25, 26, 27, 23, 24, 28].",
      "startOffset" : 243,
      "endOffset" : 267
    }, {
      "referenceID" : 22,
      "context" : "Subsequently, we derive the relationship between a conflict set and the notion of a justification (a minimal set of formulas necessary for a particular entailment to hold) which is well-known and frequently used, especially in the field of DL [25, 26, 27, 23, 24, 28].",
      "startOffset" : 243,
      "endOffset" : 267
    }, {
      "referenceID" : 26,
      "context" : "Subsequently, we derive the relationship between a conflict set and the notion of a justification (a minimal set of formulas necessary for a particular entailment to hold) which is well-known and frequently used, especially in the field of DL [25, 26, 27, 23, 24, 28].",
      "startOffset" : 243,
      "endOffset" : 267
    }, {
      "referenceID" : 34,
      "context" : "Having deduced all relevant characteristics of (minimal) conflict sets, we proceed to give a description of a method (QX, Algorithm 1) due to [36] which was originally presented as a method for finding preferred explanations (conflicts) in over-constrained CSPs, but can also be employed for an efficient computation of a minimal conflict set w.",
      "startOffset" : 142,
      "endOffset" : 146
    }, {
      "referenceID" : 58,
      "context" : "Having at our disposal a proven sound method for generation of a minimal conflict set, we continue with the delineation of a hitting set tree algorithm similar to the one originally presented in [60] which enables the computation of different minimal conflict sets by means of successive calls to QX, each time given an (adequately) modified DPI.",
      "startOffset" : 195,
      "endOffset" : 199
    }, {
      "referenceID" : 61,
      "context" : "Finally, some query selection measures are discussed [63, 74] (Section 5.",
      "startOffset" : 53,
      "endOffset" : 61
    }, {
      "referenceID" : 71,
      "context" : "Finally, some query selection measures are discussed [63, 74] (Section 5.",
      "startOffset" : 53,
      "endOffset" : 61
    }, {
      "referenceID" : 43,
      "context" : "[45]), wellformed formula (e.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 8,
      "context" : "[10]), (logical) sentence or axiom (e.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 63,
      "context" : "[65]) and axiom (in most of the description logic literature, e.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 2,
      "context" : "[3]).",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 8,
      "context" : "Whereas we assume the reader to be familiar with FOL and PL (a good introduction to PL and FOL can be found in [10]), we will give a short introduction to DL.",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 47,
      "context" : "[49]; the original works are [11, 86]) that FOL is not decidable in general, i.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 9,
      "context" : "[49]; the original works are [11, 86]) that FOL is not decidable in general, i.",
      "startOffset" : 29,
      "endOffset" : 37
    }, {
      "referenceID" : 83,
      "context" : "[49]; the original works are [11, 86]) that FOL is not decidable in general, i.",
      "startOffset" : 29,
      "endOffset" : 37
    }, {
      "referenceID" : 2,
      "context" : "For instance, a DL language allowing the formalism of equality role-value-maps which facilitates the expression of concepts like “persons whose co-workers coincide with their relatives” can be proven undecidable [3, 69].",
      "startOffset" : 212,
      "endOffset" : 219
    }, {
      "referenceID" : 67,
      "context" : "For instance, a DL language allowing the formalism of equality role-value-maps which facilitates the expression of concepts like “persons whose co-workers coincide with their relatives” can be proven undecidable [3, 69].",
      "startOffset" : 212,
      "endOffset" : 219
    }, {
      "referenceID" : 19,
      "context" : "Property 4 is satisfied, for example, for the DL language SROIQ which is the logical underpinning of OWL 2 [21].",
      "startOffset" : 107,
      "endOffset" : 111
    }, {
      "referenceID" : 40,
      "context" : "However, the complexity (2-NEXPTIME-complete [42]) of logical reasoning is intractable in the worst case for this language which implies the intractability of our methods in the worst case.",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 73,
      "context" : "Nevertheless, other DL languages applied with similar systems as those described in this paper have been showing reasonable performance [76, 63, 74].",
      "startOffset" : 136,
      "endOffset" : 148
    }, {
      "referenceID" : 61,
      "context" : "Nevertheless, other DL languages applied with similar systems as those described in this paper have been showing reasonable performance [76, 63, 74].",
      "startOffset" : 136,
      "endOffset" : 148
    }, {
      "referenceID" : 71,
      "context" : "Nevertheless, other DL languages applied with similar systems as those described in this paper have been showing reasonable performance [76, 63, 74].",
      "startOffset" : 136,
      "endOffset" : 148
    }, {
      "referenceID" : 1,
      "context" : "One example is the OWL 2 EL profile which enables polynomial time reasoning [2].",
      "startOffset" : 76,
      "endOffset" : 79
    }, {
      "referenceID" : 41,
      "context" : "For this language, the efficient reasoning service ELK has been presented by [43].",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 63,
      "context" : "For FOL, datalog is an example of a decidable sublanguage where reasoning is efficient [65].",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 4,
      "context" : "Further, restricted sublanguages of FOL can often be translated to some DL language wherefore DL positive results concerning the decidability of reasoning as well as complexity results can be adopted for these restricted FOL languages [3, chapter 4] [6].",
      "startOffset" : 250,
      "endOffset" : 253
    }, {
      "referenceID" : 48,
      "context" : "The standard knowledge representation formalism for ontologies is OWL 2 [50, 21] which relies on DL.",
      "startOffset" : 72,
      "endOffset" : 80
    }, {
      "referenceID" : 19,
      "context" : "The standard knowledge representation formalism for ontologies is OWL 2 [50, 21] which relies on DL.",
      "startOffset" : 72,
      "endOffset" : 80
    }, {
      "referenceID" : 2,
      "context" : "Description Logic (DL) [3] is a family of knowledge representation languages with a formal logic-based semantics that are designed to represent knowledge about a domain in form of concept descriptions.",
      "startOffset" : 23,
      "endOffset" : 26
    }, {
      "referenceID" : 0,
      "context" : "For example, let L := [1, 3, 4, 2] be an ordered list; then L∩{1, 2, 3} yields the set {1, 2, 3}.",
      "startOffset" : 22,
      "endOffset" : 34
    }, {
      "referenceID" : 2,
      "context" : "For example, let L := [1, 3, 4, 2] be an ordered list; then L∩{1, 2, 3} yields the set {1, 2, 3}.",
      "startOffset" : 22,
      "endOffset" : 34
    }, {
      "referenceID" : 3,
      "context" : "For example, let L := [1, 3, 4, 2] be an ordered list; then L∩{1, 2, 3} yields the set {1, 2, 3}.",
      "startOffset" : 22,
      "endOffset" : 34
    }, {
      "referenceID" : 1,
      "context" : "For example, let L := [1, 3, 4, 2] be an ordered list; then L∩{1, 2, 3} yields the set {1, 2, 3}.",
      "startOffset" : 22,
      "endOffset" : 34
    }, {
      "referenceID" : 63,
      "context" : "[65]).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 7,
      "context" : "[9]).",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 42,
      "context" : "[44]).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 5,
      "context" : "7] [7].",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 58,
      "context" : "〈K,B,P ,N 〉R the size of which is in general O(2|K|) (if all subsets of the KB K are investigated) can be reduced to a great extent by exploiting the notion of a conflict set [60, 44, 74].",
      "startOffset" : 175,
      "endOffset" : 187
    }, {
      "referenceID" : 42,
      "context" : "〈K,B,P ,N 〉R the size of which is in general O(2|K|) (if all subsets of the KB K are investigated) can be reduced to a great extent by exploiting the notion of a conflict set [60, 44, 74].",
      "startOffset" : 175,
      "endOffset" : 187
    }, {
      "referenceID" : 71,
      "context" : "〈K,B,P ,N 〉R the size of which is in general O(2|K|) (if all subsets of the KB K are investigated) can be reduced to a great extent by exploiting the notion of a conflict set [60, 44, 74].",
      "startOffset" : 175,
      "endOffset" : 187
    }, {
      "referenceID" : 23,
      "context" : "The notion of a conflict set is closely related to the notion of a justification [25, 26, 27, 23, 24, 28] which is frequently adopted in the field of the Semantic Web (cf.",
      "startOffset" : 81,
      "endOffset" : 105
    }, {
      "referenceID" : 24,
      "context" : "The notion of a conflict set is closely related to the notion of a justification [25, 26, 27, 23, 24, 28] which is frequently adopted in the field of the Semantic Web (cf.",
      "startOffset" : 81,
      "endOffset" : 105
    }, {
      "referenceID" : 25,
      "context" : "The notion of a conflict set is closely related to the notion of a justification [25, 26, 27, 23, 24, 28] which is frequently adopted in the field of the Semantic Web (cf.",
      "startOffset" : 81,
      "endOffset" : 105
    }, {
      "referenceID" : 21,
      "context" : "The notion of a conflict set is closely related to the notion of a justification [25, 26, 27, 23, 24, 28] which is frequently adopted in the field of the Semantic Web (cf.",
      "startOffset" : 81,
      "endOffset" : 105
    }, {
      "referenceID" : 22,
      "context" : "The notion of a conflict set is closely related to the notion of a justification [25, 26, 27, 23, 24, 28] which is frequently adopted in the field of the Semantic Web (cf.",
      "startOffset" : 81,
      "endOffset" : 105
    }, {
      "referenceID" : 26,
      "context" : "The notion of a conflict set is closely related to the notion of a justification [25, 26, 27, 23, 24, 28] which is frequently adopted in the field of the Semantic Web (cf.",
      "startOffset" : 81,
      "endOffset" : 105
    }, {
      "referenceID" : 35,
      "context" : "Thus, the paradigm of a justification can be a useful aid in the debugging of faulty ontologies [37].",
      "startOffset" : 96,
      "endOffset" : 100
    }, {
      "referenceID" : 3,
      "context" : "Note that sometimes justifications are referred to as MinAs (Minimal Axiom Sets) [4] or MUPS (Minimal Unsatisfiability Preserving Sub-TBoxes) [68] where the latter term is mostly used in the context of ontology debugging.",
      "startOffset" : 81,
      "endOffset" : 84
    }, {
      "referenceID" : 66,
      "context" : "Note that sometimes justifications are referred to as MinAs (Minimal Axiom Sets) [4] or MUPS (Minimal Unsatisfiability Preserving Sub-TBoxes) [68] where the latter term is mostly used in the context of ontology debugging.",
      "startOffset" : 142,
      "endOffset" : 146
    }, {
      "referenceID" : 58,
      "context" : "The notion of a (minimal) conflict set, on the other hand, has been mainly adopted in the Diagnosis community [60, 44, 57, 87, 17].",
      "startOffset" : 110,
      "endOffset" : 130
    }, {
      "referenceID" : 42,
      "context" : "The notion of a (minimal) conflict set, on the other hand, has been mainly adopted in the Diagnosis community [60, 44, 57, 87, 17].",
      "startOffset" : 110,
      "endOffset" : 130
    }, {
      "referenceID" : 55,
      "context" : "The notion of a (minimal) conflict set, on the other hand, has been mainly adopted in the Diagnosis community [60, 44, 57, 87, 17].",
      "startOffset" : 110,
      "endOffset" : 130
    }, {
      "referenceID" : 84,
      "context" : "The notion of a (minimal) conflict set, on the other hand, has been mainly adopted in the Diagnosis community [60, 44, 57, 87, 17].",
      "startOffset" : 110,
      "endOffset" : 130
    }, {
      "referenceID" : 15,
      "context" : "The notion of a (minimal) conflict set, on the other hand, has been mainly adopted in the Diagnosis community [60, 44, 57, 87, 17].",
      "startOffset" : 110,
      "endOffset" : 130
    }, {
      "referenceID" : 35,
      "context" : "For example, the author of [37] i.",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 35,
      "context" : "terminology of [37, 23], A would be called a purely derived unsatisfiable concept whereas B would be called a root unsatisfiable concept.",
      "startOffset" : 15,
      "endOffset" : 23
    }, {
      "referenceID" : 21,
      "context" : "terminology of [37, 23], A would be called a purely derived unsatisfiable concept whereas B would be called a root unsatisfiable concept.",
      "startOffset" : 15,
      "endOffset" : 23
    }, {
      "referenceID" : 35,
      "context" : "Therefore, [37] proposes to resolve root unsatisfiable concepts first since this might resolve some (purely) derived concepts as well, as in this example.",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 36,
      "context" : "[38] Let K be a KB and α a formula, both over L.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 17,
      "context" : "[19] A (minimal) diagnosis w.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 0,
      "context" : "Hence, the set of all minimal diagnoses mD〈K,B,P,N 〉R = {D1,D2,D3} = {[1], [2], [5, 7]} is obtained by computing all minimal hitting sets of mC〈K,B,P,N 〉R = {C1, C2} (cf.",
      "startOffset" : 70,
      "endOffset" : 73
    }, {
      "referenceID" : 1,
      "context" : "Hence, the set of all minimal diagnoses mD〈K,B,P,N 〉R = {D1,D2,D3} = {[1], [2], [5, 7]} is obtained by computing all minimal hitting sets of mC〈K,B,P,N 〉R = {C1, C2} (cf.",
      "startOffset" : 75,
      "endOffset" : 78
    }, {
      "referenceID" : 5,
      "context" : "Hence, the set of all minimal diagnoses mD〈K,B,P,N 〉R = {D1,D2,D3} = {[1], [2], [5, 7]} is obtained by computing all minimal hitting sets of mC〈K,B,P,N 〉R = {C1, C2} (cf.",
      "startOffset" : 80,
      "endOffset" : 86
    }, {
      "referenceID" : 0,
      "context" : "D1 = [1, 2] D2 = [1, 4] D3 = [1, 6] D4 = [2, 3, 5] D5 = [2, 3, 6] D6 = [2, 3, 8] D7 = [2, 4, 6] D8 = [2, 4, 8] D9 = [3, 5, 6] D10 = [4, 5]",
      "startOffset" : 5,
      "endOffset" : 11
    }, {
      "referenceID" : 1,
      "context" : "D1 = [1, 2] D2 = [1, 4] D3 = [1, 6] D4 = [2, 3, 5] D5 = [2, 3, 6] D6 = [2, 3, 8] D7 = [2, 4, 6] D8 = [2, 4, 8] D9 = [3, 5, 6] D10 = [4, 5]",
      "startOffset" : 5,
      "endOffset" : 11
    }, {
      "referenceID" : 0,
      "context" : "D1 = [1, 2] D2 = [1, 4] D3 = [1, 6] D4 = [2, 3, 5] D5 = [2, 3, 6] D6 = [2, 3, 8] D7 = [2, 4, 6] D8 = [2, 4, 8] D9 = [3, 5, 6] D10 = [4, 5]",
      "startOffset" : 17,
      "endOffset" : 23
    }, {
      "referenceID" : 3,
      "context" : "D1 = [1, 2] D2 = [1, 4] D3 = [1, 6] D4 = [2, 3, 5] D5 = [2, 3, 6] D6 = [2, 3, 8] D7 = [2, 4, 6] D8 = [2, 4, 8] D9 = [3, 5, 6] D10 = [4, 5]",
      "startOffset" : 17,
      "endOffset" : 23
    }, {
      "referenceID" : 0,
      "context" : "D1 = [1, 2] D2 = [1, 4] D3 = [1, 6] D4 = [2, 3, 5] D5 = [2, 3, 6] D6 = [2, 3, 8] D7 = [2, 4, 6] D8 = [2, 4, 8] D9 = [3, 5, 6] D10 = [4, 5]",
      "startOffset" : 29,
      "endOffset" : 35
    }, {
      "referenceID" : 4,
      "context" : "D1 = [1, 2] D2 = [1, 4] D3 = [1, 6] D4 = [2, 3, 5] D5 = [2, 3, 6] D6 = [2, 3, 8] D7 = [2, 4, 6] D8 = [2, 4, 8] D9 = [3, 5, 6] D10 = [4, 5]",
      "startOffset" : 29,
      "endOffset" : 35
    }, {
      "referenceID" : 1,
      "context" : "D1 = [1, 2] D2 = [1, 4] D3 = [1, 6] D4 = [2, 3, 5] D5 = [2, 3, 6] D6 = [2, 3, 8] D7 = [2, 4, 6] D8 = [2, 4, 8] D9 = [3, 5, 6] D10 = [4, 5]",
      "startOffset" : 41,
      "endOffset" : 50
    }, {
      "referenceID" : 2,
      "context" : "D1 = [1, 2] D2 = [1, 4] D3 = [1, 6] D4 = [2, 3, 5] D5 = [2, 3, 6] D6 = [2, 3, 8] D7 = [2, 4, 6] D8 = [2, 4, 8] D9 = [3, 5, 6] D10 = [4, 5]",
      "startOffset" : 41,
      "endOffset" : 50
    }, {
      "referenceID" : 1,
      "context" : "D1 = [1, 2] D2 = [1, 4] D3 = [1, 6] D4 = [2, 3, 5] D5 = [2, 3, 6] D6 = [2, 3, 8] D7 = [2, 4, 6] D8 = [2, 4, 8] D9 = [3, 5, 6] D10 = [4, 5]",
      "startOffset" : 56,
      "endOffset" : 65
    }, {
      "referenceID" : 2,
      "context" : "D1 = [1, 2] D2 = [1, 4] D3 = [1, 6] D4 = [2, 3, 5] D5 = [2, 3, 6] D6 = [2, 3, 8] D7 = [2, 4, 6] D8 = [2, 4, 8] D9 = [3, 5, 6] D10 = [4, 5]",
      "startOffset" : 56,
      "endOffset" : 65
    }, {
      "referenceID" : 4,
      "context" : "D1 = [1, 2] D2 = [1, 4] D3 = [1, 6] D4 = [2, 3, 5] D5 = [2, 3, 6] D6 = [2, 3, 8] D7 = [2, 4, 6] D8 = [2, 4, 8] D9 = [3, 5, 6] D10 = [4, 5]",
      "startOffset" : 56,
      "endOffset" : 65
    }, {
      "referenceID" : 1,
      "context" : "D1 = [1, 2] D2 = [1, 4] D3 = [1, 6] D4 = [2, 3, 5] D5 = [2, 3, 6] D6 = [2, 3, 8] D7 = [2, 4, 6] D8 = [2, 4, 8] D9 = [3, 5, 6] D10 = [4, 5]",
      "startOffset" : 71,
      "endOffset" : 80
    }, {
      "referenceID" : 2,
      "context" : "D1 = [1, 2] D2 = [1, 4] D3 = [1, 6] D4 = [2, 3, 5] D5 = [2, 3, 6] D6 = [2, 3, 8] D7 = [2, 4, 6] D8 = [2, 4, 8] D9 = [3, 5, 6] D10 = [4, 5]",
      "startOffset" : 71,
      "endOffset" : 80
    }, {
      "referenceID" : 6,
      "context" : "D1 = [1, 2] D2 = [1, 4] D3 = [1, 6] D4 = [2, 3, 5] D5 = [2, 3, 6] D6 = [2, 3, 8] D7 = [2, 4, 6] D8 = [2, 4, 8] D9 = [3, 5, 6] D10 = [4, 5]",
      "startOffset" : 71,
      "endOffset" : 80
    }, {
      "referenceID" : 1,
      "context" : "D1 = [1, 2] D2 = [1, 4] D3 = [1, 6] D4 = [2, 3, 5] D5 = [2, 3, 6] D6 = [2, 3, 8] D7 = [2, 4, 6] D8 = [2, 4, 8] D9 = [3, 5, 6] D10 = [4, 5]",
      "startOffset" : 86,
      "endOffset" : 95
    }, {
      "referenceID" : 3,
      "context" : "D1 = [1, 2] D2 = [1, 4] D3 = [1, 6] D4 = [2, 3, 5] D5 = [2, 3, 6] D6 = [2, 3, 8] D7 = [2, 4, 6] D8 = [2, 4, 8] D9 = [3, 5, 6] D10 = [4, 5]",
      "startOffset" : 86,
      "endOffset" : 95
    }, {
      "referenceID" : 4,
      "context" : "D1 = [1, 2] D2 = [1, 4] D3 = [1, 6] D4 = [2, 3, 5] D5 = [2, 3, 6] D6 = [2, 3, 8] D7 = [2, 4, 6] D8 = [2, 4, 8] D9 = [3, 5, 6] D10 = [4, 5]",
      "startOffset" : 86,
      "endOffset" : 95
    }, {
      "referenceID" : 1,
      "context" : "D1 = [1, 2] D2 = [1, 4] D3 = [1, 6] D4 = [2, 3, 5] D5 = [2, 3, 6] D6 = [2, 3, 8] D7 = [2, 4, 6] D8 = [2, 4, 8] D9 = [3, 5, 6] D10 = [4, 5]",
      "startOffset" : 101,
      "endOffset" : 110
    }, {
      "referenceID" : 3,
      "context" : "D1 = [1, 2] D2 = [1, 4] D3 = [1, 6] D4 = [2, 3, 5] D5 = [2, 3, 6] D6 = [2, 3, 8] D7 = [2, 4, 6] D8 = [2, 4, 8] D9 = [3, 5, 6] D10 = [4, 5]",
      "startOffset" : 101,
      "endOffset" : 110
    }, {
      "referenceID" : 6,
      "context" : "D1 = [1, 2] D2 = [1, 4] D3 = [1, 6] D4 = [2, 3, 5] D5 = [2, 3, 6] D6 = [2, 3, 8] D7 = [2, 4, 6] D8 = [2, 4, 8] D9 = [3, 5, 6] D10 = [4, 5]",
      "startOffset" : 101,
      "endOffset" : 110
    }, {
      "referenceID" : 2,
      "context" : "D1 = [1, 2] D2 = [1, 4] D3 = [1, 6] D4 = [2, 3, 5] D5 = [2, 3, 6] D6 = [2, 3, 8] D7 = [2, 4, 6] D8 = [2, 4, 8] D9 = [3, 5, 6] D10 = [4, 5]",
      "startOffset" : 116,
      "endOffset" : 125
    }, {
      "referenceID" : 4,
      "context" : "D1 = [1, 2] D2 = [1, 4] D3 = [1, 6] D4 = [2, 3, 5] D5 = [2, 3, 6] D6 = [2, 3, 8] D7 = [2, 4, 6] D8 = [2, 4, 8] D9 = [3, 5, 6] D10 = [4, 5]",
      "startOffset" : 116,
      "endOffset" : 125
    }, {
      "referenceID" : 3,
      "context" : "D1 = [1, 2] D2 = [1, 4] D3 = [1, 6] D4 = [2, 3, 5] D5 = [2, 3, 6] D6 = [2, 3, 8] D7 = [2, 4, 6] D8 = [2, 4, 8] D9 = [3, 5, 6] D10 = [4, 5]",
      "startOffset" : 132,
      "endOffset" : 138
    }, {
      "referenceID" : 22,
      "context" : "[24]) and (3) to assess that there are no further minimal conflict sets w.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 36,
      "context" : "4) for the systematic computation of different minimal conflict sets, or other mechanisms such as the ALL_JUST_ALG presented in [38] which computes all justifications for some particular entailment (but, some post-processing of the justifications is necessary to obtain minimal conflict sets, cf.",
      "startOffset" : 128,
      "endOffset" : 132
    }, {
      "referenceID" : 22,
      "context" : "Problem (2) and its complexity for humans has been studied in [24] with a focus on justifications in DL or OWL KBs.",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 22,
      "context" : "1, the cognitive complexity model proposed by [24] applies also to minimal conflict sets.",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 25,
      "context" : "Ways to facilitate the understanding of justifications for humans (that might be successfully applied also to conflict sets) have been addressed in [27, 26, 25].",
      "startOffset" : 148,
      "endOffset" : 160
    }, {
      "referenceID" : 24,
      "context" : "Ways to facilitate the understanding of justifications for humans (that might be successfully applied also to conflict sets) have been addressed in [27, 26, 25].",
      "startOffset" : 148,
      "endOffset" : 160
    }, {
      "referenceID" : 23,
      "context" : "Ways to facilitate the understanding of justifications for humans (that might be successfully applied also to conflict sets) have been addressed in [27, 26, 25].",
      "startOffset" : 148,
      "endOffset" : 160
    }, {
      "referenceID" : 38,
      "context" : "Moreover, there is an ontology editing browser SWOOP [40] equipped with a strikeout feature [37] that highlights parts of justifications that are relevant for the entailment by striking out all irrelevant parts.",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 35,
      "context" : "Moreover, there is an ontology editing browser SWOOP [40] equipped with a strikeout feature [37] that highlights parts of justifications that are relevant for the entailment by striking out all irrelevant parts.",
      "startOffset" : 92,
      "endOffset" : 96
    }, {
      "referenceID" : 71,
      "context" : "Two common methods employed for the computation of (minimal) diagnoses [74, 63] are the QuickXPlain algorithm [36] (in short QX) and a hitting set search tree [60, 22] (in short HS).",
      "startOffset" : 71,
      "endOffset" : 79
    }, {
      "referenceID" : 61,
      "context" : "Two common methods employed for the computation of (minimal) diagnoses [74, 63] are the QuickXPlain algorithm [36] (in short QX) and a hitting set search tree [60, 22] (in short HS).",
      "startOffset" : 71,
      "endOffset" : 79
    }, {
      "referenceID" : 34,
      "context" : "Two common methods employed for the computation of (minimal) diagnoses [74, 63] are the QuickXPlain algorithm [36] (in short QX) and a hitting set search tree [60, 22] (in short HS).",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 58,
      "context" : "Two common methods employed for the computation of (minimal) diagnoses [74, 63] are the QuickXPlain algorithm [36] (in short QX) and a hitting set search tree [60, 22] (in short HS).",
      "startOffset" : 159,
      "endOffset" : 167
    }, {
      "referenceID" : 20,
      "context" : "Two common methods employed for the computation of (minimal) diagnoses [74, 63] are the QuickXPlain algorithm [36] (in short QX) and a hitting set search tree [60, 22] (in short HS).",
      "startOffset" : 159,
      "endOffset" : 167
    }, {
      "referenceID" : 36,
      "context" : "Another approach for computing a minimal conflict set (or justification) is the “expand-and-shrink” algorithm presented in [38].",
      "startOffset" : 123,
      "endOffset" : 127
    }, {
      "referenceID" : 72,
      "context" : "However, empirical evaluations and a theoretical analysis of the best and worst case complexity of the “expand-and-shrink” method compared to QX performed in [75] revealed that the latter is preferable over the former.",
      "startOffset" : 158,
      "endOffset" : 162
    }, {
      "referenceID" : 64,
      "context" : "One common method is to avoid the indirection of diagnosis computation via minimal conflict sets and use algorithms that determine diagnoses directly [66], i.",
      "startOffset" : 150,
      "endOffset" : 154
    }, {
      "referenceID" : 12,
      "context" : "This approach has been applied for the non-interactive debugging of ontologies [14] and constraints [18].",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 16,
      "context" : "This approach has been applied for the non-interactive debugging of ontologies [14] and constraints [18].",
      "startOffset" : 100,
      "endOffset" : 104
    }, {
      "referenceID" : 73,
      "context" : "In our previous work, we adopted such a direct technique for the interactive debugging of KBs [76].",
      "startOffset" : 94,
      "endOffset" : 98
    }, {
      "referenceID" : 34,
      "context" : "[36] Let 〈K,B,P ,N 〉R be a DPI and the function SPLIT (line 13",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 34,
      "context" : "[36] LetK1,K2 be a partition ofK.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 58,
      "context" : "One way to compute minimal diagnoses from minimal conflict sets is to use a hitting set tree algorithm which was originally proposed by Reiter [60].",
      "startOffset" : 143,
      "endOffset" : 147
    }, {
      "referenceID" : 17,
      "context" : "In this work we describe methods for non-interactive and interactive diagnosis computation based on the ones used in [19, 73, 74] which are closely related to the",
      "startOffset" : 117,
      "endOffset" : 129
    }, {
      "referenceID" : 70,
      "context" : "In this work we describe methods for non-interactive and interactive diagnosis computation based on the ones used in [19, 73, 74] which are closely related to the",
      "startOffset" : 117,
      "endOffset" : 129
    }, {
      "referenceID" : 71,
      "context" : "In this work we describe methods for non-interactive and interactive diagnosis computation based on the ones used in [19, 73, 74] which are closely related to the",
      "startOffset" : 117,
      "endOffset" : 129
    }, {
      "referenceID" : 58,
      "context" : "a breadth-first search strategy as described in [60].",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 58,
      "context" : "So, we next provide formal definitions of a (partial) HS-tree and a (partial) pruned HS-tree based on the definitions given in [60].",
      "startOffset" : 127,
      "endOffset" : 131
    }, {
      "referenceID" : 58,
      "context" : "1 Notice that we use a definition of a pruned HS-tree that slightly differs from the definition given in [60] in that we inherently assume that only minimal conflict sets w.",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 58,
      "context" : "Therefore we could omit the last rule in the definition of [60].",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 20,
      "context" : "one and the same DPI [22].",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 13,
      "context" : "The induction of a probability space [15] over diagnoses facilitates incorporation of well-established probability theoretic methods into the process of KB debugging; for example, a Bayesian approach [74, 63, 44] for identifying the true diagnosis, i.",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 71,
      "context" : "The induction of a probability space [15] over diagnoses facilitates incorporation of well-established probability theoretic methods into the process of KB debugging; for example, a Bayesian approach [74, 63, 44] for identifying the true diagnosis, i.",
      "startOffset" : 200,
      "endOffset" : 212
    }, {
      "referenceID" : 61,
      "context" : "The induction of a probability space [15] over diagnoses facilitates incorporation of well-established probability theoretic methods into the process of KB debugging; for example, a Bayesian approach [74, 63, 44] for identifying the true diagnosis, i.",
      "startOffset" : 200,
      "endOffset" : 212
    }, {
      "referenceID" : 42,
      "context" : "The induction of a probability space [15] over diagnoses facilitates incorporation of well-established probability theoretic methods into the process of KB debugging; for example, a Bayesian approach [74, 63, 44] for identifying the true diagnosis, i.",
      "startOffset" : 200,
      "endOffset" : 212
    }, {
      "referenceID" : 0,
      "context" : "p : E → [0, 1] such that ∑ ω∈Ω p({ω}) = 1 which means ∑ D∈aD〈K,B,P,N〉R p({D}) = 1.",
      "startOffset" : 8,
      "endOffset" : 14
    }, {
      "referenceID" : 52,
      "context" : "Examples of such KB (ontology) developing environments are Protégé [54], Web Protégé [85], SWOOP [40], OntoEdit [83] or KAON24.",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 82,
      "context" : "Examples of such KB (ontology) developing environments are Protégé [54], Web Protégé [85], SWOOP [40], OntoEdit [83] or KAON24.",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 38,
      "context" : "Examples of such KB (ontology) developing environments are Protégé [54], Web Protégé [85], SWOOP [40], OntoEdit [83] or KAON24.",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 80,
      "context" : "Examples of such KB (ontology) developing environments are Protégé [54], Web Protégé [85], SWOOP [40], OntoEdit [83] or KAON24.",
      "startOffset" : 112,
      "endOffset" : 116
    }, {
      "referenceID" : 71,
      "context" : "On the positive side, utilization of such empirical data can yield to fault information that is very well tailored for the user and that can imply a significant reduction of computation time and user effort necessary for debugging of the KB at hand [74].",
      "startOffset" : 249,
      "endOffset" : 253
    }, {
      "referenceID" : 57,
      "context" : "Ad (c): A common fault pattern [59, 12, 39], also called anti-pattern, refers to a set of formulas that either leads to an inconsistency (logical anti-pattern) or corresponds to a potential modeling error that – alone – does not lead to a inconsistency or incoherency (non-logical anti-pattern), but still might become a source of inconsistency if merged with other formulas (cf.",
      "startOffset" : 31,
      "endOffset" : 43
    }, {
      "referenceID" : 10,
      "context" : "Ad (c): A common fault pattern [59, 12, 39], also called anti-pattern, refers to a set of formulas that either leads to an inconsistency (logical anti-pattern) or corresponds to a potential modeling error that – alone – does not lead to a inconsistency or incoherency (non-logical anti-pattern), but still might become a source of inconsistency if merged with other formulas (cf.",
      "startOffset" : 31,
      "endOffset" : 43
    }, {
      "referenceID" : 37,
      "context" : "Ad (c): A common fault pattern [59, 12, 39], also called anti-pattern, refers to a set of formulas that either leads to an inconsistency (logical anti-pattern) or corresponds to a potential modeling error that – alone – does not lead to a inconsistency or incoherency (non-logical anti-pattern), but still might become a source of inconsistency if merged with other formulas (cf.",
      "startOffset" : 31,
      "endOffset" : 43
    }, {
      "referenceID" : 23,
      "context" : "The length of formulas could be defined similarly as in [25] which provides such a definition for DL languages.",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 71,
      "context" : "Moreover, experiments in our previous work [74] have manifested that fault information of only “average” quality most often leads to a better performance than no fault information.",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 61,
      "context" : "Apart from that, we have suggested a reinforcement learning “plug-in” to a debugger which could successfully mitigate the negative effect of low-quality fault information and in many cases, in spite of the low-quality fault information, even led to lower resource consumption (user, time) than a debugger without this plug-in using good fault information [63].",
      "startOffset" : 355,
      "endOffset" : 359
    }, {
      "referenceID" : 37,
      "context" : "[39]).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 0,
      "context" : "Let p : K → [0, 1] be some function that assigns to each ax ∈ K some p(ax ) ∈ [0, 1].",
      "startOffset" : 12,
      "endOffset" : 18
    }, {
      "referenceID" : 0,
      "context" : "Let p : K → [0, 1] be some function that assigns to each ax ∈ K some p(ax ) ∈ [0, 1].",
      "startOffset" : 78,
      "endOffset" : 84
    }, {
      "referenceID" : 0,
      "context" : "Then, we denote by pnodes : 2K → [0, 1] the function that assigns to each node nd ⊆ K some pnodes(nd) ∈ [0, 1] which is obtained by means of Formula 4.",
      "startOffset" : 33,
      "endOffset" : 39
    }, {
      "referenceID" : 0,
      "context" : "Then, we denote by pnodes : 2K → [0, 1] the function that assigns to each node nd ⊆ K some pnodes(nd) ∈ [0, 1] which is obtained by means of Formula 4.",
      "startOffset" : 104,
      "endOffset" : 110
    }, {
      "referenceID" : 0,
      "context" : "Let 〈K,B,P ,N 〉R be an admissible DPI and let w : K → [0, 1] be a weight function which assigns a weight to each node n ⊆ K with the property that w(n1) > w(n2) if n1 ⊂ n2.",
      "startOffset" : 54,
      "endOffset" : 60
    }, {
      "referenceID" : 0,
      "context" : "So, the output is the set of all minimal diagnoses mD〈K,B,P,N 〉R = {[1], [2], [5, 7]}.",
      "startOffset" : 68,
      "endOffset" : 71
    }, {
      "referenceID" : 1,
      "context" : "So, the output is the set of all minimal diagnoses mD〈K,B,P,N 〉R = {[1], [2], [5, 7]}.",
      "startOffset" : 73,
      "endOffset" : 76
    }, {
      "referenceID" : 5,
      "context" : "So, the output is the set of all minimal diagnoses mD〈K,B,P,N 〉R = {[1], [2], [5, 7]}.",
      "startOffset" : 78,
      "endOffset" : 84
    }, {
      "referenceID" : 0,
      "context" : "2 would be constructed and the output would be D = {D1} = {[1]} containing just the first found and thus most probable minimal diagnosis w.",
      "startOffset" : 59,
      "endOffset" : 62
    }, {
      "referenceID" : 0,
      "context" : "Note that D1 = [1] and D2 = [2] (which is not computed) have equal probability and whether the one or the other is computed first depends only on the ordering of equally probable (in this case: equal cardinality) nodes in Q.",
      "startOffset" : 15,
      "endOffset" : 18
    }, {
      "referenceID" : 1,
      "context" : "Note that D1 = [1] and D2 = [2] (which is not computed) have equal probability and whether the one or the other is computed first depends only on the ordering of equally probable (in this case: equal cardinality) nodes in Q.",
      "startOffset" : 28,
      "endOffset" : 31
    }, {
      "referenceID" : 35,
      "context" : "Please notice that the internal “flat” representation used by Algorithm 2 which does not store a tree but only the set of open and closed nodes differs from the standard tree representation [37, 19, 82, 60] we use to depict the hitting set tree graphically in Figure 4.",
      "startOffset" : 190,
      "endOffset" : 206
    }, {
      "referenceID" : 17,
      "context" : "Please notice that the internal “flat” representation used by Algorithm 2 which does not store a tree but only the set of open and closed nodes differs from the standard tree representation [37, 19, 82, 60] we use to depict the hitting set tree graphically in Figure 4.",
      "startOffset" : 190,
      "endOffset" : 206
    }, {
      "referenceID" : 79,
      "context" : "Please notice that the internal “flat” representation used by Algorithm 2 which does not store a tree but only the set of open and closed nodes differs from the standard tree representation [37, 19, 82, 60] we use to depict the hitting set tree graphically in Figure 4.",
      "startOffset" : 190,
      "endOffset" : 206
    }, {
      "referenceID" : 58,
      "context" : "Please notice that the internal “flat” representation used by Algorithm 2 which does not store a tree but only the set of open and closed nodes differs from the standard tree representation [37, 19, 82, 60] we use to depict the hitting set tree graphically in Figure 4.",
      "startOffset" : 190,
      "endOffset" : 206
    }, {
      "referenceID" : 1,
      "context" : "This already leads to the finding of the first minimal diagnosis D1 = [2] after overall computation time of 0.",
      "startOffset" : 70,
      "endOffset" : 73
    }, {
      "referenceID" : 1,
      "context" : "As a next step, again the first and thus best open node {2, 5} is chosen from Q and labeled by ×(⊃D1) which means that the corresponding path is closed since it is a superset of an already found minimal diagnosis, namely D1 = [2].",
      "startOffset" : 226,
      "endOffset" : 229
    }, {
      "referenceID" : 1,
      "context" : "1 seconds execution time is the set of minimal diagnoses D = {[2], [5, 7]} which is a proper subset of all minimal diagnoses D〈K,B,P,N 〉R = {[1], [2], [5, 7]}.",
      "startOffset" : 62,
      "endOffset" : 65
    }, {
      "referenceID" : 5,
      "context" : "1 seconds execution time is the set of minimal diagnoses D = {[2], [5, 7]} which is a proper subset of all minimal diagnoses D〈K,B,P,N 〉R = {[1], [2], [5, 7]}.",
      "startOffset" : 67,
      "endOffset" : 73
    }, {
      "referenceID" : 0,
      "context" : "1 seconds execution time is the set of minimal diagnoses D = {[2], [5, 7]} which is a proper subset of all minimal diagnoses D〈K,B,P,N 〉R = {[1], [2], [5, 7]}.",
      "startOffset" : 141,
      "endOffset" : 144
    }, {
      "referenceID" : 1,
      "context" : "1 seconds execution time is the set of minimal diagnoses D = {[2], [5, 7]} which is a proper subset of all minimal diagnoses D〈K,B,P,N 〉R = {[1], [2], [5, 7]}.",
      "startOffset" : 146,
      "endOffset" : 149
    }, {
      "referenceID" : 5,
      "context" : "1 seconds execution time is the set of minimal diagnoses D = {[2], [5, 7]} which is a proper subset of all minimal diagnoses D〈K,B,P,N 〉R = {[1], [2], [5, 7]}.",
      "startOffset" : 151,
      "endOffset" : 157
    }, {
      "referenceID" : 70,
      "context" : "For instance, [73] reported on one DPI where computation of all minimal diagnoses, 1782 in number, is feasible.",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 0,
      "context" : "For our example DPI, a user getting the output D = mD〈K,B,P,N 〉R = {[1], [2], [5, 7]} with the computed probabilities p([1]) = 12%, p([2]) = 60% and p([5, 7]) = 28% might decide to just inspect the diagnoses that make the most probable 80% fraction of diagnoses.",
      "startOffset" : 68,
      "endOffset" : 71
    }, {
      "referenceID" : 1,
      "context" : "For our example DPI, a user getting the output D = mD〈K,B,P,N 〉R = {[1], [2], [5, 7]} with the computed probabilities p([1]) = 12%, p([2]) = 60% and p([5, 7]) = 28% might decide to just inspect the diagnoses that make the most probable 80% fraction of diagnoses.",
      "startOffset" : 73,
      "endOffset" : 76
    }, {
      "referenceID" : 5,
      "context" : "For our example DPI, a user getting the output D = mD〈K,B,P,N 〉R = {[1], [2], [5, 7]} with the computed probabilities p([1]) = 12%, p([2]) = 60% and p([5, 7]) = 28% might decide to just inspect the diagnoses that make the most probable 80% fraction of diagnoses.",
      "startOffset" : 78,
      "endOffset" : 84
    }, {
      "referenceID" : 0,
      "context" : "For our example DPI, a user getting the output D = mD〈K,B,P,N 〉R = {[1], [2], [5, 7]} with the computed probabilities p([1]) = 12%, p([2]) = 60% and p([5, 7]) = 28% might decide to just inspect the diagnoses that make the most probable 80% fraction of diagnoses.",
      "startOffset" : 120,
      "endOffset" : 123
    }, {
      "referenceID" : 1,
      "context" : "For our example DPI, a user getting the output D = mD〈K,B,P,N 〉R = {[1], [2], [5, 7]} with the computed probabilities p([1]) = 12%, p([2]) = 60% and p([5, 7]) = 28% might decide to just inspect the diagnoses that make the most probable 80% fraction of diagnoses.",
      "startOffset" : 134,
      "endOffset" : 137
    }, {
      "referenceID" : 5,
      "context" : "For our example DPI, a user getting the output D = mD〈K,B,P,N 〉R = {[1], [2], [5, 7]} with the computed probabilities p([1]) = 12%, p([2]) = 60% and p([5, 7]) = 28% might decide to just inspect the diagnoses that make the most probable 80% fraction of diagnoses.",
      "startOffset" : 151,
      "endOffset" : 157
    }, {
      "referenceID" : 1,
      "context" : "In this case, either [2] or [5, 7] would be selected, which corresponds to a wrong choice in case E → G should be entailed be the resulting solution KB after integration with the background KB B.",
      "startOffset" : 21,
      "endOffset" : 24
    }, {
      "referenceID" : 5,
      "context" : "In this case, either [2] or [5, 7] would be selected, which corresponds to a wrong choice in case E → G should be entailed be the resulting solution KB after integration with the background KB B.",
      "startOffset" : 28,
      "endOffset" : 34
    }, {
      "referenceID" : 42,
      "context" : "Thus, the principle of interactive KB debugging is based on that of Sequential Diagnosis which has been suggested by [44] as an iterative way to localize the faulty components (among an initially large set of possibilities) in malfunctioning digital circuits by performing repeated (most informative) measurements.",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 70,
      "context" : "We have shown in our previous works [73, 74] how sequential diagnosis can be applied to KBs (ontologies).",
      "startOffset" : 36,
      "endOffset" : 44
    }, {
      "referenceID" : 71,
      "context" : "We have shown in our previous works [73, 74] how sequential diagnosis can be applied to KBs (ontologies).",
      "startOffset" : 36,
      "endOffset" : 44
    }, {
      "referenceID" : 42,
      "context" : "1Note that the minimal a-posteriori expected entropy of solution candidate probabilities as a means to select the best next measurement as used in [44] is only one of many possible active learning strategies [71].",
      "startOffset" : 147,
      "endOffset" : 151
    }, {
      "referenceID" : 71,
      "context" : "〈K,B,P ,N 〉R [74].",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 61,
      "context" : "3 and [63, 73, 74] for details.",
      "startOffset" : 6,
      "endOffset" : 18
    }, {
      "referenceID" : 70,
      "context" : "3 and [63, 73, 74] for details.",
      "startOffset" : 6,
      "endOffset" : 18
    }, {
      "referenceID" : 71,
      "context" : "3 and [63, 73, 74] for details.",
      "startOffset" : 6,
      "endOffset" : 18
    }, {
      "referenceID" : 71,
      "context" : "[74, 63, 73], a q-partition is often simply referred to as partition.",
      "startOffset" : 0,
      "endOffset" : 12
    }, {
      "referenceID" : 61,
      "context" : "[74, 63, 73], a q-partition is often simply referred to as partition.",
      "startOffset" : 0,
      "endOffset" : 12
    }, {
      "referenceID" : 70,
      "context" : "[74, 63, 73], a q-partition is often simply referred to as partition.",
      "startOffset" : 0,
      "endOffset" : 12
    }, {
      "referenceID" : 71,
      "context" : "With Algorithm 4, similar versions of which can be found in [74, 63], we present a way to compute a pool QP of queries and associated q-partitions w.",
      "startOffset" : 60,
      "endOffset" : 68
    }, {
      "referenceID" : 61,
      "context" : "With Algorithm 4, similar versions of which can be found in [74, 63], we present a way to compute a pool QP of queries and associated q-partitions w.",
      "startOffset" : 60,
      "endOffset" : 68
    }, {
      "referenceID" : 0,
      "context" : "D = mD〈K,B,P,N 〉R = {D1,D2,D3,D4} = {[1], [3], [4, 5], [2, 4]}.",
      "startOffset" : 37,
      "endOffset" : 40
    }, {
      "referenceID" : 2,
      "context" : "D = mD〈K,B,P,N 〉R = {D1,D2,D3,D4} = {[1], [3], [4, 5], [2, 4]}.",
      "startOffset" : 42,
      "endOffset" : 45
    }, {
      "referenceID" : 3,
      "context" : "D = mD〈K,B,P,N 〉R = {D1,D2,D3,D4} = {[1], [3], [4, 5], [2, 4]}.",
      "startOffset" : 47,
      "endOffset" : 53
    }, {
      "referenceID" : 1,
      "context" : "D = mD〈K,B,P,N 〉R = {D1,D2,D3,D4} = {[1], [3], [4, 5], [2, 4]}.",
      "startOffset" : 55,
      "endOffset" : 61
    }, {
      "referenceID" : 3,
      "context" : "D = mD〈K,B,P,N 〉R = {D1,D2,D3,D4} = {[1], [3], [4, 5], [2, 4]}.",
      "startOffset" : 55,
      "endOffset" : 61
    }, {
      "referenceID" : 3,
      "context" : "For our manual construction, let S = {D3,D4} = {[4, 5], [2, 4]}.",
      "startOffset" : 48,
      "endOffset" : 54
    }, {
      "referenceID" : 1,
      "context" : "For our manual construction, let S = {D3,D4} = {[4, 5], [2, 4]}.",
      "startOffset" : 56,
      "endOffset" : 62
    }, {
      "referenceID" : 3,
      "context" : "For our manual construction, let S = {D3,D4} = {[4, 5], [2, 4]}.",
      "startOffset" : 56,
      "endOffset" : 62
    }, {
      "referenceID" : 75,
      "context" : "For this purpose, DL and OWL reasoners, respectively, such as Pellet [78], HermiT [77], FaCT++ [84] or KAON25 could be used with their classification and realization reasoning services.",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 74,
      "context" : "For this purpose, DL and OWL reasoners, respectively, such as Pellet [78], HermiT [77], FaCT++ [84] or KAON25 could be used with their classification and realization reasoning services.",
      "startOffset" : 82,
      "endOffset" : 86
    }, {
      "referenceID" : 81,
      "context" : "For this purpose, DL and OWL reasoners, respectively, such as Pellet [78], HermiT [77], FaCT++ [84] or KAON25 could be used with their classification and realization reasoning services.",
      "startOffset" : 95,
      "endOffset" : 99
    }, {
      "referenceID" : 39,
      "context" : "What militates for such a black-box approach is the generality and independence of a particular logic (for which an adequate glass-box reasoner exists), the easier implementation of the debugging system and potential performance issues with a glass-box approach [41].",
      "startOffset" : 262,
      "endOffset" : 266
    }, {
      "referenceID" : 71,
      "context" : "[74, 63]).",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 61,
      "context" : "[74, 63]).",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 0,
      "context" : "this DPI were given by mC〈K,B,P,N 〉R = {C1, C2} = {〈1, 3, 4〉 , 〈1, 2, 3, 5〉} as well as mD〈K,B,P,N 〉R = {D1,D2,D3,D4} = {[1], [3], [4, 5], [2, 4]}.",
      "startOffset" : 121,
      "endOffset" : 124
    }, {
      "referenceID" : 2,
      "context" : "this DPI were given by mC〈K,B,P,N 〉R = {C1, C2} = {〈1, 3, 4〉 , 〈1, 2, 3, 5〉} as well as mD〈K,B,P,N 〉R = {D1,D2,D3,D4} = {[1], [3], [4, 5], [2, 4]}.",
      "startOffset" : 126,
      "endOffset" : 129
    }, {
      "referenceID" : 3,
      "context" : "this DPI were given by mC〈K,B,P,N 〉R = {C1, C2} = {〈1, 3, 4〉 , 〈1, 2, 3, 5〉} as well as mD〈K,B,P,N 〉R = {D1,D2,D3,D4} = {[1], [3], [4, 5], [2, 4]}.",
      "startOffset" : 131,
      "endOffset" : 137
    }, {
      "referenceID" : 1,
      "context" : "this DPI were given by mC〈K,B,P,N 〉R = {C1, C2} = {〈1, 3, 4〉 , 〈1, 2, 3, 5〉} as well as mD〈K,B,P,N 〉R = {D1,D2,D3,D4} = {[1], [3], [4, 5], [2, 4]}.",
      "startOffset" : 139,
      "endOffset" : 145
    }, {
      "referenceID" : 3,
      "context" : "this DPI were given by mC〈K,B,P,N 〉R = {C1, C2} = {〈1, 3, 4〉 , 〈1, 2, 3, 5〉} as well as mD〈K,B,P,N 〉R = {D1,D2,D3,D4} = {[1], [3], [4, 5], [2, 4]}.",
      "startOffset" : 139,
      "endOffset" : 145
    }, {
      "referenceID" : 8,
      "context" : "This could be accomplished, for example, by some resolution-based reasoning procedure [10].",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 1,
      "context" : "This holds due to the construction of Q6 as UD \\ D4 = {1, 2, 3, 4, 5} \\ [2, 4] = {1, 3, 5} (recall that we use squared brackets to denote diagnoses in spite of the fact that these are sets, cf.",
      "startOffset" : 72,
      "endOffset" : 78
    }, {
      "referenceID" : 3,
      "context" : "This holds due to the construction of Q6 as UD \\ D4 = {1, 2, 3, 4, 5} \\ [2, 4] = {1, 3, 5} (recall that we use squared brackets to denote diagnoses in spite of the fact that these are sets, cf.",
      "startOffset" : 72,
      "endOffset" : 78
    }, {
      "referenceID" : 11,
      "context" : "In other words, minus reasoning time, the generation of a pool of queries is a fixed parameter tractable problem [13] in the context of interactive KB debugging.",
      "startOffset" : 113,
      "endOffset" : 117
    }, {
      "referenceID" : 71,
      "context" : "a certain quantitative measure qsm() such as information gain [74, 63] by calculating a (generally exponentially large) pool QP of queries in a first stage, whereupon qsm(Q) ∈ R is evaluated for Q ∈ QP until the one Q∗ with optimal qsm(Q∗) is found and selected as the query to be asked to the user.",
      "startOffset" : 62,
      "endOffset" : 70
    }, {
      "referenceID" : 61,
      "context" : "a certain quantitative measure qsm() such as information gain [74, 63] by calculating a (generally exponentially large) pool QP of queries in a first stage, whereupon qsm(Q) ∈ R is evaluated for Q ∈ QP until the one Q∗ with optimal qsm(Q∗) is found and selected as the query to be asked to the user.",
      "startOffset" : 62,
      "endOffset" : 70
    }, {
      "referenceID" : 71,
      "context" : "in interactive debugging scenarios [74, 63, 73].",
      "startOffset" : 35,
      "endOffset" : 47
    }, {
      "referenceID" : 61,
      "context" : "in interactive debugging scenarios [74, 63, 73].",
      "startOffset" : 35,
      "endOffset" : 47
    }, {
      "referenceID" : 70,
      "context" : "in interactive debugging scenarios [74, 63, 73].",
      "startOffset" : 35,
      "endOffset" : 47
    }, {
      "referenceID" : 71,
      "context" : "A similar strategy called CKK has been employed in [74] for the information gain measure (see Section 5.",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 0,
      "context" : "• a maximum fault tolerance σ ∈ [0, 1] and • a mode mode ∈ {static, dynamic} that determines the used method for diagnosis computation.",
      "startOffset" : 32,
      "endOffset" : 38
    }, {
      "referenceID" : 71,
      "context" : "Diverse measures that can be used as a qsm() function in this algorithm have been discussed and evaluated within the scope of interactive KB debugging in literature [74, 63] (for details see Section 5.",
      "startOffset" : 165,
      "endOffset" : 173
    }, {
      "referenceID" : 61,
      "context" : "Diverse measures that can be used as a qsm() function in this algorithm have been discussed and evaluated within the scope of interactive KB debugging in literature [74, 63] (for details see Section 5.",
      "startOffset" : 165,
      "endOffset" : 173
    }, {
      "referenceID" : 61,
      "context" : "An example of a qsm() strategy using one such metric, namely the ratio of leading diagnoses invalidated by a test case, can be found in [63].",
      "startOffset" : 136,
      "endOffset" : 140
    }, {
      "referenceID" : 42,
      "context" : "[44])",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 71,
      "context" : "In this section, we give a brief introduction to some query selection measures qsm() that have been suggested and evaluated in literature within the scope of KB or ontology debugging [74, 63].",
      "startOffset" : 183,
      "endOffset" : 191
    }, {
      "referenceID" : 61,
      "context" : "In this section, we give a brief introduction to some query selection measures qsm() that have been suggested and evaluated in literature within the scope of KB or ontology debugging [74, 63].",
      "startOffset" : 183,
      "endOffset" : 191
    }, {
      "referenceID" : 71,
      "context" : "In our previous work [74], we have discussed entropy-based (ENT()) and split-in-half (SPL()) query selection measures.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 42,
      "context" : "As shown in [44], this leads to the definition",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 61,
      "context" : "For scenarios where a-priori probabilities are vague, we have presented another more complex query selection measure RIO() in [63] which uses a reinforcement learning strategy to constantly adapt some “risk” parameter that indicates the current amount of trust in the probabilities.",
      "startOffset" : 126,
      "endOffset" : 130
    }, {
      "referenceID" : 71,
      "context" : "High trust in the probabilities means usage of ENT() which can exploit high quality fault information well as demonstrated in the experiments conducted in [74], whereas low trust involves selection of queries that guarantee a higher worst case invalidation rate, i.",
      "startOffset" : 155,
      "endOffset" : 159
    }, {
      "referenceID" : 0,
      "context" : "In this vein, first [1] and then [2] are identified as minimal diagnoses w.",
      "startOffset" : 20,
      "endOffset" : 23
    }, {
      "referenceID" : 1,
      "context" : "In this vein, first [1] and then [2] are identified as minimal diagnoses w.",
      "startOffset" : 33,
      "endOffset" : 36
    }, {
      "referenceID" : 0,
      "context" : "Since DX∪Dcalc = ∅∪{[1], [2]} has a cardinality of nmin = nmax = 2, the stop criterion of STATICHS causes it to terminate and return 〈Dcalc ∪DX,Ccalc,Q,D×〉 = 〈{[1], [2]} , {〈1, 2, 5〉} , [{5}], ∅〉 (because DX and D× are initially empty sets), as shown in the upper right column in Figure 6.",
      "startOffset" : 20,
      "endOffset" : 23
    }, {
      "referenceID" : 1,
      "context" : "Since DX∪Dcalc = ∅∪{[1], [2]} has a cardinality of nmin = nmax = 2, the stop criterion of STATICHS causes it to terminate and return 〈Dcalc ∪DX,Ccalc,Q,D×〉 = 〈{[1], [2]} , {〈1, 2, 5〉} , [{5}], ∅〉 (because DX and D× are initially empty sets), as shown in the upper right column in Figure 6.",
      "startOffset" : 25,
      "endOffset" : 28
    }, {
      "referenceID" : 0,
      "context" : "Since DX∪Dcalc = ∅∪{[1], [2]} has a cardinality of nmin = nmax = 2, the stop criterion of STATICHS causes it to terminate and return 〈Dcalc ∪DX,Ccalc,Q,D×〉 = 〈{[1], [2]} , {〈1, 2, 5〉} , [{5}], ∅〉 (because DX and D× are initially empty sets), as shown in the upper right column in Figure 6.",
      "startOffset" : 160,
      "endOffset" : 163
    }, {
      "referenceID" : 1,
      "context" : "Since DX∪Dcalc = ∅∪{[1], [2]} has a cardinality of nmin = nmax = 2, the stop criterion of STATICHS causes it to terminate and return 〈Dcalc ∪DX,Ccalc,Q,D×〉 = 〈{[1], [2]} , {〈1, 2, 5〉} , [{5}], ∅〉 (because DX and D× are initially empty sets), as shown in the upper right column in Figure 6.",
      "startOffset" : 165,
      "endOffset" : 168
    }, {
      "referenceID" : 0,
      "context" : "Then, in Algorithm 5, outside of the STATICHS procedure, the first query Q1 = {E → ¬A} is computed from the leading diagnoses set {[1], [2]}.",
      "startOffset" : 131,
      "endOffset" : 134
    }, {
      "referenceID" : 1,
      "context" : "Then, in Algorithm 5, outside of the STATICHS procedure, the first query Q1 = {E → ¬A} is computed from the leading diagnoses set {[1], [2]}.",
      "startOffset" : 136,
      "endOffset" : 139
    }, {
      "referenceID" : 0,
      "context" : "The q-partition P(Q1) associated withQ1 is 〈{[1]} , {[2]}, ∅〉.",
      "startOffset" : 45,
      "endOffset" : 48
    }, {
      "referenceID" : 1,
      "context" : "The q-partition P(Q1) associated withQ1 is 〈{[1]} , {[2]}, ∅〉.",
      "startOffset" : 53,
      "endOffset" : 56
    }, {
      "referenceID" : 0,
      "context" : "Thence, the set Dout is calculated from P(Q1) as D(Q1) = {[1]} (due to negative answer, cf.",
      "startOffset" : 58,
      "endOffset" : 61
    }, {
      "referenceID" : 1,
      "context" : "6), deleted from DX := DX ∪Dcalc to yield DX = {[2]} and added to D× to yield D× = {[1]}.",
      "startOffset" : 48,
      "endOffset" : 51
    }, {
      "referenceID" : 0,
      "context" : "6), deleted from DX := DX ∪Dcalc to yield DX = {[2]} and added to D× to yield D× = {[1]}.",
      "startOffset" : 84,
      "endOffset" : 87
    }, {
      "referenceID" : 0,
      "context" : "Whereas the branches with edge labels {5, 1} and {5, 2} correspond to proper supersets of the minimal diagnoses [1] and [2], respectively, w.",
      "startOffset" : 112,
      "endOffset" : 115
    }, {
      "referenceID" : 1,
      "context" : "Whereas the branches with edge labels {5, 1} and {5, 2} correspond to proper supersets of the minimal diagnoses [1] and [2], respectively, w.",
      "startOffset" : 120,
      "endOffset" : 123
    }, {
      "referenceID" : 5,
      "context" : "the input DPI 〈K,B,P ,N 〉R and are thus closed by the non-minimality criterion tested in the SLABEL function, the branch with edge labels {5, 7} is identified as a minimal diagnosis D3 := [5, 7] w.",
      "startOffset" : 188,
      "endOffset" : 194
    }, {
      "referenceID" : 0,
      "context" : "Further, let the user have extracted term and logical construct probabilities pK̃∪K(ax ) ∈ [0, 1] for ax ∈ K for auth from this data.",
      "startOffset" : 91,
      "endOffset" : 97
    }, {
      "referenceID" : 0,
      "context" : "This function pK̃∪K : K̃ ∪ K → [0, 1] is then provided as an input to Algorithm 5.",
      "startOffset" : 31,
      "endOffset" : 37
    }, {
      "referenceID" : 0,
      "context" : "Since STATICHS always treats the first node of Q next, it identifies the first minimal diagnosis D1 := [1, 4] w.",
      "startOffset" : 103,
      "endOffset" : 109
    }, {
      "referenceID" : 3,
      "context" : "Since STATICHS always treats the first node of Q next, it identifies the first minimal diagnosis D1 := [1, 4] w.",
      "startOffset" : 103,
      "endOffset" : 109
    }, {
      "referenceID" : 0,
      "context" : "In steps 4 © and 8 ©, two further minimal diagnoses D2 := [1, 6] and D3 := [5, 4] are detected.",
      "startOffset" : 58,
      "endOffset" : 64
    }, {
      "referenceID" : 4,
      "context" : "In steps 4 © and 8 ©, two further minimal diagnoses D2 := [1, 6] and D3 := [5, 4] are detected.",
      "startOffset" : 58,
      "endOffset" : 64
    }, {
      "referenceID" : 3,
      "context" : "In steps 4 © and 8 ©, two further minimal diagnoses D2 := [1, 6] and D3 := [5, 4] are detected.",
      "startOffset" : 75,
      "endOffset" : 81
    }, {
      "referenceID" : 0,
      "context" : "From this set of leading diagnoses DX := DX ∪ Dcalc, the probability measure pD : DX → [0, 1] is computed by the function GETPROBDIST (cf.",
      "startOffset" : 87,
      "endOffset" : 93
    }, {
      "referenceID" : 1,
      "context" : "In the same fashion, further node labelings are conducted in iteration 2 until |DX ∪ Dcalc| = | {D1,D2} ∪ {[2, 1]} | = 3 = nmin = nmax holds again.",
      "startOffset" : 107,
      "endOffset" : 113
    }, {
      "referenceID" : 0,
      "context" : "In the same fashion, further node labelings are conducted in iteration 2 until |DX ∪ Dcalc| = | {D1,D2} ∪ {[2, 1]} | = 3 = nmin = nmax holds again.",
      "startOffset" : 107,
      "endOffset" : 113
    }, {
      "referenceID" : 1,
      "context" : "Moreover, we want to point out that another minimal diagnosis (D4 = [2, 4, 6]) is found in iteration 2 before D5 is detected.",
      "startOffset" : 68,
      "endOffset" : 77
    }, {
      "referenceID" : 3,
      "context" : "Moreover, we want to point out that another minimal diagnosis (D4 = [2, 4, 6]) is found in iteration 2 before D5 is detected.",
      "startOffset" : 68,
      "endOffset" : 77
    }, {
      "referenceID" : 4,
      "context" : "Moreover, we want to point out that another minimal diagnosis (D4 = [2, 4, 6]) is found in iteration 2 before D5 is detected.",
      "startOffset" : 68,
      "endOffset" : 77
    }, {
      "referenceID" : 0,
      "context" : "Iteration 1 〉 DX ∪Dcalc = ∅ ∪ {D1,D2} = {[1], [2]} Q = [{5}] Ccalc = {〈1, 2, 5〉} D× = ∅",
      "startOffset" : 41,
      "endOffset" : 44
    }, {
      "referenceID" : 1,
      "context" : "Iteration 1 〉 DX ∪Dcalc = ∅ ∪ {D1,D2} = {[1], [2]} Q = [{5}] Ccalc = {〈1, 2, 5〉} D× = ∅",
      "startOffset" : 46,
      "endOffset" : 49
    }, {
      "referenceID" : 1,
      "context" : "Iteration 2 〉 DX ∪Dcalc = {D2} ∪ {D3} = {[2], [5, 7]}",
      "startOffset" : 41,
      "endOffset" : 44
    }, {
      "referenceID" : 5,
      "context" : "Iteration 2 〉 DX ∪Dcalc = {D2} ∪ {D3} = {[2], [5, 7]}",
      "startOffset" : 46,
      "endOffset" : 52
    }, {
      "referenceID" : 0,
      "context" : "D× = {D1} = {[1]}",
      "startOffset" : 13,
      "endOffset" : 16
    }, {
      "referenceID" : 5,
      "context" : "Iteration 3 〉 DX ∪Dcalc = {D3} ∪ ∅ = {[5, 7]}",
      "startOffset" : 38,
      "endOffset" : 44
    }, {
      "referenceID" : 0,
      "context" : "D× = {D1,D2} = {[1], [2]}",
      "startOffset" : 16,
      "endOffset" : 19
    }, {
      "referenceID" : 1,
      "context" : "D× = {D1,D2} = {[1], [2]}",
      "startOffset" : 21,
      "endOffset" : 24
    }, {
      "referenceID" : 0,
      "context" : "D X ∪ D c a lc = ∅ ∪ {D 1 ,D 2 ,D 3 } = { 1 ,4 ], [1 ,6 ], [5 ,4 ]} , Q = [{ 5 ,6 } ,{ 2 ,4 ,6 } ,{ 1 ,2 } ,{ 2 ,1 } ,{ 2 ,3 } ,{ 5 ,2 } ,{ 2 ,4 ,1 } ,{ 2 ,4 ,5 } ,{ 2 ,4 ,8 } , D × = ∅",
      "startOffset" : 50,
      "endOffset" : 57
    }, {
      "referenceID" : 4,
      "context" : "D X ∪ D c a lc = ∅ ∪ {D 1 ,D 2 ,D 3 } = { 1 ,4 ], [1 ,6 ], [5 ,4 ]} , Q = [{ 5 ,6 } ,{ 2 ,4 ,6 } ,{ 1 ,2 } ,{ 2 ,1 } ,{ 2 ,3 } ,{ 5 ,2 } ,{ 2 ,4 ,1 } ,{ 2 ,4 ,5 } ,{ 2 ,4 ,8 } , D × = ∅",
      "startOffset" : 50,
      "endOffset" : 57
    }, {
      "referenceID" : 3,
      "context" : "D X ∪ D c a lc = ∅ ∪ {D 1 ,D 2 ,D 3 } = { 1 ,4 ], [1 ,6 ], [5 ,4 ]} , Q = [{ 5 ,6 } ,{ 2 ,4 ,6 } ,{ 1 ,2 } ,{ 2 ,1 } ,{ 2 ,3 } ,{ 5 ,2 } ,{ 2 ,4 ,1 } ,{ 2 ,4 ,5 } ,{ 2 ,4 ,8 } , D × = ∅",
      "startOffset" : 59,
      "endOffset" : 66
    }, {
      "referenceID" : 0,
      "context" : "D X ∪ D c a lc = {D 1 ,D 2 } ∪ {D 5 } = { 1 ,4 ], [1 ,6 ], [2 ,1 ]} , Q = [{ 2 ,4 ,6 } ,{ 2 ,3 } ,{ 5 ,2 } ,{ 2 ,4 ,1 } ,{ 5 ,6 ,1 } ,{ 2 ,4 ,5 } ,{ 5 ,6 ,3 } , D × = {D 3 ,D 4 } = { 5 ,4 ], [2 ,4 ,6 ]}",
      "startOffset" : 50,
      "endOffset" : 57
    }, {
      "referenceID" : 4,
      "context" : "D X ∪ D c a lc = {D 1 ,D 2 } ∪ {D 5 } = { 1 ,4 ], [1 ,6 ], [2 ,1 ]} , Q = [{ 2 ,4 ,6 } ,{ 2 ,3 } ,{ 5 ,2 } ,{ 2 ,4 ,1 } ,{ 5 ,6 ,1 } ,{ 2 ,4 ,5 } ,{ 5 ,6 ,3 } , D × = {D 3 ,D 4 } = { 5 ,4 ], [2 ,4 ,6 ]}",
      "startOffset" : 50,
      "endOffset" : 57
    }, {
      "referenceID" : 1,
      "context" : "D X ∪ D c a lc = {D 1 ,D 2 } ∪ {D 5 } = { 1 ,4 ], [1 ,6 ], [2 ,1 ]} , Q = [{ 2 ,4 ,6 } ,{ 2 ,3 } ,{ 5 ,2 } ,{ 2 ,4 ,1 } ,{ 5 ,6 ,1 } ,{ 2 ,4 ,5 } ,{ 5 ,6 ,3 } , D × = {D 3 ,D 4 } = { 5 ,4 ], [2 ,4 ,6 ]}",
      "startOffset" : 59,
      "endOffset" : 66
    }, {
      "referenceID" : 0,
      "context" : "D X ∪ D c a lc = {D 1 ,D 2 } ∪ {D 5 } = { 1 ,4 ], [1 ,6 ], [2 ,1 ]} , Q = [{ 2 ,4 ,6 } ,{ 2 ,3 } ,{ 5 ,2 } ,{ 2 ,4 ,1 } ,{ 5 ,6 ,1 } ,{ 2 ,4 ,5 } ,{ 5 ,6 ,3 } , D × = {D 3 ,D 4 } = { 5 ,4 ], [2 ,4 ,6 ]}",
      "startOffset" : 59,
      "endOffset" : 66
    }, {
      "referenceID" : 1,
      "context" : "D X ∪ D c a lc = {D 1 ,D 2 } ∪ {D 5 } = { 1 ,4 ], [1 ,6 ], [2 ,1 ]} , Q = [{ 2 ,4 ,6 } ,{ 2 ,3 } ,{ 5 ,2 } ,{ 2 ,4 ,1 } ,{ 5 ,6 ,1 } ,{ 2 ,4 ,5 } ,{ 5 ,6 ,3 } , D × = {D 3 ,D 4 } = { 5 ,4 ], [2 ,4 ,6 ]}",
      "startOffset" : 191,
      "endOffset" : 201
    }, {
      "referenceID" : 3,
      "context" : "D X ∪ D c a lc = {D 1 ,D 2 } ∪ {D 5 } = { 1 ,4 ], [1 ,6 ], [2 ,1 ]} , Q = [{ 2 ,4 ,6 } ,{ 2 ,3 } ,{ 5 ,2 } ,{ 2 ,4 ,1 } ,{ 5 ,6 ,1 } ,{ 2 ,4 ,5 } ,{ 5 ,6 ,3 } , D × = {D 3 ,D 4 } = { 5 ,4 ], [2 ,4 ,6 ]}",
      "startOffset" : 191,
      "endOffset" : 201
    }, {
      "referenceID" : 4,
      "context" : "D X ∪ D c a lc = {D 1 ,D 2 } ∪ {D 5 } = { 1 ,4 ], [1 ,6 ], [2 ,1 ]} , Q = [{ 2 ,4 ,6 } ,{ 2 ,3 } ,{ 5 ,2 } ,{ 2 ,4 ,1 } ,{ 5 ,6 ,1 } ,{ 2 ,4 ,5 } ,{ 5 ,6 ,3 } , D × = {D 3 ,D 4 } = { 5 ,4 ], [2 ,4 ,6 ]}",
      "startOffset" : 191,
      "endOffset" : 201
    }, {
      "referenceID" : 1,
      "context" : "D × = {D 3 ,D 4 ,D 2 ,D 5 ,D 6 ,D 7 ,D 8 ,D 9 ,D 1 0 } = { 5 ,4 ], [2 ,4 ,6 ], [1 ,6 ], [2 ,1 ], [2 ,4 ,8 ], [5 ,6 ,3 ], [2 ,3 ,6 ], [2 ,3 ,8 ], [5 ,2 ,3 ]} ,",
      "startOffset" : 67,
      "endOffset" : 77
    }, {
      "referenceID" : 3,
      "context" : "D × = {D 3 ,D 4 ,D 2 ,D 5 ,D 6 ,D 7 ,D 8 ,D 9 ,D 1 0 } = { 5 ,4 ], [2 ,4 ,6 ], [1 ,6 ], [2 ,1 ], [2 ,4 ,8 ], [5 ,6 ,3 ], [2 ,3 ,6 ], [2 ,3 ,8 ], [5 ,2 ,3 ]} ,",
      "startOffset" : 67,
      "endOffset" : 77
    }, {
      "referenceID" : 4,
      "context" : "D × = {D 3 ,D 4 ,D 2 ,D 5 ,D 6 ,D 7 ,D 8 ,D 9 ,D 1 0 } = { 5 ,4 ], [2 ,4 ,6 ], [1 ,6 ], [2 ,1 ], [2 ,4 ,8 ], [5 ,6 ,3 ], [2 ,3 ,6 ], [2 ,3 ,8 ], [5 ,2 ,3 ]} ,",
      "startOffset" : 67,
      "endOffset" : 77
    }, {
      "referenceID" : 0,
      "context" : "D × = {D 3 ,D 4 ,D 2 ,D 5 ,D 6 ,D 7 ,D 8 ,D 9 ,D 1 0 } = { 5 ,4 ], [2 ,4 ,6 ], [1 ,6 ], [2 ,1 ], [2 ,4 ,8 ], [5 ,6 ,3 ], [2 ,3 ,6 ], [2 ,3 ,8 ], [5 ,2 ,3 ]} ,",
      "startOffset" : 79,
      "endOffset" : 86
    }, {
      "referenceID" : 4,
      "context" : "D × = {D 3 ,D 4 ,D 2 ,D 5 ,D 6 ,D 7 ,D 8 ,D 9 ,D 1 0 } = { 5 ,4 ], [2 ,4 ,6 ], [1 ,6 ], [2 ,1 ], [2 ,4 ,8 ], [5 ,6 ,3 ], [2 ,3 ,6 ], [2 ,3 ,8 ], [5 ,2 ,3 ]} ,",
      "startOffset" : 79,
      "endOffset" : 86
    }, {
      "referenceID" : 1,
      "context" : "D × = {D 3 ,D 4 ,D 2 ,D 5 ,D 6 ,D 7 ,D 8 ,D 9 ,D 1 0 } = { 5 ,4 ], [2 ,4 ,6 ], [1 ,6 ], [2 ,1 ], [2 ,4 ,8 ], [5 ,6 ,3 ], [2 ,3 ,6 ], [2 ,3 ,8 ], [5 ,2 ,3 ]} ,",
      "startOffset" : 88,
      "endOffset" : 95
    }, {
      "referenceID" : 0,
      "context" : "D × = {D 3 ,D 4 ,D 2 ,D 5 ,D 6 ,D 7 ,D 8 ,D 9 ,D 1 0 } = { 5 ,4 ], [2 ,4 ,6 ], [1 ,6 ], [2 ,1 ], [2 ,4 ,8 ], [5 ,6 ,3 ], [2 ,3 ,6 ], [2 ,3 ,8 ], [5 ,2 ,3 ]} ,",
      "startOffset" : 88,
      "endOffset" : 95
    }, {
      "referenceID" : 1,
      "context" : "D × = {D 3 ,D 4 ,D 2 ,D 5 ,D 6 ,D 7 ,D 8 ,D 9 ,D 1 0 } = { 5 ,4 ], [2 ,4 ,6 ], [1 ,6 ], [2 ,1 ], [2 ,4 ,8 ], [5 ,6 ,3 ], [2 ,3 ,6 ], [2 ,3 ,8 ], [5 ,2 ,3 ]} ,",
      "startOffset" : 97,
      "endOffset" : 107
    }, {
      "referenceID" : 3,
      "context" : "D × = {D 3 ,D 4 ,D 2 ,D 5 ,D 6 ,D 7 ,D 8 ,D 9 ,D 1 0 } = { 5 ,4 ], [2 ,4 ,6 ], [1 ,6 ], [2 ,1 ], [2 ,4 ,8 ], [5 ,6 ,3 ], [2 ,3 ,6 ], [2 ,3 ,8 ], [5 ,2 ,3 ]} ,",
      "startOffset" : 97,
      "endOffset" : 107
    }, {
      "referenceID" : 6,
      "context" : "D × = {D 3 ,D 4 ,D 2 ,D 5 ,D 6 ,D 7 ,D 8 ,D 9 ,D 1 0 } = { 5 ,4 ], [2 ,4 ,6 ], [1 ,6 ], [2 ,1 ], [2 ,4 ,8 ], [5 ,6 ,3 ], [2 ,3 ,6 ], [2 ,3 ,8 ], [5 ,2 ,3 ]} ,",
      "startOffset" : 97,
      "endOffset" : 107
    }, {
      "referenceID" : 4,
      "context" : "D × = {D 3 ,D 4 ,D 2 ,D 5 ,D 6 ,D 7 ,D 8 ,D 9 ,D 1 0 } = { 5 ,4 ], [2 ,4 ,6 ], [1 ,6 ], [2 ,1 ], [2 ,4 ,8 ], [5 ,6 ,3 ], [2 ,3 ,6 ], [2 ,3 ,8 ], [5 ,2 ,3 ]} ,",
      "startOffset" : 109,
      "endOffset" : 119
    }, {
      "referenceID" : 2,
      "context" : "D × = {D 3 ,D 4 ,D 2 ,D 5 ,D 6 ,D 7 ,D 8 ,D 9 ,D 1 0 } = { 5 ,4 ], [2 ,4 ,6 ], [1 ,6 ], [2 ,1 ], [2 ,4 ,8 ], [5 ,6 ,3 ], [2 ,3 ,6 ], [2 ,3 ,8 ], [5 ,2 ,3 ]} ,",
      "startOffset" : 109,
      "endOffset" : 119
    }, {
      "referenceID" : 1,
      "context" : "D × = {D 3 ,D 4 ,D 2 ,D 5 ,D 6 ,D 7 ,D 8 ,D 9 ,D 1 0 } = { 5 ,4 ], [2 ,4 ,6 ], [1 ,6 ], [2 ,1 ], [2 ,4 ,8 ], [5 ,6 ,3 ], [2 ,3 ,6 ], [2 ,3 ,8 ], [5 ,2 ,3 ]} ,",
      "startOffset" : 121,
      "endOffset" : 131
    }, {
      "referenceID" : 2,
      "context" : "D × = {D 3 ,D 4 ,D 2 ,D 5 ,D 6 ,D 7 ,D 8 ,D 9 ,D 1 0 } = { 5 ,4 ], [2 ,4 ,6 ], [1 ,6 ], [2 ,1 ], [2 ,4 ,8 ], [5 ,6 ,3 ], [2 ,3 ,6 ], [2 ,3 ,8 ], [5 ,2 ,3 ]} ,",
      "startOffset" : 121,
      "endOffset" : 131
    }, {
      "referenceID" : 4,
      "context" : "D × = {D 3 ,D 4 ,D 2 ,D 5 ,D 6 ,D 7 ,D 8 ,D 9 ,D 1 0 } = { 5 ,4 ], [2 ,4 ,6 ], [1 ,6 ], [2 ,1 ], [2 ,4 ,8 ], [5 ,6 ,3 ], [2 ,3 ,6 ], [2 ,3 ,8 ], [5 ,2 ,3 ]} ,",
      "startOffset" : 121,
      "endOffset" : 131
    }, {
      "referenceID" : 1,
      "context" : "D × = {D 3 ,D 4 ,D 2 ,D 5 ,D 6 ,D 7 ,D 8 ,D 9 ,D 1 0 } = { 5 ,4 ], [2 ,4 ,6 ], [1 ,6 ], [2 ,1 ], [2 ,4 ,8 ], [5 ,6 ,3 ], [2 ,3 ,6 ], [2 ,3 ,8 ], [5 ,2 ,3 ]} ,",
      "startOffset" : 133,
      "endOffset" : 143
    }, {
      "referenceID" : 2,
      "context" : "D × = {D 3 ,D 4 ,D 2 ,D 5 ,D 6 ,D 7 ,D 8 ,D 9 ,D 1 0 } = { 5 ,4 ], [2 ,4 ,6 ], [1 ,6 ], [2 ,1 ], [2 ,4 ,8 ], [5 ,6 ,3 ], [2 ,3 ,6 ], [2 ,3 ,8 ], [5 ,2 ,3 ]} ,",
      "startOffset" : 133,
      "endOffset" : 143
    }, {
      "referenceID" : 6,
      "context" : "D × = {D 3 ,D 4 ,D 2 ,D 5 ,D 6 ,D 7 ,D 8 ,D 9 ,D 1 0 } = { 5 ,4 ], [2 ,4 ,6 ], [1 ,6 ], [2 ,1 ], [2 ,4 ,8 ], [5 ,6 ,3 ], [2 ,3 ,6 ], [2 ,3 ,8 ], [5 ,2 ,3 ]} ,",
      "startOffset" : 133,
      "endOffset" : 143
    }, {
      "referenceID" : 1,
      "context" : "D × = {D 3 ,D 4 ,D 2 ,D 5 ,D 6 ,D 7 ,D 8 ,D 9 ,D 1 0 } = { 5 ,4 ], [2 ,4 ,6 ], [1 ,6 ], [2 ,1 ], [2 ,4 ,8 ], [5 ,6 ,3 ], [2 ,3 ,6 ], [2 ,3 ,8 ], [5 ,2 ,3 ]} ,",
      "startOffset" : 145,
      "endOffset" : 155
    }, {
      "referenceID" : 2,
      "context" : "D × = {D 3 ,D 4 ,D 2 ,D 5 ,D 6 ,D 7 ,D 8 ,D 9 ,D 1 0 } = { 5 ,4 ], [2 ,4 ,6 ], [1 ,6 ], [2 ,1 ], [2 ,4 ,8 ], [5 ,6 ,3 ], [2 ,3 ,6 ], [2 ,3 ,8 ], [5 ,2 ,3 ]} ,",
      "startOffset" : 145,
      "endOffset" : 155
    }, {
      "referenceID" : 0,
      "context" : "〈K,B,P ,N 〉R and three successor nodes, namely nd1 := [1], nd2 := [2] as well as nd3 := [5] with nd1.",
      "startOffset" : 54,
      "endOffset" : 57
    }, {
      "referenceID" : 1,
      "context" : "〈K,B,P ,N 〉R and three successor nodes, namely nd1 := [1], nd2 := [2] as well as nd3 := [5] with nd1.",
      "startOffset" : 66,
      "endOffset" : 69
    }, {
      "referenceID" : 0,
      "context" : "In this vein, first [1] and then [2] are identified as minimal diagnoses w.",
      "startOffset" : 20,
      "endOffset" : 23
    }, {
      "referenceID" : 1,
      "context" : "In this vein, first [1] and then [2] are identified as minimal diagnoses w.",
      "startOffset" : 33,
      "endOffset" : 36
    }, {
      "referenceID" : 0,
      "context" : "Since Dcalc = {[1], [2]} has a cardinality of nmin = nmax = 2, the stop criterion of DYNAMICHS causes it to terminate and return 〈Dcalc,Q,Ccalc,Q,D×,D⊃,Qdup〉 = 〈 {[1], [2]}, [[5]], {〈1, 2, 5〉}, ∅, ∅, []〉, as shown in the upper right column in Figure 6.",
      "startOffset" : 15,
      "endOffset" : 18
    }, {
      "referenceID" : 1,
      "context" : "Since Dcalc = {[1], [2]} has a cardinality of nmin = nmax = 2, the stop criterion of DYNAMICHS causes it to terminate and return 〈Dcalc,Q,Ccalc,Q,D×,D⊃,Qdup〉 = 〈 {[1], [2]}, [[5]], {〈1, 2, 5〉}, ∅, ∅, []〉, as shown in the upper right column in Figure 6.",
      "startOffset" : 20,
      "endOffset" : 23
    }, {
      "referenceID" : 0,
      "context" : "Since Dcalc = {[1], [2]} has a cardinality of nmin = nmax = 2, the stop criterion of DYNAMICHS causes it to terminate and return 〈Dcalc,Q,Ccalc,Q,D×,D⊃,Qdup〉 = 〈 {[1], [2]}, [[5]], {〈1, 2, 5〉}, ∅, ∅, []〉, as shown in the upper right column in Figure 6.",
      "startOffset" : 163,
      "endOffset" : 166
    }, {
      "referenceID" : 1,
      "context" : "Since Dcalc = {[1], [2]} has a cardinality of nmin = nmax = 2, the stop criterion of DYNAMICHS causes it to terminate and return 〈Dcalc,Q,Ccalc,Q,D×,D⊃,Qdup〉 = 〈 {[1], [2]}, [[5]], {〈1, 2, 5〉}, ∅, ∅, []〉, as shown in the upper right column in Figure 6.",
      "startOffset" : 168,
      "endOffset" : 171
    }, {
      "referenceID" : 0,
      "context" : "Then, in Algorithm 5, outside of the DYNAMICHS procedure, the first query Q1 = {E → ¬A} is computed from the leading diagnoses set {[1], [2]}.",
      "startOffset" : 132,
      "endOffset" : 135
    }, {
      "referenceID" : 1,
      "context" : "Then, in Algorithm 5, outside of the DYNAMICHS procedure, the first query Q1 = {E → ¬A} is computed from the leading diagnoses set {[1], [2]}.",
      "startOffset" : 137,
      "endOffset" : 140
    }, {
      "referenceID" : 0,
      "context" : "The q-partition P(Q1) associated with Q1 is 〈{[1]} , {[2]} , ∅〉.",
      "startOffset" : 46,
      "endOffset" : 49
    }, {
      "referenceID" : 1,
      "context" : "The q-partition P(Q1) associated with Q1 is 〈{[1]} , {[2]} , ∅〉.",
      "startOffset" : 54,
      "endOffset" : 57
    }, {
      "referenceID" : 0,
      "context" : "Thence, the set Dout is calculated from P(Q1) as D(Q1) = {[1]} (due to negative answer, cf.",
      "startOffset" : 58,
      "endOffset" : 61
    }, {
      "referenceID" : 1,
      "context" : "6), deleted from DX := DX ∪Dcalc to yield DX = {[2]} and added to D× to yield D× = {[1]}.",
      "startOffset" : 48,
      "endOffset" : 51
    }, {
      "referenceID" : 0,
      "context" : "6), deleted from DX := DX ∪Dcalc to yield DX = {[2]} and added to D× to yield D× = {[1]}.",
      "startOffset" : 84,
      "endOffset" : 87
    }, {
      "referenceID" : 1,
      "context" : "Namely, first (step 5 ©) the node [2] is directly labeled by valid (line 8) since it is a known minimal diagnosis w.",
      "startOffset" : 34,
      "endOffset" : 37
    }, {
      "referenceID" : 0,
      "context" : "the current DPI and three further nodes ([5, 1], [5, 2] and [5, 7], all with nd.",
      "startOffset" : 41,
      "endOffset" : 47
    }, {
      "referenceID" : 1,
      "context" : "the current DPI and three further nodes ([5, 1], [5, 2] and [5, 7], all with nd.",
      "startOffset" : 49,
      "endOffset" : 55
    }, {
      "referenceID" : 5,
      "context" : "the current DPI and three further nodes ([5, 1], [5, 2] and [5, 7], all with nd.",
      "startOffset" : 60,
      "endOffset" : 66
    }, {
      "referenceID" : 0,
      "context" : "Now, [5, 1] (first-in-first-out) is the foremost node in Q and is thus processed next and found to be a minimal diagnosis w.",
      "startOffset" : 5,
      "endOffset" : 11
    }, {
      "referenceID" : 1,
      "context" : "diagnoses Dcalc = {[2], [5, 1]}.",
      "startOffset" : 19,
      "endOffset" : 22
    }, {
      "referenceID" : 0,
      "context" : "diagnoses Dcalc = {[2], [5, 1]}.",
      "startOffset" : 24,
      "endOffset" : 30
    }, {
      "referenceID" : 0,
      "context" : "1 where the node {5, 1} never became part of Q in STATICHS due to the existence of a minimal diagnosis [1] w.",
      "startOffset" : 103,
      "endOffset" : 106
    }, {
      "referenceID" : 0,
      "context" : "the current DPI if all (known) diagnoses (here, only [1]) that are proper subsets of it have already been pruned.",
      "startOffset" : 53,
      "endOffset" : 56
    }, {
      "referenceID" : 0,
      "context" : "This leading diagnosis [5, 1] is also the reason why the second query Q2 = {E → G} is different from the second query (Y → ¬A) calculated in Example 6.",
      "startOffset" : 23,
      "endOffset" : 29
    }, {
      "referenceID" : 1,
      "context" : "1) is answered negatively and Q3 is added to N ′ yielding the current DPI 〈K,B,P ,N ∪ {Q1, Q2, Q3}〉R, the UPDATETREE function not only prunes [2] = D2 ∈ D× and adds [5, 7] = D4 ∈ DX to Q as we delineated above for the first query Q1, but adds [5, 2] ∈ D⊃ to Q as well.",
      "startOffset" : 142,
      "endOffset" : 145
    }, {
      "referenceID" : 5,
      "context" : "1) is answered negatively and Q3 is added to N ′ yielding the current DPI 〈K,B,P ,N ∪ {Q1, Q2, Q3}〉R, the UPDATETREE function not only prunes [2] = D2 ∈ D× and adds [5, 7] = D4 ∈ DX to Q as we delineated above for the first query Q1, but adds [5, 2] ∈ D⊃ to Q as well.",
      "startOffset" : 165,
      "endOffset" : 171
    }, {
      "referenceID" : 1,
      "context" : "1) is answered negatively and Q3 is added to N ′ yielding the current DPI 〈K,B,P ,N ∪ {Q1, Q2, Q3}〉R, the UPDATETREE function not only prunes [2] = D2 ∈ D× and adds [5, 7] = D4 ∈ DX to Q as we delineated above for the first query Q1, but adds [5, 2] ∈ D⊃ to Q as well.",
      "startOffset" : 243,
      "endOffset" : 249
    }, {
      "referenceID" : 1,
      "context" : "The reason for that is the deletion of the minimal diagnosis [2] w.",
      "startOffset" : 61,
      "endOffset" : 64
    }, {
      "referenceID" : 1,
      "context" : "the last-but-one DPI 〈K,B,P ,N ∪ {Q1, Q2}〉R wherefore the last evidence for the non-minimality of node [5, 2] has been deleted.",
      "startOffset" : 103,
      "endOffset" : 109
    }, {
      "referenceID" : 1,
      "context" : "Hence, the status of [5, 2] as a non-minimal diagnosis is no more justified wherefore it must be added to the queue to preserve the completeness of the algorithm w.",
      "startOffset" : 21,
      "endOffset" : 27
    }, {
      "referenceID" : 1,
      "context" : "And, indeed, [5, 2] is identified as minimal diagnosis (D5) in iteration 4.",
      "startOffset" : 13,
      "endOffset" : 19
    }, {
      "referenceID" : 5,
      "context" : "• In this example, the same minimal diagnosis [5, 7] is used to compute the finally returned solution KB as in Example 6.",
      "startOffset" : 46,
      "endOffset" : 52
    }, {
      "referenceID" : 5,
      "context" : "The only difference between both outputs is that the KB (K \\ [5, 7]) ∪ Q4 returned by DYNAMICHS in this example contains the new positive test case Q4 ∈ P ′.",
      "startOffset" : 61,
      "endOffset" : 67
    }, {
      "referenceID" : 0,
      "context" : "Iteration 1 〉 Dcalc = {D1,D2} = {[1], [2]}",
      "startOffset" : 33,
      "endOffset" : 36
    }, {
      "referenceID" : 1,
      "context" : "Iteration 1 〉 Dcalc = {D1,D2} = {[1], [2]}",
      "startOffset" : 38,
      "endOffset" : 41
    }, {
      "referenceID" : 1,
      "context" : "• replace by 〈2, 5〉 all node labels in the tree that are proper supersets of 〈2, 5〉 ⇒ D⊃ = ∅, Ccalc = {〈2, 5〉}, Q = [[2], [5]], Qdup = [], 〉 1 © 〈 A1, 2, 5 〉C",
      "startOffset" : 117,
      "endOffset" : 120
    }, {
      "referenceID" : 1,
      "context" : "Iteration 2 〉 Dcalc = {D2,D3} = {[2], [5, 1]}",
      "startOffset" : 33,
      "endOffset" : 36
    }, {
      "referenceID" : 0,
      "context" : "Iteration 2 〉 Dcalc = {D2,D3} = {[2], [5, 1]}",
      "startOffset" : 38,
      "endOffset" : 44
    }, {
      "referenceID" : 1,
      "context" : "Q = [[5, 2], [5, 7]]",
      "startOffset" : 5,
      "endOffset" : 11
    }, {
      "referenceID" : 5,
      "context" : "Q = [[5, 2], [5, 7]]",
      "startOffset" : 13,
      "endOffset" : 19
    }, {
      "referenceID" : 1,
      "context" : "QRC (D3): QX(〈{2, 7} ,B,P ,N ∪ {Q1, Q2}〉) = 〈2, 7〉 ⇒ PRUNE: 〈1, 2, 7〉 → 〈2, 7〉 ⇒ D⊃ = ∅, Ccalc = {〈2, 5〉 , 〈2, 7〉} Q = [[2], [5, 2], [5, 7]], Qdup = [] 〉 1 ©〈2, 5〉",
      "startOffset" : 120,
      "endOffset" : 123
    }, {
      "referenceID" : 1,
      "context" : "QRC (D3): QX(〈{2, 7} ,B,P ,N ∪ {Q1, Q2}〉) = 〈2, 7〉 ⇒ PRUNE: 〈1, 2, 7〉 → 〈2, 7〉 ⇒ D⊃ = ∅, Ccalc = {〈2, 5〉 , 〈2, 7〉} Q = [[2], [5, 2], [5, 7]], Qdup = [] 〉 1 ©〈2, 5〉",
      "startOffset" : 125,
      "endOffset" : 131
    }, {
      "referenceID" : 5,
      "context" : "QRC (D3): QX(〈{2, 7} ,B,P ,N ∪ {Q1, Q2}〉) = 〈2, 7〉 ⇒ PRUNE: 〈1, 2, 7〉 → 〈2, 7〉 ⇒ D⊃ = ∅, Ccalc = {〈2, 5〉 , 〈2, 7〉} Q = [[2], [5, 2], [5, 7]], Qdup = [] 〉 1 ©〈2, 5〉",
      "startOffset" : 133,
      "endOffset" : 139
    }, {
      "referenceID" : 1,
      "context" : "Iteration 3 〉 Dcalc = {D2,D4} = {[2], [5, 7]}",
      "startOffset" : 33,
      "endOffset" : 36
    }, {
      "referenceID" : 5,
      "context" : "Iteration 3 〉 Dcalc = {D2,D4} = {[2], [5, 7]}",
      "startOffset" : 38,
      "endOffset" : 44
    }, {
      "referenceID" : 1,
      "context" : "Ccalc = {〈2, 5〉 , 〈2, 7〉} D⊃ = {[5, 2]}",
      "startOffset" : 32,
      "endOffset" : 38
    }, {
      "referenceID" : 1,
      "context" : "QRC (D2): QX(〈{5} ,B,P ,N ∪ {Q1, Q2, Q3}〉) = 〈5〉 ⇒ PRUNE: 〈2, 5〉 → 〈5〉 ⇒ D⊃ = ∅, Ccalc = {〈5〉 , 〈2, 7〉} Q = [[5, 2], [5, 7]], Qdup = [] 〉 1 © 〈 A2, 5 〉C",
      "startOffset" : 109,
      "endOffset" : 115
    }, {
      "referenceID" : 5,
      "context" : "QRC (D2): QX(〈{5} ,B,P ,N ∪ {Q1, Q2, Q3}〉) = 〈5〉 ⇒ PRUNE: 〈2, 5〉 → 〈5〉 ⇒ D⊃ = ∅, Ccalc = {〈5〉 , 〈2, 7〉} Q = [[5, 2], [5, 7]], Qdup = [] 〉 1 © 〈 A2, 5 〉C",
      "startOffset" : 117,
      "endOffset" : 123
    }, {
      "referenceID" : 5,
      "context" : "Iteration 4 〉 Dcalc = {D4,D5} = {[5, 7], [5, 2]}",
      "startOffset" : 33,
      "endOffset" : 39
    }, {
      "referenceID" : 1,
      "context" : "Iteration 4 〉 Dcalc = {D4,D5} = {[5, 7], [5, 2]}",
      "startOffset" : 41,
      "endOffset" : 47
    }, {
      "referenceID" : 5,
      "context" : "QRC (D5): QX(〈{7} ,B,P ∪ {Q4} ,N ∪ {Q1, Q2, Q3}〉) = 〈7〉 ⇒ PRUNE: 〈2, 7〉 → 〈7〉 ⇒ D⊃ = ∅, Ccalc = {〈5〉 , 〈7〉} Q = [[5, 7]], Qdup = [] 〉 1 ©〈5〉",
      "startOffset" : 113,
      "endOffset" : 119
    }, {
      "referenceID" : 5,
      "context" : "Iteration 5 〉 Dcalc = {D4} = {[5, 7]}",
      "startOffset" : 30,
      "endOffset" : 36
    }, {
      "referenceID" : 0,
      "context" : "3, pK̃∪K : K̃∪K → [0, 1] is given such that pK(ax ) for ax ∈ K resulting from the application of GETAXIOMSPROBS is as given by Table 6.",
      "startOffset" : 18,
      "endOffset" : 24
    }, {
      "referenceID" : 0,
      "context" : "As outlined by the numbers i © indicating at which point in time a node is labeled, the root node (initially the empty set) is labeled first by C1 := 〈1, 2, 5〉 and three successor nodes, namely nd1 := [1], nd2 := [2] as well as nd3 := [5] with nd1.",
      "startOffset" : 201,
      "endOffset" : 204
    }, {
      "referenceID" : 1,
      "context" : "As outlined by the numbers i © indicating at which point in time a node is labeled, the root node (initially the empty set) is labeled first by C1 := 〈1, 2, 5〉 and three successor nodes, namely nd1 := [1], nd2 := [2] as well as nd3 := [5] with nd1.",
      "startOffset" : 213,
      "endOffset" : 216
    }, {
      "referenceID" : 0,
      "context" : "That is, the node [1] with pnodes([1]) = 0.",
      "startOffset" : 18,
      "endOffset" : 21
    }, {
      "referenceID" : 0,
      "context" : "That is, the node [1] with pnodes([1]) = 0.",
      "startOffset" : 34,
      "endOffset" : 37
    }, {
      "referenceID" : 1,
      "context" : "41 (as opposed to the nodes [2] and [5] with 0.",
      "startOffset" : 28,
      "endOffset" : 31
    }, {
      "referenceID" : 0,
      "context" : "The DLABEL procedure, after checking whether [1] is a non-minimal diagnosis w.",
      "startOffset" : 45,
      "endOffset" : 48
    }, {
      "referenceID" : 0,
      "context" : "〈K,B,P ,N 〉R (check is negative), computes another minimal conflict set C2 := 〈2, 4, 6〉 such that [1] ∩ C2 = ∅ (C2 is not hit by the node [1]) to constitute a label for node [1].",
      "startOffset" : 98,
      "endOffset" : 101
    }, {
      "referenceID" : 0,
      "context" : "〈K,B,P ,N 〉R (check is negative), computes another minimal conflict set C2 := 〈2, 4, 6〉 such that [1] ∩ C2 = ∅ (C2 is not hit by the node [1]) to constitute a label for node [1].",
      "startOffset" : 138,
      "endOffset" : 141
    }, {
      "referenceID" : 0,
      "context" : "〈K,B,P ,N 〉R (check is negative), computes another minimal conflict set C2 := 〈2, 4, 6〉 such that [1] ∩ C2 = ∅ (C2 is not hit by the node [1]) to constitute a label for node [1].",
      "startOffset" : 174,
      "endOffset" : 177
    }, {
      "referenceID" : 0,
      "context" : "The successor nodes [1, 2], [1, 4] and [1, 6] of [1] are generated and added to the list Q in a way that the sorting of Q in descending order of pnodes() is maintained.",
      "startOffset" : 20,
      "endOffset" : 26
    }, {
      "referenceID" : 1,
      "context" : "The successor nodes [1, 2], [1, 4] and [1, 6] of [1] are generated and added to the list Q in a way that the sorting of Q in descending order of pnodes() is maintained.",
      "startOffset" : 20,
      "endOffset" : 26
    }, {
      "referenceID" : 0,
      "context" : "The successor nodes [1, 2], [1, 4] and [1, 6] of [1] are generated and added to the list Q in a way that the sorting of Q in descending order of pnodes() is maintained.",
      "startOffset" : 28,
      "endOffset" : 34
    }, {
      "referenceID" : 3,
      "context" : "The successor nodes [1, 2], [1, 4] and [1, 6] of [1] are generated and added to the list Q in a way that the sorting of Q in descending order of pnodes() is maintained.",
      "startOffset" : 28,
      "endOffset" : 34
    }, {
      "referenceID" : 0,
      "context" : "The successor nodes [1, 2], [1, 4] and [1, 6] of [1] are generated and added to the list Q in a way that the sorting of Q in descending order of pnodes() is maintained.",
      "startOffset" : 39,
      "endOffset" : 45
    }, {
      "referenceID" : 4,
      "context" : "The successor nodes [1, 2], [1, 4] and [1, 6] of [1] are generated and added to the list Q in a way that the sorting of Q in descending order of pnodes() is maintained.",
      "startOffset" : 39,
      "endOffset" : 45
    }, {
      "referenceID" : 0,
      "context" : "The successor nodes [1, 2], [1, 4] and [1, 6] of [1] are generated and added to the list Q in a way that the sorting of Q in descending order of pnodes() is maintained.",
      "startOffset" : 49,
      "endOffset" : 52
    }, {
      "referenceID" : 0,
      "context" : "Since [1, 4] (0.",
      "startOffset" : 6,
      "endOffset" : 12
    }, {
      "referenceID" : 3,
      "context" : "Since [1, 4] (0.",
      "startOffset" : 6,
      "endOffset" : 12
    }, {
      "referenceID" : 0,
      "context" : "28) as well as [1, 6] (0.",
      "startOffset" : 15,
      "endOffset" : 21
    }, {
      "referenceID" : 4,
      "context" : "28) as well as [1, 6] (0.",
      "startOffset" : 15,
      "endOffset" : 21
    }, {
      "referenceID" : 1,
      "context" : "27) have a larger probability (as per pnodes()) than the nodes [2] (0.",
      "startOffset" : 63,
      "endOffset" : 66
    }, {
      "referenceID" : 0,
      "context" : "25), Q is given by [[1, 4], [1, 6], [2], [5], [1, 2]] when it comes to the processing of the next node.",
      "startOffset" : 20,
      "endOffset" : 26
    }, {
      "referenceID" : 3,
      "context" : "25), Q is given by [[1, 4], [1, 6], [2], [5], [1, 2]] when it comes to the processing of the next node.",
      "startOffset" : 20,
      "endOffset" : 26
    }, {
      "referenceID" : 0,
      "context" : "25), Q is given by [[1, 4], [1, 6], [2], [5], [1, 2]] when it comes to the processing of the next node.",
      "startOffset" : 28,
      "endOffset" : 34
    }, {
      "referenceID" : 4,
      "context" : "25), Q is given by [[1, 4], [1, 6], [2], [5], [1, 2]] when it comes to the processing of the next node.",
      "startOffset" : 28,
      "endOffset" : 34
    }, {
      "referenceID" : 1,
      "context" : "25), Q is given by [[1, 4], [1, 6], [2], [5], [1, 2]] when it comes to the processing of the next node.",
      "startOffset" : 36,
      "endOffset" : 39
    }, {
      "referenceID" : 0,
      "context" : "25), Q is given by [[1, 4], [1, 6], [2], [5], [1, 2]] when it comes to the processing of the next node.",
      "startOffset" : 46,
      "endOffset" : 52
    }, {
      "referenceID" : 1,
      "context" : "25), Q is given by [[1, 4], [1, 6], [2], [5], [1, 2]] when it comes to the processing of the next node.",
      "startOffset" : 46,
      "endOffset" : 52
    }, {
      "referenceID" : 0,
      "context" : "Since DYNAMICHS always treats the first node of Q next, it identifies the first minimal diagnoses D1 := [1, 4] andD2 := [1, 6] w.",
      "startOffset" : 104,
      "endOffset" : 110
    }, {
      "referenceID" : 3,
      "context" : "Since DYNAMICHS always treats the first node of Q next, it identifies the first minimal diagnoses D1 := [1, 4] andD2 := [1, 6] w.",
      "startOffset" : 104,
      "endOffset" : 110
    }, {
      "referenceID" : 0,
      "context" : "Since DYNAMICHS always treats the first node of Q next, it identifies the first minimal diagnoses D1 := [1, 4] andD2 := [1, 6] w.",
      "startOffset" : 120,
      "endOffset" : 126
    }, {
      "referenceID" : 4,
      "context" : "Since DYNAMICHS always treats the first node of Q next, it identifies the first minimal diagnoses D1 := [1, 4] andD2 := [1, 6] w.",
      "startOffset" : 120,
      "endOffset" : 126
    }, {
      "referenceID" : 1,
      "context" : "At step 5 ©, when node [2] is processed, a minimal conflict set C3 := 〈1, 3, 4〉 is computed and set as a label for [2], giving rise to the generation of three further nodes [2, 1], [2, 3] and [2, 4], all with ndi.",
      "startOffset" : 23,
      "endOffset" : 26
    }, {
      "referenceID" : 1,
      "context" : "At step 5 ©, when node [2] is processed, a minimal conflict set C3 := 〈1, 3, 4〉 is computed and set as a label for [2], giving rise to the generation of three further nodes [2, 1], [2, 3] and [2, 4], all with ndi.",
      "startOffset" : 115,
      "endOffset" : 118
    }, {
      "referenceID" : 1,
      "context" : "At step 5 ©, when node [2] is processed, a minimal conflict set C3 := 〈1, 3, 4〉 is computed and set as a label for [2], giving rise to the generation of three further nodes [2, 1], [2, 3] and [2, 4], all with ndi.",
      "startOffset" : 173,
      "endOffset" : 179
    }, {
      "referenceID" : 0,
      "context" : "At step 5 ©, when node [2] is processed, a minimal conflict set C3 := 〈1, 3, 4〉 is computed and set as a label for [2], giving rise to the generation of three further nodes [2, 1], [2, 3] and [2, 4], all with ndi.",
      "startOffset" : 173,
      "endOffset" : 179
    }, {
      "referenceID" : 1,
      "context" : "At step 5 ©, when node [2] is processed, a minimal conflict set C3 := 〈1, 3, 4〉 is computed and set as a label for [2], giving rise to the generation of three further nodes [2, 1], [2, 3] and [2, 4], all with ndi.",
      "startOffset" : 181,
      "endOffset" : 187
    }, {
      "referenceID" : 2,
      "context" : "At step 5 ©, when node [2] is processed, a minimal conflict set C3 := 〈1, 3, 4〉 is computed and set as a label for [2], giving rise to the generation of three further nodes [2, 1], [2, 3] and [2, 4], all with ndi.",
      "startOffset" : 181,
      "endOffset" : 187
    }, {
      "referenceID" : 1,
      "context" : "At step 5 ©, when node [2] is processed, a minimal conflict set C3 := 〈1, 3, 4〉 is computed and set as a label for [2], giving rise to the generation of three further nodes [2, 1], [2, 3] and [2, 4], all with ndi.",
      "startOffset" : 192,
      "endOffset" : 198
    }, {
      "referenceID" : 3,
      "context" : "At step 5 ©, when node [2] is processed, a minimal conflict set C3 := 〈1, 3, 4〉 is computed and set as a label for [2], giving rise to the generation of three further nodes [2, 1], [2, 3] and [2, 4], all with ndi.",
      "startOffset" : 192,
      "endOffset" : 198
    }, {
      "referenceID" : 0,
      "context" : "For, there is already a node [1, 2] corresponding to the set {1, 2} in Q.",
      "startOffset" : 29,
      "endOffset" : 35
    }, {
      "referenceID" : 1,
      "context" : "For, there is already a node [1, 2] corresponding to the set {1, 2} in Q.",
      "startOffset" : 29,
      "endOffset" : 35
    }, {
      "referenceID" : 1,
      "context" : "Due to the test performed in line 20, this duplicate node [2, 1] is assigned to the list Qdup which is expressed in the figure by dup.",
      "startOffset" : 58,
      "endOffset" : 64
    }, {
      "referenceID" : 0,
      "context" : "Due to the test performed in line 20, this duplicate node [2, 1] is assigned to the list Qdup which is expressed in the figure by dup.",
      "startOffset" : 58,
      "endOffset" : 64
    }, {
      "referenceID" : 0,
      "context" : "Hence, the nodes [1, 2] and [2, 1] are regarded as duplicates.",
      "startOffset" : 17,
      "endOffset" : 23
    }, {
      "referenceID" : 1,
      "context" : "Hence, the nodes [1, 2] and [2, 1] are regarded as duplicates.",
      "startOffset" : 17,
      "endOffset" : 23
    }, {
      "referenceID" : 1,
      "context" : "Hence, the nodes [1, 2] and [2, 1] are regarded as duplicates.",
      "startOffset" : 28,
      "endOffset" : 34
    }, {
      "referenceID" : 0,
      "context" : "Hence, the nodes [1, 2] and [2, 1] are regarded as duplicates.",
      "startOffset" : 28,
      "endOffset" : 34
    }, {
      "referenceID" : 1,
      "context" : "Nevertheless, ndi := [2, 1] (with ndi.",
      "startOffset" : 21,
      "endOffset" : 27
    }, {
      "referenceID" : 0,
      "context" : "Nevertheless, ndi := [2, 1] (with ndi.",
      "startOffset" : 21,
      "endOffset" : 27
    }, {
      "referenceID" : 0,
      "context" : "cs = [〈1, 2, 5〉 , 〈1, 3, 4〉]) must not be completely deleted as it might be the case that (some successor node of) ndj := [1, 2] (with ndj .",
      "startOffset" : 122,
      "endOffset" : 128
    }, {
      "referenceID" : 1,
      "context" : "cs = [〈1, 2, 5〉 , 〈1, 3, 4〉]) must not be completely deleted as it might be the case that (some successor node of) ndj := [1, 2] (with ndj .",
      "startOffset" : 122,
      "endOffset" : 128
    }, {
      "referenceID" : 1,
      "context" : "Thence, only [2, 3] and [2, 4] are added to Q as successor nodes of the processed node [2].",
      "startOffset" : 13,
      "endOffset" : 19
    }, {
      "referenceID" : 2,
      "context" : "Thence, only [2, 3] and [2, 4] are added to Q as successor nodes of the processed node [2].",
      "startOffset" : 13,
      "endOffset" : 19
    }, {
      "referenceID" : 1,
      "context" : "Thence, only [2, 3] and [2, 4] are added to Q as successor nodes of the processed node [2].",
      "startOffset" : 24,
      "endOffset" : 30
    }, {
      "referenceID" : 3,
      "context" : "Thence, only [2, 3] and [2, 4] are added to Q as successor nodes of the processed node [2].",
      "startOffset" : 24,
      "endOffset" : 30
    }, {
      "referenceID" : 1,
      "context" : "Thence, only [2, 3] and [2, 4] are added to Q as successor nodes of the processed node [2].",
      "startOffset" : 87,
      "endOffset" : 90
    }, {
      "referenceID" : 1,
      "context" : "25 and the three new nodes [5, 2], [5, 4] as well as [5, 6] are generated and assigned to Q at step 7 ©.",
      "startOffset" : 27,
      "endOffset" : 33
    }, {
      "referenceID" : 3,
      "context" : "25 and the three new nodes [5, 2], [5, 4] as well as [5, 6] are generated and assigned to Q at step 7 ©.",
      "startOffset" : 35,
      "endOffset" : 41
    }, {
      "referenceID" : 4,
      "context" : "25 and the three new nodes [5, 2], [5, 4] as well as [5, 6] are generated and assigned to Q at step 7 ©.",
      "startOffset" : 53,
      "endOffset" : 59
    }, {
      "referenceID" : 1,
      "context" : "Then, the fourth minimal conflict set C4 := 〈1, 5, 6, 8〉 is computed to label the node [2, 4] with pnodes([2, 4]) = 0.",
      "startOffset" : 87,
      "endOffset" : 93
    }, {
      "referenceID" : 3,
      "context" : "Then, the fourth minimal conflict set C4 := 〈1, 5, 6, 8〉 is computed to label the node [2, 4] with pnodes([2, 4]) = 0.",
      "startOffset" : 87,
      "endOffset" : 93
    }, {
      "referenceID" : 1,
      "context" : "Then, the fourth minimal conflict set C4 := 〈1, 5, 6, 8〉 is computed to label the node [2, 4] with pnodes([2, 4]) = 0.",
      "startOffset" : 106,
      "endOffset" : 112
    }, {
      "referenceID" : 3,
      "context" : "Then, the fourth minimal conflict set C4 := 〈1, 5, 6, 8〉 is computed to label the node [2, 4] with pnodes([2, 4]) = 0.",
      "startOffset" : 106,
      "endOffset" : 112
    }, {
      "referenceID" : 1,
      "context" : "18 and the four new nodes [2, 4, 1], [2, 4, 5], [2, 4, 6] as well as [2, 4, 8] are generated",
      "startOffset" : 26,
      "endOffset" : 35
    }, {
      "referenceID" : 3,
      "context" : "18 and the four new nodes [2, 4, 1], [2, 4, 5], [2, 4, 6] as well as [2, 4, 8] are generated",
      "startOffset" : 26,
      "endOffset" : 35
    }, {
      "referenceID" : 0,
      "context" : "18 and the four new nodes [2, 4, 1], [2, 4, 5], [2, 4, 6] as well as [2, 4, 8] are generated",
      "startOffset" : 26,
      "endOffset" : 35
    }, {
      "referenceID" : 1,
      "context" : "18 and the four new nodes [2, 4, 1], [2, 4, 5], [2, 4, 6] as well as [2, 4, 8] are generated",
      "startOffset" : 37,
      "endOffset" : 46
    }, {
      "referenceID" : 3,
      "context" : "18 and the four new nodes [2, 4, 1], [2, 4, 5], [2, 4, 6] as well as [2, 4, 8] are generated",
      "startOffset" : 37,
      "endOffset" : 46
    }, {
      "referenceID" : 1,
      "context" : "18 and the four new nodes [2, 4, 1], [2, 4, 5], [2, 4, 6] as well as [2, 4, 8] are generated",
      "startOffset" : 48,
      "endOffset" : 57
    }, {
      "referenceID" : 3,
      "context" : "18 and the four new nodes [2, 4, 1], [2, 4, 5], [2, 4, 6] as well as [2, 4, 8] are generated",
      "startOffset" : 48,
      "endOffset" : 57
    }, {
      "referenceID" : 4,
      "context" : "18 and the four new nodes [2, 4, 1], [2, 4, 5], [2, 4, 6] as well as [2, 4, 8] are generated",
      "startOffset" : 48,
      "endOffset" : 57
    }, {
      "referenceID" : 1,
      "context" : "18 and the four new nodes [2, 4, 1], [2, 4, 5], [2, 4, 6] as well as [2, 4, 8] are generated",
      "startOffset" : 69,
      "endOffset" : 78
    }, {
      "referenceID" : 3,
      "context" : "18 and the four new nodes [2, 4, 1], [2, 4, 5], [2, 4, 6] as well as [2, 4, 8] are generated",
      "startOffset" : 69,
      "endOffset" : 78
    }, {
      "referenceID" : 6,
      "context" : "18 and the four new nodes [2, 4, 1], [2, 4, 5], [2, 4, 6] as well as [2, 4, 8] are generated",
      "startOffset" : 69,
      "endOffset" : 78
    }, {
      "referenceID" : 3,
      "context" : "At step 9 ©, the third minimal diagnosis D3 := [5, 4] w.",
      "startOffset" : 47,
      "endOffset" : 53
    }, {
      "referenceID" : 0,
      "context" : "the set of leading diagnoses Dcalc = {[1, 4], [1, 6], [5, 4]}.",
      "startOffset" : 38,
      "endOffset" : 44
    }, {
      "referenceID" : 3,
      "context" : "the set of leading diagnoses Dcalc = {[1, 4], [1, 6], [5, 4]}.",
      "startOffset" : 38,
      "endOffset" : 44
    }, {
      "referenceID" : 0,
      "context" : "the set of leading diagnoses Dcalc = {[1, 4], [1, 6], [5, 4]}.",
      "startOffset" : 46,
      "endOffset" : 52
    }, {
      "referenceID" : 4,
      "context" : "the set of leading diagnoses Dcalc = {[1, 4], [1, 6], [5, 4]}.",
      "startOffset" : 46,
      "endOffset" : 52
    }, {
      "referenceID" : 3,
      "context" : "the set of leading diagnoses Dcalc = {[1, 4], [1, 6], [5, 4]}.",
      "startOffset" : 54,
      "endOffset" : 60
    }, {
      "referenceID" : 3,
      "context" : "In this case D× = {D3} = {[5, 4]} since D3 is the only minimal diagnosis that has been ruled out by the most recently added positive test case Q1.",
      "startOffset" : 26,
      "endOffset" : 32
    }, {
      "referenceID" : 3,
      "context" : "{1, 2, 6} = (〈1, 2, 5〉 ∪ 〈2, 4, 6〉) \\ [5, 4].",
      "startOffset" : 38,
      "endOffset" : 44
    }, {
      "referenceID" : 0,
      "context" : "Notice that only one call of the DLABEL procedure is required in the second iteration (for node [1, 2]) due to the test in line 8 of DYNAMICHS which is positive forD1 andD2 (sinceD1,D2 ∈ DX).",
      "startOffset" : 96,
      "endOffset" : 102
    }, {
      "referenceID" : 1,
      "context" : "Notice that only one call of the DLABEL procedure is required in the second iteration (for node [1, 2]) due to the test in line 8 of DYNAMICHS which is positive forD1 andD2 (sinceD1,D2 ∈ DX).",
      "startOffset" : 96,
      "endOffset" : 102
    }, {
      "referenceID" : 0,
      "context" : "F} is added to the positive test cases resulting in the DPI 〈K,B,P ∪ {Q1, Q2} ,N 〉R, the UPDATETREE function causes the pruning of two further nodes (D2 = [1, 6] and D4 = [1, 2]) leading to the continuance of only a single node (D1 = [1, 4]) in the memory of DYNAMICHS (see the picture with caption ’Updated Tree’ in Figure 6.",
      "startOffset" : 155,
      "endOffset" : 161
    }, {
      "referenceID" : 4,
      "context" : "F} is added to the positive test cases resulting in the DPI 〈K,B,P ∪ {Q1, Q2} ,N 〉R, the UPDATETREE function causes the pruning of two further nodes (D2 = [1, 6] and D4 = [1, 2]) leading to the continuance of only a single node (D1 = [1, 4]) in the memory of DYNAMICHS (see the picture with caption ’Updated Tree’ in Figure 6.",
      "startOffset" : 155,
      "endOffset" : 161
    }, {
      "referenceID" : 0,
      "context" : "F} is added to the positive test cases resulting in the DPI 〈K,B,P ∪ {Q1, Q2} ,N 〉R, the UPDATETREE function causes the pruning of two further nodes (D2 = [1, 6] and D4 = [1, 2]) leading to the continuance of only a single node (D1 = [1, 4]) in the memory of DYNAMICHS (see the picture with caption ’Updated Tree’ in Figure 6.",
      "startOffset" : 171,
      "endOffset" : 177
    }, {
      "referenceID" : 1,
      "context" : "F} is added to the positive test cases resulting in the DPI 〈K,B,P ∪ {Q1, Q2} ,N 〉R, the UPDATETREE function causes the pruning of two further nodes (D2 = [1, 6] and D4 = [1, 2]) leading to the continuance of only a single node (D1 = [1, 4]) in the memory of DYNAMICHS (see the picture with caption ’Updated Tree’ in Figure 6.",
      "startOffset" : 171,
      "endOffset" : 177
    }, {
      "referenceID" : 0,
      "context" : "F} is added to the positive test cases resulting in the DPI 〈K,B,P ∪ {Q1, Q2} ,N 〉R, the UPDATETREE function causes the pruning of two further nodes (D2 = [1, 6] and D4 = [1, 2]) leading to the continuance of only a single node (D1 = [1, 4]) in the memory of DYNAMICHS (see the picture with caption ’Updated Tree’ in Figure 6.",
      "startOffset" : 234,
      "endOffset" : 240
    }, {
      "referenceID" : 3,
      "context" : "F} is added to the positive test cases resulting in the DPI 〈K,B,P ∪ {Q1, Q2} ,N 〉R, the UPDATETREE function causes the pruning of two further nodes (D2 = [1, 6] and D4 = [1, 2]) leading to the continuance of only a single node (D1 = [1, 4]) in the memory of DYNAMICHS (see the picture with caption ’Updated Tree’ in Figure 6.",
      "startOffset" : 234,
      "endOffset" : 240
    }, {
      "referenceID" : 0,
      "context" : "6 that there can be only a single minimal diagnosis [1, 4] w.",
      "startOffset" : 52,
      "endOffset" : 58
    }, {
      "referenceID" : 3,
      "context" : "6 that there can be only a single minimal diagnosis [1, 4] w.",
      "startOffset" : 52,
      "endOffset" : 58
    }, {
      "referenceID" : 0,
      "context" : "Therefore, the third iteration of DYNAMICHS terminates due to Q = [] and returns the singleton set Dcalc = {[1, 4]}.",
      "startOffset" : 108,
      "endOffset" : 114
    }, {
      "referenceID" : 3,
      "context" : "Therefore, the third iteration of DYNAMICHS terminates due to Q = [] and returns the singleton set Dcalc = {[1, 4]}.",
      "startOffset" : 108,
      "endOffset" : 114
    }, {
      "referenceID" : 0,
      "context" : "Consequently, the probability pD([1, 4]) = 1 wherefore Algorithm 5 also stops executing and returns (K \\ [1, 4]) ∪ p1 ∪ Q1 ∪ Q2 as the (exact) solution to the Interactive Dynamic KB Debugging problem for the DPI 〈K,B,P ,N 〉R.",
      "startOffset" : 33,
      "endOffset" : 39
    }, {
      "referenceID" : 3,
      "context" : "Consequently, the probability pD([1, 4]) = 1 wherefore Algorithm 5 also stops executing and returns (K \\ [1, 4]) ∪ p1 ∪ Q1 ∪ Q2 as the (exact) solution to the Interactive Dynamic KB Debugging problem for the DPI 〈K,B,P ,N 〉R.",
      "startOffset" : 33,
      "endOffset" : 39
    }, {
      "referenceID" : 0,
      "context" : "Consequently, the probability pD([1, 4]) = 1 wherefore Algorithm 5 also stops executing and returns (K \\ [1, 4]) ∪ p1 ∪ Q1 ∪ Q2 as the (exact) solution to the Interactive Dynamic KB Debugging problem for the DPI 〈K,B,P ,N 〉R.",
      "startOffset" : 105,
      "endOffset" : 111
    }, {
      "referenceID" : 3,
      "context" : "Consequently, the probability pD([1, 4]) = 1 wherefore Algorithm 5 also stops executing and returns (K \\ [1, 4]) ∪ p1 ∪ Q1 ∪ Q2 as the (exact) solution to the Interactive Dynamic KB Debugging problem for the DPI 〈K,B,P ,N 〉R.",
      "startOffset" : 105,
      "endOffset" : 111
    }, {
      "referenceID" : 0,
      "context" : "Dcalc = {D1,D2,D3} = {[1, 4], [1, 6], [5, 4]}",
      "startOffset" : 22,
      "endOffset" : 28
    }, {
      "referenceID" : 3,
      "context" : "Dcalc = {D1,D2,D3} = {[1, 4], [1, 6], [5, 4]}",
      "startOffset" : 22,
      "endOffset" : 28
    }, {
      "referenceID" : 0,
      "context" : "Dcalc = {D1,D2,D3} = {[1, 4], [1, 6], [5, 4]}",
      "startOffset" : 30,
      "endOffset" : 36
    }, {
      "referenceID" : 4,
      "context" : "Dcalc = {D1,D2,D3} = {[1, 4], [1, 6], [5, 4]}",
      "startOffset" : 30,
      "endOffset" : 36
    }, {
      "referenceID" : 3,
      "context" : "Dcalc = {D1,D2,D3} = {[1, 4], [1, 6], [5, 4]}",
      "startOffset" : 38,
      "endOffset" : 44
    }, {
      "referenceID" : 4,
      "context" : "Q = [[5, 6], [2, 4, 6], [1, 2], [2, 3], [5, 2],",
      "startOffset" : 5,
      "endOffset" : 11
    }, {
      "referenceID" : 1,
      "context" : "Q = [[5, 6], [2, 4, 6], [1, 2], [2, 3], [5, 2],",
      "startOffset" : 13,
      "endOffset" : 22
    }, {
      "referenceID" : 3,
      "context" : "Q = [[5, 6], [2, 4, 6], [1, 2], [2, 3], [5, 2],",
      "startOffset" : 13,
      "endOffset" : 22
    }, {
      "referenceID" : 4,
      "context" : "Q = [[5, 6], [2, 4, 6], [1, 2], [2, 3], [5, 2],",
      "startOffset" : 13,
      "endOffset" : 22
    }, {
      "referenceID" : 0,
      "context" : "Q = [[5, 6], [2, 4, 6], [1, 2], [2, 3], [5, 2],",
      "startOffset" : 24,
      "endOffset" : 30
    }, {
      "referenceID" : 1,
      "context" : "Q = [[5, 6], [2, 4, 6], [1, 2], [2, 3], [5, 2],",
      "startOffset" : 24,
      "endOffset" : 30
    }, {
      "referenceID" : 1,
      "context" : "Q = [[5, 6], [2, 4, 6], [1, 2], [2, 3], [5, 2],",
      "startOffset" : 32,
      "endOffset" : 38
    }, {
      "referenceID" : 2,
      "context" : "Q = [[5, 6], [2, 4, 6], [1, 2], [2, 3], [5, 2],",
      "startOffset" : 32,
      "endOffset" : 38
    }, {
      "referenceID" : 1,
      "context" : "Q = [[5, 6], [2, 4, 6], [1, 2], [2, 3], [5, 2],",
      "startOffset" : 40,
      "endOffset" : 46
    }, {
      "referenceID" : 1,
      "context" : "[2, 4, 1], [2, 4, 5], [2, 4, 8]]",
      "startOffset" : 0,
      "endOffset" : 9
    }, {
      "referenceID" : 3,
      "context" : "[2, 4, 1], [2, 4, 5], [2, 4, 8]]",
      "startOffset" : 0,
      "endOffset" : 9
    }, {
      "referenceID" : 0,
      "context" : "[2, 4, 1], [2, 4, 5], [2, 4, 8]]",
      "startOffset" : 0,
      "endOffset" : 9
    }, {
      "referenceID" : 1,
      "context" : "[2, 4, 1], [2, 4, 5], [2, 4, 8]]",
      "startOffset" : 11,
      "endOffset" : 20
    }, {
      "referenceID" : 3,
      "context" : "[2, 4, 1], [2, 4, 5], [2, 4, 8]]",
      "startOffset" : 11,
      "endOffset" : 20
    }, {
      "referenceID" : 1,
      "context" : "[2, 4, 1], [2, 4, 5], [2, 4, 8]]",
      "startOffset" : 22,
      "endOffset" : 31
    }, {
      "referenceID" : 3,
      "context" : "[2, 4, 1], [2, 4, 5], [2, 4, 8]]",
      "startOffset" : 22,
      "endOffset" : 31
    }, {
      "referenceID" : 6,
      "context" : "[2, 4, 1], [2, 4, 5], [2, 4, 8]]",
      "startOffset" : 22,
      "endOffset" : 31
    }, {
      "referenceID" : 1,
      "context" : "Qdup = [[2, 1]] 〈Q1,P(Q1)〉 = 〈{B v K} , 〈{D1,D2} , {D3} , ∅〉〉",
      "startOffset" : 8,
      "endOffset" : 14
    }, {
      "referenceID" : 0,
      "context" : "Qdup = [[2, 1]] 〈Q1,P(Q1)〉 = 〈{B v K} , 〈{D1,D2} , {D3} , ∅〉〉",
      "startOffset" : 8,
      "endOffset" : 14
    }, {
      "referenceID" : 0,
      "context" : "• replace by 〈1〉 all node labels in the tree that are proper supersets of 〈1〉 ⇒ D⊃ = ∅, Ccalc = {〈1〉 , 〈2, 4, 6〉}, Q = [[1, 4], [1, 6], [1, 2]], Qdup = [] 〉",
      "startOffset" : 120,
      "endOffset" : 126
    }, {
      "referenceID" : 3,
      "context" : "• replace by 〈1〉 all node labels in the tree that are proper supersets of 〈1〉 ⇒ D⊃ = ∅, Ccalc = {〈1〉 , 〈2, 4, 6〉}, Q = [[1, 4], [1, 6], [1, 2]], Qdup = [] 〉",
      "startOffset" : 120,
      "endOffset" : 126
    }, {
      "referenceID" : 0,
      "context" : "• replace by 〈1〉 all node labels in the tree that are proper supersets of 〈1〉 ⇒ D⊃ = ∅, Ccalc = {〈1〉 , 〈2, 4, 6〉}, Q = [[1, 4], [1, 6], [1, 2]], Qdup = [] 〉",
      "startOffset" : 128,
      "endOffset" : 134
    }, {
      "referenceID" : 4,
      "context" : "• replace by 〈1〉 all node labels in the tree that are proper supersets of 〈1〉 ⇒ D⊃ = ∅, Ccalc = {〈1〉 , 〈2, 4, 6〉}, Q = [[1, 4], [1, 6], [1, 2]], Qdup = [] 〉",
      "startOffset" : 128,
      "endOffset" : 134
    }, {
      "referenceID" : 0,
      "context" : "• replace by 〈1〉 all node labels in the tree that are proper supersets of 〈1〉 ⇒ D⊃ = ∅, Ccalc = {〈1〉 , 〈2, 4, 6〉}, Q = [[1, 4], [1, 6], [1, 2]], Qdup = [] 〉",
      "startOffset" : 136,
      "endOffset" : 142
    }, {
      "referenceID" : 1,
      "context" : "• replace by 〈1〉 all node labels in the tree that are proper supersets of 〈1〉 ⇒ D⊃ = ∅, Ccalc = {〈1〉 , 〈2, 4, 6〉}, Q = [[1, 4], [1, 6], [1, 2]], Qdup = [] 〉",
      "startOffset" : 136,
      "endOffset" : 142
    }, {
      "referenceID" : 0,
      "context" : "Iteration 2 〉 Dcalc = {D1,D2,D4} = {[1, 4], [1, 6], [1, 2]}",
      "startOffset" : 36,
      "endOffset" : 42
    }, {
      "referenceID" : 3,
      "context" : "Iteration 2 〉 Dcalc = {D1,D2,D4} = {[1, 4], [1, 6], [1, 2]}",
      "startOffset" : 36,
      "endOffset" : 42
    }, {
      "referenceID" : 0,
      "context" : "Iteration 2 〉 Dcalc = {D1,D2,D4} = {[1, 4], [1, 6], [1, 2]}",
      "startOffset" : 44,
      "endOffset" : 50
    }, {
      "referenceID" : 4,
      "context" : "Iteration 2 〉 Dcalc = {D1,D2,D4} = {[1, 4], [1, 6], [1, 2]}",
      "startOffset" : 44,
      "endOffset" : 50
    }, {
      "referenceID" : 0,
      "context" : "Iteration 2 〉 Dcalc = {D1,D2,D4} = {[1, 4], [1, 6], [1, 2]}",
      "startOffset" : 52,
      "endOffset" : 58
    }, {
      "referenceID" : 1,
      "context" : "Iteration 2 〉 Dcalc = {D1,D2,D4} = {[1, 4], [1, 6], [1, 2]}",
      "startOffset" : 52,
      "endOffset" : 58
    }, {
      "referenceID" : 0,
      "context" : "QRC (D2): QX(〈{2, 4} ,B,P ∪ {Q1, Q2} ,N 〉) = 〈4〉 ⇒ PRUNE: 〈2, 4, 6〉 → 〈4〉 ⇒ D⊃ = ∅, Ccalc = {〈1〉 , 〈4〉} Q = [[1, 4]], Qdup = [] 〉 1 ©〈1〉",
      "startOffset" : 109,
      "endOffset" : 115
    }, {
      "referenceID" : 3,
      "context" : "QRC (D2): QX(〈{2, 4} ,B,P ∪ {Q1, Q2} ,N 〉) = 〈4〉 ⇒ PRUNE: 〈2, 4, 6〉 → 〈4〉 ⇒ D⊃ = ∅, Ccalc = {〈1〉 , 〈4〉} Q = [[1, 4]], Qdup = [] 〉 1 ©〈1〉",
      "startOffset" : 109,
      "endOffset" : 115
    }, {
      "referenceID" : 0,
      "context" : "Iteration 3 〉 Dcalc = {D1} = {[1, 4]}",
      "startOffset" : 30,
      "endOffset" : 36
    }, {
      "referenceID" : 3,
      "context" : "Iteration 3 〉 Dcalc = {D1} = {[1, 4]}",
      "startOffset" : 30,
      "endOffset" : 36
    }, {
      "referenceID" : 0,
      "context" : "nd is redundant 96: for node← Dup[1], .",
      "startOffset" : 33,
      "endOffset" : 36
    }, {
      "referenceID" : 58,
      "context" : "non-leaf) nodes and the adherence to the “standard” pruning rules [60] as per Definition 4.",
      "startOffset" : 66,
      "endOffset" : 70
    }, {
      "referenceID" : 70,
      "context" : "To the best of our knowledge no interactive KB debugging methods that ask a user automatically selected queries have been proposed to repair faulty (monotonic) KBs so far (except for our own previous works [73, 74, 63, 76]).",
      "startOffset" : 206,
      "endOffset" : 222
    }, {
      "referenceID" : 71,
      "context" : "To the best of our knowledge no interactive KB debugging methods that ask a user automatically selected queries have been proposed to repair faulty (monotonic) KBs so far (except for our own previous works [73, 74, 63, 76]).",
      "startOffset" : 206,
      "endOffset" : 222
    }, {
      "referenceID" : 61,
      "context" : "To the best of our knowledge no interactive KB debugging methods that ask a user automatically selected queries have been proposed to repair faulty (monotonic) KBs so far (except for our own previous works [73, 74, 63, 76]).",
      "startOffset" : 206,
      "endOffset" : 222
    }, {
      "referenceID" : 73,
      "context" : "To the best of our knowledge no interactive KB debugging methods that ask a user automatically selected queries have been proposed to repair faulty (monotonic) KBs so far (except for our own previous works [73, 74, 63, 76]).",
      "startOffset" : 206,
      "endOffset" : 222
    }, {
      "referenceID" : 66,
      "context" : "Non-interactive debugging methods for KBs (ontologies) are introduced in [68, 38, 19].",
      "startOffset" : 73,
      "endOffset" : 85
    }, {
      "referenceID" : 36,
      "context" : "Non-interactive debugging methods for KBs (ontologies) are introduced in [68, 38, 19].",
      "startOffset" : 73,
      "endOffset" : 85
    }, {
      "referenceID" : 17,
      "context" : "Non-interactive debugging methods for KBs (ontologies) are introduced in [68, 38, 19].",
      "startOffset" : 73,
      "endOffset" : 85
    }, {
      "referenceID" : 37,
      "context" : "Ranking of diagnoses and proposing a “best” diagnosis is presented in [39].",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 37,
      "context" : "In general, the work of [39] can be combined with the approaches presented in our work as ranks of logical formulas can be taken into account together with other observations for calculating the prior probabilities of minimal diagnoses (see Section 4.",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 56,
      "context" : "The idea of selecting the next query based on certain query selection measures was exploited in the generation of decisions trees [58] and for selecting measurements in the model-based diagnosis of circuits [44] (in both works, the minimal expected entropy measure was used).",
      "startOffset" : 130,
      "endOffset" : 134
    }, {
      "referenceID" : 42,
      "context" : "The idea of selecting the next query based on certain query selection measures was exploited in the generation of decisions trees [58] and for selecting measurements in the model-based diagnosis of circuits [44] (in both works, the minimal expected entropy measure was used).",
      "startOffset" : 207,
      "endOffset" : 211
    }, {
      "referenceID" : 70,
      "context" : "We extended these methods to query selection in the domain of KB debugging [73] and devised further query selection measures [74, 63].",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 71,
      "context" : "We extended these methods to query selection in the domain of KB debugging [73] and devised further query selection measures [74, 63].",
      "startOffset" : 125,
      "endOffset" : 133
    }, {
      "referenceID" : 61,
      "context" : "We extended these methods to query selection in the domain of KB debugging [73] and devised further query selection measures [74, 63].",
      "startOffset" : 125,
      "endOffset" : 133
    }, {
      "referenceID" : 44,
      "context" : "An approach for the debugging of faulty aligned KBs (ontologies) was proposed by [46].",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 44,
      "context" : "Definition 18 in [46]).",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 60,
      "context" : "We have already shown in [62, 72] that our systems can also be applied for fault localization in aligned KBs.",
      "startOffset" : 25,
      "endOffset" : 33
    }, {
      "referenceID" : 69,
      "context" : "We have already shown in [62, 72] that our systems can also be applied for fault localization in aligned KBs.",
      "startOffset" : 25,
      "endOffset" : 33
    }, {
      "referenceID" : 44,
      "context" : "The work of [46] describes approximate algorithms for computing a “local optimal diagnosis” and complete methods to discover a “global optimal diagnosis”.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 44,
      "context" : "[46] demonstrates techniques for the manual revision of the alignment as a procedure independent from debugging.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 34,
      "context" : "We rely on a divide-and-conquer algorithm [36] for the identification of a minimal conflict set C ⊆ A1,2 (in [46] C is called a MIPS, cf.",
      "startOffset" : 42,
      "endOffset" : 46
    }, {
      "referenceID" : 44,
      "context" : "We rely on a divide-and-conquer algorithm [36] for the identification of a minimal conflict set C ⊆ A1,2 (in [46] C is called a MIPS, cf.",
      "startOffset" : 109,
      "endOffset" : 113
    }, {
      "referenceID" : 17,
      "context" : "[19, 68]).",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 66,
      "context" : "[19, 68]).",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 44,
      "context" : "The “shrink” strategy applied in [46] (which is similar to the “expandand-shrink” method used in [38]), on the other hand, requires a worst case number of O(|A1,2|) calls to such a function.",
      "startOffset" : 33,
      "endOffset" : 37
    }, {
      "referenceID" : 36,
      "context" : "The “shrink” strategy applied in [46] (which is similar to the “expandand-shrink” method used in [38]), on the other hand, requires a worst case number of O(|A1,2|) calls to such a function.",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 72,
      "context" : "Empirical evaluations and a theoretical analysis of the best and worst case complexity of the “expand-and-shrink” method compared to the divide-and-conquer method performed in [75] revealed that the latter is preferable over the former.",
      "startOffset" : 176,
      "endOffset" : 180
    }, {
      "referenceID" : 44,
      "context" : "It should be noted that a similar divide-and-conquer method as used in our work could most probably be also plugged into the system in [46] instead of the “shrink” method.",
      "startOffset" : 135,
      "endOffset" : 139
    }, {
      "referenceID" : 28,
      "context" : "There are some ontology matchers which incorporate alignment repair features: CODI [30], YAM++ [51], ASMOV [32] and KOSIMap [61], for instance, employ logic-based techniques to search for a set of predefined “anti-patterns” which must not occur in the aligned ontology, either to avoid inconsistencies or incoherencies or to eliminate unwanted or redundant entailments.",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 49,
      "context" : "There are some ontology matchers which incorporate alignment repair features: CODI [30], YAM++ [51], ASMOV [32] and KOSIMap [61], for instance, employ logic-based techniques to search for a set of predefined “anti-patterns” which must not occur in the aligned ontology, either to avoid inconsistencies or incoherencies or to eliminate unwanted or redundant entailments.",
      "startOffset" : 95,
      "endOffset" : 99
    }, {
      "referenceID" : 30,
      "context" : "There are some ontology matchers which incorporate alignment repair features: CODI [30], YAM++ [51], ASMOV [32] and KOSIMap [61], for instance, employ logic-based techniques to search for a set of predefined “anti-patterns” which must not occur in the aligned ontology, either to avoid inconsistencies or incoherencies or to eliminate unwanted or redundant entailments.",
      "startOffset" : 107,
      "endOffset" : 111
    }, {
      "referenceID" : 59,
      "context" : "There are some ontology matchers which incorporate alignment repair features: CODI [30], YAM++ [51], ASMOV [32] and KOSIMap [61], for instance, employ logic-based techniques to search for a set of predefined “anti-patterns” which must not occur in the aligned ontology, either to avoid inconsistencies or incoherencies or to eliminate unwanted or redundant entailments.",
      "startOffset" : 124,
      "endOffset" : 128
    }, {
      "referenceID" : 32,
      "context" : "Another ontology matcher, LogMap 2 [34], provides integrated debugging features and the opportunity for a user to interact during this process.",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 50,
      "context" : "An interactive technique similar to our approaches was presented in [52], where a user is successively asked single KB formulas (ontology axioms) in order to obtain a partition of a given ontology into a set of desired or correct and a set of undesired or incorrect formulas.",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 50,
      "context" : "Whereas our strategies aim at finding a parsimonious solution involving minimal change to the given faulty KB in order to repair it, the method proposed in [52] pursues a (potentially) more invasive approach to KB quality assurance, namely a (reasoner-supported) exhaustive manual inspection of (parts of) a KB.",
      "startOffset" : 156,
      "endOffset" : 160
    }, {
      "referenceID" : 50,
      "context" : "Another difference of our approach compared to the one suggested in [52] is the type of queries asked to the user and the way these are selected.",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 50,
      "context" : "KB formulas) in [52] are known in advance and the challenge is to figure out the best ordering of formulas to be assessed by the user.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 50,
      "context" : "the minimal expected entropy in the set of leading diagnoses after a query has been answered), the authors in [52] employ “impact measures” which, roughly speaking,",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 78,
      "context" : "Non-interactive debugging systems published in research literature often cannot localize all possible faults (incompleteness), suggest the deletion or modification of unnecessarily large parts of the KB (nonminimality), return incorrect solutions which lead to a repaired KB not satisfying the imposed quality requirements (unsoundness) or suffer from poor scalability due to the inherent complexity of the KB debugging problem [81].",
      "startOffset" : 428,
      "endOffset" : 432
    }, {
      "referenceID" : 71,
      "context" : "in interactive debugging scenarios [74, 63, 73].",
      "startOffset" : 35,
      "endOffset" : 47
    }, {
      "referenceID" : 61,
      "context" : "in interactive debugging scenarios [74, 63, 73].",
      "startOffset" : 35,
      "endOffset" : 47
    }, {
      "referenceID" : 70,
      "context" : "in interactive debugging scenarios [74, 63, 73].",
      "startOffset" : 35,
      "endOffset" : 47
    }, {
      "referenceID" : 71,
      "context" : "A similar strategy called CKK has been employed in [74] for the information gain measure qsm() := ENT() (see Section 5.",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 22,
      "context" : "It is planned to gather corresponding data about different users in the scope of a user study and to utilize the results to achieve a model of “query hardness” (by sticking to a similar overall methodology as used in [24]) in order to come up with strategies for the determination of minimal queries that are easily understood.",
      "startOffset" : 217,
      "endOffset" : 221
    }, {
      "referenceID" : 29,
      "context" : ", how many of the formulas originally authored by a person have been corrected by other persons of higher educational level) or provenance information regarding terms occurring in the query (who has authored most of the formulas in which these terms occur?) in order to learn an “expert model” and use it to devise some kind of recommender system [31] that suggests which person to ask a particular query.",
      "startOffset" : 347,
      "endOffset" : 351
    }, {
      "referenceID" : 82,
      "context" : "An example of a system which enables the remote collaborative development of KBs (ontologies) and also provides logs of interesting usage data such as formula change logs and provenance information is Web Protégé [85].",
      "startOffset" : 213,
      "endOffset" : 217
    } ],
    "year" : 2016,
    "abstractText" : "iv",
    "creator" : "LaTeX with hyperref package"
  }
}