{
  "name" : "1205.3336.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "DISTRIBUTION OF THE SEARCH OF EVOLUTIONARY PRODUCT UNIT NEURAL NETWORKS FOR CLASSIFICATION",
    "authors" : [ "Antonio J. Tallón-Ballesteros", "Pedro A. Gutiérrez-Peña", "César Hervás-Martínez" ],
    "emails" : [ "atallon@us.es", "zamarck@yahoo.es,", "chervas@uco.es" ],
    "sections" : [ {
      "heading" : null,
      "text" : "This paper deals with the distributed processing in the search for an optimum classification model using evolutionary product unit neural networks. For this distributed search we used a cluster of computers. Our objective is to obtain a more efficient design than those net architectures which do not use a distributed process and which thus result in simpler designs. In order to get the best classification models we use evolutionary algorithms to train and design neural networks, which require a very time consuming computation. The reasons behind the need for this distribution are various. It is complicated to train this type of nets because of the difficulty entailed in determining their architecture due to the complex error surface. On the other hand, the use of evolutionary algorithms involves running a great number of tests with different seeds and parameters, thus resulting in a high computational cost.\nKEYWORDS\nNeural networks, product units, classification, distributed processing, evolutionary algorithms."
    }, {
      "heading" : "1. INTRODUCTION",
      "text" : "This research is about the distribution of processing involved in the search for the best product-unit neural network (PUNN) models [Durbin, 1990] [Martínez-Estudillo, 2006A], using evolutionary algorithms, EAs. A cluster of computers [Buyya, 1999] will be used to carry out the distribution of this processing.\nMany different types of neural network architectures have been used, but the most popular one has been the single-hidden-layer feedforward network. Amongst the numerous approaches that use neural networks in classification problems, we focus our attention on evolutionary artificial neural networks (EANNs). EANNs have been a key research area in the past decade providing an improved platform for optimizing network performance and architecture (number of hidden nodes and number of connections) simultaneously. [Miller, 1989] proposed that evolutionary computation was a very good candidate for searching the space of architectures because the fitness function associated with that space is complex, noisy, non-differentiable, multi-modal and deceptive. Since then, many evolutionary programming methods have been developed to evolve artificial neural networks, for instance [Yao, 1997] and [García-Pedrajas, 2002].\nAn objective of this paper is to design a neural network architecture that will be suitable for classification; this involves achieving a proper balance between the explorative/exploitative activities performed by a learning EA. The design of a search for good EANN models is usually based on trial and error (that is, exploring the number of nodes in the hidden layer and exploring the different parameters of the EA); the strategy follows a blind search using only a small group of possible configurations. This implies a high\ncomputational cost in order to solve a medium-sized problem, as well as the use of an EA in the search process, which makes the task practically overwhelming for even a well-equipped computer. In our case, we distribute certain network or EA parameters throughout the cluster, so that we can carry out several experiments in about the time we previously would have needed for only one.\nDifferent reasons make this distribution necessary: i) the training of this type of nets is complicated by the difficulty entailed in determining the architecture of the models (number of hidden layers, number of connections), due to the complexity of the error surface; ii) the use of an EA (stochastic search) demands a high number of runs using different seeds and parameters, giving rise to a high computational cost. The fundamental goal of this work is twofold: to improve efficiency, through the distribution of different values for the main parameters in the EA in different nodes of the cluster, and also to improve the speedup in the computation in different runs searching for models.\nThe rest of this paper is organized as follows. In section 2 we describe the proposed methodology. Section 3 continues with the distributed computation of the design of neural networks and the analysis of the results obtained. Finally, in the last section we present concluding remarks."
    }, {
      "heading" : "2. DESCRIPTION",
      "text" : "The proposed methodology consists of the use of an EA as a tool for learning the architecture and weights of a PUNN ([Martínez-Estudillo, 2006A], [Martínez-Estudillo, 2006B]). This class of multiplicative neural networks comprises such types as sigma-pi networks and product unit networks. Some advantages of PUNNs are increased information capacity and the ability to form higher-order combinations of the inputs [Durbin, 1990]. Besides that, it is possible to obtain the upper bounds of the VC dimension of product-unit neural networks similar to those obtained for sigmoidal neural networks [Schmitt, 2001]. Finally, it is a straightforward consequence of the Stone-Weierstrass Theorem to prove that product-unit neural networks are universal approximators [Martínez-Estudillo, 2006A]. Despite these advantages, product-unit based networks have a major drawback. Networks based on Product Units (PUs) have more local minima and more probability of becoming trapped in them [Ismail, 2000]. The main reason for this difficulty is that small variations in the exponents can cause large changes in the total error surface. Several efforts have been made to carry out learning methods for PUs. [Janson, 1993] developed a genetic algorithm for evolving the weights of a network based on PUs with a predefined architecture. The major problem of this kind of algorithm is how to obtain the optimal architecture before hand [Ismail, 2000].\nThe disadvantage of using EA in the training of PUNN is that the processing time could be too great with respect to the dimension of the characteristics space and to the number of classes considered in a concrete problem. So, a system has been created to allow the configuration parameter distribution throughout the different nodes in the cluster of computers."
    }, {
      "heading" : "2.1 Evolutionary Algorithm",
      "text" : "We use an EA to design the structure and learn the weights of PUNNs. The search begins with a random initial population, and, for each iteration, the population is updated using a population-update algorithm. The population is subjected to the operations of replication and mutation. Crossover is not used due to its potential disadvantages in evolving artificial networks. When using the algorithm in classification problems, we will only consider the construction of the aptitude function and the changes involved in mutation operators.\nA standard softmax activation function is used for each node of the output layer given by: ( )\n( )\n1\n( ) j\ni\nf\nj l f\ni\neg e\n\n\n\nx\nx x\n(1)\nwhere l is the number of classes, ( )jf x is the output of the node j for the pattern x and ( )jg x is the probability of this pattern belongs to the class j (Cj). Taking this into account, the function of cross-entropy error is used to evaluate individuals for a network R which is reflected in the following expression:\n1 1\n1( ) ( ln( ( ))) n l\nj i j i i j l R y g n      x (2)\nand substituting gj defined in (1),\n( )\n1 1 1\n1( ) ( ) ln( )j n l l\nfj i j i\ni j j l R y f e n   \n      \n     xx (3)\nwhere jiy is the target value for the class j in pattern i ( j iy = 1 if x  Cj and j iy = 0 otherwise), ( )jf x the output value of the neural network for the output neuron j with pattern i and n the number of patterns.\nSince we want the EA to minimise the chosen error function, we use a fitness function in the form\n  1( ) 1 ( )A R l R   . Parametric mutation consists of a simulated annealing algorithm. The severity of a mutation to an individual R is dictated by the temperature ( )T R , given by ( ) 1 ( ), 0 ( ) 1T R A R T R    . Parametric mutation is accomplished for each coefficient jiw , l j of the model with Gaussian noise, where\nthe variance depends on the temperature: 1( 1) ( ) ( )ji jiw t w t t   and 2( 1) ( ) ( ) l l j jt t t     where\n( ) (0, ( ))k kt N T R  , 1,2k  , represents a one-dimensional normally distributed random variable with mean 0 and standard deviation ( )· ( )k t T R . It should be pointed out that the modification of the exponents jiw is different from the modification of the coefficients l j , therefore 1 2  . Structural mutation implies\na modification in the function structure and allows the exploration of different regions in the search space while helping to keep the diversity of the population. There are four different structural mutations similar to the ones in the GNARL model [Angeline, 1994]: node addition, node deletion, connection addition and connection deletion. All the above mutations are made sequentially in the given order, with probability\n( )T R , in the same generation on the same network. If the probability does not select any mutation, one of the mutations is chosen at random and applied to the network. For more details about the EA see ([MartínezEstudillo, 2006A], [Martínez-Estudillo, 2006B]).\nIn our case, since what we have is a classification problem, the evolution process must be short. That is why an evolution mechanism must be selected for parameters α1 and α2 that converges toward optimum values more quickly. We use the 1/5 success rule of Rechenberg [Rechenberg, 1973]."
    }, {
      "heading" : "3. IMPLEMENTED DISTRIBUTED MODELS",
      "text" : ""
    }, {
      "heading" : "3.1 Experimental design distribution",
      "text" : "In this first case, some parameters of the EA are distributed throughout all the nodes in the cluster. These are the number of neurons in the hidden layer (neu), the maximum number of generations (gen) and optionally also the value of the output-variance (α2). We will consider a base configuration which will be modified by each of the processing nodes. Therefore, each one will have a specific mission and it will always tune the same parameters. The changes have been defined in a relative way, and so these will depend on the base configuration. Once the modifications have been made, each of the nodes will run the experiments with the new configuration. There are 8 nodes, so in the case of distributing 3 parameters, each one of them will take 2 different values and, in the case of 2 parameters, one will have 4 different values and the other one will have 2 values. For this distribution of the processing, we have used the following datasets: Balance, Cancer, Pima, Hypothyroid and Waveform, all of them from the UCI repository\n(http://www.ics.uci.edu/~mlearn/MLRepository.html). These databases are summarized in the following table:"
    }, {
      "heading" : "3.1.1 Validation technique and parameters used",
      "text" : "We have considered a cross validation experimental design called hold-out [Prechelt, 1994], that consists of splitting the data in two sets: train and test. In our case, the size of the train set is 3n/4 and n/4 for the test set, where n is the number of patterns in the problem. The results of the experimentation in this proposal have been obtained with the following common parameters / features of configuration.\nThe base configuration values depend on the database and number of parameters to distribute. There will be different configurations depending on weather 2/3 parameters are distributed.\nWaveform, the value of α2 will not be distributed."
    }, {
      "heading" : "3.1.2 Results",
      "text" : "In this section we present the results obtained in the validation set of each of the groups described with 8 configurations. We have called base configuration to the first one of each group. Amongst some of the configurations, the topology (number of neurons in the hidden layer) and the parameters (variance -α2-, maximum number of generations) are changed, and amongst others only the topology is changed. The best configuration is seen here in bold. We will show the mean values of the generalization CCR of the best model obtained, its standard deviation, the best and the worst value. The topologies will be shown with this format: Number of inputs: Hidden layer node number: Output layer node number.\n Medium size datasets (Balance, Pima and Cancer).- The following results have been obtained:"
    }, {
      "heading" : "3.1.3 Statistical Analysis",
      "text" : "We have performed a statistical analysis to compare the base configuration and the best for each one of the first three above-mentioned databases. In this comparison we have used the CCR and the number of connections obtained in the generalization process.\nIn Balance, once the algorithm has been run 30 times, using the default parameters (base configuration) and using those that obtained the best results, we present the following considerations: a) using the Kolmogorov-Smirnov (K-S) test, we conclude that the CCR distributions of the validation set and the number\nof connections in the best network model for each run are distributed with a normal distribution with asintotic signification levels greater than the standard value α = 0.05, b) under normality hypothesis, we did variance equality contrasts (Levene test) and means equality (with equal or different variances depending on the previous Levene test result). These Student’s t tests have been done considering that the 30 runs of the base and best algorithm are independent. The results for CCR show that there are no significant differences in variances (Sig = 0.253), nor in mean values (Sig = 0.352). In addition, the results for the number of connections of the best network models reveal that there are no significant differences in the variances (Sig = 0.098), however, there are differences with respect to the connection medium number with α = 0.05 (Sig = 0.000); thus, the models run with the base parameters are less significant and shorter.\nThe results for Cancer show that if we use the K-S test, we conclude that the CCR distributions of the test set and the number of connections follows a normal distribution with asintotic signification levels greater than the value α = 0.01. On the other hand, under the hypothesis of normal and independent distributions, we deduce that there are significant differences in the variances (Sig = 0.004) and in the CCR mean values (Sig = 0.001) with α = 0.05. Likewise, the results for the number of connections in the best models show that there are no significant differences in the variance, altough there are differences with respect to the average number of connections for α = 0.05 (Sig = 0.000); so the obtained models with best parameters are better regarding to CCR mean. From these results we conclude that it is preferable considering for Cancer as algorithm parameters, the ones of the best configuration.\nThe results for Pima, again, show through the K-S test that the CCR distribution in the test set and the number of connections follow a normal distribution with a level of asintotic signification greater than α = 0.01. On the other hand, under the hypothesis of normal and independent distributions, we infer that there are no significant differences in the variances (Sig = 0.118); however there are differences in the CCR mean values (Sig = 0.023) with α = 0.05. In the same way, the results for the number of connections with the best network models show that there are no significant differences in the variances (Sig = 0.305); instead there are differences in the mean number of connections for α = 0.05 (Sig = 0.000), so the model obtained with the best parameters are significantly better regarding to greater significant values in the mean CCR, though with respect to the number of connections there is a significant mean number greater that the models with base parameters. From the previous results we conclude that it is preferable for Pima to consider as algorithm parameters, the ones of the best configuration."
    }, {
      "heading" : "3.1.4 Comparison to previous results",
      "text" : "Although the generalization CCR values seem very good, we compare them to those obtained previously in the standard PUs, that do not apply distribution models. In the Table 8 appears the generalization CCR mean value of the best model obtained with the best configuration using standard PUs and distributed PUs. The best results obtained with the distribution are written in bold.\nAs we can see, the average of the results obtained in the best distributed PU models overcomes in all cases to the results achieved with standard PUs. In Balance there is a 0.23% of improvement; a variance reduction is produced as well, which indicates a greater homogeneity in the solutions. In Cancer there is a generalization CCR increment of about a 0.04% and the variance decreases slightly. Pima has a more significant improvement (0.82%); however, the variance increases slightly. In Hyphotyroid the generalization CCR increases to 1.32%. In Waveform, the improvements are about 2.03%.\nWe conclude that the improvement is greater in the bigger databases than in the smaller ones."
    }, {
      "heading" : "3.2 Processing distribution",
      "text" : ""
    }, {
      "heading" : "3.2.1 Description",
      "text" : "In this case we divide the execution number amongst some of the nodes. Our objective is to obtain a measure of the optimal number of nodes considering some of the previous databases. We have used the best configurations of the first three databases in the previous proposal."
    }, {
      "heading" : "3.2.2 Performance Analysis",
      "text" : "We aim to obtain a time performance measure of the best configurations of the first three databases in the previous proposal. Since a cluster is a parallel or distributed architecture [Buyya, 1999], we will use performance evaluation measures of parallel algorithms ([Kumar, 1994], [Wilkinson, 1999], [Ortega, 2004]), like the speedup and the efficiency. The speedup is defined as the ratio between the execution time in one processor and the parallel time with P equal processors. This definition can be applied to our case, with the consideration that we refer to nodes and not to processors.\nS = T1 / TP\nIn general, S ≤ P. In practice, the speedup is saturated when the number of processors increases. We will obtain the number of processors from which this saturation takes place. The speedup considers the execution time and it is not always the best measure to evaluate the algorithm throughput, because it varies with the number of processors. It is necessary to normalize the amounts of time to compare the throughput using different nodes. To establish this normalization, we use the efficiency which is defined as the ratio between the speedup and the number of processors. By means of efficiency we can obtain the optimal value regarding to the number of nodes that we must use to carry out experiments. We will consider 32 runs of the best configuration above-mentioned.\nIn the Table 9 we will show the execution time, the speedup and the efficiency for each of the first three databases.\nFig. 1. Speedup for Balance\nFig. 2. Efficiency for Balance\nAs can be seen, the speedup does not increase linearly, but it saturates itself. In this case, the saturation takes place from 4 nodes on. We have plotted the efficiency values to establish a comparison with all values in the same scale. So, it is easier to obtain the optimal number of computing nodes; it is only necessary to verify at which number the efficiency value is greatest. The optimal number is 4, as was indicated in the speedup graph."
    }, {
      "heading" : "4. CONCLUSIONS",
      "text" : "Once we have obtained the results of the different experiments we can conclude the following: a) two proposals have been implemented, that let distribute different features of the EA and the topology or the experimentation process; b) after the tests have been performed, we can mention that the efficiency and efficacy of the proposal are acceptable; c) we have carried out experiments with medium-size classification databases to show the model basic behaviour; d) by means of model distribution we have overcome the previous results."
    } ],
    "references" : [ {
      "title" : "An evolutionary algorithm that constructs recurrent neural networks",
      "author" : [ "Angeline", "P. J" ],
      "venue" : "IEEE Transactions on Neural Networks,",
      "citeRegEx" : "Angeline and J,? \\Q1994\\E",
      "shortCiteRegEx" : "Angeline and J",
      "year" : 1994
    }, {
      "title" : "Neural Networks for Pattern Recognition",
      "author" : [ "M. Bishop" ],
      "venue" : null,
      "citeRegEx" : "Bishop,? \\Q1995\\E",
      "shortCiteRegEx" : "Bishop",
      "year" : 1995
    }, {
      "title" : "High Performance Cluster Computing: Architectures and Systems, Vol. 1. Prentice Hall PTR, NJ",
      "author" : [ "R. Buyya" ],
      "venue" : "United States of America",
      "citeRegEx" : "Buyya,? \\Q1999\\E",
      "shortCiteRegEx" : "Buyya",
      "year" : 1999
    }, {
      "title" : "Weather forecasting with adaptive time-delay neural networks: A case study",
      "author" : [ "A Carpintero" ],
      "venue" : "In Proc. of Int. Conference On Neural Network Processing. Seul, Korea",
      "citeRegEx" : "Carpintero,? \\Q1994\\E",
      "shortCiteRegEx" : "Carpintero",
      "year" : 1994
    }, {
      "title" : "Product Units: A computationally powerful and biologically plausible extension to backpropagation networks",
      "author" : [ "R. Durbin", "D. Rumelhart" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Durbin and Rumelhart,? \\Q1990\\E",
      "shortCiteRegEx" : "Durbin and Rumelhart",
      "year" : 1990
    }, {
      "title" : "Multiobjetive cooperative coevolution of artificial neural networks",
      "author" : [ "N García-Pedrajas" ],
      "venue" : "Neural Networks,",
      "citeRegEx" : "García.Pedrajas,? \\Q2002\\E",
      "shortCiteRegEx" : "García.Pedrajas",
      "year" : 2002
    }, {
      "title" : "Global optimization algorithms for training product unit neural networks",
      "author" : [ "A. Ismail", "A.P. Engelbrecht" ],
      "venue" : "In International Joint Conference on Neural Networks",
      "citeRegEx" : "Ismail and Engelbrecht,? \\Q2000\\E",
      "shortCiteRegEx" : "Ismail and Engelbrecht",
      "year" : 2000
    }, {
      "title" : "Training product unit neural networks with genetic algorithms",
      "author" : [ "D.J. Janson", "J.F. Frenzel" ],
      "venue" : "IEEE Expert,",
      "citeRegEx" : "Janson and Frenzel,? \\Q1993\\E",
      "shortCiteRegEx" : "Janson and Frenzel",
      "year" : 1993
    }, {
      "title" : "Introduction to Parallel Computing. Design and Analysis of Algorithms",
      "author" : [ "V Kumar" ],
      "venue" : null,
      "citeRegEx" : "Kumar,? \\Q1994\\E",
      "shortCiteRegEx" : "Kumar",
      "year" : 1994
    }, {
      "title" : "Evolutionary Product Unit based Neural Networks for Regression",
      "author" : [ "Martínez-Estudillo", "A. C" ],
      "venue" : "Neural Networks,",
      "citeRegEx" : "Martínez.Estudillo and C,? \\Q2006\\E",
      "shortCiteRegEx" : "Martínez.Estudillo and C",
      "year" : 2006
    }, {
      "title" : "Hybridization of evolutionary algorithms and local search by means of a clustering method",
      "author" : [ "Martínez-Estudillo", "A. C" ],
      "venue" : "IEEE Transaction on Systems, Man and Cybernetics, Part. B: Cybernetics,",
      "citeRegEx" : "Martínez.Estudillo and C,? \\Q2006\\E",
      "shortCiteRegEx" : "Martínez.Estudillo and C",
      "year" : 2006
    }, {
      "title" : "Arquitectura de Computadores",
      "author" : [ "J Ortega" ],
      "venue" : null,
      "citeRegEx" : "Ortega,? \\Q2004\\E",
      "shortCiteRegEx" : "Ortega",
      "year" : 2004
    }, {
      "title" : "Adaptative Pattern Recognition and Neural Networks",
      "author" : [ "Y.H. Pao" ],
      "venue" : null,
      "citeRegEx" : "Pao,? \\Q1989\\E",
      "shortCiteRegEx" : "Pao",
      "year" : 1989
    }, {
      "title" : "Proben1—A set of neural network benchmark problems and benchmarking rules. Fakultät für Informatik, Univ. Karlsruhe, Karlsruhe",
      "author" : [ "L. Prechelt" ],
      "venue" : null,
      "citeRegEx" : "Prechelt,? \\Q1994\\E",
      "shortCiteRegEx" : "Prechelt",
      "year" : 1994
    }, {
      "title" : "Evolutionsstrategien: Optimierung technischer Systeme nach Prinzipien der biologischen Evolution",
      "author" : [ "I. Rechenberg" ],
      "venue" : "Holzboog Verlag. Stuttgart,",
      "citeRegEx" : "Rechenberg,? \\Q1973\\E",
      "shortCiteRegEx" : "Rechenberg",
      "year" : 1973
    }, {
      "title" : "Law discovery from financial data using neural networks",
      "author" : [ "K Saito" ],
      "venue" : "Proceedings of IEEE / IAFE / INFORMS. Conference on Computational Intelligence for Financial Engineering (CIFEr00)",
      "citeRegEx" : "Saito,? \\Q2000\\E",
      "shortCiteRegEx" : "Saito",
      "year" : 2000
    }, {
      "title" : "On the Complexity of Computing and Learning with Multiplicative Neural Networks",
      "author" : [ "M. Schmitt" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Schmitt,? \\Q2001\\E",
      "shortCiteRegEx" : "Schmitt",
      "year" : 2001
    }, {
      "title" : "Parallel Programming Techniques and applications using networked workstations and parallel computers",
      "author" : [ "B. Wilkinson", "M. Allen" ],
      "venue" : "Prentice-Hall. USA.",
      "citeRegEx" : "Wilkinson and Allen,? 1999",
      "shortCiteRegEx" : "Wilkinson and Allen",
      "year" : 1999
    }, {
      "title" : "A new evolutionary system for evolving artificial neural networks",
      "author" : [ "X. Yao", "Y. Liu" ],
      "venue" : "IEEE Transactions on Neural Networks,",
      "citeRegEx" : "Yao and Liu,? \\Q1997\\E",
      "shortCiteRegEx" : "Yao and Liu",
      "year" : 1997
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "A cluster of computers [Buyya, 1999] will be used to carry out the distribution of this processing.",
      "startOffset" : 23,
      "endOffset" : 36
    }, {
      "referenceID" : 5,
      "context" : "Since then, many evolutionary programming methods have been developed to evolve artificial neural networks, for instance [Yao, 1997] and [García-Pedrajas, 2002].",
      "startOffset" : 137,
      "endOffset" : 160
    }, {
      "referenceID" : 16,
      "context" : "Besides that, it is possible to obtain the upper bounds of the VC dimension of product-unit neural networks similar to those obtained for sigmoidal neural networks [Schmitt, 2001].",
      "startOffset" : 164,
      "endOffset" : 179
    }, {
      "referenceID" : 14,
      "context" : "We use the 1/5 success rule of Rechenberg [Rechenberg, 1973].",
      "startOffset" : 42,
      "endOffset" : 60
    }, {
      "referenceID" : 13,
      "context" : "1 Validation technique and parameters used We have considered a cross validation experimental design called hold-out [Prechelt, 1994], that consists of splitting the data in two sets: train and test.",
      "startOffset" : 117,
      "endOffset" : 133
    }, {
      "referenceID" : 2,
      "context" : "Since a cluster is a parallel or distributed architecture [Buyya, 1999], we will use performance evaluation measures of parallel algorithms ([Kumar, 1994], [Wilkinson, 1999], [Ortega, 2004]), like the speedup and the efficiency.",
      "startOffset" : 58,
      "endOffset" : 71
    }, {
      "referenceID" : 8,
      "context" : "Since a cluster is a parallel or distributed architecture [Buyya, 1999], we will use performance evaluation measures of parallel algorithms ([Kumar, 1994], [Wilkinson, 1999], [Ortega, 2004]), like the speedup and the efficiency.",
      "startOffset" : 141,
      "endOffset" : 154
    }, {
      "referenceID" : 11,
      "context" : "Since a cluster is a parallel or distributed architecture [Buyya, 1999], we will use performance evaluation measures of parallel algorithms ([Kumar, 1994], [Wilkinson, 1999], [Ortega, 2004]), like the speedup and the efficiency.",
      "startOffset" : 175,
      "endOffset" : 189
    } ],
    "year" : 2012,
    "abstractText" : "This paper deals with the distributed processing in the search for an optimum classification model using evolutionary product unit neural networks. For this distributed search we used a cluster of computers. Our objective is to obtain a more efficient design than those net architectures which do not use a distributed process and which thus result in simpler designs. In order to get the best classification models we use evolutionary algorithms to train and design neural networks, which require a very time consuming computation. The reasons behind the need for this distribution are various. It is complicated to train this type of nets because of the difficulty entailed in determining their architecture due to the complex error surface. On the other hand, the use of evolutionary algorithms involves running a great number of tests with different seeds and parameters, thus resulting in a high computational cost.",
    "creator" : "PScript5.dll Version 5.2.2"
  }
}