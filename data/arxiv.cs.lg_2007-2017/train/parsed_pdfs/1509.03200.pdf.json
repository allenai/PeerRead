{
  "name" : "1509.03200.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "technique in data mining and K-means is one of the commonly used partitioning clustering algorithm. In K-means algorithm, resulting set of clusters depend on the choice of initial centroids. If we can find initial centroids which are coherent with the arrangement of data, the better set of clusters can be obtained. This paper proposes a method based on the Dissimilarity Tree to find, the better initial centroid as well as every bit more accurate cluster with less computational time. Theory analysis and experimental results indicate that the proposed method can effectively improve the accuracy of clusters and reduce the computational complexity of the K-means algorithm.\nIndex Terms— cluster analysis; K-Means algorithm; data mining; cluster centroids; Dissimilarity Tree; computational complexity\nI. INTRODUCTION\nClustering is a method of assembling similar data into a set of clusters. So, that the data objects within the clusters are similar, whereas object located to different clusters differ. Clustering is widely practiced in numerous arenas, for example image processing, machine learning, marketing, medicines, data compression, information retrieval and so on [1]. Clustering algorithms are basically split into two classes: Partitioning algorithms and Hierarchical algorithms. A Partitioning algorithms partition the dataset into a number of sets in a single step while a Hierarchical algorithms divide the given dataset into smaller subset in a hierarchical manner [2].\nNumerous methods are available to solve clustering\nbased problems [3]. K-means algorithm is a commonly used partitioning clustering algorithms applied in numerous domains, including the initialization of some computationally expensive algorithms like Gaussian mixtures, learning vector quantization, radial basis function and Markov’s models etc. [4]. But k-means clustering algorithm selects initial centroids randomly and final cluster strongly based on the choice of initial centroids. Thus, it affects the computational time and accuracy of the cluster. Cluster result and computational time will be different for different centroid. Number of iterations needed while executing the K-means clustering algorithms and efficiency are also depends on the initial centroids [5]. Various methods have been proposed in various literature to enhance the efficiency and accuracy of K-means clustering\nalgorithms.\nIn this paper, a dissimilarity tree based method is\npresented to find the initial centroid and enhance the accuracy and efficiency of the K-means algorithm. This study is motivated by [6] in which a TP (Tree Pruning) algorithm is presented to tackle the problem of the initial medoids of PAM algorithm. This paper is structured as follows:\nSegment 2: It gives the overview of related studies. Segment 3: It discusses the standard K-means algorithm. Segment 4: It introduced the proposed algorithm. Segment 5: It experiment demonstrates the implement-\n-ation of the modified algorithm.\nSegment 6: It describes the conclusion and future work."
    }, {
      "heading" : "II. RELATED WORK",
      "text" : "The standard K-means algorithm is really sensitive to initial centroid [5]. Several methods have been proposed for finding the better initial centroid [7] [8] [9]. Some methods were also aimed to amend both the efficiency and accuracy of K-means clustering technique [10].\nA. M. Fahim et al. [7] proposed an algorithm that require less execution time compared to K-means clustering technique. In [7], the author proposed to maintain the distance to the closest cluster of previous iteration and use it to compare with distance from new centroid in the next iteration. When the current distance is smaller than or equal to the previous one, the data object remains in its cluster and there is no requirement of computing again its distances from the other cluster centroids. This saved the computational time, but initial centroids are still selected randomly.\nA. Bhattacharya et al. [8] proposed a advanced clustering algorithm, called “Divisive Correlation Clustering Algorithm (DCCA)” for clustering of genes. This algorithm is capable to generate clusters of datasets without using any initial centroids. The time complexity of this algorithm is very high.\nFang Yuan et al. [9] presented a method to find the initial centroid. This method generates more stable clusters compared to standards K-means algorithm. In this method the initial centroids are calculated, systematic way.\nMd. Sohrab Mahmud et al. [10] proposed an algorithm to find the initial centroid. This method finds a weighted average score of the dataset, then sorted this score of all data objects and separated into k subset. Finally the average for each\nsubset is taken as centroid. This algorithm generates good clusters in a minimum amount of running time."
    }, {
      "heading" : "III. K-MEAN ALGORITHM",
      "text" : "K-means is a partitioning type clustering algorithm used in data-mining and it is one of the most popular, simple and unsupervised learning algorithm [11]. The basic concept of this algorithm is to be clustered the given datasets D into k number of disjoint clusters. The algorithm consists two basic steps [5]. The first step is to select the K-initial centroids for each cluster randomly. Where k is the number of clusters. The second step is to take all data objects of dataset to the nearby centroids [5]. Euclidean distance largely used to calculate the length between all data objects and the centroids. When each of the data objects are inserted in some clusters the initial grouping is done. After this, the centroid of all clusters is again estimated by taking the average value of all the data objects of all clusters. Some data objects may update their cluster to other cluster. Again, we calculate new centroids and assigning data objects to the nearby centroid. This procedure goes on yet the convergence criteria have not been satisfied or the centroids have not become similar for two consecutive iterations [5]. The computational time complexity of the kmeans algorithm is O (nkt). Where n is the total number of all data objects, k is the total number of clusters, t is the total no of iterations of the algorithm. Pseudo code for the K-means clustering algorithm is described below [10].\n1) Euclidean distance d(ai,bi) can be calculated as\nfollows :\n    2/1\n1\n2 , \n       \nn\ni\niiii babad\nWhere, ai is (a1, a2…….ai )\nbi is (b1, b2…….bn)\n2) Criterion function is set as follows: 2\n1   \n k\ni cx\ni\ni\nxxE\nWhere x is the target object\nxi is the mean cluster ci\nThe algorithm is as follows:\nInput : O = {O1, O2, O3……. On}\nK = number of required clusters.\nOutput : C = A set of K desired clusters\nSteps:\n1. Randomly opt k data objects for initial centroids\nfrom O;\n2. Assign data objects to their nearby cluster, which has\nnearest centroids;\n3. Estimate the new intermediate value for all cluster;\n4. Go to step-2 yet convergence criteria have not been\nsatisfied."
    }, {
      "heading" : "IV. PROPOSED ALGORITHM",
      "text" : "The Dissimilarity Tree algorithm uses the mixed variable type formula to estimate the dissimilarity between data objects. Dissimilarity between objects a and b is defined as:\n     \n \n\n\n \nm f\nf ab\nm f\nf ab f\nab d badist\n1\n1 2)( ,\n\n\nWhere 0fab if either xf or yf is missing or xf = yf = 0; otherwise, 1fxy .\n f xyd is defined as:\n \nfhfh\nfff\nab hh\nba d\nminmax \n \nThe steps of dissimilarity tree based algorithm borrow the steps of TP (Tree Pruning) algorithm from [6]. This algorithm contains four steps:\nI. Estimate matrix D by using above formula.\nII. Construct a MST t, by using matrix D.\nIII. Prune the (k-1) branches which have maximum\nweight. As a result, k- sub-trees will form.\nIV. Calculate the initial centroid by computing\nintermediate value of data objects of k sub-trees.\nThe next example can described above steps:\nThere are ten sample objects in the above case and these sample objects have to split into four clusters. Every sample object consists three ascriptions. Assigned data of sample objects is given in table l. We calculate test -1 ascription to find dissimilarity matrix D1 and calculate test-2 ascription to find dissimilarity matrix D2, also calculate test-3 ascription to find dissimilarity matrix D3. Afterward that, a mean\ndissimilarity matrix Dm can get by formula 1. The result are as follows:\n              \n\n              \n\n\n0375.0625.0125.05.01175.025.0875.0\n375.0025.025.0125.0625.0625.0375.0125.05.0\n625.025.005.0125.0375.0375.0125.0375.025.0\n125.025.05.00375.0875.0875.05.0125.075.0\n5.0125.0125.0375.005.05.025.025.0375.0\n1625.0375.0875.05.00025.075.0125.0\n1625.0375.0875.05.00025.075.0125.0\n75.0375.0125.05.025.025.025.005.0125.0\n25.0125.0375.0125.025.075.075.05.00625.0\n875.05.025.075.0375.0125.0125.0125.0625.00\n1D\n              \n\n              \n\n\n0125.0375.0125.125.025.0625.0375.025.025.0\n125.0025.025.025.0625.05.025.0375.0125.0\n375.025.005.05.0375.025.00625.0125.0\n125.25.05.000875.075.05.0125.0375.0\n125.025.05.000875.075.05.0125.0375.0\n25.0625.0375.0875.0875.00125.0375.015.0\n625.05.025.075.075.0125.0025.0875.0375.0\n375.025.005.05.0375.025.00625.0125.0\n25.0375.0625.0125.0125.01875.0625.005.0\n25.0125.0125.0375.0375.05.0375.0125.05.00\n2D\n              \n\n              \n\n\n025.075.05.0125.0625.0875.0375.0625.0125.0\n25.005.025.0125.0375.0625.0125.0375.0125.0\n75.05.0025.0625.0125.0125.0375.0125.0625.0\n5.025.025.00375.0125.025.0125.0125.0375.0\n125.0125.0625.0375.005.025.025.05.00\n625.0375.0125.0125.05.005.025.005.0\n875.0625.0125.025.025.05.005.025.075.0\n375.0125.0375.0125.025.025.05.0025.025.0\n625.0375.0125.0125.05.0025.025.005.0\n125.0125.0625.0375.005.075.025.05.00\n3D\n              \n\n              \n\n\n025.0583.025.025.0792.0833.05.0375.0412.0\n25.00333.025.0167.0542.0583.025.0292.025.0\n583.0333.00417.0417.0292.025.0167.0375.0333.0\n25.025.0417.0025.0625.0667.035.0125.05.0\n25.0167.0417.025.00625.0667.0333.0125.075.0\n792.0542.0292.0625.0625.00125.0292.0292.0375.0\n833.0583.025.0667.0667.0125.00333.0583.0417.0\n5.025.0167.035.0333.0292.0333.00625.0167.0\n375.0292.0375.0125.0125.0292.0583.0625.00541.0\n412.025.0333.05.075.0375.0417.0167.0541.00\nmD\nLook for the dissimilarity Dm to draw a minimal spanning tree called Dissimilarity tree. Ten nodes and the nine branches build a Dissimilarity tree as fig 1 shows. It is required to take rid of the branch dist-(6,7), dist-(1,6), dist-(4,8) in order to cut the tree into four sub-tree, which were c1 = {1,3,8}, c2 = {2,7}, c3 = {6,9,10}, c4 = {5,4}. These four sub-trees are clusters. From these, we are easily calculate initial centroids.\nThe above described method to find initial centroids of the clusters are nearer to original centroids than the K-means algorithm where centroids are picked up arbitrary. When kmeans algorithm use this technique to find initial centroid, it converge faster than the standard K-means algorithm."
    }, {
      "heading" : "V. EXPERIMENTAL RESULTS",
      "text" : "The experimental environment of this context was AMD CPU, 2 G EMS Memory, 500 GB hard disk and Windows 8.1 OS. MATLAB 2013a is used as tool to validate the validity of improved algorithm. This paper select the multivariate datasets, selected from the UCI repository of machine learning, databases [12], that is utilized to test both the efficiency and accuracy of the proposed K-means algorithm. The same datasets have taken as input to the standard algorithm and modified algorithm. Experiment compares the accuracy and the entire running time of the sets of clusters of both the algorithms. Both algorithms requires number of desired clusters as an input. Standard K-means algorithm will take initial centroid randomly but modified algorithm will compute initial centroid automatically and find optimal centroids by the program. This paper uses wine [12], iris [12] as the datasets and table 2: shows some features of datasets.\nEach algorithm is executed ten times for both datasets. In each experiment the accuracy and timelines were computed and taken the mean values of all experiments. Tabled 3 shows the performance comparison of the algorithms. The results also showed with help of bar charts in the fig 2 & 3.\nThe outcomes of experiment show that the formation of clusters has led a less measure of execution time and quality of clusters are better in the modified algorithm as compared to the standard algorithm."
    }, {
      "heading" : "VI. CONCLUSION AND FUTURE WORK",
      "text" : "K-means algorithm is widely used in many application area but in this method the quality of the final clusters are |highly depend on the initial centroid, which are selected randomly. The modified algorithm is generating cluster result more accurately and efficiently compared to the standard K-means algorithm. This paper presents a dissimilarity tree algorithm which computes the initial centroids that reduce the number of iteration in K-means algorithm. One limitation of this algorithm is that it still use the desired no of clusters as a input. Automatic determination of the number of cluster according to the distribution of data objects in datasets is suggested as a future work"
    } ],
    "references" : [ {
      "title" : "Fine particles, thin films andex change anisotropy",
      "author" : [ "Sun Shibao", "Qin Keyun", "“Research on Modified k-means Data Cluster Algorithm” I.S. Jacobs", "C.P. Bean" ],
      "venue" : "Computer Engineering, vol.33, No.13, pp.200–201,July 2007.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Enhancing K-means Clustering Algorithm with Improved Initial Center",
      "author" : [ "Madhu Yedla", "Srinivasa Rao Pathakota", "T M Srinivasa" ],
      "venue" : "International Journal of Computer Science and Information Technologies (IJCSIT), vol. 1(2), 2010, 121-125.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Convergence Properties of the K-means Algorithms",
      "author" : [ "Leon Bottou", "Neuristique", "Yoshua Bengio" ],
      "venue" : "Dept, IRO, IESI- CNR.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 0
    }, {
      "title" : "Data Mining Concepts and Technique",
      "author" : [ "J. Han", "M. Kamber" ],
      "venue" : null,
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2001
    }, {
      "title" : "An Improved PAM Algorithm for Optimizing Initial Cluster Centre",
      "author" : [ "Feng Bo", "Hao Wenning", "Chen Gang", "Jin Dawei", "Zhao Shuining" ],
      "venue" : "IEEE, 2012, 978-1-4673-2008- 5/12.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "An Efficient Enhanced K-means Clustering Algorithm",
      "author" : [ "A.M. Fahim", "A.M. Salem", "F.A. Torkey", "M.A. Ranadan" ],
      "venue" : "Journel of Zejiang University, 10(7): 16261633, 2006.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Divisive Correlation Clustering Algorithm (DCCA) for Grouping of Genes: detecting varying patterns in expression profiles",
      "author" : [ "A. Bhattacharya", "R.K. De" ],
      "venue" : "Bioinformatics, vol. 24, pp. 1359-1366, 2008.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "A New Algorithm to Get the Initial Centroids",
      "author" : [ "F. Yuan", "Z.H. Meng", "H.X. Zhangz", "C.R. Dong" ],
      "venue" : "proceedings of the 3 International Conference on Machine Learning and Cypernetics, pp. 26-29, August 2004.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Improvement of K-means Clustering Algorithm with Better Initial Centroids based on Weighted Average",
      "author" : [ "S. Mahmud", "M. Rahman", "N. Akhtar" ],
      "venue" : "7 International Conference on Electrical and Computer Engineering, Dhaka, PP. 647-650, 20-222 December 2012,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Clustering Algorithm Research",
      "author" : [ "Sun Jigui", "Liu Jie", "Zhao Lianyu" ],
      "venue" : "Journal of software, vol. 19, no 1, pp. 48-61, January 2008.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2008
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Clustering is widely practiced in numerous arenas, for example image processing, machine learning, marketing, medicines, data compression, information retrieval and so on [1].",
      "startOffset" : 171,
      "endOffset" : 174
    }, {
      "referenceID" : 1,
      "context" : "Numerous methods are available to solve clustering based problems [3].",
      "startOffset" : 66,
      "endOffset" : 69
    }, {
      "referenceID" : 2,
      "context" : "[4].",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 3,
      "context" : "Number of iterations needed while executing the K-means clustering algorithms and efficiency are also depends on the initial centroids [5].",
      "startOffset" : 135,
      "endOffset" : 138
    }, {
      "referenceID" : 4,
      "context" : "This study is motivated by [6] in which a TP (Tree Pruning) algorithm is presented to tackle the problem of the initial medoids of PAM algorithm.",
      "startOffset" : 27,
      "endOffset" : 30
    }, {
      "referenceID" : 3,
      "context" : "The standard K-means algorithm is really sensitive to initial centroid [5].",
      "startOffset" : 71,
      "endOffset" : 74
    }, {
      "referenceID" : 5,
      "context" : "Several methods have been proposed for finding the better initial centroid [7] [8] [9].",
      "startOffset" : 75,
      "endOffset" : 78
    }, {
      "referenceID" : 6,
      "context" : "Several methods have been proposed for finding the better initial centroid [7] [8] [9].",
      "startOffset" : 79,
      "endOffset" : 82
    }, {
      "referenceID" : 7,
      "context" : "Several methods have been proposed for finding the better initial centroid [7] [8] [9].",
      "startOffset" : 83,
      "endOffset" : 86
    }, {
      "referenceID" : 8,
      "context" : "Some methods were also aimed to amend both the efficiency and accuracy of K-means clustering technique [10].",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 5,
      "context" : "[7] proposed an algorithm that require less execution time compared to K-means clustering technique.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 5,
      "context" : "In [7], the author proposed to maintain the distance to the closest cluster of previous iteration and use it to compare with distance from new centroid in the next iteration.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 6,
      "context" : "[8] proposed a advanced clustering algorithm, called “Divisive Correlation Clustering Algorithm (DCCA)” for clustering of genes.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 7,
      "context" : "[9] presented a method to find the initial centroid.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 8,
      "context" : "[10] proposed an algorithm to find the initial centroid.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 9,
      "context" : "K-MEAN ALGORITHM K-means is a partitioning type clustering algorithm used in data-mining and it is one of the most popular, simple and unsupervised learning algorithm [11].",
      "startOffset" : 167,
      "endOffset" : 171
    }, {
      "referenceID" : 3,
      "context" : "The algorithm consists two basic steps [5].",
      "startOffset" : 39,
      "endOffset" : 42
    }, {
      "referenceID" : 3,
      "context" : "The second step is to take all data objects of dataset to the nearby centroids [5].",
      "startOffset" : 79,
      "endOffset" : 82
    }, {
      "referenceID" : 3,
      "context" : "This procedure goes on yet the convergence criteria have not been satisfied or the centroids have not become similar for two consecutive iterations [5].",
      "startOffset" : 148,
      "endOffset" : 151
    }, {
      "referenceID" : 8,
      "context" : "Pseudo code for the K-means clustering algorithm is described below [10].",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 4,
      "context" : "The steps of dissimilarity tree based algorithm borrow the steps of TP (Tree Pruning) algorithm from [6].",
      "startOffset" : 101,
      "endOffset" : 104
    } ],
    "year" : 2014,
    "abstractText" : "Cluster analysis is one of the primary data analysis technique in data mining and K-means is one of the commonly used partitioning clustering algorithm. In K-means algorithm, resulting set of clusters depend on the choice of initial centroids. If we can find initial centroids which are coherent with the arrangement of data, the better set of clusters can be obtained. This paper proposes a method based on the Dissimilarity Tree to find, the better initial centroid as well as every bit more accurate cluster with less computational time. Theory analysis and experimental results indicate that the proposed method can effectively improve the accuracy of clusters and reduce the computational complexity of the K-means algorithm.",
    "creator" : "Microsoft® Word 2013"
  }
}