{
  "name" : "1301.6688.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Sanjoy Dasgupta" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "We consider the task of learning the maximum likelihood polytree from data. Our first result is a performance guarantee establishing that the op timal branching (or Chow-Liu tree), which can be computed very easily, constitutes a good ap proximation to the best polytree. We then show that it is not possible to do very much better, since the learning problem is NP-hard even to approx imately solve within some constant factor.\n1 Introduction\nThe huge popularity of probabilistic nets has created a pressing need for good algorithms that learn these struc tures from data. Currently the theoretical terrain of this problem has two major landmarks. First, it was shown by Edmonds ( 1967) that there is a very simple greedy al gorithm for learning the maximum-likelihood branching (undirected tree, or equivalently, directed tree in which each node has at most one parent). This algorithm was further simplified by Chow and Liu (1968), and as a result branch ings are often called Chow-Liu trees in the literature. Sec ond, Chickering (1996) showed that learning the structure of a general directed probabilistic net is NP-hard, even if each node is constrained to have at most two parents. His learning model uses Bayesian scoring, but the proof can be modified to accommodate a maximum-likelihood frame work.\nWith these two endposts in mind, it is natural to ask whether there is some subclass of probabilistic nets which is more general than branchings, and for which a provably good structure-learning algorithm can be found. Even if this learning problem is NP-hard, it may well be the case that a good approximation algorithm exists- that is, an algorithm whose answer is guaranteed to be close to optimal, in terms of log-likelihood or some other scoring function. We em phasize that from a theoretical standpoint an NP-hardness\nresult closes one door but opens many others. Over the last two decades, provably good approximation algorithms have been developed for a wide host of NP-hard optimization problems, including for instance the Euclidean travelling salesman problem. Are there approximation algorithms for learning structure?\nWe take the first step in this direction by considering the task of learning polytrees from data. A polytree is a di rected acyclic graph with the property that ignoring the di rections on edges yields a graph with no undirected cycles. Polytrees have more expressive power than branchings, and have turned out to be a very important class of directed probabilistic nets, largely because they permit fast exact in ference (Pearl, 1988). In the setup we will consider, we are given i.i.d. samples from an unknown distribution D over (X 1, . . . , Xn) and we wish to find some polytree with n nodes, one per X;, which models the data well. There are no assumptions about D.\nThe question \"how many samples are needed so that a good fit to the empirical distribution reflects a good fit to D?\" has been considered elsewhere (Dasgupta, 1997; Htiffgen, 1993), and so we will not dwell upon sample complexity is sues here.\nHow do we evaluate the quality of a solution? A natural choice, and one which is very convenient for these factored distributions, is to rate each candidate solution M by its log likelihood. If M assigns parents TI; to variable X;, then the maximum-likelihood choice is simply to set the conditional probabilities P(X; = x; I TI; = rr;) to their empirically ob served values. The negative log-likelihood of M is then\nn\n-ll(M) = L H(X;ITI;) , i=l\nwhere H(X;ITI;) is the conditional entropy of X; given TI; (with respect to the empirical distribution), that is to say the randomness left in X; once the values of its parents ll; are known. A discussion of entropy and related quantities can be found in the book of Cover and Thomas (1991 ). The core\ncombinatorial problem can now be defined as\nPT: Select a (possibly empty) set of parents ll; for each node X;, so that the resulting structure is a polytree and so as to minimize the quantity 2:::7=1 H (X; Ill;),\nor in deference to sample complexity considerations,\nPT(k ) : just likePT, except that each node is allowed at most k parents (we shall call these k -polytrees).\nIt is not at all clear that PT or PT(k) is any easier than learn ing the structure of a general probabilistic net. However, we are able to obtain two results for this problem. First, the optimal branching (which can be found very quickly) constitutes a provably good approximation to the best poly tree. Second, it is not possible to do very much better, be cause there is some constant c > 1 such that it is NP-hard to find any 2-polytree whose cost (negative log-likelihood) is within a multiplicative factor c of optimal. That is to say, if on input I the optimal solution to PT(2) has cost 0 PT(I), then it is NP-hard to find any 2-polytree whose cost is ::; c · 0 PT( I). To the best of our knowledge, this is the first result that treats hardness of approximation in the context of learning structure.\nAlthough our positive result may seem surprising because the class of branchings is less expressive than the class of general polytrees, it is rather good news. Branchings have the tremendous advantage of being easy to learn, with low sample complexity requirements - since each node has at most one parent, only pairwise correlations need to be es timated - and this has made them the centerpiece of some very interesting work in pattern recognition. Recently, for instance, Friedman, Geiger, and Goldszmidt (1997) have used branchings to construct classifiers, and Meilli, Jordan, and Morris (1998) have modelled distributions by mixtures of branchings.\nThere has been a lot of work on reconstructing polytrees given data which actually comes from a polytree distri bution - see, for instance, the papers of Geiger, Paz and Pearl (1990) and Acid, de Campos, Gonzalez, Molina and Perez de Ia Blanca (1991). In our framework we are try ing to approximate an arbitrary distribution using po1ytrees. Although there are various local search techniques, such as EM or gradient descent, which can be used for this prob lem, no performance guarantees are known for them. Such guarantees are very helpful, because they provide a scale along which different algorithms can be meaningfully com pared, and because they give the user some indication of how much confidence he can have in the solution.\nThe reader who seeks a better intuitive understanding of the polytree learning problem might find it useful to study the techniques used in the performance guarantee and the hard ness result. Such proofs inevitably bring into sharp focus exactly those aspects of a problem which make it difficult,\nLearning Polytrees 135\nand thereby provide valuable insight into the hurdles that must be overcome by good algorithms.\n2 An approximation algorithm\nWe will show that the optimal branching is not too far be hind the optimal polytree. This instantly gives us an O(n2) approximation algorithm for learning polytrees.\nLet us start by reviewing our expression for the cost (nega tive log-likelihood) of a candidate solution:\nn\n-ll(M) = L H(X;ITI;). (t) i=l\nThis simple additive scoring function has a very pleasing in tuitive explanation. The distribution to be modelled (call it D) has a certain inherent randomness content H (D). There is no structure whose score is less than this. The most naive structure, which has n nodes and no edges, will have score L; H(X;), where H(X;) is the entropy (randomness) of the individual variable X;. There is likewise no structure which can possibly do worse than this. We can think of this as a starting point. The goal then is to carefully add edges to decrease the entropy-cost as much as possible, while main taining a polytree structure. The following examples illus trate why the optimization problem might not be easy.\nExample 1. Suppose X 1 by itself looks like a random coin flip but its value is determined completely by X2 and X3. That is, H(X1) = 1 but H(X1IX2,X3) = 0. There is then a temptation to add edges from X 2 and X a to X 1, but it might ultimately be unwise to do so because of problematic cycles that this creates. In particular, a polytree can contain at most n - 1 edges, which means that each node must on average have (slightly less than) one parent. I\nExample 2. If X2, X3 andX4 are independent random coin flips, and X1 = X2 EEl Xa EEl X4, then adding edges from x2, X a and x4 to XI will reduce the entropy of XI by one. However, any proper subset of these three parents provides no information about X 1 ; for instance, H (X 1 l X 2, X 3) = H(XJ) = 1.1\nThe cost (t) of a candidate solution can lie anywhere in the range [0, O(n)]. The most interesting situations are those in which the optimal structure has very low cost (low en\ntropy), since it is in these cases that there is structure to be discovered.\nWe will show that the optimal branching has cost (and there fore log-likelihood) which is at most about a constant factor away from that of the optimal polytree. This is a very signif icant performance guarantee, given that this latter cost can be as low as one or as high as n.\n136 Dasgupta\n2.1 A simple bound\nThe following definition will be crucial for the rest of the development.\nDefinition. For a given probability distribution over vari ables X1, ... , Xn, let U = max; H (X;) (that is, the high est entropy of an individual node) and L =min; H(X;).\nExample 3. If all the variables are boolean and each by itself seems like a random coin flip, then U = L = 1. Similarly, if some variables have values in the range {1, 2, ... , 100} and the rest are boolean, and each variable by itself looks pretty uniform-random (within its range), then U � log2 100 < 7 and L � 1. The reader might want to convince himself that the cost (negative log-likelihood) of the optimal polytree (or the optimal branching, or the op timal directed probabilistic net) will be in the range [L, nU] . I\nWe warm up with an easy theorem which provides useful intuition about PT.\nTheorem 1. The cost (negative log-likelihood) of the op timal branching is at most (I + U/L) times than that of the optimal polytree.\nProof. Let the optimal polytree have total cost H* (we have chosen the letter H because the cost is essentially an en tropy rating, and it is helpful to think of it as such). We will use this polytree to construct a branching of cost :::; H* ( 1 + U I L ), and this will prove the theorem. In the optimal solution, let S denote the vertices with in degree more than one. Since the total number of edges in the structure is at most n - 1, each node of high indegree is effectively stealing inedges away from other nodes (cf. Ex ample 1). In particular therefore, the polytree must have at least lSI+ 1 sources (that is, nodes with no parents), imply ing that H* � JSJL. If we remove the edges into the vertices of S (so that they become sources in the graph), we are left with a branching. Each vertex of S has entropy at most U, and so the cost of this branching is:::; H* + JSIU :::; H* (1 + U I L). I\nThus in cases where the nodes have approximately equal in dividual entropies, the optimal branching is at most about a factor of two worse than the optimal polytree, in terms of log-likelihood. What happens when the ratio U I L is pretty large? This is especially a danger when different variables have vastly different ranges. Over the course of the next few lemmas, we will improve the dependence on U I L to O(logUIL), and we will show that this is tight. We start with a very crude bound which will be useful to us later.\n2.2 A better performance guarantee\nFirst we establish some simple\nNotation. Let the optimal polytree have cost H*, as before. Whenever we talk about ''parents\" and \"ancestors\" hence forth, it will be with respect to this solution. We say X is a parent of Y if there is an edge from X to Y; the defini tions of \"child\" and \"ancestor\" follow from this. A source is a node with no inedges and a sink is a node with no out edges. Denote by T x the induced subgraph consisting of node X and its ancestors. That is, Tx is a (directed) sub graph of the polytree such that its only sink is X and all the other nodes in it have directed paths to X. Let JTx I then signify the number of nodes in this subtree. For each node Z with parents II, let �(Z) = H(ZJIT), the entropy that remains in Z even after its parents are known. Clearly H* = Lz �(Z). Extend� to subgraphs Tz in the natural way: �(Tz) = LXETz �(X). Finally, all logarithms in this paper will be base two.\nSink\nThe ensuing discussion will perhaps most easily be under stood if the reader thinks of �(Z) as some positive real valued attribute of node Z (such that the sum of these values over all nodes is H*) and ignores its original definition in terms of Z's parents in the optimal polytree. We start with a few examples to clarify the notation.\nExample 4. Suppose that in the optimal polytree, node X has parents Y1, ... , Yk. Then the subtrees Tv,, ... , Tv. are disjoint parts of Tx and therefore\nk\nL�(Tv,):::; �(Tx):::; H*. i=l\nDecompositions of this kind will constitute the bread and butter of the proofs that follow. I\nExample 5. For any node X, we can upper-bound its indi vidual entropy,\nH(X) :S �(Tx),\nsince the right-hand term is the total entropy of the subtree Tx which includes X. Here's another way to think of it: denote by S the variables (including X) in the subtree Tx. Then Tx is a polytree over the variables S, and so tl.(Tx) is at least the inherent entropy H(Dis) of the target distri bution D restricted to variables S, which in turn is at least H(X) . I\nGiven the optimal polytree, we need to somehow construct a branching from it which approximates it well. Nodes with zero or one parent can remain unchanged, but for each node Z with parents Y1, Y2, . .. , Yi, I :0:: 2, we must eliminate all but one inedge. Naturally, the parent we choose to retain should be the one which contributes the most information to Z. The information lost by removing parent Yi is at most its entropy H(Y;), which in turn is upper-bounded by tl.(Ty,) (see Example 5). Therefore, we will use the simple rule: re tain the parent with highest tl.(Tv,). Formally, the increase in cost occasioned by removing all parents of Z except Yj can be bounded by:\nH(ZI}j) - H(ZIY, , Y2, . . . , Yi ) < L H(Yi)\ni;tj < Ltl.(Ty.)\ni;tj L tl.(Ty.) - maxtl.(Ty.).\n. I\nThis motivates the following\nDefinition. For any node Z with I :0:: 2 parents Y1, . . . , Yi in the optimal polytree, let the charge C(Z) beL; tl.(Ty,)- maX; tl.(Ty.). For nodes with Jess than two parents, C(Z) = 0.\nSo we apply our rule to each node with more than one par ent, and thereby fashion a branching out of the poly tree that we started with. All these edges removals are going to drive up the cost of the final structure, but by how much? Well, there is no increase for nodes with less than two parents. For other nodes Z, we have seen that the increase is at most C(Z).\nIn this section, we will charge each node Z exactly the amount C ( Z), and it will turn out that the total charges are at most '/2H* log n. Most of the work is done in the fol lowing\nLemma 2. For any node Z, the total charge for all nodes in subgraph Tz is at most 1/2tl.(Tz) log ITz 1.\nProof LetC(Tz) = LxeTz C(X) denote the total charge for nodes in subgraph Tz. We will use induction on ITzl.\nLearning Polytrees 137\nIf ITz I = 1, Z is a source and trivially C(Tz) = 0. So, assume the claim is true for all subtrees of size less than ITz I· If Z has just one parent, say Y, then C(Tz) = C(Tv ), and we are done. Assume, then, that Z has par entsY,, .. . ,}/,1 :0:: 2, and letJ; = ITv,I/ITzl. Then J, + · · · + .St � 1, and\nC(Tz) = L C(Ty.) + C(Z)\n< L 1/2A(Tv,)logJ;ITzl +\nL tl.(Ty.) - maxtl.(Ty.) . I\n< 1/2tl.(Tz) log ITzl +\nL tl.(Ty.)(1 + '/2logJ;)- miaxtl.(Ty.) i < 1/2tl.(Tz) log )Tz I.\nwhere the second line applies the inductive hypothesis to subtrees Ty, , . . . , Tv,, the third line follows from tl.(Tz) :0:: L; tl.(Ty,) (because these I subtrees are disjoint), and the fourth line is the result of (I) ignoring all .5; smaller than '/4. (2) applying Jensen's inequality, and (3) using the fact that maX; a; dominates any convex combination of {a;}. I\nNow we apply this to selected subgraphs of the optimal polytree. Our aim, again, is to bound the additional cost in curred by removing edges necessary to convert the optimal polytree into a branching. Specifically, we want to show\nn 1 ( n ) � C(X;) � '/2H* logn = 2 � tl.(X;) log n.\nTheorem 3. The cost of the optimal branching is at most (1 + 1/2 log n) times the mst of the optimal polytree.\nProof Suppose the optimal poly tree were the union of a few disjoint subtrees Tz,, . . . , Tz, (equivalently, suppose each connected component had just one sink). Then we would be in business, because we could apply the previous lemma to each of these subtrees and then sum the results. We would get\nn l\nL C(X;) LC(Tz,) i=l i=l\nI\n< L 1/2A(Tz,) log ITz, I i=l\nn\n< L l/2fl.(Xi) log n i=l\nl/2H*Iogn,\n138 Dasgupta\nwhere the second line uses Lemma 2 and the third line uses the fact that I Tz , I :S n.\nIf the optimal poly tree is not of this convenient form, then it must have some connected component which contains two sinks X andY such that Tx and Ty contain a common sub graph Tu.\nSay that Tu is connected to the rest of Ty through the edge (U, V), V E Ty - Tu. Remove this edge, and instead add the edge (X, V). This has the effect of moving Ty - Tu all the way up T x. After this change, X is no longer a sink, and for every node Z in the graph, Ll( Tz) (and thus C(Z)) has either stayed the same or increased, because each node has all the ancestors it had before and maybe a few more (re call that each Ll(W) is a fixed value associated with node W and Ll( Tz) is the sum of these values over W E Tz ). It is also the case, and this will be useful later, that the in degree of each node remains unchanged. In this manner we alter the structure so that it is a disjoint union of single-sink components, and then apply the previous argument. I\nRemark. In the optimal polytree, let no denote the number of nodes with no parents (that is, the number of sources), and n;:::2 the number of nodes with at least 2 parents. By examining what happens to nodes of different indegree in the above proof, we notice that the theorem remains true if we replace n by n0 + n>2, that is, if we ignore nodes with exactly one inedge. -\n2.3 The final improvement\nIn the argument so far, we have bounded the entropy of a node X by H(X) :S Ll( Tx ). This is a fairly tight bound in general, except when these entropy values start getting close to the maximum entropy U. We factor this in by using a slightly more refined upper bound.\nDefinition. For any node Z, let C'(Z) = min{ C(Z), U}. Then C' ( Z) is an upper bound on the extra cost incurred at node Z due to the removal of all but one parent.\nThis gives us the final improvement. Once again, we start by considering a single subtree.\nLemma 4. For any node Z, the total cost incurred by nodes in the subtree Tz is at most C' ( Tz) :S ( 5/2 + 1/2 log U / L )Ll( Tz ).\nProof. Let T C Tz denote the polytree induced by all nodes X E Tz such that Ll( Tx) > U; that is, T consists of these nodes, and any edges between them in Tz. Note that T must be connected, because if X E T, and Y E Tz is a child of X, then Y must also be in T (since Ll( Ty) <:: Ll( Tx )). Let Tj C T be those nodes of T which have j parents in T. And let B be the border nodes right outside T, that is, nodes which are in Tz - T and have children in T.\nWhat are the charges for the various nodes in T?\n(I) Each node X E T1 has all but one parent outside T, and therefore gets charged at most the sum of Ll( Ty) over its parents Y outside T (and in B). The total charge for nodes in To and T1 is\nL C'(X) + L C'(X) < L Ll( Ty) X ETa YEB\n< Ll( Tz)\nwhere the final inequality follows by noticing that the pre ceding summation is over disjoint subtrees of Tz (cf. Ex ample4).\n(2) Nodes X E T2, T3, . . • have at least two parents in T, and so get charged exactly U; however, since T is a tree we know that I To I<:: 1 + I T2I + I T3I +···, and thus\n2:::: C'(X) < I:u XETi ,i?:2 X ETa\n< L Ll( Tx) X ETa < Ll( Tz)\nwhere once again the disjoint subtrees property is used.\nThus nodes in T have total charge at most 2Ll( Tz). It re mains to assess the charges for those nodes in Tz which are\n--;\nnot in T. Split these nodes into their disjoint sub-polytrees {Tv: V E B },and consider one suchTv. Sincefl.(Tv) � U and each source in Tv has entropy at least L, we can con clude that Tv has at most U I L sources and therefore at most 2U I L nodes of indegree '# 1. The charge C'(Tv) is there fore, by Lemma 2, at most 1/2fl.(Tv) log 2U I L. We sum this over the various disjoint polytrees and find that the total charge for Tz - Tis at most 1/2fl.(Tz) log 2U I L, where upon C' (Tz) � ( 5/2 + 1/2 log U I L )t. ( Tz ), as promised. I\nTheorem 5. The cost of the optimal branching is at most ( 7/2 + 1/2 log U I L) times the cost of the optimal polytree.\nProof. It can be verified as in Theorem 3 that for charging purposes we may assume the optimal solution is a disjoint union of single-sink subgraphs. Apply the previous lemma to each of these subtrees in turn, and sum the results. I\nThis immediately gives us a reasonable approximation al gorithm for PT.\nThe bound on the ratio between the best branching and the best polytree has to depend upon log U I L. To see this, con sider a poly tree which is structured as a complete binary tree (with nl2 + 1leaves) and with edges pointing towards the single sink. All nodes are binary-valued, each internal node is the exclusive-or of its two parents, and each source has some small entropy£ = B(1ln). The nodes on the next level up each have approximately double this entropy, and so on, so that the sink has entropy about 11(1). The opti mal polytree has cost (nl2 + 1)£ = 11(1) whereas the best branching loses B( m) entropy on each level and therefore has cost about B( m log n) = B(log n) = B(log U I L).\n3 A hardness result\nNeedless to say, the decision version ofPT is NP-complete. However, the news is considerably worse than this - we will prove a hardness barrier which rules out the possibil ity of a polynomial time approximation scheme. Edmonds' algorithm solves PT( l) optimally; it will now turn out that if nodes are allowed a second parent then the problem be comes hard to approximately solve.\nTheorem 6. There is some constant c > 1 for which the problem of finding a 2-polytree whose cost is within c times\noptimal is an NP-hard optimization problem.\nProof. We shall reduce from the following canonical prob lem ( £ > 0 is some fixed constant).\nMAX3SAT(3)\nInput: A Boolean formula¢ in conjunctive normal form, in which each clause contains at most three literals and each\nLearning Polytrees 139\nvariable appears exactly three times. It is guaranteed that either there is an assignment which satisfies all the clauses (that is, ¢ is satisfiable) or no assignment satisfies more than ( 1 -£) fraction of the clauses. Question: Is ¢ satisfiable?\nThis is a decision (true/false) version of the corresponding approximation problem (find an assignment which comes close to maximizing the number of clauses satisfied). In a celebrated result of the early 1990s, it was shown that this problem is NP-hard; a good starting point for further details is a survey article by Arora and Lund (1996). Suppose we are given a formula ¢ = C1 1\\ C2 1\\ · · · 1\\ Cm over variables x1, . .. , Xn, and with the stated guarantees. We will show that there is some constant c > 1 (which de pends only upon £) such that any algorithm which can learn a 2-polytree within a multiplicative factor c of optimal (in terms of log-likelihood) can also be used to decide whether ¢ is satisfiable.\nFor this reduction we construct a probability distribution out of the formula ¢. The distribution will be specified by the following probabilistic net.\nThe edges in this graph represent correlations. Circles de note regular nodes. The squares are slightly more compli cated structures whose nature will be disclosed later; for the time being they should be thought of as no different from the circles. There are three layers of nodes.\n(1) The top layer consists of i.i.d. B (p) random variables, one per clause; here \"B (p)\" denotes a Boolean random variable with probability p of being one, that is, a coin with probability p of coming up heads. This entire layer has en tropy mH(p).\n(2) The middle layer contains two nodes per variable x;: a principal node X; which is correlated with the nodes for the three Cj 's in which x; appears, and a B ( 1/2) satellite node called R;. Let us focus on a particular variable x; which ap pears in clauses c1, c2 with one polarity and c3 with the op posite polarity- this is the general case since each variable appears exactly three times. For instance, C1 and C2 might contain x; while C3 contains X';. Then the nodes R;, X; will have the following structure.\n140 Dasgupta\nHere P; and N; are both B ( 1 h), and stand for \"previous\" and \"next.\" Note each R; is Boolean with entropy one and each X; is a four-tuple of Boolean values, with entropy H ( 2p( 1 - p)) + 3. Thus the second layer of the graph has overall entropy n(H(2p(1- p)) + 4). (3) The third layer (of n - 1 nodes) joins together consec utive nodes X;, Xi+1 of the second layer. The value of the ith node in this level, 1 ::; i ::; n- 1, is N; $ P;+1, and the entire level has entropy n - 1.\nTo sample randomly from this distribution, pick val ues C1, ... ,Cm,R1,····Rn,P1,····Pn,N1,····Nn in dependently and at random, and then set X 1 • . . . , Xn based upon these.\nSuppose that the learning algorithm has so much data that there is no sampling error. Thus it knows the exact entropies H ( ·) and conditional entropies H ( · I · ) for all combinations of variables. What kind of polytree can it construct? We\nwill show that:\n• If¢ is satisfiable then there is a 2-polytree whose cost is some value H* which can easily be computed from ¢.\n• If¢ is not satisfiable (and so at most m( 1 - f) clauses can be satisfied by any assignment) then there is no 2- polytree of cost less than cH*.\nThus, any algorithm which can find a 2-polytree within fac tor c of optimal, can also decide whether¢ is satisfiable.\nHow can we prove this? A good polytree must somehow tell us a satisfying assignment for¢. If the polytree that is learned has an edge from Ci to X;, we will say \"Cj chooses X;\" and will take this as meaning that clause Cj is satisfied on account of variable x;. There are several steps towards showing this is well-defined.\nFirst we will force the square nodes to have no inedges. This is quite easy to achieve; for instance, R1, ostensibly a single coin flip, can be rigged to avoid inedges by giving it two default inedges via the following little gadget.\nR, B(l/2)\nA\nHere A and B are two extra nodes, independent of every thing but R1, which each contain k i.i.d. copies of B(q), called A1, A2, ... , Ak and B1, ... , Bk. for some constants q < 1/2 and k > 1. R1 has now expanded to include A1 $ B1, ... , Ak $ Bk. On account of the polytree con straint, there can be at most two edges between A, B, and R1. The optimal choice is to pick edges from A and B to R1. Any other configuration will increase the cost by at least k(H(2q(1- q))- H(q)) > 1 (the constant k is cho sen to satisfy this). These suboptimal configurations might permit inedges from outside nodes which reveal something about the B(!h) part of R1; however such edges can pro vide at most one bit of information and will therefore never be chosen (they lose more than they gain). In short, such gadgets ensure that square nodes will have no inedges from the nodes in the overall graph shown above.\nNext we will show that any half-decent polytree must pos sess all 2( n - 1) connections between the second and third levels. Any missing connection between consecutive X;, X;+1 causes the cost to increase by one and in exchange permits at most one extra edge among the X; 's and Cj 's, on account of the polytree constraint. However we will see that none of these edges can decrease the cost by one.\nTherefore, all the consecutive X; 's are connected via the third layer, and each clause-node Ci in the first level can choose at most one variable-node X; in the second level (otherwise there will be a cycle).\nWe now need to make sure that the clauses which choose a variable do not suggest different assignments to that vari able. In our example above, variable x; appeared in C1, C2 with one polarity and C3 with the opposite polarity. Thus, for instance, we must somehow discourage edges from both C1 and C3 to X;, since these edges would have contra dictory interpretations (one would mean that x; is true, the other would mean that x; is false). The structure of the node X; imposes a penalty on this combination.\nChoose p so thatH (p) = 1/2, and define c5 � 1- H(2p(1p)) > 0. Assume that we start with a structure that has no edges in the first and second layers (apart from the hidden edges in the square nodes). The goal of the learning algo rithm is to add edges from the Cj 's and R; 's to the X; 's which will bring the cost down as much as possible. Which edges is it likely to add? What are the possible parents of the X; depicted above?\n(a) Just R;: the entropy of X; is decreased by c5.\n(b) R;, and one of C1, C2, Ca: the entropy of X; is de creased by 1/2.\n(b) C1, C2: the entropy decrease is 1- o.\n(c) C1, Ca or C2, Ca: the entropy decrease is 1/2- c5.\nThus C1 and Ca will not both choose variable X;, and more generally, the assignment embodied in the edges from\n--;\nclause-nodes to variable-nodes will be well-defined. Sup pose m' clauses are satisfiable (either m' = m or m' � m(1- <)). Then the edges into the X;'s will decrease the cost of the second layer by m' (1/2-J) + n.S.\nIn this way, if all clauses are satisfiable, then the optimal structure has some cost H* = 0( m), whereas if only ( 1 - f) m clauses are satisfiable, then the optimal cost is H* + B(<m) ?: cH* for some constant c > 1 which de pends only upon < and not upon <f>. Thus PT(2) is hard to approximate within some fixed constant factor. I\nThis proof relies heavily upon the degree constraint as a source of hardness. However it should be possible to pro duce similar effects using just the polytree constraint, and a little imagination.\n4 Directions for further work\nWe have seen that the optimal branching, in which each node has at most one parent, is a good approximation to the optimal polytree, in terms of log-likelihood. Any algo rithm with a better performance guarantee must be able to give nodes two or more parents when necessary. What are good heuristics for doing this? Can a linear programming approach yield something here?\nPolytrees are currently the most natural class of directed graphs for which efficient inference algorithms are known. But in fact the junction tree algorithm works quickly for any directed graph whose moralized, triangulated, undirected version has very small cliques. Is there a larger class of di rected graphs with simple characterization which permits fast inference and for which efficient, provably good learn ing algorithms can be constructed?\nAcknowledgements\nThe author is grateful to Nir Friedman for introducing him to this field, and thanks the referees for their useful com ments.\nLiterature cited\nAcid, S., de Campos, L.M., Gonziilez, A., Molina, R. & Perez de Ia Blanca, N. ( 1991) Learning with CASTLE. In Symbolic and Quantitative Approaches to Uncertainty,\nLecture Notes in Computer Science 548, 99-106. Berlin: Springer-Verlag. 99-106.\nArora, S. & Lund, C. (1996). Hardness of approximations. In Approximation algorithms for NP-hard problems, ed. D. Hochbaum. Boston, MA: PWS.\nChickering, D.M. (1996). Learning Bayesian networks is NP-complete. Learning from data: AI and statistics V, 121-130. New York: Springer.\nLearning Polytrees 141\nChow, C.K. & Liu, C.N. (1968). Approximating discrete probability distributions with dependence trees. IEEE Transactions on Information Theory, 14, 462-467.\nCover, T.M. & Thomas, J.A. (1991). Elements of informa tion theory. New York: John Wiley.\nDasgupta, S. (1997). The sample complexity of learning fixed-structure Bayesian networks. Machine Learning, 29, 165-180.Boston,MA: Kluwer.\nEdmonds, J. (1967). Optimum branchings. Journal of Re search of the National Bureau of Standards, 71 B, Vol. 4, 233-240.\nFriedman, N., Geiger, D. & Goldszmidt, M. ( 1997) Bayesian network classifiers. Machine Learning, 29, 1-37. Boston, MA: Kluwer\nGeiger, D., Paz, A. & Pearl, J. (1990) Learning causal trees fron dependence information. Proceedings of the Eighth National Conference on Artificial intelligence, 770-776. Cambridge: AAAI Press.\nHiiffgen, K.-U. (1993). Learning and robust learning of product distributions. Proceedings of the Sixth Annual ACM Conference on Computational Learning Theory,\n77-83.\nMeila, M., Jordan, M.l. & Morris, Q. ( 1998). Estimating de pendency structure as a hidden variable. MIT technical report AI-1648.\nPearl, J. (1988). Probabilistic reasoning in intelligent sys tems. San Mateo, CA: Morgan Kaufmann."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2011,
    "abstractText" : "We consider the task of learning the maximum­ likelihood polytree from data. Our first result is a performance guarantee establishing that the op­ timal branching (or Chow-Liu tree), which can be computed very easily, constitutes a good ap­ proximation to the best polytree. We then show that it is not possible to do very much better, since the learning problem is NP-hard even to approx­ imately solve within some constant factor.",
    "creator" : "pdftk 1.41 - www.pdftk.com"
  }
}