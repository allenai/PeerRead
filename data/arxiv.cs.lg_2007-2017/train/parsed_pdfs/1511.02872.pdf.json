{
  "name" : "1511.02872.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Visual Language Modeling on CNN Image Representations",
    "authors" : [ "Hiroharu Kato", "Tatsuya Harada" ],
    "emails" : [ "kato@mi.t.u-tokyo.ac.jp", "harada@mi.t.u-tokyo.ac.jp" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Measuring naturalness of images is an important problem. By measuring naturalness, one can generate realistic images or detect unnatural regions in images.\nConvolutional Neural Networks (CNNs) [34] extract and abstract features from raw image pixels hierarchically. Because representations they have learned for image classification are highly discriminative and generalized [10, 48], they are an extremely important component in computer vision. However, they are known to be insensitive to the naturalness of images. For example, images generated through them appear to be strange and unrealistic [40]. Furthermore, they are susceptible to unnatural noise or artificial fabrication [16, 41, 51]. Therefore, a method to measure naturalness can be complementary to CNN features.\nDespite the importance of measuring the naturalness of images, few alternative methods exist. Although many\nprobabilistic image models have been proposed [1, 19, 29, 33, 45], most are applicable only for small image patches, not for large natural images. Moreover, they are generally built directly on raw image pixels. Although it is a preferred property for low-level image processing such as image denoising, they have insufficient capability of modeling complex and abstract naturalness as we feel.\nIn this work, we assume that naturalness can be measured by predictability on high-level features during eye movement. For example, during moving of our eyes from left to right, one feels strangeness when viewing a scene that is not predicted by what we have seen along the way. This strangeness is presumably based on edges, shapes, or more semantic signals: not on pixel-level information.\nThis type of naturalness is studied extensively and is used widely in natural language processing [43]. Such a method, called Language Model (LM), is applied mainly for regularizing outputs of speech recognition or machine translation systems. Given a sequence of words, LM pre-\n1\nar X\niv :1\n51 1.\n02 87\n2v 1\n[ cs\n.C V\n] 9\nN ov\n2 01\n5\ndicts the next word from past words for each timestep and computes the naturalness of the entire sequence as the product of prediction accuracy of all timesteps. Although traditional n-gram models, which predict the next word from the prior n words, are still prevalent, LM based on Recurrent Neural Networks (RNNs) [39] has emerged as a favorable option because it can predict the next word using more than n words.\nMotivated by the description above, we propose a method to evaluate image naturalness by building a variant of RNNLM on mid-level image representations extracted from a pre-trained CNN. RNNLM cannot accommodate two-dimensional representations. Therefore, we apply it vertically and horizontally. We designate this method as Visual Language Model on CNN (CNN-VLM). Figure 1 presents an illustration of our method. Figure 2 presents an example of unnaturalness (or prediction error) maps obtained using our method. CNN-VLM measures the naturalness using contextual information related to high-level discriminative features of CNN, which is difficult to accomplish using probabilistic image modeling on raw pixels.\nThe concept to treat image representations like words is related to the well-known Bag-of-Visual-Words [7, 49]. However, in our case, the representations are not quantized.\nWe apply our method to 1) image reconstruction from image features and 2) eye-fixation prediction. The naturalness of images plays an important role in these two tasks. We briefly describe them below.\nImage reconstruction from image features is a task to visualize an image feature by reconstructing an image from it. To reconstruct interpretable images, regularizers of some\nkind must be imposed on images [38, 47]. We demonstrate empirically that using CNN-VLM as a regularizer enables us to generate more understandable images than those produced using two existing methods.\nHuman eye fixation points on images were shown to be predictable using Shannon’s “self-information” [5]. In fact, many attention models are explainable from the perspective of information theory [3]. Because self-information can be interpreted as unlikelihood or unnaturalness, our unnaturalness map is useful as a saliency map. We demonstrate that CNN-VLM achieves state-of-the-art performances on eye fixation prediction task in the experiment section.\nOur contributions are summarized as follows. 1) Based on the assumption that naturalness can be measured by the predictability on high-level features during eye movement, we proposed a novel method to evaluate naturalness by building a variant of RNN Language Models on pre-trained CNN representations. 2) We confirmed that using our method as a regularizer enables us to generate more understandable images from features than existing approaches. 3) We showed that unnaturalness maps produced using our method achieve state-of-the-art eye fixation prediction performance on two well-studied datasets."
    }, {
      "heading" : "2. Related work",
      "text" : "This section presents a brief review of existing approaches which are related to modeling naturalness of images. Additionally, we describe image reconstruction from features and eye fixation prediction."
    }, {
      "heading" : "2.1. Image modeling",
      "text" : "Many probabilistic image models have been proposed [1, 15, 19, 29, 33, 45, 15]. However, most are only applicable to small and fixed-size image patches of simple contents. To overcome this matter, Gregor et al. [18] used attention mechanisms and succeeded in generating very realistic images. Denton et al. [9] applied Generative Adversarial Networks [15] in a hierarchical way.\nTheis and Bethge [53] proposed a scalable image model using multi-dimensional LSTMs [17] which predict pixel values of certain locations from preceding pixels. We also use RNN for predictions like theirs. However, to capture more high-level information, we train RNN on CNN representations, not on raw pixels.\nSeveral vision papers explicitly use LM. Wu et al. [58] and Tirilly et al. [54] trained LMs on quantized local descriptors or Visual Words. Although their approach is similar to ours, they used LMs for classification, not for measuring naturalness. Ranzato et al. [42] trained a language model on a small region of videos which predicts the next time frame to learn spatial-temporal video representations."
    }, {
      "heading" : "2.2. Image reconstruction from features",
      "text" : "Reconstructing an image from its representation is an important task to understand the characteristics of the representation. Many works have addressed this problem for hand-crafted representations [8, 28, 38, 56, 57] and deep representations [11, 38, 47, 59].\nMahendran and Vedaldi [38] showed that an image can be reconstructed by gradient descent if the representation is extracted through differentiable functions. They also demonstrated that a “natural image prior” is necessary to reconstruct interpretable images. They regularized reconstructed images to eliminate spikes in raw pixels and to be within the natural RGB range. Simonyan et al. [47] adopted a similar approach and used L2 regularization on images.\nDosovitskiy and Brox [11] inverted CNN features by directly learning a CNN, which translates features to images. They demonstrated that colors and rough compositions of the original image can be reconstructed.\nOur reconstruction method is based on the work by Mahendran and Vedaldi [38]. Instead of using a hand-crafted natural image prior, we use RNNLM trained on natural images as a regularizer."
    }, {
      "heading" : "2.3. Eye fixation prediction",
      "text" : "Modeling visual attention is fundamentally important to efficiently process massive real-world data. Especially, a task to predict eye fixation points of humans has been examined extensively [3].\nBruce and Tsotsos [5] demonstrated that eye fixation points can be predicted using Shannon’s “self-information”. This information-theoretic view has been adopted for many research efforts [3]. Our method also uses a kind of selfinformation.\nMany recent methods are based on supervised training on an eye fixation dataset [25, 27, 31, 32, 36, 55]. Ours is also a trainable flexible model. However, because it is trained in an unsupervised manner, it requires images of the target domain but does not require eye fixation data. Because making a dataset is a troublesome task, it is a favorable property for practical applications."
    }, {
      "heading" : "3. Visual language modeling",
      "text" : "As illustrated in Figure 1, we measure the naturalness of an image using RNNLM and CNN. The pipeline of our method and corresponding sections are explained below.\n1. Extract a mid-level image representations from an input image using a pre-trained CNN. (Section 3.1.1.) 2. Normalize and orthogonalize them. (Section 3.2.) 3. Run RNNLM forward and backward along the x-axis\nand y-axis to obtain prediction maps and prediction error maps. (Section 3.1.2, 3.3.)\n4. Sum them up and output it as the unnaturalness score. (Section 3.3.)\nRNNLM must be trained on natural images in advance. The training procedure is described in Section 3.4.\nWe apply our method to image reconstruction from features and eye fixation prediction. We describe the details of two applications in Section 3.5 and Section 3.6."
    }, {
      "heading" : "3.1. CNN and RNN",
      "text" : "First, we briefly introduce CNN and RNN. We also describe their configuration in this work."
    }, {
      "heading" : "3.1.1 CNN",
      "text" : "CNN is a neural network that achieves state-of-the-art performance for image classification [30, 48]. To extract image representations, a CNN applies 2D convolution, nonlinear activation function, and downsampling in a hierarchical way. The weights of convolution kernels are learned from data to minimize classification error.\nMid-level representations of CNNs trained on a largescale generic image classification dataset are shown to work as a high-performance generic image feature [10, 48]. Therefore they have become the de facto standard image feature in recent years.\nWe use the outputs immediately after the convolution layers of AlexNet [30] and VGGNet [48] to extract midlevel representations. Table 1 shows layer name in the Caffe pre-trained model [24] and the output size of their convolution layers."
    }, {
      "heading" : "3.1.2 RNN",
      "text" : "RNN is a neural network used to predict a sequence given a sequence. For each timestep t, hidden unit ht receives infor-\nmation from input xt and previous hidden unit ht−1. Then ht passes information to output yt. Because ht and ht−1 are connected, yt depends on x1, x2, ..., xt. Actually, RNN can make predictions using the context of infinite length.\nThe most basic type of recurrent layer is formalized as follows.\nht = tanh (Wxxt +Whht−1 + b) . (1)\nHowever, it cannot learn long-term dependencies in fact because gradients vanish in the process of flowing through many hidden-to-hidden connections. To overcome this problem, a variant of recurrent layer called Long-Short Term Memory (LSTM) was proposed [20]. We used a LSTM recurrent layer defined as shown below.\nit = σ (Wxixt +Whiht−1 + bi) . (2) ft = σ (Wxfxt +Whfht−1 + bf ) . (3) ot = σ (Wxoxt +Whoht−1 + bo) . (4) c̃t = tanh (Wxcxt +Whcht−1 + bc) . (5) ct = it ∗ c̃t + ft ∗ ct−1. (6) ht = ot ∗ tanh (ct) . (7)\nTherein, σ represents a sigmoid activation function. We use RNN using LSTM for sequential prediction, as described later. Concretely, we stack two LSTM layers and one linear layer to predict sequences. We set the dimensions of two LSTM layers as half of the input layer."
    }, {
      "heading" : "3.2. Preprocessing",
      "text" : "For example, the representation after conv1 layer of AlexNet comprises 55 × 55 vectors of 96 dimensions. We normalize each dimension of such vectors to have zeromean and unit-variance. After normalization, we apply Principal Component Analysis (PCA) on it to orthogonalize and reduce dimensions by half. Parameters of normalization and PCA are learned from the training set of ILSVRC 2012 image classification dataset [44]."
    }, {
      "heading" : "3.3. Combination of CNN and RNNLM",
      "text" : "Language Models (LMs) are used to measure the naturalness of a sequence. Let s1, s2, ..., sT to be a sequence of D dimensional vectors of length T . The LMs decompose the probability p(s) into p(s1)p(s2|s1)p(s3|s1, s2)...p(sT |s1, s2, ..., sT−1). An intuitive interpretation of this is that the probability is determined by how the next vector is predictable from past vectors.\nCommon LMs treat st of one-hot vector which represents a word and assume multinoulli distribution on p(st|s1, ..., st−1). In contrast, we use dense realvalue vector st and assume Gaussian distribution on\np(st|s1, ..., st−1). Concretely, regarding the Gaussian distribution, we assume that the mean µt is determined from s1, ..., st−1 using RNN described in Section 3.1.2. We also assume that the variance of timestep t is Tt because where t is small the model does not know much “context” of the sequence and predicted values are less reliable. Using this assumption, p(st|s1, ..., st−1) is rewritten by N (st|µt, Tt ). The negative log likelihood of s can be written as follows.\n− log p(s) ∝ 1T ∑T t=2 t T ||st − µt|| 2 2. (8)\nIt is the weighted sum of squared prediction error. We expand this model to 2D mid-level representation of CNN by application of RNNLM forward and backward for each row and column. We apply the same RNNLM in the same axis and direction. Therefore, there are four RNNLMs per layer. Let fy,x,l to be a representation immediately after layer l of size Hl ×Wl ×Dl. Then, we define the “unnaturalness map” uy,x,l(1 ≤ y ≤ Hl − 1, 1 ≤ x ≤ Wl − 1) of fl as follows.\nuy,x,l = x+1 Wl ||fy,x+1,l − µrighty,x+1,l||22 +\nWl−x+1 Wl ||fy,x,l − µlefty,x,l||22 + y+1 Hl ||fy+1,x,l − µdowny+1,x,l||22 + Hl−y+1 Hl ||fy,x,l − µupy,x,l||22. (9)\nTherein, µrighty,x,l is predicted value of fy,x,l from fy,1,l, ..., fy,x−1,l. This corresponds to scanning of an image from left to right predicting next time-step. µlefty,x,l, µdowny,x,l, and µ up y,x,l are also defined as the similar way. We define the total unnaturalness of fl as\nul = 1 (Hl−1)(Wl−1) ∑Hl−1 y=1 ∑Wl−1 x=1 uy,x,l. (10)\nWe introduce weighting parameter λl for layer l. Then, the total unnaturalness of an image i is\nui = ∑ l λlul. (11)"
    }, {
      "heading" : "3.4. Training of RNNLM",
      "text" : "To compute naturalness, we must train RNNLM in advance by minimizing ui of many natural images. We use Stochastic Gradient Descent (SGD) and Back-Propagation Through Time (BPTT) to train RNNLM. We use the training set of ILSVRC 2012 image classification dataset [44] for training.\nWe train RNNLM on mid-level representations of AlexNet [30] and VGGNet [48]. For AlexNet, learning rate and momentum of SGD is set, respectively, to 10 and 0.9. The minibatch size is set to 16. For VGGNet, learning rate and momentum of SGD is set, respectively, to 20 and 0.9. The minibatch size is set to 1.\nFor both networks, we reduce learning rate by the factor of 0.1 for every 5000 iterations. We stop learning after 20, 000 iterations."
    }, {
      "heading" : "3.5. Image reconstruction from features",
      "text" : "Mahendran and Vedaldi [38] demonstrated that image features can be inverted to the original image by gradient descent (GD) if the feature extraction function comprises differentiable elements. Their key technique is the introduction of “natural image prior” R(i) into their objective function. We denote the original image as i and feature extraction function as φ(i). Then, using λr as the weight of the regularizer, the reconstructed image î is\nî = argmin î\n||φ(i)− φ(̂i)||22 + λrR(̂i). (12)\nThey used R(̂i), which keeps pixel values in the natural range and penalizes strong intensity change in neighboring pixels. Instead, we set R(̂i) = uî. Because uî is differentiable, the objective function can be minimized by GD."
    }, {
      "heading" : "3.6. Eye fixation prediction",
      "text" : "It has been suggested that humans look at locations where Shannon’s “self-information” is high [5]. Because self-information is identical to the negative logarithm of probability, unnaturalness map ul can be regarded as a kind of information map that predicts salient locations.\nWe use an unnaturalness map ul as a saliency map. Additionally, we apply Gaussian blur of a size of σ to ul according to common practice [32, 36, 60]. Before blurring, we take the root of ul to prevent excessive expansion of peaky values. An example of unnaturalness map or saliency map ul is presented in Figure 2."
    }, {
      "heading" : "4. Experiments",
      "text" : "In this section, we present evaluation of the effectiveness of our proposed method by application of it to two tasks: image reconstruction from features and eye fixation prediction."
    }, {
      "heading" : "4.1. Image reconstruction from features",
      "text" : "Here we evaluate our image reconstruction method proposed in Section 3.5. First, we discuss how to evaluate reconstructed images. Then we present reconstructed images of ours and compare them with results of existing methods. Subsequently, we combine our method with the work by Dosovitskiy and Brox [11] and present further improved results. Finally, we examine the role of each layer by imposing the regularizer on the target layer.\nIn common with the preceding works [11, 38], we reconstructed images from the outputs of the last fullyconnected layer of AlexNet and used the first one hundred images in the validation set of ILSVRC 2012 classification dataset [44].\nOur model has the following hyper-parameters: the set of layers l used for mid-level representations, the\nweight of unnaturalness map λl, the weight of the regularizer λr, and the learning rate and momentum of GD. In this section, unless otherwise noted, we set l ∈ {conv1, conv2, conv3, conv4, conv5}, λconvn = 10−(n−1) for n ∈ {1, 2, 3, 4, 5} and λr = 10. The learning rate and momentum of GD are set, respectively, to 221, 0.9. Initial solution of GD is sampled from Gaussian distribution, the mean and standard deviation of which are learned by RGB values of natural images using the training set of ILSVRC 2012 classification dataset [44]."
    }, {
      "heading" : "4.1.1 Evaluation method",
      "text" : "Quantitative evaluation of whether a reconstructed image is similar to the original image or not is not straightforward. Some earlier reports of the liteature [8, 57] have provided no quantitative analysis. Because using a kind of image feature can produce an unfair comparison, mean squared error [11, 28, 38] or correlation coefficient [56] between reconstructed images and original images have been used to date. Vondrick et al. [56] evaluated reconstructed images by asking humans to classify them and reported that the correlation coefficient did not always match the judgments of humans.\nTherefore, in this work, we determine similarity of images by asking humans. We provide human subjects with the original image and corresponding reconstructed images. Then they select the image from reconstructed images which is the most similar to the original image. We asked one hundred people on CrowdFlower1."
    }, {
      "heading" : "4.1.2 Results of reconstruction",
      "text" : "Figure 3 depicts original images and reconstructed images by our method and two existing methods [11, 38]. Results of our method have clear edges rather than results of Mahendran and Vedaldi [38] because their regularizers prohibited strong change of intensity in neighboring pixels.\n1http://www.crowdflower.com/\nThe method presented by Dosovitskiy and Brox [11] reconstructs overall shapes and colors well, although the details are lost because their method outputs an average of possible solutions. Our results appear to be the most similar to the original images. They are clear and understandable, which helps us to interpret what the image features capture.\nTable 2 presents results of quantitative evaluation by crowd sourcing. Of 100 images, 94 of our images are se-\nlected to the most similar images to the original images. Our results received 67.22 % of total votes by 100 people, which clearly indicates the superiority of our method.\nFigure 4 presents six cases in which our results were judged to be inferior to two other methods. Our method is not good with reconstruction of the absolute positions and colors of objects. Additionally, our method is vulnerable to “unnatural” images because it is trained on natural images."
    }, {
      "heading" : "4.1.3 Better initialization",
      "text" : "The initial solution of GD is known to affect the result strongly, especially in neural networks [13, 50]. In fact, the key to breakthroughs in deep networks resulted from smart initialization of weights [19].\nBecause results of the method by Dosovitskiy and Brox can be interpreted as the average of possible solutions, they can be good initial solutions. Figure 5 and the last row of Figure 4 portray reconstructed images initialized with the outputs of the method presented by Dosovitskiy and Brox.\nThese results were improved considerably from previous results. For some images, the absolute positions and colors of objects are corrected, which indicates that the limitations of our method are mostly attributable to the initialization strategy and that they can be compensated by the method presented by Dosovitskiy and Brox."
    }, {
      "heading" : "4.1.4 Analysis of layers",
      "text" : "Figure 6 shows images reconstructed using our method. In this case, the regularizer is imposed on one certain layer or raw image pixels.\nThe result regularized on raw pixels, which are completely understandable, indicate the importance of modeling the naturalness of high-level features for generating realistic images, not of raw image pixels.\nThe results regularized on conv2 or conv3 are less clear than the result on conv1, which implies that the information contained by lower layers affects the naturalness of images. Higher layers can regularize more abstract information, but they are insufficient by themselves for generating images."
    }, {
      "heading" : "4.2. Eye fixation prediction",
      "text" : "In this section, we evaluate our eye fixation prediction method proposed in Section 3.6. We describe details of the datasets and evaluation metrics. Subsequently, we present the results.\nOur model has two hyper-parameters: the set of layers l used for mid-level representations and the kernel size of Gaussian blur σ. We use one convolution layer of VGGNet [48] as l and set σ to 0.030."
    }, {
      "heading" : "4.2.1 Dataset",
      "text" : "Many datasets are used for eye fixation prediction. Herein, we evaluate our method on two popular datasets, called MIT1003 [27] and Toronto [5]. Additionally, we evaluate ours on MIT Saliency Benchmark [6, 26], which consists of two other datasets: MIT300 and CAT2000. The MIT Saliency Benchmark is an online benchmarking service. Evaluation is done in submission.\nMIT1003 MIT1003 Dataset [27] includes 1003 images of natural indoor and outdoor scenes and corresponding eye fixation maps. It includes 779 landscape images and 228 portrait images.\nToronto Toronto [5] includes 200 images of outdoor and indoor scenes and corresponding eye fixation maps.\nMIT300 MIT300 [6, 26] includes 300 images of natural indoor and outdoor scenes. Corresponding fixation maps are not provided. Because the protocol to collect this dataset\nis almost as the same as MIT1003, MIT1003 is useful as a training set.\nCAT2000 CAT2000 [4] is a recently introduced dataset. It includes 2000 images of 20 different image categories including action, affective, art, black & white, cartoon, fractal, indoor, inverted, jumbled, line drawing, low-resolution, noisy, object, outdoor man-made, outdoor natural, pattern, random, satellite, sketch, and social. This dataset is challenging because most categories are not natural scenes."
    }, {
      "heading" : "4.2.2 Evaluation metrics",
      "text" : "Eye fixation task can be interpreted as detection task to detect eye fixation point from an image. Therefore, areaunder-the-curve score (AUC) of ROC curve is often used for evaluation [5]. However, because humans tend to look around the center of an image, this metric assigns much value to center-biased saliency maps. To overcome this problem, shuffled AUC has been proposed [52, 61]. This metric computes AUC, not on all pixels uniformly, but on center-biased eye fixation points of other images. The shuffled AUC score of centered Gaussian is about 0.5. We use this metric for evaluation."
    }, {
      "heading" : "4.2.3 Results of benchmark dataset",
      "text" : "Figure 7 shows the shuffled AUC score obtained using our method on MIT1003 and Toronto dataset. Various l and σ are tested. The best performing setting is l = conv5 4, σ = 0.030 for MIT1003 and l = conv5 1, σ = 0.010 for Toronto.\n2This value is not posted online but is included in their paper.\nTable 3 presents our results and existing results on the MIT1003 and Toronto datasets. We compare ours with Mr-CNN [36], AWS [12], BMS [60], CA [14], eDN [55], HFT [35], ICL [22], IS [21], JUDD [27], LG [2] and QDCT [46]. Ours achieved state-of-the-art score on these well-studied datasets.\nTable 4 presents our result on MIT300 and CAT2000. We compare ours with SALICON [25], Deep Fix [31], Deep Gaze I [32], SalNet3, Mr-CNN [36], AWS [12], CA [14], WMAP [37] and IttiKoch2 [23], scores of which are available on the scoreboard of MIT Saliency Benchmark. We set l = conv5 4, σ = 0.030 for MIT300 and l = conv5 1, σ = 0.030 for CAT 2000. Our results took second place on MIT300 and first place on CAT2000."
    }, {
      "heading" : "4.2.4 Discussion",
      "text" : "It is noteworthy that higher layers produce better results as shown in Figure 7. Additionally, although we tried to combine all saliency maps of different layers by supervised training in our preliminary experiments, it did not produce any performance improvement. These results indicate that eye fixation points are determined exclusively from higherlevel signals.\nTable 5 presents our score and the average of scores of submitted results available on the scoreboard of each cate-\n3Unpublished work.\ngory on the CAT2000 dataset. Compared to other methods, ours are superior for Social, Cartoon, and Affective. Presumably, that is true because images of these categories include more high-level contents such as faces or pedestrians. Ours is inferior for Pattern, Low Resolution, and Fractal because ours are trained on natural images. Images of these categories are apparently not natural. Prediction accuracy can be improved by training of RNNLM on images of these categories.\nMost top-scoring methods on MIT Saliency Benchmark are based on supervised training on eye-fixation dataset [25, 31, 32, 36]. These methods require large eye fixation dataset of target domain. If there is no dataset or the dataset is small, the performance of these models can drop. The difference of scores of DeepFix on MIT300 and CAT2000 indicates that. Ours are based on unsupervised training. Therefore it is unaffected by this problem."
    }, {
      "heading" : "5. Conclusion",
      "text" : "In this work, based on an assumption that the naturalness can be measured by the predictability on high-level features during eye movement, we proposed a novel method to measure the naturalness of an image by building a variant of RNNLMs on the CNN features. We used it as a regularizer in reconstructing images from image features. The results of experiments show that this regularizer helps to generate more feasible images than existing approaches. We found that the naturalness of lower layers is important to generate natural images. Additionally, we evaluated “unnaturalness maps” of images as saliency maps. This was motivated by the assumption that saliency of images is based on the selfinformation of each location. We demonstrated that the proposed “unnaturalness map” achieves state-of-the-art shuffled AUC scores on two well-studied eye fixation prediction datasets. It was indicated that the naturalness of higher layers predicts eye fixation points well.\nThe naturalness of images, especially for large images that include rich contents, has not been studied well. Nonetheless, methods to assess and detect naturalness of images are extremely useful, as demonstrated in the experiments. We hope this work will provoke more active research in this field."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2015,
    "abstractText" : "Measuring the naturalness of images is important to generate realistic images or to detect unnatural regions in images. Additionally, a method to measure naturalness can be complementary to Convolutional Neural Network (CNN) based features, which are known to be insensitive to the naturalness of images. However, most probabilistic image models have insufficient capability of modeling the complex and abstract naturalness that we feel because they are built directly on raw image pixels. In this work, we assume that naturalness can be measured by the predictability on high-level features during eye movement. Based on this assumption, we propose a novel method to evaluate the naturalness by building a variant of Recurrent Neural Network Language Models on pre-trained CNN representations. Our method is applied to two tasks, demonstrating that 1) using our method as a regularizer enables us to generate more understandable images from image features than existing approaches, and 2) unnaturalness maps produced by our method achieve state-of-the-art eye fixation prediction performance on two well-studied datasets.",
    "creator" : "LaTeX with hyperref package"
  }
}