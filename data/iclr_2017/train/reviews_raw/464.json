{"conference": "ICLR 2017 conference submission", "title": "Learning to Compose Words into Sentences with Reinforcement Learning", "abstract": "We use reinforcement learning to learn tree-structured neural networks for computing representations of natural language sentences. In contrast with prior work on tree-structured models, in which the trees are either provided as input or predicted using supervision from explicit treebank annotations, the tree structures in this work are optimized to improve performance on a downstream task. Experiments demonstrate the benefit of learning task-specific composition orders, outperforming both sequential encoders and recursive encoders based on treebank annotations. We analyze the induced trees and show that while they discover some linguistically intuitive structures (e.g., noun phrases, simple verb phrases), they are different than conventional English syntactic structures.", "reviews": [{"is_meta_review": true, "comments": "In this paper, the authors propose a new method to learn hierarchical representations of sentences, based on reinforcement learning. They propose to learn a neural shift-reduce parser, such that the induced tree structures lead to good performance on a downstream task. They use reinforcement learning (more specifically, the policy gradient method REINFORCE) to learn their model. The reward of the algorithm is the evaluation metric of the downstream task. The authors compare two settings, (1) no structure information is given (hence, the only supervision comes from the downstream task) and (2) actions from an external parser is used as supervision to train the policy network, in addition to the supervision from the downstream task. The proposed approach is evaluated on four tasks: sentiment analysis, semantic relatedness, textual entailment and sentence generation.\n\nI like the idea of learning tree representations of text which are useful for a downstream task. The paper is clear and well written. However, I am not convinced by the experimental results presented in the paper. Indeed, on most tasks, the proposed model is far from state-of-the-art models:\n - sentiment analysis, 86.5 v.s. 89.7 (accuracy);\n - semantic relatedness, 0.32 v.s. 0.25 (MSE);\n - textual entailment, 80.5 v.s. 84.6 (accuracy).\nFrom the results presented in the paper, it is hard to know if these results are due to the model, or because of the reinforcement learning algorithm.\n\nPROS:\n - interesting idea: learning structures of sentences adapted for a downstream task.\n - well written paper.\nCONS:\n - weak experimental results (do not really support the claim of the authors).\n\nMinor comments:\nIn the second paragraph of the introduction, one might argue that bag-of-words is also a predominant approach to represent sentences.\nParagraph titles (e.g. in section 3.2) should have a period at the end.\n\n----------------------------------------------------------------------------------------------------------------------\nUPDATE\n\nI am still not convinced by the results presented in the paper, and in particular by the fact that one must combine the words in a different way than left-to-right to obtain state of the art results.\nHowever, I do agree that this is an interesting research direction, and that the results presented in the paper are promising. I am thus updating my score from 5 to 6.", "IS_META_REVIEW": true}, {"TITLE": "ICLR committee final decision", "OTHER_KEYS": "ICLR 2017 pcs", "comments": "All the reviewers agreed that the research direction is very interesting, and generally find the results promising. We could quibble a bit about the results not being really state-of-the-art and the choice of baselines, but I think the main claims are well supported by the experiments (i.e. the induce grammar appears to be useful for the problem in question, within the specific class of models). There are still clearly many issues unresolved, and we are yet to see if this class of methods (RL / implicit structure-based) can lead to state-of-the-art results on any important NLP problem. But it is too much to ask from a submission. I see the paper as a strong contribution to the conference.", "IS_META_REVIEW": false, "DATE": "06 Feb 2017", "is_meta_review": false}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Accept", "is_meta_review": false, "comments": "I have not much to add to my pre-review comments.\nIt's a very well written paper with an interesting idea.\nLots of people currently want to combine RL with NLP. It is very en vogue.\nNobody has gotten that to work yet in any really groundbreaking or influential way that results in actually superior performance on any highly relevant or competitive NLP task.\nMost people struggle with the fact that NLP requires very efficient methods on very large datasets and RL is super slow.\nHence, I believe this direction hasn't shown much promise yet and it's not yet clear it ever will due to the slowness of RL.\nBut many directions need to be explored and maybe eventually they will reach a point where they become relevant.\n\nIt is interesting to learn the obviously inherent grammatical structure in language though sadly again, the trees here do not yet capture much of what our intuitions are.\n\nRegardless, it's an interesting exploration, worthy of being discussed at the conference.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "24 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "official review", "is_meta_review": false, "comments": "The paper proposes to use reinforcement learning to learn how to compose the words in a sentence, i.e. parse tree, that can be helpful for the downstream tasks. To do that, the shift-reduce framework is employed and RL is used to learn the policy of the two actions SHIFT and REDUCE. The experiments on four datasets (SST, SICK, IMDB, and SNLI) show that the proposed approach outperformed the approach using predefined tree structures (e.g. left-to-right, right-to-left). \n\nThe paper is well written and has two good points. Firstly, the idea of using RL to learn parse trees using downstream tasks is very interesting and novel. And employing the shift-reduce framework is a very smart choice because the set of actions is minimal (shift and reduce). Secondly, what shown in the paper somewhat confirms the need of parse trees. This is indeed interesting because of the current debate on whether syntax is helpful.\n\nI have the following comments:\n- it seems that the authors weren't aware of some recent work using RL to learn structures for composition, e.g. Andreas et al (2016).\n- because different composition functions (e.g. LSTM, GRU, or classical recursive neural net) have different inductive biases, I was wondering if the tree structures found by the model would be independent from the composition function choice.\n- because RNNs in theory are equivalent to Turing machines, I was wondering if restricting the expressiveness of the model (e.g. reducing the dimension) can help the model focus on discovering more helpful tree structures.\n\nRef:\nAndreas et al. Learning to Compose Neural Networks for Question Answering. NAACL 2016", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Weak experimental results", "is_meta_review": false, "comments": "In this paper, the authors propose a new method to learn hierarchical representations of sentences, based on reinforcement learning. They propose to learn a neural shift-reduce parser, such that the induced tree structures lead to good performance on a downstream task. They use reinforcement learning (more specifically, the policy gradient method REINFORCE) to learn their model. The reward of the algorithm is the evaluation metric of the downstream task. The authors compare two settings, (1) no structure information is given (hence, the only supervision comes from the downstream task) and (2) actions from an external parser is used as supervision to train the policy network, in addition to the supervision from the downstream task. The proposed approach is evaluated on four tasks: sentiment analysis, semantic relatedness, textual entailment and sentence generation.\n\nI like the idea of learning tree representations of text which are useful for a downstream task. The paper is clear and well written. However, I am not convinced by the experimental results presented in the paper. Indeed, on most tasks, the proposed model is far from state-of-the-art models:\n - sentiment analysis, 86.5 v.s. 89.7 (accuracy);\n - semantic relatedness, 0.32 v.s. 0.25 (MSE);\n - textual entailment, 80.5 v.s. 84.6 (accuracy).\nFrom the results presented in the paper, it is hard to know if these results are due to the model, or because of the reinforcement learning algorithm.\n\nPROS:\n - interesting idea: learning structures of sentences adapted for a downstream task.\n - well written paper.\nCONS:\n - weak experimental results (do not really support the claim of the authors).\n\nMinor comments:\nIn the second paragraph of the introduction, one might argue that bag-of-words is also a predominant approach to represent sentences.\nParagraph titles (e.g. in section 3.2) should have a period at the end.\n\n----------------------------------------------------------------------------------------------------------------------\nUPDATE\n\nI am still not convinced by the results presented in the paper, and in particular by the fact that one must combine the words in a different way than left-to-right to obtain state of the art results.\nHowever, I do agree that this is an interesting research direction, and that the results presented in the paper are promising. I am thus updating my score from 5 to 6.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016 (modified: 21 Jan 2017)", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "implementation ", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "It was slow to train the model because you had to build a different new computational graph for each sentence. I was wondering if the slow speed is due to the way you implemented (e.g. tensorflow, python). Dynet is designed especially for this kind of problem, did you try it? ", "IS_META_REVIEW": false, "DATE": "11 Dec 2016", "is_meta_review": false}, {"TITLE": "Experiments on SST", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "", "IS_META_REVIEW": false, "DATE": "11 Dec 2016", "is_meta_review": false}, {"TITLE": "Previous work on semi-supervised binary tree constructions", "OTHER_KEYS": "Kazuma Hashimoto", "comments": "Dear authors,\n\nI've read the interesting paper and learned nice ideas.\n\nNow I've found somewhat incorrect mention in your paper.\nIn Introduction, it is said that trees are provided with sentences in Socher et al. (2011), but they jointly learn the binary tree structures according to the target task (sentiment classification) although the approach is different (reinforcement learning or autoencoder).\n\nBest,\n  Kazuma", "IS_META_REVIEW": false, "DATE": "09 Dec 2016", "is_meta_review": false}, {"TITLE": "Experiments", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "", "IS_META_REVIEW": false, "DATE": "03 Dec 2016", "is_meta_review": false}, {"TITLE": "Feedback", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "", "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "is_meta_review": false}, {"TITLE": "Why is the current \"supervised syntax\" performing worse than older \"supervised syntax\" baselines?", "OTHER_KEYS": "(anonymous)", "comments": "In several of the evaluation, the \"constituency tree LSTM\" and/or \"dependency tree LSTM\" methods perform much better than all of your proposed models, including the \"supervised syntax\" model, and sometimes even with the same number of parameters. What is the difference between your \"supervised syntax\" method and these tree LSTMs? why doesn't the supervised syntax approach perform at least as good as the Tai et al models?", "IS_META_REVIEW": false, "DATE": "21 Nov 2016", "is_meta_review": false}, {"TITLE": "comparison vs NTI", "OTHER_KEYS": "(anonymous)", "comments": "Big fan of this work. Related to the above comment, have you guys tried a simple baseline where you just assume a binary tree? I imagine it will do better than left-to-right and right-to-left (given the success of NTI), but will probably do worse than the proposed method.", "IS_META_REVIEW": false, "DATE": "11 Nov 2016", "is_meta_review": false}, {"TITLE": "Results on Stanford Sentiment Treebank", "OTHER_KEYS": "Tsendsuren Munkhdalai", "comments": "Table 2 is missing some recent results on this task. Please see the NTI and NSE results on the same task [1,2]. NTI is particularly relevant to this work because it encodes a sentence with an n-ary tree (i.e. binary tree) instead of using a parser output or learning to compose.\n\n\nThanks,\n\n\n\nRef:\n\n1. Munkhdalai, Tsendsuren, and Hong Yu. \"Neural Tree Indexers for Text Understanding.\" arXiv preprint arXiv:1607.04492 (2016).\n2. Munkhdalai, Tsendsuren, and Hong Yu. \"Neural Semantic Encoders.\" arXiv preprint arXiv:1607.04315 (2016).", "IS_META_REVIEW": false, "DATE": "09 Nov 2016", "is_meta_review": false}], "SCORE": 6, "authors": "Dani Yogatama, Phil Blunsom, Chris Dyer, Edward Grefenstette, Wang Ling", "accepted": true, "id": ""}
