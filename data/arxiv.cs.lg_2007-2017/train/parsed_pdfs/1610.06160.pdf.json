{
  "name" : "1610.06160.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Streaming Normalization: Towards Simpler and More Biologically-plausible Normalizations for Online and Recurrent Learning",
    "authors" : [ "Qianli Liao", "Kenji Kawaguchi" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "This work was supported by the Center for Brains, Minds and Machines (CBMM), funded by NSF STC award CCF - 1231216.\nar X"
    }, {
      "heading" : "1 Introduction",
      "text" : "Batch Normalization [Ioffe and Szegedy, 2015] (BN) is a highly effective technique for speeding up convergence of feedforward neural networks. It enabled recent development of ultra-deep networks [He et al., 2015] and some biologically-plausible variants of backpropagation [Liao et al., 2015]. However, despite its success, there are two major learning scenarios that cannot be handled by BN: (1) online learning and (2) recurrent learning.\nFor the second scenario recurrent learning, [Liao and Poggio, 2016] and [Cooijmans et al., 2016] independently proposed “time-specific batch normalization”: different normalization statistics are used for different timesteps of an RNN. Although this approach works well in many experiments in [Liao and Poggio, 2016] and [Cooijmans et al., 2016], it is far from perfect due to the following reasons: First, it does not work with small mini-batch or online learning. This is similar to the original batch normalization, where enough samples are needed to compute good estimates of statistical moments. Second, it requires sufficient training samples for every timestep of an RNN. It is not clear how to generalize the model to unseen timesteps. Finally, it is not biologically-plausible. While homeostatic plasticity mechanisms (e.g., Synaptic Scaling) [Turrigiano and Nelson, 2004, Stellwagen and Malenka, 2006, Turrigiano, 2008] are good biological candidates for BN, it is hard to imagine how such normalizations can behave differently for each timestep. Recently, Layer Normalization (LN) [Ba et al., 2016] was introduced to solve some of these issues. It performs well in feedforward and recurrent settings when only fully-connected layers are used. However, it does not work well with convolutional networks. A summary of normalization approaches in various learning scenarios is shown in Table 1.\nWe note that different normalization methods like BN and LN can be described in the same framework detailed in Section 3. This framework introduces Sample Normalization and General Batch Normalization (GBN) as generalizations of LN and BN, respectively. As their names imply, they either collect normalization statistics from a single sample or from a mini-batch. We explored many variants of these models in the experiment section.\nA natural and biologically-inspired extension of these methods would be Streaming Normalization: normalization statistics are collected in an online fashion from all previously seen training samples (and all timesteps if recurrent). We found numerous advantages associated with this approach: 1. it naturally supports pure online learning or learning with small mini-batches. 2. for recurrent learning, it is more biologically-plausible since a unique set of normalization statistics is maintained for all timesteps. 3. it performs well out of the box in all learning scenarios (e.g., online learning, batch learning, fully-connected, convolutional, feedforward, recurrent and mixed — recurrent and convolutional). 4. it offers a new direction of designing normalization algorithms, since the idea of maintaining online estimates of normalization statistics is independent from other design choices, and as a result any existing algorithm (e.g., in Figure 1 C and D) can be extended to a “streaming” setting.\nWe also propose Lp normalization: instead of normalizing by the second moment like BN, one can normalize by the p-th root of the p-th absolute moment (See Section 3.4 for details). In particular, L1 normalization works as well as the conventional approach (i.e., L2) in almost all learning scenarios. Furthermore, L1 normalization is easier to implement: it is simply the average of absolute values. We believe it can be used to simplify and speed up BN and our Streaming Normalization in GPGPU, dedicated hardware or embedded systems. L1 normalization may also\nbe more biologically-plausible since the gradient of the absolute value is trivial to implement, even for biological neurons.\nIn the following section, we introduce a simple training scheme, a minor but necessary component of our formulation.\n2 Online and Batch Learning with “Decoupled Accumulation and Update”\nAlthough it is not our main result, we discuss a simple but to the best of our knowledge less explored1 training scheme we call a “Decoupled Accumulation and Update” (DAU).\nConventionally, the weights of a neural network are updated every mini-batch. So the accumulation of gradients and weights update are coupled. We note that a general formulation would be that while for every mini-batch the gradients are still accumulated, one does not necessarily update the weights. Instead, the weights are updated every n mini-batches. The gradients are cleared after each weight update. Two parameters characterize this procedure: Samples per Batch (S/B) m and Batch per Update (B/U) n. In conventional training, n = 1.\nNote that this procedure is similar to but distinct from simply training larger mini-batches since every mini-batch arrives in a purely online fashion so one cannot look back into any previous mini-batches. For example, if batch normalization is present, performing this procedure with m S/B and n B/U is different from that with m ∗ n S/B and 1 B/U.\nIf m = 1, it reduces to a pure online setting where one sample arrives at a time. The key advantage of this proposal over conventional training is that we explicitly require less frequent (but more robust) weight updates. The memory requirement (in additional to storing the network weights) is storing m samples and related activations.\nThis training scheme generalizes the conventional approach, and we found that merely applying this approach greatly mitigated (although not completely solved) the catastrophic failure of training batch normalization with small mini-batches (See Figure 4). Therefore, we will use this formulation throughout our paper.\nWe also expect this scheme to benefit learning sequential recurrent networks with varying input sequence lengths. Sometimes it is more efficient to pack in a mini-batch training samples with the same sequence length. If this is the case, our approach predicts that it would be desirable to process multiple such mini-batches with varying sequence lengths before a weight update. Accumulating gradients from training different sequence lengths should provide a more robust update that works for different sequence lengths, thus better approximating the true gradient of the dataset.\nIn Streaming Normalization with recurrent networks, it is often beneficial to learn with n > 1 (i.e., more than one batch per update). The first mini-batch collects all the normalization statistics from all timesteps so that later mini-batches are normalized in a more stable way."
    }, {
      "heading" : "3 A General Framework for Normalization",
      "text" : "We propose a general framework to describe different normalization algorithms. A normalization can be thought of as a process of modifying the activation of a neuron using some statistics collected from some reference activations. We adopt three terms to characterize this process: A Normalization Operation (NormOP) is a function N(xi, si) that is applied to each neuron i to modify its value from xi to N(xi, si), where si is the Normalization Statistics (NormStats) for this neuron. NormStats is any data required to perform NormOP, collected using some function si = S(Ri), where Ri is a set of activations (could include xi) called Normalization Reference (NormRef). Different neurons may or may not share NormRef and NormStats.\n1We are not aware of this approach in the literature. If there is, please inform us.\nThis framework captures many previous normalizations algorithms as special cases. For example, the original Batch Normalization (BN) [Ioffe and Szegedy, 2015] can be described as follows: the NormRef for each neuron i is all activations in the same channel of the same layer and same batch (See Figure 1 C for illustration). The NormStats are Si ={µi,σi} — the mean and standard deviation of the activations in NormRef. The NormOp is N(xi,{µi,σi}) = xi−µiσi\nFor the recent Layer Normalization [Ba et al., 2016], NormRef is all activations in the same layer (See Figure 1 C). The NormStats and NormOp are the same as BN.\nWe group normalization algorithms into three categories:\n1. Sample Normalization (Figure 1 C): NormStats are collected from one sample.\n2. General Batch Normalization (Figure 1 D): NormStats are collected from all samples in a mini-batch.\n3. Streaming Normalization: NormStats are collected in an online fashion from all pass training samples.\nIn the following sections, we detail each algorithm and provide pseudocode."
    }, {
      "heading" : "3.1 Sample Normalization",
      "text" : "Sample normalization is the simplest category among the three. NormStats are collected only using the activations in the current layer of current sample. The computation is the same at training and test times. It handles online learning naturally. Layer Normalization [Ba et al., 2016] is an example of this category. More examples are shown in Figure 1 C.\nThe pseudocode of forward and backpropagation is show in Algorithm 1 and Algorithm 2.\nAlgorithm 1 Sample Normalization Layer: Forward\nRequire: layer input x (a sample), NormOP N(.,.), function S(.) to compute NormStats for every element of x Ensure: layer output y (a sample) s = S(x) y = N(x,s)\nAlgorithm 2 Sample Normalization Layer: Backpropagation\nRequire: ∂E∂y (a sample) where E is objective, layer input x, NormOP N(.,.), function S(.) to compute NormStats for every element of x Ensure: ∂E∂x (a sample) ∂E ∂x can be calculated using chain rule. Detail omitted."
    }, {
      "heading" : "3.2 General Batch Normalization",
      "text" : "In General Batch Normalization (GBN), NormStats are collected in some way using the activations from all samples in a training mini-batch. Note that one cannot really compute the batch NormStats at test time since test samples should be handled independently from each other, instead in a batch. To overcome this, one can simply compute running estimates of training NormStats and use them for testing (e.g., the original Batch Normalization [Ioffe and Szegedy, 2015] computes moving averages).\nMore examples of GBN are shown in Figure 1 D. The pseudocode is shown in Algorithm 3 and 4.\n(A) (B) Decoupled Accumulation and Update\nAlgorithm 3 General Batch Normalization Layer: Forward\nRequire: layer input x (a mini-batch), NormOP N(.,.), function S(.) to compute NormStats for every element of x, running estimates of NormStats ŝ, function F to update ŝ. Ensure: layer output y (a mini-batch), running estimates of NormStats ŝ if training then\ns = S(x) ŝ = F(ŝ,s) y = N(x,s)\nelse {testing} y = N(x,̂s) end if\nAlgorithm 4 General Batch Normalization Layer: Backpropagation\nRequire: ∂E∂y (a mini-batch) where E is objective, layer input x (a mini-batch), NormOP N(.,.), function S(.) to compute NormStats for every element of x, running estimates of NormStats ŝ, function F to update ŝ. Ensure: ∂E∂x (a mini-batch) ∂E ∂x can be calculated using chain rule. Detail omitted."
    }, {
      "heading" : "3.3 Streaming Normalization",
      "text" : "Finally, we present the main results of this paper. We propose Streaming Normalization: NormStats are collected in an online fashion from all past training samples. The main challenge of this approach is that it introduces infinitely long dependencies throughout the neuron’s activation history — every neuron’s current activation depends on all previously seen training samples.\nIt is intractable to perform exact backpropagation on this dependency graph for the following reasons: there is no point backpropagating beyond the last weight update (if any) since one cannot redo the weight update. This, on the other hand, would imply that one cannot update the weights until having seen all future samples. Even backpropagating within a weight update (i.e., the interval between two weight updates, consisting of n mini-batches) turns out to be problematic: one is usually not allowed to backpropagate to the previous several mini-batches since they are discarded in many practical settings.\nFor the above reasons, we abandoned the idea of performing exact backpropagation for streaming normalization. Instead, we propose two simple heuristics: Streaming NormStats and Streaming Gradients. They are discussed below and the pseudocode is shown in Algorithm 5 and Algorithm 6."
    }, {
      "heading" : "3.3.1 Streaming NormStats",
      "text" : "Streaming NormStats is a natural requirement of Streaming Normalization, since NormStats are collected from all previously seen training samples. We maintain a structure/table H1 to keep all the information needed to generate a good estimate of NormStats. and update it using a function F everytime we encounter a new training sample. Function F also generates the current estimate of NormStats called ŝ. We use ŝ to normalize instead of s. See Algorithm 5 for details.\nThere could be many potential designs for F and H1, and in our experiments we explored a particular version: we compute two sets of running estimates to keep track of the long-term and short-term NormStats: H1 = {ŝlong, ŝshort, counter}.\n• Short-term NormStats ŝshort is the exact average of NormStats s since the last weight update. The counter keeps track of the number of times a different s is encountered to compute the exact average of s.\n• Long-term NormStats ŝlong is an exponential average of ŝshort since the beginning of training.\nWhenever the weights of the network is updated: ŝlong = κ1 ∗ ŝlong + κ2 ∗ ŝshort, counter is reset to 0 and ŝshort is set to empty. In our experiments, κ1 + κ2 = 1 so that an exponential average of ŝshort is maintained in ŝlong.\nIn our implementation, before testing the model, the last weight update is NOT performed, and ŝshort is also NOT cleared, since ŝshort is needed for testing 2\nIn addition to updating H1 in the way described above, the function F also computes ŝ = α1ŝlong+α2ŝshort. Finally, ŝ is used for normalization."
    }, {
      "heading" : "3.3.2 Streaming Gradients",
      "text" : "We maintain a structure/table H2 to keep all the information needed to generate a good estimate of gradients of NormStats and update it using a function G everytime backpropagation reaches this layer. Function G also generates the current estimate of gradients of NormStats called ∂̂E∂ŝ . We use ∂̂E ∂ŝ for further backpropagation instead of ∂E∂ŝ . See Algorithm 6 for details.\nAgain, there could be many potential designs for G and H2, and we explored a particular version: we compute two sets of running estimates to keep track of the long-term and short-term gradients of NormStats: H2 = {ĝlong, ĝshort, counter}.\n• Short-term Gradients of NormStats ĝshort is the exact average of gradients of NormStats ∂E∂ŝ since the last weight update. The counter keeps track of the number of times a different ∂E∂ŝ is encountered to compute the exact average of ∂E∂ŝ .\n• Long-term Gradients of NormStats ĝlong is an exponential average of ĝshort since the beginning of training.\nWhenever the weights of network is updated: ĝlong = κ3 ∗ ĝlong + κ4 ∗ ĝshort, counter is reset to 0 and ĝshort is set to empty. In our experiments, κ3 + κ4 = 1 so that an exponential average of ĝshort is maintained in ĝlong.\nIn addition to updating H2 in the way described above, the function G also computes ∂̂E∂ŝ = β1ĝlong+β2ĝshort+β3 ∂E ∂ŝ . Finally, ∂̂E∂ŝ is used for further backpropagation.\nAlgorithm 5 Streaming Normalization Layer: Forward Require: layer input x (a mini-batch), NormOP N(.,.), function S(.) to compute NormStats for every element of x, running estimates of NormStats and/or related information packed in a structure/table H1, function F to update H1 and generate current estimate of NormStats ŝ. Ensure: layer output y (a mini-batch) and H1 (it is stored in this layer, instead of feeding to other layers), always maintain the latest ŝ in case of testing if training then\ns = S(x) {H1 ,̂s} = F(H1,s) y = N(x,̂s)\nelse {testing} y = N(x,̂s) end if\n2It also possible to simply store the last ŝ computed in training for testing, instead of storing ŝshort and re-computing ŝ in testing. These two options are mathematically equivalent.\nAlgorithm 6 Streaming Normalization Layer: Backpropagation Require: ∂E∂y (a mini-batch) where E is objective, layer input x (a mini-batch), NormOP N(.,.), function S(.) to compute NormStats for every element of x, running estimates of NormStats ŝ, running estimates of gradients and/or related information packed in a structure/table H2, function G to update H2 and generate the current estimates of gradients of NormStats ∂̂E∂ŝ . Ensure: ∂E∂x (a mini-batch) and H2 (it is stored in this layer, instead of feeding to other layers) ∂E ∂ŝ is calculated using chain rule.\n{H2, ∂̂E∂ŝ } = G(H2, ∂E ∂ŝ ) Use ∂̂E∂ŝ for further backpropagation, instead of ∂E ∂ŝ ∂E ∂x is calculated using chain rule."
    }, {
      "heading" : "3.3.3 A Summary of Streaming Normalization Design and Hyperparameters",
      "text" : "The NormOp used in this paper is N(xi,{µi,σi}) = xi−µiσi , the same as what is used by BN. The NormStats are collected using one of the Lp normalization schemes described in Section 3.4.\nWith our particular choices of F , G, H1 and H2, the following hyperparameters uniquely characterize a Streaming Normalization algorithm: α1, α2, β1, β2, β3, κ1, κ2, κ3, κ4, samples per batch m, batches per update n and a choice of mini-batch NormRef (i.e., SP1-SP5, BA1-BA6 in Figure 1 C and D).\nUnless mentioned otherwise, we use BA1 in Figure 1 D as the NormRef throughout the paper. We also demonstrate the use of other NormRefs (BA4 and BA6) in the Appendix Figure A1.\nAn important special case: If n = 1, α1 = 0, α2 = 1, β1 = 0, β2 = 0, β3 = 1, we ignore all NormStats and gradients beyond the current mini-batch. The algorithm reduces to exactly the GBN algorithm. So Streaming Normalization is strictly a generalization of GBN and thus also captures the original BN [Ioffe and Szegedy, 2015] as a special case. Recall from Section 3.3.1 that ŝshort is NOT cleared before testing the model. Thus for testing, NormStats are inherited from the last training mini-batch. It works well in practice.\nUnless mentioned otherwise, we set α1 = κ1 = κ3, α2 = κ2 = κ4, κ1 + κ2 = 1, κ3 + κ4 = 1, α1 + α2 = 1, β1 + β2 + β3 = 1. We leave it to future research to explore different choices of hyperparameters (and perhaps other F and G)."
    }, {
      "heading" : "3.3.4 Implementation Notes",
      "text" : "One minor drawback of not performing exact backpropagation is that it may break the gradient check of the entire model. One solution coulde be: (1) perform gradient check of the model without SN and then add a correctly implemented SN. (2) make sure the SN layer is correctly coded (e.g., by reducing it to standard BN using the hyperparameters discussed above and then perform gradient check)."
    }, {
      "heading" : "3.4 Lp Normalization: Calculating NormStats with Different Orders of Moments",
      "text" : "Let us discuss the function S(.) for calculating NormStats. There are several choices for this function. We propose Lp normalization. It captures the previous mean-and-standard-deviation normalization as a special case.\nFirst, mean µ is always calculated the same way — the average of the activations in NormRef. The divisive factor σ, however, can be calculated in several different ways. In Lp Normalization, σ is chosen to be the p-th root of the p-th Absolute Moment.\nHere the Absolute Moment of a distribution P (x) about a point c is:\n∫ |x− c|pP (x)dx (1)\nand the discrete form is: 1\nN N∑ i=1 |xi − c|p (2)\nLp Normalization can be performed with three settings:\n• Setting A: σ is the p-th root of the p-th absolute moment of all activations in NormRef with c being the mean µ of NormRef.\n• Setting B: σ is the p-th root of the p-th absolute moment of all activations in NormRef with c being the running estimate µ̂ of the average.\n• Setting C: σ is the p-th root of the p-th absolute moment of all activations in NormRef with c being 0.\nMost of these variants have similar performance but some are better in some situations.\nWe call it Lp normalization since it is similar to the norm in the Lp space.\nSetting B and C are better for online learning since A will give degenerate result (i.e., σ = 0) when there is only one sample in NormRef. Empirically, when there are enough samples in a mini-batch, A and B perform similarly.\nWe discuss several important special cases:\nSpecial Case A-2: setting A with n=2, σ is the standard deviation (square root of the 2nd moment) of all activations in NormRef. This setting is what is used by Batch Normalization [Ioffe and Szegedy, 2015] and Layer Normalization [Ba et al., 2016].\nSpecial Case p=1: Whenever p=1, σ is simply the average of absolute values of activations. This setting works virtually the same as p=2, but is much simpler to implement and faster to run. It might also be more biologically-plausible, since the gradient computations are much simpler."
    }, {
      "heading" : "3.5 Separate Learnable Bias and Gain Parameters",
      "text" : "The original Batch Normalization [Ioffe and Szegedy, 2015] also learns a bias and a gain (i.e., shift and scaling) parameter for each feature map. Although not usually done, but clearly these shift and scaling operations can be completely separated from the normalization layer. We implemented them as a separate layer following each normalization layer. These parameters are learned in the same way for all normalization schemes evaluated in this paper."
    }, {
      "heading" : "4 Generalization to Recurrent Learning",
      "text" : "In this section, we generalize Sample Normalization, General Batch Normalization and Streaming Normalization to recurrent learning. The difference between recurrent learning and feedforward learning is that for each training sample, every hidden layer of the network receives t activations h1, , ..., ht, instead of only one."
    }, {
      "heading" : "4.1 Recurrent Sample Normalization",
      "text" : "Sample Normalization naturally generalizes to recurrent learning since all NormStats are collected from the current layer of the current timestep. Training and testing algorithms remain the same."
    }, {
      "heading" : "4.2 Recurrent General Batch Normalization (RGBN)",
      "text" : "The generalization of GBN to recurrent learning is the same as what was proposed by [Liao and Poggio, 2016] and [Cooijmans et al., 2016]. The training procedure is exactly the same as before (Algorithm 3 and Algorithm 4). For testing, one set of running estimates of NormStats is maintained for each timestep.\nAnother way of viewing this algorithm is that the same GBN layers described in Algorithm 3 and 4 are used in the unrolled recurrent network. Each GBN layer in the unrolled network uses a different memory storage for NormStats."
    }, {
      "heading" : "4.3 Recurrent Streaming Normalization",
      "text" : "Extending Streaming Normalization to recurrent learning is straightforward – we not only stream through all the past samples, but also through all past timesteps. Thus, we maintain a unique set of running estimates of NormStats for all timesteps. This is more biologically-plausible and memory efficient than the above approach (RGBN).\nAgain, another way of viewing this algorithm is that the same Streaming Normalization layers described in Algorithm 5 and 6 are used in the original (instead of the unrolled) recurrent network. All unrolled versions of the same layer share running estimates of NormStats and other related data.\nOne caveat is that as time proceeds, the running estimates of NormStats are slightly modified. Thus when backpropagation reaches the same layer again, the NormStats are slightly different from the ones originally used for normalization. Empirically, it seems to not cause any problem on the performance. Training with “decoupled accumulation update” with Batches per Update (B/U) > 1 3 can also mitigate this problem, since it makes NormStats more stable over time."
    }, {
      "heading" : "5 Streaming Normalized RNN and GRU",
      "text" : "In our character-level language modeling task, we tried Normalized Recurrent Neural Network (RNN) and Normalized Gated Recurrent Unit (GRU) [Chung et al., 2014]. Let us use Norm(.) to denote a normalization, which can be either Sample Normalization, General Batch Normalization or Streaming Normalization. A bias and gain parameter is also learned for each neuron. We use NonLinear to denote a nonlinear function. We used hyperbolic tangent (tanh) nonlinearity in our experiments. But we observed ReLU also works. ht is the hidden activation at time t. xt is the network input at time t. W. denotes the weights. denotes elementwise multiplication.\nNormalized RNN ht = NonLinear(Norm(Wx ∗ xt) +Norm(Wh ∗ ht−1)) (3)\nNormalized GRU\ngr = Sigmoid(Norm(Wxr ∗ xt) +Norm(Whr ∗ ht−1))) (4) gz = Sigmoid(Norm(Wxz ∗ xt) +Norm(Whz ∗ ht−1))) (5)\nhnew = NonLinear(Norm(Wxh ∗ xt) +Norm(Whh ∗ (ht−1 gr))) (6) ht = gz hnew + (1− gz) ht−1 (7)\n3B/U=2 is often enough"
    }, {
      "heading" : "6 Related Work",
      "text" : "[Laurent et al., 2015] and [Amodei et al., 2015] used Batch Normalization (BN) in stacked recurrent networks, where BN was only applied to the feedforward part (i.e., “vertical” connections, input to each RNN), but not the recurrent part (i.e., “horizontal”, hidden-to-hidden connections between timesteps). [Liao and Poggio, 2016] and [Cooijmans et al., 2016] independently proposed applying BN in recurrent/hidden-to-hidden connections of recurrent networks, but separate normalization statistics must be maintained for each timestep. [Liao and Poggio, 2016] demonstrated this idea with deep multi-stage fully recurrent (and convolutional) neural networks with ReLU nonlinearities and residual connections. [Cooijmans et al., 2016] demonstrated this idea with LSTMs on language processing tasks and sequential MNIST. [Ba et al., 2016] proposed Layer Normalization (LN) as a simple normalization technique for online and recurrent learning. But they observed that LN does not work well with convolutional networks. [Salimans and Kingma, 2016] and [Neyshabur et al., 2015] studied normalization using weight reparameterizations. An early work by [Ullman and Schechtman, 1982] mathematically analyzed a form of online normalization for visual perception and adaptation."
    }, {
      "heading" : "7 Experiments",
      "text" : ""
    }, {
      "heading" : "7.1 CIFAR-10 architectures and Settings",
      "text" : "We evaluated the normalization techniques on CIFAR-10 dataset using feedforward fully-connected networks, feedforward convolutional network and a class of convolutional recurrent networks proposed by [Liao and Poggio, 2016]. The architectural details are shown in Figure 2. We train all models with learning rate 0.1 for 25 epochs and 0.01 for 5 epochs. Momentum 0.9 is used. We used MatConvNet [Vedaldi and Lenc, 2015] to implement our models.\n1x1x256\nFC-Norm-ReLU\nFC\n32x32x3\n1x1x256\nFC-Norm-ReLU\nInput\n1x1x10 Output\n1x1x256\nFC-Norm-ReLU\n(A) FC & FeedForward (B) Conv. & Feedforward (C) Conv. & Recurrent (D) Conv. & Densely Recurrent\n32x32x3\n32x32x32\nInput\n16x16x128\n1x1x128\nNorm-AvgPool\nFC\n1x1x10 Output\nNorm-ReLU-Conv\nConv\nNorm-ReLU-Conv * 2 + Idenity runs k2 times with shared weights\nNorm-ReLU-Conv * 2 + Idenity runs k1 times with shared weights\n32x32x3\n32x32x32\nInput\n16x16x128\n1x1x128\nNorm-AvgPool\nFC\n1x1x10 Output\nNorm-ReLU-Conv\nConv\nNorm-ReLU-Conv * 2\nNorm-ReLU-Conv * 2\n16x16x128\nIdentity\nIdentity\n32x32x32\n32x32x3\n32x32x64\nInput\n16x16x128\n1x1x128\nNorm-AvgPool\nFC\n1x1x10 Output\nConv\nNorm-ReLU-Conv * 2 + Idenity\nNorm-ReLU-Conv * 2 + Idenity\n8x8x256 Norm-ReLU-Conv * 2 + Idenity\nNorm-ReLU-Conv * 2Norm-ReLU-Deconv * 2\nNorm-ReLU-Conv * 2Norm-ReLU-Deconv * 2\nAll transitions within the box run k times concurrently with weights shared over time\nFigure 2: Architectures for CIFAR-10. Note that C reduces to B when k1 = k2 = 1."
    }, {
      "heading" : "7.2 Lp Normalization",
      "text" : "We show BN with Lp normalization in Figure 3. Note that Lp normalization can be applied to Layer Normalization and all other normalizations show in 1 C and D. L1 normalization works as well as L2 while being simpler to implement and faster to compute."
    }, {
      "heading" : "7.3 Online Learning or Learning with Very Small Mini-batches",
      "text" : "We perform online learning or learning with small mini-batches using architecture A in Figure 2.\nPlain Mini-batch vs. Decoupled Accumulation and Update (DAU): We show in Figure 4 comparisons between conventional mini-batch training and Decoupled Accumulation and Update (DAU).\nLayer Normalization vs. Batch Normalization vs. Streaming Normalization: We compare in Figure 5 Layer Normalization, Batch Normalization and Streaming Normalization with different choices of S/B and B/U."
    }, {
      "heading" : "7.4 Evaluating Variants of Batch Normalization",
      "text" : "Feedforward Convolutional Networks: In Figure 6, we tested algorithms shown in Figure 1 C and D using the architecture B in Figure 2. We also show the performance of our Streaming Normalization for reference.\nResNet-like convolutional RNN: In Figure 7, we tested algorithms shown in Figure 1 C and D using the architecture C in Figure 2. We also show the performance of our Streaming Normalization for reference.\nDensely Recurrent Convolutional Network: In Figure 8, we tested Time-Specific Batch Normalization and\nStreaming Normalization on the architecture D in Figure 2."
    }, {
      "heading" : "7.5 More Experiments on Streaming Normalization",
      "text" : "In Figure 9, we compare the performances of original BN (i.e., NormStats shared over time), time-specific BN, layer normalization and streaming normalization on a recurrent and convolutional network shown in Figure 2 C.\nWe evaluated different choices of hyperparameter β1,β2 and β3 in Figure 10."
    }, {
      "heading" : "7.6 Recurrent Neural Networks for Character-level Language Modeling",
      "text" : "We tried our simple implementations of vanilla RNN and GRU described in Section 5. The RNN and GRU both have 1 hidden layer with 100 units. Weights are updated using the simple Manhattan update rule described in [Liao et al., 2015]. The models were trained with learning rate 0.01 for 2 epochs and 0.001 for 1 epoch on a text file of all Shakespeare’s work concatenated. We use 99% the text file for training and 1% for validation. The training and validation softmax losses are reported. Training losses are from mini-batches so they are noisy, and we smoothed them using moving averages of 50 neighbors (using the Matlab smooth function). The test loss on the entire validation set is evaluated and recorded every 20 mini-batches. We show in Figure 11 and 12 the performances of Time-specific Batch Normalization, Layer Normalization and Streaming Normalization. Truncated BPTT was performed with 100 timesteps."
    }, {
      "heading" : "8 Discussion",
      "text" : "Biological Plausibility\nWe found that the simple “Neuron-wise normalization” (BA6 in Figure 1 D) performs very well (Figure 6 and 7). This setting does not require collecting normalization statistics from any other neurons. We show the streaming version of neuron-wise normalization in Figure A1, and the performance is again competitive. In neuron-wise normalization, each neuron simply maintains running estimates of its own mean and variance (and related gradients), and all the information is maintained locally. This approach may serve as a baseline model for biological homeostatic plasticity mechanisms (e.g., Synaptic Scaling) [Turrigiano and Nelson, 2004, Stellwagen and Malenka, 2006, Turrigiano, 2008], where each neuron internally maintains some normalization/scaling factors that depend on neuron’s firing history and can be applied and updated in a pure online fashion."
    }, {
      "heading" : "Lp Normalization",
      "text" : "Our observations about Lp normalization have several biological implications: First, we show that most Lp normalizations work similarly, which suggests that there might exist a large class of statistics that can be used for normalization. Biological systems could implement any of these methods to get the same level of performance. Second, L1 normalization is particularly interesting, since its gradient computations are much easier for biological neurons to implement.\nAs an orthogonal direction of research, it would also be interesting to study the relations between our Lp normalization (standardizing the average Lp norm of activations) and Lp regularization (discounting the the Lp norm of weights, e.g., L1 weight decay).\nTheoretical Understanding\nAlthough normalization methods have been empirically shown to significantly improve the performance of deep learning models, there is not enough theoretical understanding about them. Activation-normalized neurons behave more similarly to biological neurons whose activations are constrained into a certain range: is it a blessing or a curse? Does it affect approximation bounds of shallow and deep networks [Mhaskar et al., 2016, Mhaskar and Poggio, 2016]? It would also be interesting to see if certain normalization methods can mitigate the problems of poor local minima and saddle points, as the problems have been analysed without normalization [Kawaguchi, 2016].\nInternal Covariant Shift in Recurrent Networks\nNote that our approach (and perhaps the brain’s “synaptic scaling”) does not normalize differently for each timestep. Thus, it does not naturally handle internal covariant shift [Ioffe and Szegedy, 2015] (more precisely, covariate shift over time) in recurrent networks, which was the main motivation of the original Batch Normalization and Layer Normalization. Our results seem to suggest that internal covariate shift is not as hazardous as previously believed as long as the entire network’s activations are normalized to a good range. But more research is needed to answer this question."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work was supported by the Center for Brains, Minds and Machines (CBMM), funded by NSF STC award CCF – 1231216."
    }, {
      "heading" : "A Other Variants of Streaming Normalization",
      "text" : "0 5 10 15 20 25 30 Epoch\n5\n10\n15\n20\n25\n30\n35\n40\nTr ai\nni ng\nE rr\nor o\nn C\nIF A\nR -1\n0\nBA6-B BA6-S BA4-B BA4-S BA1-B BA1-B\n0 5 10 15 20 25 30 Epoch\n5\n10\n15\n20\n25\n30\n35\n40\nV al\nid at\nio n\nE rr\nor o\nn C\nIF A\nR -1\n0\nBA6-B BA6-S BA4-B BA4-S BA1-B BA1-B\nFigure A1: We explore other variants of Streaming Normalization with different NormRef (e.g., SP1-SP5, BA1-BA6 in 1 C and D) within each mini-batch. -B denotes the batch version. -S denotes the streaming version. The architecture is a feedforward and convolutional network (shown in Figure 2 B). Streaming significantly lowers training errors."
    } ],
    "references" : [ {
      "title" : "Deep speech 2: End-to-end speech recognition in english and mandarin",
      "author" : [ "Amodei et al", "D. 2015] Amodei", "R. Anubhai", "E. Battenberg", "C. Case", "J. Casper", "B. Catanzaro", "J. Chen", "M. Chrzanowski", "A. Coates", "G Diamos" ],
      "venue" : "arXiv preprint arXiv:1512.02595",
      "citeRegEx" : "al. et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2015
    }, {
      "title" : "Empirical evaluation of gated recurrent neural networks on sequence modeling",
      "author" : [ "Chung et al", "J. 2014] Chung", "C. Gulcehre", "K. Cho", "Y. Bengio" ],
      "venue" : "arXiv preprint arXiv:1412.3555",
      "citeRegEx" : "al. et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2014
    }, {
      "title" : "Recurrent batch normalization",
      "author" : [ "Cooijmans et al", "T. 2016] Cooijmans", "N. Ballas", "C. Laurent", "A. Courville" ],
      "venue" : "arXiv preprint arXiv:1603.09025",
      "citeRegEx" : "al. et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2016
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "He et al", "K. 2015] He", "X. Zhang", "S. Ren", "J. Sun" ],
      "venue" : "arXiv preprint arXiv:1512.03385",
      "citeRegEx" : "al. et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2015
    }, {
      "title" : "Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167",
      "author" : [ "Ioffe", "Szegedy", "S. 2015] Ioffe", "C. Szegedy" ],
      "venue" : null,
      "citeRegEx" : "Ioffe et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ioffe et al\\.",
      "year" : 2015
    }, {
      "title" : "How important is weight symmetry in backpropagation? arXiv preprint arXiv:1510.05067",
      "author" : [ "Liao et al", "Q. 2015] Liao", "J.Z. Leibo", "T. Poggio" ],
      "venue" : null,
      "citeRegEx" : "al. et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2015
    }, {
      "title" : "Bridging the gaps between residual learning, recurrent neural networks and visual cortex",
      "author" : [ "Liao", "Poggio", "Q. 2016] Liao", "T. Poggio" ],
      "venue" : "arXiv preprint arXiv:1604.03640",
      "citeRegEx" : "Liao et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Liao et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning real and boolean functions: When is deep better than shallow",
      "author" : [ "Mhaskar et al", "H. 2016] Mhaskar", "Q. Liao", "T. Poggio" ],
      "venue" : "arXiv preprint arXiv:1603.00988",
      "citeRegEx" : "al. et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2016
    }, {
      "title" : "Deep vs. shallow networks: An approximation theory perspective",
      "author" : [ "Mhaskar", "Poggio", "H. 2016] Mhaskar", "T. Poggio" ],
      "venue" : "arXiv preprint arXiv:1608.03287",
      "citeRegEx" : "Mhaskar et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Mhaskar et al\\.",
      "year" : 2016
    }, {
      "title" : "Path-sgd: Path-normalized optimization in deep neural networks",
      "author" : [ "Neyshabur et al", "B. 2015] Neyshabur", "R.R. Salakhutdinov", "N. Srebro" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "al. et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2015
    }, {
      "title" : "Weight normalization: A simple reparameterization to accelerate training of deep neural networks. arXiv preprint arXiv:1602.07868",
      "author" : [ "Salimans", "Kingma", "T. 2016] Salimans", "D.P. Kingma" ],
      "venue" : null,
      "citeRegEx" : "Salimans et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Salimans et al\\.",
      "year" : 2016
    }, {
      "title" : "Synaptic scaling mediated by glial tnf-α",
      "author" : [ "Stellwagen", "Malenka", "D. 2006] Stellwagen", "R.C. Malenka" ],
      "venue" : null,
      "citeRegEx" : "Stellwagen et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Stellwagen et al\\.",
      "year" : 2006
    }, {
      "title" : "Homeostatic plasticity in the developing nervous system",
      "author" : [ "Turrigiano", "Nelson", "G. 2004] Turrigiano", "S. Nelson" ],
      "venue" : "Nature Reviews Neuroscience,",
      "citeRegEx" : "Turrigiano et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Turrigiano et al\\.",
      "year" : 2004
    }, {
      "title" : "Adaptation and gain normalization",
      "author" : [ "Ullman", "Schechtman", "S. 1982] Ullman", "G. Schechtman" ],
      "venue" : "Proceedings of the Royal Society of London B: Biological Sciences,",
      "citeRegEx" : "Ullman et al\\.,? \\Q1982\\E",
      "shortCiteRegEx" : "Ullman et al\\.",
      "year" : 1982
    }, {
      "title" : "Matconvnet: Convolutional neural networks for matlab",
      "author" : [ "Vedaldi", "Lenc", "A. 2015] Vedaldi", "K. Lenc" ],
      "venue" : "In Proceedings of the 23rd Annual ACM Conference on Multimedia Conference,",
      "citeRegEx" : "Vedaldi et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Vedaldi et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ ],
    "year" : 2016,
    "abstractText" : "We systematically explored a spectrum of normalization algorithms related to Batch Normalization (BN) and propose a generalized formulation that simultaneously solves two major limitations of BN: (1) online learning and (2) recurrent learning. Our proposal is simpler and more biologically-plausible. Unlike previous approaches, our technique can be applied out of the box to all learning scenarios (e.g., online learning, batch learning, fully-connected, convolutional, feedforward, recurrent and mixed — recurrent and convolutional) and compare favorably with existing approaches. We also propose Lp Normalization for normalizing by different orders of statistical moments. In particular, L1 normalization is well-performing, simple to implement, fast to compute, more biologically-plausible and thus ideal for GPU or hardware implementations. This work was supported by the Center for Brains, Minds and Machines (CBMM), funded by NSF STC award CCF 1231216. 1 ar X iv :1 61 0. 06 16 0v 1 [ cs .L G ] 1 9 O ct 2 01 6 Approach FF & FC FF & Conv Rec & FC Rec & Conv Online Learning Small Batch All Combined Original Batch Normalization(BN) 3 3 7 7 7 Suboptimal 7 Time-specific BN 3 3 Limited Limited 7 Suboptimal 7 Layer Normalization 3 7* 3 7* 3 3 7* Streaming Normalization 3 3 3 3 3 3 3 Table 1: An overview of normalization techiques for different tasks. 3: works well. 7: does not work well. FF: Feedforward. Rec: Recurrent. FC: Fully-connected. Conv: convolutional. Limited: time-specific BN requires recording normalization statistics for each timestep and thus may not generalize to novel sequence length. *Layer normalization does not fail on these tasks but perform significantly worse than the best approaches.",
    "creator" : "LaTeX with hyperref package"
  }
}