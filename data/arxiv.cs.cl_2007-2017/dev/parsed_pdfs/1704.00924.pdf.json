{
  "name" : "1704.00924.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Japanese Sentiment Classification using a Tree-Structured Long Short-Term Memory with Attention",
    "authors" : [ "Ryosuke Miyazaki", "Mamoru Komachi" ],
    "emails" : [ "miyazaki-ryosuke@ed.tmu.ac.jp", "komachi@tmu.ac.jp" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Traditional approaches for sentiment classification rely on simple lexical features, such as bag-ofwords, that are not effective for many sentiment classification tasks (Pang et al., 2002). For example, the sentence “Insecticides kill pests.” contains both kill and pests, indicating negative polarity, but the overall expression is still deemed positive.\nTo address this problem of polarity shift, Nakagawa et al. (2010) presented a dependency treebased approach for the sentiment classification of a sentence. Their method assigns sentiment polarity to each subtree as a hidden variable that is not observable in the training data. The polarity of the overall sentence is then classified by TreeConditional Random Field (Tree-CRF) marginalizing over the hidden variables representing the polarities of the respective subtrees. In this manner, the model can handle polarity shifting operations such as negation. However, this method suffers from feature sparseness because almost all features are combination features.\nTo overcome the data sparseness problem, deep neural network based methods have attracted much attention because of their ability to use\ndense feature representations (Socher et al., 2011, 2013; Kim, 2014; Kalchbrenner et al., 2014; Tai et al., 2015; Zhang and Komachi, 2015). In particular, tree-structured approaches called recursive neural networks (RNN) have been shown to achieve good performance in sentiment classification tasks (Socher et al., 2011, 2013; Kim, 2014; Tai et al., 2015). While Tree-CRF employs sparse and binary feature representations, RNN avoids feature sparseness by learning dense and continuous feature representations. However, annotation for each phrase is crucial for learning RNN models. However, there is no readily available phraselevel polarity annotated corpus in any language other than English, which prevents tree-structured neural models from being easily ported to other languages.\nWe therefore propose an RNN model with an attention mechanism to compensate for the lack of phrase-level annotation. We also augment the training example with polar dictionaries. Although Kokkinos and Potamianos (2017) also provide an attention mechanism for phraselevel annotated corpus, our model performs well on sentence-level annotated corpus through the introduction of the (1) attention mechanism and (2) polar dictionary.\nThe main contributions of this work are as follows:\n• We show that RNN models can be learned from a sentence-level polarity-tagged corpus through the use of an attention mechanism.\n• We propose augmentation of the polaritytagged corpus through the use of polar dictionaries. RNN models can effectively take advantage of such additional resources.\n• We achieve the state-of-the-art performance in a Japanese sentiment classification task.\nar X\niv :1\n70 4.\n00 92\n4v 1\n[ cs\n.C L\n] 4\nA pr\n2 01\n7"
    }, {
      "heading" : "2 Attentional Tree-LSTM",
      "text" : ""
    }, {
      "heading" : "2.1 Tree-Structured Long Short-Term Memory (LSTM)",
      "text" : "Various RNN models for handling sentence representation considering syntactic structure have been studied (Socher et al., 2011, 2012, 2013; Qian et al., 2015; Tai et al., 2015; Zhu and Sobhani, 2015). RNN construct a sentence representation from their phrase representations by applying a composition function. Phrase representations can be calculated by recursively adopting composition functions. These RNN models are essentially identical to recurrent neural models in that they are not able to retain a long history.\nTai et al. (2015) addressed this problem by introducing LSTM (Hochreiter and Schmidhuber, 1997) to make RNN less prone to the exploding/vanishing gradient problem. In this paper, we use the Binary Tree-LSTM proposed by Tai et al. (2015) as an example of a tree-structured LSTM."
    }, {
      "heading" : "2.2 Softmax Classifier with Attention",
      "text" : "Owing to the lack of phrase-level annotation, sentence representation may be inaccurate because it may fail to propagate errors from the root of the tree to the terminals and preterminals in a long sentence. We propose an attention mechanism to address this problem. This so-called classifier with attention takes an attention vector representation aj in addition to a hidden representation hj as inputs: p̂θ(y | hj) = softmax ( W (s ′) [ aj hj ] + b(a) ) , (1)\naj = ∑ i aji hi, (2)\naji = g(hi, hj)∑ i′ g(hi′ , hj) , (3)\ng(hi, hj) = exp ( W (a2) tanh ( W (a1) [ hi hj ])) ,\n(4)\nwhere W (s ′) ∈ Rdl×2d,W (a1) ∈ Rda×d, and W (a2) ∈ R1×da are the parameter matrices. In Eq.4, the biases for both W (a1) and W (a2) are omitted for simplicity. The attention vector aj represents how much the classifier pays attention to the children nodes of the target node. The scalar values aji for each node are used to determine the attention vector. Figure 1 represents the softmax classifier with attention."
    }, {
      "heading" : "2.3 Distant Supervision with Polar Dictionaries",
      "text" : "Unlike the Stanford Sentiment Treebank, which is annotated with phrase-level polarity, other multilingual datasets contain only sentence-level annotation. As shown in Section 3, sentiment classification without a phrase-level annotated corpus will not learn sentence representations appropriately. However, although a phrase-level polaritytagged corpus is difficult to obtain in many languages, polar dictionaries are easy to compile (semi-)automatically. Therefore, we opt for the use of polar dictionaries as an alternative source of sentiment information.\nWe utilize the same polar dictionaries for short phrases and words as used in Nakagawa et al. (2010). The phrase in the training sets that matches an entry in the polar dictionaries is annotated with corresponding polarity. The key difference from Nakagawa et al. (2010) is that we use polar dictionaries as a hard label in a manner similar to distant supervision (Mintz et al., 2009); in contrast, in the previous work, it was used as a soft label for an initial hidden variable in TreeCRF. Teng et al. (2016) also incorporated sentiment lexicons into an recurrent neural network model. Their method predicts weights for each sentiment score of subjective words to predict a sentence label. Our method uses polar dictionaries only during the training step, while the method by Teng et al. (2016) needs polar dictionaries for both training and decoding."
    }, {
      "heading" : "2.4 Learning",
      "text" : "The cost function is a cross-entropy error function between the true class label distribution, t,(i.e.,\none hot distribution for the correct label) and the predicted label distribution, ŷ, at each labeled node:\nJ(θ) = − m∑ k=1 tk log ŷk + λ 2 ‖θ‖22, (5)\nwhere m is the number of labeled nodes in the training set1, and λ denotes an L2 regularization hyperparameter."
    }, {
      "heading" : "3 Experiments",
      "text" : "We conducted sentiment classification on a Japanese corpus."
    }, {
      "heading" : "3.1 Data",
      "text" : "We obtained pretrained word representations from word2vec2 using skip-gram model (Mikolov et al., 2013a,b,c). We learned 200 dimensional word representations on Japanese Wikipedia’s dump data (2014.11) segmented by KyTea3 (Neubig et al., 2011). For constituency parsing, we used Ckylark (Oda et al., 2015) as of 2016.07 with KyTea for word segmentation.\nWe employed a Japanese polar dictionary composed by Kobayashi et al. (2005) and Higashiyama et al. (2008)4 that contains 5,447 positive and 8,117 negative expressions. We used the NTCIR Japanese opinion corpus (NTCIR-J), which includes 997 positive and 2,400 negative sentences (Seki et al., 2007, 2008). The corpus comprised two NTCIR Japanese opinion corpora, the NTCIR-6 corpus and the NTCIR-7 corpus, as in (Nakagawa et al., 2010). We performed 10-fold cross-validation by randomly splitting each corpus into 10 parts (one for testing, one for development, and the remaining for training)."
    }, {
      "heading" : "3.2 Methods",
      "text" : "We compared our method to four baselines. All input word vectors other than those for MFS and Tree-CRF were pretrained by word2vec. We implemented our method, LogRes, and Tree-LSTM using Chainer (Tokui et al., 2015).\nThe following methods were used: 1If the data set contains only sentence-level annotation, m\nequals to the size of the data set. 2https://code.google.com/archive/p/ word2vec/ 3KyTea version-0.4.7. 4http://www.cl.ecei.tohoku. ac.jp/index.php?OpenResources/ JapaneseSentimentPolarityDictionary\nMFS. A naı̈ve baseline as it always selects the most frequent polarity (which is negative in this case).\nLogRes. A linear classifier using logistic regression. The input features are an average of word vectors in a sentence.\nTree-CRF. A dependency-based tree-structured CRF (Nakagawa et al., 2010). It is the state-ofthe-art method among our experimental datasets.\nTree-LSTM. The LSTM-based recursive neural network (Tai et al., 2015).\nTree-LSTM w/attn, dict. Our proposed method, which classifies polarity using attention and/or polar dictionaries."
    }, {
      "heading" : "3.3 Hyperparameters",
      "text" : "We tuned hyperparameters on each development set of 10-fold cross-validation. The best parameters are shown in Table 1."
    }, {
      "heading" : "4 Results",
      "text" : "The experimental results are shown in Table 2. The accuracy of RNN is much lower than that of the MFS baseline; moreover, Tree-LSTM, which is an improved RNN, is still lower than simple LogRes despite Tree-LSTM achieving state-of-theart performance on the phrase-annotated Stanford Sentiment Treebank (Tai et al., 2015). In contrast, Tree-LSTM with attention achieves compa-\nrable results to Tree-CRF. Our Tree-LSTM with attention and polar dictionary obtained the best accuracy.\nKokkinos and Potamianos (2017) also investigate attentional model for RNN. Their model only feeds attention vector into the softmax classifier, whereas our method inputs both attention vector and RNN vector, as illustrated in Figure 1. The accuracy of the model inputting only attention vector is 0.807, which is slightly lower than that of our proposed method."
    }, {
      "heading" : "5 Discussion",
      "text" : "The results described above indicate that TreeLSTM models without attention mechanism fail to learn sentence representations if phrase-level annotation is not available.\nHowever, Tree-LSTM models can learn more accurate sentence representations if the models receive phrase-level information such as that provided by polar dictionaries. For example, in our model, attention information and polar dictionary are fed into the Tree-LSTM as phrase-level information. Although Tree-LSTM with attention and a polar dictionary outperforms Tree-CRF by 1.8 points, accuracy of Tree-LSTM without a polar dictionary is lower than that of Tree-CRF. TreeLSTM with a polar dictionary performs better than Tree-LSTM with attention, showing that supervised label for each phrase seems to be important in learning Tree-LSTM models. Note that although our training size is 10 points lower than that in Nakagawa et al. (2010) because they used\n90% of the corpus for training, our method outperforms their method in terms of accuracy.\nFigures 2a shows correctly classified example. Figure 2a shows that the model classifies “ (consistency)” as positive and pays 1/3 attention to it in the final classification step; however, the model correctly classifies the sentence polarity as negative by considering “ł (cannot be found)” through most of the attention.\nFigures 2b displays an incorrectly classified example. In Figure 2b, the model pays attention to both “ (confrontation)” and “ (mitigated)”; however, it fails to predict the correct polarity of the sentence. It seems that the higher attention weight for “ (confrontation)” than for “ (mitigated)” influences the sentence prediction. To solve these errors, the composition function should also incorporate attention mechanism to handle polarity shifting correctly."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We presented a Tree-LSTM based recursive neural network using an attention mechanism and a polar dictionary. In this method, each phrase representation is fed into a classifier to predict the polarity of a phrase based on the phrase structures. Lexical items from the polar dictionary are used as supervised labels for each corresponding phrase or word in the same manner as distant supervision. Our experimental results demonstrated that the proposed method outperforms the previous methods."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "Previous approaches to training syntax-<lb>based sentiment classification models re-<lb>quired phrase-level annotated corpora,<lb>which are not readily available in many<lb>languages other than English. Thus, we<lb>propose the use of tree-structured Long<lb>Short-Term Memory with an attention<lb>mechanism that pays attention to each sub-<lb>tree of the parse tree. Experimental results<lb>indicate that our model achieves the state-<lb>of-the-art performance in a Japanese sen-<lb>timent classification task.",
    "creator" : "TeX"
  }
}