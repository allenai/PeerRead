{
  "name" : "1602.04874.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Bi-directional LSTM Recurrent Neural Network for Chinese Word Segmentation",
    "authors" : [ "Yushi Yao", "Zheng Huang" ],
    "emails" : [ "yys12345@sjtu.edu.cn", "huangzheng@sjtu.edu.cn" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "With the rapid development of deep learning, neural networks start to show its great capability in NLP tasks[Auli et al., 2013] and recent research revealed that recurrent neural networks(RNN) significantly outperforms popular statistical algorithms like Hidden Markov Model(HMM)[Zhang el al., 2003], CRF(conditional random field)[Peng et al., 2004] and neural probabilistic models[Bengio et al., 2003].\nAs a special kind of RNN, LSTM neural networks[Hochreiter et al., 1997] is verified to be efficient in modeling sequential data like speech and text [Sundermeyer et al., 2015]. More over, BLSTM neural network[Schuster et al., 1997], which is derived from LSTM network, has advantages in memorizing information for long periods\nin both directions, making great improvement in linguistic computation. Sundermeyer et al. analyzed LSTM neural network by modeling English and French[Sundermeyer et al., 2012]. Wang et al. used bi-directional LSTM into POS tagging, chunking and NER tasks and internal representations are learnt from unlabeled text for all tasks[Wang et al., 2015]. Huang et al. combined LSTM with CRF and verified the efficiency and robustness of their model in sequential tagging[Huang et al., 2015]. Ling et al focus on constructing compact vector representations of words with bi-directional LSTM, which yield state-of-the-art performance in contrast to other word-to-vector algorithms like CBOW and skip-n-gram[Ling et al., 2015]. The study that is close to ours is Chen et al, which introduced LSTM neural network into Chinese word segmentation[Chen et al., 2015], while LSTM can just memorize the past contextual information from the context. Due to the complicated and changeable structure of Chinese sentence, it’s intuitive that both future and past information need to be considered when training segmentation model.\nIn this study, in order to improve the performance of the Chinese word segmentation, we applied bi-directional LSTM networks to word segmentation task, we also constructed higher-level features of characters with BLSTM network and our contributions can be listed as follows:\n1) Our work is the first to apply and improve bidirectional LSTM network to Chinese word segmentation benchmark data sets\n2) The training framework can be regarded as an integration of generating embeddings and tagging characters and it does not need any external datasets\nThe paper is organized as follows. Section 2 describes basic idea and architecture of BLSTM network. Next, we introduce our training framework based on BLSTM network in Section 3. Section\nar X\niv :1\n60 2.\n04 87\n4v 1\n[ cs\n.L G\n] 1\n6 Fe\nb 20\n16\n4 details our experiments on Chinese dataset and summarizes our experimental results with previous research. Finally, in Section 5 we summarize key conclusions."
    }, {
      "heading" : "2 BLSTM network architecture",
      "text" : "BLSTM neural network is similar to LSTM network in structure because both of them are constructed with LSTM units[Schuster et al., 1997]. The special unit of this network is capable of learning long-term dependencies without keeping redundant context information. They work tremendously well on sequential modeling problems, and are now widely used in NLP tasks."
    }, {
      "heading" : "2.1 LSTM unit",
      "text" : "The basic structure of LSTM memory unit is composed of three essential gates and a cell state.\nAs shown in Figure 1, the memory cell contains the information it memorized at time t, the state of the memory cell is bound up together with three gates, the input vector of each gate is composed of input part and recurrent part. Forget gate controls what to abandon from the last moment, input gate decides what new information will be stored in the cell state, the output gate decides which part of the cell state will be output and the recurrent part will be updated by current cell state and fed into next iteration[Hochreiter et al., 1997].\nThe formal formulas for updating each gate and\ncell state are defined as follows:\nzt = g(Wzx t +Rzy t−1 + bz) (1)\nit = σ(Wix t +Riy t−1 + pi ct−1 + bt) (2) f t = σ(Wfx t +Rfy t−1 + pf ct−1 + bf ) (3) ct = it zt + f t ct−1 (4) ot = σ(Wox t +Roy t−1 + po ct + bo) (5) yt = ot h(ct) (6)\nHere xt ∈ Rd and yt ∈ Rd are input and output vector of the unit at timet, Wk(k = z, i, f, o) and Rk(k = z, i, f, o) are weight matrices for input part and recurrent part of different gates, bk(k = z, i, f, o) denotes bias vector and the functions σ, g and h are non-linear functions such as sigmoid or tanh, means point-wise calculation of two vectors. For completeness, we add pk(k = i, o, f) to the formulas, which denote peephole connection and is mostly used in LSTM variants."
    }, {
      "heading" : "2.2 BLSTM Network",
      "text" : "BLSTM network is designed to capture information of sequential dataset and maintain contextual features from past and future. Different from LSTM network, BLSTM network has two parallel layers propagating in two directions, the forward and backward pass of each layer are carried out in similar way of regular neural networks, these two layers memorize the information of sentences from both directions.[Schuster et al., 1997]\nSince there are two LSTM layers in our network, the vector formula should be also adjusted.\nhf t = H(Wxhfxt +Whfhfhf t−1 + bhf ) (7)\nhbt = H(Wxhbxt +Whbhbhbt−1 + bhb) (8)\nhf ∈ Rd and hb ∈ Rd denotes the output vector of forward layer and backward layer respectively, different from former research, the final output in our work yt = [hft , hbt ] is the concatenation of these two parts, which means yt ∈ R2d. We define the combination of forward and backward layers as a single BLSTM layer."
    }, {
      "heading" : "3 Training Method",
      "text" : "In order to convert the segmentation problem into a tagging problem, we assign a label for each character to indicate the segmentation. There are four kinds of labels: B, M, E, S, corresponding to the beginning, middle, end of a word, and a singlecharacter word, respectively."
    }, {
      "heading" : "3.1 Training framework",
      "text" : "The basic procedure of language modeling in our study is shown in Figure 2.\nEach character has an id which is defined in a lookup dictionary, the dictionary is constructed by collecting unique characters in the training set. Instead of one-hot representation, the characters are projected into a d-dimension space and initialized as dense vectors v ∈ Rd, we regard this initialization step as constructing embeddings for characters. Every embedding is stored in a matrix M ∈ Rd×|C| and can be retrieved by its character id. As embeddings are efficient in describing word-level features[Miklov et al., 2013], we hope character-level embeddings can also achieve good performance in CWS. Then he embeddings\nare fed into BLSTM network and the final output of BLSTM network is finally passed to a hidden layer and the softmax layer determines the tag with maximum probability of the character."
    }, {
      "heading" : "3.2 Model variant",
      "text" : "In order to further improve the structure of BLSTM network, we stack BLSTM layers based on the method of constructing RNN[Pascanu et al., 2013]. expecting to extract contextual features in higher level.\nHowever, the output of a BLSTM layer is in double size of the input vector since it is composed of two LSTM layers, and its dimension will expand dramatically when the network goes deeper, here we use a transformation matrix to compress the dimension of output vectors, and keep it the\nsame size with input vectors.\nvtran =Wtran × vo (9)\nAssume that the output vector of BLSTM layer vo ∈ R2d, the transformation matrix Wtran ∈ Rd×2d convert the vectors into lower dimension and thus the output of each BLSTM layer is kept the same dimension. As our BLSTM network gets more and more complicated, the number of parameters grows rapidly and we used dropout during training in order to avoid overfitting[Srivastava et al., 2014]."
    }, {
      "heading" : "4 Experiments",
      "text" : "Setup The dataset we used for evaluating our model on word segmentation is from Backoff 2005, which contains benchmark datasets for both simplified Chinese(PKU and MSRA) and traditional Chinese(AS and HK). All the models are trained on NVIDIA GTX Geforce 970, it took about 16 to 17 hours to train a model on GPU while more than 4 days to train on CPU, in contrast. We also changed batch size during our training process because of the limit of GPU memory. Results In this section, we will state the procedure of our experiments and how we get the model with the best performance, and we will also compare the performance of our network with state-of-theart approaches.\nSince the dimension of embedding vector will directly influence the number of parameters and model complexity, we conducted related experiments on HKCityU dataset, and try to get a suitable size of embedding first.\nTable 2 illustrate the performance of our model on HKCityU dataset with embedding vectors of different dimensions. When the dimension grows higher, the error rate becomes bigger and unstable. We can conclude that the model gets the best performance when the embedding dimension is 200, which also indicates that an efficient representation of Chinese words should not be too long.\nSecondly, we tested the performance of our model with different number of BLSTM layers.\nTable 1 shows that as we stack more BLSTM layers, the performance gets slight improvement, while adding layers becomes not so effective when the number of BLSTM layers exceeds three, which also takes quite long time to train. The result shows that LSTM units become less effective in higher level layers so we believe that there is\nno need to build very deep network for extracting contextual information.\nTable 3 lists the performances of our model as well as previous research. (Zhao et al., 2006) is a CRF model with rich feature template, (Sun and Xu, 2011) improved supervised word segmentation by exploiting features of unlabeled data and the system of (Zhang et al., 2013) applied semi-supervised approach to extract representations of label distributions from unlabeled and labeled datasets[Zhang et al., 2013]. Nevertheless, all the models or systems above focus on feature engineering, while our approach do not depend on any predesigned features thanks to the strong ability of BLSTM network in automatic feature learning.\nOur model achieved competitive performance compared to the work of Chen’s, which also used character embeddings but applied LSTM network\nto CWS, and the results suggest that BLSTM may get better performance than LSTM on segmentation, and indicates that both past and future information should be taken into account for segmentation task."
    }, {
      "heading" : "5 Conclusions",
      "text" : "In this paper, we propose to use bi-direction LSTM neural network to train the model for Chinese word segmentation, BLSTM network is quite efficient for sequential tagging task. The model learn to extract discriminative character-level features automatically and it do not require any handcraft features for segmentation or prior knowledge. Experiments conducted on SIGHAN Backoff 2005 datasets show that our model has good performance and generalization on both simplified Chinese and traditional Chinese. Our results suggest that deep neural networks work well on segmentation tasks and BLSTM networks with word embedding is an effective tagging solution and worth further exploration."
    } ],
    "references" : [ {
      "title" : "Joint language and translation modeling with recurrent neural networks",
      "author" : [ "Michael Auli", "Michel Galley", "Chris Quirk", "Geoffrey Zweig" ],
      "venue" : "In EMNLP,",
      "citeRegEx" : "Auli et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Auli et al\\.",
      "year" : 2013
    }, {
      "title" : "A neural probabilistic language model",
      "author" : [ "Yoshua Bengio", "Réjean Ducharme", "Pascal Vincent", "Christian Janvin" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Bengio et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2003
    }, {
      "title" : "Long short-term memory neural networks for chinese word segmentation",
      "author" : [ "Xinchi Chen", "Xipeng Qiu", "Chenxi Zhu", "Pengfei Liu", "Xuanjing Huang" ],
      "venue" : "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Chen et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2015
    }, {
      "title" : "Lstm: A search space odyssey",
      "author" : [ "Klaus Greff", "Rupesh Kumar Srivastava", "Jan Koutnı́k", "Bas R Steunebrink", "Jürgen Schmidhuber" ],
      "venue" : "arXiv preprint arXiv:1503.04069,",
      "citeRegEx" : "Greff et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Greff et al\\.",
      "year" : 2015
    }, {
      "title" : "Long shortterm memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? \\Q1997\\E",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Chinese word segmentation: A decade review",
      "author" : [ "Changning Huang", "Hai Zhao" ],
      "venue" : "Journal of Chinese Information Processing,",
      "citeRegEx" : "Huang and Zhao.,? \\Q2007\\E",
      "shortCiteRegEx" : "Huang and Zhao.",
      "year" : 2007
    }, {
      "title" : "Bidirectional lstm-crf models for sequence tagging",
      "author" : [ "Zhiheng Huang", "Wei Xu", "Kai Yu" ],
      "venue" : "arXiv preprint arXiv:1508.01991,",
      "citeRegEx" : "Huang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2015
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Mikolov et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "How to construct deep recurrent neural networks",
      "author" : [ "Razvan Pascanu", "Caglar Gulcehre", "Kyunghyun Cho", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1312.6026,",
      "citeRegEx" : "Pascanu et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Pascanu et al\\.",
      "year" : 2013
    }, {
      "title" : "Bidirectional recurrent neural networks",
      "author" : [ "Mike Schuster", "Kuldip K Paliwal" ],
      "venue" : "Signal Processing, IEEE Transactions on,",
      "citeRegEx" : "Schuster and Paliwal.,? \\Q1997\\E",
      "shortCiteRegEx" : "Schuster and Paliwal.",
      "year" : 1997
    }, {
      "title" : "Very deep convolutional networks for large-scale image recognition",
      "author" : [ "Karen Simonyan", "Andrew Zisserman" ],
      "venue" : "arXiv preprint arXiv:1409.1556,",
      "citeRegEx" : "Simonyan and Zisserman.,? \\Q2014\\E",
      "shortCiteRegEx" : "Simonyan and Zisserman.",
      "year" : 2014
    }, {
      "title" : "Dropout: A simple way to prevent neural networks from overfitting",
      "author" : [ "Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Srivastava et al\\.,? \\Q1929\\E",
      "shortCiteRegEx" : "Srivastava et al\\.",
      "year" : 1929
    }, {
      "title" : "From feedforward to recurrent lstm neural networks for language modeling",
      "author" : [ "Martin Sundermeyer", "Hermann Ney", "Ralf Schluter" ],
      "venue" : "Audio, Speech, and Language Processing, IEEE/ACM Transactions on,",
      "citeRegEx" : "Sundermeyer et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Sundermeyer et al\\.",
      "year" : 2015
    }, {
      "title" : "Lstm neural networks for language modeling",
      "author" : [ "Martin Sundermeyer", "Ralf Schlüter", "Hermann Ney" ],
      "venue" : "In INTERSPEECH,",
      "citeRegEx" : "Sundermeyer et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Sundermeyer et al\\.",
      "year" : 2012
    }, {
      "title" : "A unified tagging solution: Bidirectional lstm recurrent neural network with word embedding",
      "author" : [ "Peilu Wang", "Yao Qian", "Frank K Soong", "Lei He", "Hai Zhao" ],
      "venue" : "arXiv preprint arXiv:1511.00215,",
      "citeRegEx" : "Wang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2015
    }, {
      "title" : "Exploring representations from unlabeled data with co-training for chinese word segmentation",
      "author" : [ "Longkai Zhang", "Wang Houfeng", "Sun Xu", "Mairgup Mansur" ],
      "venue" : "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Zhang et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2013
    }, {
      "title" : "An improved chinese word segmentation system with conditional random field",
      "author" : [ "Hai Zhao", "Chang-Ning Huang", "Mu Li" ],
      "venue" : "In Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing,",
      "citeRegEx" : "Zhao et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2006
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "With the rapid development of deep learning, neural networks start to show its great capability in NLP tasks[Auli et al., 2013] and recent research revealed that recurrent neural networks(RNN) significantly outperforms popular statistical algorithms like Hidden Markov Model(HMM)[Zhang el al.",
      "startOffset" : 108,
      "endOffset" : 127
    }, {
      "referenceID" : 1,
      "context" : ", 2004] and neural probabilistic models[Bengio et al., 2003].",
      "startOffset" : 39,
      "endOffset" : 60
    }, {
      "referenceID" : 12,
      "context" : ", 1997] is verified to be efficient in modeling sequential data like speech and text [Sundermeyer et al., 2015].",
      "startOffset" : 85,
      "endOffset" : 111
    }, {
      "referenceID" : 13,
      "context" : "analyzed LSTM neural network by modeling English and French[Sundermeyer et al., 2012].",
      "startOffset" : 59,
      "endOffset" : 85
    }, {
      "referenceID" : 14,
      "context" : "resentations are learnt from unlabeled text for all tasks[Wang et al., 2015].",
      "startOffset" : 57,
      "endOffset" : 76
    }, {
      "referenceID" : 6,
      "context" : "combined LSTM with CRF and verified the efficiency and robustness of their model in sequential tagging[Huang et al., 2015].",
      "startOffset" : 102,
      "endOffset" : 122
    }, {
      "referenceID" : 2,
      "context" : "is close to ours is Chen et al, which introduced LSTM neural network into Chinese word segmentation[Chen et al., 2015], while LSTM can just memorize the past contextual information from the context.",
      "startOffset" : 99,
      "endOffset" : 118
    }, {
      "referenceID" : 8,
      "context" : "In order to further improve the structure of BLSTM network, we stack BLSTM layers based on the method of constructing RNN[Pascanu et al., 2013].",
      "startOffset" : 121,
      "endOffset" : 143
    }, {
      "referenceID" : 16,
      "context" : "Models PKU MSRA CityU (Zhao et al., 2006) - - 97.",
      "startOffset" : 22,
      "endOffset" : 41
    }, {
      "referenceID" : 15,
      "context" : "2 (Zhang et al., 2013) 96.",
      "startOffset" : 2,
      "endOffset" : 22
    }, {
      "referenceID" : 2,
      "context" : "4 (Chen et al., 2015) 96.",
      "startOffset" : 2,
      "endOffset" : 21
    }, {
      "referenceID" : 16,
      "context" : "(Zhao et al., 2006) is a CRF model with rich feature template, (Sun and Xu, 2011) improved supervised word segmentation by exploiting features of unlabeled data and the system of (Zhang et al.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 15,
      "context" : ", 2006) is a CRF model with rich feature template, (Sun and Xu, 2011) improved supervised word segmentation by exploiting features of unlabeled data and the system of (Zhang et al., 2013) applied semi-supervised approach to extract representations of label distributions from unlabeled and labeled datasets[Zhang et al.",
      "startOffset" : 167,
      "endOffset" : 187
    }, {
      "referenceID" : 15,
      "context" : ", 2013) applied semi-supervised approach to extract representations of label distributions from unlabeled and labeled datasets[Zhang et al., 2013].",
      "startOffset" : 126,
      "endOffset" : 146
    } ],
    "year" : 2016,
    "abstractText" : "Recurrent neural network(RNN) has been broadly applied to natural language processing(NLP) problems. This kind of neural network is designed for modeling sequential data and has been testified to be quite efficient in sequential tagging tasks. In this paper, we propose to use bi-directional RNN with long short-term memory(LSTM) units for Chinese word segmentation, which is a crucial preprocess task for modeling Chinese sentences and articles. Classical methods focus on designing and combining hand-craft features from context, whereas bi-directional LSTM network(BLSTM) does not need any prior knowledge or pre-designing, and it is expert in keeping the contextual information in both directions. Experiment result shows that our approach gets stateof-the-art performance in word segmentation on both traditional Chinese datasets and simplified Chinese datasets.",
    "creator" : "TeX"
  }
}