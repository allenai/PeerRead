{
  "name" : "1406.1143.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Identifying Duplicate and Contradictory Information in Wikipedia",
    "authors" : [ "Sarah Weissman", "Samet Ayhan", "Joshua Bradley", "Jimmy Lin" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "Introduction",
      "text" : "Readers of Wikipedia often notice that multiple articles contain highly-similar or even identical passages. In some cases these represent duplicate articles marked for merging, but content overlap arises in other cases as well. For example, the article about a hurricane and the article about the location where it made landfall might share the same content about the impact of the natural disaster. Identical content is most likely the result of copy and paste between articles, but interestingly, readers occasionally come across highlysimilar content that state contradictory facts. In a distributed environment where anyone can edit content, these observations are perhaps not surprising, and in this paper we attempt to rigorously characterize these phenomena by treating the problem as that of near-duplicate sentence detection. We adapt standard locality-sensitive hashing (LSH) techniques to identify clusters of near-duplicate sentences in Wikipedia.\nWe believe this problem is interesting in a few ways: For duplicate sentences, our analyses quantify the extent to which Wikipedia content is simply replicated, as opposed to written from scratch. In the case of near duplicates, some differences represent minor copyediting that does not change the substance of the content, but in other cases the differences represent contradictory facts. Quantifying these cases provides an indirect measure of the quality of Wikipedia in terms of self consistency.\nThis work makes no claims about the novelty of our techniques nor our implementation in MapReduce. Rather, our contribution lies in the analysis and categorization of nearduplicate sentence types. We have not seen locality-sensitive hashing applied to Wikipedia in this way before."
    }, {
      "heading" : "Related Work",
      "text" : "The problem we tackle in this paper is related to a few other problems that have been studied before. Near-duplicate detection of web pages (Henzinger 2006) is important in search because web pages are often copied or mirrored with only minor differences (e.g., ads or navigation bars); it would be desirable to return only the “canonical” versions in search results. In fact, the algorithm that we use in this paper, minhash (Broder 1997), was originally developed for exactly this purpose. Another closely-related problem is plagiarism detection (Si, Leong, and Lau 1997), or more generally, “text reuse” (Bendersky and Croft 2009). In contrast to nearduplicate detection, the focus is usually on smaller segments of text as opposed to entire documents. However, similar approaches such as shingling are applicable to both problems.\nOther similar formulations of the problem are what the data mining community calls pairwise similarity search or “all pairs” search (Bayardo, Ma, and Srikant 2007) and what the database community calls set similarity join (Vernica, Carey, and Li 2010). The task is the same: given a potentially large collection of objects, identify all pairs whose similarity is above a threshold according to some similarity metric.\nThere are two classes of solutions to the above problems: in the index-based approach, an inverted index is constructed from objects in the collection and a traversal of the index allows the similar pairs to be extracted, e.g., (Bayardo, Ma, and Srikant 2007; Lin 2009); with the hash-based approach, the basic idea is to use locality-sensitive hashing (LSH) to identify similar pairs based on hash collisions, e.g., minhash (Broder 1997). Of course, hybrid solutions are also possible. Scaling up these solutions has been accomplished by MapReduce (Lin 2009; Vernica, Carey, and Li 2010). Similarly, our approach takes advantage of minhash using a MapReduce implementation in Hadoop.\nBecause of its open nature, Wikipedia has generated much controversy over its editorial quality and factual correctness. An early study found Wikipedia’s accuracy to rival that of traditional encyclopedias (Giles 2005), but subsequent investigations have arrived at conflicting conclusions. A thorough review is beyond the scope of this short paper, but somewhat ironically, the best summary of this ongoing debate is a Wikipedia article.1 Since Wikipedia may\n1 http://en.wikipedia.org/wiki/Reliability of Wikipedia\nbe edited anonymously, information may be freely copied between web sources and even between Wikipedia articles without verification—however, this is not to say that there are no quality assurance mechanisms in Wikipedia (Stvilia et al. 2008). Although there are active communities of editors who contribute to the upkeep of various articles, much of Wikipedia is edited and expanded in an ad hoc manner. In particular, Wilkinson and Huberman (2007) found the distribution of article edits on Wikipedia to have a long tail, meaning that a small number of articles account for most of the edits, and that the number of edits is related to article quality. Articles with few edits and low editorial attention are less likely to be updated, which is a source of contradictory information based on our analysis."
    }, {
      "heading" : "Near-Duplicate Sentence Detection",
      "text" : "For near-duplicate detection we use a well-known technique called minhash (Broder 1997). We begin with a parameterized family of N hash functions Fi, 1 ≤ i ≤ N . Each sentence in a Wikipedia article is broken up into n-gram “shingles” (at the character level) and for the shingle set S a set {mins∈S(Fi(s)} of minimum hashes over the hash family is generated. The signature of a document d is represented as a vector of K minhashes randomly selected from the set of N . To increase recall we generate M signatures for each sentence (i.e., M draws of K from N ). Broder proves a straightforward relationship between minhash collisions (i.e., documents that share the same signature) and their Jaccard similarities, which forms the theoretical foundation of why and how minhash “works”; we refer the reader to the original paper for the relevant proofs.\nMapReduce Implementation We implemented minhash in MapReduce (Dean and Ghemawat 2004) using Hadoop for this study. The algorithm is as follows: each mapper receives a Wikipedia article identified by a unique docid. Inside the mapper we break the article into sentences using a regular expression; sentences that are shorter than 75 shingles or longer than 600 shingles are discarded. For each sentence, the M minhash signatures are then computed (per above). The family of hash functions is implemented using a “Multiply Shift” hashing scheme2 and generated from a random seed. For our experiments we use a 60 bit hash and a hash family of size N = 20. Each signature is emitted as the key of an intermediate key–value pair with the sentence id as the value (constructed from the docid and the sentence number).\nThe MapReduce programming model guarantees that all values associated with the same key (minhash signatures in our case) are shuffled to the same reducer and grouped together for further processing—in effect, collecting the hash collisions for us. In the reducers we receive signatures as keys and all sentence ids that share a signature as values. If there is more than one value per key, we write out all sentence ids as a cluster. This serves as input to the final cluster generation stage (more below).\n2http://en.wikipedia.org/wiki/Universal hashing"
    }, {
      "heading" : "Parameter Tuning",
      "text" : "One complexity of applying minhash to real-world datasets is the myriad of parameters that must be selected—each setting manifests a tradeoff between precision, recall, and computational effort. Our approach to parameter tuning relied on a combination of analytical calculations and hand-tuning based on examining the output. We began by fixing the hashing scheme and the size of the hash family (N = 20). We then generated 10 signatures (M = 10) per input sentence. Based on Broder (1997), the probability of a match for sentences A and B can be expressed as follows:\nP [match(A,B)] = 1− (1− sK)M\nwhere s = Jaccard(A,B). With the above settings, the effects of different K’s are shown in Figure 1. Based on this analysis, we set K = 10. This means that if we choose 0.9 Jaccard similarity as our goal (90% overlap in shingle sets), then there is a 99% chance of a match (i.e., the recall). Finally, after some hand tuning, we settled on a shingle length of 12 characters. This setting means that we obtain shingles that cross word boundaries, which allows us to capture (to some extent) word order in our similarity computations.\nBased on the parameter settings, it is possible to estimate the amount of data that is generated by our approach, which is the product of the length of each signature, the number of signatures per sentence, and the total number of sentences. This computation is useful because the amount of intermediate data provides a rough proxy for algorithm running time.\nFinally, note that it is possible to improve precision by filtering results with a second pass on the output signature groups. In this second pass we can discard false positives or apply another similarity metric (e.g., edit distance). Compared to the computational cost of minhash, such additional processing is cheap since it is applied to far less data. However, we did not implement second-pass filtering in our experiments and leave this for future work."
    }, {
      "heading" : "Final Cluster Generation",
      "text" : "The output of minhash is a set of clusters, where each cluster represents a signature collision. Since we generate multiple signatures per sentence, it is possible that a sentence appears in multiple clusters. We adopt the standard practice of merging all clusters that share at least one common sentence. Cluster merging is accomplished in one pass (outside\nMapReduce) with a union find data structure (Cormen et al. 2001). In union find each node maintains a pointer to a head node for the set. We maintain a lookup table from each sentence id to its node in the union find structure. Iterating over the input clusters, we look up the node for each sentence, merging clusters or creating new ones as needed. Merging two clusters involves changing the head node of one cluster to point to the head of the other. Once all clusters are processed we perform a pass over the lookup table to obtain the mapping from clusters to nodes. Another MapReduce job reconstructs the sentences in each cluster."
    }, {
      "heading" : "Experimental Results",
      "text" : "Our analysis was conducted on an XML dump of English Wikipedia from July 2013. The entire corpus is approximately 42 GB and contains 10.2m articles (after excluding certain non-article pages). Based on our simple sentence chunker, there are a total 135.8m sentences.\nRunning our Hadoop implementation of minhash on the entire collection took approximately 25 minutes on our cluster consisting of 16 nodes (128 cores). Code necessary for replicating these experiments have been open sourced.3\nAfter the processing pipeline, we identified 1.15m clusters, 3.50m article/sentence pairs over 1.09m articles. The clusters contain 2.36m unique sentences. Cluster sizes range from 2 to over 40k; see Figure 1 for a histogram. Most clusters are small; about 99% of the clusters have ten or fewer sentences, but about a quarter of the output sentence/article pairs fall into clusters with sizes greater than ten.\nBased on manual inspection of the cluster output, we can categorize near-duplicate clusters into one of six types. Examples are shown in Figure 3 and discussed below.\nTemplates describe sentences that have identical structure, but with different entities, facts, or figures for different topics (and thus are not contradictory). They reflect conscious attempts to impose structure across groups of articles that may be related. Since several of the largest template clusters contain tens of thousands of sentences (the largest over\n3https://github.com/seweissman/wikiduper\n40,000), it is likely that some template groups are automatically generated using bots. In many cases template sentences are found in stub articles.\nIdentical sentences are the result of copy and paste, and are often found in articles that cover similar topics or articles that are subtopics of other topics. Non-identical but highlysimilar sentences break down into two types: Copyediting refers to nearly identical sentences that differ in stylistic or otherwise non-substantive ways. They arise with minor editing after a copy and paste. Factual drift describes sentences about the same topic that provide contradictory facts. Although without detailed research, there is no way to ascertain which version (if any) is correct, we can identify a common scenario. After a copy and paste, a fact becomes out of date (e.g., the tallest building or the death toll in a disaster) and is corrected in one instance but not the others.\nReferences refer to citations, typically occurring at the end of articles. Since Wikipedia does not adhere to one single citation style, the same work may be cited differently, or multiple citations to the same venue may be similar.\nFinally, clusters that do not fit into any of the above categories are classified as other. These typically represent sentences that are highly similar, but otherwise bear no semantic relationship with each other. Sentence chunking errors often contribute to these spurious results.\nTo quantify the distribution of these six cases, we randomly sampled 2094 clusters and performed manual classification. The results are shown in Table 1. Nearly three quarters of all clusters are either identical or templates. From this table, we obtain a rough characterization of the precision of our analysis based on minhash—except for “other”, all categories are “correct” identification of near-duplicate sentences. Recall can be computed analytically, as we have shown in the previous section.\nFinally, we manually examined a few of the large clusters and noticed that they often contain a mix of different phenomena. One common pattern is distinct groups of identical sentences, where each of the groups are near duplicates. Since there are relatively few large clusters, these nuances do not have a significant impact on the figures in Table 1."
    }, {
      "heading" : "Future Work and Conclusions",
      "text" : "In this work, we applied minhash to the problem of nearduplicate sentence detection on Wikipedia. Our MapReduce implementation is highly scalable and processes English Wikipedia in a short amount of time on a modest cluster. We found that there is a substantial amount of duplicate content"
    }, {
      "heading" : "Templates",
      "text" : ""
    }, {
      "heading" : "Identical",
      "text" : ""
    }, {
      "heading" : "Copyediting",
      "text" : ""
    }, {
      "heading" : "Factual Drift",
      "text" : ""
    }, {
      "heading" : "Other",
      "text" : "in Wikipedia, and that near-duplicate sentences manifest a few phenomena, the most interesting of which is contradictory facts. How can we act upon this analysis? We could imagine a robot that monitors Wikipedia to flag inconsistencies and requests editors to intervene and resolve. Such a service would be valuable in improving the internal consistency and quality of Wikipedia.\nThere are several directions in which our work can be extended. Currently, our technique only identifies clusters of near-duplicate sentences—missing from our analysis is the notion of information flow: Which was the source article and which was the target of copying and pasting? Are there copy “chains” where content was progressively copied from one article to the next, with possible “branches” (and accumulation of errors along the way)? Our analysis of the large nearduplicate clusters suggests that there are complex edit histories that form tree-like structures. Furthermore, are there editor-specific effects? For example, is copying and pasting more likely by anonymous editors?\nFinally, templates represent interesting cases of nearduplicate sentences because they, in effect, encode structured knowledge. We could formalized such knowledge in infoboxes if they are not already: it might be possible to apply information extraction techniques to generate structured linked data from template sentences. Going in the opposite direction, we might (via a robot) create templates sentences to impose consistency across pages in the same category to ensure that certain facts are captured in prose."
    } ],
    "references" : [ {
      "title" : "R",
      "author" : [ "Bayardo" ],
      "venue" : "J.; Ma, Y.; and Srikant, R.",
      "citeRegEx" : "Bayardo. Ma. and Srikant 2007",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "W",
      "author" : [ "M. Bendersky", "Croft" ],
      "venue" : "B.",
      "citeRegEx" : "Bendersky and Croft 2009",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "A",
      "author" : [ "Broder" ],
      "venue" : "Z.",
      "citeRegEx" : "Broder 1997",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "R",
      "author" : [ "T.H. Cormen", "C.E. Leiserson", "Rivest" ],
      "venue" : "L.; and Stein, C.",
      "citeRegEx" : "Cormen et al. 2001",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "and Ghemawat",
      "author" : [ "J. Dean" ],
      "venue" : "S.",
      "citeRegEx" : "Dean and Ghemawat 2004",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Brute force and indexed approaches to pairwise document similarity comparisons with MapReduce",
      "author" : [ "J. Lin" ],
      "venue" : null,
      "citeRegEx" : "Lin,? \\Q2009\\E",
      "shortCiteRegEx" : "Lin",
      "year" : 2009
    }, {
      "title" : "R",
      "author" : [ "A. Si", "H.V. Leong", "Lau" ],
      "venue" : "W. H.",
      "citeRegEx" : "Si. Leong. and Lau 1997",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "L",
      "author" : [ "B. Stvilia", "M.B. Twidale", "Smith" ],
      "venue" : "C.; and Gasser, L.",
      "citeRegEx" : "Stvilia et al. 2008",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "M",
      "author" : [ "Vernica, R.", "Carey" ],
      "venue" : "J.; and Li, C.",
      "citeRegEx" : "Vernica. Carey. and Li 2010",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "B",
      "author" : [ "D.M. Wilkinson", "Huberman" ],
      "venue" : "A.",
      "citeRegEx" : "Wilkinson and Huberman 2007",
      "shortCiteRegEx" : null,
      "year" : 2007
    } ],
    "referenceMentions" : [ ],
    "year" : 2014,
    "abstractText" : "Our study identifies sentences in Wikipedia articles that are either identical or highly similar by applying techniques for near-duplicate detection of web pages. This is accomplished with a MapReduce implementation of minhash to identify clusters of sentences with high Jaccard similarity. We show that these clusters can be categorized into six different types, two of which are particularly interesting: identical sentences quantify the extent to which content in Wikipedia is copied and pasted, and near-duplicate sentences that state contradictory facts point to quality issues in Wikipedia.",
    "creator" : "LaTeX with hyperref package"
  }
}