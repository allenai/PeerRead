{"conference": "ICLR 2017 conference submission", "title": "Transformation-based Models of Video Sequences", "abstract": "In this work we propose a simple unsupervised approach for next frame prediction in video. Instead of directly predicting the pixels in a frame given past frames, we predict the transformations needed for generating the next frame in a sequence, given the transformations of the past frames. This leads to sharper results, while using a smaller prediction model.  In order to enable a fair comparison between different video frame prediction models, we also propose a new evaluation protocol. We use generated frames as input to a classifier trained with ground truth sequences. This criterion guarantees that models scoring high are those producing sequences which preserve discrim- inative features, as opposed to merely penalizing any deviation, plausible or not, from the ground truth. Our proposed approach compares favourably against more sophisticated ones on the UCF-101 data set, while also being more efficient in terms of the number of parameters and computational cost.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "This paper describes an approach to predict (unseen) future frames of a video given a set of known past frames. The approach is based on a CNN that, in contrast to most related papers, work in the space of affine transformations (instead of pixels or flow). Said another way, the network takes as input a set of affine transforms that describe the motion of patches in the past frames, and likewise, outputs a set of affine transforms that predict future patch motion.\n\nTo that aim, the authors make a few simplifying hypotheses, namely, that a sequence of frames can be modeled accurately enough in their patch-affine framework. This is not unreasonable. A lot of papers in the optical flow community are based on similar hypotheses, i.e. model the flow as a smoothly varying affine field (for instance see \"Locally affine sparse-to-dense matching for motion and occlusion estimation\" by Leordeanu et al., \"EpicFlow: Edge-Preserving Interpolation of Correspondences for Optical Flow\" by Revaud et al., \"Optical Flow With Semantic Segmentation and Localized Layers\" by Sevilla-Lara et al.). These methods are state of the art, which gives a hint about the validity of this kind of approach. \nIn addition, it also seems very reasonable to reformulate the prediction task as predicting motion rather than predicting raw pixels. Indeed, the (patch-affine) motion space is considerably smaller than the image space, making the problem much more tractable and amenable to high-resolution videos.\n\nWhile I agree with the authors on these points, I also find that the paper suffer from important flaws. Specifically:\n\n  - the choice of not comparing with previous approaches in term of pixel prediction error seems very \"convenient\", to say the least. While it is clear that the evaluation metric is imperfect, it is not a reason to completely dismiss all quantitative comparisons with previous work. The frames output by the network on, e.g. the moving digits datasets (Figure 4), looks ok and can definitely be compared with other papers. Yet, the authors chose not to, which is suspicious.\n  \n  - The newly proposed metric poses several problems. First, action classification is evaluated with C3D, which is not a state-of-the-art approach at all for this task. Second, this metric actually *does not* evaluate what the network is claimed to do, that is, next frame prediction. Instead, it evaluates if another network, which was never trained to distinguish between real or synthetic frames by the way, can accurately classify an action from the predicted frames. I find that this proxy metric is only weakly related to what is supposed to be measured. In adition, it does not really make sense to train a network for something else that the final task it is evaluated for.\n  \n  - how is the affine motion of patches estimated? It is only explained that the problem is solved globally (not treating each patch independently) in a pretty vague manner. Estimating the motion of all patches is akin to solving the optical flow, which is still an active subject of research. Therefore, an important flaw of the paper lies in the potentially erroneous etimation of the motion input to the network. In the videos made available, it is clear that the motion is wrongly estimated sometimes. Since the entire approach depends on this input, I find it important to discuss this aspect. How do motion estimation failures impact the network? Also, the patch-affine hypothesis does not hold when patches are large enough that they cover several objects with contradictory motion. Which appears to be the case on UCF101 videos.\n  \n  - Even ignoring the weird proxy-evaluation part, the network is still not trained end-to-end. That is, the network is trained to minimize the difference between (noisy) ground-truth and output affine transforms, instead of minimizing a loss in the actual output space (frame pixels) for which an (exact) ground-truth is available. It is true that the MSE loss on raw pixels leads to blurry results, but other types of losses do exist, for instance the gradient loss introduced by Mathieu et al. was shown to solve this issue. As noted by the authors themselves, minimizing a loss in the transformation space, where affine parameters are harder to intepret, introduces unexpected artifacts. The motion is often largely underestimated, as is obvious in Figure 5 where it is hard to tell the difference between the input and output frames. \n  \n  - The proposed approach is not sufficiently compared to previous work. In particular, the approach is closely related to \"SPATIO-TEMPORAL VIDEO AUTOENCODER WITH DIFFERENTIABLE MEMORY\" of Taraucean et al, ICLR'15. This paper also output prediction in the motion space. Experimental results should compare against it.\n\n  - The comparison with optical flow is unfair. First, the approach of Brox et al. is more than 10 years old. Second, it is not really fair to assume a constant flow for all frames. At least some basic extrapolation could be done to take into account the flow of all pairs of input frames and not just the last one. Overall, the approach is not compared to very challenging baselines.\n\n  - I disagree with the answer that the authors gave to a reviewer's question. Denote ground-truth frames as {X_0, X_1 ...} and predicted frames as {Y_1, Y_2, ...}. When asked if the videos at"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "There were serious concerns raised about the originality of the work in regard to prior methods. The authors' responses to numerous reviewer questions on this matter were also unsatisfactory. In the end, it is difficult to tell what the contribution in regard to the video prediction model is. The proposed evaluation metric is interesting, but also raised serious concerns. Finally, several reviewers raised concerns about the quality of the results.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "reasonable paper but the contribution right now is very incremental compared to previous works", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "The paper proposes a method for future frame prediction based on transformation of previous frame rather than direct pixel prediction.\n\nMany previous works have proposed similar methods. The authors in their responses state that previous work is deterministic, yet the proposed model also does not handle multimodality.\n\nFurther, i asked if they could test their method using 2 RGB frames as input and predicting the transformation as output, to be able to quantify the importance of using transformations both as input and output, since this is the first work that uses transformations as input also. The authors dismissed the suggestion by saying \"if we were to use RGB frames as input and ask the model to output future frames it would produce very blurry results\", that is, misunderstanding what the suggestion was. So, currently, it does not seem to be a valid novel contribution in this work compared to previous works.", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Reasonable paper, can be improved.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "Paper Summary\nThis paper makes two contributions -\n(1) A model for next step prediction, where the inputs and outputs are in the\nspace of affine transforms between adjacent frames.\n(2) An evaluation method in which the quality of the generated data is assessed\nby measuring the reduction in performance of another model (such as a\nclassifier) when tested on the generated data.\n\nThe authors show that according to this metric, the proposed model works better\nthan other baseline models (including the recent work of Mathieu et al. which\nuses adversarial training).\n\nStrengths\n- This paper attempts to solve a major problem in unsupervised learning\n  with videos, which is evaluating them.\n- The results show that using MSE in transform space does prevent the blurring\n  problem to a large extent (which is one of the main aims of this paper).\n- The results show that the generated data reduces the performance of the C3D\n  model on UCF-101 to a much less extent than other baselines.\n- The paper validates the assumption that videos can be approximated to quite a\n  few time steps by a sequence of affine transforms starting from an initial\nframe.\n\nWeaknesses\n- The proposed metric makes sense only if we truly just care about the performance\n  of a particular classifier on a given task. This significantly narrows the\nscope of applicability of this metric because arguably, one the important\nreasons for doing unsupervised learning is to come up a representation that is\nwidely applicable across a variety of tasks. The proposed metric would not help\nevaluate generative models designed to achieve this objective.\n\n- It is possible that one of the generative models being compared will interact\n  with the idiosyncrasies of the chosen classifier in unintended ways.\nTherefore, it would be hard to draw strong conclusions about the relative\nmerits of generative models from the results of such experiments. One way to\nameliorate this would be to use several different classifiers (C3D,\ndual-stream network, other state-of-the-art methods) and show that the ranking\nof different generative models is consistent across the choice of classifier.\nAdding such experiments would help increase certainty in the conclusions drawn\nin this paper.\n\n- Using only 4 or 8 input frames sampled at 25fps seems like very little context\n  if we really expect the model to extrapolate the kind of motion seen in\nUCF-101. The idea of working in the space of affine transforms would be much\nmore appealing if the model can be shown to really generated non-trivial motion\npatterns. Currently, the motion patterns seem to be almost linear\nextrapolations.\n\n- The model that predicts motion does not have access to content at all. It only\n  gets access to previous motion. It seems that this might be a disadvantage\nbecause the motion predictor cannot use any cues like object boundaries, or\ndecide what to do when two motion fields collide (it is probably easier to argue\nabout occlusions in content space).\n\nQuality/Clarity\nThe paper is clearly written and easy to follow. The assumptions are clearly\nspecified and validated. Experimental details seem adequate.\n\nOriginality\nThe idea of generating videos by predicting motion has been used previously.\nSeveral recent papers also use this idea. However the exact implementation in\nthis paper is new. The proposed evaluation protocol is novel.\n\nSignificance\nThe proposed evaluation method is an interesting alternative, especially if it\nis extended to include multiple classifiers representative of different\nstate-of-the-art approaches. Given how hard it is to evaluate generative models\nof videos, this paper could help start an effort to standardize on a benchmark\nset.\n\nMinor comments and suggestions\n\n(1) In the caption for Table 1: ``Each column shows the accuracy on the test set\nwhen taking a different number of input frames as input\" - ``input\" here refers\nto the input to the classifier (Output of the next step prediction model). However\nin the next sentence ``Our approach maps 16 \\times 16 patches into 8 \\times 8\nwith stride 4, and it takes 4 frames at the input\" - here ``input\" refers to\nthe input to the next step prediction model. It might be a good idea to rephrase\nthese sentences to make the distinction clear.\n\n(2) In order to better understand the space of affine transform\nparameters, it might help to include a histogram of these parameters in the\npaper. This can help us see at a glance, what is the typical range of these\n6 parameters, should we expect a lot of outliers, etc.\n\n(3) In order to compare transforms A and B, instead of ||A - B||^2, one\ncould consider A^{-1}B being close to identity as the metric. Did the authors\ntry this ?\n\n(4) \"The performance of the classifier on ground truth data is an upper bound on\nthe performance of any generative model.\" This is not *strictly* true. It is\npossible (though highly unlikely) that a generative model might make the data\nlook cleaner, sharper, or highlight some aspect of it which could improve the\nperformance of the classifier (even compared to ground truth). This is\nespecially true if the the generative model had access to the classifier, it\ncould then see what makes the classifier fire and highlight those discriminative\nfeatures in the generated output.\n\nOverall\nThis paper proposes future prediction in affine transform space. This does\nreduce blurriness and makes the videos look relatively realistic (at least to the\nC3D classifier). However, the paper can be improved by showing that the model can\npredict more non-trivial motion flows and the experiments can be strengthened by\nadding more classifiers besides than C3D.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "18 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "An interesting direction, but has many flaws", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper describes an approach to predict (unseen) future frames of a video given a set of known past frames. The approach is based on a CNN that, in contrast to most related papers, work in the space of affine transformations (instead of pixels or flow). Said another way, the network takes as input a set of affine transforms that describe the motion of patches in the past frames, and likewise, outputs a set of affine transforms that predict future patch motion.\n\nTo that aim, the authors make a few simplifying hypotheses, namely, that a sequence of frames can be modeled accurately enough in their patch-affine framework. This is not unreasonable. A lot of papers in the optical flow community are based on similar hypotheses, i.e. model the flow as a smoothly varying affine field (for instance see \"Locally affine sparse-to-dense matching for motion and occlusion estimation\" by Leordeanu et al., \"EpicFlow: Edge-Preserving Interpolation of Correspondences for Optical Flow\" by Revaud et al., \"Optical Flow With Semantic Segmentation and Localized Layers\" by Sevilla-Lara et al.). These methods are state of the art, which gives a hint about the validity of this kind of approach. \nIn addition, it also seems very reasonable to reformulate the prediction task as predicting motion rather than predicting raw pixels. Indeed, the (patch-affine) motion space is considerably smaller than the image space, making the problem much more tractable and amenable to high-resolution videos.\n\nWhile I agree with the authors on these points, I also find that the paper suffer from important flaws. Specifically:\n\n  - the choice of not comparing with previous approaches in term of pixel prediction error seems very \"convenient\", to say the least. While it is clear that the evaluation metric is imperfect, it is not a reason to completely dismiss all quantitative comparisons with previous work. The frames output by the network on, e.g. the moving digits datasets (Figure 4), looks ok and can definitely be compared with other papers. Yet, the authors chose not to, which is suspicious.\n  \n  - The newly proposed metric poses several problems. First, action classification is evaluated with C3D, which is not a state-of-the-art approach at all for this task. Second, this metric actually *does not* evaluate what the network is claimed to do, that is, next frame prediction. Instead, it evaluates if another network, which was never trained to distinguish between real or synthetic frames by the way, can accurately classify an action from the predicted frames. I find that this proxy metric is only weakly related to what is supposed to be measured. In adition, it does not really make sense to train a network for something else that the final task it is evaluated for.\n  \n  - how is the affine motion of patches estimated? It is only explained that the problem is solved globally (not treating each patch independently) in a pretty vague manner. Estimating the motion of all patches is akin to solving the optical flow, which is still an active subject of research. Therefore, an important flaw of the paper lies in the potentially erroneous etimation of the motion input to the network. In the videos made available, it is clear that the motion is wrongly estimated sometimes. Since the entire approach depends on this input, I find it important to discuss this aspect. How do motion estimation failures impact the network? Also, the patch-affine hypothesis does not hold when patches are large enough that they cover several objects with contradictory motion. Which appears to be the case on UCF101 videos.\n  \n  - Even ignoring the weird proxy-evaluation part, the network is still not trained end-to-end. That is, the network is trained to minimize the difference between (noisy) ground-truth and output affine transforms, instead of minimizing a loss in the actual output space (frame pixels) for which an (exact) ground-truth is available. It is true that the MSE loss on raw pixels leads to blurry results, but other types of losses do exist, for instance the gradient loss introduced by Mathieu et al. was shown to solve this issue. As noted by the authors themselves, minimizing a loss in the transformation space, where affine parameters are harder to intepret, introduces unexpected artifacts. The motion is often largely underestimated, as is obvious in Figure 5 where it is hard to tell the difference between the input and output frames. \n  \n  - The proposed approach is not sufficiently compared to previous work. In particular, the approach is closely related to \"SPATIO-TEMPORAL VIDEO AUTOENCODER WITH DIFFERENTIABLE MEMORY\" of Taraucean et al, ICLR'15. This paper also output prediction in the motion space. Experimental results should compare against it.\n\n  - The comparison with optical flow is unfair. First, the approach of Brox et al. is more than 10 years old. Second, it is not really fair to assume a constant flow for all frames. At least some basic extrapolation could be done to take into account the flow of all pairs of input frames and not just the last one. Overall, the approach is not compared to very challenging baselines.\n\n  - I disagree with the answer that the authors gave to a reviewer's question. Denote ground-truth frames as {X_0, X_1 ...} and predicted frames as {Y_1, Y_2, ...}. When asked if the videos at ", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "14 Dec 2016 (modified: 16 Dec 2016)", "REVIEWER_CONFIDENCE": 3}, {"DATE": "08 Dec 2016", "TITLE": "Qualitative results", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "08 Dec 2016", "TITLE": "novelty-finetuning-RGB input", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "05 Dec 2016", "TITLE": "Question regarding related work", "IS_META_REVIEW": false, "comments": "Could the authors comment on the relationship between their work and these previous works that appear to use a similar transformation-based video prediction technique?\n\nDynamic Filter Networks (NIPS 2016)\nUnsupervised Learning for Physical Interaction through Video Prediction (NIPS 2016)\nVisual Dynamics: Probabilistic Future Frame Synthesis via Cross Convolutional Networks (NIPS 2016)", "OTHER_KEYS": "ICLR 2017 conference"}, {"DATE": "02 Dec 2016", "TITLE": "Reconstructions using ground truth affine transforms", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "13 Nov 2016", "TITLE": "Missing relevant reference and comparison? ", "IS_META_REVIEW": false, "comments": "The transformation-based approach for generating the next frame in a sequence was used in ICLRw2016 ", "OTHER_KEYS": "(anonymous)"}, {"IS_META_REVIEW": true, "comments": "This paper describes an approach to predict (unseen) future frames of a video given a set of known past frames. The approach is based on a CNN that, in contrast to most related papers, work in the space of affine transformations (instead of pixels or flow). Said another way, the network takes as input a set of affine transforms that describe the motion of patches in the past frames, and likewise, outputs a set of affine transforms that predict future patch motion.\n\nTo that aim, the authors make a few simplifying hypotheses, namely, that a sequence of frames can be modeled accurately enough in their patch-affine framework. This is not unreasonable. A lot of papers in the optical flow community are based on similar hypotheses, i.e. model the flow as a smoothly varying affine field (for instance see \"Locally affine sparse-to-dense matching for motion and occlusion estimation\" by Leordeanu et al., \"EpicFlow: Edge-Preserving Interpolation of Correspondences for Optical Flow\" by Revaud et al., \"Optical Flow With Semantic Segmentation and Localized Layers\" by Sevilla-Lara et al.). These methods are state of the art, which gives a hint about the validity of this kind of approach. \nIn addition, it also seems very reasonable to reformulate the prediction task as predicting motion rather than predicting raw pixels. Indeed, the (patch-affine) motion space is considerably smaller than the image space, making the problem much more tractable and amenable to high-resolution videos.\n\nWhile I agree with the authors on these points, I also find that the paper suffer from important flaws. Specifically:\n\n  - the choice of not comparing with previous approaches in term of pixel prediction error seems very \"convenient\", to say the least. While it is clear that the evaluation metric is imperfect, it is not a reason to completely dismiss all quantitative comparisons with previous work. The frames output by the network on, e.g. the moving digits datasets (Figure 4), looks ok and can definitely be compared with other papers. Yet, the authors chose not to, which is suspicious.\n  \n  - The newly proposed metric poses several problems. First, action classification is evaluated with C3D, which is not a state-of-the-art approach at all for this task. Second, this metric actually *does not* evaluate what the network is claimed to do, that is, next frame prediction. Instead, it evaluates if another network, which was never trained to distinguish between real or synthetic frames by the way, can accurately classify an action from the predicted frames. I find that this proxy metric is only weakly related to what is supposed to be measured. In adition, it does not really make sense to train a network for something else that the final task it is evaluated for.\n  \n  - how is the affine motion of patches estimated? It is only explained that the problem is solved globally (not treating each patch independently) in a pretty vague manner. Estimating the motion of all patches is akin to solving the optical flow, which is still an active subject of research. Therefore, an important flaw of the paper lies in the potentially erroneous etimation of the motion input to the network. In the videos made available, it is clear that the motion is wrongly estimated sometimes. Since the entire approach depends on this input, I find it important to discuss this aspect. How do motion estimation failures impact the network? Also, the patch-affine hypothesis does not hold when patches are large enough that they cover several objects with contradictory motion. Which appears to be the case on UCF101 videos.\n  \n  - Even ignoring the weird proxy-evaluation part, the network is still not trained end-to-end. That is, the network is trained to minimize the difference between (noisy) ground-truth and output affine transforms, instead of minimizing a loss in the actual output space (frame pixels) for which an (exact) ground-truth is available. It is true that the MSE loss on raw pixels leads to blurry results, but other types of losses do exist, for instance the gradient loss introduced by Mathieu et al. was shown to solve this issue. As noted by the authors themselves, minimizing a loss in the transformation space, where affine parameters are harder to intepret, introduces unexpected artifacts. The motion is often largely underestimated, as is obvious in Figure 5 where it is hard to tell the difference between the input and output frames. \n  \n  - The proposed approach is not sufficiently compared to previous work. In particular, the approach is closely related to \"SPATIO-TEMPORAL VIDEO AUTOENCODER WITH DIFFERENTIABLE MEMORY\" of Taraucean et al, ICLR'15. This paper also output prediction in the motion space. Experimental results should compare against it.\n\n  - The comparison with optical flow is unfair. First, the approach of Brox et al. is more than 10 years old. Second, it is not really fair to assume a constant flow for all frames. At least some basic extrapolation could be done to take into account the flow of all pairs of input frames and not just the last one. Overall, the approach is not compared to very challenging baselines.\n\n  - I disagree with the answer that the authors gave to a reviewer's question. Denote ground-truth frames as {X_0, X_1 ...} and predicted frames as {Y_1, Y_2, ...}. When asked if the videos at"}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "There were serious concerns raised about the originality of the work in regard to prior methods. The authors' responses to numerous reviewer questions on this matter were also unsatisfactory. In the end, it is difficult to tell what the contribution in regard to the video prediction model is. The proposed evaluation metric is interesting, but also raised serious concerns. Finally, several reviewers raised concerns about the quality of the results.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "reasonable paper but the contribution right now is very incremental compared to previous works", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "The paper proposes a method for future frame prediction based on transformation of previous frame rather than direct pixel prediction.\n\nMany previous works have proposed similar methods. The authors in their responses state that previous work is deterministic, yet the proposed model also does not handle multimodality.\n\nFurther, i asked if they could test their method using 2 RGB frames as input and predicting the transformation as output, to be able to quantify the importance of using transformations both as input and output, since this is the first work that uses transformations as input also. The authors dismissed the suggestion by saying \"if we were to use RGB frames as input and ask the model to output future frames it would produce very blurry results\", that is, misunderstanding what the suggestion was. So, currently, it does not seem to be a valid novel contribution in this work compared to previous works.", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Reasonable paper, can be improved.", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "Paper Summary\nThis paper makes two contributions -\n(1) A model for next step prediction, where the inputs and outputs are in the\nspace of affine transforms between adjacent frames.\n(2) An evaluation method in which the quality of the generated data is assessed\nby measuring the reduction in performance of another model (such as a\nclassifier) when tested on the generated data.\n\nThe authors show that according to this metric, the proposed model works better\nthan other baseline models (including the recent work of Mathieu et al. which\nuses adversarial training).\n\nStrengths\n- This paper attempts to solve a major problem in unsupervised learning\n  with videos, which is evaluating them.\n- The results show that using MSE in transform space does prevent the blurring\n  problem to a large extent (which is one of the main aims of this paper).\n- The results show that the generated data reduces the performance of the C3D\n  model on UCF-101 to a much less extent than other baselines.\n- The paper validates the assumption that videos can be approximated to quite a\n  few time steps by a sequence of affine transforms starting from an initial\nframe.\n\nWeaknesses\n- The proposed metric makes sense only if we truly just care about the performance\n  of a particular classifier on a given task. This significantly narrows the\nscope of applicability of this metric because arguably, one the important\nreasons for doing unsupervised learning is to come up a representation that is\nwidely applicable across a variety of tasks. The proposed metric would not help\nevaluate generative models designed to achieve this objective.\n\n- It is possible that one of the generative models being compared will interact\n  with the idiosyncrasies of the chosen classifier in unintended ways.\nTherefore, it would be hard to draw strong conclusions about the relative\nmerits of generative models from the results of such experiments. One way to\nameliorate this would be to use several different classifiers (C3D,\ndual-stream network, other state-of-the-art methods) and show that the ranking\nof different generative models is consistent across the choice of classifier.\nAdding such experiments would help increase certainty in the conclusions drawn\nin this paper.\n\n- Using only 4 or 8 input frames sampled at 25fps seems like very little context\n  if we really expect the model to extrapolate the kind of motion seen in\nUCF-101. The idea of working in the space of affine transforms would be much\nmore appealing if the model can be shown to really generated non-trivial motion\npatterns. Currently, the motion patterns seem to be almost linear\nextrapolations.\n\n- The model that predicts motion does not have access to content at all. It only\n  gets access to previous motion. It seems that this might be a disadvantage\nbecause the motion predictor cannot use any cues like object boundaries, or\ndecide what to do when two motion fields collide (it is probably easier to argue\nabout occlusions in content space).\n\nQuality/Clarity\nThe paper is clearly written and easy to follow. The assumptions are clearly\nspecified and validated. Experimental details seem adequate.\n\nOriginality\nThe idea of generating videos by predicting motion has been used previously.\nSeveral recent papers also use this idea. However the exact implementation in\nthis paper is new. The proposed evaluation protocol is novel.\n\nSignificance\nThe proposed evaluation method is an interesting alternative, especially if it\nis extended to include multiple classifiers representative of different\nstate-of-the-art approaches. Given how hard it is to evaluate generative models\nof videos, this paper could help start an effort to standardize on a benchmark\nset.\n\nMinor comments and suggestions\n\n(1) In the caption for Table 1: ``Each column shows the accuracy on the test set\nwhen taking a different number of input frames as input\" - ``input\" here refers\nto the input to the classifier (Output of the next step prediction model). However\nin the next sentence ``Our approach maps 16 \\times 16 patches into 8 \\times 8\nwith stride 4, and it takes 4 frames at the input\" - here ``input\" refers to\nthe input to the next step prediction model. It might be a good idea to rephrase\nthese sentences to make the distinction clear.\n\n(2) In order to better understand the space of affine transform\nparameters, it might help to include a histogram of these parameters in the\npaper. This can help us see at a glance, what is the typical range of these\n6 parameters, should we expect a lot of outliers, etc.\n\n(3) In order to compare transforms A and B, instead of ||A - B||^2, one\ncould consider A^{-1}B being close to identity as the metric. Did the authors\ntry this ?\n\n(4) \"The performance of the classifier on ground truth data is an upper bound on\nthe performance of any generative model.\" This is not *strictly* true. It is\npossible (though highly unlikely) that a generative model might make the data\nlook cleaner, sharper, or highlight some aspect of it which could improve the\nperformance of the classifier (even compared to ground truth). This is\nespecially true if the the generative model had access to the classifier, it\ncould then see what makes the classifier fire and highlight those discriminative\nfeatures in the generated output.\n\nOverall\nThis paper proposes future prediction in affine transform space. This does\nreduce blurriness and makes the videos look relatively realistic (at least to the\nC3D classifier). However, the paper can be improved by showing that the model can\npredict more non-trivial motion flows and the experiments can be strengthened by\nadding more classifiers besides than C3D.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "18 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "An interesting direction, but has many flaws", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper describes an approach to predict (unseen) future frames of a video given a set of known past frames. The approach is based on a CNN that, in contrast to most related papers, work in the space of affine transformations (instead of pixels or flow). Said another way, the network takes as input a set of affine transforms that describe the motion of patches in the past frames, and likewise, outputs a set of affine transforms that predict future patch motion.\n\nTo that aim, the authors make a few simplifying hypotheses, namely, that a sequence of frames can be modeled accurately enough in their patch-affine framework. This is not unreasonable. A lot of papers in the optical flow community are based on similar hypotheses, i.e. model the flow as a smoothly varying affine field (for instance see \"Locally affine sparse-to-dense matching for motion and occlusion estimation\" by Leordeanu et al., \"EpicFlow: Edge-Preserving Interpolation of Correspondences for Optical Flow\" by Revaud et al., \"Optical Flow With Semantic Segmentation and Localized Layers\" by Sevilla-Lara et al.). These methods are state of the art, which gives a hint about the validity of this kind of approach. \nIn addition, it also seems very reasonable to reformulate the prediction task as predicting motion rather than predicting raw pixels. Indeed, the (patch-affine) motion space is considerably smaller than the image space, making the problem much more tractable and amenable to high-resolution videos.\n\nWhile I agree with the authors on these points, I also find that the paper suffer from important flaws. Specifically:\n\n  - the choice of not comparing with previous approaches in term of pixel prediction error seems very \"convenient\", to say the least. While it is clear that the evaluation metric is imperfect, it is not a reason to completely dismiss all quantitative comparisons with previous work. The frames output by the network on, e.g. the moving digits datasets (Figure 4), looks ok and can definitely be compared with other papers. Yet, the authors chose not to, which is suspicious.\n  \n  - The newly proposed metric poses several problems. First, action classification is evaluated with C3D, which is not a state-of-the-art approach at all for this task. Second, this metric actually *does not* evaluate what the network is claimed to do, that is, next frame prediction. Instead, it evaluates if another network, which was never trained to distinguish between real or synthetic frames by the way, can accurately classify an action from the predicted frames. I find that this proxy metric is only weakly related to what is supposed to be measured. In adition, it does not really make sense to train a network for something else that the final task it is evaluated for.\n  \n  - how is the affine motion of patches estimated? It is only explained that the problem is solved globally (not treating each patch independently) in a pretty vague manner. Estimating the motion of all patches is akin to solving the optical flow, which is still an active subject of research. Therefore, an important flaw of the paper lies in the potentially erroneous etimation of the motion input to the network. In the videos made available, it is clear that the motion is wrongly estimated sometimes. Since the entire approach depends on this input, I find it important to discuss this aspect. How do motion estimation failures impact the network? Also, the patch-affine hypothesis does not hold when patches are large enough that they cover several objects with contradictory motion. Which appears to be the case on UCF101 videos.\n  \n  - Even ignoring the weird proxy-evaluation part, the network is still not trained end-to-end. That is, the network is trained to minimize the difference between (noisy) ground-truth and output affine transforms, instead of minimizing a loss in the actual output space (frame pixels) for which an (exact) ground-truth is available. It is true that the MSE loss on raw pixels leads to blurry results, but other types of losses do exist, for instance the gradient loss introduced by Mathieu et al. was shown to solve this issue. As noted by the authors themselves, minimizing a loss in the transformation space, where affine parameters are harder to intepret, introduces unexpected artifacts. The motion is often largely underestimated, as is obvious in Figure 5 where it is hard to tell the difference between the input and output frames. \n  \n  - The proposed approach is not sufficiently compared to previous work. In particular, the approach is closely related to \"SPATIO-TEMPORAL VIDEO AUTOENCODER WITH DIFFERENTIABLE MEMORY\" of Taraucean et al, ICLR'15. This paper also output prediction in the motion space. Experimental results should compare against it.\n\n  - The comparison with optical flow is unfair. First, the approach of Brox et al. is more than 10 years old. Second, it is not really fair to assume a constant flow for all frames. At least some basic extrapolation could be done to take into account the flow of all pairs of input frames and not just the last one. Overall, the approach is not compared to very challenging baselines.\n\n  - I disagree with the answer that the authors gave to a reviewer's question. Denote ground-truth frames as {X_0, X_1 ...} and predicted frames as {Y_1, Y_2, ...}. When asked if the videos at ", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "14 Dec 2016 (modified: 16 Dec 2016)", "REVIEWER_CONFIDENCE": 3}, {"DATE": "08 Dec 2016", "TITLE": "Qualitative results", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "08 Dec 2016", "TITLE": "novelty-finetuning-RGB input", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "05 Dec 2016", "TITLE": "Question regarding related work", "IS_META_REVIEW": false, "comments": "Could the authors comment on the relationship between their work and these previous works that appear to use a similar transformation-based video prediction technique?\n\nDynamic Filter Networks (NIPS 2016)\nUnsupervised Learning for Physical Interaction through Video Prediction (NIPS 2016)\nVisual Dynamics: Probabilistic Future Frame Synthesis via Cross Convolutional Networks (NIPS 2016)", "OTHER_KEYS": "ICLR 2017 conference"}, {"DATE": "02 Dec 2016", "TITLE": "Reconstructions using ground truth affine transforms", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "13 Nov 2016", "TITLE": "Missing relevant reference and comparison? ", "IS_META_REVIEW": false, "comments": "The transformation-based approach for generating the next frame in a sequence was used in ICLRw2016 ", "OTHER_KEYS": "(anonymous)"}], "authors": "Joost van Amersfoort, Anitha Kannan, Marc'Aurelio Ranzato, Arthur Szlam, Du Tran, Soumith Chintala", "accepted": false, "id": "612"}