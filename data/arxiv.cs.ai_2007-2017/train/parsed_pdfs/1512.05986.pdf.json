{
  "name" : "1512.05986.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Can Pretrained Neural Networks Detect Anatomy?",
    "authors" : [ "Vlado Menkovski" ],
    "emails" : [ "vlado.menkovski@philips.com", "zharko.aleksovski@philips.com", "axel.saalbach@philips.coom", "hannes.nickisch@philips.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n51 2.\n05 98\n6v 1\n[ cs\n.C V\n] 1"
    }, {
      "heading" : "1 Introduction and Motivation",
      "text" : "Deep learning [9] (DL) is rapidly gaining momentum in computer vision [5], speech recognition [3] and medical imaging applications due to their record-breaking empirical performance and their promise to jointly learn the low-level features and the high-level decisions directly from raw data.\nOne major open research challenge is the transfer of the excellent predictive performance of DL methods, in particular convolutional neural networks (CNN), from the “data-laden” regime of computer vision1 to the “data-bounded” regime of medical imaging.\nSince the typical number of parameters in the DL models is very big the major concern for operating in “data-bounded” regimes is inability to generalize and form robust features because the small number of examples do not capture well the variability in the input space. There are various approaches to deal with this challenge. Data augmentation can be used to increase the effective size of the training set and drive the training to build invariances to specific transformations. Network architecture design can be used to hard-code prior knowledge (such as translation invariance) by design. Sophisticated regularization methods such as dropout [8] and batch normalization (BN) [4] can work to counter overfitting. Finally, global training of the feature extraction layers on abundant data domains (i.e. natural images) and then training the “classification” higher layers from the limited data domains can be used to ensure that the lower-level features are invariant to the particularities of the training dataset.\nWe choose two directions to investigate these approaches for the goal of anatomy detection in X-ray images. The first using a network trained on a large corpus of natural images for feature extraction\n1There are 106 images in 103 classes in the ILSVRC challenge, http://www.image-net.org/challenges/LSVRC.\ncombined with a SVM [1] classifier trained on the medical images. And a second approach using heavy augmentation and regularization of a deep neural network trained only on the small-sized medical image dataset."
    }, {
      "heading" : "2 Experiments and Results",
      "text" : "In order to assess the performance of specifically trained and off-the-shelf networks, radiographs from the ImageClef 2009 - Medical Image Annotation task2 were used. This challenge database consists of a broad range of X-ray images from clinical routine, with a detailed anatomical classification. For convenience, we flattened the hierarchical class representation and disregarded classes with fewer than 50 example images. In doing so we ended up with 24 unique classes. For evaluation purposes, the entire data corpus (of 14676 images) was divided into a training and test set, covering 90% and 10% of the data respectively.\nAs a first baseline, the OverFeat network (see [6]) in combination with a (linear) multi-class SVM was employed. OverFeat is a convolutional neural network that was trained on 1.2 million nonmedical images form the ImageNet2012 database with 1000 classes, and has been successfully employed as a generic feature extractor in different applications.\nFor the purpose of this experiment, we selected the fast OverFeat instantiation and used the default setting for feature extraction. This results in a 4096 dimensional representation of the images, which was subsequently used as input for the SVM classier. The regularization parameter C of the linear SVM was estimated on the training set using exhaustive search and 5-fold cross-validation.\nFor the second direction we designed a network 2 inspired by the work of Razavian et al. [7], using dropout and batch normalization with leaky ReLU activation functions [2]. In this case for training we used images of 128 by 128 pixels with various augmentations ranging from resizing and cropping, rotation, translation, shearing, stretching and flipping. The results of the two directions are given in table 1."
    }, {
      "heading" : "3 Conclusions and Perspectives",
      "text" : "We investigated when, where, and how to (re)train a convolutional neural network used for anatomy detection. Our results suggest that pretrained networks are good image descriptors even outside their training domain and that retraining only the last layer is a viable alternative to perform transfer learning in light of limited training data. We also demonstrated that sufficiently expressive models such as a very deep neural network can be trained even on relatively small number of annotations if proper augmentation and regularization is implemented. Our work is a first step towards a multimodality anatomical expert system with components trained to maximize data efficiency."
    } ],
    "references" : [ {
      "title" : "Support-vector networks",
      "author" : [ "C. Cortes", "V. Vapnik" ],
      "venue" : "Machine learning, 20(3):273–297",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification",
      "author" : [ "K. He", "X. Zhang", "S. Ren", "J. Sun" ],
      "venue" : "arXiv preprint arXiv:1502.01852",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "G",
      "author" : [ "G. Hinton", "L. Deng", "D. Yu" ],
      "venue" : "E. Dahl, A.-r. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. N. Sainath, et al. Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. Signal Processing Magazine, IEEE, 29(6):82– 97",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "author" : [ "S. Ioffe", "C. Szegedy" ],
      "venue" : "arXiv preprint arXiv:1502.03167",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "A. Krizhevsky", "I. Sutskever", "G.E. Hinton" ],
      "venue" : "Advances in Neural Information Processing Systems 25 (NIPS)",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Overfeat: Integrated recognition",
      "author" : [ "P. Sermanet", "D. Eigen", "X. Zhang", "M. Mathieu", "R. Fergus", "Y. LeCun" ],
      "venue" : "localization and detection using convolutional networks. In International Conference on Learning Representations ",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "A baseline for visual instance retrieval with deep convolutional networks",
      "author" : [ "A. Sharif Razavian", "J. Sullivan", "A. Maki", "S. Carlsson" ],
      "venue" : "International Conference on Learning Representations, May 7-9, 2015, San Diego, CA. ICLR",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Dropout: A simple way to prevent neural networks from overfitting",
      "author" : [ "N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov" ],
      "venue" : "The Journal of Machine Learning Research, 15(1):1929–1958",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Deep learning",
      "author" : [ "Y.B. Yann LeCun", "G. Hinton" ],
      "venue" : "Nature, 521:436–444",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 8,
      "context" : "Deep learning [9] (DL) is rapidly gaining momentum in computer vision [5], speech recognition [3] and medical imaging applications due to their record-breaking empirical performance and their promise to jointly learn the low-level features and the high-level decisions directly from raw data.",
      "startOffset" : 14,
      "endOffset" : 17
    }, {
      "referenceID" : 4,
      "context" : "Deep learning [9] (DL) is rapidly gaining momentum in computer vision [5], speech recognition [3] and medical imaging applications due to their record-breaking empirical performance and their promise to jointly learn the low-level features and the high-level decisions directly from raw data.",
      "startOffset" : 70,
      "endOffset" : 73
    }, {
      "referenceID" : 2,
      "context" : "Deep learning [9] (DL) is rapidly gaining momentum in computer vision [5], speech recognition [3] and medical imaging applications due to their record-breaking empirical performance and their promise to jointly learn the low-level features and the high-level decisions directly from raw data.",
      "startOffset" : 94,
      "endOffset" : 97
    }, {
      "referenceID" : 7,
      "context" : "Sophisticated regularization methods such as dropout [8] and batch normalization (BN) [4] can work to counter overfitting.",
      "startOffset" : 53,
      "endOffset" : 56
    }, {
      "referenceID" : 3,
      "context" : "Sophisticated regularization methods such as dropout [8] and batch normalization (BN) [4] can work to counter overfitting.",
      "startOffset" : 86,
      "endOffset" : 89
    }, {
      "referenceID" : 0,
      "context" : "combined with a SVM [1] classifier trained on the medical images.",
      "startOffset" : 20,
      "endOffset" : 23
    }, {
      "referenceID" : 5,
      "context" : "As a first baseline, the OverFeat network (see [6]) in combination with a (linear) multi-class SVM was employed.",
      "startOffset" : 47,
      "endOffset" : 50
    }, {
      "referenceID" : 6,
      "context" : "[7], using dropout and batch normalization with leaky ReLU activation functions [2].",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 1,
      "context" : "[7], using dropout and batch normalization with leaky ReLU activation functions [2].",
      "startOffset" : 80,
      "endOffset" : 83
    } ],
    "year" : 2015,
    "abstractText" : "Convolutional neural networks demonstrated outstanding empirical results in computer vision and speech recognition tasks where labeled training data is abundant. In medical imaging, there is a huge variety of possible imaging modalities and contrasts, where annotated data is usually very scarce. We present two approaches to deal with this challenge. A network pretrained in a different domain with abundant data is used as a feature extractor, while a subsequent classifier is trained on a small target dataset; and a deep architecture trained with heavy augmentation and equipped with sophisticated regularization methods. We test the approaches on a corpus of X-ray images to design an anatomy detection system.",
    "creator" : "LaTeX with hyperref package"
  }
}