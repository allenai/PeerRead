{
  "name" : "1606.08501.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Symmetric and antisymmetric properties of solutions to kernel-based machine learning problems",
    "authors" : [ "Giorgio Gnecco" ],
    "emails" : [ "giorgio.gnecco@imtlucca.it" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Keywords: Kernel Methods, Symmetry and Antisymmetry, Pairwise Support Vector Machines, Sequential Minimal Optimization, Optimal and Suboptimal Solutions"
    }, {
      "heading" : "1. Introduction",
      "text" : "In recent years, optimization theory has found several applications in machine learning problems: often, these are formulated as optimization problems in which, given a finite set of empirical data (e.g., a finite set of feature vectors, where each feature may represent the output of a measurement process), it is required to find a parameter vector associated with a model of such data, which is optimal according to a suitable performance index. In the context of the so-called kernel methods (Shawe-Taylor and Cristianini, 2004), examples of such problems are supervised (Cucker and Smale, 2002), unsupervised (von Luxburg, 2007), and semi-supervised (Belkin and Niyogi, 2006; Melacci and Belkin, 2011) learning, and identification of models of dynamical systems (Pillonetto et al., 2014). The performance index is typically composed of the sum of an empirical term (which measures the fitness of the model associated with a specific parameter vector in explaining the empirical data), and a regularization term, whose goal is to endow the optimal model with the capability of generalizing its predictions to new data, not used during the training of the model.\nAdditional a-priori/a-posteriori knowledge can be inserted into the optimization problem above, either requiring the satisfaction of given constraints, or penalizing in a suitable way their violation: see, e.g., the works (Gnecco et al., 2013, 2014, 2015a,b), which describe a novel approach to machine\nar X\niv :1\n60 6.\n08 50\n1v 2\n[ cs\n.L G\n] 2\n8 O\nct 2\nlearning called learning from constraints and based, respectively, on hard and soft formulations of such constraints. Indeed, several experiments, such as those reported in (Diligenti et al., 2012, Section 6), show that taking into account constraints representing additional knowledge valid for a specific machine learning problem can improve the generalization capability of the learned model. Another possible way to deal with constraints on the optimal solution of a machine learning problem is to check a-posteriori if such constraints are satisfied by its optimal solution, e.g., as a consequence of necessary optimality conditions for the specific optimization problem modeling learning. This can be also considered as a particular case of learning from constraints, and is the approach followed in this work.\nContributions. Within this framework, the goal of this paper consists in investigating a particular instance of machine learning problems belonging to the learning from constraints framework. More precisely, we investigate the case of constraints associated, respectively, with symmetry and antisymmetry of the optimal solution (with respect to a suitable transformation of the feature vectors). Such constraints are of interest, e.g., for supervised learning problems in which each supervised example is associated with two objects, as in pairwise classification (e.g., in recognizing whether two face images belong to the same person or not) (Brunner et al., 2012), and in supervised learning of preference relations (i.e., in learning an order among objects through supervised pairs of examples) (Herbrich et al., 1998). Indeed, in the first case, the label does not depend on the order of the objects (e.g., two face images) in the pair, hence also the classification produced by the trained learning machine should not depend on such an order, and a symmetry constraint is needed. In the second case, instead, the label changes sign by changing the order of the objects (e.g., two football teams that one wants to compare in performance), hence also the output of the trained learning machine should change sign, and an antisymmetry constraint is required. After introducing in Section 2 the kind of machine learning problems considered in the paper, and some basic notation in Section 3, the paper starts with the investigation of two methods to impose the satisfaction of symmetry/antisymmetry constraints, in the context of kernel methods, focusing on binary classification: the use of either symmetric or antisymmetric training sets (together with the choice of a suitable kernel, see Section 4.1), and the use of a reduced training set (together with the choice of another suitable kernel, see Section 4.2). In both cases, the elements of the training sets are ordered pairs of objects, together with their labels. Since such ordered pairs can be associated in an one-to-one way with directed arcs on a graph, the machine learning problems investigated in the paper have also application on the binary classification of directed arcs on graphs, using, for each directed arc, a set of features associated with its nodes. In this case, imposing the symmetry/antisymmetry properties above can be interpreted as assigning, respectively, the same labels and classifications or opposite labels and classifications to the two directed arcs (with opposite orientations) connecting the same pair of nodes, for each of such pairs. The starting point of this part of our investigation is the work (Brunner et al., 2012), in which the only case of symmetry constraints was studied. It is worth mentioning that, according to (Brunner et al., 2012), that work is the first one in which the symmetry of the optimal solution to the training problem of an l1-soft margin binary Support Vector Machine (SVM) classifier with a symmetric training set has been rigorously proved, for a quite general class of kernels. Taking inspiration from the invariance framework of (Király et al., 2014), we specialize this result to the presence of both individual features and two kinds of group features, then we extend it to the case of an antisymmetric training set, showing the antisymmetry of the optimal solution. This part of the investigation is mainly based on duality\ntheory in optimization (Bertsekas, 1999) and on representer theorems for SVMs (Shawe-Taylor and Cristianini, 2004). We also employ existence and uniqueness results from the theory of SVMs (Burges and Crisp, 2000), and we introduce a kernel (called in the paper skew-balanced kernel), related specifically with the antisymmetry constraint. Additionally, in Section 5, we investigate how to impose the symmetry and antisymmetry constraints when looking for suboptimal solutions to the optimization problems investigated in the paper, analyzing the behavior of a specific algorithm proposed in the literature (Platt, 1999) from this viewpoint (i.e., examining if its steps preserve or not the symmetry and antisymmetry properties). Up to our knowledge, this kind of investigation is completely novel in the literature about kernel methods. In Section 6, we provide numerical results, which are in accordance with the ones obtained theoretically. Such results also demonstrate the practical advantage of using kernels enforcing symmetry/antisymmetry of the optimal solution to the specific machine learning problem (when the model generating the data labels also satisfies one of such properties), as compared with kernels not enforcing symmetry/antisymmetry. In Section 7, we also show how our analysis can be extended to other kernel methods, namely, to support vector regression (Section 7.1) and to transductive SVMs (Section 7.2), whose application in (Dardard et al., 2016) inspired part of the theoretical investigations of the present work, providing numerical results that showed the occurrence of the antisymmetry property, for a specific choice of the kernel (the linear kernel), and to which we refer for real-world examples of individual and group features in a motion-capture context. No such theoretical analysis was performed in (Dardard et al., 2016), where the antisymmetry property was only observed a-posteriori, after training the learning machine. Finally, we outline some extensions of our analysis to the problem of arc classification on graphs, a problem for which the application of pairwise kernels was not considered in (Brunner et al., 2012). For this specific case, in Sections 7.3 and 7.4, we also define suitable graph kernels related to features associated with several pairs of nodes in the graph (including diffusion kernels on an auxiliary graph), again with the goal to impose either symmetry or antisymmetry to the optimal solution of the associated machine learning problem. More specifically, we show how to construct graph kernels inheriting the properties of the kernels considered in the first part of the paper, as such properties are useful to impose symmetry/antisymmetry constraints.\nRelated works. Besides the work (Brunner et al., 2012) mentioned above, a related paper is (Király et al., 2014), which provides the general structure of kernels incorporating prior knowledge represented by algebraic invariances. Although in principle the theory developed therein (particularly, its Theorem 3 (i)) could be used also to generate kernels satisfying the specific symmetry property considered in the present paper (but not to generate kernels satisfying, instead, the antisymmetry property), the discussion of permutation invariances needed for that purpose is only briefly detailed in Section 2.5.6 of that paper. Moreover, even though it is related to the G-invariant kernels of (Király et al., 2014), the order-invariant kernel considered in this paper is not a particular case of the former, since, in the context of the present paper, the symmetry property for a symmetric training set is not associated only with the choice of the kernel, but also with the existence of optimal symmetric dual variables. Another related work is (Vedaldi et al., 2011), where a novel approach, based on a convex relaxation of a more general nonconvex optimization problem, is proposed to incorporate invariance and equivariance in the SVM training problem. However, such an approach does not guarantee these properties for all possible choices of its parameters, as demonstrated by the numerical results presented therein. Likewise in the present paper, symmetric and antisymmetric pairwise kernels have been also considered in (Waegeman et al., 2012), which presents sufficient conditions under which\nthe use of a symmetric (respectively, antisymmetric) pairwise kernel makes it possible to approximate arbitrarily well any symmetric (respectively, antisymmetric) continuous function on a compact set. However, neither the extensions of the results of (Brunner et al., 2012) mentioned above are provided in (Waegeman et al., 2012), nor an investigation of the issue of symmetry/antisymmetry preservation of suboptimal solutions to the learning problem. For what regards the antisymmetry constraint, the present paper extends the setting of (Herbrich et al., 1998), by allowing for more general kernels, and providing several additional theoretical results (e.g., investigation of the relation between two different ways of imposing antisymmetry, construction of generic skew-balanced kernels starting from order-invariant kernels, antisymmetry preservation in suboptimal solutions to the binary classification problem, extension to regression and transdusctive SVMs). Finally, for what concerns the problem of arc classification on graphs, we recall that pairwise kernels are among the techniques that have been proposed for this purpose in the literature about social network analysis, other methods being based either on a Bayesian approach, or on a linear algebraic formulation (see the recent survey (Al Hasan and Zaki, 2011) for more details on all the three methods). In this context, an advantage of pairwise kernels is that they allow to integrate easily several features of possibly different nature, which can express, e.g., properties of the single nodes (individual features), or of the arcs joining them (group features). However, it has to be mentioned that all the pairwise kernel methods described in (Al Hasan and Zaki, 2011) refer only to the case of undirected arcs.\nOrganization of the paper. The paper is structured as follows. Section 2 illustrates briefly the kind of machine learning problems investigated in the paper. Section 3 lists some kernels that are used in the other sections. Section 4 investigates two methods to impose symmetry and antisymmetry constraints on the optimal solution of a kernel-based machine learning problem, focusing on the case of the training of an l1-soft margin binary SVM classifier. Section 5 investigates a specific algorithm (one version of the Sequential Minimal Optimization (SMO) algorithm) from the point of view of symmetry/antisymmetry preservation at each of its iterations. Section 6 provides a numerical validation of the theoretical results. Section 7 provides several extensions of the analysis, namely, to support vector regression, to transductive SVMs, and to several graph kernels, including diffusion kernels. Finally, Section 8 concludes the paper. All the proofs are reported in the Appendix."
    }, {
      "heading" : "2. Symmetry and antisymmetry constraints",
      "text" : "In this section, we provide a definition of the symmetry and antisymmetry constraints examined in the paper, together with an overview of the kind of problems examined.\nIn the following, given an ordered pair (a, b) of objects a, b, belonging to some space of objects O, and with a 6= b, we use the symbols xa, xb ∈ Rn1 to denote the row vectors of their individual features, and the symbol xab ∈ Rn2 (x̃ab ∈ Rn3 , respectively) to denote a row vector of group features that does not change at all (changes only in sign, respectively) after an exchange of order between the two objects of the ordered pair. Particularly simple examples of the three kinds of features are, respectively, position vectors, their Euclidean distance, and the relative position of the second object of the ordered pair with respect to the first one. We refer to (Dardard et al., 2016), for some more sophisticated examples of such individual and group features, i.e., those arising in the particular context of motion capture (see also footnote 23, reported in the last section).\nWith the notation above in mind, for n = 2n1 + n2 + n3, we associate the feature vector\nXab := (xa, xab, x̃ab, xb) ∈ Rn (1)\nto the ordered pair (a, b), and the feature vector\nXba := (xb, xba, x̃ba, xa) = T Xab ∈ Rn (2)\nto the ordered pair (b, a), where T : Rn → Rn is the operator defined by\nT (xa, xab, x̃ab, xb) := (xb, xab,−x̃ab, xa) .\nFocusing for the moment on the case of binary classification, and denoting by (c, d) another generic ordered pair of distinct objects c and d, and by f◦(Xcd) its classification produced by the trained classifier1, one may want to impose, depending on the specific classification task, either the condition\nf◦(Xcd) = f ◦(Xdc) (= f ◦(T Xcd)) (3)\n(symmetry constraint) or the condition\nf◦(Xcd) = −f◦(Xdc) (= −f◦(T Xcd)) (4)\n(antisymmetry constraint). In case of a regression problem, yab, yba ∈ R, f◦ denotes the obtained regression function, and the symmetry and antisymmetry constraints take the same form as above. It is worth remarking that, expressing the conditions (3) and (4) in terms of the operator T , is closely related to the invariance framework of (Király et al., 2014). As an example, and as already mentioned in Section 1, the two constraints can be used, respectively, in pairwise classification, and in supervised learning of preference relations.\nIn the paper, we investigate how to impose these kinds of constraints (either at an optimal solution of the learning problem, or even at any element of a sequence of suboptimal solutions generated by a suitable algorithm), focusing on the case in which the function f◦ is obtained through a kernel method2 (specifically, SVM classification and regression). As an example, in Section 4, taking the hint from (Brunner et al., 2012) (where the only condition (3) was considered), we consider two ways to impose condition (4) to the trained binary SVM classifier: the use of an antisymmetric training set (together with the choice of a suitable “order-invariant” kernel, see Section 4.1), and the use of a reduced training set (together with the choice of another suitable “skew-balanced” kernel, see Section 4.2). For completeness, in that section we report also analogous results (stated in terms of order-invariant and balanced kernels) related to the symmetry condition (3), which, for the particular problem considered therein, are specializations of the results obtained in (Brunner et al., 2012) to the case in which the\n1. As it is usual in machine learning problems, we distinguish between the labels that are assigned externally by a teacher (denoted, in Section 4, by yab, yba ∈ {−1,+1}, for each of the two ordered pairs (a, b) and (b, a), respectively), and the classifications f◦(Xcd), f\n◦(Xdc) ∈ {−1,+1} produced by the learning machine at the end of its training. 2. We recall that kernel methods are based on a function, called kernel, which makes it possible to express inner\nproducts between images of input vectors in a possibly infinite-dimensional feature space, obtained by applying a suitable (typically nonlinear) mapping. Then, a linear method (e.g., a linear classifier) is applied in that feature space. As such, kernel methods are able to generalize to the nonlinear case several other linear methods used in machine learning. Moreover, they are based on a strong theoretical foundation provided by Statistical Learning Theory (Vapnik, 1998), which allows one to derive bounds on the generalization error of the trained model.\noperator T is used to relate the ordered pairs (a, b) and (b, a). In Section 5, we study how to impose either condition (3) or (4) to any element of the sequence of suboptimal solutions to the binary SVM classification problem generated by a particular optimization algorithm. Numerical results supporting the theory are provided in Section 6. Such results demonstrate the practical importance of imposing either condition (3) or (4) when the model generating the data labels satisfies it, and kernel methods are used. Finally, Section 7 reports extensions of the analysis to other kernel methods: in particular, Section 7.1 deals with the extension of the results contained in Section 4 to SVM regression, Section 7.2 provides an extension of the methodology introduced in Section 5 to the analysis of another optimization algorithm used to train transductive SVMs, whereas Sections 7.3 and Section 7.4 shows how to construct two different kinds of graph kernels inheriting the properties of the kernels considered in Section 4, as such properties are useful to impose either condition (3) or (4).\nRemark 1 We conclude the section with the following considerations about individual and group features. In some cases, group features can be derived straightforwardly from individual features: e.g., if xa and xb are position vectors, then their Euclidean distance xab has the expression ‖xb − xa‖2, where ‖ · ‖2 is the l2-norm, while the relative position of the second object with respect to the first one is x̃ab = xb − xa. Similarly, if a bag-of-features (also called bag-of-words (Shawe-Taylor and Cristianini, 2004)) approach is used (i.e., each individual feature represents the frequency of occurrence of some “word”), one could define simple group features such as sums and differences of corresponding individual features. More generally, by applying suitable kernels (e.g., the tensor pairwise learning kernel reported later in Section 3), one can map individual feature vectors to group feature vectors in the feature spaces E associated with such kernels, which makes some examples of kernels reported in (Brunner et al., 2012) useful to deal with group feature vectors constructed starting from individual feature vectors. Finally, individual features can be interpreted as group features depending only on one object of the ordered pair. Then, one may conclude that only one kind of features (either individual or group features) is really needed. However, it is still useful to make a distinction between individual and group features, because group features could be derived from individual features also in a not straightforward way (this is, e.g., the case of the motion-related group features considered in (Dardard et al., 2016), which were obtained through several time-series analysis). In such a situation, it would be more natural to include them directly as inputs to the kernel, rather than to construct an equivalent kernel, which takes as inputs only the individual features, and deals with such specific group features implicitly in its feature space E, or using the mapping from the individual to the group features (if such a mapping is available), and the kernel itself results from the composition of such a mapping and a base kernel. Another case in which it is not possible to reduce group features to individual features is when the particular individual features from which such group features depend are not available (this does not exclude the case that, at the same time, other individual features, potentially useful for the specific machine learning problem, are still available)."
    }, {
      "heading" : "3. Preliminaries: some typologies of pairwise kernels",
      "text" : "To impose the symmetry and antisymmetry constraints (3) and (4), we need to introduce some basic notation about the following kinds of kernels, which are used extensively in the next sections:\n(a) pairwise kernel: given a (possibly infinite-dimensional) Euclidean space E (feature space) and a nonlinear mapping φ : Rn → E, the pairwise kernel K : Rn×Rn → R represents the inner product\nin the feature space E between images of vectors in Rn under the mapping φ, according to the following definition:\nK (Xab, Xcd) := 〈 φ (Xab) , φ (Xcd) 〉 E .\nIt is also symmetric, in the sense that\nK (Xab, Xcd) = K (Xcd, Xab) ,\nwhich follows from the definition of inner product. Moreover, since it represents an inner product, it is positive semi-definite.\n(b) balanced kernel: it is a pairwise kernel Kb that satisfies the additional property\nKb (Xab, Xcd) = K b (Xab, Xdc) ( = Kb (Xab, T Xcd) ) ,\ni.e., it is invariant under any exchange of order (c, d)→ (d, c);\n(c) skew-balanced kernel: it is a pairwise kernel Ks that satisfies the additional property\nKs (Xab, Xcd) = −Ks (Xab, Xdc) (= −Ks (Xab, T Xcd)),\ni.e., it changes in sign under any exchange of order (c, d)→ (d, c);\n(d) order-invariant kernel: it is a pairwise kernel Ko that satisfies the additional property\nKo (Xab, Xcd) = K o (Xba, Xdc) (= K o (T Xab, T Xcd)),\ni.e., it is invariant under any two simultaneous exchanges of order (a, b)→ (b, a) and (c, d)→ (d, c).\nCase (a) is just the application of the standard machine-learning definition of (symmetric and positive semi-definite) kernel (Shawe-Taylor and Cristianini, 2004) to the situation in which each feature vector represents an ordered pair of objects (rather than, e.g., a single object), whereas cases (b), (c), and (d) impose additional invariance properties (cases (b), and (d) also follow as special cases of the invariance framework considered in (Király et al., 2014)). Moreover, cases (a), (b) and (d) are specifications (due to the inclusion of both individual and group features, and the use of the operator T ) of the corresponding cases investigated in (Brunner et al., 2012)3., whereas case (c) extends in a similar way the one recently introduced independently in (Pahikkala et al., 2015)4.\nAs already observed in (Brunner et al., 2012), it follows from the definitions of pairwise kernels, balanced kernels, and order-invariant kernels, that every balanced kernel is also an order-invariant kernel. Some examples of pairwise kernels that are order-invariant but not balanced (according to\n3. We have used the term order-invariant for the case (d), for which no specific term was used in (Brunner et al., 2012). The term permutation invariant is used in (Pahikkala et al., 2015). 4. In (Pahikkala et al., 2015), this kernel has been introduced for a different investigation than ours, as it is related to spectral properties of a suitable linear operator associated with the kernel.\nthe definitions reported above in this section, in which Xab, Xba and Xcd, Xdc are further related by Xba = T Xab and Xdc = T Xcd) are the linear kernel:\nKolin (Xab, Xcd)\n:= 〈xa, xc〉Rn1 + 〈xab, xcd〉Rn2 + 〈x̃ab, x̃cd〉Rn3 + 〈xb, xd〉Rn1 = 〈xb, xd〉Rn1 + 〈xab, xcd〉Rn2 + 〈(−x̃ab), (−x̃cd)〉Rn3 + 〈xa, xc〉Rn1 = 〈xb, xd〉Rn1 + 〈xba, xdc〉Rn2 + 〈x̃ba, x̃dc〉Rn3 + 〈xa, xc〉Rn1 = Kolin (Xba, Xdc) ,\nthe homogeneous polynomial kernel of order m (where m is a positive integer):\nKopold (Xab, Xcd) := (K o lin (Xab, Xcd)) m , (5)\nand, for σ > 0, the Gaussian kernel:\nKoGauss (Xab, Xcd)\n:= exp  − ( ‖xa − xc‖22 + ‖xab − xcd‖22 + ‖x̃ab − x̃cd‖22 + ‖xb − xd‖22 )\n2σ2\n  .\nIt is worth remarking that linear, polynomial and Gaussian kernels are also considered both in (Brunner et al., 2012) and in (Király et al., 2014). An example of a pairwise kernel that is not order-invariant is given later in Section 6.\nLikewise in (Brunner et al., 2012), starting from any order-invariant kernel Ko (associated with the mapping φo) and exploiting its symmetry, it is possible to define a balanced kernel Kb in the following way:\nKb (Xab, Xcd)\n:= 1\n2 (Ko (Xab, Xcd) +K o (Xba, Xcd))\n= 1\n4\n( Ko (Xab, Xcd) +K o (Xba, Xcd) +K o (Xba, Xdc) +K o (Xab, Xdc) )\n=\n〈 1\n2\n( φo (Xab) + φ o (Xba) ) , 1\n2\n( φo (Xcd) + φ o (Xdc) )〉\nE\n, (6)\nwhere the first equality above is obtained by exploiting the order invariance of Ko, and the second one by using the definition of pairwise kernel, and collecting terms. The last line in (6) shows that Kb is associated with the mapping φb : Rn → E defined by\nφb (Xab) := 1\n2\n( φo (Xab) + φ o (Xba) ) ,\nhence, Kb really expresses an inner product in the feature space E, and is symmetric.\nSimilarly, starting from the order-invariant kernelKo, it is possible to define a skew-balanced kernel Ks in the following way:\nKs (Xab, Xcd)\n:= 1\n2 (Ko (Xab, Xcd)−Ko (Xba, Xcd))\n= 1\n4\n( Ko (Xab, Xcd)−Ko (Xba, Xcd)−Ko (Xab, Xdc) +Ko (Xba, Xdc) )\n=\n〈 1\n2\n( φo (Xab)− φo (Xba) ) , 1\n2\n( φo (Xcd)− φo (Xdc)\n)〉\nE\n, (7)\nwhere the last line shows that Ks is associated with the mapping φs : Rn → E defined by\nφs (Xab) := 1\n2\n( φo (Xab)− φo (Xba) ) .\nHence, Ks really expresses an inner product in the feature space E, and is symmetric. To exclude the trivial kernel Ks ≡ 0, the order-invariant kernel Ko in (7) is required not to be balanced (which is, as already observed, a particular case of order-invariant kernel).\nIn the following sections, to simplify the notation, in a similar way as in (Brunner et al., 2012), we use the shortcuts Koab,cd := K o (Xab, Xcd) , K b ab,cd := K b (Xab, Xcd) , and K s ab,cd := K s (Xab, Xcd) .\nRemark 2 Other pairwise kernels that are also balanced (or skew-balanced) according to the definitions reported in the paper can be constructed by composition with standard kernels, applied to subvectors of the feature vectors. More precisely, given a symmetric positive semi-definite kernel k1 : Rn1 ×Rn1 → R applied to the individual feature vectors xa, xb, xc, xd, starting from the balanced kernel\nKbDL ((xa, xb) , (xc, xd)) := 1\n2 (k1(xa, xc) + k1(xa, xd) + k1(xb, xc) + k1(xb, xd))\n(called direct sum learning pairwise kernel (Bar-Hillel et al., 2004)), one can define the balanced kernel\nK̂bDL (Xab, Xcd) := K b DL ((xa, xb) , (xc, xd)) + k2 (xab, xcd) + k3 (x̃ab, x̃cd) , (8)\nwhere k2 : Rn2 × Rn2 → R and k3 : Rn3 × Rn3 → R are symmetric positive semi-definite kernels, with k3 satisfying the additional property\nk3 (x̃ab, x̃cd) = k3 (x̃ab, x̃dc) = k3 (x̃ab,−x̃cd) (9)\n(which holds, e.g., when k3 is a homogeneous polynomial kernel of even order m, see formula (5)). Similarly, one can also define the skew-balanced kernel\nK̂sDL (Xab, Xcd) := ( KbDL ((xa, xb) , (xc, xd)) + k2 (xab, xcd) ) k3 (x̃ab, x̃cd) , (10)\nif k3 satisfies, instead of (9), the additional property\nk3 (x̃ab, x̃cd) = −k3 (x̃ab, x̃dc) = −k3 (x̃ab,−x̃cd) (11)\n(which holds, e.g., when k3 is a homogeneous polynomial kernel of odd order m, see again formula (5)). Finally, similar constructions of balanced/skew-balanced kernels as (8) and (10) can be made, starting from the balanced kernels\nKTL ((xa, xb) , (xc, xd)) := 1\n2 (k1(xa, xc)k1(xb, xd) + k1(xa, xd)k1(xb, xc)) ,\nKML ((xa, xb) , (xc, xd)) := 1\n4 (k1(xa, xc)− k1(xa, xd)− k1(xb, xc) + k1(xb, xd))2 ,\nKTM ((xa, xb) , (xc, xd)) := KTL ((xa, xb) , (xc, xd)) +KML ((xa, xb) , (xc, xd))\n(called, respectively, tensor learning pairwise kernel (Vert et al., 2007), metric learning pairwise kernel (Vert et al., 2007), and tensor metric learning pairwise kernel (Brunner et al., 2012))."
    }, {
      "heading" : "4. Two methods to impose symmetry and antisymmetry constraints on the",
      "text" : "optimal solution of a kernel-based machine learning problem\nIn the following, we illustrate two methods to impose symmetry and antisymmetry constraints on the optimal solution of a kernel-based machine learning problem. Specifically, we focus on the training of an l1-soft margin binary SVM classifier(Shawe-Taylor and Cristianini, 2004, Section 7.2.2) (see Section 7 for extensions)."
    }, {
      "heading" : "4.1 Use of symmetric/antisymmetric training sets",
      "text" : "We denote by I the set of ordered pairs (a, b) used in the training of the learning machine. The label associated with the ordered pair (a, b) is denoted by yab ∈ {−1, 1}. In the following, we assume that every time the training set I contains the ordered pair (a, b), it also contains the ordered pair (b, a). Moreover, we assume that I does not contain ordered pairs of the form (a, a). For C > 0 and an orderinvariant kernel Ko associated with the mapping φo in the feature space E, the primal optimization problem that models the training of the corresponding l1-soft margin binary SVM classifier is:\nminimizew∈E,γ∈R,{ξab∈R:(a,b)∈I} 1\n2 ‖w‖2E + C\n∑\n(a,b)∈I ξab,\ns. t. yab (〈 w, φo (Xab) 〉 E + γ ) ≥ 1− ξab, ∀(a, b) ∈ I,\nξab ≥ 0, ∀(a, b) ∈ I (12)\n(see also (Shawe-Taylor and Cristianini, 2004, Section 7.2.2)). The corresponding dual optimization problem is\nminimize{αab∈R:(a,b)∈I} G(α),\ns. t. 0 ≤ αab ≤ C, ∀(a, b) ∈ I,∑\n(a,b)∈I yabαab = 0, (13)\nwhere\nG(α) := 1\n2\n∑\n(a,b),(c,d)∈I αabαcdyabycdK\no ab,cd −\n∑\n(a,b)∈I αab. (14)\nBoth problems (12) and (13) are convex quadratic optimization problems5. By Weierstrass theorem, each of them admits an optimal solution. It is well-known from the theory of SVMs that any optimal weight vector w◦ of the primal optimization problem (12) is expressed in terms of support vectors, i.e., vectors φo (xab) associated with optimal dual variables α ◦ ab that are different from 0 at dual optimality. More precisely, starting from any optimal solution α◦ to the dual optimization problem (13), one can construct an optimal weight vector w◦ for the primal optimization problem (12) as follows6:\nw◦ := ∑\n(a,b)∈I α◦abyabφ o (xab) . (15)\nAdditionally, it follows from (Burges and Crisp, 2000, Theorem 2) that, even in case of nonuniqueness of the optimal solution to the primal optimization problem (12), the optimal weight vector w◦ is any case unique (nonuniqueness of the optimal solution to such a problem may arise from the only fact that the set of optimal values for the bias γ is a closed interval of the real line). The representation (15) is well-known as the representer theorem for l1-soft margin binary SVM classifiers (see, e.g., (Shawe-Taylor and Cristianini, 2004, Section 7.2.2)).\nThe following lemma, which is used in the proof of the next Theorem 1, specializes (Brunner et al., 2012, Lemma 1) to the case of the specific order-invariant kernels considered in this paper, for which Xab, Xba and Xcd, Xdc are further related by Xba = T Xab and Xdc = T Xcd. Moreover, differently from (Brunner et al., 2012, Lemma 1), the result also considers the case of labels satisfying, for every (a, b) ∈ I, the antisymmetry condition yab = −yba. The lemma provides a specific instance of the representer theorem for training an l1-soft margin binary SVM classifier with a symmetric/antisymmetric training set and an order-invariant kernel, by constraining the coefficients in formula (15) to satisfy α◦ab = α ◦ ba, for all the elements of the training set. Lemma 1 Let Ko be an order-invariant kernel and one of the following conditions hold:\n(a) for all (a, b) ∈ I, yab = yba (symmetry condition); (b) for all (a, b) ∈ I, yab = −yba (antisymmetry condition). Then, there exists an optimal solution α◦ to the dual optimization problem (13) for which\nα◦ab = α ◦ ba, ∀(a, b) ∈ I. (16)\nGiven an optimal solution (w◦, γ◦, {ξ◦ab ∈ R : (a, b) ∈ I}) to the primal optimization problem (12), let\nf◦(Xcd) := sgn (〈 w◦, φo (Xcd) 〉 E + γ◦ ) (17)\nbe the associated classification function, where sgn is the signum function, defined7 as\nsgn(z) :=    −1, if z < 0, 0, if z = 0,\n1, if z > 0.\n5. One difference in our formulation of the dual optimization problem with respect to (Brunner et al., 2012) is the absence of the constraint 0 ≤ αaa ≤ 2C, since the set I does not contain elements of the form (a, a). 6. The result follows by looking for a stationary point of the Lagrangian associated with the primal optimization problem (12), and particularly, by setting to 0 its gradient vector with respect to the weight vector w (see, e.g., (Shawe-Taylor and Cristianini, 2004)). 7. For symmetry reasons, we define sgn(0) := 0 (so, formally, the classification function (17) is not binary in this case, but one can interpret as degenerate the case in which the argument of sgn is 0).\nThe following result extends (Brunner et al., 2012, Theorem 2) to the present case. It provides sufficient conditions under which an optimal solution to the primal optimization problem (12) satisfies the symmetry (respectively, antisymmetry) constraint. Again, for the symmetric case, the result is just a specialization of (Brunner et al., 2012, Theorem 2) to the specific ordered-invariant kernels considered in the paper, whereas the antisymmetric case is new.\nTheorem 1 The following hold.\n(a) Let Ko be an order-invariant kernel and, for all (a, b) ∈ I, let yab = yba. Then, for any ordered pair (c, d) (even outside the training set), all the optimal solutions to the primal optimization problem (12) satisfy\nf◦(Xcd) = f ◦(Xdc) (= f ◦(T Xcd)). (18)\n(b) Let Ko be an order-invariant kernel and, for all (a, b) ∈ I, let yab = −yba. Then, there exists an optimal solution to the primal optimization problem (12) such that, for any ordered pair (c, d) (even outside the training set), one has\nf◦(Xcd) = −f◦(Xdc) (= −f◦(T Xcd)). (19)"
    }, {
      "heading" : "4.2 Use of balanced/skew-balanced kernels",
      "text" : "An alternative way to impose the symmetry/antisymmetry conditions of Theorem 1 to an optimal solution of the primal optimization problem (12) consists in solving, respectively, the following two variations (a) and (b) of the dual optimization problem (13). In the following, for every unordered pair of distinct objects a, b, we fix one of the two possible orders (say, either (a, b) or (b, a), but not both), and we denote by J the set of ordered pairs obtained by this procedure.\nLet Ko be any order-invariant kernel, and consider the following optimization problems.\n(a) Case of symmetric labels:\nminimize{βab∈R:(a,b)∈J} H b(β),\ns. t. 0 ≤ βab ≤ 2C, ∀(a, b) ∈ J,∑\n(a,b)∈J yabβab = 0, (20)\nwhere Hb(β) := 12 ∑ (a,b),(c,d)∈J βabβcdyabycdK b ab,cd − ∑ (a,b)∈J βab, and\nKbab,cd := 1\n2\n( Koab,cd +K o ba,cd ) . (21)\n(b) Case of antisymmetric labels:\nminimize{βab∈R:(a,b)∈J} H s(β),\ns. t. 0 ≤ βab ≤ 2C, ∀(a, b) ∈ J, (22)\nwhere Hs(β) := 12 ∑ (a,b),(c,d)∈J βabβcdyabycdK s ab,cd − ∑ (a,b)∈J βab, and\nKsab,cd := 1 2 ( Koab,cd −Koba,cd ) . (23)\nThe two kernels Kb and Ks in (21) and (23) are, respectively, the balanced kernel defined in (6), and the skew-balanced kernel defined in (7). Both problems (20) and (22) are convex quadratic optimization problems. By Weierstrass theorem, each of them admits an optimal solution. It is also interesting to observe that the optimization problem (20) is the dual of a primal optimization problem of the form (12), with I replaced by J , C by 2C, and φo by φb, whereas the optimization problem (22) is the dual of a primal optimization problem of the form (12) but without the bias γ as an optimization variable (more precisely, with such a bias set to 0)8, and with I replaced by J , C by 2C, and φo by φs. For this reason, likewise in Section 4.1, the classical representer theorem of l1-soft margin binary SVM classifiers applies without changes, i.e., the optimal weight vectors to both primal optimization problems are represented in terms of support vectors.\nThe following result specializes (Brunner et al., 2012, Theorem 3) to the specific order-invariant kernels of this paper in the symmetric case, and extends in the antisymmetric case. It shows that, in order to obtain the optimal classifier (17) - apart from its optimal bias9 γ◦ - for the case of order-invariant kernels and symmetric and antisymmetric labels, respectively, one can solve the dual optimization problem (13) or equivalently, respectively, the optimization problems (20) and (22).\nTheorem 2 The following hold.\n(a) Let α◦ and β◦ denote optimal solutions, respectively, to the dual optimization problem (13) and to the optimization problem (20). Then, for any ordered pair (c, d) of objects (even outside the training set), one has\n〈 w◦, φo (Xcd) 〉 E = ∑\n(a,b)∈I α◦abyabK o ab,cd =\n∑\n(a,b)∈J β◦abyabK b ab,cd. (24)\nMoreover, the optimal values of the objectives of the optimization problems (13) and (20) are the same.\n(b) Let α◦ and β◦ denote optimal solutions, respectively, to the dual optimization problem (13) and to the optimization problem (22). Then, for any ordered pair (c, d) of objects (even outside the training set), one has\n〈 w◦, φo (Xcd) 〉 E = ∑\n(a,b)∈I α◦abyabK o ab,cd =\n∑\n(a,b)∈J β◦abyabK s ab,cd. (25)\nMoreover, the optimal values of the objectives of the optimization problems (13) and (22) are the same.\nRemark 3 From a computational point of view, the optimization problems (20) and (22) involve one half of the (ordered pairs of) training examples needed to solve the dual optimization problem (13). Moreover, the kernel matrices of (20) and (22) have a number of elements which is four times smaller\n8. Indeed, the condition ∑\n(a,b)∈I yabαab = 0 in the dual optimization problem (13) arises from setting to 0 the partial\nderivative with respect to γ of the Lagrangian associated with the primal optimization problem (12) (see, e.g., (Shawe-Taylor and Cristianini, 2004)). Such a condition is absent if γ is not included as an optimization variable in the primal optimization problem (12).\n9. However, once the term 〈 w◦, φo (Xcd) 〉 E has been computed, it is well-known that one can easily find an optimal\nbias γ◦ in (12), e.g., by applying the Karush-Kuhn-Tucker (KKT) conditions.\nthan the one of (13), even though the evaluation of Kb and Ks involves two evaluations of Ko. To save memory space, one can cache the individual features (likewise in (Brunner et al., 2012)), then compute the kernel values every time they are needed, only if the group features can be easily computed from the pairs of individual features. Otherwise, one has either to store also the group features, or to store the whole kernel matrix associated with the given set of (ordered pairs of) training examples. Since each ordered pair can be interpreted as a directed arc on a graph, in practice the last two approaches are feasible only for sparse graphs, i.e., graphs whose number of edges is, e.g., linear with respect to the number of nodes."
    }, {
      "heading" : "5. Analysis of the Sequential Minimal Optimization algorithm from the point of",
      "text" : "view of symmetry/antisymmetry preservation at each iteration\nIn this section, we focus on the investigation of how it is possible to obtain either symmetric or antisymmetric suboptimal solutions to the primal and dual optimization problems (12) and (13), focusing on the behavior of a commonly-used algorithm to solve the dual problem (13), which is the Sequential Minimal Optimization (SMO) algorithm (Platt, 1999). We recall that, in each of its iterations, both primal and dual feasibility are guaranteed, whereas the Karush-Kuhn-Tucker (KKT) conditions may be violated in case of suboptimality of the current solution. At each iteration t ≥ 1, such an algorithm fixes all the dual variables to the previous values except from two of them, then solves the resulting two-variable optimization problem (in closed form, which is one of the main advantages of the algorithm with respect to other ones), resulting in a new feasible dual solution, which improves the value of the objective when the previous feasible dual solution is suboptimal. A modified version of such an algorithm, based on a suitable selection of the two dual variables to be re-optimized at each iteration (Keerthi et al., 2001), is guaranteed to converge asymptotically to an optimal solution of the dual problem (13) (Lin, 2001, 2002) and has been implemented in the software LIBSVM (Chang and Lin, 2011). More precisely, in this case, adapting the notation to the dual problem (13), the dual variables αa(t,1)b(t,1) , αa(t,2)b(t,2) selected at the iteration t are the ones that have the maximal violation of the corresponding KKT condition, according to the following formula (see (Lin, 2002, formula (2))):\n( a(t,1), b(t,1) ) ∈ argmax ({ −F ( α (t−1) ab ) : yab = 1, α (t−1) ab < C } , { F ( α (t−1) ab ) : yab = −1, α(t−1)ab > 0 }) , ( a(t,2), b(t,2) ) ∈ argmin ({ F ( α (t−1) ab ) : yab = −1, α(t−1)ab < C } , { −F ( α (t−1) ab ) : yab = 1, α (t−1) ab > 0 }) , (26)\nwhere F ( α (t−1) ab ) := ∑\n(c,d)∈I yabycdK\no ab,cdα (t−1) cd − 1, (27)\nand α(t−1) denotes the feasible dual solution coming from the previous iteration. In this framework, it is interesting to examine whether, starting from any initial feasible dual solution α(0) satisfying the symmetry condition\nα (0) ab = α (0) ba , ∀(a, b) ∈ I, (28)\n(which holds, e.g., if α(0) = 0, which is always feasible for the dual problem (13)), a similar symmetry condition\nα (t) ab = α (t) ba , ∀(a, b) ∈ I, (29)\nholds at each iteration t of the version of the SMO algorithm described by the selection rule (26) above (possibly applied, in the symmetric case, not directly to the dual problem (13), but to the equivalent one (20), since it has a similar form). In such a case, also the primal feasible solution obtained at the iteration t, i.e.,\nw(t) := ∑\n(a,b)∈I α (t) ab yabφ o (Xab) , (30)\nand the corresponding bias γ(t) determined at the same iteration (Platt, 1999, Section 12.2.3), are such that the associated classification function\nf (t)(Xcd) := sgn (〈 w(t), φo (Xcd) 〉 E + γ(t) ) (31)\nsatisfies, for any ordered pair of objects (c, d),\nf (t)(Xcd) = f (t)(Xdc) ( = f (t)(T Xcd) ) (32)\nif, for all (a, b) ∈ I, one has yab = yba, or\nf (t)(Xcd) = −f (t)(Xdc) ( = −f (t)(T Xcd) ) (33)\nif, for all (a, b) ∈ I, one has yab = −yba. As it is shown in our following result, one can indeed make the condition (29) hold for every iteration t.\nTheorem 3 The following hold.\n(a) Let Ko be an order-invariant kernel and, for all (a, b) ∈ I, let yab = yba. Let the version of the SMO algorithm described by the selection rule (26) be applied to the optimization problem (20), and denote by β(t) its solution generated at the iteration t. Moreover, let α(t) be defined as the vector with components\nα (t) ab :=    β (t) ab 2 , if (a, b) ∈ J,\nβ (t) ba 2 , if (b, a) ∈ J.\n(34)\nThen, at each iteration t, α(t) is feasible for the dual optimization problem (13), and satisfies the symmetry condition (29). Moreover, when t tends to +∞, the sequence of the α(t)’s converges to an optimal solution of the dual problem (13).\n(b) Let Ko be an order-invariant kernel and, for all (a, b) ∈ I, let yab = −yba. Let the version of the SMO algorithm described by the selection rule (26) be applied to the optimization problem (13), and suppose that α(0) satisfies the symmetry condition (28). Then, at each iteration t, it is possible to choose ( a(t,1), b(t,1) ) and ( a(t,2), b(t,2) ) according to formula (26) in such a way that α(t) satisfies\nthe symmetry condition (29). Moreover, when t tends to +∞, the sequence of the α(t)’s converges to an optimal solution of the dual problem (13).\nRemark 4 One can notice that Theorem 3 (b) does not preclude the existence of other choices of( a(t,1), b(t,1) ) and ( a(t,2), b(t,2) ) , still selected according to formula (26), for which α(t) does not satisfy the symmetry condition (29). In particular, this could happen when the argmax and argmin sets in formula (26) contain more than one element.\nRemark 5 Theorem 3 shows conditions under which the sequence of suboptimal solutions obtained applying the SMO algorithm satisfies symmetry/antisymmetry properties valid also for the optimal solution. Associated rates of convergence could be obtained by exploiting related results valid for the SMO algorithm. For instance, (Chen et al., 2006) proved linear convergence of SMO under certain assumptions.\nRemark 6 The following is an alternative way to impose the symmetry condition (29) after every iteration of the SMO algorithm with the selection rule (26), when it is applied to the dual optimization problem (13). After generating the possibly asymmetric feasible dual solution α(t), one constructs another feasible dual solution ᾱ(t) defined by ᾱ (t) ab := α (t) ba , for all (a, b) ∈ I, then generates the feasible\ndual solution ᾱ(t) defined by ᾱ(t) := 12 ( α(t) + ᾱ(t) ) which is symmetric by construction, and has the same value of the dual objective (14) as α(t) (as it follows proceeding likewise in the proof of Lemma 1)."
    }, {
      "heading" : "6. Numerical results",
      "text" : "In a first set of numerical simulations, we tested the obtained theoretical results solving the dual problem (13) using LIBSVM, starting from a set of artificial data10. For what concerns real-world applications of such properties, we refer, e.g., to our recent work (Dardard et al., 2016) for an application with real motion capture data. Other real-world applications are mentioned in Section 8.\nIn more details, to obtain the first set of numerical results, we generated the two following scenarios, respectively for the symmetric and antisymmetric case. In both cases, we considered a small number of training examples, just in order to report in the successive tables the optimal dual variables associated with all the support vectors (i.e., in the context of the paper, the ordered pairs of training examples associated with non-zero optimal dual variables), keeping at the same time the size of the tables small.\n(a) In the first scenario (with symmetric labels), we randomly generated, with 8 independent draws, an 8-dimensional feature vector of the form\nXab = (xa, xab, x̃ab, xb) ∈ R8, (35)\nwhere each subvector had 2 components, and all such components were sampled independently according to a normal distribution with mean 1 and standard deviation 1, then we attributed the label 1 to these feature vectors. Then, we repeated the procedure above generating other 8 feature vectors of the form (35), using this time a normal distribution with mean −1 and standard deviation 1, then we attributed the label −1 to these feature vectors. Finally, starting from the 16 feature vectors generated above, we generated other 16 feature vectors of the form\nXba = (xb, xba, x̃ba, xa) = (xb, xab,−x̃ab, xa) ∈ R8, (36)\n10. Notice that the fact of using artificial data here is not a severe limitation, since the goal of this first numerical study is not to compare the classification performance of one proposed machine learning method with other ones, but to investigate the symmetry/antisymmetry properties of the optimal classifier under suitable conditions on the training set and on the kernel. For the same reason, for this first set of simulations, even the relatively small cardinality of the training set and the absence of an explicit test set are not an issue. Nevertheless, for a more realistic example, see the results of the second set of simulations reported later in this section, where a larger training set was considered, together with a detailed evaluation of the generalized performance through both validation and test sets.\nthen we attributed to each of them the same label of the associated feature vector of the form (35).\n(b) In the second scenario (with antisymmetric labels), we randomly generated, with 16 independent draws, an 8-dimensional feature vector of the form (35) where each subvector had 2 components, and all components were sampled independently according to a normal distribution with mean 1 and standard deviation 1, then we attributed the label 1 to these feature vectors. Starting from these feature vectors, we generated other 16 feature vectors of the form (36), then we attributed the label −1 to them.\nIn both cases, we solved the dual problem (13) using LIBSVM, with the choice C = 1 for the penalty parameter, and using, respectively, the linear kernel, the homogeneous polynomial kernel of order m = 3, the Gaussian kernel with σ = 1 (which are all order-invariant kernels, as shown in Section 3), a kernel KP which is not order-invariant 11 (namely, the one defined as\nKP (Xab, Xcd) := XabPX ′ cd, (37)\nwhere P is a 8×8 symmetric positive definite tridiagonal matrix12 in which all the elements in the main diagonal are equal to 1, and all the other non-zero elements are equal to 0.2), and an order-invariant kernel generated from KP , namely,\nKoP (Xab, Xcd) := 1\n2 (KP (Xab, Xcd) +KP (Xba, Xdc)) =\n1\n2\n( XabPX ′ cd +XbaPX ′ dc ) . (38)\nWe also chose a sufficiently small stopping tolerance parameter (i.e., 10−6) in LIBSVM. Tables 1 and 2 show, respectively, for the first scenario and for the second scenario, the indices of the support vectors and the corresponding support vector coefficients, defined as the products of the labels of the support vectors with their associated optimal dual variables. The results show that, with all the four order-invariant kernels above, the SMO algorithm implemented in LIBSVM was able to find, in both scenarios, symmetric optimal dual variables13, which is in accordance with Lemma 1, and that the corresponding support vector coefficients were, respectively, symmetric/antisymmetric. Moreover, for the antisymmetric case, the obtained bias was negligibly small (ranging from the order 10−7 to the order 10−5, for the three kernels), which is in accordance with the choice of the bias considered in the proof of Theorem 1). Hence, due to the order-invariance of the kernels, the properties (18) and (19) were satisfied, respectively, which is in accordance with Theorem 1. When the kernel defined by\n11. Indeed, (1, 1, 1, 1, 1, 1, 1, 1)P (1, 1, 1, 1, 1, 1, 1, 1)′ = 15 6= (1, 1, 1, 1,−1,−1, 1, 1)P (1, 1, 1, 1,−1,−1, 1, 1)′ = 11, which shows that the kernel KP defined in (37) is not order-invariant. 12. A sufficient condition for the positive definiteness of a symmetric tridiagonal matrix A ∈ Rn × Rn is its strict dominance, i.e., the property |Aii| > ∑n j 6=i,j=1 |Aij |, for all i = 1, . . . , n. This sufficient condition follows from\nGersghorin’s circle theorem (Dym, 2007, Section 7.2), which states that all the eigenvalues of A belong, in the complex plane, to at least one of the Gersghorin circles Gi (for i = 1, . . . , n), whose centers and radii are defined, respectively, by Aii and ∑n j 6=i,j=1 |Aij |. 13. One can notice that, due to the small dimension - with respect to the number of training examples in the two scenarios - of the feature space associated with the linear kernel, the corresponding instance of the dual optimization problem (13) admits in general an infinite number of optimal solutions. In particular, it admits optimal solutions with asymmetric optimal dual variables. In such a case, one can still obtain an optimal solution with symmetric optimal dual variables either applying Theorem 3 (b), or following Remark 6. When there is uniqueness of the optimal solution to the dual optimization problem (13), however, there is symmetry of the optimal dual variables, as it follows from Lemma 1.\nformula (37) was used, the optimal dual variables were not symmetric, which was also expected since such a kernel is not order-invariant, hence Lemma 1 could not be applied. Moreover, although it is not reported in the table, we have also verified that in general, for such a kernel, the properties (18) and (19) were not satisfied. Moreover, for this kernel, when the antisymmetric case was considered, the obtained bias was not negligible (about 0.07, between three and five orders larger than for the other four kernels).\nTo investigate the potential benefit of using an order-invariant kernel when the model generating the data labels respects, respectively, the symmetry/antisymmetry constraint, we also performed a second set of simulations, considering two additional scenarios, in which two pairwise kernels were used, only one of which was order-invariant. To simulate a sufficiently complex and possibly realistic situation, we considered two models that generated symmetric/antisymmetric labels for each pair of objects by applying a threshold to the sum of several features, related to each of the individual and group features of the pair through a low-degree polynomial, with the coefficients of each polynomial unknown to the learning machine. In the third scenario, the labels were generated according to a symmetric label-generation model, while in fourth scenario, an antisymmetric label-generation model was used. For a fair comparison, in order to investigate the effect on the test-set classification accuracy of the presence/absence of the order-invariance property of the pairwise kernel, in these two additional\nscenarios we only considered, for several realizations of a randomly chosen14 symmetric and positive semi-definite matrix P , kernels of the form KP (which is not order-invariant with probability 1) and KoP (which is always order-invariant).\nThe models generating the data labels for the two scenarios were constructed as follows. First, two vectors c1, c2 ∈ R2 were randomly generated according to a uniform distribution in [ − 110 , 110 ]2 . Then, two univariate polynomials p1 and p2 of degree 3 were constructed, with coefficients of odd order generated independently according to a uniform distribution in [ − 110 , 110 ] . All the other coefficients of the polynomials were set to 0. Then, each ordered pair (a, b) of objects was associated with 2-dimensional vectors xa and xb of individual features (each components of which was sampled independently according to a normal distribution with mean 0 and standard deviation 125), while the\n14. More precisely, first we generated a random square matrix M ∈ R8×8 (with entries chosen independently according to a uniform distribution in [ − 1\n10 , 1 10\n] ), then we set P =M ′M .\ngroup feature vectors were defined as15 xab = xa + xb, and x̃ab = xb − xa. In the third scenario, the labels were generated according to the following rule (unknown to the learning machine):\nyab = sgn (p1 (〈c1, xa〉R2) + p1 (〈c1, xb〉R2) + p2 (〈c2, xab〉R2)) . (39)\nSimilarly, in the fourth scenario, they were generated as\nyab = sgn (p1 (〈c1, xa〉R2)− p1 (〈c1, xb〉R2) + p2 (〈c2, x̃ab〉R2)) . (40)\nThe constructions above were motivated by the fact that they guarantee the generation of symmetric and antisymmetric labels, respectively. Moreover, for the third scenario, they provide the same average number of positive/negative labels (balancedness of the two classes), whereas for the fourth scenario, the same number of positive/negative labels is guaranteed, since yba = −yab. After some trials with other choice of the parameters, these were fixed to the values reported above, in order to make the average classification problem sufficiently difficult, to minimize the risk of obtaining a classification accuracy near 100% for both kernels KP and K o P .\nIn order to do training and model selection, we first generated 25 objects for both scenarios. Then, for each of the two kernels KP and K o P , the choice of the SVM parameter C was obtained through the following ad-hoc 5-fold cross-validation (with C ranging from 5−3 to 53, assuming 7 logarithmically spaced values), to exclude, for each fold, any overlap between its training and validation sets. More precisely, in each fold, 5 of the 25 objects were discarded, and used to construct the 4 · 5 = 20 ordered pairs needed for the validation in that fold. The remaining 20 objects of the fold were used to construct the 19 ·20 = 380 pairs in its training set. Then, for each of the two kernels KP and KoP , the best value of C resulting from the cross-validation was used to re-train the SVM on a training set made of all the 24 · 25 = 600 pairs of original objects. The final classifiers were then tested on all the 99 · 100 = 9900 pairs of other 100 objects, which were drawn independently from the same probability distribution as the first 25 objects. The whole procedure (which used for training all the available ordered pairs of objects, likewise in the simulations performed in (Brunner et al., 2012)) was repeated 20 times, each time considering different realizations of P, c1, c2, p1, p2, and of the training/test objects. The results are reported in Table 3, respectively, and show that, in almost all cases, the choice of an order-invariant kernel improved the test-set classification accuracy16 of the trained classifier. Indeed, for what concerns the data reported in the first subtable, a better performance was obtained by the order-invariant kernel in 19 cases over 20, while, referring to the data of the second subtable, this occurred in 18 cases over 20. Moreover, for both subtables, the difference in performance was statistically significant, according to a two-sided Wilcoxon signed-rank test (Demšar, 2006) at confidence level α = 0.05, which provided the following p-values: p = 0.0001 < α for the data reported in the first subtable, and p = 0.0015 < α for the ones of the second subtable."
    }, {
      "heading" : "7. Extensions",
      "text" : "In the following, we discuss some extensions of the results obtained in the paper.\n15. For simplicity, in these scenarios, the group features were generated in a very simple way, using the individual features of the objects of the pair (however, see Remark 1 for more complex situations). 16. In this case, due to the balancedness of the two classes, the test-set classification accuracy is a good criterion to evaluate the performance of the classifiers. Otherwise, in other cases, other criteria such as the Area under the Receiving Operating Characteristic Curve (AUC) can be used (Fawcett, 2006)."
    }, {
      "heading" : "7.1 Extension to support vector regression",
      "text" : "Lemma 1 and Theorem 1 in Section 4.1 can be extended to support vector regression with the linear ε-insensitive loss function (described, e.g., in (Shawe-Taylor and Cristianini, 2004, Section 7.3.3)). In that case, for given C, ε > 0, the primal and dual optimization problems have, respectively, the forms\nminimizew∈E,γ∈R,{ξab,ξ̂ab∈R:(a,b)∈I} 1 2 ‖w‖2E + C\n∑\n(a,b)∈I\n( ξab + ξ̂ab ) ,\ns. t. (〈 w, φo (Xab) 〉 E + γ ) − yab ≤ ε+ ξab, ∀(a, b) ∈ I,\nyab − (〈 w, φo (Xab) 〉 E + γ ) ≤ ε+ ξ̂ab, ∀(a, b) ∈ I, ξab, ξ̂ab ≥ 0, ∀(a, b) ∈ I, (41)\nwhere yab, yba ∈ R, and\nminimize{αab,α̂ab∈R:(a,b)∈I} G r(α),\ns. t. 0 ≤ αab, α̂ab ≤ C, ∀(a, b) ∈ I,∑\n(a,b)∈I (αab − α̂ab) = 0, (42)\nwhere\nGr(α) := 1\n2\n∑\n(a,b),(c,d)∈I (αab − α̂ab) (αcd − α̂cd)Koab,cd\n+ ∑\n(a,b)∈I ε (αab + α̂ab)+\n∑\n(a,b),(c,d)∈I (αab − α̂ab) yab.\nIn more details, one obtains the following extension of Lemma 1 to the regression case (for both the symmetric/antisymmetric cases, such an extension was not detailed in (Brunner et al., 2012)).\nLemma 2 Let Ko be an order-invariant kernel and one of the following conditions hold:\n(a) for all (a, b) ∈ I, yab = yba (symmetry condition);\n(b) for all (a, b) ∈ I, yab = −yba (antisymmetry condition).\nThen,\n(a) there exists an optimal solution α◦, α̂◦ to the dual optimization problem (42) for which\nα◦ab = α ◦ ba, α̂ ◦ ab = α̂ ◦ ba, ∀(a, b) ∈ I, (43)\n(b) there exists an optimal solution α◦, α̂◦ to the dual optimization problem (42) for which\nα◦ab = α̂ ◦ ba, α̂ ◦ ab = α ◦ ba, ∀(a, b) ∈ I. (44)\nTheorem 1 extends to the regression case with no changes in the statement, apart from the interpretation of f◦ as a regression function rather than a classification one. We report in the Appendix a sketch of the proofs of such two extensions. Finally, also Theorem 2 in Section 4.2 could be extended in a similar way to the regression case (to save space and avoid introducing new notation, here we do not provide a detailed investigation of this possible extension, which would require to formulate suitable modifications of the optimization problems (20) and (22)).\n7.2 Extension to l1-soft margin transductive SVMs for binary classification of ordered pairs\nUsing the notation of Section 4.1, here we assume that the training set I is decomposed as I := IL∪IU , where the two sets IL and IU are disjoint and refer, respectively, to the supervised and the unsupervised ordered pairs. We also assume that, when (a, b) belongs to IL (respectively, IU ), also (b, a) belongs to IL (respectively, IU ). Then, the l1-soft margin transductive SVM training problem for binary classification (Joachims, 1999), applied here to the case of the classification of ordered pairs using an order-invariant kernel Ko, is formulated as the following optimization problem17:\nminimizew∈E,γ∈R,{ξab∈R:(a,b)∈I},{yab∈{−1,+1}:(a,b)∈Iu} 1\n2 ‖w‖2E + C\n∑\n(a,b)∈I ξab,\ns. t.yab (〈 w, φo (Xab) 〉 E + γ ) ≥ 1− ξab, ∀(a, b) ∈ I,\nξab ≥ 0, ∀(a, b) ∈ I (45)\nThe difference with respect to the l1-soft margin binary SVM classification of ordered pairs with an order-invariant kernel considered in Section 4.1 is the additional presence of unsupervised ordered pairs, and the fact that also their labels are among the optimization variables, which makes (45) a nonconvex\n17. For simplicity and uniformity of notation with Section 4.1, here we assume the same penalty parameter C > 0 for all the supervised and unsupervised ordered pairs, but the following discussion can be extended to the case of different penalty parameters. Moreover, we have removed from the formulation an equality constraint, which is present in the formulation of the problem given in (Sindhwani and Keerthi, 2006), but absent in the original formulation of (Joachims, 1999) (however, see also Remark 7 for an extension to this case).\noptimization problem. However, for every fixed choice of such labels, the problem has exactly the same form as (12). In order to find a local minimum for large-scale instances of the problem above, the following label-switching algorithm was proposed in (Joachims, 1999) (here formulated in the case of ordered pairs, and with an order-invariant kernel). The algorithm first assign temporary labels to all the unsupervised ordered pairs. Then, a suitable label-switching algorithm is used repeatedly to switch, at each iteration, the temporary labels of two selected ordered pairs. The algorithm has been proved in (Joachims, 1999) to converge in a finite number of iterations to a local minimum of (45), and has been extended in (Sindhwani and Keerthi, 2006) to a multiple label-switching algorithm, which has similar performance guarantees.\nIn more details, given a positive integer S, the multiple label-switching algorithm of (Sindhwani and Keerthi, 2006) and its selection rule at the iteration t work as follows:\n1. solve the problem (45) for the current fixed choice y (t) ab of the labels of the unsupervised ordered\npairs (a, b), obtaining the weight vector w(t) and the bias γ(t);\n2. identify unsupervised ordered pairs (a, b) with currently active dual variables αab and currently\npositive (respectively, negative) labels y (t) ab , and insert them in a sorted list L + (respectively, in a sorted list L−), sorting the elements in nondecreasing (respectively, nonincreasing) order with respect to the corresponding outputs, which are 〈 w(t), φo (Xab) 〉 E + γ(t);\n3. pick pairs of elements, one from each list and starting from the top of each list, until either a pair is obtained such that the output of the element from L+ is larger than the output of the element from L−, or S pairs have been picked so far;\n4. if at least one pair of elements from the two lists has been selected in step 3, switch the labels of each pair, and go to the next iteration t+ 1 ; otherwise, terminate the algorithm.\nOur following result shows that, if properly initialized and in the case of symmetric (respectively, antisymmetric) labels for the supervised ordered pairs, the multiple label-switching algorithm applied with an order-invariant kernel produces, at each iteration, solutions that satisfy the symmetry (respectively, antisymmetry) constraint.\nTheorem 4 The following hold.\n(a) Let Ko be an order-invariant kernel and, for all (a, b) ∈ IL, let yab = yba, the initial labels of the unsupevised ordered pairs satisfy, for all (a, b) ∈ IU , y(0)ab = y (0) ba , and the multiple label-switching\nalgorithm be applied with S even, to solve the optimization problem (45). Then, at each iteration t, it is possible to switch labels of some unsupervised ordered pairs according to the multiple labelswitching algorithm in such a way that, for all (a, b) ∈ IU , y(t+1)ab = y (t+1) ba , and the resulting classifier obtained at step 1 of the next iteration t + 1 satisfies, for any ordered pair of objects (c, d), the symmetry constraint\nf (t+1)(Xcd) = f (t+1)(Xdc) ( = f (t+1)(T Xcd) ) . (46)\n(b) A similar statement as (a) holds by removing the condition “S even”, and replacing yab = yba,\ny (0) ab = y (0) ba , and y (t+1) ab = y (t+1) ba with yab = −yba, y (0) ab = −y (0) ba , and y (t+1) ab = −y (t+1) ba , respectively, and the symmetry constraint (46) with the antisymmetry constraint\nf (t+1)(Xcd) = −f (t+1)(Xdc) ( = −f (t+1)(T Xcd) ) . (47)\nRemark 7 When r = 12 , Theorem 4 (b) holds also for the variation of problem (45) that includes the additional equality constraint stating that a fraction r of unsupervised ordered pairs is classified as positive by the trained machine. This is just the additional equality constraint considered in the transductive SVM formulation reported in (Sindhwani and Keerthi, 2006), which was used to obtain the numerical results of (Dardard et al., 2016). Since the kernel used in that paper was the linear kernel (which is order-invariant), and the initial labels were antisymmetric, we conclude that Theorem 4 (b) provides a justification for the observed antisymmetry of the classifier, which was obtained in (Dardard et al., 2016)."
    }, {
      "heading" : "7.3 Extension to feature vectors associated with multiple pairs of nodes in a graph",
      "text" : "As already noticed, the results obtained in the paper have application in the context of the following machine learning problem: given a graph with a possibly large number of arcs, some of which are supervised, identify a possible relationship between a set of features associated with the nodes joined by the same arc (i.e., ordered pair of nodes) and the (possibly binary, for the case of binary classification, otherwise real-valued, for the case of regression) weight of that arc, under the additional prior knowledge provided by the symmetry (or antisymmetry) of the relationship to be discovered. Now, we consider the following extension, which takes more deeply into account the structure of the graph in doing the prediction (another way to do this is reported at the end of this section). Such an extension consists in using as predictors the features associated not only with the nodes joined by the same arc, but also with their neighbors: e.g., in the case of a h-regular graph18 (i.e., a graph in which each node has the same number h of neighbors), and h ≥ 2, one could use, to predict the weight of the arc (a, b), feature vectors of the following form:\n( X\nn (1) a a , . . . , X n (h−1) a a , Xa b, Xb n(1)b , . . . , X b n (h−1) b\n) ∈ R(2h−1)n, (48)\nwhere the nodes n (1) a , . . . , n (h−1) a are the neighbors of a (with the exception of b), the nodes n (1) b , . . . , n (h−1) b are the neighbors of b (with the exception of a), and each subvector X n (l) a a , Xa b, Xb n(l)b has the form (1): e.g., in the case of the ordered pair (n (1) a , a), one has Xn(1)a a := ( x n (1) a , x n (1) a a , x̃ n (1) a a , xa ) . We assume that the classification/regression of the arc (a, b) is invariant with respect to any permutation σ1 of the set of feature subvectors Xn(1)a a , . . . , X n (h−1) a a (without permuting the other feature subvectors Xa b and Xb n(1)b , . . . , X b n (h−1) b ) and to any permutation σ2 of the set of feature subvectors X b n\n(1) b\n, . . . , X b n (h−1) b (again, without permuting the other feature subvectors X n (1) a a , . . . , X n (h−1) a a and\nXa b). This is guaranteed by the following choice of the kernel (compare with item (a) in Section 3):\n18. Here, we consider such an example in order to have a feature vector of fixed dimension, but in principle one could also extend the analysis to feature vectors of different dimensions (see also Remark 8).\n(a’) multiple-pairs kernel: it is a symmetric and positive semi-definite kernel Km, which is defined on R(2h−1)n × R(2h−1)n, and satisfies the additional property\nKm ((\nX n (σ1(1)) a a , . . . , X n (σ1(h−1)) a a , Xa b, Xb n(σ2(1))b , . . . , X b n (σ2(h−1)) b\n) ,\n( X\nn (σ3(1) c c , . . . , X n (σ3(h−1)) c c , Xc d, Xdn(σ4(1))d , . . . , X dn (σ4(h−1)) d\n))\n= Km ((\nX n (1) a a , . . . , X n (h−1) a a , Xa b, Xb n(1)b , . . . , X b n (h−1) b\n) ,\n( X\nn (1) c c , . . . , X n (h−1) c c , Xc d, Xdn(1)d , . . . , X dn (h−1) d\n))\nfor any four (possibly coincident) permutations σ1, σ2, σ3, σ4 of the set {1, . . . , h− 1}. (49)\nIn the following, we investigate conditions under which, in case of a exchange of the order between a and b, the classification/regression of the arc (b, a) as a function of the feature vector\n( X\nn (1) b b , . . . , X n (h−1) b b , Xb a, Xan(1)a , . . . , X an (h−1) a\n) ∈ R(2h−1)n (50)\nis the same as the classification/regression of the arc (a, b) as a function of the feature vector (48) in case of a “cohesion” relationship (i.e., in case of a symmetry constraint), otherwise opposite in sign (but with the same absolute value) in case of a “leadership/followership” relationship (i.e., in case of an antisymmetry constraint)19. To investigate these issues, we consider the following multiple-pairs extensions of the definitions of balanced, skew-balanced, and order-invariant kernels introduced in Section 3 (see items (b), (c), and (d) in that section):\n(b’) multiple-pairs balanced kernel: it is a multiple-pairs kernel Kmb that satisfies the additional property\nKmb ((\nX n (1) a a , . . . , X n (h−1) a a , Xa b, Xb n(1)b , . . . , X b n (h−1) b\n) ,\n( X\nn (1) c c , . . . , X n (h−1) c c , Xc d, Xdn(1)d , . . . , X dn (h−1) d\n))\n= Kmb ((\nX n (1) a a , . . . , X n (h−1) a a , Xa b, Xb n(1)b , . . . , X b n (h−1) b\n) ,\n( X\nn (1) d d , . . . , X n (h−1) d d , Xd c, Xc n(1)c , . . . , X c n (h−1) c\n)) .\n19. See also footnote 23, reported later in Section 8.\n(c’) multiple-pairs skew-balanced kernel: it is a multiple-pairs kernel Kms that satisfies the additional property\nKms ((\nX n (1) a a , . . . , X n (h−1) a a , Xa b, Xb n(1)b , . . . , X b n (h−1) b\n) ,\n( X\nn (1) c c , . . . , X n (h−1) c c , Xc d, Xdn(1)d , . . . , X dn (h−1) d\n))\n= −Kms ((\nX n (1) a a , . . . , X n (h−1) a a , Xa b, Xb n(1)b , . . . , X b n (h−1) b\n) ,\n( X\nn (1) d d , . . . , X n (h−1) d d , Xd c, Xc n(1)c , . . . , X c n (h−1) c\n)) ;\n(d’) multiple-pairs order-invariant kernel: it is a multiple-pairs kernelKmo that satisfies the additional property\nKmo ((\nX n (1) a a , . . . , X n (h−1) a a , Xa b, Xb n(1)b , . . . , X b n (h−1) b\n) ,\n( X\nn (1) c c , . . . , X n (h−1) c c , Xc d, Xdn(1)d , . . . , X dn (h−1) d\n))\n= Kmo ((\nX n (1) b b , . . . , X n (h−1) b b , Xb a, Xan(1)a , . . . , X an (h−1) a\n) ,\n( X\nn (1) d d , . . . , X n (h−1) d d , Xd c, Xc n(1)c , . . . , X c n (h−1) c\n)) . (51)\nWe also assume that, given any training example with a feature vector of the form (48), all the examples with feature vectors generated applying either a permutation of the form σ1 or a permutation of the form σ2 to its components are presented to the learning machine with its same label. In this way, due to the property (49), all these examples are actually dealt with as a single example by the learning machine. So, presenting the feature vector (X\nn (1) a a , . . . , X n (h−1) a a , Xa b, Xb n(1)b , . . . , X b n (h−1) b )\nand its label as inputs to training is equivalent (up to rescaling the penalty parameter C) to presenting them as inputs together with all the ((h− 1)!)2 − 1 admissible permutations of that feature vector, using its same label (which would be computationally unfeasible for large h). As a consequence, it is possible to identify the ordered pair (a, b) with (any of) the feature vector(s) (X\nn (1) a a , . . . , X n (h−1) a a , Xa b, Xb n(1)b , . . . , X b n (h−1) b ), and the ordered pair (b, a) with (any of) the feature\nvector(s) (X n (1) b b , . . . , X n (h−1) b b , Xb a, Xan(1)a , . . . , X an (h−1) a ). In this way, assuming that the training set I contains, for each ordered pair (a, b), also the ordered pair (b, a), the primal and dual optimization problems that model the training of an l1-soft margin binary SVM classifier with a multiple-pairs order-invariant kernel Kmo are, respectively,\nminimizew∈E,γ∈R,{ξab∈R:(a,b)∈I} 1\n2 ‖w‖2E + C\n∑\n(a,b)∈I ξab,\ns. t. yab (〈 w, φmo ( X\nn (1) a a , . . . , X n (h−1) a a , Xa b, Xb n(1) b , . . . , X b n (h−1) b )〉 E + γ\n)\n≥ 1− ξab, ∀(a, b) ∈ I, ξab ≥ 0, ∀(a, b) ∈ I (52)\n(where φmo : R(2h−1)n → E denotes the mapping associated with the multiple-pairs order-invariant kernel Kmo), and\nminimize{αab∈R:(a,b)∈I} G̃(α),\ns. t. 0 ≤ αab ≤ C, ∀(a, b) ∈ I,∑\n(a,b)∈I yabαab = 0, (53)\nwhere\nG̃(α) := 1\n2\n∑\n(a,b),(c,d)∈I αabαcdyabycdK\nmo ab,cd −\n∑\n(a,b)∈I αab, (54)\nand Kmoab,cd is a shortcut for\nKmo ((\nX n (1) a a , . . . , X n (h−1) a a , Xa b, Xb n(1)b , . . . , X b n (h−1) b\n) ,\n( X\nn (1) c c , . . . , X n (h−1) c c , Xc d, Xdn(1)d , . . . , X dn (h−1) d\n)) .\nThese optimization problems have exactly the same forms as (12) and (13), respectively, and the kernel Kmo inherits the properties of the kernel Ko appearing therein. Hence, the results of Sections 4 and 5 are extended directly to this situation in which the predictors are the features associated not only with the nodes joined by the same arc, but also with their neighbors, simply replacing the kernels Kb, Ks, and Ko in those sections by Kmb, Kms, and Kmo (here, we do not report the precise statements, because they are practically identical to the ones contained in those sections).\nRemark 8 The simplest way to guarantee that the kernel Kmo satisfies the property (49) consists in assuming (using an overloaded notation) that it has the functional form\nKmo ((\nX n (1) a a , . . . , X n (h−1) a a , Xa b, Xb n(1) b , . . . , X b n (h−1) b\n) , ( X n (1) c c , . . . , X n (h−1) c c , Xc d, Xd n(1) d , . . . , X d n (h−1) d ))\n= Kmo\n(( 1\nh− 1\nh−1∑\nl=1\nX n (l) a a\n, Xa b, 1\nh− 1\nh−1∑\nl=1\nX b n\n(l) b\n) , ( 1\nh− 1\nh−1∑\nl=1\nX n (l) c c\n, Xc d, 1\nh− 1\nh−1∑\nl=1\nX d n\n(l) d\n)) , (55)\ni.e., that it depends on the subvectors X n (l) a a , X b n (l) b , X n (l) c c , X dn (l) d only through the respective means of such subvectors20. A similar remark holds for the kernels Kmb and Kms. One can notice that, in these cases, the graph structure is taken into account taking averages of features of neighboring nodes.\nRemark 9 Starting from any kernel of the form Kmo, one can define kernels of the forms Kmb and Kms using constructions similar to the ones provided by formulas (6) and (7). Also the examples of order-invariant kernels presented in Section 4 are extended directly to analogous examples of kernels of\n20. Another advantage of this assumption is that it allows an immediate extension of the analysis to the case of graphs in which the nodes can possibly have different numbers of neighbors.\nthe form Kmo for which the property (55) holds. For instance, this holds in the case of the (multiplepairs) linear kernel, defined as\nKmolin\n(( 1\nh−1 ∑h−1 l=1 Xn(l)a a , Xa b, 1 h−1 ∑h−1 l=1 Xb n(l)b ) , ( 1 h−1 ∑h−1 l=1 Xn(l)c c , Xc d, 1 h−1 ∑h−1 l=1 Xd n(l)d ))\n:= 1(h−1)2 〈 ∑h−1 l=1 Xn(l)a a , ∑h−1 l=1 Xn(l)c c 〉Rn1 + 〈Xa b, Xc d〉Rn1 + 1(h−1)2 〈 ∑h−1 l=1 Xb n(l)b , ∑h−1 l=1 Xd n(l)d 〉Rn1 ."
    }, {
      "heading" : "7.4 Extension to diffusion kernels on an auxiliary graph",
      "text" : "We conclude describing the following other extension of our analysis to the problem of arc classification on graphs, taking into account even more deeply the graph structure. The goal is to show how one can construct a diffusion kernel (Kondor and Lafferty, 2002), (Shawe-Taylor and Cristianini, 2004, Section 9.4) on an auxiliary graph, whose nodes are a selection of the directed arcs of the original graph, providing sufficient conditions under which such diffusion kernel is balanced, or skew-balanced (see the next Theorem 5). A numerical comparison with other (unbalanced or not skew-balanced) graph kernels (Vishwanathan et al., 2010) (including other diffusion kernels), and with the multiple-pairs kernels introduced in Section 7.3, is deferred to a future investigation. In the following, likewise in Section 4.1, we assume that a set I of ordered pairs of distinct objects is given, and that, if (a, b) belongs to I, also (b, a) belongs to I.\nThe proposed construction of the auxiliary graph proceeds as follows. For each unordered pair of distinct objects a, b, we choose one (and only one) of the two possible orders (a, b) and (b, a), and we associate it with a node of the auxiliary graph. Then, we insert an (undirected) edge between two nodes of the auxiliary graph if and only if they share an object (e.g., we insert an edge between two nodes (a, b) and (a, c), but not between two nodes (a, b) and c, d)). At this point, we choose a pairwise balanced or skew-balanced kernel K to express the base similarity between two nodes of the auxiliary graph that are connected by an edge (i.e., the base similarity between (a, b) and (a, c) is Kab,ac), whereas we set to 0 the base similarity of two nodes of the auxiliary graph that are not connected by an edge. Finally, we denote by K the corresponding matrix of base similarities, which is indexed by pairs of nodes of the auxiliary graph (for simplicity, in the following we also denote by Kij its elements, i.e., i and j denote two generic nodes of the auxiliary graph). Then, for a nonincreasing sequence of nonnegative real numbers λl such that the following series (56) converges, we define the (diffusion) similarity matrix induced by the graph structure as\nKdiff := +∞∑\nl=0\nλlKl. (56)\nTo compute the similarity induced by the graph structure for a different selection of the nodes of the auxiliary graph, in principle one can repeat the construction above, changing the base similarity matrix K, due to the change in the nodes (in practice, this is not needed, due to the next Theorem 5). In this way, the diffusion kernel21 Kdiff expressing the graph-induced similarities of pairs of nodes is completely defined, apart from the similarities induced by the graph structure between elements of\n21. Differently from Section 3, this diffusion kernel is defined on a finite domain. Nevertheless, the concepts of balancedness and skew-balancedness are defined in a similar way.\nthe form (a, b) and (b, a). To define them, we set Kdiff(a,b),(b,a) := K diff (a,b),(a,b) in the case of a balanced kernel, and Kdiff(a,b),(b,a) := −Kdiff(a,b),(a,b) in the case of a skew-balanced kernel.\nTheorem 5 The following hold.\n(a) If the pairwise kernel K is balanced or skew-balanced, then the matrix Kdiff is well-defined, in the sense that each element Kdiffij does not depend on the specific construction of the nodes of the auxiliary graph that are different from i and j.\n(b) The kernel Kdiff is symmetric positive semi-definite. If K is balanced (respectively, skew-balanced), so is Kdiff .\nConcluding, Theorem 5 shows how to define balanced and skew-balanced diffusion kernels, starting from balanced and skew-balanced base kernels."
    }, {
      "heading" : "8. Conclusions",
      "text" : "We have investigated symmetry and antisymmetry properties of the optimal solutions to l1-soft margin binary SVM classifiers under symmetry/antisymmetry conditions on the labels, extending also some results to support vector regression with the linear ε-insensitive loss function. The results show that, with a suitable choice of the kernel, it is possible to impose such symmetry/antisymmetry properties, which, depending on the specific machine learning problem, may form an additional a-priori knowledge. For the symmetric case, taking the hint from the invariance framework of (Király et al., 2014), the first part of the analysis specializes the one made in (Brunner et al., 2012) to a particular transformation of feature vectors associated with a pair of objects, when one exchanges their order. Moreover, we have also detailed an extension to the antisymmetric case, which was not investigated in (Brunner et al., 2012). Additionally, we have also investigated how a classical algorithm from the literature (one version of the SMO algorithm (Keerthi et al., 2001)) is able to generate a sequence of suboptimal solutions having the same symmetry/antisymmetry properties. Up to our knowledge, this kind of investigation - which we have also performed on an algorithm used in the literature to train transductive SVMs - is completely novel. Numerical examples have confirmed the theoretical results. Finally, we have extended such results to the case in which the feature vectors used to classify/regress the arcs are associated with multiple pairs of nodes, instead than only one such pair, and we have also extended the analysis to diffusion kernels on graphs. The results contribute to fill a current gap in the literature about kernel methods, providing an additional theoretical investigation (besides the ones provided in (Herbrich et al., 1998), (Brunner et al., 2012), (Király et al., 2014) and the other references cited in the Introduction) of symmetry and antisymmetry properties of the optimal solutions to machine learning problems modeled by kernel methods.\nFor what concerns the numerical results, having used artificial data in the simulations is not a severe limitation, since the goal of our investigation in the first two scenarios was not to compare the classification performance of one proposed machine learning method with other ones, but to investigate numerically the symmetry/antisymmetry properties of the optimal classifier, validating the theoretical results. Moreover, in the third and fourth scenarios, we have compared the test-set classification performance obtained by using different kernels and demonstrated the potential advantage of kernels that impose symmetry/antisymmetry properties, when these are satisfied by the model generating the data labels. For what concerns real-world applications of such properties, we refer to (Brunner et al.,\n2012) and (Herbrich et al., 1998) for the two cases, respectively. In particular, (Brunner et al., 2012, Section 5.2) deals with a face recognition problem for which the symmetry property arises naturally (the goal therein is to recognize whether two face images belong to the same person or not), while (Herbrich et al., 1998) considers the case in which one wants to learn an order among objects through supervised examples, which naturally leads to the antisymmetric property (this learning framework has applications, e.g., in information retrieval, in regression with ordinal response, and in econometric models). The optimization problem considered therein to model learning is a particular instance of the optimization problem (22), when Ko is the linear kernel, and C = +∞. The present paper extends significantly that setting, by allowing for more general kernels, and providing several additional theoretical results related to the antisymmetric case.\nWe also refer to the recent work (Dardard et al., 2016) for an application with real motion capture data (incidentally, the antisymmetry property empirically observed when solving numerically the binary classification problem studied in (Dardard et al., 2016)22 was the source of inspiration for the theoretical investigation of the antisymmetry constraint made in the present work23). This application (summarized in footnote 23) refers actually to a setting with both supervised and unsupervised examples, and a theoretical explanation of is numerical results has been provided in Section 7.2. Moreover, for the case of symmetric labels, the developments in that section may be applied to an extension of the supervised face recognition application considered in (Brunner et al., 2012, Section 5.2) to a transductive learning setting, in which one would use both supervised and unsupervised (pairs of) examples. This would be useful for cases in which supervision is time-consuming and costly (see, e.g., (Gnecco et al., 2016, in press) for a situation in which this occurs).\nAmong other possible future developments, we mention: potential extensions to other kernel methods (e.g., Laplacian SVMs (Belkin and Niyogi, 2006), or combinations of soft and hard constraints (Gnecco et al., 2015b)) of the theoretical results about symmetry/antisymmetry of the optimal solutions; the investigation of possible symmetry/antisymmetry preserving properties of other variations of the SMO algorithm used, e.g., for SVM regression problems.\n22. See, in particular, (Dardard et al., 2016, Section 5) and the comments to Figures 6 and 7 therein. 23. In more details, the application considered in (Dardard et al., 2016) concerns the analysis of leading interactions in\na group of individuals (specifically, musicians in a string quartet). The interactions of the individuals are modeled through a weighted directed graph, where the nodes of the graph represent the individuals, whereas the weight of the arc between any two individuals expresses the intensity of a “leadership/followership” relationship between the two individuals. In this specific case, opposite arcs have opposite labels (in case of a “cohesion” relationship, the same labels would have been used, instead). The feature vector - which is associated with each pair of individuals - is constructed starting from motion capture data associated with the movement of parts of the body of each individual, and inserting in such vector both individual features and group features, obtained through several time-series analysis. After exchanging the order of the two individuals of the pair, one obtains a new feature vector, in which the individual features are only permuted, whereas some of the group features do not change at all (i.e., the “synchronicity” and “level of attention” features defined in Section 3.3 of that paper), others change only in sign (i.e., the “delay” features defined in the same section). In the specific case considered in (Dardard et al., 2016), the weigths of the ordered arcs are obtained by training, as a binary classifier, a transductive SVM with linear kernel (Sindhwani and Keerthi, 2006), whose primal problem has some similarities with the l1-soft margin binary SVM classifier. In the present work, we have focused on the case of the l1-soft margin binary SVM classifier, which is more commonly used in applications (however, an extension of our analysis to transductive SVMs is given in Section 7.2)."
    }, {
      "heading" : "Appendix: Proofs",
      "text" : "Proof of Lemma 1. The idea of the proof is similar to the one of (Brunner et al., 2012, Lemma 1), with some nontrivial changes in the antisymmetric case. We report here only the one of case (b), since the one of case (a) follows directly by specializing (Brunner et al., 2012, Lemma 1) to the definition of order-invariant kernel reported in this paper (which includes the feature-vector transformation associated with the operator T ).\nGiven any optimal solution α⋆ to the dual optimization problem (13) under condition (b), one constructs another solution ᾱ defined by ᾱab := α ⋆ ba, for all (a, b) ∈ I, which is feasible for the dual optimization problem (13) since 0 ≤ ᾱab ≤ C for all (a, b) ∈ I, and ∑\n(a,b)∈I yabᾱab =\n∑\n(a,b)∈I yabα\n⋆ ba = −\n∑\n(a,b)∈I ybaα\n⋆ ba = −\n∑\n(a,b)∈I yabα\n⋆ ab = 0,\nwhere we have exploited the fact that, for each (a, b) ∈ I, the set I contains also the element (b, a). Using also formula (14) and the definition of order-invariant kernel, one gets\n2G(ᾱ) = ∑\n(a,b),(c,d)∈I ᾱabᾱcdyabycdK\no ab,cd − 2\n∑\n(a,b)∈I ᾱab\n= ∑\n(a,b),(c,d)∈I α⋆baα ⋆ dc(−yba)(−ydc)Koba,dc − 2\n∑\n(a,b)∈I α⋆ba\n= ∑\n(a,b),(c,d)∈I α⋆abα ⋆ cdyabycdK o ab,cd − 2\n∑\n(a,b)∈I α⋆ab = 2G(α ⋆),\nhence, also ᾱ is optimal for the dual optimization problem (13). Finally, by convexity of that problem and the optimality of α⋆ and ᾱ, respectively, also the solution α◦ := 12 (α\n⋆ + ᾱ) (which satisfies the symmetry condition (16)) is feasible, and optimal for (13). Proof of Theorem 1. The proof of part (a) follows by the same arguments provided in the proof of (Brunner et al., 2012, Theorem 2). In the following, we report the proof of part (b), which differs considerably from the one of (Brunner et al., 2012, Theorem 2).\nBy Lemma 1, there exists an optimal solution α◦ to the dual optimization problem (13) for which (16) holds. This, combined with (15), the antisymmetry of the labels, and the definition of orderinvariant kernel, provides, for any ordered pair (c, d),\n〈 w◦, φo (Xcd) 〉 E\n= ∑\n(a,b)∈I α◦abyab\n〈 φo ((Xab)) , φ o (Xcd) 〉 E\n= ∑\n(a,b)∈I α◦abyabK o ab,cd = −\n∑\n(a,b)∈I α◦baybaK o ba,dc = −\n∑\n(a,b)∈I α◦abyabK o ab,dc\n= − ∑\n(a,b)∈I α◦abyab\n〈 φo (Xab) , φ o (Xdc) 〉 E\n= − 〈 w◦, φo (Xdc) 〉 E . (57)\nNow, since the kernel Ko is order-invariant and the labels satisfy, for every (a, b) ∈ I, the antisymmetry condition yab = −yba, given any optimal solution (w◦, γ◦, {ξ◦ab ∈ R : (a, b) ∈ I}) to the primal\noptimization problem (12) in which w◦ has the form (15) with optimal dual variables satisfying the symmetry constraints α◦ab = α ◦ ba, also ( w̄, γ̄, {ξ̄ab ∈ R : (a, b) ∈ I} ) (with w̄ := w◦, γ̄ := −γ◦, and ξ̄ab := ξ ◦ ba) is a feasible solution of the primal optimization problem (12), and provides the same (optimal) value for its objective function. Hence, the closed interval of optimal values for γ in the primal optimization problem (12) is symmetric around the origin, so, there exists24 an optimal solution to the primal optimization problem (12) for which γ◦ := 0. With this choice of γ◦, using (57), one obtains (19). Proof of Theorem 2. The idea of the proof is similar to the one of (Brunner et al., 2012, Theorem 3), with some nontrivial changes in the antisymmetric case. We report here only the one of case (b), which differs from that proof due to the presence of the antisymmetric labels, the absence of the bias γ among the optimization variables, and the need to check, in a different way, the feasibility of some solutions constructed in the proof. Again, the proof of case (a) follows directly by specializing (Brunner et al., 2012, Theorem 3) to the definition of order-invariant kernel reported in this paper (which includes the feature-vector transformation associated with the operator T ).\nFirst of all, recalling that any optimal weight vector w◦ of the primal optimization problem (12) has the representation (15) and is unique, one obtains that\n∑\n(a,b)∈I α◦abyabK o ab,cd =\n〈 w◦, φo (Xcd) 〉 E\n(58)\nis the same for all the optimal solutions α◦ of the optimization problem (13), even when they are not unique. Similarly, since the optimization problem (22) can be interpreted as the dual of a primal one of the form (12) with γ removed from the problem formulation (i.e., set to 0), one can show25 that any optimal weight vector ŵ◦ of such a primal optimization problem has the representation\nŵ◦ := ∑\n(a,b)∈J β◦abyabφ s (Xab) , (59)\nwhere β◦ is any optimal solution to the optimization problem (22). Moreover, it still follows26 from the proof of (Burges and Crisp, 2000, Theorem 2) that such an optimal weight vector ŵ◦ is unique, hence ∑\n(a,b)∈J β◦abyabK s ab,cd =\n〈 ŵ◦, φs (Xcd) 〉 E\n(60)\nis the same for all the optimal solutions β◦ of the optimization problem (22), even when they are not unique.\n24. This part of the proof does not extend to case (a) because, when the kernel Ko is order-invariant and the labels satisfy, for every (a, b) ∈ I, the symmetry condition yab = yba, there is no guarantee that the set of optimal biases γ’s contains γ = 0. 25. This is obtained by setting to 0 the gradient vector (with respect to the weight vector) of the Lagrangian associated with that primal optimization problem. 26. Although (Burges and Crisp, 2000, Theorem 2) refers to a primal optimization problem of the form (12) (hence, containing the bias γ as an optimization variable), its proof of uniqueness of the optimal weight vector still applies to the present situation in which γ = 0, since that proof is based on the strict convexity of the objective of the primal optimization problem with respect to the weight vector, i.e., on the strict convexity of the term 1\n2 ‖w‖2E , which holds\nalso in the absence of γ as an optimization variable.\nDue to Lemma 1 (b), among all the optimal solutions of the optimization problem (13), one can choose one for which α◦ab = α ◦ ba, for every (a, b) ∈ I. Then, one defines the vector β̄ with components\nβ̄ab := α ◦ ab + α ◦ ba, ∀(a, b) ∈ J, (61)\nwhich is a feasible solution for the optimization problem (22). Next, one gets\nβ̄abK s ab,cd = β̄ab 2\n( Koab,cd −Koba,cd ) = α◦ab + α ◦ ba\n2\n( Koab,cd −Koba,cd ) = α◦abK o ab,cd − α◦baKoba,cd. (62)\nThis, combined with yab = −yba, implies ∑ (a,b)∈I α ◦ abyabK o ab,cd = ∑ (a,b)∈J β̄abyabK s ab,cd. Then, the remaining of the proof consists in showing that β̄ is also an optimal solution of the optimization problem (22) (which proves the equality between (58) and (60), hence, formula (25)), and that the optimal values of the objectives of the two optimization problems (13) and (22) are the same.\nThe proof of the optimality of β̄ for the optimization problem (22) proceeds as follows. First of all, by using yab = −yba, the definitions (23) of Ks and (61) of β̄, the symmetry of both Ko and Ks, the skew-balancedness of Ks, and formula (62) (reversing, in some cases, the roles of (a, b) and (c, d)), one obtains\n2G(α◦) + 2 ∑\n(a,b)∈I α◦ab\n= ∑\n(a,b)∈I α◦abyab\n∑\n(c,d)∈I ycdα\n◦ cdK o ab,cd\n= ∑\n(a,b)∈I α◦abyab\n∑ (c,d)∈J ycd ( α◦cdK o ab,cd − α◦dcKoab,dc )\n= ∑\n(a,b)∈I α◦abyab\n∑ (c,d)∈J ycd ( α◦cdK o cd,ab − α◦dcKodc,ab )\n= ∑\n(a,b)∈I α◦abyab\n∑\n(c,d)∈J ycdβ̄cdK\ns cd,ab\n= ∑\n(a,b)∈J α◦abyab\n∑\n(c,d)∈J ycdβ̄cdK\ns ab,cd +\n∑\n(a,b)∈J α◦bayba\n∑\n(c,d)∈J ycdβ̄cdK\ns ba,cd\n= ∑\n(a,b)∈J α◦abyab\n∑\n(c,d)∈J ycdβ̄cdK\ns ab,cd +\n∑\n(a,b)∈J α◦ab(−yab)\n∑\n(c,d)∈J ycdβ̄cd(−Ksab,cd)\n= 2 ∑\n(a,b)∈J α◦abyab\n∑\n(c,d)∈J ycdβ̄cdK\ns ab,cd =\n∑\n(a,b)∈J β̄abyab\n∑\n(c,d)∈J ycdβ̄cdK\ns ab,cd\n= 2Hs(β̄) + 2 ∑\n(a,b)∈J β̄ab.\nThen, by exploiting again the definition of β̄, one obtains 2 ∑\n(a,b)∈I α ◦ ab = 2 ∑ (a,b)∈J β̄ab, hence\nG(α◦) = Hs(β̄). (63)\nNow, given any optimal solution β◦ of the optimization problem (22), one defines the vector ᾱ with components\nᾱab := { β◦ab 2 , if (a, b) ∈ J, β◦ba 2 , if (b, a) ∈ J,\n(64)\nwhich is a feasible solution for the optimization problem (13). Indeed, 0 ≤ ᾱab ≤ C for all (a, b) ∈ I, and the constraint ∑ (a,b)∈I yabᾱab = 0 is automatically satisfied, due to ᾱab = ᾱba and yab = −yba for all (a, b) ∈ I. Now, by the definition of order-invariant kernel, and the definition (23) of Ks and its symmetry, one gets ᾱabK o ab,cd − ᾱbaKoab,dc = β◦ab 2 ( Koab,cd −Koab,dc ) = β◦abK s ab,cd. This, combined with ycd = −ydc, provides\n2Hs(β◦) + 2 ∑\n(a,b)∈J β◦ab\n= ∑\n(a,b)∈J β◦abyab\n∑\n(c,d)∈J β◦cdycdK s ab,cd =\n∑\n(a,b)∈J β◦abyab\n∑\n(c,d)∈J β◦cdycd\n1\n2\n( Koab,cd −Koba,cd )\n= 1\n2\n∑\n(a,b)∈J β◦abyab\n∑\n(c,d)∈J\n( ᾱcdycd ( Koab,cd −Koba,cd ) + ᾱdc(−ydc) ( −Koab,dc +Koba,dc ))\n= 1\n2\n∑\n(a,b)∈J β◦abyab\n∑\n(c,d)∈I ᾱcdycd\n( Koab,cd −Koba,cd ) .\nThen, by using the definition (64) of ᾱ, one obtains β◦ab = ᾱab + ᾱba for (a, b) ∈ J , and ᾱab = ᾱba. Hence, exploiting also yab = −yba, one gets\n2Hs(β◦) + 2 ∑\n(a,b)∈J β◦ab =\n1\n2\n∑\n(a,b)∈J β◦abyab\n∑\n(c,d)∈I ᾱcdycd\n( Koab,cd −Koba,cd )\n= ∑\n(a,b)∈I ᾱabyab\n∑\n(c,d)∈I ᾱcdycdK\no ab,cd\n= 2G(ᾱ) + 2 ∑\n(a,b)∈I ᾱab.\nFinally, by exploiting again the definition of ᾱ, one obtains 2 ∑\n(a,b)∈J β ◦ ab = 2 ∑ (a,b)∈I ᾱab, then\nG(ᾱ) = Hs(β◦). (65)\nNow, assume the β̄ is not an optimal solution to the optimization problem (22). Hence, one obtains Hs(β◦) < Hs(β̄), and, by using (63) and (65), also G(ᾱ) < G(α◦), which contradicts the assumed optimality of α◦ for the optimization problem (13). Hence, one concludes that β̄ is an optimal solution to the optimization problem (22). Proof of Theorem 3. (a) The fact that α(t) is feasible for the dual optimization problem (13) and satisfies the symmetry condition (29) follows from the feasibility of β(t) for the optimization problem (20), and the definition (34). The convergence to an optimal solution of the dual problem (13) follows from the convergence of the sequence of the β(t)’s to an optimal solution β◦ of the optimization problem (20) (Lin, 2001, 2002), and from the fact that, when β(t) is replaced by β◦, formula (64) provides an optimal solution to the dual problem (13), as already shown27 in the proof of Theorem 2.\n27. The proof has been detailed for the case of antisymmetric labels, but it is extended straightforwardly to the symmetric case.\n(b) Let t ≥ 1 and assume that condition (29) is satisfied at the iteration t− 1. Then, due to such a condition, together with the order-invariance of Ko and the antisymmetry of the labels, one gets\nF ( α (t−1) ab ) = ∑\n(c,d)∈I yabycdK\no ab,cdα (t−1) cd − 1 =\n∑\n(d,c)∈I yabydcK\no ab,dcα (t−1) dc − 1\n= ∑\n(d,c)∈I yabydcK\no ba,cdα (t−1) dc − 1 =\n∑\n(c,d)∈I (−yba)(−ycd)Koba,cdα\n(t−1) cd − 1\n= ∑\n(c,d)∈I ybaycdK\no ba,cdα (t−1) cd − 1 = F ( α (t−1) ba ) (66)\nfor all (a, b) ∈ I. This, combined with the antisymmetry of the labels and the selection rule (26), shows that, at the iteration t, it is possible to choose the dual variables αa(t,1)b(t,1) , αa(t,2)b(t,2) according to formula (26) and in such a way that ( a(t,2), b(t,2) ) = ( b(t,1), a(t,1) ) .Then, the restricted dual optimization problem to be solved at the iteration t (which is obtained by fixing all the dual variables to their previous values, with the exception of αa(t,1)b(t,1) and αa(t,2)b(t,2) = αb(t,1)a(t,1)), is\nminimizeα a(t,1)b(t,1) ,α b(t,1)a(t,1)\n∈R G restricted (αa(t,1)b(t,1) , αb(t,1)a(t,1)) ,\ns. t. 0 ≤ αa(t,1)b(t,1) , αb(t,1)a(t,1) ≤ C,∑\n(a,b)∈I(t) yabαab = c\n(t), (67)\nwhere I(t) := {( a(t,1), b(t,1) ) , ( b(t,1), a(t,1) )} , c(t) := −∑(a,b)∈I\\I(t) yabα (t−1) ab , and\nGrestricted (αa(t,1)b(t,1) , αb(t,1)a(t,1))\n:= 1\n2\n∑\n(a,b),(c,d)∈I(t) αabαcdyabycdK\no ab,cd +\n1\n2\n∑\n(a,b)∈I(t),(c,d)∈I\\I(t) αabα\n(t−1) cd yabycdK o ab,cd\n+ 1\n2\n∑\n(a,b)∈I\\I(t),(c,d)∈I(t) α (t−1) ab αcdyabycdK o ab,cd −\n∑\n(a,b)∈I(t) αab −\n∑\n(a,b)∈I\\I(t) α (t−1) ab .\nMoreover, c(t) = 0, due to the antisymmetry of the labels yab, the symmetry of the α (t−1) ab ’s, and\nthe definition of the set I \\ I(t). Hence, due to the resulting constraint ∑(a,b)∈I(t) yabαab = 0, the antisymmetry of the labels yab, and the definition of the set I\n(t), the optimization problem (67) has clearly a symmetric optimal solution α◦\na(t,1)b(t,1) = α◦ b(t,1)a(t,1) . Since the vector α(t) is obtained from\nα(t−1) by the updates\nα (t) a(t,1)b(t,1) := α◦ a(t,1)b(t,1) , α (t)\nb(t,1)a(t,1) := α◦ b(t,1)a(t,1) ,\nwe conclude that condition (29) is also satisfied at the iteration t. Sketch of the proofs of the analogues of Lemma 1 and Theorem 1 for support vector regression with the linear ε-insensitive loss function. For the case of real-valued labels satisfying yab = yba, given any optimal solution (α ⋆, α̂⋆) of the optimization problem (42), one can obtain\nanother optimal solution ᾱ defined by ᾱab := α ⋆ ba, ¯̂αab := α̂ ⋆ ba, for all (a, b) ∈ I, whereas, for the case of real-valued labels satisfying yab = −yba, one can obtain another optimal solution ᾱ defined by ᾱab := α̂ ⋆ ba, ¯̂αab := α ⋆ ba, for all (a, b) ∈ I. Then, the remainings of the proofs of the analogues of Lemma 1 and Theorem 1 for support vector regression with the linear ε-insensitive loss function proceed as in the Appendix, taking also into account (Burges and Crisp, 2000, Theorem 3), which is the analogue of (Burges and Crisp, 2000, Theorem 2) for this regression context. Proof of Theorem 4. (a). At the iteration t = 0 of the multiple label-switching algorithm, the optimization problem to be solved at its step 1 has symmetric labels, and has the same form as the problem (12). Then, step 1 can be performed using the version of the SMO algorithm detailed in Section 5, as in Theorem 3 (a)28. As a consequence, the resulting classifier has weight vector w(0) and bias γ(0) satisfying, for any ordered pair (c, d), the symmetry constraint f (0)(Xcd) = f (0)(Xdc), and the current dual variables satisfy α (0) ab = α (0) ba , for all (a, b) ∈ I. Then, the two sorted lists L+ and L− at the iteration 0 can be constructed in such a way that each ordered pair in the list (say, (â, b̂)) is immediately followed by the reversed ordered pair (b̂, â) in the same list (since they have the same current dual variables, labels, and outputs). As a consequence, exploiting also the fact that the number S is even, if some multiple label-switching is performed at the step 4, then both the ordered pairs (â, b̂) and (b̂, â) have their labels switched at the next iteration t+1 = 1. Then, the optimization problem to be solved at the step 1 of the iteration 1 (if such an iteration is performed) has again symmetric labels, and has the same form as the problem (12). Hence, one can proceed as at the iteration 0, and the proof is completed using an induction argument.\n(b) In this case, at the iteration t = 0 of the multiple label-switching algorithm, the optimization problem to be solved at its step 1 has antisymmetric labels, and has the same form as the problem (12). Then, step 1 can be performed using the version of the SMO algorithm detailed in Section 5, this time as in Theorem 3 (b). As a consequence, the resulting classifier has weight vector w(0) and bias γ(0) satisfying, for any ordered pair (c, d), the antisymmetry constraint f (0)(Xcd) = −f (0)(Xdc), and the current dual variables satisfy α\n(0) ab = α (0) ba , for all (a, b) ∈ I. Then, the two sorted lists L+ and L−\nat the iteration 0 can be constructed in such a way that each ordered pair in the list (say, (â, b̂)) has the reversed ordered pair (b̂, â) in the same position in the other list (since they have the same current dual variables, but opposite labels and outputs). As a consequence, if some multiple label-switching is performed at the step 4, then both the ordered pairs (â, b̂) and (b̂, â) have their labels switched29 at the next iteration t+1 = 1. Then, the optimization problem to be solved at the step 1 of the iteration 1 (if such an iteration is performed) has again antisymmetric labels, and has the same form as the problem (12). Hence, one can proceed as at the iteration 0, and the proof is completed again using an induction argument. Proof of Theorem 5. (a) Let h = (â, b̂) be a generic node of the auxiliary graph, different from i and j, and suppose that h is redefined in the auxiliary graph as h = (b̂, â). Then, when the kernel K is balanced, all the elements of the matrix K (hence, of the matrix Kdiff) are unchanged as a consequence of this change in the definition of h. Instead, when K is skew-balanced, all the elements of K with one index equal to h change in sign. However, this does not change the value of Kdiffij , since, for every nonnegative integer l, the generic element of Klij is the summation, over all paths of length l joining\n28. In this proof, differently from Section 5, we use the superscript t = 0 to refer to the solution obtained at the termination of the SMO algorithm, when concluding the step 1 of the multiple label-switching algorithm at its iteration t = 0. 29. In this case, the condition “S even” used in part (a) is not needed.\nthe nodes i and j of the auxiliary graph, of the products of the l weights Kîhĵh encountered along each path (with î1 = i, and ĵl = j), and all such paths cannot contain an odd number of times edges joining h with other nodes of the auxiliary graph.\n(b) If the kernel K is balanced and the node h which is changed in part (a) of the proof is replaced by j, then the matrices K and Kdiff do not change, hence also the value of Kdiffij does not change. Instead, if the kernel K is skew-balanced, if i is different from j, all the terms Klij change in sign, since the node j is encountered an odd number of times along each path mentioned in the proof of part (a) (the case in which i coincides with j is of not interest, and leads to no contradiction). Now, let us fix a choice for the nodes in the auxiliary graph. Then, using the first |I|2 indices for these nodes, and the last |I|2 for the corresponding nodes associated with the opposite ordered pairs, the first lines in the proof of this part (b) show that the diffusion kernel (matrix) Kdiff has the structure\nKdiff = ( Kdiff Kdiff Kdiff Kdiff ) (68)\nwhen K is balanced, and\nKdiff = ( Kdiff −Kdiff −Kdiff Kdiff ) (69)\nwhen K is skew-balanced. In both cases, the matrix Kdiff is symmetric positive semi-definite, because Kdiff is symmetric positive semi-definite by the definition (56) (every matrix Kl is symmetric positive semi-definite, and the coefficients λl are nonnegative), and, for any row vectors x, y ∈ R |I| 2 , one has\n( x y )(Kdiff Kdiff Kdiff Kdiff )( x′ y′ ) = (x+ y)Kdiff(x+ y)′ ≥ 0, (70)\nand ( x y )( Kdiff −Kdiff −Kdiff Kdiff )( x′ y′ ) = (x− y)Kdiff(x− y)′ ≥ 0. (71)\nFinally, the proof of the balancedness (respectively, skew-balancedness) of Kdiff follows directly from (68) (respectively, from (69))."
    } ],
    "references" : [ {
      "title" : "Boosting margin based distance functions for clustering",
      "author" : [ "A. Bar-Hillel", "T. Hertz", "D. Weinshall" ],
      "venue" : "Social Network Data Analytics,",
      "citeRegEx" : "Bar.Hillel et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Bar.Hillel et al\\.",
      "year" : 2011
    }, {
      "title" : "Pairwise support vector machines and their application to large scale problems",
      "author" : [ "C. Brunner", "A. Fischer", "K. Luig", "T. Thies" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Brunner et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Brunner et al\\.",
      "year" : 2012
    }, {
      "title" : "Uniqueness of the SVM solution",
      "author" : [ "C.J.C. Burges", "D.J. Crisp" ],
      "venue" : "In Proceedings of Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Burges and Crisp.,? \\Q2000\\E",
      "shortCiteRegEx" : "Burges and Crisp.",
      "year" : 2000
    }, {
      "title" : "LIBSVM: a library for support vector machines",
      "author" : [ "C.-C. Chang", "C.-J. Lin" ],
      "venue" : "ACM Transactions on Intelligent Systems and Technology,",
      "citeRegEx" : "Chang and Lin.,? \\Q2011\\E",
      "shortCiteRegEx" : "Chang and Lin.",
      "year" : 2011
    }, {
      "title" : "A study on SMO-type decomposition methods for support vector machines",
      "author" : [ "P.-H. Chen", "R.-E. Fan", "C.-J. Lin" ],
      "venue" : "IEEE Transactions on Neural Networks,",
      "citeRegEx" : "Chen et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2006
    }, {
      "title" : "On the mathematical foundations of learning",
      "author" : [ "F. Cucker", "S. Smale" ],
      "venue" : "Bulletin of the American Mathematical Society,",
      "citeRegEx" : "Cucker and Smale.,? \\Q2002\\E",
      "shortCiteRegEx" : "Cucker and Smale.",
      "year" : 2002
    }, {
      "title" : "Automatic classification of leading interactions in a string quartet",
      "author" : [ "F. Dardard", "G. Gnecco", "D. Glowinski" ],
      "venue" : "ACM Transactions on Interactive Intelligent Systems,",
      "citeRegEx" : "Dardard et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Dardard et al\\.",
      "year" : 2016
    }, {
      "title" : "Statistical comparison of classifiers over multiple data sets",
      "author" : [ "J. Demšar" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Demšar.,? \\Q2006\\E",
      "shortCiteRegEx" : "Demšar.",
      "year" : 2006
    }, {
      "title" : "Bridgning logic and kernel machines",
      "author" : [ "M. Diligenti", "M. Gori", "M. Maggini", "L. Rigutini" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Diligenti et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Diligenti et al\\.",
      "year" : 2012
    }, {
      "title" : "Linear Algebra in Action",
      "author" : [ "H. Dym" ],
      "venue" : "AMS,",
      "citeRegEx" : "Dym.,? \\Q2007\\E",
      "shortCiteRegEx" : "Dym.",
      "year" : 2007
    }, {
      "title" : "An introduction to ROC analysis",
      "author" : [ "T. Fawcett" ],
      "venue" : "Pattern Recognition Letters,",
      "citeRegEx" : "Fawcett.,? \\Q2006\\E",
      "shortCiteRegEx" : "Fawcett.",
      "year" : 2006
    }, {
      "title" : "Learning with boundary conditions",
      "author" : [ "G. Gnecco", "M. Gori", "M. Sanguineti" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Gnecco et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Gnecco et al\\.",
      "year" : 2013
    }, {
      "title" : "A theoretical framework for supervised learning from regions",
      "author" : [ "G. Gnecco", "M. Gori", "S. Melacci", "M. Sanguineti" ],
      "venue" : null,
      "citeRegEx" : "Gnecco et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Gnecco et al\\.",
      "year" : 2014
    }, {
      "title" : "Foundations of support constraint machines",
      "author" : [ "G. Gnecco", "M. Gori", "S. Melacci", "M. Sanguineti" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Gnecco et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Gnecco et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning with mixed hard/soft pointwise constraints",
      "author" : [ "G. Gnecco", "M. Gori", "S. Melacci", "M. Sanguineti" ],
      "venue" : "IEEE Transactions on Neural Networks and Learning Systems,",
      "citeRegEx" : "Gnecco et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Gnecco et al\\.",
      "year" : 2015
    }, {
      "title" : "Supervised and semi-supervised classifiers for the detection of flood-prone areas",
      "author" : [ "G. Gnecco", "R. Morisi", "G. Roth", "M. Sanguineti", "A.C. Taramasso" ],
      "venue" : "Soft Computing,",
      "citeRegEx" : "Gnecco et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Gnecco et al\\.",
      "year" : 2016
    }, {
      "title" : "Supervised learning of preference relations",
      "author" : [ "R. Herbrich", "T. Graepel", "P. Bollmann-Sdorra", "K. Obermayer" ],
      "venue" : "In Proceedings Fachgruppentreffen Maschinelles Lernen,",
      "citeRegEx" : "Herbrich et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Herbrich et al\\.",
      "year" : 1998
    }, {
      "title" : "Transductive inference for text classification using support vector machines",
      "author" : [ "T. Joachims" ],
      "venue" : "In Proceedings of the 16th International Conference on Machine Learning (ICML",
      "citeRegEx" : "Joachims.,? \\Q1999\\E",
      "shortCiteRegEx" : "Joachims.",
      "year" : 1999
    }, {
      "title" : "Improvements to Platt’s SMO algorithm for SVM classifier design",
      "author" : [ "S.S. Keerthi", "S. Shevade", "C. Bhattacharyya", "K. Murthy" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Keerthi et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Keerthi et al\\.",
      "year" : 2001
    }, {
      "title" : "Learning with algebraic invariances, and the invariant kernel trick. arXiv: 1411.7817v1 [stat.ML",
      "author" : [ "F.J. Király", "A. Ziehe", "K.-R. Müller" ],
      "venue" : null,
      "citeRegEx" : "Király et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Király et al\\.",
      "year" : 2014
    }, {
      "title" : "Diffusion kernels on graphs and other discrete input spaces",
      "author" : [ "R.I. Kondor", "J.D. Lafferty" ],
      "venue" : "In Proceedings of the 19th International Conference on Machine Learning (ICML",
      "citeRegEx" : "Kondor and Lafferty.,? \\Q2002\\E",
      "shortCiteRegEx" : "Kondor and Lafferty.",
      "year" : 2002
    }, {
      "title" : "On the convergence of the decomposition method for support vector machines",
      "author" : [ "C.-J. Lin" ],
      "venue" : "IEEE Transactions on Neural Networks,",
      "citeRegEx" : "Lin.,? \\Q2001\\E",
      "shortCiteRegEx" : "Lin.",
      "year" : 2001
    }, {
      "title" : "Asymptotic convergence of an SMO algorithm without any assumptions",
      "author" : [ "C.-J. Lin" ],
      "venue" : "IEEE Transactions on Neural Networks,",
      "citeRegEx" : "Lin.,? \\Q2002\\E",
      "shortCiteRegEx" : "Lin.",
      "year" : 2002
    }, {
      "title" : "Laplacian Support Vector Machines Trained in the Primal",
      "author" : [ "S. Melacci", "M. Belkin" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Melacci and Belkin.,? \\Q2011\\E",
      "shortCiteRegEx" : "Melacci and Belkin.",
      "year" : 2011
    }, {
      "title" : "Spectral analysis of symmetric and antisymmetric pairwise kernels. arXiv:1506.05950v1 [cs.LG",
      "author" : [ "T. Pahikkala", "M. Viljanen", "A. Airola", "W. Waegeman" ],
      "venue" : null,
      "citeRegEx" : "Pahikkala et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Pahikkala et al\\.",
      "year" : 2015
    }, {
      "title" : "Kernel methods in system identification, machine learning and function estimation: a survey",
      "author" : [ "G. Pillonetto", "F. Dinuzzo", "T. Chen", "G. De Nicolao", "L. Ljung" ],
      "venue" : null,
      "citeRegEx" : "Pillonetto et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Pillonetto et al\\.",
      "year" : 2014
    }, {
      "title" : "Fast training of support vector machines using sequential minimal optimization. In Advances in Kernel Methods: Support Vector Learning, pages 185–208",
      "author" : [ "J.C. Platt" ],
      "venue" : null,
      "citeRegEx" : "Platt.,? \\Q1999\\E",
      "shortCiteRegEx" : "Platt.",
      "year" : 1999
    }, {
      "title" : "Kernel methods for pattern analysis",
      "author" : [ "J. Shawe-Taylor", "N. Cristianini" ],
      "venue" : null,
      "citeRegEx" : "Shawe.Taylor and Cristianini.,? \\Q2004\\E",
      "shortCiteRegEx" : "Shawe.Taylor and Cristianini.",
      "year" : 2004
    }, {
      "title" : "Large scale semi-supervised linear SVMs",
      "author" : [ "V. Sindhwani", "S.S. Keerthi" ],
      "venue" : "In Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,",
      "citeRegEx" : "Sindhwani and Keerthi.,? \\Q2006\\E",
      "shortCiteRegEx" : "Sindhwani and Keerthi.",
      "year" : 2006
    }, {
      "title" : "Learning equivariant structured output SVM regressors",
      "author" : [ "A. Vedaldi", "M. Blaschko", "A. Zisserman" ],
      "venue" : "In Proceedings of the 2011 IEEE International Conference on Computer Vision (ICCV),",
      "citeRegEx" : "Vedaldi et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Vedaldi et al\\.",
      "year" : 2011
    }, {
      "title" : "A new pairwise kernel for biological network inference with support vector machines",
      "author" : [ "J.P. Vert", "J. Qiu", "W. Noble" ],
      "venue" : "In BMC Bioinformatics,",
      "citeRegEx" : "Vert et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Vert et al\\.",
      "year" : 2007
    }, {
      "title" : "A tutorial on spectral clustering",
      "author" : [ "U. von Luxburg" ],
      "venue" : "Statistics and Computing,",
      "citeRegEx" : "Luxburg.,? \\Q2007\\E",
      "shortCiteRegEx" : "Luxburg.",
      "year" : 2007
    }, {
      "title" : "A kernel-based framework for learning graded relations from data",
      "author" : [ "W. Waegeman", "T. Pahikkala", "A. Airola", "T. Salakoski", "M. Stock", "B. De Baets" ],
      "venue" : "IEEE Transactions on Fuzzy Systems,",
      "citeRegEx" : "Waegeman et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Waegeman et al\\.",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "Abstract A particularly interesting instance of supervised learning with kernels is when each training example is associated with two objects, as in pairwise classification (Brunner et al., 2012), and in supervised learning of preference relations (Herbrich et al.",
      "startOffset" : 173,
      "endOffset" : 195
    }, {
      "referenceID" : 16,
      "context" : ", 2012), and in supervised learning of preference relations (Herbrich et al., 1998).",
      "startOffset" : 60,
      "endOffset" : 83
    }, {
      "referenceID" : 1,
      "context" : "Extending the approach proposed in (Brunner et al., 2012) (where the only symmetric case was considered), we show, focusing on support vector binary classification, how such embedding is possible through the choice of a suitable pairwise kernel, which takes as inputs the individual feature vectors and also the group feature vectors associated with the two objects.",
      "startOffset" : 35,
      "endOffset" : 57
    }, {
      "referenceID" : 27,
      "context" : "In the context of the so-called kernel methods (Shawe-Taylor and Cristianini, 2004), examples of such problems are supervised (Cucker and Smale, 2002), unsupervised (von Luxburg, 2007), and semi-supervised (Belkin and Niyogi, 2006; Melacci and Belkin, 2011) learning, and identification of models of dynamical systems (Pillonetto et al.",
      "startOffset" : 47,
      "endOffset" : 83
    }, {
      "referenceID" : 5,
      "context" : "In the context of the so-called kernel methods (Shawe-Taylor and Cristianini, 2004), examples of such problems are supervised (Cucker and Smale, 2002), unsupervised (von Luxburg, 2007), and semi-supervised (Belkin and Niyogi, 2006; Melacci and Belkin, 2011) learning, and identification of models of dynamical systems (Pillonetto et al.",
      "startOffset" : 126,
      "endOffset" : 150
    }, {
      "referenceID" : 23,
      "context" : "In the context of the so-called kernel methods (Shawe-Taylor and Cristianini, 2004), examples of such problems are supervised (Cucker and Smale, 2002), unsupervised (von Luxburg, 2007), and semi-supervised (Belkin and Niyogi, 2006; Melacci and Belkin, 2011) learning, and identification of models of dynamical systems (Pillonetto et al.",
      "startOffset" : 206,
      "endOffset" : 257
    }, {
      "referenceID" : 25,
      "context" : "In the context of the so-called kernel methods (Shawe-Taylor and Cristianini, 2004), examples of such problems are supervised (Cucker and Smale, 2002), unsupervised (von Luxburg, 2007), and semi-supervised (Belkin and Niyogi, 2006; Melacci and Belkin, 2011) learning, and identification of models of dynamical systems (Pillonetto et al., 2014).",
      "startOffset" : 318,
      "endOffset" : 343
    }, {
      "referenceID" : 1,
      "context" : ", in recognizing whether two face images belong to the same person or not) (Brunner et al., 2012), and in supervised learning of preference relations (i.",
      "startOffset" : 75,
      "endOffset" : 97
    }, {
      "referenceID" : 16,
      "context" : ", in learning an order among objects through supervised pairs of examples) (Herbrich et al., 1998).",
      "startOffset" : 75,
      "endOffset" : 98
    }, {
      "referenceID" : 1,
      "context" : "The starting point of this part of our investigation is the work (Brunner et al., 2012), in which the only case of symmetry constraints was studied.",
      "startOffset" : 65,
      "endOffset" : 87
    }, {
      "referenceID" : 1,
      "context" : "It is worth mentioning that, according to (Brunner et al., 2012), that work is the first one in which the symmetry of the optimal solution to the training problem of an l1-soft margin binary Support Vector Machine (SVM) classifier with a symmetric training set has been rigorously proved, for a quite general class of kernels.",
      "startOffset" : 42,
      "endOffset" : 64
    }, {
      "referenceID" : 19,
      "context" : "Taking inspiration from the invariance framework of (Király et al., 2014), we specialize this result to the presence of both individual features and two kinds of group features, then we extend it to the case of an antisymmetric training set, showing the antisymmetry of the optimal solution.",
      "startOffset" : 52,
      "endOffset" : 73
    }, {
      "referenceID" : 27,
      "context" : "theory in optimization (Bertsekas, 1999) and on representer theorems for SVMs (Shawe-Taylor and Cristianini, 2004).",
      "startOffset" : 78,
      "endOffset" : 114
    }, {
      "referenceID" : 2,
      "context" : "We also employ existence and uniqueness results from the theory of SVMs (Burges and Crisp, 2000), and we introduce a kernel (called in the paper skew-balanced kernel), related specifically with the antisymmetry constraint.",
      "startOffset" : 72,
      "endOffset" : 96
    }, {
      "referenceID" : 26,
      "context" : "Additionally, in Section 5, we investigate how to impose the symmetry and antisymmetry constraints when looking for suboptimal solutions to the optimization problems investigated in the paper, analyzing the behavior of a specific algorithm proposed in the literature (Platt, 1999) from this viewpoint (i.",
      "startOffset" : 267,
      "endOffset" : 280
    }, {
      "referenceID" : 6,
      "context" : "2), whose application in (Dardard et al., 2016) inspired part of the theoretical investigations of the present work, providing numerical results that showed the occurrence of the antisymmetry property, for a specific choice of the kernel (the linear kernel), and to which we refer for real-world examples of individual and group features in a motion-capture context.",
      "startOffset" : 25,
      "endOffset" : 47
    }, {
      "referenceID" : 6,
      "context" : "No such theoretical analysis was performed in (Dardard et al., 2016), where the antisymmetry property was only observed a-posteriori, after training the learning machine.",
      "startOffset" : 46,
      "endOffset" : 68
    }, {
      "referenceID" : 1,
      "context" : "Finally, we outline some extensions of our analysis to the problem of arc classification on graphs, a problem for which the application of pairwise kernels was not considered in (Brunner et al., 2012).",
      "startOffset" : 178,
      "endOffset" : 200
    }, {
      "referenceID" : 1,
      "context" : "Besides the work (Brunner et al., 2012) mentioned above, a related paper is (Király et al.",
      "startOffset" : 17,
      "endOffset" : 39
    }, {
      "referenceID" : 19,
      "context" : ", 2012) mentioned above, a related paper is (Király et al., 2014), which provides the general structure of kernels incorporating prior knowledge represented by algebraic invariances.",
      "startOffset" : 44,
      "endOffset" : 65
    }, {
      "referenceID" : 19,
      "context" : "Moreover, even though it is related to the G-invariant kernels of (Király et al., 2014), the order-invariant kernel considered in this paper is not a particular case of the former, since, in the context of the present paper, the symmetry property for a symmetric training set is not associated only with the choice of the kernel, but also with the existence of optimal symmetric dual variables.",
      "startOffset" : 66,
      "endOffset" : 87
    }, {
      "referenceID" : 29,
      "context" : "Another related work is (Vedaldi et al., 2011), where a novel approach, based on a convex relaxation of a more general nonconvex optimization problem, is proposed to incorporate invariance and equivariance in the SVM training problem.",
      "startOffset" : 24,
      "endOffset" : 46
    }, {
      "referenceID" : 32,
      "context" : "Likewise in the present paper, symmetric and antisymmetric pairwise kernels have been also considered in (Waegeman et al., 2012), which presents sufficient conditions under which",
      "startOffset" : 105,
      "endOffset" : 128
    }, {
      "referenceID" : 1,
      "context" : "However, neither the extensions of the results of (Brunner et al., 2012) mentioned above are provided in (Waegeman et al.",
      "startOffset" : 50,
      "endOffset" : 72
    }, {
      "referenceID" : 32,
      "context" : ", 2012) mentioned above are provided in (Waegeman et al., 2012), nor an investigation of the issue of symmetry/antisymmetry preservation of suboptimal solutions to the learning problem.",
      "startOffset" : 40,
      "endOffset" : 63
    }, {
      "referenceID" : 16,
      "context" : "For what regards the antisymmetry constraint, the present paper extends the setting of (Herbrich et al., 1998), by allowing for more general kernels, and providing several additional theoretical results (e.",
      "startOffset" : 87,
      "endOffset" : 110
    }, {
      "referenceID" : 6,
      "context" : "We refer to (Dardard et al., 2016), for some more sophisticated examples of such individual and group features, i.",
      "startOffset" : 12,
      "endOffset" : 34
    }, {
      "referenceID" : 19,
      "context" : "It is worth remarking that, expressing the conditions (3) and (4) in terms of the operator T , is closely related to the invariance framework of (Király et al., 2014).",
      "startOffset" : 145,
      "endOffset" : 166
    }, {
      "referenceID" : 1,
      "context" : "As an example, in Section 4, taking the hint from (Brunner et al., 2012) (where the only condition (3) was considered), we consider two ways to impose condition (4) to the trained binary SVM classifier: the use of an antisymmetric training set (together with the choice of a suitable “order-invariant” kernel, see Section 4.",
      "startOffset" : 50,
      "endOffset" : 72
    }, {
      "referenceID" : 1,
      "context" : "For completeness, in that section we report also analogous results (stated in terms of order-invariant and balanced kernels) related to the symmetry condition (3), which, for the particular problem considered therein, are specializations of the results obtained in (Brunner et al., 2012) to the case in which the",
      "startOffset" : 265,
      "endOffset" : 287
    }, {
      "referenceID" : 27,
      "context" : "Similarly, if a bag-of-features (also called bag-of-words (Shawe-Taylor and Cristianini, 2004)) approach is used (i.",
      "startOffset" : 58,
      "endOffset" : 94
    }, {
      "referenceID" : 1,
      "context" : ", the tensor pairwise learning kernel reported later in Section 3), one can map individual feature vectors to group feature vectors in the feature spaces E associated with such kernels, which makes some examples of kernels reported in (Brunner et al., 2012) useful to deal with group feature vectors constructed starting from individual feature vectors.",
      "startOffset" : 235,
      "endOffset" : 257
    }, {
      "referenceID" : 6,
      "context" : ", the case of the motion-related group features considered in (Dardard et al., 2016), which were obtained through several time-series analysis).",
      "startOffset" : 62,
      "endOffset" : 84
    }, {
      "referenceID" : 27,
      "context" : "Case (a) is just the application of the standard machine-learning definition of (symmetric and positive semi-definite) kernel (Shawe-Taylor and Cristianini, 2004) to the situation in which each feature vector represents an ordered pair of objects (rather than, e.",
      "startOffset" : 126,
      "endOffset" : 162
    }, {
      "referenceID" : 19,
      "context" : ", a single object), whereas cases (b), (c), and (d) impose additional invariance properties (cases (b), and (d) also follow as special cases of the invariance framework considered in (Király et al., 2014)).",
      "startOffset" : 183,
      "endOffset" : 204
    }, {
      "referenceID" : 1,
      "context" : "Moreover, cases (a), (b) and (d) are specifications (due to the inclusion of both individual and group features, and the use of the operator T ) of the corresponding cases investigated in (Brunner et al., 2012)3.",
      "startOffset" : 188,
      "endOffset" : 210
    }, {
      "referenceID" : 24,
      "context" : ", whereas case (c) extends in a similar way the one recently introduced independently in (Pahikkala et al., 2015)4.",
      "startOffset" : 89,
      "endOffset" : 113
    }, {
      "referenceID" : 1,
      "context" : "As already observed in (Brunner et al., 2012), it follows from the definitions of pairwise kernels, balanced kernels, and order-invariant kernels, that every balanced kernel is also an order-invariant kernel.",
      "startOffset" : 23,
      "endOffset" : 45
    }, {
      "referenceID" : 1,
      "context" : "We have used the term order-invariant for the case (d), for which no specific term was used in (Brunner et al., 2012).",
      "startOffset" : 95,
      "endOffset" : 117
    }, {
      "referenceID" : 24,
      "context" : "The term permutation invariant is used in (Pahikkala et al., 2015).",
      "startOffset" : 42,
      "endOffset" : 66
    }, {
      "referenceID" : 24,
      "context" : "In (Pahikkala et al., 2015), this kernel has been introduced for a different investigation than ours, as it is related to spectral properties of a suitable linear operator associated with the kernel.",
      "startOffset" : 3,
      "endOffset" : 27
    }, {
      "referenceID" : 1,
      "context" : "It is worth remarking that linear, polynomial and Gaussian kernels are also considered both in (Brunner et al., 2012) and in (Király et al.",
      "startOffset" : 95,
      "endOffset" : 117
    }, {
      "referenceID" : 19,
      "context" : ", 2012) and in (Király et al., 2014).",
      "startOffset" : 15,
      "endOffset" : 36
    }, {
      "referenceID" : 1,
      "context" : "Likewise in (Brunner et al., 2012), starting from any order-invariant kernel Ko (associated with the mapping φ) and exploiting its symmetry, it is possible to define a balanced kernel Kb in the following way:",
      "startOffset" : 12,
      "endOffset" : 34
    }, {
      "referenceID" : 1,
      "context" : "In the following sections, to simplify the notation, in a similar way as in (Brunner et al., 2012), we use the shortcuts Ko ab,cd := K o (Xab, Xcd) , K b ab,cd := K b (Xab, Xcd) , and K s ab,cd := K s (Xab, Xcd) .",
      "startOffset" : 76,
      "endOffset" : 98
    }, {
      "referenceID" : 30,
      "context" : "KTL ((xa, xb) , (xc, xd)) := 1 2 (k1(xa, xc)k1(xb, xd) + k1(xa, xd)k1(xb, xc)) , KML ((xa, xb) , (xc, xd)) := 1 4 (k1(xa, xc)− k1(xa, xd)− k1(xb, xc) + k1(xb, xd)) , KTM ((xa, xb) , (xc, xd)) := KTL ((xa, xb) , (xc, xd)) +KML ((xa, xb) , (xc, xd)) (called, respectively, tensor learning pairwise kernel (Vert et al., 2007), metric learning pairwise kernel (Vert et al.",
      "startOffset" : 303,
      "endOffset" : 322
    }, {
      "referenceID" : 30,
      "context" : ", 2007), metric learning pairwise kernel (Vert et al., 2007), and tensor metric learning pairwise kernel (Brunner et al.",
      "startOffset" : 41,
      "endOffset" : 60
    }, {
      "referenceID" : 1,
      "context" : ", 2007), and tensor metric learning pairwise kernel (Brunner et al., 2012)).",
      "startOffset" : 52,
      "endOffset" : 74
    }, {
      "referenceID" : 1,
      "context" : "One difference in our formulation of the dual optimization problem with respect to (Brunner et al., 2012) is the absence of the constraint 0 ≤ αaa ≤ 2C, since the set I does not contain elements of the form (a, a).",
      "startOffset" : 83,
      "endOffset" : 105
    }, {
      "referenceID" : 27,
      "context" : ", (Shawe-Taylor and Cristianini, 2004)).",
      "startOffset" : 2,
      "endOffset" : 38
    }, {
      "referenceID" : 27,
      "context" : ", (Shawe-Taylor and Cristianini, 2004)).",
      "startOffset" : 2,
      "endOffset" : 38
    }, {
      "referenceID" : 1,
      "context" : "To save memory space, one can cache the individual features (likewise in (Brunner et al., 2012)), then compute the kernel values every time they are needed, only if the group features can be easily computed from the pairs of individual features.",
      "startOffset" : 73,
      "endOffset" : 95
    }, {
      "referenceID" : 26,
      "context" : "In this section, we focus on the investigation of how it is possible to obtain either symmetric or antisymmetric suboptimal solutions to the primal and dual optimization problems (12) and (13), focusing on the behavior of a commonly-used algorithm to solve the dual problem (13), which is the Sequential Minimal Optimization (SMO) algorithm (Platt, 1999).",
      "startOffset" : 341,
      "endOffset" : 354
    }, {
      "referenceID" : 18,
      "context" : "A modified version of such an algorithm, based on a suitable selection of the two dual variables to be re-optimized at each iteration (Keerthi et al., 2001), is guaranteed to converge asymptotically to an optimal solution of the dual problem (13) (Lin, 2001, 2002) and has been implemented in the software LIBSVM (Chang and Lin, 2011).",
      "startOffset" : 134,
      "endOffset" : 156
    }, {
      "referenceID" : 3,
      "context" : ", 2001), is guaranteed to converge asymptotically to an optimal solution of the dual problem (13) (Lin, 2001, 2002) and has been implemented in the software LIBSVM (Chang and Lin, 2011).",
      "startOffset" : 164,
      "endOffset" : 185
    }, {
      "referenceID" : 4,
      "context" : "For instance, (Chen et al., 2006) proved linear convergence of SMO under certain assumptions.",
      "startOffset" : 14,
      "endOffset" : 33
    }, {
      "referenceID" : 6,
      "context" : ", to our recent work (Dardard et al., 2016) for an application with real motion capture data.",
      "startOffset" : 21,
      "endOffset" : 43
    }, {
      "referenceID" : 1,
      "context" : "The whole procedure (which used for training all the available ordered pairs of objects, likewise in the simulations performed in (Brunner et al., 2012)) was repeated 20 times, each time considering different realizations of P, c1, c2, p1, p2, and of the training/test objects.",
      "startOffset" : 130,
      "endOffset" : 152
    }, {
      "referenceID" : 7,
      "context" : "Moreover, for both subtables, the difference in performance was statistically significant, according to a two-sided Wilcoxon signed-rank test (Demšar, 2006) at confidence level α = 0.",
      "startOffset" : 142,
      "endOffset" : 156
    }, {
      "referenceID" : 10,
      "context" : "Otherwise, in other cases, other criteria such as the Area under the Receiving Operating Characteristic Curve (AUC) can be used (Fawcett, 2006).",
      "startOffset" : 128,
      "endOffset" : 143
    }, {
      "referenceID" : 1,
      "context" : "In more details, one obtains the following extension of Lemma 1 to the regression case (for both the symmetric/antisymmetric cases, such an extension was not detailed in (Brunner et al., 2012)).",
      "startOffset" : 170,
      "endOffset" : 192
    }, {
      "referenceID" : 17,
      "context" : "Then, the l1-soft margin transductive SVM training problem for binary classification (Joachims, 1999), applied here to the case of the classification of ordered pairs using an order-invariant kernel Ko, is formulated as the following optimization problem17:",
      "startOffset" : 85,
      "endOffset" : 101
    }, {
      "referenceID" : 28,
      "context" : "Moreover, we have removed from the formulation an equality constraint, which is present in the formulation of the problem given in (Sindhwani and Keerthi, 2006), but absent in the original formulation of (Joachims, 1999) (however, see also Remark 7 for an extension to this case).",
      "startOffset" : 131,
      "endOffset" : 160
    }, {
      "referenceID" : 17,
      "context" : "Moreover, we have removed from the formulation an equality constraint, which is present in the formulation of the problem given in (Sindhwani and Keerthi, 2006), but absent in the original formulation of (Joachims, 1999) (however, see also Remark 7 for an extension to this case).",
      "startOffset" : 204,
      "endOffset" : 220
    }, {
      "referenceID" : 17,
      "context" : "In order to find a local minimum for large-scale instances of the problem above, the following label-switching algorithm was proposed in (Joachims, 1999) (here formulated in the case of ordered pairs, and with an order-invariant kernel).",
      "startOffset" : 137,
      "endOffset" : 153
    }, {
      "referenceID" : 17,
      "context" : "The algorithm has been proved in (Joachims, 1999) to converge in a finite number of iterations to a local minimum of (45), and has been extended in (Sindhwani and Keerthi, 2006) to a multiple label-switching algorithm, which has similar performance guarantees.",
      "startOffset" : 33,
      "endOffset" : 49
    }, {
      "referenceID" : 28,
      "context" : "The algorithm has been proved in (Joachims, 1999) to converge in a finite number of iterations to a local minimum of (45), and has been extended in (Sindhwani and Keerthi, 2006) to a multiple label-switching algorithm, which has similar performance guarantees.",
      "startOffset" : 148,
      "endOffset" : 177
    }, {
      "referenceID" : 28,
      "context" : "In more details, given a positive integer S, the multiple label-switching algorithm of (Sindhwani and Keerthi, 2006) and its selection rule at the iteration t work as follows: 1.",
      "startOffset" : 87,
      "endOffset" : 116
    }, {
      "referenceID" : 28,
      "context" : "This is just the additional equality constraint considered in the transductive SVM formulation reported in (Sindhwani and Keerthi, 2006), which was used to obtain the numerical results of (Dardard et al.",
      "startOffset" : 107,
      "endOffset" : 136
    }, {
      "referenceID" : 6,
      "context" : "This is just the additional equality constraint considered in the transductive SVM formulation reported in (Sindhwani and Keerthi, 2006), which was used to obtain the numerical results of (Dardard et al., 2016).",
      "startOffset" : 188,
      "endOffset" : 210
    }, {
      "referenceID" : 6,
      "context" : "Since the kernel used in that paper was the linear kernel (which is order-invariant), and the initial labels were antisymmetric, we conclude that Theorem 4 (b) provides a justification for the observed antisymmetry of the classifier, which was obtained in (Dardard et al., 2016).",
      "startOffset" : 256,
      "endOffset" : 278
    }, {
      "referenceID" : 20,
      "context" : "The goal is to show how one can construct a diffusion kernel (Kondor and Lafferty, 2002), (Shawe-Taylor and Cristianini, 2004, Section 9.",
      "startOffset" : 61,
      "endOffset" : 88
    }, {
      "referenceID" : 19,
      "context" : "For the symmetric case, taking the hint from the invariance framework of (Király et al., 2014), the first part of the analysis specializes the one made in (Brunner et al.",
      "startOffset" : 73,
      "endOffset" : 94
    }, {
      "referenceID" : 1,
      "context" : ", 2014), the first part of the analysis specializes the one made in (Brunner et al., 2012) to a particular transformation of feature vectors associated with a pair of objects, when one exchanges their order.",
      "startOffset" : 68,
      "endOffset" : 90
    }, {
      "referenceID" : 1,
      "context" : "Moreover, we have also detailed an extension to the antisymmetric case, which was not investigated in (Brunner et al., 2012).",
      "startOffset" : 102,
      "endOffset" : 124
    }, {
      "referenceID" : 18,
      "context" : "Additionally, we have also investigated how a classical algorithm from the literature (one version of the SMO algorithm (Keerthi et al., 2001)) is able to generate a sequence of suboptimal solutions having the same symmetry/antisymmetry properties.",
      "startOffset" : 120,
      "endOffset" : 142
    }, {
      "referenceID" : 16,
      "context" : "The results contribute to fill a current gap in the literature about kernel methods, providing an additional theoretical investigation (besides the ones provided in (Herbrich et al., 1998), (Brunner et al.",
      "startOffset" : 165,
      "endOffset" : 188
    }, {
      "referenceID" : 1,
      "context" : ", 1998), (Brunner et al., 2012), (Király et al.",
      "startOffset" : 9,
      "endOffset" : 31
    }, {
      "referenceID" : 19,
      "context" : ", 2012), (Király et al., 2014) and the other references cited in the Introduction) of symmetry and antisymmetry properties of the optimal solutions to machine learning problems modeled by kernel methods.",
      "startOffset" : 9,
      "endOffset" : 30
    }, {
      "referenceID" : 16,
      "context" : "2012) and (Herbrich et al., 1998) for the two cases, respectively.",
      "startOffset" : 10,
      "endOffset" : 33
    }, {
      "referenceID" : 16,
      "context" : "2) deals with a face recognition problem for which the symmetry property arises naturally (the goal therein is to recognize whether two face images belong to the same person or not), while (Herbrich et al., 1998) considers the case in which one wants to learn an order among objects through supervised examples, which naturally leads to the antisymmetric property (this learning framework has applications, e.",
      "startOffset" : 189,
      "endOffset" : 212
    }, {
      "referenceID" : 6,
      "context" : "We also refer to the recent work (Dardard et al., 2016) for an application with real motion capture data (incidentally, the antisymmetry property empirically observed when solving numerically the binary classification problem studied in (Dardard et al.",
      "startOffset" : 33,
      "endOffset" : 55
    }, {
      "referenceID" : 6,
      "context" : ", 2016) for an application with real motion capture data (incidentally, the antisymmetry property empirically observed when solving numerically the binary classification problem studied in (Dardard et al., 2016)22 was the source of inspiration for the theoretical investigation of the antisymmetry constraint made in the present work23).",
      "startOffset" : 189,
      "endOffset" : 211
    }, {
      "referenceID" : 6,
      "context" : "In more details, the application considered in (Dardard et al., 2016) concerns the analysis of leading interactions in a group of individuals (specifically, musicians in a string quartet).",
      "startOffset" : 47,
      "endOffset" : 69
    }, {
      "referenceID" : 6,
      "context" : "In the specific case considered in (Dardard et al., 2016), the weigths of the ordered arcs are obtained by training, as a binary classifier, a transductive SVM with linear kernel (Sindhwani and Keerthi, 2006), whose primal problem has some similarities with the l1-soft margin binary SVM classifier.",
      "startOffset" : 35,
      "endOffset" : 57
    }, {
      "referenceID" : 28,
      "context" : ", 2016), the weigths of the ordered arcs are obtained by training, as a binary classifier, a transductive SVM with linear kernel (Sindhwani and Keerthi, 2006), whose primal problem has some similarities with the l1-soft margin binary SVM classifier.",
      "startOffset" : 129,
      "endOffset" : 158
    } ],
    "year" : 2016,
    "abstractText" : "A particularly interesting instance of supervised learning with kernels is when each training example is associated with two objects, as in pairwise classification (Brunner et al., 2012), and in supervised learning of preference relations (Herbrich et al., 1998). In these cases, one may want to embed additional prior knowledge into the optimization problem associated with the training of the learning machine, modeled, respectively, by the symmetry of its optimal solution with respect to an exchange of order between the two objects, and by its antisymmetry. Extending the approach proposed in (Brunner et al., 2012) (where the only symmetric case was considered), we show, focusing on support vector binary classification, how such embedding is possible through the choice of a suitable pairwise kernel, which takes as inputs the individual feature vectors and also the group feature vectors associated with the two objects. We also prove that the symmetry/antisymmetry constraints still hold when considering the sequence of suboptimal solutions generated by one version of the Sequential Minimal Optimization (SMO) algorithm, and we present numerical results supporting the theoretical findings. We conclude discussing extensions of the main results to support vector regression, to transductive support vector machines, and to several kinds of graph kernels, including diffusion kernels.",
    "creator" : "LaTeX with hyperref package"
  }
}