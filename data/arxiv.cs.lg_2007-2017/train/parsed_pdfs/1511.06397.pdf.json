{
  "name" : "1511.06397.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ "martin@redcatlabs.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n51 1.\n06 39\n7v 1\n[ cs\n.C L\n] 1\n9 N\nov 2\n01 5"
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "Distributed representations of words have been shown to benefit NLP tasks like parsing, named entity recognition, and sentiment analysis, as well as being used as the raw material for other deep learning tasks.\nSurprisingly, these word vector embeddings can be derived directly from raw, unannotated corpora. Once created, the vector embedding itself can be expressed simply as a list of vocabulary words (of size V ), and a matrix of size V × d, where d is the dimensionality of the embedding space.\nIn this work, a 300 dimensional GloVe embedding (Pennington et al., 2014) was used as a baseline for a number of different lossy compression methods, each of which were explored in terms of the trade-offs between compressed representation size, and fidelity when compared using the word analogy task of Mikolov et al. (2013)."
    }, {
      "heading" : "2 APPROACHES TO COMPRESSION",
      "text" : "The GloVe embeddings are produced by minimising the error in predicting the word co-occurrence matrix Xi,j from the word vectors associated with words i and j 1 :\nJ = V∑\ni,j=1\nf(Xij)(w T i w̃j + bi + b̃j − logXij) 2\nThis formulation means that there is no ordering of the components of the vectors. In addition, simple linear dependencies have already been eliminated (since if there were linear dependencies, the overall error would be higher than if the terms were independently allowed to fit more features of the target Xij ).\n1f() is a function that (i) vanishes at zero, to eliminate problems of Xij being zero in the log() term, and (ii) levels off once Xij is ‘large’, so that frequent words don’t dominate the error calculation."
    }, {
      "heading" : "2.1 REAL-VALUED ENCODINGS",
      "text" : "Since the embeddings arrive as a vector of real numbers, it is natural to see whether these can be compressed into a representation with the same dimensionality, except quantised."
    }, {
      "heading" : "2.1.1 COMPRESSION 1 : THROW AWAY DATA",
      "text" : "The most simple approach is to simply set a fixed proportion of the data in the embedding to zero.\nA slightly more sophisticated version is to select a threshold value, and set every value in a given vector representation that lies below that value to zero. The compression here is slightly offset due to the need to store an ‘is-zero’ mask, along with the remaining values."
    }, {
      "heading" : "2.1.2 COMPRESSION 2 : LINEAR QUANTISATION",
      "text" : "A straightforward approach to quanitising the embedding in n levels is to construct a linear scale of n steps between the maximum and minimum values of each element (columns in the embedding matrix).\nA simple variant, used here due to its symmetry, is to first normalize the embedding column-wise, so that each element is ∼ N(0, 1), and then encode the value using (i) 1 bit for the sign, and (ii) an index representing which of the n/2 steps in the range [0,max |x|] to use.\nTo approximately counter the non-uniform distribution of the values in the elements, a simple nonlinearity was also experimented with, whereby |x|α was quantised, where α was an experimentallyderived constant (a value of ∼ 0.4 was found to be reasonably robust)."
    }, {
      "heading" : "2.1.3 COMPRESSION 3 : ADAPTIVE LEVEL ENCODING",
      "text" : "Observing the significant non-normality of the distributions of each element (see Figure 1), an adaptive quantisation approach was devised.\nFor a given number of quantisation levels n and an element e ∼ E, create a set of quantisation levels eq that minimises: ∑\ne∈E\nmin q\n(e − eq) 2\nThis process can be performed iteratively, starting with the eq placed uniformly within the values of E sorted numerically. Gradient descent can then be performed on the error term, and updates made to the eq made until a stable configuration emerges. Note that early iterations may give rise to large gradients in the eq (since the function to be minimised has lots of sharp discontinuities in it).\nThe above optimisation is done for each element/column of the embedding independently, and a list of the respective levels of eq is stored, in addition to the index within each list for each element of the representation."
    }, {
      "heading" : "2.2 BINARY ENCODINGS",
      "text" : "Even though the methods in the previous section can reduce the bit-count required for an embedding significantly, they still impose an interdependence between the bits representing an individual element. Working with the ‘bit budget’ suggested by the previous encodings, ‘true’ binary representations of the same embedding were sought, following the intuition that a good encoding should have minimal pre-ordained structure."
    }, {
      "heading" : "2.2.1 COMPRESSION 4 : QUASI AUTO-ENCODING",
      "text" : "Assuming a binary representation B of the baseline embedding E can be found, a natural first step is to aim for a linear relationship : E = BW, for some weight matrix W.\nWhile it seems reasonable to want to jointly optimise B and W, the size of B (which will be a multiple of the size of E) makes this impractical.\nIn order to make this more tractable, a secondary mapping is set up, whereby B∗ is generated from the respective rows of E through a trainable network. This is similar to auto-encoding learning (see, for instance, Harpur & Prager (1996)), except that discovering B (which is the pure binary embedding that B∗ is tending towards) is the end-goal, rather than an intermediate step. Note also that the structure or complexity of the network transformation NN in the generative B∗ = NN(E) step is not an important factor.\nAfter testing a number of different network configurations, the structure described in Table 1 was chosen, and it includes the following features :\n1. Addition of noise : A constant, additive noise term was found to aid training greatly (see also Bengio et al. (2013)).\n2. Exclusion zone : In order to promote ‘binarisation’ after the sigmoid function, a parameterised function was included that imposed an ‘exclusion zone’ around zero, so that the network was forced to sample values further away from the origin.\n3. Sigmoid function : This was chosen to be a standard sigmoid function during training, but a hard step function during testing.\n4. ‘Dynamic frog-boiling’ : Since an optimisation of both the network weights and the binarisation B was being performed, the objective function had two components : the first being the L2 error of E∗ vs E, and the second being the ‘binary-ness’ of the generated embedding B\n∗, which was captured by measuring its variance elementwise. These two components were weighted relative to each other by a factor λ. However, this balancing factor λ (which often arises in these situations) is difficult to select before training commences, and it may only be after several hours of training that a bad initial choice is revealed. The method used here is to monitor the L2 error of E∗ vs E, and, when it is within an acceptable bound, increase the value of λ incrementally (starting from zero). This method dynamically calibrates the difficulty of the task according to progress already made, allowing the network to find a reasonable starting point before the slightly-harderto-solve objective function is triggered."
    }, {
      "heading" : "2.2.2 COMPRESSION 5 : GREEDY BINARY PCA",
      "text" : "A PCA-like approach to producing a binary embedding was investigated (for another approach, see Gregor & LeCun (2010)).\nStarting with the original embedding E0 = E, an iterative search is made for a vector wn that minimised :\n∑\ne∈En\nmin (|e−wn|2, |e|2),\nthat is to say, the vector wn is the vector that minimises the total remaining energy in En+1 if the choices available are either to subtract it from a particular embedding row, or not.\nEach iteration of this produces a vector wn, a binary representation of whether that vector should be used for any particular word (which becomes the last column of Bn), and a new residual embedding En+1.\nUltimately, aggregating these representations and vectors into matrices gives a sequence of representations : BnWn → E."
    }, {
      "heading" : "3 EXPERIMENTS",
      "text" : "A 300-dimensional embedding (created from a 6 billion token corpus built from a combination of Wikipedia 2014 and Gigaword 5) was used as the GloVe baseline for experimentation 2.\nThe word analogy task used here as the compression performance measure was as described in Mikolov et al. (2013), and the provided dataset contains 19,544 such questions, divided into semantic and syntactic subsets. The semantic questions are typically analogies about people or places, like ”Athens is to Greece as Berlin is to ?”, whereas the syntactic questions are typically analogies about verb tenses or forms of adjectives, for example ”dance is to dancing as fly is to ?”.\nTo correctly answer a question, the model must uniquely identify the missing term, with only an exact correspondence counted as a correct match. This matching is done on a cosine-similarity basis, where the answer to the question ”a is to b as c is to ?” is found by searching for the word d whose representation wd is closest to wb − wa + wc according to cosine similarity (i.e. the word where the dot-product is a maximum compared to every other word in the vocabulary). As might be expected, small distortions in the embedding space can cause large drops in performance on this exact matching task.\n2Available for download at http://www-nlp.stanford.edu/projects/glove/\nIn Figure 2, the performance of a selection of 300-dimensional full-resolution embeddings is shown (as given in (Pennington et al., 2014)). The GloVe performance was cross-checked using the downloaded embedding and the dataset derived from the word analogy task."
    }, {
      "heading" : "3.1 BASIC METHODS",
      "text" : "Also in Figure 2 are the results of the ‘throw data away’ methods, either by zeroing out columns, or thresholding (circles and triangles respectively). In addition, the hexagon marks the performance of a 100-dimensional embedding trained on the same corpus, so that it can be compared to a 300- dimensional embedding with 200 of those dimensions artificially zeroed out."
    }, {
      "heading" : "3.2 QUANTISATION ENCODINGS : LINEAR, NEAR-LINEAR AND ADAPTIVE",
      "text" : "The results of the quantisation methods are illustrated in Figure 3, where linear, simple non-linear and adaptively chosen quantisation grids were used (down triangle, up triangle and circle respectively). The numbers associated with each label correspond to the number of levels used.\nNoteably, the method ‘ada.8’ achieves a performance that is only 0.58% less than GloVe, while using only 3-bits resolution per element."
    }, {
      "heading" : "3.3 BINARY ENCODINGS",
      "text" : ""
    }, {
      "heading" : "3.3.1 QUASI AUTO-ENCODING TRAINING",
      "text" : "Figure 4 shows the performance during training of the ‘quasi auto-encoding’ method. As can be seen, the L2 error is continually being brought down to a predetermined acceptable level, while the parameter λ is slowly increased to encourage the variance of B∗ to increase (which can also be seen, although it is clearly reluctant to improve all the way to the ultimate goal which is close to 0.25 on the right-hand axis)."
    }, {
      "heading" : "3.3.2 GREEDY BINARY PCA TRAINING",
      "text" : "Figure 5 shows the performance as the greedy binary error reduction operations take place. The reduction in overall error is clear, although the error being taken out at each step becomes very small. Interestingly, the number of ‘on’ bits in each successive additional representation in Bn, tends to a very stable limit (discussed below)."
    }, {
      "heading" : "3.3.3 BINARY ENCODING RESULTS",
      "text" : "The results of the binarisation methods are illustrated in Figure 6, where ‘NN∼3’ is a 1024-bit binary embedding (approximately 3.4-bits per original embedding dimension), and the ‘pca.x’ are the embeddings produced by the binary PCA method outlined earlier. The error (over the original GloVe embedding) of ‘pca.3’ is 0.81%, which is directly comparable to the earlier ‘ada.8’ method, which scored 0.58% (i.e. the adaptive levels method was superior). Moreover, the ‘pca.8’ method requires the additional transmission of a (900×300) matrix, whereas the ‘ada.8’ method needs only a lookup table of size (8× 300)."
    }, {
      "heading" : "3.3.4 LEARNED BINARY REPRESENTATION USED ‘RAW’",
      "text" : "The foregoing results on binary representations was directed towards finding B such that E could be approximately reconstructed. These representations also require the transmission of a mapping matrix W from B to E∗.\nCompared to the memory saving achieved by using a pure binary representation of the dictionary, the overhead of including the matrix is small. On the other hand, one of the intuitions for wanting a binary representation is that it could provide better interpretability of the embedding itself.\nGiven the relative difficulty of obtaining these binary embeddings (compared to quantisation methods), only limited experimentation was possible. However, one simple test of interpretability is whether the binary representation B could be used as a vector embedding of the language on a stand-alone basis.\nSo, given the two binary embeddings produced by (i) quasi auto-encoding and (ii) binary PCA, the same cosine-similarity tests were performed as in the other experiments (simply using the values ‘0’ and ‘1’ in each place in the new embedding). These results are given in Table 2, and are discussed below."
    }, {
      "heading" : "4 DISCUSSION",
      "text" : ""
    }, {
      "heading" : "4.1 LEVEL QUANTISATION VS. MORE ‘SOPHISTICATED’ METHODS",
      "text" : "The level-quantisation approaches to compression work extremely well, and are relatively simple to implement. However, they do not accomplish the goal of learning about the underlying embedding through an efficient compression algorithm.\nIn an ideal world, the binary representations would have been more readily learned by gradient descent. However, a (wide) variety of ‘tricks’ had to be implemented in order to prevent the network from converging on solutions that avoided the goal of producing a good binary representation. This is partly due to the overall optimisation being under-constrained : A 300-element vector space is being fitted by a 1024-dimensional one (albeit the larger space is ultimately constrained to be binaryvalued). Some of this overfitting was solved through the addition of an additive noise layer, however that then lead to training times increasing."
    }, {
      "heading" : "4.2 BINARY PCA OBSERVATIONS",
      "text" : "The original motivation for searching for optimal ‘space-folding’ directions was to see whether those directions would have a revealing linguistic interpretation. Disappointingly, the process was found to be simply slicing and dicing the embedding into an ever-smaller noise sphere around the origin.\nRecognising the space-folding operation as simply a way of compressing distributions in ‘on themselves’ also explains why the number of ‘on’ bits in the binary PCA representations becomes so stable. Once the distribution of the values in an embedding dimension has become stable, the proportion of the distribution that gets folded-in at each iteration becomes constant.\nThere is a curious bump in the energy improvement in Figure 5 at iteration ∼ 300. This probably represents the initial round of modifications to each vector element finishing, and the next round of subdivision commencing (complete with the initial step of removing common offsets, which can also be observed at the very start of training). A similar effect occurs at iteration ∼ 600.\nOne proof-point that the binary PCA provides is that – using approximately the same ‘bit budget’ as the quantisation approaches – there is a binary representation that can be mapped purely linearly onto the original embedding with high fidelity."
    }, {
      "heading" : "4.3 BINARY ENCODINGS FOR INTERPRETABILITY",
      "text" : "Although the binary PCA approach proves that a binarised linear encoding is possible, the PCA process has little incentive to capture the semantic or syntatic structure in the embedding. As demonstrated by the results in Table 2, the standard mantra that auto-encodings learn useful representations is borne out. In fact, it is quite reassuring that the binary representation (which is just a by-product of fitting to an existing embedding) is itself a useful representation.\nEven if the raw binary representation isn’t as useful as hoped on a standalone basis, one small consolation is that a binary-valued encoding enables matrix multiplication to be simply a process of conditional additions."
    }, {
      "heading" : "5 CONCLUSION",
      "text" : "The techniques described in this paper can be applied to any embedding, since nothing specific to GloVe has been used 3.\nIdeally, the search for a good compression method would have lead to a representation with a good level of interpretability. As an analogue, compare JPEG encoding (which is lossy in ways that the brain is less sensitive to), to zip encoding (which is generically optimal, without giving much insight into the nature of the data being compressed).\nHowever, in terms of the goal of compressing a representation to within a given ‘bit budget’ (notably 3-bits per vector dimension with <1% error over the baseline), the adaptive quantisation method outline here is certainly effective : It is relatively simple to implement, each element of the resulting representation remains independent, and it is possible to reconstruct the full embedding on-the-fly from the small look-up table that is required in addition to the level-index representation."
    }, {
      "heading" : "ACKNOWLEDGMENTS",
      "text" : "The author thanks DC Frontiers, a Singapore-based company that has created the data-centric service ‘Handshakes’ (http://www.handshakes.com.sg/), for their willingness to support this ongoing research. DC Frontiers is the recipient of a Technology Enterprise Commercialisation Scheme grant from SPRING Singapore, under which this work took place.\n3And, given the way the GloVe embedding is constructed, it may be particularly resistant to compression."
    } ],
    "references" : [ {
      "title" : "Estimating or propagating gradients through stochastic neurons for conditional computation",
      "author" : [ "Bengio", "Yoshua", "Léonard", "Nicholas", "Courville", "Aaron C" ],
      "venue" : "CoRR, abs/1308.3432,",
      "citeRegEx" : "Bengio et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2013
    }, {
      "title" : "Learning fast approximations of sparse coding",
      "author" : [ "Gregor", "Karol", "LeCun", "Yann" ],
      "venue" : "In Proceedings of the 27th International Conference on Machine Learning",
      "citeRegEx" : "Gregor et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Gregor et al\\.",
      "year" : 2010
    }, {
      "title" : "Development of low entropy coding in a recurrent network. Network: computation",
      "author" : [ "Harpur", "George F", "Prager", "Richard W" ],
      "venue" : "in neural systems,",
      "citeRegEx" : "Harpur et al\\.,? \\Q1996\\E",
      "shortCiteRegEx" : "Harpur et al\\.",
      "year" : 1996
    }, {
      "title" : "Efficient estimation of word representations in vector space",
      "author" : [ "Mikolov", "Tomas", "Chen", "Kai", "Corrado", "Greg", "Dean", "Jeffrey" ],
      "venue" : "arXiv preprint arXiv:1301.3781,",
      "citeRegEx" : "Mikolov et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Pennington", "Jeffrey", "Socher", "Richard", "Manning", "Christopher D" ],
      "venue" : "Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP",
      "citeRegEx" : "Pennington et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "In this work, a 300 dimensional GloVe embedding (Pennington et al., 2014) was used as a baseline for a number of different lossy compression methods, each of which were explored in terms of the trade-offs between compressed representation size, and fidelity when compared using the word analogy task of Mikolov et al.",
      "startOffset" : 48,
      "endOffset" : 73
    }, {
      "referenceID" : 3,
      "context" : ", 2014) was used as a baseline for a number of different lossy compression methods, each of which were explored in terms of the trade-offs between compressed representation size, and fidelity when compared using the word analogy task of Mikolov et al. (2013). 2 APPROACHES TO COMPRESSION The GloVe embeddings are produced by minimising the error in predicting the word co-occurrence matrix Xi,j from the word vectors associated with words i and j 1 :",
      "startOffset" : 237,
      "endOffset" : 259
    }, {
      "referenceID" : 0,
      "context" : "Addition of noise : A constant, additive noise term was found to aid training greatly (see also Bengio et al. (2013)).",
      "startOffset" : 96,
      "endOffset" : 117
    }, {
      "referenceID" : 3,
      "context" : "The word analogy task used here as the compression performance measure was as described in Mikolov et al. (2013), and the provided dataset contains 19,544 such questions, divided into semantic and syntactic subsets.",
      "startOffset" : 91,
      "endOffset" : 113
    }, {
      "referenceID" : 4,
      "context" : "In Figure 2, the performance of a selection of 300-dimensional full-resolution embeddings is shown (as given in (Pennington et al., 2014)).",
      "startOffset" : 112,
      "endOffset" : 137
    } ],
    "year" : 2017,
    "abstractText" : "Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic. However, these vector space representations (created through large-scale text analysis) are typically stored verbatim, since their internal structure is opaque. Using word-analogy tests to monitor the level of detail stored in compressed rerepresentations of the same vector space, the trade-offs between the reduction in memory usage and expressiveness are investigated. A simple scheme is outlined that can reduce the memory footprint of a state-of-the-art embedding by a factor of 10, with only minimal impact on performance. Then, using the same ‘bit budget’, a binary (approximate) factorisation of the same space is also explored, with the aim of creating an equivalent representation with better interpretability.",
    "creator" : "LaTeX with hyperref package"
  }
}