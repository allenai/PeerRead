{
  "name" : "1102.2808.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Transductive Ordinal Regression",
    "authors" : [ "Chun-Wei Seah", "Ivor W. Tsang", "Yew-Soon Ong" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n10 2.\n28 08\nv5 [\ncs .L\nG ]\n3 S\nep 2\n01 2\nIndex Terms—Transductive Learning; Ordinal Regression; Ordinal Classification; Ordinal Loss Function; Support Vector Machines; Cluster Assumption;\nI. INTRODUCTION\nOrdinal regression (OR) is generally defined as the task where some input sample vectors are ranked on an ordinal scale [?], [?], [?]. In a five-star movie rating, for instance, the higher the rating, the better a movie is perceived to be. This rating can be configured as ordinal class labels {1,2,3,4,5}, which represents the number of stars a particular movie can be awarded. Hence the class labels are imbued with ordered information, i.e., a sample vector associated with class label 2 has a higher rating (or better) than another having class label 1, and having class label 3 is better off than having class label 1 and 2, and so on. Ordinal regression is also sometimes referred to interchangeably in the literature, as ordinal classification or multi-class classification models [?], [?], [?] with ordered classes. Today, ordinal regression of movie ratings such as the prediction of movie sentiment ratings, represents an important task of the sales personnel as part of their marketing strategy. Besides sentiment prediction, ordinal regression is also used in a wide area of applications that ranges from information retrieval [?], [?], collaborative filtering [?], medical analysis [?], gene expression analysis [?], to employee selection and prediction of pasture production [?].\nChun-Wei Seah, Ivor W. Tsang, Yew-Soon Ong are with School of Computer Engineering, Nanyang Technological University, Singapore 639798, e-mail {Seah0116,IvorTsang,asYSOng}@ntu.edu.sg\nInitial efforts pertaining to the use of support vector (SV) learning in ordinal regression was reported by Herbrich et al. [?]. Their work is based on a threshold model as shown in Fig. 1, in which the threshold values of each ordinal class are estimated. Then, Shashua and Levin [?] introduced two approaches for ordinal regression using the large margin principle. The first approach maximizes the margin between adjacent classes, whereas the other maximizes the sum of K − 1 margins, with K denoting the number of classes.\nBoth explicit and implicit constraints on the order of the thresholds in the model formulation, referred to as SVOR-EXC and SVOR-IMC in [?], [?], have also been considered recently. Li and Lin [?] extended their work with a framework that transforms the problem of ordinal regression to an extended binary classification, as a generalization of both SVOR-EXC and SVOR-IMC. By deriving the thresholds directly from the support vectors, a more efficient alternative, namely the Reduction Support Vector Machine, was introduced. Last but not least, as opposed to using all n data points, Zhao et al. [?] considered κ cluster representatives as the training data in SVOR-EXC, leading to significant reduction in the computational complexity, especially for large scale dataset since κ ≪ n.\nTo summarize, the field of ordinal regression has evolved in the last decade, with a plethora of noteworthy research progress made in supervised learning [?], [?], [?], [?], [?], [?], [?], [?], [?], [?]. In spite of the extensive work on this topic, existing methodologies proposed for ordinal regression may be fundamentally bounded by the lack of sufficient class labels found in the data. In particular, it is worth noting that ordinal class labels are often difficult to obtain. Specific tasks such as gene expression [?] and cell-phenotype images [?] are generally costly to annotate and calibrate due to the need for biological experts. Further, in many realistic applications of science and engineering, it may happen that deriving the labels involves hazardous experiments or the assessment of the label involves extreme conditions in resources [?]. A well known example is the movie sentiment problem where ordinal labels of movie ratings are scarce. Moreover, learning all the ordinal boundaries (between pairs of consecutive classes) generally requires considerable amount of labeled data due to the large number of unique class labels involved. Unlabeled data, on the other hand, exists in much greater abundance and are often freely available at zero cost. To take benefits from the\n2\nabundance of unlabeled patterns, the objective of the present paper is to introduce a novel transductive learning paradigm for ordinal regression, referred to here as Transductive Ordinal Regression or TOR in short.\nThe key challenge of TOR design lies in the appropriate incorporation of unlabeled data within the multi-class classification problem formulation with ordinal constraints. This involves the tasks of estimating the ordinal class label of the unlabeled data and the decision function of multiple ordinal classes simultaneously. In TOR, we consider both p(x) and p(y|x). In particular, using the p(x) of both labeled and unlabeled data, we avoid decision boundaries that lie in high density regions (i.e. p(x)) [?] by means of cluster assumption [?], [?]. In addition, the extension of classical OR to a Transductive OR paradigm is also non-trivial. To be more precise, current Transductive approaches are not designed to function well on ordinal regression (multi-class1 with ordering information) problems. Taking this cue, we present in this paper a novel transductive learning paradigm for ordinal regression [?], [?]. In particular, we formulate the ordinal-class problem as an extended binary classification problem, such that the ordinal constraints can be implicitly enforced. Subsequently, a proposed label swapping scheme for multiple class transduction is introduced to derive ordinal decision boundaries that pass through low density region of the augmented labeled and unlabeled data.\nA summary of some existing state-of-the-art ordinal regression approaches and the TSVM is outlined in Table I, where the major similarities and differences are explicitly identified with respect to ‘the type of decision boundaries’, ‘the number of classifiers to train for K ordinal classes’ and ‘whether or not cluster assumption and ordinal constraints are imposed’. Notably, TSVM requires K classifiers in order to learn the label of unlabeled data for all K classes at the same time. As such the training process of TSVM is much more time consuming and complex compared to ORs or TOR, since the latter approach only requires single classifier to be trained. Further, the prediction process of TSVM involves K classifiers and does not take the ordinal constraints into considerations.\n1For multi-class without ordering information, readers are referred to [?], [?], [?].\nWith only a single classifier, the training process of ORs and TOR is clearly more efficient.\nFor the sake of brevity, the core contributions of the present study are outlined as follows:\n1) A transductive learning paradigm of ordinal regression involving labeled and unlabeled data for learning ordinal decision functions is introduced. To the best of our knowledge, the present work serves as the first attempt that addresses the general ordinal regression problem in a transductive setting for a family of commonly used loss functions including hinge loss, logistic loss, Laplacian loss and others listed in Table II. 2) A label swapping scheme for multiple ordinal class transduction is introduced. The proof of strictly monotonic decrease in the objective function is also derived for the swapping scheme. The proposed transductive ordinal regression algorithm is thus established. 3) Numerical study showed that the TOR achieves significant accuracy improvements in terms of mean zero-one and absolute errors when pitted against other state-ofthe-art algorithms for ordinal regression and transductive support vector machines.\nThe rest of this paper is organized as follows: A brief introduction of ordinal regression is provided in Section II. Section III introduces the transductive ordinal regression (TOR) algorithm. Subsection III-A details the initialization of the pseudo-labels for unlabeled data, while the ordinal loss function used in transductive learning by means of label swapping to minimize the structural risk is described in subsection III-B. The parameters that control the importance of the labeled and unlabeled data used in the loss function are then discussed in subsection III-C. Section IV generalizes a family of well established binary functions as potential loss functions in TOR. An instantiation of TOR with hinge loss is also presented in the section. Extensive experimental results on four benchmark datasets and the real-world sentiment prediction problem are reported in Section V. Analysis and discussions pertaining to the experimental results are then provided in Section VI, while the brief conclusions of the present work are drawn in Section VII.\n3"
    }, {
      "heading" : "II. REVIEW OF ORDINAL REGRESSION",
      "text" : ""
    }, {
      "heading" : "A. Notation",
      "text" : "Throughout the rest of this paper, a superscript T denotes the transpose of a vector or a matrix. Given n labeled samples: (xi, y1), (x2, y2), ..., (xn, yn) in the data set, where xi ∈ ℜ\np represents the ith sample with ordinal class label yi ∈ {1, 2, ...,K}. Consider also a threshold model such as that depicted in Fig. 1, where a K ordinal class problem has K − 1 ordered thresholds: θ1 < θ2 < . . . < θK−1. Thus, a sample, x, is classified as Class i when the predictive output h(x) = wTx falls in the range of θi−1 < h(x) ≤ θi, where w ∈ ℜp, and θ0 = −∞ and θK = ∞ are typically assumed. For example, a Class 2 label implies an output that lies between θ1 and θ2."
    }, {
      "heading" : "B. Ordinal Regression as an Extended Binary Classification Model",
      "text" : "Ordinal regression using a threshold model generally considers the extended binary classification problem [?] of the form:\nx k i = (xi, ek) ∈ ℜ p+K−1, yki = 1− 2I[yi ≤ k],\n(1)\nfor k = 1, 2, ...,K − 1. Here ek ∈ ℜK−1 denotes a vector with the kth element as value 1 and the rest of the elements having value zero, and I[·] denotes an indicator function that returns 1 if the predicate holds, otherwise a zero is returned. Essentially, each labeled sample xi in the original data set is duplicated K − 1 times, and the kth copy is augmented with ek and is assigned with a binary label yki in the transformed problem.\nA binary classifier with a weight vector\nw̄ = (w,−θ) ∈ ℜp+K−1, (2)\nis then learned to predict yki such that (w,−θ) T x k i = w T xi − θk. Hence, the threshold θk of the threshold model is estimated using feature augmentation. Subsequently, the predictive ordinal class label of each sample, xi, is computed as:\nf(xi) = 1 +\nK−1∑\nk=1\nI[g(xki ) > 0] (3)\nwhere g(xki ) = w̄ T x k i = (w,−θ) T x k i = w T xi − θk = h(xi) − θk and I[·] is an indicator function that returns 1 if the predicate holds, otherwise a 0 is returned.\nIn this manner, besides inheriting the theoretical rigors of binary classifiers, typical caching and optimization techniques such as Sequential Minimal Optimization (SMO) [?], [?] can also be used in ordinal regression."
    }, {
      "heading" : "III. TRANSDUCTIVE ORDINAL REGRESSION",
      "text" : "In this section, we present the essential components of the proposed TOR algorithm for Ordinal Regression. In particular, we consider the ordinal regression problem where n labeled samples: (x1, yi), (x2, y2), ..., (xn, yn) and u unlabeled samples: xn+1,xn+2, ...,xn+u are available. In what follows, we introduce a novel transductive learning paradigm, referred to\nAlgorithm 1 Transductive Ordinal Regression (TOR) 1: Parameters: C1 2: Inputs: a training set including labeled and unlabeled\nsamples DL=(x1,y1),...,(xn,yn) and DU=xn+1,...,xn+u. 3: Outputs: predicted labels of DU\n// Initialization of unlabeled data’s class label 4: assign y∗ using Algorithm 2\n// transductive learning 5: set C2 =some small value (e.g. 10−5) 6: while C2 < C1 do 7: repeat 8: (w, θ):= solve (4) by fixing y∗\n9: for int k = 1;k < K;k++ do 10: if ∃(i, j) satisfying (5) then 11: if there is more than one (i, j), choose the one with the largest decrease in the loss value 12: yi = k + 1 13: yj = k 14: end if 15: end for 16: until no label is swapped 17: C2 = C2 ∗ 2 18: end while 19: return y∗\nhere as Transductive Ordinal Regression (TOR), for inferring the labels (y∗ = {yn+1, yn+2, ..., yn+u}) of u number of unlabeled data instances and modeling the prediction function, h(x), by minimizing the structural risk functional of the form:\nmin h,θ,y∗ τ(h, θ) + C1\nn∑\ni=1\nℓyi(h(xi), θ)\n+C2\nn+u∑\nj=n+1\nℓyj(h(xj), θ)\ns.t θk < θk+1 ∀k ∈ {1, ...,K − 2}\n(4)\nwhere τ is the regularizer that controls the complexity of h and θ, and C1 and C2 are the parameters that trade-offs the amount of regularization against the loss function ℓyi(·) on the labeled data and unlabeled data, respectively. Recall that ordinal regression involves a K class problem, hence the loss function in (4) can be represented by K loss functions, where each loss function represents a class depicted in Fig. 2. In another words, each sample, xi, with a class label, yi, possesses a loss function represented by ℓyi(h(xi), θ).\nThrough (4), TOR simultaneously learns the order of the decision boundaries, θ, and at the same time the pseudo-labels of unlabeled data with the decision boundaries are enforced\n4 Algorithm 2 Initialization of pseudo-labels for Unlabeled Data 1: Parameter: C1 2: Inputs: a training set including labeled and unlabeled\nsamples DL=(x1,y1),...,(xn,yn) and DU=xn+1,...,xn+u 3: Outputs: y∗ of DU\n// Start of algorithm 4: Count the number of samples numk in DL that fall into\nClass k and then compute ratiok = numk∑K i=1 numi\n5: (w, θ):=solve (4) with C2 = 0 (i.e. without DU ) 6: Compute the predicted value, wTxi, of ∀xi ∈ DU 7: Sort DU in ascending order of the predicted value to form\na sorted D∗U 8: for int k = 1;k<K;k++ do 9: assign the first ratiok of unassigned samples in D∗U\nwith label k 10: end for 11: assign the rest of unassigned samples in D∗U as K 12: return y∗\nto fall on low density regions of both labeled and unlabeled data, while satisfying the cluster assumption. In this manner, majority of the data vectors in the kth ordinal class would lie in the range of thresholds, θk−1 and θk, while loss function ℓyi(·) then caters to the remaining data (a.k.a, the outliers) that violates the cluster assumption.\nSolving (4) optimally would involve trying out all the possible combinations of assignment for y∗, resulting in a NP hard problem. Hereafter, (4) is solved by first finding h and θ while fixing y∗, then applying the swapping scheme to update y∗ and repeating the entire process until convergence is reached as outlined in Algorithm 1."
    }, {
      "heading" : "A. Pseudo-labels of Unlabeled Data Initialization",
      "text" : "The initialization phase of the TOR focuses on assigning initial pseudo-labels to the unlabeled data. By using a large margin criterion, the optimization problem may lead to trivial solutions, e.g., all unlabeled data are classified with positive labels [?], [?]. The common practice in transductive learning is to impose some class ratio constraints on the eventual labels of the unlabeled data (e.g., assuming balanced class distribution), where such an assumption has been shown to mitigate the issue of unbalanced output distribution and improves prediction performances [?]. Taking this cue, in the TOR, the pseudo-labels of the unlabeled data are constrained to match the class distribution of the labeled data. In particular, the constraints are fulfilled implicitly through the procedure of first training a supervised OR classifier on available labeled data and subsequently sorting the unlabeled data according to the values predicted by the trained supervised OR classifier. Pseudo-labels are then assigned to the sorted set with respect to the class distribution of the labeled data. The procedure to initialize the pseudo-labels of unlabeled data is outlined in Algorithm 2.\nLoss function for class yi\nLoss function for class yj\nFig. 3. Two consecutive class loss functions"
    }, {
      "heading" : "B. Transductive Learning by Label Swapping",
      "text" : "After the initialization phase to define the structural risk functional of (4), the minimization of (4) proceeds with a 2- steps label swapping procedure. The first involves fixing y∗ to solve h and θ. Next, both the derived h and θ are in-turn fixed to locate suitable y∗ that minimizes objective (4). In what follows, we define the criterion of the ordinal loss function to arrive at solution y∗ that minimizes objective (4).\nDefinition 1. Loss function ℓyi(·) is defined with the following properties:\n1) ∀i, j yi = yj − 1, h(xi) = h(xj), f(xj) < yj =⇒ ℓyi(h(xi), θ) < ℓyj(h(xj), θ) 2) ∀i, j yi = yj − 1, h(xi) = h(xj), f(xi) > yi =⇒ ℓyi(h(xi), θ) > ℓyj(h(xj), θ)\nDef. 1 defines the relationship between two consecutive classes. Referring to Fig. 2, a class k loss function is penalized in both directions. For example, the figure depicts a class 2 loss function consisting of a left and a right slanted line. In addition, the relationship between the left section (line) of two consecutive classes is depicted in Fig. 3 (which is a close up version of Fig. 2) and satisfies the first property of Def. 1. In particular, two adjacent class loss functions with the same predicative value, h, suggests the lower class loss function exhibits a smaller loss value, ℓ. In the same manner, the 2nd property of Def. 1 defines the right section of the loss function.\nUsing the loss function governed by Def. 1, in what follows, we present the details on minimizing the structural risk functional using the proposed label swapping scheme to reduce the loss term in (4). In order to minimize the objective of transductive ordinal regression in (4), the following proposition which extends Theorem 2 in [?] from binary class problems to K ordinal class problems, is introduced to cater for the ordinal constraints defined on the unlabeled data.\nProposition 2. For an ordinal loss function defined in Def. 1, swapping the label of two samples xi and xj from two adjacent classes yi and yj , i.e., yi = yj − 1, (4) observes a strictly monotonic decrease when f(xi) > yi and f(xj) < yj .\nProof: According to Def. 1, the first property assures ℓyj−1(h(xj), θ) < ℓyj (h(xj), θ) and the second property assures ℓyi+1(h(xi), θ) < ℓyi(h(xi), θ).\n5\nHence, ℓyi+1(h(xi), θ) + ℓyj−1(h(xj), θ) < ℓyi(h(xi), θ) + ℓyj(h(xj), θ) holds. Through swapping, the last term in (4) will follow a strictly monotonic decrease for fixed h and θ. After the swapping, a new decision function h′ and θ′ will be learned for (4). Since (4) is a minimization problem, we have:\nτ(h′, θ′) + C1\nn∑\ni=1\nℓyi(h ′(xi), θ ′) + C2\nn+u∑\nj=n+1\nℓyj(h ′(xj), θ ′)\n< τ(h, θ) + C1\nn∑\ni=1\nℓyi(h(xi), θ) + C2\nn+u∑\nj=n+1\nℓyj (h(xj), θ).\nMotivated by Proposition 2 and in the spirit of [?], we propose the swapping of labels between two consecutive classes (i.e. Class k and k + 1) on unlabeled data for a predictive function h and threshold values θ, when the following conditions have been met:\n∃i, j n+ 1 ≤ (i, j) ≤ n+ u, yi = k, yj = k + 1, f(xi) > yi , f(xj) < yj\n(5)\nThis ensures (4) to strictly decrease upon each swap. When more than a pair of (i,j) satisfying the conditions in (5) exists, the pair contributing to the largest decrease in the loss value is selected. Intuitively, this can be viewed as choosing the pair with highest information gain through the strategy.2"
    }, {
      "heading" : "C. Control Parameters",
      "text" : "C1 and C2 denote the control parameters of the proposed TOR detailed in Algorithm 1. In particular, C1 regulates the tradeoff between mis-classification errors on the labeled samples and the model complexity. In the same way, C2 regulates the tradeoff for the unlabeled samples. C1 denotes a user-specified parameter whereas C2 is heuristically derived in TOR. Typically, C2 is initialized with some small value and gradually increased to approach C1, in the spirit of [?]. This is a common heuristic strategy used to reduce the possibility of premature convergence and getting stuck in poor approximate solution when assigning the labels of the unlabeled data. Note that, when C2 tends to zero, the algorithm becomes a typical supervised learning problem. Therefore, increasing C2 gradually transforms the problem of ordinal regression to TOR. When the stopping criterion pertaining to C2 is reached in TOR, the assigned ordinal class label for the unlabeled data is deemed to converge. Hence, Algorithm 1 serves as a form of heuristic local search for solving (4) by means of approximation.\n2Note that the training time of this algorithm can be improved by swapping the labels from a set of unique pairs [?] since Proposition 2 guarantees the objective value in (4) to decrease. The study of improving the training time by swapping more than one pair for binary class problems has been shown in [?]. However, premature convergence might result in poor solutions. Hence, there is a tradeoff between the convergence of the training process and the quality of the solution by swapping more than one pairs. For simplicity, swapping only a pair of labels for each adjacent class is considered in the present study."
    }, {
      "heading" : "IV. GENERALIZING THE FAMILY OF BINARY LOSS",
      "text" : "FUNCTIONS IN TOR\nIn this section, we generalize a family of existing binary functions for use as potential loss function in TOR. In particular, subsection IV-A defines how K− 1 binary functions can be used as the loss function in TOR. Then, an instantiation of TOR with hinge loss is subsequently showcased in subsection IV-B. Next, label swapping of TOR for K ordinal problem is discussed in subsection IV-C."
    }, {
      "heading" : "A. Superimposing extended binary functions as the loss function of TOR",
      "text" : "Using the representation in the extended binary classification model, binary loss functions that fit in to fulfill the properties of Def. 1 (via superimposing K − 1 binary loss functions ℓyk\ni (·) defined for each extended binary class\nyki ∈ {−1, 1} of (1)) is as follows:\nℓyi(h(xi), θ) =\nK−1∑\nk=1\nℓyk i (g(xki )) (6)\nwhere xki is defined in (1) which incorporates θk. Each binary loss function, ℓyk\ni (·), has the following properties:\nDefinition 3. Binary loss function ℓyk i (·) is defined as follows:\n1) ∀a > 0 ℓ1(−a) > ℓ1(a), 2) ∀i ℓyk\ni (a) = ℓ−yk i (−a)\nIn Def. 3, the first property defines the binary loss function for yki = 1, where higher loss value is assigned to a misclassified sample relative to one that has been correctly inferred. The last property of Def. 3 defines symmetrical positive and negative class loss functions.\nProposition 4. The loss function superimposing K−1 binary loss functions that fulfills Def. 3 also fulfills Def. 1\nProof: Let us first prove the first property of Def. 1. We suppose that yi = yj − 1, h(xi) = h(xj) and f(xj) < yj . From (6), to prove ℓyi(h(xi), θ) < ℓyj(h(xj), θ) is the same as proving\n∑K−1 k=1 ℓyki (g(x k i ))− ∑K−1 k=1 ℓykj (g(x k j )) < 0.\nAssume, to the contrary, so\nK−1∑\nk=1\nℓyk i (g(xki ))−\nK−1∑\nk=1\nℓyk j (g(xkj )) ≥ 0,\n6 10 15 20 25 30 Class 2 loss function Class 3 loss function\n0\n5\n0 2 4 6 8 10 12 14 16 18 20\nh\nFig. 4. Loss function ℓyi(·) using the hinge loss and K = 5 with θ1 = 4, θ2 = 8, θ3 = 12, θ4 = 16.\nfrom (6), we have\nK−1∑\nk=1\nℓyk i (g(xki ))−\nK−1∑\nk=1\nℓyk j (g(xkj ))\n=\nyi−1∑\nk=1\nℓ1(g(x k i )) +\nK−1∑\nk=yi\nℓ−1(g(x k i ))−\nyi∑\nk=1\nℓ1(g(x k j ))\n−\nK−1∑\nk=yi+1\nℓ−1(g(x k j ))\n=\nyi−1∑\nk=1\nℓ1(g(x k i )) +\nK−1∑\nk=yi\nℓ−1(g(x k i ))−\nyi∑\nk=1\nℓ1(g(x k i ))\n−\nK−1∑\nk=yi+1\nℓ−1(g(x k i )) (since h(xi) = h(xj))\n= −ℓ1(g(x yi i )) + ℓ−1(g(x yi i )) = −ℓ1(g(x yi i )) + ℓ1(−g(x yi i )).\nThe last equality is derived from the second property of Def. 3. Since f(xj) < yj and yi = yj − 1, and from (3), we have ∑K−1 k=1 I[g(x k i ) > 0] < yi, which implies g(xyii ) < 0, or alternatively −g(x yi i ) > 0 > g(x yi i ). From the first property of Def. 3, we have ℓ1(−g(x yi i )) strictly less than ℓ1(g(x yi i )). Therefore, −ℓ1(g(x yi i )) + ℓ1(−g(x yi i )) < 0, indicates a contradiction. In the same manner, the second property of Def. 1 can be proven to hold.\nTherefore, a family of binary loss functions fulfilling the properties in Def. 3 summarized in, but not limited to Table II, can be used to minimize the structural risk functional of TOR framework in (4). The readers are referred to [?], [?] for more details on these loss functions."
    }, {
      "heading" : "B. An Instantiation of TOR using Hinge loss",
      "text" : "As mentioned in Section IV-A, our proposed framework can cater to several commonly used loss functions that satisfies Def. 3 to minimize the structural risk functional of (4). Here, we illustrate an instantiation of TOR based on the hinge loss, since it is commonly used in SVM and satisfies Def. 3. For a particular labeled data,{xi, yi}, and using the extended binary classification model representation with the bias term included\nin the decision function, the extended binary loss function ℓyk\ni (·) for a particular threshold θk can be derived as:\nmax{0, 1− yki (w̄ T x k i − b)} (7)\nwhere both the θk augmented xki and w̄ T are defined in (1) and (2), respectively. From (7), the ordinal loss function ℓyi(·) superimposing the K − 1 parts satisfies Def. 1 and becomes:\nK−1∑\nk=1\nmax{0, 1− yki (w̄ T x k i − b)} (8)\nas depicted in Fig. 4. Let τ(h, θ) = 1\n2 ‖w̄‖2 = 1 2 ‖w‖2 + 1 2 ‖θ‖2 (as derived from\n(2)) and the ordinal loss function ℓyi(·) as (8), then considering the structural risk of labeled data in (4), the extended binary classification formulation for ordinal regression [?] can be derived as:\nmin w,b,θ,ξk\ni\n1 2 ‖w‖2 + 1 2 ‖θ‖2 + C1\nn∑\ni=1\nK−1∑\nk=1\nξki ,\ns.t. yki (w Tφ(xi)− θk − b) ≥ 1− ξ k i ,\nξki ≥ 0, ∀i ∈ {1, ..., n}, k ∈ {1, ...,K − 1}, (9)\nwhere φ : ℜp 7→ F denotes the nonlinear feature mapping induced by a kernel function, and w is also in F . Thus, the decision functions in (9) become nonlinear by virtue of the kernel trick [?]. ξki denotes the slack variable that caters for the error committed by xi at the kth decision boundary.\nWith transductive learning, the labels of the unlabeled data in (4) through (9) are then optimized by:\nmin y,w,b,θ,ξk\ni\n1 2 ‖w‖2 + 1 2 ‖θ‖2 + C1\nn∑\ni=1\nK−1∑\nk=1\nξki\n+C2\nn+u∑\nj=n+1\nK−1∑\nk=1\nξkj ,\ns.t. yki (w Tφ(xi)− θk − b) ≥ 1− ξ k i ,\nξki ≥ 0, ∀i ∈ {1, ..., n}, k ∈ {1, ...,K − 1}, ykj (w Tφ(xj)− θk − b) ≥ 1− ξ k j , ξkj ≥ 0, ∀j ∈ {n+ 1, ..., n+ u}, k ∈ {1, ...,K − 1}.\n(10) Note that the ordered constraints on the thresholds in (4) are implicitly fulfilled in (9) and (10) (see the proof in [?]). Recall that, {yn+1, yn+2, ..., yn+u} is denoted by y∗. For a fixed y∗, the dual form of the inner minimization problem in (10) then becomes:\nmax α\nn+u∑\ni=1\nK−1∑\nk=1\nαki\n− 1\n2\nn+u∑\ni=1\nn+u∑\nj=1\nK−1∑\nk=1\nK−1∑\nk′=1\nαki α k′ j y k i y k′ j κ(x k i ,x k′ j )\ns.t. 0 ≤ αki ≤ C1, ∀i ∈ {1, ..., n}, k ∈ {1, ...,K − 1} 0 ≤ αkj ≤ C2, ∀j ∈ {n+ 1, ..., n+ u},\nk ∈ {1, ...,K − 1},∑n+u i=1 ∑K k=1 α k i y k i = 0,\n(11)\n7 where κ(xki ,x k j ′) = φ(xi) Tφ(x′j) + e T k ek\n′ is the resultant kernel evaluation of xki and x k j ′, and αki is the Lagrangian multiplier for the inequality constraint in (10). Note this dual is in the form of a quadratic programming (QP) problem, and thus can be easily solved using standard SVM solvers.\nIn Algorithm 1, one can use (10) to solve (4) while fixing y ∗ and then apply the swapping scheme (5) to update y∗. The entire process is then repeated until convergence is reached."
    }, {
      "heading" : "C. Discussion of label swapping for K ordinal class problem",
      "text" : "The proposition 2 for TOR is a generalization of K ordinal class problem, hence the proposition also applies to the binary class problems described in [?]. However, the TSVM in [?] cannot handle ordinal classification problems elegantly. For example, a data {x, y = 3} in a 5 class problem can be augmented to form binary data using (1) as {(x, e1), 1}, {(x, e2), 1}, {(x, e3),−1}, {(x, e4),−1}. However, swapping with another data vector may cause the dataset to violate the ordinal properties defined in (1) (e.g., {(x, e1),−1}, {(x, e2), 1}, {(x, e3),−1}, {(x, e4),−1}). In contrast, proposition 4 proved that TOR addresses this elegantly by generalizing the ordinal loss function to include commonly used binary loss functions."
    }, {
      "heading" : "V. EXPERIMENTS",
      "text" : "In this section, we investigate the efficacy of several state-ofthe-art ordinal regression algorithms and the proposed transductive ordinal regression, which are described in Table I, on a set of benchmark datasets and the task of sentiment prediction. Since existing ordinal regression models can deal with labeled data only, comparison to three ordinal state-of-the-art algorithms trained with labeled data, are also considered in the present study (namely RED-SVM3 using (9), SVOR-EXC4 and SVOR-IMC4). To investigate the effect of cluster assumption on the unlabeled data, comparison to the Multi-class transductive SVM (M-TSVM) [?] is also considered by using a multiclass training paradigm. In the experimental study, the MTSVM is trained using both labeled and unlabeled data based on a one-versus-rest approach. Since the performance of MTSVM is very sensitive to the balance constraints on the labels of the unlabeled data, a strategy similar to that proposed in Section III-A, i.e., taking the class ratio, ratiok , from the labeled data, as the balance constraints imposed on the labels of the unlabeled data, is also considered for M-TSVM. Taking the kth class for example, the constraint enforces the proportion of Class k to the rest of the unlabeled data as ratiok : 1 − ratiok. With the inclusion of M-TSVM, the impacts of ordinal knowledge on the performance metrics can be analyzed."
    }, {
      "heading" : "A. Experimental Setup",
      "text" : "For each data set, the labeled data are randomly split into different sizes (100, 150, 200, 250, 300, 350 and 400). Let s\n3http://www.work.caltech.edu/∼htlin/program/libsvm/#ordinal 4http://www.gatsby.ucl.ac.uk/∼chuwei/svor.htm\ndenotes the sample size of each dataset described in Tables III and IV, s− 400 samples then form the set of unlabeled data.\nThe cost parameter C1 of each algorithm is determined using a five-fold cross-validation procedure with log10C1 ∈ {−3,−2,−1, 0, 1, 2, 3, 4, 5}. To report statistically significant results on the unlabeled data, the average test performances of 20 independent realizations are presented.\nTo measure the classification error of the samples, mean zero-one error is employed as the performance metric and is defined as:\n1\nu\nn+u∑\ni=n+1\nI[y∗i 6= y t i ] (12)\nwhere I[·] denotes an indicator function that returns 1 if the predicate holds, otherwise a 0 is returned, and y∗i and y t i are the predicted label of the respective algorithm and the true class label, respectively.\nTo measure how far the predicted class label of the samples differ from their true class label, the mean absolute error is employed here as the performance metric, which is defined as:\n1\nu\nn+u∑\ni=n+1\n|y∗i − y t i | (13)\nwhere | · | denotes the absolute operation."
    }, {
      "heading" : "B. Benchmark data sets",
      "text" : "Four commonly used benchmark datasets5 (Abalone, Bank, California and Census) in ordinal regression problems are considered in the present study. The statistics of these benchmark datasets are summarized in Table III. These datasets were preprocessed with a quantization level of K = 5. For all algorithms, we considered the perceptron kernel [?], which is defined as follows:\n∆p − ||x− x ′ ||2,\nwhere ∆p denotes a constant. As discussed in [?], perceptron kernel can be used by SVM to construct infinite ensemble of classifiers over perceptrons. In other words, the resultant SVM classifier using perceptron kernel is equivalent to a neural network with one hidden layer containing infinite hidden neurons. Moreover, based on the Karush Kuhn Tucker (KKT) conditions, ∑n+u i=1 ∑K−1 k=1 α k i y k i = 0 as derived from (11), the term ∆p can be set to zero without changing the objective value of the dual SVM formulation [?]. As such, here we\n5http://www.liaad.up.pt/∼ltorgo/Regression/DataSets.html\n8\nconsider the simplified perceptron kernel with ∆p = 0 in the experimental study6"
    }, {
      "heading" : "C. Synthetic data set",
      "text" : "A synthetic data set with various degrees of cluster assumption is created based on our generator described in Algorithm 3 to study the performances of transductive TOR versus nontransduction RED-SVM.\nAlgorithm 3 Synthetic Data Set Generator 1: Inputs: y ∈ [1, ..K], where K is the number of ordinal\nclasses, p is a parameter to control the strength of cluster assumption\n2: for int d = 1; d ≤ 2000(K + 2); d++ do 3: if d ∈ [2000(y− 1), 2000(y+ 2)] then 4: if rand()< 0.01 then 5: xd =rand() 6: else 7: xd = 0 8: end if 9: else\n10: if rand()< 0.01p then 11: xd =rand() 12: else 13: xd = 0 14: end if 15: end if 16: end for 17: return x\nRecall that the cluster assumption holds when each class is more separable by a particular set of features, hence line 3 in Algorithm 3 defines the set of features Sy belonging to a particular class y. Specifically, a rand() function is used to generate a number xd, which is randomly drawn from a uniform distribution in the interval of 0 and 1. To simulate input vectors with < 0.01 probability of sparse features for xd ∈ Sy , we define xd = rand(), otherwise xd = 0. To define the degree of cluster assumption on feature xd /∈ Sy , we introduce parameter p and assign feature xd with some random at probability of 0.01p; otherwise, xd = 0. Note, a higher p value lead to greater overlapping among classes, thus a lower degree of cluster assumption. In the experiment, we consider p = (0.0, 0.1, ..., 0.9) and K = 5. We randomly generate 20 sets of 2500 examples, and use 200 examples as the labeled data while the remaining as unlabeled data. In addition, the data is normalized as x||x|| , and with linear kernel used in the experimental study."
    }, {
      "heading" : "D. Sentiment data sets",
      "text" : "The task of sentiment prediction is to predict the star rating of each review. The datasets for sentiment prediction7 as defined [?] were generated from Amazon.com, and comprise four categories of product reviews: Book, DVDs, Electronics and Kitchen appliances. The reviews consist of five ordinal rating label ranging from 1 to 5. A higher rating means a better review feedback. The details pertaining to the sample and feature size of the sentiment datasets are summarized in Table IV.\nIn the experimental study, we further preprocessed the datasets by removing all stop-words, normalizing each feature and performing stemming. Finally, each feature of a review is represented by its respective tf-idf value. The inner product of two reviews is defined using the cosine similarity, with linear kernel used in the experiments.\n6Perceptron kernel was reported to offer competitive results to Gaussian Kernel [?], but a benefit of perceptron kernel lies in the higher computational efficiency, which has been shown to be more than 10 times faster than Gaussian Kernel. Furthermore, perceptron kernel does not have any additional kernel parameter to be configured. In some previous study on ordinal regression problems [?], [?], the perceptron kernel was also reported to attain higher accuracies than using Gaussian Kernel.\n7www.cs.jhu.edu/∼mdredze/datasets/sentiment/\n9"
    }, {
      "heading" : "VI. DISCUSSIONS ON EXPERIMENTAL RESULTS",
      "text" : ""
    }, {
      "heading" : "A. Results on Benchmark and synthetic Datasets",
      "text" : "On the benchmark and synthetic datasets, we performed experiments for K = 5 to assess the predictive performance of various state-of-the-art algorithms. The experimental results of benchmark and synthetic datasets are discussed in subsections VI-A1 and VI-A2, respectively.\n1) Mean Zero-One and Absolute Errors on Benchmark Dataset: The results of mean zero-one error for each benchmark dataset are summarized in Fig. 5. As observed from the figures, both SVOR-IMC and SVOR-EXC exhibit similar results on all the datasets considered. RED-SVM on the other hand manifests significant improved performances over SVOR-IMC and SVOR-EXC on all the datasets, which is in line with that obtained in [?]. Notably, the proposed transductive ordinal regression algorithm, TOR, exhibits the best performances across all experiments. As shown in Fig. 5, TOR reports a minimum of 2% and up to 6% improvements, relative to SVOR-IMC and SVOR-EXC.\nAs discussed in [?], the data in high dimensional feature space such as text documents and sentiment data usually follows the cluster assumption. From the Table III and IV, the number of features of the Bank, Census and Sentiment data sets are higher. From the results reported in Fig. 5, we observed that the improvements of performance of TOR over RED-SVM are higher on the Bank and Census. This is possibly due to the Bank and Census having higher feature dimension so the datasets satisfy the cluster assumption better.\nOn the manifest of transductive learning, M-TSVM displays the worst performance on most of the experiments, relative to the other algorithms considered, especially on the California and Census datasets in Fig. 5. This is unsurprising since M-\nTSVM is designed to deal with multi-class problems that does not make use of ordinal information available in the data. Without the use of ordinal knowledge, transduction to infer the correct label of unlabeled data becomes ever more challenging.\nNext, we analyze the mean absolute errors of the benchmark regression dataset depicted in Fig. 6. The results indicate that M-TSVM, which does not impose any ordinal constraints, performed badly on all the datasets, as observed in the subfigures. On the other hand, algorithms that use the ordinal information are noted to attain competitive mean absolute errors. While emerging as superior in mean zero-one error, TOR did not top in terms of mean absolute error. We hypothesize this is due to the datasets containing continuous response variables, i.e., regression problems that have been manually quantized into 5 ranks. In Section VI-A2, we will validate our hypothesis on a synthetic dataset.\n2) Mean Zero-One and Absolute Errors on a Synthetic Ordinal Regression Dataset: Here, we analyze the label swapping procedure of the transductive approach, i.e., TOR, after the non-transductive approach, i.e., RED-SVM, using the class distribution of the labeled data for classification (the label initialization phase of TOR). The results summarized in Figure 7 indicate that the mean zero-one and absolute errors\n10\nof non-transductive RED-SVM approach deteriorates with decreasing degrees of cluster assumptions (i.e., configured via increasing parameter p). Similarly, the proposed transductive approach, i.e., TOR, which leverages the cluster assumption of the unlabeled data, exhibits lower improvements in mean zeroone and absolute errors when the degree of cluster assumption decreases (i.e., p ≥ 0.2), as depicted in Figure 8. On the other extreme, when the cluster assumption holds strong (i.e., p = 0), the improvements in both mean zero-one and absolute errors are observed to be smaller than that for p = 0.1. This can be reasoned by the decision boundaries of RED-SVM lying in the low density regions of the labeled and unlabeled data when the cluster assumption holds strong. Finally, when the cluster assumption does not hold (i.e., p ≥ 0.6), both transductive and non-transductive approaches fail.\nLater in Section VI-B, our experimental study shows that TOR attains significantly larger improvements over REDSVM in both mean zero-one and absolute errors on the real world sentiment datasets than on the benchmark datasets. The reason being that, similar to the synthetic data, the real world sentiment datasets are composed of sample data which lie in sparse high dimensional feature space, so the datasets satisfy the cluster assumption more rigorously than the benchmark datasets, since the latter contain continuous response variables that have been artificially quantized to form the ordinal labels.\n3) Sensitivity of C1 Parameter: In this subsection, we investigated the sensitivity of RED-SVM and TOR methods for different C1 parametric configurations, particularly in the discrete steps of log10C1 ∈ {−3,−2,−1, 0, 1, 2, 3}. We performed the experiments for K = 5 and with 400 labeled data. The results depicted in SubFigs. 9 (a) and (b) for Bank and Census datasets, respectively, denote the average test performances of 20 independent realizations. TOR is observed to achieve improved performance on all the settings considered, and exhibit a more stable mean zero-one error than RED-SVM across the range of C1 values. The performance of RED-SVM, on the other hand, is noted to be highly sensitive to the changes in C1 values. The robustness in TOR can be attributed to the learning from a fusion of labeled data and the density distribution estimated from the unlabeled data, when maximizing the margin of separation."
    }, {
      "heading" : "B. Results on Real World Sentiment Datasets",
      "text" : "Here, we apply the proposed TOR on a real world application, particularly, Sentiment ordinal classification datasets.\nSince SVOR-EXC and SVOR-IMC are not designed to handle the datasets with inputs that are of high dimensions like the sentiment datasets, these two algorithms are omitted from the experimental study. The results obtained on the remaining algorithms are then summarized in Fig. 10.\nNotably, TOR displayed superior performance over REDSVM, with at least 8% and up to 12% improvements in accuracy. Furthermore, even though TOR employs only a small number of 100 labeled data samples, complimented by the unlabeled data, a significantly lower error relative to REDSVM can be observed, despite the latter using a larger labeled data samples of 400. This observation clearly demonstrates the effectiveness of using unlabeled data in ordinal regression.\nThe mean absolute error metric defined in (13) is also reported for the sentiment dataset, as summarized in Fig. 11. It is worth noting that a mean absolute error larger than one indicates the average rating obtained differs from the true label by more than one rating scale. For example, RED-SVM with a mean absolute error close to one on labeled data of 100 indicates that the predicted labels of most samples differ from their respective true class labels by one unit. On the other hand, TOR is observed in Fig. 11 to exhibit significantly lower mean absolute error than the RED-SVM, thus suggesting that the predictions made by TOR are closer to the true labels on most data samples. Overall, TOR reports significantly lower mean absolute error than M-TSVM on all the datasets considered.\nAnother interesting observation that can be derived from Fig. 11 pertaining to limited labeled data available. Particularly, M-TSVM is shown to deliver a lower mean absolute error than RED-SVM under the condition of limited labeled data, which is made possible by complimenting the learning\n11\nprocess with the abundant of unlabeled data. As the number of available labeled data increases, the ordinal information learned by RED-SVM generally helps to lower the mean absolute errors as observed in Fig. 11. In contrast, TOR benefited through learning from both the ordinal knowledge and the density information of unlabeled data to arrive at the improvements in mean absolute error observed over REDSVM and M-TSVM.\nIn Figs. 10 and 11, the error bars representing the standard deviation are also presented8. As observed, the standard deviation obtained by the transductive algorithms, i.e., MTSVM and TOR, are generally smaller than the inductive RED-SVM algorithm, thus acknowledging the robustness of the transductive learning paradigm.\nNext, we analyze the label swapping procedure of the TOR in details by increasing the number of labels to be used to 2000. Fig. 12 depicts the effectiveness of label swapping after the label initialization. From the observations, label swapping effectively reduces the mean zero-one and absolute errors in Fig. 12(a) and 12(b), respectively, and while the number of labeled data increases, the improvements by TOR are decreasing. Another observation is that as the number of labeled data increases, the number of SVM training iterations within TOR will generally decrease as shown in Fig. 12(c). This is expected since as more labeled data are added into the training set, the decision boundaries become less affected by the unlabeled data. Therefore, the TOR is deem as more effective when only a small number of labeled data is available.\n8For other figures on benchmark datasets, there are too many comparison algorithms depicted in those figures. Hence, the errors bars are not provided.\nIn Fig. 12(c), it depicts the number of iterations for TOR to converge. Let T be the number of iterations for TOR to converge. The computational cost of TOR is then O(TR), where R be the computational cost of RED-SVM. However, it is notable here that the training process of TOR can be enhanced via a warm-start strategy, i.e., using the previous solution of the alpha variables as the initial alpha variables for the next iteration.\n12"
    }, {
      "heading" : "VII. CONCLUSION",
      "text" : "In this paper, by taking benefits from the abundance of unlabeled patterns, we had presented a novel transductive learning paradigm for ordinal regression, namely Transductive Ordinal Regression (TOR). To the best of our knowledge, the present work serves as the first attempt that addresses the general ordinal regression problem in a transductive setting for a family of ordinal loss functions. The family of ordinal loss functions including hinge loss, logistic loss and Laplacian loss are supported. A proposed label swapping scheme is also introduced to guarantee a strictly monotonic decrease in the objective value of the transductive ordinal function. Based on the experimental results obtained, TOR was reported to attain significant accuracy improvements over all the other algorithms considered via leveraging the cluster assumption on the unlabeled data and the ordinal constraints imposed to maximize the margin of separation between consecutive classes in ordinal regression. In situations where only few labeled data are available, TOR clearly serves as an indispensable tool."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2012,
    "abstractText" : "Ordinal regression is commonly formulated as a multi-class problem with ordinal constraints. The challenge of designing accurate classifiers for ordinal regression generally increases with the number of classes involved, due to the large number of labeled patterns that are needed. The availability of ordinal class labels, however, is often costly to calibrate or difficult to obtain. Unlabeled patterns, on the other hand, often exist in much greater abundance and are freely available. To take benefits from the abundance of unlabeled patterns, we present a novel transductive learning paradigm for ordinal regression in this paper, namely Transductive Ordinal Regression (TOR). The key challenge of the present study lies in the precise estimation of both the ordinal class label of the unlabeled data and the decision functions of the ordinal classes, simultaneously. The core elements of the proposed TOR include an objective function that caters to several commonly used loss functions casted in transductive settings, for general ordinal regression. A label swapping scheme that facilitates a strictly monotonic decrease in the objective function value is also introduced. Extensive numerical studies on commonly used benchmark datasets including the real world sentiment prediction problem are then presented to showcase the characteristics and efficacies of the proposed transductive ordinal regression. Further, comparisons to recent state-of-the-art ordinal regression methods demonstrate the introduced transductive learning paradigm for ordinal regression led to the robust and improved performance.",
    "creator" : "LaTeX with hyperref package"
  }
}