{
  "name" : "1606.03152.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Policy Networks with Two-Stage Training for Dialogue Systems",
    "authors" : [ "Mehdi Fatemi", "Layla El Asri", "Hannes Schulz", "Jing He", "Kaheer Suleman" ],
    "emails" : [ "first.last@maluuba.com" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "The statistical optimization of dialogue management in dialogue systems through Reinforcement Learning (RL) has been an active thread of re-\nsearch for more than two decades (Levin et al., 1997; Lemon and Pietquin, 2007; Laroche et al., 2010; Gašić et al., 2012; Daubigney et al., 2012). Dialogue management has been successfully modelled as a Partially Observable Markov Decision Process (POMDP) (Williams and Young, 2007; Gašić et al., 2012), which leads to systems that can learn from data and which are robust to noise. In this context, a dialogue between a user and a dialogue system is framed as a sequential process where, at each turn, the system has to act based on what it has understood so far of the user’s utterances.\nUnfortunately, POMDP-based dialogue managers have been unfit for online deployment because they typically require several thousands of dialogues for training (Gašić et al., 2010, 2012). Nevertheless, recent work has shown that it is possible to train a POMDP-based dialogue system on just a few hundred dialogues corresponding to online interactions with users (Gašić et al., 2013). However, in order to do so, pre-engineering efforts, prior RL knowledge, and domain expertise must be applied. Indeed, summary state and action spaces must be used and the set of actions must be restricted depending on the current state so that notoriously bad actions are prohibited.\nIn order to alleviate the need for a summary state space, deep RL has recently been applied to dialogue management (Mnih et al., 2013; Cuayáhuitl et al., 2015) in the context of negotiations. It was shown that deep RL performed significantly better than other heuristic or supervised approaches. The authors performed learning over a large action space of 70 actions and they also had to use restricted action sets in order to learn efficiently over this space. Besides, deep RL was not compared to other RL methods, which we do in this paper.\nIn this paper, we propose to alleviate the need\nar X\niv :1\n60 6.\n03 15\n2v 1\n[ cs\n.C L\n] 1\n0 Ju\nn 20\n16\nfor summary spaces and restricted actions using deep RL. We analyse four deep RL models: Deep Q Networks (DQN) (Mnih et al., 2013), Double DQN (DDQN) (van Hasselt et al., 2015), Deep Advantage Actor-Critic (DA2C) (Mnih et al., 2016) and a version of DA2C initialized with supervised learning (TDA2C)1 (Silver et al., 2016). All models are trained on a restaurant-seeking domain. We use the Dialogue State Tracking Challenge 2 (DSTC2) dataset to train an agenda-based user simulator (Schatzmann and Young, 2009) for online learning and to perform batch RL and supervised learning.\nWe first show that, on summary state and action spaces, deep RL converges faster than Gaussian Processes SARSA (GPSARSA) (Gašić et al., 2010). Then we show that deep RL enables us to work on the original state and action spaces. Indeed, contrary to methods such as GPSARSA, deep RL performs efficient generalization over the state space and memory requirements do not increase with the number of experiments. On the simple domain specified by DSTC2, we do not need to restrict the actions in order to learn efficiently. In order to remove the need for restricted actions in more complex domains, we advocate for the use of TDA2C and supervised learning as a pre-training step. We show that supervised learning on a small set of dialogues (only 706 dialogues) significantly bootstraps TDA2C and enables us to start learning with a policy that already selects only valid actions, which makes for a safe user experience in deployment. Therefore, we conclude that TDA2C is very appealing for the practical deployment of POMDP-based dialogue systems.\nIn Section 2 we briefly review POMDP, RL and GPSARSA. The value-based deep RL models investigated in this paper (DQN and DDQN) are described in Section 3. Policy networks and DA2C are discussed in Section 4. We then introduce the two-stage training of DA2C in Section 5. Experimental results are presented in Section 6. Finally, Section 7 concludes the paper and makes suggestions for future research."
    }, {
      "heading" : "2 Preliminaries",
      "text" : "The reinforcement learning problem consists of an environment (the user) and an agent (the system) (Sutton and Barto, 1998). The environment is de-\n1Teacher DA2C\nscribed as a set of continuous or discrete states S and at each state s ∈ S, the system can perform an action from an action spaceA(s). The actions can be continuous, but in our case they are assumed to be discrete and finite. At time t, as a consequence of an action At = a ∈ A(s), the state transitions from St = s to St+1 = s′ ∈ S. In addition, a reward signal Rt+1 = R(St, At, St+1) ∈ R provides feedback on the quality of the transition2. The agent’s task is to maximize at each state the expected discounted sum of rewards received after visiting this state. For this purpose, value functions are computed. The action-state value function Q is defined as:\nQπ(St, At) = Eπ[Rt+1 + γRt+2 + γ2Rt+3 + . . . | St = s,At = a], (1)\nwhere γ is a discount factor in [0, 1]. In this equation, the policy π specifies the system’s behaviour, i.e., it describes the agent’s action selection process at each state. A policy can be a deterministic mapping π(s) = a, which specifies the action a to be selected when state s is met. On the other hand, a stochastic policy provides a probability distribution over the action space at each state:\nπ(a|s) = P[At = a|St = s]. (2)\nThe agent’s goal is to find a policy that maximizes the Q-function at each state.\nIt is important to note that here the system does not have direct access to the state s. Instead, it sees this state through a perception process which typically includes an Automatic Speech Recognition (ASR) step, a Natural Language Understanding (NLU) step, and a State Tracking (ST) step. This perception process injects noise in the state of the system and it has been shown that modelling dialogue management as a POMDP helps to overcome this noise (Williams and Young, 2007; Young et al., 2013).\nWithin the POMDP framework, the state at time t, St, is not directly observable. Instead, the system has access to a noisy observation Ot.3 A POMDP is a tuple (S,A, P,R,O, Z, γ, b0) where S is the state space, A is the action space, P is the function encoding the transition probability:\n2In this paper, upper-case letters are used for random variables, lower-case letters for non-random values (known or unknown), and calligraphy letters for sets.\n3Here, the representation of the user’s goal and the user’s utterances.\nPa(s, s ′) = P(St+1 = s′ | St = s,At = a), R is the reward function,O is the observation space, Z encodes the observation probabilities Za(s, o) = P(Ot = o | St = s,At = a), γ is a discount factor, and b0 is an initial belief state. The belief state is a distribution over states. Starting from b0, the state tracker maintains and updates the belief state according to the observations perceived during the dialogue. The dialogue manager then operates on this belief state. Consequently, the value functions as well as the policy of the agent are computed on the belief states Bt:\nQπ(Bt, At) = Eπ ∑ t′≥t γt ′−tRt′+1 | Bt, At  π(a|b) = P[At = a|Bt = b]. (3)\nIn this paper, we use GPSARSA as a baseline as it has been proved to be a successful algorithm for training POMDP-based dialogue managers (Engel et al., 2005; Gašić et al., 2010). Formally, the Qfunction is modelled as a Gaussian process, entirely defined by a mean and a kernel: Q(B,A) ∼ GP(m, (k(B,A), k(B,A))). The mean is usually initialized at 0 and it is then jointly updated with the covariance based on the system’s observations (i.e., the visited belief states and actions, and the rewards). In order to avoid intractability in the number of experiments, we use kernel span sparsification (Engel et al., 2005). This technique consists of approximating the kernel on a dictionary of linearly independent belief states. This dictionary is incrementally built during learning. Kernel span sparsification requires setting a threshold on the precision to which the kernel is computed. As discussed in Section 6, this threshold needs to be fine-tuned for a good tradeoff between precision and performance."
    }, {
      "heading" : "3 Value-Based Deep Reinforcement Learning",
      "text" : "Broadly speaking, there are two main streams of methodologies in the RL literature: value approximation and policy gradients. As suggested by their names, the former tries to approximate the value function whereas the latter tries to directly approximate the policy. Approximations are necessary for large or continuous belief and action spaces. Indeed, if the belief space is large or continuous it might not be possible to store a value for each state in a table, so generalization over the state space is\nnecessary. In this context, some of the benefits of deep RL techniques are the following:\n• Generalisation over the belief space is efficient and the need for summary spaces is eliminated.\n• Memory requirements are limited and can be determined in advance unlike with methods such as GPSARSA.\n• Deep architectures with several hidden layers can be efficiently used for complex tasks and environments."
    }, {
      "heading" : "3.1 Deep Q Networks",
      "text" : "A Deep Q-Network (DQN) is a multi-layer neural network which maps a belief state Bt to the values of the possible actions At ∈ A(Bt = b) at that state, Qπ(Bt, At; wt), where wt is the weight vector of the neural network. Neural networks for the approximation of value functions have long been investigated (Bertsekas and Tsitsiklis, 1996). However, these methods were previously quite unstable (Mnih et al., 2013). In DQN, Mnih et al. (2013, 2015) proposed two techniques to overcome this instability-namely experience replay and the use of a target network. Experience replay is a method in which all the transitions are put in a finite pool D (Lin, 1993). Once the pool has reached its predefined maximum size, adding a new transition results in deleting the oldest transition in the pool. During training, a minibatch of transitions is uniformly sampled from the pool, i.e. (Bt, At, Rt+1, Bt+1) ∼ U(D). This method removes the instability arising from strong correlation between the subsequent transitions of an episode.4 Additionally, a target network with weight vector w− is used. This target network is similar to the Q-network except that its weights are only copied every τ steps from the Q-network, and remain fixed during all the other steps. The loss function for the Q-network at iteration t takes the following form:\nLt(wt) = E(Bt,At,Rt+1,Bt+1)∼U(D) [\n( Rt+1 + γmax\na′ Qπ(Bt+1, a ′;w−t )\n−Qπ(Bt, At;wt) )2 ] . (4)\n4Here, an episode is a dialogue."
    }, {
      "heading" : "3.2 Double DQN: Overcoming Overestimation and Instability of DQN",
      "text" : "The max operator in Equation 4 uses the same value network (i.e., the target network) to select actions and evaluate them. This increases the probability of overestimating the value of the state-action pairs (van Hasselt, 2010; van Hasselt et al., 2015). To see this more clearly, the target part of the loss in Equation 4 can be rewritten as follows:\nRt+1 + γQ π(Bt+1, argmax a Qπ(Bt+1, a;w − t );w − t ).\nIn this equation, the target network is used twice. Decoupling is possible by using the Q-network for action selection as follows (van Hasselt et al., 2015):\nRt+1 + γQ π(Bt+1, argmax a Qπ(Bt+1, a;wt);w − t ).\nThen, similarly to DQN, the Q-network is trained using experience replay and the target network is updated every τ steps. This new version of DQN, called Double DQN (DDQN), uses the two value networks in a decoupled manner, and alleviates the overestimation issue of DQN. This generally results in a more stable learning process (van Hasselt et al., 2015).\nIn the following section, we present deep RL models which perform policy search and output a stochastic policy rather than value approximation with a deterministic policy."
    }, {
      "heading" : "4 Policy Networks and Deep Advantage Actor-Critic (DA2C)",
      "text" : "A policy network is a parametrized probabilistic mapping between belief and action spaces:\nπθ(a|b) = π(a|b; θ) = P(At = a|Bt = b, θt = θ),\nwhere θ is the parameter vector (the weight vector of a neural network).5 In order to train policy networks, policy gradient algorithms have been developed (Williams, 1992; Sutton et al., 2000). Policy gradient algorithms are model-free methods which directly approximate the policy by parametrizing it. The parameters are learnt using a gradient-based optimization method.\nWe first need to define an objective function J that will lead the search for the parameters θ. This\n5For parametrization, we use w for value networks and θ for policy networks.\nobjective function defines policy quality. One way of defining it is to take the average over the rewards received by the agent. Another way is to compute the discounted sum of rewards for each trajectory, given that there is a designated start state. The policy gradient is then computed according to the Policy Gradient Theorem.\nTheorem 1 (Policy Gradient) For any differentiable policy πθ(b, a) and for the average reward or the start-state objective function, the policy gradient can be computed as\n∇θJ(θ) = Eπθ [∇θ log πθ(a|b)Q πθ(b, a)]. (5)\nPolicy gradient methods have been used successfully in different domains. Two recent examples are AlphaGo by DeepMind (Silver et al., 2016) and MazeBase by Facebook AI (Sukhbaatar et al., 2016).\nOne way to exploit Theorem 1 is to parametrize Qπθ(b, a) separately (with a parameter vector w) and learn the parameter vector during training in a similar way as in DQN. The trained Q-network can then be used for policy evaluation in Equation 5. Such algorithms are known in general as actor-critic algorithms, where theQ approximator is the critic and πθ is the actor (Sutton, 1984; Barto et al., 1990; Bhatnagar et al., 2009). This can be achieved with two separate deep neural networks: a Q-Network and a policy network.\nHowever, a direct use of Equation 5 with Q as critic is known to cause high variance (Williams, 1992). An important property of Equation 5 can be used in order to overcome this issue: subtracting any differentiable function Ba expressed over the belief space from Qπθ will not change the gradient. A good selection of Ba, which is called the baseline, can reduce the variance dramatically (Sutton and Barto, 1998). As a result, Equation 5 may be rewritten as follows:\n∇θJ(θ) = Eπθ [∇θ log πθ(a|b)Ad(b, a)], (6)\nwhere Ad(b, a) = Qπθ(b, a)−Ba(b) is called the advantage function. A good baseline is the value function V πθ , for which the advantage function becomes Ad(b, a) = Qπθ(b, a) − V πθ(b). However, in this setting, we need to train two separate networks to parametrize Qπθ and V πθ . A better approach is to use the TD error δ = Rt+1 + γV πθ(Bt+1)− V πθ(Bt) as advantage function. It can be proved that the expected value of the TD\nerror is Qπθ(b, a) − V πθ(b). If the TD error is used, only one network is needed, to parametrize V πθ(Bt) = V\nπθ(Bt;wt). We call this network the value network. We can use a DQN-like method to train the value network using both experience replay and a target network. For a transition Bt = b, At = a, Rt+1 = r and Bt+1 = b′, the advantage function is calculated as in:\nδt = r + γV πθ(b′;wt)− V πθ(b;wt). (7)\nBecause the gradient in Equation 6 is weighted by the advantage function, it may become quite large. In fact, the advantage function may act as a large learning rate. This can cause the learning process to become unstable. To avoid this issue, we add L2 regularization to the policy objective function. We call this method Deep Advantage Actor-Critic (DA2C).\nIn the next section, we show how this architecture can be used to efficiently exploit a small set of handcrafted data."
    }, {
      "heading" : "5 Two-stage Training of the Policy Network",
      "text" : "By definition, the policy network provides a probability distribution over the action space. As a result and in contrast to value-based methods such as DQN, a policy network can also be trained with direct supervised learning (Silver et al., 2016). Supervised training of RL agents has been wellstudied in the context of Imitation Learning (IL). In IL, an agent learns to reproduce the behaviour of an expert. Supervised learning of the policy was one of the first techniques used to solve this problem (Pomerleau, 1989; Amit and Mataric, 2002). This direct type of imitation learning requires that the learning agent and the expert share the same characteristics. If this condition is not met, IL can be done at the level of the value functions rather than the policy directly (Piot et al., 2015). In this paper, the data that we use (DSTC2) was collected with a dialogue system similar to the one we train so in our case, the demonstrator and the learner share the same characteristics.\nSimilarly to Silver et al. (2016), here, we initialize both the policy network and the value network on the data. The policy network is trained by minimising the categorical cross-entropy between the predicted action distribution and the demonstrated actions. The value network is trained directly through RL rather than IL to give more flex-\nibility in the kind of data we can use. Indeed, our goal is to collect a small number of dialogues and learn from them. IL usually assumes that the data corresponds to expert policies. However, dialogues collected with a handcrafted policy or in a Wizard-of-Oz (WoZ) setting often contain both optimal and sub-optimal dialogues and RL can be used to learn from all of these dialogues. Supervised training can also be done on these dialogues as we show in Section 6.\nSupervised actor-critic architectures following this idea have been proposed in the past (Benbrahim and Franklin, 1997; Si et al., 2004); the actor works together with a human supervisor to gain competence on its task even if the critic’s estimations are poor. For instance, a human can help a robot move by providing the robot with valid actions. We advocate for the same kind of methods for dialogue systems. It is easy to collect a small number of high-quality dialogues and then use supervised learning on this data to teach the system valid actions. This also eliminates the need to define restricted action sets."
    }, {
      "heading" : "6 Experiments",
      "text" : ""
    }, {
      "heading" : "6.1 Comparison of DQN and GPSARSA",
      "text" : ""
    }, {
      "heading" : "6.1.1 Experimental Protocol",
      "text" : "In this section, as a first argument in favour of deep RL, we perform a comparison between GPSARSA and DQN on simulated dialogues. We trained an agenda-based user simulator which at each dialogue turn, provides one or several dialogue act(s) in response to the latest machine act (Schatzmann et al., 2007; Schatzmann and Young, 2009). The dataset used for training this user-simulator is the Dialogue State Tracking Challenge 2 (DSTC2) (Henderson et al., 2014) dataset. State tracking is also trained on this dataset. DSTC2 includes dialogues with users who are searching for restaurants in Cambridge, UK.\nIn each dialogue, the user has a goal containing constraint slots and request slots. The constraint and request slots available in DSTC2 are listed in Table 1 in Appendix A. The constraints are the slots that the user has to provide to the system (for instance the user is looking for a specific type of food in a given area) and the requests are the slots that the user must receive from the system (for instance the user wants to know the address and phone number of the restaurant found by the system).\n0 5 10 15 0\n5\n10\n15 20 Av er ag e di al og ue le ng th\nDQN GPSARSA DQN-no-summary\n0 5 10 15 x1000 training dialogues\n2\n1\n0\n1\nAv er\nag e\nre wa\nrd s\n(a) Comparison of GPSARSA on summary spaces and DQN on summary (DQN) and original spaces (DQN-nosummary).\n0 5 10 15 0\n5\n10\n15\n20\nAv er\nag e\ndi al\nog ue\nle ng\nth\nDQN DDQN DA2C\n0 5 10 15 x1000 training dialogues\n2\n1\n0\n1\nAv er\nag e\nre wa\nrd s\n(b) Comparison of DA2C, DQN and DDQN on original spaces.\nFigure 1: Comparison of different algorithms on simulated dialogues, without any pre-training.\nSimilarly, the belief state is composed of two parts: constraints and requests. The constraint part includes the probabilities of the top two values for each constraint slot as returned by the state tracker (the value might be empty with a probability zero if the slot has not been mentioned). The request part, on the other hand, includes the probability of each request slot. For instance the constraint part might be [food: (Italian, 0.85) (Indian, 0.1) (Not mentioned, 0.05)] and the request part might be [area: 0.95] meaning that the user is probably looking for an Italian restaurant and that he wants to know the area of the restaurant found by the system. To compare DQN to GPSARSA, we work on a summary state space (Gašić et al., 2012, 2013). Each constraint is mapped to a one-hot vector, with 1 corresponding to the tuple in the grid vector gc = [(1, 0), (.8, .2), (.6, .2), (.6, .4), (.4, .4)] that minimizes the Euclidean distance to the top two probabilities. Similarly, each request slot is mapped to a one-hot vector according to the grid gr = [1, .8, .6, .4, 0.]. The final belief vector, known as the summary state, is defined as the concatenation of the constraint and request one-hot vectors. Each summary state is a binary vector of length 60 (12 one-hot vectors of length 5) and the total number of states is 512.\nWe also work on a summary action space and we use the act types listed in Table 2 in Appendix\nA. We add the necessary slot information as a post processing step. For example, the request act means that the system wants to request a slot from the user, e.g. request(food). In this case, the selection of the slot is based on min-max probability, i.e., the most ambiguous slot (which is the slot we want to request) is assumed to be the one for which the value with maximum probability has the minimum probability compared to the most certain values of the other slots. Note that this heuristic approach to compute the summary state and action spaces is a requirement to make GPSARSA tractable; it is a serious limitation in general and should be avoided.\nAs reward, we use a normalized scheme with a reward of +1 if the dialogue finishes successfully before 30 turns,6 a reward of -1 if the dialogue is not successful after 30 turns, and a reward of -0.03 for each turn. A reward of -1 is also distributed to the system if the user hangs up. In our settings, the user simulator hangs up every time the system proposes a restaurant which does not match at least one of his constraints.\nFor the deep Q-network, a Multi-Layer Perceptron (MLP) is used with two fully connected hidden layers, each having a tanh activation. The\n6A dialogue is successful if the user retrieves all the request slots for a restaurant matching all the constraints of his goal.\noutput layer has no activation and it provides the value for each of the summary machine acts. The summary machine acts are mapped to original acts using the heuristics explained previously. Both algorithms are trained with 15000 dialogues. GPSARSA is trained with -softmax exploration, which, with probability 1 − , selects an action based on the logistic distribution P[a|b] = eQ(b,a)∑\na′ e Q(b,a′) and, with probability , se-\nlects an action in a uniformly random way. From our experiments, this exploration scheme works best in terms of both convergence rate and variance. For DQN, we use a simple -greedy exploration which, with probability 1 − (same as above), uniformly selects an action and, with probability , selects an action maximizing the Qfunction. For both algorithms, is annealed to less than 0.1 over the course of training.\nIn a second experiment, we remove both summary state and action spaces for DQN, i.e., we do not perform the Euclidean-distance mapping as before but instead work directly on the probabilities themselves. Additionally, the state is augmented with the probability (returned by the state tracker) of each user act (see Table 3 in Appendix A), the dialogue turn, and the number of results returned by the database (0 if there was no query). Consequently, the state consists of 31 continuous values and two discrete values. The original action space is composed of 11 actions: offer7, select-area, select-food, select-pricerange, request-area, request-food, request-pricerange, expl-conf-area, expl-conf-food, expl-conf-pricerange, repeat. There is no post-processing via min-max selection anymore since the slot is part of the action, e.g., select-area.\nThe policies are evaluated after each 1000 training dialogues on 500 test dialogues without exploration."
    }, {
      "heading" : "6.1.2 Results",
      "text" : "Figure 1 illustrates the performance of DQN compared to GPSARSA. In our experiments with GPSARSA we found that it was difficult to find a good tradeoff between precision and efficiency.\n7This act consists of proposing a restaurant to the user. In order to be consistent with the DSTC2 dataset, an offer always contains the values for all the constraints understood by the system, e.g. offer(name = Super Ramen, food = Japanese, price range = cheap).\nIndeed, for low precision, the algorithm learned rapidly but did not reach optimal behaviour, whereas higher precision made learning extremely slow but resulted in better end-performance. On summary spaces, DQN outperforms GPSARSA in terms of convergence. Indeed, GPSARSA requires twice as many dialogues to converge. It is also worth mentioning here that the wall-clock training time of GPSARSA is considerably longer than the one of DQN due to kernel evaluation. The second experiment validates the fact that Deep RL can be efficiently trained directly on the belief state returned by the state tracker. Indeed, DQN on the original spaces performs as well as GPSARSA on the summary spaces.\nIn the next section, we train and compare the deep RL networks previously described on the original state and action spaces."
    }, {
      "heading" : "6.2 Comparison of the Deep RL Methods",
      "text" : ""
    }, {
      "heading" : "6.2.1 Experimental Protocol",
      "text" : "Similarly to the previous example, we work on a restaurant domain and use the DSTC2 specifications. We use −greedy exploration for all four algorithms with starting at 0.5 and being linearly annealed at a rate of λ = 0.99995. To speed up the learning process, the actions select-pricerange, select-area, and select-food are excluded from exploration. Note that this set does not depend on the state and is meant for exploration only. All the actions can be performed by the system at any moment.\nWe derived two datasets from DSTC2. The first dataset contains the 2118 dialogues of DSTC2. We had these dialogues rated by a human expert, based on the quality of dialogue management and on a scale of 0 to 3. The second dataset only contains the dialogues with a rating of 3 (706 dialogues). The underlying assumption is that these dialogues correspond to optimal policies.\nWe compare the convergence rates of the deep RL models in different settings. First, we compare DQN, DDQN and DA2C without any pretraining (Figure 1b). Then, we compare DQN, DDQN and TDA2C with an RL initialization on the DSTC2 dataset (Figure 2a). Finally, we focus on the advantage actor-critic models and compare DA2C, TDA2C, TDA2C with batch initialization on DSTC2, and TDA2C with batch initialization on the expert dialogues (Figure 2b).\n0 5 10 15 0\n5\n10\n15 20 Av er ag e di al og ue le ng th\nDDQN + Batch DQN + Batch DA2C + Batch\n0 5 10 15 x1000 training dialogues\n2\n1\n0\n1\nAv er\nag e\nre wa\nrd s\n(a) Comparison of DA2C, DQN and DDQN after batch initialization.\n0 5 10 15 0\n5\n10\n15\n20\nAv er\nag e\ndi al\nog ue\nle ng\nth\nSupExptBatchDA2C SupFullBatchDA2C BatchDA2C DA2C\n0 5 10 15 x1000 training dialogues\n2\n1\n0\n1\nAv er\nag e\nre wa\nrd s\n(b) Comparison of DA2C and DA2C after batch initialization (batchDA2C), and TDA2C after supervised training on expert (SupExptBatchDA2C) and non-expert data (SupFullBatchDA2C).\nFigure 2: Comparison of different algorithms on simulated dialogues, with pre-training."
    }, {
      "heading" : "6.2.2 Results",
      "text" : "As expected, DDQN converges faster than DQN on all experiments. Figure 1b shows that, without any pre-training, DA2C is the one which converges the fastest (6000 dialogues vs. 10000 dialogues for the other models). Figure 2a gives consistent results and shows that, with initial training on the 2118 dialogues of DSTC2, TDA2C converges significantly faster than the other models. Figure 2b focuses on DA2C and TDA2C. Compared to batch training, supervised training on DSTC2 speeds up convergence by 2000 dialogues (3000 dialogues vs. 5000 dialogues). Interestingly, there does not seem to be much difference between supervised training on the expert data and on DSTC2. The expert data only consists of 706 dialogues out of 2118 dialogues. Our observation is that, in the non-expert data, many of the dialogue acts chosen by the system were still appropriate, which explains that the system learns acceptable behavior from the entire dataset. This shows that supervised training, even when performed not only on optimal dialogues, makes learning much faster and relieves the need for restricted action sets. Valid actions are learnt from the dialogues and then RL exploits the good and bad dialogues to pursue training towards a high performing policy."
    }, {
      "heading" : "7 Concluding Remarks",
      "text" : "In this paper, we used policy networks for dialogue systems and trained them in a two-stage fashion: supervised training and batch reinforcement learning followed by online reinforcement learning. An important feature of policy networks is that they directly provide a probability distribution over the action space, which enables supervised training. We compared the results with other deep reinforcement learning algorithms, namely Deep Q Networks and Double Deep Q Networks. The combination of supervised and reinforcement learning is the main benefit of our method, which paves the way for developing trainable end-to-end dialogue systems. Supervised training on a small dataset considerably bootstraps the learning process and can be used to significantly improve the convergence rate of reinforcement learning in statistically optimised dialogue systems."
    }, {
      "heading" : "A Specifications of restaurant search in DTSC2",
      "text" : ""
    } ],
    "references" : [ {
      "title" : "Learning movement sequences from demonstration",
      "author" : [ "R. Amit", "M. Mataric." ],
      "venue" : "Proc. Int. Conf. on Development and Learning. pages 203–208.",
      "citeRegEx" : "Amit and Mataric.,? 2002",
      "shortCiteRegEx" : "Amit and Mataric.",
      "year" : 2002
    }, {
      "title" : "Biped dynamic walking using reinforcement learning",
      "author" : [ "H. Benbrahim", "J.A. Franklin." ],
      "venue" : "Robotics and Autonomous Systems 22:283–302.",
      "citeRegEx" : "Benbrahim and Franklin.,? 1997",
      "shortCiteRegEx" : "Benbrahim and Franklin.",
      "year" : 1997
    }, {
      "title" : "NeuroDynamic Programming",
      "author" : [ "D.P. Bertsekas", "J. Tsitsiklis." ],
      "venue" : "Athena Scientific.",
      "citeRegEx" : "Bertsekas and Tsitsiklis.,? 1996",
      "shortCiteRegEx" : "Bertsekas and Tsitsiklis.",
      "year" : 1996
    }, {
      "title" : "Natural Actor-Critic Algorithms",
      "author" : [ "S. Bhatnagar", "R. Sutton", "M. Ghavamzadeh", "M. Lee." ],
      "venue" : "Automatica 45(11).",
      "citeRegEx" : "Bhatnagar et al\\.,? 2009",
      "shortCiteRegEx" : "Bhatnagar et al\\.",
      "year" : 2009
    }, {
      "title" : "Strategic dialogue management via deep reinforcement learning",
      "author" : [ "H. Cuayáhuitl", "S. Keizer", "O. Lemon." ],
      "venue" : "arXiv:1511.08099 [cs.AI].",
      "citeRegEx" : "Cuayáhuitl et al\\.,? 2015",
      "shortCiteRegEx" : "Cuayáhuitl et al\\.",
      "year" : 2015
    }, {
      "title" : "A Comprehensive Reinforcement Learning Framework for Dialogue Management Optimisation",
      "author" : [ "L. Daubigney", "M. Geist", "S. Chandramohan", "O. Pietquin." ],
      "venue" : "IEEE Journal of Selected Topics in Signal Processing 6(8):891–",
      "citeRegEx" : "Daubigney et al\\.,? 2012",
      "shortCiteRegEx" : "Daubigney et al\\.",
      "year" : 2012
    }, {
      "title" : "Reinforcement learning with gaussian processes",
      "author" : [ "Y. Engel", "S. Mannor", "R. Meir." ],
      "venue" : "Proc. of ICML.",
      "citeRegEx" : "Engel et al\\.,? 2005",
      "shortCiteRegEx" : "Engel et al\\.",
      "year" : 2005
    }, {
      "title" : "On-line policy optimisation of bayesian spoken dialogue systems via human interaction",
      "author" : [ "M. Gašić", "C. Breslin", "M. Henderson", "D. Kim", "M. Szummer", "B. Thomson", "P. Tsiakoulis", "S.J. Young." ],
      "venue" : "Proc. of ICASSP. pages 8367–",
      "citeRegEx" : "Gašić et al\\.,? 2013",
      "shortCiteRegEx" : "Gašić et al\\.",
      "year" : 2013
    }, {
      "title" : "Policy optimisation of POMDP-based dialogue systems without state space compression",
      "author" : [ "M. Gašić", "M. Henderson", "B. Thomson", "P. Tsiakoulis", "S. Young." ],
      "venue" : "Proc. of SLT .",
      "citeRegEx" : "Gašić et al\\.,? 2012",
      "shortCiteRegEx" : "Gašić et al\\.",
      "year" : 2012
    }, {
      "title" : "Gaussian processes for fast policy optimisation of POMDP-based dialogue managers",
      "author" : [ "M. Gašić", "F. Jurčı́ček", "S. Keizer", "F. Mairesse", "B. Thomson", "K. Yu", "S. Young" ],
      "venue" : "In Proc. of SIGDIAL",
      "citeRegEx" : "Gašić et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Gašić et al\\.",
      "year" : 2010
    }, {
      "title" : "The Second Dialog State Tracking Challenge",
      "author" : [ "M. Henderson", "B. Thomson", "J. Williams." ],
      "venue" : "Proc. of SIGDIAL.",
      "citeRegEx" : "Henderson et al\\.,? 2014",
      "shortCiteRegEx" : "Henderson et al\\.",
      "year" : 2014
    }, {
      "title" : "Optimising a handcrafted dialogue system design",
      "author" : [ "R. Laroche", "G. Putois", "P. Bretier." ],
      "venue" : "Proc. of Interspeech.",
      "citeRegEx" : "Laroche et al\\.,? 2010",
      "shortCiteRegEx" : "Laroche et al\\.",
      "year" : 2010
    }, {
      "title" : "Machine learning for spoken dialogue systems",
      "author" : [ "O. Lemon", "O. Pietquin." ],
      "venue" : "Proc. of Interspeech. pages 2685–2688.",
      "citeRegEx" : "Lemon and Pietquin.,? 2007",
      "shortCiteRegEx" : "Lemon and Pietquin.",
      "year" : 2007
    }, {
      "title" : "Reinforcement learning for robots using neural networks",
      "author" : [ "L-J Lin." ],
      "venue" : "Ph.D. thesis, Carnegie Mellon University.",
      "citeRegEx" : "Lin.,? 1993",
      "shortCiteRegEx" : "Lin.",
      "year" : 1993
    }, {
      "title" : "Asynchronous methods for deep reinforcement learning",
      "author" : [ "V. Mnih", "A.P. Badia", "M. Mirza", "A. Graves", "T.P. Lillicrap", "T. Harley", "D. Silver", "K. Kavukcuoglu." ],
      "venue" : "Arxiv:1602.01783.",
      "citeRegEx" : "Mnih et al\\.,? 2016",
      "shortCiteRegEx" : "Mnih et al\\.",
      "year" : 2016
    }, {
      "title" : "Playing Atari with deep reinforcement learning",
      "author" : [ "V Mnih", "K. Kavukcuoglu", "D. Silver", "A. Graves", "I Antonoglou", "D. Wierstra", "M. Riedmiller." ],
      "venue" : "NIPS Deep Learning Workshop.",
      "citeRegEx" : "Mnih et al\\.,? 2013",
      "shortCiteRegEx" : "Mnih et al\\.",
      "year" : 2013
    }, {
      "title" : "Human-level control through deep reinforcement learning",
      "author" : [ "D. Hassabis" ],
      "venue" : "Nature",
      "citeRegEx" : "Hassabis.,? \\Q2015\\E",
      "shortCiteRegEx" : "Hassabis.",
      "year" : 2015
    }, {
      "title" : "Imitation Learning Applied to Embodied Conversational Agents",
      "author" : [ "B. Piot", "M. Geist", "O. Pietquin." ],
      "venue" : "Proc. of MLIS.",
      "citeRegEx" : "Piot et al\\.,? 2015",
      "shortCiteRegEx" : "Piot et al\\.",
      "year" : 2015
    }, {
      "title" : "Alvinn: An autonomous land vehicle in a neural network",
      "author" : [ "D.A. Pomerleau." ],
      "venue" : "Proc. of NIPS. pages 305–313.",
      "citeRegEx" : "Pomerleau.,? 1989",
      "shortCiteRegEx" : "Pomerleau.",
      "year" : 1989
    }, {
      "title" : "Agenda-based user simulation for bootstrapping a POMDP dialogue system",
      "author" : [ "J. Schatzmann", "B. Thomson", "K. Weilhammer", "H. Ye", "S. Young." ],
      "venue" : "Proc. of NAACL HLT . pages 149–152.",
      "citeRegEx" : "Schatzmann et al\\.,? 2007",
      "shortCiteRegEx" : "Schatzmann et al\\.",
      "year" : 2007
    }, {
      "title" : "The hidden agenda user simulation model",
      "author" : [ "J. Schatzmann", "S. Young." ],
      "venue" : "Proc. of TASLP 17(4):733–747.",
      "citeRegEx" : "Schatzmann and Young.,? 2009",
      "shortCiteRegEx" : "Schatzmann and Young.",
      "year" : 2009
    }, {
      "title" : "Mastering the game of go with deep neural networks and tree search",
      "author" : [ "K. Kavukcuoglu", "T. Graepel", "D. Hassabis." ],
      "venue" : "Nature 529(7587):484–489.",
      "citeRegEx" : "Kavukcuoglu et al\\.,? 2016",
      "shortCiteRegEx" : "Kavukcuoglu et al\\.",
      "year" : 2016
    }, {
      "title" : "Mazebase: A sandbox for learning from games",
      "author" : [ "S. Sukhbaatar", "A. Szlam", "G. Synnaeve", "S. Chintala", "R. Fergus." ],
      "venue" : "arxiv.org/pdf/1511.07401 [cs.LG].",
      "citeRegEx" : "Sukhbaatar et al\\.,? 2016",
      "shortCiteRegEx" : "Sukhbaatar et al\\.",
      "year" : 2016
    }, {
      "title" : "Temporal credit assignment in reinforcement learning",
      "author" : [ "R.S. Sutton." ],
      "venue" : "Ph.D. thesis, University of Massachusetts at Amherst, Amherst, MA, USA.",
      "citeRegEx" : "Sutton.,? 1984",
      "shortCiteRegEx" : "Sutton.",
      "year" : 1984
    }, {
      "title" : "Policy gradient methods for reinforcement learning with function approximation",
      "author" : [ "R.S. Sutton", "D. McAllester", "S. Singh", "Y. Mansour." ],
      "venue" : "Proc. of NIPS. volume 12, pages 1057– 1063.",
      "citeRegEx" : "Sutton et al\\.,? 2000",
      "shortCiteRegEx" : "Sutton et al\\.",
      "year" : 2000
    }, {
      "title" : "Reinforcement Learning",
      "author" : [ "R.S. Sutton", "A.G. Barto." ],
      "venue" : "MIT Press.",
      "citeRegEx" : "Sutton and Barto.,? 1998",
      "shortCiteRegEx" : "Sutton and Barto.",
      "year" : 1998
    }, {
      "title" : "Double q-learning",
      "author" : [ "H. van Hasselt." ],
      "venue" : "Proc. of NIPS. pages 2613–2621.",
      "citeRegEx" : "Hasselt.,? 2010",
      "shortCiteRegEx" : "Hasselt.",
      "year" : 2010
    }, {
      "title" : "Deep reinforcement learning with double Qlearning",
      "author" : [ "H. van Hasselt", "A. Guez", "D. Silver." ],
      "venue" : "arXiv:1509.06461v3 [cs.LG].",
      "citeRegEx" : "Hasselt et al\\.,? 2015",
      "shortCiteRegEx" : "Hasselt et al\\.",
      "year" : 2015
    }, {
      "title" : "Partially observable markov decision processes for spoken dialog systems",
      "author" : [ "J.D. Williams", "S. Young." ],
      "venue" : "Proc. of CSL 21:231–422.",
      "citeRegEx" : "Williams and Young.,? 2007",
      "shortCiteRegEx" : "Williams and Young.",
      "year" : 2007
    }, {
      "title" : "Simple statistical gradientfollowing algorithms for connectionist reinforcement learning",
      "author" : [ "R.J. Williams." ],
      "venue" : "Machine Learning 8:229– 256.",
      "citeRegEx" : "Williams.,? 1992",
      "shortCiteRegEx" : "Williams.",
      "year" : 1992
    }, {
      "title" : "POMDP-based statistical spoken dialog systems: A review",
      "author" : [ "S. Young", "M. Gasic", "B. Thomson", "J. Williams." ],
      "venue" : "Proc. IEEE 101(5):1160– 1179.",
      "citeRegEx" : "Young et al\\.,? 2013",
      "shortCiteRegEx" : "Young et al\\.",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 12,
      "context" : "The statistical optimization of dialogue management in dialogue systems through Reinforcement Learning (RL) has been an active thread of research for more than two decades (Levin et al., 1997; Lemon and Pietquin, 2007; Laroche et al., 2010; Gašić et al., 2012; Daubigney et al., 2012).",
      "startOffset" : 172,
      "endOffset" : 284
    }, {
      "referenceID" : 11,
      "context" : "The statistical optimization of dialogue management in dialogue systems through Reinforcement Learning (RL) has been an active thread of research for more than two decades (Levin et al., 1997; Lemon and Pietquin, 2007; Laroche et al., 2010; Gašić et al., 2012; Daubigney et al., 2012).",
      "startOffset" : 172,
      "endOffset" : 284
    }, {
      "referenceID" : 8,
      "context" : "The statistical optimization of dialogue management in dialogue systems through Reinforcement Learning (RL) has been an active thread of research for more than two decades (Levin et al., 1997; Lemon and Pietquin, 2007; Laroche et al., 2010; Gašić et al., 2012; Daubigney et al., 2012).",
      "startOffset" : 172,
      "endOffset" : 284
    }, {
      "referenceID" : 5,
      "context" : "The statistical optimization of dialogue management in dialogue systems through Reinforcement Learning (RL) has been an active thread of research for more than two decades (Levin et al., 1997; Lemon and Pietquin, 2007; Laroche et al., 2010; Gašić et al., 2012; Daubigney et al., 2012).",
      "startOffset" : 172,
      "endOffset" : 284
    }, {
      "referenceID" : 28,
      "context" : "Dialogue management has been successfully modelled as a Partially Observable Markov Decision Process (POMDP) (Williams and Young, 2007; Gašić et al., 2012), which leads to systems that can learn from data and which are robust to noise.",
      "startOffset" : 109,
      "endOffset" : 155
    }, {
      "referenceID" : 8,
      "context" : "Dialogue management has been successfully modelled as a Partially Observable Markov Decision Process (POMDP) (Williams and Young, 2007; Gašić et al., 2012), which leads to systems that can learn from data and which are robust to noise.",
      "startOffset" : 109,
      "endOffset" : 155
    }, {
      "referenceID" : 7,
      "context" : "Nevertheless, recent work has shown that it is possible to train a POMDP-based dialogue system on just a few hundred dialogues corresponding to online interactions with users (Gašić et al., 2013).",
      "startOffset" : 175,
      "endOffset" : 195
    }, {
      "referenceID" : 15,
      "context" : "In order to alleviate the need for a summary state space, deep RL has recently been applied to dialogue management (Mnih et al., 2013; Cuayáhuitl et al., 2015) in the context of negotiations.",
      "startOffset" : 115,
      "endOffset" : 159
    }, {
      "referenceID" : 4,
      "context" : "In order to alleviate the need for a summary state space, deep RL has recently been applied to dialogue management (Mnih et al., 2013; Cuayáhuitl et al., 2015) in the context of negotiations.",
      "startOffset" : 115,
      "endOffset" : 159
    }, {
      "referenceID" : 15,
      "context" : "We analyse four deep RL models: Deep Q Networks (DQN) (Mnih et al., 2013), Double DQN (DDQN) (van Hasselt et al.",
      "startOffset" : 54,
      "endOffset" : 73
    }, {
      "referenceID" : 14,
      "context" : ", 2015), Deep Advantage Actor-Critic (DA2C) (Mnih et al., 2016) and a version of DA2C initialized with supervised learning (TDA2C)1 (Silver et al.",
      "startOffset" : 44,
      "endOffset" : 63
    }, {
      "referenceID" : 20,
      "context" : "We use the Dialogue State Tracking Challenge 2 (DSTC2) dataset to train an agenda-based user simulator (Schatzmann and Young, 2009) for online learning and to perform batch RL and supervised learning.",
      "startOffset" : 103,
      "endOffset" : 131
    }, {
      "referenceID" : 9,
      "context" : "We first show that, on summary state and action spaces, deep RL converges faster than Gaussian Processes SARSA (GPSARSA) (Gašić et al., 2010).",
      "startOffset" : 121,
      "endOffset" : 141
    }, {
      "referenceID" : 25,
      "context" : "The reinforcement learning problem consists of an environment (the user) and an agent (the system) (Sutton and Barto, 1998).",
      "startOffset" : 99,
      "endOffset" : 123
    }, {
      "referenceID" : 28,
      "context" : "This perception process injects noise in the state of the system and it has been shown that modelling dialogue management as a POMDP helps to overcome this noise (Williams and Young, 2007; Young et al., 2013).",
      "startOffset" : 162,
      "endOffset" : 208
    }, {
      "referenceID" : 30,
      "context" : "This perception process injects noise in the state of the system and it has been shown that modelling dialogue management as a POMDP helps to overcome this noise (Williams and Young, 2007; Young et al., 2013).",
      "startOffset" : 162,
      "endOffset" : 208
    }, {
      "referenceID" : 6,
      "context" : "In this paper, we use GPSARSA as a baseline as it has been proved to be a successful algorithm for training POMDP-based dialogue managers (Engel et al., 2005; Gašić et al., 2010).",
      "startOffset" : 138,
      "endOffset" : 178
    }, {
      "referenceID" : 9,
      "context" : "In this paper, we use GPSARSA as a baseline as it has been proved to be a successful algorithm for training POMDP-based dialogue managers (Engel et al., 2005; Gašić et al., 2010).",
      "startOffset" : 138,
      "endOffset" : 178
    }, {
      "referenceID" : 6,
      "context" : "In order to avoid intractability in the number of experiments, we use kernel span sparsification (Engel et al., 2005).",
      "startOffset" : 97,
      "endOffset" : 117
    }, {
      "referenceID" : 2,
      "context" : "Neural networks for the approximation of value functions have long been investigated (Bertsekas and Tsitsiklis, 1996).",
      "startOffset" : 85,
      "endOffset" : 117
    }, {
      "referenceID" : 15,
      "context" : "However, these methods were previously quite unstable (Mnih et al., 2013).",
      "startOffset" : 54,
      "endOffset" : 73
    }, {
      "referenceID" : 13,
      "context" : "Experience replay is a method in which all the transitions are put in a finite pool D (Lin, 1993).",
      "startOffset" : 86,
      "endOffset" : 97
    }, {
      "referenceID" : 29,
      "context" : "5 In order to train policy networks, policy gradient algorithms have been developed (Williams, 1992; Sutton et al., 2000).",
      "startOffset" : 84,
      "endOffset" : 121
    }, {
      "referenceID" : 24,
      "context" : "5 In order to train policy networks, policy gradient algorithms have been developed (Williams, 1992; Sutton et al., 2000).",
      "startOffset" : 84,
      "endOffset" : 121
    }, {
      "referenceID" : 22,
      "context" : ", 2016) and MazeBase by Facebook AI (Sukhbaatar et al., 2016).",
      "startOffset" : 36,
      "endOffset" : 61
    }, {
      "referenceID" : 23,
      "context" : "Such algorithms are known in general as actor-critic algorithms, where theQ approximator is the critic and πθ is the actor (Sutton, 1984; Barto et al., 1990; Bhatnagar et al., 2009).",
      "startOffset" : 123,
      "endOffset" : 181
    }, {
      "referenceID" : 3,
      "context" : "Such algorithms are known in general as actor-critic algorithms, where theQ approximator is the critic and πθ is the actor (Sutton, 1984; Barto et al., 1990; Bhatnagar et al., 2009).",
      "startOffset" : 123,
      "endOffset" : 181
    }, {
      "referenceID" : 29,
      "context" : "However, a direct use of Equation 5 with Q as critic is known to cause high variance (Williams, 1992).",
      "startOffset" : 85,
      "endOffset" : 101
    }, {
      "referenceID" : 25,
      "context" : "A good selection of Ba, which is called the baseline, can reduce the variance dramatically (Sutton and Barto, 1998).",
      "startOffset" : 91,
      "endOffset" : 115
    }, {
      "referenceID" : 18,
      "context" : "Supervised learning of the policy was one of the first techniques used to solve this problem (Pomerleau, 1989; Amit and Mataric, 2002).",
      "startOffset" : 93,
      "endOffset" : 134
    }, {
      "referenceID" : 0,
      "context" : "Supervised learning of the policy was one of the first techniques used to solve this problem (Pomerleau, 1989; Amit and Mataric, 2002).",
      "startOffset" : 93,
      "endOffset" : 134
    }, {
      "referenceID" : 17,
      "context" : "If this condition is not met, IL can be done at the level of the value functions rather than the policy directly (Piot et al., 2015).",
      "startOffset" : 113,
      "endOffset" : 132
    }, {
      "referenceID" : 0,
      "context" : "Supervised learning of the policy was one of the first techniques used to solve this problem (Pomerleau, 1989; Amit and Mataric, 2002). This direct type of imitation learning requires that the learning agent and the expert share the same characteristics. If this condition is not met, IL can be done at the level of the value functions rather than the policy directly (Piot et al., 2015). In this paper, the data that we use (DSTC2) was collected with a dialogue system similar to the one we train so in our case, the demonstrator and the learner share the same characteristics. Similarly to Silver et al. (2016), here, we initialize both the policy network and the value network on the data.",
      "startOffset" : 111,
      "endOffset" : 613
    }, {
      "referenceID" : 1,
      "context" : "Supervised actor-critic architectures following this idea have been proposed in the past (Benbrahim and Franklin, 1997; Si et al., 2004); the actor works together with a human supervisor to gain competence on its task even if the critic’s estimations are poor.",
      "startOffset" : 89,
      "endOffset" : 136
    }, {
      "referenceID" : 19,
      "context" : "We trained an agenda-based user simulator which at each dialogue turn, provides one or several dialogue act(s) in response to the latest machine act (Schatzmann et al., 2007; Schatzmann and Young, 2009).",
      "startOffset" : 149,
      "endOffset" : 202
    }, {
      "referenceID" : 20,
      "context" : "We trained an agenda-based user simulator which at each dialogue turn, provides one or several dialogue act(s) in response to the latest machine act (Schatzmann et al., 2007; Schatzmann and Young, 2009).",
      "startOffset" : 149,
      "endOffset" : 202
    }, {
      "referenceID" : 10,
      "context" : "The dataset used for training this user-simulator is the Dialogue State Tracking Challenge 2 (DSTC2) (Henderson et al., 2014) dataset.",
      "startOffset" : 101,
      "endOffset" : 125
    } ],
    "year" : 2017,
    "abstractText" : "In this paper, we propose to use deep policy networks which are trained with an advantage actor-critic method for statistically optimised dialogue systems. First, we show that, on summary state and action spaces, deep Reinforcement Learning (RL) outperforms Gaussian Processes methods. Summary state and action spaces lead to good performance but require pre-engineering effort, RL knowledge, and domain expertise. In order to remove the need to define such summary spaces, we show that deep RL can also be trained efficiently on the original state and action spaces. Dialogue systems based on partially observable Markov decision processes are known to require many dialogues to train, which makes them unappealing for practical deployment. We show that a deep RL method based on an actor-critic architecture can exploit a small amount of data very efficiently. Indeed, with only a few hundred dialogues collected with a handcrafted policy, the actorcritic deep learner is considerably bootstrapped from a combination of supervised and batch RL. In addition, convergence to an optimal policy is significantly sped up compared to other deep RL methods initialized on the data with batch RL. All experiments are performed on a restaurant domain derived from the Dialogue State Tracking Challenge 2 (DSTC2) dataset.",
    "creator" : "LaTeX with hyperref package"
  }
}