{
  "name" : "1609.03663.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "An Experimental Study of LSTM Encoder-Decoder Model for Text Simplification",
    "authors" : [ "Tong Wang", "Ping Chen", "Kevin Amaral", "Jipeng Qiang" ],
    "emails" : [ "tong.wang001@umb.edu", "ping.chen@umb.edu", "kevin.amaral001@umb.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Keywords: Text Simplification, LSTM, Encoder-Decoder"
    }, {
      "heading" : "1 Introduction",
      "text" : "Text Simplification (TS) aims to simplify the lexical, grammatical, or structural complexity of text while retaining its semantic meaning. It can help various groups of people, including children, non-native speakers, and people with cognitive disabilities, to understand text better. The field of automatic text simplification has been researched for decades. It is generally divided into three categories: lexical simplification (LS), rulebased, and machine translation (MT) [1].\nLS is mainly used to simplify text by substituting infrequent and difficult words with frequent and easier words. However, challenges exist for the LS approach. First, a\ngreat number of transformation rules are required for reasonable coverage; second, different transformation rules should be applied based on the specific context; third, the syntax and semantic meaning of the sentence is hard to retain. Rule-based approaches use hand-crafted rules for lexical and syntactic simplification, for example, substituting difficult words in a predefined vocabulary. However, such approaches need a lot of humaninvolvement to manually define these rules, and it is impossible to give all possible simplification rules. MT-based approach regards original English and simplified English as two different languages, thus TS is the process to translate ordinary English to simplified English. Neural Machine Translation (NMT) is a newly-proposed deep learning approach and achieves very impressive results [2; 3; 4]. Unlike the traditional phrased-based MT system which operates on small components separately, NMT systems attempt to build a large neural network such that every component is tuned based on the training sentence pairs.\nNMT models are types of EncoderDecoder models, which can represent the input sequence as a vector, and then decode that vector into an output sequence. In this paper, we propose to apply Long Short-Term Memory (LSTM) [5] EncoderDecoder on TS task. And we show the LSTM Encoder-Decoder model is able to learn operation rules such as reversing, sorting, and replacing from sequence pairs, which are similar to simplification rules that change sentence structure, substitute words, and remove words. Thus this model is potentially able to learn simplification rules. We conduct experiments to show that the trained model has a high accuracy for reversal, sortar X\niv :1\n60 9.\n03 66\n3v 1\n[ cs\n.C L\n] 1\n3 Se\np 20\n16\ning, and sequence replacement. Also, the word embeddings learned from the model are close to its real meaning."
    }, {
      "heading" : "2 Related Work",
      "text" : "Automatic TS is a complicated natural language processing (NLP) task, it consists of lexical and syntactic simplification levels. Usually, hand-crafted, supervised, and unsupervised methods based on resources like English Wikipedia (EW) and Simple English Wikipedia (SEW) [6] are utilized for extracting simplification rules. It is very easy to mix up the automatic TS task and the automatic summarization task [7; 8]. TS is different from text summarization as the focus of text summarization is to reduce the length and redundant content.\nAt the lexical level, [9] proposed an lexical simplification system which only requires a large corpus of regular text to obtain word embeddings to get words similar to the complex word. [10] proposed an unsupervised method for learning pairs of complex and simpler synonyms and a context aware method for substituting one for the other. At the sentence level, [11] proposed a sentence simplification model by tree transformation based on Statistical Machine Translation (SMT). [12] presented a data-driven model based on a quasi-synchronous grammar, a formalism that can naturally capture structural mismatches and complex rewrite operations.\nThe limitation of aforementioned methods requires syntax parsing or hand-crafted rules to simplify sentences. Compared with traditional machine learning [13; 14] and data mining techniques [15; 16; 17], deep learning has shown to produce state-of-the-art results on various difficult tasks, with the help of the development of big data platforms [18; 19]. The RNN Encoder-Decoder is a very popular deep neural network model that performs exceptionally well at the machine translation task [2; 4; 3]. [1] proposed a preliminary work to use RNN Encoder-Decoder model for text simplification task, which is similar to the proposed model in this paper."
    }, {
      "heading" : "3 The Model",
      "text" : "In this section, we first briefly introduce the basic idea of Recurrent Neural Network (RNN) and Long Short-Term Memory (LSTM), then describe the LSTM EncoderDecoder model."
    }, {
      "heading" : "3.1 Recurrent Neural Network and Long Short-Term Memory",
      "text" : "RNN is a class of Neural Network in which internal units may form a directed cycle to demonstrate the state history of previous inputs. The structure of RNN makes it naturally suited for variable-length inputs such as sentences. For a sequence data (x1, ..., xT ), where at each t ∈ {1, ..., T}, the hidden state ht of the RNN is then updated via\nht = f(ht−1, xt) (1)\nWhere f is the activation function. However, the optimization of basic RNN models is difficult because its gradients vanish over long sequences. LSTM is very good at learning long range dependencies through its internal memory cells. Similar to RNN, LSTM updates its hidden state sequentially, but the updates highly depend on memory cells containing three kind of gates: the forget gate ct decides how much remembered information to forget, the update gate it decides how to update remembered information, the output gate ot decides how much the remembered information to output.\nft = σ(Wf (xt, ht−1)) (2) it = σ(Wi(xt, ht−1)) (3) c′t = tanh(Wc(xt, ht−1)) (4) ct = it c′t + ft ct−1 (5) ot = σ(Wo(xt, ht−1)) (6) ht = ot tanh(ct) (7)\nRecent works have proposed many modified LSTM models such as the gated recurrent unit (GRU) [3]. However, [20] showed that none of the LSTM variants can improve upon the standard architecture significantly. In this paper, we use the standard LSTM structure in our model."
    }, {
      "heading" : "3.2 LSTM Encoder-Decoder Model",
      "text" : "Given a source sentence X = (x1, x2, ..., xl) and the target (simplified) sentence Y = (y1, y2, ..., yl′), where xi and yi are in the same vocabulary, l and l′ are the length of each sentence. Our goal is to build a neural network to model the conditional probability p(Y |X), then train the model to maximize the probability.\nWe show our LSTM Encoder-Decoder model in Figure 1. This model uses onehot representation of words in the sequence in the input layer, and converts it to a 300- dimensional vector in the following embedding layer. We find that adding an embedding layer can significantly improve performance when the vocabulary becomes large. Then we feed word embeddings through two LSTM layers, and get a vector representation of the input sequence after finishing reading all the words. Finally, we decode this vector to output sequence through two LSTM layers and one output embedding layer.\nLet us take the input sentence “Man with high intelligence” as a difficult sentence, the output sentence “a very smart man” as the simplified sentence, and represent the pair of sentences as a pair of word indices (we made up some indices here), we have:\n[Man,with,high,intelligence]⇒[A,very,smart,man]\n[15, 27, 6, 18]⇒ [1, 2, 12, 15]\nWe only apply sorting, reversing, and replacement to the indices to simplify a sentence, where sorting and reversing could be\nhighly related to changing the structure of a sentence or simplifying a grammar, replacement could be highly related to lexical simplification or removing redundant words. Motivated by this observation, we conduct experiments to show the LSTM EncoderDecoder is able to learn these three rules automatically, and thus can potentially perform text simplification."
    }, {
      "heading" : "4 Experiments",
      "text" : "In this section, we conduct experiments to show LSTM Encoder-Decoder can perform basic operations for sequence data. Intuitively, TS should include operations like replacing difficult words with easier words, removing redundant words, simplifying syntax structure by changing word order, etc. In the following experiments, we show that a very basic LSTM Encoder-Decoder model is able to reverse, sort, and replace the elements of a sequence.\nWe implement the LSTM EncoderDecoder in Keras [21]. The model contains two LSTM layers for both the encoder and the decoder, the output is fed into a softmax layer. RMSprop [22], which generates its parameter updates using a momentum on the rescaled gradient, was used as the optimizer in out experiment since it achieves the best performance compared to other optimization methods. We utilized early stopping with patience 5 to avoid over-fitting.\nWe generate sequences of random integer numbers with length 25 as inputs,\nsince sentences are usually less than 25 words. These integers are the indices of words in the vocabulary V . We use three different vocabularies in our experiment [0, . . . , 9], [0, . . . , 99], [0, . . . , 999]. For the target outputs, we reverse, sort, and replace words in the input sequence to simulate changing the sentence structure, replacing words, and removing words. The results show that the LSTM Encoder-Decoder is able to learn the reversing, sorting, and replacement operation rules from the provided data, and thus has the potential to simplify a complex text. We use a short example below.\nReverse: [15, 27, 6, 18, 99]⇒ [99, 18, 6, 27, 15]\nSort: [15, 27, 6, 18, 99]⇒ [6, 15, 18, 27, 99]\nReplace: [15, 27, 6, 18, 99]⇒ [15, 7, 6, 18, 19]\nCombine: [15, 27, 6, 18, 99]⇒ [19, 18, 15, 7, 6]"
    }, {
      "heading" : "4.1 Reverse",
      "text" : "We first conduct experiments to show that the LSTM Encoder-Decoder can reverse a sequence after training on a large set of sequence pairs (X,Y ), where\nX = (x1, x2, ..., x25)\nY = (x25, x24, ..., x1)\nxi ∈ V\nThe results are given in Table 1. V,H,E represent the vocabulary size, the number of hidden neurons in the LSTM layer, and the training epoch, respectively.\nThe size of our training data is extremely important for the model. As shown in Table 1, the performance decreases significantly if we reduce the size of our training set from 135k to 9k. The size of the vocabulary also influences the performance. A larger vocabulary requires more training data and more hidden neurons in the LSTM layers. By increasing the number of neurons in the LSTM layers from 128 to 256, we produce a higher capacity model with more neurons that can be trained with fewer epochs and achieve a higher accuracy.\nIn general, this model can reverse an input sequence with higher than 90 percent accuracy given enough training data. On the other hand, it shows that LSTM is proficient at memorizing long-term dependencies."
    }, {
      "heading" : "4.2 Sort",
      "text" : "Even though Neural ProgrammerInterpreters (NPI) [23], a recent model, can represent and execute programs such as sorting and addition, the LSTM Encoder-Decoder is much simpler and more light-weight compared to NPI. In the following experiment, we show that the LSTM Encoder-Decoder is able to sort a sequence of integers.\nThe datasets consist of sequence pairs (X,Y ), where\nX = (x1, x2, ..., x25)\nY = sorted(x1, x2, ..., x25)\nxi ∈ V\nWe show the results in Table 2. Similarly, the size of the vocabulary, training data, and neurons influence the performance. It is harder to train if we increase the vocabulary to 1000, but the model can still learn the sorting rule with a high accuracy if provided enough training data.\nWe extracted the hidden states of the embedding layer from the trained model of the vocabularies of the size 10 and 100. Since the embedding of each word is a 300-dimensional vector, we use Principle Component Analysis (PCA) for dimension reduction and visualize the word embeddings in Figure 2. Interestingly, we find the learned embedding correctly represents the relationship between each word in 1-dimensional and 2- dimensional space. Noted that even though the inputs are integer “numbers”, these numbers are actually symbols, or word indices\nthat should be perpendicular with each other and have same distance. The model does not know, for example, the order between 1 and 2 before training. The model successfully captures the meaning and relationship between words. Similarly, if we provide the model with difficult and simple English sentence pairs, the LSTM Encoder-Decoder may be able to learn how to order words to make the sentence simpler."
    }, {
      "heading" : "4.3 Replace",
      "text" : "We next show that the LSTM EncoderDecoder can replace words in a sequence. For the sequence pairs (X,Y ), where\nX = (x1, x2, ..., x25)\nY = (y1, y2, ..., y25)\nyi = xi mod n\nxi, yi ∈ V\nWe let n = 2, 20, 200 when |V | = 10, 100, 1000. We only keep the top 20 percent of words in the vocabulary, and use\nthese words to replace all matching words in the output sequence. Lexical simplification is similar, in which we can regard the top 20 percent of words in the vocabulary as simple and common words and all the other words are complex words. Similarly, we can also think of the top 20 percent of words as meaningful and important words, other words are redundant. Therefore it also suits the task of removing redundant words. The results are shown in Table 3. Compared with the sorting operation, the replacement operation is easier to train when the vocabulary is large, or the size of training data is not large."
    }, {
      "heading" : "4.4 Combine Three Operations",
      "text" : "We have shown that the LSTM EncoderDecoder can work well on the reversing, sorting and replacement operations separately, but in reality, a sentence is usually simplified by a complex combination of these three different rules. Therefore, we combine the three operations together to see if this model can still discover the mapping rules between sequences.\nSo the data is sequence pairs (X,Y ), where Y is obtained by performing modulo for each index in X first, then sorting and reversing. The results are shown in Table 4. The LSTM Encoder-Decoder continue working very well as expected, and even as good as each of the operations alone. Therefore, the LSTM Encoder-Decoder can easily discover mapping patterns of combined operations between sequences, thus it may potentially find complicated simplification rules."
    }, {
      "heading" : "5 Conclusion and Future Work",
      "text" : "In conclusion, we find that the LSTM Encoder-Decoder model is able to learn operation rules such as reversing, sorting, and replacing from sequence pairs, which shows the model may potentially apply rules like modifying sentence structure, substituting words, and removing words for text simplification. This is a preliminary experimental study in solving the text simplification problem using deep neural networks. However, unlike the machine translation task, there are very few text simplification training corpora online. So our future work includes collecting complex and simple sentence pairs from online resources such as English Wikipedia and Simple English Wikipedia, and training our model using natural languages."
    } ],
    "references" : [ {
      "title" : "Text simplification using neural machine translation",
      "author" : [ "Tong Wang", "Ping Chen", "John Rochford", "Jipeng Qiang" ],
      "venue" : "In Thirtieth AAAI Conference on Artificial Intelligence,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2016
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1409.0473,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2014
    }, {
      "title" : "Learning phrase representations using rnn encoder-decoder for statistical machine translation",
      "author" : [ "Kyunghyun  Cho", "Bart Van Merriënboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1406.1078,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2014
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc VV Le" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2014
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1997
    }, {
      "title" : "Simple english wikipedia: a new text simplification task. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers- Volume 2, pages 665–669",
      "author" : [ "William Coster", "David Kauchak" ],
      "venue" : "Association for Computational Linguistics,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2011
    }, {
      "title" : "A new evaluation measure using compression dissimilarity on text summarization",
      "author" : [ "Tong Wang", "Ping Chen", "Dan Simovici" ],
      "venue" : "Applied Intelligence,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2016
    }, {
      "title" : "A neural attention model for abstractive sentence summarization",
      "author" : [ "Alexander M Rush", "Sumit Chopra", "Jason Weston" ],
      "venue" : "arXiv preprint arXiv:1509.00685,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2015
    }, {
      "title" : "Simplifying lexical simplification: Do we need simplified corpora? page",
      "author" : [ "Goran Glavaš", "Sanja Štajner" ],
      "venue" : null,
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2015
    }, {
      "title" : "Putting it simply: a context-  aware approach to lexical simplification. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers-Volume 2, pages 496–501",
      "author" : [ "Or Biran", "Samuel Brody", "Noémie Elhadad" ],
      "venue" : "Association for Computational Linguistics,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2011
    }, {
      "title" : "A monolingual treebased translation model for sentence simplification",
      "author" : [ "Zhemin Zhu", "Delphine Bernhard", "Iryna Gurevych" ],
      "venue" : "In Proceedings of the 23rd international conference on computational linguistics,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2010
    }, {
      "title" : "Learning to simplify sentences with quasi-synchronous grammar and integer programming. In Proceedings of the conference on empirical methods in natural language processing, pages 409–420",
      "author" : [ "Kristian Woodsend", "Mirella Lapata" ],
      "venue" : "Association for Computational Linguistics,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2011
    }, {
      "title" : "Prediction of long-lead heavy precipitation events aided by machine learning",
      "author" : [ "Yahui Di" ],
      "venue" : "In 2015 IEEE International Conference on Data Mining Workshop (ICDMW),",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2015
    }, {
      "title" : "Extended topic model for word dependency",
      "author" : [ "Tong Wang", "Vish Viswanath", "Ping Chen" ],
      "venue" : "In Proceedings of the 53th Annual Meeting of the Associatioin for Computational Linguistics (ACL-2015, Short Papers),",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2015
    }, {
      "title" : "Compression and data mining",
      "author" : [ "Dan A Simovici", "Tong Wang", "Ping Chen", "Dan Pletea" ],
      "venue" : "In 2015 International Conference on Computing, Networking and Communications (ICNC),",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2015
    }, {
      "title" : "Longlead term precipitation forecasting by hierarchical clustering-based bayesian structural vector autoregression",
      "author" : [ "Kaixun Hua", "Dan A Simovici" ],
      "venue" : "In 2016 IEEE 13th International Conference on Networking, Sensing, and Control (ICNSC),",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2016
    }, {
      "title" : "An evaluation of big data  analytics in feature selection for longlead extreme floods forecasting",
      "author" : [ "Yong Zhuang", "Kui Yu", "Dawei Wang", "Wei Ding" ],
      "venue" : "In 2016 IEEE 13th International Conference on Networking, Sensing, and Control (IC- NSC),",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2016
    }, {
      "title" : "Fresh: Fair and efficient slot configuration and scheduling for hadoop clusters",
      "author" : [ "Jiayin Wang", "Yi Yao", "Ying Mao", "Bo Sheng", "Ningfang Mi" ],
      "venue" : "In Cloud Computing (CLOUD),",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2014
    }, {
      "title" : "Cielo: An evolutionary game theoretic framework for virtual machine placement in clouds",
      "author" : [ "Yi Ren", "Jun Suzuki", "Athanasios Vasilakos", "Shingo Omura", "Katsuya Oba" ],
      "venue" : "In Future Internet of Things and Cloud (FiCloud),",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2014
    }, {
      "title" : "Lstm: A search space odyssey",
      "author" : [ "Klaus Greff", "Rupesh Kumar Srivastava", "Jan Koutńık", "Bas R Steunebrink", "Jürgen Schmidhuber" ],
      "venue" : "arXiv preprint arXiv:1503.04069,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2015
    }, {
      "title" : "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude",
      "author" : [ "Tijmen Tieleman", "Geoffrey Hinton" ],
      "venue" : "COURSERA: Neural Networks for Machine Learning,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "It is generally divided into three categories: lexical simplification (LS), rulebased, and machine translation (MT) [1].",
      "startOffset" : 116,
      "endOffset" : 119
    }, {
      "referenceID" : 4,
      "context" : "In this paper, we propose to apply Long Short-Term Memory (LSTM) [5] EncoderDecoder on TS task.",
      "startOffset" : 65,
      "endOffset" : 68
    }, {
      "referenceID" : 5,
      "context" : "Usually, hand-crafted, supervised, and unsupervised methods based on resources like English Wikipedia (EW) and Simple English Wikipedia (SEW) [6] are utilized for extracting simplification rules.",
      "startOffset" : 142,
      "endOffset" : 145
    }, {
      "referenceID" : 8,
      "context" : "At the lexical level, [9] proposed an lexical simplification system which only requires a large corpus of regular text to obtain word embeddings to get words similar to the complex word.",
      "startOffset" : 22,
      "endOffset" : 25
    }, {
      "referenceID" : 9,
      "context" : "[10] proposed an unsupervised method for learning pairs of complex and simpler synonyms and a context aware method for substituting one for the other.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 10,
      "context" : "At the sentence level, [11] proposed a sentence simplification model by tree transformation based on Statistical Machine Translation (SMT).",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 11,
      "context" : "[12] presented a data-driven model based on a quasi-synchronous grammar, a formalism that can naturally capture structural mismatches and complex rewrite",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 0,
      "context" : "[1] proposed a preliminary work to use RNN Encoder-Decoder model for text simplification task, which is similar to the proposed model in this paper.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 2,
      "context" : "Recent works have proposed many modified LSTM models such as the gated recurrent unit (GRU) [3].",
      "startOffset" : 92,
      "endOffset" : 95
    }, {
      "referenceID" : 19,
      "context" : "However, [20] showed that none of the LSTM variants can improve upon the standard architecture significantly.",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 14,
      "context" : "[15, 27, 6, 18]⇒ [1, 2, 12, 15]",
      "startOffset" : 0,
      "endOffset" : 15
    }, {
      "referenceID" : 5,
      "context" : "[15, 27, 6, 18]⇒ [1, 2, 12, 15]",
      "startOffset" : 0,
      "endOffset" : 15
    }, {
      "referenceID" : 17,
      "context" : "[15, 27, 6, 18]⇒ [1, 2, 12, 15]",
      "startOffset" : 0,
      "endOffset" : 15
    }, {
      "referenceID" : 0,
      "context" : "[15, 27, 6, 18]⇒ [1, 2, 12, 15]",
      "startOffset" : 17,
      "endOffset" : 31
    }, {
      "referenceID" : 1,
      "context" : "[15, 27, 6, 18]⇒ [1, 2, 12, 15]",
      "startOffset" : 17,
      "endOffset" : 31
    }, {
      "referenceID" : 11,
      "context" : "[15, 27, 6, 18]⇒ [1, 2, 12, 15]",
      "startOffset" : 17,
      "endOffset" : 31
    }, {
      "referenceID" : 14,
      "context" : "[15, 27, 6, 18]⇒ [1, 2, 12, 15]",
      "startOffset" : 17,
      "endOffset" : 31
    }, {
      "referenceID" : 20,
      "context" : "RMSprop [22], which generates its parameter updates using a momentum on the rescaled gradient, was used as the optimizer in out experiment since it achieves the best performance compared to other optimization methods.",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 14,
      "context" : "Reverse: [15, 27, 6, 18, 99]⇒ [99, 18, 6, 27, 15]",
      "startOffset" : 9,
      "endOffset" : 28
    }, {
      "referenceID" : 5,
      "context" : "Reverse: [15, 27, 6, 18, 99]⇒ [99, 18, 6, 27, 15]",
      "startOffset" : 9,
      "endOffset" : 28
    }, {
      "referenceID" : 17,
      "context" : "Reverse: [15, 27, 6, 18, 99]⇒ [99, 18, 6, 27, 15]",
      "startOffset" : 9,
      "endOffset" : 28
    }, {
      "referenceID" : 17,
      "context" : "Reverse: [15, 27, 6, 18, 99]⇒ [99, 18, 6, 27, 15]",
      "startOffset" : 30,
      "endOffset" : 49
    }, {
      "referenceID" : 5,
      "context" : "Reverse: [15, 27, 6, 18, 99]⇒ [99, 18, 6, 27, 15]",
      "startOffset" : 30,
      "endOffset" : 49
    }, {
      "referenceID" : 14,
      "context" : "Reverse: [15, 27, 6, 18, 99]⇒ [99, 18, 6, 27, 15]",
      "startOffset" : 30,
      "endOffset" : 49
    }, {
      "referenceID" : 14,
      "context" : "Sort: [15, 27, 6, 18, 99]⇒ [6, 15, 18, 27, 99]",
      "startOffset" : 6,
      "endOffset" : 25
    }, {
      "referenceID" : 5,
      "context" : "Sort: [15, 27, 6, 18, 99]⇒ [6, 15, 18, 27, 99]",
      "startOffset" : 6,
      "endOffset" : 25
    }, {
      "referenceID" : 17,
      "context" : "Sort: [15, 27, 6, 18, 99]⇒ [6, 15, 18, 27, 99]",
      "startOffset" : 6,
      "endOffset" : 25
    }, {
      "referenceID" : 5,
      "context" : "Sort: [15, 27, 6, 18, 99]⇒ [6, 15, 18, 27, 99]",
      "startOffset" : 27,
      "endOffset" : 46
    }, {
      "referenceID" : 14,
      "context" : "Sort: [15, 27, 6, 18, 99]⇒ [6, 15, 18, 27, 99]",
      "startOffset" : 27,
      "endOffset" : 46
    }, {
      "referenceID" : 17,
      "context" : "Sort: [15, 27, 6, 18, 99]⇒ [6, 15, 18, 27, 99]",
      "startOffset" : 27,
      "endOffset" : 46
    }, {
      "referenceID" : 14,
      "context" : "Replace: [15, 27, 6, 18, 99]⇒ [15, 7, 6, 18, 19]",
      "startOffset" : 9,
      "endOffset" : 28
    }, {
      "referenceID" : 5,
      "context" : "Replace: [15, 27, 6, 18, 99]⇒ [15, 7, 6, 18, 19]",
      "startOffset" : 9,
      "endOffset" : 28
    }, {
      "referenceID" : 17,
      "context" : "Replace: [15, 27, 6, 18, 99]⇒ [15, 7, 6, 18, 19]",
      "startOffset" : 9,
      "endOffset" : 28
    }, {
      "referenceID" : 14,
      "context" : "Replace: [15, 27, 6, 18, 99]⇒ [15, 7, 6, 18, 19]",
      "startOffset" : 30,
      "endOffset" : 48
    }, {
      "referenceID" : 6,
      "context" : "Replace: [15, 27, 6, 18, 99]⇒ [15, 7, 6, 18, 19]",
      "startOffset" : 30,
      "endOffset" : 48
    }, {
      "referenceID" : 5,
      "context" : "Replace: [15, 27, 6, 18, 99]⇒ [15, 7, 6, 18, 19]",
      "startOffset" : 30,
      "endOffset" : 48
    }, {
      "referenceID" : 17,
      "context" : "Replace: [15, 27, 6, 18, 99]⇒ [15, 7, 6, 18, 19]",
      "startOffset" : 30,
      "endOffset" : 48
    }, {
      "referenceID" : 18,
      "context" : "Replace: [15, 27, 6, 18, 99]⇒ [15, 7, 6, 18, 19]",
      "startOffset" : 30,
      "endOffset" : 48
    }, {
      "referenceID" : 14,
      "context" : "Combine: [15, 27, 6, 18, 99]⇒ [19, 18, 15, 7, 6]",
      "startOffset" : 9,
      "endOffset" : 28
    }, {
      "referenceID" : 5,
      "context" : "Combine: [15, 27, 6, 18, 99]⇒ [19, 18, 15, 7, 6]",
      "startOffset" : 9,
      "endOffset" : 28
    }, {
      "referenceID" : 17,
      "context" : "Combine: [15, 27, 6, 18, 99]⇒ [19, 18, 15, 7, 6]",
      "startOffset" : 9,
      "endOffset" : 28
    }, {
      "referenceID" : 18,
      "context" : "Combine: [15, 27, 6, 18, 99]⇒ [19, 18, 15, 7, 6]",
      "startOffset" : 30,
      "endOffset" : 48
    }, {
      "referenceID" : 17,
      "context" : "Combine: [15, 27, 6, 18, 99]⇒ [19, 18, 15, 7, 6]",
      "startOffset" : 30,
      "endOffset" : 48
    }, {
      "referenceID" : 14,
      "context" : "Combine: [15, 27, 6, 18, 99]⇒ [19, 18, 15, 7, 6]",
      "startOffset" : 30,
      "endOffset" : 48
    }, {
      "referenceID" : 6,
      "context" : "Combine: [15, 27, 6, 18, 99]⇒ [19, 18, 15, 7, 6]",
      "startOffset" : 30,
      "endOffset" : 48
    }, {
      "referenceID" : 5,
      "context" : "Combine: [15, 27, 6, 18, 99]⇒ [19, 18, 15, 7, 6]",
      "startOffset" : 30,
      "endOffset" : 48
    } ],
    "year" : 2016,
    "abstractText" : "Text simplification (TS) aims to reduce the lexical and structural complexity of a text, while still retaining the semantic meaning. Current automatic TS techniques are limited to either lexical-level applications or manually defining a large amount of rules. Since deep neural networks are powerful models that have achieved excellent performance over many difficult tasks, in this paper, we propose to use the Long Short-Term Memory (LSTM) Encoder-Decoder model for sentence level TS, which makes minimal assumptions about word sequence. We conduct preliminary experiments to find that the model is able to learn operation rules such as reversing, sorting and replacing from sequence pairs, which shows that the model may potentially discover and apply rules such as modifying sentence structure, substituting words, and removing words for TS.",
    "creator" : "LaTeX with hyperref package"
  }
}