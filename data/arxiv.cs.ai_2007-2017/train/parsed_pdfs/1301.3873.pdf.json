{
  "name" : "1301.3873.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Credal Networks under Maximum Entropy",
    "authors" : [ "Thomas Lukasiewicz" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "We apply the principle of maximum entropy to select a unique joint probability distribution from the set of all joint probability distributions speci fied by a credal network. In detail, we start by showing that the unique joint distribution of a Bayesian tree coincides with the maximum en tropy model of its conditional distributions. This result, however, does not hold anymore for gen eral Bayesian networks. We thus present a new kind of maximum entropy models, which are computed sequentially. We then show that for all general Bayesian networks, the sequential max imum entropy model coincides with the unique joint distribution. Moreover, we apply the new principle of sequential maximum entropy to in terval Bayesian networks and more generally to credal networks. We especially show that this ap plication is equivalent to a number of small local entropy maximizations.\n1 INTRODUCTION\nIn classical Bayesian networks [31], a single joint probabil ity distribution is specified by a number of conditional inde pendencies encoded in a directed acyclic graph and a num ber of single conditional probability distributions. More precisely, we assume in particular that all conditional prob ability distributions are precisely given.\nThere are, however, several reasons for which we should give up such an assumption. In realistic environments, we often deal with interval rather than point probabilities. Moreover, an agent may be unable to specify precise prob abilities due to limited resources of information or time; or an agent just wants a rough analysis of a decision situa tion. Finally, it may be the case that the decision situation itself cannot be modeled by precise probabilities.\nThe need for giving up precise probabilities in Bayesian\nnetworks is already reported by a number of publications in this direction. Intervals in the Bayesian network framework are especially discussed by Breese and Fertig [2], Tessem [37], Thone et al. [38], and Zaffalon [12]. Bayesian net works with local convex sets of conditional distributions, called credal networks (or also Quasi-Bayesian networks), are especially analyzed by Cozman [7, 8].\nOne philosophy to handle credal networks is to insist on working with sets of probability distributions. In this case, an agent follows a very cautious path by considering ev ery distribution as relevant. But this also means that there might be ambiguous situations in which the agent simply does not know what to do. Moreover, following this phi losophy also means developing completely new propaga tion and inference techniques for credal networks (see es pecially the work by Cano et al. [3, 4] and Cozman [7] for exact and approximation techniques). To our knowledge, current exact propagation techniques for credal networks do not go much beyond interval Bayesian polytrees over binary random variables [38, 12].\nAnother philosophy is to select a representative probabil ity distribution from the set of all specified distributions. In this case, an agent follows a credulous path, as the se lected distribution might be the wrong one. Nevertheless, the agent does not have to deal with ambiguities result ing from multiple distributions. Moreover, after selecting the unique representative distribution, we can simply apply classical Bayesian network techniques.\nIn this paper, we adhere to the second philosophy. In detail, we propose to use the principle of sequential maximum en tropy to select a representative distribution from the set of all distributions specified by a credal network.\nWe now describe the main ideas behind this proposal.\nA Bayesian network (D, KB) is given by a directed acyclic graph D and a set of conditional distributions KB. It de fines a unique joint probability distribution PrD[KB] by the conditional distributions KB and certain conditional in dependencies encoded in D.\n364 UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000\nInterestingly, it turns out that for Bayesian trees (D, KB), the unique joint distribution Pr D (KB] coincides with the maximum entropy model of KB (which is the unique joint distribution that is compatible with all conditional distri butions in KB and that has the greatest entropy among all such joint distributions). That is, the maximum entropy model of KB naturally respects all the conditional inde pendencies encoded in D.\nThis suggests that maximum entropy may be used to select a unique joint distribution from the set of all joint distribu tions PrD(KB] specified by a credal network (D, KB).\nIndeed, we will see that also for credal trees (D, KB), the maximum entropy model of KB naturally respects all con ditional independencies encoded in D.\nBut, for general Bayesian and credal networks (D, KB), the maximum entropy model of KB does not necessarily respect all the conditional independencies encoded in D. This result is well-known in the literature as the causality problem of minimum cross-entropy updating [31, 16].\nMoreover, also for credal trees (D, KB), selecting the maximum entropy model of KB as a representative joint distribution may contradict intuition. In detail, we may have causality problems on a meta-level.\nGeneralizing an idea that goes back to Hunter [16], we then introduce the principle of sequential maximum entropy as a selection principle that solves our two causality prob lems. More precisely, for all Bayesian networks (D, KB), the sequential maximum entropy model of KB coincides with the unique joint distribution Pr D [KB]. Moreover, for all credal networks (D, KB), the sequential maximum en tropy model of KB respects all the conditional indepen dencies encoded in D. Finally, it also turns out that the sequential maximum entropy model can be computed by a number of small local entropy maximizations.\nThe main contributions of this paper are as follows:\n• We present an 1-map for the maximum entropy model of a set of convex conditionals. Such an 1-map is a very useful characterization of conditional indepen dencies, which is important in its own right.\n• We show that for all Bayesian trees (D, KB), the maximum entropy model of KB coincides with the unique joint distribution defined by KB and the con ditional independencies encoded in D.\n• We show that for all credal trees (D, KB), the max imum entropy model of KB respects all conditional independencies encoded in D.\n• We introduce the principle of sequential maximum en tropy, which can be applied to all credal networks.\n• We show that for all Bayesian networks (D, KB), the sequential maximum entropy model of KB coincides with the unique joint distribution of (D, KB).\n• We show that for all credal networks (D, KB), the sequential maximum entropy model of KB respects all conditional independencies encoded in D.\n• We show that the sequential maximum entropy model of a credal network can be computed by a number of small local entropy maximizations.\nThe rest of this paper is organized as follows. Section 2 introduces the technical background. In Section 3, we concentrate on Bayesian and credal networks under global maximum entropy. Sections 4 and 5 focus on the principle of sequential maximum entropy. In Section 6, we finally give a summary, and an outlook on future research.\nNote that all proofs are given in full detail in [24].\n2 TECHNICAL PRELIMINARIES\nIn this section, we describe the technical background.\n2.1 CREDAL NETWORKS\nWe now give a brief introduction to Bayesian networks, in terval Bayesian networks, and credal networks (see espe cially [31], [37, 38, 12], and [7, 8], respectively).\nA Bayesian network is defined by a directed acyclic graph D over discrete random variables X 1, X 2, .. . , X n as nodes and by a conditional probability distribution Pr (X; I pa(X;)) for each variable X; and each instanti ation pa (X;) of its parents pa (X;). It specifies a unique joint probability distributionPr over X1, X2, .. . , Xn by:\nn Pr(Xl ' x2' 0 0 0 'Xn) = II Pr (X; I pa(X;)) 0\ni=l\nThat is, the joint distribution Pr is uniquely determined by the conditional distributions Pr (X; I pa (X;)) and certain conditional independencies encoded in D.\nMore generally, a credal network is defined by a directed acyclic graph D over X1, X2, . . . , Xn and by a nonempty convex set S (X; I pa (X;)) of conditional probability dis tributions Pr (X; I pa(X;)) for each variable X; and each instantiation pa (X;) of its parents pa (X;).\nWe associate a credal network with the set of all joint dis tributions that are admissible with the convex sets of condi tional distributions and the conditional independencies en coded in D [12]. See in particular [8] for other possible semantics of credal networks (especially those that involve new notions of irrelevance and independency).\nInterval Bayesian networks are a special kind of credal net works in which each S (X; I pa (X;)) can be expressed by a set of interval constraints I (X; I pa (X;)) of the form\nUNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000 365\nwhere { Xi,j, . . . , Xi,d;} with di � 2 denotes the domain of the variable Xi, 0 :S lj :S Uj :S 1 for all j E {1, . . . , di}, and h + · · · + ld, :S 1 :S u1 + · · · + ud,.\nA Bayesian (resp., interval Bayesian, credal) network is a Bayesian (resp. , interval Bayesian, credal) tree iff the as sociated undirected graph D is a directed tree (that is, a directed acyclic graph in which every node has exactly one incoming arrow, except for the root that does not have any).\n2.2 CONDITIONALS\nWe will use the language of conditionals (see especially [13, 21, 25, 26]) to represent Bayesian networks, interval Bayesian networks, and credal networks.\nLet U = {X1, ... , Xn} with n � 1 be a set of discrete ran dom variables, where each variable Xi E U has a finite and non empty domain D x, = { xi,l, . . . , Xi,d;}. The set of ba sic events Bu contains all expressions of the form Xi =Xi with Xi E U and Xi E D x,. The set of conjunctive events C u contains the true event T and all members in the clo sure of Bu under the Boolean operation 1\\ . We abbreviate the conjunctive event c 1\\ d by c, d . An instantiation of a set of variables {xi,' . .. 'xik} � u with k � 1 is a con junctive event of the form Xi, = Xi1 , • • . , Xik = Xik (the unique instantiation of the empty set of variables is T).\nA point conditional is an expression of the form (dlc)[r], where c and d are conjunctive events (called premise and conclusion, respectively) and r is a real number from [0, 1]. An interval conditional is an expression (d lc)[l, u] with conjunctive events c and d and real numbers l, u E [0, 1] such that l :S u. A convex conditional is an expression (Xi I c)[K] with a variable Xi E U, a conjunctive event c, and a finitely generated nonempty convex set K � [0, l]d' such that r1 + · · · + rd, = 1 for all (r1, ... , rd.) E K (note that K is finitely generated iff it is the convex closure of a finite number of vectors k1, . . . , k1 E [0, 1]d' with l � 0). A conditional is a point, interval, or convex conditional.\nTo define probabilistic interpretations of conjunctive events and conditionals, we introduce atomic events and the bi nary relation '=>' between atomic and conjunctive events. The set of atomic events Du contains all conjunctive events of the form Xt = Xt' x2 = X2, . . . 'Xn = Xn. The atomic event w implies the conjunctive event c, denoted w => c, iff w 1\\ --,c is a propositional contradiction (that is, each basic event inc is also contained in w).\nA probabilistic interpretation Pr is a mapping from Du to [0, 1] such that all Pr(w) with wE nu sum up to 1. Pr is extended in a well-defined way to conjunctive events c by defining Pr(c) as sum of all Pr(w) with w E nu and\nw =>c. For conjunctive events c and d with Pr(c) > 0, we write Pr(dlc) to abbreviate Pr(c, d)/ Pr (c). A proba bilistic interpretation Pr is extended to point conditionals by Pr f= (dlc)[r] iff Pr(c) = 0 or Pr(dlc) = r. Moreover,\nPr is extended to interval conditionals by Pr f= (d lc)[l, u] iff Pr(c) = 0 or Pr (dlc) E [ l, u] . Finally, Pr is extended to convex conditionals as follows:\nPr f= (Xi I c)[K] iff ( Pr(Xi = Xi,t lc), ... , Pr(Xi = Xi,d, lc)) E K\nThe notions of models and satisfiability are defined as usual: An interpretation Pr is a model of a conditional F iff Pr f= F. Pr is a model of a set of conditionals KB, denoted Pr f= KB, iff Pr is a model of all F E KB. A set of conditionals KB is satisfiable iff a model of KB exists.\n2.3 CREDAL NETWORKS AS CONDITIONALS\nWe now express Bayesian networks, interval Bayesian net works, and credal networks by sets of conditionals.\nA Bayesian network is a pair (D, KB), where D is a di rected acyclic graph over U as nodes and KB is a set of point conditionals such that:\n• For each variable Xi E U and each instantiation pa (Xi) of its parents pa(Xi), there exists a set of con ditionals KB x, lpa(X,) of the form\nwith r1 + · · · + r d, = 1.\n• KB is the union of all KB x, lpa(X;).\nGiven a Bayesian network (D, KB), we use Pr v[KB] to denote its unique joint probability distribution.\nSimilarly, an interval Bayesian network is a pair (D, KB), where D is a directed acyclic graph over U as nodes and\nKB is a set of interval conditionals such that:\n• For each variable Xi E U and each instantiation pa (Xi) of its parents pa(Xi), there exists a set of in terval conditionals KB X;lpa(X;) of the form\nWith /1 + · · · + ld; :S 1 :S Ut + · · · + Ud;.\n• KB is the union of all KB x, lpa(X;).\nFinally, a credal network is a pair (D, KB), where D is a directed acyclic graph over U as nodes and KB is a set of convex conditionals such that:\n• For each variable Xi E U and each instantiation pa (Xi) of its parents pa (Xi), there exists a set of con vex conditionals KB x, lpa(X;) of the form\n366 UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000\n• KB is the union of all KBx;lpa(X;)·\nGiven an interval Bayesian network (D, KB) or a credal network (D, KB), we use Prv[KB] to denote the set of all associated joint probability distributions.\n2.4 MAXIMUM ENTROPY MODELS\nWe now define maximum entropy models of sets of con ditionals. For the principle of maximum entropy see espe cially the work by Shannon and Weaver [35] and Jaynes [17]. Its application to probabilistic reasoning in the ar tificial intelligence context are in particular discussed by Cheeseman [6], Paris and Vencovska [29, 30], and recently also by Grove et al. [ 14].\nThe maximum entropy model (ME-model) of a satisfiable set of conditionals KB, denoted Pr ME [ KB], is the unique probabilistic interpretation Pr that is a model of KB and that has the greatest entropy H(Pr) among all the models of KB, where H(Pr) is defined as follows:\nH(Pr) = - I: Pr(w) ·log Pr(w) . wEOu\n2.5 I-MAPS OF MAXIMUM ENTROPY MODELS\nAs a first contribution of this paper, we now describe how to construct an 1-map for the ME-model of a satisfiable set of conditionals KB (that is, a characterization of conditional independencies holding in Pr ME [ KB]). Similar construc tions of undirected graphs from formulas are given in [22] and [33], which produce 1-maps for probability distribu tions under conditioning, and dependency graphs for ME models of sets of point conditionals, respectively.\nLet us briefly recall that for pairwise disjoint sets of vari ables X, Y, Z � U, we say X and Y are conditionally independent given Z in Pr iff for all instantiations x, y, and z of X, Y, and Z, respectively:\nPr(xiy, z) = Pr(xiz) whenever Pr(y, z) > 0.\nAn undirected graph (U, E) is an 1-map of a probabilistic interpretation Pr iff for all pairwise disjoint X, Y, Z � U: if X andY are separated by Z in (U, E), then X andY are conditionally independent given Z in Pr.\nThe main idea in building an 1-map of PrME[KB] is to add an edge between two different variables iff there is a conditional in KB that contains them both.\nMore formally, the undirected graph G KB = (U, E) con tains the undirected edge {X, Y} iff X and Y are two different variables that both occur in the same condi tional (dic)[r] E KB, (dic)[l, u] E KB, or (Dic)[K] E KB That is, both X and Y occur in d, both occur in c, or one of them occurs in d (resp., D) and the other one in c.\nThe following result shows that the constructed undirected graph G KB is indeed an 1-map of the ME-model of KB.\nTheorem 2.1 Let KB be a satisfiable set of conditionals. Then, GKB is an l-map of PrME[KB].\n3 GLOBAL MAXIMUM ENTROPY\nIn this section, we concentrate on Bayesian and credal net works under global maximum entropy.\n3.1 BAYES IAN NETWORKS\nWe first analyze the relationship between Bayesian net works and global maximum entropy. In detail, given a Bayesian network (D, KB), we study the relationship be tween the conditional independencies encoded in D and the conditional independencies in the ME-model of KB.\nInterestingly, as far as Bayesian trees (D, KB) are con cerned, all the conditional independencies encoded in D also hold in the ME-model of KB (that is, the conditional independencies in D are naturally entrenched in KB).\nTheorem 3.1 For all Bayesian trees (D, KB), it holds PrME[KB] = Prv[KB].\nThis remarkable result, however, does not carry over to general Bayesian networks (D, KB). In this more general case, the ME-model of KB does not necessarily respect all the conditional independencies encoded in D. This shows the following theorem, which is well-known as the causal ity problem of minimum cross-entropy updating [31, 16].\nTheorem 3.2 (essentially [31, 16]) There exist Bayesian networks (D, KB) such that PrME[KB]-::/=- Prv[KB].\n3.2 CREDAL NETWORKS\nWe now generalize the results of the previous section to credal networks (D, KB). Clearly, in the general case, by Theorem 3.2, the ME-model of KB does not necessarily respect all the conditional independencies encoded in D.\nIn the special case of credal trees (D, KB), however, the independencies of D are also respected in the ME-model of KB. That is, Theorem 3.1 carries over to credal trees.\nTheorem 3.3 For all credal trees (D, KB), it holds PrME[KB] E Prv[KB].\nThis result suggests that maximum entropy can be used to select a unique joint distribution from the set of all joint distributions specified by a credal tree. The next example, however, shows that this selection may be counter-intuitive.\nExample 3.1 Let the directed acyclic graph D over the discrete random variables A, B, and C with the domains {a, a}, { b, b}, and { c, c}, respectively, be given by Fig. 1.\nUNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000 367\nFigure 1: Directed Tree D\nAssume that A = a, B = b, and C = c represent the events \"burglary\", \"alarm sound\", and \"phone call\", respectively, of the classical burglary example [31] in which Mr. Holmes has to deal with the following scenario. A burglary at his house would probably start the alarm system, which itself would probably cause his daughter to give him a phone call. Moreover, suppose that Mr. Holmes does not know any thing else than the probability u E [0, 1] that his daughter will give him a phone call when she hears the alarm sound. That is, assume KBu = {(C = c I B = b)[u]}.\nHow does the ME-model of KBu now look like? For in stance, which is the conditional probability of alarm sound given a burglary, that is, Pr ME[KBu](B = b I A= a)?\nInterestingly, this conditional probability strongly depends on u as shown in Fig. 2 (it is given by the function f(u) = 1/ (1 + 2 uu (1- u)(l-ul )). That is, under global maximum entropy, the selected probability that the alarm starts when there is a burglary depends on the probability that the daughter calls when she hears the alarm sound.\n4 SEQUENTIAL MAXIMUM ENTROPY\nIn this section, we present the principle of sequential maxi mum entropy for selecting unique joint distributions among all joint distributions specified by a credal network.\n4.1 MAIN IDEAS\nAs shown by Theorem 3.2 and Example 3.1, there are two main problems coming along with applying the principle of maximum entropy to Bayesian and credal networks.\nThe first problem is the well-known improper handling of causal information. That is, maximum entropy pro duces undesired dependencies between the variables in the premises of conditionals (see Theorem 3. 2).\nThe second problem is that the principle of maximum en tropy also produces undesired dependencies between con ditionals on a meta-level (see Example 3.1).\nThe first problem has already been addressed in the liter ature. In detail, Hunter [16] proposed to use probabilistic counterfactuals as a more adequate representation of causal conditional probability statements. He shows that maxi mum entropy applied to such probabilistic counterfactuals handles causal information in a correct way.\nMore precisely, Hunter considers the restricted case of a Bayesian network over the directed acyclic graph\nD = ({C,Al, ... ,Ak},{C+--A;IiE{1, ... ,k})\nwith binary random variables C, A1, ... , Ak and k � 2. He represents each conditional distribution Pr( C I pa( C)) by a probabilistic counterfactual. Such probabilistic counter factuals are interpreted by probability distributions over the set of all linear orders of atomic events. Hunter then shows that minimum cross-entropy updating of a prior distribution w. r.t. the given set of probabilistic counterfactuals does not introduce any new dependencies between A1, ... , Ak. Fi nally, he remarks that in certain cases, the same effect can be obtained by keeping Pr(A1, ... , Ak) fixed to its val ues in the prior distribution and then performing minimum cross-entropy updating w.r.t. all Pr(C I pa(C)).\nThis remark will be the first important building block of our principle of sequential maximum entropy, which can be ap plied to all credal networks (D, KB). In detail, for each variable X; we will perform a maximum entropy compu tation with respect to all Pr(X; I pa(X;)), while keeping previously computed probability values fixed.\nThat is, we now just have to determine the linear order in which these single computations must be done. We will see that we can use any ordering (X1, X2, ... , Xn) of the vari ables in U that respects the structure of D. This sequential computation will then also solve our second problem with global maximum entropy reported by Example 3.1.\nWe remark, however, that the sequential maximum entropy model of KB clearly depends on the set of variables in U, their domains of values, and the structure of D.\nFinally, one intuition behind sequential maximum entropy can informally be described as follows. Assume that each arrow X --+ Y in D represents a temporal relationship in the sense that any event related to X happens temporally before any event related to Y. Hence, at any variable Z, we cannot change anymore the probabilities of instantia tions of variables \"in the past\", and we are not influenced by any probabilities related to variables \"in the future\".\n368 UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000\n4.2 BAYES IAN NETWORKS\nWe now introduce the principle of sequential maximum en tropy for Bayesian networks. We will show that for all Bayesian networks (D, KB), the sequential maximum.en tropy model of KB coincides with the unique joint distribu tion of (D, KB) (that is, the sequential maximum entropy model respects all the conditional independencies in D).\nThe main idea is to apply the principle of maximum en tropy in an sequential way. In detail, we take an ordering (X 1, X 2, ... , X n) of the variables in U that is consistent with D (that is, if Xi -+ Xi is an arrow in D, then i < j). In the sequel, we assume that the indices of the variables in u ={XI, x2, 0 0 0 'Xn} already respect this ordering.\nWe now compute ME-models with respect to each set of variables Ui = {XI,X2, ... ,Xi}, where i is increasing from 1 to n. In each iteration step i, we involve exactly those conditionals in the maximum entropy computation that have Xi in their conclusion, and we also involve new conditionals that keep fixed all what we computed so far in previous iteration steps. Note that these new conditionals are crucial, since otherwise each maximum entropy compu tation would destroy the results of previous computations by assuming dependencies where we do not want any.\nMore formally, the sequential maximum entropy model (sequential ME-model) of a Bayesian network (D, KB), denoted Pr�k[KB], is defined as PrME[KBn], where KB1 and KBi for 1 < i ::::; n are defined as follows:\nKB1 KBx1 J T KBi Upa{X;) KBx,Jpa{Xi) U\n{(w I T)[Pr ME[KBi- d (w)] l wE Ou,_1}.\nThe following result shows that the sequential ME-model respects all the conditional independencies encoded in D (note that this also shows that the sequential ME-model does not depend on the selected ordering of the variables).\nTheorem 4.1 Let (D, KB) be a Bayesian network. Then, it holds Pr�k[KB] = Prv[KB].\n4.3 CREDAL NETWORKS\nWe now apply the principle of sequential maximum en tropy to interval Bayesian networks and credal networks.\nThe sequential ME-model of an interval Bayesian (resp., credal) network is defined exactly like the sequential ME model of a classical Bayesian network (see Section 4.2).\n4.3.1 Interval Bay esian Networks\nWe first focus on interval Bayesian networks. The next result shows that their sequential ME-model can be com puted by local entropy maximizations (one for each vari able Xi E U and each instantiation of its parents pa(Xi)).\nTheorem 4.2 Let (D, KB) be an interval Bayesian net work. Let the Bayesian network (D, KB*) be built from (D, KB) by replacing each set of interval conditionals\nby the new set of conditionals\nwhere (rr, ... , rd,) is the optimal solution of the following optimization problem (over r1 , . • . , r d, 2: 0):\nd; max L -rilogri subject to CC1 (1)\nj=l\nwithCC 1 being the least set of linear constraints containing r1 + · · · +rd, = 1 and li::::; ri::::; Ujfor allj E [1 : d i]\n·\nThen, it holds Prv[KB*] = Pr�k[KB].\nAn immediate corollary is that the sequential ME-model respects all the conditional independencies encoded in D.\nCorollary 4.3 Let (D, KB) be an interval Bayesian net work. Then, it holds Pr�'H KB] E Pr v [ KB].\nMoreover, the sequential ME-model does not depend on the selected ordering of the variables.\nCorollary 4.4 Let (D,KB) be an interval Bayesian net work. Then, the same sequential ME-model of KB is ob tained for every ordering (X 1, . . . , X n) of the variables in U that is consistent with D.\nSummarizing, the sequential ME-model of an interval Bayesian network can be computed as follows. For each variable Xi E U and each instantiation of its parents pa(Xi), we perform one local entropy maximization over di variables subject to 2di + 1linear constraints.\nSuch local entropy maximizations can easily be done by ex isting maximum entropy software. For example, the system SPIRIT [33] works with point conditionals, and can thus be used to handle incomplete Bayesian networks (that is, inter val Bayesian networks that contain only conditionals of the form (djc)[r] or (djc)[O, 1]). Moreover, PIT [34, 11] also works with interval conditionals, and can thus be used for interval Bayesian networks in their full generality.\n4.3.2 General Credal Networks\nWe now generalize the results of Section 4.3.1 to credal networks. The next theorem shows that the sequential ME model can be computed by local entropy maximizations.\nTheorem 4.5 Let (D, KB) be a credal network. Let the Bayesian network (D, KB*) be built from (D, KB) by re placing each set of convex conditionals\nUNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000 369\nby the new set of conditionals\nKH�(; lpa(Xi) = {(Xi= Xi,j I pa(Xi)) [rj] l j E [1: di]},\nwhere ( ri, ... , r� ) is the optimal solution of the following optimization probiem (over r1, ... , r d; ;:::: 0):\nd; max I:: - rj logrj subject to (r1, ... , rdJ E K. (2)\nj=l\nThen, it holds Prv[KB*] = Pr�HKB].\nIt immediately follows that the sequential ME-model re spects all the conditional independencies encoded in D.\nCorollary 4.6 For all credal networks (D, KB), it holds Pr��[KB] E Prv[KB].\nMoreover, the sequential ME-model does not depend on the selected ordering of the variables.\nCorollary 4.7 Let (D, KB) be a credal network. Then, the same sequential ME-model of KB is obtained for every ordering (Xl,X2, ... ,Xn) of the variables in U that is consistent with D.\nHence, if we express convex sets by linear constraints, then the sequential ME-model can be computed as follows. For each convex conditional (Xi I pa(Xi))[K] E KB, we per form one local entropy maximization over di variables sub ject to the linear constraints that represent K.\n5 EXAMPLES\nIn this section, we give some examples to illustrate the prin ciple of sequential maximum entropy.\nExample 5.1 Consider again the interval Bayesian net work (D, KBu) of Example 3.1. Under sequential maxi mum entropy, the selected probability that the alarm starts when there is a burglary does not depend anymore on the probability that the daughter calls when she hears the alarm sound. More precisely, Pr�HKBu](B = b I A= a) is given by 0.5 and thus independent of u.\nIn the next example, we consider an interval Bayesian net work with cycles and non-binary random variables.\nExample 5.2 Let the directed acyclic graph D over the discrete random variables A, B, C, E, and F with the domains {a1,a2}, {b1,b2}, { c1,c2}, {e1,e2}, and {/1, h, /3}, respectively, be given by Fig. 3.\nSome interval conditionals in an interval Bayesian net work (D, KB) and their corresponding conditionals in the Bayesian network (D, KB*) produced by the principle of sequential maximum entropy are shown in Table 1.\nLet us consider some entailed conditional probabilities. For instance, Pr�HKB](F =hI A= al) = 0.64, while the\nminimum (resp., maximum) of Pr(F =hI A= al) sub ject to PrE Prv[KB] is given by 0.63 (resp., 0.84).\nFigure 3: Directed Tree D\nWe showed that the unique joint distribution of a Bayesian tree coincides with the maximum entropy model of its con ditional distributions. We then presented a new kind of maximum entropy models, which are computed sequen tially. We showed that for all general Bayesian networks, the sequential maximum entropy model coincides with the unique joint distribution. We then applied the new prin ciple of sequential maximum entropy to credal networks. We especially showed that this application is equivalent to a number of small local entropy maximizations.\nA very interesting topic of future research is to apply the results of this work to the framework of probabilistic logic programming [32, 15, 28]. Moreover, it would be inter esting to use the principle of sequential maximum entropy in order to add causality to probabilistic default reasoning with conditional constraints [27].\n370 UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000\nAcknowledgments\nI am very grateful to Fabio Gagliardi Cozman, Gabriele Kem-lsbemer, and Richard Neapolitan for their useful comments on an earlier version of this paper. Many thanks also to the referees for their useful suggestions.\nThis work has been partially supported by a DFG grant and the Austrian Science Fund Project N Z29-INF.\nReferences\n[ l ] F. Bacchus, A. Grove, J. Halpern, and D. Koller. From sta tistical knowledge bases to degrees of beliefs. Artif. Intell., 87:75-143, 1996. [2] J. S. Breese and K. W. Fertig. Decision making with interval influence diagrams. In Uncertainty in Artificial Intelligence 6, pp. 467-478. Elsevier Science, North-Holland, 1991.\n[3] A. Cano, J. E. Cano, and S. Moral. Convex sets of proba bilities propagation by simulated annealing. In Proceedings IPMU-94, pp. 978-983, 1994.\n[4] A. Cano and S. Moral. A genetic algorithm to approximate convex sets of probabilities. In Proceedings /PMU-96, pp. 859-864, 1996.\n[5] R. Carnap. Logical Foundations of Probability. University of Chicago Press, Chicago, 1950.\n[6] P. Cheeseman. A method of computing generalized Bayesian probability values for expert systems. In Proceed ings /JCA/-83, pp. 198-202. Morgan Kaufmann, 1983. [7] F. G. Cozman. Robustness analysis of Bayesian networks with local sets of distributions. In Proceedings UAI-97, pp. 108-115. Morgan Kaufmann, 1997. [8] F. G. Cozman. Irrelevance and independence relations in Quasi-Bayesian networks. In Proceedings UA/-98, pp. 89- 96. Morgan Kaufmann, 1998. [9] F. G. Cozman. Irrelevance and independence axioms in Quasi-Bayesian theory. In Proceedings ECSQARU-99, LNAI 1638, pp. 128-136. Springer, 1999.\n[10] B. de Finetti. Theory of Probability. Wiley, New York, 1974. [ l l] W. Ertel and M. Schramm. Combining data and knowledge by maxent-optimisation of probability distributions. In Pro ceedings PKDD '99, LNCS 1704, pp. 323-328. Springer, 1999.\n[12] E. Fagiuoli and M. Zaffalon. 2U: an exact interval propaga tion algorithm for polytrees with binary variables. Artif. In tell., 106:77-107, 1998.\n[13] A.M. Frisch and P. Haddawy. Anytime deduction for prob abilistic logic. Artif. lntell., 69:93-122, 1994.\n[14] A. J. Grove, J. H. Halpern, and D. Koller. Random worlds and maximum entropy. J. Artif. lntell. Res., 2:33-88, 1994.\n[15] P. Haddawy. Generating Bayesian networks from probabil ity logic knowledge bases. In Proceedings UA/-94, pp. 262- 269. Morgan Kaufmann, 1994.\n[16] D. Hunter. Causality and maximum entropy updating. Int. J. Approx. Reasoning, 3(1):379-406, 1989.\n[17] E. T. Jaynes. Information theory and statistical mechanics. Physical Review, 106:620-30, 1957.\n[18] E. T. Jaynes. Papers on Probability, Statistics and Statistical Physics. D. Reidel, 1983.\n[19] R. W. Johnson and J. E. Shore. Comments on and correc tions to \"Axiomatic derivation of the principle of maximum entropy and the principle of minimum cross-entropy\". IEEE Trans. lnf Theory, IT-29(6):942-943, 1983.\n[20] G. Kem-Isbemer. A logically sound method for uncer tain reasoning with quantified conditionals. In Proceedings ECSQARU/ FAPR-97, LNAI 1244, pp. 365-379. Springer, 1997.\n[21] G. Kem-Isbemer. Characterizing the principle of minimum cross-entropy within a conditional-logical framework. Ar tif. lntell., 98: 169-208, 1998.\n[22] D. Koller and J. Y. Halpern. Irrelevance and conditioning in first-order probabilistic logic. In Proceedings AAAI-96, pp. 569-576. AAAI Press I MIT Press, 1996.\n[23] H. E. Kyburg. Bayesian and non-Bayesian evidential updat ing. Artif. lntell., 31:271-293, 1987. [24] T. Lu�asiewicz. Credal networks under maximum entropy. Techmcal Report 1843-00-03, Institut fiir Inforrnationssys teme, Technische Universitiit Wien, 2000. ftp: //ftp. kr.tuwien.ac.at/pub/tr/rr0003.ps.gz.\n[25] T. Lukasiewicz. Local probabilistic deduction from taxo nomic and probabilistic knowledge-bases over conjunctive events. Int. J. Approx. Reasoning, 21(1):23-61, 1999.\n[26] T. Lukasiewicz. Probabilistic deduction with conditional constraints over basic events. J. Artif. /ntell. Res., 10: 199- 241, 1999.\n[27] T. Lukasiewicz. Probabilistic default reasoning with con ditional constraints. In Proceedings of the 8th International Workshop on Non-Monotonic Reasoning, Special Session on Uncertainty Frameworks in NMR, 2000.\n[28] T. Lukasiewicz and G. Kem-Isbemer. Probabilistic logic prograrmning under maximum entropy. In Proceedings ECSQARU-99, LNAI 1638, pp. 279-292. Springer, 1999.\n[29] J. B . . Paris and A. Vencovska. A note on the inevitability of maximum entropy. Int. J. Approx. Reasoning, 14:183-223, 1990.\n[30] J. B. Paris and A. Vencovska. In defense of the maximum entropy inference process. Int. J. Approx. Reasoning, 17:77- 103, 1997.\n[31] J. Pearl. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan Kaufmann, San\nMateo, CA, 1988.\n[32] D. Poole. Probabilistic Hom abduction and Bayesian net works. Artif. lntell., 64:81-129, 1993. [33] W. Rodder and C. -H. Meyer. Coherent knowledge process ing at maximum entropy by SPIRIT. In Proceedings UA/96, pp. 470-476. Morgan Kaufmann, 1996.\n[34] M. Schramm and W. Ertel. Reasoning with probabilities and maximum entropy: The system PIT and its application in LEXMED. In Proceedings of the Symposium on Operations Research (SOR '99), 1999.\n[35] C. E. Shannon and W. Weaver. A Mathematical Theory of Communication. University of Illinois Press, Urbana, Illi nois, 1949.\n[36] J. E. Shore and R. W. Johnson. Axiomatic derivation of the principle of maximum entropy and the principle of min imum cross-entropy. IEEE Trans. lnf. Theory, IT-26:26-37 1980. •\n[37] B. Tessem. Interval probability propagation. Int. J. Ap prox. Reasoning, 7:95-120, 1992.\n[38] H. T hone, U. Giintzer, and W. KieBling. Increased robust ness of Bayesian networks through probability intervals. Int. J. Approx. Reasoning, 17:37-76, 1997.\n[39] P. Walley. Statistical Reasoning with Imprecise Probabili ties. Chapman and Hall, New York, 1991."
    } ],
    "references" : [ {
      "title" : "From sta­ tistical knowledge bases to degrees of beliefs",
      "author" : [ "F. Bacchus", "A. Grove", "J. Halpern", "D. Koller" ],
      "venue" : "Artif. Intell.,",
      "citeRegEx" : "Bacchus et al\\.,? \\Q1996\\E",
      "shortCiteRegEx" : "Bacchus et al\\.",
      "year" : 1996
    }, {
      "title" : "Decision making with interval influence diagrams",
      "author" : [ "J.S. Breese", "K.W. Fertig" ],
      "venue" : "In Uncertainty in Artificial Intelligence",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1991
    }, {
      "title" : "Convex sets of proba­ bilities propagation by simulated annealing",
      "author" : [ "A. Cano", "J.E. Cano", "S. Moral" ],
      "venue" : "In Proceedings IPMU-94,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1994
    }, {
      "title" : "A genetic algorithm to approximate convex sets of probabilities",
      "author" : [ "A. Cano", "S. Moral" ],
      "venue" : "In Proceedings /PMU-96,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1996
    }, {
      "title" : "Logical Foundations of Probability",
      "author" : [ "R. Carnap" ],
      "venue" : "University of Chicago Press, Chicago,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1950
    }, {
      "title" : "Irrelevance and independence axioms in Quasi-Bayesian theory",
      "author" : [ "F.G. Cozman" ],
      "venue" : "In Proceedings ECSQARU-99,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1999
    }, {
      "title" : "Theory of Probability",
      "author" : [ "B. de Finetti" ],
      "venue" : null,
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1974
    }, {
      "title" : "Combining data and knowledge by maxent-optimisation of probability distributions",
      "author" : [ "M. Schramm" ],
      "venue" : "In Pro­ ceedings PKDD '99,",
      "citeRegEx" : "Schramm.,? \\Q1999\\E",
      "shortCiteRegEx" : "Schramm.",
      "year" : 1999
    }, {
      "title" : "2U: an exact interval propaga­ tion algorithm for polytrees with binary variables",
      "author" : [ "M. Zaffalon" ],
      "venue" : "Artif. In­ tell.,",
      "citeRegEx" : "Zaffalon.,? \\Q1998\\E",
      "shortCiteRegEx" : "Zaffalon.",
      "year" : 1998
    }, {
      "title" : "Anytime deduction for prob­ abilistic logic",
      "author" : [ "A.M. Frisch", "P. Haddawy" ],
      "venue" : "Artif. lntell.,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 1994
    }, {
      "title" : "Random worlds and maximum entropy",
      "author" : [ "A.J. Grove", "J.H. Halpern", "D. Koller" ],
      "venue" : "J. Artif. lntell. Res.,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1994
    }, {
      "title" : "Causality and maximum entropy updating",
      "author" : [ "D. Hunter" ],
      "venue" : "Int. J. Approx. Reasoning,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1989
    }, {
      "title" : "Information theory and statistical mechanics",
      "author" : [ "E.T. Jaynes" ],
      "venue" : "Physical Review,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 1957
    }, {
      "title" : "Papers on Probability, Statistics and Statistical Physics",
      "author" : [ "E.T. Jaynes" ],
      "venue" : "D. Reidel,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 1983
    }, {
      "title" : "Comments on and correc­ tions to \"Axiomatic derivation of the principle of maximum entropy and the principle of minimum cross-entropy",
      "author" : [ "R.W. Johnson", "J.E. Shore" ],
      "venue" : "IEEE Trans. lnf Theory,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 1983
    }, {
      "title" : "A logically sound method for uncer­ tain reasoning with quantified conditionals",
      "author" : [ "G. Kem-Isbemer" ],
      "venue" : "In Proceedings ECSQARU/ FAPR-97,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 1997
    }, {
      "title" : "Characterizing the principle of minimum cross-entropy within a conditional-logical framework",
      "author" : [ "G. Kem-Isbemer" ],
      "venue" : "Ar­ tif. lntell.,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 1998
    }, {
      "title" : "Bayesian and non-Bayesian evidential updat­",
      "author" : [ "H.E. Kyburg" ],
      "venue" : "ing. Artif. lntell.,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 1987
    }, {
      "title" : "Credal networks under maximum entropy. Techmcal Report 1843-00-03, Institut fiir Inforrnationssys­ teme",
      "author" : [ "T. Lu�asiewicz" ],
      "venue" : "Technische Universitiit Wien,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2000
    }, {
      "title" : "Local probabilistic deduction from taxo­ nomic and probabilistic knowledge-bases over conjunctive events",
      "author" : [ "T. Lukasiewicz" ],
      "venue" : "Int. J. Approx. Reasoning,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 1999
    }, {
      "title" : "Probabilistic deduction with conditional constraints over basic events",
      "author" : [ "T. Lukasiewicz" ],
      "venue" : "J. Artif. /ntell. Res.,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 1999
    }, {
      "title" : "Probabilistic default reasoning with con­ ditional constraints",
      "author" : [ "T. Lukasiewicz" ],
      "venue" : "In Proceedings of the 8th International Workshop on Non-Monotonic Reasoning, Special Session on Uncertainty Frameworks in NMR,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2000
    }, {
      "title" : "Probabilistic logic prograrmning under maximum entropy",
      "author" : [ "T. Lukasiewicz", "G. Kem-Isbemer" ],
      "venue" : "In Proceedings ECSQARU-99,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 1999
    }, {
      "title" : "A note on the inevitability of maximum entropy",
      "author" : [ "Paris", "A. Vencovska" ],
      "venue" : "Int. J. Approx. Reasoning,",
      "citeRegEx" : "Paris and Vencovska.,? \\Q1990\\E",
      "shortCiteRegEx" : "Paris and Vencovska.",
      "year" : 1990
    }, {
      "title" : "Vencovska. In defense of the maximum entropy inference process",
      "author" : [ "A.J.B. Paris" ],
      "venue" : "Int. J. Approx. Reasoning,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 1997
    }, {
      "title" : "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference",
      "author" : [ "J. Pearl" ],
      "venue" : null,
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 1988
    }, {
      "title" : "Probabilistic Hom abduction and Bayesian net­ works",
      "author" : [ "D. Poole" ],
      "venue" : "Artif. lntell.,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 1993
    }, {
      "title" : "Reasoning with probabilities and maximum entropy: The system PIT and its application in LEXMED",
      "author" : [ "M. Schramm", "W. Ertel" ],
      "venue" : "In Proceedings of the Symposium on Operations Research (SOR",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 1999
    }, {
      "title" : "A Mathematical Theory of Communication",
      "author" : [ "C.E. Shannon", "W. Weaver" ],
      "venue" : null,
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 1949
    }, {
      "title" : "Axiomatic derivation of the principle of maximum entropy and the principle of min­ imum cross-entropy",
      "author" : [ "J.E. Shore", "R.W. Johnson" ],
      "venue" : "IEEE Trans. lnf. Theory,",
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 1980
    }, {
      "title" : "Interval probability propagation",
      "author" : [ "B. Tessem" ],
      "venue" : "Int. J. Ap­ prox. Reasoning,",
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 1992
    }, {
      "title" : "Increased robust­ ness of Bayesian networks through probability intervals",
      "author" : [ "H. T hone", "U. Giintzer", "W. KieBling" ],
      "venue" : "Int. J. Approx. Reasoning,",
      "citeRegEx" : "38",
      "shortCiteRegEx" : "38",
      "year" : 1997
    }, {
      "title" : "Statistical Reasoning with Imprecise Probabili­ ties",
      "author" : [ "P. Walley" ],
      "venue" : null,
      "citeRegEx" : "39",
      "shortCiteRegEx" : "39",
      "year" : 1991
    } ],
    "referenceMentions" : [ ],
    "year" : 2011,
    "abstractText" : "We apply the principle of maximum entropy to select a unique joint probability distribution from the set of all joint probability distributions speci­ fied by a credal network. In detail, we start by showing that the unique joint distribution of a Bayesian tree coincides with the maximum en­ tropy model of its conditional distributions. This result, however, does not hold anymore for gen­ eral Bayesian networks. We thus present a new kind of maximum entropy models, which are computed sequentially. We then show that for all general Bayesian networks, the sequential max­ imum entropy model coincides with the unique joint distribution. Moreover, we apply the new principle of sequential maximum entropy to in­ terval Bayesian networks and more generally to credal networks. We especially show that this ap­ plication is equivalent to a number of small local entropy maximizations.",
    "creator" : "pdftk 1.41 - www.pdftk.com"
  }
}