{
  "name" : "1506.06490.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Answer Sequence Learning with Neural Networks for Answer Selection in Community Question Answering",
    "authors" : [ "Xiaoqiang Zhou", "Baotian Hu", "Qingcai Chen", "Buzhou Tang", "Xiaolong Wang" ],
    "emails" : [ "xiaoqiang.jeseph@gmail.com", "baotianchina@gmail.com", "qingcai.chen@gmail.com", "tangbuzhou@gmail.com", "wangxl@insun.hit.edu.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "In this paper, the answer selection problem in community question answering (CQA) is regarded as an answer sequence labeling task, and a novel approach is proposed based on the recurrent architecture for this problem. Our approach applies convolution neural networks (CNNs) to learning the joint representation of questionanswer pair firstly, and then uses the joint representation as input of the long shortterm memory (LSTM) to learn the answer sequence of a question for labeling the matching quality of each answer. Experiments conducted on the SemEval 2015 CQA dataset shows the effectiveness of our approach."
    }, {
      "heading" : "1 Introduction",
      "text" : "Answer selection in community question answering (CQA), which recognizes high-quality responses to obtain useful question-answer pairs, is greatly valuable for knowledge base construction and information retrieval systems. To recognize matching answers for a question, typical approaches model semantic matching between question and answer by exploring various features (Wang et al., 2009a; Shah and Pomerantz, 2010). Some studies exploit syntactic tree structures (Wang et al., 2009b; Moschitti et al., 2007) to measure the semantic matching between question and answer. However, these approaches require high-quality data and various external resources which may be quite difficult to obtain. To take advantage of a large quantity of raw data, deep learning based approaches (Wang et al., 2010; Hu et al., 2013) are proposed to learn the distributed representation of question-answer pair directly. One\n∗* Corresponding author\ndisadvantage of these approaches lies in that semantic correlations embedded in the answer sequence of a question are ignored, while they are very important for answer selection. Figure 1 is a example to show the relationship of answers in the sequence for a given question. Intuitively, other answers of the question are beneficial to judge the quality of the current answer.\nRecently, recurrent neural network (RNN), especially Long Short-Term Memory (LSTM) (Hochreiter et al., 2001), has been proved superiority in various tasks (Sutskever et al., 2014; Srivastava et al., 2015) and it models long term and short term information of the sequence. And also, there are some works on using convolutional neural networks (CNNs) to learn the representations of sentence or short text, which achieve state-of-the-art performance on sentiment classification (Kim, 2014) and short text matching (Hu et al., 2014).\nIn this paper, we address the answer selection problem as a sequence labeling task, which identifies the matching quality of each answer in the answer sequence of a question. Firstly, CNNs are used to learn the joint representation of ques-\nar X\niv :1\n50 6.\n06 49\n0v 1\n[ cs\n.C L\n] 2\n2 Ju\nn 20\n15\ntion answer (QA) pair. Then the learnt joint representations are used as inputs of LSTM to predict the quality (e.g., Good, Bad and Potential) of each answer in the answer sequence. Experiments conducted on the CQA dataset of the answer selection task in SemEval-20151 show that the proposed approach outperforms other state-of-the-art approaches."
    }, {
      "heading" : "2 Related Work",
      "text" : "Prior studies on answer selection generally treated this challenge as a classification problem via employing machine learning methods, which rely on exploring various features to represent QA pair. Huang et al. (2007) integrated textual features with structural features of forum threads to represent the candidate QA pairs, and used support vector machine (SVM) to classify the candidate pairs. Beyond typical features, Shah and Pomerantz (2010) trained a logistic regression (LR) classifier with user metadata to predict the quality of answers in CQA. Ding et al. (2008) proposed an approach based on conditional random fields (CRF), which can capture contextual features from the answer sequence for the semantic matching between question and answer. Additionally, the translation-based language model was also used for QA matching by transferring the answer to the corresponding question (Jeon et al., 2005; Xue et al., 2008; Zhou et al., 2011). The translation-based methods suffer from the informal words or phrases in Q&A archives, and perform less applicability in new domains.\nIn contrast to symbolic representation, Wang et al. (2010) proposed a deep belief nets (DBN) based semantic relevance model to learn the distributed representation of QA pair. Recently, the convolutional neural networks (CNNs) based sentence representation models have achieved successes in neural language processing (NLP) tasks. Yu et al. (2014) proposed a convolutional sentence model to identify answer contents of a question from Q&A archives via means of distributed representations. The work in Hu et al. (2014) demonstrated that 2-dimensional convolutional sentence models can represent the hierarchical structures of sentences and capture rich matching patterns between two language objects.\n1http://alt.qcri.org/semeval2015/task3/"
    }, {
      "heading" : "3 Approach",
      "text" : "We consider the answer selection problem in CQA as a sequence labeling task. To label the matching quality of each answer for a given question, our approach models the semantic links between successive answers, as well as the semantic relevance between question and answer. Figure 1 summarizes the recurrent architecture of our model (RCNN). The motivation of R-CNN is to learn the useful context to improve the performance of answer selection. The answer sequence is modeled to enrich semantic features.\nAt each step, our approach uses the pre-trained word embeddings to encode the sentences of QA pair, which then is used as the input vectors of the model. Based on the joint representation of QA pair learned from CNNs, the LSTM is applied in our model for answer sequence learning, which makes a prediction to each answer of the question with softmax function."
    }, {
      "heading" : "3.1 Convolutional Neural Networks for QA Joint Learning",
      "text" : "Given a question-answer pair at the step t, we use convolutional neural networks (CNNs) to learn the joint representation pt for the pair. Figure 2 illustrates the process of QA joint learning, which includes two stages: summarizing the meaning of the question and an answer, and generating the joint representation of QA pair.\nTo obtain high-level sentence representations of the question and answer, we set 3 hidden layers in two convolutional sentence models respectively. The output of each hidden layer is made up of a set of 2-dimensional arrays called feature map parameters (wm, bm). Each feature map is the outcome of one convolutional or pooling filter. Each pooling layer is followed an activation function σ. The output of themth hidden layer is computed as\nEq. 1:\nHm = σ(pool(wmHm−1 + bm)) (1)\nHere, H0 is one real-value matrix after sentence semantic encoding by concatenating the word vectors with sliding windows. It is the input of deep convolution and pooling, which is similar to that of traditional image input.\nFinally, we combine the two sentence models by adding an additional layer Ht on the top. The learned joint representation pt for QA pair is formalized as Eq. 2:\npt = σ(wtHt + bt) (2)\nwhere σ is an activation function, and the input vector is constructed by concatenating the sentence representations of question and answer."
    }, {
      "heading" : "3.2 LSTM for Answer Sequence Learning",
      "text" : "Based on the joint representation of QA pair, the LSTM unit of our model performs answer sequence learning to model semantic links between continuous answers. Unlike the traditional recurrent unit, the LSTM unit modulates the memory at each time step, instead of overwriting the states. The key component of LSTM unit is the memory cell ct which has a state over time, and the LSTM unit decides to modify and add the memory in the cell via the sigmoidal gates: input gate it, forget gate ft and output gate ot. The implementation of the LSTM unit in our study is close the one discussed by Graves (2013). Given the joint representation pt at time t, the memory cell ct is updated by the input gate’s activation it and the forget gate’s activation ft. The updating equation is given by Eq. 3:\nct = ftct−1+ittanh(Wxcpt+Whcht−1+bc) (3)\nThe LSTM unit keeps to update the context by discarding the useless context in forget gate ft and adding new content from input gate it. The extents to modulate context for these two gates are computed as Eq. 4 and Eq. 5:\nit = σ(Wxipt +Whih(t−1) +Wcict−1 + bi) (4)\nft = σ(Wxfpt+Whfht−1+Wcfct−1+ bf ) (5)\nWith the updated cell state ct, the final output from LSTM unit ht is computed as Eq 6:\not = σ(Wxopt +Whoht−1 +Wcoct + bo) (6)\nht = ottanh(ct) (7)\nNote that (W∗, b∗) is the parameters of LSTM unit, in which Wcf ,Wci , and Wco are diagonal matrices.\nAccording to the output ht at each time step, our approach estimates the conditional probability of the answer sequence over answer classes, it is given by Eq. 8:\nP (y1, ..., yT |c, p1, ..., pt−1) = T∏ t=1 p(yt|c, y1, ..., yt−1) (8)\nHere, (y1, ..., yT ) is the corresponding label sequence for the input sequence (p1, ..., pt−1), and the class distribution p(yt|c, y1, ..., .yt−1) is represented by a softmax function."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Experiment Setup",
      "text" : "Experimental Dataset: We conduct experiments on the public dataset of the answer selection challenge in SemEval 2015. This dataset consists of three subsets: training, development, and test sets, and contains 3,229 questions with 21,062 answers.\nThe answers falls into three classes: Good, Bad, and Potential, accounting for 51%, 39%, and 10% respectively. The statistics of the dataset are summarized in Table 1, where #question/answer denotes the number of questions/answers, and length stands for the average number of answers for a question. Competitor Methods: We compare our approach against the following competitor methods:\nSVM (Huang et al., 2007): An SVM-based method with bag-of-words (textual features), nontextual features, and features based on topic model (i.e., latent Dirichlet allocation, LDA).\nCRF (Ding et al., 2008): A CRF-based method using the same features as the SVM approach.\nDBN (Wang et al., 2010): Taking bag-of-words representation, the method applies deep belief nets to learning the distributed representation of QA pair, and predicts the class of answers using a logistic regression classifier on the top layer.\nmDBN (Hu et al., 2013): In contrast to DBN, multimodal DBN learns the joint representations of textual features and non-textual features rather than bag-of-words.\nCNN: Using word embedding, the CNNs based model in Hu et al. (2014) is used to learn the representations of questions and answers, and a logistic regression classifier is used to predict the class of answers. Evaluation Metrics: The evaluation metrics include Macro − precision(P ), Macro − recall(R), Macro − F1(F1), and F1 scores of the individual classes. According to the evaluation results on the development set, all the hyperparameters are optimized on the training set. Model Architecture and Training Details: The CNNs of our model for QA joint representation learning have 3 hidden layers for modeling question and answer sentence respectively, in which each layer has 100 feature maps for convolution and pooling operators. The window sizes of convolution for each layer are [1×1, 2×2, 2×2], the window sizes of pooling are [2 × 2, 2 × 2, 1 × 1]. For the LSTM unit, the size of input gate is set to 200, the sizes of forget gate, output gate, and memory cell are all set to 360.\nStochastic gradient descent (SGD) algorithm via back-propagation through time is used to train the model. To prevent serious overfitting, early stopping and dropout (Hinton et al., 2012) are used during the training procedure. The learning rate\nλ is initialized to be 0.01 and is updated dynamically according to the gradient descent using the ADADELTA method (Zeiler, 2012). The activation functions (σ, γ) in our model adopt the rectified linear unit (ReLU) (Dahl et al., 2013). In addition, the word embeddings for encoding sentences are pre-trained with the unsupervised neural language model (Mikolov et al., 2013) on the Qatar Living data2."
    }, {
      "heading" : "4.2 Results and Analysis",
      "text" : "Table 2 summarizes the Macro-averaged results. The F1 scores of the individual classes are presented in Table 3.\nIt is clear to see that the proposed R-CNN approach outperforms the competitor methods over the Macro-averaged metrics as expected from Table 2. The main reason lies in that R-CNN takes advantages of the semantic correlations between successive answers by LSTM, in addition to the semantic relationships between question and answer. The joint representation of QA pair learnt by CNNs also captures richer matching patterns between question and answer than other methods.\nIt is notable that the methods based on deep learning perform more powerful than SVM and CRF, especially for complicate answers (e.g., Potential answers). In contrast, SVM and CRF using a large amount of features perform better for the answers that have obvious tendency (e.g., Good and Bad answers). The main reason is that the distributed representation learnt from deep learning architecture is able to capture the semantic relationships between question and answer. On the other hand, the feature-engineers in both SVM and CRF suffer from noisy information of CQA and the feature sparse problem for short questions and answers.\nCompared to DBN and mDBN, CNN and RCNN show their superiority in modeling QA pair.\n2http://alt.qcri.org/semeval2015/task3/index.php?id=dataand-tools\nThe convolutional sentence models, used in CNN and R-CNN, can learn the hierarchical structure of language object by deep convolution and pooling operators. In addition, both R-CNN and CNN encode the sentence into one tensor, which makes sure the representation contains more semantic features than the bag-of-words representation in DBN and mDBN.\nThe improvement achieved by R-CNN over CNN demonstrates that answer sequence learning is able to improve the performance of the answer selection in CQA. Because modeling the answer sequence can enjoy the advantage of the shared representation between successive answers, and complement the classification features with the learnt useful context from previous answers. Furthermore, memory cell and gates in LSTM unit modify the valuable context to pass onwards by updating the state of RNN during the learning procedure.\nThe main improvement of R-CNN against with the competitor methods comes from the Potential answers, which are much less than other two type of answers. It demonstrates that R-CNN is able to process the unbalance data. In fact, the Potential answers are most difficult to identify among the three types of answers as Potential is an intermediate category (Màrquez et al., 2015). Nevertheless, R-CNN achieves the highest F1 score of 15.22% on Potential answers. In CQA, Q&A archives usually form one multi-parties conversation when the asker gives feedbacks (e.g., “ok” and “please”) to users responses, indicating that the answers of one question are sematic related. Thus, it is easy to understand that R-CNN performs better performance than competitor methods, especially on the recall. The reason is that R-CNN can model semantic correlations between successive answers to learn the context and the long range dependencies in the answer sequence."
    }, {
      "heading" : "5 Conclusions and Future Work",
      "text" : "In this paper, we propose an answer sequence learning model R-CNN for the answer selection task by integrating LSTM unit and CNNs. Based on the recurrent architecture of our model, our approach is able to model the semantic link between successive answers, in addition to the semantic relevance between question and answer. Experimental results demonstrate that our approach can learn the useful context from the answer sequence to improve the performance of answer selection in CQA.\nIn the future, we plan to explore the methods on training the unbalance data to improve the overall performances of our approach. Based on this work, more research can be conducted on topic recognition and semantic roles labeling for human-human conversations in real-world.\nAcknowledgments: This work was supported in part by National 863 Program of China (2015AA015405), NSFCs (National Natural Science Foundation of China) (61402128, 61473101, 61173075 and 61272383). We thank the anonymous reviewers for their insightful comments."
    } ],
    "references" : [ {
      "title" : "Improving deep neural networks for lvcsr using rectified linear units and dropout",
      "author" : [ "Dahl et al.2013] George E. Dahl", "Tara N. Sainath", "Geoffrey E. Hinton" ],
      "venue" : "In ICASSP,",
      "citeRegEx" : "Dahl et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Dahl et al\\.",
      "year" : 2013
    }, {
      "title" : "Using conditional random fields to extract contexts and answers of questions from online forums",
      "author" : [ "Ding et al.2008] Shilin Ding", "Gao Cong", "Chin yew Lin", "Xiaoyan Zhu" ],
      "venue" : "Proceedings of ACL08: HLT",
      "citeRegEx" : "Ding et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Ding et al\\.",
      "year" : 2008
    }, {
      "title" : "Generating sequences with recurrent neural networks. CoRR, abs/1308.0850",
      "author" : [ "Alex Graves" ],
      "venue" : null,
      "citeRegEx" : "Graves.,? \\Q2013\\E",
      "shortCiteRegEx" : "Graves.",
      "year" : 2013
    }, {
      "title" : "Improving neural networks by preventing co-adaptation of feature detectors. CoRR, abs/1207.0580",
      "author" : [ "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov" ],
      "venue" : null,
      "citeRegEx" : "Hinton et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2012
    }, {
      "title" : "Gradient flow in recurrent nets: the difficulty of learning long-term dependencies",
      "author" : [ "Yoshua Bengio", "Paolo Frasconi", "Jürgen Schmidhuber" ],
      "venue" : null,
      "citeRegEx" : "Hochreiter et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Hochreiter et al\\.",
      "year" : 2001
    }, {
      "title" : "Multimodal dbn for predicting high-quality answers in cqa portals",
      "author" : [ "Hu et al.2013] Haifeng Hu", "Bingquan Liu", "Baoxun Wang", "Ming Liu", "Xiaolong Wang" ],
      "venue" : "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics",
      "citeRegEx" : "Hu et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2013
    }, {
      "title" : "Convolutional neural network architectures for matching natural language sentences",
      "author" : [ "Hu et al.2014] Baotian Hu", "Zhengdong Lu", "Hang Li", "Qingcai Chen" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Hu et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2014
    }, {
      "title" : "Extracting chatbot knowledge from online discussion forums",
      "author" : [ "Huang et al.2007] Jizhou Huang", "Ming Zhou", "Dan Yang" ],
      "venue" : "In Proceedings of the 20th International Joint Conference on Artifical Intelligence,",
      "citeRegEx" : "Huang et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2007
    }, {
      "title" : "Finding similar questions in large question and answer archives",
      "author" : [ "Jeon et al.2005] Jiwoon Jeon", "W. Bruce Croft", "Joon Ho Lee" ],
      "venue" : "In Proceedings of the 14th ACM International Conference on Information and Knowledge Management,",
      "citeRegEx" : "Jeon et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Jeon et al\\.",
      "year" : 2005
    }, {
      "title" : "Convolutional neural networks for sentence classification. CoRR, abs/1408.5882",
      "author" : [ "Yoon Kim" ],
      "venue" : null,
      "citeRegEx" : "Kim.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kim.",
      "year" : 2014
    }, {
      "title" : "Semeval-2015 task 3: Answer selection in community question answering",
      "author" : [ "James Glass", "Walid Magdy", "Alessandro Moschitti", "Preslav Nakov", "Bilal Randeree" ],
      "venue" : "In Proceedings of the 9th International Workshop",
      "citeRegEx" : "Màrquez et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Màrquez et al\\.",
      "year" : 2015
    }, {
      "title" : "Efficient estimation of word representations in vector space. CoRR, abs/1301.3781",
      "author" : [ "Kai Chen", "Greg Corrado", "Jeffrey Dean" ],
      "venue" : null,
      "citeRegEx" : "Mikolov et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Exploiting syntactic and shallow semantic kernels for question answer classification",
      "author" : [ "Silvia Quarteroni", "Roberto Basili", "Suresh Manandhar" ],
      "venue" : "In Proceedings of the 45th Annual Meeting",
      "citeRegEx" : "Moschitti et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Moschitti et al\\.",
      "year" : 2007
    }, {
      "title" : "Evaluating and predicting answer quality in community qa",
      "author" : [ "Shah", "Pomerantz2010] Chirag Shah", "Jefferey Pomerantz" ],
      "venue" : "In Proceedings of the 33rd International ACM SIGIR Conference on Research and Development in Information Retrieval,",
      "citeRegEx" : "Shah et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Shah et al\\.",
      "year" : 2010
    }, {
      "title" : "Unsupervised learning of video representations using lstms",
      "author" : [ "Elman Mansimov", "Ruslan Salakhutdinov" ],
      "venue" : null,
      "citeRegEx" : "Srivastava et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Srivastava et al\\.",
      "year" : 2015
    }, {
      "title" : "Sequence to sequence learning with neural networks. CoRR, abs/1409.3215",
      "author" : [ "Oriol Vinyals", "Quoc V. Le" ],
      "venue" : null,
      "citeRegEx" : "Sutskever et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "Extracting chinese question-answer pairs from online forums",
      "author" : [ "Wang et al.2009a] Baoxun Wang", "Bingquan Liu", "Chengjie Sun", "Xiaolong Wang", "Lin Sun" ],
      "venue" : "In IEEE International Conference on Systems, Man, and Cybernetics (SMC),",
      "citeRegEx" : "Wang et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2009
    }, {
      "title" : "A syntactic tree matching approach to finding similar questions in communitybased qa services",
      "author" : [ "Wang et al.2009b] Kai Wang", "Zhaoyan Ming", "TatSeng Chua" ],
      "venue" : "In Proceedings of the 32Nd International ACM SIGIR Conference on Research and",
      "citeRegEx" : "Wang et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2009
    }, {
      "title" : "Modeling semantic relevance for question-answer pairs in web social communities",
      "author" : [ "Wang et al.2010] Baoxun Wang", "Xiaolong Wang", "Chengjie Sun", "Bingquan Liu", "Lin Sun" ],
      "venue" : "In Proceedings of the 48th Annual Meeting of the Association",
      "citeRegEx" : "Wang et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2010
    }, {
      "title" : "Retrieval models for question and answer archives",
      "author" : [ "Xue et al.2008] Xiaobing Xue", "Jiwoon Jeon", "W. Bruce Croft" ],
      "venue" : "In Proceedings of the 31st Annual International ACM SIGIR Conference on Research and Development in Information",
      "citeRegEx" : "Xue et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Xue et al\\.",
      "year" : 2008
    }, {
      "title" : "Deep learning for answer sentence selection. CoRR, abs/1412.1632",
      "author" : [ "Yu et al.2014] Lei Yu", "Karl Moritz Hermann", "Phil Blunsom", "Stephen Pulman" ],
      "venue" : null,
      "citeRegEx" : "Yu et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2014
    }, {
      "title" : "ADADELTA: an adaptive learning rate method. CoRR, abs/1212.5701",
      "author" : [ "Matthew D. Zeiler" ],
      "venue" : null,
      "citeRegEx" : "Zeiler.,? \\Q2012\\E",
      "shortCiteRegEx" : "Zeiler.",
      "year" : 2012
    }, {
      "title" : "Phrase-based translation model for question retrieval in community question answer archives",
      "author" : [ "Zhou et al.2011] Guangyou Zhou", "Li Cai", "Jun Zhao", "Kang Liu" ],
      "venue" : "In Proceedings of the 49th Annual Meeting of the Association for Computational Lin-",
      "citeRegEx" : "Zhou et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 12,
      "context" : "Some studies exploit syntactic tree structures (Wang et al., 2009b; Moschitti et al., 2007) to measure the semantic matching between question and answer.",
      "startOffset" : 47,
      "endOffset" : 91
    }, {
      "referenceID" : 18,
      "context" : "To take advantage of a large quantity of raw data, deep learning based approaches (Wang et al., 2010; Hu et al., 2013) are proposed to learn the distributed representation of question-answer pair directly.",
      "startOffset" : 82,
      "endOffset" : 118
    }, {
      "referenceID" : 5,
      "context" : "To take advantage of a large quantity of raw data, deep learning based approaches (Wang et al., 2010; Hu et al., 2013) are proposed to learn the distributed representation of question-answer pair directly.",
      "startOffset" : 82,
      "endOffset" : 118
    }, {
      "referenceID" : 4,
      "context" : "Recently, recurrent neural network (RNN), especially Long Short-Term Memory (LSTM) (Hochreiter et al., 2001), has been proved superiority in various tasks (Sutskever et al.",
      "startOffset" : 83,
      "endOffset" : 108
    }, {
      "referenceID" : 15,
      "context" : ", 2001), has been proved superiority in various tasks (Sutskever et al., 2014; Srivastava et al., 2015) and it models long term and short term information of the sequence.",
      "startOffset" : 54,
      "endOffset" : 103
    }, {
      "referenceID" : 14,
      "context" : ", 2001), has been proved superiority in various tasks (Sutskever et al., 2014; Srivastava et al., 2015) and it models long term and short term information of the sequence.",
      "startOffset" : 54,
      "endOffset" : 103
    }, {
      "referenceID" : 9,
      "context" : "And also, there are some works on using convolutional neural networks (CNNs) to learn the representations of sentence or short text, which achieve state-of-the-art performance on sentiment classification (Kim, 2014) and short text matching (Hu et al.",
      "startOffset" : 204,
      "endOffset" : 215
    }, {
      "referenceID" : 6,
      "context" : "And also, there are some works on using convolutional neural networks (CNNs) to learn the representations of sentence or short text, which achieve state-of-the-art performance on sentiment classification (Kim, 2014) and short text matching (Hu et al., 2014).",
      "startOffset" : 240,
      "endOffset" : 257
    }, {
      "referenceID" : 8,
      "context" : "Additionally, the translation-based language model was also used for QA matching by transferring the answer to the corresponding question (Jeon et al., 2005; Xue et al., 2008; Zhou et al., 2011).",
      "startOffset" : 138,
      "endOffset" : 194
    }, {
      "referenceID" : 19,
      "context" : "Additionally, the translation-based language model was also used for QA matching by transferring the answer to the corresponding question (Jeon et al., 2005; Xue et al., 2008; Zhou et al., 2011).",
      "startOffset" : 138,
      "endOffset" : 194
    }, {
      "referenceID" : 22,
      "context" : "Additionally, the translation-based language model was also used for QA matching by transferring the answer to the corresponding question (Jeon et al., 2005; Xue et al., 2008; Zhou et al., 2011).",
      "startOffset" : 138,
      "endOffset" : 194
    }, {
      "referenceID" : 6,
      "context" : "Huang et al. (2007) integrated textual features with structural features of forum threads to represent the candidate QA pairs, and used support vector machine (SVM) to classify the candidate pairs.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 6,
      "context" : "Huang et al. (2007) integrated textual features with structural features of forum threads to represent the candidate QA pairs, and used support vector machine (SVM) to classify the candidate pairs. Beyond typical features, Shah and Pomerantz (2010) trained a logistic regression (LR) classifier with user metadata to predict the quality of answers in CQA.",
      "startOffset" : 0,
      "endOffset" : 249
    }, {
      "referenceID" : 1,
      "context" : "Ding et al. (2008) proposed an approach based on conditional random fields (CRF), which can capture contextual features from the answer sequence for the semantic matching between question and answer.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 14,
      "context" : "In contrast to symbolic representation, Wang et al. (2010) proposed a deep belief nets (DBN) based semantic relevance model to learn the distributed representation of QA pair.",
      "startOffset" : 40,
      "endOffset" : 59
    }, {
      "referenceID" : 14,
      "context" : "In contrast to symbolic representation, Wang et al. (2010) proposed a deep belief nets (DBN) based semantic relevance model to learn the distributed representation of QA pair. Recently, the convolutional neural networks (CNNs) based sentence representation models have achieved successes in neural language processing (NLP) tasks. Yu et al. (2014) proposed a convolutional sentence model to identify answer contents of a question from Q&A archives via means of distributed representations.",
      "startOffset" : 40,
      "endOffset" : 348
    }, {
      "referenceID" : 5,
      "context" : "The work in Hu et al. (2014) demonstrated that 2-dimensional convolutional sentence models can represent the hierarchical structures of sentences and capture rich matching patterns between two language objects.",
      "startOffset" : 12,
      "endOffset" : 29
    }, {
      "referenceID" : 2,
      "context" : "The implementation of the LSTM unit in our study is close the one discussed by Graves (2013). Given the joint representation pt at time t, the memory cell ct is updated by the input gate’s activation it and the forget gate’s activation ft.",
      "startOffset" : 79,
      "endOffset" : 93
    }, {
      "referenceID" : 7,
      "context" : "SVM (Huang et al., 2007): An SVM-based method with bag-of-words (textual features), nontextual features, and features based on topic model (i.",
      "startOffset" : 4,
      "endOffset" : 24
    }, {
      "referenceID" : 1,
      "context" : "CRF (Ding et al., 2008): A CRF-based method using the same features as the SVM approach.",
      "startOffset" : 4,
      "endOffset" : 23
    }, {
      "referenceID" : 18,
      "context" : "DBN (Wang et al., 2010): Taking bag-of-words representation, the method applies deep belief nets to learning the distributed representation of QA pair, and predicts the class of answers using a logistic regression classifier on the top layer.",
      "startOffset" : 4,
      "endOffset" : 23
    }, {
      "referenceID" : 5,
      "context" : "mDBN (Hu et al., 2013): In contrast to DBN, multimodal DBN learns the joint representations of textual features and non-textual features rather than bag-of-words.",
      "startOffset" : 5,
      "endOffset" : 22
    }, {
      "referenceID" : 5,
      "context" : "CNN: Using word embedding, the CNNs based model in Hu et al. (2014) is used to learn the representations of questions and answers, and a logistic regression classifier is used to predict the class of answers.",
      "startOffset" : 51,
      "endOffset" : 68
    }, {
      "referenceID" : 3,
      "context" : "To prevent serious overfitting, early stopping and dropout (Hinton et al., 2012) are used during the training procedure.",
      "startOffset" : 59,
      "endOffset" : 80
    }, {
      "referenceID" : 21,
      "context" : "01 and is updated dynamically according to the gradient descent using the ADADELTA method (Zeiler, 2012).",
      "startOffset" : 90,
      "endOffset" : 104
    }, {
      "referenceID" : 0,
      "context" : "The activation functions (σ, γ) in our model adopt the rectified linear unit (ReLU) (Dahl et al., 2013).",
      "startOffset" : 84,
      "endOffset" : 103
    }, {
      "referenceID" : 11,
      "context" : "In addition, the word embeddings for encoding sentences are pre-trained with the unsupervised neural language model (Mikolov et al., 2013) on the Qatar Living data2.",
      "startOffset" : 116,
      "endOffset" : 138
    }, {
      "referenceID" : 10,
      "context" : "In fact, the Potential answers are most difficult to identify among the three types of answers as Potential is an intermediate category (Màrquez et al., 2015).",
      "startOffset" : 136,
      "endOffset" : 158
    } ],
    "year" : 2015,
    "abstractText" : "In this paper, the answer selection problem in community question answering (CQA) is regarded as an answer sequence labeling task, and a novel approach is proposed based on the recurrent architecture for this problem. Our approach applies convolution neural networks (CNNs) to learning the joint representation of questionanswer pair firstly, and then uses the joint representation as input of the long shortterm memory (LSTM) to learn the answer sequence of a question for labeling the matching quality of each answer. Experiments conducted on the SemEval 2015 CQA dataset shows the effectiveness of our approach.",
    "creator" : "LaTeX with hyperref package"
  }
}