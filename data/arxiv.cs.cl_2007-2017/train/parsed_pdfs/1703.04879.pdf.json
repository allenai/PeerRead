{
  "name" : "1703.04879.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Sparse Named Entity Classification using Factorization Machines",
    "authors" : [ "Ai Hirata" ],
    "emails" : [ "hirata-ai@ed.tmu.ac.jp", "komachi@tmu.ac.jp" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Named entity classification is the task of classifying text-based elements into various categories , including places, names, dates, times, and monetary values. A bottleneck in named entity classification, however, is the data problem of sparseness, because new named entities continually emerge, making it rather difficult to maintain a dictionary for named entity classification. Thus, in this paper, we address the problem of named entity classification using matrix factorization to overcome the problem of feature sparsity. Experimental results show that our proposed model, with fewer features and a smaller size, achieves competitive accuracy to state-of-the-art models."
    }, {
      "heading" : "1 Introduction",
      "text" : "To date, standard approaches to named entity classification rely on supervised models, that typically require a large-scale annotated corpus and a widecoverage dictionary. However, since new named entities arise regularly, it becomes increasingly difficult to maintain an up-to-date dictionary and/or adapt a named entity classifier to a new domain; for example, sequence labeling techniques that use feature templates (Finkel et al., 2005; Sarawagi and Cohen, 2004) are not robust for unknown named entities because their feature space is very sparse (Primadhanty et al., 2015). This problem worsens when we attempt to use a combination of features for sparse named entity classification.\nTherefore, in this paper, we propose the use of matrix factorization for named entity classification\nto consider the relationships between sparse features. Through our experiments, we achieved competitive accuracy to models developed in previous works in terms of using fewer features and compactness using factorization machines (Rendle, 2010). The main contributions of this paper are as follows:\n• We address the data sparseness problem in unknown named entity classification using factorization machines.\n• We demonstrate that factorization machines achieve state-of-the-art performance in sparse named entity classification task using a reduced feature set and a compact model."
    }, {
      "heading" : "2 Related Work",
      "text" : "A standard approach to named entity classification is to formulate a task as a sequence labeling problem and use a supervised method, such as conditional random fields (Lafferty et al., 2001; Finkel et al., 2005; Sarawagi and Cohen, 2004). These studies heavily rely on feature templates for learning combinations of features; however, since combinations of features in conventional supervised learning are treated independently, this approach is not robust for named entities that do not appear in the training data.\nTo address the task of unknown named entity classification, Primadhanty et al. (2015) explored the use of sparse combinatorial features. They proposed a log-bilinear model that defines a score function considering interactions between features; the score function is regularized via a nuclear norm on a feature weight matrix. Further, heir method employs\nar X\niv :1\n70 3.\n04 87\n9v 1\n[ cs\n.C L\n] 1\n5 M\nar 2\nsingular value decomposition (SVD)-based regularization to handle the combination of features. They reported that their regularization achieved higher accuracy than L1 and L2 regularization, frequently used in natural language processing (Okanohara and Tsujii, 2009).\nHowever, nuclear norm regularization (i.e., SVDbased regularization) is not necessarily the best way to incorporate interactions between features, because it does not directly optimize classification accuracy. Therefore, our proposed method treats sparse features using matrix factorization from a different perspective: we decompose a feature weight matrix using factorization machines as to directly optimize classification accuracy using a large margin method similar to support vector machines (SVMs) and passive-agressive algorithms (Vapnik, 1995; Crammer et al., 2006)."
    }, {
      "heading" : "3 Factorization Machines",
      "text" : "In this paper, we propose the use of factorization machines (Rendle, 2010) for unknown named entity classification. Using this approach, we can employ the same objective function as SVMs and yet performs matrix factorization to handle sparse combinatorial features. Matrix factorization yields better generalizations over a sparse feature matrix (Madhyastha et al., 2014).\nFactorization machines with interaction degree d = 2 use the following equation for prediction:\nŷ(x) := w0+ n∑ i=1 wixi+ n∑ i=1 n∑ j=i+1 〈vi, vj〉xixj (1)\nHere, x is an instance, xi represents the i-th dimension of the feature x, n is the number of features, w ∈ Rn is a weight vector, and w0 ∈ R is a bias term. Factorization machines incorporate interactions between variables vi, vj as the third term of Equation (1). Here, 〈·, ·〉 is the inner product of two vectors of size k, i.e.,\n〈vi, vj〉 := k∑\nf=1\nvi,f · vj,f (2)\nwhere vi is the i-th element of matrix V ∈ Rn×k and k is a hyperparameter representing the dimension of matrix decomposition. To consider interactions between features, we only need to calculate the inner\nproduct of a decomposed matrix n×k times. Therefore, we do not incur high computational costs even though the number of interacting features is large.\nNote that even though the polynomial kernel of SVMs take combinations of features into account, it treats them independently. Conversely, factorization machines take advantage of interactions between features using a low-dimensional feature matrix via matrix factorization. Because factorization machines can learn combinations of infrequent features thanks to matrix factorization, we expect that factorization machines will correctly classify named entities that seldom appear in the training corpus.\nIn a binary classification task, factorization machines use hinge loss to optimize parameters. Here, parameter learning can be accomplished via Markov chain Monte Carlo or stochastic gradient descent."
    }, {
      "heading" : "4 Experiments",
      "text" : "As described above, we aim to classify named entities that rarely appear in a given training corpus. We compared factorization machines with a log-linear model, a polynomial-kernel SVM, and a state-ofthe-art log-bilinear model using nuclear norm for regularization (Primadhanty et al., 2015)."
    }, {
      "heading" : "4.1 Settings",
      "text" : "Data. We used the dataset provided by Primadhanty et al. (2015); this dataset was created for evaluating unknown named entity classification and is\nbased on the CoNLL-2003 English dataset, which omits named entity candidates that appear in the training data from the development and test data.\nTable 1 shows the number of tokens and types in the given dataset. This dataset contains five tags: person (PER), location (LOC), organization (ORG), miscellaneous (MISC), and non-entities (O).\nFeatures. We used a subset of features from experiments performed by Primadhanty et al. (2015). Table 3 summarizes the features used in our experiment, including context and entity features.\nTools. In terms of tools, we used scikit-learn 0.17 to implement a log-linear model and polynomial kernel in an SVM. Further, we employed libFM 1.4.21 (Rendle, 2012) to build a named entity classifier using factorization machines.\nIn the interaction of both the SVM and the factorization machine, we fixed the degree of the polynomial kernel to d = 2. We also tuned other parameters such as learning methods, learning rate and regularization methods based on development data. Further, we used a one-versus-all strategy to build a multiclass classifier.\nEvaluation metrics. For our evaluation, we used precision, recall, and F1-score. The scores were calculated on all tags except for non-entities (O)."
    }, {
      "heading" : "4.2 Results",
      "text" : "Table 2 presents results of our experiments. Note that Primadhanty et al. (2015) used additional features such as Brown clustering and parts-of-speech (POS) features, which we did not use. Table 4 and Figure 2 show the performance and precision-recall curves of named entity classification for each tag, respectively.\n1http://www.libfm.org/\nWe observed here that, aside from LOC, we obtained competitive results to the state-of-the-art named entity classifier proposed by Primadhanty et al. (2015) with fewer features. Overall, the microaveraged F1 score improved by 1.4 points.\nFrom these results, we conclude that unknown named entity classification can be successfully achieved by taking combinatorial features into account using factorization machines."
    }, {
      "heading" : "5 Discussion",
      "text" : "Experimental results show that performance on ORG was improved. For example, the term “VicePresident” appears in both contexts of ORG and O, and our method correctly handled this sparse combination of context and entity features.\nThe accuracy of LOC, however, was lower than that of the log-bilinear model (Primadhanty et al., 2015). Upon investigating the confusion matrix, we found that the LOC tag was often misclassified as PER. We therefore conclude here that clustering and POS features are necessary to distinguish these tags.\nFigure 1 plots the F1-score of our proposed method as dimension k changes for matrix factor-\nization using the same development data as that of Primadhanty et al. (2015). Our method yielded the best F1-score (i.e., 57.1) at k = 5, whereas the log-bilinear model achieved the best F1-score (i.e., 61.73) at k = 40. These results show that factorization machines require a compact model to achieve state-of-the-art results on the test set of this corpus.\nIt would be interesting to point out that the performance of our factorization machines approach on the development dataset was lower than that of the log-bilinear model by 4.6 points. This phenomenon may occur because the log-bilinear model overfits to sparse combinatorial features even with nuclear norm regularization; further factorization machines typically have better generalization abilities than those of nuclear norm regularization.\nBoth our approach and the methods of Primadhanty et al. (2015) address the problem of incor-\nporating sparse combinatorial features by dimension reduction (i.e., matrix factorization); however, they differ in terms of the objective function to be optimized. Primadhanty et al. (2015) use maximum likelihood estimation as an objective function; whereas other objective functions such as hinge loss can be used in factorization machines."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this paper, we proposed the use of factorization machines to handle the combinations of sparse features in unknown named entity classification. Our experimental results showed that we were able to achieve competitive accuracy to state-of-the-art methods using fewer features and a compact model. For future work, we aim to extend this framework to sequence labeling, thereby improving overall named entity recognition."
    } ],
    "references" : [ {
      "title" : "Online Passive-Agressive Algorithms",
      "author" : [ "Crammer et al.2006] Koby Crammer", "Ofer Dekel", "Joseph Keshet", "Shai Shalev-Shwartz", "Yoram Singer" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Crammer et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Crammer et al\\.",
      "year" : 2006
    }, {
      "title" : "Incorporating Nonlocal Information into Information Extraction Systems by Gibbs Sampling",
      "author" : [ "Trond Grenager", "Christopher Manning" ],
      "venue" : "In Proceedings of ACL,",
      "citeRegEx" : "Finkel et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Finkel et al\\.",
      "year" : 2005
    }, {
      "title" : "Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data",
      "author" : [ "Andrew McCallum", "Fernando C.N. Pereira" ],
      "venue" : "In Proceedings of ICML,",
      "citeRegEx" : "Lafferty et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Lafferty et al\\.",
      "year" : 2001
    }, {
      "title" : "Learning Task-specific Bilexical Embeddings",
      "author" : [ "Xavier Carreras", "Ariadna Quattoni" ],
      "venue" : "In Proceedings of COLING,",
      "citeRegEx" : "Madhyastha et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Madhyastha et al\\.",
      "year" : 2014
    }, {
      "title" : "Learning Combination Features with L1 Regularization",
      "author" : [ "Okanohara", "Tsujii2009] Daisuke Okanohara", "Jun’ichi Tsujii" ],
      "venue" : "In Proceedings of NAACL,",
      "citeRegEx" : "Okanohara et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Okanohara et al\\.",
      "year" : 2009
    }, {
      "title" : "Low-Rank Regularization for Sparse Conjunctive Feature Spaces: An Application to Named Entity Classification",
      "author" : [ "Xavier Carreras", "Ariadna Quattoni" ],
      "venue" : "In Proceedings of ACL-IJCNLP,",
      "citeRegEx" : "Primadhanty et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Primadhanty et al\\.",
      "year" : 2015
    }, {
      "title" : "Factorization Machines",
      "author" : [ "Steffen Rendle" ],
      "venue" : "In Proceedings of ICDM,",
      "citeRegEx" : "Rendle.,? \\Q2010\\E",
      "shortCiteRegEx" : "Rendle.",
      "year" : 2010
    }, {
      "title" : "Factorization Machines with libFM",
      "author" : [ "Steffen Rendle" ],
      "venue" : "ACM Transactions on Intelligent Systems and Technology,",
      "citeRegEx" : "Rendle.,? \\Q2012\\E",
      "shortCiteRegEx" : "Rendle.",
      "year" : 2012
    }, {
      "title" : "Semi-Markov Conditional Random Fields for Information Extraction",
      "author" : [ "Sarawagi", "Cohen2004] Sunita Sarawagi", "William W. Cohen" ],
      "venue" : "In Proceedings of NIPS,",
      "citeRegEx" : "Sarawagi et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Sarawagi et al\\.",
      "year" : 2004
    }, {
      "title" : "The Nature of Statistical Learning",
      "author" : [ "Vladimir N. Vapnik" ],
      "venue" : null,
      "citeRegEx" : "Vapnik.,? \\Q1995\\E",
      "shortCiteRegEx" : "Vapnik.",
      "year" : 1995
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "However, since new named entities arise regularly, it becomes increasingly difficult to maintain an up-to-date dictionary and/or adapt a named entity classifier to a new domain; for example, sequence labeling techniques that use feature templates (Finkel et al., 2005; Sarawagi and Cohen, 2004) are not robust for unknown named entities because their feature space is very sparse (Primadhanty et al.",
      "startOffset" : 247,
      "endOffset" : 294
    }, {
      "referenceID" : 5,
      "context" : ", 2005; Sarawagi and Cohen, 2004) are not robust for unknown named entities because their feature space is very sparse (Primadhanty et al., 2015).",
      "startOffset" : 119,
      "endOffset" : 145
    }, {
      "referenceID" : 6,
      "context" : "Through our experiments, we achieved competitive accuracy to models developed in previous works in terms of using fewer features and compactness using factorization machines (Rendle, 2010).",
      "startOffset" : 174,
      "endOffset" : 188
    }, {
      "referenceID" : 2,
      "context" : "A standard approach to named entity classification is to formulate a task as a sequence labeling problem and use a supervised method, such as conditional random fields (Lafferty et al., 2001; Finkel et al., 2005; Sarawagi and Cohen, 2004).",
      "startOffset" : 168,
      "endOffset" : 238
    }, {
      "referenceID" : 1,
      "context" : "A standard approach to named entity classification is to formulate a task as a sequence labeling problem and use a supervised method, such as conditional random fields (Lafferty et al., 2001; Finkel et al., 2005; Sarawagi and Cohen, 2004).",
      "startOffset" : 168,
      "endOffset" : 238
    }, {
      "referenceID" : 1,
      "context" : ", 2001; Finkel et al., 2005; Sarawagi and Cohen, 2004). These studies heavily rely on feature templates for learning combinations of features; however, since combinations of features in conventional supervised learning are treated independently, this approach is not robust for named entities that do not appear in the training data. To address the task of unknown named entity classification, Primadhanty et al. (2015) explored the use of sparse combinatorial features.",
      "startOffset" : 8,
      "endOffset" : 420
    }, {
      "referenceID" : 9,
      "context" : "Therefore, our proposed method treats sparse features using matrix factorization from a different perspective: we decompose a feature weight matrix using factorization machines as to directly optimize classification accuracy using a large margin method similar to support vector machines (SVMs) and passive-agressive algorithms (Vapnik, 1995; Crammer et al., 2006).",
      "startOffset" : 328,
      "endOffset" : 364
    }, {
      "referenceID" : 0,
      "context" : "Therefore, our proposed method treats sparse features using matrix factorization from a different perspective: we decompose a feature weight matrix using factorization machines as to directly optimize classification accuracy using a large margin method similar to support vector machines (SVMs) and passive-agressive algorithms (Vapnik, 1995; Crammer et al., 2006).",
      "startOffset" : 328,
      "endOffset" : 364
    }, {
      "referenceID" : 6,
      "context" : "In this paper, we propose the use of factorization machines (Rendle, 2010) for unknown named entity classification.",
      "startOffset" : 60,
      "endOffset" : 74
    }, {
      "referenceID" : 3,
      "context" : "Matrix factorization yields better generalizations over a sparse feature matrix (Madhyastha et al., 2014).",
      "startOffset" : 80,
      "endOffset" : 105
    }, {
      "referenceID" : 5,
      "context" : "obtained from Primadhanty et al. (2015), with the number of",
      "startOffset" : 14,
      "endOffset" : 40
    }, {
      "referenceID" : 5,
      "context" : "94 log-bilinear model (Primadhanty et al., 2015) 62.",
      "startOffset" : 22,
      "endOffset" : 48
    }, {
      "referenceID" : 5,
      "context" : "We compared factorization machines with a log-linear model, a polynomial-kernel SVM, and a state-ofthe-art log-bilinear model using nuclear norm for regularization (Primadhanty et al., 2015).",
      "startOffset" : 164,
      "endOffset" : 190
    }, {
      "referenceID" : 5,
      "context" : "We used the dataset provided by Primadhanty et al. (2015); this dataset was created for evaluating unknown named entity classification and is",
      "startOffset" : 32,
      "endOffset" : 58
    }, {
      "referenceID" : 5,
      "context" : "Table 3: Features used in our experiment; note that this is a subset of features used in Primadhanty et al. (2015)’s experiment.",
      "startOffset" : 89,
      "endOffset" : 115
    }, {
      "referenceID" : 5,
      "context" : "We used a subset of features from experiments performed by Primadhanty et al. (2015). Table 3 summarizes the features used in our experiment, including context and entity features.",
      "startOffset" : 59,
      "endOffset" : 85
    }, {
      "referenceID" : 7,
      "context" : "21 (Rendle, 2012) to build a named entity classifier using factorization machines.",
      "startOffset" : 3,
      "endOffset" : 17
    }, {
      "referenceID" : 5,
      "context" : "Note that Primadhanty et al. (2015) used additional features such as Brown clustering and parts-of-speech (POS) features, which we did not use.",
      "startOffset" : 10,
      "endOffset" : 36
    }, {
      "referenceID" : 5,
      "context" : "We observed here that, aside from LOC, we obtained competitive results to the state-of-the-art named entity classifier proposed by Primadhanty et al. (2015) with fewer features.",
      "startOffset" : 131,
      "endOffset" : 157
    }, {
      "referenceID" : 5,
      "context" : "The accuracy of LOC, however, was lower than that of the log-bilinear model (Primadhanty et al., 2015).",
      "startOffset" : 76,
      "endOffset" : 102
    }, {
      "referenceID" : 5,
      "context" : "50 log-bilinear model (Primadhanty et al., 2015) 73.",
      "startOffset" : 22,
      "endOffset" : 48
    }, {
      "referenceID" : 5,
      "context" : "ization using the same development data as that of Primadhanty et al. (2015). Our method yielded the best F1-score (i.",
      "startOffset" : 51,
      "endOffset" : 77
    }, {
      "referenceID" : 5,
      "context" : "Both our approach and the methods of Primadhanty et al. (2015) address the problem of incorporating sparse combinatorial features by dimension reduction (i.",
      "startOffset" : 37,
      "endOffset" : 63
    }, {
      "referenceID" : 5,
      "context" : "Both our approach and the methods of Primadhanty et al. (2015) address the problem of incorporating sparse combinatorial features by dimension reduction (i.e., matrix factorization); however, they differ in terms of the objective function to be optimized. Primadhanty et al. (2015) use maximum likelihood estimation as an objective function; whereas other objective functions such as hinge loss can be used in factorization machines.",
      "startOffset" : 37,
      "endOffset" : 282
    } ],
    "year" : 2017,
    "abstractText" : "Named entity classification is the task of classifying text-based elements into various categories , including places, names, dates, times, and monetary values. A bottleneck in named entity classification, however, is the data problem of sparseness, because new named entities continually emerge, making it rather difficult to maintain a dictionary for named entity classification. Thus, in this paper, we address the problem of named entity classification using matrix factorization to overcome the problem of feature sparsity. Experimental results show that our proposed model, with fewer features and a smaller size, achieves competitive accuracy to state-of-the-art models.",
    "creator" : "LaTeX with hyperref package"
  }
}