{
  "name" : "1405.0792.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "On Exact Learning Monotone DNF from Membership Queries",
    "authors" : [ "Hasan Abasi", "Nader H. Bshouty" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n40 5.\n07 92\nv1 [\ncs .L\nG ]\nWe first present new lower bounds for this problem and then present deterministic and randomized adaptive algorithms with query complexities that are almost optimal. All the algorithms we present in this paper run in time linear in the query complexity and the number of variables n. In addition, all of the algorithms we present in this paper are asymptotically tight for fixed r and/or s."
    }, {
      "heading" : "1 Introduction",
      "text" : "We consider the problem of learning a monotone DNF with at most s terms, where each monotone term contains at most r variables (s term r-MDNF) from membership queries [1]. This is equivalent to the problem of learning a general hypergraph using hyperedge-detecting queries, a problem that is motivated by applications arising in chemical reaction and genome sequencing."
    }, {
      "heading" : "1.1 Learning Hypergraph",
      "text" : "A hypergraph is H = (V,E) where V is the set of vertices and E ⊆ 2V is the set of edges. The dimension of the hypergraph H is the cardinality of\nthe largest set in E. For a set S ⊆ V , the edge-detecting queries QH(S) is answered “Yes” or “No”, indicating whether S contains all the vertices of at least one edge of H. Our learning problem is equivalent to learning a hidden hypergraph of dimension r using edge-detecting queries.\nThis problem has many applications in chemical reactions and genome sequencing. In chemical reactions, we are given a set of chemicals, some of which react and some which do not. When multiple chemicals are combined in one test tube, a reaction is detectable if and only if at least one set of the chemicals in the tube reacts. The goal is to identify which sets react using as few experiments as possible. The time needed to compute which experiments to do is a secondary consideration, though it is polynomial for the algorithms we present [5]. See [13, 7, 3, 2, 4, 5] for more details and other applications."
    }, {
      "heading" : "1.2 Previous Results",
      "text" : "In [5], Angluin and Chen presented an deterministic optimal adaptive learning algorithm for learning s-term 2-MDNF. They also gave a lower bound of Ω((2s/r)r/2 + rs logn) for learning the class of s-term r-MDNF when r < s. In [4], Angluin and Chen gave a randomized algorithm for s-term r-uniform MDNF (the size of each term is exactly r) that asks O(24rs · poly(r, log n)) membership queries. For s-term r-MDNF where r ≤ s, they gave a randomized learning algorithm that asks O(2r+r\n2/2s1+r/2 · poly(log n)) membership queries.\nLiterature has also addressed learning some subclasses of s-term 2-MDNF. Those classes have specific applications to genome sequencing. See [13, 7, 3, 2, 4, 5]. In this paper we are interested in learning the class of all s-term r-MDNF formulas for any r and s."
    }, {
      "heading" : "1.3 Our Results",
      "text" : "In this paper, we distinguish between two cases: s ≥ r and s < r. For s < r, we first prove the lower bound O((r/s)s−1 + rs log n). We then give three algorithms. Algorithm I is a deterministic algorithm that asks O(rs−1 + rs log n) membership queries. Algorithm II is a deterministic algorithm that asks O(s · N((s − 1; r); sr) + rs log n) membership queries where N((s − 1; r); sr) is the size of (sr, (s − 1, r))-cover free family (see Subsection 2.2 for the definition of cover free) that can be constructed in time linear in its size. An (sr, (s−1, r))-cover free family of size (r/s)s−1+o(1) is known to exist. For some r and s (for example r = o(s log s log log s)),\nsuch a bound can be achieved in linear time and therefore for those cases, algorithm II is almost optimal. Algorithm III is a randomized algorithm that asks\nO\n((\ns+ r\ns\n)√ sr log(sr) + rs log n ) = O ( (r\ns\n)s−1+o(1) + rs log n\n)\nmembership queries. This algorithm is almost optimal. For the case s ≥ r, Angluin and Chen, [5], gave the lower bound Ω((2s/r)r/2 + rs log n). We give two algorithms that are almost tight. The first algorithm, Algorithm IV, is a deterministic algorithm that asks (crs)r/2+1.5 + rs logn membership queries for some constant c. The second algorithm, Algorithm V, is a randomized algorithm that asks (c′s)r/2+0.75+ rs log n membership queries for some constant c′.\nAll the algorithms we present in this paper run in time linear in the query complexity and n. Additionally, all the algorithms we describe in this paper are asymptotically tight for fixed r and s.\nThe following table summarizes our results. We have removed the term rs log n from all the bounds to be able to fit this table in this page. Det. and Rand. stands for deterministic algorithm and randomized algorithm, respectively.\nLower Bound Rand./ Upper Bound r, s rs log n+ Algorithm Det. rs log n+ r > s (\nr s\n)s−1 Alg. I Det. rs−1\nAlg. II Det. s ·N((s − 1; r); sr) Alg. III Rand. (log r) √ ses (\nr s + 1\n)s\nr ≤ s ( 2s r )r/2 Alg. IV. Det. (3e)r(rs)r/2+1.5\nAlg. IV. Rand. √ r(3e)r(log s)sr/2+1"
    }, {
      "heading" : "2 Definitions and Notations",
      "text" : "For a vector w, we denote by wi the ith entry of w. For a positive integer j, we denote by [j] the set {1, 2, . . . , j}.\nLet f(x1, x2, . . . , xn) be a Boolean function from {0, 1}n to {0, 1}. For an assignment a ∈ {0, 1}n we say that f is ξ in a (or a is ξ in f) if f(a) = ξ. We say that a is zero in xi if ai = 0. For a set of variables S, we say that a is zero in S if for every xi ∈ S, a is zero in xi. Denote Xn = {x1, . . . , xn}.\nFor a Boolean function f(x1, . . . , xn), 1 ≤ i1 < i2 < · · · < ik ≤ n and σ1, . . . , σk ∈ {0, 1} we denote by\nf |xi1=σ1,xi2=σ2,··· ,xik=σk\nthe function f when fixing the variables xij to σj for all j ∈ [k]. We denote by a|xi1=σ1,xi2=σ2,··· ,xik=σk the assignment a where each aij is replaced by σj for all j ∈ [k]. Note that\nf |xi1=σ1,xi2=σ2,··· ,xik=σk(a) = f(a|xi1=σ1,xi2=σ2,··· ,xik=σk).\nWhen σ1 = · · · = σk = ξ and S = {xi1 , . . . , xik}, we denote\nf |xi1=σ1,xi2=σ2,··· ,xik=σk\nby f |S←ξ. In the same way, we define a|S←ξ. We denote by 1n = (1, 1, . . . , 1) ∈ {0, 1}n.\nFor two assignments a, b ∈ {0, 1}n, we write a ≤ b if for every i, ai ≤ bi. A Boolean function f : {0, 1}n → {0, 1} is monotone if for every two assignments a, b ∈ {0, 1}n, if a ≤ b then f(a) ≤ f(b). Recall that every monotone Boolean function f has a unique representation as a reduced monotone DNF. That is, f = M1 ∨M2 ∨ · · · ∨Ms where each monomial Mi is an ANDs of input variables, and for every monomial Mi there is a unique assignment a(i) such that f(a(i)) = 1 and for every j ∈ [n] where a(i)j = 1 we have f(a(i)|xj=0) = 0. We call such assignment a minterm of the function f . Notice that every monotone DNF can be uniquely determined by its minterms.\nFor a monotone DNF, f(x1, x2, . . . , xn) = M1 ∨ M2 ∨ · · · ∨ Ms, and a variable xi, we say that xi is t-frequent if it appears in more than or equal to t terms. A monotone DNF f is called read k monotone DNF, if none of its variables is k + 1-frequent."
    }, {
      "heading" : "2.1 Learning Model",
      "text" : "Consider a teacher (or a black box) that has a target function f : {0, 1}n → {0, 1} that is s-term r-MDNF. The teacher can answer membership queries. That is, when receiving a ∈ {0, 1}n it returns f(a). A learning algorithm is an algorithm that can ask the teacher membership queries. The goal of the learning algorithm is to exactly learn (exactly find) f with minimum number of membership queries and optimal time complexity.\nIn our algorithms, for a function f we will denote byMQf the oracle that answers the membership queries. That is, for a ∈ {0, 1}n, MQf (a) = f(a)."
    }, {
      "heading" : "2.2 Cover-Free Families",
      "text" : "The problem (n, (s, r))-cover-free family [12] is equivalent to the following problem: A (n, (s, r))-cover-free family is a set A ⊆ {0, 1}n such that for\nevery 1 ≤ i1 < i2 < · · · < id ≤ n where d = s + r and every J ⊆ [d] of size |J | = s there is a ∈ A such that aik = 0 for all k ∈ J and aij = 1 for all j 6∈ J . Denote by N((s; r);n) the minimum size of such set. The lower bounds in [16] are\nN((s; r);n) ≥ Ω ( (s+ r)\nlog (s+r\ns\n)\n(\ns+ r\ns\n)\nlog n\n)\n.\nIt is known that a set of random\nm = O\n(\n√\nmin(r, s)\n(\ns+ r\ns\n)(\n(s + r) log n+ log 1\nδ\n))\n(1)\nvectors a(i) ∈ {0, 1}n, where each a(i)j is 1 with probability r/(s + r), is a (n, (s, r))-cover free family with probability at least 1− δ.\nIn [8], Bshouty gave a deterministic construction of (n, (s, r))-CFF of size\nC := min((2e)srs+3, (2e)rsr+3) log n\n=\n(\ns+ r\nr\n)\n2min(s log s,r log r)(1+o(1)) log n (2)\nthat can be constructed in time C ·n. Fomin et. al. in [11] gave a construction of size\nD :=\n(\ns+ r\nr\n)\n2 O ( r+s log log(r+s) ) log n (3)\nthat can be constructed in time D ·n. The former bound, (2), is better than the latter when s ≥ r log r log log r or r ≥ s log s log log s. We also note that the former bound, (2), is almost optimal, i.e.,\n(\ns+ r\nr\n)1+o(1)\nlog n,\nwhen r = sω(1) or r = so(1) and the latter bound, (3), is almost optimal when\no(s log log s log log log s) = r = ω\n(\ns\nlog log s log log log s\n)\n."
    }, {
      "heading" : "3 Lower Bounds",
      "text" : "In this section, we prove some lower bounds."
    }, {
      "heading" : "3.1 General Lower Bound",
      "text" : "In this section, we prove that the information theoretic lower bound for learning a class C from membership queries is also a lower bound for any randomized learning algorithm. We believe it is a folklore result, but we could not find the proof in the literature. We first state the following informationtheoretic lower bound for deterministic learning algorithm,\nLemma 1. Let C be any class of Boolean function. Then any deterministic learning algorithm for C must ask at least log |C| membership queries.\nWe now prove,\nLemma 2. Let C be any class of boolean function. Then any Monte Carlo (and therefore, Las Vegas) randomized learning algorithm that learns C with probability at least 3/4 must ask at least log |C| − 1 membership queries.\nProof. Let A be a randomized algorithm that for every f ∈ C and an oracle MQf that answers membership queries for f , asks m membership queries and satisfies\nPrs[A(MQf , s) = f ] ≥ 3\n4\nwhere s ∈ {0, 1}N is chosen randomly uniformly for some large N . Consider the random variable Xf (s) that is 1 if A(MQf , s) = f and 0, otherwise. Then for every f , Es[Xf ] ≥ 3/4. Therefore, for random uniform f ∈ C\n3/4 ≤ Ef [Es[Xf ]] = Es[Ef [Xf (s)]].\nand by Markov Bound for at least 1/2 of the elements s ∈ {0, 1}N we have Ef [Xf (s)] ≥ 1/2. Let S ⊆ {0, 1}N be the set of such elements. Then |S| ≥ 2N/2. Let s0 ∈ S and Cs0 ⊆ C the class of functions f where Xf (s0) = 1. Then |Cs0 | ≥ |C|/2 and A(MQf , s0) is a deterministic algorithm that learns the class Cs0 . Using the information theoretic lower bound for deterministic algorithm, we conclude that A(MQf , s0) must ask at least\nm ≥ log |Cs0 | = log(1/2) + log |C|\nmembership queries.\nSpecifically, we have,\nCorollary 3. Any Monte Carlo (and therefore Las Vegas) randomized learning algorithm for the class of s-term r-MDNF must ask on average at least rs log n membership queries."
    }, {
      "heading" : "3.2 Two Lower Bounds",
      "text" : "In this section, we give two lower bounds. The first is from [4] and the second follows using the same techniques used in [9].\nIn [4], Angluin and Chen proved,\nTheorem 4. Let r and s be integers. Let k and ℓ be two integers such that\nℓ ≤ r, s ≥ ( k\n2\n)\nℓ+ 1.\nAny (Monte Carlo) randomized learning algorithm for the class of s-term r-MDNF must ask at least kℓ − 1 membership queries.\nSpecifically, when s >> r we have the lower bound\nΩ\n(\n(\n2s\nr\n)r/2 )\nmembership queries. Also, for any integer λ where\n(\nλ\n2\n) r + 1 ≤ s < ( λ+ 1\n2\n)\nr\nwe have the lower bound λr − 1.\nWe now prove the following lower bound,\nTheorem 5. Let r and s be integers and ℓ and t be two integers such that\nℓ− ⌊ ℓ\nt\n⌋ ≤ r, ⌊ ℓ\nt\n⌋\n≤ s− 1.\nAny (Monte Carlo) randomized learning algorithm for the class of s-term r-MDNF must ask at least t⌊ℓ/t⌋ membership queries.\nSpecifically, for r >> s we have the lower bound\n(r\ns\n)s−1 .\nand for any constant integer λ and λs ≤ r < (λ + 1)s we have the lower bound\n(λ+ 1)s−1.\nProof. Let m = ⌊ℓ/t⌋. Consider the monotone terms Mj = x(j−1)t+1 · · · xjt for j = 1, 2, . . . ,m. Define Mi,k where i = 1, . . . ,m and k = 1, . . . , t the monotone term Mi without the variable x(i−1)t+k . Let Mk1,k2,...,km = M1,k1M2,k2 · · ·Mm,km . The only way we can distinguish between the two hypothesis f = M1∨M2∨ · · · ∨Mm and g = M1∨M2∨ · · · ∨Mm∨Mk1,k2,...,km is by guessing an assignment that is 1 in all its first mt entries except for the entire k1, t+k2, 2t+k3, . . . , (m−1)t+km. That is, by guessing k1, k2, . . . , km. This takes an average of tm guesses. Since both f and g are s-term r-MDNF, the result follows.\nFor r >> s, we choose ℓ = r and t such that ⌊ℓ/t⌋ = s − 1. Since s− 1 = ⌊ℓ/t⌋ ≥ ℓ/t− 1, we have t ≥ r/s and the result follows.\nFor λs ≤ r < (λ+ 1)s, proving the lower bound for r = λs is sufficient. Take t = λ+ 1 and ℓ = (λ+ 1)s − 1."
    }, {
      "heading" : "4 Optimal Algorithms for Monotone DNF",
      "text" : "In this section, we present the algorithms (Algorithm I-V) that learn the class of s-term r-MDNF. We first give a simple algorithm that learns one term. We then give three algorithms (Algorithm I-III) for the case r > s and two algorithms (Algorithm IV-V) for the case s ≥ r."
    }, {
      "heading" : "4.1 Learning One Monotone Term",
      "text" : "In this section, we prove the following result.\nLemma 6. Let f(x) = M1∨M2∨· · ·∨Ms be the target function where each Mi is a monotone term of size at most r. Suppose g(x) = M1∨M2∨· · ·∨Ms′ and h(x) = Ms′+1∨Ms′+2∨· · ·∨Ms. If a is an assignment such that g(a) = 0 and h(a) = 1, then a monotone term in h(x) can be found with\nO ( r log n\nr\n)\nmembership queries.\nProof. First notice that since g is monotone, for any b ≤ a we have g(b) = 0. Our algorithm finds a minterm b ≤ a of f and therefore b is a minterm of h.\nFirst, if the number of ones in a is 2r, then we can find a minterm by flipping each bit in a that does change the value of f and get a minterm. This takes at most 2r membership queries.\nIf the number of ones in a is w > 2r, then we divide the entries of a that are equal to 1 into 2r disjoint sets S1, S2, . . . , S2r where for every i, the size\nof Si is either ⌊w/(2r)⌋ or ⌈w/(2r)⌉. Now for i = 1, 2, . . . , 2r, we flip all the entries of Si in a to zero and ask a membership query. If the function is one, we keep those entries 0. Otherwise we set them back to 1 and proceed to i+1. At the end of this procedure, at most r sets are not flipped. Therefore, at least half of the bits in a are flipped to zero using 2r membership queries. Therefore, the number of membership queries we need to get a minterm is 2r log(n/2r) + 2r.\nWe will call the above procedure Find-Term."
    }, {
      "heading" : "4.2 The case r > s",
      "text" : "In this section, we present three algorithms, two deterministic and one randomized. We start with the deterministic algorithm."
    }, {
      "heading" : "4.2.1 Deterministic Algorithm",
      "text" : "Consider the class s-term r-MDNF. Let f be the target function. Given s− ℓ monotone terms M1 ∨M2 ∨ · · · ∨Ms−ℓ that are known to the learning algorithm to be in f . The learning algorithm goal is to find a new monotone term. In order to find a new term we need to find an assignment a that is zero in M1∨M2∨· · ·∨Ms−ℓ and 1 in the function f . Then by the procedure Find-Term in Subsection 4.1, we get a new term in O(r log n) additional membership queries.\nTo find such an assignment, we present three algorithms: Algorithm I: (Exhaustive Search) choose a variable from each Mi and set it to zero and set all the other variables to 1. The set of all such assignments is denoted by A. If f is 1 in some a ∈ A, then find a new term using Find-Term.\nWe now show,\nLemma 7. If f 6≡ h, then Algorithm I finds a new term in rs−ℓ+O(r log n) membership queries.\nProof. Since the number of variables in each term in h := M1∨M2∨· · ·∨Ms−ℓ is at most r the number of assignments in A is at most rs−ℓ. Since we choose one variable from each term in h and set it to zero, all the assignments in A are zero in h. We now show that one of the assignments in A must be 1 in f , and therefore a new term can be found.\nLet b be an assignment that is 1 in f and zero in h. Such assignment exists because otherwise f ⇒ h and since h ⇒ f we get f ≡ h. Since\nh(b) = 0 there is at least one variable xji in each Mi that is zero in b. Then the assignment a := 1n|xj1=0,...,xjs−ℓ=0 is in A and h(a) = 0. Since a ≥ b we also have f(a) = 1.\nThe number of queries in this algorithm is s ∑\nℓ=1\nO ( rs−ℓ + r log n ) = O(rs−1 + rs log n).\nWe now present the second algorithm. Recall that Xn = {x1, . . . , xn}.\nWe now show,\nLemma 8. If f 6≡ h, then Algorithm II finds a new term in N((s − ℓ, r), (s − ℓ)r) +O(r log n) membership queries. Proof. Let h := M1 ∨M2 ∨ · · · ∨Ms−ℓ. Let b be an assignment that is 1 in f and zero in h. Since h(b) = 0, there is at least one variable xji in each Mi that is zero in b. Consider the set U = {xji |i = 1, . . . , s− ℓ}. Since f(b) = 1 there is a new term M in f that is one in b. That is, all of its variables are one in b. Let W be the set of all variables in M . Since A is (|V |, (s− ℓ, r))CFF and since |U ∪ (W ∩ V )| ≤ s− ℓ+ r there is an assignment a ∈ A that is 0 in each variable in U and is one in each variable in W ∩ V . Since a′ is also 0, in each variable in U we have h(a′) = 0. Since a′ is one in each variable in W ∩ V and one in each variable W\\V , we have M(a′) = 1 and therefore f(a′) = 1. This completes the proof.\nThe number of queries in Algorithm II is\ns−1 ∑\nℓ=1\nN((s− ℓ, r), (s − ℓ)r) + r log n = O(sN((s− 1, r), sr) + rs log n)."
    }, {
      "heading" : "4.2.2 Randomized Algorithm",
      "text" : "Our third algorithm, Algorithm III, is a randomized algorithm. It is basically Algorithm II where an (rs, (s− 1, r))-CFF A is randomly constructed, as in (1). Notice that an (rs, (s − 1, r))-CFF is also an (|V |, (s − ℓ, r))-CFF, so it can be used in every round of the algorithm. The algorithm fails if there is a new term that has not been found and this happens if and only if A is not (rs, (s − 1, r))-CFF. So the failure probability is δ. By (1), this gives a Monte Carlo randomized algorithm with query complexity\nO (√ s ( s+ r\ns\n)(\nr log r + log 1\nδ\n)\n+ rs logn\n)\n."
    }, {
      "heading" : "4.3 The case r < s",
      "text" : "In this section, we present two algorithms. Algorithm IV is deterministic and Algorithm V is randomized. We start with the deterministic algorithm."
    }, {
      "heading" : "4.3.1 Deterministic Algorithm",
      "text" : "In this section, we present Algorithm IV, used when r < s. For this case, we prove the following,\nTheorem 9. There is a deterministic learning algorithm for the class of s-term r-MDNF that asks\nO ( (3e)r(rs)r/2+1.5 + rs log n ) ,\nmembership queries.\nBefore proving this theorem, we first prove learnability in simpler settings. We prove the following,\nLemma 10. Let f(x1, x2, . . . , xn) = M1 ∨ · · · ∨ Ms be the target s-term r-MDNF. Suppose the learning algorithm knows some of the terms, h = M1∨M2∨ · · · ∨Ms−ℓ and knows that Ms−ℓ+1 is of size r′. Suppose that h is a read k monotone DNF. Then, there exists an algorithm that finds a new term (not necessarily Ms−ℓ+1) using\nO ( N((r′k; r′); sr)) + r log n ) ,\nmembership queries.\nProof. Consider the algorithm in Figure 2. Let V be the set of variables that appear in h. Let M := Ms−ℓ+1. Let U be the set of variables in M and W = U ∩ V . Each variable in W can appear in at most k terms in h. Let w.l.o.g h′ := M1 ∨ · · · ∨ Mt be those terms. Notice that t ≤ |W |k ≤ r′k. In each term Mi, i ≤ t one can choose a variable xji that is not in W . This is because, if all the variable in Mi are in W , then M ⇒ Mi and then f is not reduced MDNF.\nLet Z = {xji |i = 1, . . . , t}. Since |Z| ≤ t ≤ r′k and |U | ≤ r′ there is a ∈ A that is 0 in every variable in Z and is 1 in every variable in U . Now notice that a′ in step 3.1 in the algorithm is the same as a over the variables in Z and therefore h′(a′) = 0. Also a′ is the same as a over the variables in U and therefore M(a′) = 1. Now notice that since Mi(a′) = 0 for i ≤ t, in step 3.4 in the algorithm we only flip a′i that correspond to variables in the terms Mi, i > t. The set of variables in each other term Mi, i > t is disjoint with U . Therefore if for some i > t, Mi(a\n′) = 1 then setting any variable xj in Mi that is one in a\n′ to zero will not change the values M(a′) = 1 and (from monotonicity) h′(a′) = 0. Eventually, we will have an assignment a′′ that satisfies h(a′′) = 0 and M(a′′) = 1 which implies f(a′′) = 1.\nIn the following lemma, we remove the restriction on h.\nLemma 11. Let f(x1, x2, . . . , xn) = M1 ∨ · · · ∨Ms be the target s-term rMDNF. Suppose some of the terms, h = M1 ∨M2 ∨ . . . ∨Ms−ℓ, are already known to the learning algorithm. Then, for any integer d, there exists an\nalgorithm that finds a new term using\nO\n(\nr ∑\ni=1\n( r √ ds\ni\n)\nN(((r − i) √ s/d; (r − i)); rs) + r log n ) ,\nmembership queries.\nProof. Consider the algorithm in Figure 3.\nFirst note that in step 2.2, f(A) is considered in LearnRead as a function in all the variables Xn. Note also that the oracle MQf(A) can be simulated by MQf , since f(A)(a) = f(a|R←0,S\\R←1).\nLet W be the set of variables that appear in M := Ms−ℓ+1 and R = S∩W . Note that A is zero in all S\\R and 1 in R and therefore f(A) is now a read √\ns/d andM(A) contains at most |W\\R| ≤ r−|R| variables. Therefore, when we run LearnRead(MQf(A), s, ℓ, r − |R|) we find an assignment a′′ that is 1 in M(A) and zero in f(A) and then a′′|R←0,S\\R←1 is one in f and zero in h.\nWe now find the number of queries. By the Pigeon hole principle, there are at most |S| ≤ r √ ds that are √\ns/d-frequent. The number of sets R ⊆ S of size i is (r √ ds i )\n. For each set, we run LearnRead(MQf(A), s, ℓ, r − |R|) that by Lemma 10 asks N(((r − i) √\ns/d; (r − i)); rs) queries. This implies the result.\nWe now prove our main result. We choose d = r. Then by the construc-\ntion (2), we have\n( r √ ds\ni\n)\nN(((r − i) √ s/d; (r − i)); rs) ≤ (\ner √ rs\ni\n)i\n(2e)r−i ( (r − i)√s√ r )r−i+3\n≤ er2r−i( √ rs)r+3 (r\ni\n)i ( r − i r\n)r−i+3\n≤ er2r−i ( r\ni\n) ( √ rs)r+3.\nand therefore\nr ∑\ni=1\n( r √ ds\ni\n)\nN(((r − i) √ s/d; (r − i)); rs) ≤ (3e)r(rs)r/2+1.5."
    }, {
      "heading" : "4.3.2 Randomized Algorithm",
      "text" : "In this section, we give a randomized algorithm for the case s > r. The randomized algorithm is the same as the deterministic one, except that each CFF is constructed randomly, as in (1) with probability of success 1− δ/s. We choose d = 1 and get\n( r √ ds\ni\n)\nN(((r − i) √ s/d; (r − i)); rs)\n≤ (\ner √ s\ni\n)i√ r(e( √ s+ 1))r−i ( 2s log rs+ log s\nδ\n)\n.\n≤ er2r−i (r\ni\n)i √ rsr/2(s log s+ log(1/δ))\nand therefore\nr ∑\ni=1\n( r √ ds\ni\n)\nN(((r − i) √ s/d; (r − i)); rs) ≤ √ r(3e)rsr/2(s log s+ log(1/δ))."
    }, {
      "heading" : "5 Conclusion and Open Problems",
      "text" : "In this paper, we gave an almost optimal adaptive exact learning algorithms for the class of s-term r-MDNF. When r and s are fixed, the bounds are asymptotically tight. Some gaps occur between the lower bounds and upper bounds. For r ≥ s, the gap is cs for some constant c and for r ≤ s the gap is rr/2. It is interesting to close these gaps. Finding a better deterministic construction of CFF will give better deterministic algorithms.\nAnother challenging problem is finding tight bounds for non-adaptive learning of this class."
    } ],
    "references" : [ {
      "title" : "Queries and Concept Learning",
      "author" : [ "D. Angluin" ],
      "venue" : "Machine Learning",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1987
    }, {
      "title" : "Learning a Hidden Subgraph",
      "author" : [ "A. Alon", "V. Asodi" ],
      "venue" : "SIAM J. Discrete Math",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2005
    }, {
      "title" : "Learning a Hidden Matching",
      "author" : [ "N. Alon", "R. Beigel", "S. Kasif", "S. Rudich", "B. Sudakov" ],
      "venue" : "SIAM J. Comput",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2004
    }, {
      "title" : "Learning a Hidden Hypergraph",
      "author" : [ "D. Angluin", "J. Chen" ],
      "venue" : "Journal of Machine Learning Research",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2006
    }, {
      "title" : "Learning a hidden graph using O(log n) queries per edge",
      "author" : [ "D. Angluin", "J. Chen" ],
      "venue" : "J. Comput. Syst. Sci",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2008
    }, {
      "title" : "Algorithmic construction of sets for krestrictions",
      "author" : [ "N. Alon", "D. Moshkovitz", "S. Safra" ],
      "venue" : "ACM Transactions on Algorithms,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2006
    }, {
      "title" : "An optimal procedure for gap closing in whole genome shotgun sequencing",
      "author" : [ "R. Beigel", "N. Alon", "S. Kasif", "M. Serkan Apaydin", "L. Fortnow" ],
      "venue" : "RECOMB",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2001
    }, {
      "title" : "Testers and their Applications",
      "author" : [ "N.H. Bshouty" ],
      "venue" : "Electronic Collouium on Computational Complexity (ECCC) 19:11,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2012
    }, {
      "title" : "Asking Questions to Minimize Errors",
      "author" : [ "N.H. Bshouty", "S.A. Goldman", "Thomas R. Hancock", "Sleiman Matar" ],
      "venue" : "J. Comput. Syst. Sci",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1996
    }, {
      "title" : "Attribute-Efficient Learning in Query and Mistakebound Models",
      "author" : [ "N.H. Bshouty", "L. Hellerstein" ],
      "venue" : "COLT",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1996
    }, {
      "title" : "Efficient Computation of Representative Sets with Applications in Parameterized and Exact Algorithms",
      "author" : [ "F.V. Fomin", "D. Lokshtanov", "S. Saurabh" ],
      "venue" : "SODA",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2014
    }, {
      "title" : "Nonrandom binary superimposed codes",
      "author" : [ "W.H. Kautz", "R.C. Singleton" ],
      "venue" : "IEEE Trans. Inform. Theory,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1964
    }, {
      "title" : "Reconstructing a Hamiltonian Cycle by Querying the Graph: Application to DNA Physical Mapping",
      "author" : [ "V. Grebinski", "G. Kucherov" ],
      "venue" : "Discrete Applied Mathematics",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 1998
    }, {
      "title" : "Families of k-independent sets",
      "author" : [ "D.J. Kleitman", "J. Spencer" ],
      "venue" : "Discrete Mathematics,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1972
    }, {
      "title" : "Splitters and Near-optimal Derandomization",
      "author" : [ "M. Naor", "L.J. Schulman", "A. Srinivasan" ],
      "venue" : "FOCS 95,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 1995
    }, {
      "title" : "Some New Bounds for Cover-free Families",
      "author" : [ "D.R. Stinson", "R. Wei", "L. Zhu" ],
      "venue" : "Journal of Combinatorial Theory, Series A,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2000
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "1 Introduction We consider the problem of learning a monotone DNF with at most s terms, where each monotone term contains at most r variables (s term r-MDNF) from membership queries [1].",
      "startOffset" : 182,
      "endOffset" : 185
    }, {
      "referenceID" : 4,
      "context" : "The time needed to compute which experiments to do is a secondary consideration, though it is polynomial for the algorithms we present [5].",
      "startOffset" : 135,
      "endOffset" : 138
    }, {
      "referenceID" : 12,
      "context" : "See [13, 7, 3, 2, 4, 5] for more details and other applications.",
      "startOffset" : 4,
      "endOffset" : 23
    }, {
      "referenceID" : 6,
      "context" : "See [13, 7, 3, 2, 4, 5] for more details and other applications.",
      "startOffset" : 4,
      "endOffset" : 23
    }, {
      "referenceID" : 2,
      "context" : "See [13, 7, 3, 2, 4, 5] for more details and other applications.",
      "startOffset" : 4,
      "endOffset" : 23
    }, {
      "referenceID" : 1,
      "context" : "See [13, 7, 3, 2, 4, 5] for more details and other applications.",
      "startOffset" : 4,
      "endOffset" : 23
    }, {
      "referenceID" : 3,
      "context" : "See [13, 7, 3, 2, 4, 5] for more details and other applications.",
      "startOffset" : 4,
      "endOffset" : 23
    }, {
      "referenceID" : 4,
      "context" : "See [13, 7, 3, 2, 4, 5] for more details and other applications.",
      "startOffset" : 4,
      "endOffset" : 23
    }, {
      "referenceID" : 4,
      "context" : "2 Previous Results In [5], Angluin and Chen presented an deterministic optimal adaptive learning algorithm for learning s-term 2-MDNF.",
      "startOffset" : 22,
      "endOffset" : 25
    }, {
      "referenceID" : 3,
      "context" : "In [4], Angluin and Chen gave a randomized algorithm for s-term r-uniform MDNF (the size of each term is exactly r) that asks O(24rs · poly(r, log n)) membership queries.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 12,
      "context" : "See [13, 7, 3, 2, 4, 5].",
      "startOffset" : 4,
      "endOffset" : 23
    }, {
      "referenceID" : 6,
      "context" : "See [13, 7, 3, 2, 4, 5].",
      "startOffset" : 4,
      "endOffset" : 23
    }, {
      "referenceID" : 2,
      "context" : "See [13, 7, 3, 2, 4, 5].",
      "startOffset" : 4,
      "endOffset" : 23
    }, {
      "referenceID" : 1,
      "context" : "See [13, 7, 3, 2, 4, 5].",
      "startOffset" : 4,
      "endOffset" : 23
    }, {
      "referenceID" : 3,
      "context" : "See [13, 7, 3, 2, 4, 5].",
      "startOffset" : 4,
      "endOffset" : 23
    }, {
      "referenceID" : 4,
      "context" : "See [13, 7, 3, 2, 4, 5].",
      "startOffset" : 4,
      "endOffset" : 23
    }, {
      "referenceID" : 4,
      "context" : "For the case s ≥ r, Angluin and Chen, [5], gave the lower bound Ω((2s/r)r/2 + rs log n).",
      "startOffset" : 38,
      "endOffset" : 41
    }, {
      "referenceID" : 11,
      "context" : "2 Cover-Free Families The problem (n, (s, r))-cover-free family [12] is equivalent to the following problem: A (n, (s, r))-cover-free family is a set A ⊆ {0, 1}n such that for",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 15,
      "context" : "The lower bounds in [16] are",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 7,
      "context" : "In [8], Bshouty gave a deterministic construction of (n, (s, r))-CFF of size C := min((2e)r, (2e)s) log n = (",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 10,
      "context" : "in [11] gave a construction of size D := (",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 3,
      "context" : "The first is from [4] and the second follows using the same techniques used in [9].",
      "startOffset" : 18,
      "endOffset" : 21
    }, {
      "referenceID" : 8,
      "context" : "The first is from [4] and the second follows using the same techniques used in [9].",
      "startOffset" : 79,
      "endOffset" : 82
    }, {
      "referenceID" : 3,
      "context" : "In [4], Angluin and Chen proved, Theorem 4.",
      "startOffset" : 3,
      "endOffset" : 6
    } ],
    "year" : 2014,
    "abstractText" : "In this paper, we study the problem of learning a monotone DNF with at most s terms of size (number of variables in each term) at most r (s term r-MDNF) from membership queries. This problem is equivalent to the problem of learning a general hypergraph using hyperedge-detecting queries, a problem motivated by applications arising in chemical reactions and genome sequencing. We first present new lower bounds for this problem and then present deterministic and randomized adaptive algorithms with query complexities that are almost optimal. All the algorithms we present in this paper run in time linear in the query complexity and the number of variables n. In addition, all of the algorithms we present in this paper are asymptotically tight for fixed r and/or s.",
    "creator" : "LaTeX with hyperref package"
  }
}