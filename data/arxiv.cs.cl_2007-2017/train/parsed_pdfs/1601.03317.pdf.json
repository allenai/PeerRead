{
  "name" : "1601.03317.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Implicit Distortion and Fertility Models for Attention-based Encoder-Decoder NMT Model",
    "authors" : [ "Shi Feng", "Shanghai Jiao Tong", "Shujie Liu", "Mu Li", "Ming Zhou" ],
    "emails" : [ "sjtufs@gmail.com", "shujliu@microsoft.com", "muli@microsoft.com", "mingzhou@microsoft.com" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Neural machine translation has shown promising results lately. Most NMT methods follow the encoder-decoder framework proposed by (Cho et al., 2014), which typically consists of two RNNs: the encoder RNN reads the source sentence and transform it into vector representation; the decoder RNN takes the vector representation and generates the target sentence word by word. The decoder will stop once a special symbol denoting the end of the sentence is generated. This encoder-decoder framework can be used on general sequence-to-sequence tasks (Sutskever et al.,\n†Work done while Shi was an intern at Microsoft Research.\n2014), like question answering and text summarization. After some modification, for example replacing the RNN encoder with a CNN, the model can also be applied to tasks like image captioning (Vinyals et al., 2014; Xu et al., 2015). In the following discussion, we focus on the task of machine translation.\nIn the original encoder-decoder model, although the encoder RNN generates a set of hidden states, one at each position of the source sentence, the decoder only takes the last one. This design in effect compresses the variable-length source sentence into a fixed-length context vector, with the information of each source word implicitly stored in the context vector. Thus the decoder cannot easily make full use of the whole sequence of encoder hidden states. To make it more flexible and generalize the fixed-length representation to a variablelength one, it was proposed to use attention mechanism for machine translation (Bahdanau et al., 2014).\nAttention mechanism was first proposed to allow models to learn alignments between different modalities, e.g., between image objects and agent actions in the dynamic control problem (Mnih et al., 2014).\nIn (Bahdanau et al., 2014), attention mechanism was applied to machine translation to learn an alignment between source words and target words. Fig. 1 shows a sample alignment given by attention mechanism.\nWith the ability of learning alignments between different modalities from attention mechanism, attention-based encoder-decoder model is more powerful than just encoder-decoder and has been used for many tasks like question answering (Hermann et al., 2015), speech recognition (Bahdanau\nar X\niv :1\n60 1.\n03 31\n7v 3\n[ cs\n.C L\n] 2\n2 Ja\nn 20\n16\net al., 2015; Chorowski et al., 2014), image captioning (Xu et al., 2015) and visual question answering (Xu and Saenko, 2015; Chen et al., 2015; Shih et al., 2015). In these applications, variations of attention mechanism were proposed to enhance its performance."
    }, {
      "heading" : "2 Problems of Attention Mechanism",
      "text" : "By training the encoder-decoder model with attention mechanism, we get an alignment from target word to source word. This alignment helps translation by allowing re-ordering. But since the alignment by attention is not always accurate, we observed that in many cases where the alignment is incorrect, the translation quality is significantly damaged. We attribute this kind of problem to the lack of explicit distortion and fertility models in attention-based encoder-decoder model."
    }, {
      "heading" : "2.1 Lack of Distortion Model",
      "text" : "In SMT, the distortion model controls how the words are re-ordered. In Fig. 2 we show an example alignment given by attention mechanism where incorrect alignment in the middle of the sentence caused the translation to go wrong afterwards. We focus on the later part of the sentence where the correct translation should be:\n“...and warned that the election to be held on january 30th next year would not be and end to\nserious violence in iraq.” which get translated into: “...and warned that it would not be the end of iraq’s serious violence next year.” From the alignment matrix we can see that, the word “预定” (means “scheduled”) in source sentence is attended to by “would”, but the next attention jumped to “大选 (means “election”), while it should focus on “明年” (means “next year”) or on the date. After this incorrect re-ordering, the meaning of the source sentence is twisted in the translation. If the attention is aware of the previous alignment, it should be able to order “next year” after “election” and reserve the mearning of source sentence correctly.\nIn this kind of casese, the translation can go wrong due to incorrect re-ordering. We attribute this kind of problem due to the lack of distortion model in attention-based encoder-decoder."
    }, {
      "heading" : "2.2 Lack of Fertility Model",
      "text" : "In SMT, the fertility model controls how many target words are translated from a source word. We observe two phenomena related to the lack of fertility model in attention-based encoder-decoder: the problem of repetition and the problem of coverage.\nProblem of Repetition Fig. 3 shows an example of repetition problem in alignment. For consecutive words, the attention mechanism focused on the same position in the source sentence, resulting in repetition in the translation, “the organization of europe and the organization of cooperation in europe”.\nProblem of Coverage Fig. 4 shows an example of coverage problem in alignment. We see that some part of the source sentence was not attended to, resulting in significant loss of content in the translation.\nThese two problems are due to the lack of fertility model in NMT: in the first case, some source words are translated into too many target words, while in the second case, some sources words are translted into too few target words.\nAlthough attention mechanism already makes\nthe encoder-decoder more flexible by allowing re-ordering, the observed problems demonstrated some restrictions of it. Motivated by these observations, we propose additions of implicit distortion and fertility models to attention-based encoder decoder. In Sec. 4, we introduce RECATT and RNNATT which are designed as an implicit distortion models. In Sec. 5, we introduce CONDDEC which is designed as an implicit fertility model. We verify that the proposed methods can resolve the observed problems in our experiments in Sec. 8.2."
    }, {
      "heading" : "3 Attention-based Encoder-Decoder",
      "text" : "We start by reviewing the RNN used in NMT papers and the encoder-decoder with attention mechanism from (Bahdanau et al., 2014)."
    }, {
      "heading" : "3.1 Gated Recurrent Unit",
      "text" : "Gated Recurrent Unit (GRU) (Cho et al., 2014) is an RNN alternative similar to LSTM (Hochreiter and Schmidhuber, 1997). It was used in NMT papers (Cho et al., 2014; Bahdanau et al., 2014) and we will use GRU as RNN in our paper. Like normal RNN, GRU computes its hidden state hi based on the input xi and previous hidden state hi−1:\nhi = RNN(hi−1,xi)\nwhich is computed with update gate and reset gate, formally defined by:\nri = σ(W rxi +U rhi−1) h′i = tanh(ri ◦Uhi−1 +Wxi) zi = σ(W zxi +U zhi−1) hi = (1− zi) ◦ h′i + zi ◦ hi−1\nwhere xi is the input, hi−1 is the previous hidden state. zi and ri are the values of update gate and reset gate respectively. ◦ denotes bit-wise product. Biases are dropped for simplicity."
    }, {
      "heading" : "3.2 RNNSearch (Bahdanau et al., 2014)",
      "text" : "Encoder The encoder used in RNNSEARCH (Bahdanau et al., 2014) is a bi-directional RNN. It consists of two independent RNNs, one reading the source sentence from left to right, another from right to left:\n−→s i = RNN(−→s i−1,xi) ←−s i = RNN(←−s i+1,xi)\nwhere xi is the word embedding of source word at position i. The representation at position i is then defined as the concatenation of −→s i and←−s i:\nsi = [ −→s i←−s i ]\nDecoder with Attention Unlike the decoder from (Cho et al., 2014) which takes only the last representation, the decoder with attention mechanism can make full use of the whole representation set sj . The decoder is assisted by a unit that provides a dynamic context ci:\nci = ATT(hi−1, {sj})\nAt each decoder step, the attention unit takes both the previous decoder hidden state hi−1 and the set of encoder representations {sj} as input, outputs a weighted average of encoder hidden states as the context ci. It uses a match function α to match hi−1 with each sj and generates the weight wij for sj .\neij = v T tanhα(hi−1, sj)\nwij = exp(eij)∑ k exp(eik)\nci = ∑ j wijsj\nThe match function can take on many forms, which is analyzed in (Luong et al., 2015).\nIn our paper we use the sum match function, which is a more common choice as used in (Bahdanau et al., 2014).\nαsum(a, b) = W αa+Uαb\nThe context ci is then used by the decoder:\nhi = RNN(hi−1,yi−1, ci)\nwhere yi−1 is the embedding of previous target word.\nTo predict a target word at position i, the decoder state hi is concatenated with ci and yi−1 and fed through deep out (Pascanu et al., 2012) with a single maxout hidden layer (Goodfellow et al., 2013), followed by a softmax. We follow this structure in this paper."
    }, {
      "heading" : "4 Recurrent Attention Mechanisms",
      "text" : "In Fig. 5 we show an abstraction of the decoderattention structure.\nWe note that attention mechanism treats the encoder states as a set, not a sequence, while the source sentence order is crucial to re-ordering. And the state of re-ordering given to the attention unit is all embedded in the hidden state of the decoder - the attention unit itself does not have memory.\nMotivated by the analysis in Sec. 2.1, we propose to add recurrent paths to the decoderattention structure to provide the attention unit with more information the re-ordering. With these recurrent paths, instead of making the decoder remembering what the state of re-ordering is, recurrent attention mechanism explicitly keeps track of this information."
    }, {
      "heading" : "4.1 RecAtt",
      "text" : "In this section we introduce our proposed recurrent attention mechanism, RecAtt.\nWe pass the previous context directly to the at-\ntention unit to inform it about the alignment from the previous step.\nThe decoder with RecAtt follows:\nci = ATT(hi−1, ci−1{sj}) hi = RNN(hi−1,yi−1, ci)\nWhere the modified attention mechanism RecAtt follows:\neij = v T tanhα(hi−1, ci−1, sj)\nwij = exp(eij)∑ k exp(eik)\nci = ∑ j wijsj\nThe modified sum match function is:\nαsum(a, b, c) = W αa+Uαb+ V αc\nWe note that RecAtt is purely content-based - the recurrent attention information is the context vector instead of the weights. We show in our experiments that making attention unit itself recurrent is very important to improving end-to-end translation performance."
    }, {
      "heading" : "4.2 RNNAtt",
      "text" : "RecAtt designed with the aim of adding a distortion model. In RecAtt, only the previous attentiongenerated context is used in the recurrent path, so it only has a “short-term memory”. To make it\nmore flexible and have a longer memory, we propose RNNATT, as shown in Fig. 7. The attention unit now keeps a hidden state and in effect becomes a complete RNN.\nci = ATT OUT(qi−1, {sj}) qi = ATT RNN(qi−1,hi−1, ci)\nhi = RNN(yi−1,hi−1, ct)\nwhere ATT OUT is the original attention unit which applies the match function and softmax, ATT RNN denotes the hidden state qi computation of the attention unit."
    }, {
      "heading" : "5 Conditioned Decoder",
      "text" : "As analyzed in Sec. 2.2, attention mechanism might produce incorrect alignment and lowquality translation due to the lack of explicit distortion and fertility models. To address this issue, we propose conditioned decoder, CONDDEC, which uses a condition vector to represent what information has been extracted from the source sentence. This can be seen as an implicit fertility model, the condition vector can keep track of how many target words are translated from each source word. We use a structure similar to (Wen et al., 2015) where a predefined condition is used to guide natural language generation. Different from that method, we use a trainable condition initialized with the last encoder hidden state. At each decoding step, the condition is updated with the decoder state and used to compute the next decoder state. The decoder GRU with attention and condition sdi is defined as adding an extra decay gate vdi to decoder:\nri = σ(W rxi +U rhi−1 + V rci) h′i = tanh(ri ◦Uhi−1 +Wxi + V ci) zi = σ(W zxi +U zhi−1 + V zci)\ndi = σ(W dxi +U dhi−1 + V dci)\nsdi = di ◦ sdi−1 hi = (1− zi) ◦ h′i + zi ◦ hi−1 + tanh(V hsdi)\nLet T be the length of the source sentence. We further penalize the condition by adding the following two costs to the categorical cross-entropy cost of the translation model:\nStep-decay cost We restrict the decay gate from extracting too much information from the condi-\ntion. So we add a cost term:\ncostdecay = 1\nT T∑ j=1 ||sdj − sdj−1||2\nLeft-over cost We want the decoder to extract as much information as possible from the condition after reading the source sentence. So we add a cost term:\ncostleft = ||sdT ||2\nThese two costs are added to the categorical cross-entropy cost of the translation model. At training time, the costs are used to enforce a fertility model and are ignored at test time."
    }, {
      "heading" : "6 Related work",
      "text" : "There are variations of attention mechanism that have recurrent paths similar to that of RECATT. In this section, we review these models and compare those decoder-attention structures."
    }, {
      "heading" : "6.1 InputFeed (Luong et al., 2015)",
      "text" : "In (Luong et al., 2015) the authors explored several variations of attention mechanism, including different match functions and local attention. We focus on the input-feeding method proposed in this paper because it is recurrent-like. INPUTFEED passes the previous attention output to the decoder together with current attention output, to further inform the decoder with previous alignment decisions.\nci = ATT(hi−1, {si}) ht = RNN(hi−1,yi−1, ci, ci−1)\nThis attention mechanism is purely contentbased - the recurrent information is the context given by attention mechanism instead of weights. Note that the recurrent information is used outside the attention function, directly to the decoder, which makes it different from RecAtt, where the recurrent information is passed to the attention unit."
    }, {
      "heading" : "6.2 HybridAtt1(Chorowski et al., 2014)",
      "text" : "In (Chorowski et al., 2014) the authors proposed an attention mechanism with a recurrent path. When computing the current set of weights on encoder states, the attention unit takes the previous weights and penalize the jump distance. It computes the average attention center, which is mi−1 = ∑ j j ∗ w(i−1),j . Then it adjusts the weight of each encoder state by its distance from that center.\nmi−1 = ∑ j j · wi−1,j\neij = v T tanhα(hi−1, sj) e′ij = Logistic(j −mi−1) · exp(eij)\nwij = e′ij∑ k e ′ ik\nci = ∑ j wijsj\nThis is a content-based attention with locationbased recurrent attention, which is characterized by using an average attention center. Note that the recurrent information is used outside the attention unit, to adjusting the weights, which makes it different from RecAtt."
    }, {
      "heading" : "6.3 HybridAtt2(Bahdanau et al., 2015)",
      "text" : "In paper (Bahdanau et al., 2015) the authors followed the previous one and improved HYBRIDATT1 by integrating the recurrent location information into attention function. It first extracts feature vectors gi by doing convolution with previous weights Q ∗ wi−1, then uses these feature vectors to predict new weights.\ngi = Q ∗wi−1 eij = v T tanhα(hi−1, sj , gij)\nwij = exp(eij)∑ k exp(eik)\nci = ∑ j wijsj\nwhere ∗ denotes convolution. This is also a content-based attention mechanism with location-based recurrent attention. The difference between this method and HYBRIDATT1 is that the recurrent information is integrated into the attention function.\nWe note that HYBRIDATT1 and HYBRIDATT2 were proposed for speech recognition task, which requires less re-ordering compared to translation. Although these models improved the performance of encoder-decoder on speech recognition, they not necessarily will on machine translation. We included these models because they have structures similar to RECATT.\nOther variations of attention mechanism with similar recurrent paths include (Mnih et al., 2014), (Chen et al., 2015). In these works, the authors used attention mechanism on image classification and visual question answering respectively. The variations of attention mechanism they used are location-based attention, which is more reasonable\nfor image-related tasks. Due to these reasons we do not review or compare their methods in this work."
    }, {
      "heading" : "7 Experimental Setup",
      "text" : "In this section we describe the data used in our experiments, our evaluation methods and our validation procedure.\nDatasets For training, we use NIST ChineseEnglish training set excluding the Hong Kong Law and Hong Kong Hansard (0.5m sentence pairs after exclusion). For testing, we use Nist2005 dataset (1082 sentence pairs). For validation, we use Nist2003 dataset (913 sentence pairs). Validation set is only used for early-stopping and training process monitoring.\nFollowing (Bahdanau et al., 2014), we use source and target dictionaries of size 30000, covering 97.4% and 98.9% of the vocabularies respectively. Out-of-vocabulary words are replaced with a special token 〈UNK〉.\nPost-processing We perform post-processing based on the alignment given by attention mechanism. For each translated target word, we choose the source word assigned with the highest attention weight as the aligned word.\n〈UNK〉’s in the translated sentence are replaced with the correct translation of the aligned source word. We make a simple word-level translation table from the alignment result given by GIZA++ (Och and Ney, 2003) from the training set: for each source word, we choose the most frequently aligned target word.\nEvaluation Performance is evaluated by BLEU score (Papineni et al., 2002) over the test set.\nWe compare 6 models, RNNSEARCH (Cho et al., 2014), HYBRIDATT2 (Bahdanau et al., 2015), INPUTFEED (Luong et al., 2015), and three proposed models, RECATT, RNNATT and CONDDEC. We skip HYBRIDATT1 because we have HYBRIDATT2 as an improved version.\nWe benchmark the 6 NMT models with our implementation of hierarchical phrase-based SMT from (Chiang, 2007), with standard features, denoted as SMT.\nValidation Validation is done by calculating the BLEU score over the validation set without\npost-processing, using MultiBleu.perl script from (Bahdanau et al., 2014). For each model, we choose the parameters of the highest validation score.\nModel Training The encoder and decoder have 1000 hidden units each. The dimension of source and target word embedding is 620. Following (Bahdanau et al., 2014), we use dropout rate 0.5.\nWe remove sentences of length over 50 words from the training set. We use batch size of 80 with 12 batches pre-fetched and sorted by the sentence length.\nEach model is trained with AdaGrad (Duchi et al., 2011) on K40m GPU for approximately 4 days, finishing over 400000 updates, equivalent to 640 epochs.\nWhen testing trained models, we use beam search (Graves, 2012; Boulanger-Lewandowski et al., 2013; Sutskever et al., 2014) with beam size of 12."
    }, {
      "heading" : "8 Results",
      "text" : ""
    }, {
      "heading" : "8.1 Quantitative",
      "text" : "BLEU scores on the test set are shown in (Table 1). RECATT performed best among NMT models, with and without post-processing. RECATT achieved a 2.1 BLEU score improvement over the original RNNSEARCH.\nNote that RECATT also gained the most improvement from post-processing, 5.04 BLEU points. In the post-processing, we use a naive translation table which is generated purely from the training data so the effect of post-processing depends largely on the quality of the alignment. Thus the gain from post-processing can be seen as a measurement of the quality of attention-\ngenerated alignment, and from this we see that RECATT improved attention mechanism.\nCONDDEC out-performed RNNSEARCH by 1 BLEU point, both with and without postprocessing.\nAll three of our proposed models out-performed the phrase-based SMT baseline.\nThe combination of CONDDEC with RECATT and RNNATT is a work in progress."
    }, {
      "heading" : "8.2 Qualitative",
      "text" : "As mentioned in Sec. 2, the original attentionbased encoder-decoder has some problems due to the lack of distortion and fertility models. In this section we will qualitatively evaluate how our models resolved these problems.\nDistortion We show the alignment and translation by RECATT in Fig. 11 on the same sentence of Fig. 2. In the alignment by RECATT, it can be seen that “will not” are correctly aligned to “不 会” (means “will not”) and “next year” is correctly ordered to describe “the election to be held” instead of “riot in iraq”. The transltion quality of the whole sentence is also higher.\nFertility: Coverage In Fig. 12 we show the alignments given by RNNSEARCH and RECATT.\nFrom the alignment of RNNSEARCH, we can observe the problem of coverage where the later part of the source sentence is lost in the translation, while the alignment given by RECATT does not have this problem and covered the whole source sentence.\nWe observed that RECATT can also resolve the coverage problem. This is because a correct alignment can be very helpful in preventing the incorrect generation of end-of-sentence symbol. In Fig. 13 we show an example. In the alignment by RNNSEARCH, when generating the word after “,” (last row), the attention is not very concentrated, leading to the generation of end-of-sentence symbol. While in the alignment by RECATT when generating that word, the attention correctly focused on “较” (means “more”) with high confidence, leading to the correct generation of “more”.\nFertility: Repetition In Fig. 14 we see that the problem of repetition occurred in the alignment by RNNSEARCH. “东方 快车” (means “midnight express”) is repeatedly focused on and translated into “moon ice” and “night of the midnight of\nthe night”. CONDDEC produces both the correct alignment and the correct translation “midnight express”.\nLong Repetition We observed that RECATT can also resolve the repetition problem. Because the previous attention-generated context was passed to the attention unit, the attention can decide not to focus on the same position as last time. But since it only has a short-term memory, in some cases\nthe alignment by RECATT has long repetitions as shown in Fig. 15.\nAlthough RNNATT did not perform as well as RECATT in terms of BLEU score, we observe that it can resolve the long repetition problem that is hard for RECATT to handle. In Fig. 16 we show the alignments by RNNSEARCH and RNNATT. First we can see that RNNSEARCH did not handle this sentence well, with incorrect alignment and low-quality translation, which shows that this sentence is hard to translate. However the alignment by RNNATT is more accurate and the translation quality is higher than both RNNSEARCH and RECATT, with no long repetition problem.\nOne possible reason for the low BLEU score of RNNATT is that the path from translation cost to the attention recurrent unit is too long and the model is hard to train. Improving end-to-end performance and exploring alternative structures for RNNATT is a work in progress."
    }, {
      "heading" : "9 Conclusions",
      "text" : "In this paper we noted some problems occurred in neural machine translation due to the lack of distortion and fertility model.\nTo resolve these problems, we proposed to add implicit distortion and fertility models to attention-based encoder-decoder. We proposed recurrent attention mechanism, RECATT and RNNATT for distortion model, and CONDDEC for fertility model. We compared our models with other related variations.\nWe evaluated our methods both quantitatively and qualitatively. In Chinese-English translation, RECATT gained an improvement of 2 BLEU points over the original attention mechanism and out-performed all related models. CONDDEC also out-performed the original attention mechanism by 1 BLEU point. By analyzing the alignment ma-\ntrix produced by attention mechanism, we demonstrated that our proposed methods help resolve the observed problems.\nWe are working on the combination of CONDDEC with RECATT and RNNATT. We will also explore alternative structures of recurrent attention mechanism and try to improve the end-to-end performance of RNNATT."
    } ],
    "references" : [ {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "arXiv preprint arXiv:1409.0473.",
      "citeRegEx" : "Bahdanau et al\\.,? 2014",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2014
    }, {
      "title" : "End-to-end attention-based large vocabulary speech recognition",
      "author" : [ "Dzmitry Bahdanau", "Jan Chorowski", "Dmitriy Serdyuk", "Philemon Brakel", "Yoshua Bengio." ],
      "venue" : "arXiv preprint arXiv:1508.04395.",
      "citeRegEx" : "Bahdanau et al\\.,? 2015",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2015
    }, {
      "title" : "Audio chord recognition with recurrent neural networks",
      "author" : [ "Nicolas Boulanger-Lewandowski", "Yoshua Bengio", "Pascal Vincent." ],
      "venue" : "ISMIR, pages 335– 340.",
      "citeRegEx" : "Boulanger.Lewandowski et al\\.,? 2013",
      "shortCiteRegEx" : "Boulanger.Lewandowski et al\\.",
      "year" : 2013
    }, {
      "title" : "Abccnn: An attention based convolutional neural network for visual question answering",
      "author" : [ "Kan Chen", "Jiang Wang", "Liang-Chieh Chen", "Haoyuan Gao", "Wei Xu", "Ram Nevatia." ],
      "venue" : "arXiv preprint arXiv:1511.05960.",
      "citeRegEx" : "Chen et al\\.,? 2015",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2015
    }, {
      "title" : "Hierarchical phrase-based translation",
      "author" : [ "David Chiang." ],
      "venue" : "computational linguistics, 33(2):201–228.",
      "citeRegEx" : "Chiang.,? 2007",
      "shortCiteRegEx" : "Chiang.",
      "year" : 2007
    }, {
      "title" : "Learning phrase representations using rnn encoder-decoder for statistical machine translation",
      "author" : [ "Kyunghyun Cho", "Bart Van Merriënboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio." ],
      "venue" : "arXiv preprint",
      "citeRegEx" : "Cho et al\\.,? 2014",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2014
    }, {
      "title" : "End-to-end continuous speech recognition using attention-based recurrent nn: First results",
      "author" : [ "Jan Chorowski", "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "arXiv preprint arXiv:1412.1602.",
      "citeRegEx" : "Chorowski et al\\.,? 2014",
      "shortCiteRegEx" : "Chorowski et al\\.",
      "year" : 2014
    }, {
      "title" : "Adaptive subgradient methods for online learning and stochastic optimization",
      "author" : [ "John Duchi", "Elad Hazan", "Yoram Singer." ],
      "venue" : "The Journal of Machine Learning Research, 12:2121–2159.",
      "citeRegEx" : "Duchi et al\\.,? 2011",
      "shortCiteRegEx" : "Duchi et al\\.",
      "year" : 2011
    }, {
      "title" : "Maxout networks",
      "author" : [ "Ian J Goodfellow", "David Warde-Farley", "Mehdi Mirza", "Aaron Courville", "Yoshua Bengio." ],
      "venue" : "arXiv preprint arXiv:1302.4389.",
      "citeRegEx" : "Goodfellow et al\\.,? 2013",
      "shortCiteRegEx" : "Goodfellow et al\\.",
      "year" : 2013
    }, {
      "title" : "Sequence transduction with recurrent neural networks",
      "author" : [ "Alex Graves." ],
      "venue" : "arXiv preprint arXiv:1211.3711.",
      "citeRegEx" : "Graves.,? 2012",
      "shortCiteRegEx" : "Graves.",
      "year" : 2012
    }, {
      "title" : "Teaching machines to read and comprehend",
      "author" : [ "Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom." ],
      "venue" : "Advances in Neural Information Processing Systems, pages 1684–",
      "citeRegEx" : "Hermann et al\\.,? 2015",
      "shortCiteRegEx" : "Hermann et al\\.",
      "year" : 2015
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural computation, 9(8):1735–1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Effective approaches to attentionbased neural machine translation",
      "author" : [ "Minh-Thang Luong", "Hieu Pham", "Christopher D Manning." ],
      "venue" : "arXiv preprint arXiv:1508.04025.",
      "citeRegEx" : "Luong et al\\.,? 2015",
      "shortCiteRegEx" : "Luong et al\\.",
      "year" : 2015
    }, {
      "title" : "Recurrent models of visual attention",
      "author" : [ "Volodymyr Mnih", "Nicolas Heess", "Alex Graves" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Mnih et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Mnih et al\\.",
      "year" : 2014
    }, {
      "title" : "A systematic comparison of various statistical alignment models",
      "author" : [ "Franz Josef Och", "Hermann Ney." ],
      "venue" : "Computational linguistics, 29(1):19–51.",
      "citeRegEx" : "Och and Ney.,? 2003",
      "shortCiteRegEx" : "Och and Ney.",
      "year" : 2003
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu." ],
      "venue" : "Proceedings of the 40th annual meeting on association for computational linguistics, pages 311–318. Association for",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "On the difficulty of training recurrent neural networks",
      "author" : [ "Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio." ],
      "venue" : "arXiv preprint arXiv:1211.5063.",
      "citeRegEx" : "Pascanu et al\\.,? 2012",
      "shortCiteRegEx" : "Pascanu et al\\.",
      "year" : 2012
    }, {
      "title" : "Where to look: Focus regions for visual question answering",
      "author" : [ "Kevin J Shih", "Saurabh Singh", "Derek Hoiem." ],
      "venue" : "arXiv preprint arXiv:1511.07394.",
      "citeRegEx" : "Shih et al\\.,? 2015",
      "shortCiteRegEx" : "Shih et al\\.",
      "year" : 2015
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc VV Le." ],
      "venue" : "Advances in neural information processing systems, pages 3104–3112.",
      "citeRegEx" : "Sutskever et al\\.,? 2014",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "Show and tell: A neural image caption generator",
      "author" : [ "Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan." ],
      "venue" : "arXiv preprint arXiv:1411.4555.",
      "citeRegEx" : "Vinyals et al\\.,? 2014",
      "shortCiteRegEx" : "Vinyals et al\\.",
      "year" : 2014
    }, {
      "title" : "Semantically conditioned lstm-based natural language generation for spoken dialogue systems",
      "author" : [ "Tsung-Hsien Wen", "Milica Gasic", "Nikola Mrksic", "PeiHao Su", "David Vandyke", "Steve Young." ],
      "venue" : "arXiv preprint arXiv:1508.01745.",
      "citeRegEx" : "Wen et al\\.,? 2015",
      "shortCiteRegEx" : "Wen et al\\.",
      "year" : 2015
    }, {
      "title" : "Ask, attend and answer: Exploring question-guided spatial attention for visual question answering",
      "author" : [ "Huijuan Xu", "Kate Saenko." ],
      "venue" : "arXiv preprint arXiv:1511.05234.",
      "citeRegEx" : "Xu and Saenko.,? 2015",
      "shortCiteRegEx" : "Xu and Saenko.",
      "year" : 2015
    }, {
      "title" : "Show, attend and tell: Neural image caption generation with visual attention",
      "author" : [ "Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Aaron Courville", "Ruslan Salakhutdinov", "Richard Zemel", "Yoshua Bengio." ],
      "venue" : "arXiv preprint arXiv:1502.03044.",
      "citeRegEx" : "Xu et al\\.,? 2015",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : "Most NMT methods follow the encoder-decoder framework proposed by (Cho et al., 2014), which typically consists of two RNNs: the encoder RNN reads the source sentence and transform it into vector representation; the decoder RNN takes the vector representation and generates the target sentence word by word.",
      "startOffset" : 66,
      "endOffset" : 84
    }, {
      "referenceID" : 19,
      "context" : "After some modification, for example replacing the RNN encoder with a CNN, the model can also be applied to tasks like image captioning (Vinyals et al., 2014; Xu et al., 2015).",
      "startOffset" : 136,
      "endOffset" : 175
    }, {
      "referenceID" : 22,
      "context" : "After some modification, for example replacing the RNN encoder with a CNN, the model can also be applied to tasks like image captioning (Vinyals et al., 2014; Xu et al., 2015).",
      "startOffset" : 136,
      "endOffset" : 175
    }, {
      "referenceID" : 0,
      "context" : "To make it more flexible and generalize the fixed-length representation to a variablelength one, it was proposed to use attention mechanism for machine translation (Bahdanau et al., 2014).",
      "startOffset" : 164,
      "endOffset" : 187
    }, {
      "referenceID" : 13,
      "context" : ", between image objects and agent actions in the dynamic control problem (Mnih et al., 2014).",
      "startOffset" : 73,
      "endOffset" : 92
    }, {
      "referenceID" : 0,
      "context" : "In (Bahdanau et al., 2014), attention mechanism was applied to machine translation to learn an alignment between source words and target words.",
      "startOffset" : 3,
      "endOffset" : 26
    }, {
      "referenceID" : 10,
      "context" : "With the ability of learning alignments between different modalities from attention mechanism, attention-based encoder-decoder model is more powerful than just encoder-decoder and has been used for many tasks like question answering (Hermann et al., 2015), speech recognition (Bahdanau ar X iv :1 60 1.",
      "startOffset" : 233,
      "endOffset" : 255
    }, {
      "referenceID" : 22,
      "context" : ", 2014), image captioning (Xu et al., 2015) and visual question answering (Xu and Saenko, 2015; Chen et al.",
      "startOffset" : 26,
      "endOffset" : 43
    }, {
      "referenceID" : 21,
      "context" : ", 2015) and visual question answering (Xu and Saenko, 2015; Chen et al., 2015; Shih et al., 2015).",
      "startOffset" : 38,
      "endOffset" : 97
    }, {
      "referenceID" : 3,
      "context" : ", 2015) and visual question answering (Xu and Saenko, 2015; Chen et al., 2015; Shih et al., 2015).",
      "startOffset" : 38,
      "endOffset" : 97
    }, {
      "referenceID" : 17,
      "context" : ", 2015) and visual question answering (Xu and Saenko, 2015; Chen et al., 2015; Shih et al., 2015).",
      "startOffset" : 38,
      "endOffset" : 97
    }, {
      "referenceID" : 0,
      "context" : "We start by reviewing the RNN used in NMT papers and the encoder-decoder with attention mechanism from (Bahdanau et al., 2014).",
      "startOffset" : 103,
      "endOffset" : 126
    }, {
      "referenceID" : 5,
      "context" : "Gated Recurrent Unit (GRU) (Cho et al., 2014) is an RNN alternative similar to LSTM (Hochreiter and Schmidhuber, 1997).",
      "startOffset" : 27,
      "endOffset" : 45
    }, {
      "referenceID" : 11,
      "context" : ", 2014) is an RNN alternative similar to LSTM (Hochreiter and Schmidhuber, 1997).",
      "startOffset" : 46,
      "endOffset" : 80
    }, {
      "referenceID" : 5,
      "context" : "It was used in NMT papers (Cho et al., 2014; Bahdanau et al., 2014) and we will use GRU as RNN in our paper.",
      "startOffset" : 26,
      "endOffset" : 67
    }, {
      "referenceID" : 0,
      "context" : "It was used in NMT papers (Cho et al., 2014; Bahdanau et al., 2014) and we will use GRU as RNN in our paper.",
      "startOffset" : 26,
      "endOffset" : 67
    }, {
      "referenceID" : 0,
      "context" : "2 RNNSearch (Bahdanau et al., 2014) Encoder The encoder used in RNNSEARCH (Bahdanau et al.",
      "startOffset" : 12,
      "endOffset" : 35
    }, {
      "referenceID" : 0,
      "context" : ", 2014) Encoder The encoder used in RNNSEARCH (Bahdanau et al., 2014) is a bi-directional RNN.",
      "startOffset" : 46,
      "endOffset" : 69
    }, {
      "referenceID" : 5,
      "context" : "Decoder with Attention Unlike the decoder from (Cho et al., 2014) which takes only the last representation, the decoder with attention mechanism can make full use of the whole representation set sj .",
      "startOffset" : 47,
      "endOffset" : 65
    }, {
      "referenceID" : 12,
      "context" : "The match function can take on many forms, which is analyzed in (Luong et al., 2015).",
      "startOffset" : 64,
      "endOffset" : 84
    }, {
      "referenceID" : 0,
      "context" : "In our paper we use the sum match function, which is a more common choice as used in (Bahdanau et al., 2014).",
      "startOffset" : 85,
      "endOffset" : 108
    }, {
      "referenceID" : 16,
      "context" : "To predict a target word at position i, the decoder state hi is concatenated with ci and yi−1 and fed through deep out (Pascanu et al., 2012) with a single maxout hidden layer (Goodfellow et al.",
      "startOffset" : 119,
      "endOffset" : 141
    }, {
      "referenceID" : 8,
      "context" : ", 2012) with a single maxout hidden layer (Goodfellow et al., 2013), followed by a softmax.",
      "startOffset" : 42,
      "endOffset" : 67
    }, {
      "referenceID" : 20,
      "context" : "We use a structure similar to (Wen et al., 2015) where a predefined condition is used to guide natural language generation.",
      "startOffset" : 30,
      "endOffset" : 48
    }, {
      "referenceID" : 12,
      "context" : "1 InputFeed (Luong et al., 2015)",
      "startOffset" : 12,
      "endOffset" : 32
    }, {
      "referenceID" : 12,
      "context" : "In (Luong et al., 2015) the authors explored several variations of attention mechanism, including different match functions and local attention.",
      "startOffset" : 3,
      "endOffset" : 23
    }, {
      "referenceID" : 6,
      "context" : "2 HybridAtt1(Chorowski et al., 2014)",
      "startOffset" : 12,
      "endOffset" : 36
    }, {
      "referenceID" : 6,
      "context" : "In (Chorowski et al., 2014) the authors proposed an attention mechanism with a recurrent path.",
      "startOffset" : 3,
      "endOffset" : 27
    }, {
      "referenceID" : 1,
      "context" : "3 HybridAtt2(Bahdanau et al., 2015)",
      "startOffset" : 12,
      "endOffset" : 35
    }, {
      "referenceID" : 1,
      "context" : "In paper (Bahdanau et al., 2015) the authors followed the previous one and improved HYBRIDATT1 by integrating the recurrent location information into attention function.",
      "startOffset" : 9,
      "endOffset" : 32
    }, {
      "referenceID" : 13,
      "context" : "Other variations of attention mechanism with similar recurrent paths include (Mnih et al., 2014), (Chen et al.",
      "startOffset" : 77,
      "endOffset" : 96
    }, {
      "referenceID" : 3,
      "context" : ", 2014), (Chen et al., 2015).",
      "startOffset" : 9,
      "endOffset" : 28
    }, {
      "referenceID" : 0,
      "context" : "Following (Bahdanau et al., 2014), we use source and target dictionaries of size 30000, covering 97.",
      "startOffset" : 10,
      "endOffset" : 33
    }, {
      "referenceID" : 14,
      "context" : "We make a simple word-level translation table from the alignment result given by GIZA++ (Och and Ney, 2003) from the training set: for each source word, we choose the most frequently aligned target word.",
      "startOffset" : 88,
      "endOffset" : 107
    }, {
      "referenceID" : 15,
      "context" : "Evaluation Performance is evaluated by BLEU score (Papineni et al., 2002) over the test set.",
      "startOffset" : 50,
      "endOffset" : 73
    }, {
      "referenceID" : 5,
      "context" : "We compare 6 models, RNNSEARCH (Cho et al., 2014), HYBRIDATT2 (Bahdanau et al.",
      "startOffset" : 31,
      "endOffset" : 49
    }, {
      "referenceID" : 1,
      "context" : ", 2014), HYBRIDATT2 (Bahdanau et al., 2015), INPUTFEED (Luong et al.",
      "startOffset" : 20,
      "endOffset" : 43
    }, {
      "referenceID" : 12,
      "context" : ", 2015), INPUTFEED (Luong et al., 2015), and three proposed models, RECATT, RNNATT and CONDDEC.",
      "startOffset" : 19,
      "endOffset" : 39
    }, {
      "referenceID" : 4,
      "context" : "We benchmark the 6 NMT models with our implementation of hierarchical phrase-based SMT from (Chiang, 2007), with standard features, denoted as SMT.",
      "startOffset" : 92,
      "endOffset" : 106
    }, {
      "referenceID" : 0,
      "context" : "perl script from (Bahdanau et al., 2014).",
      "startOffset" : 17,
      "endOffset" : 40
    }, {
      "referenceID" : 0,
      "context" : "Following (Bahdanau et al., 2014), we use dropout rate 0.",
      "startOffset" : 10,
      "endOffset" : 33
    }, {
      "referenceID" : 7,
      "context" : "Each model is trained with AdaGrad (Duchi et al., 2011) on K40m GPU for approximately 4 days, finishing over 400000 updates, equivalent to 640 epochs.",
      "startOffset" : 35,
      "endOffset" : 55
    }, {
      "referenceID" : 9,
      "context" : "When testing trained models, we use beam search (Graves, 2012; Boulanger-Lewandowski et al., 2013; Sutskever et al., 2014) with beam size of 12.",
      "startOffset" : 48,
      "endOffset" : 122
    }, {
      "referenceID" : 2,
      "context" : "When testing trained models, we use beam search (Graves, 2012; Boulanger-Lewandowski et al., 2013; Sutskever et al., 2014) with beam size of 12.",
      "startOffset" : 48,
      "endOffset" : 122
    }, {
      "referenceID" : 18,
      "context" : "When testing trained models, we use beam search (Graves, 2012; Boulanger-Lewandowski et al., 2013; Sutskever et al., 2014) with beam size of 12.",
      "startOffset" : 48,
      "endOffset" : 122
    } ],
    "year" : 2016,
    "abstractText" : "Neural machine translation has shown very promising results lately. Most NMT models follow the encoder-decoder framework. To make encoder-decoder models more flexible, attention mechanism was introduced to machine translation and also other tasks like speech recognition and image captioning. We observe that the quality of translation by attention-based encoder-decoder can be significantly damaged when the alignment is incorrect. We attribute these problems to the lack of distortion and fertility models. Aiming to resolve these problems, we propose new variations of attention-based encoderdecoder and compare them with other models on machine translation. Our proposed method achieved an improvement of 2 BLEU points over the original attentionbased encoder-decoder.",
    "creator" : "TeX"
  }
}