{"conference": "ICLR 2017 conference submission", "title": "Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling", "abstract": "Recurrent neural networks have been very successful at predicting sequences of words in tasks such as language modeling. However, all such models are based on the conventional classification framework, where the model is trained against one-hot targets, and each word is represented both as an input and as an output in isolation. This causes inefficiencies in learning both in terms of utilizing all of the information and in terms of the number of parameters needed to train. We introduce a novel theoretical framework that facilitates better learning in language modeling, and show that our framework leads to tying together the input embedding and the output projection matrices, greatly reducing the number of trainable variables. Our framework leads to state of the art performance on the Penn Treebank with a variety of network models.", "reviews": [{"is_meta_review": true, "comments": "This paper gives a theoretical motivation for tieing the word embedding and output projection matrices in RNN LMs. The argument uses an augmented loss function which spreads the output probability mass among words with close word-embedding. \n\nI see two main drawbacks from this framework:\nThe augmented loss function has no trainable parameters and is used for only for regularization. This is not expected to give gains with large enough datasets. \nThe augmented loss is heavily \u201cengineered\u201d to produce the desired result of parameter tying. It\u2019s not clear what happens if you try to relax it a bit, by adding parameters, or estimating y~ in a different way. \n\nNevertheless the argument is very interesting, and clearly written.\nThe simulated results indeed validate the argument, and the PTB results seem promising.\n\nMinor comments:\nSection 3:\nCan you clarify if y~ is conditioned on the t example or on the entire history.\nEq. 3.5: i is enumerated over V (not |V|)", "IS_META_REVIEW": true}, {"TITLE": "ICLR committee final decision", "OTHER_KEYS": "ICLR 2017 pcs", "comments": "pros:\n - nice results on the tasks that justify acceptance of the paper\n \n cons:\n - In my opinion its a big stretch to describe this paper as a novel framework. The reasons for using the specific contrived augmented loss is based on the good results it produces. I view it more as regularization.\n - The \"theoretical justification\" for coupling of the input and output layers is based on the premise that the above regularization is the correct thing to do. Since that's really not justified by some kind of theory, I think its questionable to call this simple observation a theoretical justification.\n - Tying weights on the inputs and output layers is far from novel.", "IS_META_REVIEW": false, "DATE": "06 Feb 2017", "is_meta_review": false}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Nice justification for parameter sharing", "is_meta_review": false, "comments": "This work offers a theoretical justification for reusing the input word embedding in the output projection layer. It does by proposing an additional loss that is designed to minimize the distance between the predictive distribution and an estimate of the true data distribution. This is a nice setup since it can effectively smooth over the labels given as input. However, the construction of the estimate of the true data distribution seems engineered to provide the weight tying justification in Eqs. 3.6 and 3.7.\n\nIt is not obvious why the projection matrix L in Eq 3.6 (let's rename it to L') should be the same as that in Eq. 2.1. For example, L' could be obtained through word2vec embeddings trained on a large dataset or it could be learned as an additional set of parameters. In the case that L' is a new learned matrix, it seems the result in Eq 4.5 is to use an independent matrix for the output projection layer, as is usually done.\n\nThe experimental results are good and provide support for the approximate derivation done in section 4, particularly the distance plots in figure 1.\n\nMinor comments:\nThird line in abstract: where model -> where the model\nSecond line in section 7: into space -> into the space\nShouldn't the RHS in Eq 3.5 be \\sum \\tilde{y_{t,i}}(\\frac{\\hat{y}_t}{\\tilde{y_{t,i}}} - e_i) ?", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "review", "is_meta_review": false, "comments": "This paper provides a theoretical framework for tying parameters between input word embeddings and output word representations in the softmax.\nExperiments on PTB shows significant improvement.\nThe idea of sharing or tying weights between input and output word embeddings is not new (as noted by others in this thread), which I see as the main negative side of the paper. The proposed justification appears new to me though, and certainly interesting.\nI was concerned that results are only given on one dataset, PTB, which is now kind of old in that literature. I'm glad the authors tried at least one more dataset, and I think it would be nice to find a way to include these results in the paper if accepted.\nHave you considered using character or sub-word units in that context?\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Nice argument for parameter sharing, promising results. ", "is_meta_review": false, "comments": "This paper gives a theoretical motivation for tieing the word embedding and output projection matrices in RNN LMs. The argument uses an augmented loss function which spreads the output probability mass among words with close word-embedding. \n\nI see two main drawbacks from this framework:\nThe augmented loss function has no trainable parameters and is used for only for regularization. This is not expected to give gains with large enough datasets. \nThe augmented loss is heavily \u201cengineered\u201d to produce the desired result of parameter tying. It\u2019s not clear what happens if you try to relax it a bit, by adding parameters, or estimating y~ in a different way. \n\nNevertheless the argument is very interesting, and clearly written.\nThe simulated results indeed validate the argument, and the PTB results seem promising.\n\nMinor comments:\nSection 3:\nCan you clarify if y~ is conditioned on the t example or on the entire history.\nEq. 3.5: i is enumerated over V (not |V|) ", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "15 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Questions", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "", "IS_META_REVIEW": false, "DATE": "30 Nov 2016", "is_meta_review": false}, {"TITLE": "Tying the input and output word representations used to be common", "OTHER_KEYS": "Andriy Mnih", "comments": "I just wanted to point out that is used to be common to use the same embedding matrix for the input and target words in a neural language model. For example, the \"cycling architecture\" proposed in Yoshua Bengio's first language modelling paper [1] does that, as do all three models proposed in [2].\n\n[1] Bengio, Y., Ducharme, R., Vincent, P. and Jauvin, C. A Neural Probabilistic Language Model. NIPS 2000\n[2] Mnih, A., & Hinton, G. Three new graphical models for statistical language modelling. ICML 2007", "IS_META_REVIEW": false, "DATE": "16 Nov 2016", "is_meta_review": false}], "SCORE": 8, "authors": "Hakan Inan, Khashayar Khosravi, Richard Socher", "accepted": true, "id": ""}
