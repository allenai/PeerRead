{
  "name" : "1503.04269.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "An Emphatic Approach to the Problem of Off-policy Temporal-Difference Learning",
    "authors" : [ "Richard S. Sutton", "Rupam Mahmood", "Martha White" ],
    "emails" : [ "sutton@cs.ualberta.ca", "ashique@cs.ualberta.ca", "whitem@cs.ualberta.ca" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Keywords: temporal-difference learning, off-policy training, function approximation, convergence, stability"
    }, {
      "heading" : "1. Parametric Temporal-Difference Learning",
      "text" : "Temporal-difference (TD) learning is perhaps the most important idea to come out of the field of reinforcement learning. The problem it solves is that of efficiently learning to make a sequence of long-term predictions about how a dynamical system will evolve over time. The key idea is to use the change (temporal difference) from one prediction to the next as an error in the earlier prediction. For example, if you are predicting on each day what the stock-market index will be at the end of the year, and events lead you one day to make a much lower prediction, then a TD method would infer that the predictions made prior to the drop were probably too high; it would adjust the parameters of its prediction function so as to make lower predictions for similar situations in the future. This approach can be contrasted with conventional approaches to prediction, which would wait until the end of the year when the final stock-market index would be known before adjusting any parameters, or else make only short-term (e.g., one-day) predictions and then iterate them to produce a year-end prediction. The TD approach is more convenient computationally because it requires less memory and because its computations are spread out uniformly over\nc© Richard S. Sutton, A. Rupam Mahmood & Martha White.\nar X\niv :1\n50 3.\n04 26\n9v 1\n[ cs\n.L G\n] 1\n4 M\nthe year (rather than being bunched together all at the end of the year). A less obvious advantage of the TD approach is that it often produces statistically more accurate answers than conventional approaches (Sutton 1988).\nParametric temporal-difference learning was first studied as the key “learning by generalization” algorithm in Samuel’s (1959) checker player. Sutton (1988) introduced the TD(λ) algorithm and proved convergence in the mean of episodic linear TD(0), the simplest parametric TD method. The potential power of parametric TD learning was convincingly demonstrated by Tesauro (1992, 1995) when he applied TD(λ) combined with neural networks and self play to obtain ultimately the world’s best backgammon player. Dayan (1992) proved convergence in expected value of episodic linear TD(λ) for all λ ∈ [0, 1], and Tsitsiklis and Van Roy (1997) proved convergence with probability one of discounted continuing linear TD(λ). Watkins (1989) extended TD learning to control in the form of Q-learning and proved its convergence in the tabular case (without function approximation, Watkins & Dayan 1992), while Rummery (1995) extended TD learning to control in an on-policy form as the Sarsa(λ) algorithm. Bradtke and Barto (1996) and Boyan (1999) extended linear TD learning to a least-squares form called LSTD(λ). Parametric TD methods have also been developed as models of animal learning (e.g., Sutton & Barto 1981, 1990, Klopf 1988, Ludvig, Sutton & Kehoe 2012) and as models of the brain’s reward systems (Schultz, Dayan & Montague 1997, Suri 2002), where they have been particularly influential (e.g., Niv & Schoenbaum 2002, Dayan & Niv 2008, O’Doherty 2012). Sutton (2009, 2012) has suggested that parametric TD methods could be key not just to learning about reward, but to the learning of world knowledge generally, and to perceptual learning.\nWithin reinforcement learning, TD learning is typically used to learn approximations to the value function of a Markov decision process (MDP). Here the value of a state s, denoted vπ(s), is defined as the sum of the expected long-term discounted rewards that will be received if the process starts in s and subsequently takes actions as specified by the decisionmaking policy π, called the target policy. If there are a small number of states, then it may be practical to approximate the function vπ by a table, but more generally a parametric form is used, such as a polynomial, multi-layer neural network, or linear mapping. Also key is the source of the data, in particular, the policy used to interact with the MDP. If the data is obtained while following the target policy π, then good convergence results are available for linear function approximation. This case is called on-policy learning because learning occurs while “on” the policy being learned about. In the alternative, off-policy case, one seeks to learn about vπ while behaving (selecting actions) according to a different policy called the behavior policy, which we denote by µ. Baird (1995) showed definitively that parametric TD learning was much less robust in the off-policy case by exhibiting counterexamples for which both linear TD(0) and linear Q-learning had unstable expected updates and, as a result, the parameters of their linear function approximation diverged to infinity. This is a serious limitation, as the off-policy aspect is key to Q-learning (perhaps the single most popular reinforcement learning algorithm), to learning from historical data and from demonstrations, and to the idea of using TD learning for perception and world knowledge.\nOver the years, several different approaches have been taken to solving the problem of off-policy learning. Baird (1995) proposed an approach based on gradient descent in the Bellman error for general parametric function approximation that has the desired computa-\ntional properties, but which requires access to the MDP for double sampling and which in practice often learns slowly. Gordon (1995, 1996) proposed restricting attention to function approximators that are averagers, but this does not seem to be possible without storing many of the training examples, which would defeat the primary strength that we seek to obtain from parametric function approximation. The LSTD(λ) method was always robust to off-policy training (e.g., Lagoudakis & Parr 2003, Yu 2010, Mahmood, van Hasselt & Sutton in press), but its per-step computational complexity is quadratic in the number of parameters of the function approximator, as opposed to the linear complexity of TD(λ) and the other methods. Perhaps the most successful approach to date is the gradient-TD approach (e.g., Maei 2011, Sutton et al. 2009, Maei et al. 2010), including hybrid methods such as HTD (Hackman 2012). Gradient-TD methods are of linear complexity and guaranteed to converge for appropriately chosen step-size parameters but are more complex than TD(λ) because they require a second auxiliary set of parameters with a second step size that must be set in a problem-dependent way for good performance. The studies by White (in preparation), Geist and Scherrer (2014), and Dann, Neumann, and Peters (2014) represent our best experience with gradient-TD and related methods.\nIn this paper we explore a new approach to solving the problem of off-policy TD learning with function approximation. The approach has novel elements but is similar to that developed by Precup, Sutton, and Dasgupta in 2001. They proposed to use importance sampling to reweight the updates of linear TD(λ), emphasizing or de-emphasizing states as they were encountered, and thereby create a weighting equivalent to the stationary distribution under the target policy, from which the results of Tsitsiklis and Van Roy (1997) would apply and guarantee convergence. As we discuss later, this approach has very high variance and was eventually abandoned in favor of the gradient-TD approach. The new approach we explore in this paper is similar in that it also varies emphasis so as to reweight the distribution of linear TD(λ) updates, but to a different goal. The new goal is to create a weighting equivalent to the followon distribution for the target policy started in the stationary distribution of the behavior policy. The followon distribution weights states according to how often they would occur prior to termination by discounting if the target policy was followed. From this state weighting, stability of the expected update is then proved using theory similar to that originally developed for TD(λ) (Sutton 1988).\nIn this paper we first treat the simplest algorithm for which the convergence difficulties of off-policy temporal-difference (TD) learning arise—the TD(0) algorithm with linear function approximation. We examine the conditions under which the expected update of on-policy TD(0) is stable, then why those conditions do not apply under off-policy training, and finally how they can be recovered for off-policy training using established importance-sampling methods together with the emphasis idea. After introducing the basic idea of emphasis algorithms using the special case of TD(0), we then develop the general case. In particular, we consider a case with general state-dependent discounting and bootstrapping functions, and with arbitrary user-specified allocation of function approximation resources. Our new theoretical results and the emphatic TD(λ) algorithm are presented fully for this general case. Empirical examples elucidating the main theoretical results are presented in the last section prior to the conclusion."
    }, {
      "heading" : "2. On-policy Convergence of TD(0)",
      "text" : "To begin, let us review the conditions for convergence of conventional TD(λ) under on-policy training with data from a continuing finite Markov decision process. Consider the simplest function approximation case, that of linear TD(λ) with λ = 0 and constant discount-rate parameter γ ∈ [0, 1). Conventional linear TD(0) is defined by the following update to the parameter vector wt ∈ Rn, made at each of a sequence of time steps t = 0, 1, 2, . . ., on transition from state St ∈ S to state St+1 ∈ S, taking action At ∈ A and receiving reward Rt+1 ∈ R:\nwt+1 . = wt + α ( Rt+1 + γw > t x(St+1)−w>t x(St) ) x(St), (1)\nwhere α > 0 is a step-size parameter, and x(s) ∈ Rn is the feature vector corresponding to state s. The notation “ . =” indicates an equality by definition rather than one that follows from previous definitions. In on-policy training, the actions are chosen according to a target policy π : A×S→ [0, 1], where π(a|s) .= P{At=a|St=s}. The state and action sets S and A are assumed to be finite, but the number of states is assumed much larger than the number of learned parameters, |S| .= N n, so that function approximation is necessary. We use linear function approximation, in which the inner product of the parameter vector and the feature vector for a state is meant to be an approximation to the value of that state:\nw>t x(s) ≈ vπ(s) . = Eπ[Gt|St=s] , (2)\nwhere Eπ[·] denotes an expectation conditional on all actions being selected according to π, and Gt, the return at time t, is defined by\nGt . = Rt+1 + γRt+2 + γ 2Rt+3 + · · · . (3)\nThe TD(0) update (1) can be rewritten to make the stability issues more transparent: wt+1 = wt + α ( Rt+1x(St)︸ ︷︷ ︸\nbt∈Rn −x(St) (x(St)− γx(St+1))>︸ ︷︷ ︸ At∈Rn×n wt ) = wt + α(bt −Atwt) (4) = (I− αAt)wt + αbt.\nThe matrix At multiplies the parameter wt and is thereby critical to the stability of the iteration. To develop intuition, consider the special case in which At is a diagonal matrix. If any of the diagonal elements are negative, then the corresponding diagonal element of I−αAt will be greater than one, and the corresponding component of wt will be amplified, which will lead to divergence if continued. The second term (αbt) does not affect the stability of the iteration. On the other hand, if the diagonal elements of At are all positive, then α can be chosen smaller than the largest of them, such that I− αAt is diagonal with all diagonal elements between 0 and 1. In this case the first term of the update tends to shrink wt, and stability is assured. In general, wt will be reduced toward zero whenever At is positive definite.1\n1. A real matrix A is defined to be positive definite in this paper iff y>Ay > 0 for any vector y 6= 0.\nIn actuality, however, At and bt are random variables that vary from step to step, in which case stability is determined by the steady-state expectation limt→∞ E[At]. In our setting, after an initial transient, states will be visited according to the steady-state distribution under π. We represent this distribution by a vector dπ, each component of which gives the limiting probability of being in a particular state2 [dπ]s . = dπ(s) . = limt→∞ P{St=s}, which we assume exists and is positive at all states (any states not visited with nonzero probability can be removed from the problem). The special property of the steady-state distribution is that once the process is in it, it remains in it. Let Pπ denote the N×N matrix of transition probabilities [Pπ]ij . = ∑ a π(a|i)p(j|i, a) where p(j|i, a) . = P{St+1 =j|St= i, At=a}. Then the special property of dπ is that P>π dπ = dπ. (5)\nFor any algorithm that can be written in the form (4), we define its expected update as:\nu(w) . = w + α(b−Aw), (6)\nwhere A . = limt→∞ E[At] and b . = limt→∞ E[bt]. We say that the algorithm and its expected update are stable iff the A matrix is positive definite, and that they are unstable if A is not positive semi-definite. Baird’s (1995) counterexample, for example, is not technically about TD(λ), but about the expected update; in effect, he showed that TD(λ)’s A is not positive semi-definite.\nIf an algorithm is stable, then its parameter vector will not diverge and, if α is reduced according to an appropriate schedule, it may converge with probability one. If convergence does occur, it is to a parameter vector w∞ at which the expected update is zero. Thus Aw∞ = b, or w∞ = A\n−1b. (Positive definiteness of the matrix A assures the existence of its inverse.) The full technical conditions for convergence are complex and beyond the scope of this paper. Here we consider only the positive definiteness of A, which we consider a key prerequisite for stability and convergence.\nNow let us return to analyzing on-policy TD(0). Here the A matrix is\nA = lim t→∞ E[At] = lim t→∞\nEπ [ x(St) (x(St)− γx(St+1))> ] = ∑ s dπ(s) x(s) ( x(s)− γ ∑ s′ [Pπ]ss′x(s ′)\n)> = X>Dπ(I− γPπ)X,\nwhere X is the N × n matrix with the x(s) as its rows, and Dπ is the N × N diagonal matrix with dπ on its diagonal. This A matrix is typical of those we consider in this paper in that it consists of X> and X wrapped around a distinctive N × N matrix that varies with the algorithm and the setting, and which we call the key matrix. An A matrix of this form will be positive definite whenever the corresponding key matrix is positive definite.3 In this case the key matrix is Dπ(I− γPπ). 2. Here and throughout the paper we use brackets with subscripts to denote the individual elements of\nvectors and matrices. 3. Strictly speaking, positive definiteness of the key matrix assures only that A is positive semi-definite,\nbecause it is possible that Xy = 0 for some y 6= 0, in which case y>Ay will be zero as well. To rule this out, we assume, as is commonly done, that the columns of X are linearly independent (i.e., that the\nFor a key matrix of this type, positive definiteness is assured if all of its columns sum to a nonnegative number. This was shown by Sutton (1988, p. 27) based on two previously established theorems. One theorem says that any matrix A is positive definite if and only if S = A + A> is positive definite. The second theorem says that any symmetric matrix S is positive definite if all of its diagonal entries are positive and greater than or equal to the sum of the corresponding off-diagonal entries, with inequality in at least one case. For our key matrix, Dπ(I− γPπ), the diagonal entries are positive and the off-diagonal entries are negative, so all we have to show is that each row sum plus the corresponding column sum is positive or zero, with at least one positive. The row sums are all positive because Pπ is a stochastic matrix and γ < 1. Thus it only remains to show that the column sums are nonnegative. For the jth column, the sum is∑\ni [Dπ(I− γPπ)]ij = ∑ i ∑ k [Dπ]ik[I− γPπ]kj\n= ∑ i [Dπ]ii[I− γPπ]ij\n= ∑ i dπ(i)[I− γPπ]ij = [d>π (I− γPπ)]j = [d>π − γd>πPπ)]j = [d>π − γd>π ]j (by (5)) = (1− γ)dπ(j) > 0.\nThus, the key matrix and its A matrix are positive definite, and on-policy TD(0) is stable. Additional conditions and a schedule for reducing α over time (as in Tsitsiklis and Van Roy 1997) are needed to prove convergence with probability one, w∞ = A\n−1b, but the analysis above includes the most important steps that vary from algorithm to algorithm."
    }, {
      "heading" : "3. Instability of Off-policy TD(0)",
      "text" : "Before developing the off-policy setting in detail, it is useful to understand informally why TD(0) is susceptible to instability. TD learning involves learning an estimate from an estimate, which can be problematic if there is generalization between the two estimates. For example, suppose there is a transition between two states with the same feature representation except that the second is twice as big:\n2w 0 2w\nwhere here w and 2w are the estimated values of the two states—that is, their feature representations are a single feature that is 1 for the first state and 2 for the second. Now\nfeatures are not redundant), and thus that Xy = 0 only if y = 0. If this were not true, then convergence (if it occurs) may not be to a unique w∞, but rather to a subspace of parameter vectors all of which produce the same approximate value function.\nsuppose that w is 10 and the reward on the transition is 0. The transition is then from a state valued at 10 to a state valued at 20. If γ is near 1 and α is 0.1, then w will be increased to approximately 11. But then the next time the transition occurs there will be an even bigger increase in value, from 11 to 22, and a bigger increase in w, to approximately 12.1. If this transition is experienced repeatedly on its own, then the system is unstable and the parameter increases without bound—it diverges. We call this the w→2w problem.\nIn on-policy learning, repeatedly experiencing just this single problematic transition cannot happen, because, after the highly-valued 2w state has been entered, it must then be exited. The transition from it will either be to a lesser or equally-valued state, in which case w will be significantly decreased, or to an even higher-valued state which must in turn be followed by an even larger decrease in its estimated value or a still higher-valued state. Eventually, the promise of high value must be made good in the form of a high reward or else estimates will be decreased, and this ultimately constrains w and forces stability and convergence. In the off-policy case, however, if there is a deviation from the target policy then the promise is excused and need never be fulfilled. Later in this section we present a complete example of how the w→2w problem can cause instability and divergence.\nWith these intuitions, we now detail our off-policy setting. As in the on-policy case, the data is a single, infinite-length trajectory of actions, rewards, and feature vectors generated by a continuing finite Markov decision process. The difference is that the actions are selected not according to the target policy π, but according to a different behavior policy µ : A× S→ [0, 1], yet still we seek to estimate state values under π (as in (2)). Of course, it would be impossible to estimate the values under π if the actions that π would take were never taken by µ and their consequences were never observed. Thus we assume that µ(a|s) > 0 for every state and action for which π(a|s) > 0. This is called the assumption of coverage. It is trivially satisfied by any -greedy or soft behavior policy. As before we assume that there is a stationary distribution dµ(s) . = limt→∞ P{St=s} > 0,∀s ∈ S, with corresponding N -vector dµ.\nEven if there is coverage, the behavior policy will choose actions with proportions different from the target policy. For example, some actions taken by µ might never be chosen by π. To address this, we use importance sampling to correct for the relative probability of taking the action actually taken, At, in the state actually encountered, St, under the target and behavior policies:\nρt . = π(At|St) µ(At|St) . (7)\nThis quantity is called the importance sampling ratio at time t. Note that its expected value is one:\nEµ[ρt|St=s] = ∑ a µ(a|s)π(a|s) µ(a|s) = ∑ a π(a|s) = 1. (8)\nThe ratio will be exactly one only on time steps on which the action probabilities for the two policies are exactly the same; these time steps can be treated the same as in the on-policy case. On other time steps the ratio will be greater or less than one depending on whether the action taken was more or less likely under the target policy than under the behavior policy, and some kind of correction is needed.\nIn general, for any random variable Zt+1 dependent on St, At and St+1, we can recover its expectation under the target policy by multiplying by the importance sampling ratio:\nEµ[ρtZt+1|St=s] = ∑ a µ(a|s)π(a|s) µ(a|s)Zt+1\n= ∑ a π(a|s)Zt+1 = Eπ[Zt+1|St=s] , ∀s ∈ S. (9)\nWe can use this fact to begin to adapt TD(0) for off-policy learning (Precup, Sutton & Singh 2000). We simply multiply the whole TD(0) update (1) by ρt:\nwt+1 . = wt + ρt α ( Rt+1 + γw > t xt+1 −w>t xt ) xt (10)\n= wt + α ( ρtRt+1xt︸ ︷︷ ︸\nbt\n− ρtxt (xt − γxt+1)>︸ ︷︷ ︸ At wt\n) ,\nwhere here we have used the shorthand xt . = x(St). Note that if the action taken at time t is never taken under the target policy in that state, then ρt = 0 and there is no update on that step, as desired. We call this algorithm off-policy TD(0).\nOff-policy TD(0)’s A matrix is\nA = lim t→∞ E[At] = lim t→∞\nEµ [ ρtxt (xt − γxt+1)> ] = ∑ s dµ(s)Eµ [ ρtxt (xt − γxt+1)>\n∣∣∣St = s] = ∑ s dµ(s)Eπ [ xt (xt − γxt+1)>\n∣∣∣St = s] (by (9)) = ∑ s dµ(s) x(s) ( x(s)− γ ∑ s′ [Pπ]ss′x(s ′)\n)> = X>Dµ(I− γPπ)X, (11)\nwhere Dµ is the N ×N diagonal matrix with the stationary distribution dµ on its diagonal. Thus, the key matrix that must be positive definite is Dµ(I − γPπ) and, unlike in the onpolicy case, the distribution and the transition probabilities do not match. We do not have an analog of (5), P>π dµ 6= dµ, and in fact the column sums may be negative and the matrix not positive definite, in which case divergence of the parameter is likely.\nA simple w→2w example of divergence that fits the setting in this section is shown in Figure 1. From each state there are two actions, left and right, which take the process to the left or right states. All the rewards are zero. As before, there is a single parameter w and the single feature is 1 and 2 in the two states such that the approximate values are w and 2w as shown. The behavior policy is to go left and right with equal probability from both states, such that equal time is spent on average in both states, dµ = (0.5, 0.5)\n>. The target policy is to go right in both states. We seek to learn the value from each state given that\nthe right action is continually taken. The transition probability matrix for this example is:\nPπ = [ 0 1 0 1 ] .\nThe key matrix is Dµ(I− γPπ) = [ 0.5 0 0 0.5 ] × [ 1 −0.9 0 0.1 ] = [ 0.5 −0.45 0 0.05 ] . (12)\nWe can see an immediate indication that the key matrix may not be positive definite in that the second column sums to a negative number. More definitively, one can show that it is not positive definite by multiplying it on both sides by y = X = (1, 2)>:\nX>Dµ(I− γPπ)X = [ 1 2 ] × [ 0.5 −0.45 0 0.05 ] × [ 1 2 ] = [ 1 2 ] × [ −0.4 0.1 ] = −0.2.\nThat this is negative means that the key matrix is not positive definite. We have also calculated here the A matrix; it is this scalar, A = −0.2. Clearly, this expected update and algorithm are not stable.\nIt is also easy to see the instability of this example more directly, without matrices. We know that only transitions under the right action cause updates, as ρt will be zero for the others. Assume for concreteness that initially wt = 10 and α = 0.1. On a right transition from the first state the update will be\nwt+1 = wt + ρtα ( Rt+1 + γw > t xt+1 −w>t xt ) xt\n= 10 + 2 · 0.1 (0 + 0.9 · 10 · 2− 10 · 1) 1 = 10 + 1.6,\nwhereas, on a right transition from the second state the update will be wt+1 = wt + ρtα ( Rt+1 + γw > t xt+1 −w>t xt ) xt\n= 10 + 2 · 0.1 (0 + 0.9 · 10 · 2− 10 · 2) 2 = 10− 0.8.\nThese two transitions occur equally often, so the net change will be positive. That is, w will increase, moving farther from its correct value, zero. Everything is linear in w, so the next time around, with a larger starting w, the increase in w will be larger still, and divergence occurs. A smaller value of α would not prevent divergence, only reduce its rate."
    }, {
      "heading" : "4. Off-policy Stability of Emphatic TD(0)",
      "text" : "The deep reason for the difficulty of off-policy learning is that the behavior policy may take the process to a distribution of states different from that which would be encountered under the target policy, yet the states might appear to be the same or similar because of function approximation. Earlier work by Precup, Sutton and Dasgupta (2001) attempted to completely correct for the different state distribution using importance sampling ratios to reweight the states encountered. It is theoretically possible to convert the state weighting from dµ to dπ using the product of all importance sampling ratios from time 0, but in practice this approach has extremely high variance. It works in theory because then the key matrix is Dπ(I− γPπ) again, which we know to be positive definite.\nMost subsequent works abandoned the idea of completely correcting for the state distribution. For example, the work on gradient-TD methods (e.g., Sutton et al. 2009, Maei 2011) seeks to minimize the mean-squared projected Bellman error weighted by dµ. We call this an excursion setting because we can think of the contemplated switch to the target policy as an excursion from the steady-state distribution of the behavior policy, dµ. The excursions would start from dµ and then follow π until termination, followed by a resumption of µ and thus a gradual return to dµ. Of course these excursions never actually occur during off-policy learning, they are just contemplated, and thus the state distribution in fact never leaves dµ. It is the excursion view that we take in this paper, but still we use techniques similar to those introduced by Precup et al. (2001) to determine an emphasis weighting that corrects for the state distribution, only toward a different goal.\nThe excursion notion suggests a different weighting of TD(0) updates. We consider that at every time step we are beginning a new contemplated excursion from the current state. The excursion thus would begin in a state sampled from dµ. If an excursion started it would pass through a sequence of subsequent states and actions prior to termination. Some of the actions that are actually taken (under µ) are relatively likely to occur under the target policy as compared to the behavior policy, while others are relatively unlikely; the corresponding states can be appropriately reweighted based on importance sampling ratios. Thus, there will still be a product of importance sampling ratios, but only since the beginning of the excursion, and the variance will also be tamped down by the discounting; the variance will be much less than in the earlier approach. This is the simplest case of an off-policy emphasis algorithm: the update at time t is emphasized or de-emphasized proportional to a new scalar variable Ft, defined byF−1 = 0 and\nFt . = γρt−1Ft−1 + 1, ∀t > 0, (13)\nwhich we call the followon trace. Specifically, we define emphatic TD(0) by the following update:\nwt+1 . = wt + αFtρt ( Rt+1 + γw > t xt+1 −w>t xt ) xt (14)\n= wt + α ( FtρtRt+1xt︸ ︷︷ ︸\nbt\n−Ftρtxt (xt − γxt+1)>︸ ︷︷ ︸ At wt\n)\nEmphatic TD(0)’s A matrix is\nA = lim t→∞ E[At] = lim t→∞\nEµ [ Ftρtxt (xt − γxt+1)> ] = ∑ s dµ(s) lim t→∞ Eµ [ Ftρtxt (xt − γxt+1)>\n∣∣∣St = s] = ∑ s dµ(s) lim t→∞\nEµ[Ft|St = s]︸ ︷︷ ︸ f(s)\nEµ [ ρtxt (xt − γxt+1)> ∣∣∣St = s]\n= ∑ s f(s)Eπ [ xt (xt − γxt+1)> ∣∣∣St = s] (by (9)) = ∑ s f(s) x(s) ( x(s)− γ ∑ s′ [Pπ]ss′x(s ′)\n)> = X>F(I− γPπ)X, (15)\nwhere F is a diagonal matrix with diagonal elements f(s) . = dµ(s) limt→∞ Eµ[Ft|St=s], which we exists. As we show later, the vector f ∈ RN with components [f ]s .= f(s) can be written as\nf = dµ + γP > π dµ + ( γP>π )2 dµ + · · · (16)\n= ( I− γP>π )−1 dµ. (17)\nThe key matrix is F (I− γPπ), and the sum of its jth column is∑ i [F(I− γPπ)]ij = ∑ i ∑ k [F]ik[I− γPπ]kj\n= ∑ i [F]ii[I− γPπ]ij\n= ∑ i [f ]i[I− γPπ]ij = [f>(I− γPπ)]j = [d>µ (I− γPπ)−1(I− γPπ)]j (using (17)) = [d>µ ]j\n= dµ(j)\n> 0.\nThus, the key matrix is positive definite and the expected update is stable. Emphatic TD(0) is the simplest TD algorithm with linear function approximation proven to be stable under off-policy training.\nThe w → 2w example presented earlier in Figure 1 provides some insight into how replacing Dµ by F changes the key matrix to make it positive definite. In general, f is the expected number of time steps that would be spent in each state during an excursion starting\nfrom the behavior distribution dµ. From (16), it is dµ plus where you would get to in one step from dµ, plus where you would get in two steps, etc., with appropriate discounting. In the example, excursions under the target policy take you to the second state (2w) and leave you there. You are only in the first state (w) if you start there, and only for one step, so f(1) = dµ(1) = 0.5. For the second state, you can either start there, with probability 0.5, or you can get there on the second step (certain except for discounting), with probability 0.9, or on the third step, with probability 0.92, etc, so f(2) = 0.5 + 0.9 + 0.92 + 0.93 + · · · = 0.5 + 0.9 · 10 = 9.5. Thus, the key matrix is now\nF(I− γPπ) = [ 0.5 0 0 9.5 ] × [ 1 −0.9 0 0.1 ] = [ 0.5 −0.45 0 0.95 ] . (18)\nNote that because F is a diagonal matrix, its only effect is to scale the rows. Here it emphasizes the lower row by more than a factor of 10 compared to the upper row, thereby causing the key matrix to have positive column sums and be positive definite. The F matrix emphasizes the second state, which would occur much more often under the target policy than it does under the behavior policy."
    }, {
      "heading" : "5. The General Case",
      "text" : "We turn now to a more general case of off-policy learning with linear function approximation. The objective is still to evaluate a policy π from a single trajectory under a different policy µ, but now the value of a state is defined not with respect to a constant discount rate γ ∈ [0, 1], but with respect to a discount rate that varies from state to state according to a discount function γ : S → [0, 1] such that ∏∞k=1 γ(St+k) = 0,w.p.1, ∀t. That is, our approximation is still defined by (2), but now (3) is replaced by\nGt . = Rt+1 + γ(St+1)Rt+2 + γ(St+1)γ(St+2)Rt+3 + · · · , (19)\nState-dependent discounting specifies a temporal envelope within which received rewards are accumulated. If γ(St) = 0, then the time of accumulation is fully terminated at t, and if γ(St) < 1, then it is partially terminated. We call both of these soft termination because they are like the termination of an episode, but the actual trajectory is not affected. Soft termination ends the accumulation of rewards into a return, but the state transitions continue oblivious to the termination.\nSoft termination is particularly natural in the excursion setting, where it makes it easy to define excursions of finite and definite duration. For example, consider the deterministic MDP shown in Figure 2. There are five states, three of which do not discount at all, γ(s) = 1, and are shown as circles, and two of which cause complete soft termination, γ(s) = 0, and are shown as squares. The terminating states do not end anything other than the return; actions are still selected in them and, dependent on the action selected, they transition to next states indefinitely without end. In this MDP there are two actions, left and right, which deterministically cause transitions to the left or right except at the edges, where there may be a self transition. The reward on all transitions is +1. The behavior policy is to select left 2/3rds of the time in all states, which causes more time to be spent in states on the left than on the right. The stationary distribution can be shown\nto be dµ ≈ (0.52, 0.26, 0.13, 0.06, 0.03)>; more than half of the time steps are spent in the leftmost terminating state.\nConsider the target policy π that selects the right action from all states. The correct value vπ(s) of each state s is written above it in the figure. For both of the two rightmost states, the right action results in a reward of 1 and an immediate termination, so their values are both 1. For the middle state, following π (selecting right repeatedly) yields two rewards of 1 prior to termination. There is no discounting (γ=1) prior to termination, so the middle state’s value is 2, and similarly the values go up by 1 for each state to its left, as shown. These are the correct values. The approximate values depend on the parameter vector wt as suggested by the expressions shown inside each state in the figure. These expressions use the notation wi to denote the ith component of the current parameter vector wt. In this example, there are five states and only three parameters, so it is unlikely, and indeed impossible, to represent vπ exactly. We will return to this example later in the paper.\nIn addition to enabling definitive termination, as in this example, state-dependent discounting enables a much wider range of predictive questions to be expressed in the form of a value function (Sutton et al. 2011, Modayil, White & Sutton 2014, Sutton, Rafols & Koop 2006), including option models (Sutton, Precup & Singh 1999, Sutton 1995). For example, with state-dependent discounting one can formulate questions both about what will happen during a way of behaving and what will be true at its end. A general representation for predictions is a key step toward the goal of representing world knowledge in verifiable predictive terms (Sutton 2009, 2012). The general form is also useful just because it enables us to treat uniformly many of the most important episodic and continuing special cases of interest.\nA second generalization, developed for the first time in this paper, is to explicitly specify the relative interest in accurately valuing different states. Recall that in parametric function approximation there are typically many more states than parameters (N n), and thus it is not possible to value all of them accurately. Valuing some states more accurately inherently means valuing others less accurately, at least asymptotically. In the tabular case where much of the theory of reinforcement learning originated, this tradeoff is not an issue because the estimates of each state are independent of each other, but with function approximation it is necessary to specify relative interest in order to make the problem well defined. Nevertheless, in the function approximation case little attention has been paid in the literature to specifing the relative importance of different states (an exception is Thomas 2014), though there are intimations of this in the initiation set of options (Sutton, Precup & Singh 1999). In the past it was typically assumed that we were interested in valuing states in direct proportion to how often they occur, but this is not always the case. For example, in episodic problems we often care primarily about the value of the first state, or\nof earlier states generally (as discussed by Thomas 2014). Here we allow the user to specify the relative interest in each state with a nonnegative interest function i : S → [0,∞). Our objective is to minimize the Mean Square Value Error (MSVE) with states weighted both by how often they occur and by our interest in them:\nMSVE(w) . = ∑ s∈S dµ(s)i(s) ( vπ(s)−w>x(s) )2 . (20)\nFor example, in the 5-state example in Figure 2, we could choose i(s) = 1,∀s ∈ S, in which case we would be primarily interested in attaining low error in the states on the left side, which are visited much more often under the behavior policy. If we want to counter this, we might chose i(s) larger for states toward the right. Of course, with parametric function approximation we presumably do not have access to the states as individuals, but certainly we could set i(s) as a function of the features in s. In this example, choosing i(s) = 1 + x2(s) + 2x3(s) (where xi(s) denotes the ith component of x(s)) would shift the focus on accuracy among the states to the right, making it substantially more balanced.\nThe third and final generalization that we introduce in this section is general bootstrapping. Conventional TD(λ) uses a bootstrapping parameter λ ∈ [0, 1]; we generalize this to a bootstrapping function λ : S → [0, 1] specifying a potentially different degree of bootstrapping, 1− λ(s), for each state s. As a notational shorthand, let us use λt .= λ(St) and γt . = γ(St). Then we can define a general notion of bootstrapped return, the λ-return with state-dependent bootstrapping and discounting:\nGλt . = Rt+1 + γt+1 [ (1− λt+1)w>t xt+1 + λt+1Gλt+1 ] .\nThe λ-return plays a key role in the theoretical understanding of TD methods, in particular, in their forward views (Sutton & Barto 1998, Sutton, Mahmood, Precup & van Hasselt 2014). In the forward view, Gλt is thought of as the target for the update at time t, even though it is not available until many steps later (when complete termination γ(Sk) = 0 has occurred for the first time for some k > t). General bootstrapping of this form has been partially developed in several previous works (Sutton 1995, Sutton & Barto 1998, Maei & Sutton 2010, Sutton et al. 2014).\nGiven these generalizations, we can now specify our final new algorithm, emphatic TD(λ), by the following four equations:\nwt+1 . = wt + α ( Rt+1 + γt+1w > t xt+1 −w>t xt ) et (21)\net . = ρt (γtλtet−1 +Mtxt) , with e−1 . = 0 (22) Mt . = λt i(St) + (1− λt)Ft (23)\nFt . = ρt−1γtFt−1 + i(St), with F−1 . = 0, (24)\nwhere Ft ≥ 0 is a scalar memory called the followon trace. The quantity Mt ≥ 0 is termed the emphasis on step t."
    }, {
      "heading" : "6. Off-policy Stability of Emphatic TD(λ)",
      "text" : "As usual, to analyze the stability of the new algorithm we examine its A matrix. The stochastic update can be written:\nwt+1 . = wt + α ( Rt+1 + γt+1w > t xt+1 −w>t xt ) et\n= wt + α (\netRt+1︸ ︷︷ ︸ bt − et (xt − γt+1xt+1)>︸ ︷︷ ︸ At wt\n) .\nThus,\nA = lim t→∞ E[At] = lim t→∞\nEµ [ et (xt − γt+1xt+1)> ] = ∑ s dµ(s) lim t→∞ Eµ [ et (xt − γt+1xt+1)>\n∣∣∣St=s] = ∑ s dµ(s) lim t→∞ Eµ [ ρt (γtλtet−1 +Mtxt) (xt − γt+1xt+1)>\n∣∣∣St=s] = ∑ s dµ(s) lim t→∞\nEµ[(γtλtet−1 +Mtxt)|St=s]︸ ︷︷ ︸ e(s)∈Rn\nEµ [ ρt (xt − γxt+1)> ∣∣∣St=s]\n= ∑ s e(s)Eπ[xt − γxt+1|St=s]> (by (9))\n= ∑ s e(s)\n( x(s)− γ\n∑ s′ [Pπ]ss′x(s ′) )> = E(I− γPπ)X, (25)\nwhere E is an N × n matrix E> .= [e(1), · · · , e(N)], and e(s) ∈ Rn is defined by4:\ne(s) . = dµ(s) lim\nt→∞ Eµ[γtλtet−1 +Mtxt|St=s] (assuming this exists)\n= dµ(s) lim t→∞ Eµ[Mt|St=s]︸ ︷︷ ︸ m(s) xt + γ(s)λ(s)dµ(s) lim t→∞ Eµ[et−1|St=s]\n= m(s)x(s)+γ(s)λ(s)dµ(s) ∑ s̄,ā P{St−1= s̄, At−1= ā|St=s} lim t→∞ Eµ[et−1|St−1= s̄, At−1= ā]\n= m(s)x(s) + γ(s)λ(s)dµ(s) ∑ s̄,ā dµ(s̄)µ(ā|s̄)p(s|s̄, ā) dµ(s) lim t→∞ Eµ[et−1|St−1 = s̄, At−1 = ā]\n=m(s)x(s)+γ(s)λ(s) ∑ s̄,ā dµ(s̄)µ(ā|s̄)p(s|s̄, ā) π(ā|s̄) µ(ā|s̄) limt→∞Eµ[γt−1λt−1et−2+Mt−1φt−1|St−1=s̄]\n= m(s)x(s) + γ(s)λ(s) ∑ s̄ (∑ ā π(ā|s̄)p(s|s̄, ā) ) e(s̄)\n4. Note that this is a slight abuse of notation; et is a vector random variable, one per time step, and e(s) is a vector expectation, one per state.\n= m(s)x(s) + γ(s)λ(s) ∑ s̄ [Pπ]s̄se(s̄).\nWe now introduce three N ×N diagonal matrices: M, which has the m(s) .= dµ(s) limt→∞ Eµ[Mt|St=s] on its diagonal; Γ, which has the γ(s) on its diagonal; and Λ, which has the λ(s) on its diagonal. With these we can write the equation above entirely in matrix form, as\nE> = X>M + E>PπΓΛ\n= X>M + X>MPπΓΛ + X >M(PπΓΛ) 2 + · · · = X>M(I−PπΓΛ)−1.\nFinally, combining this equation with (25) we obtain\nA = X>M(I−PπΓΛ)−1(I−PπΓ)X, (26) and through similar steps one can also obtain emphatic TD(λ)’s b vector,\nb = Erπ = X >M(I−PπΓΛ)−1rπ, (27)\nwhere rπ is the N -vector of expected immediate rewards from each state under π. Emphatic TD(λ)’s key matrix, then, is M(I − PπΓΛ)−1(I − PπΓ). To prove that it is positive definite we will follow the same strategy as we did for emphatic TD(0). The first step will be to write the last part of the key matrix in the form of the identity matrix minus a probability matrix. To see how this can be done, consider a slightly different setting in which actions are taken according to π, and in which 1− γ(s) and 1− λ(s) are considered probabilities of ending by terminating or by bootstrapping, respectively. That is, for any starting state, a trajectory involves a state transition according to Pπ, possibly terminating according to I−Γ, then possibly ending with a bootstrapping event according to I−Λ, and then, if neither of these occur, continuing with another state transition and more chances to end, and so on until an ending of one of the two kinds occurs. For any start state i ∈ S, consider the probability that the trajectory ends in state j ∈ S with an ending event of the bootstrapping kind (according to I−Λ). Let Pλπ be the matrix with this probability as its ijth component. This matrix can be written\nPλπ = PπΓ(I−Λ) + PπΓΛPπΓ(I−Λ) + PπΓ(ΛPπΓ)2(I−Λ) + · · · (28)\n= ( ∞∑ k=0 (PπΓΛ) k ) PπΓ(I−Λ) = (I−PπΓΛ)−1PπΓ(I−Λ) = (I−PπΓΛ)−1(PπΓ−PπΓΛ) = (I−PπΓΛ)−1(PπΓ− I + I−PπΓΛ) = I− (I−PπΓΛ)−1(I−PπΓ). (29)\nThus,\nM(I−Pλπ) = M(I− (I− (I−PπΓΛ)−1(I−PπΓ) = M(I−PπΓΛ)−1(I−PπΓ), (30)\nis another way of writing emphatic TD(λ)’s key matrix (cf. (26)). This gets us considerably closer to our goal of proving that the key matrix is positive definite. It is now immediate that its diagonal entries are nonnegative (with at least one positive) and that its off diagonal entries are nonpositive. It is also immediate that its row sums are nonnegative (with at least one positive).\nThere remains what is typically the hardest condition to satisfy: that the column sums are nonnegative. To show this we have to analyze M, and to do that we first analyze the N -vector f with components f(s) . = dµ(s) limt→∞ Eµ[Ft|St=s] (we assume that this limit and expectation exist). Analyzing f will also pay the debt we incurred in Section 3 when we claimed without proof that f (in the special case treated in that section) was as given by (17). In the general case:\nf(s) = dµ(s) lim t→∞\nEµ[Ft|St=s]\n= dµ(s) lim t→∞\nEµ[i(St) + ρt−1γtFt−1|St=s]\n= dµ(s)i(s) + dµ(s)γ(s) ∑ s̄, ā P{St−1 = s̄, At−1 = ā|St=s} π(ā|s̄) µ(ā|s̄) limt→∞Eµ[Ft−1|St−1 = s̄]\n= dµ(s)i(s) + dµ(s)γ(s) ∑ s̄, ā dµ(s̄)µ(ā|s̄)p(s|s̄, ā) dµ(s) π(ā|s̄) µ(ā|s̄) limt→∞Eµ[Ft−1|St−1 = s̄]\n= dµ(s)i(s) + γ(s) ∑ s̄, ā π(ā|s̄)p(s|s̄, ā)dµ(s̄) lim t→∞ Eµ[Ft−1|St−1 = s̄]\n= dµ(s)i(s) + γ(s) ∑ s̄ [Pπ]s̄sf(s̄).\nThis equation can be written in matrix-vector form, letting i be the N -vector with components [i]s . = dµ(s)i(s):\nf = i + ΓP>π f\n= i + ΓP>π i + (ΓP > π ) 2i + · · · (31) = ( I− ΓP>π )−1 i.\nThis proves (17), because there i(s) = 1,∀s (thus i = dµ), and γ(s) = γ,∀s. We are now ready to analyze M, the diagonal matrix with the m(s) on its diagonal:\nm(s) = dµ(s) lim t→∞\nEµ[Mt|St=s]\n= dµ(s) lim t→∞\nEµ[λt i(St) + (1− λt)Ft|St=s] (by (23))\n= dµ(s)λ(s)i(s) + (1− λ(s)) f(s),\nor, in matrix-vector form, letting m be the N -vector with components m(s),\nm = Λi + (I−Λ)f = Λi + (I−Λ) ( i + ΓP>π i + (ΓP > π ) 2i + · · · )\n(using (31)) = Λi + i−Λi + (I−Λ) ( ΓP>π i + (ΓP > π ) 2i + · · · )\n= i + (I−Λ) ( ΓP>π + (ΓP > π ) 2 + · · · ) i\n= i + (I−Λ)ΓP>π ( I + ΓP>π + (ΓP > π ) 2 + · · · ) i\n= i + ( ΓP>π −ΛΓP>π )( I− ΓP>π )−1 i\n= i + ( (I−ΛΓP>π )− (I− ΓP>π ) )( I− ΓP>π )−1 i\n= i + ( I−ΛΓP>π )( I− ΓP>π )−1 i− i\n= ( I−ΛΓP>π )( I− ΓP>π )−1 i (32)\n= (( (I−PπΓΛ)−1 (I−PπΓ) )>)−1 i\n= ( I− ( I− (I−PπΓΛ)−1 (I−PπΓ) )>)−1 i\n= ( I−Pλπ >)−1 i. (using (29))\nNow we are ready for the final step of the proof, showing that all the columns of the key matrix M(I −Pλπ) sum to a positive number or zero. With the steps we have already done, this is immediate and after the pattern of Sutton (1988), as in the emphatic TD(0) section. The sum of the jth column is∑\ni [M(I−Pλπ)]ij = ∑ i ∑ k [M]ik[I−Pλπ]kj\n= ∑ i [M]ii[I−Pλπ]ij\n= ∑ i [m]i[I−Pλπ]ij = [m>(I−Pλπ)]j = [i>(I−Pλπ)−1(I−Pλπ)]j = [i>]j\n= dµ(j)i(j) ≥ 0,\nwith inequality for at least one j. This completes the proof that the key matrix is positive definite and thus that general emphatic TD(λ) and its expected update are stable. This\nresult can be summarized in the following theorem, the main result of this paper, which we have just proved:\nTheorem 1 (Stability of Emphatic TD(λ)) For any\n• Markov decision process {St, At, Rt+1}∞t=0 with finite state and actions sets S and A,\n• behavior policy µ with a stationary invariant distribution dµ(s) > 0,∀s ∈ S,\n• target policy π with coverage, i.e., s.t., if π(a|s) > 0, then µ(a|s) > 0,\n• discount function γ : S→ [0, 1] s.t. ∏∞k=1 γ(St+k) = 0,w.p.1,∀t > 0, • bootstrapping function λ : S→ [0, 1],\n• interest function i : S→ [0,∞) with i(s) > 0 for at least one s ∈ S,\n• feature function x : S → Rn s.t. the matrix X ∈ R|S|×n with the x(s) as its rows has linearly independent columns,\nthe A matrix of linear emphatic TD(λ) (as given by (21–24), and assuming the existence of limt→∞ E[Ft|St= s] and limt→∞ E[et|St= s], ∀s ∈ S),\nA = lim t→∞ Eµ[At] = lim t→∞\nEµ [ et (xt − γt+1xt+1)> ] = X>M(I−Pλπ)X, (33)\nis positive definite. Thus the algorithm and its expected update are stable.\nStability in this sense is generally necessary but not sufficient to guarantee convergence of the parameter vector wt as the step-size parameter α is reduced appropriately over time. However, if convergence does occur, it will be to the vector w∞ at which the expected update (6) is zero, in other words, to\nAw∞ = b or w∞ = A −1b. (34)\nThis solution can be characterized as a minimum (in fact, a zero) of the Projected Bellman Error (PBE, Sutton et al. 2009) using the λ-dependent Bellman operator T (λ) : RN → RN (Tsitiklis & Van Roy 1997) and the weighting of states according to their emphasis. For our general case, we need a version of the T (λ) operator extended to state-dependent discounting and bootstrapping. This operator looks ahead to future states to the extent that they are bootstrapped from, that is, according to Pλπ, taking into account the reward received along the way. The appropriate operator, in vector form, is\nT (λ)v . = (I−PπΓΛ)−1rπ + Pλπv.\nThis operator is a contraction with fixed point v = vπ. Recall that our approximate value function is Xw, and thus the difference between Xw and T (λ)(Xw) is a Bellmanerror vector. The projection of this with respect to the feature matrix and the emphasis\nweighting is the emphasis-weighted PBE:\nPBE(w) . = Π ( T (λ)(Xw)−Xw ) . = X(X>MX)−1X>M ( T (λ)(Xw)−Xw ) (see Sutton et al. 2009)\n= X(X>MX)−1X>M ( (I−PπΓΛ)−1rπ + PλπXw −Xw )\n= X(X>MX)−1 ( b + X>M(Pλπ − I)Xw ) (by (27)) = X(X>MX)−1 (b−Aw) . (by (33))\nFrom (34), b−Aw∞ is zero, so it is immediate that PBE(w∞) = 0."
    }, {
      "heading" : "7. Derivation of the Emphasis Algorithm",
      "text" : "The emphasis algorithm is based on the idea that if we are updating a state by a TD method, then we should also update each state that it bootstraps from, in direct proportion. For example, suppose we decide to make an update at time t with unit strength, perhaps because i(St) = 1, then at time t + 1 we have γ(St+1) = 1 and λ(St+1) = 0. Because of the latter, we are fully bootstrapping from the value estimate at t + 1 and thus we should also make an update at time t+ 1 with emphasis equal to t’s emphasis. If instead λ(St+1) = 0.5, then t+ 1 would gain a half unit of emphasis, and the remaining half would still be available to allocate to t+ 2, or to later times, depending on their λs. And of course there may be some emphasis allocated directly to t + 1 if i(St+1) > 0. Discounting and importance sampling also have an effect. At each step t, if γ(St) < 1, then there is some degree of termination and to that extent there is no longer any chance of bootstrapping from later time steps. Another way bootstrapping may be cut off is if ρt = 0 (a complete deviation from the target policy). More generally, if ρ 6= 1, then the opportunity for bootstrapping is scaled up or down proportionally.\nIt may seem difficult to work out precisely how each time step’s estimates bootstrap from which later states’ estimates for all cases. Fortunately, it has already been done. Equation (6) of the paper by Sutton, Mahmood, Precup, and van Hasselt (2014) specifies this in their “forward view” of off-policy TD(λ) with general state-dependent discounting and bootstrapping. From this equation (and their (5)) it is easy to determine the degree to which the update to the value estimate at time k bootstraps from and thus depends on the value estimates of each subsequent time t. It is\nρk ( t−1∏ i=k+1 γiλiρi ) γt(1− λt).\nIt follows then that the total emphasis on time t, Mt, should be the sum of this quantity for all times k < t, each times the emphasis Mk for those earlier times, plus any intrinsic\ninterest i(St) in time t:\nMt . = i(St) + t−1∑ k=0 Mkρk ( t−1∏ i=k+1 γiλiρi ) γt(1− λt)\n= λti(St) + (1− λt)i(St) + (1− λt)γt t−1∑ k=0 ρkMk t−1∏ i=k+1 γiλiρi = λti(St) + (1− λt)Ft,\nwhich is (23), where\nFt . = i(St) + γt t−1∑ k=0 ρkMk t−1∏ i=k+1 γiλiρi\n= i(St) + γt ( ρt−1Mt−1 +\nt−2∑ k=0 ρkMk t−1∏ i=k+1 γiλiρi\n)\n= i(St) + γt ( ρt−1Mt−1 + ρt−1λt−1γt−1\nt−2∑ k=0 ρkMk t−2∏ i=k+1 γiλiρi\n)\n= i(St) + γtρt−1 ( λt−1i(St−1) + (1− λt−1)Ft−1︸ ︷︷ ︸\nMt−1\n+λt−1γt−1 t−2∑ k=0 ρkMk t−2∏ i=k+1 γiλiρi\n)\n= i(St) + γtρt−1 ( Ft−1 + λt−1 ( −Ft−1 + i(St−1) + γt−1\nt−2∑ k=0 ρkMk t−2∏ i=k+1\nγiλiρi︸ ︷︷ ︸ Ft−1\n))\n= i(St) + γtρt−1Ft−1,\nwhich is (24), completing the derivation of the emphasis algorithm."
    }, {
      "heading" : "8. Empirical Examples",
      "text" : "In this section we present empirical results with example problems that verify and elucidate the formal results already presented. A thorough empirical comparison of emphatic TD(λ) with other methods is beyond the scope of the present article.\nThe main focus in this paper, as in much previous theory of TD algorithms with function approximation, has been on the stability of the expected update. If an algorithm is unstable, as Q-learning and off-policy TD(λ) are on Baird’s (1995) counterexample, then there is no chance of its behaving in a satisfactory manner. On the other hand, stability of the expected update does not imply convergence, and in fact emphatic TD(λ) does not always converge. It is stable and will never diverge, but there are cases where it does not converge in a conventional sense. Off-policy algorithms involve products of potentially an infinite number of importance-sampling ratios, which can lead to fluxuations of infinite variance that cannot be averaged away even if the step-size parameter is reduced towards zero.\nAs an example of what can happen, let’s look again at the w→ 2w problem shown in Figure 1 (and shown again in the upper left of Figure 3 below). Consider what happens to Ft in this problem if we have interest only in the first state, and the right action happens to be taken on every step (i.e., i(S0) = 1 then i(St) = 0,∀t > 0, and At = right,∀t ≥ 0). In this case, from (24),\nFt = ρt−1γtFt−1 + i(St) = t−1∏ j=0 ρjγ = (2 · 0.9)t,\nwhich of course goes to infinity as t→∞. Of course, the probability of this specific infinite action sequence is zero, and in fact Ft will rarely take on very high values. In particular, the expected value of Ft remains finite at\nEµ[Ft] = 0.5 · 2 · 0.9 · Eµ[Ft−1] + 0.5 · 0 · 0.9 · Eµ[Ft−1] = 0.9 · Eµ[Ft−1] = 0.9t,\nwhich tends to zero as t → ∞. Nevertheless, this problem is indeed a difficult case, as the variance of Ft is infinite:\nVar[Ft] = E [ F 2t ] − (E[Ft])2\n= 0.5t(2t0.9t)2 − (0.9t)2\n= (0.92 · 2)t − (0.92)t = 1.62t − 0.81t,\nwhich tends to ∞ as t→∞. So what actually happens on this problem? The thin blue lines in Figure 3 (left) show the trajectories of the single parameter w over time in 50 runs with this problem with λ=0 and α=0.001, starting at w=1.0. We see that most trajectories of emphatic TD(0) rapidly approach the correct value of w= 0, but a few make very large steps away from zero and then return. Because the variance of Ft (and thus of Mt and et) is infinite, true convergence never occurs; there is always a small chance of an extremely large fluxuation taking w far away from zero. This is not convergence, but neither is it divergence or instability. Offpolicy TD(0), on the other hand, does diverge to infinity both in its expected update and in all individual runs, as shown.\nFor comparison, Figure 3 (right) shows trajectories for a w→ 2w problem in which Ft and all the other variables and their variances are bounded. In this problem, the target policy of selecting right on all steps leads to a soft terminal state (γ(s) = 0) with fixed value zero, which then transitions back to start again in the leftmost state, as shown in the upper right of the figure. (This is an example of how one can reproduce the conventional notions of terminal state and episode in a soft termination setting.) Here we have chosen the behavior policy to take the action left with probability 0.9, so that its stationary distribution distinctly favors the left state, whereas the target policy would spend equal time in each of the two states. This change increases the variance of the updates, so we used a smaller\nstep size, α = 0.0001; other settings were unchanged. Conventional off-policy TD(0) still diverges in this case, but emphatic TD(0) converges reliably to zero.\nFinally, Figure 4 shows trajectories for the 5-state example shown earlier (and again in the upper part of the figure). In this case, everything is bounded under the target policy, and both algorithms converge. The emphatic algorithm achieves a lower MSVE in this example (but we do not mean to claim any general empirical advantage for emphatic TD(λ) at this time).\nAlso shown in these figures as a thick dark line is the trajectory of the deterministic expected-update algorithm: wt+1 = u(wt). Tsitsiklis and Van Roy (1997) argued that, for small step-size parameters and in the steady-state distribution, on-policy TD(λ) follows its expected-update algorithm in an “average” sense, and we see much the same here for emphatic TD(λ).\nThese examples show that although emphatic TD(λ) is stable for any MDP and all functions λ, γ, and i, for some problems and functions the parameter vector itself does not converge, but instead continues to fluxuate with a chance of arbitrarily large deviations even if the step-size parameter, α, is reduced asymptotically towards zero. It is not clear how great of a problem this non-convergence is. Certainly it is much less of a problem than the actual divergence that can occur with off-policy TD(λ) (stability of the expected update precludes this). It is worth noting that the same kind of non-convergence arises with gradient-TD methods with long eligibility traces. For example, the updates of GTD(λ) and GQ(λ) with λ = 1 will be of infinite variance on Baird’s counterexample and on the example in Figure 1, and thus on these problems their parameter vector will converge only\nin expected value and not with probability one. Nevertheless, gradient-TD methods should probably be considered slightly better than emphatic TD(λ) in this regard because for them convergence can always be guaranteed by choosing λ = 0 (and by making appropriate choices for the two step-size parameters), whereas for emphatic TD(λ) this choice merely shifts the accumulation of high variance from the eligibility trace to the emphasis.\nAre there other algorithmic variations or choices that we can make which would ensure convergence of emphatic TD(λ)? First, note that we have not formally established minimal conditions that ensure emphatic TD(λ)’s convergence. It is not difficult to ensure convergence if Ft and et are assumed to be bounded, but weaker conditions, such as finite variance, may be sufficient. We have chosen not to present a conventional convergence proof in this paper in part to save space, but also in part because the assumptions that we would have to make at the current time are significantly stronger than what we believe is necessary. We conjecture that convergence of the parameter can be assured whenever the eligibility trace has finite variance. Second, note that infinite variance probably should not be considered the only or even the most important problem. In practice, very high variance can be just as problematic in that it would imply a very small step-size parameter and thus very slow learning.\nHigh variance frequently arises in off-policy algorithms when they are Monte Carlo algorithms (no TD learning) or they have eligibility traces with high λ (at λ = 1, TD algorithms become Monte Carlo algorithms). In both cases the root problem is the same:\nimportance sampling ratios that become very large when multiplied together. For example, in the w→ 2w problem discussed at the beginning of this section, the ratio was only two, but the products of successive twos rapidly produced a very large Ft. Thus, the first way in which variance can be controlled is to ensure that large products cannot occur. We are actually concerned with products of both ρts and γts. Occasional termination (γ = 0), as in the 5-state problem, is thus one reliable way of preventing high variance. Another is through choice of the target and behavior policies that together determine the importance sampling ratios. For example, one could define the target policy to be equal to the behavior policy whenever the followon or eligibility traces exceeds some threshold. These tricks can also be done prospectively. White (in preparation) proposed that the learner compute at each step the variance of what GTD(λ)’s traces would be on the following step. If the variance is in danger of becoming too large, then λt is reduced for that step to prevent it. For emphatic TD(λ), the same conditions could be used to adjust γt or one of the policies to prevent the variance from growing too large. Another idea for reducing variance is to use weighted importance sampling, which Precup et al. (2001) proved converges robustly even with infinite variance samples, together with the ideas of Mahmood et al. (in press) for extending weighted importance sampling to linear function approximation. Finally, a good solution may even be found by something as simple as bounding the values of Ft or et. This would limit variance at the cost of bias, which might be a good tradeoff if done properly."
    }, {
      "heading" : "9. Conclusions and Future Work",
      "text" : "We have introduced a way of varying the emphasis or strength of the update of TD learning algorithms from step to step, based on importance sampling, that should result in much lower variance than previous methods (Precup et al. 2001). In particular, we have introduced the emphatic TD(λ) algorithm and shown that it solves the problem of instability that plagues conventional TD(λ) when applied in off-policy training situations in conjunction with linear function approximation. Compared to gradient-TD methods, emphatic TD(λ) is simpler in that it has a single weight vector and a single step size rather than two of each. Both methods may undergo large fluxuations even when their expected updates are stable, but so far it appears easier to prevent this with gradient-TD methods. The per-time-step complexities of gradient-TD and emphatic-TD methods are both linear in the number of parameters; both are much simpler than quadratic complexity methods such LSTD(λ) and its off-policy variants.\nWe have presented a few empirical examples of emphatic TD(0) compared to conventional TD(0) adapted to off-policy training. These examples illustrate some of emphatic TD(λ)’s basic strengths and weaknesses, but a proper empirical comparison with other methods remains for future work. Note that emphatic TD(λ) is a new algorithm even in the on-policy case. Emphatic methods allocate their function approximation resources more selectively than conventional methods; it seems likely to us that emphatic TD(λ) (and an emphatic version of LSTD(λ)) would have lower asymptotic error than conventional TD(λ) even on on-policy problems. Extensions of the emphasis idea to action-value and control methods such as Sarsa(λ) and Q(λ), and to true-online forms (van Seijen & Sutton 2014), are also natural. These and other important topics remain for future work."
    }, {
      "heading" : "Acknowledgements",
      "text" : "The authors thank Hado van Hasselt, Doina Precup, and Huizhen Yu for insights and discussions contributing to the results presented in this paper, and the entire Reinforcement Learning and Artificial Intelligence Research Group for providing the environment to nurture and support this research. We gratefully acknowledge funding from Alberta Innovates – Technology Futures and from the Natural Sciences and Engineering Research Council of Canada."
    } ],
    "references" : [ {
      "title" : "Residual algorithms: Reinforcement learning with function approximation",
      "author" : [ "L.C. Baird" ],
      "venue" : "In Proceedings of the 12th International Conference on Machine Learning,",
      "citeRegEx" : "Baird,? \\Q1995\\E",
      "shortCiteRegEx" : "Baird",
      "year" : 1995
    }, {
      "title" : "Least-squares temporal difference learning",
      "author" : [ "J.A. Boyan" ],
      "venue" : "In Proceedings of the 16th International Conference on Machine Learning,",
      "citeRegEx" : "Boyan,? \\Q1999\\E",
      "shortCiteRegEx" : "Boyan",
      "year" : 1999
    }, {
      "title" : "Linear least-squares algorithms for temporal difference learning. Machine Learning 22:33–57",
      "author" : [ "S. Bradtke", "A.G. Barto" ],
      "venue" : null,
      "citeRegEx" : "Bradtke and Barto,? \\Q1996\\E",
      "shortCiteRegEx" : "Bradtke and Barto",
      "year" : 1996
    }, {
      "title" : "The convergence of TD(λ) for general λ. Machine Learning 8:341–362",
      "author" : [ "P. Dayan" ],
      "venue" : null,
      "citeRegEx" : "Dayan,? \\Q1992\\E",
      "shortCiteRegEx" : "Dayan",
      "year" : 1992
    }, {
      "title" : "Reinforcement learning: The good, the bad and the ugly",
      "author" : [ "P. Dayan", "Y. Niv" ],
      "venue" : "Current Opinion in Neurobiology",
      "citeRegEx" : "Dayan and Niv,? \\Q2008\\E",
      "shortCiteRegEx" : "Dayan and Niv",
      "year" : 2008
    }, {
      "title" : "Policy evaluation with temporal differences: A survey and comparison",
      "author" : [ "C. Dann", "G. Neumann", "J. Peters" ],
      "venue" : "Journal of Machine Learning Research",
      "citeRegEx" : "Dann et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Dann et al\\.",
      "year" : 2014
    }, {
      "title" : "Off-policy learning with eligibility traces: A survey",
      "author" : [ "M. Geist", "B. Scherrer" ],
      "venue" : "Journal of Machine Learning Research",
      "citeRegEx" : "Geist and Scherrer,? \\Q2014\\E",
      "shortCiteRegEx" : "Geist and Scherrer",
      "year" : 2014
    }, {
      "title" : "Stable function approximation in dynamic programming",
      "author" : [ "G.J. Gordon" ],
      "venue" : "Proceedings of the 12th International Conference on Machine Learning,",
      "citeRegEx" : "Gordon,? \\Q1995\\E",
      "shortCiteRegEx" : "Gordon",
      "year" : 1995
    }, {
      "title" : "Stable fitted reinforcement learning",
      "author" : [ "G.J. Gordon" ],
      "venue" : "Advances in Neural Information Processing Systems: Proceedings of the 1995 Conference,",
      "citeRegEx" : "Gordon,? \\Q1996\\E",
      "shortCiteRegEx" : "Gordon",
      "year" : 1996
    }, {
      "title" : "Faster Gradient-TD Algorithms. MSc thesis, University of Alberta",
      "author" : [ "L. Hackman" ],
      "venue" : null,
      "citeRegEx" : "Hackman,? \\Q2012\\E",
      "shortCiteRegEx" : "Hackman",
      "year" : 2012
    }, {
      "title" : "A neuronal model of classical conditioning",
      "author" : [ "A.H. Klopf" ],
      "venue" : "Psychobiology",
      "citeRegEx" : "Klopf,? \\Q1988\\E",
      "shortCiteRegEx" : "Klopf",
      "year" : 1988
    }, {
      "title" : "Least squares policy iteration",
      "author" : [ "M. Lagoudakis", "R. Parr" ],
      "venue" : "Journal of Machine Learning Research",
      "citeRegEx" : "Lagoudakis and Parr,? \\Q2003\\E",
      "shortCiteRegEx" : "Lagoudakis and Parr",
      "year" : 2003
    }, {
      "title" : "Evaluating the TD model of classical conditioning",
      "author" : [ "E.A. Ludvig", "R.S. Sutton", "E.J. Kehoe" ],
      "venue" : "Learning & behavior",
      "citeRegEx" : "Ludvig et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Ludvig et al\\.",
      "year" : 2012
    }, {
      "title" : "Gradient Temporal-Difference Learning Algorithms. PhD thesis, University of Alberta",
      "author" : [ "H.R. Maei" ],
      "venue" : null,
      "citeRegEx" : "Maei,? \\Q2011\\E",
      "shortCiteRegEx" : "Maei",
      "year" : 2011
    }, {
      "title" : "GQ(λ): A general gradient algorithm for temporaldifference prediction learning with eligibility traces",
      "author" : [ "H.R. Maei", "R.S. Sutton" ],
      "venue" : "In Proceedings of the Third Conference on Artificial General Intelligence,",
      "citeRegEx" : "Maei and Sutton,? \\Q2010\\E",
      "shortCiteRegEx" : "Maei and Sutton",
      "year" : 2010
    }, {
      "title" : "Toward off-policy learning control with function approximation",
      "author" : [ "H.R. Maei", "Szepesvári", "Cs", "S. Bhatnagar", "R.S. Sutton" ],
      "venue" : "In Proceedings of the 27th International Conference on Machine Learning,",
      "citeRegEx" : "Maei et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Maei et al\\.",
      "year" : 2010
    }, {
      "title" : "Multi-timescale nexting in a reinforcement learning robot",
      "author" : [ "J. Modayil", "A. White", "R.S. Sutton" ],
      "venue" : "Adaptive Behavior",
      "citeRegEx" : "Modayil et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Modayil et al\\.",
      "year" : 2014
    }, {
      "title" : "Dialogues on prediction errors. Trends in cognitive sciences",
      "author" : [ "Y. Niv", "G. Schoenbaum" ],
      "venue" : null,
      "citeRegEx" : "Niv and Schoenbaum,? \\Q2008\\E",
      "shortCiteRegEx" : "Niv and Schoenbaum",
      "year" : 2008
    }, {
      "title" : "Beyond simple reinforcement learning: The computational neurobiology of reward learning and valuation",
      "author" : [ "J.P. O’Doherty" ],
      "venue" : "European Journal of Neuroscience",
      "citeRegEx" : "O.Doherty,? \\Q2012\\E",
      "shortCiteRegEx" : "O.Doherty",
      "year" : 2012
    }, {
      "title" : "Off-policy temporal-difference learning with function approximation",
      "author" : [ "D. Precup", "R.S. Sutton", "S. Dasgupta" ],
      "venue" : "In Proceedings of the 18th International Conference on Machine Learning,",
      "citeRegEx" : "Precup et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Precup et al\\.",
      "year" : 2001
    }, {
      "title" : "Eligibility traces for off-policy policy evaluation",
      "author" : [ "D. Precup", "R.S. Sutton", "S. Singh" ],
      "venue" : "In Proceedings of the 17th International Conference on Machine Learning,",
      "citeRegEx" : "Precup et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Precup et al\\.",
      "year" : 2000
    }, {
      "title" : "Problem Solving with Reinforcement Learning",
      "author" : [ "G.A. Rummery" ],
      "venue" : "PhD thesis,",
      "citeRegEx" : "Rummery,? \\Q1995\\E",
      "shortCiteRegEx" : "Rummery",
      "year" : 1995
    }, {
      "title" : "Some studies in machine learning using the game of checkers",
      "author" : [ "A.L. Samuel" ],
      "venue" : "IBM Journal on Research and Development 3:210–229",
      "citeRegEx" : "Samuel,? \\Q1959\\E",
      "shortCiteRegEx" : "Samuel",
      "year" : 1959
    }, {
      "title" : "A neural substrate of prediction and reward",
      "author" : [ "W. Schultz", "P. Dayan", "P.R. Montague" ],
      "venue" : "Science",
      "citeRegEx" : "Schultz et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Schultz et al\\.",
      "year" : 1997
    }, {
      "title" : "TD models of reward predictive responses in dopamine neurons",
      "author" : [ "R.E. Suri" ],
      "venue" : "Neural Networks",
      "citeRegEx" : "Suri,? \\Q2002\\E",
      "shortCiteRegEx" : "Suri",
      "year" : 2002
    }, {
      "title" : "Learning to predict by the methods of temporal differences",
      "author" : [ "R.S. Sutton" ],
      "venue" : "Machine Learning",
      "citeRegEx" : "Sutton,? \\Q1988\\E",
      "shortCiteRegEx" : "Sutton",
      "year" : 1988
    }, {
      "title" : "TD models: Modeling the world at a mixture of time scales",
      "author" : [ "R.S. Sutton" ],
      "venue" : "In Proceedings of the 12th International Conference on Machine Learning,",
      "citeRegEx" : "Sutton,? \\Q1995\\E",
      "shortCiteRegEx" : "Sutton",
      "year" : 1995
    }, {
      "title" : "The grand challenge of predictive empirical abstract knowledge. Working Notes of the IJCAI-09 Workshop on Grand Challenges for Reasoning from Experiences",
      "author" : [ "R.S. Sutton" ],
      "venue" : null,
      "citeRegEx" : "Sutton,? \\Q2009\\E",
      "shortCiteRegEx" : "Sutton",
      "year" : 2009
    }, {
      "title" : "Beyond reward: The problem of knowledge and data",
      "author" : [ "R.S. Sutton" ],
      "venue" : "In Proceedings of the 21st International Conference on Inductive Logic Programming,",
      "citeRegEx" : "Sutton,? \\Q2012\\E",
      "shortCiteRegEx" : "Sutton",
      "year" : 2012
    }, {
      "title" : "Toward a modern theory of adaptive networks: Expectation and prediction",
      "author" : [ "R.S. Sutton", "A.G. Barto" ],
      "venue" : "Psychological Review",
      "citeRegEx" : "Sutton and Barto,? \\Q1981\\E",
      "shortCiteRegEx" : "Sutton and Barto",
      "year" : 1981
    }, {
      "title" : "Time-derivative models of Pavlovian reinforcement",
      "author" : [ "R.S. Sutton", "A.G. Barto" ],
      "venue" : null,
      "citeRegEx" : "Sutton and Barto,? \\Q1990\\E",
      "shortCiteRegEx" : "Sutton and Barto",
      "year" : 1990
    }, {
      "title" : "Reinforcement Learning: An Introduction",
      "author" : [ "R.S. Sutton", "A.G. Barto" ],
      "venue" : null,
      "citeRegEx" : "Sutton and Barto,? \\Q1998\\E",
      "shortCiteRegEx" : "Sutton and Barto",
      "year" : 1998
    }, {
      "title" : "A new Q(λ) with interim forward view and Monte Carlo equivalence",
      "author" : [ "R.S. Sutton", "A.R. Mahmood", "D. Precup", "H. van Hasselt" ],
      "venue" : "In Proceedings of the 31st International Conference on Machine Learning",
      "citeRegEx" : "Sutton et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Sutton et al\\.",
      "year" : 2014
    }, {
      "title" : "Fast gradient-descent methods for temporal-difference learning with linear function approximation",
      "author" : [ "R.S. Sutton", "H.R. Maei", "D. Precup", "S. Bhatnagar", "D. Silver", "Szepesvári", "Cs", "E. Wiewiora" ],
      "venue" : "In Proceedings of the 26th International Conference on Machine Learning,",
      "citeRegEx" : "Sutton et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Sutton et al\\.",
      "year" : 2009
    }, {
      "title" : "Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction",
      "author" : [ "R.S. Sutton", "J. Modayil", "M. Delp", "T. Degris", "P.M. Pilarski", "A. White", "D. Precup" ],
      "venue" : "In Proceedings of the 10th International Conference on Autonomous Agents and Multiagent Systems,",
      "citeRegEx" : "Sutton et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Sutton et al\\.",
      "year" : 2011
    }, {
      "title" : "Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning",
      "author" : [ "R.S. Sutton", "Precup D", "S. Singh" ],
      "venue" : "Artificial Intelligence",
      "citeRegEx" : "Sutton et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Sutton et al\\.",
      "year" : 1999
    }, {
      "title" : "Temporal abstraction in temporal-difference networks",
      "author" : [ "R.S. Sutton", "E.J. Rafols", "A. Koop" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "Sutton et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Sutton et al\\.",
      "year" : 2006
    }, {
      "title" : "Practical issues in temporal difference learning",
      "author" : [ "G. Tesauro" ],
      "venue" : "Machine Learning",
      "citeRegEx" : "Tesauro,? \\Q1992\\E",
      "shortCiteRegEx" : "Tesauro",
      "year" : 1992
    }, {
      "title" : "Temporal difference learning and TD-Gammon",
      "author" : [ "G. Tesauro" ],
      "venue" : "Communications of the ACM",
      "citeRegEx" : "Tesauro,? \\Q1995\\E",
      "shortCiteRegEx" : "Tesauro",
      "year" : 1995
    }, {
      "title" : "Bias in natural actor–critic algorithms",
      "author" : [ "P. Thomas" ],
      "venue" : "In Proceedings of the 31st International Conference on Machine Learning. JMLR",
      "citeRegEx" : "Thomas,? \\Q2014\\E",
      "shortCiteRegEx" : "Thomas",
      "year" : 2014
    }, {
      "title" : "An analysis of temporal-difference learning with function approximation",
      "author" : [ "J.N. Tsitsiklis", "B. Van Roy" ],
      "venue" : "IEEE Transactions on Automatic Control",
      "citeRegEx" : "Tsitsiklis and Roy,? \\Q1997\\E",
      "shortCiteRegEx" : "Tsitsiklis and Roy",
      "year" : 1997
    }, {
      "title" : "True online TD(λ)",
      "author" : [ "H. van Seijen", "R.S. Sutton" ],
      "venue" : "In Proceedings of the 31st International Conference on Machine Learning",
      "citeRegEx" : "Seijen and Sutton,? \\Q2014\\E",
      "shortCiteRegEx" : "Seijen and Sutton",
      "year" : 2014
    }, {
      "title" : "Learning from Delayed Rewards",
      "author" : [ "Watkins", "C.J.C. H" ],
      "venue" : null,
      "citeRegEx" : "Watkins and H.,? \\Q1989\\E",
      "shortCiteRegEx" : "Watkins and H.",
      "year" : 1989
    }, {
      "title" : "Q-learning. Machine Learning 8:279–292",
      "author" : [ "Watkins", "C.J.C. H", "P. Dayan" ],
      "venue" : null,
      "citeRegEx" : "Watkins et al\\.,? \\Q1992\\E",
      "shortCiteRegEx" : "Watkins et al\\.",
      "year" : 1992
    }, {
      "title" : "Convergence of least squares temporal difference methods under general conditions",
      "author" : [ "H. Yu" ],
      "venue" : "In Proceedings of the 27th International Conference on Machine Learning,",
      "citeRegEx" : "Yu,? \\Q2010\\E",
      "shortCiteRegEx" : "Yu",
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 25,
      "context" : "A less obvious advantage of the TD approach is that it often produces statistically more accurate answers than conventional approaches (Sutton 1988).",
      "startOffset" : 135,
      "endOffset" : 148
    }, {
      "referenceID" : 15,
      "context" : "Parametric temporal-difference learning was first studied as the key “learning by generalization” algorithm in Samuel’s (1959) checker player.",
      "startOffset" : 111,
      "endOffset" : 127
    }, {
      "referenceID" : 15,
      "context" : "Parametric temporal-difference learning was first studied as the key “learning by generalization” algorithm in Samuel’s (1959) checker player. Sutton (1988) introduced the TD(λ) algorithm and proved convergence in the mean of episodic linear TD(0), the simplest parametric TD method.",
      "startOffset" : 111,
      "endOffset" : 157
    }, {
      "referenceID" : 0,
      "context" : "Dayan (1992) proved convergence in expected value of episodic linear TD(λ) for all λ ∈ [0, 1], and Tsitsiklis and Van Roy (1997) proved convergence with probability one of discounted continuing linear TD(λ).",
      "startOffset" : 0,
      "endOffset" : 13
    }, {
      "referenceID" : 0,
      "context" : "Dayan (1992) proved convergence in expected value of episodic linear TD(λ) for all λ ∈ [0, 1], and Tsitsiklis and Van Roy (1997) proved convergence with probability one of discounted continuing linear TD(λ).",
      "startOffset" : 0,
      "endOffset" : 129
    }, {
      "referenceID" : 0,
      "context" : "Dayan (1992) proved convergence in expected value of episodic linear TD(λ) for all λ ∈ [0, 1], and Tsitsiklis and Van Roy (1997) proved convergence with probability one of discounted continuing linear TD(λ). Watkins (1989) extended TD learning to control in the form of Q-learning and proved its convergence in the tabular case (without function approximation, Watkins & Dayan 1992), while Rummery (1995) extended TD learning to control in an on-policy form as the Sarsa(λ) algorithm.",
      "startOffset" : 0,
      "endOffset" : 223
    }, {
      "referenceID" : 0,
      "context" : "Dayan (1992) proved convergence in expected value of episodic linear TD(λ) for all λ ∈ [0, 1], and Tsitsiklis and Van Roy (1997) proved convergence with probability one of discounted continuing linear TD(λ). Watkins (1989) extended TD learning to control in the form of Q-learning and proved its convergence in the tabular case (without function approximation, Watkins & Dayan 1992), while Rummery (1995) extended TD learning to control in an on-policy form as the Sarsa(λ) algorithm.",
      "startOffset" : 0,
      "endOffset" : 405
    }, {
      "referenceID" : 0,
      "context" : "Bradtke and Barto (1996) and Boyan (1999) extended linear TD learning to a least-squares form called LSTD(λ).",
      "startOffset" : 0,
      "endOffset" : 25
    }, {
      "referenceID" : 0,
      "context" : "Bradtke and Barto (1996) and Boyan (1999) extended linear TD learning to a least-squares form called LSTD(λ).",
      "startOffset" : 29,
      "endOffset" : 42
    }, {
      "referenceID" : 0,
      "context" : "Baird (1995) showed definitively that parametric TD learning was much less robust in the off-policy case by exhibiting counterexamples for which both linear TD(0) and linear Q-learning had unstable expected updates and, as a result, the parameters of their linear function approximation diverged to infinity.",
      "startOffset" : 0,
      "endOffset" : 13
    }, {
      "referenceID" : 0,
      "context" : "Baird (1995) showed definitively that parametric TD learning was much less robust in the off-policy case by exhibiting counterexamples for which both linear TD(0) and linear Q-learning had unstable expected updates and, as a result, the parameters of their linear function approximation diverged to infinity. This is a serious limitation, as the off-policy aspect is key to Q-learning (perhaps the single most popular reinforcement learning algorithm), to learning from historical data and from demonstrations, and to the idea of using TD learning for perception and world knowledge. Over the years, several different approaches have been taken to solving the problem of off-policy learning. Baird (1995) proposed an approach based on gradient descent in the Bellman error for general parametric function approximation that has the desired computa-",
      "startOffset" : 0,
      "endOffset" : 705
    }, {
      "referenceID" : 9,
      "context" : "2010), including hybrid methods such as HTD (Hackman 2012).",
      "startOffset" : 44,
      "endOffset" : 58
    }, {
      "referenceID" : 6,
      "context" : "The studies by White (in preparation), Geist and Scherrer (2014), and Dann, Neumann, and Peters (2014) represent our best experience with gradient-TD and related methods.",
      "startOffset" : 39,
      "endOffset" : 65
    }, {
      "referenceID" : 6,
      "context" : "The studies by White (in preparation), Geist and Scherrer (2014), and Dann, Neumann, and Peters (2014) represent our best experience with gradient-TD and related methods.",
      "startOffset" : 39,
      "endOffset" : 103
    }, {
      "referenceID" : 25,
      "context" : "From this state weighting, stability of the expected update is then proved using theory similar to that originally developed for TD(λ) (Sutton 1988).",
      "startOffset" : 135,
      "endOffset" : 148
    }, {
      "referenceID" : 25,
      "context" : "The approach has novel elements but is similar to that developed by Precup, Sutton, and Dasgupta in 2001. They proposed to use importance sampling to reweight the updates of linear TD(λ), emphasizing or de-emphasizing states as they were encountered, and thereby create a weighting equivalent to the stationary distribution under the target policy, from which the results of Tsitsiklis and Van Roy (1997) would apply and guarantee convergence.",
      "startOffset" : 76,
      "endOffset" : 405
    }, {
      "referenceID" : 0,
      "context" : "Baird’s (1995) counterexample, for example, is not technically about TD(λ), but about the expected update; in effect, he showed that TD(λ)’s A is not positive semi-definite.",
      "startOffset" : 0,
      "endOffset" : 15
    }, {
      "referenceID" : 22,
      "context" : "Earlier work by Precup, Sutton and Dasgupta (2001) attempted to completely correct for the different state distribution using importance sampling ratios to reweight the states encountered.",
      "startOffset" : 24,
      "endOffset" : 51
    }, {
      "referenceID" : 13,
      "context" : "2009, Maei 2011) seeks to minimize the mean-squared projected Bellman error weighted by dμ. We call this an excursion setting because we can think of the contemplated switch to the target policy as an excursion from the steady-state distribution of the behavior policy, dμ. The excursions would start from dμ and then follow π until termination, followed by a resumption of μ and thus a gradual return to dμ. Of course these excursions never actually occur during off-policy learning, they are just contemplated, and thus the state distribution in fact never leaves dμ. It is the excursion view that we take in this paper, but still we use techniques similar to those introduced by Precup et al. (2001) to determine an emphasis weighting that corrects for the state distribution, only toward a different goal.",
      "startOffset" : 6,
      "endOffset" : 703
    }, {
      "referenceID" : 25,
      "context" : "With the steps we have already done, this is immediate and after the pattern of Sutton (1988), as in the emphatic TD(0) section.",
      "startOffset" : 80,
      "endOffset" : 94
    }, {
      "referenceID" : 25,
      "context" : "Equation (6) of the paper by Sutton, Mahmood, Precup, and van Hasselt (2014) specifies this in their “forward view” of off-policy TD(λ) with general state-dependent discounting and bootstrapping.",
      "startOffset" : 29,
      "endOffset" : 77
    }, {
      "referenceID" : 0,
      "context" : "If an algorithm is unstable, as Q-learning and off-policy TD(λ) are on Baird’s (1995) counterexample, then there is no chance of its behaving in a satisfactory manner.",
      "startOffset" : 71,
      "endOffset" : 86
    }, {
      "referenceID" : 19,
      "context" : "Another idea for reducing variance is to use weighted importance sampling, which Precup et al. (2001) proved converges robustly even with infinite variance samples, together with the ideas of Mahmood et al.",
      "startOffset" : 81,
      "endOffset" : 102
    }, {
      "referenceID" : 19,
      "context" : "We have introduced a way of varying the emphasis or strength of the update of TD learning algorithms from step to step, based on importance sampling, that should result in much lower variance than previous methods (Precup et al. 2001).",
      "startOffset" : 214,
      "endOffset" : 234
    } ],
    "year" : 2015,
    "abstractText" : "In this paper we introduce the idea of improving the performance of parametric temporaldifference (TD) learning algorithms by selectively emphasizing or de-emphasizing their updates on different time steps. In particular, we show that varying the emphasis of linear TD(λ)’s updates in a particular way causes its expected update to become stable under off-policy training. The only prior model-free TD methods to achieve this with per-step computation linear in the number of function approximation parameters are the gradientTD family of methods including TDC, GTD(λ), and GQ(λ). Compared to these methods, our emphatic TD(λ) is simpler and easier to use; it has only one learned parameter vector and one step-size parameter. On the other hand, the range of problems for which it is stable but does not converge with probability one is larger than for gradient-TD methods. Our treatment includes general state-dependent discounting and bootstrapping functions, and a way of specifying varying degrees of interest in accurately valuing different states.",
    "creator" : "LaTeX with hyperref package"
  }
}