{
  "name" : "1406.2732.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Deep Epitomic Convolutional Neural Networks",
    "authors" : [ "George Papandreou" ],
    "emails" : [ "gpapan@ttic.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n40 6.\n27 32\nv1 [\ncs .C\nV ]\n1 0\nJu n\n20 14"
    }, {
      "heading" : "1 Introduction",
      "text" : "Deep learning offers a powerful framework for learning increasingly complex representations for visual recognition tasks. The work of Krizhevsky et al. [15] convincingly demonstrated that deep neural networks can be very effective in classifying images in the challenging Imagenet benchmark [5], significantly outperforming computer vision systems built on top of engineered features like SIFT [20]. Their success spurred a lot of interest in the machine learning and computer vision communities. Subsequent work has improved our understanding and has refined certain aspects of this class of models [28]. A number of different studies have further shown that the features learned by deep neural networks are generic and can be successfully employed in a black-box fashion in other datasets or tasks such as image detection [4, 7, 22, 24, 26, 28].\nThe deep learning models that so far have proven most successful in image recognition tasks are feed-forward convolutional neural networks trained in a supervised fashion to minimize a regularized training set classification error by back-propagation. Their recent success is partly due to the availability of large annotated datasets and fast GPU computing, and partly due to some important methodological developments such as dropout regularization and rectifier linear activations [15]. However, the key building blocks of deep neural networks for images have been around for many years [17]: (1) convolutional multi-layer neural networks with small receptive fields that spatially share parameters within each layer. (2) Gradual abstraction and spatial resolution reduction after each convolutional layer as we ascend the network hierarchy, most effectively via maxpooling [10, 25].\nIn this work we build a deep neural network around the epitomic representation [12]. The image epitome is a data structure appropriate for learning translation-aware image representations, naturally disentagling appearance and position modeling of visual patterns. In the context of deep learning,\nan epitomic convolution layer substitutes a pair of consecutive convolution and max-pooling layers typically used in deep convolutional neural networks. In epitomic matching, for each regularlyspaced input data patch in the lower layer we search across filters in the epitomic dictionary for the strongest response. In max-pooling on the other hand, for each filter in the dictionary we search within a window in the lower input data layer for the strongest response. Epitomic matching is thus an input-centered dual alternative to the filter-centered standard max-pooling.\nWe investigate two main deep epitomic network model variants. Our first variant employs a dictionary of mini-epitomes at each network layer. Each mini-epitome is only slightly larger than the corresponding input data patch, just enough to accomodate for the desired extent of position invariance. For each input data patch, the mini-epitome layer outputs a single value per mini-epitome, which is the maximum response across all filters in the mini-epitome. Our second topographic variant uses just a few large epitomes at each network layer. For each input data patch, the topographic epitome layer outputs multiple values per large epitome, which are the local maximum responses at regularly spaced positions within each topography.\nWe quantitatively evaluate the proposed model primarily in image classification experiments on the Imagenet ILSVRC-2012 large-scale image classification task. We train the model by error backpropagation to minimize the classification log-loss, similarly to [15]. Our best mini-epitomic variant achieves 13.6% top-5 error on the validation set, which is 0.6% better than a conventional maxpooled convolutional network of comparable structure whose error rate is 14.2%. Note that the error rate of the original model in [15] is 18.2%, using however a smaller network. All these performance numbers refer to classification with a single network. We also find that the proposed epitomic model converges faster, especially when the filters in the dictionary are mean- and contrast-normalized, which is related to [28]. We have found this normalization to also accelerate convergence of standard max-pooled networks. We further show that a deep epitomic network trained on Imagenet can be effectively used as black-box feature extractor for tasks such as Caltech-101 image classification. Finally, we report excellent image classification results on the MNIST and CIFAR-10 benchmarks with smaller deep epitomic networks trained from scratch on these small-image datasets.\nRelated work Our model builds on the epitomic image representation [12], which was initially geared towards image and video modeling tasks. Single-level dictionaries of image epitomes learned in an unsupervised fashion for image denoising have been explored in [1, 2]. Recently, single-level mini-epitomes learned by a variant of K-means have been proposed as an alternative to SIFT for image classification [23]. To our knowledge, epitomes have not been studied before in conjunction with deep models or learned to optimize a supervised objective.\nThe proposed epitomic model is closely related to maxout networks [8]. Similarly to epitomic matching, the response of a maxout layer is the maximum across filter responses. The critical difference is that the epitomic layer is hard-wired to model position invariance, since filters extracted from an epitome share values in their area of overlap. This parameter sharing significantly reduces the number of free parameters that need to be learned. Maxout is typically used in conjunction with max-pooling [8], while epitomes fully substitute for it. Moreover, maxout requires random input perturbations with dropout during model training, otherwise it is prone to creating inactive features. On the contrary, we have found that learning deep epitomic networks does not require dropout in the convolutional layers – similarly to [15], we only use dropout regularization in the fully connected layers of our network.\nOther variants of max pooling have been explored before. Stochastic pooling [27] has been proposed in conjunction with supervised learning. Probabilistic pooling [19] and deconvolutional networks [29] have been proposed before in conjunction with unsupervised learning, avoiding the theoretical and practical difficulties associated with building probabilistic models on top of max-pooling. While we do not explore it in this paper, we are also very interested in pursuing unsupervised learning methods appropriate for the deep epitomic representation.\nThe topographic variant of the proposed epitomic model naturally learns topographic feature maps. Adjacent filters in a single epitome share values in their area of overlap, and thus constitute a hardwired topographic map. This relates the proposed model to topographic ICA [9] and related models [13, 16, 21], which are typically trained to optimize unsupervised objectives."
    }, {
      "heading" : "2 Deep Epitomic Convolutional Networks",
      "text" : ""
    }, {
      "heading" : "2.1 Mini-Epitomic deep networks",
      "text" : "We first describe a single layer of the mini-epitome variant of the proposed model, with reference to Fig. 1. In standard max-pooled convolution, we have a dictionary of K filters of spatial size W×W pixels spanning C channels, which we represent as real-valued vectors {wk}Kk=1 with W · W · C elements. We apply each of them in a convolutional fashion to everyW×W input patch {xi} densely extracted from each position in the input layer which also has C channels. A reduced resolution output map is produced by computing the maximum response within a small D×D window of displacements p ∈ Ninput around positions i in the input map which are D pixels apart from each other. The output map {zi,k} of standard max-pooled convolution has spatial resolution reduced by a factor of D across each dimension and will consist of K channels, one for each of the K filters. Specifically:\n(zi,k, pi,k) ← max p∈Nimage\nx T i+pwk (1)\nwhere pi,k points to the input layer position where the maximum is attained.\nIn the proposed epitomic convolution scheme we replace the filters with larger mini-epitomes {vk}Kk=1 of spatial size V×V pixels, where V = W +D−1. Each mini-epitome contains D\n2 filters {wk,p}Kk=1 of size W×W , one for each of the D×D displacements p ∈ Nepit in the epitome. We sparsely extract patches {xi} from the input layer on a regular grid with stride D pixels. In the proposed epitomic convolution model we reverse the role of filters and input layer patches, computing the maximum response over epitomic positions rather than input layer positions:\n(yi,k, pi,k) ← max p∈Nepitome\nx T i wk,p (2)\nwhere pi,k now points to the position in the epitome where the maximum is attained. Since the input position is fixed, we can think of epitomic matching as an input-centered dual alternative to the filter-centered standard max-pooling.\nComputing the maximum response over filters rather than image positions resembles the maxout scheme of [8], yet in the proposed model the filters within the epitome are constrained to share values in their area of overlap.\nSimilarly to max-pooled convolution, the epitomic convolution output map {yi,k} has K channels and is subsampled by a factor of D across each spatial dimension. Epitomic convolution has the same computational cost as max-pooled convolution. For each output map value, they both require computing D2 inner products followed by finding the maximum response. Epitomic convolution requires D2 times more work per input patch, but this is fully offset by the fact that we extract input patches sparsely with a stride of D pixels.\nSimilarly to standard max-pooling, the main computational primitive is multi-channel convolution with the set of filters in the epitomic dictionary, which we implement as matrix-matrix multiplication and carry out on the GPU, using the cuBLAS library.\nTo build a deep epitomic model, we stack multiple epitomic convolution layers on top of each other. The output of each layer passes through a rectified linear activation unit yi,k ← max(yi,k + βk, 0) and fed as input to the subsequent layer, where βk is the bias. Similarly to [15], the final two layers of our network for Imagenet image classification are fully connected and are regularized by dropout. We learn the model parameters (epitomic weights and biases for each layer) in a supervised fashion by error back propagation. We present full details of our model architecture and training methodology in the experimental section."
    }, {
      "heading" : "2.2 Topographic deep networks",
      "text" : "We have also experimented with a topographic variant of the proposed deep epitomic network. For this we use a dictionary with just a few large epitomes of spatial size V ×V pixels, with V ≥ W + D − 1. We retain the local maximum responses over D×D neighborhoods spaced D pixels apart in each of the epitomes, thus yielding (⌊((V −W +1)−D)/D⌋+1)2 output values for each of the K epitomes in the dictionary. The mini-epitomic variant can be considered as a special case of the topographic one when V = W +D − 1."
    }, {
      "heading" : "2.3 Optional mean and contrast normalization",
      "text" : "Motivated by [28], we have also explored the effect of filter mean and contrast normalization on deep epitomic network training. More specifically, we considered a variant of the model where the epitomic convolution responses are computed as:\n(yi,k, pi,k) ← max p∈Nepitome\nx T i w̄k,p ‖w̄k,p‖λ (3)\nwhere w̄k,p is a mean-normalized version of the filters and ‖w̄k,p‖λ , (w̄Tk,pw̄k,p + λ) 1/2 is their contrast, with λ = 0.01 a small positive constant. This normalization requires only a slight modification of the stochastic gradient descent update formula and incurs negligible computational overhead. Note that the contrast normalization explored here is slightly different than the one in [28], who only scale down the filters whenever their contrast exceeds a pre-defined threshold.\nWe have found the mean and contrast normalization of Eq. (3) to be crucial for learning the topographic version of the proposed model. We have also found that it significantly accelerates learning of the mini-epitome version of the proposed model, as well as the standard max-pooled convolutional model, without however significantly affecting the final performance of these two model."
    }, {
      "heading" : "3 Image Classification Experiments",
      "text" : ""
    }, {
      "heading" : "3.1 Image classification tasks",
      "text" : "We have performed most of our experimental investigation on the Imagenet ILSVRC-2012 dataset [5], focusing on the task of image classification. This dataset contains more than 1.2 million training images, 50,000 validation images, and 100,000 test images. Each image is assigned to one out of 1,000 possible object categories. Performance is evaluated using the top-5 classification error. Such large-scale image datasets have proven so far essential to successfully train big deep neural networks with supervised criteria.\nSimilarly to other recent works [4, 24, 28], we also evaluate deep epitomic networks trained on Imagenet as a black-box visual feature front-end on the Caltech-101 benchmark [6]. This involves classifying images into one out of 102 possible image classes. We further consider two standard classification benchmarks involving thumbnail-sized images, the MNIST digit [18] and the CIFAR10 [14], both involving classification into 10 possible classes."
    }, {
      "heading" : "3.2 Network architecture and training methodology",
      "text" : "For our Imagenet experiments, we compare the proposed deep mini-epitomic and topographic deep networks with deep convolutional networks employing standard max-pooling. For fair comparison, we use as similar architectures as possible, involving in all cases six convolutional layers, followed by two fully-connected layers and a 1000-way softmax layer. We use rectified linear activation units throughout the network. Similarly to [15], we apply local response normalization (LRN) to the output of the first two convolutional layers and dropout to the output of the two fully-connected layers.\nThe architecture of our baseline Max-Pool network is specified on Table 1. It employs max-pooling in the convolutional layers 1, 2, and 6. To accelerate computation, it uses an image stride equal to\n2 pixels in the first layer. It has a similar structure with the Overfeat model [26], yet significantly fewer neurons in the convolutional layers 2 to 6. Another difference with [26] is the use of LRN, which to our experience facilitates training.\nThe architecture of the proposed Epitomic network is specified on Table 2. It has exactly the same number of neurons at each layer as the Max-Pool model but it uses mini-epitomes in place of convolution + max pooling at layers 1, 2, and 6. It uses the same filter sizes with the Max-Pool model and the mini-epitome sizes have been selected so as to allow the same extent of translation invariance as the corresponding layers in the baseline model. We use input image stride equal to 4 pixels and further perform epitomic search with stride equal to 2 pixels in the first layer to also accelerate computation.\nThe architecture of our second proposed Topographic network is specified on Table 3. It uses four epitomes at layers 1, 2 and eight epitomes at layer 6 to learn topographic feature maps. It uses the same filter sizes as the previous two models and the epitome sizes have been selected so as each layer produces roughly the same number of output channels when allowing the same extent of translation invariance as the corresponding layers in the other two models.\nWe have also tried variants of the three models above where we activate the mean and contrast normalization scheme of Section 2.3 in layers 1, 2, and 6 of the network.\nWe followed the methodology of [15] in training our models. We used stochastic gradient ascent with learning rate initialized to 0.01 and decreased by a factor of 10 each time the validation error stopped improving. We used momentum equal to 0.9 and mini-batches of 128 images. The weight decay factor was equal to 5×10−4. Importantly, weight decay needs to be turned off for the layers that use mean and contrast normalization. Training each of the three models takes two weeks using a single NVIDIA Titan GPU. Similarly to [4], we resized the training images to have small dimension equal to 256 pixels while preserving their aspect ratio and not cropping their large dimension. We also subtracted for each image pixel the global mean RGB color values computed over the whole Imagenet training set. During training, we presented the networks with 220×220 crops randomly sampled from the resized image area, flipped left-to-right with probability 0.5, also injecting global color noise exactly as in [15]. During evaluation, we presented the networks with 10 regularly sampled image crops (center + 4 corners, as well as their left-to-right flipped versions)."
    }, {
      "heading" : "3.3 Weight visualization",
      "text" : "We visualize in Figure 2 the layer weights at the first layer of the networks above. The networks learn receptive fields sensitive to edge, blob, texture, and color patterns."
    }, {
      "heading" : "3.4 Classification results",
      "text" : "We report at Table 4 our results on the Imagenet ILSVRC-2012 benchmark, also including results previously reported in the literature [15,26,28]. These all refer to the top-5 error on the validation set and are obtained with a single network. Our best result at 13.6% with the proposed Epitomic-Norm network is 0.6% better than the baseline Max-Pool result at 14.2% error. Our Topographic-Norm network scores less well, yielding 15.4% error rate, which however is still better than [15,28]. Mean and contrast normalization had little effect on final performance for the Max-Pool and Epitomic models, but we found it essential for learning the Topographic model. The improved performance that we got with the Max-Pool baseline network compared to Overfeat [26] is most likely due to our use of LRN and aspect ratio preserving image resizing. When preparing this manuscript, we became aware of the work of [4] that reports an even lower 13.1% error rate with a max-pooled network, using however significantly more neurons than we do in the convolutional layers 2 to 5.\nWe next assess the quality of the proposed model trained on Imagenet as black-box feature extractor for Caltech-101 image classification. For this purpose, we used the 4096-dimensional output of the last fully-connected layer, without doing any fine-tuning of the network weights for the new task. We trained a 102-way SVM classifier using libsvm [3] and the default regularization parameter. For this experiment we just resized the Caltech-101 images to size 220×220 without preserving their aspect ratio and computed a single feature vector per image. We normalized the feature vector to have unit length before feeding it into the SVM. We report at Table 5 the mean classification accuracy obtained with the different networks. The proposed Epitomic model performs at 87.8%, 0.5% better than the baseline Max-Pool model.\nWe have also performed experiments with the epitomic model on classifying small images on the MNIST and CIFAR-10 datasets. For these tasks we have trained much smaller networks from scratch, using three epitomic convolutional layers, followed by one fully-connected layer and the final softmax classification layer. Because of the small training set sizes, we have found it beneficial to also employ dropout regularization in the epitomic convolution layers. At Table 6 we report the classification error rates we obtained. Our results are comparable to maxout [8], which achieves state-of-art results on these tasks."
    }, {
      "heading" : "3.5 Mean-contrast normalization and convergence speed",
      "text" : "We comment on the learning speed and convergence properties of the different models we experimented with on Imagenet. We show in Figure 3 how the top-5 validation error improves as learning progresses for the different models we tested, with or without mean+contrast normalization. For reference, we also include a corresponding plot we re-produced for the original model of Krizhevsky et al. [15]. We observe that mean+contrast normalization significantly accelerates convergence of both\nepitomic and max-pooled models, without however significantly influencing the final model quality. The epitomic models exhibit somewhat improved convergence behavior during learning compared to the max-pooled baselines whose performance fluctuates more."
    }, {
      "heading" : "4 Conclusions",
      "text" : "In this paper we have explored the potential of the epitomic representation as a building block for deep neural networks. We have shown that an epitomic layer can successfully substitute a pair of consecutive convolution and max-pooling layers. We have proposed two deep epitomic variants, one featuring mini-epitomes that empirically performs best in image classification, and one featuring large epitomes and learns topographically organized feature maps. We have shown that the proposed epitomic model performs around 0.5% better than the max-pooled baseline on the challenging Imagenet benchmark and other image classification tasks.\nIn future work, we are very interested in developing methods for unsupervised or semi-supervised training of deep epitomic models, exploiting the fact that the epitomic representation is more amenable than max-pooling for incorporating image reconstruction objectives.\nReproducibility We implemented the proposed methods by extending the excellent Caffe software framework [11]. When this work gets published we will publicly share our source code and configuration files with exact parameters fully reproducing the results reported in this paper.\nAcknowledgments We gratefully acknowledge the support of NVIDIA Corporation with the donation of GPUs used for this research."
    } ],
    "references" : [ {
      "title" : "Sparse and redundant modeling of image content using an imagesignature-dictionary",
      "author" : [ "M. Aharon", "M. Elad" ],
      "venue" : "SIAM J. Imaging Sci., 1(3):228–247",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Sparse image representation with epitomes",
      "author" : [ "L. Benoı̂t", "J. Mairal", "F. Bach", "J. Ponce" ],
      "venue" : "In Proc. CVPR,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2011
    }, {
      "title" : "LIBSVM: a library for support vector machines",
      "author" : [ "C.-C. Chang", "C.-J. Lin" ],
      "venue" : "ACM Trans. on Intel. Systems and Tech., 2(3)",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Return of the devil in the details: Delving deep into convolutional nets",
      "author" : [ "K. Chatfield", "K. Simonyan", "A. Vedaldi", "A. Zisserman" ],
      "venue" : "arXiv",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Imagenet: A large-scale hierarchical image database",
      "author" : [ "J. Deng", "W. Dong", "R. Socher", "L. Li-Jia", "K. Li", "L. Fei-Fei" ],
      "venue" : "Proc. CVPR",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Learning generative visual models from few training examples: An incremental Bayesian approach tested on 101 object categories",
      "author" : [ "L. Fei-Fei", "R. Fergus", "P. Perona" ],
      "venue" : "Proc. CVPR Workshop",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Rich feature hierarchies for accurate object detection and semantic segmentation",
      "author" : [ "R. Girshick", "J. Donahue", "T. Darrell", "J. Malik" ],
      "venue" : "Proc. CVPR",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Maxout networks",
      "author" : [ "I. Goodfellow", "D. Warde-Farley", "M. Mirza", "A. Courville", "Y. Bengio" ],
      "venue" : "Proc. ICML",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Topographic independent component analysis",
      "author" : [ "A. Hyvärinen", "P. Hoyer", "M. Inki" ],
      "venue" : "Neur. Comp., 13(7):1527–1558",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "What is the best multi-stage architecture for object recognition? In Proc",
      "author" : [ "K. Jarrett", "K. Kavukcuoglu", "M. Ranzato", "Y. LeCun" ],
      "venue" : "ICCV, pages 2146–2153",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Caffe: An open source convolutional architecture for fast feature embedding",
      "author" : [ "Y. Jia" ],
      "venue" : "http://caffe.berkeleyvision.org/",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Epitomic analysis of appearance and shape",
      "author" : [ "N. Jojic", "B. Frey", "A. Kannan" ],
      "venue" : "Proc. ICCV, pages 34–41",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Learning invariant features through topographic filter maps",
      "author" : [ "K. Kavukcuoglu", "M. Ranzato", "R. Fergus", "Y. LeCun" ],
      "venue" : "Proc. CVPR",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Learning multiple layers of features from tiny images",
      "author" : [ "A. Krizhevsky" ],
      "venue" : "Technical report",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "ImageNet classification with deep convolutional neural networks",
      "author" : [ "A. Krizhevsky", "I. Sutskever", "G. Hinton" ],
      "venue" : "Proc. NIPS",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Building high-level features using large scale unsupervised learning",
      "author" : [ "Q. Le", "M. Ranzato", "R. Monga", "M. Devin", "G. Corrado", "K. Chen", "J. Dean", "A. Ng" ],
      "venue" : "Proc. ICML",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Gradient-based learning applied to document recognition",
      "author" : [ "Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner" ],
      "venue" : "Proc. IEEE, 86(11):2278–2324",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations",
      "author" : [ "H. Lee", "R. Grosse", "R. Ranganath", "A.Y. Ng" ],
      "venue" : "Proc. ICML",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Distinctive image features from scale-invariant keypoints",
      "author" : [ "D. Lowe" ],
      "venue" : "IJCV, 60(2):91–110",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Topographic product models applied to natural scene statistics",
      "author" : [ "S. Osindero", "M. Welling", "G. Hinton" ],
      "venue" : "Neur. Comp., 18:381–414",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Joint deep learning for pedestrian detection",
      "author" : [ "W. Ouyang", "X. Wang" ],
      "venue" : "Proc. ICCV",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Modeling image patches with a generic dictionary of mini-epitomes",
      "author" : [ "G. Papandreou", "L.-C. Chen", "A. Yuille" ],
      "venue" : "Proc. CVPR",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "CNN features off-the-shelf: An astounding baseline for recognition",
      "author" : [ "A. Razavian", "H. Azizpour", "J. Sullivan", "S. Carlsson" ],
      "venue" : "Proc. CVPR Workshop",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Hierarchical models of object recognition in cortex",
      "author" : [ "M. Riesenhuber", "T. Poggio" ],
      "venue" : "Nature neuroscience, 2(11):1019–1025",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Overfeat: Integrated recognition, localization and detection using convolutional networks",
      "author" : [ "P. Sermanet", "D. Eigen", "X. Zhang", "M. Mathieu", "R. Fergus", "Y. LeCun" ],
      "venue" : null,
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2014
    }, {
      "title" : "Stochastic pooling for regularization of deep convolutional neural networks",
      "author" : [ "M. Zeiler", "R. Fergus" ],
      "venue" : null,
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2013
    }, {
      "title" : "Visualizing and understanding convolutional networks",
      "author" : [ "M. Zeiler", "R. Fergus" ],
      "venue" : "arXiv",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Deconvolutional networks",
      "author" : [ "M. Zeiler", "D. Krishnan", "G. Taylor", "R. Fergus" ],
      "venue" : "Proc. CVPR",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 14,
      "context" : "[15] convincingly demonstrated that deep neural networks can be very effective in classifying images in the challenging Imagenet benchmark [5], significantly outperforming computer vision systems built on top of engineered features like SIFT [20].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 4,
      "context" : "[15] convincingly demonstrated that deep neural networks can be very effective in classifying images in the challenging Imagenet benchmark [5], significantly outperforming computer vision systems built on top of engineered features like SIFT [20].",
      "startOffset" : 139,
      "endOffset" : 142
    }, {
      "referenceID" : 18,
      "context" : "[15] convincingly demonstrated that deep neural networks can be very effective in classifying images in the challenging Imagenet benchmark [5], significantly outperforming computer vision systems built on top of engineered features like SIFT [20].",
      "startOffset" : 242,
      "endOffset" : 246
    }, {
      "referenceID" : 26,
      "context" : "Subsequent work has improved our understanding and has refined certain aspects of this class of models [28].",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 3,
      "context" : "A number of different studies have further shown that the features learned by deep neural networks are generic and can be successfully employed in a black-box fashion in other datasets or tasks such as image detection [4, 7, 22, 24, 26, 28].",
      "startOffset" : 218,
      "endOffset" : 240
    }, {
      "referenceID" : 6,
      "context" : "A number of different studies have further shown that the features learned by deep neural networks are generic and can be successfully employed in a black-box fashion in other datasets or tasks such as image detection [4, 7, 22, 24, 26, 28].",
      "startOffset" : 218,
      "endOffset" : 240
    }, {
      "referenceID" : 20,
      "context" : "A number of different studies have further shown that the features learned by deep neural networks are generic and can be successfully employed in a black-box fashion in other datasets or tasks such as image detection [4, 7, 22, 24, 26, 28].",
      "startOffset" : 218,
      "endOffset" : 240
    }, {
      "referenceID" : 22,
      "context" : "A number of different studies have further shown that the features learned by deep neural networks are generic and can be successfully employed in a black-box fashion in other datasets or tasks such as image detection [4, 7, 22, 24, 26, 28].",
      "startOffset" : 218,
      "endOffset" : 240
    }, {
      "referenceID" : 24,
      "context" : "A number of different studies have further shown that the features learned by deep neural networks are generic and can be successfully employed in a black-box fashion in other datasets or tasks such as image detection [4, 7, 22, 24, 26, 28].",
      "startOffset" : 218,
      "endOffset" : 240
    }, {
      "referenceID" : 26,
      "context" : "A number of different studies have further shown that the features learned by deep neural networks are generic and can be successfully employed in a black-box fashion in other datasets or tasks such as image detection [4, 7, 22, 24, 26, 28].",
      "startOffset" : 218,
      "endOffset" : 240
    }, {
      "referenceID" : 14,
      "context" : "Their recent success is partly due to the availability of large annotated datasets and fast GPU computing, and partly due to some important methodological developments such as dropout regularization and rectifier linear activations [15].",
      "startOffset" : 232,
      "endOffset" : 236
    }, {
      "referenceID" : 16,
      "context" : "However, the key building blocks of deep neural networks for images have been around for many years [17]: (1) convolutional multi-layer neural networks with small receptive fields that spatially share parameters within each layer.",
      "startOffset" : 100,
      "endOffset" : 104
    }, {
      "referenceID" : 9,
      "context" : "(2) Gradual abstraction and spatial resolution reduction after each convolutional layer as we ascend the network hierarchy, most effectively via maxpooling [10, 25].",
      "startOffset" : 156,
      "endOffset" : 164
    }, {
      "referenceID" : 23,
      "context" : "(2) Gradual abstraction and spatial resolution reduction after each convolutional layer as we ascend the network hierarchy, most effectively via maxpooling [10, 25].",
      "startOffset" : 156,
      "endOffset" : 164
    }, {
      "referenceID" : 11,
      "context" : "In this work we build a deep neural network around the epitomic representation [12].",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 14,
      "context" : "We train the model by error backpropagation to minimize the classification log-loss, similarly to [15].",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 14,
      "context" : "Note that the error rate of the original model in [15] is 18.",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 26,
      "context" : "We also find that the proposed epitomic model converges faster, especially when the filters in the dictionary are mean- and contrast-normalized, which is related to [28].",
      "startOffset" : 165,
      "endOffset" : 169
    }, {
      "referenceID" : 11,
      "context" : "Related work Our model builds on the epitomic image representation [12], which was initially geared towards image and video modeling tasks.",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 0,
      "context" : "Single-level dictionaries of image epitomes learned in an unsupervised fashion for image denoising have been explored in [1, 2].",
      "startOffset" : 121,
      "endOffset" : 127
    }, {
      "referenceID" : 1,
      "context" : "Single-level dictionaries of image epitomes learned in an unsupervised fashion for image denoising have been explored in [1, 2].",
      "startOffset" : 121,
      "endOffset" : 127
    }, {
      "referenceID" : 21,
      "context" : "Recently, single-level mini-epitomes learned by a variant of K-means have been proposed as an alternative to SIFT for image classification [23].",
      "startOffset" : 139,
      "endOffset" : 143
    }, {
      "referenceID" : 7,
      "context" : "The proposed epitomic model is closely related to maxout networks [8].",
      "startOffset" : 66,
      "endOffset" : 69
    }, {
      "referenceID" : 7,
      "context" : "Maxout is typically used in conjunction with max-pooling [8], while epitomes fully substitute for it.",
      "startOffset" : 57,
      "endOffset" : 60
    }, {
      "referenceID" : 14,
      "context" : "On the contrary, we have found that learning deep epitomic networks does not require dropout in the convolutional layers – similarly to [15], we only use dropout regularization in the fully connected layers of our network.",
      "startOffset" : 136,
      "endOffset" : 140
    }, {
      "referenceID" : 25,
      "context" : "Stochastic pooling [27] has been proposed in conjunction with supervised learning.",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 17,
      "context" : "Probabilistic pooling [19] and deconvolutional networks [29] have been proposed before in conjunction with unsupervised learning, avoiding the theoretical and practical difficulties associated with building probabilistic models on top of max-pooling.",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 27,
      "context" : "Probabilistic pooling [19] and deconvolutional networks [29] have been proposed before in conjunction with unsupervised learning, avoiding the theoretical and practical difficulties associated with building probabilistic models on top of max-pooling.",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 8,
      "context" : "This relates the proposed model to topographic ICA [9] and related models [13, 16, 21], which are typically trained to optimize unsupervised objectives.",
      "startOffset" : 51,
      "endOffset" : 54
    }, {
      "referenceID" : 12,
      "context" : "This relates the proposed model to topographic ICA [9] and related models [13, 16, 21], which are typically trained to optimize unsupervised objectives.",
      "startOffset" : 74,
      "endOffset" : 86
    }, {
      "referenceID" : 15,
      "context" : "This relates the proposed model to topographic ICA [9] and related models [13, 16, 21], which are typically trained to optimize unsupervised objectives.",
      "startOffset" : 74,
      "endOffset" : 86
    }, {
      "referenceID" : 19,
      "context" : "This relates the proposed model to topographic ICA [9] and related models [13, 16, 21], which are typically trained to optimize unsupervised objectives.",
      "startOffset" : 74,
      "endOffset" : 86
    }, {
      "referenceID" : 7,
      "context" : "Computing the maximum response over filters rather than image positions resembles the maxout scheme of [8], yet in the proposed model the filters within the epitome are constrained to share values in their area of overlap.",
      "startOffset" : 103,
      "endOffset" : 106
    }, {
      "referenceID" : 14,
      "context" : "Similarly to [15], the final two layers of our network for Imagenet image classification are fully connected and are regularized by dropout.",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 26,
      "context" : "Motivated by [28], we have also explored the effect of filter mean and contrast normalization on deep epitomic network training.",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 26,
      "context" : "Note that the contrast normalization explored here is slightly different than the one in [28], who only scale down the filters whenever their contrast exceeds a pre-defined threshold.",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 4,
      "context" : "We have performed most of our experimental investigation on the Imagenet ILSVRC-2012 dataset [5], focusing on the task of image classification.",
      "startOffset" : 93,
      "endOffset" : 96
    }, {
      "referenceID" : 3,
      "context" : "Similarly to other recent works [4, 24, 28], we also evaluate deep epitomic networks trained on Imagenet as a black-box visual feature front-end on the Caltech-101 benchmark [6].",
      "startOffset" : 32,
      "endOffset" : 43
    }, {
      "referenceID" : 22,
      "context" : "Similarly to other recent works [4, 24, 28], we also evaluate deep epitomic networks trained on Imagenet as a black-box visual feature front-end on the Caltech-101 benchmark [6].",
      "startOffset" : 32,
      "endOffset" : 43
    }, {
      "referenceID" : 26,
      "context" : "Similarly to other recent works [4, 24, 28], we also evaluate deep epitomic networks trained on Imagenet as a black-box visual feature front-end on the Caltech-101 benchmark [6].",
      "startOffset" : 32,
      "endOffset" : 43
    }, {
      "referenceID" : 5,
      "context" : "Similarly to other recent works [4, 24, 28], we also evaluate deep epitomic networks trained on Imagenet as a black-box visual feature front-end on the Caltech-101 benchmark [6].",
      "startOffset" : 174,
      "endOffset" : 177
    }, {
      "referenceID" : 13,
      "context" : "We further consider two standard classification benchmarks involving thumbnail-sized images, the MNIST digit [18] and the CIFAR10 [14], both involving classification into 10 possible classes.",
      "startOffset" : 130,
      "endOffset" : 134
    }, {
      "referenceID" : 14,
      "context" : "Similarly to [15], we apply local response normalization (LRN) to the output of the first two convolutional layers and dropout to the output of the two fully-connected layers.",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 24,
      "context" : "It has a similar structure with the Overfeat model [26], yet significantly fewer neurons in the convolutional layers 2 to 6.",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 24,
      "context" : "Another difference with [26] is the use of LRN, which to our experience facilitates training.",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 14,
      "context" : "We followed the methodology of [15] in training our models.",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 3,
      "context" : "Similarly to [4], we resized the training images to have small dimension equal to 256 pixels while preserving their aspect ratio and not cropping their large dimension.",
      "startOffset" : 13,
      "endOffset" : 16
    }, {
      "referenceID" : 14,
      "context" : "5, also injecting global color noise exactly as in [15].",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 14,
      "context" : "Model Krizhevsky Zeiler-Fergus Overfeat Max-Pool Max-Pool Epitomic Epitomic Topographic [15] [28] [26] + norm + norm + norm Top-5 Error 18.",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 26,
      "context" : "Model Krizhevsky Zeiler-Fergus Overfeat Max-Pool Max-Pool Epitomic Epitomic Topographic [15] [28] [26] + norm + norm + norm Top-5 Error 18.",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 24,
      "context" : "Model Krizhevsky Zeiler-Fergus Overfeat Max-Pool Max-Pool Epitomic Epitomic Topographic [15] [28] [26] + norm + norm + norm Top-5 Error 18.",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 26,
      "context" : "Model Zeiler-Fergus Max-Pool Max-Pool Epitomic Epitomic Topographic [28] + norm + norm + norm Mean Accuracy 86.",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 14,
      "context" : "We report at Table 4 our results on the Imagenet ILSVRC-2012 benchmark, also including results previously reported in the literature [15,26,28].",
      "startOffset" : 133,
      "endOffset" : 143
    }, {
      "referenceID" : 24,
      "context" : "We report at Table 4 our results on the Imagenet ILSVRC-2012 benchmark, also including results previously reported in the literature [15,26,28].",
      "startOffset" : 133,
      "endOffset" : 143
    }, {
      "referenceID" : 26,
      "context" : "We report at Table 4 our results on the Imagenet ILSVRC-2012 benchmark, also including results previously reported in the literature [15,26,28].",
      "startOffset" : 133,
      "endOffset" : 143
    }, {
      "referenceID" : 14,
      "context" : "4% error rate, which however is still better than [15,28].",
      "startOffset" : 50,
      "endOffset" : 57
    }, {
      "referenceID" : 26,
      "context" : "4% error rate, which however is still better than [15,28].",
      "startOffset" : 50,
      "endOffset" : 57
    }, {
      "referenceID" : 24,
      "context" : "The improved performance that we got with the Max-Pool baseline network compared to Overfeat [26] is most likely due to our use of LRN and aspect ratio preserving image resizing.",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 3,
      "context" : "When preparing this manuscript, we became aware of the work of [4] that reports an even lower 13.",
      "startOffset" : 63,
      "endOffset" : 66
    }, {
      "referenceID" : 2,
      "context" : "We trained a 102-way SVM classifier using libsvm [3] and the default regularization parameter.",
      "startOffset" : 49,
      "endOffset" : 52
    }, {
      "referenceID" : 7,
      "context" : "Our results are comparable to maxout [8], which achieves state-of-art results on these tasks.",
      "startOffset" : 37,
      "endOffset" : 40
    }, {
      "referenceID" : 14,
      "context" : "[15].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 7,
      "context" : "Table 6: Classification error rates on small image datasets for maxout [8] and the proposed miniepitomic deep network: (a) MNIST.",
      "startOffset" : 71,
      "endOffset" : 74
    }, {
      "referenceID" : 10,
      "context" : "Reproducibility We implemented the proposed methods by extending the excellent Caffe software framework [11].",
      "startOffset" : 104,
      "endOffset" : 108
    } ],
    "year" : 2014,
    "abstractText" : "Deep convolutional neural networks have recently proven extremely competitive in challenging image recognition tasks. This paper proposes the epitomic convolution as a new building block for deep neural networks. An epitomic convolution layer replaces a pair of consecutive convolution and max-pooling layers found in standard deep convolutional neural networks. The main version of the proposed model uses mini-epitomes in place of filters and computes responses invariant to small translations by epitomic search instead of max-pooling over image positions. The topographic version of the proposed model uses large epitomes to learn filter maps organized in translational topographies. We show that error backpropagation can successfully learn multiple epitomic layers in a supervised fashion. The effectiveness of the proposed method is assessed in image classification tasks on standard benchmarks. Our experiments on Imagenet indicate improved recognition performance compared to standard convolutional neural networks of similar architecture. Our models pre-trained on Imagenet perform excellently on Caltech-101. We also obtain competitive image classification results on the smallimage MNIST and CIFAR-10 datasets.",
    "creator" : "gnuplot 4.2 patchlevel 6 "
  }
}