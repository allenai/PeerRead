{
  "name" : "1603.09025.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Recurrent Batch Normalization",
    "authors" : [ "Tim Cooijmans", "Nicolas Ballas", "César Laurent", "Aaron Courville" ],
    "emails" : [ "firstname.lastname@umontreal.ca" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Recurrent neural network architectures such as LSTM [10] and GRU [6] have recently exhibited state-of-the-art performance on a wide range of complex sequential problems including speech recognition [1], machine translation [3] and image and video captioning [26, 27]. Top-performing models, however, are based on very high-capacity networks that are computationally intensive and costly to train. Effective optimization of recurrent neural networks is an active area of study [22, 18, 20].\nIt is known that for deep feed-forward neural networks, covariate shift [23, 11] degrades the efficiency of training. Covariate shift is a change in distribution of the inputs to a model. This occurs continuously during training of feed-forward neural networks, where changing the parameters of a layer affects the distribution of the inputs to all layers above it. As a result, the upper layers are continually adapting to the shifting input distribution and unable to learn effectively. This internal covariate shift [11] may play an especially important role in recurrent neural networks, which resemble very deep feed-forward networks.\nBatch normalization [11] is a recently proposed technique for controlling the distributions of feedforward neural network activations, thereby reducing internal covariate shift. It involves standardizing the activations going into each layer, enforcing their means and variances to be invariant to changes in the parameters of the underlying layers. This effectively decouples each layer’s parameters from those of other layers, leading to a better-conditioned optimization problem. Indeed, deep neural networks trained with batch normalization converge significantly faster and generalize better.\nAlthough batch normalization has demonstrated significant training speed-ups and generalization benefits in feedforward networks, it has proven difficult to apply in recurrent architectures [14, 1]. It has found limited use in stacked RNNs, where the normalization is applied “vertically”, i.e. to the input of each RNN, but not “horizontally” between timesteps. RNNs are deepest in the time direction, and as such batch normalization would be most beneficial when applied horizontally. However, it has been hypothesized [14] that applying batch normalization in this way hurts training because of exploding gradients due to repeated rescaling.\nar X\niv :1\n60 3.\n09 02\n5v 1\n[ cs\n.L G\n] 3\n0 M\nar 2\nOur findings run counter to this hypothesis. We show that it is both possible and highly beneficial to apply batch normalization in the hidden-to-hidden transition of recurrent models. In particular, we describe a re-parameterization of LSTM that involves batch normalization and demonstrate that doing so speeds up optimization and improves generalization. In addition, we empirically analyze the gradient backpropagation and show that proper initialization of the batch normalization parameters is crucial to avoiding vanishing gradient.\nWe evaluate our proposal on several sequential problems and show that our LSTM reparameterization consistently outperforms the LSTM baseline across tasks, in terms of both time to convergence and performance. In particular, we achieve state-of-the-art performance on Sequential MNIST [15] and character-level language modeling [19] on Penn Treebank [17] and text8 [16].\nSection 2 introduces RNNs and batch normalization in detail. In Section 3, we discuss batch normalization in the recurrent setting. Section 4 investigates the impact of the batch normalization scale parameter on the gradient back-propagation for recurrent models . We show in Section 5 our evaluations of the proposed re-parameterization on a variety of tasks."
    }, {
      "heading" : "2 Prerequisites",
      "text" : ""
    }, {
      "heading" : "2.1 LSTM",
      "text" : "Long Short-Term Memory (LSTM) networks are an instance of a more general class of recurrent neural networks (RNNs), which we shall briefly review. Given an input sequence X = (x1,x2, ...xT ), an RNN defines a sequence of hidden states ht according to\nht = φ(Whht−1 +Wxxt + b), (1)\nwhere Wh ∈ Rdh×dh ,Wx ∈ Rdx×dh ,b ∈ Rdh and the initial state h0 ∈ Rdh are model parameters. A popular choice for the activation function φ is tanh.\nRNNs are popular in sequence modeling because of their natural ability to process variable-length sequences. Training RNNs using first-order stochastic gradient descent (SGD) however is notoriously difficult due to the well-known problem of exploding/vanishing gradients [5, 9, 22]. Gradient vanishing occurs when states ht are not influenced by small changes in much earlier states hτ , t τ , preventing learning of long-term dependencies in the input data. While the long-term dependencies problem is unsolvable in absolute [5], it has been demonstrated that different RNN re-parameterizations, such as the LSTM [10], GRU [6] or i/u-RNN [15, 2] can help mitigate the vanishing gradient problem.\nIn what follows, we focus on the LSTM architecture [10] with recurrent transition given by f̃t ĩt õt g̃t  = Whht−1 +Wxxt + b (2) ct = σ(f̃t) ct−1 + σ(̃it) tanh(g̃t) (3) ht = σ(õt) tanh(ct), (4)\nwhere Wh ∈ Rdh×4dh ,WxRdx×4dh ,b ∈ R4dh and the initial states h0 ∈ Rdh , c0 ∈ Rdh are model parameters. σ is the logistic sigmoid function, and the operator denotes the Hadamard product.\nThe LSTM differs from simple RNNs in that it has an additional memory cell ct whose update is nearly linear which allows the gradient to flow back through time more easily. In addition, unlike the RNN which overwrites its content at each time-step, the update of the LSTM cell is regulated by a set of gates. The forget gate ft determines the extent to which information is carried over from the previous time-step, and the input gate it controls the flow of information from the current input xt. The output gate ot allows the model to read from the cell. This carefully controlled interaction with the cell is what allows the LSTM to robustly retain information for long periods of time."
    }, {
      "heading" : "2.2 Batch Normalization",
      "text" : "Covariate shift [23] is a phenomenon in machine learning where the features presented to a model change in distribution during the course of training. In order for learning to succeed in the presence of covariance shift, the model’s parameters must be adjusted not just to learn the concept at hand but also to adapt to the changing distribution of the inputs. In deep neural networks, this problem manifests as internal covariance shift [11], where changing the parameters of a layer affects the distribution of the inputs to all layers above it.\nBatch Normalization [11] is a recently proposed network re-parameterization that aims to reduce internal covariate shift. It does so by standardizing the activations using statistical estimates of their means and standard deviations. However, it does not decorrelate the activations as the matrix inversion involved would be too expensive.\nThe batch normalizing transform is as follows:\nBN(h; γ, β) = β + γ h− Ê(h)√ V̂ar(h) +\n(5)\nwhere h ∈ Rd is the vector of (pre)activations to be normalized, γ ∈ Rd, β ∈ Rd are model parameters that determine the mean and standard deviation of the normalized activation, and ∈ R is a regularization hyperparameter. The division should be understood to proceed elementwise.\nAt training time, the statistics E(h) and Var(h) are estimated by the sample mean and sample variance of the current minibatch. This allows for backpropagation through the statistics, preserving the convergence properties of stochastic gradient descent. During inference, the statistics are typically estimated based on the entire training set, so as to produce a deterministic prediction."
    }, {
      "heading" : "3 Batch-Normalized LSTM",
      "text" : "This section introduces a re-parameterization of LSTM that takes advantage of batch normalization. Contrary to previous work [14, 1], we leverage batch normalization in both the input-to-hidden and\nthe hidden-to-hidden transformations. We introduce the batch-normalizing transform BN( · ; γ, β) into the LSTM as follows:\n f̃t ĩt õt g̃t  = BN(Whht−1; γh, βh) + BN(Wxxt; γx, βx) + b (6) ct = σ(f̃t) ct−1 + σ(̃it) tanh(g̃t) (7) ht = σ(õt) tanh(BN(ct; γc, βc)) (8)\nIn our formulation, we normalize the recurrent term Whht−1 and the input term Wxxt separately. Normalizing these terms individually gives the model better control over the relative contribution of the terms using the γh and γx parameters. We set βh = βx = 0 to avoid unnecessary redundancy, instead relying on the pre-existing parameter vector b to account for both biases. In order to leave the LSTM dynamics intact and preserve the gradient flow through ct, we do not apply batch normalization on the cell states.\nThe batch normalization transform relies on batch statistics to standardize the LSTM activations. It would seem natural to share the statistics that are used for normalization across time, just as recurrent neural networks share their parameters over time. However, we have found that simply averaging statistics over time severely degrades performance. Although LSTM activations do converge to a stationary distribution, we have empirically observed that their statistics during the initial transient differ significantly as figure 1 shows. Consequently, we recommend using separate statistics for each timestep to preserve information of the initial transient phase in the activations.\nGeneralizing the model to sequences longer than those seen during training is straightforward thanks to the rapid convergence of the activations to their steady-state distributions (cf. figure 1). For our experiments we estimate the population statistics separately for each timestep 1, . . . , Tmax where Tmax is the length of the longest training sequence. When at test time we need to generalize beyond Tmax, we use the population statistic of time Tmax for all time steps beyond it.\nDuring training we estimate the statistics across the minibatch, independently for each timestep. At test time we use estimates obtained by averaging the minibatch estimates over the training set."
    }, {
      "heading" : "4 Initializing γ for Gradient Flow",
      "text" : "Although batch normalization allows for easy control of the pre-activation variance through the γ parameters, common practice is to normalize to unit variance. We suspect that the previous difficulties with recurrent batch normalization reported in the literature [14, 1] are largely due to improper initialization of the batch normalization parameters, and γ in particular. In this section we demonstrate the impact of γ on gradient flow.\nIn Figure 2(a) we show how the pre-activation variance impacts gradient propagation in a simple RNN on the sequential MNIST task described in Section 5.1. Since backpropagation operates in reverse, the plot is best read from right to left. The quantity plotted is the norm of the gradient of the loss with respect to the hidden state at different time steps. For large values of γ, the norm quickly goes to zero as gradient is propagated back in time. For small values of γ the norm is nearly constant.\nFigure 2(b) shows empirically how the expected derivative of the tanh nonlinearity changes with the variance of the argument. When the input variance is low, the input tends to be close to the origin where the derivative is close to 1. As the variance increases, the expected derivative decreases as the input is more likely to be in the saturation regime. At unit variance, the expected derivative is much smaller than 1.\nWe conjecture that this is what causes the gradient to vanish, and recommend initializing γ to a small value. In our trials we found that values of 0.01 or lower caused instabilities during training. Our choice of 0.1 seems to work well across tasks."
    }, {
      "heading" : "5 Experiments",
      "text" : "This section presents an empirical evaluation of the proposed batch-normalized LSTM on four different tasks. Results show that batch-normalized LSTM achieves convergence faster than a baseline LSTM on all the tasks, and, can also lead to better generalization.\nNote that for all the experiments, we initialize the batch normalization scale and shift parameters γ and β to 0.1 and 0 respectively."
    }, {
      "heading" : "5.1 Sequential MNIST",
      "text" : "We evaluate our batch-normalized LSTM on a sequential version of the MNIST classification task [15]. The model processes each image one pixel at a time and finally predicts the label. We consider both sequential MNIST tasks, MNIST and permuted MNIST (pMNIST). In MNIST, the pixels are processed from left to right, top to bottom. In pMNIST the pixels are processed in a fixed random order.\nModel text8\ntd-LSTM [28] 1.63 HF-MRNN [19] 1.54 skipping RNN [21] 1.48\nBN-LSTM (ours) 1.39\nTable 3: Bits-per-character on the text8 test sequence.\nOur baseline consists of an LSTM with 100 hidden units, with a softmax classifier to produce a prediction from the final hidden state. We use orthogonal initialization for all weight matrices, except for the hidden-to-hidden weight matrix which we initialize to be the identity matrix, as this yields better generalization performance on this task for both models. The model is trained using RMSProp [24] with learning rate of 10−3 and 0.9 momentum. We apply gradient clipping at 1 to avoid exploding gradients.\nThe in-order MNIST task poses a unique problem for our model: the input for the first hundred or so timesteps is constant across examples since the upper pixels are almost always black. This causes the variance of the hidden states to be exactly zero for a long period of time. Normalizing these zero-variance activations involves division by a small number at many timesteps, which causes the gradient to explode. We work around this by injecting noise into the initial hidden states. Although the normalization amplifies the noise to signal level, we find that it does not hurt performance compared to data-dependent ways of initializing the hidden states.\nIn Figure 3 we show the validation accuracy while training for both LSTM and batch-normalized LSTM (BN-LSTM). BN-LSTM converges faster than LSTM on both tasks. Additionally, we observe that BN-LSTM generalizes significantly better on pMNIST. It has been highlighted in [2] that pMNIST contains many longer term dependencies across pixels than in the original pixel ordering, where a lot of structure is local. A recurrent network therefore needs to characterize dependencies across varying time scales in order to solve this tasks. Our results suggest that BN-LSTM is better able to capture these long-term dependencies.\nTable 1 reports the test set accuracy of the early stop model for LSTM and BN-LSTM using the population statistics. Recurrent batch normalization leads to better test score, especially for pMNIST where models have to leverage long-term temporal depencies. In addition, Table 1 shows that our batch-normalized LSTM achieves state of the art on both MNIST and pMNIST."
    }, {
      "heading" : "5.2 Character-level Penn Treebank",
      "text" : "We evaluate our model on the task of character-level language modeling on the Penn Treebank corpus [17] according to the train/valid/test partition of [19]. For training we segment the training sequence into examples of length 100. The training sequence does not cleanly divide by 100, so for each epoch we randomly crop a subsequence that does and segment that instead.\nOur baseline is an LSTM with 1000 units, trained to predict the next character using a softmax classifier on the hidden state ht. We use stochastic gradient descent on minibatches of size 64, with gradient clipping at 1.0 and step rule determined by Adam [12] with learning rate 0.002. We use orthogonal initialization for all weight matrices. The setup for the batch-normalized LSTM is the same in all respects except for the introduction of batch normalization as detailed in 3.\nWe show the learning curves in figure 4(a). BN-LSTM converges faster and generalizes better than the LSTM baseline. Figure 4(b) shows the generalization of our model to longer sequences. We observe that using the population statistics improves generalization performance, which confirms that repeating the last population statistic (cf. section 3) is a viable strategy. In table 2 we report the performance of our best models (early stopped on validation performance) on the Penn Treebank test sequence and compare them to previous work.1 It shows that our BN-LSTM model compares favorably to state-of-art models."
    }, {
      "heading" : "5.3 Text8",
      "text" : "We evaluate our model on a second character-level language modeling task on the text8 dataset [16]. This dataset is derived from Wikipedia and consists of a sequence of 100M characters including only alphabetical characters and spaces. We follow previous authors [19, 28] and use the first 90M characters for training, the next 5M for validation and the final 5M characters for testing. We train on nonoverlapping sequences of length 180.\nOur model is a BN-LSTM with 2000 units, trained to predict the next character using a softmax classifier on the hidden state ht. We use stochastic gradient descent on minibatches of size 64, with gradient clipping at 1.0 and step rule determined by Adam [12] with learning rate 0.01. All weight matrices were initialized to be orthogonal.\nWe early-stop on validation performance and report the test performance of the resulting model in table 3. Our model achieves state of the art."
    }, {
      "heading" : "5.4 Teaching Machines to Read and Comprehend",
      "text" : "To demonstrate the generality and practical applicability of our re-parameterization, we explore the use of batch-normalized LSTM in a unidirectional Attentive Reader Model [8].2 We evaluate two variants. The first variant, referred to as BN-LSTM, consists of the vanilla Attentive Reader model with the LSTM simply replaced by our BN-LSTM re-parameterization. We further introduce\n1We do not compare with [7] as they evaluate in a different setting. 2We make use of the existing implementation available at https://github.com/caglar/\nAttentive_reader.\nbatch normalization into the attention computations, normalizing each term going into the tanh nonlinearities, to obtain another variant which we term BN-everywhere.\nAll three variants are trained using the exact same hyperparameters. The hidden state is composed of 240 units. We use stochastic gradient descent on minibatches of size 64, with gradient clipping at 10 and step rule determined by Adam [12] with learning rate 8× 10−5. We evaluate the Attentive Reader Model along with our batch-normalized variants on the question answering task using the CNN corpus, with placeholders to replace the named entities. We follow a similar preprocessing pipeline to [8]. However contrary to [8], we limit the number of sentences in each passage to 4 in order to save computation. We choose which sentences to keep by performing a string matching between the question and the passage. We rank the sentences in the text according to the string matching score and use only the top 4 sentences for each passage in the dataset. The training, validation and test sets are all preprocessed using this same procedure. After this procedure, the validation set has a recall of 57% – the passage contains the answer in only 57% of the passage/question pairs. This puts an upper bound on the accuracy that can be achieved.\nDuring the training, we randomly sample the examples with replacement and shuffle the order of the placeholders in each text inside the minibatch. We use the whole vocabulary for the input and the answer which consists of 65829 unique words.\nFigure 5 shows the learning curves for the different variants of the attentive reader. BN-LSTM trains dramatically faster than the LSTM baseline. BN-everywhere, applying batch normalization in the attention computations as well, in turn shows a significant improvement over BN-LSTM. In addition, both BN-LSTM and BN-everywhere show a generalization benefit over the baseline. We emphasize that these are preliminary results on the validation set; full results are forthcoming."
    }, {
      "heading" : "6 Conclusion",
      "text" : "Contrary to previous findings [14, 1], we have demonstrated that batch-normalizing the hidden states of recurrent neural networks greatly improves optimization. Indeed, doing so yields benefits similar to those of batch normalization in feed-forward neural networks: our proposed BN-LSTM trains faster and generalizes better on a variety of tasks including language modeling and questionanswering. We have argued that proper initialization of the batch normalization parameters is crucial, and suggest that previous difficulties [14, 1] were due in large part to improper initialization."
    }, {
      "heading" : "Acknowledgements",
      "text" : "The authors would like to acknowledge the support of the following agencies for research funding and computing support: NSERC, Calcul Québec, Compute Canada, the Canada Research Chairs and CIFAR. We would also like to thank the developers of Theano [4] and the Blocks and Fuel [25] libraries, for developing such powerful tools for scientific computing. We thank Çağlar Gülçehre for sharing his implementation of Attentive Reader and for helping us with experiments, and David Krueger, Saizheng Zhang, Ishmael Belghazi and Yoshua Bengio for discussions and suggestions."
    } ],
    "references" : [ {
      "title" : "Deep speech 2: Endto-end speech recognition in english and mandarin",
      "author" : [ "Dario Amodei", "Rishita Anubhai", "Eric Battenberg", "Carl Case", "Jared Casper", "Bryan Catanzaro", "Jingdong Chen", "Mike Chrzanowski", "Adam Coates", "Greg Diamos" ],
      "venue" : "arXiv preprint arXiv:1512.02595,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2015
    }, {
      "title" : "Unitary evolution recurrent neural networks",
      "author" : [ "Martin Arjovsky", "Amar Shah", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1511.06464,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2015
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "D. Bahdanau", "K. Cho", "Y. Bengio" ],
      "venue" : "arXiv preprint arXiv:1409.0473,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2014
    }, {
      "title" : "Theano: new features and speed improvements",
      "author" : [ "Frédéric Bastien", "Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian J. Goodfellow", "Arnaud Bergeron", "Nicolas Bouchard", "Yoshua Bengio" ],
      "venue" : "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2012
    }, {
      "title" : "Learning long-term dependencies with gradient descent is difficult",
      "author" : [ "Y. Bengio", "P. Simard", "P. Frasconi" ],
      "venue" : "Neural Networks, IEEE Transactions on,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1994
    }, {
      "title" : "Learning phrase representations using rnn encoder-decoder for statistical machine translation",
      "author" : [ "K. Cho", "B. Van Merriënboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio" ],
      "venue" : "arXiv preprint arXiv:1406.1078,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2014
    }, {
      "title" : "Generating sequences with recurrent neural networks",
      "author" : [ "Alex Graves" ],
      "venue" : "arXiv preprint arXiv:1308.0850,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2013
    }, {
      "title" : "Teaching machines to read and comprehend",
      "author" : [ "Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2015
    }, {
      "title" : "Untersuchungen zu dynamischen neuronalen netzen",
      "author" : [ "Sepp Hochreiter" ],
      "venue" : "Master’s thesis, Institut fur Informatik, Technische Universitat,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1991
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1997
    }, {
      "title" : "Batch normalization: Accelerating deep network training by reducing internal covariate",
      "author" : [ "Sergey Ioffe", "Christian Szegedy" ],
      "venue" : "shift. CoRR,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2015
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik Kingma", "Jimmy Ba" ],
      "venue" : "arXiv preprint arXiv:1412.6980,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2014
    }, {
      "title" : "Regularizing rnns by stabilizing activations",
      "author" : [ "David Krueger", "Roland Memisevic" ],
      "venue" : "arXiv preprint arXiv:1511.08400,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2015
    }, {
      "title" : "Batch normalized recurrent neural networks",
      "author" : [ "César Laurent", "Gabriel Pereyra", "Philémon Brakel", "Ying Zhang", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1510.01378,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2015
    }, {
      "title" : "A simple way to initialize recurrent networks of rectified linear units",
      "author" : [ "Quoc V Le", "Navdeep Jaitly", "Geoffrey E Hinton" ],
      "venue" : "arXiv preprint arXiv:1504.00941,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2015
    }, {
      "title" : "Large text compression benchmark",
      "author" : [ "Matt Mahoney" ],
      "venue" : "URL: http://www. mattmahoney. net/text/text. html,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2009
    }, {
      "title" : "Building a large annotated corpus of english: The penn treebank",
      "author" : [ "Mitchell P. Marcus", "Mary Ann Marcinkiewicz", "Beatrice Santorini" ],
      "venue" : "Comput. Linguist.,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 1993
    }, {
      "title" : "Learning recurrent neural networks with hessian-free optimization",
      "author" : [ "James Martens", "Ilya Sutskever" ],
      "venue" : "In Proceedings of the 28th International Conference on Machine Learning (ICML-",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2011
    }, {
      "title" : "Subword language modeling with neural networks. preprint (http://www",
      "author" : [ "Tomáš Mikolov", "Ilya Sutskever", "Anoop Deoras", "Hai-Son Le", "Stefan Kombrink", "J Cernocky" ],
      "venue" : "fit. vutbr. cz/imikolov/rnnlm/char. pdf),",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2012
    }, {
      "title" : "Persistent contextual neural networks for learning symbolic data sequences",
      "author" : [ "Yann Ollivier" ],
      "venue" : "CoRR, abs/1306.0514,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2013
    }, {
      "title" : "Regularization and nonlinearities for neural language models: when are they needed",
      "author" : [ "Marius Pachitariu", "Maneesh Sahani" ],
      "venue" : "arXiv preprint arXiv:1301.5650,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2013
    }, {
      "title" : "On the difficulty of training recurrent neural networks",
      "author" : [ "Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1211.5063,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2012
    }, {
      "title" : "Improving predictive inference under covariate shift by weighting the log-likelihood function",
      "author" : [ "Hidetoshi Shimodaira" ],
      "venue" : "Journal of statistical planning and inference,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2000
    }, {
      "title" : "Lecture 6.5—RmsProp: Divide the gradient by a running average of its recent magnitude",
      "author" : [ "T. Tieleman", "G. Hinton" ],
      "venue" : "COURSERA: Neural Networks for Machine Learning,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2012
    }, {
      "title" : "Show, attend and tell: Neural image caption generation with visual attention",
      "author" : [ "Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Aaron Courville", "Ruslan Salakhutdinov", "Richard Zemel", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1502.03044,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2015
    }, {
      "title" : "Describing videos by exploiting temporal structure",
      "author" : [ "Li Yao", "Atousa Torabi", "Kyunghyun Cho", "Nicolas Ballas", "Christopher Pal", "Hugo Larochelle", "Aaron Courville" ],
      "venue" : "In Proceedings of the IEEE International Conference on Computer Vision,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2015
    }, {
      "title" : "Architectural complexity measures of recurrent neural networks",
      "author" : [ "Saizheng Zhang", "Yuhuai Wu", "Tong Che", "Zhouhan Lin", "Roland Memisevic", "Ruslan Salakhutdinov", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1602.08210,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 9,
      "context" : "Recurrent neural network architectures such as LSTM [10] and GRU [6] have recently exhibited state-of-the-art performance on a wide range of complex sequential problems including speech recognition [1], machine translation [3] and image and video captioning [26, 27].",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 5,
      "context" : "Recurrent neural network architectures such as LSTM [10] and GRU [6] have recently exhibited state-of-the-art performance on a wide range of complex sequential problems including speech recognition [1], machine translation [3] and image and video captioning [26, 27].",
      "startOffset" : 65,
      "endOffset" : 68
    }, {
      "referenceID" : 0,
      "context" : "Recurrent neural network architectures such as LSTM [10] and GRU [6] have recently exhibited state-of-the-art performance on a wide range of complex sequential problems including speech recognition [1], machine translation [3] and image and video captioning [26, 27].",
      "startOffset" : 198,
      "endOffset" : 201
    }, {
      "referenceID" : 2,
      "context" : "Recurrent neural network architectures such as LSTM [10] and GRU [6] have recently exhibited state-of-the-art performance on a wide range of complex sequential problems including speech recognition [1], machine translation [3] and image and video captioning [26, 27].",
      "startOffset" : 223,
      "endOffset" : 226
    }, {
      "referenceID" : 24,
      "context" : "Recurrent neural network architectures such as LSTM [10] and GRU [6] have recently exhibited state-of-the-art performance on a wide range of complex sequential problems including speech recognition [1], machine translation [3] and image and video captioning [26, 27].",
      "startOffset" : 258,
      "endOffset" : 266
    }, {
      "referenceID" : 25,
      "context" : "Recurrent neural network architectures such as LSTM [10] and GRU [6] have recently exhibited state-of-the-art performance on a wide range of complex sequential problems including speech recognition [1], machine translation [3] and image and video captioning [26, 27].",
      "startOffset" : 258,
      "endOffset" : 266
    }, {
      "referenceID" : 21,
      "context" : "Effective optimization of recurrent neural networks is an active area of study [22, 18, 20].",
      "startOffset" : 79,
      "endOffset" : 91
    }, {
      "referenceID" : 17,
      "context" : "Effective optimization of recurrent neural networks is an active area of study [22, 18, 20].",
      "startOffset" : 79,
      "endOffset" : 91
    }, {
      "referenceID" : 19,
      "context" : "Effective optimization of recurrent neural networks is an active area of study [22, 18, 20].",
      "startOffset" : 79,
      "endOffset" : 91
    }, {
      "referenceID" : 22,
      "context" : "It is known that for deep feed-forward neural networks, covariate shift [23, 11] degrades the efficiency of training.",
      "startOffset" : 72,
      "endOffset" : 80
    }, {
      "referenceID" : 10,
      "context" : "It is known that for deep feed-forward neural networks, covariate shift [23, 11] degrades the efficiency of training.",
      "startOffset" : 72,
      "endOffset" : 80
    }, {
      "referenceID" : 10,
      "context" : "This internal covariate shift [11] may play an especially important role in recurrent neural networks, which resemble very deep feed-forward networks.",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 10,
      "context" : "Batch normalization [11] is a recently proposed technique for controlling the distributions of feedforward neural network activations, thereby reducing internal covariate shift.",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 13,
      "context" : "Although batch normalization has demonstrated significant training speed-ups and generalization benefits in feedforward networks, it has proven difficult to apply in recurrent architectures [14, 1].",
      "startOffset" : 190,
      "endOffset" : 197
    }, {
      "referenceID" : 0,
      "context" : "Although batch normalization has demonstrated significant training speed-ups and generalization benefits in feedforward networks, it has proven difficult to apply in recurrent architectures [14, 1].",
      "startOffset" : 190,
      "endOffset" : 197
    }, {
      "referenceID" : 13,
      "context" : "However, it has been hypothesized [14] that applying batch normalization in this way hurts training because of exploding gradients due to repeated rescaling.",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 14,
      "context" : "In particular, we achieve state-of-the-art performance on Sequential MNIST [15] and character-level language modeling [19] on Penn Treebank [17] and text8 [16].",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 18,
      "context" : "In particular, we achieve state-of-the-art performance on Sequential MNIST [15] and character-level language modeling [19] on Penn Treebank [17] and text8 [16].",
      "startOffset" : 118,
      "endOffset" : 122
    }, {
      "referenceID" : 16,
      "context" : "In particular, we achieve state-of-the-art performance on Sequential MNIST [15] and character-level language modeling [19] on Penn Treebank [17] and text8 [16].",
      "startOffset" : 140,
      "endOffset" : 144
    }, {
      "referenceID" : 15,
      "context" : "In particular, we achieve state-of-the-art performance on Sequential MNIST [15] and character-level language modeling [19] on Penn Treebank [17] and text8 [16].",
      "startOffset" : 155,
      "endOffset" : 159
    }, {
      "referenceID" : 4,
      "context" : "Training RNNs using first-order stochastic gradient descent (SGD) however is notoriously difficult due to the well-known problem of exploding/vanishing gradients [5, 9, 22].",
      "startOffset" : 162,
      "endOffset" : 172
    }, {
      "referenceID" : 8,
      "context" : "Training RNNs using first-order stochastic gradient descent (SGD) however is notoriously difficult due to the well-known problem of exploding/vanishing gradients [5, 9, 22].",
      "startOffset" : 162,
      "endOffset" : 172
    }, {
      "referenceID" : 21,
      "context" : "Training RNNs using first-order stochastic gradient descent (SGD) however is notoriously difficult due to the well-known problem of exploding/vanishing gradients [5, 9, 22].",
      "startOffset" : 162,
      "endOffset" : 172
    }, {
      "referenceID" : 4,
      "context" : "While the long-term dependencies problem is unsolvable in absolute [5], it has been demonstrated that different RNN re-parameterizations, such as the LSTM [10], GRU [6] or i/u-RNN [15, 2] can help mitigate the vanishing gradient problem.",
      "startOffset" : 67,
      "endOffset" : 70
    }, {
      "referenceID" : 9,
      "context" : "While the long-term dependencies problem is unsolvable in absolute [5], it has been demonstrated that different RNN re-parameterizations, such as the LSTM [10], GRU [6] or i/u-RNN [15, 2] can help mitigate the vanishing gradient problem.",
      "startOffset" : 155,
      "endOffset" : 159
    }, {
      "referenceID" : 5,
      "context" : "While the long-term dependencies problem is unsolvable in absolute [5], it has been demonstrated that different RNN re-parameterizations, such as the LSTM [10], GRU [6] or i/u-RNN [15, 2] can help mitigate the vanishing gradient problem.",
      "startOffset" : 165,
      "endOffset" : 168
    }, {
      "referenceID" : 14,
      "context" : "While the long-term dependencies problem is unsolvable in absolute [5], it has been demonstrated that different RNN re-parameterizations, such as the LSTM [10], GRU [6] or i/u-RNN [15, 2] can help mitigate the vanishing gradient problem.",
      "startOffset" : 180,
      "endOffset" : 187
    }, {
      "referenceID" : 1,
      "context" : "While the long-term dependencies problem is unsolvable in absolute [5], it has been demonstrated that different RNN re-parameterizations, such as the LSTM [10], GRU [6] or i/u-RNN [15, 2] can help mitigate the vanishing gradient problem.",
      "startOffset" : 180,
      "endOffset" : 187
    }, {
      "referenceID" : 9,
      "context" : "In what follows, we focus on the LSTM architecture [10] with recurrent transition given by  f̃t ĩt õt g̃t  = Whht−1 +Wxxt + b (2)",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 22,
      "context" : "Covariate shift [23] is a phenomenon in machine learning where the features presented to a model change in distribution during the course of training.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 10,
      "context" : "In deep neural networks, this problem manifests as internal covariance shift [11], where changing the parameters of a layer affects the distribution of the inputs to all layers above it.",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 10,
      "context" : "Batch Normalization [11] is a recently proposed network re-parameterization that aims to reduce internal covariate shift.",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 13,
      "context" : "Contrary to previous work [14, 1], we leverage batch normalization in both the input-to-hidden and",
      "startOffset" : 26,
      "endOffset" : 33
    }, {
      "referenceID" : 0,
      "context" : "Contrary to previous work [14, 1], we leverage batch normalization in both the input-to-hidden and",
      "startOffset" : 26,
      "endOffset" : 33
    }, {
      "referenceID" : 13,
      "context" : "We suspect that the previous difficulties with recurrent batch normalization reported in the literature [14, 1] are largely due to improper initialization of the batch normalization parameters, and γ in particular.",
      "startOffset" : 104,
      "endOffset" : 111
    }, {
      "referenceID" : 0,
      "context" : "We suspect that the previous difficulties with recurrent batch normalization reported in the literature [14, 1] are largely due to improper initialization of the batch normalization parameters, and γ in particular.",
      "startOffset" : 104,
      "endOffset" : 111
    }, {
      "referenceID" : 14,
      "context" : "We evaluate our batch-normalized LSTM on a sequential version of the MNIST classification task [15].",
      "startOffset" : 95,
      "endOffset" : 99
    }, {
      "referenceID" : 14,
      "context" : "TANH-RNN [15] 35.",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 14,
      "context" : "0 iRNN [15] 97.",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 1,
      "context" : "0 uRNN [2] 95.",
      "startOffset" : 7,
      "endOffset" : 10
    }, {
      "referenceID" : 26,
      "context" : "4 sTANH-RNN [28] 98.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 18,
      "context" : "HF-MRNN [19] 1.",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 12,
      "context" : "41 Norm-stabilized LSTM [13] 1.",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 18,
      "context" : "39 ME n-gram [19] 1.",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 26,
      "context" : "td-LSTM [28] 1.",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 18,
      "context" : "63 HF-MRNN [19] 1.",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 20,
      "context" : "54 skipping RNN [21] 1.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 23,
      "context" : "The model is trained using RMSProp [24] with learning rate of 10−3 and 0.",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 1,
      "context" : "It has been highlighted in [2] that pMNIST contains many longer term dependencies across pixels than in the original pixel ordering, where a lot of structure is local.",
      "startOffset" : 27,
      "endOffset" : 30
    }, {
      "referenceID" : 16,
      "context" : "We evaluate our model on the task of character-level language modeling on the Penn Treebank corpus [17] according to the train/valid/test partition of [19].",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 18,
      "context" : "We evaluate our model on the task of character-level language modeling on the Penn Treebank corpus [17] according to the train/valid/test partition of [19].",
      "startOffset" : 151,
      "endOffset" : 155
    }, {
      "referenceID" : 11,
      "context" : "0 and step rule determined by Adam [12] with learning rate 0.",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 15,
      "context" : "We evaluate our model on a second character-level language modeling task on the text8 dataset [16].",
      "startOffset" : 94,
      "endOffset" : 98
    }, {
      "referenceID" : 18,
      "context" : "We follow previous authors [19, 28] and use the first 90M characters for training, the next 5M for validation and the final 5M characters for testing.",
      "startOffset" : 27,
      "endOffset" : 35
    }, {
      "referenceID" : 26,
      "context" : "We follow previous authors [19, 28] and use the first 90M characters for training, the next 5M for validation and the final 5M characters for testing.",
      "startOffset" : 27,
      "endOffset" : 35
    }, {
      "referenceID" : 11,
      "context" : "0 and step rule determined by Adam [12] with learning rate 0.",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 7,
      "context" : "To demonstrate the generality and practical applicability of our re-parameterization, we explore the use of batch-normalized LSTM in a unidirectional Attentive Reader Model [8].",
      "startOffset" : 173,
      "endOffset" : 176
    }, {
      "referenceID" : 6,
      "context" : "We do not compare with [7] as they evaluate in a different setting.",
      "startOffset" : 23,
      "endOffset" : 26
    }, {
      "referenceID" : 7,
      "context" : "Figure 5: Error rate on the validation set for the Attentive Reader models on a variant of the CNN QA task [8].",
      "startOffset" : 107,
      "endOffset" : 110
    }, {
      "referenceID" : 11,
      "context" : "We use stochastic gradient descent on minibatches of size 64, with gradient clipping at 10 and step rule determined by Adam [12] with learning rate 8× 10−5.",
      "startOffset" : 124,
      "endOffset" : 128
    }, {
      "referenceID" : 7,
      "context" : "We follow a similar preprocessing pipeline to [8].",
      "startOffset" : 46,
      "endOffset" : 49
    }, {
      "referenceID" : 7,
      "context" : "However contrary to [8], we limit the number of sentences in each passage to 4 in order to save computation.",
      "startOffset" : 20,
      "endOffset" : 23
    }, {
      "referenceID" : 13,
      "context" : "Contrary to previous findings [14, 1], we have demonstrated that batch-normalizing the hidden states of recurrent neural networks greatly improves optimization.",
      "startOffset" : 30,
      "endOffset" : 37
    }, {
      "referenceID" : 0,
      "context" : "Contrary to previous findings [14, 1], we have demonstrated that batch-normalizing the hidden states of recurrent neural networks greatly improves optimization.",
      "startOffset" : 30,
      "endOffset" : 37
    }, {
      "referenceID" : 13,
      "context" : "We have argued that proper initialization of the batch normalization parameters is crucial, and suggest that previous difficulties [14, 1] were due in large part to improper initialization.",
      "startOffset" : 131,
      "endOffset" : 138
    }, {
      "referenceID" : 0,
      "context" : "We have argued that proper initialization of the batch normalization parameters is crucial, and suggest that previous difficulties [14, 1] were due in large part to improper initialization.",
      "startOffset" : 131,
      "endOffset" : 138
    }, {
      "referenceID" : 3,
      "context" : "We would also like to thank the developers of Theano [4] and the Blocks and Fuel [25] libraries, for developing such powerful tools for scientific computing.",
      "startOffset" : 53,
      "endOffset" : 56
    } ],
    "year" : 2016,
    "abstractText" : "We propose a reparameterization of LSTM that brings the benefits of batch normalization to recurrent neural networks. Whereas previous works only apply batch normalization to the input-to-hidden transformation of RNNs, we demonstrate that it is both possible and beneficial to batch-normalize the hidden-to-hidden transition, thereby reducing internal covariate shift between time steps. We evaluate our proposal on various sequential problems such as sequence classification, language modeling and question answering. Our empirical results show that our batch-normalized LSTM consistently leads to faster convergence and improved generalization.",
    "creator" : "LaTeX with hyperref package"
  }
}