{
  "name" : "1603.07442.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Pixel-Level Domain Transfer",
    "authors" : [ "Donggeun Yoo", "Namil Kim", "Sunggyun Park", "Anthony S. Paek", "In So Kweon" ],
    "emails" : [ "dgyoo@rcv.kaist.ac.kr", "nikim@rcv.kaist.ac.kr", "sunggyun@kaist.ac.kr", "apaek@lunit.io", "iskweon@kaist.ac.kr" ],
    "sections" : null,
    "references" : [ {
      "title" : "Generative adversarial nets",
      "author" : [ "I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio" ],
      "venue" : "Advances in Neural Information Processing Systems.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Fundamentals of cognition",
      "author" : [ "M.W. Eysenck" ],
      "venue" : "Psychology Press East Sussex, UK, USA and Canada",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Reducing the dimensionality of data with neural networks",
      "author" : [ "G.E. Hinton", "R.R. Salakhutdinov" ],
      "venue" : "Science 313(5786)",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Deep boltzmann machines",
      "author" : [ "R. Salakhutdinov", "G.E. Hinton" ],
      "venue" : "International conference on artificial intelligence and statistics.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Extracting and composing robust features with denoising autoencoders",
      "author" : [ "P. Vincent", "H. Larochelle", "Y. Bengio", "P.A. Manzagol" ],
      "venue" : "Proceedings of the 25th international conference on Machine learning, ACM",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Draw: A recurrent neural network for image generation",
      "author" : [ "K. Gregor", "I. Danihelka", "A. Graves", "D. Rezende", "D. Wierstra" ],
      "venue" : "Proceedings of The 32nd International Conference on Machine Learning.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Deep unsupervised learning using nonequilibrium thermodynamics",
      "author" : [ "J. Sohl-Dickstein", "E. Weiss", "N. Maheswaranathan", "S. Ganguli" ],
      "venue" : "Proceedings of The 32nd International Conference on Machine Learning.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Generative image modeling using spatial lstms",
      "author" : [ "L. Theis", "M. Bethge" ],
      "venue" : "Advances in Neural Information Processing Systems.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Adapting visual category models to new domains",
      "author" : [ "K. Saenko", "B. Kulis", "M. Fritz", "T. Darrell" ],
      "venue" : "Computer Vision–ECCV 2010. Springer",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "What you saw is not what you get: Domain adaptation using asymmetric kernel transforms",
      "author" : [ "B. Kulis", "K. Saenko", "T. Darrell" ],
      "venue" : "Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, IEEE",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Domain adaptation for object recognition: An unsupervised approach",
      "author" : [ "R. Gopalan", "R. Li", "R. Chellappa" ],
      "venue" : "Computer Vision (ICCV), 2011 IEEE International Conference on, IEEE",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Learning and transferring mid-level image representations using convolutional neural networks",
      "author" : [ "M. Oquab", "L. Bottou", "I. Laptev", "J. Sivic" ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Deep domain adaptation for describing people based on fine-grained clothing attributes",
      "author" : [ "Q. Chen", "J. Huang", "R. Feris", "L.M. Brown", "J. Dong", "S. Yan" ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Cross-domain image retrieval with a dual attribute-aware ranking network",
      "author" : [ "J. Huang", "R.S. Feris", "Q. Chen", "S. Yan" ],
      "venue" : "Proceedings of the IEEE International Conference on Computer Vision.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Attribute2image: Conditional image generation from visual attributes",
      "author" : [ "X. Yan", "J. Yang", "K. Sohn", "H. Lee" ],
      "venue" : "arXiv preprint arXiv:1512.00570",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Conditional generative adversarial nets",
      "author" : [ "M. Mirza", "S. Osindero" ],
      "venue" : "arXiv preprint arXiv:1411.1784",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Deep generative image models using a laplacian pyramid of adversarial networks",
      "author" : [ "E.L. Denton", "S. Chintala", "R Fergus" ],
      "venue" : "Advances in Neural Information Processing Systems.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Unsupervised representation learning with deep convolutional generative adversarial networks",
      "author" : [ "A. Radford", "L. Metz", "S. Chintala" ],
      "venue" : "arXiv preprint arXiv:1511.06434",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Learning to generate chairs with convolutional neural networks",
      "author" : [ "A. Dosovitskiy", "J. Tobias Springenberg", "T. Brox" ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Deep multi-scale video prediction beyond mean square error",
      "author" : [ "M. Mathieu", "C. Couprie", "Y. LeCun" ],
      "venue" : "arXiv preprint arXiv:1511.05440",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Backpropagation applied to handwritten zip code recognition",
      "author" : [ "Y. LeCun", "B. Boser", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackel" ],
      "venue" : "Neural computation 1(4)",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 1989
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "A. Krizhevsky", "I. Sutskever", "G.E. Hinton" ],
      "venue" : "Advances in neural information processing systems.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Imagenet large scale visual recognition challenge",
      "author" : [ "O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M Bernstein" ],
      "venue" : "International Journal of Computer Vision 115(3)",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Cnn features off-the-shelf: an astounding baseline for recognition",
      "author" : [ "A. Razavian", "H. Azizpour", "J. Sullivan", "S. Carlsson" ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Multi-scale pyramid pooling for deep convolutional representation",
      "author" : [ "D. Yoo", "S. Park", "J.Y. Lee", "I. Kweon" ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Learning to compare image patches via convolutional neural networks",
      "author" : [ "S. Zagoruyko", "N. Komodakis" ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "To generate realistic target images, we employ the real/fake-discriminator in Generative Adversarial Nets [1], but also introduce a novel domain-discriminator to make the generated image relevant to the input image.",
      "startOffset" : 106,
      "endOffset" : 109
    }, {
      "referenceID" : 1,
      "context" : "To generate mental images [2] of ourselves wearing clothes on a hanger is an effortless work for our brain.",
      "startOffset" : 26,
      "endOffset" : 29
    }, {
      "referenceID" : 2,
      "context" : "Image generation has been attempted by a long line of works [3,4,5] but generating realistic images has been challenging since an image itself is high dimensional and has complex relations between pixels.",
      "startOffset" : 60,
      "endOffset" : 67
    }, {
      "referenceID" : 3,
      "context" : "Image generation has been attempted by a long line of works [3,4,5] but generating realistic images has been challenging since an image itself is high dimensional and has complex relations between pixels.",
      "startOffset" : 60,
      "endOffset" : 67
    }, {
      "referenceID" : 4,
      "context" : "Image generation has been attempted by a long line of works [3,4,5] but generating realistic images has been challenging since an image itself is high dimensional and has complex relations between pixels.",
      "startOffset" : 60,
      "endOffset" : 67
    }, {
      "referenceID" : 0,
      "context" : "However, recently a few machines have succeeded in generating realistic images [1,6,7,8], with the drastic advances of deep learning.",
      "startOffset" : 79,
      "endOffset" : 88
    }, {
      "referenceID" : 5,
      "context" : "However, recently a few machines have succeeded in generating realistic images [1,6,7,8], with the drastic advances of deep learning.",
      "startOffset" : 79,
      "endOffset" : 88
    }, {
      "referenceID" : 6,
      "context" : "However, recently a few machines have succeeded in generating realistic images [1,6,7,8], with the drastic advances of deep learning.",
      "startOffset" : 79,
      "endOffset" : 88
    }, {
      "referenceID" : 7,
      "context" : "However, recently a few machines have succeeded in generating realistic images [1,6,7,8], with the drastic advances of deep learning.",
      "startOffset" : 79,
      "endOffset" : 88
    }, {
      "referenceID" : 8,
      "context" : "different image domain has been proposed in computer vision [9,10,11,12,13,14], but all these adaptations take place in the feature space, i.",
      "startOffset" : 60,
      "endOffset" : 78
    }, {
      "referenceID" : 9,
      "context" : "different image domain has been proposed in computer vision [9,10,11,12,13,14], but all these adaptations take place in the feature space, i.",
      "startOffset" : 60,
      "endOffset" : 78
    }, {
      "referenceID" : 10,
      "context" : "different image domain has been proposed in computer vision [9,10,11,12,13,14], but all these adaptations take place in the feature space, i.",
      "startOffset" : 60,
      "endOffset" : 78
    }, {
      "referenceID" : 11,
      "context" : "different image domain has been proposed in computer vision [9,10,11,12,13,14], but all these adaptations take place in the feature space, i.",
      "startOffset" : 60,
      "endOffset" : 78
    }, {
      "referenceID" : 12,
      "context" : "different image domain has been proposed in computer vision [9,10,11,12,13,14], but all these adaptations take place in the feature space, i.",
      "startOffset" : 60,
      "endOffset" : 78
    }, {
      "referenceID" : 13,
      "context" : "different image domain has been proposed in computer vision [9,10,11,12,13,14], but all these adaptations take place in the feature space, i.",
      "startOffset" : 60,
      "endOffset" : 78
    }, {
      "referenceID" : 14,
      "context" : "However, training the converter is not straightforward because the target is not deterministic [15].",
      "startOffset" : 95,
      "endOffset" : 99
    }, {
      "referenceID" : 0,
      "context" : "[1] propose for generating realistic images.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 0,
      "context" : "Secondly, in addition to the domain discriminator, we also employ the discriminator of [1], which is supervised by the labels of “real” or “fake”, to produce realistic images.",
      "startOffset" : 87,
      "endOffset" : 90
    }, {
      "referenceID" : 2,
      "context" : "The image-generative models can be grouped into two families; generative parametric approaches [3,4,5] and adversarial approaches [1,16,17,18].",
      "startOffset" : 95,
      "endOffset" : 102
    }, {
      "referenceID" : 3,
      "context" : "The image-generative models can be grouped into two families; generative parametric approaches [3,4,5] and adversarial approaches [1,16,17,18].",
      "startOffset" : 95,
      "endOffset" : 102
    }, {
      "referenceID" : 4,
      "context" : "The image-generative models can be grouped into two families; generative parametric approaches [3,4,5] and adversarial approaches [1,16,17,18].",
      "startOffset" : 95,
      "endOffset" : 102
    }, {
      "referenceID" : 0,
      "context" : "The image-generative models can be grouped into two families; generative parametric approaches [3,4,5] and adversarial approaches [1,16,17,18].",
      "startOffset" : 130,
      "endOffset" : 142
    }, {
      "referenceID" : 15,
      "context" : "The image-generative models can be grouped into two families; generative parametric approaches [3,4,5] and adversarial approaches [1,16,17,18].",
      "startOffset" : 130,
      "endOffset" : 142
    }, {
      "referenceID" : 16,
      "context" : "The image-generative models can be grouped into two families; generative parametric approaches [3,4,5] and adversarial approaches [1,16,17,18].",
      "startOffset" : 130,
      "endOffset" : 142
    }, {
      "referenceID" : 17,
      "context" : "The image-generative models can be grouped into two families; generative parametric approaches [3,4,5] and adversarial approaches [1,16,17,18].",
      "startOffset" : 130,
      "endOffset" : 142
    }, {
      "referenceID" : 0,
      "context" : "[1].",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 15,
      "context" : "Mirza and Osindero [16] extend GAN to a class conditional version, and Denton et al.",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 16,
      "context" : "[17] improve the image resolution in a coarse-to-fine fashion.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 17,
      "context" : "[18] have proposed architectures named Deep Convolutional GANs, which is stable to be trained, and have succeeded in generating high quality images.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 5,
      "context" : "Besides the two families, a recurrent network based model [6] and a deconvolutional network based model [19] have also been proposed.",
      "startOffset" : 58,
      "endOffset" : 61
    }, {
      "referenceID" : 18,
      "context" : "Besides the two families, a recurrent network based model [6] and a deconvolutional network based model [19] have also been proposed.",
      "startOffset" : 104,
      "endOffset" : 108
    }, {
      "referenceID" : 15,
      "context" : "We replace the generator with our converter which is an image-conditioned model, while [16] is class-conditional and [15] is attribute-conditional.",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 14,
      "context" : "We replace the generator with our converter which is an image-conditioned model, while [16] is class-conditional and [15] is attribute-conditional.",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 19,
      "context" : "[20] is similar to ours in that it is conditioned with video frames to produce next frames.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 8,
      "context" : "For visual recognition, many methods to adapt domains [9,10,11] have been proposed.",
      "startOffset" : 54,
      "endOffset" : 63
    }, {
      "referenceID" : 9,
      "context" : "For visual recognition, many methods to adapt domains [9,10,11] have been proposed.",
      "startOffset" : 54,
      "endOffset" : 63
    }, {
      "referenceID" : 10,
      "context" : "For visual recognition, many methods to adapt domains [9,10,11] have been proposed.",
      "startOffset" : 54,
      "endOffset" : 63
    }, {
      "referenceID" : 20,
      "context" : "Especially for the recent use of the deep convolutional neural network [21], it has been common to",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 21,
      "context" : "pre-train a large network [22] over ImageNet [23] and transfer the parameters to a target domain [12,24,25].",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 22,
      "context" : "pre-train a large network [22] over ImageNet [23] and transfer the parameters to a target domain [12,24,25].",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 11,
      "context" : "pre-train a large network [22] over ImageNet [23] and transfer the parameters to a target domain [12,24,25].",
      "startOffset" : 97,
      "endOffset" : 107
    }, {
      "referenceID" : 23,
      "context" : "pre-train a large network [22] over ImageNet [23] and transfer the parameters to a target domain [12,24,25].",
      "startOffset" : 97,
      "endOffset" : 107
    }, {
      "referenceID" : 24,
      "context" : "pre-train a large network [22] over ImageNet [23] and transfer the parameters to a target domain [12,24,25].",
      "startOffset" : 97,
      "endOffset" : 107
    }, {
      "referenceID" : 12,
      "context" : "[13] and Huang et al.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 13,
      "context" : "[14] address a gap between fashion shopping mall images and unconstrained human images for the clothing attribute recognition [13] and the product retrieval [14].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 12,
      "context" : "[14] address a gap between fashion shopping mall images and unconstrained human images for the clothing attribute recognition [13] and the product retrieval [14].",
      "startOffset" : 126,
      "endOffset" : 130
    }, {
      "referenceID" : 13,
      "context" : "[14] address a gap between fashion shopping mall images and unconstrained human images for the clothing attribute recognition [13] and the product retrieval [14].",
      "startOffset" : 157,
      "endOffset" : 161
    }, {
      "referenceID" : 0,
      "context" : "Generative Adversarial Nets (GAN) [1] is a generalized framework for generative models which [17,18,20] and we utilize for visual data.",
      "startOffset" : 34,
      "endOffset" : 37
    }, {
      "referenceID" : 16,
      "context" : "Generative Adversarial Nets (GAN) [1] is a generalized framework for generative models which [17,18,20] and we utilize for visual data.",
      "startOffset" : 93,
      "endOffset" : 103
    }, {
      "referenceID" : 17,
      "context" : "Generative Adversarial Nets (GAN) [1] is a generalized framework for generative models which [17,18,20] and we utilize for visual data.",
      "startOffset" : 93,
      "endOffset" : 103
    }, {
      "referenceID" : 19,
      "context" : "Generative Adversarial Nets (GAN) [1] is a generalized framework for generative models which [17,18,20] and we utilize for visual data.",
      "startOffset" : 93,
      "endOffset" : 103
    }, {
      "referenceID" : 19,
      "context" : "It has been well known that MSE is prone to produce blurry images because it inherently assumes that the pixels are drawn from Gaussian distribution [20].",
      "startOffset" : 149,
      "endOffset" : 153
    }, {
      "referenceID" : 0,
      "context" : "As in [1,17,18], the discriminator network guides the converter to produce realistic target under the supervision of real/fake.",
      "startOffset" : 6,
      "endOffset" : 15
    }, {
      "referenceID" : 16,
      "context" : "As in [1,17,18], the discriminator network guides the converter to produce realistic target under the supervision of real/fake.",
      "startOffset" : 6,
      "endOffset" : 15
    }, {
      "referenceID" : 17,
      "context" : "As in [1,17,18], the discriminator network guides the converter to produce realistic target under the supervision of real/fake.",
      "startOffset" : 6,
      "endOffset" : 15
    }, {
      "referenceID" : 19,
      "context" : "One supervision candidate to let the converter C meet the condition is the combined use of MSE with the real/fake loss, just as what [20] does for the video prediction.",
      "startOffset" : 133,
      "endOffset" : 137
    }, {
      "referenceID" : 17,
      "context" : "The architecture of the real/fake discriminator is identical to that of [18] as illustrated in Fig.",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 25,
      "context" : "Several architecture families have been proposed to feed a pair of images to compare them but a simple stack across the channel axis has shown the best performance as studied in [26].",
      "startOffset" : 178,
      "endOffset" : 182
    }, {
      "referenceID" : 0,
      "context" : "With these two losses, we basically follow the adversarial training procedure of [1], as explained in Sec.",
      "startOffset" : 81,
      "endOffset" : 84
    }, {
      "referenceID" : 12,
      "context" : "[13] also has presented a similar fashion dataset dealing with two domains.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 17,
      "context" : "[18] to stabilize the training.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 14,
      "context" : "To compensate the effect, as suggested in [15], we also measure the root mean square error in another setting where the “Converter+RF-Discrim.",
      "startOffset" : 42,
      "endOffset" : 46
    }, {
      "referenceID" : 0,
      "context" : "The user-study scores are normalized to a range of [0, 1].",
      "startOffset" : 51,
      "endOffset" : 57
    }, {
      "referenceID" : 0,
      "context" : "The scores are averaged and normalized to a range of [0, 1].",
      "startOffset" : 53,
      "endOffset" : 59
    } ],
    "year" : 2017,
    "abstractText" : "We present an image-conditional image generation model. The model transfers an input domain to a target domain in semantic level, and generates the target image in pixel level. To generate realistic target images, we employ the real/fake-discriminator in Generative Adversarial Nets [1], but also introduce a novel domain-discriminator to make the generated image relevant to the input image. We verify our model through a challenging task of generating a piece of clothing from an input image of a dressed person. We present a high quality clothing dataset containing the two domains, and succeed in demonstrating decent results.",
    "creator" : "LaTeX with hyperref package"
  }
}