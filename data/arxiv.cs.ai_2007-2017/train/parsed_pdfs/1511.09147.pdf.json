{
  "name" : "1511.09147.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Scaling POMDPs For Selecting Sellers in E-markets—Extended Version",
    "authors" : [ "Athirai A. Irissappane", "Frans A. Oliehoek", "Jie Zhang" ],
    "emails" : [ "athirai001@e.ntu.edu.sg", "frans.oliehoek@liverpool.ac.uk", "zhangj@ntu.edu.sg" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "In many domains, agents need to determine the trustworthiness (quality) of other agents before interacting with them. Specifically, in e-marketplaces, buying agents need to reason about the quality of sellers and determine which sellers to do business with (referred to as the seller selection problem). When buyers have no previous experience with sellers, they can obtain advice by querying other buyers (called advisors). However, some advisors may be untrustworthy and provide misleading opinions to promote or demote the sellers (Irissappane and Zhang 2015).\nThe Partially Observable Markov Decision Process (POMDP) is a framework for sequential decision making under uncertainty, suitable for e-markets, where buyers often need to make decisions with limited information about the sellers and advisors. Regan, Cohen, and Poupart (2005) propose the Advisor POMDP, for the seller selection problem, which, rather than trying to achieve the most accurate estimate of sellers, tries to select good sellers optimally with respect to its belief. Seller and Advisor Selection (SALE) POMDP (Irissappane, Oliehoek, and Zhang 2014) extends Advisor POMDP to additionally deal with trust propagation, by introducing queries about advisors. The SALE POMDP formalism enables maximizing buyer satisfaction by optimally trading off information gaining (querying advisors) and exploiting (selecting a seller) actions, and experiments have shown very good results in practice. Also, the approach\nis easily generalizable to deal with more general problems with trust-propagation components, such as routing in Wireless Sensor Networks (WSNs) (Irissappane et al. 2015).\nUnfortunately, these POMDP approaches suffer from scalability issues. Finding optimal policies for POMDPs is, in general, computationally intractable (PSPACE complete) and POMDP solvers computing exact solutions, e.g., value iteration do not scale to more than a handful of states (Cassandra, Kaelbling, and Littman 1994). While approximation algorithms have been shown to supply good policies rapidly even for problems with very large state spaces (Spaan 2012; Silver and Veness 2010), the scalability of the SALE POMDP, which is based on one such method (Poupart 2005), is limited to about 10 agents (sellers and advisors). For larger number of agents, the solution time grows to the order of hours and solution quality degenerates, precluding the SALE POMDP from exploiting the presence of more sellers and advisors.\nThis paper proposes a novel method, referred to as the Mixture of POMDP Experts (MOPE) approach, for dealing with very large trust-propagation problems such as SALE POMDPs with many sellers and advisors. The key idea is to divide the large seller selection POMDP problem into a multitude of computationally tractable smaller (sub)-POMDPs, each containing a subset of sellers and advisors. The actions of the sub-POMDPs (SPs) are then aggregated, to find the best action in the process of selecting a good seller.\nThe MOPE approach exploits the structure of the Dynamic Bayesian Network that represents the transition and observation probabilities of the SALE POMDP: query actions do not affect the actual states but only the agent’s beliefs over the state factors, making it easier to decompose a large seller selection problem into smaller sub-problems that approximate the larger problem. Due to the improved scalability of MOPE, it can leverage the presence of more advisors to make more informed decisions about sellers, when the size of the seller selection problem increases. Extensive evaluation in a simulated e-marketplace demonstrates that MOPE can scale up to a hundred agents (millions of states and thousands of actions), outperforming the state-of-theart POMCP (Silver and Veness 2010) approach, while using less computation time. We also demonstrate that MOPE can bring scalability to other domains by showing results for wireless sensor networks with up to 40 neighboring nodes. ar X\niv :1\n51 1.\n09 14\n7v 2\n[ cs\n.A I]\n9 D\nec 2\n01 5"
    }, {
      "heading" : "2 Background",
      "text" : "This paper mainly relies on POMDPs, which can be used to represent decision making problems under uncertainty in terms of states, actions, transitions, observations and rewards. We refer to Kaelbling et al. (1998), Spaan (2012) for a comprehensive introduction to POMDPs. Here, we try to convey the most basic intuitions by briefly describing the Seller and Advisor Selection (SALE) POMDP (Irissappane, Oliehoek, and Zhang 2014), which is the main application for the technique we propose in this paper. States. Each state is represented using a number of state factors, such as the quality levels of each seller (qj ∈ {high, low}), each advisor (ui ∈ {trustworthy, untrustworthy}) and status of the transaction (sat ∈ {not started, satisfactory, unsatisfactory, gave up, finished}). Actions and Transitions. For query actions such as seller query(i,j) ((SQ)(i,j)), i.e., ask advisor i about seller j and advisor query(i,i′) ((AQ)(i,i′)), i.e., ask advisor i about another advisor i′, the states do not change. ForBUYj action, to buy from seller j, the state transitions to successful (sat = satisfactory) on buying from a good seller and unsuccessful (sat = unsatisfactory) on buying from a bad seller. For do not buy (DNB) action, i.e., do not buy from any seller, the state transitions to sat = gave up. Rewards. There is small cost for the query actions. A reward/penalty is associated with a successful/unsuccessful transaction. There is a penalty for takingDNB action, when there is a seller of high quality, otherwise a reward is given. Observations. After SQij , AQii′ actions, an observation o ∈ {good, bad}, corresponding to the quality of seller j and o ∈ {trustworthy, untrustworthy} corresponding to the quality of advisor i′ is received, respectively. After BUYj action, the agent can also receive an observation based on the actual quality of seller j, allowing to reuse the updated beliefs, in case of multiple transactions. The observation probabilities are such that trustworthy advisors give more accurate and consistent answers than untrustworthy ones.\nThe SALE POMDP agent interacts with its environment for an indefinite number of time steps and we model the problem using an infinite horizon. During this interaction, the agent maintains a belief b ∈ B, i.e., a probability distribution over states. If b(s) specifies the probability of s (for all s), we can derive b′ an updated belief after taking some action a and receiving an observation o using the Bayes’ rule. A POMDP policy π : B → A, maps belief b ∈ B to an action a ∈ A. A policy π is associated with a value function V (b), specifying the expected total reward of executing policy π starting from b, with discount factor γ. The main objective of the POMDP agent is to find an optimal policy π∗, which maximizes V (b) (Eqn. 1). The value function can also be represented in terms of Q-functions, given by Eqn. 2, where, bao is the belief state resulting from b after taking action a and receiving observation o ∈ O.\nV ∗(b)=max π\nE [∑\nt\nγtR(s, a, s′)|π, b ] =max a∈A Q∗(b, a) (1)\nQ∗(b, a) = ∑ s∈S b(s)R(s, a) + γ ∑ o∈Ω p(o|b, a)V ∗(bao) (2)\nBy computing the optimal value function, we can optimize long-term rewards by picking maximizing actions. This stands in contrast to myopic approaches that maximize the immediate rewards R. Such approaches are inherently unsuitable for seller selection: in order to correctly value the different query actions, one needs to reason about their impact on the future beliefs and the associated value of information. In order to actually compute V ∗ (approximately) one could rely on state-of-the-art flat solvers such as SARSOP (Kurniawati, Hsu, and Lee 2008), but these do provide very limited scalability (Oliehoek, Gokhale, and Zhang 2012), since the number of states grows exponentially with the number of agents n (i.e., sellers and advisors). Therefore, Irissappane, Oliehoek, and Zhang (2014) employ a solution method, factored Perseus (Poupart 2005), that exploits the factored representation of this domain, thus allowing to scale to roughly 10 agents. Beyond that solution times go up significantly while solution quality drops. Apart from the number of state factors themselves, a difficulty is that the number of actions grows with order O(n2) as the query actions involve pairs of agents."
    }, {
      "heading" : "3 A SingleExpert Baseline",
      "text" : "In this paper, we propose techniques to exploit the structure present in (settings like) the SALE POMDP. Here, we introduce a baseline algorithm as an intuitive starting point for the more advanced method we propose in the next section.\nThis baseline, called SingleExpert, is basically a method to apply the SALE POMDP for large problems. That is, when faced with a SALE POMDP instance with many sellers and advisors, we can randomly select a subset of agents that is small enough to model and solve as a SALE POMDP and use the resulting policy. Since the qj and ui variables do not influence each other, defining such a sub-POMDP (SP) is trivial as it merely amounts to deleting all non-selected state variables as well as actions and observations that pertain to them. Also, the resulting model is a small SALE POMDP, thus we can find a good solution for it. While this voluntary restriction on the set of sellers and advisors that will be reasoned about could be limiting, it is quite possible that it may lead to acceptable performance and it might be better than incorrectly reasoning about all of the agents. We call this approach the ‘SingleExpert’ approach, since the randomly selected SP acts as a (single) expert as to what action to take in the larger problem."
    }, {
      "heading" : "4 Mixture of POMDP Experts (MOPE)",
      "text" : "While we argue that SingleExpert might have its merit, clearly, we want to develop methods that can exploit large pools of potential sellers and advisors. To accomplish this, we introduce the Mixture of POMDP Experts (MOPE) framework. SingleExpert exploits a particular property of trust propagation-like domains: constructing an SP is possible because the state variables encoding seller and advisor qualities do not affect each other and cannot be influenced by actions. In fact, interaction of these variables only arises\nin the agent’s beliefs manifested as correlations induced by the coupling via observations. For example, if we query advisor i about seller j and receive observation bad, it not only increases the probability of the seller being low quality (qj = low) and advisor being ui = trustworthy, but also of (qj = high, ui = untrustworthy). The MOPE framework aims to take this insight further by approximating such correlations using smaller clusters of variables, as in variational inference approaches (Koller and Friedman 2009), leading to the idea of representing the larger problem using a number of smaller SPs and leveraging their solutions. That is, rather than considering a single expert, we will want to consider many SPs."
    }, {
      "heading" : "4.1 MOPE Algorithm Overview",
      "text" : "Algorithm 1 gives a brief overview of the MOPE framework. We first form the SPs by randomly selecting a subset of sellers and advisors (Mk in Line 1). Each SP is solved to obtain the optimal policy and thereby its maximum expected total reward V ∗k (Line 2)\n1. When SPs have the same agent composition (number of sellers and advisors), the found V ∗k can be reused amongst them. Therefore, in our implementation we always select such uniformly composed SPs. We define V as a set of votes v collected from each SP. Each vote v = (a, q) is a set containing the action a suggested by the SP and its associated Q-value q.\nTo maintain beliefs about all the state factors, it is possible to maintain the local beliefs in each SP, in parallel. However, doing so: 1) we need to deal with the actions not present in a SP as its local belief will be updated only if the SP contains the executed action ā; 2) we cannot properly take into account the influence of state factors not modeled in the SP on the belief, which may lead to inconsistent beliefs in different SPs. Thus, we propose to maintain and update the beliefs b ∈ B at the global level, i.e., involving all state factors.\nAt each time step, for each SP, we first extract its current local belief bk (Line 6) from the global belief b. Based on bk, we obtain its vote v, i.e, its recommended action a and the associated q value (Line 7). The overall best action ā is obtained (Line 8) by aggregating all the votes v ∈ V . Action ā is then executed (Line 9) and an observation o is received (Line 10), based on which the global beliefs are updated (Line 11). The following subsections give a more detailed description of the techniques used in the framework."
    }, {
      "heading" : "4.2 Dividing into Sub-POMDPs",
      "text" : "We randomly select subsets of sellers and advisors from the whole populationW to decomposeM into a number of SPs. If SPA is the number of SPs that each agent can be a part of and APS is the number of agents each SP should contain, the total number of SPs necessary for the seller selection problem is given by |W |*SPA/APS. Also, APS is chosen such that the SPs can be computationally tractable.\n4.3 AggregateVotes(V) Here, we describe different ways to aggregate the votes V .\n1 In practice, we may not solve the SPs optimally, and use the best policy and accompanying value function that we could find.\nAlgorithm 1: The Mixture of POMDP Experts (MOPE) framework.\nInput : M, a large SALE POMDP 1 Randomly splitM into SPs {M1, . . . ,MK} 2 Solve all SPs, yielding {V ∗1 , . . . , V ∗K} 3 foreach TimeStep t do 4 V ← ∅ ; //the set of votes 5 for k ∈ {1 . . .K} do 6 bk ← DetermineLocalBelief(b, k) 7 V ← V ∪ {VoteFromSP(k, bk, V ∗k )} 8 ā← AggregateVotes(V) 9 Execute(ā)\n10 o← receiveObservation() 11 b′ ← GlobalBeliefUpdate(b, ā, o)\nParallel Max-Q. Here, the best action ā is selected as the action with the maximum Q-value (ā = arg maxa∈V q), among those present in V . Also, Parallel Max-Q maintains, in parallel, a set B = {b1, . . . , bK} of local beliefs (corresponding to the SPs). We will use B as the global belief, in this case. GlobalBeliefUpdate(b,ā,o) is performed such that the beliefs bk in each SP are updated using the Bayes’ rule in parallel, when ā ∈ Ak (actions in Mk) and o ∈ Ok. When either of these conditions fails, no belief update takes place.\nTo analyse the performance of Parallel Max-Q for a given decomposition D = {M1, . . . ,MK} of SPs, we derive a lower bound on its performance. Specifically, we show that the expected sum of rewards V pmqD realized by parallel Max-Q for a decomposition D, is at least as much as the optimal value V ∗k realized by picking any SingleExpert Mk ∈ D. For this, we need to make two assumptions: the decomposition D = {M1, . . . ,MK} is nonoverlapping (i.e., no two SPs Mi,Mj contain the same seller or advisor state factors), and the true initial state distribution β0(s) is factored along the decomposition (i.e., β0(s) = β01(s1)× β02(s2) · · · · × β0K(sK)). Theorem 1. If the decomposition D = {M1, . . . ,MK} is non-overlapping, and the true initial state distribution β0 is factored along the decomposition, then the value realized by Parallel Max-Q is at least as much as the value of the best Single Expert: V pmqD (B0) ≥ maxk∈{1,...,K} V ∗k (β0k).\nThe proof of Theorem 1 is given in Appendix A, which includes a detailed theoretical analysis on the value realized by Parallel Max-Q. However, when the SPs are overlapping, Parallel Max-Q at times, can perform worse than the Best SingleExpert due to inconsistent beliefs across SPs. Imagine that there is a decomposition with two SPs (SP 1, SP 2) with a high overlap: there is one seller which is present in both SPs, but each SP has some private advisors. Also, assume that there is an uniform initial belief such that the values of the SPs are equal, and Parallel Max-Q selects a ‘winning’ SP, say SP 1, randomly. Subsequently, the executed action is a seller query that asks one of the private advisors of SP 1 about the (shared) seller, and if the answer is ‘bad’, the belief of SP 1 gets updated to reflect a lower probability of the seller being high quality. Next, however, Parallel Max-Q\n6 7 8 9 10 0\n20\n40\n60\n80\nNo. of Agents\nV a lu e\nMax-Q\nParallel Max-Q\nSingleExpert(5)\nFigure 1: Max-Q vs Parallel Max-Q\nwill switch to SP 2 where the belief has not been altered. As such SP 2 is overestimating the value because its belief is no longer is in sync with the true distribution. This overestimation of the value may lead to unnecessary information gaining actions which have costs associated with them and eventually may lead to Parallel Max-Q performing worse than the Best SingleExpert. Max-Q. To address the issue of inconsistent beliefs, here in Max-Q, the beliefs are not maintained in parallel, instead they are maintained and updated (using Bayes’ rule) at the global level, i.e., involving all state factors, as it helps to propagate information (about the sellers and advisors) across SPs. We empirically show the advantage2 of Max-Q over Parallel Max-Q, for a decomposition D in Fig. 1 by plotting their values for different seller selection problems, comprising of 6−10 agents. We also show the value of SingleExpert (randomly chosen 5 agent SP) in Fig. 1. Majority Voting. As Parallel Max-Q and Max-Q select the action of the maximizing SP (with the maximum Q-value), they consider the value that the action will generate for a single sub-problem. It is likely that certain actions are more useful for many sub-problems and it is better to select the action with a higher value in all SPs than the action with the highest value in one single SP. Here, we formalize one such technique called the Majority Voting approach.\nAlgorithm 2 describes the Majority Voting approach in detail. Based on the votes v ∈ V , we first count the number of SPs which suggested the action a (Line 3). We also determine the mean Q-value associated with each action a using the qvalsum[] and meanQs[] variables (Lines 4− 6).\nWhile using the Majority Voting technique, we need to consider the fact that not every SP will have the same set of actions as it depends on which sellers and advisors are present in the SP. For instance, while each SP has the action DNB, the action, say SQ(a12,s23) will only be present in a SP containing both advisor12 and seller23. Thus, most SQi,j and AQi,i′ actions might not be represented in any SP, and the ones present may be represented in just one SP.\n2 Results are statistically verified by paired t-test (α=0.05).\nAlgorithm 2: AggregateVotes by Majority Voting Input : V , the set of votes //Count votes for regular actions 1 foreach v ∈ V do 2 (a, q)← v ; //unpack vote 3 counts[a] += 1; 4 qvalsum[a] += q; 5 foreach a ∈ A do 6 meanQs[a] = qvalsum[a] / counts[a]; //Count votes for abstract actions 7 foreach v ∈ V do 8 AV = {(ã, q)} ← AbstractedVotes(v); 9 foreach (ã, q) ∈ AV do 10 counts[ã] += 1; 11 qvalsum[ã] += q; 12 foreach ã ∈ Ã do 13 meanQs[ã] = qvalsum[ã] / counts[ã]; //Select best abstract action and refine 14 ã∗ = arg maxã(counts[ã] ∗meanQs[ã] ); 15 ā = Refine(ã∗, counts,meanQs); 16 return ā\nTo address this, we make use of the additional information present in the actions of each SP by formulating the concept of abstract actions. Consider the case where the belief indicates that there is a reasonable chance that seller23 is of high quality, but it falls just short of being sufficient to select the BUY23 action. Here, it is very likely that all query actions that ask about seller23 that are represented in some SPs (we will denote this set by SQ(X,s23), where ‘X’ denotes an unbound variable) will have a high value in those SPs. As such, voting on abstract actions (called Level L1 abstract actions), such as SQ(X,s23), SQ(a12,Y ), AQ(X,a30), AQ(a12,Y ), BUY(Y ), DNB can potentially help overcome the problem of sparsely represented actions.\nHowever, only SPs which contain seller23 will have a SQ(X,s23) action, still resulting in unbalanced voting. Thus, rather than only abstracting away just one argument, we abstract away both arguments leading to abstract actions SQ(X,Y ) and AQ(X,Y ). Doing this leads to a situation where every SP has abstract actions SQ(X,Y ), AQ(X,Y ), BUY(Y ) andDNB (called Level L2 actions). But, still there may arise scenarios where DNB actions can outnumber the SQ(X,Y ), AQ(X,Y ), BUY(Y ) actions, individually, especially in cases when all good sellers are concentrated only to a group of SPs. Thus we consider (Level L3) abstract actions DNB and Others ∈ {SQ(X,Y ), AQ(X,Y ), BUY(Y )}, resulting in a 3 level abstraction hierarchy shown in Fig. 2.\nThe L1, L2 and L3 abstract actions lead to new questions about which ones should be included in the Majority Voting technique. In this work, we empirically investigate these questions by considering a number of so-called voting hierarchies. In H1 hierarchy, only L1 abstract actions are considered and the best abstract action ã∗ is chosen, after which the concrete action ā is chosen. In hierarchy H2, first the\nSQ(X,Y) , AQ(X,Y) , BUY(Y)\nSQ(i,j) AQ(i,i’)\nBUY(Y)SQ(X,j),SQ(i,Y) AQ(X,i’),AQ(i,Y)\nDNB, Others L3\nL2\nL1 DNB\nDNB\nDNB\nBUY(j) (a) H3\nFigure 2: H3 Voting hierarchy\nbest abstract action among the L2 abstract actions is determined, followed by the best L1 abstract action and finally the concrete action. In hierarchy H3 (shown in Fig. 2), first the best L3 abstract action is determined followed by L2, L1 best abstract actions and then the concrete action.\nIn Algorithm 2, we maintain a separate set of votes AV for abstract actions ã. In Line 8, we determine all the abstract actions that correspond to the regular action a contained in vote v. Subsequently, we increment their counts[ã] and meanQs[ã] (Lines 9-13). Then, in Line 14, the best abstract action ã∗ is first selected, which is subsequently refined to determine the best concrete action. This refinement process depends on the employed voting hierarchy. For instance when using the H1 hierarchy,\nā = arg max a∈A(ã∗)\ncounts[a] ∗meanQs[a],\nwhere A(ã∗) denotes the set of concrete actions consistent with abstract action ã∗."
    }, {
      "heading" : "4.4 Belief Update",
      "text" : "Though we can maintain and perform exact belief updates at the global level, i.e., involving all state factors, using the Bayes’ rule, such exact inference is complex and does not scale to more than 10 agents. Therefore, we propose to employ the approximate inference methods. In particular, we apply Factored Frontier (FF) (Murphy and Weiss 2001), which maintains the belief in fully factored form, i.e., as the product of marginals of state factors xi: b(s) = ∏|s| i=1 b̂(xi). Thus the beliefs for each SP can directly be extracted via bk(s) = ∏ xi∈Xk b̂(xi), where Xk denotes the set of state factors that are a part of the sub-POMDP Mk. While FF is a simple algorithm, and other choices are possible, it does allow influence of variables to propagate through the network and our experiments suggest that FF performs quite well."
    }, {
      "heading" : "5 Experiments",
      "text" : "Here, we empirically investigate the solution quality and scalability of the proposed Mixture of POMDP experts (MOPE) technique in the e-marketplace domain. We are primarily interested to see if the added scalability can actually translate into additional value from the buyer’s perspective.\nExperimental Setup. We analyze different design considerations for MOPE (SPA=4 SPs per agent and APS=5 agents per SP with a uniform composition for all SPs comprised of 1 seller and 4 advisors, such that we can reuse V*, as described in Sec. 4.1) and compare it with:\n1. the original SALE POMDP. We assume uniform initial beliefs and compute the SALE POMDP optimal policy using Symbolic Perseus (Poupart 2005).\n2. SingleExpert(5), i.e., a randomly selected 5-agent SP, serving as the lower bound.\n3. POMCP (Silver and Veness 2010), an online planning approach which requires a number of random simulations (we use 10, 000 simulations per selected action) to estimate the potential for long-term reward.\n4. an optimistic heuristic value Vmaxv , which is the value obtained by running many simulations of MOPE (Majority Voting with H3 hierarchy and SPA=8) on ‘ideal’ global problems (i.e., on 100 agent problems with good sellers and trustworthy advisors). We consider such a heuristic as we know that beginning with a most favourable state (which in our case is the presence of good sellers and trustworthy advisors in the SPs, as they have a higher probability of resulting in successful transactions), results in best performance while executing a POMDP policy.\n5. the Q-MDP value Vqmdp, which is the value obtained by considering the states to be fully observable in the next time step (Littman, Cassandra, and Kaelbling 1995). Though majority of our (query) actions do not have value while computing Vqmdp, we still consider the QMDP value as it can serve as an upper bound.\nWe conduct experiments in a simulated e-marketplace, where buyers need to choose sellers as successful transaction partners. We measure the average error ∈ [0, 1] in terms of the percentage of ‘unsuccessful transactions’ (buying from a bad seller or taking the DNB action in the presence of a good seller) and value, i.e, the discounted (0.95) reward in the process of choosing a seller. The buyer pays a cost of 1 for querying advisors about other advisors, 10 for querying about a seller, gains 100 for choosing a good seller or taking DNB when no seller is of good quality, loses 100 for choosing a bad seller or taking DNB when there is a good seller. The number of sellers is 20% of the whole population W and number of advisors is 80% among which 20% are untrustworthy. All the results are values averaged over 500 iterations from the point of view of a single buyer. We consider single transaction settings, where the buyer has no previous experience with the seller.\nTo analyze the scalability, we increase the number of agents W in the e-marketplace from 6 − 100 (size of the corresponding seller selection problem, is given in Table 1)\nand measure the performance of the approaches in Fig. 3-5. As SALE POMDP does not scale effectively to more than 10 agents (ran out of time while computing the policy), its performance is not shown for W>10 in the figures."
    }, {
      "heading" : "5.1 Influence of using Factored Frontier (FF)",
      "text" : "Fig. 3(a-b) show the influence of using FF for the belief update. We see that while the approximation introduced by FF leads to a reduction in value compared to using exact belief updates, the difference is quite small. Analysis of Different Design Schemes for the Majority Voting MOPE Approach. Fig. 4 shows the analysis of the different design considerations, such as the performance of voting hierarchies, influence of SPA and APS for the Majority Voting MOPE approach. In Fig. 4(a-b), we analyse the performance of the H1, H2 and H3 hierarchies while using the Majority Voting MOPE approach. We see that H3 hierarchy outperforms H1 and H2. Also, we see that for (most) cases where the SALE POMDP is able to provide an answer, it is performing slightly better than H3. This is expected since it does a full POMDP reasoning over the entire state space. However, for larger problems, the difference in performance becomes negligible and when including more advisors, H3 finds policies that lead to significantly smaller errors and higher payoffs. SingleExpert(5) achieves a constant performance as it always considers a group of 5 agents to make decisions. The performance of all other approaches increase with the number of agents as there are more advisors to seek information about the sellers.\nIn Fig. 4(c-d), we analyse the influence of the number of SPs per agent (SPA), using H3 Majority Voting (with default SPA=4). Fig. 4(c-d) show that performance of H3 increases with SPA, i.e., H3S8 (SPA=8) shows the best performance and H3S2 (SPA=2) shows the least performance. This is because, on increasing SPA, the total number of SPs consid-\nered increase, resulting in more informed decision making. We see that H3 and H3S8 outperform SALE POMDP for 10 agents, suggesting that the quality of Symbolic Perseus degrades for larger problems. Fig. 4(e-f) show the influence of the number of agents per SP (APS) for the H3S8 technique. H3S8A6 (APS=6), H3S8A7 (APS=7) and H3S8A8 (APS=8) outperform H3S8 (default APS=5) as increasing APS improves performance by reasoning over a larger state space. Importantly, we see how this enables MOPE to accumulate a significantly higher value (H3S8A7 obtains a value of 72 for 100 agents) than the best SALE POMDP value (65 for 10 agents). We expect that the lower performance of H3S8A8 compared to H3S8A7 is caused by a relative degradation of the solution quality of the (larger) SPs. However, H3S8A6, H3S8A7 and H3S8A8 involve greater policy computation time than H3S8.\nComparison with Max-Q, POMCP, Vmaxv and Vqmdp. In Fig. 5, we compare the performance of H3S8 along with Max-Q (SPA=8, APS=5 and using the FF algorithm for belief update). We have shown the error and value for the POMCP approach in Table 2 separately, to retain the clarity in Fig. 5(a-b). We see that H3S8 outperforms both MaxQ and POMCP. As the number of agents increases, performance of POMCP decreases, as it requires a larger number of simulations to sample the beliefs and histories about the agents. Also, POMCP does not scale well with the number of actions (which is large in these problems). We have not shown the POMCP results for W > 25 due to the complexity of the simulations.\nFig. 5(b) also shows the Vmaxv and Vqmdp values. Specifically, we consider the Vmaxv value, in order show the performance of the Majority Voting MOPE scheme (H3S8), under the most favourable conditions. We know that beginning with a most favourable state always results in best performance while executing a POMDP policy. In our case, the most favourable state represents the presence of good quality sellers and trustworthy advisors in the SPs, as they have a higher probability of resulting in successful transactions, thereby leading to greater value. As we can see, the Vmaxv value is greater than the value obtained by H3S8 for normal problems (in which sellers can also be of low quality and advisors can be untrustworthy). Vqmdp is the upper bound value and looks like a piecewise function because of the same number of sellers in some of the problems. Thus, Fig. 5(b) shows the lower bound, i.e., the value of SingleExpert(5), optimistic heuristic value Vmaxv , and the upper bound Vqmdp for a 5-agent decomposition.\nFig. 6 shows the policy computation time for each seller selection problem involving 6 to 100 agents. For POMCP, we measure the simulation time per episode. We see that the time taken by H3S8, SingleExpert(5), Max-Q is less than SALE POMDP and POMCP. Also, the constant time 22s for H3S8, SingleExpert(5) and Max-Q is due to using the same 5-agent policy for all SPs.\nPerformance in WSN Domain. While the MOPE approach can improve the scalability of the SALE POMDP model in the e-marketplace domain (as shown in Fig. 5), it can also be applied to improve the scalability of the POMDP models in other domains, which follow a similar trust propagation structure as the SALE POMDP. To verify this, we also apply the MOPE approach (H3S4 Majority Voting with APS=3,\nSPA=4) to improve the scalability of the SRP model (see (Irissappane et al. 2015) for details) in the WSN domain and compare it with: 1) the original SRP model; and 2) SingleExpert(3) with 3 agents. We use the same simulation settings as used in (Irissappane et al. 2015). Fig. 7(a-b) show that SRP performs better than H3S4 for 3 − 5 neighbors. However, it cannot provide solutions for more than 5 neighbors, while H3S4 can scale up to 40 neighbors, generating a much higher value. Also, the policy computation time is 73s for H3S4 and 736s for the SRP model for 5 neighbors."
    }, {
      "heading" : "6 Related Work",
      "text" : "There is extensive literature on scalable solutions to solving POMDPs. Point-Based Value Iteration (PBVI) (Pineau, Gordon, and Thrun 2003) computes a value function over a finite subset of the belief space. A point based algorithm explores the belief space, focusing on the reachable belief states, while maintaining a value function by applying the point-based backup operator. Bounded policy iteration (BPI) (Poupart and Boutilier 2003) incrementally constructs a finite state controller by alternating policy improvement and policy evaluation until a local optimum is reached by slowly increasing the number of nodes. Gradient ascent (Aberdeen and Baxter 2002) restricts its search to controllers of a bounded size. However, the above approaches scale only to thousands of states (Poupart and Boutilier 2004).\nIn structured domains, further scaling can be achieved by exploiting compact representations (Feng and Hansen 2001; Guestrin, Koller, and Parr 2001b; Veiga et al. 2014; Poupart and Boutilier 2004), such as decision trees (Boutilier and Poole 1996), algebraic decision diagrams (ADDs) (Hansen and Feng 2000), or by indirectly compressing the belief space into a small subspace by value-directed compression (VDC) (Poupart 2005), one of which is also applied in the regular SALE POMDP model.\nWhile all the above are offline policy computation algorithms, recently an online POMDP planning algorithm called POMCP (Silver and Veness 2010) has successfully scaled up to very large problems. POMCP is based on Monte Carlo tree search, which tries to break the curse of dimensionality and history by sampling states from the current belief and histories with a black-box simulator. On the other hand, in our approach, we use offline policy computation (Poupart 2005), to compute optimal policies for each sub-POMDP, while still achieving better scalability than POMCP (as shown in our experiments).\nSome approaches use a similar concept of decomposing a (PO)MDP into smaller sub-problems. Meuleau et al. (1998) assume that sub-problems are very weakly coupled: each sub-problem corresponds to an independent subtask whose state/action spaces do not directly influence the other tasks. In contrast, MOPE divides a single large POMDP problem into SPs, which can contain overlapping state variables/actions. Similar to our work, Williams and Young (2007) consider a more general decomposition, but they rely on domain specific heuristics, while we investigate several general methods to aggregate the recommendations from all SPs. Yadav et al. (2015) also propose an approach which decomposes a POMDP into SPs, but these are formed in a very different way: by sampling values for sub-sets of hidden state factors. A major difference between all these works and ours, is that their sub-problems directly follow from the domain. In contrast, in our approach, the number of sub-problems can be chosen to control the time vs. quality trade-off.\nDecomposition has also been a popular technique in multiagent planning approaches (Guestrin, Koller, and Parr 2001a; Becker et al. 2003; Nair et al. 2003; Goldman and Zilberstein 2008; Witwicki and Durfee 2010; Oliehoek, Witwicki, and Kaelbling 2012; Amato and Oliehoek 2015; Oliehoek, Spaan, and Witwicki 2015). However, in all these cases structure is exploited that is particular to the multiagent setting by extending insights from factored (PO)MDP approaches and when applied to single-agent problems such as a SALE POMDP these methods do not offer any additional benefits.\nMOPE can be interpreted as a type of ensemble method (Dietterich 2000). In particular, there is a resemblance to random forests (Breiman 2001): the way that they randomly select features is not unlike our random selection of state factors (seller and advisor variables)."
    }, {
      "heading" : "7 Conclusion and Future Work",
      "text" : "We propose the Mixture of POMDP Experts (MOPE) technique to address the scalability issues in solving large seller selection (SALE) POMDP problems for e-marketplaces. MOPE works by dividing the large POMDP problem into computationally tractable smaller sub-POMDPs and then aggregates the actions of the sub-POMDPs. Extensive evaluation shows that MOPE achieves a reasonable approximation to the SALE POMDP for small problems and can scale up to a hundred agents by effectively exploiting the presence of more advisors to generate significantly higher buyer satisfaction. We also show that MOPE improves the scalability of a POMDP model in the sensor network domain.\nWe conduct experiments to select the best action hierarchy to be used in the MOPE approach. However, whether empirically determining good hierarchies for other problems (other than seller selection problems) will be possible is an interesting open question, which we would like to investigate as future work. We will also analyze more sophisticated ways (e.g., using community detection) of dividing the subPOMDPs rather than random partitioning."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work is supported by the A*STAR SERC grant (1224104047) awarded to Dr. Jie Zhang, NWO Innovational Research Incentives Scheme Veni #639.021.336 awarded to Dr. Frans A. Oliehoek and the Institute for Media Innovation at Nanyang Technological University."
    } ],
    "references" : [ {
      "title" : "and Baxter",
      "author" : [ "D. Aberdeen" ],
      "venue" : "J.",
      "citeRegEx" : "Aberdeen and Baxter 2002",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "F",
      "author" : [ "C. Amato", "Oliehoek" ],
      "venue" : "A.",
      "citeRegEx" : "Amato and Oliehoek 2015",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "C",
      "author" : [ "R. Becker", "S. Zilberstein", "V. Lesser", "Goldman" ],
      "venue" : "V.",
      "citeRegEx" : "Becker et al. 2003",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "and Poole",
      "author" : [ "C. Boutilier" ],
      "venue" : "D.",
      "citeRegEx" : "Boutilier and Poole 1996",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "M",
      "author" : [ "A.R. Cassandra", "L.P. Kaelbling", "Littman" ],
      "venue" : "L.",
      "citeRegEx" : "Cassandra. Kaelbling. and Littman 1994",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "2000",
      "author" : [ "Dietterich", "T. G" ],
      "venue" : "Ensemble methods in machine learning. In Multiple Classifier Systems, volume",
      "citeRegEx" : "Dietterich 2000",
      "shortCiteRegEx" : null,
      "year" : 1857
    }, {
      "title" : "E",
      "author" : [ "Z. Feng", "Hansen" ],
      "venue" : "A.",
      "citeRegEx" : "Feng and Hansen 2001",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "and Zilberstein",
      "author" : [ "C.V. Goldman" ],
      "venue" : "S.",
      "citeRegEx" : "Goldman and Zilberstein 2008",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Multiagent planning with factored MDPs",
      "author" : [ "Koller Guestrin", "C. Parr 2001a] Guestrin", "D. Koller", "R. Parr" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Guestrin et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Guestrin et al\\.",
      "year" : 2001
    }, {
      "title" : "Solving factored POMDPs with linear value functions",
      "author" : [ "Koller Guestrin", "C. Parr 2001b] Guestrin", "D. Koller", "R. Parr" ],
      "venue" : "In Proceedings of the IJCAI Workshop on Planning under Uncertainty and Incomplete Information,",
      "citeRegEx" : "Guestrin et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Guestrin et al\\.",
      "year" : 2001
    }, {
      "title" : "and Feng",
      "author" : [ "E.A. Hansen" ],
      "venue" : "Z.",
      "citeRegEx" : "Hansen and Feng 2000",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "and Zhang",
      "author" : [ "A.A. Irissappane" ],
      "venue" : "J.",
      "citeRegEx" : "Irissappane and Zhang 2015",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "P",
      "author" : [ "A.A. Irissappane", "J. Zhang", "F.A. Oliehoek", "Dutta" ],
      "venue" : "S.",
      "citeRegEx" : "Irissappane et al. 2015",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "F",
      "author" : [ "Irissappane, A.A.", "Oliehoek" ],
      "venue" : "A.; and Zhang, J.",
      "citeRegEx" : "Irissappane. Oliehoek. and Zhang 2014",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "A",
      "author" : [ "L.P. Kaelbling", "M.L. Littman", "Cassandra" ],
      "venue" : "R.",
      "citeRegEx" : "Kaelbling. Littman. and Cassandra 1998",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "and Friedman",
      "author" : [ "D. Koller" ],
      "venue" : "N.",
      "citeRegEx" : "Koller and Friedman 2009",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "W",
      "author" : [ "H. Kurniawati", "D. Hsu", "Lee" ],
      "venue" : "S.",
      "citeRegEx" : "Kurniawati. Hsu. and Lee 2008",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "L",
      "author" : [ "M.L. Littman", "A.R. Cassandra", "Kaelbling" ],
      "venue" : "P.",
      "citeRegEx" : "Littman. Cassandra. and Kaelbling 1995",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "T",
      "author" : [ "N. Meuleau", "M. Hauskrecht", "K.-E. Kim", "L. Peshkin", "L.P. Kaelbling", "Dean" ],
      "venue" : "L.; and Boutilier, C.",
      "citeRegEx" : "Meuleau et al. 1998",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "and Weiss",
      "author" : [ "K. Murphy" ],
      "venue" : "Y.",
      "citeRegEx" : "Murphy and Weiss 2001",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Taming decentralized POMDPs: Towards efficient policy computation for multiagent settings",
      "author" : [ "Nair" ],
      "venue" : "In Proceedings of the 18th International Joint Conference on Artificial Intelligence (IJCAI),",
      "citeRegEx" : "Nair,? \\Q2003\\E",
      "shortCiteRegEx" : "Nair",
      "year" : 2003
    }, {
      "title" : "A",
      "author" : [ "Oliehoek, F.A.", "Gokhale" ],
      "venue" : "A.; and Zhang, J.",
      "citeRegEx" : "Oliehoek. Gokhale. and Zhang 2012",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "S",
      "author" : [ "F.A. Oliehoek", "M.T. Spaan", "Witwicki" ],
      "venue" : "J.",
      "citeRegEx" : "Oliehoek. Spaan. and Witwicki 2015",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "L",
      "author" : [ "F.A. Oliehoek", "S.J. Witwicki", "Kaelbling" ],
      "venue" : "P.",
      "citeRegEx" : "Oliehoek. Witwicki. and Kaelbling 2012",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Point-based value iteration: An anytime algorithm for POMDPs",
      "author" : [ "Gordon Pineau", "J. Thrun 2003] Pineau", "G. Gordon", "S. Thrun" ],
      "venue" : "In Proceedings of the 18th International Joint Conference on Artificial Intelligence (IJCAI),",
      "citeRegEx" : "Pineau et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Pineau et al\\.",
      "year" : 2003
    }, {
      "title" : "and Boutilier",
      "author" : [ "P. Poupart" ],
      "venue" : "C.",
      "citeRegEx" : "Poupart and Boutilier 2003",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "and Boutilier",
      "author" : [ "P. Poupart" ],
      "venue" : "C.",
      "citeRegEx" : "Poupart and Boutilier 2004",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "The Advisor-POMDP: A principled approach to trust through reputation in electronic markets",
      "author" : [ "Cohen Regan", "K. Poupart 2005] Regan", "R. Cohen", "P. Poupart" ],
      "venue" : "In Proceedings of the International Conference on Privacy, Security and Trust (PST)",
      "citeRegEx" : "Regan et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Regan et al\\.",
      "year" : 2005
    }, {
      "title" : "and Veness",
      "author" : [ "D. Silver" ],
      "venue" : "J.",
      "citeRegEx" : "Silver and Veness 2010",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "M",
      "author" : [ "Spaan" ],
      "venue" : "T. J.",
      "citeRegEx" : "Spaan 2012",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "C",
      "author" : [ "T.S. Veiga", "M.T. Spaan", "P.U. Lima", "Brodley" ],
      "venue" : "E.; and Stone, P.",
      "citeRegEx" : "Veiga et al. 2014",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "and Young",
      "author" : [ "J.D. Williams" ],
      "venue" : "S.",
      "citeRegEx" : "Williams and Young 2007",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "E",
      "author" : [ "S.J. Witwicki", "Durfee" ],
      "venue" : "H.",
      "citeRegEx" : "Witwicki and Durfee 2010",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Preventing HIV spread in homeless populations using PSINET",
      "author" : [ "Yadav" ],
      "venue" : "In Proceedings of the 27th Conference on Innovative Applications of Artificial Intelli-",
      "citeRegEx" : "Yadav,? \\Q2015\\E",
      "shortCiteRegEx" : "Yadav",
      "year" : 2015
    } ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "In multiagent e-marketplaces, buying agents need to select good sellers by querying other buyers (called advisors). Partially Observable Markov Decision Processes (POMDPs) have shown to be an effective framework for optimally selecting sellers by selectively querying advisors. However, current solution methods do not scale to hundreds or even tens of agents operating in the e-market. In this paper, we propose the Mixture of POMDP Experts (MOPE) technique, which exploits the inherent structure of trust-based domains, such as the seller selection problem in e-markets, by aggregating the solutions of smaller sub-POMDPs. We propose a number of variants of the MOPE approach that we analyze theoretically and empirically. Experiments show that MOPE can scale up to a hundred agents thereby leveraging the presence of more advisors to significantly improve buyer satisfaction.",
    "creator" : "LaTeX with hyperref package"
  }
}