{
  "name" : "1707.06456.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Revisiting Selectional Preferences for Coreference Resolution",
    "authors" : [ "Benjamin Heinzerling", "Nafise Sadat Moosavi", "Michael Strube" ],
    "emails" : [ "benjamin.heinzerling@h-its.org", "nafise.moosavi@h-its.org", "michael.strube@h-its.org" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Selectional preferences have long been claimed to be useful for coreference resolution. In his seminal work on “Resolving Pronominal References” Hobbs (1978) proposed a semantic approach that requires reasoning about the “demands the predicate makes on its arguments.” For example, selectional preferences allow resolving the pronoun it in the text “The Titanic hit an iceberg. It sank quickly.” Here, the predicate sink ‘prefers’ certain subject arguments over others: It is plausible that a ship sinks, but implausible that an iceberg does.\nWork on the automatic acquisition of selectional preferences has shown considerable progress (Dagan and Itai, 1990; Resnik, 1993; Agirre and Martinez, 2001; Pantel et al., 2007; Erk, 2007; Ritter et al., 2010; Van de Cruys, 2014). However, today’s coreference resolvers (Martschat and Strube, 2015; Wiseman et al., 2016; Clark and Manning, 2016a, i.a.) capture selectional preferences only\n∗ These authors contributed equally to this work.\nimplicitly at best, e.g., via a given mention’s dependency governor and other contextual features.\nSince negative results do not often get reported, there is no clear evidence in the literature regarding the non-utility of particular knowledge sources. Consequently, an absence of the explicit modeling of selectional preferences in the recent literature is an indicator that incorporating this knowledge source has not been very successful for coreference resolution.\nMore than ten years ago, Kehler et al. (2004) declared the “non-utility of predicate-argument structures for pronoun resolution” and observed that minor improvements on a small dataset were due to fortuity rather than selectional preferences having captured meaningful world knowledge relations.\nThe claim by Kehler et al. (2004) is based on selectional preferences extracted from a, by current standards, small number of 2.8m predicateargument pairs. Furthermore, they employ a simple (linear) maximum entropy classifier, which requires manual definition of feature combinations and is unlikely to fully capture the complex interaction between selectional preferences and other coreference features. Therefore, it is worth revisiting how a better selectional preference model affects the performance of a more complex coreference resolver.\nIn this work, we propose a fine-grained, highcoverage model of selectional preferences and study its impact on a state-of-the-art, non-linear coreference resolver. We show that the incorporation of our selectional preference model improves the performance. However, it is debatable whether such small improvements, that cost notable extra time or resources, are advantageous.\nar X\niv :1\n70 7.\n06 45\n6v 1\n[ cs\n.C L\n] 2\n0 Ju\nl 2 01\n7"
    }, {
      "heading" : "2 Modeling Selectional Preferences",
      "text" : "The main design choice when modeling selectional preferences is the selection of a relation inventory, i.e. the concepts and entities that can be relation arguments, and the semantic relationships that hold between them.\nPrior work has studied many relation inventories. Predicate-argument statistics for word-word pairs (eat, food)1 are easy to obtain but do not generalize to unseen pairs (Dagan and Itai, 1990). Class-based approaches generalize via word-class pairs (eat, /nutrient/food) (Resnik, 1993) or classclass pairs (/ingest, /nutrient/food) (Agirre and Martinez, 2001), but require disambiguation of words to classes and are limited by the coverage of the lexical resource providing such classes (e.g. WordNet).\nOther possible relation inventories include semantic representations such as FrameNet frames and roles, event types and arguments, or abstract meaning representations. While these semantic representations are arguably well-suited to model meaningful world knowledge relationships, automatic annotation is limited in speed and accuracy, making it difficult to obtain a large number of such “more semantic” predicate-argument pairs. In comparison, syntactic parsing is both fast and accurate, making it trivial to obtain a large number of accurate, albeit “less semantic” predicate-argument pairs. The drawback of a syntactic model of selectional preferences is susceptibility to lexical and syntactic variation. For example, The Titanic sank and The ship went under differ lexically and syntactically, but would have the same or a very similar representation in a semantic framework such as FrameNet.\nOur model of selectional preferences (Figure 1)\n1Examples due to Agirre and Martinez (2001).\novercomes this drawback via distributed representation of predicate-argument pairs, using (syntactic) dependencies that were specifically designed for semantic downstream tasks, and by resolving named entities to their fine-grained entity types.\nDistributed representation. Inspired by Structured Vector Space (Erk and Padó, 2008), we embed predicates and arguments into a lowdimensional space in which (representations of) predicate slots are close to (representations of) their plausible arguments, as should be arguments that tend to fill the same slots of similar predicates, and predicate slots that have similar arguments. For example, captain should be close to pilot, ship to airplane, the subject of steer close to both captain and pilot, and also to, e.g., the subject of drive. Such a space allows judging the plausibility of unseen predicate-argument pairs.2\nWe construct this space via dependency-based word embeddings (Levy and Goldberg, 2014). To see why this choice is better-suited for modeling selectional preferences than alternatives such as word2vec (Mikolov et al., 2013) or GloVe (Pennington et al., 2014), consider the following example:\ncaptain nsubj←−− steers dobj−−→ ship\n:: ::\npilot nsubj←−− steers dobj−−→ airplane\nHere, captain and ship, have high syntagmatic similarity, i.e., these words are semantically related and tend to occur close to each other. This also holds for pilot and airplane. In contrast, captain and pilot, as well as ship and airplane have high paradigmatic similarity, i.e., they are seman-\n2Prior work generalizes to unseen predicate-argument pairs via WordNet synsets (Resnik, 1993), a generalization corpus (Erk, 2007), or tensor factorization (Van de Cruys, 2010). Closest to our approach is neural model by Van de Cruys (2014), which, however, has much lower coverage since it is limited to 7k verbs and 30k arguments.\ntically similar and occur in similar contexts. A model of selectional preferences requires paradigmatic similarity: The representations of captain and pilot in such a model should be similar, since they both can plausibly fill the subject slot of the predicate steer. Due to their use of linear context windows, word2vec and GloVe capture syntagmatic similarity, while dependency-based embeddings capture paradigmatic similarity (cf. Levy and Goldberg, 2014).\nEnhanced++ dependencies. Due to distributed representation, our model generalizes over syntactic variation such as active/passive alternations: For example, steer@dobj3 is highly similar to steer@nsubjpass (see Appendix for more examples). To further mitigate the effect of employing syntax as a proxy for semantics, we use Enhanced++ dependencies (Schuster and Manning, 2016). Enhanced++ dependencies aim to support semantic applications by modifying syntactic parse trees to better reflect relations between content words. For example, the plain syntactic parse of the sentence Both of the girls laughed identifies Both as subject of laughed. The Enhanced++ representation introduces a subject relation between girls and laughed, which allows learning more meaningful selectional preferences: Our model should learn that girls (and other humans) laugh, while learning that an unspecified both laughs is not helpful.\nFine-grained entity types. A good model of selectional preferences needs to generalize over named entities. For example, having encountered sentences like The Titanic sank, our model should be able to judge the plausibility of an unseen sentence like The RMS Lusitania sank. For popular named entities, we can expect the learned representations of Titanic and RMS Lusitania to be similar, allowing our model to generalize, i.e., it can judge the plausibility of The RMS Lusitania sank by virtue of the similarity between Titanic and RMS Lusitania. However, this will not work for rare or emerging named entities, for which no, or only low-quality, distributed representations have been learned. To address this issue, we incorporate fine-grained entity typing (Ling and Weld). For each named entity encountered during training, we generate an additional training instance by replacing the named entity with its entity type,\n3In this work, a predicate’s argument slots are denoted predicate@slot.\ne.g. (Titanic, sank@nsubj) yields (/product/ship, sank@nsubj)."
    }, {
      "heading" : "3 Implementation",
      "text" : "We train our model by combining term-context pairs from two sources. Noun phrases and their dependency context are extracted from GigaWord (Parker et al., 2011) and entity types in context from Wikilinks (Singh et al., 2012). Term-context pairs are obtained by parsing each corpus with the Stanford CoreNLP dependency parser (Manning et al., 2014). After filtering, this yields ca. 1.4 billion phrase-context pairs such as (Titanic, sank@nsubj) from GigaWord and ca. 12.9 million entity type-context pairs such as (/product/ship, sank@nsubj) from Wikilinks. Finally, we train dependency-based embeddings using the generalized word2vec version by Levy and Goldberg (2014), obtaining distributed representations of selectional preferences. To identify fine-grained types of named entities at test time, we first perform entity linking using the system by Heinzerling et al. (2016), then query Freebase (Bollacker et al., 2008) for entity types and apply the mapping to fine-grained types by Ling and Weld.\nThe plausibility of an argument filling a particular predicate slot can now be computed via the cosine similarity of their associated embeddings. For example, in our trained model, the similarity of (Titanic, sank@nsubj) is 0.11 while the similarity of (iceberg, sank@nsubj) is -0.005, indicating that an iceberg sinking is less plausible."
    }, {
      "heading" : "4 Do Selectional Preferences Benefit Coreference Resolution?",
      "text" : "We now investigate the effect of incorporating selectional preferences, implicitly and explicitly, in coreference resolution.\nFigure 2 shows the selectional preference similarity of 10.000 coreferent and 10.000 noncoreferent mention pairs sampled randomly from the CoNLL 2012 training set. As we can see, while coreferent mention pairs are more similar than non-coreferent mention pairs according to the selectional preference similarity, there is not a direct relation between the similarity values and the coreferent relation. This indicates that coreference does not have a linear relation to the selectional preference similarities. However, it is worth investigating how these similarity values affect the overall performance when they are combined with\nother knowledge sources in a non-linear way.\nWe select the ranking model of deep-coref (Clark and Manning, 2016b) as our baseline. deep-coref is a neural model that combines the input features through several hidden layers. Baseline in Table 1 reports our baseline results on the CoNLL 2012 test set. The results are reported using MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005), the average F1 score of these three metrics, i.e. CoNLL score, and LEA (Moosavi and Strube, 2016b). deepcoref includes the embeddings of the dependency governor of mentions. Combined with the relative position of a mention to its governor, deep-coref may be able to implicitly capture selectional preferences to some extent. −gov in Table 1 represents deep-coref performance when governors are not incorporated. As we can see, the exclusion of the governor information does not affect the performance. This result shows that the implicit mod-\neling of selectional preferences does not provide any additional information to the coreference resolver.\nFor each mention, we consider (1) the whole mention string, (2) the whole mention string without articles, (3) mention head, (4) context representation, i.e. governor@dependency-relation, and (5) entity types if the mention is a named entity. We obtain an embedding for each of the above properties if they exist in the selectional preference model, otherwise we set them to unknown.\nFor each (antecedent, anaphor) pair, we consider all the acquired embeddings of anaphor and antecedent. We try two different ways of incorporating this knowledge into deep-coref including: (1) incorporating the computed embeddings directly as a new set of inputs, i.e. +embedding in Table 2. We add a new hidden layer on top of the new embeddings and combine its output with outputs of the hidden layers associated with other sets of inputs; and (2) computing a similarity value between all possible combinations of the antecedentanaphor acquired embeddings and then binarizing all similarity values, i.e. +binned sim. in Table 2.\nProviding selectional preference embeddings directly to deep-coref adds more complexity to the baseline coreference resolver. Yet, it performs onpar with +binned sim. on the development set and generalizes worse on the test set. +SP in Table 1 is the performance of +binned sim. on the test set. As we can see from the results, adding selectional\npreferences as binary features improves over the baseline.\nReinforce in Table 1 presents the results of the reward-rescaling model of Clark and Manning (2016a) that are so far the highest reported results on the official test set. The reward rescaling model of Clark and Manning (2016a) casts the ranking model of Clark and Manning (2016b) in the reinforcement learning framework which considerably increases the training time, from two days to six days in our experiments.\nWe analyze how our selectional preference model affects the resolution of various types of mentions. We use Martschat and Strube (2014)’s toolkit 4 to perform recall and error analyses. The differences in the number of recall and precision errors in +SP compared to baseline on the test set are reported in Table 4.\nBy using our selectional preference features, the number of recall errors decreases for all types of mentions. The recall error reduction is more prominent for pronouns. On the other hand, the number of precision errors increases for all types of mentions. The increase in the precision error is the highest for common nouns. Overall, +SP creates about 260 more links than baseline.\nTable 3 lists a few examples from the development set in which +SP creates a link that baseline does not. It also includes the similarity that has a high value for the linked mentions and probably is the reason for creating the link. For instance, in the first example, based on our model, similarity(impact@nsubj,shows@nsubj) is known and it is also higher than similarity(impact@dobj,shows@nsubj).\n4https://github.com/smartschat/cort\nIn order to estimate a higher bound on the expected performance boost, we run the baseline and +SP models only on anaphoric mentions. By using anaphoric mentions, the performance improves by one percent, based on both the CoNLL score and LEA. This result indicates that the incorporation of selectional preferences creates many links for nonanaphoric mentions, which in turn decreases precision. Therefore, the overall performance does not improve substantially when system mentions are used. deep-coref incorporates anaphoricity scores at resolution time. One possible way to further improve the results of +SP is to incorporate anaphoricity scores at the input level. In this way, the coreference resolver could learn to use selectional preferences mainly for mentions that are more likely to be anaphoric. However, given that the F1 score of current anaphoricity determiners or singleton detectors is only around 85 percent (Moosavi and Strube, 2016a, 2017), the effect of using system anaphoricity scores might be small."
    }, {
      "heading" : "5 Conclusions",
      "text" : "We introduce a new model of selectional preferences, which combines dependency-based word embeddings and fine-grained entity types. In order to be effective, a selectional preference model should (1) have a high coverage so it can be used for large datasets like CoNLL, and (2) be combined with other knowledge sources in a nonlinear way. Our selectional preference model slightly improves coreference resolution performance, but considering the extra resources that are required to train the model, it is debatable whether such small improvements are advantageous for solving coreference."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The authors would like to thank the three anonymous reviewers for their helpful comments. This work has been supported by the German Research Foundation (DFG) as part of the Research Training Group Adaptive Preparation of Information from Heterogeneous Sources (AIPHES) under grant No. GRK 1994/1 and the Klaus Tschira\nFoundation, Heidelberg, Germany."
    } ],
    "references" : [ {
      "title" : "Learning class-to-class selectional preferences",
      "author" : [ "Eneko Agirre", "David Martinez." ],
      "venue" : "Proceedings of the ACL 2001 Workshop on Computational Natural Language Learning (ConLL), pages 15–22.",
      "citeRegEx" : "Agirre and Martinez.,? 2001",
      "shortCiteRegEx" : "Agirre and Martinez.",
      "year" : 2001
    }, {
      "title" : "Algorithms for scoring coreference chains",
      "author" : [ "Amit Bagga", "Breck Baldwin." ],
      "venue" : "Proceedings of the 1st International Conference on Language Resources and Evaluation, Granada, Spain, 28–30 May 1998, pages 563–566.",
      "citeRegEx" : "Bagga and Baldwin.,? 1998",
      "shortCiteRegEx" : "Bagga and Baldwin.",
      "year" : 1998
    }, {
      "title" : "Freebase: A collaboratively created graph database for structuring human knowledge",
      "author" : [ "Kurt Bollacker", "Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor." ],
      "venue" : "Proceedings of the 2008 ACM SIGMOD International Conference on Management",
      "citeRegEx" : "Bollacker et al\\.,? 2008",
      "shortCiteRegEx" : "Bollacker et al\\.",
      "year" : 2008
    }, {
      "title" : "Deep reinforcement learning for mention-ranking coreference models",
      "author" : [ "Kevin Clark", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, Austin, Tex., 1–5 November",
      "citeRegEx" : "Clark and Manning.,? 2016a",
      "shortCiteRegEx" : "Clark and Manning.",
      "year" : 2016
    }, {
      "title" : "Improving coreference resolution by learning entitylevel distributed representations",
      "author" : [ "Kevin Clark", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
      "citeRegEx" : "Clark and Manning.,? 2016b",
      "shortCiteRegEx" : "Clark and Manning.",
      "year" : 2016
    }, {
      "title" : "A non-negative tensor factorization model for selectional preference induction",
      "author" : [ "Tim Van de Cruys." ],
      "venue" : "Natural Language Engineering, 16(4):417–437.",
      "citeRegEx" : "Cruys.,? 2010",
      "shortCiteRegEx" : "Cruys.",
      "year" : 2010
    }, {
      "title" : "A neural network approach to selectional preference acquisition",
      "author" : [ "Tim Van de Cruys." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 26– 35, Doha, Qatar.",
      "citeRegEx" : "Cruys.,? 2014",
      "shortCiteRegEx" : "Cruys.",
      "year" : 2014
    }, {
      "title" : "Automatic processing of large corpora for the resolution of anaphora references",
      "author" : [ "Ido Dagan", "Alon Itai." ],
      "venue" : "Proceedings of the 13th International Conference on Computational Linguistics, Helsinki, Finland, 20–25 August 1990, volume 3, pages 330–",
      "citeRegEx" : "Dagan and Itai.,? 1990",
      "shortCiteRegEx" : "Dagan and Itai.",
      "year" : 1990
    }, {
      "title" : "A simple, similarity-based model for selectional preferences",
      "author" : [ "Katrin Erk." ],
      "venue" : "Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics, Prague, Czech Republic, 23–30 June 2007, pages 216–223.",
      "citeRegEx" : "Erk.,? 2007",
      "shortCiteRegEx" : "Erk.",
      "year" : 2007
    }, {
      "title" : "A structured vector space model for word meaning in context",
      "author" : [ "Katrin Erk", "Sebastian Padó." ],
      "venue" : "Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 897–906.",
      "citeRegEx" : "Erk and Padó.,? 2008",
      "shortCiteRegEx" : "Erk and Padó.",
      "year" : 2008
    }, {
      "title" : "HITS at TAC KBP 2015: Entity discovery and linking, and event nugget detection",
      "author" : [ "Benjamin Heinzerling", "Alex Judea", "Michael Strube." ],
      "venue" : "Proceedings of the Text Analysis Conference, National Institute of Standards and Technol-",
      "citeRegEx" : "Heinzerling et al\\.,? 2016",
      "shortCiteRegEx" : "Heinzerling et al\\.",
      "year" : 2016
    }, {
      "title" : "Resolving pronominal references",
      "author" : [ "Jerry R. Hobbs." ],
      "venue" : "Lingua, 44:311–338.",
      "citeRegEx" : "Hobbs.,? 1978",
      "shortCiteRegEx" : "Hobbs.",
      "year" : 1978
    }, {
      "title" : "The (non)utility of predicate-argument frequencies for pronoun interpretation",
      "author" : [ "Andrew Kehler", "Douglas Appelt", "Lara Taylor", "Aleksandr Simma." ],
      "venue" : "Proceedings of the Human Language Technology Conference of the North Ameri-",
      "citeRegEx" : "Kehler et al\\.,? 2004",
      "shortCiteRegEx" : "Kehler et al\\.",
      "year" : 2004
    }, {
      "title" : "Dependencybased word embeddings",
      "author" : [ "Omer Levy", "Yoav Goldberg." ],
      "venue" : "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 302–308.",
      "citeRegEx" : "Levy and Goldberg.,? 2014",
      "shortCiteRegEx" : "Levy and Goldberg.",
      "year" : 2014
    }, {
      "title" : "Fine-grained entity recognition",
      "author" : [ "Xiao Ling", "Daniel S. Weld" ],
      "venue" : "In Proceedings of the 26th Conference on the Advancement of Artificial Intelligence, Toronto, Ontario, Canada,",
      "citeRegEx" : "Ling and Weld.,? \\Q2012\\E",
      "shortCiteRegEx" : "Ling and Weld.",
      "year" : 2012
    }, {
      "title" : "On coreference resolution performance metrics",
      "author" : [ "Xiaoqiang Luo." ],
      "venue" : "Proceedings of the Human Language Technology Conference and the 2005 Conference on Empirical Methods in Natural Language Processing, Vancouver, B.C., Canada, 6–8",
      "citeRegEx" : "Luo.,? 2005",
      "shortCiteRegEx" : "Luo.",
      "year" : 2005
    }, {
      "title" : "The Stanford CoreNLP natural language processing toolkit",
      "author" : [ "Christopher D. Manning", "Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven J. Bethard", "David McClosky." ],
      "venue" : "Proceedings of 52nd Annual Meeting of the Association for Computa-",
      "citeRegEx" : "Manning et al\\.,? 2014",
      "shortCiteRegEx" : "Manning et al\\.",
      "year" : 2014
    }, {
      "title" : "Recall error analysis for coreference resolution",
      "author" : [ "Sebastian Martschat", "Michael Strube." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, Doha, Qatar, 25–29 October 2014, pages 2070–2081.",
      "citeRegEx" : "Martschat and Strube.,? 2014",
      "shortCiteRegEx" : "Martschat and Strube.",
      "year" : 2014
    }, {
      "title" : "Latent structures for coreference resolution",
      "author" : [ "Sebastian Martschat", "Michael Strube." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 3:405–418.",
      "citeRegEx" : "Martschat and Strube.,? 2015",
      "shortCiteRegEx" : "Martschat and Strube.",
      "year" : 2015
    }, {
      "title" : "Efficient estimation of word representations in vector space",
      "author" : [ "Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean." ],
      "venue" : "Proceedings of the ICLR 2013 Workshop Track.",
      "citeRegEx" : "Mikolov et al\\.,? 2013",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Search space pruning: A simple solution for better coreference resolvers",
      "author" : [ "Nafise Sadat Moosavi", "Michael Strube." ],
      "venue" : "Proceedings of the",
      "citeRegEx" : "Moosavi and Strube.,? 2016a",
      "shortCiteRegEx" : "Moosavi and Strube.",
      "year" : 2016
    }, {
      "title" : "Which coreference evaluation metric do you trust? A proposal for a link-based entity aware metric",
      "author" : [ "Nafise Sadat Moosavi", "Michael Strube." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1:",
      "citeRegEx" : "Moosavi and Strube.,? 2016b",
      "shortCiteRegEx" : "Moosavi and Strube.",
      "year" : 2016
    }, {
      "title" : "Use generalized representations, but do not forget surface features",
      "author" : [ "Nafise Sadat Moosavi", "Michael Strube." ],
      "venue" : "Proceedings of the 2nd Workshop on Coreference Resolution Beyond OntoNotes (CORBON 2017), pages 1–7, Valencia, Spain.",
      "citeRegEx" : "Moosavi and Strube.,? 2017",
      "shortCiteRegEx" : "Moosavi and Strube.",
      "year" : 2017
    }, {
      "title" : "ISP: Learning inferential selectional preferences",
      "author" : [ "Patrick Pantel", "Rahul Bhagat", "Bonaventura Coppola", "Timothy Chklovski", "Eduard Hovy." ],
      "venue" : "Human Language Technologies 2007: The Conference of the North American Chapter of the Association",
      "citeRegEx" : "Pantel et al\\.,? 2007",
      "shortCiteRegEx" : "Pantel et al\\.",
      "year" : 2007
    }, {
      "title" : "English Gigaword Fifth Edition",
      "author" : [ "Robert Parker", "David Graff", "Junbo Kong", "Ke Chen", "Kazuaki Maeda." ],
      "venue" : "LDC2011T07.",
      "citeRegEx" : "Parker et al\\.,? 2011",
      "shortCiteRegEx" : "Parker et al\\.",
      "year" : 2011
    }, {
      "title" : "GloVe: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, Doha, Qatar, 25–29 October",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Selection and Information: A Class-based Approach to Lexical Relationships",
      "author" : [ "Philip Resnik." ],
      "venue" : "Ph.D. thesis, Department of Computer and Information Science, University of Pennsylvania, Philadelphia, Penn.",
      "citeRegEx" : "Resnik.,? 1993",
      "shortCiteRegEx" : "Resnik.",
      "year" : 1993
    }, {
      "title" : "A latent dirichlet allocation method for selectional preferences",
      "author" : [ "Alan Ritter", "Mausam", "Oren Etzioni." ],
      "venue" : "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 424–434.",
      "citeRegEx" : "Ritter et al\\.,? 2010",
      "shortCiteRegEx" : "Ritter et al\\.",
      "year" : 2010
    }, {
      "title" : "Enhanced english universal dependencies: An improved representation for natural language understanding tasks",
      "author" : [ "Sebastian Schuster", "Christopher D. Manning." ],
      "venue" : "Proceedings of the Tenth International Conference on Language Resources and Eval-",
      "citeRegEx" : "Schuster and Manning.,? 2016",
      "shortCiteRegEx" : "Schuster and Manning.",
      "year" : 2016
    }, {
      "title" : "Wikilinks: A large-scale cross-document coreference corpus labeled via links to Wikipedia",
      "author" : [ "Sameer Singh", "Amarnag Subramanya", "Fernando Pereira", "Andrew McCallum." ],
      "venue" : "Technical Report UM-CS-2012-015, University of Mas-",
      "citeRegEx" : "Singh et al\\.,? 2012",
      "shortCiteRegEx" : "Singh et al\\.",
      "year" : 2012
    }, {
      "title" : "A modeltheoretic coreference scoring scheme",
      "author" : [ "Marc Vilain", "John Burger", "John Aberdeen", "Dennis Connolly", "Lynette Hirschman." ],
      "venue" : "Proceedings of the 6th Message Understanding Conference (MUC-6), pages 45–52, San Mateo, Cal. Morgan",
      "citeRegEx" : "Vilain et al\\.,? 1995",
      "shortCiteRegEx" : "Vilain et al\\.",
      "year" : 1995
    }, {
      "title" : "Learning global features for coreference resolution",
      "author" : [ "Sam Wiseman", "Alexander M. Rush", "Stuart Shieber." ],
      "venue" : "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-",
      "citeRegEx" : "Wiseman et al\\.,? 2016",
      "shortCiteRegEx" : "Wiseman et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : "Work on the automatic acquisition of selectional preferences has shown considerable progress (Dagan and Itai, 1990; Resnik, 1993; Agirre and Martinez, 2001; Pantel et al., 2007; Erk, 2007; Ritter et al., 2010; Van de Cruys, 2014).",
      "startOffset" : 93,
      "endOffset" : 229
    }, {
      "referenceID" : 26,
      "context" : "Work on the automatic acquisition of selectional preferences has shown considerable progress (Dagan and Itai, 1990; Resnik, 1993; Agirre and Martinez, 2001; Pantel et al., 2007; Erk, 2007; Ritter et al., 2010; Van de Cruys, 2014).",
      "startOffset" : 93,
      "endOffset" : 229
    }, {
      "referenceID" : 0,
      "context" : "Work on the automatic acquisition of selectional preferences has shown considerable progress (Dagan and Itai, 1990; Resnik, 1993; Agirre and Martinez, 2001; Pantel et al., 2007; Erk, 2007; Ritter et al., 2010; Van de Cruys, 2014).",
      "startOffset" : 93,
      "endOffset" : 229
    }, {
      "referenceID" : 23,
      "context" : "Work on the automatic acquisition of selectional preferences has shown considerable progress (Dagan and Itai, 1990; Resnik, 1993; Agirre and Martinez, 2001; Pantel et al., 2007; Erk, 2007; Ritter et al., 2010; Van de Cruys, 2014).",
      "startOffset" : 93,
      "endOffset" : 229
    }, {
      "referenceID" : 8,
      "context" : "Work on the automatic acquisition of selectional preferences has shown considerable progress (Dagan and Itai, 1990; Resnik, 1993; Agirre and Martinez, 2001; Pantel et al., 2007; Erk, 2007; Ritter et al., 2010; Van de Cruys, 2014).",
      "startOffset" : 93,
      "endOffset" : 229
    }, {
      "referenceID" : 27,
      "context" : "Work on the automatic acquisition of selectional preferences has shown considerable progress (Dagan and Itai, 1990; Resnik, 1993; Agirre and Martinez, 2001; Pantel et al., 2007; Erk, 2007; Ritter et al., 2010; Van de Cruys, 2014).",
      "startOffset" : 93,
      "endOffset" : 229
    }, {
      "referenceID" : 12,
      "context" : "More than ten years ago, Kehler et al. (2004) declared the “non-utility of predicate-argument",
      "startOffset" : 25,
      "endOffset" : 46
    }, {
      "referenceID" : 12,
      "context" : "The claim by Kehler et al. (2004) is based on selectional preferences extracted from a, by cur-",
      "startOffset" : 13,
      "endOffset" : 34
    }, {
      "referenceID" : 7,
      "context" : "Predicate-argument statistics for word-word pairs (eat, food)1 are easy to obtain but do not generalize to unseen pairs (Dagan and Itai, 1990).",
      "startOffset" : 120,
      "endOffset" : 142
    }, {
      "referenceID" : 26,
      "context" : "Class-based approaches generalize via word-class pairs (eat, /nutrient/food) (Resnik, 1993) or classclass pairs (/ingest, /nutrient/food) (Agirre and",
      "startOffset" : 77,
      "endOffset" : 91
    }, {
      "referenceID" : 0,
      "context" : "Examples due to Agirre and Martinez (2001). overcomes this drawback via distributed represen-",
      "startOffset" : 16,
      "endOffset" : 43
    }, {
      "referenceID" : 9,
      "context" : "tured Vector Space (Erk and Padó, 2008), we embed predicates and arguments into a lowdimensional space in which (representations of) predicate slots are close to (representations of) their plausible arguments, as should be arguments",
      "startOffset" : 19,
      "endOffset" : 39
    }, {
      "referenceID" : 13,
      "context" : "We construct this space via dependency-based word embeddings (Levy and Goldberg, 2014).",
      "startOffset" : 61,
      "endOffset" : 86
    }, {
      "referenceID" : 19,
      "context" : "word2vec (Mikolov et al., 2013) or GloVe (Pennington et al.",
      "startOffset" : 9,
      "endOffset" : 31
    }, {
      "referenceID" : 25,
      "context" : ", 2013) or GloVe (Pennington et al., 2014), consider the following example:",
      "startOffset" : 17,
      "endOffset" : 42
    }, {
      "referenceID" : 26,
      "context" : "Prior work generalizes to unseen predicate-argument pairs via WordNet synsets (Resnik, 1993), a generalization corpus (Erk, 2007), or tensor factorization (Van de Cruys, 2010).",
      "startOffset" : 78,
      "endOffset" : 92
    }, {
      "referenceID" : 8,
      "context" : "Prior work generalizes to unseen predicate-argument pairs via WordNet synsets (Resnik, 1993), a generalization corpus (Erk, 2007), or tensor factorization (Van de Cruys, 2010).",
      "startOffset" : 118,
      "endOffset" : 129
    }, {
      "referenceID" : 5,
      "context" : "Prior work generalizes to unseen predicate-argument pairs via WordNet synsets (Resnik, 1993), a generalization corpus (Erk, 2007), or tensor factorization (Van de Cruys, 2010). Closest to our approach is neural model by Van de Cruys (2014), which, however, has much lower coverage since it is limited to 7k verbs and 30k arguments.",
      "startOffset" : 163,
      "endOffset" : 240
    }, {
      "referenceID" : 28,
      "context" : "ing syntax as a proxy for semantics, we use Enhanced++ dependencies (Schuster and Manning, 2016).",
      "startOffset" : 68,
      "endOffset" : 96
    }, {
      "referenceID" : 24,
      "context" : "(Parker et al., 2011) and entity types in context from Wikilinks (Singh et al.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 29,
      "context" : ", 2011) and entity types in context from Wikilinks (Singh et al., 2012).",
      "startOffset" : 51,
      "endOffset" : 71
    }, {
      "referenceID" : 16,
      "context" : "Term-context pairs are obtained by parsing each corpus with the Stanford CoreNLP dependency parser (Manning et al., 2014).",
      "startOffset" : 99,
      "endOffset" : 121
    }, {
      "referenceID" : 13,
      "context" : "generalized word2vec version by Levy and Goldberg (2014), obtaining distributed representations of selectional preferences.",
      "startOffset" : 32,
      "endOffset" : 57
    }, {
      "referenceID" : 2,
      "context" : "(2016), then query Freebase (Bollacker et al., 2008) for entity types and apply the mapping to fine-grained types by Ling and Weld.",
      "startOffset" : 28,
      "endOffset" : 52
    }, {
      "referenceID" : 4,
      "context" : "We select the ranking model of deep-coref (Clark and Manning, 2016b) as our baseline.",
      "startOffset" : 42,
      "endOffset" : 68
    }, {
      "referenceID" : 30,
      "context" : "The results are reported using MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005), the average F1 score of these three metrics, i.",
      "startOffset" : 35,
      "endOffset" : 56
    }, {
      "referenceID" : 1,
      "context" : ", 1995), B3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005), the average F1 score of these three metrics, i.",
      "startOffset" : 12,
      "endOffset" : 37
    }, {
      "referenceID" : 15,
      "context" : ", 1995), B3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005), the average F1 score of these three metrics, i.",
      "startOffset" : 45,
      "endOffset" : 56
    }, {
      "referenceID" : 21,
      "context" : "CoNLL score, and LEA (Moosavi and Strube, 2016b).",
      "startOffset" : 21,
      "endOffset" : 48
    }, {
      "referenceID" : 3,
      "context" : "Reinforce in Table 1 presents the results of the reward-rescaling model of Clark and Manning (2016a) that are so far the highest reported results on the official test set.",
      "startOffset" : 75,
      "endOffset" : 101
    }, {
      "referenceID" : 3,
      "context" : "of Clark and Manning (2016a) casts the ranking model of Clark and Manning (2016b) in the reinforcement learning framework which considerably increases the training time, from two days to six days in our experiments.",
      "startOffset" : 3,
      "endOffset" : 29
    }, {
      "referenceID" : 3,
      "context" : "of Clark and Manning (2016a) casts the ranking model of Clark and Manning (2016b) in the reinforcement learning framework which considerably increases the training time, from two days to six days in our experiments.",
      "startOffset" : 3,
      "endOffset" : 82
    }, {
      "referenceID" : 17,
      "context" : "We use Martschat and Strube (2014)’s toolkit 4 to perform recall and error analyses.",
      "startOffset" : 7,
      "endOffset" : 35
    } ],
    "year" : 2017,
    "abstractText" : "Selectional preferences have long been claimed to be essential for coreference resolution. However, they are mainly modeled only implicitly by current coreference resolvers. We propose a dependencybased embedding model of selectional preferences which allows fine-grained compatibility judgments with high coverage. We show that the incorporation of our model improves coreference resolution performance on the CoNLL dataset, matching the state-of-the-art results of a more complex system. However, it comes with a cost that makes it debatable how worthwhile such improvements are.",
    "creator" : "LaTeX with hyperref package"
  }
}