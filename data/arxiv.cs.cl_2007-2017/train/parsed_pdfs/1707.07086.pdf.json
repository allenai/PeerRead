{
  "name" : "1707.07086.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Identifying civilians killed by police with distantly supervised entity-event extraction",
    "authors" : [ "Katherine A. Keith", "Abram Handler", "Michael Pinkham", "Cara Magliozzi", "Joshua McDuffie", "Brendan O’Connor" ],
    "emails" : [ "kkeith@cs.umass.edu,", "brenocon@cs.umass.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "We propose a new, socially-impactful task for natural language processing: from a news corpus, extract names of persons who have been killed by police. We present a newly collected police fatality corpus, which we release publicly, and present a model to solve this problem that uses EM-based distant supervision with logistic regression and convolutional neural network classifiers. Our model outperforms two off-the-shelf event extractor systems, and it can suggest candidate victim names in some cases faster than one of the major manually-collected police fatality databases.\nAppendix, software, and data are available online at: http://slanglab.cs.umass. edu/PoliceKillingsExtraction/\n[This paper appears in Proceedings of EMNLP 2017. This version includes the appendix.]"
    }, {
      "heading" : "1 Introduction",
      "text" : "The United States government does not keep systematic records of when police kill civilians, despite a clear need for this information to serve the public interest and support social scientific analysis. Federal records rely on incomplete cooperation from local police departments, and human rights statisticians assess that they fail to document thousands of fatalities (Lum and Ball, 2015).\nNews articles have emerged as a valuable alternative data source. Organizations including The Guardian, The Washington Post, Mapping Police Violence, and Fatal Encounters have started to build such databases of U.S. police killings\nby manually reading millions of news articles1 and extracting victim names and event details. This approach was recently validated by a Bureau of Justice Statistics study (Banks et al., Dec. 2016) which augmented traditional policemaintained records with media reports, finding twice as many deaths compared to past government analyses. This suggests textual news data has enormous, real value, though manual news analysis remains extremely laborious.\nWe propose to help automate this process by extracting the names of persons killed by police from event descriptions in news articles (Table 1). This can be formulated as either of two cross-document entity-event extraction tasks:\n1. Populating an entity-event database: From a corpus of news articles D(test) over timespan T , extract the names of persons killed by police during that same timespan (E(pred)).\n2. Updating an entity-event database: In addition toD(test), assume access to both a historical database of killings E(train) and a historical news corpus D(train) for events that oc-\n1Fatal Encounters director D. Brian Burghart estimates he and colleagues have read 2 million news headlines and ledes to assemble its fatality records that date back to January, 2000 (pers. comm.); we find FE to be the most comprehensive publicly available database.\nar X\niv :1\n70 7.\n07 08\n6v 1\n[ cs\n.C L\n] 2\n2 Ju\nl 2 01\n7\ncurred before T . This setting often occurs in practice, and is the focus of this paper; it allows for the use of distantly supervised learning methods.2\nThe task itself has important social value, but the NLP research community may be interested in a scientific justification as well. We propose that police fatalities are a useful test case for event extraction research. Fatalities are a well defined type of event with clear semantics for coreference, avoiding some of the more complex issues in this area (Hovy et al., 2013). The task also builds on a considerable information extraction literature on knowledge base population (e.g. Craven et al. (1998)). Finally, we posit that the field of natural language processing should, when possible, advance applications of important public interest. Previous work established the value of textual news for this problem, but computational methods could alleviate the scale of manual labor needed to use it.\nTo introduce this problem, we:\n• Define the task of identifying persons killed by police, which is an instance of crossdocument entity-event extraction (§3.1).\n• Present a new dataset of web news articles collected throughout 2016 that describe possible fatal encounters with police officers (§3.2).\n• Introduce, for the database update setting, a distant supervision model (§4) that incorporates feature-based logistic regression and convolutional neural network classifiers under a latent disjunction model.\n• Demonstrate the approach’s potential usefulness for practitioners: it outperforms two off-the-shelf event extractors (§5) and finds 39 persons not included in the Guardian’s “The Counted” database of police fatalities as of January 1, 2017 (§6). This constitutes a promising first step, though performance needs to be improved for real-world usage."
    }, {
      "heading" : "2 Related Work",
      "text" : "This task combines elements of information extraction, including: event extraction (a.k.a. semantic parsing), identifying descriptions of events and\n2Konovalov et al. (2017) studies the database update task where edits to Wikipedia infoboxes constitute events.\ntheir arguments from text, and cross-document relation extraction, predicting semantic relations over entities. A fatality event indicates the killing of a particular person; we wish to specifically identify the names of fatality victims mentioned in text. Thus our task could be viewed as unary relation extraction: for a given person mentioned in a corpus, were they killed by a police officer?\nPrior work in NLP has produced a number of event extraction systems, trained on text data hand-labeled with a pre-specified ontology, including ones that identify instances of killings (Li and Ji, 2014; Das et al., 2014). Unfortunately, they perform poorly on our task (§5), so we develop a new method.\nSince we do not have access to text specifically annotated for police killing events, we instead turn to distant supervision—inducing labels by aligning relation-entity entries from a gold standard database to their mentions in a corpus (Craven and Kumlien, 1999; Mintz et al., 2009; Bunescu and Mooney, 2007; Riedel et al., 2010). Similar to this work, Reschke et al. (2014) apply distant supervision to multi-slot, template-based event extraction for airplane crashes; we focus on a simpler unary extraction setting with joint learning of a probabilistic model. Other related work in the crossdocument setting has examined joint inference for relations, entities, and events (Yao et al., 2010; Lee et al., 2012; Yang et al., 2015).\nFinally, other natural language processing efforts have sought to extract social behavioral event databases from news, such as instances of protests (Hanna, 2017), gun violence (Pavlick et al., 2016), and international relations (Schrodt and Gerner, 1994; Schrodt, 2012; Boschee et al., 2013; O’Connor et al., 2013; Gerrish, 2013). They can also be viewed as event database population tasks, with differing levels of semantic specificity in the definition of “event.”"
    }, {
      "heading" : "3 Task and Data",
      "text" : ""
    }, {
      "heading" : "3.1 Cross-document entity-event extraction for police fatalties",
      "text" : "From a corpus of documents D, the task is to extract a list of candidate person names, E , and for each e ∈ E find\nP (ye = 1 | xM(e)). (1)\nHere y ∈ {0, 1} is the entity-level label where ye = 1 means a person (entity) e was killed by\npolice; xM(e) are the sentences containing mentionsM(e) of that person. A mention i ∈ M(e) is a token span in the corpus. Most entities have multiple mentions; a single sentence can contain multiple mentions of different entities."
    }, {
      "heading" : "3.2 News documents",
      "text" : "We download a collection of web news articles by continually querying Google News3 throughout 2016 with lists of police keywords (i.e police, officer, cop etc.) and fatality-related keywords (i.e. kill, shot, murder etc.). The keyword lists were constructed semi-automatically from cosine similarity lookups from the word2vec pretrained word embeddings4 in order to select a high-recall, broad set of keywords. The search is restricted to what Google News defines as a “regional edition” of “United States (English)” which seems to roughly restrict to U.S. news though we anecdotally observed instances of news about events in the U.K. and other countries. We apply a pipeline of text extraction, cleaning, and sentence de-duplication described in the appendix."
    }, {
      "heading" : "3.3 Entity and mention extraction",
      "text" : "We process all documents with the open source spaCy NLP package5 to segment sentences, and extract entity mentions. Mentions are token spans that (1) were identified as “persons” by spaCy’s named entity recognizer, and (2) have a (firstname, lastname) pair as analyzed by the HAPNIS rule-\n3https://news.google.com/ 4https://code.google.com/archive/p/word2vec/ 5Version 0.101.0, https://spacy.io/\nbased name parser,6 which extracts, for example, (John, Doe) from the string Mr. John A. Doe Jr..7\nTo prepare sentence text for modeling, our preprocessor collapses the candidate mention span to a special TARGET symbol. To prevent overfitting, other person names are mapped to a different PERSON symbol; e.g. “TARGET was killed in an encounter with police officer PERSON.”\nThere were initially 18,966,757 and 6,061,717 extracted mentions for the train and test periods respectively. To improve precision and computational efficiency, we filtered to sentences that contained at least one police keyword and one fatality keyword. This filter reduced positive entity recall a moderate amount (from 0.68 to 0.57), but removed 99% of the mentions, resulting in the |M| counts in Table 2.8\nOther preprocessing steps included heuristics for extraction and name cleanups and are detailed in the appendix."
    }, {
      "heading" : "4 Models",
      "text" : "Our goal is to classify entities as to whether they have been killed by police (§4.1). Since we do not have gold-standard labels to train our model, we turn to distant supervision (Craven and Kumlien, 1999; Mintz et al., 2009), which heuristically aligns facts in a knowledge base to text in a corpus to impute positive mention-level labels for supervised learning. Previous work typically examines distant supervision in the context of binary relation extraction (Bunescu and Mooney, 2007; Riedel et al., 2010; Hoffmann et al., 2011), but we are\n6http://www.umiacs.umd.edu/∼hal/HAPNIS/ 7For both training and testing, we use a name matching assumption that a (firstname, lastname) match indicates coreference between mentions, and between a mention and a fatality database entity. This limitation does affect a small number of instances—the test set database contains the unique names of 453 persons but only 451 unique (firstname, lastname) tuples—but relaxing it raises complex issues for future work, such as how to evaluate whether a system correctly predicted two different fatality victims with the same name.\n8In preliminary experiments, training and testing an ngram classifier (§4.4) on the full mention dataset without keyword filtering resulted in a worse AUPRC than after the filter.\nconcerned with the unary predicate “person was killed by police.” As our gold standard knowledge base (G), we use Fatal Encounters’ (FE) publicly available dataset: around 18,000 entries of victim’s name, age, gender and race as well as location, cause and date of death. (We use a version of the FE database downloaded Feb. 27, 2017.) We compare two different distant supervision training paradigms (Table 3): “hard” label training (§4.2) and “soft” EM-based training (§4.3). This section also details mention-level models (§4.4,§4.5) and evaluation (§4.6)."
    }, {
      "heading" : "4.1 Approach: Latent disjunction model",
      "text" : "Our discriminative model is built on mention-level probabilistic classifiers. Recall a single entity will have one or more mentions (i.e. the same name occurs in multiple sentences in our corpus). For a given mention i in sentence xi, our model predicts whether the person is described as having been killed by police, zi = 1, with a binary logistic model,\nP (zi = 1 | xi) = σ(βTfγ(xi)). (2)\nWe experiment with both logistic regression (§4.4) and convolutional neural networks (§4.5) for this component, which use logistic regression weights β and feature extractor parameters γ. Then we must somehow aggregate mention-level decisions to determine entity labels ye.9 If a human reader were to observe at least one sentence that states a person was killed by police, they would infer that person was killed by police. Therefore we aggregate an entity’s mention-level labels with a deterministic disjunction:\nP (ye = 1 | zM(e)) = 1 { ∨i∈M(e) zi } . (3)\nAt test time, zi is latent. Therefore the correct inference for an entity is to marginalize out the model’s uncertainty over zi:\nP (ye = 1|xM(e)) = 1− P (ye = 0|xM(e)) (4) = 1− P (zM(e) = ~0 | xM(e)) (5)\n= 1− ∏\ni∈M(e)\n(1− P (zi = 1 | xi)). (6)\nEq. 6 is the noisyor formula (Pearl, 1988; Craven and Kumlien, 1999). Procedurally, it counts strong\n9An alternative approach is to aggregate features across mentions into an entity-level feature vector (Mintz et al., 2009; Riedel et al., 2010); but here we opt to directly model at the mention level, which can use contextual information.\nprobabilistic predictions as evidence, but can also incorporate a large number of weaker signals as positive evidence as well.10\nIn order to train these classifiers, we need mention-level labels (zi) which we impute via two different distant supervision labeling methods: “hard” and “soft.”"
    }, {
      "heading" : "4.2 “Hard” distant label training",
      "text" : "In “hard” distant labeling, labels for mentions in the training data are heuristically imputed and directly used for training. We use two labeling rules. First, name-only:\nzi = 1 if ∃e ∈ G(train) : name(i) = name(e). (7)\nThis is the direct unary predicate analogue of Mintz et al. (2009)’s distant supervision assumption, which assumes every mention of a goldpositive entity exhibits a description of a police killing.\nThis assumption is not correct. We manually analyze a sample of positive mentions and find 36 out of 100 name-only sentences did not express a police fatality event—for example, sentences contain commentary, or describe killings not by police. This is similar to the precision for distant supervision of binary relations found by Riedel et al. (2010), who reported 10–38% of sentences did not express the relation in question.\nOur higher precision rule, name-and-location, leverages the fact that the location of the fatality is also in the Fatal Encounters database and requires both to be present:\nzi = 1 if ∃e ∈ G(train) : name(i) = name(e) and location(e) ∈ xi. (8)\nWe use this rule for training since precision is slightly better, although there is still a considerable level of noise."
    }, {
      "heading" : "4.3 “Soft” (EM) joint training",
      "text" : "At training time, the distant supervision assumption used in “hard” label training is flawed: many positively-labeled mentions are in sentences that\n10In early experiments, we experimented with other, more ad-hoc aggregation rules with a “hard”-trained model. The maximum and arithmetic mean functions performed worse than noisyor, giving credence to the disjunction model. The sum rule ( ∑ i P (zi = 1 | xi)) had similar ranking performance as noisyor—perhaps because it too can use weak signals, unlike mean or max—though it does not yield proper probabilities between 0 and 1.\ndo not assert the person was killed by a police officer. Alternatively, at training time we can treat zi as a latent variable and assume, as our model states, that at least one of the mentions asserts the fatality event, but leave uncertainty over which mention (or multiple mentions) conveys this information. This corresponds to multiple instance learning (MIL; Dietterich et al. (1997)) which has been applied to distantly supervised relation extraction by enforcing the at least one constraint at training time (Bunescu and Mooney, 2007; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Ritter et al., 2013). Our approach differs by using exact marginal posterior inference for the Estep.\nWith zi as latent, the model can be trained with the EM algorithm (Dempster et al., 1977). We initialize the model by training on the “hard” distant labels (§4.2), and then learn improved parameters by alternating E- and M-steps.\nThe E-step requires calculating the marginal posterior probability for each zi,\nq(zi) := P (zi | xM(ei), yei). (9)\nThis corresponds to calculating the posterior probability of a disjunct, given knowledge of the output of the disjunction, and prior probabilities of all disjuncts (given by the mention-level classifier).\nSince P (z | x, y) = P (z, y | x)/P (y | x),\nq(zi = 1) = P (zi = 1, yei = 1|xM(ei))\nP (yei = 1|xM(ei)) . (10)\nThe numerator simplifies to the mention prediction P (zi = 1 | xi) and the denominator is the entity-level noisyor probability (Eq. 6). This has the effect of taking the classifier’s predicted probability and increasing it slightly (since Eq. 10’s denominator is no greater than 1); thus the disjunction constraint implies a soft positive labeling. In the case of a negative entity with ye = 0, the disjunction constraint implies all zM(e) stay clamped to 0 as in the “hard” label training method.\nThe q(zi) posterior weights are then used for the M-step’s expected log-likelihood objective:\nmax θ ∑ i ∑ z∈{0,1} q(zi = z) logPθ(zi = z | xi).\n(11) This objective (plus regularization) is maximized with gradient ascent as before.\nThis approach can be applied to any mentionlevel probabilistic model; we explore two in the next sections."
    }, {
      "heading" : "4.4 Feature-based logistic regression",
      "text" : "We construct hand-crafted features for regularized logistic regression (LR) (Table 4), designed to be broadly similar to the n-gram and syntactic dependency features used in previous work on feature-based semantic parsing (e.g. Das et al. (2014); Thomson et al. (2014)). We use randomized feature hashing (Weinberger et al., 2009) to efficiently represent features in 450,000 dimensions, which achieved similar performance as an explicit feature representation. The logistic regression weights (β in Eq. 2) are learned with scikitlearn (Pedregosa et al., 2011).11 For EM (soft-LR) training, the test set’s area under the precision recall curve converges after 96 iterations (Fig. 1).\n11With FeatureHasher, L2 regularization, ‘lbfgs’ solver, and inverse strength C = 0.1, tuned on a development dataset in “hard” training; for EM training the same regularization strength performs best."
    }, {
      "heading" : "4.5 Convolutional neural network",
      "text" : "We also train a convolutional neural network (CNN) classifier, which uses word embeddings and their nonlinear compositions to potentially generalize better than sparse lexical and n-gram features. CNNs have been shown useful for sentence-level classification tasks (Kim, 2014; Zhang and Wallace, 2015), relation classification (Zeng et al., 2014) and, similar to this setting, event detection (Nguyen and Grishman, 2015). We use Kim (2014)’s open-source CNN implementation,12 where a logistic function makes the final mention prediction based on max-pooled values from convolutional layers of three different filter sizes, whose parameters are learned (γ in Eq. 2). We use pretrained word embeddings for initialization,13 and update them during training. We also add two special vectors for the TARGET and PERSON symbols, initialized randomly.14\nFor training, we perform stochastic gradient descent for the negative expected log-likelihood (Eq. 11) by sampling with replacement fifty mentionlabel pairs for each minibatch, choosing each (i, k) ∈M×{0, 1} with probability proportional to q(zi = k). This strategy attains the same expected gradient as the overall objective. We use “epoch” to refer to training on 265,700 examples (approx. twice the number of mentions). Unlike EM for logistic regression, we do not run gradient descent to convergence, instead applying an Estep every two epochs to update q; this approach is related to incremental and online variants of EM (Neal and Hinton, 1998; Liang and Klein, 2009), and is justified since both SGD and E-steps improve the evidence lower bound (ELBO). It is also similar to Salakhutdinov et al. (2003)’s expectation gradient method; their analysis implies the gradient calculated immediately after an Estep is in fact the gradient for the marginal loglikelihood. We are not aware of recent work that uses EM to train latent-variable neural network models, though this combination has been explored (e.g. Jordan and Jacobs (1994))"
    }, {
      "heading" : "4.6 Evaluation",
      "text" : "On documents from the test period (Sept–Dec 2016), our models predict entity-level labels\n12https://github.com/yoonkim/CNN sentence 13From the same word2vec embeddings used in §3. 14Training proceeds with ADADELTA (Zeiler, 2012). We tested several different settings of dropout and L2 regularization hyperparameters on a development set, but found mixed results, so used their default values.\nP (ye = 1 | xM(e)) (Eq. 6), and we wish to evaluate whether retrieved entities are listed in Fatal Encounters as being killed during Sept–Dec 2016. We rank entities by predicted probabilities to construct a precision-recall curve (Fig. 4, Table 5). Area under the precision-recall curve (AUPRC) is calculated with a trapezoidal rule; F1 scores are shown for convenient comparison to non-ranking approaches (§5).\nExcluding historical fatalities: Our model gives strong positive predictions for many people who were killed by police before the test period (i.e. before Sept 2016), when news articles contain discussion of historical police killings. We exclude these entities from evaluation, since we want to simulate an update to a fatality database (Fig 2). Our test dataset contains 1,148 such historical entities.\nData upper bound: Of the 452 gold entities in the FE database at test time, our news corpus only contained 258 (Table 2), hence the data up-\nper bound of 0.57 recall, which also gives an upper bound of 0.57 on AUPRC. This is mostly a limitation of our news corpus; though we collect hundreds of thousands of news articles, it turns out Google News only accesses a subset of relevant web news, as opposed to more comprehensive data sources manually reviewed by Fatal Encounters’ human experts. We still believe our dataset is large enough to be realistic for developing better methods, and expect the same approaches could be applied to a more comprehensive news corpus."
    }, {
      "heading" : "5 Off-the-shelf event extraction baselines",
      "text" : "From a practitioner’s perspective, a natural first approach to this task would be to run the corpus of police fatality documents through pre-trained, “off-the-shelf” event extractor systems that could identify killing events. In modern NLP research, a major paradigm for event extraction is to formulate a hand-crafted ontology of event classes, annotate a small corpus, and craft supervised learn-\ning systems to predict event parses of documents. We evaluate two freely available, off-the-shelf event extractors that were developed under this paradigm: SEMAFOR (Das et al., 2014), and the RPI Joint Information Extraction System (RPIJIE) (Li and Ji, 2014), which output semantic structures following the FrameNet (Fillmore et al., 2003) and ACE (Doddington et al., 2004) event ontologies, respectively.15 Pavlick et al. (2016) use RPI-JIE to identify instances of gun violence.\nFor each mention i ∈ M we use SEMAFOR and RPI-JIE to extract event tuples of the form ti = (event type, agent, patient) from the sentence xi. We want the system to detect (1) killing events, where (2) the killed person is the target mention i, and (3) the person who killed them is a police officer. We implement a small progression of these neo-Davidsonian (Parsons, 1990) conjuncts with rules to classify zi = 1 if:16\n• (R1) the event type is ‘kill.’ • (R2) R1 holds and the patient token span\ncontains ei. 15Many other annotated datasets encode similar event structures in text, but with lighter ontologies where event classes directly correspond with lexical items—including PropBank, Prague Treebank, DELPHI-IN MRS, and Abstract Meaning Representation (Kingsbury and Palmer, 2002; Hajic et al., 2012; Oepen et al., 2014; Banarescu et al., 2013). We assume such systems are too narrow for our purposes, since we need an extraction system to handle different trigger constructions like “killed” versus “shot dead.”\n16For SEMAFOR, we use the FrameNet ‘Killing’ frame with frame elements ‘Victim’ and ‘Killer’. For RPI-JIE, we use the ACE ‘life/die’ event type/subtype with roles ‘victim’ and ‘agent’. SEMAFOR defines a token span for every argument; RPI-JIE/ACE defines two spans, both a head word and entity extent; we use the entity extent. SEMAFOR only predicts spans as event arguments, while RPI-JIE also predicts entities as event arguments, where each entity has a within-text coreference chain over one or more mentions; since we only use single sentences, these chains tend to be small, though they do sometimes resolve pronouns. For determining R2 and R3, we allow a match on any of an entity’s extents from any of its mentions.\n• (R3) R2 holds and the agent token span contains a police keyword.\nAs in §4.1 (Eq. 3), we aggregate mention-level zi predictions to obtain entity-level predictions with a deterministic OR of zM(e).\nRPI-JIE under the full R3 system performs best, though all results are relatively poor (Table 6). Part of this is due to inherent difficulty of the task, though our task-specific model still outperforms (Table 5). We suspect a major issue is that these systems heavily rely on their annotated training sets and may have significant performance loss on new domains, or messy text extracted from web news, suggesting domain transfer for future work."
    }, {
      "heading" : "6 Results and discussion",
      "text" : "Significance testing: We would like to test robustness of performance results to the finite datasets with bootstrap testing (Berg-Kirkpatrick et al., 2012), which can accomodate performence metrics like AUPRC. It is not clear what the appropriate unit of resampling should be—for example, parsing and machine translation research in NLP often resamples sentences, which is inappropriate for our setting. We elect to resample documents in the test set, simulating variability in the generation and retrieval of news articles. Standard errors for one model’s AUPRC and F1 are in the range 0.004–0.008 and 0.008–0.010 respectively; we also note pairwise significance test results. See appendix for details.\nOverall performance: Our results indicate our model is better than existing computational methods methods to extract names of people killed by police, by comparing to F1 scores of off-the-shelf extractors (Table 5 vs. Table 6; differences are statistically significant).\nWe also compare entities extracted from our test dataset to the Guardian’s “The Counted” database of U.S. police killings during the span of the test period (Sept.–Dec., 2016),17 and found 39 persons they did not include in the database, but who were in fact killed by police. This implies our approach could augment journalistic collection efforts. Additionally, our model could help practitioners by presenting them with sentence-level information in the form of Table 7; we hope this could decrease the amount of time and emotional toll required to maintain real-time updates of police fatality databases.\n17https://www.theguardian.com/us-news/series/ counted-us-police-killings, downloaded Jan. 1, 2017.\nCNN: Model predictions were relatively unstable during the training process. Despite the fact that EM’s evidence lower bound objective (H(Q) + EQ[logP (Z, Y |X)]) converged fairly well on the training set, test set AUPRC substantially fluctuated as much as 2% between epochs, and also between three different random initializations for training (Fig. 3). We conducted these multiple runs initially to check for variability, then used them to construct a basic ensemble: we averaged the three models’ mention-level predictions before applying noisyor aggregation. This outperformed the individual models—especially for EM training—and showed less fluctuation in AUPRC, which made it easier to detect convergence. Reported performance numbers in Table 5 are with the average of all three runs from the final epoch of training.\nLR vs. CNN: After feature ablation we found that hard-CNN and hard-LR with n-gram features (N1-N5) had comparable AUPRC values (Table 5). But adding dependency features (D1-D4) caused the logistic regression models to outperform the neural networks (albeit with bare significance: p = 0.046). We hypothesize these dependency features capture longer-distance semantic relationships between the entity, fatality trigger word, and police officer, which short n-grams cannot. Moving to sequence or graph LSTMs may better capture such dependencies.\nSoft (EM) training: Using the EM algorithm gives substantially better performance: for the CNN, AUC improves from 0.130 to 0.164, and for LR, from 0.142 to 0.193. (Both improvements are statistically significant.) Logistic regression with EM training is the most accurate model. Examining the precision-recall curves (Fig. 4), many of the gains are in the higher confidence predictions (left side of figure). In fact, the soft EM model makes fewer strongly positive predictions: for example, hard-LR predicts ye = 1 with more than 99% confidence for 170 out of 24,550 test set entities, but soft-LR does so for only 24. This makes sense given that the hard-LR model at training time assumes that many more positive entity mentions are evidence of a killing than they are in reality (§4.2).\nManual analysis: Manual analysis of false positives indicates misspellings or mismatches of names, police fatalities outside of the U.S., people who were shot by police but not killed, and names of police officers who were killed are com-\nmon false positive errors (see detailed table in the appendix). This suggests many prediction errors are from ambiguous or challenging cases.18\nFuture work: While we have made progress on this application, more work is necessary for accuracy to be high enough to be useful for practitioners. Our model allows for the use of mentionlevel semantic parsing models; systems with explicit trigger/agent/patient representations, more like traditional event extraction systems, may be useful, as would more sophisticated neural network models, or attention models as an alternative to disjunction aggregation (Lin et al., 2016).\nOne goal is to use our model as part of a semi-automatic system, where people manually review a ranked list of entity suggestions. In this case, it is more important to focus on improving recall—specifically, improving precision at highrecall points on the precision-recall curve. Our best models, by contrast, tend to improve precision at lower-recall points on the curve. Higher recall may be possible through cost-sensitive training (e.g. Gimpel and Smith (2010)) and using features from beyond single sentences within the document.\nFurthermore, our dataset could be used to contribute to communication studies, by exploring research questions about the dynamics of media attention (for example, the effect of race and geography on coverage of police killings), and discussions of historical killings in news—for example, many articles in 2016 discussed Michael Brown’s 2014 death in Ferguson, Missouri. Improving NLP analysis of historical events would also be useful for the event extraction task itself, by delineating between recent events that re-\n18We attempted to correct non-U.S. false positive errors by using CLAVIN, an open-source country identifier, but this significantly hurt recall.\nquire a database update, versus historical events that appear as “noise” from the perspective of the database update task. Finally, it may also be possible to adapt our model to extract other types of social behavior events."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work was partially supported by the Amazon Web Services (AWS) Cloud Credits for Research program. Thanks to D. Brian Burghart for advice on police fatalities tracking, and to David Belanger, Trapit Bansal, Patrick Verga, Rajarshi Das, and Taylor Berg-Kirkpatrick for feedback."
    }, {
      "heading" : "A Document retrieval from Google News",
      "text" : "Our news dataset is created using documents gathered via Google News. Specifically, we issued search queries to Google News19 United States (English) regional edition throughout 2016. Our scraper issued queries with terms from two lists: (1) a list of 22 words closely related to police officers and (2) a list of 21 words closely related to killing. These lists were semi-automatically constructed by looking up the nearest neighbors of “police” and “kill” (by cosine distance) from Google’s public release of word2vec vectors pretrained on a very large (proprietary) Google News corpus,20 and then manually excluding a small number of misspelled words or redundant capitalizations (e.g. “Police” and “police”).\nOur list of police words includes: police, officer, officers, cop, cops, detective, sheriff, policeman, policemen, constable, patrolman, sergeant, detectives, patrolmen, policewoman, constables, trooper, troopers, sergeants, lieutenant, deputies, deputy.\nOur list of kill words includes: kill, kills, killing, killings, killed, shot, shots, shoot, shoots,\n19https://news.google.com/ 20https://code.google.com/archive/p/word2vec/\nshooting, murder, murders, murdered, beat, beats, beating, beaten, fatal, homicide, homicides.\nWe construct one word queries using single terms drawn from one of the two lists, as well as two-word queries which consist of one word drawn from each list (e.g. “police shoot” or “cops gunfire”), yielding 505 different queries (22×21 + 22 + 21), each of which was queried approximately once per hour throughout 2016.21 This yielded a list of recent results matching the query; the scraper downloaded documents whose URL it had not seen before, eventually collecting 1,162,300 web pages (approx. 3000 per day)."
    }, {
      "heading" : "B Document preprocessing",
      "text" : "Once documents are downloaded from URLs collected via Google news queries, we apply text extraction with the Lynx browser22 to extract text from HTML. (Newer open-source packages, like Boilerpipe and Newspaper, exist for text extraction, but we observed they often failed on our web data.)\n21We also collected data during part of 2015; the volume of search results varied over time due to changes internal to Google News. After the first few weeks in 2016, the volume was fairly constant.\n22Version 2.8"
    }, {
      "heading" : "C Mention-level preprocessing",
      "text" : "From the corpus of scraped news documents, to create the mention-level dataset (i.e. the set of sentences containing candidate entities) we :\n1. Apply the Lynx text-based web browser to extract all a webpage’s text.\n2. Segment sentences in two steps:\n(a) Segment documents to fragments of text (typically, paragraphs) by splitting on Lynx’s representation of HTML paragraph, list markers, and other dividers: double newlines and the characters -,*, |, + and #.\n(b) Apply spaCy’s sentence segmenter (and NLP pipeline) to these paragraph-like text fragments.\n3. De-duplicate sentences as described in detail below.\n4. Remove sentences that have fewer than 5 tokens or more than 200.\n5. Remove entities (and associated mentions) that\n(a) Contain punctuation (except for periods, hyphens and apostrophes).\n(b) Contain numbers. (c) Are one token in length.\n6. Strip any “’s” occurring at the end of named entity spans.\n7. Strip titles (i.e. Ms., Mr. Sgt., Lt.) occurring in entity spans. (HAPNIS sometimes identifies these types of titles; this step basically augments its rules.)\n8. Filter to mentions that contain at least one police keyword and at least one fatality keyword.\nAdditionally, we remove literal duplicate sentences from our mention-level dataset, eliminating all but one duplicated sentence. We select the earliest sentence by download time of its scraped webpage.\nD Noisyor numerical stability\nUnder “hard” training, many entities at test time have probabilities very close to 1; in some cases, higher than 1 − e−1000. This happens for entities with a very large number of mentions, where the naive implementation of noisyor as p = 1 −\n∏ i(1−pi) has numerical underflow, causing many ties with entities having p = 1. In fact, random tiebreaking for ordering these entity predictions can give moderate variance to the AUPRC. (Part of the issue is that floating point numbers have worse tolerance near 1 than near 0.)\nInstead, we rank entity predictions by the log of the complement probability (i.e. 1000 for p = 1− e−1000):\nlog ( 1− P (ye = 1 | xM(e)) ) = ∑ i logP (zi = 0 | xi)\nThis is more stable, and while there are a small number of ties, the standard deviation of AUPRC across random tie breakings is less than 10−10."
    }, {
      "heading" : "E Manual analysis of results",
      "text" : "Manual analysis is available in Table 8."
    }, {
      "heading" : "F Bootstrap",
      "text" : "We conduct three different methods of bootstrap resampling, varying the objects being sampled:\n1. Entities\n2. Documents\n3. Documents, with deduplication of mentions.23\nWe resample both test-set entities and test-set documents because we are currently unaware of literature that provides reasoning for one over the\n23To implement, we take the 10,000 samples (with replacement) of documents, and reduce them to the unique set of drawn documents. This effectively removes duplicate mentions that occur in method 2 when the same document is drawn more than once in a sample.\nother, and both are arguably relevant in our context. The bootstrap sampling model assumes a given dataset represents a finite sample from a theoretically infinite population, and asks what variability there would be if a finite sample were to be drawn again from the population. This has different interpretations for entity and document resampling. Resampling entities measures robustness due to variability in the names that occur in the documents. Resampling documents measures robustness due to variability in our data source— for example, if our document scraping procedure was altered, or potentially, if the news generation process was changed. Since both entities and documents are not i.i.d., these are both dissatisfying assumptions.\nWe also conduct resampling of documents with deduplication of mentions since, during development, we found our noisy-or metric was sensitive to duplicate mentions; this deduplication step effectively includes running our analysis pipeline’s sentence deduplication for each bootstrap sample.\nIn Fig. 9, we augment the results from Fig. 5 with standard errors calculated from B = 10, 000 bootstrap samples given the three methods for sampling described above. Document resampling tends to give smaller standard errors than entity resampling, which is to be expected since there is a larger number of documents than entities. We analyze our results using the standard errors and significance tests from method 3.\nWe examine the statistical significance of difference between models with a one-sided hypothesis test. Our statistic is\nTij = AUPRCmodel j −AUPRCmodel i.\nWe use hypotheses H0 : T ≤ 0 and H1 : T > 0. As above, we take 10,000 bootstrap samples and\nfind T b statistic of each sample b ∈ {1..10000}. Then we compute p-values\np-valueij = Count(T bij ≤ 0)\n10000 .\nFinally, since in the observed data, one model is better than the other, we are interested the null hypothesis that the apparently-worse model outperforms the apparently-better model. Therefore the final p-value comparing systems i and j is actually calculated as min(pij , pji), since the different directions correspond to the fraction of bootstrap samples with Tij ≤ 0 versus Tij > 0; these values are shown in Fig. 10. (Note pji = 1−pij in expectation.) While this seems to follow standard practice in bootstrap hypothesis testing in NLP (BergKirkpatrick et al., 2012), we note that MacKinnon (2009) argues to instead multiply that by two (i.e., calculate 2min(pij , pji)) to conduct a two-sided test that correctly gives p ∼ Unif(0, 1) when a null hypothesis of equivalent performance is true."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "We propose a new, socially-impactful task for natural language processing: from a news corpus, extract names of persons who have been killed by police. We present a newly collected police fatality corpus, which we release publicly, and present a model to solve this problem that uses EM-based distant supervision with logistic regression and convolutional neural network classifiers. Our model outperforms two off-the-shelf event extractor systems, and it can suggest candidate victim names in some cases faster than one of the major manually-collected police fatality databases. Appendix, software, and data are available online at: http://slanglab.cs.umass. edu/PoliceKillingsExtraction/ [This paper appears in Proceedings of EMNLP 2017. This version includes the appendix.]",
    "creator" : "TeX"
  }
}