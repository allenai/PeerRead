{
  "name" : "1506.02816.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Leveraging Textual Features for Best Answer Prediction in Community-based Question Answering",
    "authors" : [ "George Gkotsis", "Carlos Pedrinaci", "John Domingue", "Maria Liakata" ],
    "emails" : [ "firstname.lastname@open.ac.uk", "m.liakata@warwick.ac.uk" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n50 6.\n02 81\n6v 2\n[ cs\n.C L\n] 1\n7 Ju\nn 20\n15\nLeveraging Textual Features for Best Answer Prediction in Community-based Question Answering\nGeorge Gkotsis, Carlos Pedrinaci, John Domingue\nKnowledge Media Institute The Open University Milton Keynes, UK\nfirstname.lastname@open.ac.uk\nMaria Liakata Dept. of Computer Science\nUniversity of Warwick Coventry, UK\nm.liakata@warwick.ac.uk\nOne of the intriguing problems in Community-based Question Answering (CQA) research is the automatic identification of the best answer, which is expected to benefit various stakeholders. First of all, since several answers are provided for each question, the readers of these websites will be able to process the candidate answers more efficiently and mitigate the “information overload” phenomenon. Secondly, a mechanism that identifies high quality answers will increase awareness within the community and will help to put more effort into questions that remain poorly answered. For instance, in StackOverflow1(SO) alone, as of September 2013, we found that approximately 33% of the questions have yet to be marked as resolved (i.e., out of the 5 million, 1.7 million questions have no answer marked as “accepted”).\nResearchers in related fields have used lexical, syntactic, and discourse features to produce a predictive model of readers’ judgments [3]. In several cases, the use of shallow features, i.e. features that do not employ semantic or syntactic parsing such as sentence length or word length, have been shown to be effective in assessing properties such as ease of reading or usefulness. However, with respect to CQA, research efforts towards the exploitation of shallow features report relatively low results. To improve the efficacy of their models, researchers refer to more contextual information, such as the score of each answer, the comments received or the reputation of the user [1]. However, these features may not be readily available since a) comments and scores introduce an inherent delay, and b) features based on reputation may not be applicable on a newly formed community or pose a threat to its development (i.e. preferential attachment) and result in the reinforcement of the pre-existing community hierarchy.\nIn our approach, we revisit the case of shallow linguistic features and use features found in [3]. Figure 1 shows the average feature values for the accepted answers together with the non-accepted ones of SO using a one-month window time frame2. As seen from the figure, the linguistic features clearly differentiate the accepted from the non-accepted answers. More specifically, accepted answers tend to be longer, use a less common vocabulary, contain longer words, more words per sentence and the longest sentences are lengthier. Even though the above remarks look promising concerning best answer prediction, when training a binary classifier prediction remains weak (58% precision and 0.56 F-Measure\n1http://stackoverflow.com/ 2Similar behaviour is identified for all StackExchange websites and is omitted due to space limitations.\n2009 2010 2011 2012 2013 0\n20000\n40000\n60000\n80000\n100000 Activity\n2009 2010 2011 2012 2013 400\n450\n500\n550\n600\n650\n700 Average Length\n2009 2010 2011 2012 2013 −10.35\n−10.30\n−10.25\n−10.20\n−10.15 Average Log Likehood\n2009 2010 2011 2012 2013\nCreationDate\n3.90\n3.95\n4.00\n4.05\n4.10\n4.15\n4.20\n4.25\n4.30 Average Characters Per Word\n2009 2010 2011 2012 2013\nCreationDate\n30\n35\n40\n45\n50\n55 Average Longest Sentence\nAccepted Answers Non-Accepted Answers\n2009 2010 2011 2012 2013\nCreationDate\n20\n22\n24\n26\n28\n30\n32 Average Words Per Sentence\nFigure 1: Activity and values of the linguistic features (y-axis) for the StackOverflow dataset over time (x-axis). Top left sub-plot shows the number of answers posted every month. The remaining subplots show the average values for the accepted and non-accepted answers.\non average for all StackExchange - SE - websites). A more thorough investigation towards the explanation of this poor performance leads us to identify two main issues. Firstly, as illustrated in Figure 1, the characteristics of language evolve over time. Secondly, while a steady gap between the average values of accepted and non-accepted endures, a large inherent diversity of the posts persists together with a large variance. Finally, a cross-examination of absolute values between different SE websites has shown us that language characteristics differ significantly across SE websites. Since the results that we obtained for a classification based on shallow features are comparable to similar approaches (e.g. [4]) these results will constitute our baseline for evaluating the proposed solution."
    }, {
      "heading" : "1. FEATURE DISCRETISATION",
      "text" : "Our solution called discretisation is presented in detail in [2] and asserts the adoption of a novel way of leveraging shallow features and overcome the above limitations. Intuitively, our approach is to treat the collection of answers for each question as an information unit which can improve the training process. Instead of treating each answer independently of the other answers it is competing with, our approach is to assess the value of the features of each answer in relation to the corresponding features of its competitors. We introduce a new set of features that stem from the linguistic features used so far: instead of dealing with continuous\nvalues, these new features are the result of grouping, sorting, and discretisation.\nWe will present an example for the Length feature. Let us consider the example of Table 1 where for one question there are two candidate answers (i.e., question with Id 5 having answers with Id 6 and 7). We have already shown previously (Figure 1) that the longer an answer is, the more likely it is to be accepted. In order to represent this preference, we group all answers by their corresponding questions (grouping). For each group, we then sort the answers (sorting) and assign a rank for each answer, starting from 1 and incrementing this rank by 1 (discretisation). Sorting is done either in descending or ascending order, so as the lowest rank is assigned to the answers that are marked as accepted (in this example, we use the information that longer answers are more likely to be accepted, hence descending order is conducted). For the example of Table 1, the answer with the longest Length will receive LengthD of value 1 (answer Id 6 with length 250) while the answer that comes second a value of 2 (answer Id 7 with length 200 - note that we are representing the discretised form of each feature as featureD). The result of this process is the introduction of an equal number of linguistic features without the usage of any further information (apart from the association of a question and its corresponding answers3)."
    }, {
      "heading" : "2. EVALUATION",
      "text" : "Table 2 presents the results when using different sets of features and 10-fold validation. The table contains the average values for 21 SE websites (including SO) as the output of different evaluations on 4 million questions and more than 8 million answers. Initially, we use the absolute values of textual features with low results (58% precision, Case 1). The second and third Cases both utilise the discretised features, while the third is additionally using the other set of features (i.e. AnswerCount and CreationDate). Cases 2 and 3 constitute our proposed prediction method. Furthermore Case 4 refers to a “traditional” approach that relies in plain linguistics and user-reputation ratings. We can see that while a whole new set of features is added into the dataset, the performance of classification remains lower than Case 3, which is linguistics-based. Case 5 keeps the user ratings in addition to incorporating all features of Case 3. Hence, classification accuracy is the highest compared to all previous classifications, but almost identical to Case 3 which is strictly based on content and discretisation (higher F-Measure 0.77 vs. 0.76, higher AUC 0.88 vs. 0.87). Finally, Case 6 uses all previous features, including the answer ratings. This set of features uses all features but most importantly user-entered scores and manages to outperform all of the previous cases.\n3Note that other approaches typically omit this information.\nCase 6 shows that the information contained within answer ratings is independent – to a certain extent – of the information found in previous features.\nIn summary, results in Table 2 show that the discretisation of linguistic features manages to outperform significantly the classifier based on linguistic features only. Moreover, we can also see that user rating features such as reputation do not improve our classification, a sign that discretisation is a process that extracts very useful information and delivers very strong results.\nThe whole approach described here has been implemented and is offered for free as web browser plugin and a web service (https://acqua.kmi.open.ac.uk). In the future, we intend to explore the applicability of our methodology elsewhere and investigate further the effect of textual quality on answer selection and impact in online fora and social media."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work was supported by the CARRE (No.611140) and COMPOSE (No.317862) projects funded by the European Commission Framework Programme 7."
    }, {
      "heading" : "3. REFERENCES",
      "text" : "[1] A. Anderson, D. Huttenlocher, J. Kleinberg, and J. Leskovec.\nDiscovering value from community activity on focused question answering sites: a case study of stack overflow. In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 850–858. ACM, 2012.\n[2] G. Gkotsis, K. Stepanyan, C. Pedrinaci, J. Domingue, and M. Liakata. It’s all in the content: State of the art best answer prediction based on discretisation of shallow linguistic features. In Proceedings of the 2014 ACM Conference on Web Science, WebSci ’14, pages 202–210, New York, NY, USA, 2014. ACM. [3] E. Pitler and A. Nenkova. Revisiting readability: A unified framework for predicting text quality. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 186–195. Association for Computational Linguistics, 2008. [4] Q. Tian, P. Zhang, and B. Li. Towards predicting the best answers in community-based question-answering services. In Seventh International AAAI Conference on Weblogs and Social Media, 2013."
    } ],
    "references" : [ {
      "title" : "Discovering value from community activity on focused question answering sites: a case study of stack overflow",
      "author" : [ "A. Anderson", "D. Huttenlocher", "J. Kleinberg", "J. Leskovec" ],
      "venue" : "In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2012
    }, {
      "title" : "It’s all in the content: State of the art best answer prediction based on discretisation of shallow linguistic features",
      "author" : [ "G. Gkotsis", "K. Stepanyan", "C. Pedrinaci", "J. Domingue", "M. Liakata" ],
      "venue" : "In Proceedings of the 2014 ACM Conference on Web Science,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2014
    }, {
      "title" : "Revisiting readability: A unified framework for predicting text quality",
      "author" : [ "E. Pitler", "A. Nenkova" ],
      "venue" : "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2008
    }, {
      "title" : "Towards predicting the best answers in community-based question-answering services",
      "author" : [ "Q. Tian", "P. Zhang", "B. Li" ],
      "venue" : "In Seventh International AAAI Conference on Weblogs and Social Media,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "Researchers in related fields have used lexical, syntactic, and discourse features to produce a predictive model of readers’ judgments [3].",
      "startOffset" : 135,
      "endOffset" : 138
    }, {
      "referenceID" : 0,
      "context" : "or the reputation of the user [1].",
      "startOffset" : 30,
      "endOffset" : 33
    }, {
      "referenceID" : 2,
      "context" : "In our approach, we revisit the case of shallow linguistic features and use features found in [3].",
      "startOffset" : 94,
      "endOffset" : 97
    }, {
      "referenceID" : 3,
      "context" : "[4]) these results will constitute our baseline for evaluating the proposed solution.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 1,
      "context" : "FEATURE DISCRETISATION Our solution called discretisation is presented in detail in [2] and asserts the adoption of a novel way of leveraging shallow features and overcome the above limitations.",
      "startOffset" : 84,
      "endOffset" : 87
    } ],
    "year" : 2015,
    "abstractText" : "One of the intriguing problems in Community-based Question Answering (CQA) research is the automatic identification of the best answer, which is expected to benefit various stakeholders. First of all, since several answers are provided for each question, the readers of these websites will be able to process the candidate answers more efficiently and mitigate the “information overload” phenomenon. Secondly, a mechanism that identifies high quality answers will increase awareness within the community and will help to put more effort into questions that remain poorly answered. For instance, in StackOverflow(SO) alone, as of September 2013, we found that approximately 33% of the questions have yet to be marked as resolved (i.e., out of the 5 million, 1.7 million questions have no answer marked as “accepted”). Researchers in related fields have used lexical, syntactic, and discourse features to produce a predictive model of readers’ judgments [3]. In several cases, the use of shallow features, i.e. features that do not employ semantic or syntactic parsing such as sentence length or word length, have been shown to be effective in assessing properties such as ease of reading or usefulness. However, with respect to CQA, research efforts towards the exploitation of shallow features report relatively low results. To improve the efficacy of their models, researchers refer to more contextual information, such as the score of each answer, the comments received or the reputation of the user [1]. However, these features may not be readily available since a) comments and scores introduce an inherent delay, and b) features based on reputation may not be applicable on a newly formed community or pose a threat to its development (i.e. preferential attachment) and result in the reinforcement of the pre-existing community hierarchy. In our approach, we revisit the case of shallow linguistic features and use features found in [3]. Figure 1 shows the average feature values for the accepted answers together with the non-accepted ones of SO using a one-month window time frame. As seen from the figure, the linguistic features clearly differentiate the accepted from the non-accepted answers. More specifically, accepted answers tend to be longer, use a less common vocabulary, contain longer words, more words per sentence and the longest sentences are lengthier. Even though the above remarks look promising concerning best answer prediction, when training a binary classifier prediction remains weak (58% precision and 0.56 F-Measure",
    "creator" : "LaTeX with hyperref package"
  }
}