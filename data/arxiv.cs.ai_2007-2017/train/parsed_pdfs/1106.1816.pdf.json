{
  "name" : "1106.1816.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Gal A. Kaminka", "galk s.biu.a", "David V. Pynadath", "Milind Tambe" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "Journal of Arti ial Intelligen e Resear h 17 (2002) 83 135 Submitted 10/01; published 08/02",
      "text" : "Monitoring Teams by Overhearing:A Multi-Agent Plan-Re ognition Approa hGal A. Kaminka galk s.biu.a .ilComputer S ien e DepartmentBar Ilan UniversityRamat Gan 52900, IsraelDavid V. Pynadath pynadath isi.eduMilind Tambe tambe us .eduComputer S ien e Department and Information S ien es InstituteUniversity of Southern California4676 Admiralty WayLos Angeles, CA 90292, USA Abstra tRe ent years are seeing an in reasing need for on-line monitoring of teams of oop-erating agents, e.g., for visualization, or performan e tra king. However, in monitoringdeployed teams, we often annot rely on the agents to always ommuni ate their stateto the monitoring system. This paper presents a non-intrusive approa h to monitoring byoverhearing, where the monitored team's state is inferred (via plan-re ognition) from team-members' routine ommuni ations, ex hanged as part of their oordinated task exe ution,and observed (overheard) by the monitoring system. Key hallenges in this approa h in- lude the demanding run-time requirements of monitoring, the s ar eness of observations(in reasing monitoring un ertainty), and the need to s ale-up monitoring to address poten-tially large teams. To address these, we present a set of omplementary novel te hniques,exploiting knowledge of the so ial stru tures and pro edures in the monitored team: (i)an e ient probabilisti plan-re ognition algorithm, well-suited for pro essing ommuni- ations as observations; (ii) an approa h to exploiting knowledge of the team's so ial be-havior to predi t future observations during exe ution (redu ing monitoring un ertainty);and (iii) monitoring algorithms that trade expressivity for s alability, representing only ertain useful monitoring hypotheses, but allowing for any number of agents and theirdi erent a tivities to be represented in a single oherent entity. We present an empiri alevaluation of these te hniques, in ombination and apart, in monitoring a deployed teamof agents, running on ma hines physi ally distributed a ross the ountry, and engaged in omplex, dynami task exe ution. We also ompare the performan e of these te hniquesto human expert and novi e monitors, and show that the te hniques presented are apableof monitoring at human-expert levels, despite the di ulty of the task.1. Introdu tionRe ent years have seen tremendous growth of appli ations involving distributed multi-agentteams, formed of agents that ollaborate on a spe i joint task (e.g., Jennings, 1995; Pe- hou ek, Marik, & Stepankova, 2000, 2001; Kumar & Cohen, 2000; Kumar, Cohen, &Levesque, 2000; Horling, Benyo, & Lesser, 2001; Lenser, Bru e, & Veloso, 2001; Barber &Martin, 2001). This growth has led to in reasing need for monitoring te hniques that allow a 2002 AI A ess Foundation and Morgan Kaufmann Publishers. All rights reserved.\nKaminka, Pynadath, & Tambesyntheti agent or human operator to monitor and identify the state of the distributed team.Previous work has dis ussed the riti al role of monitoring in visualization (e.g., Ndumu,Nwana, Lee, & Collis, 1999), in identifying failures in exe ution (e.g., Horling et al., 2001),in providing advi e to improve performan e (e.g., Aiello, Busetta, Dona, & Sera ni, 2001),and in fa ilitating ollaboration between the monitoring agent and the members of the team(e.g., Grosz & Kraus, 1996).This paper fo uses on monitoring ooperative agent teams by overhearing their inter-nal ommuni ations. This allows a human operator or a syntheti agent to monitor the oordinated exe ution of a task, by listening to the messages team-members ex hange withea h other. It ontrasts with previous te hniques that are impra ti al in settings wheredire t observations of the team members are unavailable (e.g., when team-members arephysi ally distributed away from the observer), or in large-s ale appli ations omposed ofalready-deployed agents that are dynami ally integrated to jointly exe ute a task.For example, one ommon te hnique, report-based monitoring, requires ea h monitoredteam-member to ommuni ate its state to the monitoring agent at regular intervals, or atleast whenever the team-member hanges its state. Su h reporting provides the monitoringagent with a urate information on the state of the team. Unfortunately, report-based mon-itoring su ers from several di ulties in monitoring large deployed teams of interest in thereal-world (see Se tion 2 for a detailed dis ussion): First, it requires intrusive modi ationsto the behavior of agents, su h that they report their state as needed by the di erent moni-toring appli ations. However, sin e agents are already deployed, su h repeated modi ationsto the behavior of the agents are di ult to implement and omplex to manage. In par-ti ular, lega y and proprietary systems are notoriously expensive to modify (for instan e, onsider the notorious modi ations to address the Year 2000 bug, also known as Y2K).Se ond, the bandwidth requirements of report-based monitoring (whi h relies on ommuni- ation hannels) an be unrealisti (Jennings, 1993, 1995; Grosz & Kraus, 1996; Pe hou eket al., 2000, 2001; Ver outer, Beaune, & Sayettat, 2000). In addition, network delays andunreliable or lossy ommuni ation hannels are a key on ern with report-based monitoringapproa hes.We therefore advo ate an alternative monitoring approa h, based on multi-agent keyholeplan-re ognition (Tambe, 1996; Huber & Hadley, 1997; Devaney & Ram, 1998; Intille &Bobi k, 1999; Kaminka & Tambe, 2000). In this approa h, the monitoring system infersthe unobservable state of the agents based on their observable a tions, using knowledge ofthe plans that give rise to the a tions. This approa h is non-intrusive, requiring no hangesto agents' behaviors; and it allows for hanges in the requested monitoring information.It assumes a ess to knowledge of plans that may explain observable a tion however thisknowledge is readily available to the monitoring system as we assume it is deployed in a ollaborative environment. Indeed, in some ases, the monitoring system may be deployedby the human operator of the team. An additional bene t of a plan-re ognition approa his that it an rely on inferen e to ompensate for o asional ommuni ation losses, and antherefore be robust to ommuni ation failures.In general, the only observable a tions of agents in a distributed team are their routine ommuni ations, whi h the agents ex hange as part of task exe ution (Ndumu et al., 1999).Fortunately, the growing popularity of agent integration tools (Tambe, Pynadath, Chauvat,Das, & Kaminka, 2000; Martin, Cheyer, & Moran, 1999) and agent ommuni ations (Finin,84\nMonitoring Teams by OverhearingLabrou, & May eld, 1997; Reed, 1998) in reases standardization of aspe ts of agent ommu-ni ations, and provides in reasing opportunities for observing and interpreting inter-agent ommuni ations. We assume that monitored agents are truthful in their messages, sin ethey are ommuni ating to their teammates; and that they are not attempting to de eivethe monitoring agent or prevent it from overhearing (as it is deployed by the human oper-ator of the team). Given a (possibly sto hasti ) model of the plans that the agents maybe exe uting, a monitoring system using plan-re ognition an infer the urrent state of theagents from su h observed routine messages.However, the appli ation of plan-re ognition te hniques for overhearing poses signi ant hallenges. First, a key hara teristi of the overhearing task is the s ar ity of observations.Explanations for overheard messages (i.e., the observed a tions) an sometimes be fairly easyto disambiguate, but un ertainty arises be ause there are relatively few of them to observe:team members annot and do not in pra ti e ontinuously ommuni ate among themselvesabout their state (Jennings, 1995; Grosz & Kraus, 1996). Thus team-members hange theirstate while keeping quiet. Another key hara teristi of overhearing is that the observablea tions are inherently multi-agent a tions: When agents ommuni ate, it is only a singleagent that sends the messages. The others impli itly a t their role in the ommuni ations bylistening. Yet despite the s ar ity of observable ommuni ations, and the multi-agent natureof the observed a tions, a monitoring system must infer the state of all agents in the team,at all times. Previous investigations of multi-agent plan-re ognition (Tambe, 1996; Devaney& Ram, 1998; Intille & Bobi k, 1999; Kaminka & Tambe, 2000) have typi ally made theassumption that all hanges to the state of agents have an observable e e t: Un ertaintyresulted from ambiguity in the explanations for the observed a tions. Furthermore, theseinvestigations have addressed settings where observable a tions were individual (ea h a tionis arried out by a single agent).In addition to these hallenges that are unique to overhearing, a monitoring system mustaddress additional hallenges stemming from the use of monitoring in servi e of visualiza-tion. The representation and algorithms must support soft real-time response; reasoningmust be done qui kly to be useful for visualization. Furthermore, real-world appli ationsdemand te hniques that an s ale up as the number of agents in reases, for monitoring largeteams. However, many urrent representations for plan-re ognition are omputationally in-tense (e.g., Kjærul , 1992), or only address single-agent re ognition tasks (e.g., Pynadath& Wellman, 2000). Multi-agent plan-re ognition investigations have typi ally not expli itlyaddressed s alability on erns (Devaney & Ram, 1998; Intille & Bobi k, 1999).This paper presents Overseer, an implemented monitoring system apable of moni-toring large distributed appli ations omposed of previously-deployed agents. Overseerbuilds on previous work in multi-agent plan-re ognition (Tambe, 1996; Intille & Bobi k,1999; Kaminka & Tambe, 2000) by utilizing knowledge of the relationships between agentsto understand how their de isions intera t. However, as previous te hniques proved insu - ient, Overseer in ludes a number of novel multi-agent plan-re ognition te hniques thataddress the s ar ity of observations, as well as the severe response-time and s ale-up re-quirements imposed by realisti appli ations. Key ontributions in lude: (i) a linear timeprobabilisti plan-re ognition representation and asso iated algorithms, whi h exploit thenature of observed ommuni ations for e ien y; (ii) a method for addressing unavailableobservations by exploiting knowledge of the so ial pro edures of teams to e e tively predi t85\nKaminka, Pynadath, & Tambe(and hen e e e tively monitor) future observations during normal and failed exe ution, thusallowing inferen e from la k of su h observations; and (iii) YOYO*, an algorithm that usesknowledge of the team organizational stru ture (team-hierar hy) to model the agent team(with all the di erent parallel a tivities taken by individual agents) using a single stru ture,instead of modeling ea h agent individually. YOYO* sa ri es some expressivity (the abilityto a urately monitor the team in ertain oordination failure states) for signi ant gains ine ien y and s alability.We present a rigorous evaluation of Overseer's di erent monitoring te hniques in oneof its appli ation domains and show that the te hniques presented result in signi ant booststo Overseer's monitoring a ura y and e ien y, beyond te hniques explored in previouswork. We evaluate Overseer's apability to address lossy observations, a key on ern withreport-based monitoring. Furthermore, we evaluate Overseer's performan e in omparisonwith human expert and novi e monitors, and show that Overseer's performan e is ompa-rable to that of human experts, despite the di ulty of the task, and Overseer's relian eon omputationally-simple te hniques. One of the key lessons that we draw in Overseeris that a ombination of omputationally- heap multi-agent plan-re ognition te hniques, ex-ploiting knowledge of the expe ted stru tures and intera tions among team-members, anbe ompetitive with approa hes whi h fo us on a urate modeling of individual agents (andmay be omputationally expensive).This paper is organized as follows. Se tion 2 presents the motivation for the designof Overseer, using examples from an a tual distributed appli ation in whi h Overseerwas applied. Se tion 3 presents a novel single-agent plan-re ognition representation andasso iated algorithms, parti ularly suited to monitoring an agent based on its observed ommuni ations. Se tion 4 explores several methods Overseer uses to address un ertaintyin using this representation for monitoring a team of agents. Se tion 5 presents YOYO*,whi h allows e ient reasoning using the methods previously dis ussed. Se tion 6 presentsan evaluation of the di erent te hniques in orporated in YOYO*. Se tion 7 ontrasts thete hniques presented with previous related investigations, and nally, Se tion 8 on ludesand presents our plans for future work. In addition, several appendi es present all pseudo- ode for algorithms dis ussed in the text, and portions of the data used in our experiments,for those readers who may wish to repli ate the experiments.2. Motivation and Illustrative ExamplesSeveral onsiderations, based on our experien e with a tual distributed appli ations, havedire ted us towards the plan-re ognition approa h we advo ate in this paper. We presentthese onsiderations in the ontext of an illustrative omplex distributed appli ation, whi hwe also use for evaluating Overseer in Se tion 6. In this appli ation, a distributed team of11 to 20 agents exe utes a simulation of an eva uation of ivilians from a threatened lo ation.The integrated system allows a human ommander to intera tively provide lo ations of thestranded ivilians, safe areas for eva uation and other key points. Simulated heli opters then y a oordinated mission to eva uate the ivilians, relying on various information agents todynami ally obtain information about enemy threats, (re)plan routes to avoid threats andobsta les, et . The distributed team is omposed of diverse agents from four di erent re-sear h groups: A Qui kset multi-modal ommand input agent (Cohen, Johnston, M Gee,86\nMonitoring Teams by OverhearingOviatt, Pittman, Smith, Chen, & Clow, 1997), a Retsina route planner (Payne, Sy ara,Lewis, Lenox, & Hahn, 2000), the Ariadne information agent (Knoblo k, Minton, Am-bite, Ashish, Modi, Muslea, Philpot, & Tejada, 1998) and eight syntheti heli opter pilots(Tambe, Johnson, Jones, Koss, Laird, Rosenbloom, & S hwamb, 1995).The agents were not designed to work together on this task they were already builtand deployed prior to the reation of the team. The team is integrated using Team ore(Tambe et al., 2000), whi h a omplishes integration by wrapping ea h agent with aproxy that maintains ollaboration with other agents (via their own proxies). The proxiesand agents form a team, jointly exe uting a distributed appli ation des ribed by a team-oriented program. Su h a program onsists of: A team hierar hy, where a team de omposes into subteams, and sub-subteams. A plan hierar hy, whi h ontains team plans that de ompose into subteam plans Assignment of teams from the team hierar hy to plans in the plan hierar hy.As an example, Figure 1-a shows a part of the team/subteam hierar hy used in theeva uation-domain (des ribed below). Here, for instan e, TRANSPORT is a subteam ofFLIGHT-TEAM, itself a subteam of TASK-FORCE. Figure 1-b shows an abbreviated plan-hierar hy for the same domain. High-level team plans, su h as Eva uate, typi ally de- ompose into other team plans, su h as Pro ess-Orders, and, ultimately, into leaf-levelplans that are exe uted by individuals. Temporal transitions are used to onstrain the or-der of exe ution of plans. There are teams assigned to exe ute the plans, e.g., the TASKFORCE team jointly exe utes Eva uate, while only the TRANSPORT subteam exe utesthe Transport-Operations (Transport-Ops) step. The team-oriented program forthis appli ation onsists of about 40 team-plans. Some plans may get exe uted repeatedlythough, so ea h agent may exe ute up to hundreds of plan steps as part of the exe ution ofa single team-oriented program.To exe ute the team-oriented program, ea h proxy uses a domain-independent teamworkmodel, alled STEAM (Tambe, 1997). The teamwork model automati ally generates the ommuni ation messages required to ensure appropriate oordination among the proxies.For instan e, STEAM requires that if an agent privately obtains a belief bel that terminatesa team plan, then that agent should send a message to the rest of the team to terminate thatteam plan, along with the private belief bel that led to that termination. To avoid jammingthe ommuni ation hannels with a ood of messages about every single plan, STEAM hooses to ommuni ate sele tively. Thus, whereas ommuni ating about the initiation andtermination of ea h and every plan would have led to 2000 or more messages generated inone run, only about 100 messages get ex hanged in any one run when using STEAM (Tambeet al., 2000).Figure 2 displays some of the messages ex hanged among team members in the eva -uation appli ation, through the use of STEAM. The rst message is sent from a proxy alled teamqui kset to members of a team TEAM-EVAC (another name for TASKFORCE). The ontent of this message indi ates that the team should terminate a plan alled determine-number-of-helos. The se ond message is sent from a proxy alled87\nKaminka, Pynadath, & Tambe TASK FORCE\nFLIGHT TEAM\nTRANSPORTESCORT\nROUTE PLANNER\nESCORT FOLLOW\nTRANSPORT DIVISION 1\n...\n.....\nESCORT LEAD\n(a) (b)\nEVACUATE\n.....\n[TASK FORCE]\nEXECUTE MISSION\n[TASK FORCE]\nPROCESS [TASK FORCE]\nMANEUVERS ZONE LANDING\n....\nFLY-FLIGHT PLAN\n[FLIGHT TEAM]\nFLY-CONTROL ROUTE....\nORDERS\n[FLIGHT TEAM]\n[FLIGHT TEAM]\nTRANSPORT OPERATIONS ESCORT [ESCORT] [TRANSPORT] OPERATIONS\nORDERS GET\n[GET ORDERS]\nGET ORDERS ROLE\nFigure 1: Portions of the team-hierar hy (a) and plan-hierar hy (b) used in our domain.Dotted lines show temporal transitions. team_auto2 to members of a subteam TEAM-ESCORT-FOLLOW (a subteam of ES-CORTS). The ontent of this message indi ates that the subteam should establish ommit-ment to a plan named prepare-to-exe ute-mission. The online appendix presents samplelogs of the overheard messages from omplete runs, as well as the plan and team hierar hiesfor the eva uation appli ation.As dis ussed in Se tion 1, the apability for automati ally monitoring the progress ofthe team is riti al. This need for team monitoring is further ampli ed in distributedsettings, sin e a human operator in one pla e annot dire tly observe the agents exe utingin a remote lo ation. For instan e, in trial runs of the eva uation simulation appli ationdes ribed above, monitoring sometimes required a series of franti phone alls among humanoperators in di erent states, trying to verify the su essful exe ution of the system as it wasoperating. And even when this agent team was o-lo ated on multiple omputers in oneroom, the diversity of agents made it extremely di ult for an observer to automati allymonitor the state of the team just from observing the di erent agent output s reens.Overseer was built to provide su h monitoring by tra king the routine ommuni a-tions among the agents (Figure 2). Using plan-re ognition, it allows humans and agentsto query about the present and future likely plans of the entire team, its subteams andindividuals to monitor progress, ompute likelihoods of failure, et . However, given thatthe agent team ommuni ates sele tively about the plans being exe uted, Overseer's plan-re ognition fa es signi ant un ertainty. Furthermore, Overseer must be able to answerqueries on-line, and must therefore work e iently. As dis ussed later, addressing these hallenges has required several novel team-based plan-re ognition te hniques to be devel-oped.Several onsiderations have led us away from report-based monitoring for this and otherTeam ore appli ations. First, report-based monitoring requires that agents' ode be mod-i ed to ommuni ate the reports needed for monitoring; as monitoring requirements hange88\nMonitoring Teams by Overhearing\nLog Message Re eived; Fri Sep 17 18:27:54 1999:Logging Agent: teamqui ksetMessage==> tell: ontent teamqui kset terminate-jpg onstant determine-number-of-helosnumber-of-helos-determined *yes* 4 4 98 kqml_string:re eiver TEAM-EVAC 9 kqml_word:reply-with nil 3 kqml_word:team TEAM-EVAC 9 kqml_word:sender teamqui kset 12 kqml_word:kqml-msg-id 21547+tsevet.isi.edu+7 22 kqml_wordLog Message Re eived; Fri Sep 17 18:30:35 1999:Logging Agent: TEAM_auto2Message==> tell: ontent TEAM_auto2 establish- ommitment prepare-to-exe ute-mission58 kqml_string:re eiver TEAM-ESCORT-FOLLOW 18 kqml_word:reply-with nil 3 kqml_word:team TEAM-ESCORT-FOLLOW 18 kqml_word:sender TEAM_auto2 10 kqml_word:kqml-msg-id 20752+dui.isi.edu+16 20 kqml_wordFigure 2: Example KQML messages used as observations by Overseer.\n89\nKaminka, Pynadath, & Tambefrom one appli ation to the next, so does the information needed about ea h agent. Un-fortunately, the agents and their proxies are already deployed in several government lab-oratories and universities. Modifying the agents at ea h deployed lo ation is problemati and intrusive modi ations interfere with arefully designed timing spe i ations of giventasks, requiring further modi ations by other agent developers. The distributed nature ofTeam ore implies that there is no entralized server whi h ontrols the behavior of theagents, but instead hanges are required in the di erent proxy types. Indeed, in general,modifying lega y and proprietary appli ations (in luding the integration ar hite ture) is of ourse known to be a di ult pro ess, and so a solution that requires onstant modi ationsto the agents and ar hite ture will not s ale up.A se ond important onsideration was the omputational and bandwidth requirements ofreport-based monitoring. As has been repeatedly noted in the literature, one annot expe tagents to be able to ommuni ate ontinuously and fully monitor all other agents (e.g.,Jennings, 1993, 1995; Grosz & Kraus, 1996; Pe hou ek et al., 2001; Ver outer et al., 2000).In a team of 11 (used as an example in this paper), regularly s heduled state reports from theagents at the required temporal resolution would require approximately 50,000 messages tobe sent during a 15-minute run, with the number nearly doubling when we rea h 20 agents.If we instead have the 11 agents only report on state hanges, announ ing plan initiationand termination, approximately 2,000 messages have to be sent. However, this is still anorder-of-magnitude more than the normal 100 messages or so that are ex hanged by the11 agents as part of routine exe ution. Even if the network ould support the bandwidthne essary for report-based monitoring, there is also a signi ant omputational burden onthe monitoring system to pro ess all the in oming reports.On the other hand, a plan-re ognition approa h seemed like a natural t for the task.First, it doesn't require any hanges in the behavior of the monitored agents, and is thusvery suitable for monitoring agents that are already deployed. Se ond, it doesn't add any omputational burdens to the monitored agents or the network, sin e it uses only whatobservations are already available. Third, the main knowledge sour e plan-re ognition sys-tems typi ally rely on a plan library is in fa t easily available in a essible form to themonitoring system from the team-oriented program whi h is used to integrate the agents,sin e the operator deploying the monitoring system is assumed to be the one to des ribethe integration team-oriented program in the rst pla e. Thus plan-re ognition's sometimes riti ized assumption of a orre t plan-library is in fa t satis ed fully in this monitoringappli ation.Note that this assumption holds even if agents are not all using the same integra-tion ar hite ture: The only knowledge we rely on is a (possibly sto hasti ) model of how omponents of exe ution t together, and the ommuni ations that are used to integratethem. Therefore, while this paper fo uses on team-oriented programs (des ribed above), thete hniques introdu ed appear generalizable to other types of representation languages fordistributed systems, su h as TÆMS (De ker, 1995), team-oriented programming (Tidhar,1993a) and others. Furthermore, the plan-library need not ontain implemetation details only the names of the key steps. Thus even agents utilizing radi ally-di erent representationsthan a plan-hierar hy an be monitored, as long as they have exe ution states orrespondingto the team-oriented program (whi h they have to have in any ase in order to oordinatewith other team-members). 90\nMonitoring Teams by OverhearingMonitoring by overhearing poses unique hallenges as previously dis ussed. However, italso o ers unique opportunities for plan re ognition. We had earlier stated our assumptionthat agents are truthful in their ommuni ations, and do not seek to de eive their teammatesor the monitoring system, nor prevent overhearing in any way (e.g., en ryption). Thisassumption is justi ed as the monitoring system is deployed by the operator of the monitoredagents, or by an agent team-member. Failures of the team to oordinate (e.g., due to lo kasyn hrony or unintentional erroneous messages) will therefore ause orresponding failuresin monitoring. However, we do not make additional assumptions about the messages beyondthose that are made by the monitored agents themselves.This assumption allows a plan-re ognition system to treat observations with ertainty:When a message is overheard terminating plan X, the monitoring system an infer with ertainty that indeed the plan X is no longer exe uted. However, this does not eliminateplan re ognition ambiguity. First, multiple instantiations of plan X may exist, and themessage does not spe ify whi h one was terminated. Se ond, upon termination of the plan,the monitored team-member must often hoose between multiple alternative plan steps tofollow X, and yet this hoi e is not evident in the observations. Indeed, the di ultyof monitoring by overhearing is demonstrated by human monitoring performan e: Novi ehuman monitors have managed to only a hieve approximately 60% a ura y on average.3. Monitoring a Team of Agents as Separate IndividualsIn this se tion, we present a representation and asso iated baseline algorithms to supportoverhearing based on the plan-hierar hy and team-hierar hy. We begin by making an as-sumption of agent independen e, where observations and beliefs about one agent's state ofexe ution have no bearing on our beliefs about another agent's state. This assumption anbe ontrasted with another: If we assume instead that team-members are su essful in their oordination, then knowing that one agent has begun exe uting a joint plan would natu-rally in rease the likelihood that its teammates have begun as well, as agents would not be onsidered independent. In fa t, su essful teamwork requires interdependen y among theagents (Grosz, 1996).However, an initial assumption of agent independen e provides a baseline of omparison,as it more losely follows urrent approa hes to multi-agent plan re ognition, whi h oftenassume that observations about ea h individual agent are ontinuously available. Laterse tions (Se tions 4 and 5) will highlight the unique hallenges ta kled in monitoring byoverhearing, and will take agent interdependen ies into a ount.We thus begin by maintaining a separate plan re ognizer for ea h agent. Ea h re og-nizer observes only those messages that its respe tive agent sends. On the basis of theseobservations, the re ognizer maintains a probabilisti estimate of the state of exe ution ofthe various plans the agent may be urrently exe uting. Knowledge of the plans assigned toagents and their team memberships is available in our appli ation from the plan-hierar hyand team-hierar hy of the team-oriented program used in onstru ting the monitored appli- ation.Se tion 3.1 presents the language we use for the probabilisti representation of a team-oriented program. We exploit various independen e properties within team-oriented pro-grams to a hieve a ompa t representation of the possible plan states of the agents. Se -91\nKaminka, Pynadath, & Tambetion 3.2 presents an algorithm for updating the re ognizer's beliefs about the agents' planstates upon the observation of a message. This algorithm performs the update with ane ien y gained by exploiting the parti ular semanti s of ommuni ated messages, namelythat ea h su h message is an observation that indi ates the initiation/termination of a par-ti ular plan with ertainty. Se tion 3.3 presents an algorithm for updating the re ognizer'sbeliefs about the agents' plan states when no message has been observed. In the absen eof any su h eviden e, this algorithm e iently updates the re ognizer's beliefs by using atemporal model of the agents' plan exe ution that makes a strong Markovian assumption.Finally, Se tion 3.4 presents the overall re ognition pro edure, as well as an illustration and omplexity analysis of that pro edure.3.1 Plan-State RepresentationWe address un ertainty in monitoring through a probabilisti model that supports quanti-tative evaluation of the re ognized plan hypotheses. Sin e we are monitoring these agentsthrough the duration of their exe ution, we use a time series of plan-state variables. Atea h point in time, the agent's plan state is the state of the team-oriented program that itis urrently exe uting, i.e., a path from root to leaf in the team-oriented program tree. Werepresent the plans in the program by a set of boolean random variables, fXtg, where ea hvariable Xt is true if and only if the agent is a tively exe uting plan X at time t. We thenrepresent our beliefs about the agent's a tual state at time t as a probability distribution overall variables fXtg. The distribution takes into a ount dependen ies among the di erentplans in the team-oriented program (e.g., parent- hild relationships), as well as the tempo-ral dependen ies between the plan state at times t and t + 1. To simplify the dependen ystru ture, it is useful to introdu e additional boolean random variables, done(X; t), that aretrue if and only if plan X was exe uted at time t 1 and its exe ution has terminated attime t.There are a number of possible representations for apturing the distribution and per-forming inferen e over these variables. However, the generality of the plan hierar hy, thedynami nature of the domain, and the requirements of the task eliminate most existing ap-proa hes from onsideration. For instan e, we ould potentially generate a DBN Dynami Belief Network (Kjærul , 1992) to represent the probabilisti distribution over the planvariables. To do so, we in lude nodes representing all of the plan variables, Xt, as well asrepresenting done(X; t). The links among these nodes represent the stru ture of the planhierar hy (e.g., parent- hild relationships, temporal onstraints), and we an ll in the on-ditional probability tables a ordingly. We also represent the temporal progress of the teamby in luding nodes for the variables at the next time sli e, Xt+1. We add links from the Xtnodes to the Xt+1 nodes and represent the dynami s in the onditional probability tableson those links. For ea h transition from a node Xt to a node Yt+1 (X 6= Y ), we wouldalso add binary nodes indi ating the observation of a message along that transition. Thus,for a plan hierar hy with M plan nodes, the orresponding DBN representation will haveO(4M +M2) = O(M2) binary random variables.The standard DBN inferen e algorithms maintain a belief state, bt, representing theposterior probability distribution over the variables in time sli e, t, onditioned on all ofthe observations made so far (from time 0 t). These inferen e algorithms an update the92\nMonitoring Teams by Overhearingbelief state to in orporate new eviden e about any variables, Xt, and they an also omputethe next time-ti k's belief state, bt+1. We an extra t the desired probability over plan-state variables by examining the posterior probabilities stored in bt. Given the dependen ystru ture of our plan model, the spa e and time omplexity of performing inferen e usingthis DBN (either in orporating a single observation, or omputing bt+1) is O(2M2) for asingle agent.This DBN method is not su iently e ient to support on-line monitoring in real-worlddomains, sin e on ea h and every time step, the re ognizer must perform an inferentialstep of exponential omputational omplexity. There exist single-agent plan-re ognitionte hniques that avoid the exponential omplexity of DBNs by using a representation andinferen e algorithms aimed at the parti ular properties of the plan-re ognition task (e.g.,Pynadath & Wellman, 2000). Su h spe ialized representations avoid the full generalityof DBNs, while still apturing a broad lass of interesting planning agent models. Givena spe ialized representation, the single-agent plan-re ognition algorithms an exploit theparti ular stru ture of the plan models to a hieve e ient online inferen e.Drawing our inspiration from the su ess of this work in single-agent domains, we adopta similar methodology in our multi-agent domain. In other words, we have developed anovel plan-re ognition representation more suited to apturing team-oriented programs. Thestru tural assumptions we make in this representation support e ient inferen e with ourspe ialized algorithms, as well as more naturally supporting an extension to represent inter-agent dependen ies (as dis ussed in Se tion 4).We represent the team-oriented plan as a dire ted graph, whose verti es are plans, andwhose edges signify temporal and hierar hi al de omposition transitions between plans:Children edges denote hierar hi al de omposition of a plan into sub-plans. Sibling edgesdenote temporal orderings between plans. Following the stru ture of the plan hierar hy, thevariables fXtg form a dire ted onne ted graph, su h that ea h node Xt has at most onehierar hi al-de omposition in oming transition from a parent node (representing its parentplan), and any number of temporal in oming transitions from plans that pre ede it in orderof exe ution. The graph may ontain multiple nodes for a single plan, if the plan is the po-tential hild of multiple parent plans. The node may have any number of temporal outgoingtransitions to immediate su essor sibling nodes (representing plans that may follow it inorder of exe ution), and any number of hierar hi al-de omposition outgoing transitions tothe node's rst hildren (i.e., those that will be exe uted rst by a de omposition of theplan Xt. The graph forms a tree along hierar hi al de omposition transitions, so that noplan an have itself as a des endent. On the other hand, there may be y les along temporaltransitions (to siblings). In other words, a plan may have an outgoing temporal transitionto itself (meaning that it an be sele ted for exe ution again upon termination), or to anode that has a temporal path leading ba k to the plan (meaning that it is the rst nodein a temporal sequen e of plans that may be exe uted repeatedly). It may also have twoalternative temporal paths leading indire tly from one node to another.To perform inferen e with this representation, we borrow the standard DBN inferen ealgorithms' notion of a belief state, bt. As in the DBN ase, the belief state represents theposterior probability distribution over the variables in time sli e, t, onditioned on all of theobservations made so far. In addition, for ea h plan, we distinguish between a state of a tualexe ution and a blo ked state, indi ating that exe ution has terminated, but exe ution of93\nKaminka, Pynadath, & Tambea su essor has not yet begun (perhaps be ause the agent is in the pro ess of sending amessage). Thus, bt(X; blo k) is our belief that X has terminated, but the agent has notbegun exe ution of a su essor; bt(X;:blo k) is then our belief at time t that the monitoredagent is urrently exe uting X, whi h has not yet terminated. More pre isely, we de nebt(X; blo k) Pr(Xt; done(X; t + 1)jE) and bt(X;:blo k) Pr(Xt;:done(X; t + 1)jE),where E again denotes all of the eviden e we have re eived so far. If the re ognizer observesa message from an agent at time t, it updates its previous belief state, bt, by in orporatingthe eviden e into its new belief state, bt+1, a ording to the method des ribed in Se tion3.2. If it does not observe a message from an agent at time t, it propagates belief into itsnew belief state, bt+1, using the method des ribed in Se tion 3.3 to simulate plan exe utionover time.3.2 Belief Update with Observed MessageWhile observing team ommuni ations, the re ognizer an expe t to o asionally re eiveeviden e in the form of messages (sent by an individual agent member) that identify eitherplan initiation or termination. In in orporating this eviden e, we exploit the assumptionthat the agents are truthful in their messages. In other words, if we observe an initiationmessage for a plan, X, at time t, then Xt is true with ertainty. Likewise, if we observea termination message for a plan, X, at time t, then done(X; t + 1) is true with ertainty.More pre isely, the algorithms presented in this se tion are spe ialized to exploit the prop-erty of observed ommuni ations, where for any observation , either Pr(Xtj ; E) = 1 orPr(done(X; t)j ; E) = 1, for any possible previously observed eviden e, E .Though messages are assumed truthful, there still remains ambiguity. First, while amessage uniquely spe i es the relevant plan, it does not uniquely spe ify the relevant node.In other words, the re ognizer is still unsure about whi h parti ular Xt node the messagerefers to, sin e the graph may ontain multiple Xt nodes onsistent with the message. Fur-thermore, when a message announ es termination of a plan (even with no ambiguity aboutthe orresponding node), there still remains ambiguity about the next plan sele ted by theagent.The observations available in the overhearing tasks of immediate interest to us fall intothis level of ambiguity. In our eva uation s enario example, there are two nodes orrespond-ing to the plan land-troops, be ause there is one instan e of land-troops for pi king upthe people to be transported and another for dropping them o . If the re ognizer observesa message indi ating that an agent has initiated exe ution of land-troops, then there isambiguity about whi h of the two instan es is urrently relevant. Furthermore, there mayexist ambiguity about whi h plan the agent will sele t after terminating land-troops.Algorithm 1 presents the pseudo- ode for the omplete pro edure for in orporating evi-den e from observations.In orporating Eviden e of an Observed Initiation Message (lines 3 8) Supposethat, at time t, we have observed a message, msg, that orresponds to initiation. If only oneplan, X, is onsistent with msg, then we know, with ertainty, that the agent is exe uting X,regardless of whatever eviden e we have previously observed. Therefore, we an simply setour belief that Xt is true to be 1.0. If multiple plans are onsistent with msg, we distributethe unit probability over ea h onsistent plan, weighted by our prior belief in seeing the given94\nMonitoring Teams by OverhearingAlgorithm 1 In orporate-Eviden e(msg m, beliefs b, plans M)1: Initialize distributions b0; bt+1 0:0 for all plans in M2: for all plans X 2M onsistent with m do3: if m is an initiation message then4: for all plans W that pre ede X do5: b0(X;:blo k) b0(X;:blo k) + bt(W; blo k) wx wx6: else {m is a termination message}7: for all plans Y 2M that su eed X do8: b0(Y;:blo k) b0(Y;:blo k) + bt(X; blo k) xy xy9: Normalize distribution b010: for all plans X 2M with b0 > 0 do11: bt+1(X;:blo k) b0(X;:blo k)12: Propagate-Down(X; b0(X;:blo k); b;M)13: tmp X14: while parent(tmp) 6= null do15: bt+1(parent(tmp);:blo k) bt+1(parent(tmp);:blo k) + bt+1(tmp;:blo k)16: tmp parent(tmp)message. This prior belief depends on all prede essor plans of X that may have terminatedprior to seeing this message.To support the omputation of the beliefs over transitions from prede essor plans tosu essors, as well as the beliefs of seeing a message for a given transition, Overseer storestwo parameters: and . The former is the probability of entering a su essor plan, X,given that prede essor plan, W , has just ompleted: wx Pr(Xt+1jWt; done(W; t + 1)).The latter is the probability of seeing a message, given that the agent took the spe i edtransition: wx Pr(msgtjWt; done(W; t + 1);Xt+1). We an use previous runs to a quiresuitable values for these parameters, and , by produ ing a frequen y ount over transitionsand messages seen during those runs (see Se tion 4.2 for more dis ussion of the use of inOverseer).Therefore, given the observation of an initiation message, msg, at time t, we wish todistribute the unit probability over all plans, X, (in the unblo ked state) that are onsistentwith msg. We an derive our new belief in plan X at time t+ 1 as follows:Pr(Xt+1jmsg; E) =Pr(msg;Xt+1jE)Pr(msgjE)The denominator is simply a normalization fa tor, and it is the same for all andidate plans,X. Therefore, we ignore it in this derivation, and fo us on only the numerator, whi h we an expand over all possible prede essor plans, W , and possible termination states of W :/XW Pr(msg;Xt+1;Wt; done(W; t + 1)jE)+XW Pr(msg;Xt+1;Wt;:done(W; t + 1)jE)95\nKaminka, Pynadath, & TambeThe se ond term is 0, sin e we annot pro eed from W to X if W has not terminated.In the se ond term, we an expand the joint probability into its omponent onditionalprobabilities: /XW [Pr(msgjWt; done(W; t+ 1);Xt+1; E) Pr(Xt+1jWt; done(W; t + 1); E) Pr(Wt; done(W; t + 1)jE)℄We assume that the probability of sending a message and the distribution over plan tran-sitions obey a Markov property, so that they are independent of the plan history beforetime t, given the urrent plan at time t. Thus, the rst two onditional probabilities areindependent of our previous history of observations. The third is exa tly our previous beliefthat W is blo ked: /XW [Pr(msgjWt; done(W; t+ 1);Xt+1) Pr(Xt+1jWt; done(W; t + 1)) bt(W; blo k)℄The rst two onditional probabilities are exa tly our parameters, and :/XW wx wxbt(W; blo k) (1)Lines 4 5 of Algorithm 1 perform exa tly the derived summation of Equation 1 (thenormalization step is arried out on line 9 (see below). A similar pro edure is followed whena message is observed indi ating the termination of X (lines 6 8). In su h a ase, we knowthat the agent was exe uting X in the previous time step but that it has moved on to somesu essor. Thus, for ea h of X's potential su essor plans Y , we set our belief in Y to beproportional to a transition probability, similar to that for the initiation message:Pr(Yt+1jmsg; E) =Pr(msg; Yt+1jE)Pr(msgjE)The denominator is again a normalization fa tor that we ignore. We an expand the nu-merator over possible states of X's exe ution:/Pr(msg; Yt+1;Xt; done(X; t + 1)jE)+ Pr(msg; Yt+1;:Xt; done(X; t + 1)jE)+ Pr(msg; Yt+1;Xt;:done(X; t+ 1)jE)+ Pr(msg; Yt+1;:Xt;:done(X; t + 1)jE)96\nMonitoring Teams by OverhearingOnly the rst term is nonzero, sin e the others orrespond to states of exe ution that arein onsistent with the observed message:/Pr(msg; Yt+1;Xt; done(X; t + 1)jE)We an rewrite this joint probability as a produ t of onditional probabilities:/Pr(msgjXt; done(X; t+ 1); Yt+1; E) Pr(Yt+1jXt; done(X; t+ 1); E) Pr(Xt; done(X; t + 1)jE)We again use our Markovian assumptions to simplify the onditional probabilities, and werewrite the third probability using our belief state:/Pr(msgjXt; done(X; t+ 1); Yt+1) Pr(Yt+1jXt; done(X; t + 1)) bt(X; blo k)Finally, we rewrite the rst two onditional probabilities using our parameters, and :/ xy xybt(X; blo k) (2)Lines 7 8 of Algorithm 1 perform exa tly the derived summation of Equation 2.Normalization of the sum (line 9). Line 9 normalizes the sum to re apture a well-formed probability distribution. Note that the normalization step must take into a ountthe fa t that eviden e may be in orporated for plan steps where one is an an estor ofanother in whi h ase the eviden e for the an estor plan is probabilisti ally redundant.The more spe i eviden e (for the des endent plan) will be more useful for visualization,as it is more a urate.Propagation of Eviden e (lines 10 16) Finally, the re al ulated beliefs are set (line11) and then the hanges are re ursively propagated down the de omposition hierar hy tothe plan's hildren (line 12), via the all to Algorithm 2. In addition, the re al ulated beliefsare propagated up to the plan's an estors in the de omposition hierar hy (lines 13 16), sin eeviden e of a hild plan being a tive is eviden e of its parent being a tive as well. We assumehere that we have no knowledge about the relative likelihood of the hild plans, so we treatea h as equally likely. If we had additional knowledge about these likelihoods, we ouldeasily exploit it in our Propagate-Down algorithm.Algorithm 2 Propagate-Down(plan Y , probability , beliefs b, plans M)1: C f j 2M; rst hild of Y g2: 0 = j C j3: for all plans 2 C do4: bt+1(Y;:blo k) bt+1(Y;:blo k) + 05: Propagate-Down( ; 0; b;M) 97\nKaminka, Pynadath, & Tambe3.3 Belief Update with No ObservationIn overhearing tasks, there is a great deal of un ertainty about when agents omplete theexe ution of their plan steps, sin e agents do not ne essarily send messages upon everytermination or initiation of a plan. Therefore, if no messages are observed at time t, then thesystem's beliefs for time t+1 must be al ulated based on the possibility that the agents mayhave initiated or terminated plans without sending any messages. To support the ne essarybelief update, we need a model of plan exe ution that provides us with a probability of plantermination over time (i.e., Pr(done(X; t))). In prin iple, this probability distribution anbe arbitrarily omplex, and its stru ture may vary enormously from domain to domain, andeven from plan to plan within the same domain. In some domains, obtaining an a uratemodel of this distribution requires omplex knowledge a quisition from domain experts orelse a omplex learning pro ess on the part of the agent. In addition, an a urate modelmay be too omplex to support e ient online inferen e.Overseer instead uses a temporal model that supports both e ient inferen e andsimple parameter estimation pro edures. Overseer models the duration of a (leaf) plan,X, as an exponential random variable. In other words, the probability of the plan ompletingexe ution within time units in reases as 1 e X . The single parameter, X , orrespondsto 1/(mean duration of X), whi h we an easily a quire from domain experts or previousruns. As for inferen e, the exponential random variable has a Markovian property, in thatthe probability of the plan's ompletion between times t and t+ 1 isPr(done(X; t + 1)jXt) 1 e x ;independent of how long the agent has been exe uting X before time t. This strong assump-tion may not fully hold in some real-world domains, but it is often a good approximation.Also, the error asso iated with this approximation may be a eptable, given the enormousgain in inferential e ien y (as we show in the remainder of this se tion).These e ien y gains manifest themselves when Overseer rolls the model forwardin time to ompute its belief state for the next time sli e. Given the exponential randomvariable as a model of plan duration, the probability of ompletion of a leaf plan is a onstant,1 e x , for ea h plan X. For plans with hildren, the probability of ompletion is exa tlythe probability of ompletion of its last hild (a ording to the temporal ordering of the hildren).Having omputed the probability of plan termination, Overseer then evaluates whi hplan the agent may exe ute next. It examines the possible su essors and, for ea h, om-putes the probability of taking the orresponding transition, onditioned on the fa t that nomessage was observed (1 xy), and on the prior probability of taking this message ( xy).Again, as mentioned in Se tion 3.2, Overseer makes a Markovian assumption that theplan history before time t does not a e t the likelihood of the various transitions. Giventhis assumption, it an ombine the two parameters, and , to get the desired onditional98\nMonitoring Teams by Overhearingprobability of the transition, given that we observed no message:Pr(Yt+1jXt; done(X; t + 1);:msgt)=Pr(:msgtjXt; done(X; t+ 1); Yt+1) Pr(Yt+1jXt; done(X; t + 1))Pr(:msgtjXt; done(X; t + 1))= (1 xy) xyXZ Pr(:msgtjXt; done(X; t + 1); Zt+1) Pr(Zt+1jXt; done(X; t + 1))= (1 xy) xyXZ (1 xz) xz=(1 xy) xy X (3)The normalizing denominator, X , is the sum of the numerator over all possible su es-sors, Y , whi h we an pre- ompute o -line. We an use the value of X to determine thelikelihood that the agent will send a message upon terminating plan X at time t. In thespe ial ase when X = 0, Equation 3 is not well-de ned, as all possible transitions fromX require a message. In this ase, the agent annot have begun exe ution of any su essor,even though it has ompleted exe ution of X. X is therefore the probability mass signifyingour belief that the agent is no longer exe uting X at time t + 1, and is not waiting for amessage (i.e., it is in a blo ked state). In other words, it is our in reased belief that theagent is exe uting one of X's immediate su essors at time t + 1, given that we have seenno message.Algorithm 3 presents the pseudo- ode for the pro ess of propagating the probabilitiesforward in time when a message is not observed. First, it initializes all the values to 0 (lines1 5). The pro ess ontinues by going over all plans X 2 M , in post-order we explore hildren plans (i.e., plans rea hable by hierar hi al de omposition transitions) before theirparents, and sibling plans in order of exe ution. For ea h plan, the algorithm exe utes fourstages: (1) It determines the plan's outgoing probabilities (lines 7 10); (2) it determines x,the outgoing probability mass that is propagated along the outgoing temporal transitionswithout being blo ked by waiting for a message (lines 11 12); (3) it propagates x alongthe non-blo ked temporal outgoing transitions (lines 13 20); and nally (4) it omputes ourbelief that the agent will exe ute the plan at the next time-ti k bt+1(X;:blo k) or will beblo king (lines 21 22). The remainder of this se tion explains these four stages in detail.Cal ulating the outgoing probability outx (lines 7 10). In Algorithm 3, the variableoutx represents the total temporal outgoing probability from plan, X, given our belief thatthe agent was exe uting X at time t. If a plan X is a leaf, then we derive its temporaloutgoing probability, outx, from the temporal model dis ussed previously, given our beliefthat the agent is urrently exe uting X (lines 7 8). If X is a parent, lines 9 10 are, in fa t,redundant: They serve only to remind the reader that for a parent, Y , outy follows fromY 's hildren when they exe ute line 20. This depends riti ally on the post-order traversalof the plan-hierar hy: the outgoing probability of a parent Y is derived from the outgoingprobabilities of its last hierar hi al-de omposition hildren, and thus all hildren's outgoingprobabilities must be al ulated before their parents'.99\nKaminka, Pynadath, & TambeAlgorithm 3 Propagate-Forward(beliefs b, plans M)1: for all plans X 2M do2: bt+1(X;:blo k) 0:03: bt+1(X; blo k) 0:04: outx 0:05: x 0:06: for all plans X 2M in post-order do { hildren in temporal order before parents}7: if X is a leaf then8: outx bt(X;:blo k)(1 e x) { al ulate probability of X terminating at time t}9: else {X is a parent}10: outx is known { be ause post-order guarantees all hildren set it in line 20}11: for all temporal outgoing transitions Tx!y from X do12: x x + (1 xy) xy13: if x > 0 then {some transition an be taken}14: for all temporal outgoing transitions Tx!y from X do15: outx(1 xy) xy16: if Tx!y leads to a su essor plan Y then17: bt+1(Y;:blo k) bt+1(Y;:blo k) + 18: Propagate-Down(Y; ; b;M)19: else {Tx!y is a terminating transition}20: outparent(x) outparent(x) + (1 xy) xy {parent's outgoing probability is its hil-dren's}21: bt+1(X; blo k) bt+1(X; blo k) + outx x22: bt+1(X;:blo k) bt+1(X;:blo k) outxDetermining the non-blo ked outgoing probability x (lines 11 12). The prob-ability, x is the sum over all possible values of the numerator in Equation 3 (i.e., over alltemporal outgoing transitions originating in X), as illustrated in the derivation. As we seein line 21, x is riti al for al ulating the belief that the agent has terminated exe ution ofX; but has not yet begun exe ution of a su essor (i.e., the belief bt+1(X; blo k) that theagent is blo king).Propagating x along temporal outgoing transitions (lines 13 20). This is thekey omponent in the propagation. For every temporal outgoing transition Tx!y, Over-seer al ulates , a temporary variable that holds the probability mass orresponding toOverseer's belief in the joint event of (i) the agent having ompleted exe ution of X, (ii)the agent taking the transition TX!Y , and (iii) the agent doing so without sending out anobservable message. The al ulation of is derived as follows: = Probability that X is done ^ no message was observed ^ agent hose Tx!y= Pr(done(X; t)jXt) Pr(:msgjXt; done(X; t)) Pr(Yt+1jXt; done(X; t);:msgt)= outx x (1 xy) xy x= outx (1 xy) xy (4)If the transition Tx!y leads to a su essor plan Y (lines 16 18), then is added toY 's future state (at time t + 1) as temporal in oming probability. Sin e de ompositionis assumed to be immediate, this in oming probability is propagated (added) to Y 's rst100\nMonitoring Teams by Overhearing hildren (Algorithm 2). If there are multiple rst hildren, then they denote alternative plande ompositions for a single agent, and we ompute the probability over them by dividingthe probability in oming to the parent among them. If any hildren have rst hild plansof their own, we distribute this new in oming probability in turn, using the same method.Only in the next time-step does the algorithm propagate from rst hildren to the next hild, in order of exe ution. The reason for this is that we assume that all plans take atleast a single time step to omplete.If the transition Tx!y is the spe ial- ase termination transition (line 19 20), then X hasno su essors. In this ase, the outgoing temporal probability is added to X's parent's out-going probability outparent(x) so that it may be used when propagating parent(x)'s temporaloutgoing probability along its own temporal outgoing transitions. Note again that the post-order traversal of the plan-hierar hy guarantees that all hildren are explored before theirparents, thus outparent(x) is fully omputed by the time the algorithm rea hes parent(x).Computing X's new blo ked and non-blo ked probabilities (lines 21 22). Nowthat the outgoing probability mass has been propagated to X's hildren and siblings, theonly steps remaining involve re- al ulation of Overseer's belief in X's blo ked and non-blo ked states. The total temporal outgoing probability (whether blo ked or not) is outx; itmust be subtra ted from future belief that the agent is exe uting X. The probability massthat left bt(X;:blo k) but is blo king on a message that was not observed by Overseer isoutx x. It is added to X's future blo ked state.3.4 Dis ussionThe overhearing approa h outlined in this se tion maintains a separate plan-re ognitionme hanism for ea h agent, ignoring any inter-agent dependen ies. Using an array of indi-vidual models (Figure 3) that are updated with the passage of time, or as messages areobserved, the state of a team is taken to be the ombination of the most likely state of ea hindividual agent. Algorithm 4 embodies this approa h: It is alled every time ti k, olle tsall messages that are observed, and updates the state of the agents. EVACUATE\n.....\n[TASK FORCE]\nEXECUTE MISSION\n[TASK FORCE]\nPROCESS [TASK FORCE]\nMANEUVERS ZONE LANDING\n....\nFLY-FLIGHT PLAN\n[FLIGHT TEAM]\nFLY-CONTROL ROUTE....\nORDERS\n[FLIGHT TEAM]\n[FLIGHT TEAM]\nTRANSPORT OPERATIONS ESCORT [ESCORT] [TRANSPORT] OPERATIONS\n[GET ORDERS] ORDERS GET\nEVACUATE\n.....\n[TASK FORCE]\nEXECUTE MISSION\n[TASK FORCE]\nPROCESS [TASK FORCE]\nMANEUVERS ZONE LANDING\n....\nFLY-FLIGHT PLAN\n[FLIGHT TEAM]\nFLY-CONTROL ROUTE....\nORDERS\n[FLIGHT TEAM]\n[FLIGHT TEAM]\nTRANSPORT OPERATIONS ESCORT [ESCORT] [TRANSPORT] OPERATIONS\n[GET ORDERS] ORDERS GET\nFigure 3: Array of single-agent re ognizers one for ea h agent.As an illustration of the operation of this algorithm, onsider the example domain ofthe eva uation s enario. Overseer begins with a belief that the agent is exe uting its top-level plan (and its rst hild, Pro ess-Orders) at time 0 (i.e., b0(Eva uate;:blo k) = 1:0,b0(Pro essOrders;:blo k) = 1:0). If Overseer observes a message about the initiation ofFly-Flight-Plan by one of the heli opters, then it applies In orporate-Eviden e (Algo-101\nKaminka, Pynadath, & TambeAlgorithm 4 Array-Overseer(beliefs b, plan-hierar hy array M [℄, agents A)1: for all Agents a 2 A do2: if A message ma from a was observed then3: In orporate-Eviden e(ma; b; M [a℄)4: else {No message was sent by a }5: Propogate-Forward(b; M [a℄)rithm 1). From the plan-hierar hy (Figure 1b) it is known that Pro ess-Orders annot be apossible urrent or future plan of the agent, and that the heli opter in question is exe utingFly-Flight-Plan, i.e., bt(Pro essOrders;:blo k) = 0, bt(F lyF lightP lan;:blo k) = 1:0.This probability mass is propagated to Fly-Flight-Plan's rst hildren, of whi h there isone, and thus the belief in this hild is set to 1.0 as well.After some time passes and no message is observed, there is un ertainty as to whetherFly-Flight-Plan and Landing-Zone-Maneuvers are a tive, as both are possible futurestates, and the duration of Fly-Flight-Plan is un ertain. Overseer would still assigna probability of 1.0 to the top-level plan Eva uate. However, some probability mass fromFly-Flight-Plan would be propagated every time-ti k to Landing-Zone-Maneuvers byPropagate-Forward (Algorithm 3). For ea h su h propagation, the in oming temporalprobability mass being added to the belief in the exe ution of Landing-Zone-Maneuverswould be propagated to its rst hildren immediately. Assuming that the heli opter agentis free to sele t either Transport-Operations or Es ort-Operations, the in oming proba-bility would be split evenly and added to the prior belief in ea h of the two rst hildren.In the same temporal propagation step, any outgoing belief from these rst hildren wouldbe propagated via their own outgoing temporal transitions.The inferen e pro edure des ribed by Algorithms 1 4 exploits the parti ular stru ture ofour representation in ways that more general existing algorithms annot. The pseudo- odedemonstrates that for a single monitored agent, both types of belief updates have a time omplexity linear in the number of plans and transitions in M , i.e., O(M). Thus for Nagents, the spa e and time omplexity of Algorithm 4 is O(MN).We gain this e ien y ( ompared to an approa h su h as DBN) from two sour es. First,we make a Markovian assumption that the probability of observing a message depends ononly the relevant plan being a tive, independently of exe ution history. With this assump-tion, we an in orporate eviden e, based on only our beliefs at time t. Se ond, we makeanother Markovian assumption in the temporal model, allowing our propagation algorithmto reason forward to time t + 1 based on only our beliefs at time t, without regard forprevious history.4. Monitoring a Team by OverhearingThe previous se tion has outlined an e ient plan-re ognition me hanism that is parti ularlysuitable for monitoring a single agent based on its ommuni ations. Monitoring a team wasa hieved by monitoring ea h member of the team independently of the others. Unfortunately,although the time omplexity of this approa h is a eptable, its monitoring (re ognition)results are poor. The evaluation in Se tion 6.1 provides more details, but, in short, theaverage a ura y using this approa h over all experiments was less than 4%.102\nMonitoring Teams by OverhearingThe main ause for this low a ura y is the s ar ity of observations, one of the identifying hara teristi s of monitoring by overhearing. As previously dis ussed, agents often swit htheir state unobservably (i.e., without sending a message). Therefore, the monitoring system riti ally needs to estimate orre tly the times at whi h agents swit h state. Sin e someagents rarely ommuni ate (i.e., there are very few observations about them), varian e intheir temporal behavior (with respe t to the system's predi tions) tends to ause large errorsin monitoring.To address this issue, we bring ba k for dis ussion the agent independen e assumptionwhi h we have made in the previous se tion. After all, team-members do not ommuni ateindependently of ea h other: Communi ation in a team is an a tion that is intended to hange the state of a listener (Cohen & Levesque, 1990). Agents that only rarely send amessage may still hange their state upon re eiving a message. In other words, althoughobserved messages are used in the previous se tion to update the belief in the state ofthe sender, they ould also be used to update the state of any listeners. To do this, themonitoring system must know about the relationships between the team-members.Knowledge of the so ial stru tures enables additional sophisti ated forms of monitoring.For instan e, in order to maintain their so ial stru tures, team-members ommuni ate withea h other predi tably, during parti ular points in the exe ution of a task. Su h predi tionsof future observable behavior ommuni ations an be used to further redu e the un er-tainty. However, it is often the ase that while it an be di ult to orre tly predi t thata spe i agent will ommuni ate at a spe i point in task exe ution, it is easy to predi tthat some team-member will. Knowledge of the pro edures employed by a team to maintainits so ial stru tures an be very useful allows a monitoring system to make su h predi tions.To reason about the e e ts of ommuni ations on re eivers, and about future observ-able behavior of team-members, a monitoring system must utilize knowledge of the so- ial stru tures and so ial pro edures used by team-members to maintain these stru tures.Su h exploitation of so ial knowledge for monitoring is alled So ially-Attentive Monitoring(Kaminka & Tambe, 2000). This se tion dis usses these on epts in detail.4.1 Exploiting So ial Stru turesWhile omputationally heap, the approa h des ribed earlier proved insu ient in the eva -uation domain. In monitoring by overhearing tasks, the monitoring system must addresss ar e observations, as agents rarely ommuni ate all at the same time. Indeed, in the eva -uation appli ation, only a single message was observed (on average) for every 20 ombinedindividual state hanges.Under su h hallenging onditions, a system for monitoring by overhearing must ometo rely extensively on its ability to estimate when agents hange their internal state with-out sending a message. The representation presented earlier used a simple, but e ient,temporal model to do this, based on the estimated average duration of plans. However, wehave found high varian e in the a tual duration of plan exe ution, ompared to the durationpredi ted by the average-duration model: Plan exe ution times vary depending on the external environment. For instan e, whenall the agents in the team are running on a lo al network, their response times to queries103\nKaminka, Pynadath, & Tambemay be shorter than when ommuni ating a ross ontinents. Indeed, laten y times inthe Internet vary greatly, and are di ult to predi t. Plan exe ution times vary depending on when a plan-step is exe uted internally. Forinstan e, the traveling plans, used repeatedly within the given eva uation team-oriented program, take anywhere from 15 se onds to almost two minutes to exe ute,depending on the parti ular route being followed. Plan exe ution times vary depending on the out ome of a plan-step. For instan e, whenthe route-planner is fun tioning orre tly, it responds within a few se onds. However,when it rashes it does not return an answer at all, and the other agents wait for arelatively long time before relying on a time-out to de ide that it had failed.This problem an be addressed in prin iple by a more expressive model of exe ution duration,for instan e taking into a ount the internal exe ution ontext. However, in pra ti e, su ha model would likely be mu h more expensive omputationally, as it would need to relyon knowledge of previous and future steps, breaking the Markovian assumption (e.g., todetermine duration based on when a plan-step is exe uted, an improved temporal modelwould have to reason about the likelihood that a given instan e of the plan-step is these ond instan e, as opposed to a third). As appli ations grow in s ale in the real world, anin reasingly more omplex temporal model would have to be ontinuously re ned to overthe in reasingly omplex temporal behavior of agents. Fortunately, a temporal model isonly one way in whi h a monitoring system an estimate the times in whi h agents hangetheir internal state unobservedly.An alternative method for estimating unobserved state hanges is to utilize known de-penden ies between agents to exploit eviden e about the state of one agent to infer the stateof another. In parti ular, it is often true in team settings that one agent would send a mes-sage intending to a e t the state of all its re eivers in a parti ular way. Thus in prin iple,under the assumption that the re eivers do hange their state predi tably, an observationof su h a message an be used as eviden e in the inferen e of the sender's state, as well asall re eivers', i.e., the state of all team-members. We an trade the agent independen e as-sumption made earlier with an assumption of su essful oordination. This is a reasonableassumption in team settings, given that agents are a tively attempting to maintain theirteamwork with su h ommuni ations (Tambe, 1997; Kumar et al., 2000; Dunin-Kepli z &Verbrugge, 2001).The e e ts of a message on a re eiver are dependent on the relationship between thesender and the re eiver (where we take su h a relationship to be des ribed by a mathemati alrelation between the possible states of the sender and the re eiver). In prin iple, su h rela-tionships underly so ial stru tures stru tures of intera tions between agents that make thede isions of one team-member dependent, to some predi table degree, on those of its team-mates. Using knowledge of these dependen ies, a monitoring agent may use observations ofa ommuni ation a tion by an agent to infer the possible state of another.One simple example of su h a stru ture is ommon in many teams (e.g., Jennings, 1993;Kinny, Ljungberg, Rao, Sonenberg, Tidhar, & Werner, 1992), and indeed is present alsoin our appli ation: roles that govern whi h team-members undertake what tasks in servi e104\nMonitoring Teams by Overhearingof the team goal. Su h roles ideally bias the de ision me hanism of the team-members to-wards making de isions that are appropriate for their roles. Thus knowledge of the roles ofteam-members an be useful to ounter the un ertainty fa ed by a monitoring agent. Forinstan e, suppose the monitoring agent knows that in the eva uation appli ation, a par-ti ular team-member is to hoose Transport-Ops, rather than Es ort-Ops, as a hild ofLanding-Zone-Maneuvers (be ause the team-member belongs to the TRANSPORT team,rather than the ESCORT team). This knowledge an redu e the un ertainty the monitor-ing agent has under the assumption that the team-member did not in orre tly hoose aninappropriate plan for its role. Overseer in fa t uses knowledge of roles in su h a mannerto alleviate un ertainty. This monitoring use of role information has been used in previouswork (Tambe, 1996; Intille & Bobi k, 1999), dis ussed in Se tion 7.However, a mu h more important so ial stru ture exists in teams. Agents in teams worktogether, as team-member are ideally in agreement about their joint goals and plans (Cohen& Levesque, 1991; Levesque, Cohen, & Nunes, 1990; Jennings, 1995; Grosz & Kraus, 1996,1999; Tambe, 1997; Ri h & Sidner, 1997; Lesh, Ri h, & Sidner, 1999; Kumar & Cohen,2000; Kumar et al., 2000). This phenomenon sometimes alled team oheren e (Kaminka& Tambe, 2000) holds at di erent levels in the team. Agents in an atomi subteam worktogether on the plans sele ted for the subteam, subteams work together with sibling subteamson higher level joint plans, et . Individual agents may still hoose their own exe ution, butthey do so in servi e of agreed-upon joint plans. Provided the monitoring agent knows whatplans are to be jointly exe uted by whi h subteams, and what transitions are to be takentogether by whi h subteams, it an use oheren e as a heuristi , preferring hypotheses inwhi h team-members are in agreement about their joint plans, over hypotheses in whi hthey are in disagreement.For example, suppose that the entire team is known to be exe uting Fly-Flight-Plan(Figure 1-b). Now, a message from one member of the TRANSPORT subteam is observed,indi ating that it has begun exe ution of the Transport-Ops plan step. Sin e this plan stepis to be jointly exe uted by all members of the TRANSPORT subteam (and only them),we an use oheren e to prefer the hypothesis that the other subteam members have alsoinitiated exe ution of Transport-Ops. Furthermore, sin e this plan-step is in servi e of theLanding-Zone-Maneuvers plan, whi h is to be jointly exe uted by the TRANSPORT andESCORT subteams, we an prefer the oherent hypothesis that team-members of ESCORTare exe uting Landing-Zone-Maneuvers. Now, based on their known role, we an now omeba k down the plan-hierar hy and infer that members of the ESCORT subteam are exe utingEs ort-Ops, et .This knowledge of the expe ted relationships, and in parti ular knowledge of whi h plansare joint to team-members (i.e., are subje t to oheren e), is part of the spe i ation of adistributed appli ation and an thus be provided to an overhearing system by the designeror operator. In fa t, it is often readily available, sin e it is used by the agents themselvesin their oordination. For instan e, we have earlier dis ussed the assumption that team-oriented programs are available to the monitoring agent, and that these hold knowledgeabout what plans in the hierar hy are to be exe uted by whi h (sub)teams is en oded in theplan-hierar hy. The team hierar hy ontains the knowledge about what subteam/agent ispart of another subteam. 105\nKaminka, Pynadath, & TambeCoheren e an be a very powerful heuristi . It assumes non-failing ases, where team-members su essfully maintain their joint exe ution of parti ular plans. Under this assump-tion, eviden e about a de ision made by one team-member in uen es (through oheren e),our belief of what its team-mates have de ided. And la king su h eviden e, oheren e prefershypotheses in whi h at least the team-members have made joint de isions. For instan e, sup-pose a transition from a team plan is to be taken only by the TRANSPORT team. Undernon-failure ir umstan es, there are only two oherent hypotheses onsidering this transi-tion: Either all members of TRANSPORT took the transition, or none did. Eviden e forone member, supporting one of these hypotheses, an be used to infer the state of the othermembers.The signi an e of this property of oheren e is that if the monitoring system an redu ethe un ertainty for even one agent, then this redu tion will be ampli ed through the useof the oheren e heuristi to apply to the other agents as well. The use of the oheren eheuristi an thus lead to a signi ant boost in monitoring a ura y, sin e the number ofhypotheses underlying any further (probabilisti ) disambiguation is ut down dramati ally.Se tion 6.1 provides an in-depth evaluation of the use of oheren e and knowledge of rolesto sele t plan re ognition hypotheses in Overseer.The use of oheren e signi antly in reases the time omplexity of the omputation.At the very least, it requires setting inter-agent links in the array of plan re ognizers usedby Overseer (Se tion 3.4), su h that these links represent a probabilisti asso iation be-tween plans that are to be exe uted jointly (in ontrast with the temporal and hierar hi de omposition transitions used thus far). For instan e, if a spe i plan X is to be exe utedjointly by agents A and B, then su h a link would be onstru ted between the variable XAt(representing agent A's exe ution of a plan X) and the variable XBt (representing agent B'sexe ution of the same plan). In general, there would be N (N 1)2 su h inter-agent links be-tween N agents, for ea h one of the joint plans (of whi h there are at most M). Thus givenN agents, and the array of re ognizers M [℄, where ea h individual agent's plan-hierar hyis of size M , the run-time omplexity of an exa t-inferen e algorithm would be at leastO(MN2) and quite likely mu h worse (sin e in general there is an exponential number of oherent and non- oherent hypotheses to sele t from). In the next se tion (Se tion 5.1),we des ribe a highly s alable (in the number of agents) representation for reasoning about oherent hypotheses.4.2 Exploiting Pro edures that Maintain So ial Stru turesA monitoring system an exploit knowledge of the pro edures agents use to maintain theirso ial stru tures to alleviate some of the un ertainty resulting from the s ar eness of obser-vations. For instan e, if the monitoring system ould a urately predi t future observablebehavior of monitored agents, then while it has not observed the predi ted behavior, themonitoring system may infer that the agents have not rea hed the state asso iated with thepredi ted behavior. Thus su h predi tions an be used to eliminate monitoring hypotheses,by setting an individual agent's XY probabilities to re e t a predi tion that a message willbe transmitted by the agent as its exe ution of X terminates and it initiates Y . For instan e,in our own appli ation, the Ariadne information agent is queried for possible threats beforeea h route is followed in the eva uation. It may therefore be possible to predi t that before106\nMonitoring Teams by Overhearingea h route is taken by the heli opters, a message will be sent by the Ariadne agent to itsteammates; thus while no su h message is observed, the Ariadne agent an be inferred tohave not yet exe uted this step. Furthermore, under the assumption of oheren e (dis ussedabove), the monitoring system may further infer that all team-members have not yet exe- uted this step, i.e., a new route was not taken by the team. Su h inferen e is obviouslydependent on the system's observational apabilities, but we have found it to be useful evenunder lossy observations by the monitoring system (see Se tion 6.2).However, in general, su h spe i individual predi tions an be di ult to make. Team-members are often engaged in joint tasks, whi h require many agents to ta kle a problemtogether. In these settings, predi ting individual ommuni ations may be impossible. Forinstan e, onsider a distributed sear h problem in whi h a target solution is to be foundsomewhere in the sear h-spa e; di erent areas of the sear h spa e are divided amongst theagents, with the understanding that the rst to nd the target will ommuni ate with theothers. It would be di ult to a urately predi t whi h one of the agents will ommuni ate( nd the target), sin e if we ould predi t that, we ould fo us all agents' e orts on that areaalone. Yet it is easy to predi t that at least one agent will nd the target and ommuni ate.Similarly, in the eva uation appli ation, it may be di ult to predi t whi h heli opter willrea h the ivilians rst but it is easy to predi t that one of them will, and will then ommuni ate their lo ation.Indeed, teams utilize so ial pro edures or onventions (Jennings, 1993) by whi h team-members maintain their relationships with one another. Removal of the agent independen eassumption allows the monitoring system to exploit knowledge of su h pro edures, by mak-ing predi tions as to the behavior of team-members in oordinating with one another. Forinstan e, knowledge of the failure-re overy pro edures used by a team to re over from oordi-nation failures allows the monitoring system to predi t the future behavior of team-membersin ase of failed exe ution. Similarly, knowledge of the ommuni ation pro edures used bythe team (as part of its team-members' oordination) allows predi ting future observablemessages future intera tions between team-members without ne essarily spe ifying a par-ti ular individual agent that will arry them out.For example, suppose Overseer overhears a message indi ating that the ight team hasinitiated joint exe ution of Fly-Flight-Plan (Figure 1-b). After some time has passed, it isnow possible that the team is either still exe uting Fly-Flight-Plan, or it has terminatedit already and begun joint exe ution of Landing-Zone-Maneuvers. However, if Over-seer knows that at least one team-member will expli itly ommuni ate after terminatingFly-Flight-Plan and before initiating Landing-Zone-Maneuvers, then while su h ommu-ni ations are not observed, the monitoring system an eliminate the possibility that the teamis exe uting the latter, eliminating any un ertainty in this ase (only Fly-Flight-Plan ispossible).We leave dis ussion of how te hni ally a so ial pro edure of the form at least one team-member will ommuni ate when its subteam will take this transition from X to Y an be onverted into XY values to the next se tion, where we present a te hnique for representingteam-wide probabilities in a way that allows e ient reasoning. In the remainder of thisse tion, we address instead how knowledge of su h so ial pro edures may be a quired.So ial pro edures of ommuni ations may be simple per- ase rules, or may involve omplex algorithms. For instan e, Jennings (1993) suggests using heuristi appli ation-107\nKaminka, Pynadath, & Tambedependent rules to determine ommuni ation de isions. STEAM (Tambe, 1997) insteaduses a de ision-theoreti pro edure that onsiders the ost of ommuni ation and the ostof mis oordination in the de ision to ommuni ate. Other pro edures have been proposedas well (e.g., Cohen & Levesque, 1991; Jennings, 1995; Ri h & Sidner, 1997). However,regardless of their omplexity, a key point is that a monitoring system does not ne essarilyhave to have full knowledge of these pro edures in order to exploit them for predi tions: itonly needs to approximate their out ome, sin e it an use a ombination of te hniques to ombat plan-re ognition ambiguity, rather than relying just on one te hnique.The de isions of so ial pro edures an be a quired by learning from previous runs ofthe system. Although a detailed exploration of appropriate learning me hanisms is out-side the s ope of this paper, we provide a stri t demonstration of the feasibility of learningso ial pro edures by simple rote-learning, whi h proved e e tive in generating a useful om-muni ations model that signi antly redu ed the un ertainty in monitoring the eva uationappli ation. This simple me hanism re ords during exe ution whi h plans are expli itly ommuni ated about, and whether they were initiated or terminated. The learned rules aree e tive immediately, and are stored for future monitoring of the same task.Figures 4a d present the results from using of this rote-learning me hanism in fourdi erent runs on the same tasks. The X-axis denotes observed ommuni ation message-ex hanges as the task progresses. Overall, between 22 and 45 ex hanges take pla e in arun, ea h ex hange in luding between one and a dozen broad ast messages in whi h agentsannoun e termination or initiation of a plan. The Y-axis shows the number of hypotheses onsidered byOverseer after seeing ea h message, without using any probabilisti temporalknowledge. Thus greater un ertainty about whi h hypothesis is orre t would be re e tedby higher values on the Y-axis. At the beginning of task exe ution, all possible plans are onsidered possible, sin e we ignore temporal knowledge in this graph. As progress is madeon the task, less and less steps remain possible before the end is rea hed, and so we expe tto see a gradual (non-monotoni ) de line as we move along the X-axis. A te hnique thatsu essfully eliminates hypotheses from onsiderations results in Y-axis values lower thanthose of this baseline exe ution urve.In Figure 4, the line marked No Learning shows this baseline (i.e., no predi tions, andwith the learning omponent turned o ). The baseline shows that a relatively high level ofambiguity exists, sin e the system annot make any predi tions about future states of theagents, other than that they are possible. When the learning te hnique is applied on-line(i.e., any message seen is immediately used for future predi tions), some learned experien e isimmediately useful, and ambiguity is redu ed somewhat (the line marked On-Line Learning).However, some ex hanges are either en ountered late during task exe ution, or are seen onlyon e. Those annot be e e tively used to redu e the ambiguity of the monitoring system onthe rst run. However, the third line (After Learning) presents the number of hypotheses onsidered when a fully-learned model is used. Here, the model was learned on run G, thenapplied without any modi ations in the other runs of the system. As an be seen, it showsa signi antly redu tion in the number of hypotheses onsidered by Overseer. Furtherevaluation of the use of ommuni ations predi tions is presented in Se tions 6.1 and 6.2;however, a full exploration of the use of learning for this task is beyond the s ope of thispaper. 108\nMonitoring Teams by Overhearing\n0\n5\n10\n15\n20\n25\n0 5 10 15 20 25 30 35 40 45\nN um\nbe r\nof R\nec og\nni ze\nd Pl\nan s\nObserved Communication Exchanges\nNo learning On-line learning\nUsing previously learned predictions\n(a) Learning in experiment C 0 5\n10\n15\n20\n25\n0 5 10 15 20 25 30 35 40 45\nN um\nbe r\nof R\nec og\nni ze\nd Pl\nan s\nObserved Communication Exchanges\nNo learning On-line learning\nUsing previously learned predictions\n(b) Experiment E 0 5 10 15 20 25\n0 5 10 15 20 25 30 35 40 45\nN um\nbe r\nof R\nec og\nni ze\nd Pl\nan s\nObserved Communication Exchanges\nNo learning On-line learning\nUsing previously learned predictions\n( ) Experiment G 0 5\n10\n15\n20\n25\n0 5 10 15 20 25 30 35 40 45\nN um\nbe r\nof R\nec og\nni ze\nd Pl\nan s\nObserved Communication Exchanges\nNo learning On-line learning\nUsing previously learned predictions\n(d) Experiment IFigure 4: Learning of ommuni ation de isions in di erent experiments. 109\nKaminka, Pynadath, & Tambe4.3 Dis ussionA key hara teristi of monitoring by overhearing tasks is the s ar ity of observations avail-able to the monitoring system. Fortunately, the observations available to the monitoringsystem an often be viewed as observations ofmulti-agent a tions: The sender of the messagenot only hanges its own state, but often also intends to hange the state of the re ipients(Cohen & Levesque, 1990). Thus even a single observation an be used as eviden e forinferring the state of both sender and re eivers. This stands in ontrast to previous work,whi h addressed monitoring of multiple single-agent a tions.In monitoring a team, the monitoring system an use knowledge of so ial stru tures andpro edures to exploit information about the a tivities of one team-member, in hypothesiz-ing about the a tivities of another team-member. These te hniques are not spe i to therepresentation presented earlier. For instan e, an in reased belief in one agent's exe utionof a plan X based on eviden e for a teammate's exe ution of X an be also used by on-stru ting appropriate probabilisti links between nodes representing these beliefs in a largeDBN representing the two agents. If we start with the DBN representation as dis ussed inSe tion 3.1, we an repli ate the single-agent network ( ontaining M plans) for ea h of theN separate agents. The number of nodes is then O(M2N), sin e we represent the plans andtransitions for ea h individual agent. We an also introdu e the appropriate inter-agent linksto apture the inter-agent dependen ies represented by our model of teamwork. However,upon introdu ing su h links, the omputational omplexity of performing DBN inferen eexplodes to O(2M2N ).Obviously, su h so ial reasoning an be omputationally expensive, even with the ef- ient representation des ribed earlier. The next se tion provides details of an e ientme hanism for reasoning about a team using information about role and oheren e, and uti-lizing ommuni ations predi tions. Using this me hanism, the te hniques des ribed in thisse tion have resulted in an a ura y of up to 97% (84% average a ross all experiments) ompared to average 4% without the use of so ial knowledge. Se tions 6.1 and 6.2 presenta detailed dis ussion of these results.5. Plan-Re ognition for OverhearingThe previous se tion has outlined so ially-attentive monitoring te hniques, alleviating theun ertainty in monitoring a team of agents by exploiting knowledge of the so ial stru turesand so ial pro edures of the monitored team. It dis ussed using oheren e and role mainte-nan e to exploit knowledge of the ideal agreement of agents that spe i plans be exe utedtogether, and that other spe i plans are assigned to agents ful lling their roles. Further-more, it dis ussed disambiguation based on predi tions of future observable behavior, basedon knowledge of the so ial pro edures employed by team-members. These disambiguationheuristi s eliminate many (in orre t) hypotheses from being onsidered. However, reasoningusing these te hniques an be omputationally expensive.This se tion presents an e ient algorithm, building on the representation previouslypresented, whi h fa ilitates s alable monitoring by overhearing of large teams. The key ideahere is to represent only those hypotheses whi h the heuristi s would have onsidered valid,eliminating from onsideration plans and transitions that would be onsidered illegal withthe heuristi s. Relying on the team-hierar hy for bookeeping, all oherent hypotheses are110\nMonitoring Teams by Overhearingrepresented using a single re ognizer instead of an array of re ognizers, o ering onsiderables alability in team monitoring. However, sin e the algorithm an no longer represent ertainhypotheses, this s alability omes at the expense of expressivity. We dis uss the s alablerepresentation and the trade-o it o ers below.5.1 E ient Reasoning with Team Coheren eCoheren e is a very strong onstraint, sin e for a team of agents there are only a linearnumber (O(M) where M is the size of the plan-hierar hy) of oherent hypotheses, but anexponential number of in oherent hypotheses (O(MN ) where N is the number of agents; theproof is in Appendix A). We an exploit this property by designing monitoring algorithmsthat reason only about the linear number of oherent hypotheses, and therefore o er betters alability as the number of agents in reases. Su h algorithms may not be able to reasonabout in oherent hypotheses, and are therefore less expressive. However, Se tion 6 demon-strates that the level of a ura y even with su h limited expressiveness is su ient for ourpurposes. Furthermore, algorithms that reason only about oherent hypotheses may stillbe able to dete t in oherent hypotheses, representing a failure state in whi h two or moreteam-members are in disagreement with ea h other.We begin by presenting the YOYO* algorithm, an e ient te hnique for reasoning about oherent hypotheses (Algorithm 5). YOYO* repla es the array-based algorithm des ribedearlier (Algorithm 4). Similarly to it, YOYO* is alled every time ti k. If no message isobserved, the state of the entire team is propagated forward in time. Otherwise, all observedmessages are olle ted together and used as eviden e for the (di erent) plans implied by thesemessages.YOYO*'s key novelty is that it relies on a single plan-hierar hy that is used to representall team-members together (regardless of their number), instead of an array of su h stru -tures. In other words, ea h variable Xt represents Overseer's belief that all agents in theteams asso iated with X (as des ribed in the team-oriented program) are exe uting the planX at time t. Thus YOYO* makes extensive use of the information asso iating plans andtransitions in M with teams and subteams in H, the team-hierar hy. The team hierar hyplays a riti al bookeeping role in this respe t, sin e it maintains the knowledge riti al for orre tly applying oheren e in the single re ognizer.This key distin tion between YOYO* and the array-based approa h auses a subtle,but riti al, di eren e in the way probabilities are propagated along transitions. In a plan-hierar hyM of an individual agent, part of an array of su h models, ea h outgoing transitionrepresented a hierar hi al de omposition or temporal step that the agent is allowed to take.Alternative outgoing transitions therefore represent alternative paths of exe ution availableto the agent. On the other hand, in a plan-hierar hy M used by YOYO*, alternativeoutgoing transitions tagged by di erent subteams (that are not an estors of one another)represent not a de ision point for the agent, but alternative paths of exe ution as de idedby the agents' roles and team-memberships.This reates a riti al di eren e in how the values of XY and XY are to be interpreted.Where previously (in Se tion 3) the value of xy referred to the probability that a spe i agent will take a transition X ! Y (given that it has terminated exe ution of X), inYOYO* it refers to the probability that an entire team will take the transition together.111\nKaminka, Pynadath, & Tambe\nAlgorithm 5 YOYO*(plan-hierar hy M , team-hierar hy H , beliefs b)1: if no new messages are observed then2: Team-Propagate-Forward(b, M)3: else4: Initialize distributions b0; bt+1 0 for all plans U 2M . ; Initialize I; E to be empty sets.5: for all Messages mi do6: I I [ fX j X 2M; mi is a an initiation message; X onsistent with mig7: E E [ fY j Y 2M; mi is a termination message; Y onsistent with mig8: for all plans X 2 I do9: T teammsg(X) {T is the agent sending the message initiating X}10: for all plans W 2M that pre ede X , where the transition W ! X is allowed for T do11: b0(X;:blo k) b0(X;:blo k) + bt(W; blo k) wx wx12: for all plans X 2 E do13: T teammsg(X) {T is the agent sending the message terminating X}14: for all plans Y 2 M;Y =2 I that su eed X , where the transition X ! Y is allowed for Tdo15: b0(Y;:blo k) b0(X;:blo k) + bt(X; blo k) xy xy16: Normalize distribution b0 taking teams into a ount17: for all plans X where b0(X;:blo k) > 0 do18: bt+1(X;:blo k) b0(X;:blo k)19: Team-Propagate-Down(X; b0(X;:blo k); b;M)20: T team(X)21: P X22: while parent(P ) 6= null do23: bt+1(parent(P );:blo k) bt+1(P;:blo k)24: if team(parent(P )) = parentteam(T ) then25: S ale(parent(P ); T; P; b)26: T = parentteam(T )27: P parent(P )\n112\nMonitoring Teams by OverhearingYOYO* is unable to represent hypotheses in whi h some team-members take one transition,and others do not unless these two di erent groups of members form di erent subteamsthat are represented in the team-hierar hy, and the di erent transitions are tagged as beingallowed for the di erent subteams.The value of XY is also interpreted di erently, in a very riti al way. Where in theprevious se tions it was taken to represent the probability that a spe i individual will ommuni ate when a transition X ! Y is taken, in YOYO* its value represents insteadthe probability that one or more team-members will ommuni ate when the transition istaken by the team. Thus it no longer refers to individual agents, but to a (sub-)team. Inthis way, YOYO* solves the issue of how to represent predi tions of the type at least oneteam-member will ommuni ate when this step is rea hed , dis ussed previously.For example, suppose YOYO* sets the belief that the team is exe uting theLanding-Zone-Maneuvers plan-step to some probability p. Landing-Zone-Maneuvers, inYOYO*, has two ( rst) hildren: Es ort-Ops and Transport-Ops, to be exe uted by mem-bers of th ESCORT and TRANSPORT subteams, respe tively. Unlike in the individualagent ase, the probability p should not be divided among these two hildren, but shouldbe dupli ated to them: A belief that the entire team is exe uting Landing-Zone-Maneuversimplies an equally-likely belief that the TRANSPORT subteam is exe uting Transport-Ops,and that the ESCORT subteam is exe uting Es ort-Ops. We explain YOYO*'s operationin detail below:No message is observed (lines 1 2). Sin e no observations are available, the state of theentire team is jointly propagated forward in time by alling Team-Propagate-forward(Algorithm 7, Appendix A). This is a slightly modi ed version of the propagate-forward(Algorithm 3) that takes di erent subteams into a ount in propagating beliefs: Given sometotal outgoing probability (either to a sibling or hild transition), if the outgoing transitionsare to be taken by di erent teams where one team is not an an estor of another (su h asthe TRANSPORT and ESCORT sub-teams), the same total probability would be used forea h transition, instead of splitting the outgoing probability between the transitions. Ap-propriately, Team-Propagate-forward relies on a modi ed version of the Propagate-Down algorithm (Algorithm 2), alled Team-Propagate-Down (Algorithm 6, AppendixA). This latter algorithm is also used in the in orporation of eviden e (lines 3 27). Therun-time omplexity of the propagation pro ess is O(M).One or more messages are observed (lines 3 7). If one or more messages are ob-served (sin e YOYO* is a single algorithm monitoring multiple potential message senders,more than one message may be observed at on e), YOYO* begins to in orporate these ob-servations into the maintained beliefs about the team. This pro ess is somewhat similar tothe In orporate-Eviden e algorithm, des ribed earlier (Algorithm 1), but takes into a - ount multiple observations (sin e all N agents may have sent a message). Multiple messages(from di erent agents) may all refer to the same plan, but YOYO* must not in orporateeviden e for them multiple times.The simple loop (lines 5 7) builds the set I (of initialized plans) and E (of terminatedplans) by going over all in oming messages that have arrived at time t. The run-time omplexity of this pro ess (in the worst ase) is O(N). Here, YOYO* does better than the113\nKaminka, Pynadath, & Tambearray approa h, sin e multiple messages always ause multiple updates in the array, but inYOYO*, multiple messages may all refer to a single plan, thus triggering a single update.In orporating eviden e about initiated and terminated plans (lines 8 15). Forea h one of these plans X in I (line 8), YOYO* now sets the new belief b0, weighted by anyprior belief in X's initiation (lines 10 11), similarly to how this is done in the in orporate-eviden e algorithm (Algorithm 1), but taking into a ount the team implied by the senderof the pro essed message (line 9). This is done by a lookup into M using the sender T(teammsg(mi)): Only transitions in M that T is allowed to take are followed. By de nition,any transition that is allowed to be taken by a super-team of T is allowed for T . A similarpro ess is then done with any termination messages (lines 12 15), but of ourse looking atpossible su essors of any plans onsistent with the messages. However, sin e we do notwant to ause updates in both line 11 and line 15 in ases where a termination message andan initiation message refer to the same transition, the loop over the plans Y (line 14) skipsany plans whi h have already been addressed in the previous step. Overall, the run-time omplexity of this pro ess is O(M).Normalizing the temporary distribution b0 (line 16). The temporary distributionb0 resulting from the pro essing of initiation and termination messages is normalized, in asimilar fashion to the analogous step in algorithm 1. However, the pro ess must take intoa ount not only the plan-hierar hy in question, but also the team-hierar hy. Unlike a typi alnormalization pro edure, eviden e for two di erent plans, sele ted by two di erent teams,may not ne essarily ompete with ea h other, and therefore may not ne essarily requirenormalization. For instan e, if two messages are observed, one implying that team A hasinitiated exe ution of plan P , and another implying that team B has initiated exe utionof plan Q, then if P and Q are both hildren of a joint parent J (exe uted jointly by thetwo subteams A;B), then the same normalized likelihood (1.0) should be assigned to P andQ (and J but this will be assigned to it by the propagation steps des ribed below). Therun-time omplexity of this pro ess is O(M).Propagating the eviden e up and down M (lines 17 27). First, the beliefs are setfor ea h plan implied by the observations, and its hildren (lines 18 19). Then, the team Tthat is to exe ute this plan is determined by a lookup into M using team(X) (line 20). NowYOYO* begins to propagate the eviden e up to the plan's parents (lines 21 26). Any beliefin the hild plan is propagated and added to the belief in its parent (line 23). However,if the parent plan (parent(P )) is to be exe uted by a super-team of the urrent team T ,then any hange to its probability must be propagated to its other hildren, that are to beexe uted by other (subteams). Thus the upward propagation is alternated with downwardpropagation along hierar hi al de omposition transitions1. This downward step is exe utedwhenever the team that is responsible for joint exe ution of the parent plan is no longerthe urrent subteam being onsidered (T ), but its parent team in the team-hierar hy H,given by parentteam(T ) (lines 24 26). When this ondition is satis ed, any hange in thebeliefs about the parent plan must be propagated down to any hildren it has that are to beexe uted by other subteams. This is done via the S ale algorithm (Algorithm 8, AppendixA).1. This alternating upward-downward propagation is the origin for YOYO*'s name.114\nMonitoring Teams by OverhearingThe downward propagation (line 25) implements a subtle but riti al step: It re-alignsany beliefs YOYO* maintains about subteams other than those implied by the message sothat these beliefs are made oherent with existing eviden e. The S ale pro edure, whi hre-distributes the new state probability of a parent among its hildren, su h that ea h hildgets s aled based on its relative weight in the parent. The end result is that the stateprobabilities of the hildren are made to sum up to the state probability of the parent. Thepro ess is re ursive, but never re-visits a subtree, sin e it is only arried out for hierar hi al-de omposition transitions that were not previously updated.On e this downward propagation is done, YOYO* updates the urrent team to be itsparent in the team-hierar hy, in line 26. Note that the all to parentteam re e ts a lookupin the team-hierar hy H, rather than the plan-hierar hy M . Finally, regardless of whetherdownward propagation took pla e, the temporary variable P is updated to limb up thehierar hi al de omposition in M (line 27).Ea h iteration through the loop begun on line 17 is O(M +H) sin e in the worst aseboth the plan-hierar hy and team-hierar hy are traversed. However, this loop many repeat(in the worst ase) for ea h of the plans in the plan-hierar hy, and thus overall, the run-time omplexity of this pro ess is O(M(M +H)) = O(M2 +MH).An example run of YOYO*. The following example illustrates YOYO*'s inferen eupon an observation of a message. Suppose a single member of the TRANSPORT subteam ommuni ates that it is initiating the Transport-Ops plan. Upon observing this message,YOYO* looks up the sender, to determine what transitions an be taken by it (line 8). It thenpro eeds to determine the new beliefs in team T 's exe ution of the Transport-Ops plan (lines9 10, then 16), and in orporates these new beliefs to re e t a mu h in reased belief that theTRANSPORT subteam is exe uting Transport-Ops and its hildren (lines 18 19). Sin ethis plan's parent, Landing-Zone-Maneuvers, is not null, YOYO* enters the loop in lines22 27. First, it in reases the belief in the exe ution of the parent (line 23). Then, it he ksthe ondition on line 24: Indeed, the team that is to exe ute Landing-Zone-Maneuvers isTEAM-FLY-OUT, the parent of the TRANSPORT subteam (i.e., Landing-Zone-Maneuversis to be exe uted jointly by the TRANSPORT and ESCORT subteams). YOYO* there-fore alls the S ale pro edure (line 25) to re-adjust Landing-Zone-Maneuvers' other hildren subtrees. Landing-Zone-Maneuvers has two hierar hi al-de omposition hildren:Transport-Ops (whi h YOYO* has already updated) whi h is to be exe uted by theTRANSPORT subteam, and Es ort-Ops, whi h is to be exe uted by the ESCORT subteam.S ale limbs down from Landing-Zone-Maneuvers to Es ort-Ops, in reasing YOYO*'sbeliefs that the ESCORT team is exe uting the Es ort-Ops plan. This pro ess re-alignsany prior beliefs YOYO* had about the likelihood that Es ort-Ops was being exe utedwith urrent eviden e, in e e t updating beliefs about the plans exe uted by the ESCORTsubteam, based on a single observation made of a member of the TRANSPORT team. Thepro ess now repeats this loop until the entire set of beliefs is updated and aligned withrespe t to the observed message.5.2 S alability in the Number of AgentsYOYO* o ers signi ant omputational advantages when ompared to the individual rep-resentation (array) approa h. YOYO* requires only a single, fully-expanded plan-hierar hy115\nKaminka, Pynadath, & Tambeto represent the entire team. This hierar hy is a union of all the individual agent plan-hierar hies, ontaining all transitions and plans, tagged by the subteams that are allowedto exe ute them. In addition YOYO* uses a single opy of the team hierar hy. SupposeM is the size of the plan-hierar hy, H is the size of the team-hierar hy, and N the numberof agents in the team. When agents are added to the monitored team, the team hierar hygrows by one new node that represents the new agent, and is onne ted to the appropriatesub-team in the team hierar hy. YOYO*'s spa e omplexity is therefore O(M +H). Sin eH grows with N , we ould write it O(M + N) ( ompare to the array approa h: O(MN),Algorithm 4).To analyze YOYO*'s run-time omplexity, we have to onsider the behavior of Algorithm5 separately in ases where no ommuni ations are observed, and in ases where at leastone message is observed. If no messages are observed, then an update takes the form ofa single all to Team-Propagate-Forward (Algorithm 7), an O(M) pro ess. This is learly a best- ase s enario for YOYO*. If one agent ommuni ates, then YOYO* wouldhave to go through M and H in its upward-downward propagation pro ess only on e, thusO(M +H) = O(M +N).The worst ase s enario for YOYO* o urs if all agents send messages, and ea h one ofthese N messages refers to a di erent plan (messages about the same plans would be mergedin lines 5 7). In this ase, there would be up to M di erent plans for whi h eviden e exists,and ea h one of them would require a separate update through lines 17 27. Thus YOYO*'srun-time omplexity in this ase isO(N +M +M +M(M +H)) = O(N +M2 +MH) = O(N +M2 +MN)Clearly, this worst- ase annot be ontinuously sustained by a monitored team, sin e agents annot ontinuously ommuni ate about their state. We thus believe that the average asein real-world domains with many agents would be mu h loser to the O(M + N) asepresented earlier (see Se tion 6.4 for empiri evaluation). In any ase, YOYO*'s omplexity ompares favorably with a pro edure reasoning about oherent hypotheses using an arrayof re ognizers, an O(MN2) pro ess (at least), even if only one agent ommuni ates (Se tion4.1).5.3 Dis ussionYOYO* expli itly represents a team as a single oherent entity. Its spa e and run-time re-quirements are preferable to the array based approa h when the number of agents grow, andit onsiderably simpli es reasoning about oheren e and ommuni ations predi tions. Onthe other hand, YOYO* sa ri es the apability to represent failing team a tivities (in o-herent hypotheses), where one team-member is exe uting one team-plan while its teammateis exe uting another. This does not at all mean that individual a tions taken by agentsare somehow lo ked together in syn hronous exe ution, or that individual agents must allexe ute the same individual a tion at the same time. For instan e, two team-members A; Bthat are ea h exe uting a ompletely di erent path of exe ution at the same time (i.e., plansteps A1; :::; Ak and B1; :::; Bl) an be easily represented by a plan hierar hy that in ludesan overall joint plan J , having two rst hierar hi al de omposition hildren, A1 and B1, to116\nMonitoring Teams by Overhearingbe sele ted by A and B, respe tively. A1 would have an outgoing temporal transition to A2,et . and similarly B1 would have an outgoing temporal transition to B2, et . Sin e J is tobe exe uted by the two team-members jointly, any initial eviden e for any one of the agentexe uting any of its individual plans would be used by YOYO* as eviden e for the otherteam-member having begun its own parallel exe ution of its own individual exe ution path.Further eviden e about one agent exe uting its own individual a tions would only in reasethe likelihood that the other agent is ontinuing its own exe ution, at its own pa e. However,it would be impossible for YOYO* to orre tly represent a monitoring hypothesis in whi hA is exe uting some hild of J , Ai, while B is exe uting some plan that is not J , nor a hildof J . Given the results of the evaluation we ondu ted (Se tion 6), whi h demonstrated theimportan e of oheren e in a urate visualization, the tradeo of expressivity vs. s alabilityis justi ed: Overseer's a ura y was mu h improved due to the use of oheren e.Although YOYO* sa ri es the apability to reason about ertain failure (in oheren e)hypotheses, it is still apable of supporting failure-dete tion, an important se ondary goal ofvisualization. In earlier work, we have shown the merits of oheren e in servi e of dete tingdisagreements in a team, in parti ular demonstrating that oherent monitoring leads tosound entralized disagreement dete tion, and may lead to sound and omplete disagreementdete tion under spe i ir umstan es (Kaminka & Tambe, 2000). As YOYO* is in fa t avery e ient way to reason about oherent hypotheses, it provides a good basis for providingsound disagreement dete tion results.A on ern about the generality of the te hnique may be raised based on YOYO*'s re-lian e on the team-hierar hy. However, we believe it is reasonable to expe t that large, omplex, real-world multi-agent systems of the type targeted by this paper would havean organizational hierar hy of some sort asso iated with them (see, for instan e, Tidhar,1993b). Human organizations ertainly demonstrate the emergen e of su h hierar hies, es-pe ially as the organizations grow larger (e.g., big orporations, government organizations)or ta kle mission- riti al tasks (e.g., military organizations). In addition, team-hierar hiesfor omputational agents are riti al for planning, for maintaining network and system se u-rity, et . Thus we believe our use of a team-hierar hy is not a weakness in our approa h, asorganizational stru tures will be ome as wide-spread in omputational multi-agent systemsas they already are in human multi-agent systems. Indeed, it may be possible to graduallylearn a team-hierar hy for a given oordinated team for the purpose of monitoring; however,dis ussion of this possibility is outside the s ope of this paper.Indeed, using a team-hierar hy, we an apply our assumption of oheren e to other rep-resentations and algorithms as well. For instan e, if we start with the DBN representationof the team from Se tion 4.3, we an unify the multiple random variables used to repre-sent the separate agents into a single random variable for an overall team/subteam. As inYOYO*, the size of the representation grows with the size of the plan hierar hy, and notthe number of agents. Thus, the number of nodes will be the same as for the single-agent ase, O(M2), as dis ussed in Se tion 3.1. However, again, the omplexity of inferen e inanswering plan-re ognition queries will still be exponential in the number of nodes, O(2M2).117\nKaminka, Pynadath, & Tambe6. EvaluationThis se tion presents a detailed evaluation of the di erent ontributions ontained withinOverseer. We begin by exploring the relative ontribution of ea h te hnique to the su essof Overseer as a whole (Se tion 6.1). We then fo us on evaluating Overseer's use of ommuni ations predi tions with respe t to lossless and lossy observations (Se tion 6.2).We then present a omparison of Overseer's performan e with that of human expertsand non-experts (Se tion 6.3). Finally, we empiri ally evaluate YOYO*'s s alability in ourappli ation domain (Se tion 6.4).6.1 A ura y EvaluationThe rst part of the evaluation tests the ontribution of the di erent te hniques in Over-seer to the su essful re ognition of the orre t state of the team-members. Figure 5 om-pares the average a ura y for a sample of our a tual runs, marked A through J (X-axis).In ea h su h 10 20-minute run, the team exe uted its task ompletely. At di erent pointsduring the exe ution, the a tual state of the system was ompared to the state predi tedby Overseer, where the predi tion was taken to be the urrent most-likely hypothesis.Ea h run had 22 45 su h omparisons (data-points). The per entage of orre t monitoringhypotheses for ea h run a ross those omparisons is given in the 0-1 (0-100%) range, on theY-axis.\nA B C D E F G H I J 0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nTemporal Coherent Coherent, Temporal Coherent, Comm Coherent, Temporal, Comm\nEvaluation Run\nA ve\nra ge\nA cc\nur ac\ny\nFigure 5: Per ent a ura y in sample runs.The a ura y when using the individual models with no oheren e (as in Se tion 3) ispresented in the leftmost bar (marked Temporal) in ea h group (Figure 5), and is learlyvery low. This approa h is a straightforward attempt at monitoring multiple agents by mon-itoring ea h individual, without onsidering the intera tions between them, as des ribed inSe tion 3. The next bar presents the monitoring a ura y when only oheren e is usedto rule out hypotheses (Se tion 5.1), with ties broken randomly. The next bar to the right(Coherent, Temporal) presents the results of ombining both oheren e and the probabilisti temporal model (Se tions 3 and 5.1). Then, the bar marked (Coherent, Comm) shows thee e ts of ombining the use of oheren e with the use of predi tions based on knowledge of118\nMonitoring Teams by Overhearingthe ommuni ation pro edures used by the team (Se tion 4.2). Here, the ommuni ationspredi tions were used to restri t the set of oherent hypotheses onsidered, with ties bro-ken randomly. The remaining bar (Coheren e, Temporal, Comm) presents the monitoringa ura y in ea h run using the ombination of all te hniques.The results presented in Figure 5 demonstrate the e e tiveness of the so ially-attentivemonitoring te hniques we presented. First, the results show that the oheren e heuristi brings the a ura y up by 15 30% without using any probabilisti reasoning. This boost inperforman e is a parti ularly interesting result, be ause of the relation between the oheren ete hnique and previous te hniques explored in the literature (Tambe, 1996; Intille & Bobi k,1999). Previous work has su essfully used the relationships between agents to in rease thea ura y of monitoring. The boost in Overseer's a ura y based on the use of role andteamwork relationships on rms the results from previous investigations. However, theresults also demonstrate that the te hnique is not su ient in this domain.Overseer adds a number of novel te hniques not addressed in previous work. The rst su h te hnique ombines oheren e with a temporal model of plan-duration (Coherent,Temporal), and it results in signi ant in reases to the a ura y, be ause the probabilisti temporal information now allows Overseer to better handle the la k of observations. Apossible alternative, whi h we explore in this evaluation, is to rely instead on the ommu-ni ations predi tions to rule out hypotheses about future states that may or may not havebeen rea hed (Coherent, Comm). It is therefore interesting to ompare the performan e ofthese two te hniques by omparing the (Coherent, Temporal) and (Coherent, Comm) bars.In almost all runs the average a ura y when using oheren e and ommuni ations pre-di tions is signi antly higher than when using oheren e and the temporal model. This isdespite the fa t that the more e e tive oheren e te hnique uses arbitrary (random) sele tionamong the available hypotheses: The reason for this is that in many ases the ommuni a-tion predi tions are powerful enough to rule out all hypotheses but one or two, signi antlyde reasing the un ertainty of the agents' plan-horizons. Thus even a random sele tion standsa better han e than a more informed (by a temporal model) sele tion among many more(10 20) hypotheses.However, runs J and B show a reversal of this trend ompared to the other runs. Figures6a b show the a umulative number of errors as task exe ution progresses during run I(Figure 6-a) and during run J (Figure 6-b). An error is de ned as a failure to hoose the orre t hypothesis as the most likely one (i.e., the most likely hypothesis does not re e tthe true state of the agent/team). Ea h message ex hange orresponds to one to a dozenmessages ommuni ated by the agents, establishing or terminating a plan. In the two gures,a lower slope means better performan e (less errors). The line marked Coherent shows thea umulative number of errors if only oheren e is used to sele t the orre t hypothesis most su h hoi es turn out to be erroneous sin e a random hoi e is made among the ompeting hypotheses. The line marked Coherent, Temporal shows the results using both oheren e and the temporal model to hoose the most likely hypothesis. Similarly, the linemarked Coherent, Comm shows the results using both oheren e and the ommuni ationspredi tions. Finally, the remaining line displays the results of using the ombined te hnique,using oheren e, the temporal model, and the ommuni ations predi tions.In Figure 6-a, we see that the two te hniques (Coherent, Temporal and Coherent, Comm)have almost equal slopes and result in almost equal number of errors at the end of run I,119\nKaminka, Pynadath, & Tambe\n2EVHUYHG 0HVVDJH ([FKDQJHV\n$F FXP\nXOD WLY\nH (U\nURU V\n&RKHUHQW &RKHUHQW &RPP &RKHUHQW 7HPSRUDO\n&RKHUHQW 7HPSRUDO &RPP(a) Run I\n2EVHUYHG 0HVVDJH ([FKDQJHV\n$F FXP\nXOD WLY\nH (U\nURU V\n&RKHUHQW\n&RKHUHQW &RPP &RKHUHQW 7HPSRUDO &RKHUHQW 7HPSRUDO &RPP(b) Run JFigure 6: A umulative number of errors in runs I and J.though from Figure 5 we know that due to the alleviated un ertainty, the use of ommuni- ations predi tions leads to overall higher probability of su ess (i.e., the Coherent, Commte hnique results in fewer alternative hypotheses, and thus has a better han e of being or-re t). However, in Figure 6-b we see that in run J the situation has hanged dramati ally.First, we see that the two lines are no longer similar. The line marked Coherent, Comm hasgreater slope than in run I, indi ating that the ommuni ations predi tions are not able toredu e the un ertainty, resulting in lower average a ura y. Se ond, we see that the tem-poral model results in many less errors, as eviden ed by the mu h slower-rising slope of theline marked Coherent, Temporal. Thus in this ase, the a tual duration of plans mat hedthe temporal model more a urately than in other runs.In trying to understand this di eren e between runs J, B and the other runs of thesystem, we dis overed that runs J and B involved relatively more failures on the part ofteam-members, in luding agents rashing or not responding at all. The ommuni ationspredi tions, however, were learned based on su essful runs and thus did not orre tlypredi t the ommuni ation messages that would result as the team dete ted and re overedfrom the failures. Thus the un ertainty was not alleviated, and the arbitrary sele tion wasmade among relatively many hypotheses. This explains the relatively lower a ura y of the(Coherent, Comm) te hnique in run J and B. This learly shows a limitation of the simplelearning approa h we took, and we intend to address it in future work. However, thereare other fa tors that in uen e the a ura y of the ommuni ation models, sin e this lowera ura y did not o ur in other runs where failures have o urred.The results of the Coherent, Temporal te hnique vary as well. We have been able todetermine that failures ause a relative in rease in the relative a ura y of the Coherent,Temporal te hnique. However, varian e in the results is due to additional fa tors. In run C,for instan e, this te hnique results in relatively higher a ura y, but no failure has o urred.Certainly, the mission spe i ations themselves di er between runs, ma hine loads ause themission exe ution to run slower or faster, et . The great varian e in the temporal behaviorof the system was the prin ipal reason for our using the ommuni ation predi tion. Thisvarian e is obvious in the graphs. 120\nMonitoring Teams by OverhearingIn summary, despite the varian e in the results of the Coherent, Temporal te hnique (dueto varian e in the temporal behavior of the system and the simpli ity of the temporal model),and the possible sensitivity of the Coherent, Comm te hnique to learned predi tions, it is lear that the two te hniques work well in ombination, building on the oheren e heuristi ,and ompensating for ea h other's weaknesses. In all runs, the ombined te hnique Coherent,Temporal, Comm was superior to either te hnique alone. Its performan e varied between72% a ura y (Run E) to 97% (Run I). The average a ura y a ross all runs of this all- ombination te hnique was 84%, resulting in very signi ant in reases in a ura y omparedto the initial solution with whi h we began our investigation (less than 4%), and to humannovi e performan e (see Se tion 6.3). Thus the ommuni ations predi tions need not beperfe t, and the temporal knowledge need not be pre ise, in order to be useful.6.2 Evaluating the Use of Communi ations Predi tionsOne key question about the use of the ommuni ations predi tions is their sensitivity to lossof observations. The e a y of the te hnique (see Figure 5) stems from its apability to makeinferen es based on an expe ted future observation. The predi tions used in the previousse tion assumed no observation loss, i.e., if a predi tion stated that a parti ular messagewas to be observed, than the probability assigned to this predi tion was 1.0. But in settingsinvolving lossy observation streams, su h inferen e will prove in orre t, as Overseer will wait for the observation and will therefore not orre tly monitor the a tual state of team-members.To evaluate the predi tions' sensitivity to observation loss, we hose three of the exper-imental runs, E, I, and J, whi h represent the extreme performan e results of Overseer:Run E had the lowest a ura y (72%), Run I had the highest (97%), and run J showed aninteresting reverse in relative performan e of the Coherent, Temporal and Coherent, Comm(see Figure 5). For ea h of these runs, we simulated observation loss at a rate of 10%,repeating ea h trial three times with di erent random seeds. In other words, we ran a totalof 9 trials, in whi h a random 10% of the messages to be observed by Overseer werenot observable to Overseer (though they still rea hed the eva uation team-members team-performan e was identi al to the original settings). We then set the predi tions toappropriately use 90% 10% settings: ea h expe ted message was predi ted to appear with0.9 probability (as opposed to 1.0 probability originally).The results of these experiments are presented in Figure 7. For ea h of the three di erentruns, two bars are presented. The left (shaded) bar shows the original results as presented inthe previous se tion (i.e., with no observation loss, and no treatment of possible loss in thepredi tions). The right bar shows the average a ura y a hieved by Overseer on the threetrials (for ea h run) in whi h 10% of the observations were not observable to Overseer.The error-bars on the right bar mark the minimum and maximum a ura y values a hievedin the three trials for ea h run. Run I's error-bars are unseen sin e all three trials resultedin the same a ura y.There are a number of promising on lusions that an be drawn from these results.First, in both runs E and I, Overseer's average a ura y dropped by less than 8%, i.e.,the performan e of Overseer dropped by less than the level of loss introdu ed. Indeed,in run E, in whi h the original performan e was the poorest, there was almost no hange121\nKaminka, Pynadath, & Tambe\n0\n0.2\n0.4\n0.6\n0.8\n1\nRun E Run I Run J\nA ve\nra ge\nA cc\nur ac\ny\nFigure 7: Comparison of average a ura y results with 0% and 10% observation losses.in performan e. Performan e in run J did drop by slightly more than 10%, and that anbe at least partially explained by run J's previously dis ussed failures to exploit the om-muni ations predi tions. Thus one promising on lusion to be drawn from these results isthat Overseer's performan e an degrade gra efully, at a rate omparable to the rate ofdegradation to Overseer's input.A se ond on lusion is that Overseer's performan e under observation-loss settings isfairly invariant. Again, both run E and I, whi h an be onsidered normative, show verylittle (if any) varian e from one trial to the next, despite the hange in the sele tion ofobservations to be made unobserved from one trial to the next. Even run J, whi h is nota representative of the normative runs, shows little varian e with respe t to its averagea ura y under observation loss. This result suggests that while there may be a drop inperforman e with observation loss (as expe ted), Overseer performs onsistently undervarying lossy settings.6.3 Overseer and Human Monitoring by OverhearingAnother important fa et to the evaluation of Overseer examines its performan e in om-parison to that of novi e and expert monitors of the eva uation appli ation. This evaluationsheds some light on the di ulty of the monitoring task, and demonstrates that Over-seer's performan e is omparable (sometimes higher, sometimes lower) to human expertperforman e, and signi antly better than that of novi es.To ondu t this evaluation, we examined the same three runs representatives of Over-seer's bounds on performan e dis ussed above (runs E, I, and J). The rst author of thispaper served as an expert monitor, having as mu h experien e in overhearing in the eva ua-122\nMonitoring Teams by Overhearingtion appli ation as possible (and spe i ally in the a tual test runs E, I and J)2. We estab-lished a group of novi e monitors, made up from ve subje ts who were generally familiarwith hierar hi al ontrol stru tures but unfamiliar with either monitoring by overhearing orwith the eva uation appli ation or its omponent agents. Ea h subje t was presented withprinted books (one for ea h run) ontaining the overheard messages (in human-readableform), the same messages overheard by Overseer under optimal (lossless) onditions. Asreferen e material, ea h subje t was given a opy of the plan-hierar hy, team-hierar hy, andthe same average duration information available to Overseer (the parameter for di erentleaf plans). For ea h overheard message, a se ond line of print indi ated the time passedsin e overhearing the message, and the subje t was asked to write down their best estimatefor the agents' urrent state (i.e., after the message was overheard and the spe i ed timepassed). If they felt di erent agents or di erent sub-teams had di erent states, they were tospe ify what ea h agent or subteam is doing. We emphasize that the subje ts were presentedwith exa tly the same runs on whi h Overseer was evaluated.The a tual test pro ess began with a short explanation of the task, with a full expla-nation of the plan-hierar hy (in luding answering any questions the subje ts had about thesemanti s of di erent transitions, et .), and with a short test run whi h allowed ea h sub-je t to use the plan-hierar hy and team hierar hy (but without providing any feedba k asto the subje t's a ura y). Then, on e all questions had been answered, the subje ts werepresented with the test books and were given unlimited time to omplete them, in any order(though on e started on a book, they were required to nish it). Subje ts were not allowed tolook forward at the next message before ompleting their answer, nor to go ba k and hangetheir previous answers. They were en ouraged to seek assistan e in lo ating plans (in theplan-hierar hy) onsistent with messages, but no information was provided as to whi h planmay be orre t if two or more plans were onsistent. After all subje ts nished with all runs,the test books were taken together and a s ore was omputed. Ea h orre t predi tion wasawarded a 1, in orre t predi tion a 0 same standard used in s oring Overseer's a u-ra y. A non-spe i answer (i.e., it was somewhere in the sub-tree of Fly-Flight-Plan )was onsidered to be an in orre t predi tion, as subje ts were repeatedly instru ted that aspe i answer was required.The results of the test are presented in Figure 8. As in previous graphs, the Y-axisdenotes per entage of a urate monitoring hypotheses a ross all the data-points in a run(22 45, depending on the run). The X-axis has three ategories, for the three di erent runs.For ea h run, the left bar (marked Novi es ) presents the average a ura y a hieved bythe novi e monitors, the middle bar (marked Experts ) presents the a ura y a hieved bythe human expert monitor, and the nal bar ( YOYO* ) re-prints the results presented inFigure 5 above. The results show that the average a ura y of the novi es is learly inferiorto that of the expert monitor and to Overseer. Overseer's performan e is above thatof the human expert in runs I and J. However, the human expert does mu h better thanOverseer in run E.We draw several on lusions from these results. First, the monitoring task Overseerfa ed in the eva uation appli ation is not trivial: The novi es failed to a hieve more than2. We have had to settle for one expert sin e training an expert in this task is very time onsuming andrequires mu h familiarity with the internals of the eva uation appli ation as well as the TEAMCOREar hite ture. 123\nKaminka, Pynadath, & Tambe70% on average (in their best run), and generally performed signi antly worse (by 15%and more) than a human expert. Se ond, Overseer's performan e in di erent runs was omparable to that of the human expert (sometimes better, sometimes worse). However,Overseer's performan e tended to follow the same trend as the novi es. In other words,Overseer's a ura y tended to go up and down on di erent runs in a similar mannerto that of the average novi e human monitor, while the expert's a ura y remained fairly onstant a ross all runs.\n0\n0.2\n0.4\n0.6\n0.8\n1\nI J E\nNovices Experts YOYO*\nFigure 8: A ura y of human novi e and expert monitors ompared to Overseer.6.4 Evaluating YOYO*'s Trading of Expressivity for S alabilityWe examine a key trade-o between the expressivity and e ien y involved in the plan-re ognition te hniques we have presented. From the a ura y dis ussion above, it is learthat oheren e is a useful heuristi . YOYO* takes an extreme approa h, stri tly ruling outreasoning about in oheren es. It is impossible for YOYO*, for instan e, to represent anin oheren e in whi h two team-members are in disagreement about the plan exe uted bythe ommon team. It may thus be impossible for YOYO* to expli itly represent hypothe-ses asso iated with ommuni ation losses and delays, whi h ause su h in oheren es. Anapproa h in whi h ea h individual is represented separately allows for su h representation,and in this respe t is more expressive. However, with a few failure- he ks in pla e, YOYO*is able to dete t many in oheren es, as previously dis ussed.On the other hand, YOYO* o ers signi ant omputational s alability with respe t tothe number of agents monitored. Analysis of YOYO*'s omplexity (in ontrast to the array124\nMonitoring Teams by Overhearingapproa h) was already presented in Se tion 5.2, and we follow it here with empiri al evalu-ation. Figure 9 reports on the spa e requirement of YOYO* and the array-based approa hin three di erent domains: the eva uation domain, where YOYO* has been evaluated anddeployed, and two additional domains in whi h we have built multi-agent teams ModSAF(Tambe et al., 1995; Calder, Smith, Courteman he, Mar, & Ceranowi z, 1993) and RoboCup(Tambe, Adibi, Al-Onaizan, Erdem, Kaminka, Marsella, & Muslea, 1999; Marsella, Adibi,Al-Onaizan, Kaminka, Muslea, Tallis, & Tambe, 2001). YOYO* is urrently being evaluatedin these domains, and while it has not yet been fully deployed there, we believe the partialexisting implementations are su ient to provide robust proje tions of the spa e savingsa hieved in these domains. We believe that su h proje ted savings of implementation inthese two domains ould provide a rough guide as to the savings that designers ould expe tfrom deploying YOYO* in additional domains.For ea h domain, Figure 9 ompares the spa e requirements of the array-based approa h(left bar) with those of YOYO* (right bar). In addition, the dark-shaded region on topof ea h bar shows the spa e required for representing ea h additional agent in the twoapproa hes, under the assumption that no additional plans are added to the plan-hierar hyas more agents are added. As dis ussed above, this assumption is favorable to the array-based representation. The gure shows the signi ant spa e savings a hieved by YOYO*.First, in representing the teams in their urrent size, YOYO*'s spa e requirements aresigni antly smaller. Furthermore, YOYO*'s savings really shine when we examine thes alability of the two approa hes. While the array-based approa h requires at least theamount of spa e shown in the gure as darkly-shaded area, YOYO*'s requirements growby one node with ea h additional agent. Its spa e requirements for representing additionalagents are so small, that they don't show in the gure.\n0\n300\n600\n900\nArray YOYO* Array YOYO* Array YOYO*\nEvacuation (11 Agents) RoboCup (11 Agents) ModSAF (3 Agents)\nN um\nbe r\nof N\nod es\nEach Additional Agent Current Application\nFigure 9: Empiri al savings in applying YOYO* in the eva uation and other domains.Earlier, in Se tion 5.2, we have analyzed YOYO*'s worst ase run-time omplexity, butargued that this worst ase behavior is very extreme, and annot be sustained in pra ti esin e it involves ontinuous ommuni ations among all agents, the infeasibility of whi h125\nKaminka, Pynadath, & Tambeprovided the motivation for exploring a plan-re ognition approa h. As further eviden efor the average ase, onsider the eva uation appli ation, where agents ommuni ate onaverage on e every 20 state hanges. In this appli ation, agents ommuni ate in parallel in4 or 5 ex hanges (out of dozens), but in all ases but one, su h parallel ommuni ations allreferred to the same plan, thus still requiring only a single update in YOYO* (see dis ussionin Se tion 5.2). Only on e during task exe ution would 3 agents (out of 11) be expe ted to ommuni ate in parallel about di erent plans, a s enario still di erent than YOYO*'s worst ase s enario.The average length of task exe ution in this domain is approximately 900 time-ti ks.The array approa h would update the state of ea h agent, at ea h time ti k, whether amessage would appear or not. Thus its average omplexity per time-ti k is the same asits worst- ase, whi h is at least O(MN2). For YOYO*, the average omplexity would besigni antly di erent: 899 out of 900 time-ti ks it would result in an O(M+H) pro ess, andonly one time (out of 900) it would be result in a pro ess three times as expensive (updatingthe state of 3 di erent agents). The worst ase s enario did not o ur at all in any of thedi erent runs.7. Related WorkAiello et al. (2001) present several bene ts to overhearing agent onversations. They suggestthat the overhearer may infer the intent of the agents engaged in onversations, and o erspe i suggestions for improving the agents' performan e. For instan e, overhearing a onversation between two agents about a keyword sear h on the web, the overhearer maysuggest alternative keywords to ondu t the same sear h. This work is losely related toour resear h on Overseer, and indeed points out several potential additional bene ts ofoverhearing te hnology. However, in ontrast to our work, Aiello et al. do not address theproblem of intent- or plan-re ognition. They do not present algorithms for inferring plans,nor for disambiguating re ognized plans.Overseer di ers from most previous work on plan-re ognition in being fo used onmonitoring multiple agents, not a single agent. While previous work in multi-agent planre ognition has either fo used on exploiting expli it teamwork reasoning (e.g., Tambe,1996), or expli itly reasoning about un ertainty when re ognizing multi-agent plans (e.g.,Devaney & Ram, 1998; Intille & Bobi k, 1999), a key novelty in Overseer is that ite e tively blends these two threads together. We provide a detailed dis ussion below.Like Overseer, RESCteam (Tambe, 1996) reasons expli itly about team intentionsfor inferring team plans from observations, similarly to Overseer's use of the oheren eheuristi . RESCteam uses oheren e to restri t the spa e requirements of the plan-libraryused, similarly to YOYO*. However, Overseer uses a more advan ed teamwork model(e.g., it an predi t failure states and re overy a tions), uses knowledge about pro eduresused by a team (i.e., ommuni ation de isions), and also expli itly reasons about un ertaintyand time, allowing it to answer queries related to the likelihood of urrent and future teamplans (issues not addressed in RESCteam). Indeed, RESCteam does not expli itly representordering onstraints between plans, and does not address s ar e observations: It assumesthat observations are available that a ount for possible hanges in the state of ea h of theobserved agents. 126\nMonitoring Teams by OverhearingWork su h as (Devaney & Ram, 1998; Intille & Bobi k, 1999) fo uses on expli itlyaddressing un ertainty in plan re ognition in multi-agent ontexts, but does not exploitexpli it notions of teamwork. Devaney and Ram (1998) use pattern mat hing to re ognizeteam-ta ti s in military operations. Their approa h relies on team-plan libraries, veri ed bydomain experts, that ombine the team- and plan-hierar hies; the organizational knowledgeis not expli itly represented in their te hnique. Similarly, Intille and Bobi k (1999) relyentirely on oordination onstraints among agents to re ognize team-ta ti s in football, andin this sense use a so ially-attentive te hnique that prefers hypotheses in whi h agents aremaintaining their roles. Intille and Bobi k's work uses a single stru ture for ea h di erentre ognized ta ti . Both investigations use position tra e data of the monitored human teams.Our work di ers from (Devaney & Ram, 1998; Intille & Bobi k, 1999) in several ways.First, these previous investigations have been applied in settings where observations are on-tinuously available about ea h monitored agent. In ontrast, Overseer is targeted towardsoverhearing, where limited observations are available, both in time, and in the number ofagents a tually observed. Overseer introdu es a number of novel te hniques (su h as the ommuni ations predi tions) whi h are useful in su h settings. A se ond important di er-en e is the underlying representation used in reasoning. We introdu e a novel representationparti ularly suited for monitoring by overhearing, while Intille and Bobi k rely on standardbelief networks, onstru ted in a parti ular way to support reasoning about spatial/temporal oordination. Finally, the expli it use we make of teamwork and organizational stru ture(the team-hierar hy) enables YOYO* in prin iple to reason about oordination and team-work failures, where the previous monitoring te hniques would fail to re ognize the team'sa tions (Intille & Bobi k, 1999).Huber (1996) reports on the use of probabilisti plan re ognition in servi e of observation-based oordination in the Net-trek domain, and shows that agents using plan re ognitionfor oordination outperform agents using ommuni ations for oordination. Huber takes oordination to be ooperative a tions on the part of the self- interested agents, e.g., joiningan agent in atta king a ommon enemy. Huber's work does not exploit any knowledge ofrelationships between the agents to limit the omputation or in rease the a ura y. Huber'ssystem does allow for some un ertainty aused by missing observations, but in ontrast toour work, does not introdu e spe ialized me hanisms (su h as ours) to expli itly addressthese.Plan Re ognition Bayesian Networks (PRBNs) (Charniak & Goldman, 1993) provide avery general model for plan events, eviden e, and inferen e. However, a PRBN is a stati Bayesian network, so it must in lude nodes for all plans and observations throughout theexe ution of the plans. Therefore, instead of representing only the events of a single timestep (as in the DBNs des ribed in Se tion 3.1), it must in lude nodes over all time steps.Therefore, for N agents, exe uting a plan hierar hy of size M , over a nite time horizonof T steps, the number of nodes in the network will be O(TNM2). Inferen e will have aspa e/time omplexity exponential in the number of nodes, O(2TNM2), whi h is prohibitiveover the lengths of exe ution found in our example domains (e.g., T = 900).The representation used by YOYO* is related to existing approa hes to the modelingof sto hasti pro esses, in parti ular those used for probabilisti plan re ognition. Therepresentation we present perhaps most losely resembles Hidden Markov Models (HMMs)(Rabiner, 1989), used for plan-re ognition in (Han & Veloso, 1999). One ould, in theory,127\nKaminka, Pynadath, & Tamberepresent the plan state of a team of agents within the un onstrained state spa e of anHMM. However, the HMM state spa e would have to represent all possible ombinationsof the individual plan states of the agents, so the size of the HMM state spa e would beexponential in the number of agents and plans. Thus, the standard algorithms for HMMinferen e would not be able to exploit the stru ture of the plan and team hierar hies, nor theparti ular forms of eviden e (as des ribed in Se tion 3.2), in the way that we do in YOYO*.Generalized versions of the HMMmodel (Ghahramani & Jordan, 1997; Jordan, Ghahramani,& Saul, 1997) ould more ompa tly represent the same state spa e as in YOYO*, but exa tinferen e is intra table for these models. These models have more e ient algorithms forapproximate inferen e, but these would have di ulty with the determinism present in ourplanning models.Pynadath and Wellman report on the Probabilisti State-Dependent Grammar (PSDG)model (2000) that avoids the full omplexity of DBN inferen e by making simplifying as-sumptions appropriate for plan re ognition. However, while PSDG an in orporate broader lasses of inferen e than YOYO*, it is intended for single-agent plan re ognition, and doesnot support on urren y in a general enough fashion for multi-agent plan re ognition.Goldman, Geib and Miller (1999) develop a on eptual model for Bayesian plan re ogni-tion whi h does in lude, as one of its key novelties, the ability to infer the plans of a singleagent from la k of observation of its a tion. However, Goldman et al. deal with a di erentissue altogether than the one our ommuni ations predi tions address. Their frameworklooks at a sequen e of observations, in whi h an observation may be missing, but observa-tions of a tions following it appear. Their framework then allows inferen e that plans thatshould have given rise to the missing observation an be ruled out as re ognition hypothe-ses. In ontrast, our approa h uses the ommuni ations predi tions to make inferen e ofplan-steps that did not yet o ur. Overseer probabilisti ally expe ts the predi tions to ome true, and does not infer additional information from a missing (predi ted) observationthat is followed by another. In addition, our approa h is fully implemented and deployed inmulti-agent settings, rather than single agent.A omplementary line of work (in the ontext of the TEAMCORE ar hite ture) hasfo used on intended plan-re ognition for monitoring, where team-members may adapt their ommuni ations su h that monitoring is made easier (Tambe et al., 2000). This work(i) redu ed, but did not eliminate un ertainty, and (ii) did not present any methods toaddress un ertainty, as we do here, However, it presents an interesting future dire tion forOverseer's development.8. Summary and Future WorkThis paper introdu ed monitoring by overhearing, a te hnique that will be in reasinglyimportant with the growing need to monitor agent systems, parti ularly distributed or de-ployed. We presented Overseer, a system for monitoring teams by overhearing the routine ommuni ations team-members ex hange as part of the exe ution of their joint tasks. Moni-toring by overhearing, while being a plan-re ognition task, presents hara teristi hallengesnot previously addressed. These in lude the s ar ity of observations ompared to the rateof hange in agent's state, and the fa t that agents are not individually observable, as theobservations are essentially of multi-agent a tions. In addition to these, familiar hallenges128\nMonitoring Teams by Overhearingsu h as demanding response times and maintaining performan e in fa e of a s ale-up in thenumber of monitored agents, are also present.To address these hallenges, Overseer employs a number of novel te hniques, whi hexploit knowledge of the relationships between the agents to alleviate un ertainty and in- rease e ien y of monitoring: (i) An e ient probabilisti algorithm for plan-re ognition,parti ularly suited for monitoring ommuni ations; (ii) YOYO*, an approa h for e ientmaintenan e of re ognition of oherent hypotheses; and (iii) use of so ial stru tures andpro edures, e.g., team oheren e and ommuni ations to maintain oheren e, to alleviateun ertainty. To demonstrate the generality of these te hniques, we have dis ussed the po-tential use of these te hniques with representations other than a plan-hierar hy, in parti ularDBNs (Kjærul , 1992).We provided an in-depth empiri al evaluation of these te hniques in one of the domainsin whi h Overseer is applied. The evaluation arefully examines the ontribution of ea hte hnique to the overall re ognition su ess, and demonstrates that these te hniques workbest together, as they omplement relative weaknesses of ea h other. The paper also pre-sented an evaluation of the s alability of YOYO*, and its performan e under onditions ofobservation loss. Finally, we presented a omparison of Overseer's performan e with thatof human expert and novi e monitors, and demonstrated that Overseer performan e is omparable to that of human experts, despite the di ulty of the monitoring task.Several opportunities for future resear h dire tions arise from the experimental results.First, the use of rote-learning to predi t when messages will be observed (provided as feasi-bility demonstration), proved e e tive for normative runs. However, the simple me hanismwas damaging when rare patterns of ommuni ations arose, as some of the experiments haveshown. In-depth exploration of the role of learning is therefore one of the dire tions we hopeto pursue in the future. In addition, learning me hanisms that an derive plan-hierar hyand team-hierar hy stru tures from re ords of onversations are also of mu h interest.A knowledgementsThis paper is based in part on an Agents-2001 paper by the same authors (Kaminka, Py-nadath, & Tambe, 2001). Parts of this resear h were arried out while the rst author wasa Post Do torate Fellow at the Computer S ien e Department, Carnegie Mellon University.We thank Manuela Veloso for her enthusiasti support of this proje t at Carnegie MellonUniversity, and we thank Yves Lespéran e, Vi tor Lesser, George Bekey, Je Ri kel, andDan O'Leary for useful omments. Oshra Kaminka deserves spe ial thanks for her help inanalyzing and pro essing the data. This resear h was supported by DARPA awards F30602-98-2-0108, F30602-98-2-0135, and F30602-00-2-0549, managed by the Air For e Resear hLabs/Rome site.Appendix A. Additional algorithms and proofsThis appendix ontains the pseudo- ode for all algorithms des ribed in the paper, for whi hpseudo- ode was not provided in the body of the text itself. These in lude the modi ationsto the propagation pro edures ne essary for propagation within YOYO*. In addition, we129\nKaminka, Pynadath, & Tambeprovide a proof that the number of oherent hypotheses for N agents is linear in the size ofthe plan-library M .A.1 The Number of In oherent and Coherent HypothesesLet Mi be the monitoring plan-library for agent i; 1 i N . When monitoring agent i, amonitoring system reasons about monitoring hypotheses inMi. In other words, we an viewMi as the nite set of all possible plans agent i may be exe uting. Given a query as to theagent's urrent state by the monitoring system, the plan-re ognition algorithm pi ks someki spe i members of Mi as hypotheses as to the urrent state of the agent all these setsof hypotheses mi where jmij = ki.To onstru t an overall team hypothesis, the monitoring system must ombine the indi-vidual hypotheses to form a hypothesis for the team's state. For ea h agent i, the monitoringsystem hooses one individual hypothesis hi 2 mi. The ombination of these forms the teamstate hypothesis. If there is no un ertainty about the state of any of the agent, i.e., ki = 1for all i, then one team hypothesis exists. However, if un ertainty exists about the stateagents, then learly, the pro ess of sele ting individual hypotheses be omes ombinatorialin nature, as all possible ombinations of all individual hypotheses are possible in prin iple.Let us onsider how many oherent hypotheses exist. If we restri t ourselves to oherenthypotheses, then the sele tion of individual hypotheses for ea h agent are onstrained su hthat the sele tions are in agreement the same individual hypothesis is sele ted for ea hagent. Given a sele tion of an individual state hypothesis h1 2 m1 for the rst agent, wemust hoose h2 2 m2 for the se ond agent, h3 2 m3 for the third agent, et ., su h thath1 = h2 = h3 = ::: = hN . Sin e there are not more than k1 jM1j individual statehypotheses for the rst agent, it follows that the number of oherent team-state hypothesesis bounded by jM1j, i.e., the size of the plan library for the agents. In fa t, the number of oherent hypotheses is bounded by minki sin e only members of mminki an be mat hedwith members of the other individual hypothesis sets, m. In ontrast, by de nition, all otherteam-state hypotheses are in oherent. There will be k1 k2 k3 ::: kN (min ki) of thesehypotheses.A.2 YOYO* Propagation Algorithms (Se tion 5.1)The algorithms presented in this se tion support those presented in the main text of thepaper, and are provided here for ompleteness. Some of them may ontain a step whi hiterates over all teams that an take an outgoing transition (e.g., line 1 of algorithm 6, orline 13 of algorithm 7). This step requires some further lari ation: When iterating overall outgoing teams that meet the ondition, the algorithm onsults the team-hierar hy to arry out the iteration only for the topmost teams (in terms of the team-hierar hy) thatmeet the ondition. For instan e, in our appli ation domain, the team TASK-FORCE has(among others) two subteams TRANSPORTS and ESCORTS. If a transition is allowedto be taken by TRANSPORTS only, then an iteration over all teams that are allowed totake the transition will not onsider either ESCORTS or TASK-FORCE. However, if thetransition allows TASK-FORCE, then the iteration step will take pla e only on e it willbe exe uted on e for the team TASK-FORCE, whi h is the parent team for TRANSPORTSand ESCORTS. 130\nMonitoring Teams by Overhearing Algorithm 6 Team-Propagate-Down(plan Y , probability , beliefs, b, plans M)1: for all teams T who are allowed to take an outgoing hierar hi al-de omposition transition fromY do2: CT f j 2M; rst hild of Y; is to be taken by team Tg3: 0 = j CT j4: for all plans 2 CT do5: bt+1(Y;:blo k) bt+1(Y;:blo k) + 06: Team-Propagate-Down( ; 0; b;M) Algorithm 7 Team-Propagate-Forward(team-hierar hy H, beliefs b, plans M)1: for all plans X 2M do2: bt+1(X;:blo k) 0:03: bt+1(X; blo k) 0:04: outx 0:05: x 0:06: for all plans X 2M in post-order do { hildren in temporal order before parents}7: if X is a leaf then8: outx bt(X;:blo k)(1 e x) { al ulate probability of X terminating at time t}9: else {X is a parent}10: outx is known { be ause post-order guarantees all hildren set it in line 21}11: for all temporal outgoing transitions Tx!y from X do12: x x + (1 xy) xy13: for all teams E who are allowed to take a temporal outgoing transition do14: if x > 0 then {some transition an be taken}15: for all temporal outgoing transitions Tx!y from X to be taken by E do16: outx(1 xy) xy17: if Tx!y leads to a su essor plan Y then18: bt+1(Y;:blo k) bt+1(Y;:blo k) + 19: Team-Propagate-Down(Y; ; b;M)20: else {Tx!y is a terminating transition}21: outparent(x) outparent(x) + (1 xy) xy {parent's outgoing probability is its hil-dren's}22: bt+1(X; blo k) bt+1(X; blo k) + outx x23: bt+1(X;:blo k) bt+1(X;:blo k) outx\n131\nKaminka, Pynadath, & TambeAlgorithm 8 below may require some lari ations. First, it is important to note that theplans Y (line 1) are traversed in pre-order parents before hildren. The s aling al ulationdepends on the parent having the s aled probability. Se ond, the iteration over sub-plansY essentially aptures all plans in the subtree rooted in the parent plan P , ex ept for thosein the subtree rooted by P 's hild X, whi h already has been adjusted by YOYO* prior tothe all to this algorithm. In fa t, the use of X's team T to s ale only other plans makessure that any of X's siblings, that are alternatives to X for the team T , do not get s aled.This is orre t be ause this pro edure is alled when in orporating eviden e for X (ratherthan any of its siblings).Algorithm 8 S ale(parent plan P , team T , hild plan X, beliefs b)1: for all subplans Y of P , where team(Y ) 6= T , in pre-order do2: bt+1(Y;:blo k) bt+1(Y;:blo k)+ bt(Y;:blo k)bt(parent(Y );:blo k)bt+1(parent(Y );:blo k)3: bt+1(Y; blo k) bt+1(Y; blo k)+ bt(Y;blo k)bt(parent(Y );:blo k)bt+1(parent(Y );:blo k)Referen esAiello, M., Busetta, P., Dona, A., & Sera ni, L. (2001). Ontologi al overhearing. In In-telligent Agents VIII, Pro eedings of the international workshop on Agents, Theories,Ar hite tures, and Languages (ATAL-2001).Barber, K. S., & Martin, C. E. (2001). Dynami reorganization of de ision-making groups. InPro eedings of the Fifth International Conferen e on Autonomous Agents (Agents-01),pp. 513 520. ACM Press.Calder, R. B., Smith, J. E., Courteman he, A. J., Mar, J. M. F., & Ceranowi z, A. Z. (1993).Modsaf behavior simulation and ontrol. In Pro eedings of the Third Conferen e onComputer Generated For es and Behavioral Reresentation Orlando, Florida. Institutefor Simulation and Training, University of Central Florida.Charniak, E., & Goldman, R. P. (1993). A Bayesian model of plan re ognition. Arti ialIntelligen e, 64 (1), 53 79.Cohen, P. R., Johnston, M., M Gee, D., Oviatt, S., Pittman, J., Smith, I., Chen, L., & Clow,J. (1997). Qui kset: Multimodal intera tion for distributed appli ations. In Pro eedingsof the Fifth Annual International Multimodal Conferen e (Multimedia '97), pp. 31 40.Cohen, P. R., & Levesque, H. J. (1990). Rational intera tion as the basis for ommuni a-tion. In Cohen, P. R., Morgan, J., & Polla k, M. E. (Eds.), Intentions in Commu-ni ation, Systems Development Foundation Ben hmark Series, hap. 12, pp. 221 255.MIT Press.Cohen, P. R., & Levesque, H. J. (1991). Teamwork. Nous, 35.De ker, K. (1995). Environment Centered Analysis and Design of Coordination Me hanisms.Ph.D. thesis, Department of Computer S ien e, University of Massa husetts, Amherst.Devaney, M., & Ram, A. (1998). Needles in a haysta k: Plan re ognition in large spatialdomains involving multiple agents. In Pro eedings of the Fifteenth National Conferen eon Arti ial Intelligen e (AAAI-98), pp. 942 947 Madison, WI.132\nMonitoring Teams by OverhearingDunin-Kepli z, B., & Verbrugge, R. (2001). The role of dialogue in olle tive problemsolving. In Pro eedings of fth International Symposium on the Logi al Formalizationof Commonsense Reasoning (Commonsense 2001), pp. 89 104.Finin, T., Labrou, Y., & May eld (1997). KQML as an agent ommuni ation language. InBradshaw, J. (Ed.), Software Agents. MIT Press.Ghahramani, Z., & Jordan, M. I. (1997). Fa torial hidden Markov models.Ma hine Learning,29, 245 275.Goldman, R. P., Geib, C. W., & Miller, C. A. (1999). A new model of plan re ognition.In Pro eedings of the Conferen e on Un ertainty in Arti ial Intelligen e (UAI-1999)Sto kholm, Sweden.Grosz, B. (1996). Collaborating systems. AI Magazine, 17 (2).Grosz, B. J., & Kraus, S. (1999). The evolution of SharedPlans. In Wooldridge, M., & Rao,A. (Eds.), Foundations and Theories of Rational Agen y, pp. 227 262.Grosz, B. J., & Kraus, S. (1996). Collaborative plans for omplex group a tions. Arti ialIntelligen e, 86, 269 358.Han, K., & Veloso, M. (1999). Automated robot behavior re ognition applied to roboti so - er. In Pro eedings of the IJCAI-99 Workshop on Team Behavior and Plan-Re ognition.Also appears in Pro eedings of the 9th International Symposium of Roboti s Resear h(ISSR-99).Horling, B., Benyo, B., & Lesser, V. (2001). Using self-diagnosis to adapt organizationalstru tures. In Pro eedings of the Fifth International Conferen e on Autonomous Agents(Agents-01), pp. 529 536.Huber, M. J. (1996). Plan-Based Plan Re ognition Models for the E e tive Coordination ofAgents Through Observation. Ph.D. thesis, University of Mi higan.Huber, M. J., & Hadley, T. (1997). Multiple roles, multiple teams, dynami environment:Autonomous netrek agents. In Johnson, W. L. (Ed.), Pro eedings of the First Interna-tional Conferen e on Autonomous Agents (Agents-97), pp. 332 339 Marina del Rey,CA. ACM Press.Intille, S. S., & Bobi k, A. F. (1999). A framework for re ognizing multi-agent a tion fromvisual eviden e. In Pro eedings of the Sixteenth National Conferen e on Arti ialIntelligen e (AAAI-99), pp. 518 525. AAAI Press.Jennings, N. R. (1993). Commitments and onventions: the foundations of oordination inmulti-agent systems. Knowledge Engineering Review, 8 (3), 223 250.Jennings, N. R. (1995). Controlling ooperative problem solving in industrial multi-agentsystems using joint intentions. Arti ial Intelligen e, 75 (2), 195 240.Jordan, M. I., Ghahramani, Z., & Saul, L. K. (1997). Hidden Markov de ision trees. InMozer, M. C., Jordan, M. I., & Pets he, T. (Eds.), Advan es in Neural InformationPro essing Systems, Vol. 9, p. 501. The MIT Press.Kaminka, G. A., Pynadath, D. V., & Tambe, M. (2001). Monitoring deployed agent teams.In Pro eedings of the Fifth International Conferen e on Autonomous Agents (Agents-01), pp. 308 315. 133\nKaminka, Pynadath, & TambeKaminka, G. A., & Tambe, M. (2000). Robust multi-agent teams via so ially-attentivemonitoring. Journal of Arti ial Intelligen e Resear h, 12, 105 147.Kinny, D., Ljungberg, M., Rao, A., Sonenberg, E., Tidhar, G., & Werner, E. (1992). Plannedteam a tivity. In Castelfran hi, C., & Werner, E. (Eds.), Arti ial So ial Systems,Le ture notes in AI 830, pp. 227 256. Springer Verlag, New York.Kjærul , U. (1992). A omputational s heme for reasoning in dynami probabilisti net-works. In Pro eedings of the Conferen e on Un ertainty in Arti ial Intelligen e (UAI-1992), pp. 121 129 San Mateo, CA. Morgan Kaufmann.Knoblo k, C. A., Minton, S., Ambite, J. L., Ashish, N., Modi, P. J., Muslea, I., Philpot,A. G., & Tejada, S. (1998). Modeling Web sour es for information integration. InPro eedings of the Fifteenth National Conferen e on Arti ial Intelligen e (AAAI-98).Kumar, S., & Cohen, P. R. (2000). Towards a fault-tolerant multi-agent system ar hite ture.In Pro eedings of the Fourth International Conferen e on Autonomous Agents (Agents-00), pp. 459 466 Bar elona, Spain. ACM Press.Kumar, S., Cohen, P. R., & Levesque, H. J. (2000). The adaptive agent ar hite ture: A hiev-ing fault-toleran e using persistent broker teams. In Pro eedings of the Fourth Inter-national Conferen e on Multiagent Systems (ICMAS-00), pp. 159 166 Boston, MA.IEEE Computer So iety.Lenser, S., Bru e, J., & Veloso, M. (2001). Cmpa k: A omplete software system for au-tonomous legged so er robots. In Pro eedings of the Fifth International Conferen eon Autonomous Agents (Agents-01), pp. 204 211. ACM Press.Lesh, N., Ri h, C., & Sidner, C. L. (1999). Using plan re ognition in human- omputer ol-laboration. In Pro eedings of the Seventh International Conferen e on User Modelling(UM-99) Ban , Canada.Levesque, H. J., Cohen, P. R., & Nunes, J. H. T. (1990). On a ting together. In Pro eedingsof the Eigth National Conferen e on Arti ial Intelligen e (AAAI-90) Menlo-Park,CA. AAAI Press.Marsella, C. S., Adibi, J., Al-Onaizan, Y., Kaminka, G. A., Muslea, I., Tallis, M., & Tambe,M. (2001). On being a teammate: Experien es a quired in the design of robo up teams.Journal of Autonomous Agents and Multi-Agent Systems, 4 (1 2).Martin, D. L., Cheyer, A. J., & Moran, D. B. (1999). The open agent ar hite ture: Aframework for building distributed software systems. Applied Arti ial Intelligen e,13 (1-2), 92 128.Ndumu, D. T., Nwana, H. S., Lee, L. C., & Collis, J. C. (1999). Visualizing and debuggingdistributed multi-agent systems. In Pro eedings of the Third International Conferen eon Autonomous Agents (Agents-99). ACM Press.Payne, T. R., Sy ara, K., Lewis, M., Lenox, T. L., & Hahn, S. (2000). Varying the userintera tion within multi-agent systems. In Pro eedings of the Fourth InternationalConferen e on Autonomous Agents (Agents-00), pp. 412 418.Pe hou ek, M., Marik, V., & Stepankova, O. (2000). Role of a quaintan e models in anagent-based produ tion planning. In Klus h, M., & Kers hberg, L. (Eds.), Cooperative134\nMonitoring Teams by OverhearingInformation Agents IV, Pro eedings of the Fourth International Workshop (CIA-2000),No. 1860 in LNAI, pp. 179 190. Springer Verlag.Pe hou ek, M., Marik, V., & Stepankova, O. (2001). Towards redu ing ommuni ation tra in multi-agent systems. Journal of Applied System Studies.Pynadath, D. V., & Wellman, M. P. (2000). Probabilisti state-dependent grammars for planre ognition. In Pro eedings of the Conferen e on Un ertainty in Arti ial Intelligen e(UAI-2000), pp. 507 514.Rabiner, L. R. (1989). A tutorial on Hidden Markov Models and sele ted appli ations inspee h re ognition. Pro eedings of the IEEE, 77 (2), 257 286.Reed, C. (1998). Dialogue frames in agent ommuni ations. In Pro eedings of the ThirdInternational Conferen e on Multiagent Systems (ICMAS-98), pp. 246 253.Ri h, C., & Sidner, C. L. (1997). COLLAGEN: When agents ollaborate with people.In Johnson, W. L. (Ed.), Pro eedings of the First International Conferen e on Au-tonomous Agents (Agents-97), pp. 284 291 Marina del Rey, CA. ACM Press.Tambe, M. (1996). Tra king dynami team a tivity. In Pro eedings of the National Confer-en e on Arti ial Intelligen e (AAAI).Tambe, M. (1997). Towards exible teamwork. Journal of Arti ial Intelligen e Resear h,7, 83 124.Tambe, M., Adibi, J., Al-Onaizan, Y., Erdem, A., Kaminka, G. A., Marsella, S. C., &Muslea, I. (1999). Building agent teams using an expli it teamwork model and learning.Arti ial Intelligen e, 111 (1), 215 239.Tambe, M., Johnson, W. L., Jones, R., Koss, F., Laird, J. E., Rosenbloom, P. S., & S hwamb,K. (1995). Intelligent agents for intera tive simulation environments. AI Magazine,16 (1).Tambe, M., Pynadath, D. V., Chauvat, N., Das, A., & Kaminka, G. A. (2000). Adaptiveagent integration ar hite tures for heterogeneous team members. In Pro eedings ofthe Fourth International Conferen e on Multiagent Systems (ICMAS-00), pp. 301 308Boston, MA.Tidhar, G. (1993a). Team oriented programming: Preliminary report. Te h. rep. 41, Aus-tralian Arti ial Intelligen e Institute, Melbourne, Australia.Tidhar, G. (1993b). Team oriented programming: So ial stru tures. Te h. rep. 47, AustralianArti ial Intelligen e Institute, Melbourne, Australia.Ver outer, L., Beaune, P., & Sayettat, C. (2000). Towards open distributed informationsystems by the way of a multi-agent on eption framework. In Working Notes ofthe AAAI-2000 Workshop on Agent-Oriented Information Systems (AOIS-2000), pp.29 38. 135"
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2011,
    "abstractText" : null,
    "creator" : "dvips(k) 5.86 Copyright 1999 Radical Eye Software"
  }
}