{
  "name" : "1701.05681.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Git Blame Who?: Stylistic Authorship Attribution of Small, Incomplete Source Code Fragments",
    "authors" : [ "Edwin Dauber", "Aylin Caliskan", "Richard Harang" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "We propose a technique for authorship attribution of contributor accounts containing small source code samples, such as those that can be obtained from version control systems or other direct comparison of sequential versions. We show that while application of previous methods to individual small source code samples yields an accuracy of about 73% for 106 programmers as a baseline, by ensembling and averaging the classification probabilities of a sufficiently large set of samples belonging to the same author we achieve 99% accuracy for assigning the set of samples to the correct author. Through these results, we demonstrate that attribution is an important threat to privacy for programmers even in real-world collaborative environments such as GitHub. Additionally, we propose the use of calibration curves to identify samples by unknown and previously unencountered authors in the open world setting. We show that we can also use these calibration curves in the case that we do not have linking information and thus are forced to classify individual samples directly. This is because the calibration curves allow us to identify which samples are more likely to have been correctly attributed. Using such a curve can help an analyst choose a cut-off point which will prevent most misclassifications, at the cost of causing the rejection of some of the more dubious correct attributions."
    }, {
      "heading" : "1. Introduction",
      "text" : "Many employees of large companies have clauses in their contracts which claim that all of their related work is the intellectual property of the company. For programmers, this means any source code produced during their employment could be claimed by the company. These programmers are then contractually required to get explicit permission to contribute to open source projects, and failing to do so can result in termination. Recently, the legality and reality of this situation has been a topic of discussion and debate on Twitter [29]. But is it really possible for an employer to determine if anonymous open source contributions were\nmade by one of their employees? Past work has shown that files written by a single individual are attributable, but most open source projects, like most professional code, is written and revised collaboratively.\nAssuming a known set of suspect programmers, such as the employees of a company, and some form of segmentation and grouping by authorship, such as accounts on a version control system, we present a technique which performs stylistic authorship attribution of a collection of partial source code samples written by the same programmer with up to 99% accuracy for a set of 106 suspect programmers. By contrast, the current state-of-the-art technique for source code attribution achieves an individual accuracy of at most 73% under standard circumstances. We also present a technique using the classifier’s output probability, or confidence, to address the open world problem in which the true programmer could be someone outside of the suspect set. We construct calibration curves to indicate the accuracy for collections which were attributed with given confidence, and analysts can use these curves to set a threshold below which to more carefully examine authorship due to higher probability of being outside of the suspect set [21]. We then proceed to show how this calibration curve can be used at the level of individual samples to mitigate the cost of misattribution in the absense of grouping of samples. In this scenario, the threshold will need to be set higher to catch most misattributions, and will cause a larger percentage of correctly attributed samples to require review.\nThis work is valuable standalone, but even more important as validation that source code authorship segmentation is possible. Segmentation naturally exists with version control on public repositories, and there are real world use scenarios for performing authorship attribution on accounts on such repositories. For collaboratively written code which is not managed on such repositories, we present our high accuracy as evidence that it is reasonable to expect that segmentation at a reasonable level is possible.\nPrevious work has attributed authorship to whole code files collected either from small suspect sets or from datasets which are near to laboratory condition, such as single authored code submitted to the Google Code Jam [2], [6]. While there are real-world applications for this work, it ignores the difficult properties of code written “in the wild”. By contrast, we examine small samples of real world source code collected from the online collaborative platform\nar X\niv :1\n70 1.\n05 68\n1v 2\n[ cs\n.L G\n] 1\n6 M\nar 2\n01 7\nGitHub and show high accuracy attributing the author from a suspect set of over 100 programmers [1]. This addresses known weaknesses in past work1 and opens new possibilities for future work. We note that the samples we work with can be as small as a single line of code, and so can contain very little information with which to determine authorship. We also present a technique to handle the open world problem, which has been minimally explored in previous work.\nIn this paper, we demonstrate attribution under various scenarios. The easiest scenario, which we refer to as multiple sample attribution, features the closed world assumption, under which we know that the author is among our suspects, and the version control assumption, under which we assume that the multi-authored code file is segmented and those segments are grouped by (unknown) author. Then we remove the closed world assumption and address the open world version of multiple sample attribution. We also show results with a relaxed version of the version control assumption which only assumes segmentation, and not grouping, for comparison purposes and to demonstrate the usefulness of the calibration curve. We refer to this version of the problem as single sample attribution, and observe it both under the closed world assumption and in the open world.\nIn this paper, we experiment with several subsets of GitHub data. Our first subset contains 15 programmers, and with this dataset we can perform single sample attribution with accuracy near 70% with a calibration curve identifying confidence levels which are highly accurate. We can also achieve nearly 97% accuracy with groups of 5 samples and 100% accuracy with groups of 35 or more samples for multiple sample attribution. Our second main subset contains 106 programmers, and with this dataset we can perform single sample attribution with 70% accuracy and multiple sample attribution with accuracy over 95% for even pairs of samples, and 99% for groups of at least 15 samples. Further, we show that even weak classifiers which achieve only 38% accuracy for single sample attribution can achieve over 90% accuracy for multiple sample attribution with sets of at least 10 samples."
    }, {
      "heading" : "2. Related Work",
      "text" : "We observe two primary categories of related work. We draw on past work in the area of source code authorship attribution, and then we look at some critical work in the area of plaintext authorship attribution. While the two domains have evolved different feature sets and classification techniques, recent work in text authorship attribution is closely related to our work."
    }, {
      "heading" : "2.1. Source Code Authorship Attribution",
      "text" : "An important overall note about related work is that we are the first to attempt attribution of short, incomplete, and typically uncompilable code samples. To our knowlege,\n1. “I will believe that code stylometry works when it can be shown to work on big github commit histories instead of GCJ dataset” [12]\nall past attempts at source code authorship attribution have worked with complete code samples which solve some problem, while we work with small building blocks of code which are often not a complete solution to anything.\nThe most critical piece of prior research related to our work is the work of Caliskan-Islam et al. using random forests to perform authorship attribution of Google Code Jam submissions, with features extracted from the abstract syntax tree [9]. We use their feature set and classification method as the base for our work, but rather than look at complete source code files we look at small pieces of code that have been attributed to individual authors by use of git blame. As a result of focusing on small segments of code, our feature vectors are much more sparse and we are unable to prune the feature set through use of information gain as they did. Their work also looks at the open world problem, and uses classification confidence to set a threshold below which to reject classifications. Further, their work is with data from Google Code Jam, which as a programming competition creates laboratory conditions for the data, while our work is on data from GitHub, giving us real world conditions. We demonstrate that their techniques can be adapted to handle more difficult attribution tasks which are of interest in real world situations and have not been previously examined.\nThere are many other proposed methods and feature sets for source code de-anonymization, but those methods had worse accuracy and smaller suspect sets. Therefore, while combining these techniques may allow us to boost accuracy, for the purposes of this work we do not consider them further. Frantzeskou et al. used byte level n-grams to achieve high accuracy with small suspect sets [13], [14], [15]. The use of abstract syntax trees for authorship attribution was pioneered by Pellin and used on pairs of Java programs in order to ensure that the studied programs had the same functionality [23]. Ding and Samadzadeh studied a set of 46 programmers and Java using statistical methods [10]. MacDonnel et al. analyzed C++ code from a set of 7 professional programmers using neural networks, multiple discriminant analysis, and case-based reasoning [19]. Burrows et al. proposed techniques that achieved high accuracy for small suspect sets, but had poor scalability [6], [7], [8]\nSpafford and Weeber were among the first to suggest performing authorship attribution on source code [25]. However, while they proposed some features to do so, they did not propose an automated method nor a case study. Hayes and Offutt performed a manual statistical analysis of 5 professional programmers and 15 graduate students, and found that programmers do have distinguishable styles which they use consistently [16].\nFor our ground truth we use git blame to assign authorship to individual lines of code. Git blame is a heuristic which attributes code to the last author to modify that code. Meng et al. proposed a tool called git-author which assigns weighted values to contributions in order to better represent the evolution of a line of code [20]. This tool creates a repository graph with commits as nodes and development dependencies as edges, and then defines strucutral author-\nship and weighted authorship for each line of code, where structural authorship is a subgraph of the respository graph with respect to that line of code and weighted authorship is a vector of programmer contributions derived from structural authorship."
    }, {
      "heading" : "2.2. Text Authorship Attribution",
      "text" : "The primary piece of related research in the domain of text authorship attribution is the work by Overdorf and Greenstadt in cross-domain authorship attribution [22]. This work links authorship between blogs, tweets, and reddit comments. This work is related to ours in two primary ways. First, and most obviously, they work with short text in the forms of tweets and reddit comments. For these domains, they use a technique of merging text before extracting features. We propose a similar merging technique, although we merge after extracting features rather than before. More significantly, they also use a method of averaging the probabilities for multiple samples to classify collections of samples. We demonstrate that this technique is similarly applicable in the source code domain, and that we get excellent results even with averaging a small number of samples. Additionaly, they make no effort to classify individual tweets, while we successfully classify samples of code as short as a few lines.\nOur work is also related to research in the area of text segmentation. Akiva and Koppel proposed an unsupervised method to identify the author of individual sentences in an unsegmented multi-authored document, as a generalization of previous work by Koppel et al. on decomposition of artificially combined biblical books [4], [17]. This work is related not only to our future goal of performing source code segmentation, but also in terms of scale of classification. In their technique, they cluster chunks and then classify sentences, with a processing step using stronger classifications to adjust weaker classifications. Since we assume we already have segmentation, our work is most related to the classification step. We classify small segments of source code and then use other segments to improve our accuracy. However, their technique for supporting classification is a sequential gap-filling technique while we aggregate classification predictions.\nAnother segmentation technique is a sliding window technique to divide a document into sections by individual authors proposed by Fifield et al. [11]. This technique relies on multiple clustering passes over the text, where each clustered document is of reasonable length. Our individual pieces are on a smaller scale than theirs, and we do not need to perform multiple evaluations per sample to attribute them. We mention this work because the goal of the work is related to ours, even though the technique is very different.\nBecause we are interested in the open world problem, we had to look beyond those works to the work of Stolerman et al. [26]. They introduce a method called Classify-Verify which augments classification with verification. Authorship verification is the problem of determining if a document D was written by an author A. Among the verification methods\nthey consider is classifier confidence. We apply the same intuition to source code to determine which classifications to trust and which to reject."
    }, {
      "heading" : "3. Methodology",
      "text" : "Our method is summarized in Figure 1. We begin by collecting C++ repositories on GitHub, and then breaking collaborative files from those repositories into smaller pieces using git blame. The details of this are described in Section 3.2. For each individual piece of code, we extract the abstract syntax tree and use that to extract a feature vector, as described in Section 3.3. We then proceed to perform attribution of each sample using a random forest as described in Section 3.4. Then we average the classifier probability of linked samples as described in Section 3.5 and construct a calibration curve as described in Section 3.6."
    }, {
      "heading" : "3.1. Problem Statement",
      "text" : "In this paper we consider ourselves to be in the role of the analyst attempting to attribute source code and break anonymity. We assume that the collaboratively written code we are examining has been pre-segmented by author. This segmentation may be from version control commits, git blame, or some other method of decompositon; we only assume that we have small samples which can be reasonably attributed to a single individual. We also assume that we have training data which consists of similarly segmented code samples by our suspect programmers, rather than full files. Note that this later assumption does not particularly limit us in practical application because we can artificially segment a single authored file if necessary.\nIn our primary case, we assume that our segmented samples are linked by the segmentation method. Through version control methods this would correspond to accounts, while through other methods this may correspond to clusters generated through a clustering algorithm. We refer to this case as multiple sample attribution or as account attribution.\nFormally, we have a set of source code samples D which was written by an unknown author A and a set of n suspects A1 . . . An, and for each suspect Ai we have a set of samples Di. We then want to correctly attribute D to an Ai. Where not otherwise stated, we assume the closed world, in which one of the n suspects is the true author.\nWhile we believe that most forms of segmentation naturally lead to linking, we acknowledge that by presenting a technique based on linking we may create the assumption that to defend against it one only needs to contribute in a way which cannot be linked, such as through guest “accounts” or throwaway accounts. Therefore, we not only evaluate the baseline where the cardinallity of our code sample sets is always 1, but also present ways analysts can interpret the results to compensate for the lower accuracy. We refer to this scenario as single sample attribution."
    }, {
      "heading" : "3.2. Data Preparation",
      "text" : "We collected data from public C++ repositories on GitHub. We collected repositories which list C++ as the primary language, starting from 14 seed contributors and spidering through their collaborators. Doing this, we collected data from 1649 repositories and 1178 programmers, although in future processing steps we found that many of these programmers and repositories had insufficient data. Additionally, some of these programmers were renames or group accounts, while some repositories included text other than C++ code which had to be discounted. After eliminating those and setting the threshold to at least 150 samples per author with at least 1 line of actual code (not whitespace or comments), we were left with 106 programmers. We note that this threshold was chosen with an experimental mindset to ensure that we had sufficient data for both training and testing sets.\nWe then used git blame on each line of code, and for each set of consecutive lines blamed to the same programmer we encapsulated those lines in a dummy main function and extracted features from the abstract syntax tree as in [9]. However, unlike in [9] we cannot use information gain to prune the feature set due to having extremely sparse feature vectors and therefore very few features with individual information gain.\nWe then removed all samples which occured multiple times and pruned the data set to an equal number of samples per author. Our overall dataset included 106 programmers, each with at least 150 samples. For some experiments we also used a smaller dataset with 15 programmers with 385 samples each, from which we also used subsets with 150 samples each and 250 samples each.\nWe then constructed specific subsets of the 106 author dataset. The first such dataset contains 90 samples of at least 3 lines of code each for 96 programmers. The other contains 90 programmers with variation in the number of samples and minimum lines of code, and will be discussed in more detail in the results section.\nWhile we acknowledge that the ground truth for git blame is weaker than for commits, we chose to use git blame for three primary reasons. Firstly, commits can include deletions and syntactically invalid constructs, both of\nwhich introduce complications in data extraction. Secondly, gathering data from git blame is faster when attempting to determine authorship of parts of chosen files, while collecting commits would be faster for attempting to determine the authorship of an individual account. Thirdly, we believe the results of git blame are closer to the results we would achieve if we were to use some technique to segment code which is not version controlled on a publicly accessible repositiory.\nWe took the 15 programmer dataset and extracted detailed corpus statistics. Table 1 shows the count and percentage of samples with various numbers of lines of code, excluding the dummy main framing. We note that while the large samples are of similar size to some full files, none of the samples are complete files, and that half of the samples with over 100 lines of code belong to the same programmer."
    }, {
      "heading" : "3.3. Features",
      "text" : "In this work we use a feature set derived from the work of Caliskan-Islam et al. [9]. Our primary features come from the abstract syntax tree and include nodes and node bigrams [3]. The abstract syntax tree, or AST, is a tree representation of source code with nodes representing syntactic constructs in the code, and we used the fuzzy parser joern to extract them [28]. This parsing allows generating the AST without a complete build and without the complete build environment. Thus, it allows us to extract the AST even for our partial code samples. We also include word unigrams, api symbols,\nand keywords. Our feature set includes both raw and TFIDF versions of many of our features. TFIDF is a measure combining the raw frequency with a measure of how many authors use the feature. Due to the sparsity of the feature set, we do not use information gain to prune the set, and instead keep the entire feature set, minus any features which are constant across all samples [24]. Information gain is an entropy based measure of the usefullness of a feature for spliting data into classes by itself, and because of the sparsity of our feature vectors is zero for most features. For the 15 programmer dataset, we have 62,561 features, of which for any given sample an average of 62,471 are zero-valued, and the 106 programmer dataset has 451,368 features."
    }, {
      "heading" : "3.4. Single Sample Attribution",
      "text" : "For this case, we assume that we have no information about the authorship of the samples we are attempting to classify. As a use case, we can imagine the case where the sample is git blamed to an anonymous user rather than to an account, or when a cautious individual is creating a new account for every commit. Therefore, we can only classify at the level of the individual sample. For this, we perform cross-validation with random forests, as in [9]. Random forests are ensemble multi-class classifiers which combine multiple decision trees which vote on which class to assign to an instance [5]. This serves as a baseline for our work.\nIdeally, we would then continue as in Section 3.5 for multiple sample attribution. However, in the event that we cannot, we apply a technique to help analysts better interpret the results. Because we suspect that some samples will remain which are difficult if not impossible to classify, we want to know at what level of confidence, or the output probability for the selected class, we can accept a prediction. Therefore, we create a calibration curve for our classifier. We bin the samples based on the highest classifier probability output in increments of 10%, and report the accuracy for samples in each interval. For random forests, the output probabilities refer to the percentage of trees which vote for the given class. While we acknowledge that the specifics of such a curve may vary for different instances of the problem and we recommend using cross-validation with known samples to prepare such a curve in order to identify a threshold for accepting a prediction based on the stakes, we expect that the overall shape of the calibration curve will remain similar for different datasets, and so ours can be used as a guide."
    }, {
      "heading" : "3.5. Multiple Sample Attribution",
      "text" : "For this case, we assume that we were able to group the samples as in an account and want to identify the owner. Therefore, we can leverage the group of samples to identify the author of all of them. We identified two main ways to do this. These experiments have as parameters the number of combined samples and the number of cross-validation folds,\nand we ensured that product of the two was always either the total number of samples per author or a divisor of it.\nOur first method is sample merging. For this, we added the feature vectors of samples together. We tried merging both in the extracted (alphabetical by sample name) order and in random order. By merging in the extracted order, we maintain code locality, while merging in the random order spread code throughout merged groups. We refer to merging maintaining locality as ordered merge and merging dispersing locality as random merge. For this method we used three experimental setups. In the first, we combined samples in both our known training set and our testing set. In the second, we only combined our training samples, and in the third we only combined our testing samples.\nOur second proposed method is also our preferred method. This method does not involve any adjustment to the feature vectors. Instead, it requires performing the same classification as for single samples but then aggregating results for the samples that we would have merged by our other methods. We aggregate the probability distribution output of the classifier rather than the predicted classes, and then take as the prediction for the aggregated samples the class with the highest averaged probability."
    }, {
      "heading" : "3.6. The Open World",
      "text" : "For the open world, we propose a variation of the calibration curves described in Section 3.4. We perform attribution as normal according to either single sample attribution or account attribution, and our goal is to use a threshold to separate samples by unknown authors which were attributed to a suspect due to the mechanics of the classifier from samples correctly attributed to one of our suspects.\nFor the purposes of our experimentation, we used our 15 programmer dataset. We performed an initial proof-ofconcept experiment using a small set of unknown authored samples and 250 samples of training data per known programmer. Following that, we performed experiments in the following way. We divided our 15 programmer dataset into three disjoint subsets of five programmers. We performed three rounds of experiments, and for each round we took one of the subsets as the set of unknown authors U . Then we performed 11-fold cross-validation on the remaining data, adding all of documents from U to the evaluation set. We then binned the samples as in our calibration curves, with each bin maintaining counts of correct attributions, incorrect attributions of samples belonging to authors not in U , and samples belonging to authors in U , which we refer to as being “out of world”. We note that devising our experiments in this way allowed us to heavily bias our evaluation set in favor of samples by programmers outside of our suspect set, analyzing 317,625 out of world samples and 57750 “in world” samples between all rounds.\nWe then evaluated thresholds at the lower bound of each bin in terms of precision and recall, but rather than calculate the precision and recall of the classifier itself we computed precision and recall with respect to the three classification\ncounts maintained by the bins, using the threshold as the selector. Precision is a measure of the percentage of selected instances which belong to the desired category while recall is a measure of the percentage of instances belonging to the desired category which were selected. We calculated precision and recall according to the following three criteria: correct classifications above the threshold, out of world samples below the threshold, and samples which are either out of world or classified incorrectly below the threshold.\nFor real world applications, we suggest using a similar approach prior to introducing the actual samples of interest. We then suggest setting a threshold based on these values and the needs of the application. For the purposes of this paper, however, we calculate the F1 score for each of the three criteria and use this to predict what may be useful thresholds, and to evaluate the power of the technique. F1 scores are calculated according to the formula 2 ∗ (precision ∗ recall)/(precision + recall), and are a harmonic average of the two values."
    }, {
      "heading" : "3.7. Dataset Size Factors",
      "text" : "It is well established that the amount of training data available and the number of suspects have an important effect on classification accuracy. In our problem, we have two ways of varying the amount of training data available. One way is to vary the minimum number of lines of code per sample, and the other is to vary the number of samples used per programmer. To observe these effects, we performed single sample attribution on numerous different subsets of our larger dataset, obtained via stratified random sampling by author, and experimented with different levels of merging for given subsets.\nTo furhter examine the effects of dataset size, we took the 15 author dataset and limited to only samples which are one line of code long. We then vary the number of samples per author from 10 to 130 in increments of 10 and use 10- fold cross-validation."
    }, {
      "heading" : "3.8. Special Case Attribution Tasks",
      "text" : "In addition to the standard attribution task, sometimes we want to solve special cases. In particular, we consider the two tasks. The first task is a two-class attribution scenario, in which we have two people who claim authorship and we want to determine which wrote the code. The other task is verification, in which we have one suspect and want to know whether or not that person wrote the code. For both of these, we use our 15 programmer dataset. For the twoclass scenario, we check each pair of programmers for single sample attribution. For verification, for each programmer we create another “author” from samples from the remaining 14. We then perform a two-class single sample attribution classification task between the single author and the collected other 14. For both experiments, we used 11-fold cross-validation."
    }, {
      "heading" : "3.9. Ground Truth Considerations",
      "text" : "We acknowledge that ground truth in the GitHub environment is not perfect. First, we know that it is possible that code is copied from other sources, and therefore is not the original contribution of the credited author. Furthermore, using git blame at the line level indicates the last author to touch the line, not the original author of all of the parts of the line. We note that despite the inherent “messiness” of the data, it is important to evaluate on data collected from the wild which well reflects realistic application of our techniques. Therefore, we devised an experiment to try to quantify the contribution of inaccurate ground truth.\nFor this experiment, we perform cross validation on Google Code Jam data as is used by [9] with 250 authors and 9 files each. We then set a parameter m for number of ground truth corruptions to perform. Each corruption is an author swap between two files. We use Google Code Jam data because it gives us more reliable ground truth, and so we can control for the quality of the ground truth, while if we had used GitHub data we would not know the amount of corruption already present. The results are described in Section 4.6."
    }, {
      "heading" : "4. Results",
      "text" : ""
    }, {
      "heading" : "4.1. Single Sample Attribution",
      "text" : "From the 106 programmer dataset, our baseline single sample attribution accuracy was 73% for 500 trees. We note that while this is much lower than the accuracies reported by [9], the data itself is very different. The work of [9] attributes whole source code files written privately with an average of 70 lines of code per file, while our work attributes pieces of files written publicly and collaboratively with an average of 4.9 lines of code per file. Intuitively, it is reasonable to believe that our dataset contains samples which are much harder to classify than those found in previous datasets.\nFigure 2 shows the calibration curve constructed from this experiment. The calibration curve shows that our classifier is conservative: the predicted probability is lower than the actual accuracy obtained (a known feature of random forests). Even when the classifier confidence is less than 10% we still do better than random chance. More interestingly, it shows that with even 40% classifier confidence we have nearly 90% accuracy and with 50% classifier confidence we have nearly 95% accuracy among samples with at least that confidence. Thus, depending on the consequences of being wrong, we could choose an appropriate bound, such as 40% or 50%, as the classification confidence threshold. While this will cause us to lose correct classifications, in real settings false negatives are often less harmful than false positives, and the attributions of samples below the threshold can still be useful in guiding further investigation. Furthermore, even if we cannot accept an attribution due to low confidence, it still gives us a starting point for further investigation.\nWith the 15 programmer dataset we performed crossvalidation with 5, 7, 11, 35, 55, and 77 folds, and our baseline results ranged from 70% accuracy for 5 folds and 71% accuracy for 77 folds. Because of this limited variability, we conclude that the number of folds is not particularly important for single sample attribution as long as we maintain sufficient training data.\nFigure 3 shows a calibration curve created with the 15 programmer dataset and 11 fold cross-validation. On this run, our overall accuracy was 69%. This calibration curve was similar to the calibration curve generated from the 106 programmer dataset, suggesting that calibration curves for different problems will be similar. Therefore, we can make general recommendations to use either 40% or 50% confidence as a threshold to protect against negative consequences of mis-attribution.\nWe also counted the number of samples which fell into\neach confidence interval for both data sets. Figure 4 shows the percentages of samples which fall in each interval. From this, we see that for the 106 programmer dataset, most of the samples fall in the lower confidence intervals, which explains why our accuracy is only 73% while most intervals have accuracy over 90%. For the 15 programmer dataset we can see that the curve is biased in favor of samples in the range of 10% to 30% confidence, which explains the fact that although most of the intervals have high accuracy our overall accuracy is only 69%. We note that the interval from 0% confidence to 10% confidence does not actually contain 0 samples, but has less than 0.1% of the samples.\nOur calibration analysis suggests that our low overall accuracy compared to previous results on full source code files from Google Code Jam is likely in part a result of our dataset containing data which is difficult to classify. Detailed manual analysis would be required to determine if the data is difficult to classify because it is trivial, copied from somewhere else, or mislabeled due to reliance on git blame for ground truth.\nWe performed a preliminary analysis, and we noticed some characterstics shared by many, although not all, of the misclassificatons. Many of the misclassified samples were trivial, and contained only very basic programming structures. The majority of the misclassified instances had only a few abstract syntax tree nodes per line of code, with many of the longer samples averaging less than one node per line of code. 57.4% of the misclassified samples had only 1 line of code, and 43.4% of the of the samples with only 1 line of code were misclassified. The average length of misclassified samples was 3.7 lines of code, while correctly classified samples were 5.7 lines of code long on average. This means that many of our misclassified samples have only a few abstract syntax tree nodes and most of the information comes from the specific word unigrams which make up the code. As we know from [9], word unigrams provide less information than abstract syntax tree nodes. Therefore, it is to be expected that samples for which most of the already\nsmall amount of information comes from word unigrams rather than from abstract syntax tree nodes would prove more difficult to classify.\nThe calibration curve also suggests why we do so well with our classification result aggregation. Because our classifier is so conservative, our misclassificatons tend to have very even spreads of low probabilities compared to our correct classifications, which means that they do not easily outweigh the probabilities of correctly classified instances. This leads to the whole group being correctly classified."
    }, {
      "heading" : "4.2. Multiple Sample Attribution",
      "text" : "For speed of experiments, we started with our 15 programmer dataset and then repeated the most successful experiment with our larger datasets.\nFirst we used our 15 programmer dataset and attempted attribution between merged samples and individual samples. For these experiments, we kept 15 samples per programmer as individual samples and merged the rest into 9 merged samples of 15 samples each. Because we were comparing with individual samples, when we merged samples we normalized them by averaging features rather than adding.\nTable 2 shows the results of this experiment. These results show that averaging both training and testing samples is best, and that averaging either the training samples or the test samples without averaging the other worsens the results.\nFigure 5 shows the accuracies for our 15 programmer dataset using the various merging methods. Merging in order yields some improvement over classifying individual samples, but our accuracy is greatly improved by merging randomly. But the best technique is to combine classification results rather than samples. However, we note that merging the samples is faster than aggregating the results and has lower memory requirements, so at large scale might be preferable.\nWe note that a large source of the difference between ordered merging and random merging is the locality of code samples, and that in this respect aggregation is more like random merging. On one hand, reducing locality with random merging could cause some functionality specific code to be spread across merged samples and inflate our accuracy, but on the other hand better spreading the samples means that each collection of samples is more representative of the overall style. Furthermore, functionality specific code is an acknowledged problem in researching source code\nauthorship attribution, and is the reason most experiments conducted under laboratory conditions require that programmers have the same set of functionalities in the code.\nFrom these aggregation results, we conclude that the quality of the grouping of samples is important to the accuracy improvement, and that while merging samples is effective it is better to keep the variation in the training sets and only combine the classification samples after performing classification. We also notice that all of the accuracies are better than the baseline accuracy for single sample classification. Therefore, we recommend using the strategy of training on small samples from the suspects and then using the classification aggregation strategy on the samples belonging to the same account. This is the strategy we continue to use for the remaining experiments.\nFigure 6 shows the results of varying the number of training samples and aggregated result classification samples. These results show that accuracies of about 90% can be obtained with a minimum of about 10 training samples if there are at least about 50 aggregated samples. However, using more than 60 training samples has dramatically diminishing returns, and aggregating more than about 20 samples also has dramatically diminishing returns. We see that having a minimal number of training samples is essential, but once we pass that we can improve accuracy either by adding more training samples or by aggregating more classification results.\nBecause we concluded that aggregating the classification results is superior to merging samples, we focused on result aggregation for larger scale experiments. Figure 7 shows the results for varying numbers of aggregated results from our 96 programmer dataset. We note two important observations. First, even aggregating the results of two classifications gives us a dramatic increase of accuracy, allowing us to go from 75% accuracy to 95% accuracy. Second, increasing the number of aggregated results gives a boost in accuracy with diminishing returns as the number of samples increases.\nFigure 8 shows the results for varying numbers of aggregated results from our 106 programmer dataset. Classi-\nfication with our standard parameters including 500 trees in the random forest with unlimited depth and using all available training data takes approximately 20 hours on a 32 core 240GB RAM machine. Here we see that we can get 70% accuracy for single sample attribution and reach 95% accuracy for pairs of samples and 99% accuracy for sets of 15 samples.\nIn order to perform a faster classification on this large dataset, we also trained a random forest with parameters that are known to result in lower accuracies. For example, the number of trees is 50 with a maximum allowed depth of 50, while using 50% of the folds for training instead of 100%. Figure 9 shows the results from this\nexperiment. We obtain classification results within a few minutes with these settings which result in 38% accuracy. In this experiment, we observe that averaging classification probability distributions improves classification significantly even when using a suboptimal classifier. Averaging only two classification distributions increases accuracy from 38% to 77%. Furthermore, by aggregating 50 samples we are still able to achieve 99% accuracy."
    }, {
      "heading" : "4.3. The Open World",
      "text" : "Figure 10 shows the results of our initial open world experiment. In this experiment, we used the 15 programmer dataset as our suspects and trained a model on these programmers. We then tested this model on the remaining programmers in our larger dataset to simulate an open world experiment. Our results suggest that the calibration curve method is a viable way to address the open world problem. We notice that for our dataset, samples which do not belong\nto the suspect set usually have classification confidence below 20% and the highest such confidence is 23%, while most incorrect classifications occur with confidence below 40%. Because we have a similar calibration curve for 106 programmers, we expect that we would have similarly useful thresholds for that expanded dataset as well as for other datasets.\nWe also notice that the percentiles tend to match up with each other between classification results. For example, we observe that the best 75% of the correct classifications have confidence greater than the worst 75% of incorrect classifications. We also see that the best 50% of the correct classifications have confidence greater than all but the outliers among the incorrect classifications. Our results suggest that misclassificatons due to the open world scenario are similar to general misclassificatons with respect to classification confidence with an even lower confidence threshold and that an open world can be handled by discarding such low confidence predictions, which we would likely already discard or handle skeptically based on our calibration curve (see Figure 3). However, our remaining experiments show that this is likely a quirk of the specific match of supsects authors and unknown authors.\nFigure 11 shows the results of our open world experiments using 10 suspect programmers and 5 unknown programmers for the single sample attribution case. We note that the overall accuracy, ignoring the out of world samples, is 68.3%. While we consider selecting a threshold to be application specific because both the importance and values of the precision and recall measures may vary, when we calculated the F1 scores we determined that, depending on whether identifying correctly predicted samples or out of world samples is more important, the ideal threshold is\nlikely either 60% or 70% confidence. At 60% confidence, we have an F1 score of .503 for correctly attributed instances above the threshold, with higher precision than recall. At 70% confidence we have F1 score of .927 for out of world samples below the threshold and .958 for either out of world or incorrectly attributed samples below the threshold, with recall higher than precision for both measures. As we increase the threshold the precision for correct attributions rises sharply because as the confidence increases it becomes much more likely that the attribution was correct. However, the recall for correct attributions falls sharply because we have many samples which are attributed with low confidence, and once we reach about 30% confidence we start having many correct attributions among them. For the out of world and incorrect samples, we notice that precision is consistently high but falls slowly. When combined with the fact that the recall rises sharply early before leveling off, this suggests that we quickly identify the majority of the out of world and incorrect samples while discarding relatively few correctly attributed samples, and so once we go beyond a threshold of about 40% confidence we are mostly losing correct attributions and not identifying out of world or incorrect attributions. Taken together, this reinforces what we noticed previously in Figure 10: correct attributions have a different, although overlapping, confidence distribution from incorrect attributions and out of world samples.\nFigure 12 show the results of our open world experiments with 10 suspect programmers and 5 unknown programmers for the multiple sample attribution case for collections of 7 samples. We note that the overall accuracy, ignoring the out of world samples, is 98.0%. The optimal F1 score for correctly attributed instances above the threshold is 30%, with F1 score of .691 respectively. The optimal F1 score for the other two measures is for the threshold of 40%. For collections of 7 samples, those optimal scores are .941\nand .943 respectively. In all of those cases, recall is higher than precision. We also experimented with collections of 5 samples, with the same trends but slightly lower accuracy and F1 scores.\nFigure 13 shows an ROC curve for the task of identifying false attributions using a threshold. This analysis can help evaluate acceptable trade-offs, which can then assist in choosing a threshold. From these experiments, we can conclude several things. First, while there are cases in which out of world samples may be mistaken as belonging to one of the suspect programmers by using this techique, these cases are rare. In these experiments, as we increased the threshold we started with a few dramatic cuts to the percentage of out of world samples above the the threshold, correctly identifying over 90% of such samples in only a few increments for even the harder problem of single sample attribution and then continuing to identify about 97% and then over 99% in the next few increments. In all of these experiments, while raising the threshold causes us to doubt correct classifications, it allows us to correctly identify incorrect classifications and out of world samples more quickly. While we observe that our technique cannot completely separate correctly classified samples from either incorrectly attributed samples or out of world samples, we can use it to easily find trade-offs that allow us to identify most out of world samples and to trust our remaining attributions to a high degree. It is also notable that our results suggest that the reason it is hard to separate correctly attributable samples from out of world samples is not because it is hard to identify out of world samples but because some samples are harder to attribute than others. We can also observe that easier problems allow for lower confidence thresholds and fewer correct attributions misidentified as incorrect attributions, with the threshold for 5 or 7 sample account attribution\nrequiring classifier confidence of only half that needed for single sample attribution with similar results."
    }, {
      "heading" : "4.4. Dataset Size Factors",
      "text" : "To test the effect of the amount of training data, we experiment with 90 samples of at least five lines of code per author for 50 authors using cross-validation with both 10 folds and 30 folds, using 500 trees for each experiment. In the 10 fold case, each experiment is trained on six fewer samples than in the 30 fold case. We obtain 74% accuracy for 10 fold cross-validaton and 81% accuracy for 30 fold cross-validation. We conclude that the six training file difference results in an increase in classification accuracy of 7%.\nWe also use three levels of merging to test the effect of the number of merged samples and the number of available training samples. Our best accuracy using the merging technique, 83%, comes from merging 10 samples and performing 9 fold cross-validation. When we merge more samples, we have insufficient training data and so have worse results. Merging 15 files and performing 6 fold crossvalidation gives us 81% accuracy, and merging 30 files and performing 3 fold cross-validation has an accuracy of 70%. So we see that merging has a limit in terms of benefits, and does not outweigh having too few training samples.\nTable 3 summarizes the results for the 90 sample and 50 author dataset. It also includes result aggregation for 90 samples with 47 authors. Aggregating nine samples results in 99% accuracy, which is far better than any of the results from the 50 author dataset, providing further support that aggregating the classification probabilities is the best method for multiple sample attribution.\nTo test the comparative effects of number of training samples versus number of lines of code in the training samples, we use four different datasets which are evaluated under the same conditions. Each dataset includes 90 programmers which are trained using the conditions in Table 4, then tested on single samples. We see that increasing\nthe number of samples from four to ten improves accuracy from 54% to 76%, even though it means using samples with fewer lines of code. Adding more further smaller training samples does not appear to result in a benefit for single sample attribution.\nIn order to further examine the effect of the size of the dataset, we took our 15 programmer dataset and constructed subsets such that each sample contained only a single line of code and perfromed 10-fold cross-validation. As shown in Figure 14, the maximum accuracy for these experiments is about 65% which is somewhat lower than than the 69% to 71% for the whole dataset. This shows us that these small samples may be somewhat harder to attribute than the longer ones, but they are not largely so. From a dataset size perspective, the number of samples per author is much more important than the size of the samples."
    }, {
      "heading" : "4.5. Special Case Attribution Tasks",
      "text" : "Table 5 shows the accuracy results of our special case attribution tasks, presented with the average accuracy, min-\nTABLE 5. SPECIAL CASE ATTRIBUTION TASKS\nAttribution Task Average Minimum Maximum Two-Class Attribution 91.8% 77.1% 96.6% Verification 88.1% 79.9% 96.5%\nFigure 15. These are the results for ground truth corruption in the Google Code Jam dataset.\nimum accuracy, and maximum accuracy across all 15 programmers for verification and all pairs for two-class attribution. We can see that these tasks are easier than the single sample attribution task. The variability also gives some insight into our previous accuracy results. From the spread of accuracies, we can see that some programmers are harder to attribute than others."
    }, {
      "heading" : "4.6. Ground Truth Corruption",
      "text" : "Figure 15 shows the accuracy for various levels of ground truth corruption for varying percentages of corrupted labels in the Google Code Jam dataset. We observe that the magnitude of the decline in accuracy is close to the magnitude of the incorrect ground truth labels for relatively small amounts of corruption. Therefore, we conclude that individual incorrect labels have only minimal effect on the overall quality of the classifier, and that it would take serious systemic ground truth problems to cause extreme classification problems. These results are not surprising because random forests are known to be robust against mislabeled data due to using bootstrap sampling, which causes each sample to only affect some of the trees in the overall classifier."
    }, {
      "heading" : "5. Discussion",
      "text" : "From these results, we conclude that a major difficulty in attributing git blame level source code samples is the high within-author variance of such samples: short segments of code that may be distributed across a range of tasks contain limited and in many cases ambiguous stylometric information. Variance reduction techniques are required to obtain strong predictions from such data. All of our techniques to simultaneously classify multiple samples decrease\nthe variance in our classification set, and therefore increase our accuracy.\nWe also note that it is possible that our techniques decrease the effect of ground truth problems. By aggregating samples, either at the level of feature vectors or at the time of classification, we can reduce the effect of individual samples with poor ground truth, assuming such samples are relatively uncommon."
    }, {
      "heading" : "5.1. Poorly correlated test examples improve ensemble accuracy",
      "text" : "Simple averaging of probabilistic predictors has long been known to yield an ensemble classifier that has significantly improved generalization performance and robustness to error over any individual component [18]. This improvement has also long been known to be inversely related to the degree of correlation between the predictions of the classifiers on a single example [27].\nThe standard approach for averaging considers an ensemble of learners h1, . . . hT , and takes the overall classification of a single sample x to be:\nH (x) = 1\nT T∑ i=1 hi(x)\nWe examine an interesting variation on this problem where, instead of submitting a single test sample to a diverse set of classifiers, we submit a diverse collection of test samples which are known to share the same unknown label x(i)1 , x (i) 2 . . . x (i) n to a single classifier, and average their outputs to obtain a final classification:\nH (i) = 1\nn n∑ j=1 h ( x (i) j ) The underlying intuition remains unchanged: if the erroneous components of any given prediction are approximately uncorrelated between different code samples from a single author (i.e., weight for incorrect predictions is approximately uniformly distributed over those incorrect predictions, given a sufficient number of samples), then in taking the average prediction across samples these errors will cancel each other out, resulting in an improved prediction over any of the individual samples.\nWe evaluated this theory in the 15-author data set by examining the coefficient of variation (the ratio of the standard deviation to the mean) across individual sample predictions across several test ensemble sizes (data not shown). Reliably, this variation is minimized for the correct prediction, indicating that from individual sample to individual sample within the ensemble, the predicted probability for the incorrect labels varies singificantly relative to the mean, suggesting that they are in fact approximately uncorrelated. Significantly, when the coefficient of variation for the correct label is not the smallest, these samples tend to have the lowest ensemble confidence and also are most likely to be incorrectly predicted."
    }, {
      "heading" : "5.2. Merging test samples reduces feature variance",
      "text" : "In this section we examine the statistical basis for the difference in classification accuracy for a large number of small code fragments versus a smaller number of arbitrarily merged code fragments in which the feature vectors are averaged. We do so by examining the 15-author sample data, in which each distinct continuous segment of code obtained via git blame is converted into a feature vector. Within this data, we examine both the sparsity pattern of the feature vectors, as well as an approximate measure of the signal-tonoise ratio of the individual features both individually and under combination.\nFirst, the individual feature vectors of this data are quite sparse; out of 2434 features, on average any given feature vector has only 74 nonzero features. Such sparsity can significantly impact the ability of bagging classifiers to find good partitions of the data at each split, particularly when features are subsampled in an effort to increase diversity between the trees in the ensemble, as for any given candidate feature to be split on many samples will be zero-valued and so indistinguishable. Merging multiple feature vectors reduces this sparsity as shown in Table 6 and improves the ability of the classifier to find good splits of the data at internal nodes.\nIncreasing the number of samples considered at each internal node can also somewhat alleviate this problem but cannot increase the test accuracy for unmerged data to that of the merged samples. Complete data is omitted for space, however for unmerged samples the default value of 50 features yielded an accuracy of approximately 71% via stratified cross-validation. The best performance we identified via cross-validated parameter search on the number of features to use at each split was 73% accuracy using 400 features at each split. For comparison merging five samples yields a cross-validated accuracy of approximately 94% with the default number of features. Sparsity thus explains a limited portion of the poor performance of the unmerged samples.\nIn an attempt to explain the remaining gap in accuracy, we also examined the variance of the features in a method similar to an ANOVA test. While an ANOVA test itself is inappropriate due to the non-normal distribution of the features, a similar intuition can be applied. If the difference between classes for a given feature – as expressed by intraclass variance – accounts for only a small proportion of the total variance of the data, meaning that the bulk of variation for a given feature occurs within classes, then we may anticipate that this feature provides little information about\nthe the corresponding class. Informally, we may say that such a feature exhibits a low signal (between-class variance) to noise (within-class variance) ratio.\nWe therefore calculate a “pseudo-F-statistic” for each separate feature in much the same way as a standard F statistic, with the understanding that the distributional assumptions underlying the traditional interpretation of the standard F-statistic are grossly violated and therefore calculating a p-value is not possible2. We find the residual error as the weighted mean of the class variances, the between-class variance as the variance of the per-class means with respect to the overall means, and calculate our pseudo-F-statistic as the ratio of treatment to residual variation. These pseudo-Fstatistics then may be (loosely) thought of as a measure of the signal-to-noise ratio for a given feature.\nFor the 15-author dataset, in which each distinct continuous segment of code obtained via git blame is converted into a feature vector, we find that while the lower quantiles of the data are relatively stable, the upper tail extends significantly between the unmerged and merged samples, and continues to increase as the number of merged samples increases, as seen in Table 7 and detailed in Figure 16.\nThis suggests that, for a significant subset of features, the proportion of the variance explained by the difference between class-conditional means is larger for the merged data than for the individual sample data, and hence the different classes are typically better-separated with respect to these features. If we look at which features have pseudof-statistics in the top quartile across each merged set, we observe that these features are relatively consistent: 320 (out of 608) features occur in the top quartile of all merged sets; this suggests that this particular subset of features is increasingly discriminative for this problem as the number of merged samples increases.\nThis difference in pseudo-F-statistics between the individual sample and merged data can be explained by the count-based nature of many of our features. While they are normalized to take on values in a continuous range, prenormalization they are discrete counts which may be viewed as draws from a multinomial. When the number of total draws are small, the resulting variance of the estimator of the underlying parameter is generally high and the estimator is generally poor. As many of the individual samples that we consider in this section contain a single line of code, the\n2. It is also worth noting that, as in our feature selection step, our “pseudo-F-statistics” examine the features with an implicit assumption of independence; higher-order effects are not considered.\nnumber of keywords and AST node types is limited, directly corresponding to such a “small-n” scenario. Under more standard analysis this would be dealt with by combining counts to obtain a more robust estimator, and indeed, this is effectively what is done to the count-based features when we merge samples into a single larger sample."
    }, {
      "heading" : "6. Future Work",
      "text" : "Our main results assume that we know that the correct programmer is one of our suspects and that we have a segmentation either in form of commits or git-blame. We have made strides in removing these assumptions, and would like to continue to do so.\nIn this work we have already presented a way to remove the closed world assumption. This assumption is common to stylometric work, but does not often match real world use cases. In this paper, we have presented an easy to use technique which allows easy elimination of most out of world samples at the cost of eliminating many correct, but difficult to trust, attributions in the case of single sample attribution, and some for account attribution. While this technique provides a solid start towards addressing the open world problem, it would be preferable to find a technique which sacrifices fewer correctly attributed samples while simultaneously improving our ability to trust the attributions of those difficult samples.\nThe primary assumption which remains can be considered the version control assumption. While this can be a reasonable assumption, as most large collaborative projects are written under version control, we are not guaranteed access to the repository itself. If we do not have access to the repository, the assumption does not hold. To remove the need for this assumption, we would need to perform source code segmentation which would split a large source code file into components by author. Alternately, we could attempt to perform sliding window attribution on source code files.\nAdditionally, we consider that we might not know the group of authors responsible for the single code file, and so we would like to perform either multi-label stylometry or single-label stylometry with groups as labels in order to identify the set of authors responsible for the file, so that we know which authors to segment it into. Along these lines, we would also be interested in finding techniques for learning useful meta-information about source code, including the number of programmers.\nWe further assume that commits and blames are similar types of data and that our results for blames would also hold true for valid commits. However, it would be useful to confirm this, and if this hypothesis is false then we would like to find a technique allowing us to classify commits as well as blames. Furthermore, we would like to use gitauthor as a best of both worlds scenario and to examine extensions of this work into multi-label classification for individual samples as well as whole files.\nBecause the accuracy for account attribution is superior to the accuracy for single sample attribution, we would like to find an unsupervised method to transform situations which would force us into the single sample attribution scenario into a scenario closer to account attribution.\nWhile we believe that there are important security applications of the ability to perform attribution at this level, we are also concerned about the serious privacy ramifications of this technology existing. Therefore, we hope to develop techniques to allow programmers to better anonymize themselves. We hope to find ways which concerned programmers can use to prevent themselves from being easily identified. We also hope to develop rules which indicate what makes a source code sample harder to attribute."
    }, {
      "heading" : "7. Conclusion",
      "text" : "In this paper we present several contributions. We show that we can take pre-segmented code and identify individual segments’ authorship with accuracy over 70% and that the classifier’s confidence is conservative, and thus can give guidance on when to trust the result. In particular, our results suggest trusting classifications with confidence over 40%, or 50% if the consequences of being wrong are severe. When we have multiple samples identified as belonging to the same individual, such as accounts, we can attribute these accounts with high accuracy, even if we only have two such samples to classify. We demonstrate that coding style is distributed among fragments, so we can get better accuracy by combining multiple fragments by merging or averaging, and that to build a good model, at least 40 to 50 segment samples are needed for training. Additionally, we analyze what differentiates the problem of identifying programmers of whole files versus git blamed samples. We also show an easy to use technique which identifies a majority of samples written by programmers outside of the suspect set, at the cost of causing us to reject correct but dubious attributions.\nWe show that for 106 suspect programmers, we can attribute individual small samples with over 73% accuracy with a good classifier. However, this classification is time\nconsuming, and so we show that with a far weaker classifier we can get accuracy of 38%. We then show that by collecting pairs of samples we are able to increase even this weak classifier to 77% accuracy, sets of 10 samples give us 90% accuracy, and sets of 50 samples are enough to give us 99% accuracy, while for our good classifier pairs of samples give us 95% accuracy and sets of 15 give us 99% accuracy.\nTo get these high accuracies, we proposed a technique of taking groups of samples known to be written by the same anonymous individual and averaging the classification probability distributions for the samples in the group to classify the entire group. We consider this a reasonable method to use in the common situation of code under version control in environments such as GitHub. In the event that aggregating classifications is too costly in time or memory, we also propose a technique based on merging samples.\nIn the absense of the ability to aggregate samples, either by merging the feature vectors or by combining the predicted results, we propose a method to guard against misclassificatons in high stakes situtions which can also be applied to open world conditions. In situations where the costs of being wrong are high, such as prosecuting an innocent person or wrongful termination of an employee, we suggest using cross validation to build a calibration curve and setting a confidence threshold for classification.\nThrough a variation of this concept, we present a technique to deal with the open world problem. We show that by using cross-validation along with code files known to be written by authors outside the suspect set to construct bins it is possible to calculate precision and recall values for important categories such as correct attributions above the threshold and out of world samples below the threshold and use those values to select a threshold before encountering true unknown data. The threshold can then be used to identify trustworthy attributions and samples which are likely not by one of the suspects of interest.\nOur work creates confidence that source code authorship segmentation can work, and that we can either perform a supervised segmentation or unsupervised segmentation and then attribute the resulting segments using the method we propose in this paper. In the more immediate sense, we show that it is possible to identify the author of even small, single contributions and easy to attribute collections of such contributions, such as those belonging to an account.\nIn light of our results, we recommend that anyone who chooses to contribute to public repositories but is in danger of reprecussions if identified use separate anonymous accounts for each contribution and make as small, generic, and scattered contributions as possible so as to increase chances of producing difficult to attribute samples. From our work, we have observed that having even two classification samples known to be by the same individual results in extremely high accuracy. Therefore, forcing the harder single sample attribution problem, and therefore maintaining the possibility of remaining anonymous, requires making it impossible to identify two classification samples as belonging to the same person."
    } ],
    "references" : [ {
      "title" : "Compilers, Principles, Techniques",
      "author" : [ "A.V. Aho", "R. Sethi", "J.D. Ullman" ],
      "venue" : "Addison wesley,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 1986
    }, {
      "title" : "A generic unsupervised method for decomposing multi-author documents",
      "author" : [ "N. Akiva", "M. Koppel" ],
      "venue" : "Journal of the American Society for Information Science and Technology, 64(11):2256–2264,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Random forests",
      "author" : [ "L. Breiman" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Source code authorship attribution",
      "author" : [ "S. Burrows" ],
      "venue" : "PhD thesis, RMIT University, nov",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Source code authorship attribution using n-grams",
      "author" : [ "S. Burrows", "S.M. Tahaghoghi" ],
      "venue" : "Proceedings of the Twelth Australasian Document Computing Symposium, Melbourne, Australia, RMIT University, pages 32–39. Citeseer,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Application of information retrieval techniques for source code authorship attribution",
      "author" : [ "S. Burrows", "A.L. Uitdenbogerd", "A. Turpin" ],
      "venue" : "Database Systems for Advanced Applications, pages 699–713. Springer,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "De-anonymizing programmers via code stylometry",
      "author" : [ "A. Caliskan-Islam", "R. Harang", "A. Liu", "A. Narayanan", "C. Voss", "F. Yamaguchi", "R. Greenstadt" ],
      "venue" : "24th USENIX Security Symposium (USENIX Security 15), pages 255–270,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Extraction of java program fingerprints for software authorship identification",
      "author" : [ "H. Ding", "M.H. Samadzadeh" ],
      "venue" : "Journal of Systems and Software, 72(1):49–57,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Unsupervised authorship attribution",
      "author" : [ "D. Fifield", "T. Follan", "E. Lunde" ],
      "venue" : "arXiv preprint arXiv:1503.07613,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Examining the significance of high-level programming features in source code author classification",
      "author" : [ "G. Frantzeskou", "S. MacDonell", "E. Stamatatos", "S. Gritzalis" ],
      "venue" : "Journal of Systems and Software, 81(3):447–460,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Identifying authorship by byte-level n-grams: The source code author profile (scap) method",
      "author" : [ "G. Frantzeskou", "E. Stamatatos", "S. Gritzalis", "C.E. Chaski", "B.S. Howald" ],
      "venue" : "International Journal of Digital Evidence, 6(1):1–18,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Effective identification of source code authors using byte-level information",
      "author" : [ "G. Frantzeskou", "E. Stamatatos", "S. Gritzalis", "S. Katsikas" ],
      "venue" : "Proceedings of the 28th international conference on Software engineering, pages 893–896. ACM,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Recognizing authors: an examination of the consistent programmer hypothesis",
      "author" : [ "J.H. Hayes", "J. Offutt" ],
      "venue" : "Software Testing, Verification and Reliability, 20(4):329–356,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Unsupervised decomposition of a document into authorial components",
      "author" : [ "M. Koppel", "N. Akiva", "I. Dershowitz", "N. Dershowitz" ],
      "venue" : "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pages 1356–1364. Association for Computational Linguistics,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Synergy of clustering multiple back propagation networks",
      "author" : [ "W.P. Lincoln", "J. Skrzypek" ],
      "venue" : "Advances in neural information processing systems, pages 650–657,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 1990
    }, {
      "title" : "Software forensics for discriminating between program authors using case-based reasoning, feedforward neural networks and multiple discriminant analysis",
      "author" : [ "S.G. MacDonell", "A.R. Gray", "G. MacLennan", "P.J. Sallis" ],
      "venue" : "Neural Information Processing, 1999. Proceedings. ICONIP’99. 6th International Conference on, volume 1, pages 66–71. IEEE,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Mining software repositories for accurate authorship",
      "author" : [ "X. Meng", "B.P. Miller", "W.R. Williams", "A.R. Bernat" ],
      "venue" : "2013 IEEE International Conference on Software Maintenance, pages 250–259. IEEE,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Predicting good probabilities with supervised learning",
      "author" : [ "A. Niculescu-Mizil", "R. Caruana" ],
      "venue" : "Proceedings of the 22nd international conference on Machine learning, pages 625–632. ACM,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Blogs, twitter feeds, and reddit comments: Cross-domain authorship attribution",
      "author" : [ "R. Overdorf", "R. Greenstadt" ],
      "venue" : "PoPETs, 2016(3):155– 171,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Using classification techniques to determine source code authorship",
      "author" : [ "B.N. Pellin" ],
      "venue" : "White Paper: Department of Computer Science, University of Wisconsin,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Induction of decision trees",
      "author" : [ "J.R. Quinlan" ],
      "venue" : "Machine learning, 1(1):81– 106,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 1986
    }, {
      "title" : "Software forensics: Can we track code to its authors",
      "author" : [ "E.H. Spafford", "S.A. Weeber" ],
      "venue" : "Computers & Security,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 1993
    }, {
      "title" : "Classify, but verify: Breaking the closed-world assumption in stylometric authorship attribution",
      "author" : [ "A. Stolerman", "R. Overdorf", "S. Afroz", "R. Greenstadt" ],
      "venue" : "IFIP Working Group, volume 11,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Error correlation and error reduction in ensemble classifiers",
      "author" : [ "K. Tumer", "J. Ghosh" ],
      "venue" : "Connection science, 8(3-4):385–404,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "Modeling and discovering vulnerabilities with code property graphs",
      "author" : [ "F. Yamaguchi", "N. Golde", "D. Arp", "K. Rieck" ],
      "venue" : "Proc. of IEEE Symposium on Security and Privacy (S&P),",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Twitter feed: “I just heard from an intern at Apple that they disallow her from contributing to open source on her own time. That’s illegal, right?",
      "author" : [ "zooko (@zooko" ],
      "venue" : null,
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 25,
      "context" : "Recently, the legality and reality of this situation has been a topic of discussion and debate on Twitter [29].",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 17,
      "context" : "We construct calibration curves to indicate the accuracy for collections which were attributed with given confidence, and analysts can use these curves to set a threshold below which to more carefully examine authorship due to higher probability of being outside of the suspect set [21].",
      "startOffset" : 282,
      "endOffset" : 286
    }, {
      "referenceID" : 3,
      "context" : "Previous work has attributed authorship to whole code files collected either from small suspect sets or from datasets which are near to laboratory condition, such as single authored code submitted to the Google Code Jam [2], [6].",
      "startOffset" : 225,
      "endOffset" : 228
    }, {
      "referenceID" : 6,
      "context" : "using random forests to perform authorship attribution of Google Code Jam submissions, with features extracted from the abstract syntax tree [9].",
      "startOffset" : 141,
      "endOffset" : 144
    }, {
      "referenceID" : 9,
      "context" : "used byte level n-grams to achieve high accuracy with small suspect sets [13], [14], [15].",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 10,
      "context" : "used byte level n-grams to achieve high accuracy with small suspect sets [13], [14], [15].",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 11,
      "context" : "used byte level n-grams to achieve high accuracy with small suspect sets [13], [14], [15].",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 19,
      "context" : "The use of abstract syntax trees for authorship attribution was pioneered by Pellin and used on pairs of Java programs in order to ensure that the studied programs had the same functionality [23].",
      "startOffset" : 191,
      "endOffset" : 195
    }, {
      "referenceID" : 7,
      "context" : "Ding and Samadzadeh studied a set of 46 programmers and Java using statistical methods [10].",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 15,
      "context" : "analyzed C++ code from a set of 7 professional programmers using neural networks, multiple discriminant analysis, and case-based reasoning [19].",
      "startOffset" : 139,
      "endOffset" : 143
    }, {
      "referenceID" : 3,
      "context" : "proposed techniques that achieved high accuracy for small suspect sets, but had poor scalability [6], [7], [8] Spafford and Weeber were among the first to suggest performing authorship attribution on source code [25].",
      "startOffset" : 97,
      "endOffset" : 100
    }, {
      "referenceID" : 4,
      "context" : "proposed techniques that achieved high accuracy for small suspect sets, but had poor scalability [6], [7], [8] Spafford and Weeber were among the first to suggest performing authorship attribution on source code [25].",
      "startOffset" : 102,
      "endOffset" : 105
    }, {
      "referenceID" : 5,
      "context" : "proposed techniques that achieved high accuracy for small suspect sets, but had poor scalability [6], [7], [8] Spafford and Weeber were among the first to suggest performing authorship attribution on source code [25].",
      "startOffset" : 107,
      "endOffset" : 110
    }, {
      "referenceID" : 21,
      "context" : "proposed techniques that achieved high accuracy for small suspect sets, but had poor scalability [6], [7], [8] Spafford and Weeber were among the first to suggest performing authorship attribution on source code [25].",
      "startOffset" : 212,
      "endOffset" : 216
    }, {
      "referenceID" : 12,
      "context" : "Hayes and Offutt performed a manual statistical analysis of 5 professional programmers and 15 graduate students, and found that programmers do have distinguishable styles which they use consistently [16].",
      "startOffset" : 199,
      "endOffset" : 203
    }, {
      "referenceID" : 16,
      "context" : "proposed a tool called git-author which assigns weighted values to contributions in order to better represent the evolution of a line of code [20].",
      "startOffset" : 142,
      "endOffset" : 146
    }, {
      "referenceID" : 18,
      "context" : "The primary piece of related research in the domain of text authorship attribution is the work by Overdorf and Greenstadt in cross-domain authorship attribution [22].",
      "startOffset" : 161,
      "endOffset" : 165
    }, {
      "referenceID" : 1,
      "context" : "on decomposition of artificially combined biblical books [4], [17].",
      "startOffset" : 57,
      "endOffset" : 60
    }, {
      "referenceID" : 13,
      "context" : "on decomposition of artificially combined biblical books [4], [17].",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 8,
      "context" : "[11].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 22,
      "context" : "[26].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 6,
      "context" : "We then used git blame on each line of code, and for each set of consecutive lines blamed to the same programmer we encapsulated those lines in a dummy main function and extracted features from the abstract syntax tree as in [9].",
      "startOffset" : 225,
      "endOffset" : 228
    }, {
      "referenceID" : 6,
      "context" : "However, unlike in [9] we cannot use information gain to prune the feature set due to having extremely sparse feature vectors and therefore very few features with individual information gain.",
      "startOffset" : 19,
      "endOffset" : 22
    }, {
      "referenceID" : 6,
      "context" : "[9].",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 0,
      "context" : "Our primary features come from the abstract syntax tree and include nodes and node bigrams [3].",
      "startOffset" : 91,
      "endOffset" : 94
    }, {
      "referenceID" : 24,
      "context" : "The abstract syntax tree, or AST, is a tree representation of source code with nodes representing syntactic constructs in the code, and we used the fuzzy parser joern to extract them [28].",
      "startOffset" : 183,
      "endOffset" : 187
    }, {
      "referenceID" : 20,
      "context" : "Due to the sparsity of the feature set, we do not use information gain to prune the set, and instead keep the entire feature set, minus any features which are constant across all samples [24].",
      "startOffset" : 187,
      "endOffset" : 191
    }, {
      "referenceID" : 6,
      "context" : "For this, we perform cross-validation with random forests, as in [9].",
      "startOffset" : 65,
      "endOffset" : 68
    }, {
      "referenceID" : 2,
      "context" : "Random forests are ensemble multi-class classifiers which combine multiple decision trees which vote on which class to assign to an instance [5].",
      "startOffset" : 141,
      "endOffset" : 144
    }, {
      "referenceID" : 6,
      "context" : "For this experiment, we perform cross validation on Google Code Jam data as is used by [9] with 250 authors and 9 files each.",
      "startOffset" : 87,
      "endOffset" : 90
    }, {
      "referenceID" : 6,
      "context" : "We note that while this is much lower than the accuracies reported by [9], the data itself is very different.",
      "startOffset" : 70,
      "endOffset" : 73
    }, {
      "referenceID" : 6,
      "context" : "The work of [9] attributes whole source code files written privately with an average of 70 lines of code per file, while our work attributes pieces of files written publicly and collaboratively with an average of 4.",
      "startOffset" : 12,
      "endOffset" : 15
    }, {
      "referenceID" : 6,
      "context" : "As we know from [9], word unigrams provide less information than abstract syntax tree nodes.",
      "startOffset" : 16,
      "endOffset" : 19
    }, {
      "referenceID" : 14,
      "context" : "Simple averaging of probabilistic predictors has long been known to yield an ensemble classifier that has significantly improved generalization performance and robustness to error over any individual component [18].",
      "startOffset" : 210,
      "endOffset" : 214
    }, {
      "referenceID" : 23,
      "context" : "This improvement has also long been known to be inversely related to the degree of correlation between the predictions of the classifiers on a single example [27].",
      "startOffset" : 158,
      "endOffset" : 162
    } ],
    "year" : 2017,
    "abstractText" : "Program authorship attribution has implications for the privacy of programmers who wish to contribute code anonymously. While previous work has shown that complete files that are individually authored can be attributed, we show here for the first time that accounts belonging to open source contributors containing short, incomplete, and typically uncompilable fragments can also be effectively attributed. We propose a technique for authorship attribution of contributor accounts containing small source code samples, such as those that can be obtained from version control systems or other direct comparison of sequential versions. We show that while application of previous methods to individual small source code samples yields an accuracy of about 73% for 106 programmers as a baseline, by ensembling and averaging the classification probabilities of a sufficiently large set of samples belonging to the same author we achieve 99% accuracy for assigning the set of samples to the correct author. Through these results, we demonstrate that attribution is an important threat to privacy for programmers even in real-world collaborative environments such as GitHub. Additionally, we propose the use of calibration curves to identify samples by unknown and previously unencountered authors in the open world setting. We show that we can also use these calibration curves in the case that we do not have linking information and thus are forced to classify individual samples directly. This is because the calibration curves allow us to identify which samples are more likely to have been correctly attributed. Using such a curve can help an analyst choose a cut-off point which will prevent most misclassifications, at the cost of causing the rejection of some of the more dubious correct attributions.",
    "creator" : "LaTeX with hyperref package"
  }
}