{
  "name" : "1405.2476.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n40 5.\n24 76\nv4 [\ncs .L\nG ]\n1 0\nO ct\n2 01\n1. Introduction\nTransducers, introduced by [28], are a type of abstract machine which defines a relation between two formal languages. As such, they are interpreted as modeling translation in any context where formal languages are applicable. We provide no background on formal languages in this paper; an overview of the subject can be found in [9] and [32]. Alternatively, transducers can be viewed as a generalization of finite state machines. This view was introduced by Mohri, who uses transducers in the context of natural language processing [24, 25] and [26].\nA fundamental task when studying the theory of transducers is to look for classes of transducers that can be learned given access to some form of data. If a class of transducers, C , is found to be learnable, then a predictive model can be produced in any application where a translation from the class C is in use. The significance of transducers, specifically expanding the range of the learnable classes, is clear from the scope of applications of transducers. Among many others, some well known applications are in the fields of morphology and phonology [31], machine translation [3, 14, 15], web wrappers [11], speech [24] and pattern recognition [5]. In each of these cases, different classes of transducers are examined with characteristics suitable to the application. Distinguishing characteristics of different classes include determinism properties, the use of probabilites or weights, as well as details of the types of transitions that are permitted.\n1.1. Transducer learning. An important step in the theory of transducers was the development of the algorithm Ostia. Introduced in [29], Ostia was designed for language comprehension tasks [38]. A number of elaborations on the original algorithm have since arisen, many of them aimed at trying to circumvent the restriction to total functions that limited Ostia. Typically, these attempts involved adding some new source of information. For example, Ostia-N uses negative (input) examples and Ostia-D supposes the algorithm has some knowledge of the domain of the function [30]. Similar ideas were explored later by [22] and [17]. An application of Ostia for active learning is presented in [36]. Using dictionaries and word alignments has been tested by [37]. A demonstrated practical success of Ostia came in 2006. The Tenjinno competition [34] was won by [16] using an Ostia inspired algorithm.\n1.2. Towards nondeterminism with transducers. Non-deterministic transducers pose numerous complex questions – even parsing becomes a difficult problem [12, 13]. Interest in non-deterministic models remains, however, as the limitations of subsequential transducers make them unacceptable for most applications. The first lifting of these constraints was proposed by [2]. They propose a model in which the final states may have multiple outputs. In his PhD thesis, Akram introduced a notion of semi-determinism [1] that strikes a balance between complete non-determinism and the very restrictive subsequential class.\nKey words and phrases. Grammatical Inference, Semi-Deterministic Transducers.\n1\nHe provided an example witnessing that semi-deterministic transducers are a proper generalization of deterministic transducers, but did not pursue the topic further, focusing instead on probabilistic subsequential transducers. We examine an equivalent formulation of Akram’s semi-determinism based on methods of mathematical logic. In particular, by viewing the definition from a higher level of the ranked universe, we convert what would be a general relation into a well-defined function. [23] provides an overview of a number of important topics in set theory including the ranked and definable universes. Some more recent developments in set theory is [21].\nA significant obstacle in learning non-deterministic transducers is the fact that an absence of information cannot be interpreted. One approach to overcoming this problem is to use probabilities. We eschew the probabilistic approach in favor of a collection of methods that have their antecedents in Beros’s earlier work distinguishing learning models [6] and determining the arithmetic complexity of learning models [7].\nAn earlier version of this work was presented at the International Conference on Grammatical Inference [8]. In this version, we provide more of the algorithms involved in learning semi-deterministic transducers and prove that the algorithms converge. We also establish the relationship between semideterministic transducers and two other natural extensions of deterministic transducers and the bi-languages they generate, specifically p-subsequential transducers and finitary, finite-state, and bounded relations (definitions of these terms are provided in Section 9.1). Finally, we show that semi-deterministic transducers and the associated bi-languages fail two closure properties: closure under composition and closure under bi-language reversal.\n2. Notation\nWe make use of following common notation in the course of this paper. Throughout, the symbols x, y and z denote strings and a and b will denote elements of a given alphabet. We shall use the standard notation λ for the empty string.\n• The concatenation of two strings, x and y, is denoted by xy. We write x ≺ y if there is a string z , λ such that y = xz. We write x y if x ≺ y or x = y. This order is called the prefix order. • For a set of strings, S , T [S ] = {x : (∃y ∈ S ) ( x y )\n} is the prefix closure of S . • A tree is a set of strings, S , such that T [S ] = S . S ′ is a subtree of S if both S and S ′ are trees\nand S ′ is contained in S . A strict subtree is a subtree that is not equal to the containing tree. • P(X) = {Y : Y ⊆ X} and P∗(X) = {Y : Y ⊆ X ∧ |Y | < ∞}. • We will use elements of N both as numbers and as sets. In particular, we use the following\ninductive definition: 0 = ∅ and, given 0, . . . , n, we define n + 1 = {0, . . . , n}. • Following the notation of set theory, the string x = a0 . . . an is a function with domain n + 1.\nThus, x↾k = a0 . . . ak−1 for k ≤ n + 1. |x| is the length of x and x− is the truncation x↾(|x| − 1). Note that the last element of x is x(|x| − 1) and the last element of x− is x(|x| − 2). • Again, drawing on set theory terminology, we call two functions, f and g, compatible if (∀x ∈ dom( f ) ∩ dom(g))( f (x) = g(x)). • We write x ‖ y if x = y, x ≺ y or x ≻ y and say x and y are comparable. Otherwise, we write x ⊥ y and say that x and y are incomparable. • By <lex and <llex we denote the lexicographic and length-lexicographic orders, respectively. • For an alphabet Σ, Σ∗ is the set of all finite strings over Σ. A tree over Σ is a tree whose members\nare members of Σ∗, where the ordering of the tree is consistent with the prefix order on Σ∗ and the tree is prefix closed. • We reserve a distinguished character, #, which we exclude from all alphabets under consideration and we will use # to indicate the end of a word. We will write x# when we append the # character to x.\n3. Bi-Languages and Transducers\nBi-languages are the fundamental objects of study. They capture the semantic correspondence between two languages. In principle, this correspondence does not specify any ordering of the two languages, but translation is always done from one language to another language. As such, we refer to the input and the output languages of a bi-language. For notational simplicity, in everything that follows Σ is the alphabet for input languages and Ω is the alphabet for output languages. Using this notation, the input language is a subset of Σ∗ and the output language is a subset of Ω∗. We now present the standard definition of a bi-language.\nDefinition 3.1. Consider two languages, L ⊆ Σ∗ and K ⊆ Ω∗. A bi-language from L to K is a subset of L × K with domain L.\nFor our purposes, we wish to indicate the direction of translation and to aggregate all translations of a single string. To this end, in the remainder of this paper, we will use the following equivalent definition of a bi-language.\nDefinition 3.2. Consider two languages, L ⊆ Σ∗ and K ⊆ Ω∗. A bi-language from L to K is a function f : L → P(K). L is said to be the input language and K the output language of f . When defined without reference to a specific output language, a bi-language is simply a function f : L → P(Ω∗). If f and g are two bi-languages, then f is a sub bi-language of g if dom( f ) ⊆ dom(g) and for all x ∈ dom( f ), f (x) ⊆ g(x). A finite subset D of L × K is consistent with f if for every 〈x, X〉 ∈ D, X ∈ f (x).\nNote that for a bi-language f from L to K, we do not require that ⋃\nx∈L f (x) = K. We are interested in languages whose generating syntax is some form of transducer.\nDefinition 3.3. A transducer G is a tuple 〈states[G], I,Σ,Ω, E〉.\n(1) states[G] is a finite set of states. I ⊆ states[G] is the set of initial states. (2) Σ and Ω are the input alphabet and output alphabet, respectively – finite sets of characters which\ndo not contain the reserved symbol #. (3) E ⊆ states[G] × states[G] × (Σ∗ ∪ {#}) × P∗(Ω∗) is a finite relation called the transition rela-\ntion. An element e ∈ E is called a transition with e = 〈start(e), end(e), input(e), output(e)〉. If input(e) = #, then e is called a #-transition.\nA transducer is said to generate or induce the bi-language which consists of all pairs of strings 〈x, Y〉 ∈ Σ∗ ×Ω∗ such that:\n(1) (∃x0, . . . , xn ∈ Σ∗)(x = x0 . . . xn), (2) (∃e0, . . . , en+1 ∈ E)(∃q ∈ I) ( (∀i ∈ {1, . . . , n}) ( xi = input(ei)∧ end(ei) = start(ei+1) ) ∧ start(e0) =\nq ∧ input(en+1) = # )\nand (3) there are Yi ∈ output(ei) for i ≤ n + 1 such that Y = Y0Y1 · · ·Yn+1.\nThis paper addresses semi-deterministic bi-languages which are bi-languages generated by semideterministic transducers. These were defined in [1]. We use an equivalent formulation.\nDefinition 3.4. A semi-deterministic transducer (SDT) is a transducer with a unique initial state such that\n(1) input(e) ∈ Σ ∪ {#} for every transition e, (2) given a state, q, and a ∈ Σ, there is at most one transition, e, with start(e) = q and input(e) = a\nand (3) given a transition, e, output(e) is a finite set of pairwise incomparable strings inΩ∗ (i.e., output(e) ∈\nP∗(Ω∗) ∧ (∀X, Y ∈ output(e)) ( X ⊥ Y ) ).\nA semi-deterministic bi-language (SDBL) is a bi-language that can be generated by an SDT.\nTwo useful properties of SDTs follow from the definition. First, if e ∈ E and λ ∈ output(e), then output(e) = {λ}. Second, although there may be multiple translations of a single string, every input string follows a unique path through an SDT. The precise meaning of this is made clear in the next definition. We must also note that, while SDBLs can be infinite, the image of any member or finite subset of L is finite. Thus, an SDBL is a function f : L → P∗(Ω∗).\nDefinition 3.5. Let G be an SDT with input language L. A path through G is a string e0 . . . ek ∈ E∗, where E is the set of transitions, such that start(ei+1) = end(ei) for i < k. G[p] is the collection of all outputs of G that can result from following path p. px is the unique path through G, e0 . . . ek ∈ E∗, defined by x ∈ Σ∗ such that start(e0) is the unique initial state of G, if such a path exists. We denote the final state of the path px by qx.\n4. Ordering maximal antichains\nWhen parsing sets of strings, we will often use the following operations.\nDefinition 4.1. Let S and P be two sets of strings. • P ∗ S = {xy : x ∈ P ∧ y ∈ S }. • P−1S = {y : (∃x ∈ P) ( xy ∈ S ) }. For notational simplicity, we define x−1S = {x}−1S , P−1x = P−1{x}, x ∗ S = {x} ∗ S and P ∗ x = P ∗ {x} for a string x.\nProposition 4.2. ∗ is associative, but is not commutative.\nProof. Associativity follows from the associativity of concatenation. To see that ∗ is not commutative, consider A = {a} and B = {a, b}. A ∗ B = {aa, ab} and B ∗ A = {aa, ba}.\nThe following definitions and results pertain to sets of strings and trees over finite alphabets.\nDefinition 4.3. Given a set of strings, S , we call P ⊆ T [S ] a maximal antichain of S if (∀x, y ∈ P) (\nx ⊥ y ∨ x = y )\nand (∀x ∈ S )(∃y ∈ P)(y ‖ x). P is a valid antichain of S if P is a maximal antichain of S and (∀x, y ∈ P) ( x−1T [S ] = y−1T [S ] ) . We define, Vac(S ) = {P : P is a valid antichain of S }.\nExample 4.4. Consider the following set of strings over the alphabet {a, b}:\nS = {a5, a4b, a2ba, a2b2, ba4, ba3b, baba, bab2, b2a3, b2a2b, b3a, b4}.\nGraphically, we can represent S as a tree where branching left indicates an a and branching right indicates a b. In the picture below to the right, we highlight the four valid antichains of S : P0 = {λ}, P1 = {a2, ba, b2}, P2 = {a4, a2b, ba3, bab, b2a2, b3} and P3 = S . Note that S is only a valid antichain of itself because it contains no comparable strings. The members of the four valid antichains are connected via dotted lines in the right picture (P0 has only one member and therefore includes no dotted lines). For reference a maximal antichain that is not valid is included in the picture on the left and its members are joined with a dotted line.\nIn the next figure, we focus on the valid antichain P1.\nObserve that the portions of the tree below each of a2, ba and b2 are identical; the terminal nodes of all three sub-trees are {a3, a2b, ab, b2}. It is this equivalence of suffixes that makes P1 a valid antichain.\nThe concept of equivalence we have developed closely parallels that of Nerode equivalence [27] in which two strings in a language are equivalent if there is no extension in the language that distinguishes the two strings.\nIt is interesting to note that the valid antichains in the above example have a natural linear ordering. As we shall see in Theorem 4.9, this is not an artifact of the particular example, but is true of any finite set S .\nProposition 4.5. Suppose that P is a valid antichain of a set of strings S and Q is a valid antichain of P, then Q is a valid antichain of S .\nProof. Let P be a valid antichain of a set of strings S and let Q be a valid antichain of P. Every member of T [S ] is either a prefix or an extension of a member of P. Since P consists of incomparable strings, each member of P has a member of Q as a prefix. Thus, Q is a maximal antichain of S . To see that Q is a valid antichain, observe that if x, y ∈ Q, then x−1T [P] = y−1T [P]. Since z−1T [S ] = w−1T [S ] for all z,w ∈ P, x−1T [S ] = y−1T [S ], thus Q is a valid antichain.\nDefinition 4.6. For P and Q, sets of strings over some common alphabet, we say that P <ac Q (P is “antichain less than” Q) if either\n• |P| < |Q|, or • |P| = |Q| and, for all x ∈ P and y ∈ Q, if x ‖ y, then x ≺ y.\nWe will use valid antichains to parse a set of strings as one would parse a single string into a prefix and suffix. The validity of an antichain ensures that the corresponding suffix set is well-defined.\nProposition 4.7. Let S be a finite set of incomparable strings. If P is a valid antichain of S , then P ∗ (P−1S ) = S .\nProof. Observe that, if P is a valid antichain of S , then T [P−1S ] = x−1T [S ] for all x ∈ P.\nThe antichain ordering (<ac) has particularly nice properties when applied to Vac(S ), where S is a finite set of strings.\nProposition 4.8. If P and Q are maximal antichains of the same finite set of strings, then there is a relation R ⊆ P × Q such that\n• dom(R) = P, • ran(R) = Q, • xRy ↔ x ‖ y.\nFurthermore, if |P| = |Q| and P ‖ac Q, then R is a well-defined and bijective function.\nProof. Define R = {〈x, y〉 : x ∈ P∧ y ∈ Q∧ x ‖ y}. Since P and Q are maximal antichains, for each x ∈ P there is y ∈ Q such that x ‖ y hence, dom(R) ⊇ P. Similarly, for each y ∈ Q there is an x ∈ P such that x ‖ y thus, ran(R) ⊇ Q. By the definition of R, dom(R) ⊆ P, ran(R) ⊆ Q and xRy ↔ x ‖ y. If |P| = |Q| and P ‖ac Q, then for each x ∈ P there is a unique comparable y ∈ Q and vice versa. Consequently, R is well-defined and bijective in this case.\nTheorem 4.9. If S is a finite set of strings, then ( Vac(S ), <ac ) is a finite linear order.\nProof. Consider a finite set of strings, S , and let T = T [S ]. We begin by fixing P, Q ∈ Vac(S ). We may assume that |P| = |Q|; if |P| , |Q|, then P <ac Q or Q <ac P. We pick an element x ∈ P and observe that, by Proposition 4.8, there is a y ∈ Q such that x ‖ y.\nSuppose that x = y and let x′ be any other member of P. By Proposition 4.8, there is a y′ ∈ Q such that x′ ‖ y′. Since P and Q are valid antichains and x = y, x′−1T = x−1T = y−1T = y′−1T . Given that x′ ‖ y′, T is finite and x′−1T = y′−1T we conclude that x′ = y′. Now assume x ≺ y. In the case y ≺ x simply exchange the roles of x and y. As above, we pick x′ ∈ P and any comparable element y′ ∈ Q. Clearly y−1T is a strict subtree of x−1T and hence, y′−1T is a strict subtree of x′−1T . We conclude that x′ ≺ y′.\nWe have shown that any two members of Vac(S ) are comparable. The remaining order properties follow immediately from the definitions.\nWhile the proof of Theorem 4.9 is quite simple, we highlight it as a theorem because it is the critical result for the applications of valid antichains that follow. Note that <ac may not be a linear order on an arbitrary collection of maximal antichains.\nCorollary 4.10. Let S 0, S 1, S 2, . . . be a sequence of finite sets. ⋂\ni∈N Vac(S i) is linearly ordered under <ac.\nProof. Any subset of a linear order is a linear order. Since ⋂\ni∈N Vac(S i) ⊆ Vac(S 0), the claim follows.\nDefinition 4.11. Given a set of strings, S , a finite sequence of sets of strings, P0, . . . , Pn, is a factorization of S if S = P0 ∗ · · · ∗ Pn and Pi , {λ} for i ≤ n. Such a factorization is said to be maximal if, for each i ∈ N, Vac(Pi) = {{λ}, Pi}.\nNote that having Vac(Pi) = {{λ}, Pi} for each factor, Pi, in a factorization is equivalent to having Pi+1 be the <ac-least non-trivial valid antichain of P−1i · · · P −1 0 S .\nExample 4.12. We consider the following set of strings:\nS = {a5, a4b, a3ba2, a3bab, a3b2a, a3b3, aba2, abab, ab2a2, ab2ab, ab3a, ab4, ba4,\nba3b, ba2ba2, ba2bab, ba2b2a, ba2b3, b2a2, b2ab, b3a2, b3ab, b4a2, b4ab, b5a, b6}.\nIn the figure below, we display the tree, T [S ], as well as the <ac-least non-trivial valid antichain, P0 = {a, b}.\nThe corresponding set of suffixes is P−10 S = {a 4, a3b, a2ba2, a2bab, a2b2a, a2b3, ba2, bab, b2a2, b2ab, b3a, b4}. Iterating, we find the next factor is P1 = {a2, b} and its set of suffixes is (P0 ∗ P1)−1S = {a2, ab, ba2, bab, b2a, b3}.\nWe next pick P2 = {a, ba, b2}. Once we factor out P2, all that remains is {a, b}. The only antichains of {a, b} are {λ} and {a, b}, both of which are valid antichains. We pick the final factor to be P3 = {a, b} and conclude that P0 ∗ P1 ∗ P2 ∗ P3 is a maximal factorization of S .\nCorollary 4.13. Up to possible reordering of commutative terms, every finite set of incomparable strings has a unique maximal factorization.\nProof. Let S be a finite set of incomparable strings. We will apply the iterative process illustrated in Example 4.12 to S . Define P0 to be the <ac-least non-trivial valid antichain of S . If P0 = S , then the process is complete. By Theorem 4.9, the choice of P0 is unique. Suppose we have defined P0, P1, . . . , Pn. Let S n = P−1n · · · P −1 0 S . To be explicit, S n = P −1 n (P −1 n−1(· · · (P −1 0 S ))). Define Pn+1 to be the <ac-least nontrivial valid antichain of S n. As before, the choice is unique. If Pn+1 = S n, then the process is complete. Otherwise, we proceed to the next iteration.\nSince Vac(S ) is finite, the process must terminate. The uniqueness of the factorization follows from the uniqueness of the choices made at each stage of the process.\nObserve that the interative process described above specifies a unique order for the terms of the unique maximal factorization. When the terms are listed in the order specified by this process, we will say that the factorization is in canonical order.\n5. Semi-Deterministic Bi-Languages\nIn this section, we prove the existence of a canonical SDT for every SDBL. Determining the canonical SDT for an SDBL is done in two phases. First, a “maximal” function on prefixes of the input language is found. Finding such a maximal function is analogous to the onwarding performed in algorithms such as OSTIA and can be loosely described as the process of moving decisions earlier in the translation process. Second, subsets of the domain on which the function has identical outputs are conflated in a largely standard merging process. Merging produces a finite-order equivalence relation on T [L]. Using this equivalence relation, we can define the canonical SDT.\n5.1. Semi-Deterministic Functions.\nDefinition 5.1. Let f be an SDBL over L. F : T [L] → P∗(Ω∗) is a semi-deterministic function (SDF) of f if, for x ∈ L, f (x) = F(x↾1)∗F(x↾2)∗· · ·∗F(x)∗F(x#). We define ΠF(x) = F(x↾1)∗F(x↾2)∗· · ·∗F(x). If F and F′ are SDFs of f , we say that F ≤sd f F′ if ΠF(x) is a valid antichain of ΠF′(x) for all x. The SDF induced by f is the SDF, F, such that F(x) = {λ} for all x ∈ T [L] and F(x#) = f (x) for all x ∈ L.\nExample 5.2. Suppose that A, B,C ⊆ Ω∗ are finite, non-empty and not equal to {λ}. Let Σ = {a} be the input alphabet. Define an SDBL, f , over L = {a2} by f (a2) = A ∗ B ∗ C. We define two incomparable SDFs of f as follows. The first SDF: F(λ) = {λ}, F(a) = A ∗ B, F(a2) = {λ} and F(a2#) = C. The second SDF: F′(λ) = {λ}, F′(a) = A, F′(a2) = B ∗ C and F′(a2#) = {λ}. Since ΠF(a) is not a valid antichain of ΠF′(a), F sd f F′. Likewise, since ΠF′(a2) is not a valid antichain of ΠF(a2), F′ sd f F.\nExample 5.2 demonstrates that ≤sd f is not a linear ordering of the SDFs of a fixed SDBL. Nonetheless, there is a ≤sd f -maximum SDF of f .\nTheorem 5.3. If f is an SDBL over L, then there is a ≤sd f -maximum SDF of f .\nProof. For x ∈ T [L], let S be the collection of all members of L that extend x and let x0 be the <llex-least member of S . By Corollary 4.13, for every y ∈ S there is a unique maximal factorization of f (y). Let P0 ∗ · · · ∗ Pn denote the unique maximal factorization of f (x0). Let P0 ∗ · · · ∗ Pi be the longest common initial segment of all factorizations of members of { f (x) : x ∈ S } when the terms of the factorizations are listed in canonical order. We define Px to be the product of this longest common factorization.\nWe define Fm(λ) = {λ} and define Fm inductively on the members of T [L] in <llex-order as follows. Suppose we are considering x ∈ T [L] and Fm has already been defined on all <llex-lesser members of T [L]. We define Fm(x) = (ΠFm(x−))−1Px. If y ∈ L and Fm(y) is defined, we set Fm(y#) = (ΠFm(y))−1 f (y).\nIf x ≺ y, then ΠFm(x) is a valid antichain of Fm(y) and (Fm(y))−1 f (y) is well-defined. Consequently, Fm is a well defined function with domain T [L]. If F is any SDF of f and x is an arbitrary member of T [L], then ΠF(x),ΠFm(x) ∈ Vac( f (x0)), where x0 is the <llex-least extension of x in L. By Theorem 4.9, for any x ∈ T [L], ΠF(x) and ΠFm(x) are <ac-comparable. Furthermore, ΠF(x),ΠFm(x) ∈ Vac( f (y)) for all y ≻ x. Given the construction of Fm, if Fm(x) <ac F(x), then there must be a y ∈ L such that x ≺ y and ΠF(x) < Vac( f (y)) – which is not possible. Thus, Fm is a ≤sd f -maximum SDF of f .\nDefinition 5.4. Let f be an SDBL with maximal SDF F. For x ∈ dom(F) and F′ an SDF of f , we say that F′ is onward at x if for all y ∈ dom(F), y x implies that F′(y) = F(y). If F′ is onward at λ, then we say that F′ is onward.\nIn Section 8, we use the concept of onwarding to build the maximal SDF from data.\n5.2. Merging. The second phase of building a canonical form for SDTs is to define an equivalence relation on the domain of a maximum SDF. This means identifying which paths lead to the same state.\nDefinition 5.5. Let F be an SDF of f over L and x ∈ T [L]. We define futureF [x] : x−1T [L] → R, where R is the range of F, such that futureF [x](y) = F(xy). If x, y ∈ dom(F), we say that x ≡ y if futureF [x] = futureF [y]. Given x, we define x to be the <llex-least element of dom(F) that is equivalent to x.\nProposition 5.6. (1) ≡ is an equivalence relation on the domain of an SDF. (2) If x ≡ y and xz, yz ∈ T [L], then xz ≡ yz. (3) If F is an SDF of f over L, then there are only a finite number of ≡-equivalence classes on the\ndomain of F.\nProof. Part 1 follows from the fact that equality is an equivalence relation. Part 2 follows from the definition of ≡. To prove part 3, let G be an SDT that generates f and let qx be a state of G which can be reached by the input string x ∈ T [L]. For any y ∈ T [L], if py leads to qx, then x ≡ y as their futures are the same. Thus, ≡ induces an equivalence relation on (hence, a partition of) the states of G. Since there is at least one state in each equivalence class, the fact that |states[G]| < ∞ implies that there are only finitely many equivalence classes.\nLemma 5.7. Let F be an SDF of f over L. There is an n such that for all x, y ∈ T [L], x ≡ y if and only if future[x]↾xΣn = future[y]↾yΣn.\nProof. The proof follows immediately from Proposition 5.6, part 3. Since there are only a finite number of possible futures, there is a finite portion of each that uniquely identifies it. Let n be the maximum depth of the paths required to obtain the identifying portion of each future. We have obtained the desired n.\nWe can think of the identifying bounded future of an equivalence class as a sort of signature, an analogue of the famous locking sequence for Gold style learning [10].\nThe maximum SDF and the equivalence relation on its domain depend only on the underlying SDBL. Thus, we have defined a machine-independent canonical form. As a footnote, we demonstrate here how to produce an SDT from the canonical form which is unique up to isomorphism.\nDefinition 5.8. Let f be an SDBL, let Fm be the maximum SDF for f and let ≡ be the equivalence relation on the domain of Fm. Define a finite state machine, G f , as follows:\n• states[G f ] = {rx : x ∈ T [L]} (in other words, a set of blank states indexed by {x : x ∈ T [L]}). • The initial state is rλ. • EG f = {〈rx− , rx, x(|x| − 1), Fm(x)〉 : x ∈ T [L]} ∪ {〈rx, rλ, #, Fm(x#)〉 : x ∈ L}\nWe call G f the canonical SDT for f .\nAs noted prior to the definition, the maximum SDF depends only on the SDBL. Thus, we are justified in calling the above SDT a canonical SDT. Although L and T [L] may be infinite sets, the set of transitions, EG f , and the set of states, states[G f ], are finite by Proposition 5.6. Also, observe that the method of defining an SDT from an SDF described in Definition 5.8 can be used to define a unique SDT from any SDF. Since every SDT also defines a unique SDF, there is a bijection between SDFs and SDTs for a given SDBL.\nTheorem 5.9. Let f be an SDBL. G f is an SDT that generates f .\nProof. Clearly, G f is a finite state transducer. If P0, · · · , Pn are sets of incomparable strings, then S = P0 ∗ · · · ∗ Pn also consists of incomparable strings. To see this, suppose x = x0 · · · xn and y = y0 · · · yn are such that x ≺ y and xi, yi ∈ Pi for all i ≤ n. If i be least such that xi , yi, then xi ≺ yi and Pi contains two\ncomparable strings. Thus, the outputs of all transitions of G f consist of incomparable strings, as they are factors of the elements of the range of f .\nWe must show that G f generates f . G f and f have the same domain. Let Fm be the maximal SDF of f . If x ∈ T [L], then G f [px] = ΠFm(x), thus, G f generates f .\n5.3. An Example. To illustrate the canonical form that we have now defined, we exhibit a transducer not in canonical form together with its canonical form.\n6. The learning models\nThere are two principal learning models in grammatical inference: identification in the limit [20] and PAC-learning [35]. Each of these models admits variants depending on what additional sources of information are provided. In order to learn semi-deterministic transducers, we use queries [4] as an additional resource. These queries are very limited; the oracle will be interrogated about a possible translation pair and the oracle will return either a true or false.\nDefinition 6.1. Let f be a bi-language. The translation query [x, Y] f returns true if Y ∈ f (x) and false otherwise. We call this oracle [ f ]. Where it is clear from context, we will write [x, Y] instead of [x, Y] f .\nEquivalently, the oracle answers membership queries about the graph of the bi-language. We also prove that learning is not possible without queries. The precise definition of learning we use is adapted from the one used in [18]:\nDefinition 6.2. An algorithm, A, polynomial identifies in the limit with translation queries a class of transducers, C , if for any G ∈ C there is a set, CS G, such that on any D ⊇ CS G contained in the bi-language induced by G, A outputs a G′ equivalent to G. The algorithm must converge within a polynomial amount of time in |D| and |G|; |CS G | must be polynomial in |G|. |G|, |D| and |CS G | denote the number of bits required to encode the objects G, D and CS G, respectivly.\nNote that in the above definition the number of calls to the oracle is also bounded by the overall complexity of the algorithm and is therefore polynomial in the size of the sample.\nFor Theorem 7.2, we use a different model of learning: identification in the limit from positive data. We give the definition below.\nDefinition 6.3. An algorithm, A, identifies in the limit from positive data a class of transducers, C , if for any G ∈ C and any infinite enumeration of the bi-language induced by G, the algorithm A outputs a finite number of distinct transducers on the initial segments of the enumeration. The only transducer that is output infinitely many times must be equivalent to G.\n7. SDBLs are not learnable\nWe assume domain knowledge (i.e., access to the characteristic function of the input language). In the proof of the following theorem, we encode a standard example of a “topological” failure of identification in the limit. In particular, we encode the family H = {N} ∪ {A ⊆ N : |A| < ∞} into a sequence of SDTs.\nDefinition 7.1. Let f be a bi-language. We define DK f to be the oracle that, when asked about x, returns a boolean value DK f (x). If DK f (x) = true, then x is in the input language of f (in other words, the domain of f ). Otherwise, x is not in the input language of f . An algorithm which has access to DK f is said to have domain knowledge about f .\nTheorem 7.2. There is a collection of SDBLs, C, such that no algorithm can identify C in the limit from positive data, even given domain knowledge of each member of C.\nProof. To avoid degenerate cases, we assume the output alphabet has at least two characters, A and B, and the input alphabet has at least one character, a. We exhibit a sequence of SDTs, {Gi}i∈N, such that no program can successfully learn every member of the sequence. In the following graphical representation of {Gi}i∈N we omit the #-transitions, instead indicating terminal nodes with a double border.\nLet fi be the SDBL generated by the SDT Gi. Fix any learning algorithm and let M be the function such that, given data D, the hypothesis made by the learning algorithm is M(D). We inductively define an enumeration of a bi-language generated by some member of the sequence, {Gi}i∈N. Define Xi = 〈ai, Ai〉〈ai, Bi〉 and X ji = 〈a\nj, A j〉〈a j+1, A j+1〉 · · · 〈a j+i, A j+i〉. Let n1 be least such that M(X1X1n1 ) codes G1. If no such n1 exists, then there is an enumeration of f1 which the chosen algorithm fails to identify. Thus, without loss of generality, we may assume such an n1 exists. Similarly, we pick n2 to be least such that M(X1X1n1 X2X 2 n2) codes G2. Proceeding in this fashion, either we reach a stage where some nk cannot be found and the algorithm has failed to learn fk or we have built an enumeration of G0 on which the algorithm changes its hypothesis an infinite number of times. In either case, learning has failed. C = { fi : i ∈ N} is the desired collection of SDBLs.\n8. Learning with translation queries\nIn the remainder of the paper, we exhibit an algorithm that can learn any SDBL, f , in the limit, provided the algorithm has access to the oracles DK f and [ f ]. We present the algorithms that witness the learnability of SDBLs and summarize the result in Theorem 8.5.\n8.1. The characteristic sample. The characteristic sample must contain sufficient data to unambiguously perform two operations: onwarding and merging. Throughout this section f is an SDBL over L and G is the canonical SDT that generates f . We define x̂ to be the <llex-least member of L that extends\nx. We now proceed to define the characteristic sample for f , denoted CS f . We will make extensive use of px, qx and G[px] in this section (see Definition 3.5).\nThe first component of the characteristic sample provides the data required to recognize which maximal antichains of a set of translations are not valid. In order to illustrate the concept, consider f (a#), the translations along a path involving only one non-# transition. Let X be the <llex-least member of f (a#). Every maximal antichain of f (a#) contains a prefix of X and every prefix of X is a member of at most one element of Vac( f (a#)). If X0 is a prefix of X that is not in a valid antichain, then there is a Z ∈ f (a#) such that for any Z0 ≺ Z, either\n(1) there is a Z1 such that Z0Z1 ∈ f (a#) and X0Z1 < f (a#), or (2) there is a X1 such that X0X1 ∈ f (a#) and Z0X1 < f (a#).\nIn other words, X0 and Z0 have different futures. Thus, for each prefix which is not an element of a valid antichain, there is a translation pair that witnesses this fact. The following figure illustrates the two cases with the possible witnessing strings marked by dashed lines.\nTo describe the required information in the general case, let x0, . . . , xk enumerate the minimal paths to each of the states of G. Let x0, . . . , xn enumerate x0, . . . xk together with all possible one-step extensions of the paths x0, . . . , xk. Note that n is bounded by |states[G]| + |states[G]||E|, where E is the transition relation for G. Fix i ≤ n. If |xi| > 0, let P be the <ac-greatest antichain that is a member of Vac( f (x−i y)) for all strings y such that x−i y ∈ L; if |xi| = 0, define P = {λ}. Define X to be the <llex-least member of P−1 f (x̂i). For each X0 ≺ X that is not a member of a valid antichain of P−1 f (x̂i), there is a Y ∈ P−1 f (x̂i) no prefix of which has the same future in P−1 f (x̂i) as X0 and there is a translation in f (x̂i) witnessing the different futures. We denote the set of such witnessing translation pairs, one for each prefix of X not in a valid antichain, by S i. Let Z be the <llex-least member of P. Let N0(xi) = {〈x̂i, ZX〉} ∪ S i and define N0( f ) = ⋃\ni≤n N0(xi). Observe that N0( f ) is polynomial in the size of G. Consider x ∈ T [L]. Let Vac = ⋂\nx≺y∈L Vac( f (y)). For each P ∈ Vac( f (x)) \\ Vac, observe that there is an example that witnesses the fact that P is not in Vac. Such examples demonstrate violations of either the maximality or the validity of the given antichain. In either case, the witness is a single element of the graph of f (a paired string and translation). Since Vac( f (x)) is finite, the number of examples needed to eliminate all incorrect maximal antichains is also finite. We define N1(x) to be the set which consists of exactly one example for each member of Vac( f (x)) \\Vac. For the sake of a unique definition, we assume that we always choose the <llex-least example – although this is not essential. We can now define the second component of CS f : N1( f ) = ⋃\nq∈states[G] N1(x̂q). N0 and N1 are required to perform onwarding correctly. In order to perform merges, we must include enough data to identify the equivalence classes of states whose futures are the same. There are two ways in which the futures may differ:\n(1) there is a string, z, such that xz ∈ L, but yz < L or (2) for X ∈ G[px] and Y ∈ G[py], there are z and Z such that XZ ∈ G[pxz], but YZ < G[pyz].\nFor each member of states[G] there is a finite collection of examples which uniquely identify the state. Let N2(qx) be a canonically chosen collection of such examples for qx. Let e be a transition and p̂ be the <llex-least path starting at the initial state, ending with a #-transition and including e. Define N∗2(e) to be the set of those translations of p̂ each of which uses a different output of the transition e and is <llex-least amongst the translations of p̂ that use that output. |N∗2(e)| = |output(e)|. We define the final component of CS f as follows.\nN2( f ) = ⋃\nx∈W\nN2(qx) ∪ ⋃\ne∈EG\nN∗2(e),\nwhere W consists of the minimal paths to each state of G as well as all paths that are immediate extensions of those paths.\nDefinition 8.1. For an SDBL, f , we define the characteristic sample of f , CS f = N0( f )∪N1( f )∪N2( f ).\n8.2. Algorithms. In all the algorithms that follow, loops over prefixes of a string will proceed in order of increasing length. Also, when a subroutine returns multiple outputs (e.g., returns all the elements of an array) we assume that an appropriate loop is executed to load the returned values into the selected variables in the main program.\n8.2.1. Initializing the transducer.\nDefinition 8.2. Given a string x over the input alphabet of an SDT G, we say that G is tree-like below x if every path which begins at qx ends at a state which is the end state of exactly one transition. These states are called the states below x. G is said to be tree-like if it is tree-like below its unique initial state.\nConsider a dataset, D. We define an initial transducer by creating a state for every member of T [dom(D)]. A tree-like transducer is produced where all transitions output only λ except for the #- transitions at members of dom(D). All outputs in the dataset are assigned to the #-transitions.\nAlgorithm 1: Forming the initial tree-like transducer (INITIAL) Data: A finite collection of translation pairs, D. Result: A tree-like SDT, GD. for 〈x, X〉 ∈ D do states[GD] ∪ {rx} → states[GD] EGD ∪ {e # x = 〈rx, rλ, #, X〉} → EGD\nif x , λ then for y ≺ x do states[GD] ∪ {ry} → states[GD] EGD ∪ {ey = 〈ry− , ry, y(|y| − 1), λ〉} → EGD\nreturn GD The transducer that results from a run of Algorithm 1 recognizes the translations in D and no other\ntranslations.\n8.2.2. Generating an array of all valid antichains. In order to simplify the presentation of the algorithms, we will not include the algorithms for several simple functions. In particular, we will assume that LEXORDER(A) takes an array, A, as an input and returns an array with the same contents as A, but in lexicographic order. LLEXORDER(A) performs the same function, but for the <llex-ordering. LEXLEAS T and LLEXLEAS T will be applied to sets and arrays and will return the <lex- and <llex-least member, respectively. For sets of strings P and S , we will use the operations P−1S and P ∗ S as built-in arithmetic operations. Given an input string, x, output strings, Z and W, and a set of translation pairs, D, the function COMPARE(x, Z,W,D) returns true if, for every 〈x, ZR〉, 〈x,WS 〉 ∈ D, the queries [x,WR] f\nand [x, ZS ] f return values of true. Otherwise, COMPARE(x, Z,W,D) returns false. Applying the same notation used above, if x is an input string, then x̂ is the <llex-least member of L extending x. Using these functions, we define an algorithm to create a list of all valid antichains when considering the tree of outputs of a single input string.\nAlgorithm 2: List the valid antichains (VAC) Data: A finite collection of translation pairs, D; x ∈ L; Xℓ, the current least translation prefix for x. Result: An array, A, of all maximal antichains of the translations of x in D which extend Xℓ and\nare not provably invalid. X−1 ℓ {Y : Y ≻ Xℓ ∧ 〈x, Y〉 ∈ D} → T LLEXLEAS T (T ) → Z for W ≺ Z do\nW → AC[0] for R ∈ T ∧ R , Z do\nfor V ≺ R do COMPARE(x, XℓW, XℓV,D) → status if status = true then\nV → AC[|AC|] break\nif status = false then break\nif status = true then AC → A[|A|]\nreturn A\nOne of the inputs of Algorithm 2 is the “current least translation prefix of x”. The current translation prefix will converge to the <llex-least output string generated along the unique path corresponding to x. Xℓ provides a canonical output prefix for testing outputs using translation queries. The first step of Algorithm 2 restricts D to the tree of translation pairs whose second component extends the least translation prefix. Every antichain of the tree must contain a prefix of the <llex-least member of the tree. Because of the linear ordering of the valid antichains (see Theorem 4.9), there is at most one valid antichain for each prefix of the least member of the tree. COMPARE is used to look for matching nodes to form valid antichains. As can be seen in the figure, all valid antichains include prefixes of the <llex-least member and no two valid antichains contain the same prefix. This\nprovides both a bound on the number of valid antichains and a convenient method to search for the valid antichains.\nWe formalize the above intuition in the proof of the following lemma.\nLemma 8.3. Let D be a finite set consistent with an SDBL f over L with canonical transducer G and x ∈ L. Suppose CS f ⊆ D, x is <llex-least among y ∈ L such that qy = qx, Xℓ is the least translation prefix of x and X is the <llex-least member of f (x). Given inputs D, x and Xℓ and given access to translation queries about f , Algorithm 2 outputs an array of antichains A such that if V is the set of valid antichains of translations of x which extend Xℓ, then each antichain in A is extended by an antichain in V and each antichain in V contains a unique antichain in A. Furthermore, A contains the unique antichain which is a valid antichain of all translations of y that extend Xℓ for all y ∈ L such that y x.\nProof. Since x is <llex-least such that px is a path to the state at which px terminates, CS f (hence, D also) contains 〈x, X〉. Furthermore, for each prefix Y such that Xℓ Y X, if Y is not a member of a valid antichain, then D contains witnessing strings so that this can be determined using translation queries (this is the content of N0(x) defined in Section 8.1). Algorithm 2 performs exactly those translation queries necessary to determine that Y is not a member of a valid antichain. Thus, the array of antichains that the algorithm returns will correctly exclude all subsets of maximal antichains that contain such a Y.\nWe now show that each antichain in the output must be a subset of a valid antichain. For the sake of a contradiction, suppose an antichain in A contains Y and Z where Xℓ Y X and Y is a member of a valid antichain, but the unique member of V that contains Y does not contain Z. By the definition of the N0 component of CS f , D will contain a string such that when COMPARE is run, Y and Z will be flagged as not members of the same valid antichain.\nLet F be the <sd f -maximum SDF for f . By the definition of N2( f ), for each Z ∈ F(x) there must be a Y such that 〈x̂, Y〉 ∈ D and XℓZ Y. Consequently, A will contain {Xℓ} ∗ F(x), which is the unique antichain which is a valid antichain of the extensions of Xℓ in f (y) for all y ∈ L such that x y.\n8.2.3. Performing onwarding on a single node. The next algorithm takes an array of antichains and produces the <ac-greatest antichain that appears to be a valid antichain of all trees of outputs on inputs extending x. As the data may still be incomplete, testing the validity for other trees is done using translation queries.\nAlgorithm 3: Testing an array of antichains against a dataset (TESTVPS) Data: A string, x, over the input alphabet; an array, A, of antichains for the output tree of input x̂; a\ncollection of translation pairs, D. Result: The <ac-greatest member of the array, A, for which there is no evidence in D that the selected antichain is not valid for all output trees in the future of x. for i = |A| − 1; i ≥ 0; i − − do\n‘not valid’ → status for 〈xy, Z〉 ∈ D do\nfor R ∈ A[i] do if R ≺ Z then\nR−1Z → W ‘valid’ → status for Q ∈ A[i] do\nif [xy, QW] f = false then ‘not valid’ → status break\nif status = ‘not valid’ then break\nif status = ‘valid’ then return A[i]\nObserve that there will always be a valid antichain that causes the above algorithm to terminate; if there is no other, then it will terminate on {λ}. In the following algorithm, we use null to test for the existence of an optional argument.\nAlgorithm 4: Onwarding a tree-like portion of a transducer (ONWARD) Data: A string x; a transducer, G, which is tree-like below a string, x; Xℓ, the current least\ntranslation prefix for x; a collection of translation pairs, D; a set of strings, S (optional). Result: A transducer that differs from G only on transitions whose end state is qx or a state below x. S → P if P = null then\nVAC(D, x, Xℓ) → A T ES TVPS (x, A,D) → P\noutput(ex) ∗ P → output(ex) for y ∈ dom(D) ∧ x ≺ y do\nP−1output(ey) → output(ey)\nThe purpose of Algorithm 4 is to advance as much translation as possible in a tree-like portion of a transducer.\n8.2.4. Merging states. Following conventions presented in [19], we will label states during the learning process as red states if it is not possible to merge them with any <llex-lesser state. Initially, only the input state, qλ, is a red state. We proceed through the states in <llex-order. When a new state is found that cannot be merged with any red state, then it becomes a new red state.\nThe next algorithm we present merges two states if there is no evidence that the underlying transducer behaves differently on extensions of the inputs of the two states. In this operation, we assume that the first argument is a red state, the second argument is not, and that onwarding has already been performed for both states. In order to present the algorithm succinctly, we define a function similar to COMPARE from Section 8.2.2. Define FUTURE(x, y,G,D) = true if\n( ∀X ∈ G[px] ∩ ran(D),Y ∈ G[py] ∩ ran(D), 〈z, Z〉 ∈ D )\n(\n(x z ∧ X Z → [y(x−1z), Y0(X−1Z)] f = true)\n∧ (y z ∧ Y Z → [x(y−1z), X0(Y−1Z)] f = true)\n)\n,\nwhere X0 = LLEXLEAS T (G[px]) and Y0 = LLEXLEAS T (G[py]). Otherwise, FUTURE(x, y,G,D) = false. Note that finding LLEXLEAS T (G[px]) does not require enumeration all elements of G[px], which\ncould be exponential in the length of x. To determine LLEXLEAS T (G[px]), one need only find the least element of each set of translations along the path px.\nAlgorithm 5: MERGE Data: A red state, qx; a non-red state, qy; a transducer, G, that is tree-like below qy; a collection of\ntranslation pairs, D. Result: A transducer; a boolean value of true if the two states have been merged and false otherwise. FUTURE(x, y,G,D) → status if status = true then\nqx → end(ey) states[G] \\ {qy} → states[G] for z ∈ dom(D) ∧ z ≻ y do\nfor y ≺ w z do if qx(y−1w) ∈ states[G] then states[G] \\ {qw} → states[G]\nqx(y−1w) → start(ew) qx(y−1w)input(ew) → end(ew)\nreturn 〈G, true〉 else\nreturn 〈G, false〉\nIf G is a transducer generated from a dataset, it is likely that G will include non-equivalent states for which there is no evidence in their futures to distinguish them. Ultimately, this will not be an obstacle to learning because if the characteristic sample has appeared, there will be enough data to distinguish earlier states that will be processed first.\n8.2.5. The learning algorithm. Our final algorithm combines onwarding and merging into a single process. We proceed through the states of the initial transducer in <llex-order, first onwarding and then attempting to merge with lesser states. If a state cannot be merged with any lesser state, it is fixed and\nwill not subsequently be changed. The fact that such states are fixed is recorded by their membership in a set red.\nAlgorithm 6: Learning an SDT Data: A collection of translation pairs, D. Result: A transducer. INIT IAL(D) → G0 LLEXORDER(states[G0]) → S qλ → red[0] 0 → i for qx ∈ S do\nif qx ∈ red ∨ qx− < red then continue else ONWARD(x,G, LLEXLEAS T (G[px]),D) → G for qy ∈ red do\nMERGE(qy, qx,G,D) → 〈G, status〉 if status = true then\nbreak\nif status = false then x → red[i] i + +\nLemma 8.4. Let f be an SDBL with canonical SDT G and let D be a finite set consistent with f which contains CS f . At every stage during the execution of Algorithm 6 with input D and given access to translation queries about f , if G′ is the SDT constructed so far and px is a path through G′ that exclusively involves red states, then G[px] = G′[px]. Furthermore, if x and y are strings such that such that their unique paths px and py terminate at different red states of G′, then the states qx and qy of G are distinct.\nProof. Let G0 be the SDT that results from Algorithm 1 and let F be the <sd f -maximum SDF for f . We prove the lemma by induction. Initially, the only red state is the initial state and the lemma holds trivially. Now suppose that the lemma holds for G′ at the beginning of an iteration of the main for-loop in Algorithm 6. Let G′′ be the result of executing the next iteration of the for-loop.\nIf no new red states have been added, then a previously non-red state, q1, had Algorithm 4 applied to it and was merged with a red state, q0. Let x be the <llex-least string such that the path px in G′ ends at qx = q1. Since q1 was not a red state, G′ must have been tree-like below x and because of the induction hypothesis, the correct least translation prefix for x will have been used by Algorithm 4. Furthermore, since the unique state in G′ with a transition to q1 is a red state, by the definition of N0( f ), D contains examples to guarantee that Algorithm 2 correctly identifies valid antichains. Thus, Algorithm 4 must have identified the unique <ac-greatest antichain which is a valid antichain of f (y) for all y ∈ L such that y x. By the induction hypothesis, CS f ⊆ D must contain examples that uniquely identify the future of q0. Since precisely those examples from D will be tested for q1 using translation queries when Algorithm 5 is run, the fact that q0 and q1 were merged implies that q0 ≡ q1. Consequently, all paths that only visits red states of G′′ satisfy the lemma. Since no new red states were introduced, the second conclusion in the statement of the lemma follows automatically from the induction hypothesis.\nNow suppose that a new red state, q1, is selected. Since the state was marked as a red state, no <llex-lesser state is equivalent, meaning that if x is <llex-least such that px in G′ ends at q1, then x defines the <llex-least path to some state of G. Consequently, CS f ⊆ D contains examples to guarantee that Algorithm 4 identifies the correct antichain, F(x). Since q1 was not merged with any existing red state, there must be examples in D that distinguish q1 from each of the red states. Thus, for each such red state,\nq, we know that q1 . q. Since the for-loop in Algorithm 6 proceeds through the states in <llex-order, this means that if x is <llex-least such that the path px in G′ ends at q1, then x is <llex-least such that the state qx in G has the same future as q1. We may conclude, therefore, that CS f ⊆ D contains examples that uniquely identify the future of q1. Finally, to prove that G′′[pz] = G[pz] for any z such that pz that only visits red states of G′′ we need only consider paths that end at q1 and do not visit q1 at any other point. There is only one state, q, in G′′ that has a transition to q1 (because G′′ is tree-like below q1) and that state is a red state in both G′ and G′′. Thus, G[pz−] = G′[pz−] = G′′[pz−]. The transition from q to q1 with input z(|z| − 1) has output F(x), thus G′′[pz] = G′′[pz−] ∗ F(x) = G[pz].\n8.3. Learnability of SDBLs.\nTheorem 8.5. The class of SDBLs is polynomially identifiable in the limit with translation queries.\nProof. Let f be an SDBL with canonical SDT G and let D be a collection of translation pairs consistent with f and containing CS f . We apply Algorithm 6 to learn f from D. To prove that Algorithm 6 identifies f in the limit in polynomial time, we must verify three claims. First, we must show that the size of the chosen characteristic sample is polynomial in the size of the canonical transducer of the target. Second, we must show that the algorithm terminates within a number of steps that is polynomial in the size of the canonical transducer of the target and in the size of the given data. Third, we must show the SDT produced by Algorithm 6 generates f .\nThe first claim is easy. As noted in the section in which CS f was defined, N0( f ), N1( f ) and N2( f ) are all polynomial in the size of G.\nAn inspection of the algorithms shows that they converge in polynomial time and that only a polynomial number of translation queries are made. We conclude that the second claim is true.\nFinally, we prove the third claim. Algorithm 6 terminates at the point when every state in the SDT generated by Algorithm 1 has been either marked as a red state or merged with another state. Suppose that G′ is the output of Algorithm 6. When the algorithm terminates, every state is a red state. Consequntly, for any x for which there is a path px in G′, by Lemma 8.4 G[px] = G′[px]. Thus, we need only prove that for all x, if x defines a path through G then x defines a path through G′. By the definition of N0( f ), every transition in G is used at least once by translations in CS f . Fix x which defines a path through G and let xa be an extension by the single character a. If xa also defines a path through G, then G has a transition e such that start(e) is the final state of the path defined by x and input(e) = a. Let G0 be the SDT generated by Algorithm 1 and let 〈z0az1, Z0YZ1〉 be a translation pair in CS f such that the path through G defined by z0az1 uses e when translating the character a. Let qz0 and qz0a be the states of G0 corresponding to the initial segments z0 and z0a of z0az1. Let x0 be the <llex-least string to the final state of the path through G defined by x. Let qx0 be state of G0 corresponding to x0. By the definition of N2( f ), CS f contains examples that uniquely identify the future of qx0 . When FUTURE(x0, z0a,G\n′′,D) is run at some point during the execution of Algorithm 6 (where G′′ is the current form of transducer under construction), the two strings will be recognized as having the same futures and will be merged. Thus, if we assume that the paths through G′ defined by x and x0 are the same, then xa defines a path through G′. By induction on the length of x, we have shown the every string that defines a path through G also defines a path through G′.\n9. Related Results\nWe establish the relationship between SDTs and two other classes of transducers that are not entirely deterministic: p-subsequential transducers and transducers that recognize R f f b relations. We also look at some properties of SDBLs; specifically, we show that SDBLs are not closed under composition or reversal.\n9.1. Other Forms of Non-Determinism. The following definition is adapted from [2].\nDefinition 9.1. A transducer is said to be p-subsequential for some p ∈ N if for every transition e in the transition relation, |output(e)| = 1 unless input(e) = #, in which case |output(e)| ≤ p. We say the a bi-language is p-subsequential if it is generated by a p-subsequential transducer.\nDefinition 9.2. [33] A binary relation, R, is finitary if for every x ∈ dom(R) the set {y : 〈x, y〉 ∈ R} is finite. If there is a number n ∈ N such that |{y : 〈x, y〉 ∈ R}| ≤ n for all x, then the relation is said to be bounded. Such a relation is finite-state if there is a transducer that generates R. We say that a relation is R f f b if it is finitary, finite-state and bounded. We say that a transducer is R f f b if the bi-language it generates is R f f b.\nProposition 9.3. Every p-subsequential bi-language is R f f b.\nProof. A p-subsequential bi-language is finitary and bounded as every input string has at most p distinct translations. It is also clearly finite-state as it is generated by a p-subsequential transducer. Thus, every p-subsequential bi-language is also R f f b.\nProposition 9.4. There is a p-subsequential bi-language which is not an SDBL.\nProof. If f is an SDBL, x ∈ dom( f ) and X, Y ∈ f (x), then either X = Y or X and Y are incomparable. Thus, f : {a} → {{A, AA}} such that f (a) = {A, AA} is not an SDBL. It is, however, p-subsequential.\nProposition 9.5. There is an SDBL which is neither p-subsequential nor R f f b.\nProof. Let G be a transducer with a single state, which is a terminal state, and one transition which starts and ends at the unique state, has input a and output {A, B}. G is shown in Figure 9.\nThe set of translation pairs recognized by G is {〈an, X〉 : n ∈ N∧ X ∈ {A, B}n}, which is not a bounded relation as an is in the domain for every n and has 2n translations. Since it is not bounded, it is neither R f f b nor p-subsequential.\nProposition 9.6. There is an R f f b bi-language which is neither an SDBL nor p-subsequential.\nProof. Let G be the transducer in the following figure.\nThe set of translation pairs recognized by G is S = {〈an+2, Bn+2〉 : n ∈ N} ∪ {〈an+2, ABnA〉 : n ∈ N}. Suppose G′ is an SDT that recognizes S . For every n, the input string an+2 has exactly two translations: Bn+2 and ABnA. Since G′ is has finitely many states, the first transition on the path through G′ defined by an+2 with an output other than {λ} must have {AX, BY} as a subset for some strings X and Y. Furthermore, the last transition on the path with output other than {λ} must have {ZA,WB} as a subset for some strings Z and W. This implies that an+2 should have at least four distinct translations, which is a contradiction.\nSimilarly, suppose S is recognized by a p-subsequential transducer Gp. Since the two translations of an+2 differ at their first character, it must be that for every path through Gp all translation occurs at the final transition (since only terminal transitions may have more than one output). Given this observation, Gp must have infinitely many states – one for each string an+2. This is a contradiction.\n9.2. Composition and Reversal of SDBLs.\nDefinition 9.7. Let f and g be two bi-languages. We define the composition of f and g to be the bilanguage h such that dom(h) = dom( f ) and for every x ∈ dom( f ), h(x) = ⋃\nX∈ f (x) g(X). We define the reversal of f to be the bi-language k with domain S = ⋃\nx∈dom( f ) f (x) such that k(X) = {x ∈ dom( f ) : X ∈ f (x)}.\nProposition 9.8. There are SDBLs whose composition is not an SDBL.\nProof. Let G f and Gg be the following two SDTs and let f and g be the SDBLs generated by them.\nObserve that if h is the composition of f and g, then h(a) = {0, 01}. Since 0 is a prefix of 01, h cannot be an SDBL.\nProposition 9.9. There is an SDBL whose reversal is not an SDBL.\nProof. Let G be the following SDT and let f be the SDBL generated by G.\nFor each n, the reversal of f has two translations of An+2, banb and canc. From this observation we see that the reversal of f cannot be an SDBL for exactly the same reason that the bi-language generated by the transducer in Figure 10 cannot be an SDBL.\n10. Conclusion\nWe have presented a novel algorithm that learns a powerful class of transducers with the help of reasonable queries. A probabilistic version of these transducers was defined in [1]. We are unaware of any results involving this version. As both probabilities and translation queries can serve the purpose of answering questions about translation pairs not present in the given data, it seems possible that probabilistic transducers could be learned without translation queries, with statistical analysis taking the role of translation queries.\nThe learnability of SDBLs represents a significant advance in our ability to identify underlying structure in the challenging situation where the structure is non-deterministic. While it does not strictly expand existing models (as can be seen from Proposition 9.4), it does contribute an enormous class of new bi-languages which are beyond the scope of deterministic transducers.\n11. Acknowledgements\nWe would like the thank the anonymous referees for many useful comments, corrections and suggestions – specifically, for suggesting that we examine the relationship between SDBLs, p-subsequential bi-languages and R f f b relations.\nReferences\n[1] H. I. Akram. Learning Probabilistic Subsequential Transducers. PhD thesis, Technische Universität München, 2013. [2] C. Allauzen and M. Mohri. p-subsequentiable transducers. In Implementation and Application of Automata, 7th International\nConference, Ciaa 2002, Revised Papers, volume 2608 of Lncs, pages 24–34. Springer-Verlag, 2002. [3] J. C. Amengual, J. M. Benedı́, F. Casacuberta, A. Castaño, A. Castellanos, V. M. Jiménez, D. Llorens, A. Marzal, M. Pastor,\nF. Prat, E. Vidal, and J. M. Vilar. The EuTrans-I speech translation system. Machine Translation, 15(1):75–103, 2001. [4] D. Angluin. Queries and concept learning. Machine Learning Journal, 2:319–342, 1987. [5] M. Bernard, J.-C. Janodet, and M. Sebban. A discriminative model of stochastic edit distance in the form of a conditional\ntransducer. In Y. Sakakibara, S. Kobayashi, K. Sato, T. Nishino, and E. Tomita, editors, Grammatical Inference: Algorithms and Applications, Proceedings of Icgi ’06, volume 4201 of Lnai, pages 240–252. Springer-Verlag, 2006.\n[6] A. Beros. Anomalous vacillatory learning. Journal of Symbolic Logic, 78(4):1183–1188, 12 2013. [7] A. Beros. Learning theory in the arithmetic hierarchy. Journal of Symbolic Logic, 79(3):908–927, 9 2014. [8] Achilles Beros and Colin de la Higuera. A canonical semi-deterministic transducer. arXiv preprint arXiv:1405.2476, 2014. [9] J. Berstel. Transductions and context-free languages. Teubner, Leipzig, 1979.\n[10] M. Blum and L. Blum. Towards a mathematical theory of inductive inference. Information and Control, 28:125–155, 1975. [11] J. Carme, R. Gilleron, A. Lemay, and J. Niehren. Interactive learning of node selecting tree transducer. Machine Learning\nJournal, 66:33–67, 2007. [12] F. Casacuberta and C. de la Higuera. Optimal linguistic decoding is a difficult computational problem. Pattern Recognition\nLetters, 20(8):813–821, 1999. [13] F. Casacuberta and C. de la Higuera. Computational complexity of problems on probabilistic grammars and transducers. In\nA. L. de Oliveira, editor, Grammatical Inference: Algorithms and Applications, Proceedings of Icgi ’00, volume 1891 of Lnai, pages 15–24. Springer-Verlag, 2000. [14] F. Casacuberta and E. Vidal. Machine translation with inferred stochastic finite-state transducers. Computational Linguistics, 30(2):205–225, 2004. [15] A. Clark. Partially supervised learning of morphology with stochastic transducers. In Proceedings of the Sixth Natural Language Processing Pacific Rim Symposium, pages 341–348, 2001. [16] A. Clark. Large scale inference of deterministic transductions: Tenjinno problem 1. In Y. Sakakibara, S. Kobayashi, K. Sato, T. Nishino, and E. Tomita, editors, Grammatical Inference: Algorithms and Applications, Proceedings of Icgi ’06, volume 4201 of Lnai, pages 227–239. Springer-Verlag, 2006. [17] F. Coste, D. Fredouille, C. Kermorvant, and C. de la Higuera. Introducing domain and typing bias in automata inference. In G. Paliouras and Y. Sakakibara, editors, Grammatical Inference: Algorithms and Applications, Proceedings of Icgi ’04, volume 3264 of Lnai, pages 115–126. Springer-Verlag, 2004. [18] C. de la Higuera. Characteristic sets for polynomial grammatical inference. Machine Learning Journal, 27:125–138, 1997. [19] Colin de la Higuera. Grammatical Inference: Learning Automata and Grammars. Cambridge University Press, 2010. [20] E. M. Gold. Language identification in the limit. Information and Control, 10(5):447–474, 1967. [21] Thomas Jech. Set theory. Springer Monographs in Mathematics. Springer-Verlag, Berlin, 2003. The third millennium edition,\nrevised and expanded. [22] C. Kermorvant and C. de la Higuera. Learning languages with help. In P. Adriaans, H. Fernau, and M. van Zaannen, edi-\ntors, Grammatical Inference: Algorithms and Applications, Proceedings of Icgi ’02, volume 2484 of Lnai, pages 161–173. Springer-Verlag, 2002. [23] K. Kunen. Set theory: an introduction to independence proofs, volume 102 of Studies in Logic and the Foundations of Mathematics. North-Holland Publishing Co., Amsterdam-New York, 1980. [24] M. Mohri. Finite-state transducers in language and speech processing. Computational Linguistics, 23(3):269–311, 1997. [25] M. Mohri. Minimization algorithms for sequential transducers. Theoretical Computer Science, 234:177–201, 2000. [26] M. Mohri, F. C. N. Pereira, and M. Riley. The design principles of a weighted finite-state transducer library. Theoretical\nComputer Science, 231(1):17–32, 2000. [27] A. Nerode. Linear automaton transformations. Proceedings of the American Mathematical Society, 9(4):541–544, 1958. [28] Maurice Nivat. Transductions des langages de chomsky. In Annales de l’institut Fourier, volume 18/1, pages 339–455. Institut\nFourier, 1968. [29] J. Oncina, P. Garcı́a, and E. Vidal. Learning subsequential transducers for pattern recognition interpretation tasks. Pattern\nAnalysis and Machine Intelligence, 15(5):448–458, 1993. [30] J. Oncina and M. A. Varó. Using domain information during the learning of a subsequential transducer. In L. Miclet and\nC. de la Higuera, editors, Proceedings of Icgi ’96, number 1147 in Lnai, pages 301–312. Springer-Verlag, 1996. [31] B. Roark and R. Sproat. Computational Approaches to Syntax and Morphology. Oxford University Press, 2007. [32] J. Sakarovitch. Elements of Automata Theory. Cambridge University Press, 2009. [33] Jacques Sakarovitch and Rodrigo De Souza. On the decidability of bounded valuedness for transducers. In Mathematical\nFoundations of Computer Science 2008, volume 5162 of Lncs, pages 588–600. Springer, 2008. [34] B. Starkie, M. van Zaanen, and D. Estival. The Tenjinno machine translation competition. In Y. Sakakibara, S. Kobayashi,\nK. Sato, T. Nishino, and E. Tomita, editors, Grammatical Inference: Algorithms and Applications, Proceedings of Icgi ’06, volume 4201 of Lnai, pages 214–226. Springer-Verlag, 2006. [35] L. G. Valiant. A theory of the learnable. Communications of the Association for Computing Machinery, 27(11):1134–1142, 1984. [36] J. M. Vilar. Query learning of subsequential transducers. In L. Miclet and C. de la Higuera, editors, Proceedings of Icgi ’96, number 1147 in Lnai, pages 72–83. Springer-Verlag, 1996. [37] J. M. Vilar. Improve the learning of subsequential transducers by using alignments and dictionaries. In A. L. de Oliveira, editor, Grammatical Inference: Algorithms and Applications, Proceedings of Icgi ’00, volume 1891 of Lnai, pages 298–312. Springer-Verlag, 2000. [38] J.M. Vilar, V. M. Jiménez, J-C. Amengual, A. Castellanos, D. Llorens, and E. Vidal. Text and speech translation by means of subsequential transducers. Natural Language Engineering, 2(4):351–354, 1996."
    } ],
    "references" : [ {
      "title" : "Learning Probabilistic Subsequential Transducers",
      "author" : [ "H.I. Akram" ],
      "venue" : "PhD thesis, Technische Universität München",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "p-subsequentiable transducers",
      "author" : [ "C. Allauzen", "M. Mohri" ],
      "venue" : "Implementation and Application of Automata, 7th International Conference, Ciaa 2002, Revised Papers, volume 2608 of Lncs, pages 24–34. Springer-Verlag",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "J",
      "author" : [ "J.C. Amengual" ],
      "venue" : "M. Benedı́, F. Casacuberta, A. Castaño, A. Castellanos, V. M. Jiménez, D. Llorens, A. Marzal, M. Pastor, F. Prat, E. Vidal, and J. M. Vilar. The EuTrans-I speech translation system. Machine Translation, 15(1):75–103",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Queries and concept learning",
      "author" : [ "D. Angluin" ],
      "venue" : "Machine Learning Journal, 2:319–342",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 1987
    }, {
      "title" : "A discriminative model of stochastic edit distance in the form of a conditional transducer",
      "author" : [ "M. Bernard", "J.-C. Janodet", "M. Sebban" ],
      "venue" : "Y. Sakakibara, S. Kobayashi, K. Sato, T. Nishino, and E. Tomita, editors, Grammatical Inference: Algorithms and Applications, Proceedings of Icgi ’06, volume 4201 of Lnai, pages 240–252. Springer-Verlag",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Anomalous vacillatory learning",
      "author" : [ "A. Beros" ],
      "venue" : "Journal of Symbolic Logic, 78(4):1183–1188,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2013
    }, {
      "title" : "Learning theory in the arithmetic hierarchy",
      "author" : [ "A. Beros" ],
      "venue" : "Journal of Symbolic Logic, 79(3):908–927,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2014
    }, {
      "title" : "A canonical semi-deterministic transducer",
      "author" : [ "Achilles Beros", "Colin de la Higuera" ],
      "venue" : "arXiv preprint arXiv:1405.2476,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2014
    }, {
      "title" : "Transductions and context-free languages",
      "author" : [ "J. Berstel" ],
      "venue" : "Teubner, Leipzig",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 1979
    }, {
      "title" : "Towards a mathematical theory of inductive inference",
      "author" : [ "M. Blum", "L. Blum" ],
      "venue" : "Information and Control, 28:125–155",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 1975
    }, {
      "title" : "Interactive learning of node selecting tree transducer",
      "author" : [ "J. Carme", "R. Gilleron", "A. Lemay", "J. Niehren" ],
      "venue" : "Machine Learning Journal, 66:33–67",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Optimal linguistic decoding is a difficult computational problem",
      "author" : [ "F. Casacuberta", "C. de la Higuera" ],
      "venue" : "Pattern Recognition Letters,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1999
    }, {
      "title" : "Computational complexity of problems on probabilistic grammars and transducers",
      "author" : [ "F. Casacuberta", "C. de la Higuera" ],
      "venue" : "Grammatical Inference: Algorithms and Applications, Proceedings of Icgi ’00,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2000
    }, {
      "title" : "Machine translation with inferred stochastic finite-state transducers",
      "author" : [ "F. Casacuberta", "E. Vidal" ],
      "venue" : "Computational Linguistics, 30(2):205–225",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Partially supervised learning of morphology with stochastic transducers",
      "author" : [ "A. Clark" ],
      "venue" : "Proceedings of the Sixth Natural Language Processing Pacific Rim Symposium, pages 341–348",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Large scale inference of deterministic transductions: Tenjinno problem 1",
      "author" : [ "A. Clark" ],
      "venue" : "Y. Sakakibara, S. Kobayashi, K. Sato, T. Nishino, and E. Tomita, editors, Grammatical Inference: Algorithms and Applications, Proceedings of Icgi ’06, volume 4201 of Lnai, pages 227–239. Springer-Verlag",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "and C",
      "author" : [ "F. Coste", "D. Fredouille", "C. Kermorvant" ],
      "venue" : "de la Higuera. Introducing domain and typing bias in automata inference. In G. Paliouras and Y. Sakakibara, editors, Grammatical Inference: Algorithms and Applications, Proceedings of Icgi ’04, volume 3264 of Lnai, pages 115–126. Springer-Verlag",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Characteristic sets for polynomial grammatical inference",
      "author" : [ "C. de la Higuera" ],
      "venue" : "Machine Learning Journal,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 1997
    }, {
      "title" : "Grammatical Inference: Learning Automata and Grammars",
      "author" : [ "Colin de la Higuera" ],
      "venue" : null,
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2010
    }, {
      "title" : "Language identification in the limit",
      "author" : [ "E.M. Gold" ],
      "venue" : "Information and Control, 10(5):447–474",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 1967
    }, {
      "title" : "The third millennium edition, revised and expanded",
      "author" : [ "Thomas Jech" ],
      "venue" : "Set theory. Springer Monographs in Mathematics. Springer-Verlag, Berlin,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2003
    }, {
      "title" : "Learning languages with help",
      "author" : [ "C. Kermorvant", "C. de la Higuera" ],
      "venue" : "Grammatical Inference: Algorithms and Applications, Proceedings of Icgi ’02,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2002
    }, {
      "title" : "Set theory: an introduction to independence proofs",
      "author" : [ "K. Kunen" ],
      "venue" : "volume 102 of Studies in Logic and the Foundations of Mathematics. North-Holland Publishing Co., Amsterdam-New York",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 1980
    }, {
      "title" : "Finite-state transducers in language and speech processing",
      "author" : [ "M. Mohri" ],
      "venue" : "Computational Linguistics, 23(3):269–311",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Minimization algorithms for sequential transducers",
      "author" : [ "M. Mohri" ],
      "venue" : "Theoretical Computer Science, 234:177–201",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "The design principles of a weighted finite-state transducer library",
      "author" : [ "M. Mohri", "F.C.N. Pereira", "M. Riley" ],
      "venue" : "Theoretical Computer Science, 231(1):17–32",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Linear automaton transformations",
      "author" : [ "A. Nerode" ],
      "venue" : "Proceedings of the American Mathematical Society, 9(4):541–544",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 1958
    }, {
      "title" : "Transductions des langages de chomsky",
      "author" : [ "Maurice Nivat" ],
      "venue" : "In Annales de l’institut Fourier,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 1968
    }, {
      "title" : "P",
      "author" : [ "J. Oncina" ],
      "venue" : "Garcı́a, and E. Vidal. Learning subsequential transducers for pattern recognition interpretation tasks. Pattern Analysis and Machine Intelligence, 15(5):448–458",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 1993
    }, {
      "title" : "Using domain information during the learning of a subsequential transducer",
      "author" : [ "J. Oncina", "M.A. Varó" ],
      "venue" : "L. Miclet and C. de la Higuera, editors, Proceedings of Icgi ’96, number 1147 in Lnai, pages 301–312. Springer-Verlag",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "Computational Approaches to Syntax and Morphology",
      "author" : [ "B. Roark", "R. Sproat" ],
      "venue" : "Oxford University Press",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Elements of Automata Theory",
      "author" : [ "J. Sakarovitch" ],
      "venue" : "Cambridge University Press",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "On the decidability of bounded valuedness for transducers",
      "author" : [ "Jacques Sakarovitch", "Rodrigo De Souza" ],
      "venue" : "In Mathematical Foundations of Computer Science 2008, volume 5162 of Lncs,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2008
    }, {
      "title" : "M",
      "author" : [ "B. Starkie" ],
      "venue" : "van Zaanen, and D. Estival. The Tenjinno machine translation competition. In Y. Sakakibara, S. Kobayashi, K. Sato, T. Nishino, and E. Tomita, editors, Grammatical Inference: Algorithms and Applications, Proceedings of Icgi ’06, volume 4201 of Lnai, pages 214–226. Springer-Verlag",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "A theory of the learnable",
      "author" : [ "L.G. Valiant" ],
      "venue" : "Communications of the Association for Computing Machinery, 27(11):1134–1142",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 1984
    }, {
      "title" : "Query learning of subsequential transducers",
      "author" : [ "J.M. Vilar" ],
      "venue" : "L. Miclet and C. de la Higuera, editors, Proceedings of Icgi ’96, number 1147 in Lnai, pages 72–83. Springer-Verlag",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "Improve the learning of subsequential transducers by using alignments and dictionaries",
      "author" : [ "J.M. Vilar" ],
      "venue" : "A. L. de Oliveira, editor, Grammatical Inference: Algorithms and Applications, Proceedings of Icgi ’00, volume 1891 of Lnai, pages 298–312. Springer-Verlag",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Text and speech translation by means of subsequential transducers",
      "author" : [ "J.M. Vilar", "V.M. Jiménez", "J-C. Amengual", "A. Castellanos", "D. Llorens", "E. Vidal" ],
      "venue" : "Natural Language Engineering, 2(4):351–354",
      "citeRegEx" : "38",
      "shortCiteRegEx" : null,
      "year" : 1996
    } ],
    "referenceMentions" : [ {
      "referenceID" : 27,
      "context" : "Introduction Transducers, introduced by [28], are a type of abstract machine which defines a relation between two formal languages.",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 8,
      "context" : "We provide no background on formal languages in this paper; an overview of the subject can be found in [9] and [32].",
      "startOffset" : 103,
      "endOffset" : 106
    }, {
      "referenceID" : 31,
      "context" : "We provide no background on formal languages in this paper; an overview of the subject can be found in [9] and [32].",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 23,
      "context" : "This view was introduced by Mohri, who uses transducers in the context of natural language processing [24, 25] and [26].",
      "startOffset" : 102,
      "endOffset" : 110
    }, {
      "referenceID" : 24,
      "context" : "This view was introduced by Mohri, who uses transducers in the context of natural language processing [24, 25] and [26].",
      "startOffset" : 102,
      "endOffset" : 110
    }, {
      "referenceID" : 25,
      "context" : "This view was introduced by Mohri, who uses transducers in the context of natural language processing [24, 25] and [26].",
      "startOffset" : 115,
      "endOffset" : 119
    }, {
      "referenceID" : 30,
      "context" : "Among many others, some well known applications are in the fields of morphology and phonology [31], machine translation [3, 14, 15], web wrappers [11], speech [24] and pattern recognition [5].",
      "startOffset" : 94,
      "endOffset" : 98
    }, {
      "referenceID" : 2,
      "context" : "Among many others, some well known applications are in the fields of morphology and phonology [31], machine translation [3, 14, 15], web wrappers [11], speech [24] and pattern recognition [5].",
      "startOffset" : 120,
      "endOffset" : 131
    }, {
      "referenceID" : 13,
      "context" : "Among many others, some well known applications are in the fields of morphology and phonology [31], machine translation [3, 14, 15], web wrappers [11], speech [24] and pattern recognition [5].",
      "startOffset" : 120,
      "endOffset" : 131
    }, {
      "referenceID" : 14,
      "context" : "Among many others, some well known applications are in the fields of morphology and phonology [31], machine translation [3, 14, 15], web wrappers [11], speech [24] and pattern recognition [5].",
      "startOffset" : 120,
      "endOffset" : 131
    }, {
      "referenceID" : 10,
      "context" : "Among many others, some well known applications are in the fields of morphology and phonology [31], machine translation [3, 14, 15], web wrappers [11], speech [24] and pattern recognition [5].",
      "startOffset" : 146,
      "endOffset" : 150
    }, {
      "referenceID" : 23,
      "context" : "Among many others, some well known applications are in the fields of morphology and phonology [31], machine translation [3, 14, 15], web wrappers [11], speech [24] and pattern recognition [5].",
      "startOffset" : 159,
      "endOffset" : 163
    }, {
      "referenceID" : 4,
      "context" : "Among many others, some well known applications are in the fields of morphology and phonology [31], machine translation [3, 14, 15], web wrappers [11], speech [24] and pattern recognition [5].",
      "startOffset" : 188,
      "endOffset" : 191
    }, {
      "referenceID" : 28,
      "context" : "Introduced in [29], Ostia was designed for language comprehension tasks [38].",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 37,
      "context" : "Introduced in [29], Ostia was designed for language comprehension tasks [38].",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 29,
      "context" : "For example, Ostia-N uses negative (input) examples and Ostia-D supposes the algorithm has some knowledge of the domain of the function [30].",
      "startOffset" : 136,
      "endOffset" : 140
    }, {
      "referenceID" : 21,
      "context" : "Similar ideas were explored later by [22] and [17].",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 16,
      "context" : "Similar ideas were explored later by [22] and [17].",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 35,
      "context" : "An application of Ostia for active learning is presented in [36].",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 36,
      "context" : "Using dictionaries and word alignments has been tested by [37].",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 33,
      "context" : "The Tenjinno competition [34] was won by [16] using an Ostia inspired algorithm.",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 15,
      "context" : "The Tenjinno competition [34] was won by [16] using an Ostia inspired algorithm.",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 11,
      "context" : "Non-deterministic transducers pose numerous complex questions – even parsing becomes a difficult problem [12, 13].",
      "startOffset" : 105,
      "endOffset" : 113
    }, {
      "referenceID" : 12,
      "context" : "Non-deterministic transducers pose numerous complex questions – even parsing becomes a difficult problem [12, 13].",
      "startOffset" : 105,
      "endOffset" : 113
    }, {
      "referenceID" : 1,
      "context" : "The first lifting of these constraints was proposed by [2].",
      "startOffset" : 55,
      "endOffset" : 58
    }, {
      "referenceID" : 0,
      "context" : "In his PhD thesis, Akram introduced a notion of semi-determinism [1] that strikes a balance between complete non-determinism and the very restrictive subsequential class.",
      "startOffset" : 65,
      "endOffset" : 68
    }, {
      "referenceID" : 22,
      "context" : "[23] provides an overview of a number of important topics in set theory including the ranked and definable universes.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 20,
      "context" : "Some more recent developments in set theory is [21].",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 5,
      "context" : "We eschew the probabilistic approach in favor of a collection of methods that have their antecedents in Beros’s earlier work distinguishing learning models [6] and determining the arithmetic complexity of learning models [7].",
      "startOffset" : 156,
      "endOffset" : 159
    }, {
      "referenceID" : 6,
      "context" : "We eschew the probabilistic approach in favor of a collection of methods that have their antecedents in Beros’s earlier work distinguishing learning models [6] and determining the arithmetic complexity of learning models [7].",
      "startOffset" : 221,
      "endOffset" : 224
    }, {
      "referenceID" : 7,
      "context" : "An earlier version of this work was presented at the International Conference on Grammatical Inference [8].",
      "startOffset" : 103,
      "endOffset" : 106
    }, {
      "referenceID" : 0,
      "context" : "These were defined in [1].",
      "startOffset" : 22,
      "endOffset" : 25
    }, {
      "referenceID" : 26,
      "context" : "The concept of equivalence we have developed closely parallels that of Nerode equivalence [27] in which two strings in a language are equivalent if there is no extension in the language that distinguishes the two strings.",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 9,
      "context" : "We can think of the identifying bounded future of an equivalence class as a sort of signature, an analogue of the famous locking sequence for Gold style learning [10].",
      "startOffset" : 162,
      "endOffset" : 166
    }, {
      "referenceID" : 19,
      "context" : "The learning models There are two principal learning models in grammatical inference: identification in the limit [20] and PAC-learning [35].",
      "startOffset" : 114,
      "endOffset" : 118
    }, {
      "referenceID" : 34,
      "context" : "The learning models There are two principal learning models in grammatical inference: identification in the limit [20] and PAC-learning [35].",
      "startOffset" : 136,
      "endOffset" : 140
    }, {
      "referenceID" : 3,
      "context" : "In order to learn semi-deterministic transducers, we use queries [4] as an additional resource.",
      "startOffset" : 65,
      "endOffset" : 68
    }, {
      "referenceID" : 17,
      "context" : "The precise definition of learning we use is adapted from the one used in [18]:",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 18,
      "context" : "Following conventions presented in [19], we will label states during the learning process as red states if it is not possible to merge them with any <llex-lesser state.",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 1,
      "context" : "The following definition is adapted from [2].",
      "startOffset" : 41,
      "endOffset" : 44
    }, {
      "referenceID" : 32,
      "context" : "[33] A binary relation, R, is finitary if for every x ∈ dom(R) the set {y : 〈x, y〉 ∈ R} is finite.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 0,
      "context" : "A probabilistic version of these transducers was defined in [1].",
      "startOffset" : 60,
      "endOffset" : 63
    } ],
    "year" : 2016,
    "abstractText" : "We prove the existence of a canonical form for semi-deterministic transducers with sets of pairwise incomparable output strings. Based on this, we develop an algorithm which learns semi-deterministic transducers given access to translation queries. We also prove that there is no learning algorithm for semideterministic transducers that uses only domain knowledge.",
    "creator" : "LaTeX with hyperref package"
  }
}