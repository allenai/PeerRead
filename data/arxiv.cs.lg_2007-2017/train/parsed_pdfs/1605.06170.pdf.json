{
  "name" : "1605.06170.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Evaluation System for a Bayesian Optimization Service",
    "authors" : [ "Ian Dewancker", "Michael McCourt", "Scott Clark", "Alexandra Johnson" ],
    "emails" : [ "IAN@SIGOPT.COM", "MIKE@SIGOPT.COM", "SCOTT@SIGOPT.COM", "PATRICK@SIGOPT.COM", "ALEXANDRA@SIGOPT.COM", "L2KE@UWATERLOO.CA" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "SigOpt offers an optimization service to help customers tune complex systems, simulations and models. Our optimization engine applies several concepts from Bayesian optimization (Bergstra et al., 2011; Snoek et al., 2012; Shahriari et al., 2015) and machine learning to optimize customers metrics as quickly as possible. In particular, we consider problems where the maximum is sought for an expensive function f : X → R,\nxopt = argmax x∈X f(x),\nwithin a domain X ⊂ Rd which is a bounding box.\nHyperparameter optimization for machine learning models is of particular relevance as the computational costs for evaluating model variations is high, d is typically small, and hyperparameter gradients are typically not available.\nSigOpt’s core optimization engine is a closed-source fork of the open-source MOE project (Clark et al., 2014). The\nSigOpt service supports a succinct set of web API endpoints for optimizing objective functions. The evaluation system was built with three high level goals in mind:\n• Capable of performing end-to-end testing of service • Facilitate comparisons between algorithm versions • Facilitate comparisons against external baselines\nOur evaluation system consists of an extensive benchmark suite of test functions, automated analysis of performance metrics, and visualization tools for test summarization. The system runs using on-demand cloud infrastructure."
    }, {
      "heading" : "2. Metrics",
      "text" : "The SigOpt service aims to maximize objective functions. The performance metrics we consider for comparisons on a given objective function are the best value seen by the end of the optimization ( Best Found ), and the area under the best seen curve ( AUC ). The AUC metric can help to better differentiate performance, as shown in Figure 1.\nar X\niv :1\n60 5.\n06 17\n0v 1\n[ cs\n.L G\n] 1\n9 M\nay 2\nThe stochastic nature of the optimization algorithms under consideration require that the performance metrics be interpreted statistically. That is, the optimization performance on a given function will inherently vary from one run to the next, so multiple runs on a given function are required to discern statistically significant changes. Generally, optimization algorithms are run 20 times on each function and the distributions of the performance metrics are compared using the non-parametric Mann-Whitney U test, which has been suggested in previous empirical studies of Bayesian optimization methods (Hutter et al., 2011). Further discussion of these metrics and statistical analysis is presented in (Dewancker et al., 2016)"
    }, {
      "heading" : "3. Benchmark Suite",
      "text" : "The tests for our evaluation system consist of closed-form optimization functions (McCourt, 2016) which are extensions of an earlier set proposed by (Gavana, 2013) for black-box optimization evaluation. These functions are fast to evaluate and extensible. We sought a collection that exhibited a wide variety of properties e.g. non-smooth, oscillatory. Some representative functions and corresponding properties of interest are shown in Figure 2\nDesign bias is an important concern when constructing any benchmark test suite or dataset. One example of a design bias we initially encountered in our test suite was functions having optima in predictable locations, for example, at the domain midpoint or on integer coordinates. In this benchmark suite we have made an effort to appropriately classify and segregate functions of this type, though further work is required to identify and resolve less obvious biases."
    }, {
      "heading" : "4. Infrastructure",
      "text" : "Obtaining the empirical distributions of the performance metrics for every test function in our benchmark suite requires significant computational resources. Fortunately, these evaluation tasks are embarrassingly parallel since each function optimization can be done independently of the others in the test suite, and each repeated run on the same function is also independent of other repeated runs.\nTo co-ordinate this effort, lightweight function evaluation processes are run concurrently on a large master machine with many cores. Each process communicates with an on-demand cluster of SigOpt API workers, which in turn co-ordinate each optimization request with a cluster of instances running the SigOpt optimization engine as well as a database used by the service. The database persists important state for each optimization and is central to the production service. Baseline optimization methods are run on the master machine directly.\nInstances for the evaluation system are created as needed using cloud compute providers AWS and DigitalOcean. We found it was helpful to replicate our production optimization flow as closely as possible. By re-creating much of the SigOpt production flow for the evaluation system, several issues and bugs were exposed relating to the API and database in addition to the core SigOpt optimization engine. Results from every run are archived in a simple, extensible JSON format and stored in AWS S3. An interactive web application is used to present evaluation summarizations and inspect results."
    }, {
      "heading" : "5. Visualization Tools",
      "text" : "Performance metrics and best seen traces are collected during each optimization run on all test functions. In raw form, this information is daunting to summarize and extract actionable insights from. To assist in quickly summarizing\nthese results and drilling down into the performance results on particular test functions we developed an interactive web application which hosts various visualizations of the evaluation data."
    }, {
      "heading" : "5.1. Comparative Optimization Traces",
      "text" : "An important tool when diagnosing or comparing optimization efforts on a given function is the best seen trace. The trace represents the best value of the objective metric seen after each function evaluation. In the Bayesian optimization setting, each function evaluation is assumed to be expensive and so the efficiency of methods is most naturally compared using this measurement. Each trace on a given function is stochastic, so the interquartile range of all traces and the median trace is plotted. Traces are always produced in a comparative setting; either between two versions of SigOpt, or between SigOpt and an external optimization method. Figure 5.1 shows a comparative version of SigOpt compared to a particle swarm optimization (PSO) implementation (Lee, 2014).\nA comparative metric summary table is also provided for each best seen trace visualization. This table summarizes the results of the Mann-Whitney U test performed on each method’s metric distribution for the given test function. Optimization method A is defined to be significantly better than method B on a given metric if the expected value of that metric is higher when using method A and the null hypothesis of the Man-Whitney U test is rejected with statistical significance when comparing the two metric empirical distributions. More formally, a win for method A over B using metric M is defined as :\nsignf win(A > B)M = E[MA] > E[MB ] ∧ pval(MA,MB) < 0.01\nWe currently only consider the Best Found and AUC metrics, both of which are desired to be maximized, however\nour metric definition and reporting structure is extensible and can support metrics which are desired to be minimized."
    }, {
      "heading" : "5.2. Comparative p-value Histograms",
      "text" : "While the best seen traces are useful for inspecting performance on individual functions, it is also useful to have visualizations that help summarize the complete relative performances between two optimization methods on a given metric. Towards this end, comparative histograms are generated representing the distribution of test functions over p-value ranges for a given metric. For each metric M , we split the test functions into two sets and create two histograms of the p-values returned by the Mann-Whitney U tests on the empirical distributions produced after evaluation runs. Example histograms are shown in Figure 5.2\nwins(A > B)M = { func | E[MA] > E[MB ] } wins(B > A)M = { func | E[MB ] > E[MA] }\nThe histogram is interactive and each p-value bin can be clicked to inspect the best seen traces of all functions in that range. This flow is particularly useful for investigating unexpected performance regressions or improvements on particular functions. Intuitively, this chart visualizes a spread on the performance differences of two methods for a given\nmetric. A large number of functions binned in the center of the histogram implies many test functions exhibit significant metric performance differences. Conversely, when more functions are allocated to the outer bins, this implies that most metric differences are not significant and the methods are probably comparable."
    }, {
      "heading" : "5.3. Comparative Total Performance Tables",
      "text" : "The p-value histogram is useful for summarizing performance differences between methods on a given metric, however it is often useful to quickly understand a measure of the total relative performance between two methods summarized over all metrics. Towards this end, we generate summary tables that count the number of wins, loses, ties and mixed performance comparisons between methods. An example table is show below in Table 1\nThe total wins count represents the number of test functions where at least one metric has improved with statistical significance and all other metrics have not changed with statistical significance.\ntotal wins(A > B) = |{ func | ∃M (1) :\nE[M (1)A ] > E[M (1) B ] ∧ pval(M (1) A ,M (1) B ) <= 0.01 ∧\n( ∀M (2) 6= M (1), E[M (2)A ] < E[M (2) B ] :\npval(M (2)A ,M (2) B ) > 0.01 )}|\nThe total ties count is defined by the number of test functions where no metric has changed with statistical significance\ntotal ties(A==B) = |{ func | ∀M : pval(MA,MB) > 0.01 }|\nMixed results are functions where there exists one metric that increases with statistical significance and another that decreases with statistical significance.\ntotal mixed(A <> B) = |{ func | ∃M (1),M (2) :\nE[M (1)A ] > E[M (1) B ] ∧ pval(M (1) A ,M (1) B ) <= 0.01 ∧ E[M (2)A ] < E[M (2) B ] ∧ pval(M (2) A ,M (2) B ) <= 0.01 }|"
    }, {
      "heading" : "6. Conclusions",
      "text" : "Our evaluation system has been become a valuable analysis tool when considering algorithm or system changes to the SigOpt optimization service. Data driven performance analysis is an effective way to enable faster iteration and evaluation of a wide spectrum of ideas. The system continues to guide improvements to the core SigOpt service by providing empirical comparisons between internal changes and alternative methods from the Bayesian optimization community, as well helping to expose errors and bugs."
    } ],
    "references" : [ {
      "title" : "Algorithms for hyper-parameter optimization",
      "author" : [ "Bergstra", "James S", "Bardenet", "Rémi", "Bengio", "Yoshua", "Kégl", "Balázs" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Bergstra et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Bergstra et al\\.",
      "year" : 2011
    }, {
      "title" : "MOE: A global, black box optimization engine for real world metric optimization",
      "author" : [ "Clark", "Scott", "Liu", "Eric", "Frazier", "Peter", "Wang", "JiaLei", "Oktay", "Deniz", "Vesdapunt", "Norases" ],
      "venue" : "https://github.com/Yelp/MOE,",
      "citeRegEx" : "Clark et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2014
    }, {
      "title" : "A stratified analysis of bayesian optimization methods",
      "author" : [ "Dewancker", "Ian", "McCourt", "Michael", "Clark", "Scott", "Hayes", "Patrick", "Johnson", "Alexandra", "Ke", "George" ],
      "venue" : "arXiv preprint arXiv:1603.09441,",
      "citeRegEx" : "Dewancker et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Dewancker et al\\.",
      "year" : 2016
    }, {
      "title" : "AMPGO global optimization benchmark functions",
      "author" : [ "Gavana", "Andrea" ],
      "venue" : "https://github.com/andyfaff/ ampgo,",
      "citeRegEx" : "Gavana and Andrea.,? \\Q2013\\E",
      "shortCiteRegEx" : "Gavana and Andrea.",
      "year" : 2013
    }, {
      "title" : "Sequential model-based optimization for general algorithm configuration",
      "author" : [ "Hutter", "Frank", "Hoos", "Holger H", "Leyton-Brown", "Kevin" ],
      "venue" : "In Learning and Intelligent Optimization,",
      "citeRegEx" : "Hutter et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Hutter et al\\.",
      "year" : 2011
    }, {
      "title" : "pyswarm : Particle swarm optimization (PSO) with constraint support",
      "author" : [ "Lee", "Abraham" ],
      "venue" : "https://github. com/tisimst/pyswarm,",
      "citeRegEx" : "Lee and Abraham.,? \\Q2014\\E",
      "shortCiteRegEx" : "Lee and Abraham.",
      "year" : 2014
    }, {
      "title" : "Optimization Test Functions",
      "author" : [ "McCourt", "Michael" ],
      "venue" : "https: //github.com/sigopt/evalset,",
      "citeRegEx" : "McCourt and Michael.,? \\Q2016\\E",
      "shortCiteRegEx" : "McCourt and Michael.",
      "year" : 2016
    }, {
      "title" : "Taking the human out of the loop: A review of bayesian optimization",
      "author" : [ "Shahriari", "Bobak", "Swersky", "Kevin", "Wang", "Ziyu", "Adams", "Ryan P", "de Freitas", "Nando" ],
      "venue" : "Technical report, Universities of Harvard,",
      "citeRegEx" : "Shahriari et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Shahriari et al\\.",
      "year" : 2015
    }, {
      "title" : "Practical bayesian optimization of machine learning algorithms",
      "author" : [ "Snoek", "Jasper", "Larochelle", "Hugo", "Adams", "Ryan P" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Snoek et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Snoek et al\\.",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Our optimization engine applies several concepts from Bayesian optimization (Bergstra et al., 2011; Snoek et al., 2012; Shahriari et al., 2015) and machine learning to optimize customers metrics as quickly as possible.",
      "startOffset" : 76,
      "endOffset" : 143
    }, {
      "referenceID" : 8,
      "context" : "Our optimization engine applies several concepts from Bayesian optimization (Bergstra et al., 2011; Snoek et al., 2012; Shahriari et al., 2015) and machine learning to optimize customers metrics as quickly as possible.",
      "startOffset" : 76,
      "endOffset" : 143
    }, {
      "referenceID" : 7,
      "context" : "Our optimization engine applies several concepts from Bayesian optimization (Bergstra et al., 2011; Snoek et al., 2012; Shahriari et al., 2015) and machine learning to optimize customers metrics as quickly as possible.",
      "startOffset" : 76,
      "endOffset" : 143
    }, {
      "referenceID" : 1,
      "context" : "SigOpt’s core optimization engine is a closed-source fork of the open-source MOE project (Clark et al., 2014).",
      "startOffset" : 89,
      "endOffset" : 109
    }, {
      "referenceID" : 4,
      "context" : "Generally, optimization algorithms are run 20 times on each function and the distributions of the performance metrics are compared using the non-parametric Mann-Whitney U test, which has been suggested in previous empirical studies of Bayesian optimization methods (Hutter et al., 2011).",
      "startOffset" : 265,
      "endOffset" : 286
    }, {
      "referenceID" : 2,
      "context" : "Further discussion of these metrics and statistical analysis is presented in (Dewancker et al., 2016)",
      "startOffset" : 77,
      "endOffset" : 101
    } ],
    "year" : 2016,
    "abstractText" : "Bayesian optimization is an elegant solution to the hyperparameter optimization problem in machine learning. Building a reliable and robust Bayesian optimization service requires careful testing methodology and sound statistical analysis. In this talk we will outline our development of an evaluation framework to rigorously test and measure the impact of changes to the SigOpt optimization service. We present an overview of our evaluation system and discuss how this framework empowers our research engineers to confidently and quickly make changes to our core optimization engine",
    "creator" : "LaTeX with hyperref package"
  }
}