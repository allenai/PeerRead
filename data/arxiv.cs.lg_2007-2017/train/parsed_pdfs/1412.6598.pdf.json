{
  "name" : "1412.6598.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Sobhan Naderi Parizi", "Andrea Vedaldi", "Andrew Zisserman", "Pedro Felzenszwalb" ],
    "emails" : [ "sobhan@brown.edu", "az}@robots.ox.ac.uk", "pff@brown.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Part-based representations have been shown to be very useful for image classification. Learning part-based models is often viewed as a two-stage problem. First, a collection of informative parts is discovered, using heuristics that promote part distinctiveness and diversity, and then classifiers are trained on the vector of part responses. In this paper we unify the two stages and learn the image classifiers and a set of shared parts jointly. We generate an initial pool of parts by randomly sampling part candidates and selecting a good subset using `1/`2 regularization. All steps are driven directly by the same objective namely the classification loss on a training set. This lets us do away with engineered heuristics. We also introduce the notion of negative parts, intended as parts that are negatively correlated with one or more classes. Negative parts are complementary to the parts discovered by other methods, which look only for positive correlations."
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "Computer vision makes abundant use of the concept of “part”. There are at least three key reasons why parts are useful for representing objects or scenes. One reason is the existence of non-linear and non-invertible nuisance factors in the generation of images, including occlusions. By breaking an object or image into parts, at least some of these may be visible and recognizable. A second reason is that parts can be recombined in a model to express a combinatorial number of variants of an object or scene. For example parts corresponding to objects (e.g. a laundromat and a desk) can be rearranged in a scene, and parts of objects (e.g. the face and the clothes of a person) can be replaced by other parts. A third reason is that parts are often distinctive of a particular (sub)category of objects (e.g. cat faces usually belong to cats).\nDiscovering good parts is a difficult problem that has recently raised considerable interest (Juneja et al. (2013); Doersch et al. (2013); Sun & Ponce (2013)). The quality of a part can be defined in different ways. Methods such as (Juneja et al. (2013); Doersch et al. (2013)) decouple learning parts and image classifiers by optimizing an intermediate objective that is only heuristically related to classification. Our first contribution is to learn instead a system of discriminative parts jointly with the image classifiers, optimizing the overall classification performance on a training set. We propose a unified framework for training all of the model parameters jointly (Section 3). We show that joint training can substantially improve the quality of the models (Section 5).\nar X\niv :1\n41 2.\n65 98\nv2 [\ncs .C\nV ]\n1 1\nA pr\n2 01\n5\nA fundamental challenge in part learning is a classical chicken-and-egg problem: without an appearance model, examples of a part cannot be found, and without having examples an appearance model cannot be learned. To address this methods such as (Juneja et al. (2013); Endres et al. (2013)) start from a single random example to initialize a part model, and alternate between finding more examples and retraining the part model. As the quality of the learned part depends on the initial random seed, thousands of parts are generated and a distinctive and diverse subset is extracted by means of some heuristic. Our second contribution is to propose a simple and effective alternative (Section 4). We still initialize a large pool of parts from random examples; we use these initial part models, each trained from a single example, to train image classifiers using `1/`2 regularization as in (Sun & Ponce (2013)). This removes uninformative and redundant parts through group sparsity. This simple method produces better parts than more elaborate alternatives. Joint training (Section 5) improve the quality of the parts further.\nOur pipeline, comprising random part initialization, part selection, and joint training is summarized in Figure 2. In Section 5 we show empirically that, although our part detectors have the same form as the models in (Juneja et al. (2013); Sun & Ponce (2013)), they can reach a higher level of performance using a fraction of the number of parts. This translates directly to test time speedup. We present experiments with both HOG (Dalal & Triggs (2005)) and CNN (Krizhevsky et al. (2012)) features and improve the state-of-the-art results on the MIT-indoor dataset (Quattoni & Torralba (2009)) using CNN features.\nA final contribution of our paper is the introduction of the concept of negative parts, i.e. parts that are negatively correlated with respect to a class (Section 2). These parts are still informative as “counter-evidence” for the class. In certain formulations, negative parts are associated to negative weights in the model and in others with negative weight differences."
    }, {
      "heading" : "1.1 RELATED WORK",
      "text" : "Related ideas in part learning have been recently explored in (Singh et al. (2012); Juneja et al. (2013); Sun & Ponce (2013); Doersch et al. (2013)). The general pipeline in all of these approaches is a two-stage procedure that involves pre-training a set of discriminative parts followed by training a classifier on top of the vector of the part responses. The differences in these methods lay in the details of how parts are discovered. Each approach uses a different heuristic to find a collection of parts such that each part scores high on a subset of categories (and therefore is discriminative) and, collectively, they cover a large area of an image after max-pooling (and therefore are descriptive). Our goal is similar, but we achieve part diversity, distinctiveness, and coverage as natural byproducts of optimizing the “correct” objective function, i.e. the final image classification performance.\nReconfigurable Bag of Words (RBoW) model Naderi et al. (2012) is another part-based model used for image classification. RBoW uses latent variables to define a mapping from image regions to part models. In contrast, the latent variables in our model define a mapping from parts to image regions.\nIt has been shown before (Girshick & Malik (2013)) that joint training is important for the success of part-based models in object detection. Differently from them, however, we share parts among multiple classes and define a joint optimization in which multiple classifiers are learned concurrently. In particular, the same part can vote strongly for a subset of the classes and against another subset.\nThe most closely related work to ours is (Lobel et al. (2013)). Their model has two sets of parameters; a dictionary of visual words θ and a set of weights u that specifies the importance the visual words in each category. Similar to what we do here, Lobel et al. (2013) trains u and θ jointly (visual words would be the equivalent of part filters in our terminology). However, they assume that u is non-negative. This assumption does not allow for “negative parts” as we describe in Section 2.\nThe concept of negative parts and relative attributes (Parikh & Grauman (2011)) are related in that both quantify the relative strength of visual patterns. Our parts are trained jointly using using image category labels as the only form of suppervision, whereas the relative attributes in (Parikh & Grauman (2011)) were trained independently using labeled information about the strength of hand picked attributes in training images."
    }, {
      "heading" : "2 PART-BASED MODELS AND NEGATIVE PARTS",
      "text" : "We model an image class using a collection of parts. A part may capture the appearance of an entire object (e.g. bed in a bedroom scene), a part of an object (e.g. drum in the laundromat scene), a rigid composition of multiple objects (e.g. rack of clothes in a closet scene), or a region type (e.g. ocean in a beach scene).\nLet x be an image. We use H(x) to denote the space of latent values for a part. In our experiments H(x) is a set of positions and scales in a scale pyramid. To test if the image x contains part j at location zj ∈ H(x), we extract features ψ(x, zj) and take the dot product of this feature vector with a part filter wj . Let s(x, zj , wj) denote the response of part j at location zj in x. Since the location of a part is unknown, it is treated as a latent variable which is maximized over. This defines the response r(x,wj) of a part in an image,\ns(x, zj , wj) = wj · ψ(x, zj), r(x,wj) = max zj∈H(x) s(x, zj , wj). (1)\nGiven a collection of m parts w = (w1, . . . , wm), their responses are collected in an m-dimensional vector of part responses r(x,w) = (r(x,w1); . . . ; r(x,wm)). In practice, filter responses are pooled within several distinct spatial subdivisions (Lazebnik et al. (2006)) to encode weak geometry. In this case we have R pooling regions and r(x,w) is an mR-dimensional vector maximizing part responses in each pooling region. For the rest of the paper we assume a single pooling region to simplify notation.\nPart responses can be used to predict the class of an image. For example, high response for “bed” and “lamp” would suggest the image is of a “bedroom” scene. Binary classifiers are often used for multi-class classification with a one-vs-all setup. DPMs (Felzenszwalb et al. (2010)) also use binary classifiers to detect objects of each class. For a binary classifier we can define a score function fβ(x) for the foreground hypothesis. The score combines part responses using a vector of part weights u,\nfβ(x) = u · r(x,w), β = (u,w). (2) The binary classifier predicts y = +1 if fβ(x) ≥ 0, and y = −1 otherwise. Negative parts in a binary classifier: If uj > 0 we say part j is a positive part for the foreground class and if uj < 0 we say part j is a negative part for the foreground class.\nIntuitively, a negative part provides counter-evidence for the foureground class; i.e. r(x,wj) is negatively correlated with fβ(x). For example, since cows are not usually in bedrooms a high response from a cow filter should penalize the score of a bedroom classifier.\nLet β = (u,w) be the parameters of a binary classifier. We can multiply wj and divide uj by a positive value α to obtain an equivalent model. If we use α = |uj | we obtain a model where u ∈ {−1,+1}m. However, in general it is not possible to transform a model where uj is negative into a model where uj is positive because of the max in (1).\nWe note that, when u is non-negative the score function fβ(x) is convex in w. On the other hand, if there are negative parts, fβ(x) is no longer convex in w. If u is non-negative then (2) reduces to the scoring function of a latent SVM, and a special case of a DPM. By the argument above when u is non-negative we can assume u = 1 and (2) reduces to\nfβ(x) = m∑ j=1 max zj∈H(x) wj · ψ(x, zj) = max z∈Z(x) w ·Ψ(x, z), (3)\nwhere Z(x) = H(x)m, and Ψ(x, z) = (ψ(x, z1); . . . ;ψ(x, zm)). In the case of a DPM, the feature vector Ψ(x, z) and the model parameters contain additional terms capturing spatial relationships between parts. In a DPM all part responses are positively correlated with the score of a detection. Therefore DPMs do not use negative parts."
    }, {
      "heading" : "2.1 NEGATIVE PARTS IN MULTI-CLASS SETTING",
      "text" : "In the previous section we showed that certain one-vs-all part-based classifiers, including DPMs, cannot capture counter-evidence from negative parts. This limitation can be addressed by using more general models with two sets of parameters β = (u,w) and a scoring function fβ(x) = u · r(x,w), as long as we allow u to have negative entries.\nNow we consider the case of a multi-class classifier where part responses are weighted differently for each category but all categories share the same set of part filters. A natural consequence of part sharing is that a positive part for one class can be used as a negative part for another class.\nLet Y = {1, . . . , n} be a set of n categories. A multi-class part-based model β = (w, u) is defined by m part filters w = (w1, . . . , wm) and n vectors of part weights u = (u1, . . . , un) with uy ∈ Rm. The shared filters w and the weight vector uy define parameters βy = (w, uy) for a scoring function for class y. For an input x the multi-class classifier selects the class with highest score\nŷβ(x) = arg max y∈Y fβy (x) = arg max y∈Y\nuy · r(x,w) (4)\nWe can see u as an n×mmatrix. Adding a constant to a column of u does not change the differences between scores of two classes fβa(x) − fβb(x). This implies the function ŷ is invariant to such transformations. We can use a series of such transformations to make all entries in u non-negative, without changing the classifier. Thus, in a multi-class part-based model, unlike the binary case, it is not a significant restriction to require the entries in u to be non-negative. In particular the sign of an entry in uy does not determine the type of a part (positive or negative) for class y.\nNegative parts in a multi-class classifier: If ua,j > ub,j we say part j is a positive part for class a relative to b. If ua,j < ub,j we say part j is a negative part for class a relative to b.\nAlthough adding a constant to a column of u does not affect ŷ, it does impact the norms of the part weight vectors uy . For an `2 regularized model the columns of u will sum to zero. Otherwise we can subtract the column sums from each column of u to decrease the `2 regularization cost without changing ŷ and therefore the classification loss. We see that in the multi-class part-based model constrainig u to have non-negative entries only affects the regularization of the model."
    }, {
      "heading" : "3 JOINT TRAINING",
      "text" : "In this section we propose an approach for joint training of all parameters β = (w, u) of a multiclass part-based model. Training is driven directly by classification loss. Note that a classification loss objective is sufficient to encourage diversity of parts. In particular joint training encourages part filters to complement each other. We have found that joint training leads to a substantial improvement in performance (see Section 5). The use of classification loss to train all model parameters also leads to a simple framework that does not rely on multiple heuristics.\nLet D = {(xi, yi)}ki=1 denote a training set of labeled examples. We train β using `2 regularization for both the part filters w and the part weights u (we think of each as a single vector) and the multi-class hinge loss, resulting in the objective function:\nO(u,w) = λw||w||2 + λu||u||2 + k∑ i=1 max { 0, 1 + (max y 6=yi uy · r(xi, w))− uyi · r(xi, w) } (5)\n= λw||w||2 + λu||u||2 + k∑ i=1 max { 0, 1 + max y 6=yi (uy − uyi) · r(xi, w) } (6)\nWe use block coordinate descent for training, as summarized in Algorithm 1. This alternates between (Step 1) optimizing u while w is fixed and (Step 2) optimizing w while u is fixed. The first step reduces to a convex structural SVM problem (line 3 of Algorithm 1). If u is non-negative the second step could be reduced to a latent structural SVM problem defined by (5). We use a novel approach that allows u to be negative (lines 4-7 of Algorithm 1) described below.\nAlgorithm 1 Joint training of model parameters by optimizing O(u,w) in Equation 6. 1: initialize the part filters w = (w1, . . . , wm) 2: repeat 3: u := arg minu′ O(u\n′, w) (defined in Equation 6) 4: repeat 5: wold := w 6: w := arg minw′ Bu(w\n′, wold) (defined in Equation 7) 7: until convergence 8: until convergence 9: output β = (u,w)\nSTEP 1: LEARNING PART WEIGHTS (LINE 3 OF ALGORITHM 1)\nThis involves computing arg minu′ O(u ′, w). Since w is fixed λw||w||2 and r(xi, w) are constant. This makes the optimization problem equivalent to training a multi-class SVM where the i-th training example is represented by an m-dimensional vector of part responses r(xi, w). This is a convex problem that can be solved using standard methods.\nSTEP 2: LEARNING PART FILTERS (LINES 4-7 OF ALGORITHM 1)\nThis involves computing arg minw′ O(u,w ′). Since u is fixed λu||u||2 is constant. While r(xi, wj) is convex in w (it is a maximum of linear functions) the coefficients uy,j − uyi,j may be negative. This makes the objective function (6) non-convex. Lines 4-7 of Algorithm 1 implement the CCCP algorithm (Yuille & Rangarajan (2003)). In each iteration we construct a convex bound using the previous estimate of w and update w to be the minimizer of the bound.\nLet s(x, z, w) = (s(x, z1, w1); . . . ; s(x, zm, wm)) to be the vector of part responses in image x when the latent variables are fixed to z = (z1, . . . , zm). We construct a convex upper bound on O by replacing r(xi, wj) with s(xi, zi,j , wj) in (6) when uy,j − uyi,j < 0. We make the bound tight for the last estimate of the part filters wold by selecting zi,j = arg maxzj∈H(xi) s(xi, zj , w old j ). Then a convex upper bound that touches O at wold is given by λu||u||2 +Bu(w,wold) with\nBu(w,w old) = λw||w||2+ k∑ i=1 max { 0, 1+max y 6=yi (uy−uyi)· [ Sy,yir(xi, w)+S̄y,yis(xi, zi, w) ]} (7)\nHere, for a pair of categories (y, y′), Sy,y′ and S̄y,y′ are m × m diagonal 0-1 matrices such that S̄y,y′(j, j) = 1− Sy,y′(j, j) and Sy,y′(j, j) = 1 if and only if uy,j − uy′,j ≥ 0. The matrices S and S̄ select r(xi, wj) when uy,j−uyi,j ≥ 0 and s(xi, zi,j , wj) when uy,j−uyi,j < 0. This implements the convex upper-bound outlined above.\nLine 6 of the algorithm updates the part filters by minimizing Bu(w,wold). Optimizing this function requires significant computational and memory resources. In the supplementary material (Section A) we give details of how our optimization method works in practice."
    }, {
      "heading" : "4 PART GENERATION AND SELECTION",
      "text" : "The joint training objective in (6) is non-convex making Algorithm 1 sensitive to initialization. Thus, the choice of initial parts can be crucial in training models that perform well in practice. We devote the first two steps of our pipeline to finding good initial parts (Figure 2). We then use those parts to initialize the joint training procedure of Section 3.\nIn the first step of our pipeline we randomly generate a large pool of initial parts. Generating a part involves picking a random training image (regardless of the image category labels) and extracting features from a random subwindow of the image followed by whitening (Hariharan et al. (2012)). To whiten a feature vector f we use Σ−1(f − µ) where µ and Σ are the mean and covariance of all patches in all training images. We estimate µ and Σ from 300,000 random patches. We use the norm of the whitened features to estimate discriminability of a patch. Patches with large whitened feature norm are farther from the mean of the background distribution in the whitened space and,\nhence, are expected to be more discriminative. Similar to (Aubry et al. (2013)) we discard the 50% least discriminant patches from each image prior to generating random parts.\nOur experimental results with HOG features (Figure 3) show that randomly generated parts using the procedure described here perform better than or comparable to previous methods that are much more involved (Juneja et al. (2013); Doersch et al. (2013); Sun & Ponce (2013)). When using CNN features we get very good results using random parts alone, even before part-selection and training of the part filters (Figure 4).\nRandom part generation may produce redundant or useless parts. In the second step of our pipeline we train image classifiers u using `1/`2 regularization (a.k.a. group lasso) to select a subset of parts from the initial random pool. We group entries in each column of u. Let ρj denote the `2-norm of the j-th column of u. The `1/`2 regularization is defined by Rg(u) = λ ∑m j=1 ρj .\nIf part j is not uninformative or redundant ρj (and therefore all entries in the j-th column of u) will be driven to zero by the regularizer. We train models using different values for λ to generate a target number of parts. The number of selected parts decreases monotonically as λ increases. Figure 8 in the supplement shows this. We found it important to retrain u after part selection using `2 regularization to obtain good classification performance."
    }, {
      "heading" : "5 EXPERIMENTS",
      "text" : "We evaluate our methods on the MIT-indoor dataset (Quattoni & Torralba (2009)). We compare performance of models with randomly generated parts, selected parts, and jointly trained parts. We also compare performance of HOG and CNN features. The dataset has 67 indoor scene classes. There are about 80 training and 20 test images per class. Recent part-based methods that do well on this dataset (Juneja et al. (2013); Doersch et al. (2013); Sun & Ponce (2013)) use a large number of parts (between 3350 and 13400).\nHOG features: We resize images (maintaining aspect ratio) to have about 2.5M pixels. We extract 32-dimensional HOG features (Dalal & Triggs (2005); Felzenszwalb et al. (2010)) at multiple scales. Our HOG pyramid has 3 scales per octave. This yields about 11,000 patches per image. Each part filter wj models a 6×6 grid of HOG features, so wj and ψ(x, zj) are both 1152-dimensional. CNN features: We extract CNN features at multiple scales from overlapping patches of fixed size 256×256 and with stride value 256/3 = 85. We resize images (maintaining aspect ratio) to have about 5M pixels in the largest scale. We use a scale pyramid with 2 scales per octave. This yields about 1200 patches per image. We extract CNN features using Caffe (Jia et al. (2014)) and the hybrid neural network from (Zhou et al. (2014)). The hybrid network is pre-trained on images from ImageNet (Deng et al. (2009)) and PLACES (Zhou et al. (2014)) datasets. We use the output of the 4096 units in the penultimate fully connected layer of the network (fc7). We denote these features by HP in our plots.\nPart-based representation: Our final image representation is an mR-dimensional vector of part responses wherem is the number of shared parts andR is the number of spatial pooling regions. We use R = 5 pooling regions arranged in a 1×1 + 2×2 grid. To make the final representation invariant to horizontal image flips we average the mR-dimensional vector of part responses for image x and its right-to-left mirror image x′ to get [r(x,w) + r(x′, w)] /2 as in (Doersch et al. (2013)).\nWe first evaluate the performance of random parts. Given a pool of randomly initialized parts (Section 4), we train the part weights u using a standard `2-regularized linear SVM; we then repeat the experiment by selecting few parts from a large pool using `1/`2 regularization (Section 4). Finally, we evaluate joint training (Section 3). While joint training significantly improves performance, it comes at a significantly increased computational cost.\nFigure 3 shows performance of HOG features on the MIT-indoor dataset. Because of the high dimensionality of the HOG features and the large space of potential placements in a HOG pyramid we consider a 10-class subset of the dataset for experiments with a large number of parts using HOG features. The subset comprises bookstore, bowling, closet, corridor, laundromat, library, nursery, shoeshop, staircase, and winecellar. Performance of random parts increases as we use more parts. Flip invariance and part selection consistently improve results. Joint training improves the performance even further by a large margin achieving the same level of performance as the\nselected parts using much fewer parts. On the full dataset, random parts already outperform the results from Juneja et al. (2013), flip invariance boosts the performance beyond Sun & Ponce (2013). Joint training dominates other methods. However, we could not directly compare with the best performance of Doersch et al. (2013) due to the very large number of parts they use.\nFigure 4 shows performance of CNN features on MIT-indoor dataset. As a baseline we extract CNN features from the entire image (after resizing to 256×256 pixels) and train a multi-class linear SVM. This obtains 72.3% average performance. This is a strong baseline. Razavian et al. (2014) get 58.4% using CNN trained on ImageNet. They improve the result to 69% after data augmentation.\nWe applied PCA on the 4096 dimensional features to make them more compact. This is essential for making the joint training tractable both in terms of running time and memory footprint. Figure 4-left shows the effect of PCA dimensionality reduction. It is surprising that we lose only 1% in accuracy with 160 PCA coefficients and only 3.5% with 60 PCA coefficients. We also show how performance changes when a random subset of dimensions is used. For joint training we use 60 PCA coefficients.\nFigure 4-right shows performance of our part-based models using CNN features. For comparison with HOG features we also plot result of Doersch et al. (2013). Note that part-based representation improves over the CNN extracted on the entire image. With 13400 random parts we get 77.1% (vs 72.3% for CNN on the entire image). The improvement is from 68.2% to 72.4% when we use 60 PCA coefficients. Interestingly, the 60 PCA coefficients perform better than the full CNN features when only a few parts are used (up to 1000). The gap increases as the number of parts decreases.\nWe do part selection and joint training using 60 PCA coefficients of the CNN features. We select parts from an initial pool of 1000 random parts. Part selection is most effective when a few parts are used. Joint training improves the quality of the selected parts. With only 372 jointly trained parts we obtain 73.3% classification performance which is even better than 13400 random parts (72.4%).\nThe significance of our results is two fold: 1) we demonstrate a very simple and fast to train pipeline for image classification using randomly generated parts; 2) we show that using part selection and joint training we can obtain similar or higher performance using much fewer parts. The gain is largest for CNN features (13400/372 ≈ 36 times). This translates to 36x speed up in test time. See Section D of the supplement for detailed run-time analysis of our method."
    }, {
      "heading" : "5.1 VISUALIZATION OF THE MODEL",
      "text" : "Figure 5 shows the part weight matrix after joint training a model with 52 parts on the full MITindoor dataset. This model uses 60 PCA coefficients from the HP CNN features. Figure 6 shows top scoring patches for a few parts before and after joint training. The parts correspond to the model illustrated in Figure 5. The benefit of joint training is clear. The part detections are more consistent and “clean” after joint training. The majority of the detections of part 25 before joint training are seats. Joint training filters out most of the noise (mostly coming from bed and sofa) in this part. Part 46 consistently fires on faces even before joint training. After joint training, however, the part becomes more selective to a single face and the detections become more localized.\nFigure 7 illustrates selectivity of a few parts. Each row shows the highest scoring detections of a particular part on test images. The part indices in the first column match those of Figure 5. Even though\nmost detections look consistently similar the images usually belong to multiple classes demonstrating part sharing across categories. For example, while part 17 appears to capture bed the images belong to hospitalroom, childrensroom, and bedroom classes. While part 25 appears to capture seats the images belong to waitingroom, library, auditorium, and insidebus. Conversely, multiple parts may capture the same semantic concept. For example, parts 3, 16, and 35 appear to capture shelves but they seem to be tuned specifically to shelves in pantry, store, and book-shelves respectively. Some parts respond to a part of an object; e.g. part 40 and 46 respond to leg and face. Others find entire objects or even composition of multiple objects. For example, parts 6, 17, 37, 43 detect laundromats, beds, cabinets, and monitor. Part 29 detects composition of seats-and-screen.\nThe part weight matrix u (Figure 5) helps us better understand how parts assists classification. Part 6 has significantly high weight for class laundromat and it appears to be a good detector thereof. Part 27 fires strongly on game/sports-related scenes. The weight matrix reveals that this part is strongly correlated with gameroom, casino, and poolinside. Part 17 fires strongly on bed and it has the highest weight for hospitalroom, children room, bedroom, and operating room.\nWeight matrix also identifies negative parts. An interesting example is part 46 (the face detector). It has the lowest weight for buffet, classroom, computerroom, and garage. This suggests that part 46 is a negative part for these classes relative to others. This is rather surprising because one would expect to find people in scenes such as classroom and computerroom. We examined all training images of these classes and found no visible faces in them except for 1 image in classroom and 3 images in computerroom with hardly visible faces and 1 image in garage with a clear face in it."
    }, {
      "heading" : "6 CONCLUSIONS",
      "text" : "We presented a simple pipeline to train part-based models for image classification. All model parameters are trained jointly in our framework; this includes shared part filters and class-specific part weights. All stages of our training pipeline are driven directly by the same objective namely the classification performance on a training set. In particular, our framework does not rely on adhoc heuristics for selecting discriminative and/or diverse parts. We also introduced the concept of “negative parts” for part-based models.\nModels based on our randomly generated parts perform better than almost all previously published work despite the profound simplicity of the method. Using CNN features and random parts we obtain 77.1% accuracy on the MIT-indoor dataset, improving the state-of-the-art. We also showed that part selection and joint training can be used to train a model that achieves better or the same level of performance as a system with randomly generated parts while using much fewer parts.\nJoint training alternates between training part weights and updating part filters. This process can be initiated before the first or the second step leading to two different initialization schemes. Currently we use random examples to initialize the part filters. It would also be possible to initialize the entries in u based on how a hypothetical part is correlated with a class; negatively, irrelevant, or positively. Training the part filters would then learn part models that fit this description."
    }, {
      "heading" : "B PART SELECTION",
      "text" : "As mentioned in Section 4 of the paper, we use group sparsity to select useful parts from a pool of randomly initialized parts. We use the same formulation as in Sun & Ponce (2013). Part selection is done by optimizing the following objective function:\nλ m∑ j=1 ρj + k∑ i=1 max{0,max y 6=yi (uy − uyi) · r(xi, w) + 1} (17)\nwhere ρj = √∑ y u 2 y,j is the `2-norm of the column of u that corresponds to part j. This objective\nfunction is convex. We minimize it using stochastic gradient descent. This requires repeatedly taking a small step in the opposite direction of a sub-gradient of the function. LetRg(u) = λ ∑m j=1 ρj . The partial derivative ∂Rg∂uy = uy ρj\nexplodes as ρj goes to zero. Thus, we round the ρj’s as they approach zero. We denote the rounded version by τj and define them as follows\nτj = { ρj if ρj > ρ2j 2 + 2 if ρj ≤\nThe constants in the second case are set so that τj is continuous; that is ρ2j 2 + 2 = ρj when ρj = . In summary, part selection from an initial pool of parts w = (w1, . . . , wm) involves optimizing the following objective function:\nλ m∑ j=1 τj + k∑ i=1 max{0,max y 6=yi (uy − uyi) · r(xi, w) + 1} (18)\nWe can control the sparsity of the solution to this optimization problem by changing the value of λ. In Figure 8 we plot ρj for all parts in decreasing order. Each curve corresponds to the result obtained with a different λ value. These plots suggest that the number of selected parts (i.e. parts whose ρj is larger than a threshold that depends on ) decreases monotonically as λ increases. We adjust λ to obtain a target number of selected parts."
    }, {
      "heading" : "C MORE ON VISUALIZATION OF THE MODEL",
      "text" : "We complement Section 5.1 of the paper by providing more visualizations of jointly trained parts. Figure 9 shows the part filters and the weight matrix after joint training a model with 52 parts on the 10-class subset of MIT-indoor dataset. This model uses HOG features. The part weight matrix determines whether a part is positive or negative with respect to two categories. For example, part 42 is positive for bookstore and library relative to laundromat. Part 29 is positive for laundromat relative to bookstore and library. Part 37 is positive for library relative to bookstore so it can be used in combination with the other two parts to distinguish between all three categories bookstore, library, and laundromat. Figure 10 illustrates the top scoring patches for these three parts.\nFigure 11 complements Figure 7 of the paper. The part indices in the first column match those of Figure 5. The rows show the highest scoring detections of a particular part on test images.\nPart 1 fires on clothing-rack, part 22 appear to find container, and part 33 detects table-top. There are parts that capture low-level features such as the mesh pattern of part 31 and the high-frequency horizontal stripes of part 41. Also, there are parts that are selective for certain colors. For example, part 9 appears to respond to specific red patterns (in particular fruits and flowers). Part 51 appears to fire on yellow-food dishes. Part 48 is very well tuned to finding text.\nAccording to the weight matrix (see Figure 5 in the paper) Part 14 is highly weighted for nursery and staircase classes and it appears to detect a row of vertical-bars. Part 21 is highly weighted for laundromat, library, and cloister and it appears to respond strongly to arch. Also note that part 21 is a strong negative part for bookstore relative to library. Presence of an arch, in fact, is a very sensible differentiating pattern that could tell library apart from bookstore."
    }, {
      "heading" : "D PROCESSING TIME",
      "text" : "Test time: the test procedure of our models involves three simple steps: 1) convolving part filters with the test image, 2) computing the part-based representation 3) finding the class with the highest\nclassification score. Step 1 takes O(mhd) time where m, h, and d are the number of parts, latent locations, and dimensionality of the patch features. Step 2 takes O(hR) time where R is the number of pooling regions. Step 3 takes O(nmR) time where n is the number of classes. The bottleneck in test time is step 1 and 3 both of which depend on the number of parts m. So, a decrease in m directly affects the test time. Note that both of these two steps are embarrassingly parallel processes.\nTraining time: the training procedure involves two main steps: 1) learning part weights (line 3 in Algorithm 1) and 2) learning part filters (lines 4-7 in Algorithm 1). The first step is a standard multi-class SVM problem and is relatively fast to train. The bottleneck in training is the second step.\nLearning part filters involves multiple nested loops: 1) joint training loop (lines 2-8 in Algorithm 1), 2) relabeling loop (lines 4-7 in Algorithm 1), 3) cache update loop (lines 4-9 in Algorithm 2), and 4) the constraint generation loop of the QP solver (lines 3-10 in Algorithm 3). The number of iterations each loop takes depends on the training data and the hyper parameters of the model (i.e. λw and λu).\nWe report the running time of our joint training algorithm separately for one experiment that uses HOG features and one that uses CNN features as the dimensionality of the features and the number of latent locations they consider is different.\nIn our current implementation it takes 5 days to do joint training with 120 shared parts on the full MIT-indoor dataset on a 16-core machine using HOG features. It takes 2.5 days to do joint training with 372 parts on the full dataset on a 8 core machine using 60-dimensional PCA-reduced CNN features. Note that these time include learning all shared part filters and all 67 class-specific part weight vectors on a single machine. In both experiments finding the most violated constraint (line 8 in Algorithm 3) takes more than half of the total running time. The second bottleneck for HOG features is growing the caches (line 6 in Algorithm 2). This involves convolving the part filters (1152 dimensional HOG templates) with all training images (each containing 11000 candidate locations). With the CNN features, however, the second bottleneck becomes the QP solver (line 7 in Algorithm 2). The QP solver that we use only uses a single core. In both cases the ratio of the time taken by the first bottleneck to the second one is 4 to 1.\nThe pipeline in previous methods such as (Juneja et al. (2013); Sun & Ponce (2013); Doersch et al. (2013)) has several steps. For example, to discover parts, Juneja et al. (2013) applies multiple superpixel segmentations on the image to find initial seeds, trains exemplar LDA for each seed, enhances the candidate parts by harvesting similar patches in the dataset, and computes the entropy of the top-50 detections of each part over categories. They discard parts with high entropy as well as duplicates. Despite using several heuristics these methods are slow too. Doersch et al. (2013) do not comment on the processing time of their method in the paper but we know from personal correspondence that their code takes a long time to run. However, most of the steps in their method are independent; e.g. they start their method from multiple initial points to find the discriminative modes, they train 1-vs-all classifiers, etc. So, they distribute the processing load on a big cluster in order to run their experiments.\nOur experimental results showed that we can obtain better performance than Juneja et al. (2013) and Sun & Ponce (2013) using a pool of randomly initialized parts (see Figure 3). Note that creating a pool of random parts is very straightforward and fast. It only takes extracting features from random subwindow of a random image and applying a simple feature transformation on them (see Section 4)."
    } ],
    "references" : [ {
      "title" : "Painting-to-3D model alignment via discriminative visual elements",
      "author" : [ "Aubry", "Mathieu", "Russell", "Bryan", "Sivic", "Josef" ],
      "venue" : "ACM Transactions on Graphics,",
      "citeRegEx" : "Aubry et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Aubry et al\\.",
      "year" : 2013
    }, {
      "title" : "Histograms of oriented gradients for human detection",
      "author" : [ "Dalal", "Navneet", "Triggs", "Bill" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "Dalal et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Dalal et al\\.",
      "year" : 2005
    }, {
      "title" : "Imagenet: A large-scale hierarchical image database",
      "author" : [ "Deng", "Jia", "Dong", "Wei", "Socher", "Richard", "Li", "Li-Jia", "Kai", "Fei-Fei" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "Deng et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Deng et al\\.",
      "year" : 2009
    }, {
      "title" : "Mid-level visual element discovery as discriminative mode seeking",
      "author" : [ "Doersch", "Carl", "Gupta", "Abhinav", "Efros", "Alexei" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Doersch et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Doersch et al\\.",
      "year" : 2013
    }, {
      "title" : "Learning collections of part models for object recognition",
      "author" : [ "Endres", "Ian", "Shih", "Kevin", "Jiaa", "Johnston", "Hoiem", "Derek" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "Endres et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Endres et al\\.",
      "year" : 2013
    }, {
      "title" : "Object detection with discriminatively trained part based models",
      "author" : [ "Felzenszwalb", "Pedro", "Girshick", "Ross", "McAllester", "David", "Ramanan", "Deva" ],
      "venue" : null,
      "citeRegEx" : "Felzenszwalb et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Felzenszwalb et al\\.",
      "year" : 2010
    }, {
      "title" : "Training deformable part models with decorrelated features",
      "author" : [ "Girshick", "Ross", "Malik", "Jitendra" ],
      "venue" : "In ICCV,",
      "citeRegEx" : "Girshick et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Girshick et al\\.",
      "year" : 2013
    }, {
      "title" : "Discriminative decorrelation for clustering and classication",
      "author" : [ "Hariharan", "Bharath", "Malik", "Jitendra", "Ramanan", "Deva" ],
      "venue" : "In ECCV,",
      "citeRegEx" : "Hariharan et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Hariharan et al\\.",
      "year" : 2012
    }, {
      "title" : "Caffe: Convolutional architecture for fast feature embedding",
      "author" : [ "Jia", "Yangqing", "Shelhamer", "Evan", "Donahue", "Jeff", "Karayev", "Sergey", "Long", "Jonathan", "Girshick", "Ross", "Guadarrama", "Sergio", "Darrell", "Trevor" ],
      "venue" : null,
      "citeRegEx" : "Jia et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Jia et al\\.",
      "year" : 2014
    }, {
      "title" : "Cutting-plane training of structural svms",
      "author" : [ "Joachims", "Thorsten", "Finley", "Thomas", "Yu", "Chun-Nam John" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Joachims et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Joachims et al\\.",
      "year" : 2009
    }, {
      "title" : "Blocks that shout: Distinctive parts for scene classification",
      "author" : [ "Juneja", "Mayank", "Vedaldi", "Andrea", "C.V. Jawahar", "Zisserman", "Andrew" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "Juneja et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Juneja et al\\.",
      "year" : 2013
    }, {
      "title" : "Imagenet classication with deep convolutional neural networks",
      "author" : [ "Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Krizhevsky et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Krizhevsky et al\\.",
      "year" : 2012
    }, {
      "title" : "Beyond bag of features: Spatial pyramid matching for recognizing natural scene categories",
      "author" : [ "Lazebnik", "Svetlana", "Schmid", "Cordelia", "Ponce", "Jean" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "Lazebnik et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Lazebnik et al\\.",
      "year" : 2006
    }, {
      "title" : "Hierarchical joint max-margin learning of mid and top level representations for visual recognition",
      "author" : [ "Lobel", "Hans", "Vidal", "Rene", "Soto", "Alvaro" ],
      "venue" : "In ICCV,",
      "citeRegEx" : "Lobel et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Lobel et al\\.",
      "year" : 2013
    }, {
      "title" : "Reconfigurable models for scene recognition",
      "author" : [ "Naderi", "Sobhan", "Oberlin", "John", "Felzenszwalb", "Pedro" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "Naderi et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Naderi et al\\.",
      "year" : 2012
    }, {
      "title" : "Recognizing indoor scenes",
      "author" : [ "Quattoni", "Ariadna", "Torralba", "Antonio" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "Quattoni et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Quattoni et al\\.",
      "year" : 2009
    }, {
      "title" : "Cnn features off-the-shelf: an astounding baseline for recognition",
      "author" : [ "Razavian", "Ali Sharif", "Azizpour", "Hossein", "Sullivan", "Josephine", "Carlsson", "Stefan" ],
      "venue" : "In CVPR DeepVision workshop,",
      "citeRegEx" : "Razavian et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Razavian et al\\.",
      "year" : 2014
    }, {
      "title" : "Unsupervised discovery of mid-level discriminative patches",
      "author" : [ "Singh", "Saurabh", "Gupta", "Abhinav", "Efros", "Alexei" ],
      "venue" : "In ECCV,",
      "citeRegEx" : "Singh et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Singh et al\\.",
      "year" : 2012
    }, {
      "title" : "Learning discriminative part detectors for image classification and cosegmentation",
      "author" : [ "Sun", "Jian", "Ponce", "Jean" ],
      "venue" : "In ICCV,",
      "citeRegEx" : "Sun et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2013
    }, {
      "title" : "The concave-convex procedure",
      "author" : [ "Yuille", "Alan", "Rangarajan", "Anand" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Yuille et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Yuille et al\\.",
      "year" : 2003
    }, {
      "title" : "Learning deep features for scene recognition using places database",
      "author" : [ "Zhou", "Bolei", "Lapedriza", "Agata", "Xiao", "Jianxiong", "Torralba", "Antonio", "Oliva", "Aude" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Zhou et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 9,
      "context" : "Discovering good parts is a difficult problem that has recently raised considerable interest (Juneja et al. (2013); Doersch et al.",
      "startOffset" : 94,
      "endOffset" : 115
    }, {
      "referenceID" : 3,
      "context" : "(2013); Doersch et al. (2013); Sun & Ponce (2013)).",
      "startOffset" : 8,
      "endOffset" : 30
    }, {
      "referenceID" : 3,
      "context" : "(2013); Doersch et al. (2013); Sun & Ponce (2013)).",
      "startOffset" : 8,
      "endOffset" : 50
    }, {
      "referenceID" : 3,
      "context" : "(2013); Doersch et al. (2013); Sun & Ponce (2013)). The quality of a part can be defined in different ways. Methods such as (Juneja et al. (2013); Doersch et al.",
      "startOffset" : 8,
      "endOffset" : 146
    }, {
      "referenceID" : 3,
      "context" : "(2013); Doersch et al. (2013); Sun & Ponce (2013)). The quality of a part can be defined in different ways. Methods such as (Juneja et al. (2013); Doersch et al. (2013)) decouple learning parts and image classifiers by optimizing an intermediate objective that is only heuristically related to classification.",
      "startOffset" : 8,
      "endOffset" : 169
    }, {
      "referenceID" : 9,
      "context" : "To address this methods such as (Juneja et al. (2013); Endres et al.",
      "startOffset" : 33,
      "endOffset" : 54
    }, {
      "referenceID" : 4,
      "context" : "(2013); Endres et al. (2013)) start from a single random example to initialize a part model, and alternate between finding more examples and retraining the part model.",
      "startOffset" : 8,
      "endOffset" : 29
    }, {
      "referenceID" : 4,
      "context" : "(2013); Endres et al. (2013)) start from a single random example to initialize a part model, and alternate between finding more examples and retraining the part model. As the quality of the learned part depends on the initial random seed, thousands of parts are generated and a distinctive and diverse subset is extracted by means of some heuristic. Our second contribution is to propose a simple and effective alternative (Section 4). We still initialize a large pool of parts from random examples; we use these initial part models, each trained from a single example, to train image classifiers using `1/`2 regularization as in (Sun & Ponce (2013)).",
      "startOffset" : 8,
      "endOffset" : 650
    }, {
      "referenceID" : 4,
      "context" : "(2013); Endres et al. (2013)) start from a single random example to initialize a part model, and alternate between finding more examples and retraining the part model. As the quality of the learned part depends on the initial random seed, thousands of parts are generated and a distinctive and diverse subset is extracted by means of some heuristic. Our second contribution is to propose a simple and effective alternative (Section 4). We still initialize a large pool of parts from random examples; we use these initial part models, each trained from a single example, to train image classifiers using `1/`2 regularization as in (Sun & Ponce (2013)). This removes uninformative and redundant parts through group sparsity. This simple method produces better parts than more elaborate alternatives. Joint training (Section 5) improve the quality of the parts further. Our pipeline, comprising random part initialization, part selection, and joint training is summarized in Figure 2. In Section 5 we show empirically that, although our part detectors have the same form as the models in (Juneja et al. (2013); Sun & Ponce (2013)), they can reach a higher level of performance using a fraction of the number of parts.",
      "startOffset" : 8,
      "endOffset" : 1107
    }, {
      "referenceID" : 4,
      "context" : "(2013); Endres et al. (2013)) start from a single random example to initialize a part model, and alternate between finding more examples and retraining the part model. As the quality of the learned part depends on the initial random seed, thousands of parts are generated and a distinctive and diverse subset is extracted by means of some heuristic. Our second contribution is to propose a simple and effective alternative (Section 4). We still initialize a large pool of parts from random examples; we use these initial part models, each trained from a single example, to train image classifiers using `1/`2 regularization as in (Sun & Ponce (2013)). This removes uninformative and redundant parts through group sparsity. This simple method produces better parts than more elaborate alternatives. Joint training (Section 5) improve the quality of the parts further. Our pipeline, comprising random part initialization, part selection, and joint training is summarized in Figure 2. In Section 5 we show empirically that, although our part detectors have the same form as the models in (Juneja et al. (2013); Sun & Ponce (2013)), they can reach a higher level of performance using a fraction of the number of parts.",
      "startOffset" : 8,
      "endOffset" : 1127
    }, {
      "referenceID" : 4,
      "context" : "(2013); Endres et al. (2013)) start from a single random example to initialize a part model, and alternate between finding more examples and retraining the part model. As the quality of the learned part depends on the initial random seed, thousands of parts are generated and a distinctive and diverse subset is extracted by means of some heuristic. Our second contribution is to propose a simple and effective alternative (Section 4). We still initialize a large pool of parts from random examples; we use these initial part models, each trained from a single example, to train image classifiers using `1/`2 regularization as in (Sun & Ponce (2013)). This removes uninformative and redundant parts through group sparsity. This simple method produces better parts than more elaborate alternatives. Joint training (Section 5) improve the quality of the parts further. Our pipeline, comprising random part initialization, part selection, and joint training is summarized in Figure 2. In Section 5 we show empirically that, although our part detectors have the same form as the models in (Juneja et al. (2013); Sun & Ponce (2013)), they can reach a higher level of performance using a fraction of the number of parts. This translates directly to test time speedup. We present experiments with both HOG (Dalal & Triggs (2005)) and CNN (Krizhevsky et al.",
      "startOffset" : 8,
      "endOffset" : 1322
    }, {
      "referenceID" : 4,
      "context" : "(2013); Endres et al. (2013)) start from a single random example to initialize a part model, and alternate between finding more examples and retraining the part model. As the quality of the learned part depends on the initial random seed, thousands of parts are generated and a distinctive and diverse subset is extracted by means of some heuristic. Our second contribution is to propose a simple and effective alternative (Section 4). We still initialize a large pool of parts from random examples; we use these initial part models, each trained from a single example, to train image classifiers using `1/`2 regularization as in (Sun & Ponce (2013)). This removes uninformative and redundant parts through group sparsity. This simple method produces better parts than more elaborate alternatives. Joint training (Section 5) improve the quality of the parts further. Our pipeline, comprising random part initialization, part selection, and joint training is summarized in Figure 2. In Section 5 we show empirically that, although our part detectors have the same form as the models in (Juneja et al. (2013); Sun & Ponce (2013)), they can reach a higher level of performance using a fraction of the number of parts. This translates directly to test time speedup. We present experiments with both HOG (Dalal & Triggs (2005)) and CNN (Krizhevsky et al. (2012)) features and improve the state-of-the-art results on the MIT-indoor dataset (Quattoni & Torralba (2009)) using CNN features.",
      "startOffset" : 8,
      "endOffset" : 1357
    }, {
      "referenceID" : 4,
      "context" : "(2013); Endres et al. (2013)) start from a single random example to initialize a part model, and alternate between finding more examples and retraining the part model. As the quality of the learned part depends on the initial random seed, thousands of parts are generated and a distinctive and diverse subset is extracted by means of some heuristic. Our second contribution is to propose a simple and effective alternative (Section 4). We still initialize a large pool of parts from random examples; we use these initial part models, each trained from a single example, to train image classifiers using `1/`2 regularization as in (Sun & Ponce (2013)). This removes uninformative and redundant parts through group sparsity. This simple method produces better parts than more elaborate alternatives. Joint training (Section 5) improve the quality of the parts further. Our pipeline, comprising random part initialization, part selection, and joint training is summarized in Figure 2. In Section 5 we show empirically that, although our part detectors have the same form as the models in (Juneja et al. (2013); Sun & Ponce (2013)), they can reach a higher level of performance using a fraction of the number of parts. This translates directly to test time speedup. We present experiments with both HOG (Dalal & Triggs (2005)) and CNN (Krizhevsky et al. (2012)) features and improve the state-of-the-art results on the MIT-indoor dataset (Quattoni & Torralba (2009)) using CNN features.",
      "startOffset" : 8,
      "endOffset" : 1462
    }, {
      "referenceID" : 13,
      "context" : "Related ideas in part learning have been recently explored in (Singh et al. (2012); Juneja et al.",
      "startOffset" : 63,
      "endOffset" : 83
    }, {
      "referenceID" : 9,
      "context" : "(2012); Juneja et al. (2013); Sun & Ponce (2013); Doersch et al.",
      "startOffset" : 8,
      "endOffset" : 29
    }, {
      "referenceID" : 9,
      "context" : "(2012); Juneja et al. (2013); Sun & Ponce (2013); Doersch et al.",
      "startOffset" : 8,
      "endOffset" : 49
    }, {
      "referenceID" : 3,
      "context" : "(2013); Sun & Ponce (2013); Doersch et al. (2013)).",
      "startOffset" : 28,
      "endOffset" : 50
    }, {
      "referenceID" : 3,
      "context" : "(2013); Sun & Ponce (2013); Doersch et al. (2013)). The general pipeline in all of these approaches is a two-stage procedure that involves pre-training a set of discriminative parts followed by training a classifier on top of the vector of the part responses. The differences in these methods lay in the details of how parts are discovered. Each approach uses a different heuristic to find a collection of parts such that each part scores high on a subset of categories (and therefore is discriminative) and, collectively, they cover a large area of an image after max-pooling (and therefore are descriptive). Our goal is similar, but we achieve part diversity, distinctiveness, and coverage as natural byproducts of optimizing the “correct” objective function, i.e. the final image classification performance. Reconfigurable Bag of Words (RBoW) model Naderi et al. (2012) is another part-based model used for image classification.",
      "startOffset" : 28,
      "endOffset" : 873
    }, {
      "referenceID" : 3,
      "context" : "(2013); Sun & Ponce (2013); Doersch et al. (2013)). The general pipeline in all of these approaches is a two-stage procedure that involves pre-training a set of discriminative parts followed by training a classifier on top of the vector of the part responses. The differences in these methods lay in the details of how parts are discovered. Each approach uses a different heuristic to find a collection of parts such that each part scores high on a subset of categories (and therefore is discriminative) and, collectively, they cover a large area of an image after max-pooling (and therefore are descriptive). Our goal is similar, but we achieve part diversity, distinctiveness, and coverage as natural byproducts of optimizing the “correct” objective function, i.e. the final image classification performance. Reconfigurable Bag of Words (RBoW) model Naderi et al. (2012) is another part-based model used for image classification. RBoW uses latent variables to define a mapping from image regions to part models. In contrast, the latent variables in our model define a mapping from parts to image regions. It has been shown before (Girshick & Malik (2013)) that joint training is important for the success of part-based models in object detection.",
      "startOffset" : 28,
      "endOffset" : 1157
    }, {
      "referenceID" : 3,
      "context" : "(2013); Sun & Ponce (2013); Doersch et al. (2013)). The general pipeline in all of these approaches is a two-stage procedure that involves pre-training a set of discriminative parts followed by training a classifier on top of the vector of the part responses. The differences in these methods lay in the details of how parts are discovered. Each approach uses a different heuristic to find a collection of parts such that each part scores high on a subset of categories (and therefore is discriminative) and, collectively, they cover a large area of an image after max-pooling (and therefore are descriptive). Our goal is similar, but we achieve part diversity, distinctiveness, and coverage as natural byproducts of optimizing the “correct” objective function, i.e. the final image classification performance. Reconfigurable Bag of Words (RBoW) model Naderi et al. (2012) is another part-based model used for image classification. RBoW uses latent variables to define a mapping from image regions to part models. In contrast, the latent variables in our model define a mapping from parts to image regions. It has been shown before (Girshick & Malik (2013)) that joint training is important for the success of part-based models in object detection. Differently from them, however, we share parts among multiple classes and define a joint optimization in which multiple classifiers are learned concurrently. In particular, the same part can vote strongly for a subset of the classes and against another subset. The most closely related work to ours is (Lobel et al. (2013)).",
      "startOffset" : 28,
      "endOffset" : 1572
    }, {
      "referenceID" : 3,
      "context" : "(2013); Sun & Ponce (2013); Doersch et al. (2013)). The general pipeline in all of these approaches is a two-stage procedure that involves pre-training a set of discriminative parts followed by training a classifier on top of the vector of the part responses. The differences in these methods lay in the details of how parts are discovered. Each approach uses a different heuristic to find a collection of parts such that each part scores high on a subset of categories (and therefore is discriminative) and, collectively, they cover a large area of an image after max-pooling (and therefore are descriptive). Our goal is similar, but we achieve part diversity, distinctiveness, and coverage as natural byproducts of optimizing the “correct” objective function, i.e. the final image classification performance. Reconfigurable Bag of Words (RBoW) model Naderi et al. (2012) is another part-based model used for image classification. RBoW uses latent variables to define a mapping from image regions to part models. In contrast, the latent variables in our model define a mapping from parts to image regions. It has been shown before (Girshick & Malik (2013)) that joint training is important for the success of part-based models in object detection. Differently from them, however, we share parts among multiple classes and define a joint optimization in which multiple classifiers are learned concurrently. In particular, the same part can vote strongly for a subset of the classes and against another subset. The most closely related work to ours is (Lobel et al. (2013)). Their model has two sets of parameters; a dictionary of visual words θ and a set of weights u that specifies the importance the visual words in each category. Similar to what we do here, Lobel et al. (2013) trains u and θ jointly (visual words would be the equivalent of part filters in our terminology).",
      "startOffset" : 28,
      "endOffset" : 1781
    }, {
      "referenceID" : 11,
      "context" : "In practice, filter responses are pooled within several distinct spatial subdivisions (Lazebnik et al. (2006)) to encode weak geometry.",
      "startOffset" : 87,
      "endOffset" : 110
    }, {
      "referenceID" : 5,
      "context" : "DPMs (Felzenszwalb et al. (2010)) also use binary classifiers to detect objects of each class.",
      "startOffset" : 6,
      "endOffset" : 33
    }, {
      "referenceID" : 7,
      "context" : "Generating a part involves picking a random training image (regardless of the image category labels) and extracting features from a random subwindow of the image followed by whitening (Hariharan et al. (2012)).",
      "startOffset" : 185,
      "endOffset" : 209
    }, {
      "referenceID" : 0,
      "context" : "Similar to (Aubry et al. (2013)) we discard the 50% least discriminant patches from each image prior to generating random parts.",
      "startOffset" : 12,
      "endOffset" : 32
    }, {
      "referenceID" : 0,
      "context" : "Similar to (Aubry et al. (2013)) we discard the 50% least discriminant patches from each image prior to generating random parts. Our experimental results with HOG features (Figure 3) show that randomly generated parts using the procedure described here perform better than or comparable to previous methods that are much more involved (Juneja et al. (2013); Doersch et al.",
      "startOffset" : 12,
      "endOffset" : 357
    }, {
      "referenceID" : 0,
      "context" : "Similar to (Aubry et al. (2013)) we discard the 50% least discriminant patches from each image prior to generating random parts. Our experimental results with HOG features (Figure 3) show that randomly generated parts using the procedure described here perform better than or comparable to previous methods that are much more involved (Juneja et al. (2013); Doersch et al. (2013); Sun & Ponce (2013)).",
      "startOffset" : 12,
      "endOffset" : 380
    }, {
      "referenceID" : 0,
      "context" : "Similar to (Aubry et al. (2013)) we discard the 50% least discriminant patches from each image prior to generating random parts. Our experimental results with HOG features (Figure 3) show that randomly generated parts using the procedure described here perform better than or comparable to previous methods that are much more involved (Juneja et al. (2013); Doersch et al. (2013); Sun & Ponce (2013)).",
      "startOffset" : 12,
      "endOffset" : 400
    }, {
      "referenceID" : 6,
      "context" : "Recent part-based methods that do well on this dataset (Juneja et al. (2013); Doersch et al.",
      "startOffset" : 56,
      "endOffset" : 77
    }, {
      "referenceID" : 2,
      "context" : "(2013); Doersch et al. (2013); Sun & Ponce (2013)) use a large number of parts (between 3350 and 13400).",
      "startOffset" : 8,
      "endOffset" : 30
    }, {
      "referenceID" : 2,
      "context" : "(2013); Doersch et al. (2013); Sun & Ponce (2013)) use a large number of parts (between 3350 and 13400).",
      "startOffset" : 8,
      "endOffset" : 50
    }, {
      "referenceID" : 2,
      "context" : "(2013); Doersch et al. (2013); Sun & Ponce (2013)) use a large number of parts (between 3350 and 13400). HOG features: We resize images (maintaining aspect ratio) to have about 2.5M pixels. We extract 32-dimensional HOG features (Dalal & Triggs (2005); Felzenszwalb et al.",
      "startOffset" : 8,
      "endOffset" : 252
    }, {
      "referenceID" : 2,
      "context" : "(2013); Doersch et al. (2013); Sun & Ponce (2013)) use a large number of parts (between 3350 and 13400). HOG features: We resize images (maintaining aspect ratio) to have about 2.5M pixels. We extract 32-dimensional HOG features (Dalal & Triggs (2005); Felzenszwalb et al. (2010)) at multiple scales.",
      "startOffset" : 8,
      "endOffset" : 280
    }, {
      "referenceID" : 2,
      "context" : "(2013); Doersch et al. (2013); Sun & Ponce (2013)) use a large number of parts (between 3350 and 13400). HOG features: We resize images (maintaining aspect ratio) to have about 2.5M pixels. We extract 32-dimensional HOG features (Dalal & Triggs (2005); Felzenszwalb et al. (2010)) at multiple scales. Our HOG pyramid has 3 scales per octave. This yields about 11,000 patches per image. Each part filter wj models a 6×6 grid of HOG features, so wj and ψ(x, zj) are both 1152-dimensional. CNN features: We extract CNN features at multiple scales from overlapping patches of fixed size 256×256 and with stride value 256/3 = 85. We resize images (maintaining aspect ratio) to have about 5M pixels in the largest scale. We use a scale pyramid with 2 scales per octave. This yields about 1200 patches per image. We extract CNN features using Caffe (Jia et al. (2014)) and the hybrid neural network from (Zhou et al.",
      "startOffset" : 8,
      "endOffset" : 861
    }, {
      "referenceID" : 2,
      "context" : "(2013); Doersch et al. (2013); Sun & Ponce (2013)) use a large number of parts (between 3350 and 13400). HOG features: We resize images (maintaining aspect ratio) to have about 2.5M pixels. We extract 32-dimensional HOG features (Dalal & Triggs (2005); Felzenszwalb et al. (2010)) at multiple scales. Our HOG pyramid has 3 scales per octave. This yields about 11,000 patches per image. Each part filter wj models a 6×6 grid of HOG features, so wj and ψ(x, zj) are both 1152-dimensional. CNN features: We extract CNN features at multiple scales from overlapping patches of fixed size 256×256 and with stride value 256/3 = 85. We resize images (maintaining aspect ratio) to have about 5M pixels in the largest scale. We use a scale pyramid with 2 scales per octave. This yields about 1200 patches per image. We extract CNN features using Caffe (Jia et al. (2014)) and the hybrid neural network from (Zhou et al. (2014)).",
      "startOffset" : 8,
      "endOffset" : 917
    }, {
      "referenceID" : 2,
      "context" : "The hybrid network is pre-trained on images from ImageNet (Deng et al. (2009)) and PLACES (Zhou et al.",
      "startOffset" : 59,
      "endOffset" : 78
    }, {
      "referenceID" : 2,
      "context" : "The hybrid network is pre-trained on images from ImageNet (Deng et al. (2009)) and PLACES (Zhou et al. (2014)) datasets.",
      "startOffset" : 59,
      "endOffset" : 110
    }, {
      "referenceID" : 2,
      "context" : "The hybrid network is pre-trained on images from ImageNet (Deng et al. (2009)) and PLACES (Zhou et al. (2014)) datasets. We use the output of the 4096 units in the penultimate fully connected layer of the network (fc7). We denote these features by HP in our plots. Part-based representation: Our final image representation is an mR-dimensional vector of part responses wherem is the number of shared parts andR is the number of spatial pooling regions. We use R = 5 pooling regions arranged in a 1×1 + 2×2 grid. To make the final representation invariant to horizontal image flips we average the mR-dimensional vector of part responses for image x and its right-to-left mirror image x′ to get [r(x,w) + r(x′, w)] /2 as in (Doersch et al. (2013)).",
      "startOffset" : 59,
      "endOffset" : 745
    }, {
      "referenceID" : 9,
      "context" : "On the full dataset, random parts already outperform the results from Juneja et al. (2013), flip invariance boosts the performance beyond Sun & Ponce (2013).",
      "startOffset" : 70,
      "endOffset" : 91
    }, {
      "referenceID" : 9,
      "context" : "On the full dataset, random parts already outperform the results from Juneja et al. (2013), flip invariance boosts the performance beyond Sun & Ponce (2013). Joint training dominates other methods.",
      "startOffset" : 70,
      "endOffset" : 157
    }, {
      "referenceID" : 3,
      "context" : "However, we could not directly compare with the best performance of Doersch et al. (2013) due to the very large number of parts they use.",
      "startOffset" : 68,
      "endOffset" : 90
    }, {
      "referenceID" : 3,
      "context" : "However, we could not directly compare with the best performance of Doersch et al. (2013) due to the very large number of parts they use. Figure 4 shows performance of CNN features on MIT-indoor dataset. As a baseline we extract CNN features from the entire image (after resizing to 256×256 pixels) and train a multi-class linear SVM. This obtains 72.3% average performance. This is a strong baseline. Razavian et al. (2014) get 58.",
      "startOffset" : 68,
      "endOffset" : 425
    }, {
      "referenceID" : 3,
      "context" : "However, we could not directly compare with the best performance of Doersch et al. (2013) due to the very large number of parts they use. Figure 4 shows performance of CNN features on MIT-indoor dataset. As a baseline we extract CNN features from the entire image (after resizing to 256×256 pixels) and train a multi-class linear SVM. This obtains 72.3% average performance. This is a strong baseline. Razavian et al. (2014) get 58.4% using CNN trained on ImageNet. They improve the result to 69% after data augmentation. We applied PCA on the 4096 dimensional features to make them more compact. This is essential for making the joint training tractable both in terms of running time and memory footprint. Figure 4-left shows the effect of PCA dimensionality reduction. It is surprising that we lose only 1% in accuracy with 160 PCA coefficients and only 3.5% with 60 PCA coefficients. We also show how performance changes when a random subset of dimensions is used. For joint training we use 60 PCA coefficients. Figure 4-right shows performance of our part-based models using CNN features. For comparison with HOG features we also plot result of Doersch et al. (2013). Note that part-based representation improves over the CNN extracted on the entire image.",
      "startOffset" : 68,
      "endOffset" : 1171
    }, {
      "referenceID" : 20,
      "context" : "HP denotes the hybrid features from Zhou et al. (2014). Left: the effect of dimensionality reduction on performance of the CNN features extracted from the entire image.",
      "startOffset" : 36,
      "endOffset" : 55
    } ],
    "year" : 2015,
    "abstractText" : "Part-based representations have been shown to be very useful for image classification. Learning part-based models is often viewed as a two-stage problem. First, a collection of informative parts is discovered, using heuristics that promote part distinctiveness and diversity, and then classifiers are trained on the vector of part responses. In this paper we unify the two stages and learn the image classifiers and a set of shared parts jointly. We generate an initial pool of parts by randomly sampling part candidates and selecting a good subset using `1/`2 regularization. All steps are driven directly by the same objective namely the classification loss on a training set. This lets us do away with engineered heuristics. We also introduce the notion of negative parts, intended as parts that are negatively correlated with one or more classes. Negative parts are complementary to the parts discovered by other methods, which look only for positive correlations.",
    "creator" : "LaTeX with hyperref package"
  }
}