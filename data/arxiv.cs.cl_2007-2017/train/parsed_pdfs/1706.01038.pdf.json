{
  "name" : "1706.01038.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Improving Legal Information Retrieval by Distributional Composition with Term Order Probabilities",
    "authors" : [ "Danilo S. Carvalho", "Vu Duc Tran", "Khanh Van Tran", "Minh Le Nguyen" ],
    "emails" : [ "nguyenml}@jaist.ac.jp" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n70 6.\n01 03\n8v 1\n[ cs\n.I R\n] 4\nJ un\n2 01"
    }, {
      "heading" : "1 Introduction",
      "text" : "The ability of answering questions is a long sought goal in the field of Natural Language Processing (NLP). Legal questions, in particular, pose a big challenge to cur-\n∗Supported by CNPq – Brazil scholarship grant\nCopyright c© by the paper’s authors. Copying permitted for private and academic purposes.\nIn: A. Editor, B. Coeditor (eds.): Proceedings of the XYZ Workshop, Location, Country, DD-MMM-YYYY, published at http://ceur-ws.org\nrent NLP techniques, due to their often complex syntactical structure and domain dependent terminology. As we experience an explosive growth in legal document availability through digital means, the need for higher efficiency Legal Information Retrieval (IR) and Question Answering (QA) methods becomes critical for the practice of legal profession. Current increase in information analysis capabilities is not keeping up with such intense growth, leading to under-utilization of available legal resources and to potential for information quality issues. This also brings up the matter of professional ethics and liability on law practice, due to the fundamental importance of relevant and correct information in legal practice.\nA basic step into answering a legal question is retrieving the relevant legal information, e.g., laws, facts and previous verdicts, and aligning their contents in order to decide their applicability to the given question. This is a challenging problem, since laws are written with abstraction in mind, to cover most possible scenarios of any predicted situation. A semantically motivated NLP framework is then needed to allow abstraction and realization of the written law. With this in mind, approaches for both abstraction [AR03] and realization [BFP+05] have been proposed, with Machine Learning (ML) techniques taking an increasingly important role in language analysis [GQ05, CMB+15]. Among the ML approaches, distributional semantics have recently shown promising results in general domain IR [GRMJ15, ZKBA15, PDS+16], but still perform and generalize poorly in the legal domain, due to the difficulty of training under topic-fragmented data sets of relatively small size. For this reason, an approach combining lexical statistics and distributional semantics would be appropriate to leverage both accuracy in literal matching and the possibility of limited abstraction in the form of word/sentence embeddings. However, exploration of such combination has been limited, to the best knowledge of the authors.\nIn this work, we propose a two-stage method for Legal Information Retrieval aimed at Question Answering, in the context of the Competition on Legal Information Extraction/Entailment (COLIEE). The stages are: 1) Relevance analysis and 2) Relevance disambiguation. The method is based on a mixed n-gram language model for relevance analysis, complemented by distributional semantic similarity on cases in which relevance cannot be decided. A technique for obtaining sentence representations from word embeddings is used, which is able to capture semantic information from a sentence, and has advantage when relevant texts have little lexical matching.\nThe remainder of this paper is organized as follows: Section 2 presents related works and relevant results; Section 3 details the Legal Question Answering problem and the COLIEE competition shared task; Section 4 explains our approach to the competition problem; Section 5 presents the experimental setting, results and some discussion about the findings; Finally, Section 6 offers some concluding remarks."
    }, {
      "heading" : "2 Related Work",
      "text" : "Recent developments in Legal Information Retrieval (LIR) and Legal Question Answering (LQA) include the work of Liu, Chen and Ho [LCH15], which presented a method called three-phase prediction (TPP) for retrieval of relevant statutes in Taiwanese criminal law, given queries presented in non-legal language. It employed a hierarchical ranking approach for law corpora, combining several Information Retrieval techniques, as well as Machine Learning and feature selection. The use of distributional semantic representation into LQA has an interesting case in the work of Kim et. al. [KXLG16], in which a SVM-based ranking method using training features such as lemmatized words intersection, dependency pairs and TF-IDF is used for LIR, while Recognition of Textual Entailment (RTE) was performed by a binary SVM classifier, trained on a set of features including semantic similarity calculated from word2vec [MSC+13] embeddings. This method won the combined LQA (LIR + RTE) COLIEE competition in 2016.\nThe creation of sentence embeddings on limited training data scenarios was approached by Carvalho and Nguyen [CN17], using a probability table obtained from word ordering in the corpus sentences, to calculate an attention index for composition of word embeddings through a simple summation formula. The method improved overall accuracy on segmentation of patent documents from the US patent office.\nIn the context of the solo LIR COLIEE competition, [KHJ+16] proposed an ensemble similarity using a least square method (LSM) and linear discrim-\ninant analysis (LDA ensemble) including a variety of features such as lexical similarity, syntactic similarity and semantic similarity. This work showed the best LIR performance in 2016. Our previous main work in COLIEE [CNCN16] introduced a ranking method called R2NC (Ranking Related N-gram Collections), based on a mixed size n-gram language model, which used links between the documents (articles) in the legal corpus to build n-gram collections for each of them, and a variant of TF-IDF scoring to rank them. It achieved a LIR 2nd place in 2015.\nThe method here proposed explores the use of distributional sentence representations obtained through the use of Carvalho and Nguyen’s method [CN17] as a deciding factor in LIR for cases in which R2NC has ambiguous rankings, i.e., arbitrarily close, or insecure scores, i.e., arbitrarily low. This is done by a set of simple rules for exchanging or adding documents in the R2NC retrieved document list."
    }, {
      "heading" : "3 Legal Question Answering – COLIEE",
      "text" : "Answering a legal question comprises: (i) collecting the knowledge required for understanding the given question, and then (ii) inferring the appropriate and correct answer. In the context of the Competition on Legal Information Extraction/Entailment (COLIEE)1, the question is a legal statement varying from specific to general cases, and the required knowledge is embodied in the law itself, in the form of organized articles that compose a fragment of the Japanese Civil Code. The Japanese Civil Code is composed by a collection of numbered articles, each one containing a set of declarations pertaining to a specific topic under law, e.g., labor contracts, mortgages. Given the knowledge from the relevant law articles, the legal statement shall either agree or disagree with the interpretation of the articles, which leads to either affirmative or negative answer accordingly.\nIn COLIEE 2017, activities (i) and (ii) are separated in corresponding phases :\n• Phase One (IR): given a legal question, retrieving relevant articles from the provided part of the Japanese Civil Code.\n• Phase Two (Answering): given a question, from the system retrieved list of relevant articles to the question, deciding the entailment relationship between the retrieved articles and the provided question.\nLegal text inherently distinguishes itself from other types of written communication, by the uniqueness of\n1webdocs.cs.ualberta.ca/m̃iyoung2/COLIEE2017/\nboth its content and intent: to express rules and situations where they apply. This should be done in an abstractive way and with no ambiguity, such that the rules shall be applied only to the intended cases and no case is under conflicting rules. Those requirements certainly enforce a language with stricter terminology and syntax, a higher abstraction level, and with semantics that are foreign or even conflicting with common language use. Such characteristics make the use of Distributed Semantics to be corpus specific on legal text. However, for answering legal questions it is critical to identify the corpus specific and common senses of terms, since law is to be applied in daily life, both of them are used. Besides, another noteworthy characteristic of legal text is its preference for longer sentences, with enumeration or itemization, causing more difficulty for automatic parsing.\nFor this competition, we decided to focus efforts on the Information Retrieval aspect (phase one). Thus, the contributions in this work do not cover the Answering of the questions (phase two), which also deals with Recognition of Textual Entailment (RTE) between questions and articles."
    }, {
      "heading" : "4 Proposed Approach",
      "text" : "Given a legal question, presented in natural language, the legal information retrieval comprises two consecutive stages: 1) relevance analysis and 2) relevance disambiguation. Firstly, a ranked list with a limited number of relevant articles is obtained by using R2NC [CNCN16] (Section 4.1). Next, the first two articles in the ranked list are evaluated over ambiguity (the scores are too close from each other) and insecurity (the scores are too low), under specified thresholds. If the articles’ scores are ambiguous or insecure, sentence embeddings are obtained for both the question and each article in the ranked list using word2vec [MSC+13] and TOP [CN17] (Section 4.3). A new ranked list is obtained by calculating the highest sentence embedding cosine similarity of each pair (question, article). Finally, the two lists are compared through a set of rules, and a decisive set of relevant articles is selected. This process was developed after preliminary experiments past competition experience indicated a very high correlation of ambiguous or low scoring articles and retrieval mistakes. A diagram of the overall process flow is shown in Fig. 1. The next sections present each stage in detail."
    }, {
      "heading" : "4.1 Relevance analysis",
      "text" : "The relevance analysis stage was done entirely with R2NC [CNCN16], which can be summarized in the following process:\n1. Collect the content for each article;\n2. Check references between articles and annotate;\n3. Tokenize and POS-tag;\n4. Remove stopwords: determiners, conjunctions, prepositions and punctuation;\n5. Lemmatize words;\n6. Generate n-grams;\n7. Expand the n-gram set, by including referenced articles’ n-grams;\n8. Associate article number and references;\n9. Store the model.\nExcept for step 4, each step adds new information to the model. The information is obtained from the text, references, and morphological analysis, e.g., POS-tags, lemmas. If an article has references, its n-gram set incorporates the references’ n-grams. In this way, all the necessary information for interpretation of any single article is self-contained. Besides the n-grams, links between the articles are also stored. The same process is repeated for the questions to include the training data\ninformation, and n-gram sets from the trained questions are included in the associated articles’ n-gram models.\nTokenization and lemmatization were done using NLTK 2 (v. 3.2.1) with the Punkt tokenizer and WordNetLemmatizer modules, respectively. Those modules were used with their unchanged default models and settings, trained with the Punk corpus and WordNet, respectively. POS-tagging was done using StanfordTagger3 (v. 3.5.2), using the unchanged englishleft3words-distsim model, which is trained on the partof-speech tagged WSJ section of the Penn Treebank corpus. Fig. 2 illustrates the R2NC process flow. Fig. 3 illustrates the n-gram model creation scheme.\nThe relative relevance of an article with regard to the content of a question is scored using the following formula:\nscore =\n∑ ∀t idf(t)\nIq × |q ng set|+ Iart × |art ng set| ,\nt ∈ (q ng set ∩ art ng set)\n(1)\nwhere q ng set is the set of n-grams for the question, art ng set is the set of n-grams for the article in the stored model, Iq is the relative significance of the question n-gram set size and Iart is the relative significance of the article n-gram set size. idf(t) is the Inverse Document Frequency for the term t over the articles collection\nidf(t) = log N\ndft (2)\nwhere N is the total number of articles and dft is the number of articles in which t appears. Both Iq and Iart are parameters. The scored articles are then ranked from the highest score to the lowest."
    }, {
      "heading" : "4.2 Term Order Probabilities",
      "text" : "The Term Order Probabilities (TOP) [CN17] is an inexpensive method for combining word embeddings into sentence or document embeddings, while keeping word order information and highlighting or attenuating uncommon/common word order combinations, respectively.\nIt consists in two steps:\n1. Calculate P (t1, t2): the probability of any pair of terms (words, n-grams) t1 and t2 appearing in this particular order in the corpus. P (t1, t2) is calculated as:\nP (t1, t2, d) = #(t1, t2, d)\n#(t1, t2, d) + #(t2, t1, d) (3)\n2www.nltk.org 3nlp.stanford.edu/software/tagger.shtml\nwhere #(X) is the number of occurrences of X in the reference corpus and d is the maximum distance between t1 and t2.\n2. Combine the embeddings into one, using the formula\n∑k i=0 e(ti) + ∑k i,j=0:i<j (e(ti) + e(tj)) ∗ (1− P (ti, tj))\nk + ∑k\ni=0 k − i (4)\nwhere e(ti) are the term ti embeddings and k is the length of the sentence. The resulting vector is the sum of the weighted combinations of all embedding pairs in the sentence, and range of j limits a fixed size window of distance d for each term, improving efficiency in longer sentences. The contribution of each term to the sentence embedding is weighted by an “attention index” (1−P (ti, tj)), representing how unlikely the term is to appear in that context. In this way, uncommon patterns have a higher contribution, helping to distinguish even between similar sentences.\nThe probability table Pn×n is calculated for the entire target corpus, where n is the vocabulary size. It is a sparse matrix that can be efficiently stored and accessed. TOP differs from other sentence embedding methods, such as Paragraph Vectors [LM14], in that it can work well with a relatively reduced amount of textual data for training. For this reason, it was chosen for obtaining embeddings from the COLIEE questions and articles’ sentences, considered a very small corpus by current standards (see Section 5).\nAlthough the TOPmethod can use a variety of word embeddings, in this work we chose word2vec [MSC+13] to obtain the distributional representations of words. Thus, TOP mentions henceforth mean the Term Order Probabilities method applied to word2vec embeddings."
    }, {
      "heading" : "4.3 Relevance disambiguation",
      "text" : "Having obtained a ranked list of articles from the previous stage, the relevance disambiguation stage is triggered when the R2NC scores of the first and second ranked articles are close or ambiguous, falling under the following ambiguity condition:\nR score(a1, q)−R score(a2, q) < R ambi thresh\nwhere R score(ai, q) is the R2NC score of the article ai for the question q, and R ambi thresh is the parameter representing the specified lower bound of ranking ambiguity by R2NC . Then a candidate list\nusing TOP cosine similarity ranking is created by selecting top k articles under the condition:\nT cand(q) = {ai | ∀i ≤ k, j < i |T score(ai, q) −\nT score(aj , q)| < T diff thresh}\nwhere T cand(q) is the candidate list from TOP , T score(ai, q) is the TOP score, i.e. similarity, of the article ai for the question q, and T diff thresh represents the upper bound relative to the first ranked article by TOP . Under the prior condition, at most top k articles having close TOP scores are selected into T cand(q). If any of the articles retrieved by R2NC is also in T cand(q), it is selected as the relevant article. Additionally, an aggregated ranked list is generated by linear ranking ensemble from R2NC and TOP for each article in T cand(q). The top 1 of the aggregated ranked list is added to the output relevant list (preferably R2NC in cases of equality).\nIf previous steps result in no article selected, the system checks if the article ranked first by R2NC has\nlow score and the other ranked first by TOP has high score under confidence conditions:\n• R score(a1, q) < R confid thresh\n• T score(a1, q) > T confid thresh\nwhere R confid thresh, T confid thresh are the confidence thresholds over R2NC , and TOP scoring respectively. If the prior condition is satisfied, the first ranked article by TOP is selected as the relevant article. Otherwise, R2NC output is selected as relevant articles."
    }, {
      "heading" : "5 Experiments and Results",
      "text" : ""
    }, {
      "heading" : "5.1 Experimental Setup",
      "text" : "The legal question answering dataset was obtained from the published data for the COLIEE shared task 4, consisting in a text file with a fragment of the Japanese Civil Code translated into English and a set of XML files with training data. The training set for the two tasks contains 580 pairs (question, relevant articles). The Japanese Civil Code fragment contains 1057 articles, with a total of approximately 1700 sentences and 71500 words. Experiments are then conducted to evaluate Information Retrieval methods.\nAdditional data used in the experiments includes the training segment of “1 billion word language model benchmark” corpus [CMS+13] and the complete Japanese Civil Code5, which were used to train the Distributional Semantics model. The combined size of the corpora after balancing is approximately 1.2 billion words.\nWe focus on detailed experiments for R2NC and R2NC+TOP , but not solely TOP . Preliminary experiments showed that TOP similarity ranking alone is consistently worse than R2NC .\n4webdocs.cs.ualberta.ca/m̃iyoung2/COLIEE2017/ 5www.japaneselawtranslation.go.jp"
    }, {
      "heading" : "5.2 Parameter adjustment",
      "text" : "R2NC relative significance parameters were adjusted by leave-one-out validation on the training data. The best setting was Iq = 0.98, Iart = 0.02.\nVariants of TOP models were trained as follows. The data for training word embedding models: lemmatized or non-lemmatized texts. The data for training TOP models: the training questions and provided articles, with or without the whole Japanese Civil Law. The best setting was using non-lemmatized text for training the word embedding model, and training TOP models without the whole Japanese Civil Law.\nR2NC+TOP parameters were selected by conducting leave-one-out experiments with adjusting the parameters over certain ranges (Table 1). The adjustment results higher in some certain values between the mean and bounds of each range, then lower on the bounds, while in the middle, we got steady performance, hence average values were selected."
    }, {
      "heading" : "5.3 Evaluation Method",
      "text" : "For the relevance analysis stage, leave-one-out validation was used to evaluate the potential recall of the model for a limited size ranked list of articles. Performance for phase one was evaluated using precision (P), recall (R) and F-measure (F) as metrics (Eqs. (5), (6) and (7)).\nP = Cr\nRt (5)\nR = Cr\nRl (6)\nF = 2(P ∗R)\nP +R (7)\nwhere Cr counts the correctly retrieved articles for all queries, Rt counts the retrieved articles for all queries, Rl counts the relevant articles for all queries, Cq counts the queries correctly confirmed as true or false and Q counts all the queries."
    }, {
      "heading" : "5.4 Competition Results",
      "text" : "The approach here presented was ranked at second place in the LIR competition (phase one). The third place was achieved by a pure R2NC run. The results, shown in Table 2, indicate a small improvement in\nboth precision and recall, meaning that the added relevance disambiguation stage contributed one or more relevant articles to the R2NC retrieved list without introducing non-relevant ones.\nAdditional experiments were performed after the competition, with the ground truth data being made available by the COLIEE organizers. As shown in Table 3, it was possible to obtain further improvement over the competition results.\nThis was achieved by changing R2NC to a more aggressive setting of Iq = 0.99, Iart = 0.01, with extreme penalization of article length. This setting resulted in marginal gains in the training data, but a better overall result in the competition data set."
    }, {
      "heading" : "5.5 Error Analysis and Discussion",
      "text" : "This subsection answers the question: how does TOP help improve on top of R2NC? The retrieval system with R2NC supported by TOP shows the improvement on top of R2NC in the following two examples (Example 1,and 2). On the way of investigating the contribution of TOP , we further analyze other examples where TOP results better than R2NC . The analysis suggests the semantic characteristics of TOP over R2NC which is limited to lexical matching.\nIn question H28-22-4, R2NC selects Article 606 as the relevant article while the combination selects Article 613 which is actually relevant (Example 1). Looking at the two articles, R2NC results in ambiguity (R2NC scores are 0.808 versus 0.798 for Article 606 and 613 respectively). It turns out that the two articles are very lexically similar to the question. The difference is in logical structures. On one hand, the effectuation of the first paragraph of Article 606 is very similar with the requisite of the question. On the other hand, the effectuation of the question is matched with\nExample 1: Question H28-22-4. R2NC selects nonrelevant Article 606. R2NC supported by TOP selects relevant Article 613.\nQuestion H28-22-4. In cases where a lessee lawfully subleases a leased Thing, if the lessor assumes an obligation to the lessee to effect repairs of the leased Things, the lessor shall also assume a direct obligation to the sublesee to effect repairs of the leased Things.\n[✗][R2NC ]Article 606. (1) A lessor shall assume an obligation to effect repairs necessary for using and taking the profits of the leased Things. (2) The lessee may not refuse if the lessor intends to engage in any act that is necessary for the preservation of the leased Thing.\n[✓][R2NC+TOP ]Article 613. (1) If a lessee lawfully subleases a leased Thing, the sublessee shall assume a direct obligation to the lessor. In such cases, advance payment of rent may not be asserted against the lessor. (2) The provisions of the preceding paragraph shall not preclude the lessor from exercising his/her rights against the lessee.\nthe effectuation the first paragraph of Article 613 in the reversed way. That is ”sublessee” and ”lessor” change their roles between the question and Article 613.\nExample 2: Question H28-34-4. R2NC selects nonrelevant Article 763. R2NC supported by TOP selects relevant Article 975.\nQuestion H28-34-4. A husband and wife may make their will on the same certificate.\n[✗][R2NC ]Article 763. A husband and wife may divorce by agreement.\n[✓][R2NC +TOP ]Article 975. A will may not be made by two or more persons on the same certificate.\nIn another configuration of R2NC , with adjusting the relative significance (Iq = 0.99, Iart = 0.01), R2NC results in ambiguity in the returned ranked articles for question H28-34-4 for which R2NC +TOP selects Article 975 instead of Article 763 (Example 2). In this question, Article 763 mentions about ”husband and wife may ...” but not ”make their will on the same certificate”. Despite that, the match is so decisive by R2NC that Article 763 is picked instead of Article 975 because of word scattering in the relevant article. Even then, TOP is able to pick up the order of the scattered words, the expressions ”will ... made ... on the same certificate” and ”make ... will ... on the same certificate” are similar in TOP vector space.\nThis indicates a TOP advantage on capturing disjoint expressions that make the core of the sentence topic, which may also include inflections and conjugations.\nTo assess these characteristics of TOP , we observe some cases where TOP draws correct outputs while R2NC fails. Those are of questions H28-11-2, H28-222, and H28-26-5 (Example 3,4, and 5).\nExample 3: Question H28-11-2. R2NC selects nonrelevant Article 305 and 296. TOP selects relevant Article 304 (ranked 2nd by R2NC ).\nQuestion H28-11-2. Extension of security interest to proceeds of collateral may be done with respect to a right of retention, a statutory lien, a pledge and a mortgage.\n[✗][R2NC ]Article 305. The provisions of Article 296 shall apply mutatis mutandis to statutory liens.\n[✗][R2NC ]Article 296. A holder of a right of retention may exercise his/her rights against the whole of the Thing retained until his/her claim is satisfied in its entirety.\n[✓][TOP ]Article 304. (1) A statutory lien may also be exercised against Things including monies that the obligor is to receive as a result of the sale, lease or loss of, or damage to, the subject matter of the statutory lien; provided, however, that the holder of the statutory lien must attach the same before the payment or delivery of the monies or other Thing. (2) The provisions of the preceding paragraph shall likewise apply to the consideration for real rights established by the obligor on the subject matter of the statutory lien.\nIn question H28-11-2 (Example 3), TOP shows the advantage of semantical similarity over lexical matching by R2NC . Article 305 with complementary text from Article 296 has higher lexical matching with the question than Article 304. While Article 305 complemented by Article 296 shares phrases ”a right of retention”, and ”statutory lien”, Article 304 only shares phrase ”statutory lien” with the question, then, certainly has lower score than Article 305 by R2NC using lexical matching. On the other side, in the distributed vector space, ”collateral” in the question and ”payment”, and ”monies” in Article 304 are similar, which benefits from distributional word similarity capability of TOP .\nIn question H28-22-2 (Example 4), the relevant Article 572 is selected by TOP , but ranked 8th by R2NC . The relevant score of Article 572 given the question by R2NC is heavily penalized by the long length of the\nExample 4: Question H28-22-2. R2NC selects nonrelevant Article 635. TOP selects relevant Article 572 (ranked 8th by R2NC ).\nQuestion H28-22-2. In cases where there is a special agreement, in a contract of sale, to the effect that the seller will not provide the warranties against defects, if the seller knew but did not disclose that there is any latent defect in the subject matter of a sale, he/she may not be released from the warranties against defects.\n[✗][R2NC ]Article 635. If there is any defect in the subject matter of work performed and the purpose of the contract cannot be achieved because of the defect, the party ordering the work may cancel the contract; provided, however, that this shall not apply to a building or other structure on land.\n[✓][TOP ]Article 572. Even if the seller makes a special agreement to the effect that the seller will not provide the warranties set forth from Article 560 through to the preceding Article, the seller may not be released from that responsibility with respect to any fact that the seller knew but did not disclose, and with respect to any right that the seller himself/herself created for or assigned to a third party.\narticle complemented with a considerable number of referred articles. Even after reducing the effect of article length in R2NC computation, Article 572 is still ranked 3rd. TOP , however, has the advantage of only focusing on the most similar sentence in the article.\nIn question H28-26-5 (Example 5), while it is obvious that the question is a perfect match of the first paragraph of Article 648, R2NC and TOP selections are different. R2NC with the penalty from article length, and the inclusion of relevant question gives a low score for Article 648, hence, selects Article 656. Besides, TOP only looks at the highest match, then selects Article 648.\nThe analysis suggests complementary characteristics of TOP and R2NC and shows potential of exploiting TOP to improve the retrieval system.\nExample 5: Question H28-26-5. R2NC selects nonrelevant Article 656. TOP selects relevant Article 648 (ranked 4th by R2NC ).\nQuestion H28-26-5. In the absence of any special agreements, the mandatary may claim remuneration from the mandator.\n[✗][R2NC ]Article 656. The provisions of this Section shall apply mutatis mutandis to mandates of business that do not constitute juristic acts.\nQuestion H25-29-E. (Relevant to Article 656) A mandatary of a quasi-mandate contract may not claim remuneration from a mandator before performance, even with special agreements that a mandatary may claim remuneration before he/she administers the mandated business.\n[✓][TOP ]Article 648. (1) In the absence of any special agreements, the mandatary may not claim remuneration from the mandator. (2) In cases where the mandatary is to receive remuneration, the mandatary may not claim the same until and unless he/she has performed the mandated business; provided, however, that if the remuneration is specified with reference to period, the provisions of Paragraph 2 of Article 624 shall apply mutatis mutandis. (3) If the mandate terminates during performance due to reasons not attributable to the mandatary, the mandatary may demand remuneration in proportion to the performance already completed."
    }, {
      "heading" : "6 Conclusion",
      "text" : "Information Retrieval in the legal domain is a challenging task, due to its unique combination of complex syntax, domain dependency and high abstraction level. Such combination presents a valuable ground for the application of semantically motivated NLP techniques, capable of a limited level of abstraction. Distributional semantic representations fit this category of techniques, but despite promising results for general IR, still lack on performance in the legal domain. Despite this, in this work we propose a method for combining a pure lexical approach, based on n-gram statistics, with distributional sentence representations in the context of Competition on Legal Information Extraction/Entailment (COLIEE). The combination is done by means of disambiguation rules, applied when the lexical approach is deemed insufficient to decide on the set of relevant documents for a given query, also knowing that the distributional approach is weak by itself.\nCompetition results and further experiments indicate that it is possible to obtain small gains in overall retrieval performance through the proposed approach. Analysis of the errors and improvements observed in the training and competition data revealed comple-\nmentary characteristics from the lexical and distributional approaches, e.g., sensitivity to document size, which can be exploited in order to cover each other’s weaknesses and further improve performance."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This work was supported by JSPS KAKENHI Grant number 15K16048, JSPS KAKENHI Grant Number JP15K12094, and CREST, JST."
    } ],
    "references" : [ {
      "title" : "Law, learning and representation",
      "author" : [ "Kevin D. Ashley", "Edwina L. Rissland" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "Ashley and Rissland.,? \\Q2003\\E",
      "shortCiteRegEx" : "Ashley and Rissland.",
      "year" : 2003
    }, {
      "title" : "Legal question answering using paraphrasing and entailment analysis",
      "author" : [ "Mi-Young Kim", "Ying Xu", "Yao Lu", "Randy Goebel" ],
      "venue" : "In Tenth International Workshop on Juris-informatics (JURISIN),",
      "citeRegEx" : "Kim et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2016
    }, {
      "title" : "Predicting associated statutes for legal problems",
      "author" : [ "Yi-Hung Liu", "Yen-Liang Chen", "WuLiang Ho" ],
      "venue" : "Information Processing & Management,",
      "citeRegEx" : "Liu et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2015
    }, {
      "title" : "Distributed representations of sentences and documents",
      "author" : [ "Quoc V Le", "Tomas Mikolov" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Le and Mikolov.,? \\Q2014\\E",
      "shortCiteRegEx" : "Le and Mikolov.",
      "year" : 2014
    } ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "The current information analysis capabilities of legal professionals are still lagging behind the explosive growth in legal document availability through digital means, driving the need for higher efficiency Legal Information Retrieval (IR) and Question Answering (QA) methods. The IR task in particular has a set of unique challenges that invite the use of semantic motivated NLP techniques. In this work, a two-stage method for Legal Information Retrieval is proposed, combining lexical statistics and distributional sentence representations in the context of Competition on Legal Information Extraction/Entailment (COLIEE). The combination is done by means of disambiguation rules, applied over the lexical rankings when those deemed unreliable for a given query. Competition and experimental results indicate small gains in overall retrieval performance using the proposed approach. Additionally, a analysis of error and improvement cases is presented for a better understanding of the contributions.",
    "creator" : "LaTeX with hyperref package"
  }
}