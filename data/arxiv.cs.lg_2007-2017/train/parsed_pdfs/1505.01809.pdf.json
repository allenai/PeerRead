{
  "name" : "1505.01809.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Language Models for Image Captioning: The Quirks and What Works",
    "authors" : [ "Jacob DevlinF", "Hao Cheng", "Hao Fang", "Saurabh Gupta", "Li Deng", "Xiaodong HeF", "Geoffrey ZweigF", "Margaret MitchellF" ],
    "emails" : [ "jdevlin@microsoft.com", "xiaohe@microsoft.com", "gzweig@microsoft.com", "memitc@microsoft.com" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Recent progress in automatic image captioning has shown that an image-conditioned language model can be very effective at generating captions. Two leading approaches have been explored for this task. The first decomposes the problem into an initial step that uses a convolutional neural network to predict a bag of words that are likely to be present in a caption; then in a second step, a maximum entropy language model (ME LM) is used to generate a sentence that covers a minimum number of the detected words (Fang et al., 2015). The second approach uses the activations\nfrom final hidden layer of an object detection CNN as the input to a recurrent neural network language model (RNN LM). This is referred to as a Multimodal Recurrent Neural Network (MRNN) (Karpathy and Fei-Fei, 2015; Mao et al., 2015; Chen and Zitnick, 2015). Similar in spirit is the the log-bilinear (LBL) LM of Kiros et al. (2014).\nIn this paper, we study the relative merits of these approaches. By using an identical state-ofthe-art CNN as the input to RNN-based and MEbased models, we are able to empirically compare the strengths and weaknesses of the language modeling components. We find that the approach of directly generating the text with an MRNN1 outperforms the ME LM when measured by BLEU on the COCO dataset (Lin et al., 2014),2 but this recurrent model tends to reproduce captions in the training set. In fact, a simple k-nearest neighbor approach, which is common in earlier related work (Farhadi et al., 2010; Mason and Charniak, 2014), performs similarly to the MRNN. In contrast, the ME LM generates the most novel captions, and does the best at captioning images for which there is no close match in the training data. With a Deep Multimodal Similarity Model (DMSM) incorporated,3 the ME LM significantly outperforms other methods according to human judgments. In sum, the contributions of this paper are as follows:\n1. We compare the use of discrete detections and continuous valued CNN activations as the conditioning information for language models trained to generate image captions.\n2. We show that a simple k-nearest neighbor retrieval method performs at near state-of-theart for this task and dataset.\n3. We demonstrate that a state-of-the-art 1In our case, a gated recurrent neural network (GRNN) is\nused (Cho et al., 2014), similar to an LSTM. 2This is the largest image captioning dataset to date. 3As described by Fang et al. (2015).\nar X\niv :1\n50 5.\n01 80\n9v 3\n[ cs\n.C L\n] 1\n4 O\nct 2\n01 5\nMRNN-based approach tends to reconstruct previously seen captions; in contrast, the two stage ME LM approach achieves similar or better performance while generating relatively novel captions.\n4. We advance the state-of-the-art BLEU scores on the COCO dataset.\n5. We present human evaluation results on the systems with the best performance as measured by automatic metrics.\n6. We explore several issues with the statistical models and the underlying COCO dataset, including linguistic irregularities, caption repetition, and data set overlap."
    }, {
      "heading" : "2 Models",
      "text" : "All language models compared here are trained using output from the same state-of-the-art CNN. The CNN used is the 16-layer variant of VGGNet (Simonyan and Zisserman, 2014) which was initially trained for the ILSVRC2014 classification task (Russakovsky et al., 2015), and then finetuned on the Microsoft COCO data set (Fang et al., 2015; Lin et al., 2014)."
    }, {
      "heading" : "2.1 Detector Conditioned Models",
      "text" : "We study the effect of leveraging an explicit detection step to find key objects/attributes in images before generation, examining both an ME LM approach as reported in previous work (Fang et al., 2015), and a novel LSTM approach introduced here. Both use a CNN trained to output a bag of words indicating the words that are likely to appear in a caption, and both use a beam search to find a top-scoring sentence that contains a subset of the words. This set of words is dynamically adjusted to remove words as they are mentioned.\nWe refer the reader to Fang et al. (2015) for a full description of their ME LM approach, whose 500-best outputs we analyze here.4 We also include the output from their ME LM that leverages scores from a Deep Multimodal Similarity Model (DMSM) during n-best re-ranking. Briefly, the DMSM is a non-generative neural network model which projects both the image pixels and caption text into a comparable vector space, and scores their similarity.\nIn the LSTM approach, similar to the ME LM approach, we maintain a set of likely wordsD that\n4We will refer to this system as D-ME.\nhave not yet been mentioned in the caption under construction. This set is initialized to all the words predicted by the CNN above some threshold α.5 The words already mentioned in the sentence history h are then removed to produce a set of conditioning words D \\ {h}. We incorporate this information within the LSTM by adding an additional input encoded to represent the remaining visual attributes D \\ {h} as a continuous valued auxiliary feature vector (Mikolov and Zweig, 2012). This is encoded as f(sh−1 +∑\nv∈D\\{h} gv + Uqh,D), where sh−1 and gv are respectively the continuous-space representations for last word h−1 and detector v ∈ D \\ {h}, U is learned matrix for recurrent histories, and f(·) is the sigmoid transformation."
    }, {
      "heading" : "2.2 Multimodal Recurrent Neural Network",
      "text" : "In this section, we explore a model directly conditioned on the CNN activations rather than a set of word detections. Our implementation is very similar to captioning models described in Karpathy and Fei-Fei (2015), Vinyals et al. (2014), Mao et al. (2015), and Donahue et al. (2014). This joint vision-language RNN is referred to as a Multimodal Recurrent Neural Network (MRNN).\nIn this model, we feed each image into our CNN and retrieve the 4096-dimensional final hidden layer, denoted as fc7. The fc7 vector is then fed into a hidden layer H to obtain a 500- dimensional representation that serves as the initial hidden state to a gated recurrent neural network (GRNN) (Cho et al., 2014). The GRNN is trained jointly with H to produce the caption one word at a time, conditioned on the previous word and the previous recurrent state. For decoding, we perform a beam search of size 10 to emit tokens until an END token is produced. We use a 500-dimensional GRNN hidden layer and 200- dimensional word embeddings.\n2.3 k-Nearest Neighbor Model\nBoth Donahue et al. (2015) and Karpathy and FeiFei (2015) present a 1-nearest neighbor baseline. As a first step, we replicated these results using the cosine similarity of the fc7 layer between each test set image t and training image r. We randomly emit one caption from t’s most similar training image as the caption of t. As reported in previous results, performance is quite poor, with a BLEU\n5In all experiments in this paper, α=0.5.\nscore of 11.2%. However, we explore the idea that we may be able to find an optimal k-nearest neighbor consensus caption. We first select the k = 90 nearest training images of a test image t as above. We denote the union of training captions in this set as C = c1, ..., c5k.6 For each caption ci, we compute the n-gram overlap F-score between ci and each other caption in C. We define the consensus caption c∗ to be caption with the highest mean n-gram overlap with the other captions in C. We have found it is better to only compute this average among ci’s m = 125 most similar captions, rather than all of C. The hyperparameters k and m were obtained by a grid search on the validation set.\nA visual example of the consensus caption is given in Figure 1. Intuitively, we are choosing a single caption that may describe many different images that are similar to t, rather than a caption that describes the single image that is most similar to t. We believe that this is a reasonable approach to take for a retrieval-based method for captioning, as it helps ensure incorrect information is not mentioned. Further details on retrieval-based methods are available in, e.g., (Ordonez et al., 2011; Hodosh et al., 2013)."
    }, {
      "heading" : "3 Experimental Results",
      "text" : ""
    }, {
      "heading" : "3.1 The Microsoft COCO Dataset",
      "text" : "We work with the Microsoft COCO dataset (Lin et al., 2014), with 82,783 training images, and the validation set split into 20,243 validation images and 20,244 testval images. Most images contain multiple objects and significant contextual information, and each image comes with 5 human-\n6Each training image has 5 captions."
    }, {
      "heading" : "LM PPLX BLEU METEOR",
      "text" : "annotated captions. The images create a challenging testbed for image captioning and are widely used in recent automatic image captioning work."
    }, {
      "heading" : "3.2 Metrics",
      "text" : "The quality of generated captions is measured automatically using BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014). BLEU roughly measures the fraction of N -grams (up to 4 grams) that are in common between a hypothesis and one or more references, and penalizes short hypotheses by a brevity penalty term.7 METEOR (Denkowski and Lavie, 2014) measures unigram precision and recall, extending exact word matches to include similar words based on WordNet synonyms and stemmed tokens. We also report the perplexity (PPLX) of studied detectionconditioned LMs. The PPLX is in many ways the natural measure of a statistical LM, but can be loosely correlated with BLEU (Auli et al., 2013)."
    }, {
      "heading" : "3.3 Model Comparison",
      "text" : "In Table 1, we summarize the generation performance of our different models. The discrete detection based models are prefixed with “D”. Some example generated results are show in Table 2.\nWe see that the detection-conditioned LSTM LM produces much lower PPLX than the detection-conditioned ME LM, but its BLEU score is no better. The MRNN has the lowest PPLX, and highest BLEU among all LMs stud-\n7We use the length of the reference that is closest to the length of the hypothesis to compute the brevity penalty.\nied in our experiments. It significantly improves BLEU by 2.1 absolutely over the D-ME LM baseline. METEOR is similar across all three LMbased methods.\nPerhaps most surprisingly, the k-nearest neighbor algorithm achieves a higher BLEU score than all other models. However, as we will demonstrate in Section 3.5, the generated captions perform significantly better than the nearest neighbor captions in terms of human quality judgements.\n3.4 n-best Re-Ranking\nIn addition to comparing the ME-based and RNNbased LMs independently, we explore whether combining these models results in an additive improvement. To this end, we use the 500-best list from the D-ME and add a score for each hypothesis from the MRNN.8 We then re-rank the hypotheses using MERT (Och, 2003). As in previous work (Fang et al., 2015), model weights were optimized to maximize BLEU score on the validation set. We further extend this combination approach to the D-ME model with DMSM scores included during re-ranking (Fang et al., 2015).\nResults are show in Table 3. We find that combining the D-ME, DMSM, and MRNN achieves a 1.6 BLEU improvement over the D-ME+DMSM."
    }, {
      "heading" : "3.5 Human Evaluation",
      "text" : "Because automatic metrics do not always correlate with human judgments (Callison-Burch et al., 2006; Hodosh et al., 2013), we also performed human evaluations using the same procedure as in Fang et al. (2015). Here, human judges were presented with an image, a system generated caption, and a human generated caption, and were asked which caption was “better”.9 For each condition, 5 judgments were obtained for 1000 images from the testval set.\n8The MRNN does not produce a diverse n-best list. 9The captions were randomized and the users were not\ninformed which was which.\nResults are shown in Table 4. The DME+DMSM outperforms the MRNN by 5 percentage points for the “Better Or Equal to Human” judgment, despite both systems achieving the same BLEU score. The k-Nearest Neighbor system performs 1.4 percentage points worse than the MRNN, despite achieving a slightly higher BLEU score. Finally, the combined model does not outperform the D-ME+DMSM in terms of human judgments despite a 1.6 BLEU improvement.\nAlthough we cannot pinpoint the exact reason for this mismatch between automated scores and human evaluation, a more detailed analysis of the difference between systems is performed in Sections 4 and 5."
    }, {
      "heading" : "4 Language Analysis",
      "text" : "Examples of common mistakes we observe on the testval set are shown in Table 5. The D-ME system has difficulty with anaphora, particularly within the phrase “on top of it”, as shown in examples (1), (2), and (3). This is likely due to the fact that is maintains a local context window. In contrast, the MRNN approach tends to generate such anaphoric relationships correctly.\nHowever, the D-ME LM maintains an explicit coverage state vector tracking which attributes have already been emitted. The MRNN implicitly maintains the full state using its recurrent layer, which sometimes results in multiple emission mistakes, where the same attribute is emitted more than once. This is particularly evident when coordination (“and”) is present (examples (4) and (5))."
    }, {
      "heading" : "4.1 Repeated Captions",
      "text" : "All of our models produce a large number of captions seen in the training and repeated for different images in the test set, as shown in Table 6 (also observed by Vinyals et al. (2014) for their LSTM-based model). There are at least two potential causes for this repetition.\nFirst, the systems often produce generic captions such as “a close up of a plate of food”, which may be applied to many publicly available images. This may suggest a deeper issue in the training and evaluation of our models, which warrants more discussion in future work. Second, although the COCO dataset and evaluation server10 has encouraged rapid progress in image captioning, there may be a lack of diversity in the data. We also note that although caption duplication is an issue in all systems, it is a greater issue in the MRNN than the D-ME+DMSM."
    }, {
      "heading" : "5 Image Diversity",
      "text" : "The strong performance of the k-nearest neighbor algorithm and the large number of repeated captions produced by the systems here suggest a lack of diversity in the training and test data.11\nWe believe that one reason to work on image captioning is to be able to caption compositionally novel images, where the individual components of the image may be seen in the training, but the entire composition is often not.\nIn order to evaluate results for only compositionally novel images, we bin the test images based on visual overlap with the training data. For each test image, we compute the fc7 cosine similarity with each training image, and the mean value of the 50 closest images. We then compute BLEU on the 20% least overlapping and 20% most\n10http://mscoco.org/dataset/ 11This is partially an artifact of the manner in which the Microsoft COCO data set was constructed, since each image was chosen to be in one of 80 pre-defined object categories.\noverlapping subsets. Results are shown in Table 7. The DME+DMSM outperforms the k-nearest neighbor approach by 2.5 BLEU on the “20% Least” set, even though performance on the whole set is comparable. Additionally, the D-ME+DMSM outperforms the MRNN by 2.1 BLEU on the “20% Least” set, but performs 2.1 BLEU worse on the “20% Most” set. This is evidence that DME+DMSM generalizes better on novel images than the MRNN; this is further supported by the relatively low percentage of captions it generates seen in the training data (Table 6) while still achieving reasonable captioning performance. We hypothesize that these are the main reasons for the strong human evaluation results of the DME+DMSM shown in Section 3.5."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We have shown that a gated RNN conditioned directly on CNN activations (an MRNN) achieves better BLEU performance than an ME LM or LSTM conditioned on a set of discrete activations; and a similar BLEU performance to an ME LM combined with a DMSM. However, the ME LM + DMSM method significantly outperforms the MRNN in terms of human quality judgments. We hypothesize that this is partially due to the lack of novelty in the captions produced by the MRNN. In fact, a k-nearest neighbor retrieval algorithm introduced in this paper performs similarly to the MRNN in terms of both automatic metrics and human judgements.\nWhen we use the MRNN system alongside the DMSM to provide additional scores in MERT reranking of the n-best produced by the imageconditioned ME LM, we advance by 1.6 BLEU points on the best previously published results on the COCO dataset. Unfortunately, this improvement in BLEU does not translate to improved human quality judgments."
    } ],
    "references" : [ {
      "title" : "Joint language and translation modeling with recurrent neural networks",
      "author" : [ "Michael Auli", "Michel Galley", "Chris Quirk", "Geoffrey Zweig." ],
      "venue" : "Proc. Conf. Empirical Methods Natural Language Process. (EMNLP), pages 1044–1054.",
      "citeRegEx" : "Auli et al\\.,? 2013",
      "shortCiteRegEx" : "Auli et al\\.",
      "year" : 2013
    }, {
      "title" : "Re-evaluation the role of bleu in machine translation research",
      "author" : [ "Chris Callison-Burch", "Miles Osborne", "Philipp Koehn." ],
      "venue" : "EACL, volume 6, pages 249–256.",
      "citeRegEx" : "Callison.Burch et al\\.,? 2006",
      "shortCiteRegEx" : "Callison.Burch et al\\.",
      "year" : 2006
    }, {
      "title" : "Mind’s eye: A recurrent visual representation for image caption generation",
      "author" : [ "Xinlei Chen", "C. Lawrence Zitnick." ],
      "venue" : "Proc. Conf. Comput. Vision and Pattern Recognition (CVPR).",
      "citeRegEx" : "Chen and Zitnick.,? 2015",
      "shortCiteRegEx" : "Chen and Zitnick.",
      "year" : 2015
    }, {
      "title" : "Learning phrase representations using RNN encoder-decoder for statistical machine translation",
      "author" : [ "Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio." ],
      "venue" : "CoRR.",
      "citeRegEx" : "Cho et al\\.,? 2014",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2014
    }, {
      "title" : "Meteor universal: language specific translation evaluation for any target language",
      "author" : [ "Michael Denkowski", "Alon Lavie." ],
      "venue" : "Proc. EACL 2014 Workshop Statistical Machine Translation.",
      "citeRegEx" : "Denkowski and Lavie.,? 2014",
      "shortCiteRegEx" : "Denkowski and Lavie.",
      "year" : 2014
    }, {
      "title" : "Long-term recurrent convolutional networks for visual recognition and description",
      "author" : [ "Jeff Donahue", "Lisa Anne Hendricks", "Sergio Guadarrama", "Marcus Rohrbach", "Subhashini Venugopalan", "Kate Saenko", "Trevor Darrell." ],
      "venue" : "arXiv:1411.4389 [cs.CV].",
      "citeRegEx" : "Donahue et al\\.,? 2014",
      "shortCiteRegEx" : "Donahue et al\\.",
      "year" : 2014
    }, {
      "title" : "Long-term recurrent convolutional networks for visual recognition and description",
      "author" : [ "Jeffrey Donahue", "Lisa Anne Hendricks", "Sergio Guadarrama", "Marcus Rohrbach", "Subhashini Venugopalan", "Kate Saenko", "Trevor Darrell." ],
      "venue" : "Proc. Conf. Comput. Vision",
      "citeRegEx" : "Donahue et al\\.,? 2015",
      "shortCiteRegEx" : "Donahue et al\\.",
      "year" : 2015
    }, {
      "title" : "From captionons to visual concepts and back",
      "author" : [ "Hao Fang", "Saurabh Gupta", "Forrest Iandola", "Rupesh Srivastava", "Li Deng", "Piotr Dollá", "Margaret Mitchell", "John C. Platt", "C. Lawrence Zitnick", "Geoffrey Zweig." ],
      "venue" : "Proc. Conf. Comput. Vision and Pat-",
      "citeRegEx" : "Fang et al\\.,? 2015",
      "shortCiteRegEx" : "Fang et al\\.",
      "year" : 2015
    }, {
      "title" : "Every picture tells a story: generating sentences from images",
      "author" : [ "Ali Farhadi", "Mohsen Hejrati", "Mohammad Amin Sadeghi", "Peter Young", "Cyrus Rashtchian", "Julia Hockenmaier", "David Forsyth." ],
      "venue" : "Proc. European Conf. Comput. Vision (ECCV),",
      "citeRegEx" : "Farhadi et al\\.,? 2010",
      "shortCiteRegEx" : "Farhadi et al\\.",
      "year" : 2010
    }, {
      "title" : "Framing image description as a ranking task: data models and evaluation metrics",
      "author" : [ "Micah Hodosh", "Peter Young", "Julia Hockenmaier." ],
      "venue" : "J. Artificial Intell. Research, pages 853–899.",
      "citeRegEx" : "Hodosh et al\\.,? 2013",
      "shortCiteRegEx" : "Hodosh et al\\.",
      "year" : 2013
    }, {
      "title" : "Deep visualsemantic alignments for generating image descriptions",
      "author" : [ "Andrej Karpathy", "Li Fei-Fei." ],
      "venue" : "Proc. Conf. Comput. Vision and Pattern Recognition (CVPR).",
      "citeRegEx" : "Karpathy and Fei.Fei.,? 2015",
      "shortCiteRegEx" : "Karpathy and Fei.Fei.",
      "year" : 2015
    }, {
      "title" : "Multimodal neural language models",
      "author" : [ "Ryan Kiros", "Ruslan Salakhutdinov", "Richard Zemel." ],
      "venue" : "Proc. Int. Conf. Machine Learning (ICML).",
      "citeRegEx" : "Kiros et al\\.,? 2014",
      "shortCiteRegEx" : "Kiros et al\\.",
      "year" : 2014
    }, {
      "title" : "Microsoft COCO: Common objects in context",
      "author" : [ "Tsung-Yi Lin", "Michael Maire", "Serge Belongie", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Dollár", "C. Lawrence Zitnick." ],
      "venue" : "arXiv:1405.0312",
      "citeRegEx" : "Lin et al\\.,? 2014",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2014
    }, {
      "title" : "Deep captioning with multimodal recurrent neural networks (m-RNN)",
      "author" : [ "Junhua Mao", "Wei Xu", "Yi Yang", "Jiang Wang", "Alan L. Yuille." ],
      "venue" : "Proc. Int. Conf. Learning Representations (ICLR).",
      "citeRegEx" : "Mao et al\\.,? 2015",
      "shortCiteRegEx" : "Mao et al\\.",
      "year" : 2015
    }, {
      "title" : "Domainspecific image captioning",
      "author" : [ "Rebecca Mason", "Eugene Charniak." ],
      "venue" : "CoNLL.",
      "citeRegEx" : "Mason and Charniak.,? 2014",
      "shortCiteRegEx" : "Mason and Charniak.",
      "year" : 2014
    }, {
      "title" : "Context dependent recurrent neural network language model",
      "author" : [ "Tomas Mikolov", "Geoffrey Zweig." ],
      "venue" : "SLT, pages 234–239.",
      "citeRegEx" : "Mikolov and Zweig.,? 2012",
      "shortCiteRegEx" : "Mikolov and Zweig.",
      "year" : 2012
    }, {
      "title" : "Minimum error rate training in statistical machine translation",
      "author" : [ "Franz Josef Och." ],
      "venue" : "ACL, ACL ’03.",
      "citeRegEx" : "Och.,? 2003",
      "shortCiteRegEx" : "Och.",
      "year" : 2003
    }, {
      "title" : "Im2Text: Describing images using 1 million captioned photogrphs",
      "author" : [ "Vicente Ordonez", "Girish Kulkarni", "Tamara L. Berg." ],
      "venue" : "Proc. Annu. Conf. Neural Inform. Process. Syst. (NIPS).",
      "citeRegEx" : "Ordonez et al\\.,? 2011",
      "shortCiteRegEx" : "Ordonez et al\\.",
      "year" : 2011
    }, {
      "title" : "BLEU: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu." ],
      "venue" : "Proc. Assoc. for Computational Linguistics (ACL), pages 311– 318.",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "ImageNet Large Scale Visual Recognition Challenge",
      "author" : [ "Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael Bernstein", "Alexander C. Berg", "Li Fei-Fei." ],
      "venue" : "Interna-",
      "citeRegEx" : "Russakovsky et al\\.,? 2015",
      "shortCiteRegEx" : "Russakovsky et al\\.",
      "year" : 2015
    }, {
      "title" : "Very deep convolutional networks for large-scale image recognition",
      "author" : [ "Karen Simonyan", "Andrew Zisserman." ],
      "venue" : "arXiv preprint.",
      "citeRegEx" : "Simonyan and Zisserman.,? 2014",
      "shortCiteRegEx" : "Simonyan and Zisserman.",
      "year" : 2014
    }, {
      "title" : "Show and tell: a neural image caption generator",
      "author" : [ "Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan." ],
      "venue" : "Proc. Conf. Comput. Vision and Pattern Recognition (CVPR).",
      "citeRegEx" : "Vinyals et al\\.,? 2014",
      "shortCiteRegEx" : "Vinyals et al\\.",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : "The first decomposes the problem into an initial step that uses a convolutional neural network to predict a bag of words that are likely to be present in a caption; then in a second step, a maximum entropy language model (ME LM) is used to generate a sentence that covers a minimum number of the detected words (Fang et al., 2015).",
      "startOffset" : 311,
      "endOffset" : 330
    }, {
      "referenceID" : 10,
      "context" : "This is referred to as a Multimodal Recurrent Neural Network (MRNN) (Karpathy and Fei-Fei, 2015; Mao et al., 2015; Chen and Zitnick, 2015).",
      "startOffset" : 68,
      "endOffset" : 138
    }, {
      "referenceID" : 13,
      "context" : "This is referred to as a Multimodal Recurrent Neural Network (MRNN) (Karpathy and Fei-Fei, 2015; Mao et al., 2015; Chen and Zitnick, 2015).",
      "startOffset" : 68,
      "endOffset" : 138
    }, {
      "referenceID" : 2,
      "context" : "This is referred to as a Multimodal Recurrent Neural Network (MRNN) (Karpathy and Fei-Fei, 2015; Mao et al., 2015; Chen and Zitnick, 2015).",
      "startOffset" : 68,
      "endOffset" : 138
    }, {
      "referenceID" : 2,
      "context" : ", 2015; Chen and Zitnick, 2015). Similar in spirit is the the log-bilinear (LBL) LM of Kiros et al. (2014).",
      "startOffset" : 8,
      "endOffset" : 107
    }, {
      "referenceID" : 12,
      "context" : "We find that the approach of directly generating the text with an MRNN1 outperforms the ME LM when measured by BLEU on the COCO dataset (Lin et al., 2014),2 but this recurrent model tends to reproduce captions in the training set.",
      "startOffset" : 136,
      "endOffset" : 154
    }, {
      "referenceID" : 8,
      "context" : "In fact, a simple k-nearest neighbor approach, which is common in earlier related work (Farhadi et al., 2010; Mason and Charniak, 2014), performs similarly to the MRNN.",
      "startOffset" : 87,
      "endOffset" : 135
    }, {
      "referenceID" : 14,
      "context" : "In fact, a simple k-nearest neighbor approach, which is common in earlier related work (Farhadi et al., 2010; Mason and Charniak, 2014), performs similarly to the MRNN.",
      "startOffset" : 87,
      "endOffset" : 135
    }, {
      "referenceID" : 3,
      "context" : "In our case, a gated recurrent neural network (GRNN) is used (Cho et al., 2014), similar to an LSTM.",
      "startOffset" : 61,
      "endOffset" : 79
    }, {
      "referenceID" : 3,
      "context" : "In our case, a gated recurrent neural network (GRNN) is used (Cho et al., 2014), similar to an LSTM. This is the largest image captioning dataset to date. As described by Fang et al. (2015). ar X iv :1 50 5.",
      "startOffset" : 62,
      "endOffset" : 190
    }, {
      "referenceID" : 20,
      "context" : "The CNN used is the 16-layer variant of VGGNet (Simonyan and Zisserman, 2014) which was initially trained for the ILSVRC2014 classification task (Russakovsky et al.",
      "startOffset" : 47,
      "endOffset" : 77
    }, {
      "referenceID" : 19,
      "context" : "The CNN used is the 16-layer variant of VGGNet (Simonyan and Zisserman, 2014) which was initially trained for the ILSVRC2014 classification task (Russakovsky et al., 2015), and then finetuned on the Microsoft COCO data set (Fang et al.",
      "startOffset" : 145,
      "endOffset" : 171
    }, {
      "referenceID" : 7,
      "context" : ", 2015), and then finetuned on the Microsoft COCO data set (Fang et al., 2015; Lin et al., 2014).",
      "startOffset" : 59,
      "endOffset" : 96
    }, {
      "referenceID" : 12,
      "context" : ", 2015), and then finetuned on the Microsoft COCO data set (Fang et al., 2015; Lin et al., 2014).",
      "startOffset" : 59,
      "endOffset" : 96
    }, {
      "referenceID" : 7,
      "context" : "We study the effect of leveraging an explicit detection step to find key objects/attributes in images before generation, examining both an ME LM approach as reported in previous work (Fang et al., 2015), and a novel LSTM approach introduced here.",
      "startOffset" : 183,
      "endOffset" : 202
    }, {
      "referenceID" : 7,
      "context" : "We refer the reader to Fang et al. (2015) for a full description of their ME LM approach, whose 500-best outputs we analyze here.",
      "startOffset" : 23,
      "endOffset" : 42
    }, {
      "referenceID" : 15,
      "context" : "We incorporate this information within the LSTM by adding an additional input encoded to represent the remaining visual attributes D \\ {h} as a continuous valued auxiliary feature vector (Mikolov and Zweig, 2012).",
      "startOffset" : 187,
      "endOffset" : 212
    }, {
      "referenceID" : 8,
      "context" : "Our implementation is very similar to captioning models described in Karpathy and Fei-Fei (2015), Vinyals et al.",
      "startOffset" : 69,
      "endOffset" : 97
    }, {
      "referenceID" : 8,
      "context" : "Our implementation is very similar to captioning models described in Karpathy and Fei-Fei (2015), Vinyals et al. (2014), Mao et al.",
      "startOffset" : 69,
      "endOffset" : 120
    }, {
      "referenceID" : 8,
      "context" : "Our implementation is very similar to captioning models described in Karpathy and Fei-Fei (2015), Vinyals et al. (2014), Mao et al. (2015), and Donahue et al.",
      "startOffset" : 69,
      "endOffset" : 139
    }, {
      "referenceID" : 5,
      "context" : "(2015), and Donahue et al. (2014). This joint vision-language RNN is referred to as a Multimodal Recurrent Neural Network (MRNN).",
      "startOffset" : 12,
      "endOffset" : 34
    }, {
      "referenceID" : 3,
      "context" : "The fc7 vector is then fed into a hidden layer H to obtain a 500dimensional representation that serves as the initial hidden state to a gated recurrent neural network (GRNN) (Cho et al., 2014).",
      "startOffset" : 174,
      "endOffset" : 192
    }, {
      "referenceID" : 5,
      "context" : "Both Donahue et al. (2015) and Karpathy and FeiFei (2015) present a 1-nearest neighbor baseline.",
      "startOffset" : 5,
      "endOffset" : 27
    }, {
      "referenceID" : 5,
      "context" : "Both Donahue et al. (2015) and Karpathy and FeiFei (2015) present a 1-nearest neighbor baseline.",
      "startOffset" : 5,
      "endOffset" : 58
    }, {
      "referenceID" : 17,
      "context" : ", (Ordonez et al., 2011; Hodosh et al., 2013).",
      "startOffset" : 2,
      "endOffset" : 45
    }, {
      "referenceID" : 9,
      "context" : ", (Ordonez et al., 2011; Hodosh et al., 2013).",
      "startOffset" : 2,
      "endOffset" : 45
    }, {
      "referenceID" : 12,
      "context" : "We work with the Microsoft COCO dataset (Lin et al., 2014), with 82,783 training images, and the validation set split into 20,243 validation images and 20,244 testval images.",
      "startOffset" : 40,
      "endOffset" : 58
    }, {
      "referenceID" : 7,
      "context" : "†: From (Fang et al., 2015).",
      "startOffset" : 8,
      "endOffset" : 27
    }, {
      "referenceID" : 18,
      "context" : "The quality of generated captions is measured automatically using BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014).",
      "startOffset" : 71,
      "endOffset" : 94
    }, {
      "referenceID" : 4,
      "context" : ", 2002) and METEOR (Denkowski and Lavie, 2014).",
      "startOffset" : 19,
      "endOffset" : 46
    }, {
      "referenceID" : 4,
      "context" : "7 METEOR (Denkowski and Lavie, 2014) measures unigram precision and recall, extending exact word matches to include similar words based on WordNet synonyms and stemmed tokens.",
      "startOffset" : 9,
      "endOffset" : 36
    }, {
      "referenceID" : 0,
      "context" : "The PPLX is in many ways the natural measure of a statistical LM, but can be loosely correlated with BLEU (Auli et al., 2013).",
      "startOffset" : 106,
      "endOffset" : 125
    }, {
      "referenceID" : 7,
      "context" : "†: previously reported and reconfirmed BLEU scores from (Fang et al., 2015).",
      "startOffset" : 56,
      "endOffset" : 75
    }, {
      "referenceID" : 16,
      "context" : "8 We then re-rank the hypotheses using MERT (Och, 2003).",
      "startOffset" : 44,
      "endOffset" : 55
    }, {
      "referenceID" : 7,
      "context" : "As in previous work (Fang et al., 2015), model weights were optimized to maximize BLEU score on the validation set.",
      "startOffset" : 20,
      "endOffset" : 39
    }, {
      "referenceID" : 7,
      "context" : "We further extend this combination approach to the D-ME model with DMSM scores included during re-ranking (Fang et al., 2015).",
      "startOffset" : 106,
      "endOffset" : 125
    }, {
      "referenceID" : 1,
      "context" : "Because automatic metrics do not always correlate with human judgments (Callison-Burch et al., 2006; Hodosh et al., 2013), we also performed human evaluations using the same procedure as in Fang et al.",
      "startOffset" : 71,
      "endOffset" : 121
    }, {
      "referenceID" : 9,
      "context" : "Because automatic metrics do not always correlate with human judgments (Callison-Burch et al., 2006; Hodosh et al., 2013), we also performed human evaluations using the same procedure as in Fang et al.",
      "startOffset" : 71,
      "endOffset" : 121
    }, {
      "referenceID" : 1,
      "context" : "Because automatic metrics do not always correlate with human judgments (Callison-Burch et al., 2006; Hodosh et al., 2013), we also performed human evaluations using the same procedure as in Fang et al. (2015). Here, human judges were presented with an image, a system generated caption, and a human generated caption, and were asked which caption was “better”.",
      "startOffset" : 72,
      "endOffset" : 209
    }, {
      "referenceID" : 21,
      "context" : "All of our models produce a large number of captions seen in the training and repeated for different images in the test set, as shown in Table 6 (also observed by Vinyals et al. (2014) for their LSTM-based model).",
      "startOffset" : 163,
      "endOffset" : 185
    } ],
    "year" : 2015,
    "abstractText" : "Two recent approaches have achieved state-of-the-art results in image captioning. The first uses a pipelined process where a set of candidate words is generated by a convolutional neural network (CNN) trained on images, and then a maximum entropy (ME) language model is used to arrange these words into a coherent sentence. The second uses the penultimate activation layer of the CNN as input to a recurrent neural network (RNN) that then generates the caption sequence. In this paper, we compare the merits of these different language modeling approaches for the first time by using the same state-ofthe-art CNN as input. We examine issues in the different approaches, including linguistic irregularities, caption repetition, and data set overlap. By combining key aspects of the ME and RNN methods, we achieve a new record performance over previously published results on the benchmark COCO dataset. However, the gains we see in BLEU do not translate to human judgments.",
    "creator" : "TeX"
  }
}