{
  "name" : "1610.03147.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Context-Aware Online Learning for Course Recommendation of MOOC Big Data",
    "authors" : [ "Yifan Hou" ],
    "emails" : [ "panzhou@hust.edu.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n61 0.\n03 14\n7v 1\n[ cs\n.L G\n] 1\n1 O\nct 2\n01 6\nIndex Terms—MOOC, big data, context bandit, course recommendation, online learning\nI. INTRODUCTION\nMOOC is a concept first proposed in 2008 and known to the world in 2012 [1] [2]. Not being accustomed to the traditional teaching model or being desirous to find a unique learning style, a growing number of people have partiality for learning on MOOCs. Advanced thoughts and novel ideas give great vitality to MOOC, and over 15 million users have marked in Coursera [3] which is a platform of it. Course recommender system helps the learner to find the requisite course directly in the course ocean of numerous MOOC platforms such like Coursera, edX, Udacity and so on [4]. However, due to the rapid growth rate of users, the amount of needed courses\nYifan Hou and Pan Zhou are with School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan 430074, China.\nTing Wang is with Computer Science and Engineering, Lehigh University, PA 18015, USA.\nYuchong Hu is with School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, 430074 China.\nDapeng Oliver Wu is with Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL 32611, USA.\nContacting email: panzhou@hust.edu.cn This work was supported by the National Science Foundation of China\nunder Grant 61401169, Grant CNS-1116970, and Grant NSFC 61529101.\nhave been expanding continuously. According to the survey about the completion rate of MOOC, only 4% people finish their chosen courses. Therefore, finding a preferable course resources and locating them in the massive data bank, e.g., cloud computing and storage platforms, would be a daunting “needle-in-a-haystack” problem.\nOne key challenge in future MOOC course recommendation are processing tremendous data that bears the feature of volume, variety, velocity, variability and veracity [5] of big data. Precisely, the recommender system for MOOC big data needs to handle the dynamic changing, heterogonous sources, priorily unknown scale and nearly infinite course data effectively. Moreover, since the Internet and clouding computing services are turning in the direction of supporting different users around the world, cultural difference, geographic disparity and education level, the recommender system needs to consider the features of students (learners), i.e., one has his/her unique preference in evaluating a course in MOOC. For example, someone pays more attention to the quality of exercises while the other one focuses the classroom rhythm more. We use the concept of context to represent those mentioned features as the learners’ personalized information. The context space is encoded as a multidimensional space (dX dimensions), where dX is the number of features. As such, the recommendation becomes student-specific, which improves the recommending accuracy. Hence, appending context information to the model of handling the courses is ineluctable [7] [8].\nPrevious context-aware algorithms such as [29] only perform well with the known scale of recommendation datasets. Specifically, the algorithm in [29] would rank all courses in MOOC as leaf nodes, then it clusters some relevance courses together as course nodes to build their parent nodes based on the historical information and current users’ features. The algorithm keeps clustering the course nodes and build their parent nodes until the root node (bottom-up design). If there comes a new course, all the nodes and clusters are changed and needed to compute again. As for the MOOC big data, since the number of courses keeps increasing and becoming fairly tremendous, algorithms in [29] are prohibitive to be applied.\nOur main theme in this paper is recommending courses in tremendous datasets to students in real-time based on their preferences. The course data are stored in course cloud(s) and new courses can be loaded at any time. We devise a top-down binary tree to denote and record the process of partitioning course datasets, and every node in the tree is a set of courses. The course scores feedback from students in marking system are denoted as rewards. Specifically, there is only one root course node in the binary tree at first. Every time a course is\nrecommended, a reward is fed back from the student which is used to improve the next recommending accuracy. The reward structure consists as a unknown stochastic function of context and course features at each recommendation, and our algorithm concerns the expected reward of every node. Then the course binary tree divide the current node into two child nodes and select one course randomly in the node with the current best expected value. It omits most of courses in the node that wouldn’t be selected to greatly improve the learning performance. It also supports incoming new courses to the existing nodes as unselected items without changing the current built pattern of the tree.\nHowever, other challenges that influence on the recommending accuracy still remain. In practice, we observe that the number of courses keeps increasing and the in-mummery storage cost of one online course is about 1GB in average, which is fairly large. Therefore, how to store the tremendous course data and to process the course data effectively become a challenge. Most previous works [29] [30] could only realize the linear space complexity, however it’s not promising for MOOC big data. We propose the distributed storage scheme to store the course data with many small storage units separately. For example, the storage units may be divided based on the platforms of MOOC or the languages of courses. On the one hand, this method can make invoking process effectively with little extra costs on course recommendation. On the other hand, we prove the space complexity can be bounded sublinearly under the optimal condition which is better than [29].\nIn summary, we propose an effective context-aware and online learning algorithm for course big data recommendation to offer the courses to learners in MOOCs. The main contributions are listed as follows:\n• The algorithm can accommodate to highly-dynamic increasing course database environments, realizing the real big data support by the course tree that could index nearly infinite and dynamic changing datasets. • We consider context-awareness for better personalized course recommendations, and devise effective context partition scheme that greatly improves the learning rate and accuracy for different featured students. • Our proposed distributed storage model stores data with distributed units rather than single storage carrier, allowing the system to utilize the course data better and performing well with huge amount of data. • Our algorithms enjoy superior time and space complexity. The time complexity is bounded linearly, which means the algorithms have a higher learning rate than preious methods in MOOC. For the space complexity, we prove that it is linear in the primary algorithm and sublinear in distributed storage algorithm under optimal condition.\nThe rest of paper are organized as follows: Section II reviews related works and compares with our algorithms. Section III formulates the recommendation problem and algorithm models. Section IV and Section V illustrate our algorithms and bound their regret. Section VI analyzes the space complexity of our algorithms and compares the theoretical results with existing works. In Section VII, we verify the algorithms\nby experiment results and compare with relevant previous algorithms [29] [31]. Section VIII concludes the paper."
    }, {
      "heading" : "II. RELATED WORKS",
      "text" : "A plethora of previous works of exists on recommending algorithms. As for MOOC, the two major tactics to actualize the algorithm are filtering-based approaches and online learning methods [11]. Apropos of filtering-based approaches, there are some branches such like collaborative filtering [12] [9], content-based filtering [13] and hybrid approaches [14] [15]. The collaborative filtering approach gathers the students’ learning records together and then classifies them into groups based on the characteristics provided, recommending a course from the group’s learning records to new students [12] [9]. Content-based filtering recommends a course to the learner which is relevant to the learning records before [13]. Hybrid approach is the combination of the two methods. The filteringbased approaches can perform better at the beginning than online learning algorithm. However, when the data come to very large-scale or become stochastic, the filtering-based approaches lose the accuracy and become incapable of utilizing the history records adequately. Meanwhile, not having the context makes the method unable to recommend courses precisely by taking every learner’s preference into account.\nOnline learning can overcome the deficiencies of filteringbased approaches. Most of the previous works of recommending a course utilize the adaptive learning [16] [17] [18]. In [16], the CML model was presented. This model combines the cloud, personalized course map and adaptive MOOC learning system together, which is quite comprehensive for the course recommend with context-awareness. Nevertheless, as for big data, the model is not efficient enough since the works couldn’t handle the dynamic datasets and these may have a high cost in time with near “infinite” datasets. The similar works are widely distributed in [19]–[24] as contextual bandit problems. In these works, the systems know the rewards of selected ones and record them every time, which means the course feedback can be gathered from learners after they receive the recommended course. There is no work before that realize contextual bandits with infinitely increasing datasets. Our work is motivated from [24] for big data support, however we consider the contextaware online learning for the first time with delicately devised context partition schemes for MOOC big data."
    }, {
      "heading" : "III. PROBLEM FORMULATION",
      "text" : "In this section, we present the system model, context model, course model and the regret definition. Besides, we define some relevant notations and preliminary definitions.\nFig. 1 illustrates the model of operation. The senior users such as professors or network platforms upload the course resources including the pre-recorded videos for open classed, live videos for online course and class tests, etc. of MOOC to course cloud. Then the system collects learners’ essential information of features, for instance, the reputation of professors, course language, the educational level concerning courses with appropriate difficulty, the quality in presentations, etc. When the system obtains the information, it recommends\ncourses which the users haven’t learned before to the learners as the alternative choices. Then learners give courses satisfaction payoffs to the system based on their preferences such like some learners prefer computer science and others prefer classical music."
    }, {
      "heading" : "A. System Model",
      "text" : "We use time slots t = 1, 2, · · · , T to delegate rounds. In each time slot, there are three running states: (1) a learner with an exclusive context information comes into our model; (2) the model recommends a course from the current course cluster to the learner; (3) the learner provides a payoff due to the newly recommended course to the system. Afterwards the model will learn how to preform better in the next recommended procedure on account of the rewards obtained. We give concepts of three essential elements:\nLearners: We denote the set of incoming learners as S = 1, 2, · · ·) during the run of the algorithm. When a learner comes, the system will suggest a course based on the previou records of payoffs and context information.\nContexts: Context is illustrated as set X , with the space being continuous. Besides, we assume the context space is a dX -dimension space which means the context x ∈ X is a vector of dX -dimension. For example, the context vector includes educational levels from zero basis to PhD in related fields, the geographical positions and the language backgrounds. With the normalization of context information, we suppose the context space is X = [0, 1]dX which is a unit hypercube.DX (x, x′) is used to delegate the distance between context. We use the Lipschitz condition to define the distance.\nAssumption 1. There exists constant LX > 0 such that for all context x, x′ ∈ X , we have DX (x,x′) ≤ LX ||x− x′||α, where || • || denotes the Euclidian norm in RdX .\nNote that the Lipschitz constants LX are not required to be known by our recommendation algorithm. They will only be used in quantifying its performance. Assumption 1 indicates the initial maximal context deviation whose threshold is defined by LX . We present the context distance mathematically with LX and α.\nCourses: We model the set of courses as a dC -dimension vector space, where each vector contains dC course features. The number of courses is unknown thus we can’t determine dC before attaining various dataset. Since the number of courses grows exponentially, we manage the number to infinity to adapt to development trend of MOOC big data. Specifically, we consider the course model unknown distribution sets. Similar to the context, we define the course space as C and the correlation distance of courses as DC (c, c′) to indicate the relativity between the two courses.\nDefinition 1. DC over C is a non-negative mapping: DC(c, c\n′) ≥ 0 (C2 → R). DC(c, c) = 0 when c = c′. We assume that the two courses which are more relevant have the smaller correlation distance (DC) between them. For example, the courses teached both in English have closer distance than the courses with different languages as far as the language feature of context.\nFig. 2 illustrates the relationship between context information and course information over reward. To better illustrate the relations, we degenerate the dimensions of them to 1 dX = dC = 1. Thus we take the context information and course details as two horizontal axes in rectangular coordinate system. From the schematic diagram, the reward varies with the context and course changing. To be more specific, for a determined learner whose context is constant, the reward differs from courses shown in blue plane coordinate system. On the other hand, for a determined course shown in crystal plane coordinate system, different people have different attitudes. Practically, we still take the reward dimension 1. Besides, we expanding the dimension of context to dX and the dimension of course to dC ."
    }, {
      "heading" : "B. Context Model for Individualization",
      "text" : "Since the context space is a dX -dimensional vector space, we normalize every dimension of context range from 0 to 1. The realistic meaning of the dimensions denotes dX unrelated features such as ages, cultural backgrounds, nationalities etc, representing the preferences of the learner. We define the slicing number of context unit hypercube as nT , indicating the number of grades in the dimension. To have a better formulation, PT = {P1,P2,· · · ,P(nT )dX } is used to denote the sliced chronological sub-hypercubes. As illustrated in Fig. 3, we let dX = 3 and nT = 2. We divide every axis into 2\nparts and the number of sub-hypercubes is (nT )dX = 8. For the simplicity, we use the center point in the sub-hypercube to represent the specific contexts in the cube. With this model of context, we divide the different users into (nT )dX types."
    }, {
      "heading" : "C. Course Set Model for Recommendation",
      "text" : "For the course model, we use the binary tree to index the while course dataset. The courses are gathered together and then it will be divided into two parts by the algorithm based (on the regret information) which is exploring process of the tree model. The number of nodes in the depth h is 2h, thus we let Nh,i (1 ≤ i ≤ 2h) represent the node in the depth of h. The root node of the binary course tree is a set of the whole courses N0,1 = C. And with the exploration of the tree, the two child nodes contain all the courses from the parent node, and they never intersect with each other, Nh,i = Nh+1,2i−1 ∪Nh+1,2i, Nh,i∩Nh,j = ∅ for any i 6= j. Thus, the C can be covered by the regions of Nh,i at any depth C =\n2h∪ 1 Nh,i.\nTo better describe the nodes, we define the diam(Nh,i) to indicate the size of course nodes.\ndiam(Nh,i) = sup c,c′∈Nh,i\nDC(c, c ′) for any c, c′ ∈ Nh,i.\nThe distance between courses can be represented as the gap between course languages, course time length, course types and any others which indicate the discrepancy. We denote the size of nodes with the largest distance in the course dataset. Note that the diameter is based on the distance, and that can be adjusted by selecting different mappings. To the convenient my analysis, we make some reasonable assumptions as follows.\nAssumption 2. For any node Nh,i, there exists constant k1 and m ∈ (0, 1), where we can get diam(Nh,i) ≤ k1(m)h.\nWith assumption 2 we can limit the size of nodes with k1(m)\nh. We utilize exponential convergence to keep the threshold of nodes diameter. Since the model tree is binary tree, the number of nodes increases exponentially with the depth rising. The data we get are stochastic (i.i.d.), so we use the mean reward to handle the model. For each context subhypercube, there is only one overall optimal course in a node of each depth and each node has a local optimal course. We define the course gap with the help of mean reward f(r).\nAssumption 3. For all courses c1, c2 ∈ C in the same context point, they satisfy f(rPtc1 )− f(rPtc2 ) ≤ max{f(rPt∗)−f(rPtc1 ), DC(c1, c2)}. (1)\nAssumption 3 uses the suboptimal gap and distance to denote the mean reward gap between course c1 and course c2. With this assumption, we only need to consider the suboptimal node’s gap and distance rather than the gap between two normal nodes directly."
    }, {
      "heading" : "D. The Regret of Learning",
      "text" : "Simply, the regret indicates the loss of accuracy in the recommended procedure due to the unknown dynamics. We assume the reward rt ∈ [0, 1] is the indication of accuracy. According to the reward we define the regret as\nR(T ) = T ∑\nt=1 rPt∗ − E\n[\nT ∑ t=1 (I(r̂t = rt))\n]\n, (2)\nwhere the rt represents the reward in time t and the subscript ∗ means the optimal solution, i.e., r∗ is the best reward.\nRegret shows the convergence rate of the optimal recommended option. When the regret is sublinear R(T ) = O(T γ) where 0 < γ < 1, the algorithm will finally converge to the optimal solution. In the following section we will propose our algorithms with sublinear regret."
    }, {
      "heading" : "IV. REPARTIMIENTO HIERARCHICAL TREE",
      "text" : "In this section we propose our main online learning algorithm to mine a course in MOOC big data."
    }, {
      "heading" : "A. Algorithm",
      "text" : "The algorithm is called Repartimiento Hierarchical Trees (RHT) and the pseudocode is given in Algorithm 1. In this algorithm we first find the arrived learners’ context subhypercube Pt from the context space and replace the original context with the center point xt in that sub-hypercube. And we use the set Γ to denote all the nodes whose courses have been recommended and set Ω to show the exploration path. Besides, we introduce some new concepts, the Bound BPth,i which is the upper bound of reward and the Estimation EPth,i which is the estimated value of reward in the tree with depth h of Pt context sub-hypercube. In each time, the algorithm finds one course node whose EPth,i is highest in the set Γ and walks to the node with the route Ω, selecting one course from that node and recommending it for the reward from learner. The procedure is shown in Fig. 4. When the reward feeds back, the algorithm refreshes EPth,i of nodes of the current tree based on BPth,i and rewards rt.\nSince exploring is a top-down process, after we refresh the upper bound of reward in course nodes, we update the estimation value from bottom to the top. In the end of algorithm 1, it refreshes the tree with EPth,i values.\nAlgorithm 2 shows the exploration process in RHT. When we turn to explore new course nodes, the model prefers to select the nodes with higher estimation value. In other words, the essence of selecting new node is reducing the size of course cluster to make it easier to find the optimal courses for learners. After the new nodes being chosen, it will be taken in the sets ΓPt and ΩPt for the next calculation.\nIn algorithm 3 we use BPth,i to indicate the upper bound of highest reward. The first term µPth,i is the average rewards, and\nAlgorithm 1 Repartimiento Hierarchical Trees (RHT) Require : The constants k1 > 0, m ∈ (0, 1), the learner’s context xt. Auxiliary function : Exploration and Bound Updating Initialization : For all context sub-hypercubes belonging to PT ΓPt = {NPt0,1} EPt1,i = ∞ for i = 1, 2.\n1: for t = 1, 2, ...T do 2: for dt = 0, 1, 2...dX do 3: Find the context interval in dt dimension 4: end for 5: Get the context sub-hypercube Pt 6: xt ← center point of Pt 7: NPth,i ← NPt0,1 8: ΩPt ← NPth,i 9: Exploration (ΓPt)\n10: Select a course from the node NPth,i randomly and recommend to the learner 11: Get the reward rt 12: for all Pt ∈ PT do 13: Bound Updating (ΩPt ) 14: ΩPttemp ← ΩPt 15: for ΩPttemp 6= NPt0,1 do 16: NPth,i ← one leaf of ΩPt 17: EPth,i ← min { BPth,i,max{EPth+1,2i−1, EPth+1,2i} } 18: Delete the NPth,i from Ω Pt temp 19: end for 20: end for 21: end for\nAlgorithm 2 Exploration (A sub-procedure used in RHT)\n1: while NPth,i ∈ ΓPt do 2: if EPth+1,2i−1 > E Pt h+1,2i then 3: Temp ← 1 4: else if EPth+1,2i−1 < E Pt h+1,2i then 5: Temp ← 0 6: else 7: Temp ∼ B(0.5) 8: end if 9: NPth,i ← NPth+1,2i−Temp\n10: ΩPt ← ΩPt ∪NPth,i 11: end while 12: ΓPt ← NPth,i ∪ ΓPt\nAlgorithm 3 Bound Updating (A sub-procedure used in RHT)\n1: for all NPth,i ∈ ΩPt do 2: TPth,i ← TPth,i + 1 3: µ̂Pth,i ← [ (TPth,i − 1)µ̂Pth,i + rt ] /T Pt h,i 4: BPth,i ← µ̂Pth,i+ √ k2 lnT/T Pt h,i+k1(m Pt)h+LX( √ dX nT )α 5: end for 6: EPth+1,2i−1 ← ∞ 7: EPth+1,2i ← ∞\nthey come from the learners’ payoffs µPth,i = (TPt−1)µPt h,i +r Pt t TPt . The second one √\nk2 lnT/T Pt h,i indicates the deviation in\nexploration-exploitation problem. It’s the tradeoff between the exploration which ensures the scope and exploitation which guarantees the depth. And the third term is the deviation in the course nodes. As for the last term, since we substitute the subhyercube center point for the previous context, we utilizes the Lipschitz constant LX and the diagonal of the sub-hypercube√\ndX nT to denote the deviation LX( √ dX nT )α in context."
    }, {
      "heading" : "B. Regret Analyze",
      "text" : "According to the defintion of regret, all the nodes which have been selected bring regret. We consider the regret in one sub-hypercube Pt and get the sum of it at last. Since the regret is the deviation from the optimal courses, we need to define the optimal course at first.\nDefinition 2. To locate the node with an affirmatory way we define the optimal track as ℓPt∗h,i = h∪\nh′=1 NPt h′,i′ h′ .\nThe track is the aggregate of the optimal nodes whose depth from 0 to h. To represent the regret precisely, we need to define minimum suboptimality gap which indicates the difference between the optimal course in that node and the overall optimal course to better describe the model.\nDefinition 3. The Minimum Suboptimality Gap and the context gap are\nDPth,i = f(r Pt∗)− f(rPt∗h,i ), LX( √ dX nT )α. (3)\nTheoretically speaking, the minimum suboptimality gap of NPth,i is the expected reward defference between overall optimal course and the best one in NPth,i, and the context gap is the difference between the original point and center point in context sub-hypercube Pt. However what we can get is mean payoff rather the expectation value, so we will bound this loss in the following lemma. As for the context regret, we take the largest difference that it can bring about in sub-hypercube.\nAfter the definitions, we can find a measurement to divide all the nodes into two kinds for our following proof. Based on the definition 3, we let the set φPt to be the 2[k1(mPt)h+\nLX( √ dX nT )α]-optimal nodes in the depth of h\nφPt =\n{\nNPth,i ∣ ∣f(rPt∗)−f(rPt∗h,i )≤2 [ k1(m Pt)h+LX( √ dX nT )α ]\n}\n.\nNote that we call the nodes in set φPt as optimal nodes and those out of it as suboptiaml nodes.\nWe define the regret when one node is selected above, however we should determine how many times the node has been chosen in the recommending process. We start it with suboptimal nodes. Based on definition 2 and definition 3, the suboptimal nodes are divorced from ℓPt∗h,i in some depth.\nLemma 1. Nodes NPth,i are suboptimal, and in the depth j (1 ≤ j ≤ h − 1) the path is out of the best track. For a random integer q, we can get the expect times of the node NPth,i and it’s descendants in Pt are\nE[TPth,i(t ′)]≤q+\nt′ ∑\nn=q+1 P\n{\n[\nBPth,i(n)>f(r Pt∗) and TPth,i(n)>q\n]\nor [\nBPtj,ih′ (n)≤f(r Pt∗) for j∈{q+1, ..., n−1}\n]\n}\n.\nProof: See Appendix A. We determine the threshold of the selected times of the node NPth,i and it’s descendants by lemma 1. Since we don’t know in time T how many times this context sub-hypercube Pt has been selected, we use context time t′ to represent the total times in Pt. The sum of t′ is the total time ∑\nPt\nt′ = T .\nHowever, from lemma 1 we can’t get the expectation of node chosen times directly, thus we lead to lemma 2 to modify the loss between mean value and expectation.\nLemma 2. Based on the Assumption 2, the inequation\nP\n{\nBPth,i(t ′) ≤ f(rPt∗)\n}\n≤ (t′)−2k2+1 (4) holds for all optimal nodes NPth,i in any time context time t ′.\nProof: See Appendix B. Lemma 2 determines the threshold of optimal nodes, which means the BPth,i will more than the expectation of reward finally. This guarantees the accuracy of our BPth,i with strict mathematical proofs. With the connection of lemma 1 and lemma 2, we can get the precise results about the nodes chosen times in lemma 3.\nLemma 3. For the suboptimal nodes NPth,i in φ Pt , if q satisfies\nq ≥ 4k2 lnT[ D\nPt h,i −k1(mPt )h−LX( √ dX nT ) α ]2 , (5)\nThen for all t′ ≥ 1, we have E[TPth,i(t\n′)] ≤ 4k2 lnT[ k1(mPt ) h+LX( √ dX nT ) α ]2 +M, (6)\nwhere the M is a constant less than 5.\nProof: See Appendix C. We use the deviation of context and course to represent played times in this lemma. Practically speaking, we find a upper bound for the course nodes chosen times, which means we can determine one nodes’ regret during the process. But this is not sufficient to bound the whole regret, what we have to know last is how many nodes are there. We divide the nodes into two parts based on the course model as ΓPt =\nφPt ∪ (φPt)c. For the convenience, we use the sets of depth to illustrate the node sets\nΓPt =\n[\n∑\nh\nφPth\n] ∪ [ ∑\nh\n(φPth ) c ] . (7)\nDefinition 4. We lead to the concept of packing number. Assuming the size of course space is 1, in the depth of h the node size threshold is k1(mPt)h, So the packing number is\nκPth\n( ∪NPth,i, Ra ) = K [Ra]dC , (8)\nwhere K is the adjustment constant and Ra is the packing balls’ radius.\nLemma 4. In the same context sub-hypercube, the number of the 2 [ k1(m Pt)h + LX( √ dX nT )α ]\n-optimal nodes can be bounded as\n∣ ∣ ∣ φPth ∣ ∣ ∣ ≤ K [ k1(m Pt)h + LX( √ dX nT )α ]−dC . (9)\nProof: Since the course number is infinite that we can’t know the data exactly, the dimension of course can’t be determined. There exists a constant d′, ∣\n∣ ∣φPth\n∣ ∣ ∣ ≤ κPth ( ∪{NPth,i ∈ φPth }, k1(mPt)h + LX( √ dX nT )α )\n≤ K ( k1(m Pt)h + LX( √ dX nT )α )−d′ . (10)\nWe take the minimal d′ as the dimension of course dC . Since the MOOC big data has various data structures, knowing the exact details of courses is impossible. Thus we can’t determine the accurate dimension dC . After we know the regret in selecting one node, the times when a node has been selected and the nubmer of chosen nodes, we can bound the whole regret in theory 1.\nTheorem 1. From the lemma above, we can get the regret of RHT\nE[R(T )] = O\n(\nT dX+dC+1 dX+dC+2 (lnT ) 1 dX+dC+2\n)\n. (11)\nProof: As for MOOC, the three terms represent three process of recommending a course. E[R1(T )] is brought from the optimal course nodes during the recommendation. E[R2(T )] represents the regret in suboptimal course nodes at first and E[R3(T )] is brought from the follow-up procedures. Thus E[R3(T )] is much larger than E[R2(T )], which means E[R2(T )] doesn’t matter for the whole regret.\nFor the node diversion φPth and (φ Pt h ) c, we divide the regret into three parts\nE[R(T )] = E[R1(T )] + E[R2(T )] + E[R3(T )].\nwhere ΓPt1 contains the descendants of φ Pt H , Γ Pt 2 contains the nodes φPth the depth from 1 to H and Γ Pt 3 contains nodes in (φPth ) c. Note that ΓPt3 is the child of nodes in φ Pt H since the suboptimal nodes won’t be played deeper and we use the nodes to represent itself and descendants. The depth H is a constant to be determined later. Note that T = ∑\nt′, when all the times T are in the same context sub-hypercube, the regret is smallest. And we consider the situation that all the times T are distributed uniformly. In this extreme situation, all the context sub-hypercube has the same times t′.\nFor E[R1(T )], the regret is generated from the optimal course clusters whose courses have been recommended. Since\nthe number of nodes can’t be bounded precisely, and the number of all the nodes played is t′, thus we suppose the nodes have been played with maximum times t′.\nE[R1(T )] ≤ 4 [ k1(m Pt)H + LX( √ dX nT )α ] T . (12)\nAs for the second term whose depth is from 1 to H , these nodes represent the regrets generated by primary nodes. For the MOOC, it’s the regret in the worse course clusters which have been selected at the beginning of the recommendation.\nE[R2(T )] ≤ ∑\nPt\nH ∑\nh=1\n4 [ k1(m Pt) h + LX( √ dX nT ) α] ∣ ∣ ∣ φPth ∣ ∣ ∣\n≤ 4K(nT ) dX\n[k1(mPt )h] dC\nH ∑\nh=0\n4 [ k1(m Pt) h + LX( √ dX nT ) α] .\nFrom Lemma 4 we can know the number of suboptimal\nnodes in depth h are ∣ ∣\n∣φPth\n∣ ∣ ∣≤K [ k1(m Pt)h+LX( √ dX nT )α ]−dC ,\nand the number of the context sub-hypercubes is (nT )dX . Thus the last inequation can be derived. There we take the Pt whose regret deviation is most as the represented context sub-hypercube and H in Pt, thus the sum of regret can be presented in that way.\nWhen it comes to last term, it’s the regret in the suboptimal course nodes in the deeper recommendation process. And the regret bound is\nE[R3(T )]≤ ∑\nPt\nH ∑\nh=1\n4 [ k1(m Pt) h +LX( √ dX nT ) α] ∑\nN Pt h,i ∈(φPt h ) c\n∣ ∣ ∣(φPth ) c ∣ ∣ ∣.\nWe note that the nodes in ΓPt3 is the child node of Γ Pt 2 , since all the nodes in ΓPt3 is the root nodes of the suboptimal nodes Thus the number of ΓPt3 is less than twice of Γ Pt 2 . Then we can get ∑\nPt\nH ∑\nh=1\n4 [ k1(m Pt) h + LX( √ dX nT ) α ]\n∑\nN Pt h,i ∈(φPt h ) c\n∣ ∣ ∣(φPth ) c ∣ ∣ ∣\n≤ ∑ h\n8K(nT ) dX\n[k1(mPt)h] dC\n\n\n\n4k2 lnT [\nk1(mPt ) h+LX (\n√ dX nT ) α ]2 +M\n\n\n\n.\nAnd the nodes are 2 [ k1(m Pt)h + LX( √ dX nT )α ] suboptimal, we can get DPth,i − k1(mPt)h − LX( √ dX nT\n)α ≥ k1(mPt)h + LX( √ dX nT\n)α. Note that the second term is infinitesimal of higher order of the third one mathematically, thus we focus more on the first term and the last term since the decisive factors of regret is the first one and last one. We notice that with the depth increasing, E[R1(T )] decreases but E[R3(T )] increases. When we let their complexity to be equal, we can get the minimum.\nAs for a context sub-hypercube Pt, all the nodes have been played bring two kinds of regret: the context regret LX( √ dX nT )α and course node regret k1(mPt)H . The complexity of first term is\nO { 4 [ k1(m Pt)H + LX( √ dX nT )α ] T } . (13)\nAs for the last term, the complexity is\nO\n\n\n∑\nh\n8K(nT ) dX\n[k1(mPt )h] dC\n\n 4k2 lnT [\nk1(mPt ) h+LX(\n√ dX nT ) α ]2 +M\n\n\n\n\n= O\n(\nlnT (nT ) dX+1\n[k1(mPt )H ] dC+1\n)\n.\n(14)\nWhen the two terms is equal, we can get K lnT (nT ) dX+1\n[k1(mPt )H ] dC+1\n= [ k1(m Pt)H + LX( √ dX nT )α ] T. (15)\nSince the context deviation and the course deviation is unknow, to minimize the deviation we assume the two deviation is equal too, which means k1(mPt) H = LX( √ dX nT ) α\n. To simplify the problem, we take α = 1 and k1 = 2, thus\nwe can get nT = ( T lnT\n) 1\ndX+dC+2 and the regret complexity is\nO\n(\nT dX+dC+1 dX+dC+2 (ln T ) 1 dX+dC+2\n)\n.\nFor the conclusion we can make sure the average regret will finally converge to zero, which means the algorithm can find the optimal courses for the learners at last. Note that the tree exists actually, we store the tree in the cloud and during the recommending process. Since the dataset is fairly large in the future, using the distributed storage technology to solve storage problems is inescapable."
    }, {
      "heading" : "V. DISTRIBUTED STORED COURSE RECOMMENDATION TREE",
      "text" : ""
    }, {
      "heading" : "A. Distributed Algorithm for Multiple Course Storages",
      "text" : "In this section, we turn to present a new algorithm called Distributed Storage Repartimiento Hierarchical Trees (DSRHT), which we can improve the storage pressure by using distributed units to store the course data in clouds. The practical situation may be very complex, therefore we bound the number of distributed nd units with our model of tree 2z−1 ≤ nd ≤ 2z, where z is the depth of the tree and 2z is the number of nodes in that depth. In the next illustration we substitute the number nd with 2z.\nIn algorithm 4 we still find the context sub-hypercube at first. Then since there are 2z distributed units, we first ascertain these top nodes. Based on the attained information the algorithm can start to find the course by utilizing the Bound and Estimation.\nDetermining the number of distributed units is needed, we assume there are 2z storage nodes, and the number satisfies\n2z ≤ ( T lnT )\ndX+dC−1\ndX+dC+2 , mPt ∈ [ 12 , 1). (16) Since the number of the distributed units is limit, and the time is very large thus the assumption is resonable. As for mPt , since we take the binary tree model, everytime the course cluster is divided to two clusters, which means that the m is larger than 0.5 is quite reasonable.\nThe algorithm selects the initial nodes at depth z at first, and then process data similarly to RHT. When it comes to MOOC, the algorithm will start at the distributed nodes such as the website or other platforms. For the tree partiotion, the difference is that we leave the course nodes whose depth is less than out z to cut down the storage cost. In the complexity section we will prove that the storage can be bounded linearly in the best situation."
    }, {
      "heading" : "B. Regret Analyze",
      "text" : "In this subsection we prove the regret result in DSRHT can be bounded sublinearly. Since we use the model to determine the threshold of the number of distributed units, we can\nAlgorithm 4 Distributed Course Rcommendation Tree Require : The constants k1 > 0, m ∈ (0, 1), the number of the storage unit 2z and the learner’s context xt. Auxiliary function : Exploration and Bound Updating Initialization : For all context sub-hypercubes belonging to PT ΓPt = {NPtz,1, NPtz,2...NPtz,2z} EPtz,i = ∞ for i = 1, 2...2z\n1: for t =1,2,...T do 2: for dt = 0, 1, 2...dX do 3: Find the context interval in dt dimension 4: end for 5: Get the context sub-hypercube Pt 6: xt ← center point of Pt 7: for j=1,2...2z − 1 do 8: if NPtz,j < N Pt z,j+1 then 9: NPtz,j = N Pt z,j+1\n10: end if 11: end for 12: NPth,i ← NPtz,j 13: ΩPt ← NPth,i 14: Exploration (ΓPt) 15: Select a course from the node NPth,i randomly and recommend to the learner 16: Get the reward rt 17: for all Pt ∈ PT do 18: Bound Updating (ΩPt ) 19: ΩPttemp ← ΩPt 20: for ΩPttemp 6= ∅ do 21: NPth,i ← one leaf of ΩPt 22: EPth,i ← min { BPth,i,max{EPth+1,2i−1, EPth+1,2i} } 23: Delete the NPth,i from Ω Pt temp 24: end for 25: end for 26: end for\ncontinue to use the lemma in RHT algorithm. To be more specific, we transform the distributed problem into the RHT starting from depth z rather than the root node.\nNow we need to redivide the nodes contrast to the RHT algorithm ΓPt = ΓPt1 +Γ Pt 2 +Γ Pt 3 +Γ Pt 4 , where the first third partitions are the same and the last one is the node at depth z which won’t be played based on the algorithm 1. For the node diversion φPth and (φ Pt h )\nc. The depth H (z ≤ H) is a constant to be selected later.\nTheorem 2. The regret of the distributed stored algorithm is\nE[R(T )] = O\n(\nT dX+dC+1 dX+dC+2 (lnT ) 1 dX+dC+1\n)\n, (17)\nProof: See Appendix D. Compared to the RHT algorithm, we notice that the regret bound is slightly increases, and its beginning performance is not as well as RHT. However, the algorithm can handle the practical problem better. Considering the explosive data in future, this alogrithm will perform better than RHT."
    }, {
      "heading" : "VI. STORAGE COMPLEXITY",
      "text" : "The storage problem has been existing in big data for a long time, so how to use the distributed storage technology to handle the problem matters a lot. In this section, we anlalyze the two algorithms’ space complexity mathematically. We use S(T ) to represent the storage space complexity. For RHT algorithm, since the storage is from the root node, and it’s fairly simple to know the space complexity is linear O(E[S(T )]) = O(T ).\nTheorem 3. In the optimal condition, we take the number of\nstorage units satisfied 2z = ( T lnT )\ndX+dC−1\ndX+dC+2 , then we can get the storage space complexity\nE[S(T )]=O\n(\nT dX+dC−1 dX+dC+2\n(\nT 3 dX+dC+2 −(lnT ) dX+dC+2 dX+dC−1\n))\n.\nProof: Every round t has to explore a new leaf node. To get the optimal result, we suppose the depth is as deepest as\nwe can choose z =\n⌊\ndX+dC−1 dX+dC+2 ln( TlnT )\nln 2\n⌋\n. Under the condition\nthat t < 2z+1, we have S1(T ) ≤ 2z = ( T lnT )\ndX+dC−1\ndX+dC+2 , when the time t ≥ 2z+1, after one round there is one unplayed node being selected, so the second part is S2(T ) ≤ T − 2z+1 = T−2 (\nT lnT\n)\ndX+dC−1 dX+dC+2 . Thus we can get the storage complexity\nE[S(T )] = O\n(\nT − ( T lnT )\ndX+dC−1 dX+dC+2\n)\n. (18)\nSince the value of z is changeable, appropriate value can make the space complexity sublinear. From (18), if the data dimension is fairly large, the space complexity will be relative small. However, the large dataset and tremendous distributed units will make the algorithm learning too slow and make regret very large. Thus taking a appropriate parameter is crucial. There are many platforms about MOOC, and gathering all the data together is fairly difficult and high-cost. Thus the DSRHT can handle the practical situation better and improve the storage condition in the same time.\nBesides, we compare our algorithms with some similar works which are all tree partition. In the table 1 we list the theoretical analyze results. Since the ACR [29] has a ergodic process, the time complexity and regret bound is relative high compared with others. As for HCT [31], having no context information makes the algorithm has a very low space complexity and regret bound compared with our algorithms."
    }, {
      "heading" : "VII. NUMERICAL RESULTS",
      "text" : "In this section, we present: (1) the source of dataset; (2) the sum of regrets are sublinear and the average regret converges to 0 finally; (3) we compare the regret bounds of our algorithms with other similar works; (4) distributed storage method can reduce the space complexity. Fig. 5 illustrates the MOOC operation pattern in edX [27]. The right side is the teaching window and learning resources, and the left includes lessons content, homepage, forums and other function options."
    }, {
      "heading" : "A. Description of the Dataset",
      "text" : "We take the dataset which contains feedback information and course details from the edX [27] and the intermedi-\nTABLE I: Theoretical Comparison\nAlgorithm Context Infinity Time Complexity Space Complexity Regret\nACR [29] Yes No O ( T 2 +KET )\nO\n(\nE ∑\nl=0\nKl + T\n)\nO\n(\nT\ndI+dC+1 dI+dC+2 lnT\n)\nHCT [31] No Yes O(T lnT ) O\n(\nT d d+2 (lnT ) 2 d+2\n)\nO\n(\nT d+1 d+2 (lnT ) 1 d+2\n)\nRHT Yes Yes O(T lnT ) O(T ) O\n(\nT\ndX+dC+1 dX+dC+2 (lnT ) 1 dX+dC+2\n)\nDSRHT Yes Yes O(T lnT ) O (T − 2z) O\n(\nT dX+dC+1 dX+dC+2 (lnT ) 1 dX+dC+1\n)\nary website of MOOC [6]. In those platforms, the context dimensions contain nationality, gender, age and the highest education level, therefore we take dX = 4. Besides, there is a questionnaire from Harvard University in edX to improve your context information, enlarging the dimension to around thirty dX = 30. As for the course dimensions, they comprise starting time, language, professional level, provided school, course and program proportion, whether it’s self-paced, subordinative subject etc. For the feedback system, we can acquire reward information from review plates and forums. Thoroughly, the reward is produced from two aspects, which are marking system and comments from forums.\nFor the users, when a novel field comes into vogue, tremendous people will get acess to this field in seconds. The data we get include 2× 105 learners using MOOC in those platforms, and the average number of courses the learners comment is around 30. As for our algorithm, it focuses on the group of learners in the same context sub-hypercube rather than individuals. Thus when next time users comes with context information and historical records, we just treat them as the new training data without distinguishing them. However the number of people is limited, even if generating a course is time-costing, the number of course is unlimited and education runs through the development of human being. Our algorithm pays more attention to the future highly inflated MOOC curriculum resources, and existing data bank is not tremendous enough to demonstrate the superiority of our algorithm since MOOC is a new field in education. We find 11352 courses from those platforms including plenty of finished courses. The number of courses doubles every year. And based on the trend, the quantity will be more than forty thousand times within 20 years. To give consideration to both accuracy and scale of sources of data, we copy the original sources to forty five thousand times to satisfy the number requirements. Thus we extend the 11352 course data acquired from edX to around 5 × 108 to simulate future explosive data size of courses in 2030."
    }, {
      "heading" : "B. Experimental Setup",
      "text" : "As for our algorithm, the final training number of data is over 6 × 106 and the number of courses is about 5 × 108 which means for the training number we can regard the course number as infinite. Three works are introduced as follow.\n• Adaptive Clustering Recommendation Algorithm (ACR) [29]: The algorithm injects contextual factors capable of adapting to more learners, however, when the course database is fairly large, ergodic process in this model can’t handle the dataset well. • High Confidence Tree algorithm (HCT) [31]: The algorithm supports unlimited dataset however large it is, but there is only one learner for the recommendation model since it doesn’t take context into consideration. • We consider both infinity and context, thus our model can better suit future MOOC situation. In DSRHT we sacrifice some immediate interests to get better long-term performance.\nTo verify the conclusions practically, we divide experiment into following three steps:\n1) Step 1.: In this step we compare our RHT algorithm with the two previous work which are ACR [29] and HCT [31] with different size of training data. We input over 6× 106 training data including context information and feedback records in the reward space mentioned in the section of database description into the three models, and then the models will start to recommend the courses stored in the cloud. In consideration of HCT not supporting context, we normalize all the context information to the same (center point of unit context hypercube). Since the reward distribution is stochastic, we simulate 10 times to get the average values where the interfere of random factor is restrained. Then the two regret tendency diagrams are plotted to evaluate algorithms performances.\n2) Step 2.: We use the DSRHT algorithm to simulate the results. The RHT algorithm can be seemed as degraded DSRHT with z = 0, and we compare the DSRHT algorithm with different parameters z. Without loss of generality, we take\nz = 0, z = 10 and z =\n⌊\ndX+dC−1 dX+dC+2 ln( TlnT )\nln 2\n⌋\n≈ 20. Then we plot the regret and z diagram to analyze the constant optimal parameter.\n3) Step 3.: We record the storage data to analyze the space complexity of those four algorithms. First we upload 517.68TB indexing information of courses to our school servers and perform the four algorithms successively. In the process of training, we record the regret for six times. And in the end of training, we record the space usage of the tree which represent the training cost. As for the DSRHT, we use the virtual partitions in school servers to simulate the distributed stored course data. Specifically, we reupload the course data to the school servers in 1024 virtual partiotions, and then perform the DSRHT algorithm."
    }, {
      "heading" : "C. Results and Analysis",
      "text" : "We analyze our algorithm from two different angles: Comparing with other two works and comparing with itself with different parameter z. In each direction, we compare the regret first, and analyze the average regret. And then we discuss the accuracies based on the average regret. At last we will compare the storage conditions from different algorithms.\nIn Fig. 6 and Fig. 7 we compare the RHT algorithm with ACR and HCT. From the Fig. 6 (Regret diagram), we can get that our method is better than the two others which has less regret from the beginning. The HCT algorithm performs better than ACR when it starts. With time going on, the ACR’s\nregret comes to be lower than HCT. From the Fig. 7 (Average Regret diagram), HCT’s average regret is less than ACR at first, result also showing that ACR performs slightly better than HCT finally.\nTable 2 records the average accuracies which is the total rewards divdided by the number of training data. “Num” is the number of training data. We find that when the time increases, all the three algorithm can get promoted. Our algorithm has the highest accuracies during the learning period. The ACR performs not good when the process starts, which is 65.43% and is worse than HCT 81.02%. Finally, ACR converges to 88.79% but HCT is still 83.98%. When it comes to our algorithm, it’s 91.87% which is much better than HCT.\nFig. 8 and Fig. 9 analyze DSRHT algorithm by using different parameters z as 0, 10 and 20. From the diagrams we find that comparing z = 0 and z = 10, we can validate what we said before: Storage improvement focus more on the\nlong run performance. However, when z = 20, the algorithm has taken a lot of time to start recommend course precisely. Even if finally the accuracy catches the RHT algorithm at the end, the effect is not as well as we expect.\nTable 3 illustrates the problem more precisely. When the training number is less than 4×106, the condition that z = 20 is worst in the three conditions. After that, it come to catch the RHT 91.09% with 91.33%. Thus we can see selecting the ditributed storage number can’t pursuit the quantity only, whether it’s practical makes sense as well.\nAs for the storage analyze, we use the detailed information of courses to represent courses data, and the whole course storage is 517.68TB. To be more intuitionistic, we use the ratio of actual space occupied and course space occupied to denote storage ratio. From table 4 we know that ACR [29] algorithm is not suitable for real big data since the storage ratio reaches 24.287. HCT [31] algorithm performs well in space complexity which is better than RHT. As for DSRHT, the storage ratio is 4.118 which is less than HCT and nearly half of RHT."
    }, {
      "heading" : "VIII. CONCLUSION",
      "text" : "In this paper, we have presented RHT and DSRHT algorithms for the courses recommendation in MOOC big data. To better fit with the changeable dataset in the future we extend the number of objects to infinite. Paying attention to the individualization in recommender system, we inject the context-awareness into our algorithm. Futhermore, we use distributed storage to relieve the storing pressure and make it more suitable for big data. We have also test our algorithm and compare it with two similar algorithms. As for our ongoing work, we intend to reduce space complexity by the combined method of HCT [31] and distributed storage to improve storage condition further."
    }, {
      "heading" : "APPENDIX A PROOF OF LEMMA 1",
      "text" : "Proof: We assume that the path is out of the best in the depth of k, thus we can know the estimation in the depth k+1 have deviation of the actual value, which means EPtk+1,i(k+1)′ ≤ EPtk+1,i′ (the first one is the best path node and the second is\nthe node selected in the depth of k + 1). According to the definition of Bound and Estimation, we can know that with the depth decreasing, the Estimation of node decrases either. Thus we can know that EPtk+1,i(k+1)′ ≤ E Pt k+1,i′ ≤ EPth,i ≤ BPth,i.\nWe divide the set {\nBPth,i(t ′) ≥ EPtk+1,i(k+1)′ (t\n′) }\ninto {\nBPth,i(t ′) > f(rPt∗)\n} ∪ { f(rPt∗) ≥ EPtk+1,i(k+1)′ (t ′) }\n. According to the definition of Bound and Estimation again, we can get {\nf(rPt∗) ≥ EPtk+1,i(k+1)′ (t ′) }\n⊂ { f(rPt∗)≥BPtk+1,i(k+1)′(t ′) } ∪ { BPtk+1,i(k+1)′(t ′)≥EPtk+1,i(k+1)′(t ′) } ⊂ {\nf(rPt∗)≥BPtk+1,i(k+1)′ (t ′) } ∪ { f(rPt∗)≥EPtk+1,i(k+1)′ (t ′) }\n⊂ { f(rPt∗)≥BPtk+1,i(k+1)′ (t ′) } ∪ { f(rPt∗)≥EPtk+2,i(k+2)′ (t ′) } .\nBased on the conclusion above, we know {\nf(rPt∗)≥EPtk+1,i(k+1)′(t ′) } ⊂ { f(rPt∗)≥BPtk+1,i(k+1)′(t ′) }\nn−1∪ j=k+2\n{ f(rPt∗) ≥ EPtk+2,i(k+2)′(t ′) } . (19)\nWhen it comes to TPth,i(t ′), the pratical path including the\nnode NPth,i means that { BPth,i(t ′) ≥ EPtk+1,i(k+1)′ (t ′) } . So\nE\n[ TPth,i(t ′) ] = t′ ∑\nn=1 P\n{\nBPth,i(t ′) ≥ EPtk+1,i(k+1)′ (t ′), TPth,i(t ′) ≤ q\n}\n+ t′ ∑\nn=1 P\n{\nBPth,i(t ′) ≥ EPtk+1,i(k+1)′ (t ′), TPth,i(t ′) > q\n}\n≤ q+ t′ ∑\nn=q+1 P\n{[\nBPth,i(n)>f(r Pt∗) and TPth,i(n)>q\n]\nor [\nBPtj,ih′(n)≤f(r Pt∗) for j∈{q+1, ..., n−1}\n]}\n.\nIn the inequation, we let the first term to be 1 at each time so it grows to q. In the second term, since the TPth,i(n) > q, the terms when n ≤ q are zero and with the help of inequation (19) we can get the conclusion."
    }, {
      "heading" : "APPENDIX B PROOF OF LEMMA 2",
      "text" : "Proof: According to Assumption 3, we let c1, c2 which have been select from the same context be in a same optimal node and let c1 be the best course, thus we can get\nf(rPt∗)− f(rPtc2 ) ≤ diam(N Pt h,i) ≤ k1(mPt)h. (20)\nSince in the context sub-hypercube we change the context spcae with a point, we need to inject the context deviation\nf(rPt∗)− f(rPtc2 ) ≤ k1(mPt)h+LX( √ dX nT )α. (21)\nWe note the event when the track go through the node NPth,i as event NPth,i ∈ ℓPtH,I . Therefore, P {\nBPth,i(t ′) ≤ f(rPt∗) and TPth,i(t′) ≥ 1\n}\n= P\n{\nµ̂Pth,i(t ′) +\n√\nk2 lnT/T Pt h,i(t ′) + k1(m Pt)h\n+LX( √ dX nT )α ≤ f(rPt∗) and TPth,i(t′) ≥ 1 }\n= P\n{\n[\nµ̂Pth,i(t ′)+k1(m Pt)h+LX( √ dX nT )α−f(rPt∗) ] TPth,i(t ′)\n≤ − √\nk2(lnT )T Pt h,i(t ′) and TPth,i(t ′) ≥ 1\n}\n= P\n{\nt′ ∑\nn=1\n( f(r̄Ptc )− f(rPtcn ) ) I\n{ NPth,i ∈ ℓPtH,I }\n+ t′ ∑\nn=1\n[\nf(rPtcn )+k1(m Pt) h +LX( √ dX nT ) α −f(rPt∗) ] I { NPth,i∈ℓPtH,I }\n≤ − √\nk2(lnT )T Pt h,i(t ′) and TPth,i(t ′) ≥ 1\n}\n≤ P {\nt′ ∑\nn=1 (f(r̄Ptc )− f(rPtcn ))I{N Pt h,i ∈ ℓPtH,I}\n≤ − √\nk2(lnT )T Pt h,i(t ′) and TPth,i(t ′) ≥ 1\n}\n.\nThe last inequation is based on the expression (21), since the second term is positive and we drop it to get the last expression.\nFor the convenience of illustration, we pick the n when I {\nNPth,i ∈ ℓPtH,I } is equal to 1. We use ⌣r Pt c to indicate the r Pt c\nhappened in I { NPth,i ∈ ℓPtH,I } . Thus,\nP\n{\nt′ ∑\nn=1\n( f(r̄Ptc )− f(rPtcn ) ) I\n{ NPth,i ∈ ℓPtH,I }\n≤ − √\nk2(lnT )T Pt h,i(t ′) and TPth,i(t ′) ≥ 1\n}\n≤ P { t′ ∑\nn=1\n( f(r̄Ptc )− f(rPtcn ) ) I\n{ NPth,i ∈ ℓPtH,I }\n≤ − √\nk2(lnT )T Pt h,i(t ′) and TPth,i(t ′) ≥ 1\n}\n= P\n{ T Pt h,i\n(t′) ∑\nn=1\n(\n⌣ r Pt c − ⌣ r Pt\ncn\n)\n≤ − √\nk2(lnT )T Pt h,i(t ′) and TPth,i(t ′) ≥ 1\n}\n≤ t′ ∑\nn=1 P\n{\nn ∑\nj=1\n(\nf( ⌣ r Pt c )− f( ⌣ r Pt cj ) ) ≤ − √ k2(lnT )n\n}\n.\nWe consider the situation when n = 1, 2...TPth,i(t ′) and the fact that TPth,i(t ′) ≤ t′. Besides, the last inequation use the union bound theory and loose the threshold t′ ∑\nn=1 P\n{\nn ∑\nj=1\n(\nf( ⌣ r Pt c )− f( ⌣ r Pt cj ) ) ≤ − √ k2(ln T )n\n}\n≤ t′ ∑\nn=1 exp(−2k2 lnT ) ≤ (t′)−2k2+1.\n(22)\nNote that the sum of time T represents the contextual sum of time since the number of courses in the context sub-hypercube is stochastic. And for the convenience, we use T as the sum of time. With the help of Hoeffding-Azuma inequality [26], we get the conclusion."
    }, {
      "heading" : "APPENDIX C PROOF OF LEMMA 3",
      "text" : "Proof: With the help of the assumption q ≥ 4k2 lnT\n[\nD Pt h,i −k1(mPt )h−LX( √ dX nT ) α ]2 , we can get\nD Pt h,i −k1(mPt ) h−LX(\n√ dX\nnT ) α\n2 ≥ √ k2lnT q . (23)\nThus,\nP\n{\nBPth,i(t ′) > f(rPt∗) and TPth,i(t\n′) ≥ q }\n= P {\nµ̂Pth,i(t ′)+\n√\nk2 lnT/T Pt h,i(t ′)+k1(m Pt)h+LX( √ dX nT )α\n> f(rPt∗h,i ) +D Pt h,i and T Pt h,i(t\n′) ≥ q }\n≤ P {\nµ̂Pth,i(t ′) +\n√\nk2lnT q + k1(m Pt)h + LX( √ dX nT )α\n> f(rPt∗h,i ) +D Pt h,i and T Pt h,i(t\n′) ≥ q }\n= P\n{\n[µ̂Pth,i(t ′)− f(rPt∗h,i )]> [\nD Pt h,i −k1(mPt ) h−LX(\n√ dX\nnT ) α\n2 ]\nand TPth,i(t ′) ≥ q\n}\n.\nWhen we multiply TPth,i(t ′) with both sides, we can get the\ninequations below.\nP\n{\n[µ̂Pth,i(t ′)− f(rPt∗h,i )] > [\nD Pt h,i −k1(mPt ) h−LX(\n√ dX\nnT ) α\n2 ]\nand TPth,i(t ′) ≥ q\n}\n= P { t′ ∑\nn=1 (f(r̄Ptn )− f(rPth,i))I{NPth,i ∈ ℓPtH,I}\n> [ D\nPt h,i −k1(mPt ) h−LX(\n√ dX\nnT ) α\n2 ]T Pt h,i(t ′) and TPth,i(t ′)≥q\n}\n.\nWith the conclusion of Lemma 2, we can get that\nP\n{ t′ ∑\nn=1\n( f(r̄Ptn )− f(rPtcn ) ) I\n{ NPth,i ∈ ℓPtH,I }\n> [ D\nPt h,i −k1(mPt ) h−LX(\n√ dX\nnT ) α\n2 ]T Pt h,i(t ′) and TPth,i(t ′) ≥ q\n}\n≤ (t′)−2k2+1. According to Lemma 1 and the prerequisite in Lemma 3, we\nselect upper bound of q as 4k2lnT[ D\nPt h,i −k1(mPt )h−LX( √ dX nT ) α ]2 + 1.\nThus,\nE\n[ TPth,i(t ′) ] ≤ t′ ∑\nn=q+1 P\n{\n[\nBPth,i(n) > f(r Pt∗) and TPth,i(n) > q\n]\nor [\nBPtj,ih′(n)≤f(r Pt∗) for j∈{q+1, ..., n−1}\n]\n}\n+ 4k2lnT[ D\nPt h,i −k1(mPt )h−LX( √ dX nT ) α ]2 + 1\n≤ 4k2lnT[ D\nPt h,i −k1(mPt )h−LX( √ dX nT ) α ]2 +1\n+ t′ ∑\nn=q+1\n[\n(t′) −2k2+1+n−2k2+2\n]\n.\nAnd we take the constant k2 ≥ 1,\n1 + t′ ∑\nn=q+1\n[\n(t′) −2k2+1+n−2k2+2\n]\n≤ 4 ≤ M, (24)\nthus we can get the conclusion Lemma 3."
    }, {
      "heading" : "APPENDIX D PROOF OF THEOREM 2",
      "text" : "Proof: Since we substitute the j with j′, Lemma are still valid for distributed stored algorithm.\nBased on the segmentation, the regret can be presented with E[R(T )] = E[R1(T )] + E[R2(T )] + E[R3(T )] + E[R4(T )]. For E[R1(T )], since it’s the same as the algorithm 1, so we can get the first term as\nE[R1(T )] ≤ 4 [ k1(m Pt)H + LX( √ dX nT )α ] T . (25)\nThe depth is from z to H , revealing that H > z. To satisfy\nthis, we suppose 2H ≥ ( T lnT )\ndX+dC−1 dX+dC+2 . Since the exploration\nprocess started from depth z, the depth we can select satisfy the inequation above. Thus the second term’s regret bound is\nE[R2(T )] ≤ ∑\nPt\nH ∑\nh=z\n4 [ k1(m Pt) h + LX( √ dX nT ) α] ∣ ∣ ∣φPth ∣ ∣ ∣\n≤ 4K(nT ) dX\n[k1(mPt )h] dC\nH ∑\nh=z\n4 [ k1(m Pt) h + LX( √ dX nT ) α] .\n(26)\nWe choose the context sub-hypercube whose regret bound is biggest to continue the inequation (26). And as for the third term, the regret bound is\nE[R3(T )]≤ ∑\nPt\nH ∑\nh=z\n4 [ k1(m Pt) h +LX( √ dX nT ) α] ∑\nN Pt h,i ∈(φPt h ) c\n∣ ∣ ∣ (φPth ) c ∣ ∣ ∣ .\nWe notice that since the nodes in ΓPt3 is the child node of ΓPt2 . To be more specific, in the binary tree, the child nodes is more than parent nodes but less than twice, thus the number of ΓPt3 is less than twice of Γ Pt 2 .\n∑\nPt\nH ∑\nh=z\n4 [ k1(m Pt) h + LX( √ dX nT ) α] ∑\nN Pt h,i ∈(φPt h ) c\n∣ ∣ ∣(φPth ) c ∣ ∣ ∣\n≤ 8K(nT ) dX\n[k1(mPt )h] dC\n\n\n\n4k2lnT [\nD Pt h,i −k1(mPt )h−LX( √ dX nT ) α ]2 +M\n\n\n\n≤ 8K(nT ) dX\n[k1(mPt )h] dC\n\n\n\n4k2lnT [\nk1(mPt) h+LX(\n√ dX nT ) α ]2 +M\n\n\n\n.\nSimilar to the RHT, the second term is infinitesimal of higher order of E[R3(T )]. When it comes to the fourth term, we notice that since the depth of z is bounded, and the worst situation happens when z is maximum.\nE[R4(T )] ≤ (2z − 1)\n\n\n\n4k2lnT [\nk1(mPt ) h+LX (\n√ dX nT ) α ]2 +M\n\n\n\n. (27)\nSince we know that 2z ≤ ( T lnT )\ndX+dC−1 dX+dC+2 , we have\nE[R4(T )] = O\n(\n(\nT lnT\n)\ndX+dC−1 dX+dC+2 lnT (nT ) 2\n)\n= O\n(\nT dX+dC+1 dX+dC+2 (lnT ) 1 dX+dC+1\n) . (28)\nFrom theorem 1, we minimize the deviation by making two deviation equal too, which means k1(mPt) H = LX( √ dX nT ) α . For the simplicity we let k1 = 2 and α = 1, thus the slicing number nT = ( T lnT ) 1\ndX+dC+2 . Now we need to prove that there exists H that can satisfy\n2H ≥ ( T\nlnT\n)\ndX+dC−1 dX+dC+2\n, k1(m Pt)H = LX( √ dX nT ). Consid-\nering the complexity and ignoring constant, we can get that when mPt ∈ [ 12 , 1) the H exists.\nBased on the attained infromation above we can get the\nregret complexity is O\n(\nT dX+dC+1 dX+dC+2 (lnT ) 1 dX+dC+1\n)\n."
    } ],
    "references" : [ {
      "title" : "The Year of the MOOC",
      "author" : [ "L. Pappano" ],
      "venue" : "The New York Times, 2014.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Universities Abroad Join Partnerships on the Web",
      "author" : [ "T. Lewin" ],
      "venue" : "New York Times, 2013.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "MOOCs make their move",
      "author" : [ "A. Brown" ],
      "venue" : "The Bent, vol. 104, no. 2, pp. 13-17, 2013.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Big data for development: a review of promises and challenges",
      "author" : [ "M. Hilbert" ],
      "venue" : "Development Policy Review, pp. 135-174, 2016.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Managing open educational resources on the web of data",
      "author" : [ "G. Paquette", "A. Miara" ],
      "venue" : "International Journal of Advanced Computer Science and Applications (IJACSA), vol. 5, no. 8, 2014.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Competencybased personalization for Massive Online Learning",
      "author" : [ "G. Paquette", "O. Marino", "D. Rogozan", "M. Lonard" ],
      "venue" : "Avaliable at: http;//r-libre.teluq.ca/491/1/SLE-Competency-based%20personalisationrevised%2013.08.2014.pdf.2014.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "MOOC performance prediction via clickstream data and social learning networks",
      "author" : [ "C.G. Brinton", "M. Chiang" ],
      "venue" : "IEEE Conference on Computer Communications (INFOCOM), pp. 2299-2307, 2015.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "X-armed bandits",
      "author" : [ "S. Bubeck", "R. Munos", "G. Stoltz" ],
      "venue" : "Journal of Machine Learning Research pp. 1655-1695, 2011.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Toward the next generation of recommender systems: a survey of the state-of-the-art and possible extensions",
      "author" : [ "G. Adomavicius", "A. Tuzhilin" ],
      "venue" : "IEEE Transactions on Knowledge and Data Engineering, vol. 17, no. 6, pp. 734-749, 2005.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "A group recommender system for online course study",
      "author" : [ "D. Yanhui", "W. Dequan", "Z. Yongxin" ],
      "venue" : "International Conference on Information Technology in Medicine and Education, pp. 318-320, 2015.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Content-based recommendation over a customer network for ubiquitous shopping",
      "author" : [ "M.J. Pazzani", "D. Billsus" ],
      "venue" : "IEEE Transactions on Services Computing, vol. 2, no. 2, pp. 140-151, 2009.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Hybrid recommender systems: Survey and experiments",
      "author" : [ "R. Burke" ],
      "venue" : "User Modeling and User-adapted Interaction, vol. 16, no. 2, pp. 325- 341. 2007.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "An efficient hybrid music recommender system using an incrementally trainable probabilistic generative model",
      "author" : [ "K. Yoshii", "M. Goto", "K. Komatani", "T. Ogata", "H.G. Okuno" ],
      "venue" : "IEEE Transactions on Audio, Speech, Language Processing, vol. 16, no. 2, pp. 435-447, 2008.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Make adaptive learning of the MOOC: The CML model",
      "author" : [ "L. Yanhong", "Z. Bo", "G. Jianhou" ],
      "venue" : "International Conference on Computer Science and Education (ICCSE), pp. 1001-1004, 2015.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "A proposed framework for an adaptive learning of Massive Open Online Courses (MOOCs)",
      "author" : [ "A. Alzaghoul", "E. Tovar" ],
      "venue" : "International Conference on Remote Engineering and Virtual Instrumentation (REV), pp. 127-132, 2016.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "A model of adaptation in online learning environments (LMSs and MOOCs)",
      "author" : [ "C. Cherkaoui", "A. Qazdar", "A. Battou", "A. Mezouary", "A. Bakki", "D. Mamass", "A. Qazdar", "B. Er-Raha" ],
      "venue" : "International Conference on Intelligent Systems: Theories and Applications (SITA), pp. 1-6, 2015.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Online learning with prior knowledge",
      "author" : [ "E. Hazan", "N. Megiddo" ],
      "venue" : "Learning Theory. Berlin, Germany: Springer-Verlag, pp. 499C513, 2007.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Contextual bandits with similarity information",
      "author" : [ "A. Slivkins" ],
      "venue" : "J. Mach. Learn. Res., vol. 15, no. 1, pp. 2533C2568, Jan. 2014.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "The EpochCGreedy algorithm for contextual multi-armed bandits",
      "author" : [ "J. Langford T. Zhang" ],
      "venue" : "Proc. NIPS, pp. 1096C1103, 2007.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Contextual bandits with linear payoff functions",
      "author" : [ "W. Chu", "L. Li", "L. Reyzin", "R.E. Schapire" ],
      "venue" : "Proc. AISTATS, pp. 208C214, 2011.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Contextual multi-armed bandits",
      "author" : [ "T. Lu", "D. Pl", "M. Pl" ],
      "venue" : "Proc. AISTATS, pp. 485C492, 2010.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "X-armed bandits[J",
      "author" : [ "S Bubeck", "R Munos", "G Stoltz" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2011
    }, {
      "title" : "Distributed online big data classification using context information",
      "author" : [ "C. Tekin", "M. van der Schaar" ],
      "venue" : "IEEE Annual Allerton Conference: Communication, Control, and Computing, pp. 1435-1442, 2013.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Probability inequalities for sums of bounded random variables",
      "author" : [ "W. Hoeffding" ],
      "venue" : "Journal of the American Statistical Association, pp. 13-30, 1963.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 1963
    }, {
      "title" : "Barycentric lagrange interpolation",
      "author" : [ "J.P. Berrut", "L.N. Trefethen" ],
      "venue" : "pp. 501-517, 2004.",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Online learning in large-scale contextual recommender systems",
      "author" : [ "L. Song", "C. Tekin", "M. van der Schaar" ],
      "venue" : "vol. pp, no. 99, 2015.",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Contextual bandits with similarity information[J",
      "author" : [ "A. Slivkins" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2014
    }, {
      "title" : "Online Stochastic Optimization under Correlated Bandit Feedback",
      "author" : [ "M.G. Azar", "A. Lazaric", "E. Brunskill" ],
      "venue" : "pp. 1557-1565, 2014. 13",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "INTRODUCTION MOOC is a concept first proposed in 2008 and known to the world in 2012 [1] [2].",
      "startOffset" : 85,
      "endOffset" : 88
    }, {
      "referenceID" : 1,
      "context" : "INTRODUCTION MOOC is a concept first proposed in 2008 and known to the world in 2012 [1] [2].",
      "startOffset" : 89,
      "endOffset" : 92
    }, {
      "referenceID" : 2,
      "context" : "Course recommender system helps the learner to find the requisite course directly in the course ocean of numerous MOOC platforms such like Coursera, edX, Udacity and so on [4].",
      "startOffset" : 172,
      "endOffset" : 175
    }, {
      "referenceID" : 3,
      "context" : "One key challenge in future MOOC course recommendation are processing tremendous data that bears the feature of volume, variety, velocity, variability and veracity [5] of big data.",
      "startOffset" : 164,
      "endOffset" : 167
    }, {
      "referenceID" : 4,
      "context" : "Hence, appending context information to the model of handling the courses is ineluctable [7] [8].",
      "startOffset" : 89,
      "endOffset" : 92
    }, {
      "referenceID" : 5,
      "context" : "Hence, appending context information to the model of handling the courses is ineluctable [7] [8].",
      "startOffset" : 93,
      "endOffset" : 96
    }, {
      "referenceID" : 25,
      "context" : "Previous context-aware algorithms such as [29] only perform well with the known scale of recommendation datasets.",
      "startOffset" : 42,
      "endOffset" : 46
    }, {
      "referenceID" : 25,
      "context" : "Specifically, the algorithm in [29] would rank all courses in MOOC as leaf nodes, then it clusters some relevance courses together as course nodes to build their parent nodes based on the historical information and current users’ features.",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 25,
      "context" : "As for the MOOC big data, since the number of courses keeps increasing and becoming fairly tremendous, algorithms in [29] are prohibitive to be applied.",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 25,
      "context" : "Most previous works [29] [30] could only realize the linear space complexity, however it’s not promising for MOOC big data.",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 26,
      "context" : "Most previous works [29] [30] could only realize the linear space complexity, however it’s not promising for MOOC big data.",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 25,
      "context" : "On the other hand, we prove the space complexity can be bounded sublinearly under the optimal condition which is better than [29].",
      "startOffset" : 125,
      "endOffset" : 129
    }, {
      "referenceID" : 25,
      "context" : "In Section VII, we verify the algorithms by experiment results and compare with relevant previous algorithms [29] [31].",
      "startOffset" : 109,
      "endOffset" : 113
    }, {
      "referenceID" : 27,
      "context" : "In Section VII, we verify the algorithms by experiment results and compare with relevant previous algorithms [29] [31].",
      "startOffset" : 114,
      "endOffset" : 118
    }, {
      "referenceID" : 8,
      "context" : "As for MOOC, the two major tactics to actualize the algorithm are filtering-based approaches and online learning methods [11].",
      "startOffset" : 121,
      "endOffset" : 125
    }, {
      "referenceID" : 9,
      "context" : "Apropos of filtering-based approaches, there are some branches such like collaborative filtering [12] [9], content-based filtering [13] and hybrid approaches [14] [15].",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 6,
      "context" : "Apropos of filtering-based approaches, there are some branches such like collaborative filtering [12] [9], content-based filtering [13] and hybrid approaches [14] [15].",
      "startOffset" : 102,
      "endOffset" : 105
    }, {
      "referenceID" : 10,
      "context" : "Apropos of filtering-based approaches, there are some branches such like collaborative filtering [12] [9], content-based filtering [13] and hybrid approaches [14] [15].",
      "startOffset" : 131,
      "endOffset" : 135
    }, {
      "referenceID" : 11,
      "context" : "Apropos of filtering-based approaches, there are some branches such like collaborative filtering [12] [9], content-based filtering [13] and hybrid approaches [14] [15].",
      "startOffset" : 158,
      "endOffset" : 162
    }, {
      "referenceID" : 12,
      "context" : "Apropos of filtering-based approaches, there are some branches such like collaborative filtering [12] [9], content-based filtering [13] and hybrid approaches [14] [15].",
      "startOffset" : 163,
      "endOffset" : 167
    }, {
      "referenceID" : 9,
      "context" : "The collaborative filtering approach gathers the students’ learning records together and then classifies them into groups based on the characteristics provided, recommending a course from the group’s learning records to new students [12] [9].",
      "startOffset" : 233,
      "endOffset" : 237
    }, {
      "referenceID" : 6,
      "context" : "The collaborative filtering approach gathers the students’ learning records together and then classifies them into groups based on the characteristics provided, recommending a course from the group’s learning records to new students [12] [9].",
      "startOffset" : 238,
      "endOffset" : 241
    }, {
      "referenceID" : 10,
      "context" : "Content-based filtering recommends a course to the learner which is relevant to the learning records before [13].",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 13,
      "context" : "Most of the previous works of recommending a course utilize the adaptive learning [16] [17] [18].",
      "startOffset" : 82,
      "endOffset" : 86
    }, {
      "referenceID" : 14,
      "context" : "Most of the previous works of recommending a course utilize the adaptive learning [16] [17] [18].",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 15,
      "context" : "Most of the previous works of recommending a course utilize the adaptive learning [16] [17] [18].",
      "startOffset" : 92,
      "endOffset" : 96
    }, {
      "referenceID" : 13,
      "context" : "In [16], the CML model was presented.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 16,
      "context" : "The similar works are widely distributed in [19]–[24] as contextual bandit problems.",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 21,
      "context" : "The similar works are widely distributed in [19]–[24] as contextual bandit problems.",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 21,
      "context" : "Our work is motivated from [24] for big data support, however we consider the contextaware online learning for the first time with delicately devised context partition schemes for MOOC big data.",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 0,
      "context" : "With the normalization of context information, we suppose the context space is X = [0, 1]X which is a unit hypercube.",
      "startOffset" : 83,
      "endOffset" : 89
    }, {
      "referenceID" : 0,
      "context" : "We assume the reward rt ∈ [0, 1] is the indication of accuracy.",
      "startOffset" : 26,
      "endOffset" : 32
    }, {
      "referenceID" : 25,
      "context" : "Since the ACR [29] has a ergodic process, the time complexity and regret bound is relative high compared with others.",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 27,
      "context" : "As for HCT [31], having no context information makes the algorithm has a very low space complexity and regret bound compared with our algorithms.",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 25,
      "context" : "Algorithm Context Infinity Time Complexity Space Complexity Regret ACR [29] Yes No O (",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 27,
      "context" : "HCT [31] No Yes O(T lnT ) O (",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 25,
      "context" : "5: MOOC Learning Model • Adaptive Clustering Recommendation Algorithm (ACR) [29]: The algorithm injects contextual factors capable of adapting to more learners, however, when the course database is fairly large, ergodic process in this model can’t handle the dataset well.",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 27,
      "context" : "• High Confidence Tree algorithm (HCT) [31]: The algorithm supports unlimited dataset however large it is, but there is only one learner for the recommendation model since it doesn’t take context into consideration.",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 25,
      "context" : ": In this step we compare our RHT algorithm with the two previous work which are ACR [29] and HCT [31] with different size of training data.",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 27,
      "context" : ": In this step we compare our RHT algorithm with the two previous work which are ACR [29] and HCT [31] with different size of training data.",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 25,
      "context" : "Number ×10 Algorithm ACR [29] HCT [31] RHT 1 65.",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 27,
      "context" : "Number ×10 Algorithm ACR [29] HCT [31] RHT 1 65.",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 25,
      "context" : "TABLE IV: Average Storage Cost ACR [29] HCT [31] RHT DSRHT (z=10) Storage Cost (TB) 12573 2762 4123 2132 Storage Ratio 24.",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 27,
      "context" : "TABLE IV: Average Storage Cost ACR [29] HCT [31] RHT DSRHT (z=10) Storage Cost (TB) 12573 2762 4123 2132 Storage Ratio 24.",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 25,
      "context" : "From table 4 we know that ACR [29] algorithm is not suitable for real big data since the storage ratio reaches 24.",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 27,
      "context" : "HCT [31] algorithm performs well in space complexity which is better than RHT.",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 27,
      "context" : "As for our ongoing work, we intend to reduce space complexity by the combined method of HCT [31] and distributed storage to improve storage condition further.",
      "startOffset" : 92,
      "endOffset" : 96
    }, {
      "referenceID" : 23,
      "context" : "With the help of Hoeffding-Azuma inequality [26], we get the conclusion.",
      "startOffset" : 44,
      "endOffset" : 48
    } ],
    "year" : 2017,
    "abstractText" : "The Massive Open Online Course (MOOC) has expanded significantly in recent years. With the widespread of MOOC, the opportunity to study the fascinating courses for free has attracted numerous people of diverse educational backgrounds all over the world. In the big data era, a key research topic for MOOC is how to mine the needed courses in the massive course databases in cloud for each individual (course) learner accurately and rapidly as the number of courses is increasing fleetly. In this respect, the key challenge is how to realize personalized course recommendation as well as to reduce the computing and storage costs for the tremendous course data. In this paper, we propose a big data-supported, contextaware online learning-based course recommender system that could handle the dynamic and infinitely massive datasets, which recommends courses by using personalized context information and historical statistics. The context-awareness takes the personal preferences into consideration, making the recommendation suitable for people with different backgrounds. Besides, the algorithm achieves the sublinear regret performance, which means it can gradually recommend the mostly preferred and matched courses to learners. Unlike other existing algorithms, ours bounds the time complexity and space complexity linearly. In addition, our devised storage module is expanded to the distributed-connected clouds, which can handle massive course storage problems from heterogenous sources. Our experiment results verify the superiority of our algorithms when comparing with existing works in the big data setting.",
    "creator" : "LaTeX with hyperref package"
  }
}