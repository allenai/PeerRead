{
  "name" : "1704.04856.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Neural Architecture for Generating Natural Language Descriptions from Source Code Changes",
    "authors" : [ "Pablo Loyola", "Edison Marrese-Taylor", "Yutaka Matsuo" ],
    "emails" : [ "pablo@weblab.t.u-tokyo.ac.jp", "emarrese@weblab.t.u-tokyo.ac.jp", "matsuo@weblab.t.u-tokyo.ac.jp" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Source code, while conceived as a set of structured and sequential instructions, inherently reflects human intent: it encodes the way we command a machine to perform a task. In that sense, it is expected that it follows to some extent the same distributional regularities that a proper natural language manifests (Hindle et al., 2012). Moreover, the unambiguous nature of source code, comprised in plain and human-readable format, allows an indirect way of communication between developers, a phenomenon boosted in recent years given the current software development paradigm, where billions of lines code are written in a distributed and asynchronous way (Gousios et al., 2014).\nThe scale and complexity of software systems these days has naturally led to explore automated ways to support developers’ code comprehension (Letovsky, 1987) from a linguistic perspective. One of these attempts is automatic summarization, which aims to generate a compact representation\nof the source code in a portion of natural language (Haiduc et al., 2010).\nWhile existing code summarization methods are able to provide relevant insights about the purpose and functional features of the code, their scope is inherently static. In contrast, software development can be seen as a sequence of incremental changes, intended to either generate a new functionality or to repair an existing one. Source code changes are critical for understanding program evolution, which motivated us to explore if it is possible to extend the notion of summarization to encode code changes into natural language representations, i.e., develop a model able to explain a source code level modification. With this, we envision a tool for developers that is able to i) ease the comprehension of the dynamics of the system, which could be useful for debugging and repairing purposes and ii) automate the documentation of source code changes.\nTo this end, we rely on the concept of code commit, the standard contribution procedure implemented in modern subversion systems (Gousios et al., 2014), which provides both the actual change and a short explanatory paragraph. Our model consists of an encoder-decoder architecture which is trained on a set of triples conformed by the version of a system before and after the change, along with the comment. Given the high heterogeneity of the modalities involved, we rely on an attention mechanism to efficiently learn the parts of the sequences that are more expressive and have more explanatory power.\nWe performed an empirical study on twelve real world software systems, from which we obtained the commit activity to evaluate our model. Our experiments explored in-project and crossproject scenarios, and our results showed that the proposed model is able to generate semantically sound descriptions.\nar X\niv :1\n70 4.\n04 85\n6v 1\n[ cs\n.C L\n] 1\n7 A\npr 2\n01 7"
    }, {
      "heading" : "2 Related Work",
      "text" : "The use natural language processing to support software engineering tasks has increased consistently over the years, mainly in terms of source code search, traceability and program feature location (Panichella et al., 2013; Asuncion et al., 2010).\nThe emergence of unifying paradigms that explicitly relate programming and natural languages in distributional terms (Hindle et al., 2012) and the availability of large corpus mainly from open source software opened the door for the use of language modeling for several tasks (Raychev et al., 2015). Examples of this are approaches for learning program representations (Mou et al., 2016), bug localization (Huo et al.), API suggestion (Gu et al., 2016) and code completion (Raychev et al., 2014).\nSource code summarization has received special attention, ranging from the use of information retrieval techniques to the addition of physiological features such as eye tracking (Rodeghero et al., 2014). In recent years several representation learning approaches have been proposed, such as (Allamanis et al., 2016), where the authors employ a convolutional architecture embedded inside an attention mechanism to learn an efficient mapping between source code tokens and natural language keywords.\nMore recently, (Iyer et al., 2016) proposed a encoder-decoder model that learns to summarize from Stackoverflow data, which contains snippet of code along with descriptions. Both approaches share the use of attention mechanisms (Bahdanau et al., 2014) to overcome the natural disparity between the modalities when finding relevant token alignments. Although we also use an attention mechanism, we differ from them in the sense we are targeting the changes in the code rather than the description of a file.\nIn terms of specifically working on code change summarization, Cortés-Coy et al. (2014); LinaresVásquez et al. (2015) propose a method based on a set of rules that considers the type and impact of the changes, and (Buse and Weimer, 2010) combines summarization with symbolic execution. To the best of our knowledge, our approach represents the first attempt to generate natural language descriptions from code changes without the use of hand-crafted features, a desirable setting given the heterogeneity of the data involved."
    }, {
      "heading" : "3 Proposed Model",
      "text" : "Our model assumes the existence of T versions of a given project {v1, . . . , vT }. Given a pair of consecutive versions (vt−1, vt), we define the tuple (Ct, Nt), where Ct = ∆tt−1(v) represents a code snippet associated to changes over v in time t and Nt represents its corresponding natural language (NL) description. Let C be the set of all source code snippets and N be the set of all descriptions in NL. We consider a training corpus with T code snippets and summary pairs (Ct, Nt), 1 ≤ t ≤ T , Ct ∈ C , Nt ∈ N. Then, for a given code snippet Ck ∈ C, the goal of our model is to produce the most likely NL description N?.\nConcretely, similarly to (Iyer et al., 2016), we use an attention-augmented encoder-decoder architecture. The encoder can be seen as a lookup layer, which simply reads through the source input sequence and returns the embedded tokens. The decoder is a RNN that reads this representation and generates NL words one at a time based on its current hidden state and guided by a global attention model (Luong et al., 2015). We model the probability of a description as a product of the conditional next-word probabilities. More formally, for each NL token ni ∈ Nt we define,\nhi = f(ni−1E, hi−1) (1)\np(ni|n1, ..., ni−1) ∝W tanh(W1hi +W2ai) (2)\nwhere E is the embedding matrix for NL tokens, ∝ denotes a softmax operation, hi represents the hidden state and ai is the contribution from the attention model on the source code. W , W1 and W2 are trainable combination matrices. The decoder repeats the recurrence until a fixed number of words or a special END token is generated. The attention contribution ai is defined as ai = ∑k j=1 αi,j · cjF , where cj ∈ Ct is a source code token, F is the source code token embedding matrix and αi,j is:\nαi,j = exp (h>i cjF )∑\ncj∈Ct exp (h > i cjF )\n(3)\nWe use a dropout-regularized LSTM cell for the decoder (Zaremba et al., 2015) and also add dropout at the NL embeddings and at the output softmax layer, to prevent over-fitting. We added special START and END tokens to our training sequences and replaced all tokens and output words occurring less than 2 and 3 times, respectively,\nwith a special UNK token. We set the maximum code and NL length to be 100 tokens. For decoding, we approximate N? by performing a beam search on the space of all possible summaries using the model output, with a beam size of 10 and a maximum summary length of 20 words.\nTo evaluate the quality of our generated descriptions we use both METEOR (Lavie and Agarwal, 2007) and sentence level BLEU-4 (Papineni et al., 2002). Since the training objective does not directly optimize for these scores, we compute METEOR on our validation set after every epoch and save the intermediate model that gives the maximum score as the final model. For evaluation on our test set we used the BLEU-4 score."
    }, {
      "heading" : "4 Empirical Study",
      "text" : "Data and pre-processing: We captured historical data from twelve open source projects hosted on Github based on their popularity and maturity, selecting 3 projects for each of the following languages: python, java, javascript and c++. For each project, we downloaded diff files and metadata of the full commit history. Diff files encode per-line differences between two files or sets of files in a standard format, allowing us to recover source code changes in each commit at the line level. On the other hand, medatada allows us to recover information such as the author and message of each commit.\nThe extracted commit messages were processed using the Penn Treebank tokenizer (Marcus et al., 1993), which nicely deals with punctuation and other text marks. To obtain a source code representation of each commit, we parsed the diff files and used a lexer (Brandl, 2016) to tokenize their contents in a per-line fashion allowing us to maximize the amount of source code recovered from the diff files. Data and source code available1.\nExperimental Setup: Given the flat structure of the diff file, source code in contiguous lines might not necessarily correspond to originally neighboring code lines. Moreover, they might come from different files in the project. To deal with this issue, we first worked only with those commits that modify a single file in the project; we call this the atomicity assumption. By using only atomic commits we reduced our training data by an average of roughly 50%, but in exchange we made sure all the extracted code lines came from\n1http://github.com/epochx/commitgen\nthe same file. At the same time, we expect to maximize the likelihood of observing a direct relation between the commit message and the lines altered.\nWe then relaxed our atomicity assumption and experimented with the full commit history. Given our maximum sequence length constrain of 100 tokens, we only observed an average of 1,97% extra data on each project. Since source code lines may come from different files, we added a delimiting token NEW FILE when corresponding.\nWe were also interested in studying the performance of the model in a cross-project setting. Given the additional challenges that this involves, we designed a more controlled experiment. Starting from the atomic dataset, we selected commits that only add or only remove code lines, conforming a derived dataset that we call uni-action. We chose the python language to maximize the available data. See Table 1.\nResults and Discussion: We begin by training our model on the atomic dataset. As baseline we used MOSES (Koehn et al., 2007) which although is designed as a phrase-based machine translation system, was previously used by Iyer et al. (2016) to generate text from source code. Concretely, we treated the tokenized code snippet as the source language and the NL description as the target. We trained a 3-gram language model using KenLM (Heafield et al., 2013) and used mGiza to obtain alignments. For validation, we use minimum error rate training (Bertoldi et al., 2009; Och, 2003) in our validation set.\nAs Table 3 shows, our model trained on atomic data outperforms the baseline in all but one project with an average gain of 5 BLEU points. In particular, we observe bigger gains for java projects such as CoreNLP and guava. We hypothesize this is because program differences in Java tend to be longer than the rest. While this impacts on training time, at the same time it allows the model to\nwork with a larger vocabulary space. On the other hand, our model performs similarly to MOSES for the node and slightly worse for the youtube-dl. A detailed inspection of the NL messages for node showed that many of them exhibit a fixed pattern in their structure. We believe this rigidity restrains the generation capabilities of the decoder, making it more prone to memorization.\nTable 2 shows examples of generated descriptions for real changes and their references. Results suggest that our model is able to generate semantically sound descriptions for the changes. We can also visualize the summarizing power of the model, as seen in the Theano and bitcoin examples. We observe a tendency to choose more general terms over too specific ones meanwhile also avoiding irrelevant words such as numbers or names. Results also suggest the emergence of rephrasing capabilities, specifically in the second example from Theano. Finally, our generated descriptions are, in most cases, semantically well correlated to the reference descriptions. We also report not so successful results, such as case of youtube-dl, where we can see signs of memorization on the generated descriptions.\nRegarding the cross-project setting experiments on python, we obtained BLEU scores of 14.6 and 18.9 for only-adding and only-removing instances in the uni-action dataset, respectively. We also obtained validation accuracies up to 43.94%, suggesting feasibility in this more challenging scenario. Moreover, as the generated descriptions from the keras project in Table 2 show, the model is still able to generate semantically sound descriptions.\nDespite the small data increase, we also trained our model on full datasets as a way to confirm the generative power of our model. In particular, we wanted to test the model is able leverage on atomic data to also capture and compress multifile changes. As shown in Table 3, results in terms of BLEU and validation accuracy manifest reasonable consistency, despite the higher disparity be-\ntween source code and natural language on this dataset, which means the model was able to learn representations with more compressive power.\nSoft alignments derived from Figure 1, which shows examples of attention heatmaps, illustrate how the model effectively associates source code tokens with meaningful words."
    }, {
      "heading" : "5 Conclusion and Future work",
      "text" : "We proposed an encoder-decoder model for automatically generating natural descriptions from source code changes. We believe our current results suggest that the idea is feasible and, if improved, could represent a contribution for the understanding of software evolution from a linguistic perspective. As future work, we will consider improving the model by allowing feature learning from richer inputs, such as abstract syntax trees and also functional data, such as execution traces."
    } ],
    "references" : [ {
      "title" : "A convolutional attention network for extreme summarization of source code",
      "author" : [ "Miltiadis Allamanis", "Hao Peng", "Charles Sutton." ],
      "venue" : "arXiv preprint arXiv:1602.03001 .",
      "citeRegEx" : "Allamanis et al\\.,? 2016",
      "shortCiteRegEx" : "Allamanis et al\\.",
      "year" : 2016
    }, {
      "title" : "Software traceability with topic modeling",
      "author" : [ "Hazeline U. Asuncion", "Arthur U. Asuncion", "Richard N. Taylor." ],
      "venue" : "Proceedings of the 32Nd ACM/IEEE International Conference on Software Engineering - Volume 1. ACM,",
      "citeRegEx" : "Asuncion et al\\.,? 2010",
      "shortCiteRegEx" : "Asuncion et al\\.",
      "year" : 2010
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "arXiv preprint arXiv:1409.0473 .",
      "citeRegEx" : "Bahdanau et al\\.,? 2014",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2014
    }, {
      "title" : "Improved minimum error rate training in moses",
      "author" : [ "Nicola Bertoldi", "Haddow Barry", "Jean-Baptiste Fouet." ],
      "venue" : "The Prague Bulletin of Mathematical Linguistics pages 1–11.",
      "citeRegEx" : "Bertoldi et al\\.,? 2009",
      "shortCiteRegEx" : "Bertoldi et al\\.",
      "year" : 2009
    }, {
      "title" : "Pygments: Python syntax highlighter",
      "author" : [ "Georg Brandl." ],
      "venue" : "http://pygments.org.",
      "citeRegEx" : "Brandl.,? 2016",
      "shortCiteRegEx" : "Brandl.",
      "year" : 2016
    }, {
      "title" : "Automatically documenting program changes",
      "author" : [ "Raymond P.L. Buse", "Westley R. Weimer." ],
      "venue" : "Proceedings of the IEEE/ACM International Conference on Automated Software Engineering. ACM, New York, NY, USA, ASE ’10, pages 33–42.",
      "citeRegEx" : "Buse and Weimer.,? 2010",
      "shortCiteRegEx" : "Buse and Weimer.",
      "year" : 2010
    }, {
      "title" : "On automatically generating commit messages via summarization of source code changes",
      "author" : [ "Luis Fernando Cortés-Coy", "Mario Linares Vásquez", "Jairo Aponte", "Denys Poshyvanyk." ],
      "venue" : "SCAM. volume 14, pages 275–284.",
      "citeRegEx" : "Cortés.Coy et al\\.,? 2014",
      "shortCiteRegEx" : "Cortés.Coy et al\\.",
      "year" : 2014
    }, {
      "title" : "An exploratory study of the pullbased software development model",
      "author" : [ "Georgios Gousios", "Martin Pinzger", "Arie van Deursen." ],
      "venue" : "Proceedings of the 36th International Conference on Software Engineering. ACM, pages 345–355.",
      "citeRegEx" : "Gousios et al\\.,? 2014",
      "shortCiteRegEx" : "Gousios et al\\.",
      "year" : 2014
    }, {
      "title" : "Deep api learning",
      "author" : [ "Xiaodong Gu", "Hongyu Zhang", "Dongmei Zhang", "Sunghun Kim." ],
      "venue" : "Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering. ACM, New",
      "citeRegEx" : "Gu et al\\.,? 2016",
      "shortCiteRegEx" : "Gu et al\\.",
      "year" : 2016
    }, {
      "title" : "Supporting program comprehension with source code summarization",
      "author" : [ "Sonia Haiduc", "Jairo Aponte", "Andrian Marcus." ],
      "venue" : "Proceedings of the 32nd ACM/IEEE International Conference on Software Engineering-Volume 2. ACM, pages 223–226.",
      "citeRegEx" : "Haiduc et al\\.,? 2010",
      "shortCiteRegEx" : "Haiduc et al\\.",
      "year" : 2010
    }, {
      "title" : "Scalable modified Kneser-Ney language model estimation",
      "author" : [ "Kenneth Heafield", "Ivan Pouzyrevsky", "Jonathan H. Clark", "Philipp Koehn." ],
      "venue" : "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics. Sofia, Bulgaria,",
      "citeRegEx" : "Heafield et al\\.,? 2013",
      "shortCiteRegEx" : "Heafield et al\\.",
      "year" : 2013
    }, {
      "title" : "On the naturalness of software",
      "author" : [ "Abram Hindle", "Earl T Barr", "Zhendong Su", "Mark Gabel", "Premkumar Devanbu." ],
      "venue" : "2012 34th International Conference on Software Engineering (ICSE). IEEE, pages 837– 847.",
      "citeRegEx" : "Hindle et al\\.,? 2012",
      "shortCiteRegEx" : "Hindle et al\\.",
      "year" : 2012
    }, {
      "title" : "Summarizing source code using a neural attention model",
      "author" : [ "Srinivasan Iyer", "Ioannis Konstas", "Alvin Cheung", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume",
      "citeRegEx" : "Iyer et al\\.,? 2016",
      "shortCiteRegEx" : "Iyer et al\\.",
      "year" : 2016
    }, {
      "title" : "Meteor: An automatic metric for mt evaluation with high levels of correlation with human judgments",
      "author" : [ "Alon Lavie", "Abhaya Agarwal." ],
      "venue" : "Proceedings of the Second Workshop on Statistical Machine Translation. Associa-",
      "citeRegEx" : "Lavie and Agarwal.,? 2007",
      "shortCiteRegEx" : "Lavie and Agarwal.",
      "year" : 2007
    }, {
      "title" : "Cognitive processes in program comprehension",
      "author" : [ "Stanley Letovsky." ],
      "venue" : "Journal of Systems and software 7(4):325–339.",
      "citeRegEx" : "Letovsky.,? 1987",
      "shortCiteRegEx" : "Letovsky.",
      "year" : 1987
    }, {
      "title" : "Changescribe: A tool for automatically generating commit messages",
      "author" : [ "Mario Linares-Vásquez", "Luis Fernando Cortés-Coy", "Jairo Aponte", "Denys Poshyvanyk." ],
      "venue" : "Proceedings of the 37th International Conference on Software Engineering-",
      "citeRegEx" : "Linares.Vásquez et al\\.,? 2015",
      "shortCiteRegEx" : "Linares.Vásquez et al\\.",
      "year" : 2015
    }, {
      "title" : "Effective approaches to attention-based neural machine translation",
      "author" : [ "Thang Luong", "Hieu Pham", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Compu-",
      "citeRegEx" : "Luong et al\\.,? 2015",
      "shortCiteRegEx" : "Luong et al\\.",
      "year" : 2015
    }, {
      "title" : "Building a large annotated corpus of english: The penn treebank",
      "author" : [ "Mitchell P Marcus", "Mary Ann Marcinkiewicz", "Beatrice Santorini." ],
      "venue" : "Computational linguistics 19(2):313–330.",
      "citeRegEx" : "Marcus et al\\.,? 1993",
      "shortCiteRegEx" : "Marcus et al\\.",
      "year" : 1993
    }, {
      "title" : "Convolutional neural networks over tree structures for programming language processing",
      "author" : [ "Lili Mou", "Ge Li", "Lu Zhang", "Tao Wang", "Zhi Jin." ],
      "venue" : "Proc. AAAI. AAAI Press, pages 1287–1293.",
      "citeRegEx" : "Mou et al\\.,? 2016",
      "shortCiteRegEx" : "Mou et al\\.",
      "year" : 2016
    }, {
      "title" : "Minimum error rate training in statistical machine translation",
      "author" : [ "Franz Josef Och." ],
      "venue" : "Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, Sapporo, Japan, pages 160–",
      "citeRegEx" : "Och.,? 2003",
      "shortCiteRegEx" : "Och.",
      "year" : 2003
    }, {
      "title" : "How to effectively use topic models for software engineering tasks? an approach based on genetic algorithms",
      "author" : [ "Annibale Panichella", "Bogdan Dit", "Rocco Oliveto", "Massimiliano Di Penta", "Denys Poshyvanyk", "Andrea De Lucia." ],
      "venue" : "In",
      "citeRegEx" : "Panichella et al\\.,? 2013",
      "shortCiteRegEx" : "Panichella et al\\.",
      "year" : 2013
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu." ],
      "venue" : "Proceedings of 40th Annual Meeting of the Association for Computational Linguis-",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "Predicting program properties from big code",
      "author" : [ "Veselin Raychev", "Martin Vechev", "Andreas Krause." ],
      "venue" : "ACM SIGPLAN Notices. ACM, volume 50, pages 111–124.",
      "citeRegEx" : "Raychev et al\\.,? 2015",
      "shortCiteRegEx" : "Raychev et al\\.",
      "year" : 2015
    }, {
      "title" : "Code completion with statistical language models",
      "author" : [ "Veselin Raychev", "Martin Vechev", "Eran Yahav." ],
      "venue" : "ACM SIGPLAN Notices. ACM, volume 49, pages 419–428.",
      "citeRegEx" : "Raychev et al\\.,? 2014",
      "shortCiteRegEx" : "Raychev et al\\.",
      "year" : 2014
    }, {
      "title" : "Improving automated source code summarization via an eyetracking study of programmers",
      "author" : [ "Paige Rodeghero", "Collin McMillan", "Paul W McBurney", "Nigel Bosch", "Sidney D’Mello" ],
      "venue" : "In Proceedings of the 36th International Conference on Software En-",
      "citeRegEx" : "Rodeghero et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Rodeghero et al\\.",
      "year" : 2014
    }, {
      "title" : "Recurrent neural network regularization",
      "author" : [ "Wojciech Zaremba", "Ilya Sutskever", "Vinyals Oriol." ],
      "venue" : "Proceedings of the 3rd International Conference on Learning Representations.",
      "citeRegEx" : "Zaremba et al\\.,? 2015",
      "shortCiteRegEx" : "Zaremba et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 11,
      "context" : "In that sense, it is expected that it follows to some extent the same distributional regularities that a proper natural language manifests (Hindle et al., 2012).",
      "startOffset" : 139,
      "endOffset" : 160
    }, {
      "referenceID" : 7,
      "context" : "lions of lines code are written in a distributed and asynchronous way (Gousios et al., 2014).",
      "startOffset" : 70,
      "endOffset" : 92
    }, {
      "referenceID" : 14,
      "context" : "The scale and complexity of software systems these days has naturally led to explore automated ways to support developers’ code comprehension (Letovsky, 1987) from a linguistic perspective.",
      "startOffset" : 142,
      "endOffset" : 158
    }, {
      "referenceID" : 9,
      "context" : "One of these attempts is automatic summarization, which aims to generate a compact representation of the source code in a portion of natural language (Haiduc et al., 2010).",
      "startOffset" : 150,
      "endOffset" : 171
    }, {
      "referenceID" : 7,
      "context" : "To this end, we rely on the concept of code commit, the standard contribution procedure implemented in modern subversion systems (Gousios et al., 2014), which provides both the actual change and a short explanatory paragraph.",
      "startOffset" : 129,
      "endOffset" : 151
    }, {
      "referenceID" : 20,
      "context" : "cation (Panichella et al., 2013; Asuncion et al., 2010).",
      "startOffset" : 7,
      "endOffset" : 55
    }, {
      "referenceID" : 1,
      "context" : "cation (Panichella et al., 2013; Asuncion et al., 2010).",
      "startOffset" : 7,
      "endOffset" : 55
    }, {
      "referenceID" : 11,
      "context" : "plicitly relate programming and natural languages in distributional terms (Hindle et al., 2012) and the availability of large corpus mainly from open source software opened the door for the use of language modeling for several tasks (Raychev et al.",
      "startOffset" : 74,
      "endOffset" : 95
    }, {
      "referenceID" : 22,
      "context" : ", 2012) and the availability of large corpus mainly from open source software opened the door for the use of language modeling for several tasks (Raychev et al., 2015).",
      "startOffset" : 145,
      "endOffset" : 167
    }, {
      "referenceID" : 18,
      "context" : "ing program representations (Mou et al., 2016), bug localization (Huo et al.",
      "startOffset" : 28,
      "endOffset" : 46
    }, {
      "referenceID" : 8,
      "context" : "), API suggestion (Gu et al., 2016) and code completion (Raychev et al.",
      "startOffset" : 18,
      "endOffset" : 35
    }, {
      "referenceID" : 23,
      "context" : ", 2016) and code completion (Raychev et al., 2014).",
      "startOffset" : 28,
      "endOffset" : 50
    }, {
      "referenceID" : 0,
      "context" : "In recent years several representation learning approaches have been proposed, such as (Allamanis et al., 2016), where the authors employ a convolutional architecture embedded inside an attention mechanism to learn an efficient mapping",
      "startOffset" : 87,
      "endOffset" : 111
    }, {
      "referenceID" : 12,
      "context" : "More recently, (Iyer et al., 2016) proposed a encoder-decoder model that learns to summarize",
      "startOffset" : 15,
      "endOffset" : 34
    }, {
      "referenceID" : 2,
      "context" : "Both approaches share the use of attention mechanisms (Bahdanau et al., 2014) to overcome the natural disparity between the modalities when finding relevant token alignments.",
      "startOffset" : 54,
      "endOffset" : 77
    }, {
      "referenceID" : 5,
      "context" : "(2015) propose a method based on a set of rules that considers the type and impact of the changes, and (Buse and Weimer, 2010) combines summarization with symbolic execution.",
      "startOffset" : 103,
      "endOffset" : 126
    }, {
      "referenceID" : 5,
      "context" : "In terms of specifically working on code change summarization, Cortés-Coy et al. (2014); LinaresVásquez et al.",
      "startOffset" : 63,
      "endOffset" : 88
    }, {
      "referenceID" : 5,
      "context" : "In terms of specifically working on code change summarization, Cortés-Coy et al. (2014); LinaresVásquez et al. (2015) propose a method based on a set of rules that considers the type and impact of the changes, and (Buse and Weimer, 2010) combines summarization with symbolic execution.",
      "startOffset" : 63,
      "endOffset" : 118
    }, {
      "referenceID" : 12,
      "context" : "Concretely, similarly to (Iyer et al., 2016), we use an attention-augmented encoder-decoder architecture.",
      "startOffset" : 25,
      "endOffset" : 44
    }, {
      "referenceID" : 16,
      "context" : "The decoder is a RNN that reads this representation and generates NL words one at a time based on its current hidden state and guided by a global attention model (Luong et al., 2015).",
      "startOffset" : 162,
      "endOffset" : 182
    }, {
      "referenceID" : 25,
      "context" : "We use a dropout-regularized LSTM cell for the decoder (Zaremba et al., 2015) and also add dropout at the NL embeddings and at the output softmax layer, to prevent over-fitting.",
      "startOffset" : 55,
      "endOffset" : 77
    }, {
      "referenceID" : 21,
      "context" : "2007) and sentence level BLEU-4 (Papineni et al., 2002).",
      "startOffset" : 32,
      "endOffset" : 55
    }, {
      "referenceID" : 17,
      "context" : "using the Penn Treebank tokenizer (Marcus et al., 1993), which nicely deals with punctuation and other text marks.",
      "startOffset" : 34,
      "endOffset" : 55
    }, {
      "referenceID" : 4,
      "context" : "To obtain a source code representation of each commit, we parsed the diff files and used a lexer (Brandl, 2016) to tokenize their contents in a per-line fashion allowing us to maximize the amount of source code recovered from the diff files.",
      "startOffset" : 97,
      "endOffset" : 111
    }, {
      "referenceID" : 12,
      "context" : ", 2007) which although is designed as a phrase-based machine translation system, was previously used by Iyer et al. (2016) to generate text from source code.",
      "startOffset" : 104,
      "endOffset" : 123
    }, {
      "referenceID" : 10,
      "context" : "trained a 3-gram language model using KenLM (Heafield et al., 2013) and used mGiza to obtain alignments.",
      "startOffset" : 44,
      "endOffset" : 67
    }, {
      "referenceID" : 3,
      "context" : "For validation, we use minimum error rate training (Bertoldi et al., 2009; Och, 2003) in our validation set.",
      "startOffset" : 51,
      "endOffset" : 85
    }, {
      "referenceID" : 19,
      "context" : "For validation, we use minimum error rate training (Bertoldi et al., 2009; Och, 2003) in our validation set.",
      "startOffset" : 51,
      "endOffset" : 85
    } ],
    "year" : 2017,
    "abstractText" : "We propose a model to automatically describe changes introduced in the source code of a program using natural language. Our method receives as input a set of code commits, which contains both the modifications and message introduced by an user. These two modalities are used to train an encoder-decoder architecture. We evaluated our approach on twelve real world open source projects from four different programming languages. Quantitative and qualitative results showed that the proposed approach can generate feasible and semantically sound descriptions not only in standard in-project settings, but also in a cross-project setting.",
    "creator" : "LaTeX with hyperref package"
  }
}