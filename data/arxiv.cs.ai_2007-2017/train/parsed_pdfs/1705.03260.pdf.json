{
  "name" : "1705.03260.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Evidence for the size principle in semantic and perceptual domains",
    "authors" : [ "Joshua C. Peterson" ],
    "emails" : [ "(peterson.c.joshua@gmail.com)", "griffiths@berkeley.edu)" ],
    "sections" : [ {
      "heading" : "Introduction",
      "text" : "In the seminal work of Shepard (1987), the notion of stimulus similarity was made concrete through its interpretation as stimulus generalization. It was shown that, across species (including humans), generalization probabilities follow an exponential law with respect to an internal psychological space. Specifically, the probability that y is in some set C that contains x (what Shepard terms a “consequential subset”) is an exponentially decreasing function of distance (d) in psychological space:\nsxy = e−d(x,y). (1)\nShepard termed this phenomenon the Universal Law of Generalization, in that it should apply to any intelligent agent, anywhere in the universe. This result has been used in numerous cognitive models that invoke similarity (e.g., Nosofsky, 1986; Kruschke, 1992).\nIn spite of this, one could argue that generalization from a single stimulus to another does not adequately describe the full scope of human behavior. Indeed, in a concept learning task, people are asked to generalize from multiple examples of a concept. To capture this, Tenenbaum and Griffiths (2001) extended Shepard’s original Bayesian derivation of the law to rationally integrate information about multiple instances. The resulting model defines the probability of generalization (that y is in C) as a sum of the probabilities of all hypotheses h about the true set C that include both x and y,\np(y ∈C | x) = ∑ h:y∈h p(h | x). (2)\nThe posterior probability of each hypothesis is given by Bayes’ rule,\np(h | x) = p(x | h)p(h) p(x) . (3)\nThe prior p(h) represents the learner’s knowledge about the consequential region before observing x. The likelihood p(x | h) depends on our assumptions about how the process that generated x relates to the set h. The key innovation over Shepard’s model is the use of the likelihood function\np(x | h) = { 1 |h| x ∈ h 0 otherwise\n(4)\nwhere |h| is the number of objects in the set picked out by h. The motivation for this choice of likelihood function is the size principle, which uses the assumption of random sampling to justify the idea that smaller hypotheses should be given greater weight (see Tenenbaum and Griffiths, 2001, for a demonstration of this when x represents multiple examples).\nThe value of the size principle lies in the fact that it allows for the benefit of multiple examples of a concept to influence generalization. Assuming samples are drawn independently, the likelihood of a hypothesis for n samples is simply the likelihood of that hypothesis for a single sample to the power of n. From this, it can be shown that generalization tightens as the number of examples increases, consistent with human judgments (see Tenenbaum & Griffiths, 2001).\nThe size principle thus plays an important role in understanding generalization, placing equal importance on determining whether it actually describes human similarity judgments in a wide range of settings. If the size principle is disconfirmed, an alternative augmentation of Shepard’s model is needed to explain generalization from multiple instances. If it can be confirmed to hold broadly, it is a good candidate for a second law of generalization, or an amendment of the original law. In this paper we build on previous work evaluating the evidence for the size principle (Navarro & Perfors, 2010), providing a novel and more direct methodology and a broader empirical evaluation that includes rich perceptual feature spaces."
    }, {
      "heading" : "Evaluating the Size Principle",
      "text" : "In this section we describe previous work evaluating the size principle and provide the details of our approach."
    }, {
      "heading" : "Previous work",
      "text" : "Navarro and Perfors (2010) made three important contributions towards understanding the scope of the size principle. First, they made explicit a link between the Bayesian model of generalization and a classic model of similarity judgment. The similarities between a set of objects can be summarized in a similarity matrix S, where the entry in the ith row and jth column gives the similarity si j between objects i and j.\nar X\niv :1\n70 5.\n03 26\n0v 1\n[ cs\n.A I]\n9 M\nay 2\n01 7\nThe additive clustering model (Shepard & Arabie, 1979) decomposes such a similarity matrix into the matrix product of a feature-by-object matrix F, its transpose, and a diagonal weight matrix W,\nS = FWFT . (5)\nThe feature matrix F is binary and can represent any of a broad set of structures including partitions, hierarchies, and overlapping clusters, and can either be inferred by a number of different models or generated directly by participants. The individual entries of S are defined as\nsi j = N f\n∑ k=1 wk fik f jk. (6)\nNavarro and Perfors (2010) pointed out that each feature could be taken as a single hypothesis h, as it likewise picks out a set of objects with a common property. Having made this link, the degree of generalization between objects i and j predicted by the Bayesian model can be put in the same format as Equation 6: a weighted sum of common features (a similar point was made by Tenenbaum & Griffiths, 2001). The equivalence can be seen if we let wk represent the posterior p(h | x) and fk be the kth hypothesis (hk), since fik f jk selects only the features that contain both objects. If the prior probabilities of the different hypotheses are similar, the likelihood (and the size principle) will dominate and\nwk ∝ 1 |hk| . (7)\nUsing the link between hypotheses and features, Navarro and Perfors (2010) made their second contribution: an alternative derivation showing that the relationship predicted by the size principle can hold even in the absence of random sampling. They argued that learners encode the similarity structure of the world by learning a set of features F that efficiently approximate that structure. Under this view, a “coherent” feature is said to be one for which all objects that possess that feature exhibit high similarity. If a learner seeks a set of features that are high in coherence, the size principle emerges even in the absence of sampling since the variability in the distribution of similarities between objects sharing a feature is a function of |hk|.\nThe third contribution that Navarro and Perfors (2010) made was to evaluate this prediction using data from the Leuven Natural Concept Database (LNCD; De Deyne et al., 2008). This database consists of human-generated feature matrices for a large number of objects, as well as pair-wise similarity ratings for those objects. Navarro and Perfors (2010) observed that, under some simplifying assumptions, the size principle predicts that the similarity between objects that share a feature will be inversely related to the size of that feature. They showed that this prediction was borne out in 11 different domains analyzed in the LNCD.\nDirectly testing the size principle The method adopted by Navarro and Perfors (2010) depends on the derived relationship between the similarity of objects that share a feature and the number of those objects. However, the link that they established between Bayesian clustering and\nthe additive clustering model (Equations 5-7) can also be used to directly test the size principle. Since both models take the same mathematical form, we can directly test the size principle by estimating the weights wk for a set of features F and verifying that Equation 7 holds. If we take the logarithm of both sides of this equation, we obtain the linear relationship\nlogwk =− log |hk|+ c (8)\nwhich can be evaluated by correlating wk with the number of objects that possess feature k. Given a feature matrix F and similarity matrix S, the weights wk can be obtained through linear regression (Peterson, Abbott, & Griffiths, 2016). In additive clustering the weights are often constrained to be nonnegative. To obtain such weights, we employ a non-negative least squares algorithm (Lawson & Hanson, 1995). We can thus directly test the size principle in any domain where a feature matrix and a corresponding similarity matrix are available. In the remainder of the paper we consider two different sources of such matrices: semantic feature norms and perceptual neural networks."
    }, {
      "heading" : "Semantic Hypothesis Spaces",
      "text" : "We first evaluate evidence for the size principle using two groups of datasets in which people judge the similarity of words. Both datasets contain human similarity ratings of noun pairs corresponding to concrete objects (e.g., “zebra” and “lion”) and lists of binary feature labels associated with each object that can be filtered by frequency of mention. We\ncall these features “semantic” because they are linguistic descriptions of general concepts that exclude perceptual-level information associated with actual instances of that concept or brought to mind when the instance is perceived."
    }, {
      "heading" : "Semantic Dataset Group 1",
      "text" : "Following Navarro and Perfors (2010), the first evaluation dataset is comprised of similarity and feature matrices from the Leuven Natural Concept Database (De Deyne et al., 2008). It includes data for 15 categories (Kitchen Utensils, Clothing, Vegetables, Professions, Fish, Sports, Birds, Fruit, Reptiles, Insects, Tools, Vehicles, Musical Instruments, Mammals, and Weapons), each containing ∼20− 30 exemplars. Binary feature matrices for each category contain ∼200− 300 unique features each. The feature descriptions are much broader than merely visually apparent features (e.g., “has wings”, “eats fruit”, “is attracted by shiny objects”)."
    }, {
      "heading" : "Semantic Dataset Group 2",
      "text" : "The second dataset group consisted of 17 similarity matrices from a variety of sources throughout the literature. The experimental contexts and methodologies differed considerably compared to the those in group 1. All but one of these datasets (SIMLEX) were taken from the similarity data repository on the website of Dr. Michael Lee (http://faculty.sites.uci.edu/mdlee/similarity-data/). SIMLEX was taken from a larger word similarity dataset (Hill, Reichart, & Korhonen, 2016). The majority of the datasets (Birds, Clothing1, Clothing2, Fish, Fruit1, Fruit2, Furniture1,\nFurniture2, Tools, Vegetables1, Vegetables2, Weapons1, and Weapons2) are from Romney, Brewer, and Batchelder (1993). For dataset pairs such as (Vegetables1, Vegetables2), the first contains more prototypical items than the second. Since none of these datasets contain corresponding object-feature data, we matched objects from each set to the feature norms reported in McRae, Cree, Seidenberg, and McNorgan (2005)."
    }, {
      "heading" : "Analysis & Results",
      "text" : "For each dataset, we computed the element-wise multiplication of each pair of rows in F and used non-negative least squares to regress this matrix onto the corresponding empirical similarity values. We then computed the log of all nonzero weights, as well as the log of the feature sizes (column sums of the F matrix for which there was a corresponding non-zero weight). The resulting log weights and log feature sizes are z-score normalized and plotted in Figures 1 and 2 for each category in each subgroup. Red lines indicate perfect -1 slopes as predicted by the size principle, whereas black lines are best fitting lines to the actual data. The corresponding correlation coefficients are reported in Tables 1 and 2 along with a number of other statistics to be discussed.\nAverage Pearson and Spearman correlations were -0.43 and -0.47, respectively for group 1, and -0.63 and -0.61 for group2. For nearly all individual datasets in all groups, coefficients are consistently negative, with the exception of Animals11 in group 2, which along with Animals5 are the only datasets with no published method. Correlations were generally stronger for group 2. All correlations in group 1 were significant at the α = 0.05 level except for the Reptiles and Mammals datasets. In contrast, virtually no correlations were significant in group 2 given the small number of features with non-zero coefficients, however one-sample t-tests confirmed that the mean slopes were significantly less than 0 for both Pearson (t(16) =−7.65, p < 0.0001) and Spearman (t(16) =−7.65, p < 0.0001) correlations.\nThe FR (feature ratio) column indicates how many coefficients were positive out of the total possible. Although there were many more features overall in group 1, the average percentage of features with non-zero weights was comparable (28% and 23% respectively).\nFinally, model performance in predicting similarity (R2) is reported in the R2MP column, and indicates the degree to which the feature sets are sufficient to accurately predict human similarity judgments. (R2) values for group 1 are markedly higher than group 2 (which have many fewer features) and match reliability ceilings reported in the original experiments."
    }, {
      "heading" : "Perceptual Hypothesis Spaces",
      "text" : "While evidence for the size principle seems apparent from studies of semantic hypothesis spaces, there has been no work attempting to verify the operation of the principle for concrete objects, especially with complex, real-world instances of these objects such as natural images. The featural representations of such instances are complex and include innumerable details not contained in semantic descriptions of the\ngeneral case, rendering explicit feature descriptions difficult. Here, we offer a method to overcome this challenge by leveraging representations learned from deep neural networks."
    }, {
      "heading" : "Perceptual Features",
      "text" : "Recent work (Peterson et al., 2016) has provided evidence that deep image feature spaces can be used to approximate human similarity judgments for complex natural stimuli. For our analysis, we extracted image features from an augmented version of Alexnet with a binarized final hidden layer (Wu, Lin, & Tang, 2015). This allows for a comparison both to non-\nperceptual binary feature sets (i.e., features from the previous section) and non-binary perceptual feature sets (i.e., previous work on similarity prediction)."
    }, {
      "heading" : "Stimuli & Data Collection",
      "text" : "We obtained pairwise image similarity ratings for 5 sets of 120 images (animals, fruits, furniture, vegetables, vehicles) using Amazon Mechanical Turk, following Peterson et al. (2016). Examples of images in each dataset are given in Figure 4. The image sets represent basic level categories, with 20-40 subordinate categories in each.\nSubjects rated at least 4 unique pairs of images and we required that at least 10 unique subjects rate each possible pair. Each experiment yielded a 120×120 similarity matrix."
    }, {
      "heading" : "Analysis & Results",
      "text" : "As before, we computed the pairwise multiplication of each pair of rows in F (120 images× 4096 neural features) and regressed this matrix onto the corresponding empirical similarity values. The resulting weights and feature sizes are plotted in Figure 3 for each category, and the corresponding correlations are reported in Table 3.\nLike the previous semantic datasets, only a small portion of the total features obtained non-zero weights, although the average percentage was much smaller (∼ 4%). Given that the\nfull feature set is meant to characterize 1000 mostly qualitatively distinct categories from which they were learned (Deng et al., 2009), whereas features from the semantic datasets were relevant only to the objects in each group, this discrepancy is to be expected.\nIn all five datasets, correlation coefficients are moderate, negative, and significant at the α= 0.001 level. Average Pearson and Spearman correlation was 0.42 in both cases. Variance explained in similarity matrices was comparable to previous work on predicting similarity from deep features, but was generally reduced given the constraint of binary features and non-negative weights."
    }, {
      "heading" : "Discussion",
      "text" : "We have attempted to provide a direct evaluation of the size principle in both semantic and perceptual hypothesis spaces. In some cases, the correlations we report using our method are weaker overall than those reported in past work (Navarro & Perfors, 2010), but are consistently negative nonetheless. If anything, this discrepancy serves as a caution to trusting a single method for evaluating the size principle.\nAcross all datasets, variance explained in similarity judgments ranged from .03 to .94, however these fluctuations don’t appear to vary systematically with the magnitude of the size principle effect, This may indicate that the size principle should emerge with respect to both “good” and “bad” feature sets, so long as they are related to the objects and vary in their inclusiveness.\nFurthermore, it appears that the size principle can be shown to operate in more ecologically valid stimulus comparisons\nsuch as visual image pairs. In cases such as these, the specific visual details of the image are relevant, and our feature sets derived from convolutional neural networks included only these features. There may be hundreds of small visual details that are only present in novel instantiations of familiar objects that we encounter on a daily basis and that actually represent the more abstract concepts used in semantic datasets. These results may also have implications for the method of estimating human psychological representations recently proposed by Peterson et al. (2016). In this work, it was shown that human similarity judgments for natural images can be estimated by a linear transformation of deep network features, and the current results imply that this transformation is perhaps partly accounted for by the size principle. This finding may lead to better methods for approximating complex human representations based on psychological theories.\nIt is apparent from the FR columns of each table that few of the total features were used in the actual models. This may be due in part to useless features, or features associated with too many or too few objects. It may also be due to multicollinearity in our feature matrices (some columns are linear combinations of others). These are unique consequences of using a regression model. For this reason, our method may be less susceptible to over-representing certain features that are redundant. On the other hand, the size principle is meant to address the problem of redundant hypotheses directly, and it may be an undesirable property of our model that these hypotheses are eliminated through other means, which is perhaps the cost of direct estimation of the weights in the additive clustering framework. In any case, this variability in the amount of non-redundant features does not appear to co-vary with the size principle in any systematic way.\nThe only notable discrepancy between our results and the predictions of the size principle is the variation in the magnitude of the negative slopes obtained, which does not appear to depend on model performance, number of features, or even aspects of the dataset groups or individual datasets. Semantic dataset group 2 had more large slopes (e.g., SIMLEX) than group 1, but also had many small slopes. Similar datasets from group 1 (e.g., Fruit and Vegetables) had fairly dissimilar slopes, and nearly identical datasets from group 2 (e.g., Fruit1 and Fruit2) had widely varying slopes. Prototypicality doesn’t seem to matter either, since Fruit1 and Vegetables1 have smaller slopes than Fruit2 and Vegetables2, but Clothing1 and Vehicles1 have larger slopes than Clothing 2 and Vehicles2. Furthermore, we can find examples of both natural and artificial stimuli with comparable slopes. For these reasons, it is unclear what the source of these deviations could be. It is possible that certain experimental contexts encourage a focus on certain featural comparisons that can be represented by a weighting of our feature sets, and so still allow for good model fit. Alternatively, it may be an artifact of the weight estimation algorithm, in which case it will be useful to compare alternative methods.\nOur results provide broad evidence for the size principle\nregardless of the assumptions that are employed to derive it. Thus, the size principle is a good candidate for a second universal law of generalization, and can be motivated both by rational theories based on strong sampling and feature learning. Further, a 1|h| law can provide a solid basis for generalizing from multiple instances, a behavior that we should expect to find in any intelligent agent, anywhere in the universe."
    } ],
    "references" : [ {
      "title" : "Exemplar by feature applicability matrices and other dutch normative data for semantic concepts",
      "author" : [ "S. De Deyne", "S. Verheyen", "E. Ameel", "W. Vanpaemel", "M.J. Dry", "W. Voorspoels", "G. Storms" ],
      "venue" : "Behavior research methods,",
      "citeRegEx" : "Deyne et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Deyne et al\\.",
      "year" : 2008
    }, {
      "title" : "Imagenet: A large-scale hierarchical image database",
      "author" : [ "J. Deng", "W. Dong", "R. Socher", "Li", "L.-J", "K. Li", "L. Fei-Fei" ],
      "venue" : "In Computer vision and pattern recognition,",
      "citeRegEx" : "Deng et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Deng et al\\.",
      "year" : 2009
    }, {
      "title" : "Simlex-999: Evaluating semantic models with (genuine) similarity estimation",
      "author" : [ "F. Hill", "R. Reichart", "A. Korhonen" ],
      "venue" : null,
      "citeRegEx" : "Hill et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Hill et al\\.",
      "year" : 2016
    }, {
      "title" : "Alcove: An exemplar-based connectionist model of category learning",
      "author" : [ "J.K. Kruschke" ],
      "venue" : "Psychological review,",
      "citeRegEx" : "Kruschke,? \\Q1992\\E",
      "shortCiteRegEx" : "Kruschke",
      "year" : 1992
    }, {
      "title" : "Solving least squares problems",
      "author" : [ "C.L. Lawson", "R.J. Hanson" ],
      "venue" : null,
      "citeRegEx" : "Lawson and Hanson,? \\Q1995\\E",
      "shortCiteRegEx" : "Lawson and Hanson",
      "year" : 1995
    }, {
      "title" : "Semantic feature production norms for a large set of living and nonliving things",
      "author" : [ "K. McRae", "G.S. Cree", "M.S. Seidenberg", "C. McNorgan" ],
      "venue" : "Behavior research methods,",
      "citeRegEx" : "McRae et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "McRae et al\\.",
      "year" : 2005
    }, {
      "title" : "Similarity, feature discovery, and the size principle",
      "author" : [ "D.J. Navarro", "A.F. Perfors" ],
      "venue" : "Acta Psychologica,",
      "citeRegEx" : "Navarro and Perfors,? \\Q2010\\E",
      "shortCiteRegEx" : "Navarro and Perfors",
      "year" : 2010
    }, {
      "title" : "Attention, similarity, and the identification–categorization relationship",
      "author" : [ "R.M. Nosofsky" ],
      "venue" : "Journal of experimental psychology: General,",
      "citeRegEx" : "Nosofsky,? \\Q1986\\E",
      "shortCiteRegEx" : "Nosofsky",
      "year" : 1986
    }, {
      "title" : "Adapting deep network features to capture psychological representations",
      "author" : [ "J. Peterson", "J. Abbott", "T. Griffiths" ],
      "venue" : "In Proceedings of the 38th annual conference of the cognitive science society",
      "citeRegEx" : "Peterson et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Peterson et al\\.",
      "year" : 2016
    }, {
      "title" : "Predicting clustering from semantic structure",
      "author" : [ "A.K. Romney", "D.D. Brewer", "W.H. Batchelder" ],
      "venue" : "Psychological Science,",
      "citeRegEx" : "Romney et al\\.,? \\Q1993\\E",
      "shortCiteRegEx" : "Romney et al\\.",
      "year" : 1993
    }, {
      "title" : "Toward a universal law of generalization for psychological science",
      "author" : [ "R.N. Shepard" ],
      "venue" : null,
      "citeRegEx" : "Shepard,? \\Q1987\\E",
      "shortCiteRegEx" : "Shepard",
      "year" : 1987
    }, {
      "title" : "Additive clustering: Representation of similarities as combinations of discrete overlapping properties",
      "author" : [ "R.N. Shepard", "P. Arabie" ],
      "venue" : "Psychological Review,",
      "citeRegEx" : "Shepard and Arabie,? \\Q1979\\E",
      "shortCiteRegEx" : "Shepard and Arabie",
      "year" : 1979
    }, {
      "title" : "Generalization, similarity, and bayesian inference",
      "author" : [ "J.B. Tenenbaum", "T.L. Griffiths" ],
      "venue" : "Behavioral and brain sciences,",
      "citeRegEx" : "Tenenbaum and Griffiths,? \\Q2001\\E",
      "shortCiteRegEx" : "Tenenbaum and Griffiths",
      "year" : 2001
    }, {
      "title" : "Adjustable bounded rectifiers: Towards deep binary representations. arXiv preprint arXiv:1511.06201",
      "author" : [ "Z. Wu", "D. Lin", "X. Tang" ],
      "venue" : null,
      "citeRegEx" : "Wu et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 10,
      "context" : "In the seminal work of Shepard (1987), the notion of stimulus similarity was made concrete through its interpretation as stimulus generalization.",
      "startOffset" : 23,
      "endOffset" : 38
    }, {
      "referenceID" : 3,
      "context" : "This result has been used in numerous cognitive models that invoke similarity (e.g., Nosofsky, 1986; Kruschke, 1992).",
      "startOffset" : 78,
      "endOffset" : 116
    }, {
      "referenceID" : 11,
      "context" : "To capture this, Tenenbaum and Griffiths (2001) extended Shepard’s original Bayesian derivation of the law to rationally integrate information about multiple instances.",
      "startOffset" : 17,
      "endOffset" : 48
    }, {
      "referenceID" : 5,
      "context" : "(7) Using the link between hypotheses and features, Navarro and Perfors (2010) made their second contribution: an alternative derivation showing that the relationship predicted by the size principle can hold even in the absence of random sampling.",
      "startOffset" : 52,
      "endOffset" : 79
    }, {
      "referenceID" : 5,
      "context" : "(7) Using the link between hypotheses and features, Navarro and Perfors (2010) made their second contribution: an alternative derivation showing that the relationship predicted by the size principle can hold even in the absence of random sampling. They argued that learners encode the similarity structure of the world by learning a set of features F that efficiently approximate that structure. Under this view, a “coherent” feature is said to be one for which all objects that possess that feature exhibit high similarity. If a learner seeks a set of features that are high in coherence, the size principle emerges even in the absence of sampling since the variability in the distribution of similarities between objects sharing a feature is a function of |hk|. The third contribution that Navarro and Perfors (2010) made was to evaluate this prediction using data from the Leuven Natural Concept Database (LNCD; De Deyne et al.",
      "startOffset" : 52,
      "endOffset" : 819
    }, {
      "referenceID" : 6,
      "context" : "The method adopted by Navarro and Perfors (2010) depends on the derived relationship between the similarity of objects that share a feature and the number of those objects.",
      "startOffset" : 22,
      "endOffset" : 49
    }, {
      "referenceID" : 5,
      "context" : "Following Navarro and Perfors (2010), the first evaluation dataset is comprised of similarity and feature matrices from the Leuven Natural Concept Database (De Deyne et al.",
      "startOffset" : 10,
      "endOffset" : 37
    }, {
      "referenceID" : 8,
      "context" : "Recent work (Peterson et al., 2016) has provided evidence that deep image feature spaces can be used to approximate human similarity judgments for complex natural stimuli.",
      "startOffset" : 12,
      "endOffset" : 35
    }, {
      "referenceID" : 8,
      "context" : "120 images (animals, fruits, furniture, vegetables, vehicles) using Amazon Mechanical Turk, following Peterson et al. (2016). Examples of images in each dataset are given in Figure 4.",
      "startOffset" : 102,
      "endOffset" : 125
    }, {
      "referenceID" : 1,
      "context" : "full feature set is meant to characterize 1000 mostly qualitatively distinct categories from which they were learned (Deng et al., 2009), whereas features from the semantic datasets were relevant only to the objects in each group, this discrepancy is to be expected.",
      "startOffset" : 117,
      "endOffset" : 136
    }, {
      "referenceID" : 8,
      "context" : "These results may also have implications for the method of estimating human psychological representations recently proposed by Peterson et al. (2016). In this work, it was shown that human similarity judgments for natural images can be estimated by a linear transformation of deep network features, and the current results imply that this transformation is perhaps partly accounted for by the size principle.",
      "startOffset" : 127,
      "endOffset" : 150
    } ],
    "year" : 2017,
    "abstractText" : "Shepard’s Universal Law of Generalization offered a compelling case for the first physics-like law in cognitive science that should hold for all intelligent agents in the universe. Shepard’s account is based on a rational Bayesian model of generalization, providing an answer to the question of why such a law should emerge. Extending this account to explain how humans use multiple examples to make better generalizations requires an additional assumption, called the size principle: hypotheses that pick out fewer objects should make a larger contribution to generalization. The degree to which this principle warrants similarly law-like status is far from conclusive. Typically, evaluating this principle has not been straightforward, requiring additional assumptions. We present a new method for evaluating the size principle that is more direct, and apply this method to a diverse array of datasets. Our results provide support for the broad applicability of the size principle.",
    "creator" : "LaTeX with hyperref package"
  }
}