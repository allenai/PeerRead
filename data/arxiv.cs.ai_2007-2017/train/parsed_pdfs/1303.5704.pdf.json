{
  "name" : "1303.5704.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "\"Conditional Inter-Causally Independent\" node distributions, a property of \"noisy-or\" models",
    "authors" : [ "John Mark Agosta" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "This paper examines the interdependence generated between two parent nodes with a common instantiated child node, such as two hypotheses sharing common evidence. The relation so generated has been termed \"inter causal.\" It is shown by construction that inter-causal independence is possible for bi nary distributions at one state of evidence. For such \"CICI\" distributions, the two mea sures of inter-causal effect, \"multiplicative synergy\" and \"additive synergy\" are equal. The well known \"noisy-or\" model is an ex ample of such a distribution. This introduces novel semantics for the noisy-or, as a model of the degree of conflict among competing hy potheses of a common observation.\nIn a general Bayesian network, the relation between a pair of nodes can be predictive, meaning we are inter ested in the effect of a node upon its successors, or, oppositely, diagnostic, where we infer the state of a node from knowledge of its successors. \\\\1e can define yet a third relation between nodes that are neither suc cessors of each other, but share a common successor. Such a relation has been termed inter-causal. (Henrion and Druzel 1990, p.10] For example, in the simplest di agram with this property, nodes A and B in Figure one are inter-causally related to each other by their com mon evidence at node e. This relation is a property of the clique formed by \"marrying the parents\" of e, not by the individual effects of the arcs into e. In this paper I derive the quantitative inter-causal properties due to evidence nodes constructed from the noisy-or\" model.\nThe interest in inter-causal relations occurs in the pro cess of abduction, that is, reasoning from evidence back to the hypotheses that explain the evidence. This arises in problems of interpretation, where more than one hypothesis may be suggested by a piece of evi dence. (Goldman and Charniak 1990] Having multiple\nexplanations denotes the ambiguity due to not having enough information to entirely resolve which hypothe sis offers the true explanation. This paper shows how to construct an evidence node that expresses this am biguity by the degree of conflict between hypotheses. We apply this elsewhere (Agosta 1991] as a compo nent in building a \"recognition network\" where rele vant hypotheses are created \"on the fly\" as possible interpretations of the evidence.\nThe implicit relation between A and B due to shared evidence has been extensively explored as the prop erty of one hypothesis to \"explain away\" another. These are cases where, given evidence and the asser tion of one hypothesis, the other hypothesis can be disqualified as a cause of the evidence. This paper ex plores how this dependency induced between hypothe ses changes with the evidence. Interestingly, with bi nary variables, the induced dependency may vary, and as shown by the noisy-or, disappear for certain states of evidence.\nThis paper characterizes quantitatively the depen dency between A and B that stems from the likelihood matrix at e. Capital letters such as A and B denote\n10 Agosta\nunobserved random variables and lower case letters de note variables when they have been observed: e+ for E = true and e- forE= false.\nDependencies between two hypotheses' existence can occur in two senses: they conflict, so as the proba bility of one hypothesis' existence increases, the other decreases-we say one tends to exclude the other; or, as one increases the other increases also. The latter relation shall be called collaboration. First I discuss some of the basic independence properties of the net work shown in figure one as it depends on the state of node e. Next I consider how the conditional distribu tion of node e leads to conditional dependence of its parents, using the \"noisy-or\" model as an example for node e. Finally I propose a quantitative parameteriza tion of the dependence generated between the parent nodes.\n1.1 INTER-CAUSAL INDEPENDENCE\nThe definition of d-separation [Pearl 1988, p.117] pro vides general conditions about the conditional inde pendence of nodes that are parents of a common evi dence node. In figure one, nodes A and B must be in dependent when their common successor is uninstanti ated, or has any instantiated successors. The converse is not always true: it is possible to construct cases where A and B remain conditionally independent after e has been observed.1 The d-separation theorem ap plies to the structure of the network: this conditional case extends it to the property of the distributions for a common successor node.\nTo construct such an independence conserving node, consider first the case where all variables are binary valued. The likelihood matrix for node e is:\n[ r s ] def { _1 } t u = p e A B such that r�r p{e-IA = a+ B = b+ },\ns �r p{ e-1 A= a- B = b+} and so 011. Taking expectation over B, the likelihood ratio seen by A, p{ e-1 a+} / p{ e-1 a-}, will be in the range be tween r/s and tfu. It is evident that, if the likelihood ratios in each row are the same, then the likelihood ratio seen by the other parent, A, will be constant for any value of B. Thus the expected likelihood ratio for A will be independent of the distribution of the other parent, node B. The same argument applies to the columns, and so to the relation of B upon A.\nThis property generalizes to random variables with more than two states where each row in the likelihood\n1W. Buntine has pointed out that this is also a well known property of the logistic distribution, which may be thought of as a continous version of the noisy-or.\nmatrix differs only by a ratio, so that the row space is of rank one. Using a well known result from lin ear algebra, the row rank equals the column rank, so the same argument applies to the columns' likelihood ratios. This suggests a way to construct such a matrix:\nProposition 1: Independence is preserved between direct predecessors A and B of a common successor node E for one state of the evidence e-, if the com bined likelihood matrix is proportional to the \"outer product\" of the vectors for each individual likelihood:\nThis is shown by solving for p{ AI Be-} for any p{ A}, with Bayes' rule:\np{AIBe-} p{ e-1 A B }p{ A}\nEA[p{e IAB}p{A}] Substituting in the likelihood, and simplifying :\np{ e-1 A }p{ e-1 B }p{ A} EA[P{ e-1 A }p{ e-1 B }p{ A}]\np{ e-A} - {AI -} p{ e-} p e .\nI will call this independence condition between pre decessor nodes conditional on one state of the com mon evidence \"conditional inter-causal independence,\" or CICI. This condition on the likelihood distribu tion serves as a qualification on the conditions of d separation for specified states of evidence at E.\nSince the likelihood matrix appears in both numera tor and denominator of Bayes' rule, scaling the like lihood by a constant affects neither l.h.s. nor r.h.s. Thus in the binary case, where the likelihoods are a = p{ e-1 a+}, b = p{ e-1 b+ }, the outer product of the two likelihood vectors with a scaling factor, c, is general form for a CICI relation matrix:\n[ � � ] [ abc a(1-b)c (1 -a)bc ] (1-a)(1-b)c · I will call this the \"singular matrix\" model. The in dependence constraint removes one degree of freedom, leaving the matrix to be specified with three parame ters. For binary variables, this constraint is equivalent to the relation matrix having a determinant equal to zero. This follows from the proposition:\nCorollary 1: The determinant of a likelihood ma trix of binary valued random variables, p{ el A B }, of rank one equals zero. Thus det p{ e I A B } = 0 im plies that p{ AI Be} = p{ AI e }. Multiplying out the determinant gives det p{ el A B} = ru-st, the quan tity referred to as \"multiplicative synergy\" by Ilenrion. [Henrion, Druzdzel 1990]\n\"Conditional Inter-Causally Independent\" Node Distributions, a Property of \"Noisy-Or\" Models 11\nThis independence relation p{ AI BE} = p{ AI E} holds for CICI nodes at both certainty for one value of e = E as well as for complete ignorance of E. The next questions are 1) whether this independence is implied for all distributions p{ E}, and conversely 2) whether there are necessarily states of E for which CICI nodes do create conditional dependence. If 1) is true, CICI evidence nodes would be degenerate and serve no pur pose.\nTo answer the first question we test if is it possible to have a relation matrix that is rank one at each state of the evidence. In that case the relation matrix would be factorable for every state of the evidence. In the binary case, this pair of constraints for both E = e+ and E = e- can be shown, with some algebra, to im ply that the likelihood ratios for one of the two parents must be constant and equal to one. This means that effectively there is no arc from that parent to the evi dence. This independence is implied by a more general result of [Geiger and I-Ieckerman 1990] about \"transi tive distributions\" for which connectedness in graphi cal representations is equivalent to dependence among the distributions. Strictly positive binary distributions are one case of transitive distributions.\nNow the converse, to show when the likelihood is fac torable at one state of evidence it creates dependencies among parents at others. Let the evidence be a binary node, factorable at E = e-. Then by Bayes rule, at the other state of the evidence:\np{BIAe+} p{ B}\n=\np{e+jAB}\np{e+jA}\n(1-p{e-IA}p{e-IB}) 1-p{ e I A}\nThe right side cannot be factored into A and B factors, and is dependent upon A.\n1.2 The noisy or The noisy-or model is an example that illustrates the dependencies generated by CICI likelihoods:\nProposition 2: A \"noisy-or\" is a case of a CICI node. This can be shown by writing the noisy-or for evidence e+ as\nwhere q; = 1 -p;, the reliability probabilities. It is evident that for evidence e- , the likelihood matrix is a matrix of ones minus this. Calculating its determi nant,\ndet II - p{ e+ I A B} I = det p{ e-1 A B} = 0.\nWith CICI nodes I will, by convention, label the evi dence e- at which independence occurs.\nThe other way to build a CICI node is from the \"sin gular matrix model,\" mentioned in the previous sec tion, where the singular matrix represents the likeli hood p{ e-1 A B } . What is the relation between these two models? They both have three degrees of freedom. Equating and solving obtains c = q0, q2 = b/(1-b), q, = a/(1-a). Since all terms must be probabilities in the range of (0, 1), the noisy-or can be identified with the singular matrix model only when the singular ma trix parameters are restricted to 0 < a, b < 1/2. This is because the noisy-or model enforces a size ordering among matrix entries, the largest entry being in the upper left hand corner. There are three other cases, 0 < a < 1/2 :S b < 1 , 0 < b < 1/2 :S a < 1 and 1/2 :S a, b < 1. These are equivalent to the noisy-or matrix with the row terms switched, the column terms switched, or both switched. These four generalizations cover the range of binary CICI relation nodes.\n1.3 THE DEGREE OF INTER-CAUSAL EFFECT\nWe have seen that inter-causal independence among a node's parents depends upon the common node's evidence. In the binary case, forcing inter-causal in dependence at one state of the evidence precludes it from the other state. We have also seen that, in the binary case, the rank one condition for independence is easily tested by looking for a zero determinant of the likelihood matrix. The next question is, what does the value of a non-zero determinant indicate about the ef fect of A upon B?\n1.3.1 Qualitative effects The value of this determinant varies from minus unity to plus unity as the relation between parents goes from extreme exclusion to extreme collaboration. At each extreme the parents A and B are deterministically de pendent. Then either the parents are mutual exclu sive, a condition already discussed, or they are forced to have identical distributions. To force identity be tween parents, the relation matrix becomes an identity matrix. For exclusion it is one minus this matrix zeros on the diagonal and ones off-diagonal. Call these extremes \"complete collaboration and \"complete ex clusion.\" Thus a relation matrix with complete col laboration for e+ will have complete exclusion for e-. These two matrices and their linear combinations are not CICI matrices, except for the trivial case of a con stant matrix.\nTo be able to use the determinant measure-the mul tiplicative synergy-to characterize the relation be tween parents, I must first establish that the sign of this property of the likelihood matrix is invariant to Bayes' rule: The next theorem shows that the sign of\n12 Agosta\nthe multiplicative synergy equals the sign of the CICI relation between parents not just for p{ e I A B } , but for p{ AI Be}, and all other permutations that may be generated by Bayes' rule.\nLemma: Multiplication of a likelihood matrix, L(X, Y);j by any positive probability vector v(X); does not change the sign of the likelihood's determi nant.\nTo show this: Multiplication by a row vector variable is equivalent to a term-by-term multiplication of matri ces where the vector is replicated to fill out the columns of its matrix. This, in turn, is equivalent to matrix multiplication where the vector values fill the diagonal of a matrix, with all other entries zero. Write this di agonal matrix derived from the vector as d( v );; . From linear algebra there is the result that the determinant of a product equals the product of each matrix's de terminant, thus\ndet d( v );; L(X, Y);i == det d( v );; det L(X, Y);i. The determinant of the diagonal matrix is merely the product of terms along the diagonal, a number between zero and one. We can now show:\nProposition 3: Exclusion or collaboration (the sign of the multiplicative synergy) is given by the sign of the determinant of p{ e I A B } and is invariant to all permutations derivable by Bayes' rule of this likelihood matrix for a given conditioning.\nBayes' rule consists of multiplying the likelihood ma trix by one probability vector, the prior, then divid ing it by another, the pre-posterior. By the previous lemma, multiplication by the prior multiplies the like lihood's determinant by a positive number. Division by the pre-posterior likewise multiplies it by the re ciprocal, another positive number. Both operations preserve the sign of the likelihood determinant. Note that since the conditioning of the likelihood must be preserved;det p{ e+ I A B} > 0 does not necessarily im ply that det p{ a+ IE B} > 0.\n1.3.2 Comparision to other measures of diagnostic and inter-causal relations\nInter-causality has been examined as a qualitative re lation by Wellman. (Wellman 1988) In the tradition of non-numeric, automatic reasoning methods for plan ning, he has developed an abstraction of influence di agrams where each influence is described by its sign. These \"qualitative probabilistic networks\" can formu late decision tradeoffs by considering dominance rela tionships among alternatives. Such networks are con structed from two kinds of qualitative relations: the first, qualitative influences, describes the relation be tween two variables; the second is the relation between influences that he terms qualitative synergy, which cor responds to inter-causality. Here is his definition of\nsynergy, in our notation: (p. 74)\nDefinition:( Qualitative synergy) Variables A and Bare positively synergistic onE, written Y+(EI A B), or just y+ E, if and only if, for every x, a1, a2, b1, b2, eo, a1 ?: a2, b1 ?: b2 implies\np{ eo I a1 b1 x } - p{ eo I a2 b1 x} :S p{ eo I a1 b2 x } - p{ eo I a2 b2 x}.\nSimilarly in the last relation, substitute \"?:\" for nega tively synergistic and \"==\" for zero synergy.\nHenrion has called this quantity \"additive synergy\" to distinguish it from the multiplicative synergy mea sure defined previously. In comparison to \\Vellman, our definition of \"quantitative additive synergy\" takes the liberty of assigning a value toY whose sign corre sponds to the sign of the synergy:\nYe+ � r Y(E == e+IAB) == r+ u-s - t .\nWellman does recognize in his examples the implied inter-causal relation between A and B due to the syn ergistic properties of the likelihood. As a further dis tinction, Wellman takes pains to extend his definition over all states of conditioning variables x, which he calls the context. This would be useless for our quan titative definition; however it serves his purpose of de termining dominance relations. Unlike his definition however, I define aY e for each conditioning of E in the likelihood matrix. Since qualitative synergy is de rived from a stochastic dominance relation on contin uous variables, to apply it to the case of binary vari ables he introduces a sign ordering convention such that e+ > e-. In my framework, his definition is equivalent to just the case where E == e+. As such, the manner in which this relation depends upon the evidential support at E is not devdoped in his exam ples.\n1.3.3 Relation between additive and multiplicative synergies\nAs seen, for purposes of characterizing the effects be tween inter-causal nodes, I have modified definitions of synergy to be conditional on the states of binary variables. The next part develops a constraint among determinants (multiplicative synergy measures) of the same relation matrix with different states of binary evidence.\nProposition 4: Additive synergy equals the sum of the determinant measures, det e, for both states of ev idence. Expressed as a formula,\nYe+ == det e+ - det e-,\n\"Conditional Inter-Causally Independent\" Node Distributions, a Property of \"Noisy-Or\" Models 13\nwhere det e+ is defined to equal determinant IP{ E = e+ I AB } I, and likewise det e- to equal the de terminant IP{ E = e-1 AB }1. Further, Y changes sign when the state of evidence is negated. To demonstrate, since E is a binary variable,\nYe- Y(E=e-IAB) 1-r + 1-u-(1-s)-(1-t) s+t-r-u=-Ye+.\nTo see the relation between multiplicative and additive synergies, write out\ndete- (1-r)(1-u)- (1-s)(1-t) s+t-r-u+ru-st Ye-+ dete+,\nor Y e+ = det e+ -det e-\nProposition 5: Multiplicative and additive synergy are equal for CICI relation matrices. If one of the states of evidence forces independence (e.g., is CICI) then the determinant for that state disappears. Thus for CICI nodes the relation between additive and mul tiplicative synergy is: det e+ = Y e+, that is, both measures are equivalent.\nThe additive-multiplicative synergy relation makes it easy to show the following:\nProposition 6: Noisy-or matrices are exclusionary nodes for E = e+.\nSince det e- = 0, one can use the previous result to show Ye+ < 0. See [Agosta 1991).\nA typical situation expressed by a noisy-or is the rela tion between seeing cat prints in someone's house and inferring which kind of cat they have as a pet. The ex clusionary property of noisy-or nodes is the essence of their ability to \"explain away\" one hypothesized cause as another cause becomes more likely. Thus upon see ing paw prints, one cause-a pet blue Persian-tends to exclude their being also a short haired red tabby in the house.2 If we comb the house and find no paw prints, the explanations remain independent: we are no wiser about relati vc probabili tics of the household's domestic animals, even though we may justifiably tend to doubt they own a pet.\nHow would collaborative nodes, e.g. Y e+ > 0 nodes, be constructed? Recall the result in Linear Algebra that switching a pair of rows or columns of a matrix switches the sign of a matrix's determinant. Thus they can be built from exclusionary nodes by switching the off-diagonal and on-diagonal elements.\n2For the model to apply strictly, there should be no relation between lovers of different kinds of cats; that is, being a Persian owner should not, in itself, make the house hold more or less likely to own a short haired tabby. (This example is inspired by (M. Henrion, 1990].)\n1.3.4 The range of inter-causal dependency\nHow can the dependency be described quantitatively? This inter-causal dependency is not just a consequence of the diagnostic dependencies between the parents, A and B, and the evidence; rather it may be thought of as the relation between these dependencies. At the ex tremes of complete inter-causal dependency, the indi vidual (marginal) likelihoods p{ El A} and p{ El B} are completely determined by the marginals of the other predecessor: there is no additional freedom in the diagnostic relation between hypothesis and evi dence. In comparison, when A and B are inter-causally conditionally independent, the diagnostic support be tween hypothesis and evidence for each can be speci fied independently.\nAs a consequence of proposition 3, there is a qualita tive correspondence, where the sign of the determinant of likelihood matrix terms p{ el A B} corresponds to the sign of the induced dependency of p{ AI B }. Their quantitative relation is not as obvious. Note that un like the determinant, det e+, p{ AI B} is homogeneous of zeroth order in the likelihood terms. That is, scaling the entries in the likelihood matrix does not change the dependence among parents, as can be seen from the following version of Bayes' rule:\n{BI A } = p{ei AB}p{B} p e En[p{eiAB}]\nThis means that multiplying all terms of p{ el A B} by a constant changes the value of the determinant but leaves p{ Bl A e} unchanged, destroying the one to one correspondence between the multiplicative syn ergy and any quantitative characterization of the inter causal relation p{ e I A B } .\nTo explore the quantitative relation, the next section shows the construction of the algebraic solution for one parent's belief as a function of the rest of the clique's nodes.\nFigure 2: The noisy-or belief surface\n14 Agosta"
    }, {
      "heading" : "2 CONSTRUCTIVE SOLUTION OF THE BINARY VARIABLE INTER-CAUSAL DEPENDENC Y",
      "text" : "By deriving the belief of one parent as a function of the probabilities of the other nodes in the clique, one may examine its quantitative behavior completely. This function is a three-dimensional surface, a marginal probability of one parent as a function of the sup port for their common evidence and the other parent. This \"belief function surface\" shows the combined ef fect on one parent of the diagnostic and inter-causal influences.\nThe solution technique used is similar to the clique potential methods. All the nodes are formed into one clique potential, >II, proportional to the joint for the state space of all nodes. The probability that we solve for is the posterior on B as a function of the proba bilities of other nodes in the network. To make pre cise the sensitivity of one node's marginal on other nodes' probabilities, think of the \"knobs\" to control the other nodes as pi (1r ) and lambda (>. ) messages to the nodes; 1r messages as the root nodes, and >. mes sages to leaf nodes. These messages can be specified independently of each other, whereas in general the marginal probabilities of nodes cannot, since they are not independent. In this case there is one >. message, to the evidence node, E, and two 1r messages, one for each parent, of which we are mainly interested in the 1r for the \"other\" parent, A.\nThe posterior on B is a function of both parent pri ors, 1r( a) and 1r( b), the evidence likelihood matrix, p{ El A B}, and the \"evidential support,\" or the >. message that the evidence receives. To show the func tional dependence, I write p{ Bl1r(a) 1r(b) >.(e)}. It is important to distinguish this from p{ Bl A E }, which is a tabulation of probabilities at each combination of points in the state space, rather than a function of probabilities. The potential, >II, is a 2 x 2 x 2 matrix, the product of all terms. To obtain the posterior on B, sum over all other variables, then normalize by the sum of all eight terms. These definitions are used for clarity:\ndef + def ( +) def ( +) a= 1r(A = a ), b = 1r B = b , f = >. E = e ,\nso that,\nThus,\n>V(a, b, f)= 1r(A)1r(B)>.(E)p{ El AB }.\nLAE >V(a, b, f) p{ BIJT(a)1r(b)>.(e)} = \"\" >II( b f) l...JABE a, '\nb[a(Jr + (1- !)(1- r)) + (1- a)(sf + (1- !)(1- s))] , (1- b)[a(Jt + (1- !)(1- t)) + (1- a)(uf + (1- !)(1- u))] LABE >V(a, b, f)\nThe numerator is a two-valued vector forb+ and b-. It is normalized by the denominator, which is precisely the sum of the two terms in the numerator. Figure two shows a graph of this \"belief surface\" as a func tion of and a, for 1r(b) = 1/2. The values from this example are for a symmetric noisy-or likelihood matrix. The conditional independence of the parent node probabilities is evident by the constant value of the function for all values of a at both f = 0; that is, e- = E and f = 1/2, complete ignorance of E. The exclusionary property is evident along the edge f = 1, where B is inversely related to a. The graph may be thought of as a combination of the diagnos tic relation, where decreasing f increases the belief in both parents, together with an inter-causal exclusion ary relation when e+ = E. The inter-causal relation is slight when 1/2 � f � 0 since at both extremes of this interval the inter-causal dependence disappears.\nThe exclusive relation between the beliefs of A and B are described by the f = >.(e) = 1 edge of the belief function surface, shown here in figure three:\nThe two beliefs move in opposite directions, with p{ Bl1r(a.) 1r(b) .\\(e)= 1} having a maximum approxi mately at wand a minimum no lower than 1r(b). Pre viously I mentioned a relation matrix for complete ex clusion, which forced p{ Bl e} + p{ AI e} = 1. In com parison to this partial exclusion at f = 1, the complete exclusion probability, p{ Bl e }, descends to zero from unity. The outstanding difference between complete exclusion and that generated by a CICI node is this lower bound that prevents B 's belief from ever being driven below its value without the CICI node. Thus \"CICI partial exclusion\" cannot defeat other support for a node's belief.\nThe degree of inter-causal exclusion is limited by the diagnostic effect when their combination operates in\n\"Conditional Inter-Causally Independent\" Node Distributions, a Property of \"Noisy-Or\" Models 15\nopposite directions. One way to think of this is that the positive effect of the diagnostic support dominates the negative effect of exclusion. In the \"cat household\" example, if we see a blue Persian after having seen paw prints, our belief in the presence of a short haired red tabby cannot be less than our prior belief about the tabby. To formalize this property of CICI nodes:\nProposition 7: For p{ e+ I A B} in the form of a noisy-or, p{ Bl A e+} > 1r(b) for all values of 1r(a). In terms of the graph in figure two, this constrains the e+ half of the belief function surface to lie above the prior value, and the e- half to lie below. The surface intersects the 1r(b) valued horizontal plane only along the E = 1/2 line.\nTo show\np{ Bl1r(a) = 11r(b)A(e) = 1} > 1r(b),\nwrite it out in functional form;\nbr > b\nbr+(1-b)t '\nwhich reduces to r> t, an assumption of the noisy-or.\nIn a corresponding manner the size ordering relative to B of the other three vertices of the belief surface can be demonstrated. Each vertex value is an increasing function of the prior on B and the ratio of a pair of ele ments in the likelihood matrix. For both p{ Bl e-a+ } and p{ Bl e-a-}, the \"independent edge\" vertices, the ratios are equal: (1-r)/(1-t) = (1- s)/(1-u) . This is just a restatement of the det e- = 0 condition.\nThe \"independent edge\" value p{ Bl e-} and the \"positive exclusion\" value p{ Bl a- e+ }, the two ex treme values of the surface, describe the surface com pletely, and have physical significance in the model. I will use them to effectively factor the relation into a two parameter model of the likelihood, in the \"fac tored\" form of a symmetric noisy-or:\nkw ] w for 0 < k < 1, 0 < w < 1.\nWith the belief surface we can describe qualitatively both parameters' effects. As w increases, the \"posi tive exclusion\" vertex, p{ Bl a-e+ }, increases also. As k decreases, the vertex probabilities become more extreme. At the same time, the \"negative exclu sion\" vertex approaches 1r(b). This is also true for non-symmetric noisy-or's, thus the degree of free dom that was lost to the symmetry assumption has only marginal effect on the surface shape. Further, when 1r(b) approaches either zero or one it pulls the whole surface with it, for instance as 1r(b) -+ 1 then p{BIAE}-. 1.\nTo derive the vertex values in the limit of small k and large w, approximate the values by first order expan sions in k and w . First this lemma, by which one may approximate rational functions whose numerator and denominator differ by a \"small\" amount:\nLemma: Since\n1 z2 -- =1+z+l-z 1-z\nthis approximation holds:\n1 l- z = 1 + z + O(z2)?:: 1 + z, for z small.\nWith this formula the best linear approximation to a rational polynomial is obtained without the need to write out the derivative.\nProposition 8: The \"independent edge\" probability p{ Bl e-} at 1r(b) = 1/2 is independent ofw and equals k / ( 1 + k). This follows exactly since\nFurther, k sets an upper bound for this probability, since it follows that for all k and b,\np{ Bl e-} < k.\nProposition 9: The \"negative exclusion\" corner p{ Bl a-e-} approaches B from above, such that\nSince the inequality is bounded by O(z2), this prob ability approaches b, linearly in k, as k approaches 0. When k is small p{ Bl a-e-} is well approximated by b.\nProposition 10: The \"positive exclusion\" probability p{ Bl a-e+} is bounded below to O(z2) such that\n(1-b)(1-w) p{Bia-e+}>1- .\nb(l-kw)\nFurther, when k is small and b is near 1/2, this limit is approximately equal to w.\nTo summarize, it is a good approximation that the be lief surface, and hence any CICI distribution, can be specified by limits to the minimum and maximum val ues of the surface, which imply the conditional prob abilities of the parent nodes at different states of ev idence. These probabilities lead directly to estimates\n16 Agosta\nof the symmetric CICI likelihood parameters; k ap proaching the \"independent edge\" conditional proba bility, and w approaching the \"positive exclusion\" con ditional probability. The remaining vertex, the \"nega tive exclusion\" conditional probability closely approx imates the parent's prior. The error in the approxima tion is second order in k and 1 - w, and the approxi mation becomes exact as k --+ 0 and w --+ 1.\n3 DISCUSSION\nA major finding of this paper is that the CICI effect of evidence is secondary to its diagnostic effects. Thus the relative effect between hypotheses-call it the ob served exclusion-is also a consequence of the degree of direct support for the hypotheses as much as it is affected by the partial exclusion controled by the noisy-or parameter, w. The more that two related hy potheses have direct support, the less that secondary inter-causal effects appear. Thus the refutational ef fect of w on a hypothesis due to conflicting hypotheses decreases as other support for the hypothesis increases.\nThis paper has explored the properties of CICI evi dence nodes. The properties are two: First, when it is certain that the evidence is absent, e.g. at e-, the CICI node leaves dependencies among the hypothe sis set unchanged. For hypotheses that are otherwise independent, this reduces the connectivity of the net work, and thus simplifies the complexity of the prob ability updating algorithm. Secondly, at the other ex treme when the evidence, e+, is present, the CICI node generates partial exclusion (or collaboration) among the set, in the sense that the exclusion can not de crease other evidential support, only increase support in the lack of other evidence.\nThere are several consequences of building a network of nodes with these properties. First, the conditional independence property implies the exclusion property, so we either accept both, or neither. It is a general property of common evidence nodes, not only CICI nodes, that shared evidence generates dependencies among hypotheses; and we have seen that we cannot have independence among the existence of hypotheses for all states of evidence. As a consequence, it is proba bilistically inconsistent to treat common evidence sep arately, inferring each hypothesis independently. This can be summed up in the phrase \"ambiguity implies conflict,\" meaning that alternate, competing explana tions must probabilistically exclude each other. Con versely, they could also be collaborating explanations that become coupled by common evidence. What is not possible is for two perfectly good explanations of a common effect to be probabilistically independent of each other for all states of the evidence.\nMultiple parent nodes are the elements from which to build networks of multiply connected hypotheses. This technique is similar to other \"constraint propa-\ngation networks\" of hypotheses where typically inter hypothesis constraints are expressed without interven ing nodes. Constraint networks typically can prop agate a small change through all nodes in a network, because of their similar properties to sets of simultane ous equations. In comparision, inter-causal constraints tend to have a quickly attentuated effect among chains of nodes, since the percent change diminishes from a node to its neighbor. Inter-causal constraints are best thought of as resulting in a secondary set of effects that tend to increase the discrimination of diagnostic inference among hypotheses.\nAcknowledgements\nMy grateful acknowledgements to Tom Binford, l\\1ax Henrion, Harold Lehmann, Gregory Provan, Ross Shachter and Mike Wellman for their comments and suggestions. Also to Wally Mann and Margaret Miller for help with the figures.\nReferences\nAgosta, J. M., \"The structure of Bayes networks for visual recognition,\" Proc. 4th conf. on uncertainty in A.I., 1988, pp.1 -8.\n__ , Probabilistic recognition networks, an applica tion of influence diagrams to visual recognition, PhD Thesis, submitted April 1991.\nGeiger, D. and D. Heckerman, \"Separable and Transi tive Graphoids\", Proc. of the 6th con f. on uncertainty and A.!., 1990, pp. 538-545.\nGoldman, R. P. and E. Charniak, \"Dynamic construc tion of belief networks,\" Proc. of the 6th Conf in Un certainty and A.!., 1990, pp. 90-97.\nHenrion, M. and M. Druzdzel, \"Qualitative propaga tion and scenario-based approaches to explanation of probabilistic reasoning,\" Proc. of the 6th Conf on un certainty and A.I., 1990, pp. 10-20.\nPearl, J ., Probabilistic Reasoning in Intelligent Sys tems, (San Mateo, CA: Morgan Kaufman:, 1988.)\nWellman, M. P., Formulation of Tradeoffs in planning under uncertainty,( Massachusetts: Cambridge, MIT Thesis MIT/LCS/TR-427, August 1988.)"
    } ],
    "references" : [ {
      "title" : "The structure of Bayes networks for visual recognition,",
      "author" : [ "J.M. Agosta" ],
      "venue" : "Proc. 4th conf. on uncertainty in A.I.,",
      "citeRegEx" : "Agosta,? \\Q1988\\E",
      "shortCiteRegEx" : "Agosta",
      "year" : 1988
    }, {
      "title" : "Dynamic construc­ tion of belief networks,",
      "author" : [ "R.P. Goldman", "E. Charniak" ],
      "venue" : "Proc. of the 6th Conf in Un­ certainty and A.!.,",
      "citeRegEx" : "Goldman and Charniak,? \\Q1990\\E",
      "shortCiteRegEx" : "Goldman and Charniak",
      "year" : 1990
    }, {
      "title" : "propaga­ tion and scenario-based approaches to explanation of probabilistic reasoning,",
      "author" : [ "M. Henrion", "M. Druzdzel", "\"Qualitative" ],
      "venue" : "Proc. of the 6th Conf on un­ certainty and A.I.,",
      "citeRegEx" : "Henrion et al\\.,? \\Q1990\\E",
      "shortCiteRegEx" : "Henrion et al\\.",
      "year" : 1990
    }, {
      "title" : "Formulation of Tradeoffs in planning under uncertainty,( Massachusetts: Cambridge, MIT Thesis MIT/LCS/TR-427",
      "author" : [ "tems", "(San Mateo", "CA: Morgan Kaufman", "M.P. 1988.) Wellman" ],
      "venue" : null,
      "citeRegEx" : "tems et al\\.,? \\Q1988\\E",
      "shortCiteRegEx" : "tems et al\\.",
      "year" : 1988
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "(Goldman and Charniak 1990] Having multiple explanations denotes the ambiguity due to not having enough information to entirely resolve which hypothe­ sis offers the true explanation.",
      "startOffset" : 0,
      "endOffset" : 27
    } ],
    "year" : 2011,
    "abstractText" : "This paper examines the interdependence generated between two parent nodes with a common instantiated child node, such as two hypotheses sharing common evidence. The relation so generated has been termed \"inter­ causal.\" It is shown by construction that inter-causal independence is possible for bi­ nary distributions at one state of evidence. For such \"CICI\" distributions, the two mea­ sures of inter-causal effect, \"multiplicative synergy\" and \"additive synergy\" are equal. The well known \"noisy-or\" model is an ex­ ample of such a distribution. This introduces novel semantics for the noisy-or, as a model of the degree of conflict among competing hy­ potheses of a common observation. In a general Bayesian network, the relation between a pair of nodes can be predictive, meaning we are inter­ ested in the effect of a node upon its successors, or, oppositely, diagnostic, where we infer the state of a node from knowledge of its successors. \\\\1e can define yet a third relation between nodes that are neither suc­ cessors of each other, but share a common successor. Such a relation has been termed inter-causal. (Henrion and Druzel 1990, p.10] For example, in the simplest di­ agram with this property, nodes A and B in Figure one are inter-causally related to each other by their com­ mon evidence at node e. This relation is a property of the clique formed by \"marrying the parents\" of e, not by the individual effects of the arcs into e. In this paper I derive the quantitative inter-causal properties due to evidence nodes constructed from the noisy-or\" model. The interest in inter-causal relations occurs in the pro­ cess of abduction, that is, reasoning from evidence back to the hypotheses that explain the evidence. This arises in problems of interpretation, where more than one hypothesis may be suggested by a piece of evi­ dence. (Goldman and Charniak 1990] Having multiple explanations denotes the ambiguity due to not having enough information to entirely resolve which hypothe­ sis offers the true explanation. This paper shows how to construct an evidence node that expresses this am­ biguity by the degree of conflict between hypotheses. We apply this elsewhere (Agosta 1991] as a compo­ nent in building a \"recognition network\" where rele­ vant hypotheses are created \"on the fly\" as possible interpretations of the evidence. The implicit relation between A and B due to shared evidence has been extensively explored as the prop­ erty of one hypothesis to \"explain away\" another. These are cases where, given evidence and the asser­ tion of one hypothesis, the other hypothesis can be disqualified as a cause of the evidence. This paper ex­ plores how this dependency induced between hypothe­ ses changes with the evidence. Interestingly, with bi­ nary variables, the induced dependency may vary, and as shown by the noisy-or, disappear for certain states of evidence. Figure 1: The relationship between hypotheses is de­ termined by their common evidence 1 EVIDENCE NODES THAT ARE",
    "creator" : "pdftk 1.41 - www.pdftk.com"
  }
}