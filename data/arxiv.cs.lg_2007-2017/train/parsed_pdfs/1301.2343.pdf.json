{
  "name" : "1301.2343.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Planning by Prioritized Sweeping with Small Backups",
    "authors" : [ "Harm van Seijen" ],
    "emails" : [ "harm.vanseijen@ualberta.ca", "sutton@cs.ualberta.ca" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n30 1.\n23 43\nv1 [\ncs .A\nI] 1\n0 Ja\nn 20\n13"
    }, {
      "heading" : "1. Introduction",
      "text" : "In reinforcement learning (RL) (Kaelbling et al., 1996; Sutton & Barto, 1998), an agent seeks an optimal control policy for a sequential decision problem in an initially unknown environment. The environment provides feedback on the agent’s behavior in the form of a reward signal. The agent’s goal is to maximize the expected return, which is the discounted sum of rewards over future timesteps. An important performance measure in RL is the sample efficiency, which refers to the number of environment interactions that is required to obtain a good policy.\nMany solution strategies improve the policy by iteratively improving a state-value or action-value function, which provide estimates of the expected return under a given policy for (environment) states or state-action\npairs, respectively. Different approaches for updating these value functions exist. In terms of sample efficiency, one of the most effective approaches is to estimate the environment model using observed samples and to compute, at each time step, the (action-)value function that is optimal with respect to the model estimate using planning techniques. A popular planning technique used for this is value iteration (VI) (Sutton, 1988; Watkins, 1989), which performs sweeps of backups through the state or state-action space, until the (action-)value function has converged.\nA drawback of using VI is that it is computationally very expensive, making it infeasible for many practical applications. Fortunately, efficient approximations can be obtained by limiting the number of backups that is performed per timestep. A very effective approximation strategy is prioritized sweeping (Moore & Atkeson, 1993; Peng & Williams, 1993), which prioritizes backups that are expected to cause large value changes. This paper introduces a new backup that enables a dramatic improvement in the efficiency of prioritized sweeping.\nThe main idea behind this new backup is as following. Consider that we are interested in some estimate A that is constructed from a sum of other estimates Xi. The estimate A can be computed using a full backup:\nA ← ∑\ni\nXi .\nIf the estimates Xi are updated, A can be recomputed by redoing the above backup. Alternatively, if we know that only Xj received a significant value change, we might want to update A for only Xj. Let us indicate the old value ofXj , used to construct the current value of A, as xj . A can then be updated by subtracting this old value and adding the new value:\nA ← A− xj +Xj .\nThis kind of backup, which we call a small backup, is computationally cheaper than the full backup. The\ntrade-off is that, in general, more memory is required for storing the estimates xi associated with A. In planning, where the X estimates correspond to state-value estimates and A corresponds to a state or state-action estimate, this is not a serious restriction, because a full model is stored already. The additional memory required has the same order of complexity as the memory required for storage of the model.\nThe core advantage of small backups over full backups is that they enable finer control over the planning process. This allows for more effective update strategies, resulting in improved trade-offs between computation time and quality of approximation of the VI solution (and hence sample efficiency). We demonstrate this empirically by showing that a prioritized sweeping implementation based on small backups yields a substantial performance improvement over the two classical implementations (Moore & Atkeson, 1993; Peng & Williams, 1993).\nIn addition, we demonstrate the relevance of small backups in domains with severe constraints on computation time, by showing that a method that performs one small backup per time step has an equal computation time complexity as TD(0), the classical method that performs one sample backup per timestep. Since sample backups introduce sampling variance, they require a step-size parameter to be tuned for optimal performance. Small backups, on the other hand, do not introduce sampling variance, allowing for a parameter-free implementation. We empirically demonstrate that the performance of a method that performs one small backup per time step is similar to the optimal performance of TD(0), achieved by carefully tuning the step-size parameter."
    }, {
      "heading" : "2. Reinforcement Learning Framework",
      "text" : "RL problems are often formalized as Markov decision processes (MDPs), which can be described as tuples 〈S,A,P ,R, γ〉 consisting of S, the set of all states; A, the set of all actions; Ps ′\nsa = Pr(s ′|s, a), the transition\nprobability from state s ∈ S to state s′ when action a ∈ A is taken; Rsa = E{r|s, a}, the reward function giving the expected reward r when action a is taken in state s; and γ, the discount factor controlling the weight of future rewards versus that of the immediate reward.\nActions are selected at discrete timesteps t = 0, 1, 2, ... according to a policy π : S ×A → [0, 1], which defines for each action the selection probability conditioned on the state. In general, the goal of RL is to improve the policy in order to increase the return G, which is\nthe discounted cumulative reward\nGt = rt+1 + γ rt+2 + γ 2 rt+3 + ... =\n∞ ∑\nk=1\nγk−1 rt+k ,\nwhere rt+1 is the reward received after taking action at in state st at timestep t.\nThe prediction task consists of determining the value function V π(s), which gives the expected return when policy π is followed, starting from state s. V π(s) can be found by making use of the Bellman equations for state values, which state the following:\nV π(s) = Rs + γ ∑\ns′\nPs ′ s V π(s′) , (1)\nwhere Rs= ∑ a π(s, a)Rsa and P s′ s = ∑ a π(s, a)P s′ sa.\nModel-based methods use samples to update estimates of the transition probabilities, P̂s ′\ns , and reward function, R̂s. With these estimates, they can iteratively improve an estimate V of V π, by performing full backups, derived from Equation (1):\nV (s) ← R̂s + γ ∑\ns′\nP̂s ′\ns V (s ′) . (2)\nIn the control task, methods often aim to find the optimal policy π∗, which maximizes the expected return. This policy is the greedy policy with respect to the optimal action-value function Q∗(s, a), which gives the expected return when taking action a in state s, and following π∗ thereafter. This function is the solution to the Bellman optimality equation for action-values:\nQ∗(s, a) = Rsa + γ ∑\ns′\nPs ′\nsa max a′ Q∗(s′, a′) . (3)\nThe optimal value function is related to the optimal action-value function through: V ∗(s) = maxa Q ∗(s, a).\nModel-based methods can iteratively improve estimatesQ of Q∗ by performing full backups derived from Equation (3):\nQ(s, a) ← R̂sa + γ ∑\ns′\nP̂s ′\nsa max a′ Q(s′, a′) , (4)\nwhere R̂sa and P̂ s′ sa are estimates of Rsa and P s′\nsa, respectively.\nModel-free methods do not maintain an model estimate, but update a value function directly from samples. A classical example of a sample backup, based on sample (s, r, s′) is the TD(0) backup:\nV (s) ← V (s) + α (r + γV (s′)− V (s)) , (5)\nwhere α is the step-size parameter."
    }, {
      "heading" : "3. Small Backup",
      "text" : "This section introduces the small backup. We start with small state-value backups for the prediction task. Section 3.3 discusses small action-value backups for the control task."
    }, {
      "heading" : "3.1. Value Backups",
      "text" : "In this section, we introduce a small backup version of the full backup for prediction (backup 2). In the introduction, we showed that a small backup requires storage of the component values that make up the current value of a variable. In the case of a small value backup, the component values correspond to the values of successor states. We indicate these values by the function Us : S × S → IR. So, Us(s\n′) is the value estimate of s′ associated with s.\nUsing Us, V (s) can be updated with only the current value of a single successor state, s′, as demonstrated by the following theorem. The three backups shown in the theorem form together the small backup.\nTheorem 3.1 If the current relation between V (s) and Us is given by\nV (s) = R̂s + γ ∑\ns′′\nP̂s ′′\ns Us(s ′′) , (6)\nthen, after performing the following backups:\ntmp ← V (s′) (7)\nV (s) ← V (s) + γPs ′\ns [V (s ′)− Us(s ′)] (8)\nUs(s ′) ← tmp , (9)\nrelation (6) still holds, but Us(s ′) is updated to V (s′).\nProof Backup (8) subtract the component in relation (6) corresponding to s′ from V (s) and adds a new component based on the current value estimate of s′:\nV (s) ← V (s)− γP̂s ′ s Us(s ′) + γP̂s ′ s V (s ′) .\nHence, relation (6) is maintained, while Us(s ′) is updated. Note that V (s′) needs to be stored in a temporary variable, since backup (8) can alter the value of V (s′) if s′ = s."
    }, {
      "heading" : "3.2. Value Correction after Model Update",
      "text" : "Theorem 3.1 relies on relation (6) to hold. If the model gets updated, this relation now longer holds. In this section, we discuss how to restore relation (6) in a computation-efficient way for the commonly used model estimate:\nP̂s ′ s ← N s′ s /Ns (10)\nR̂s ← R sum s /Ns , (11)\nwhere Ns counts the number of times state s is visited, Ns ′\ns counts the number of times s ′ is observed as\nsuccessor state of s, and Rsums is the sum of observed rewards for s.\nTheorem 3.2 If currently, the following relation holds:\nV (s) = R̂s + γ ∑\ns′′\nP̂s ′′\ns Us(s ′′) ,\nand a sample (s, r, s′) is observed, then, after performing the backups:\nNs ← Ns + 1; N s′ s ← N s′ s + 1 (12) V (s) ← [ V (s)(Ns − 1) + r + γUs(s ′) ] /Ns . (13)\nthe relation still holds, but with updated values for R̂s and P̂s ′′\ns .\nProof (sketch) Backup (13) updates V (s) by computing a weighted average of V (s) and r + γUs(s\n′). The value change this causes is the same as the value change caused by updating the model and then performing a full backup of s based on Us.\nAlgorithm 1 shows pseudo-code for a general class of prediction methods based on small backups. Surpisngly, while it is a planning method, R̂s is never explicitly computed, saving time and memory. Note that the computation per time step is fully independent of the number of successor states. Members of this class need to specify the number of iterations (line 8) as well as a strategy for selecting state-successor pairs (line 9).\nAlgorithm 1 Prediction with Small Backups\n1: initialize V (s) arbitrarily for all s 2: initialize Us(s ′) = V (s′) for all s, s′ 3: initialize Ns, N s′ s to 0 for all s, s ′ 4: loop {over timesteps} 5: observe transition (s, r, s′) 6: Ns ← Ns + 1; N s′ s ← N s′ s + 1 7: V (s) ← [ V (s)(Ns − 1) + r + γ Us(s ′) ] /Ns\n8: loop {for a number of iterations} 9: select a pair (s̄, s̄′) with N s̄ ′\ns̄ > 0 10: tmp ← V (s̄′) 11: V (s̄) ← V (s̄) + γN s̄ ′\ns̄ /Ns̄ · [V (s̄ ′)− Us̄(s̄ ′)]"
    }, {
      "heading" : "12: Us̄(s̄",
      "text" : "′) ← tmp 13: end loop 14: end loop"
    }, {
      "heading" : "3.3. Action-value Backups",
      "text" : "Before we can discuss small action-value backups, we have to discuss a more efficient implementation of the\nfull action-value backup. Backup (4) has a computation time complexity of O(|S||A|). A more efficient implementation can be obtained by storing statevalues, besides action-values, according to V (s) = maxa Q(s, a). Backup (4) can then be implemented by:\nQ(s, a) ← R̂sa + γ ∑\ns′\nV (s′) (14)\nV (s) ← max a′ Q(s, a) . (15)\nThe combined computation time of these backups is O(|S| + |A|), a considerable reduction.\nBackup (14) is similar in form as the prediction backup. Hence, we can make a small backup version of it similar to the one in the prediction case. The theorems below are the control versions of the theorems for the prediction case. They can be proven in a similar way as the prediction theorems.\nTheorem 3.3 If the current relation between Q(s, a) and Usa is given by\nQ(s, a) ← R̂sa + γP s′′\nsa\n∑\ns′′\nUsa(s ′′) , (16)\nthen, performing the following backups:\nQ(s, a) ← Q(s, a) + γPs ′\nsa[V (s ′)− Usa(s ′)]\nUsa(s ′) ← V (s′) ,\nmaintains this relation while updating Usa(s ′) to V (s′).\nTheorem 3.4 If relation (16) holds and a sample (s, a, r, s′) is observed, then, after performing backups\nNsa ← Nsa + 1; N s′ sa ← N s′ sa + 1\nQ(s, a) ← [ Q(s, a)(Nsa − 1) + r + γUsa(s ′) ] /Nsa ,\nrelation (16) still holds, but with updated values for R̂sa and P̂ s′′ sa .\nA small action-value backup is a finer-grained version of backup (14): performing a small backup of Q(s, a) for each successor state is equivalent (in computation time complexity and effect) as performing backup (14) once. While in principle, backup (15) can be performed after each small backup, it is not very efficient to do so, since small backups make many small changes. More efficient planning can be obtained when backup (15) is performed only once in a while.\nIn Section 4, we discuss an implementation of prioritized sweeping based on small action-value backups."
    }, {
      "heading" : "3.4. Small Backups versus Sample Backups",
      "text" : "A small backup has in common with a sample backup that both update a state value based on the current value of only one of the successor states. In addition, they share the same computation time complexity and their effect is in general smaller than that of a full backup.\nA disadvantage of a sample backup, with respect to a small backup, is that it introduces sampling variance, caused by a stochastic environment. This requires the use of a step-size parameter to enable averaging over successor states (and rewards). A small backup does not introduce sampling variance, since it is implicitly based on an expectation over successor states. Hence, it does not require tuning of a step-size parameter for optimal performance.\nA second disadvantage of a sample backup is that it affects the perceived distribution over action outcomes, which places some restrictions on reusing samples. For example, a model-free technique like experience replay (Lin, 1992), which stores experience samples in order to replay them at a later time, can introduce bias, which reduces performance, if some samples are replayed more often than others. For small backups this does not hold, since the process of learning the model is independent from the backups based on the model. This allows small backups to be combined with effective selection strategies like prioritized sweeping."
    }, {
      "heading" : "4. Prioritized Sweeping with Small Backups",
      "text" : "Prioritized sweeping (PS) makes the planning step of model-based RL more efficient by using a heuristic (a ‘priority’) for selecting backups that favours backups that are expected to cause a large value change. A priority queue is maintained that determines which values are next in line for receiving backups.\nThere are two main implementations: one by Moore & Atkeson (1993) and one by Peng & Williams (1993) 1. All PS methods have in common that they perform backups in what we call update cycles. By adjusting the number of update cycles that is performed per time step, the computation time per time step can be controlled. Below, we discuss in detail what occurs in an update cycle for the two classical PS implementations.\n1We refer to the version of ‘queue-Dyna’ for stochastic domains, which is different from the version for deterministic domains."
    }, {
      "heading" : "4.1. Classical Prioritized Sweeping Implementations",
      "text" : "In the Moore & Atkeson implementation the elements in the queue are states and the backups are full value backups. In control, a full value backup is different from backup (2). Instead, it is equivalent (in effect and computation time) to performing backup (14) for each action, followed by backup (15). Hence, the associated computation time has complexity O(|S||A| + |A|).\nAn update cycle consists of the following steps. First, the top state is removed from the queue, and receives a full value backup. Let s bet the top state and ∆Vs the value change caused by the backup. Then, for all predecessor state-action pairs (s̄, ā) a priority p is computed, using:\np ← Pss̄ā · |∆Vs| . (17)\nIf s̄ is not yet on the queue, then it is added with priority p. If s̄ is on the queue already, but its current priority is smaller than p, then the priority of s̄ is upgraded to p.\nThe Peng & Williams implementation differs from the Moore & Atkeson implementation in that the backup is not a full value backup. Instead, it is a backup with the same effect as a small action-value backup, but with a computational complexity of O(|S| + |A|). So, it is a cheaper backup than a full backup, but its value change is (much) smaller. The backup requires a state-action-successor triple. Hence, these triples are the elements on the queue. Predecessors are added to the queue with a priorities that estimate the actionvalue change."
    }, {
      "heading" : "4.2. Small Backup implementation",
      "text" : "A natural small backup implementation might appear to be an implementation similar to that of Peng & Williams, but with the main backup implemented more efficiently. The low computational cost of a small backup, however, allows for a much more powerful implementation. The pseudo-code of this implementation is shown in Algorithm 2. Below, we discuss some key characteristics of the algorithm.\nThe computation time of a small backup is so low, that it is comparable to the priority computation in the classical PS implementations. Therefore, instead of computing a priority for each predecessor and performing a backup for the element with the highest priority in the next update cycle, we can perform a small backup for all predecessors. This raises the question of what to put in the priority queue and what type of backup to perform for the top element. The natural\nanswer is to put states in the priority queue and to perform backup (15) for the top state.\nThe priority associated with a state is based on the change in action-value that has occurred due to small backups, since the last value backup. This priority assures that states with a large discrepancy between the state value and action-values, receive a value backup first.\nOne surprising aspect of the algorithm is that it does not use the function Usa, which forms an essential part of small action-value backups. The reason is that due to the specific backup strategy used by the algorithm, Usa(s\n′) is equal to V (s′) for all state-action pairs (s, a) and all successor states s′. Hence, instead of using Usa, V can be used, saving memory and simplifying the code.\nTable 1 shows the computation time complexity of an update cycle for the different PS implementations. The small backup implementation is the cheapest one among the three."
    }, {
      "heading" : "5. Experimental Results",
      "text" : "In this section, we evaluate the performance of a minimal version of Algorithm 1, as well as the performance of Algorithm 2."
    }, {
      "heading" : "5.1. Small backup versus Sample backup",
      "text" : "We compare the performance of TD(0), which performs one sample backup per time step, with a version of Algorithm 1 that performs one small backup per time step. Specifically, its number of iterations (line 8) is 1, and the selected state-successor pair (line 9) is the pair corresponding to the most recent transition.\nTheir performance is compared on two evaluation tasks, both consisting of 10 states, laid out in a circle. State transitions only occur between neighbours. The transition probabilities for both tasks are generated by a random process. Specifically, the transition probability to a neighbour state is generated by a random number between 0 and 1 and normalized such that\nAlgorithm 2 Prioritized Sweeping with Small Backups\n1: initialize V (s) arbitrarily for all s 2: initialize Q(s, a) = Qprev(s, a) = V (s) for all s, a 3: initialize Nsa, N s′ sa to 0 for all s, a, s ′ 4: loop {over episodes} 5: initialize s 6: repeat {for each step in the episode} 7: select action a, based on Q(s, ·) 8: take action a, observe r and s′ 9: Nsa ← Nsa + 1; N s′ sa ← N s′\nsa + 1 10: Q(s, a) ← [ Q(s, a)(Nsa−1)+r+γV (s ′) ] /Nsa\n11: p ← |Q(s, a)−Qprev(s, a)| 12: if s not on queue or p > current priority s, then promote s̄ to p 13: for a number of update cycles do 14: remove top state s̄′ from queue 15: for all b: Qprev(s̄\n′, b) ← Q(s̄′, b) 16: tmp ← V (s̄′) 17: V (s̄′) ← maxb Q(s̄\n′, b) 18: ∆V ← V (s̄′)− tmp 19: for all (s̄, ā) pairs with N s̄ ′\ns̄ā > 0 do 20: Q(s̄, ā) ← Q(s̄, ā) + γN s̄ ′\ns̄ā/Ns̄ā ·∆V 21: p ← |Q(s̄, ā)−Qprev(s̄, ā)| 22: if s̄ not on queue or p > current priority s̄, then promote s̄ to p 23: end for 24: end for 25: s ← s′ 26: until s is terminal 27: end loop\nthe sum of the transition probabilities to the left and right neighbour is 1. The reward for counter-clockwise transitions is always +1. The reward for clockwise transitions is different for the two tasks. In the first task, a clockwise transition results in a reward of -1; in the second task, it results in a reward of +1. The discount factor γ is 0.95 and the initial state values are 0.\nFor TD(0), we performed experiments with a constant step-size for values between 0 and 1 with steps of 0.02. In addition, we performed experiments with a decaying, state-dependent step-size, according to\nα(s) = 1\nd · (Ns − 1) + 1 , (18)\nwhere Ns is the number of times state s has been visited, and d specifies the decay rate. We used values of d between 0 and 1 with steps of 0.02. Note that for d = 0, α(s) = 1, and for d = 1, α(s) = 1/Ns.\nEach time a transition is observed and the corresponding backup is performed, the root-mean squared (RMS) error over all states is determined. The average RMS error over the first 10.000 transitions, normalized with the initial error, determines the performance. Figure 1 shows this performance, averaged over 100 runs. The standard error is negligible: the maximum standard error in the first task was 0.0057 (after normalization) and in the second task 0.0007. Note that the performance for d = 0 is equal to the performance for α = 1, as it should, by definition. The normalized performance for α = 0 is 1, since no learning occurs in this case.\nThese experiments demonstrate three things. First, the optimal step-size can vary a lot between different tasks. Second, selecting a sub-optimal step-size can cause large performance drops. Third, a small-backup, which is parameter-free, has a performance similar to the performance of TD(0) with optimized step-size. Since the computational complexity is the same, the small backup is a very interesting alternative to the sample backup in domains with tight constraints on the computation time, where previously only sample backups where viable. Keep in mind that a sample backup does require a model estimate, so if there are also tight constraints on the memory, a sample backup might still be the only option."
    }, {
      "heading" : "5.2. Prioritized Sweeping",
      "text" : "We compare the performance of prioritized sweeping with small backups (Algorithm 2) with the two classical implementations of Moore&Atkeson and Peng&Williams on the maze task depicted in the top of Figure 2. The reward received at each time step is -1 and the discount factor is 0.99. The agent can take four actions, corresponding to the four compass directions, which stochastically move the agent to a different square. The bottom of Figure 2 shows the relative action outcomes of a ‘north’ action. In free space, an action can result in 15 possible successor states, each with equal probability. When the agent is close to a wall, this number decreases.\nTo obtain an upper bound on the performance, we also compared against a method that performs value iteration (until convergence) at each time step, using the most recent model estimate.\nAs exploration strategy, the agent select with 5% probability a random action, instead of the greedy one. On top of that, we use the ‘optimism in the face of uncertainty’ principle, as also used by Moore & Atkeson. This means that as long as a state-action pair has not been visited for at least M times, it’s value is defined as\nsome optimistically defined value (0 for our maze task), instead of the value based on the model estimate. We optimized M for the value iteration method, resulting in M = 4, and used this value for all methods.\nWe performed experiments for 1, 3, 5 and 10 update cycles per time step. Figure 3 shows the average return over the first 200 episodes for the different methods. The results are averaged over 100 runs. The maximum standard deviation is 0.1 for all methods, except for the method of Peng & Williams, which had a maximum standard deviation of 1.0.\nThe computation time per update cycle was about the same for the three different PS implementations, with a small advantage for the small backup implementation, which shows that the O(Pre) computation (see Table 1) is dominant in this task. The computation time per observation of the value iteration method was more than 400 times as high as a single update cycle.\nPS with small backups turns out to be very effective. With only a single update cycle, the value-iteration result can be closely approximated, in contrast to the\ntwo classical implementations. The results also show that the Peng & Williams method performs considerably worse than the one of Moore & Atkeson in the considered domain. This can be explained by the different backups they perform. The effect of the backup of Peng & Williams is proportional to the transition probability, which in most cases is 1\n15 . In contrast,\nthe Moore & Atkeson method performs a full backup each update cycle. While the small backup implementation also uses backups that are proportional to the transition probability, it performs a lot more backups per update cycle. Specifically, a number that is proportional to the number of predecessors. In general, this number will increase when the stochasticity of the domain increases."
    }, {
      "heading" : "6. Discussion",
      "text" : "Prioritized sweeping can be viewed as a generalization of the idea of replaying of experience in backward order (Lin, 1992), which by itself is related to eligibility traces (Sutton, 1988; Watkins, 1989; Sutton & Singh, 1994). What all these techniques have in common is that new information (which can be value changes, but at its core all value changes originate from new data) is propagated backwards. Whereas backward replay and eligibility traces use the recent trajectory for backward\npropagation of information, prioritized sweeping uses a model estimate for this. Hence, it propagates new information more broadly.\nWhat gives the performance edge to the small backup implementation is that it implements the principle of backward updating in a cleaner and more efficient way. One update cycle of Algorithm 2 represents, in a way, the ultimate backwards backup: all predecessors are updated with the current value of a chosen state, which is selected because it recently experienced a large value change. In contrast, the other PS implementation place the predecessors in a queue and backup only the state with the highest priority in the next update cycle. On top of that, the computation time per update cycle is lower for the small backup implementation (see Table 1).\nThe new implementation of PS introduced in this paper would be impossible without the new backup. The small backup allows for very targeted updates that are computationally very cheap. This enables finer control over how computation time is spend, which is what drives the new PS implementation."
    }, {
      "heading" : "7. Conclusion",
      "text" : "We demonstrated in this paper that the planning step in model-based reinforcement learning method can be done substantially more efficient by making use of small backups. These backups are finer-grained version of a full backup, which allow for more control over how the available computation time is spend. This\nmakes new, more efficient, update strategies possible. In addition, small backups can be useful in domains with very tight time constraints, offering a parameterfree alternative to sample backups, which were up to now often the only feasible option for such domains."
    } ],
    "references" : [ {
      "title" : "Reinforcement learning: A survey",
      "author" : [ "L.P. Kaelbling", "M.L. Littman", "A.P. Moore" ],
      "venue" : "Journal of Artificial Intelligence Research,",
      "citeRegEx" : "Kaelbling et al\\.,? \\Q1996\\E",
      "shortCiteRegEx" : "Kaelbling et al\\.",
      "year" : 1996
    }, {
      "title" : "Self-improving reactive agents based on reinforcement learning, planning and teaching",
      "author" : [ "L.J. Lin" ],
      "venue" : "Machine learning,",
      "citeRegEx" : "Lin,? \\Q1992\\E",
      "shortCiteRegEx" : "Lin",
      "year" : 1992
    }, {
      "title" : "Prioritized sweeping: Reinforcement learning with less data and less real time",
      "author" : [ "A. Moore", "C. Atkeson" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Moore and Atkeson,? \\Q1993\\E",
      "shortCiteRegEx" : "Moore and Atkeson",
      "year" : 1993
    }, {
      "title" : "Efficient learning and planning within the dyna framework",
      "author" : [ "J. Peng", "R.J. Williams" ],
      "venue" : "Adaptive Behavior,",
      "citeRegEx" : "Peng and Williams,? \\Q1993\\E",
      "shortCiteRegEx" : "Peng and Williams",
      "year" : 1993
    }, {
      "title" : "Learning to predict by the methods of temporal differences",
      "author" : [ "R.S. Sutton" ],
      "venue" : "Machine learning,",
      "citeRegEx" : "Sutton,? \\Q1988\\E",
      "shortCiteRegEx" : "Sutton",
      "year" : 1988
    }, {
      "title" : "Reinforcement Learning: An Introduction",
      "author" : [ "R.S. Sutton", "A.G. Barto" ],
      "venue" : null,
      "citeRegEx" : "Sutton and Barto,? \\Q1998\\E",
      "shortCiteRegEx" : "Sutton and Barto",
      "year" : 1998
    }, {
      "title" : "On step-size and bias in temporal-difference learning",
      "author" : [ "R.S. Sutton", "S.P. Singh" ],
      "venue" : "In Proceedings of the Eight Yale Workshop on Adaptive and Learning Systems",
      "citeRegEx" : "Sutton and Singh,? \\Q1994\\E",
      "shortCiteRegEx" : "Sutton and Singh",
      "year" : 1994
    }, {
      "title" : "Learning from delayed rewards",
      "author" : [ "C. Watkins" ],
      "venue" : "PhD thesis, King’s College,",
      "citeRegEx" : "Watkins,? \\Q1989\\E",
      "shortCiteRegEx" : "Watkins",
      "year" : 1989
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "In reinforcement learning (RL) (Kaelbling et al., 1996; Sutton & Barto, 1998), an agent seeks an optimal control policy for a sequential decision problem in an initially unknown environment.",
      "startOffset" : 31,
      "endOffset" : 77
    }, {
      "referenceID" : 4,
      "context" : "A popular planning technique used for this is value iteration (VI) (Sutton, 1988; Watkins, 1989), which performs sweeps of backups through the state or state-action space, until the (action-)value function has converged.",
      "startOffset" : 67,
      "endOffset" : 96
    }, {
      "referenceID" : 7,
      "context" : "A popular planning technique used for this is value iteration (VI) (Sutton, 1988; Watkins, 1989), which performs sweeps of backups through the state or state-action space, until the (action-)value function has converged.",
      "startOffset" : 67,
      "endOffset" : 96
    }, {
      "referenceID" : 1,
      "context" : "For example, a model-free technique like experience replay (Lin, 1992), which stores experience samples in order to replay them at a later time, can introduce bias, which reduces performance, if some samples are replayed more often than others.",
      "startOffset" : 59,
      "endOffset" : 70
    }, {
      "referenceID" : 1,
      "context" : "Prioritized sweeping can be viewed as a generalization of the idea of replaying of experience in backward order (Lin, 1992), which by itself is related to eligibility traces (Sutton, 1988; Watkins, 1989; Sutton & Singh, 1994).",
      "startOffset" : 112,
      "endOffset" : 123
    }, {
      "referenceID" : 4,
      "context" : "Prioritized sweeping can be viewed as a generalization of the idea of replaying of experience in backward order (Lin, 1992), which by itself is related to eligibility traces (Sutton, 1988; Watkins, 1989; Sutton & Singh, 1994).",
      "startOffset" : 174,
      "endOffset" : 225
    }, {
      "referenceID" : 7,
      "context" : "Prioritized sweeping can be viewed as a generalization of the idea of replaying of experience in backward order (Lin, 1992), which by itself is related to eligibility traces (Sutton, 1988; Watkins, 1989; Sutton & Singh, 1994).",
      "startOffset" : 174,
      "endOffset" : 225
    } ],
    "year" : 2013,
    "abstractText" : "Efficient planning plays a crucial role in model-based reinforcement learning. Traditionally, the main planning operation is a full backup based on the current estimates of the successor states. Consequently, its computation time is proportional to the number of successor states. In this paper, we introduce a new planning backup that uses only the current value of a single successor state and has a computation time independent of the number of successor states. This new backup, which we call a small backup, opens the door to a new class of model-based reinforcement learning methods that exhibit much finer control over their planning process than traditional methods. We empirically demonstrate that this increased flexibility allows for more efficient planning by showing that an implementation of prioritized sweeping based on small backups achieves a substantial performance improvement over classical implementations.",
    "creator" : "LaTeX with hyperref package"
  }
}