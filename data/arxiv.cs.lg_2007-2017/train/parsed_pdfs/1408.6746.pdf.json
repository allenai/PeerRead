{
  "name" : "1408.6746.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Non-Standard Words as Features for Text Categorization",
    "authors" : [ "Slobodan Beliga", "Sanda Martinčić-Ipšić" ],
    "emails" : [ "smarti}@inf.uniri.hr" ],
    "sections" : [ {
      "heading" : null,
      "text" : "texts using Non-Standard Words (NSW) as features. NonStandard Words are: numbers, dates, acronyms, abbreviations, currency, etc. NSWs in Croatian language are determined according to Croatian NSW taxonomy. For the purpose of this research, 390 text documents were collected and formed the SKIPEZ collection with 6 classes: official, literary, informative, popular, educational and scientific. Text categorization experiment was conducted on three different representations of the SKIPEZ collection: in the first representation, the frequencies of NSWs are used as features; in the second representation, the statistic measures of NSWs (variance, coefficient of variation, standard deviation, etc.) are used as features; while the third representation combines the first two feature sets. Naive Bayes, CN2, C4.5, kNN, Classification Trees and Random Forest algorithms were used in text categorization experiments. The best categorization results are achieved using the first feature set (NSW frequencies) with the categorization accuracy of 87%. This suggests that the NSWs should be considered as features in highly inflectional languages, such as Croatian. NSW based features reduce the dimensionality of the feature space without standard lemmatization procedures, and therefore the bag-of-NSWs should be considered for further Croatian texts categorization experiments.\nKeywords: text categorization, non-standard words, collection representation, features, accuracy\nI. INTRODUCTION\nCategorization or classification is tasked to classify a given data instance into a pre-specified set of categories. In the domain of text categorization, the task is called text categorization (TC) – given a set of categories (subjects) and a collection of text documents, the process of finding the correct topics for each document [1]. Typically, texts are represented with vectors of word’s frequencies in the bag-of-words model. The problem with this approach is the dimensionality of the features vectors, which is equal to the number of different words in the collection [1, 2].\nOne of the solutions for the dimensionality problem is Feature Selection which can be: heuristic, guided by linguistic and domain knowledge, or statistical [3].\nIn linguistically guided approach, the dimensionality of the features vector is reduced with stop words (a, an, the, of, etc.) omission and lemmatization. Lemmatization\nchanges each inflected word form with its lemma (walked, walks, walking – with lemma walk) and therefore reduces the dimensionality of feature vectors, which is of essence in morphologically rich languages such as Croatian. Some feature selection approaches ignore terms that are too frequent or too rare according to empirically chosen thresholds [3]. As data sets become more complex, heuristics are usually not sufficient. Therefore, another solution is to use measures such as Information Gain and Chi-Squere, to define relevance of each feature [4].\nAnother approach to dimensionality problem is Dimensionality Reduction by Feature Extraction. It implies a much smaller set of synthetic features from the original feature set, using synthetic features rather than naturally occurring words. This will avoid the problems inherited from natural language (polysemy, homonymy, etc.). Using word groups, instead of individual words as features, with a high degree of semantic relatedness, proved to be justified in clustering [1]. Systematic approach - Latent Semantic Indexing (LSI) is based on this principle [1].\nIn this paper we propose opposed approach based on Non-Standard Words (NSWs) as features. Instead of bagof-words, we construct the bag-of-NSWs, and use them to train different text classifiers. The peculiarity of proposed approach is in using NSWs which would usually be omitted from the text during the text cleaning procedures. Non-standard words are: numbers, dates, time, abbreviations, acronyms, currency, measurement units, etc. Text normalization transforms NSWs in their extended form: the abbreviation /dr./ into /doctor/ or acronym /SMS/ into /Short Message Service/.\nThe motivation for this work rises from an idea that discarded characters and numbers are sufficient for discrimination of classification of the text categories, reducing the feature vector dimensionality at the same time [2].\nThe paper is organized as follows: Section II. presents methodology of text normalization and categorization, Section III. presents the data collection, NSW features extraction procedures and training of classification models. The results are presented in the end of Section III. The paper concludes with discussion and some future work considerations.\nThis research is partially supported by the grant 13.13.2.2.07 of University of Rijeka.\nII. METODOLOGY AND RELATED WORK"
    }, {
      "heading" : "A. NSW extraction",
      "text" : "Traditionally, in the field of natural language processing (NLP), normalization is the process of transforming the unit into its normal form (i.e. lemma). Normalization is also the first step in the text preprocessing of Text-to-Speech (TTS) systems. It is performed in the normalization module, which is responsible to identify a NSW token and to transform it into its expanded form [5, 6]. Text Normalization for Croatian Speech Synthesis is described in [7]. In this work, the Croatian NSW taxonomy driven rule based approach was used for NSWs extraction [7, 8].\nModule for normalization initially detects the NSW and separates it from standard words. Then, detected NSW is transformed into its expanded form, suitable for the TTS system. In this experiment, the process of transforming NSW to expanded form is not required. Instead, we detect each NSW and determine its NSW type as is suggested in taxonomy for classification of nonstandard words in Croatian language [8]. In addition, every occurrence of NSW for each NSW type is counted. Our approach identifies NSWs and then normalizes detected NSWs following the taxonomy of Croatian NSWs by combining programmed rules and the lookup dictionary."
    }, {
      "heading" : "B. Text categorization",
      "text" : "Text categorization is tasked with assigning one of predefined classes to a document. Usually text categorization is performed using a bag-of-words model borrowed from information retrieval (IR), and most often based on TF-IDF model [1-3]. Documents are represented as feature vectors, which are, according to the bag-ofwords model, unordered list of words with the frequency of their occurrence in a document. Inverse document frequency is used to model the rareness of terms in a document collection. Feature vectors are used to train the classifier, usually Bayesian classifier, k-nearest neighbor (kNN), classification tree or support vector machines (SVM), etc. [1, 3, 9]. Some of the techniques for the\nvector dimensionality or the feature space reduction are: stop words and NSW removal [10], lemmatization or morphological normalization [11] - NLP based principles; selecting the most discriminative subset of features (document frequency, information gain, mutual information, odds ratio, etc.) – called feature selection [4, 12] - or deriving new smaller set calculated from original features – called feature extraction [13].\nOur approach is presented in Figure 1.: in the first step - we detect all NSWs from the text; in the second step – for detected NSWs we assign its type (number, abbreviation, date, etc.); in the third step – we construct the bag-of-NSWs; in the fourth step – we train the classifiers. Finally, as a result of the text categorization we obtain the texts which are organized as categorized collection of texts – every text is associated with one of the possible categories.\nIII. NSW BASED TEXT CATEGORIZATION"
    }, {
      "heading" : "A. Data (SKIPEZ)",
      "text" : "For the purposes of normalization and categorization tasks we prepared the collection of texts in Croatian language – SKIPEZ (Službeno, Književno, Informativno, Popularno, Edukativno, Znanstveno). SKIPEZ contains 390 texts organized in 6 predefined classes: official, literature, informative, popular, educational and scientific. The distribution of text over classes is balanced so each class contains exactly 65 texts. Texts were selected according to their relevance to the class and according to the % of contained NSWs, because we needed a balance between both aspects in order to train a classifier. SKIPEZ statistics is shown in Table I., and additional details about SKIPEZ are in [14].\nThe content of texts in each subclass was carefully selected, in order to capture as many different and representative text in each class used and contain representative set of NSWs, in order to compile the exhausted list of Croatian NSWs is shown in Table II."
    }, {
      "heading" : "B. NSW Extraction Procedure",
      "text" : "The NSWs were extracted from the SKIPEZ text collection according to the approach presented in [7, 8]. Here, we only detected NSWs types according to this taxonomy, resulting in feature vectors consisting of NSWs types frequencies for each document in the collection. A feature vector for each text document in collection contains 85 values (calculated from NSW types frequencies):\n STRING part - 15 values: ordinal and nominal roman numbers, simple or compound abbreviations, abbreviations without full stop, chemical elements, measurement units, currency an monetary units, acronyms, inflected and special acronyms, symbols, emoticons and suffixes.\n NUMBER part - 21 values: numeric format date, time period, time, proportion, dimensions, short references, short and long telephone number, interval, positive and negative decimal numbers, decimal interval, positive and negative fractions, exponents, ordinal and nominal numbers, ordinal and nominal intervals, positive and negative nominal numbers, ISDN/UDK numbers.\n COMBINED part - 20 values: 3 different date formats, geographical coordinates, long references, biblical references, enumerations of tables, figures and titles, road designation, TV show label, traffic abbreviations, compound measurement units, vitamins, house numbers, chemical and mathematical formulas, program code, registration number, argo-emoticons, domain specific NSWs, and NSWs of unknown type.\n DERIVED part - 29 values: total of NSWs in the number class, total of NSWs in the string class, total of NSWs in combined class, total of NSWs in text, total of words in text, number of different NSW types, number of empty NSW types, ratio between filled features and subclasses, ratio between filled and empty NSW types, coefficient as number of NSW divided with number of words in text. Moreover, there are also features\nThe first representation of SKIPEZ collection is a feature set composed from NSW type frequencies. For each document the features vector contains 85 values (as listed above).\nThe second representation of text, based on the NSW frequencies, represents the base from which the values of the features are derived. In this case, the statistics serves as a tool for obtaining metadata from the previously calculated frequency of NSW occurrence. For each element of the population, therefore each vector, we calculate some of its numerical characteristics called statistical features [15, 16]. The main idea when choosing a feature vector is utilizing the basic statistical features, which can describe the dispersion of identified patterns of NSW in a particular text.\nThe statistical characteristics of each population (each vector) were counted. We observed the vector elements as a sequence of elements and determined the individual values of absolute and relative measures of dispersion: the arithmetic mean, the range of variation, standard deviation, variance, coefficient of variation, and also a measure of flatness and a measure of asymmetry. We then calculated the statistical characteristics exclusively for superclasses (string, number, and string + number) – higher classes from taxonomy (Figure 2.). This second group of calculations observes dispersion patterns within the superclass, i.e. numerically describes the statistical characteristics of the NSW dispersion in three superclasses. We calculated the following characteristics: the arithmetic mean of superclass, interquartile q3, interquartile q1, interquartile Iq, coefficient of quartile deviation and coefficient of variation.\nThe third representation of texts is formed as a union from the features of the first and second representations, therefore includes all their values. Through the combination of features, we try to take advantage of the knowledge that was previously extracted from all the texts, on the principle \"the more, the better\".\nIn experiments with all 3 feature sets we conduct an iterative process of adding the created vectors for each text, we gradually form a bag-of-NSWs model (term-bydocument matrix) - see Figure 1. Since we used supervised machine learning, during the bag-of-NSWs construction to each feature vector we assign its actual category (see Table II.)."
    }, {
      "heading" : "C. Categorization Results",
      "text" : "We trained the Naive Bayes, Classification Tree, kNN, CN2, C4.5, and Random Forest using the Orange [17] toolkit. As the measure of classification performance we used accuracy (1) [9].\n(1)\nwhere a correct classification means that the learned model predicts the same class as the original class of the test case.\nThe results of trained classifiers (Naïve Bayes, Classification Tree, kNN, CN2, C4.5 and Random Fortest) are obtained for three different feature representations using 5 fold cross-validation. The text categorization results using the first feature set - NSW frequencies are shown in Table III.; using statistical features in Table IV. and for combination of features in Table V.\nThe experimental results can be analyzed twofold: by accuracy of different classifiers and by accuracy with respect to different types of representations in collection -\nFigure 3. Random Forest achieves the highest accuracy and Naïve Bayes the lowest in all representation of the collection. The accuracy of all classifiers with statistical features is generally worse than the frequency features\nHe was driving 120 km/h.\nNSW EXTRACTION BASED ON NSW TAXONOMY FOR CROATIAN LANGUAGE\n+ char.\nsymbol abbrev.\ncurrencypennies units chem.\nel.\nkn Fe\nEUR\nsimple complex\nt km/h\ncomplex simple\n=\ndate 0 time 0 proportion 0 tel. number 0\nordinal num. 0 nominal num. 1 Roman num. 0\nISDN/UDK 0 … … abbreviations 0 acronyms 0\nsymbols 0 … emoticons 0 measurement units 1\n… math formulas 0\n…\nfeature vector\nFigure 2. Forming a feature vector in Bag-of-NSWs\n(Table IV.). With statistical features, Random Forest achieved a slightly higher accuracy (increase less than 1%). Chakrabarti, Yang and Pedersen in [4] state that numerous techniques of selection of features improve the performance of the classifier only marginally. This was confirmed also with our experiment, although the selection of features used non-standard features of the vector.\nExperimental results can be analyzed according to text categories as well. Comparison of text categorization accuracy, only using Random Forest, is presented in Table VI. The highest accuracy was achieved for the class of informative texts, while the worst was achieved in the class of scientific and educational texts. The obtained results of classification leave room for discussion about the possibility of unification or reduction of scientific and educational domains because in this 2 categories is the highest number of misclassified texts. The science and education are closely related and they share the division of the subareas, fields and branches.\nThe worst scores for educational and scientific domain can be explained as an error that is probably caused by the fact that the educational and scientific texts are equally distributed in the scientific and artistic areas, fields and branches as required by the National Council for Science, and for Republic of Croatia Ministry of Science, Education and Sports, available at [18]. On the basis of this criterion texts are selected for SKIPEZ collection. Classifier failed to recognize the scientific style of writing. Encyclopedic texts, journals, articles and dissertations are apparently very similar to school textbooks, teaching materials and manuals through the NSW perspective.\nIV. DISCUSSION AND CONCLUSION\nThis paper presents categorization of Croatian texts using Non-Standard Words (NSW) as features. Results suggest that non-standard word forms can be used as features for representation of texts in text categorization. The associated NSW taxonomy is suitable form of background knowledge on the basis of which NSW can be automatically extracted from texts. It has been shown that the NSWs carry enough information about the nature of text, which is suitable for further classification. With this approach, it is possible to significantly reduce the dimensionality of feature vectors (space) and at the same time achieve good text categorization results. Using NSWs feature, the feature vectors have several times lower dimensions then the original ones and this way reduces the problem of sparse data.\nBag-of-NSWs approach is suitable for inflectional languages because extremely reduced dimensionality (only 85 features) which can avoid the procedure of lemmatization of words or morphological normalization.\nObtain results should be compared with standard TFIDF classification method. Since the literature presents the support vector machine as successful, it would be appropriate to examine whether a classification using SVM provides better results than the Random Forest algorithm. In further work we plan to extend our work to\nthe field of micro texts classification, since SMS and twitter texts contain a lot of NSWs."
    } ],
    "references" : [ {
      "title" : "The Text Mining Handbook - Advanced Approaches in Analyzing Unstructured Data",
      "author" : [ "R. Feldman", "J. Sanger" ],
      "venue" : "New York: Cambridge University Press",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "An Introduction to Information Retrieval",
      "author" : [ "C.D. Manning", "P. Raghavan", "H. Schütze" ],
      "venue" : "Cambridge University Press",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Mininh the Web - Discovering knowladge from hypertext data",
      "author" : [ "S. Chakrabarti" ],
      "venue" : "Morgan Kaufmann Publishers",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "A comparative study on Feature Selection in Text Categorisation",
      "author" : [ "Y. Yang", "J.O. Pedersen" ],
      "venue" : null,
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1997
    }, {
      "title" : "Normalization of Non-standard Words",
      "author" : [ "R Sproat" ],
      "venue" : "Computer Speech & Language",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2001
    }, {
      "title" : "Lightly Supervised Learning of Text Normalization: Russian Number Names",
      "author" : [ "R. Sproat" ],
      "venue" : "IEEE Workshop on Spoken Language Technology,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2010
    }, {
      "title" : "Text Normalization for Croatian Speech Synthesis",
      "author" : [ "S. Beliga", "S. Martinčić-Ipšić" ],
      "venue" : "Proc. MIPRO junior - Student Papers, 382-387",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "S",
      "author" : [ "S. Beliga", "M. Pobar" ],
      "venue" : "Martinčić-Ipšić, Normalization of Non- Standard Words in Croatian Texts,« u TSD 2011, Plsen",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Web Data Mining - Exploring Hyperlinks",
      "author" : [ "B. Liu" ],
      "venue" : "Contents, and Usage Data, Berlin: Springer",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Text Categorization and Sorting of Web Search Results",
      "author" : [ "M Radovanović", "M. Ivanović", "Z. Budimac" ],
      "venue" : "Computing and Informatics, 28(6), pp. 861–893",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Language morphology offset: Text classification on a Croatian–English parallel corpus",
      "author" : [ "M. Malenica", "T. Šmuc", "J. Šnajder", "B. Dalbelo Bašić" ],
      "venue" : "Information Processing & Management, 44(1), 325-339",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Feature subset selection in text-learning",
      "author" : [ "D. Mladenić" ],
      "venue" : "Machine Learning ECML98.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Word co-occurrence features for text classification",
      "author" : [ "F. Figueiredo at al" ],
      "venue" : "Information Systems,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2011
    }, {
      "title" : "Klasifikacija tekstova temeljem nestandardnih oblika riječi u hrvatskome jeziku",
      "author" : [ "S. Beliga" ],
      "venue" : "Diplomski rad. Sveučilište u Rijeci - Odjel za informatiku",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Vjerojatnost i statistika I i II",
      "author" : [ "N. Sarapa" ],
      "venue" : "Zagreb: Školska knjiga",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 1993
    }, {
      "title" : "Primjenjena statistika",
      "author" : [ "I. Šošić" ],
      "venue" : "Zagreb: Školska knjiga",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2006
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "In the domain of text categorization, the task is called text categorization (TC) – given a set of categories (subjects) and a collection of text documents, the process of finding the correct topics for each document [1].",
      "startOffset" : 217,
      "endOffset" : 220
    }, {
      "referenceID" : 0,
      "context" : "The problem with this approach is the dimensionality of the features vectors, which is equal to the number of different words in the collection [1, 2].",
      "startOffset" : 144,
      "endOffset" : 150
    }, {
      "referenceID" : 1,
      "context" : "The problem with this approach is the dimensionality of the features vectors, which is equal to the number of different words in the collection [1, 2].",
      "startOffset" : 144,
      "endOffset" : 150
    }, {
      "referenceID" : 2,
      "context" : "One of the solutions for the dimensionality problem is Feature Selection which can be: heuristic, guided by linguistic and domain knowledge, or statistical [3].",
      "startOffset" : 156,
      "endOffset" : 159
    }, {
      "referenceID" : 2,
      "context" : "Some feature selection approaches ignore terms that are too frequent or too rare according to empirically chosen thresholds [3].",
      "startOffset" : 124,
      "endOffset" : 127
    }, {
      "referenceID" : 3,
      "context" : "Therefore, another solution is to use measures such as Information Gain and Chi-Squere, to define relevance of each feature [4].",
      "startOffset" : 124,
      "endOffset" : 127
    }, {
      "referenceID" : 0,
      "context" : "Using word groups, instead of individual words as features, with a high degree of semantic relatedness, proved to be justified in clustering [1].",
      "startOffset" : 141,
      "endOffset" : 144
    }, {
      "referenceID" : 0,
      "context" : "Systematic approach - Latent Semantic Indexing (LSI) is based on this principle [1].",
      "startOffset" : 80,
      "endOffset" : 83
    }, {
      "referenceID" : 1,
      "context" : "The motivation for this work rises from an idea that discarded characters and numbers are sufficient for discrimination of classification of the text categories, reducing the feature vector dimensionality at the same time [2].",
      "startOffset" : 222,
      "endOffset" : 225
    }, {
      "referenceID" : 4,
      "context" : "It is performed in the normalization module, which is responsible to identify a NSW token and to transform it into its expanded form [5, 6].",
      "startOffset" : 133,
      "endOffset" : 139
    }, {
      "referenceID" : 5,
      "context" : "It is performed in the normalization module, which is responsible to identify a NSW token and to transform it into its expanded form [5, 6].",
      "startOffset" : 133,
      "endOffset" : 139
    }, {
      "referenceID" : 6,
      "context" : "Text Normalization for Croatian Speech Synthesis is described in [7].",
      "startOffset" : 65,
      "endOffset" : 68
    }, {
      "referenceID" : 6,
      "context" : "In this work, the Croatian NSW taxonomy driven rule based approach was used for NSWs extraction [7, 8].",
      "startOffset" : 96,
      "endOffset" : 102
    }, {
      "referenceID" : 7,
      "context" : "In this work, the Croatian NSW taxonomy driven rule based approach was used for NSWs extraction [7, 8].",
      "startOffset" : 96,
      "endOffset" : 102
    }, {
      "referenceID" : 7,
      "context" : "Instead, we detect each NSW and determine its NSW type as is suggested in taxonomy for classification of nonstandard words in Croatian language [8].",
      "startOffset" : 144,
      "endOffset" : 147
    }, {
      "referenceID" : 0,
      "context" : "Usually text categorization is performed using a bag-of-words model borrowed from information retrieval (IR), and most often based on TF-IDF model [1-3].",
      "startOffset" : 147,
      "endOffset" : 152
    }, {
      "referenceID" : 1,
      "context" : "Usually text categorization is performed using a bag-of-words model borrowed from information retrieval (IR), and most often based on TF-IDF model [1-3].",
      "startOffset" : 147,
      "endOffset" : 152
    }, {
      "referenceID" : 2,
      "context" : "Usually text categorization is performed using a bag-of-words model borrowed from information retrieval (IR), and most often based on TF-IDF model [1-3].",
      "startOffset" : 147,
      "endOffset" : 152
    }, {
      "referenceID" : 0,
      "context" : "[1, 3, 9].",
      "startOffset" : 0,
      "endOffset" : 9
    }, {
      "referenceID" : 2,
      "context" : "[1, 3, 9].",
      "startOffset" : 0,
      "endOffset" : 9
    }, {
      "referenceID" : 8,
      "context" : "[1, 3, 9].",
      "startOffset" : 0,
      "endOffset" : 9
    }, {
      "referenceID" : 9,
      "context" : "Some of the techniques for the vector dimensionality or the feature space reduction are: stop words and NSW removal [10], lemmatization or morphological normalization [11] - NLP based principles; selecting the most discriminative subset of features (document frequency, information gain, mutual information, odds ratio, etc.",
      "startOffset" : 116,
      "endOffset" : 120
    }, {
      "referenceID" : 10,
      "context" : "Some of the techniques for the vector dimensionality or the feature space reduction are: stop words and NSW removal [10], lemmatization or morphological normalization [11] - NLP based principles; selecting the most discriminative subset of features (document frequency, information gain, mutual information, odds ratio, etc.",
      "startOffset" : 167,
      "endOffset" : 171
    }, {
      "referenceID" : 3,
      "context" : ") – called feature selection [4, 12] - or deriving new smaller set calculated from original features – called feature extraction [13].",
      "startOffset" : 29,
      "endOffset" : 36
    }, {
      "referenceID" : 11,
      "context" : ") – called feature selection [4, 12] - or deriving new smaller set calculated from original features – called feature extraction [13].",
      "startOffset" : 29,
      "endOffset" : 36
    }, {
      "referenceID" : 12,
      "context" : ") – called feature selection [4, 12] - or deriving new smaller set calculated from original features – called feature extraction [13].",
      "startOffset" : 129,
      "endOffset" : 133
    }, {
      "referenceID" : 13,
      "context" : ", and additional details about SKIPEZ are in [14].",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 6,
      "context" : "NSW Extraction Procedure The NSWs were extracted from the SKIPEZ text collection according to the approach presented in [7, 8].",
      "startOffset" : 120,
      "endOffset" : 126
    }, {
      "referenceID" : 7,
      "context" : "NSW Extraction Procedure The NSWs were extracted from the SKIPEZ text collection according to the approach presented in [7, 8].",
      "startOffset" : 120,
      "endOffset" : 126
    }, {
      "referenceID" : 14,
      "context" : "For each element of the population, therefore each vector, we calculate some of its numerical characteristics called statistical features [15, 16].",
      "startOffset" : 138,
      "endOffset" : 146
    }, {
      "referenceID" : 15,
      "context" : "For each element of the population, therefore each vector, we calculate some of its numerical characteristics called statistical features [15, 16].",
      "startOffset" : 138,
      "endOffset" : 146
    }, {
      "referenceID" : 8,
      "context" : "As the measure of classification performance we used accuracy (1) [9].",
      "startOffset" : 66,
      "endOffset" : 69
    }, {
      "referenceID" : 3,
      "context" : "Chakrabarti, Yang and Pedersen in [4] state that numerous techniques of selection of features improve the performance of the classifier only marginally.",
      "startOffset" : 34,
      "endOffset" : 37
    } ],
    "year" : 2014,
    "abstractText" : "This paper presents categorization of Croatian texts using Non-Standard Words (NSW) as features. NonStandard Words are: numbers, dates, acronyms, abbreviations, currency, etc. NSWs in Croatian language are determined according to Croatian NSW taxonomy. For the purpose of this research, 390 text documents were collected and formed the SKIPEZ collection with 6 classes: official, literary, informative, popular, educational and scientific. Text categorization experiment was conducted on three different representations of the SKIPEZ collection: in the first representation, the frequencies of NSWs are used as features; in the second representation, the statistic measures of NSWs (variance, coefficient of variation, standard deviation, etc.) are used as features; while the third representation combines the first two feature sets. Naive Bayes, CN2, C4.5, kNN, Classification Trees and Random Forest algorithms were used in text categorization experiments. The best categorization results are achieved using the first feature set (NSW frequencies) with the categorization accuracy of 87%. This suggests that the NSWs should be considered as features in highly inflectional languages, such as Croatian. NSW based features reduce the dimensionality of the feature space without standard lemmatization procedures, and therefore the bag-of-NSWs should be considered for further Croatian texts categorization experiments.",
    "creator" : "Microsoft® Office Word 2007"
  }
}