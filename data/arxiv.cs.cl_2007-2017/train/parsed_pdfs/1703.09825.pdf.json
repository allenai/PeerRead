{
  "name" : "1703.09825.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ "aalhotha@cs.uwaterloo.ca", "jhoey@cs.uwaterloo.ca" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n70 3.\n09 82\n5v 1\n[ cs\n.C L\n] 2\n8 M\nar 2\n01 7\ngraph-based sentiment lexicon induction methods by incorporating distributed and semantic word representations in building the similarity graph to expand a threedimensional sentiment lexicon. We also implemented and evaluated the label propagation using four different word representations and similarity metrics. Our comprehensive evaluation of the four approaches was performed on a single data set, demonstrating that all four methods can generate a significant number of new sentiment assignments with high accuracy. The highest correlations (τ = 0.51) and the lowest error (mean absolute error < 1.1%), obtained by combining both the semantic and the distributional features, outperformed the distributional-based and semantic-based label-propagation models and approached a supervised algorithm."
    }, {
      "heading" : "1 Introduction",
      "text" : "Sentiment analysis (SA) is a rapidly growing area of interest in natural language processing (NLP). Sentiment analysis is useful for a variety of important applications, such as recommendation system, virtual assistants, and health informatics. Much SA relies on lexicons mapping words to sentiment, which are either manually annotated or automatically generated from a small set of seed words. Many researchers and companies have explored methods of expanding and re-generating sentiment lexicons to reduce the cost of manual annotation and to compensate for the lack of existing annotated data and the dynamic and\nfluctuating nature of human emotion. However, most sentiment lexicon expansion methods attach a polarity value (i.e., negative, positive, or neutral) (Stone et al., 1968) or real-valued onedimensional scores (Baccianella et al., 2010) to the words. It is well known; however, that one dimension is insufficient to adequately characterise the complexity of emotion (Fontaine et al., 2007).\nIn a large set of cross-cultural studies in the 1950s, Osgood showed that concepts carried a culturally dependent, shared affective meaning that could be characterised to a great extent using three simple dimensions of evaluation (good versus bad), potency (powerful versus powerless), and activity (lively versus quiet) (Osgood, 1957). This semantic differential scale of evaluation, potency, and activity (EPA) is thought to represent universal and cross-cultural dimensions of affective meaning for words.\nBased on this work, several three-dimensional sentiment lexicons have been manually labeled using surveys in different countries (Heise, 2010). Words in these lexicons are measured on a scale from −4.3 (infinitely bad, powerless, or inactive) to +4.3 (infinitely good, powerful, or lively) (Berger and Zelditch, 2002; Heise, 2007).1 In these surveys, participants are asked to rate identities (e.g., teacher, mother), behaviors (e.g., help, coach), adjectives (e.g., big, stubborn), institutions (e.g., hospital, school) or scenarios (e.g. combinations of identities, behaviours, adjectives and institutions) (Heise, 2010) on 5- item scales ranging from ”Infinitely negative (e.g., bad/powerless/inactive)” to ”Infinitely positive (e.g., good/powerful/active)”, which are then mapped to the [−4.3, 4.3] scale. These man-\n1The range [−4.3, 4.3] is a historical convention\nual annotation methods are labor-intensive, timeconsuming and they produce a relatively small number of words.\nIn this paper, we utilize the semantic and distributed words representation to expand these three-dimensional sentiment lexicons in a semisupervised fashion. We also evaluated four different approaches of computing the affinity matrix using a semantic (dictionary-based) features, singular value decomposition word embedding, neural word embedding word vector, and combining both neural word embedding and semantic features. The highest results were obtained using the semantic and neural word embedding model with a rank correlation score τ = 0.51 on recreating two sentiment lexicon (Warriner et al., 2013) and the General Inquirer (Stone et al., 1968). The results also show that the highest rank correlation scores of the three dimension were for evaluation (E) while the lowest were for the potency (P). We also evaluated our induced EPA scores against some of the state-of-the-art methods in lexicon expansion, and our method shows an improvement in the τ correlation and F1 score over these algorithms.\nOur contributions are fivefold: 1) this is the first work that extensively examines methods of multidimensional lexicon expansion (we compute the evaluation, potency, and activity (valence, dominance, and arousal) scores instead of only computing the evaluative factor ( valence), 2) we propose a label propagation algorithm that is built upon both the semantic and distributed word representations, 3) we performed a comprehensive evaluation of four algorithms against a manually annotated dataset as well as a supervised learning algorithm, 4) we sample seed words from the corpus or dictionary instead of using the commonly used fixed seed words (e.g., good, bad, happy, sad etc.), 5) we created a significantly large threedimensional lexicon of ∼ 3M words that could be leveraged by researchers in fields of sentiment analysis and social science.\nOur proposed approaches 1) reduce the cost of manual annotation of sentiment lexicons; 2) integrate the affective meaning of today’s’ growing vocabulary (e.g., selfie, sexting), and 3) identify and incorporate the variance in attitudes towards words (e.g., same-sex marriage, abortion)."
    }, {
      "heading" : "2 Related Work",
      "text" : "The lexicon augmentation methods in this study were performed using variations of word representations and similarity metrics. This section provides a short background about the various vector space models that are used."
    }, {
      "heading" : "2.1 Statistical language modeling",
      "text" : "Statistical language model (or vector space model (VSM)) is a distributional estimation of various language phenomena estimated by employing statistical techniques on real world data. Representing language phenomena in terms of parameters has proven to be useful in various natural language processing (NLP), speech recognition, and information retrieval (IR) tasks. To capture the semantic or syntactic properties and represent words as proximity in n-dimensional space, several VSMs have been proposed ranging from the simple onehot representation that regards words as atomic symbols of the co-occurrence with other words in a vocabulary to a neural word embedding that represents words in a dense and more compact representation.\nThe most commonly used word representation is the distributional word embeddings representing word based on the co-occurrence statistics with other words in a document or corpora (Harris, 1981; Firth, 1957). The dimensionality of this sparse representation can be reduced using Singular value decomposition (Eckart and Young, 1936), Latent Semantic Analysis (Landauer and Dumais, 1997) or Principal Component Analysis (Jolliffe, 2002).\nNeural word embeddings has recently gained a lot of attention in NLP and deep learning. Neural word embeddings represent words in a lowdimensional, continuous space where each dimension corresponds to semantic or syntactic latent features. Similar to distributional word embeddings, neural word embeddings are usually based upon co-occurrence statistics, but they are more compact, less sensitive to data sparsity, and able to represent an exponential number of word clusters (Bengio et al., 2006) (Mikolov et al., 2010, 2011)."
    }, {
      "heading" : "2.2 Acquisition of Sentiment Lexicon",
      "text" : "Similar to other NLP tasks, sentiment lexicon induction methods can be achieved using two main approaches corpus-based or thesaurus-\nbased. Turney and Littman (Turney et al., 2003) proposed a corpus-based lexicon learning method by first applying (TF-IDF) weighting on matrices of words and context, using SVD, and then computing the semantic orientation with a set of seed words.\nThesaurus-based methods use the lexical relationship such as the depth of a concept in taxonomy tree (Wu and Palmer, 1994) or edge counting (Collins and Quillian, 1969) to build sentiment lexicons. Similar to Turney’s PMI approach (KAMPS, 2004) they use WordNet based relatedness metric between words and given seed words.\nSemi-supervised graph-based models that propagate information over lexical graphs have also been explored. The polarity-propagation or sense propagation algorithm induces sentiment polarity of unlabeled words given seed words (positive, negative) and the lexical relationships between them (e.g., word-net synonym, antonym) (Strapparava and Valitutti, 2004; Esuli and Sebastiani, 2006). Some researchers have developed a weighted label propagation algorithm that propagates a continuous sentiment score from seed words to lexically related words (Godbole et al., 2007). Velikovich et al. (Velikovich et al., 2010) proposed web-based graph propagation to elicit polarity lexicons. The graph is built upon a co-occurrence frequency matrix and cosine similarity (edges) between words and seed words (nodes). Then, both a positive and a negative polarity magnitude will be computed for each node in the graph which is equal to the sum over the max weighted path from every seed word (either positive or negative).\nSeveral recent studies have utilized word embeddings to generate sentiment lexicons, such as a regression model that uses structured skipgram 600 word embedding to create a Twitterbased sentiment lexicon (Astudillo et al., 2015). Another study transforms dense word embedding vectors into a lower dimensional (ultradense) representation by training a two objective function gradient descent algorithms on lexicon resources (Rothe et al., 2016). A recent study has also proposed a label propagation based model that uses word embedding, built using singular value decomposition (SVD) and PMI, to induce a domain-specific sentiment lexicon (Hamilton et al., 2016).\nFew studies have looked at multidimen-\nsional sentiment lexicon expansion. Kamps et al. (KAMPS, 2004) use a WordNet-based metric to elicit semantic orientation of adjectives. The generated lexicon was evaluated against the manually constructed list of Harvard IV-4 General Inquirer (Stone et al., 1968). Kamps et al.’s work focuses only on adjectives and assigns them a binary value (either good or bad, potent or impotent, etc.). A three-dimensional sentiment lexicon was extended using a thesaurus-based label propagation algorithm based upon WordNet similarity (Alhothali and Hoey, 2015), and their results were compared against the Ontario dataset (MacKinnon, 2006)."
    }, {
      "heading" : "3 Method",
      "text" : ""
    }, {
      "heading" : "3.1 Graph-based Label-Propagation",
      "text" : "Expanding sentiment lexicons using graph-based propagation algorithms was pursued previously and found to give higher accuracy in comparison with other standard methods (Hu and Liu, 2004; Andreevskaia and Bergler, 2006; Rao and Ravichandran, 2009). To evaluate the effectiveness of graph-based approaches in expanding multidimensional sentiment lexicons, in this paper, we use the label propagation algorithm (Zhu and Ghahramani, 2002; Zhou et al., 2004), combined with four methods for computing words vectors and word similarities. The label propagation algorithms rely on the idea of building a similarity graph with labeled (seed words/paradigm words) and unlabeled nodes (words). The labels or scores of the known nodes (words) are then propagated through the graph to the unlabeled nodes by repeatedly multiplying the weight matrix (affinity matrix) against the labels or scores vector.\nFollowing the same principle, the graph label propagation algorithm in this paper: 1) creates a set of labeled L = (Xl, Yl) and unlabeled data points or words U = (Xu, Yu) where |U |+ |L| = |V |, V is all the words in the vocabulary set, X is the word, and Y is the sentiment (E, P, A scores) attached to that word; 2) constructs an undirected weighted graph G = {E,V,W} where V is a set of vertices (words), E edges, W is an |V | × |V | weight matrix ( where wij ≥ 0); 3) Compute the random walk normalized Laplacian matrix ∆ = D−1W (where D is the degree matrix); 4) initializes the labeled nodes/words Yl with their EPA values, and the unlabeled nodes/words Yu with ze-\nroes; 4) propagates the sentiment scores to adjacent nodes by computing Y ← ∆Y (weighted by a factor α) and clamps the labeled nodes Yl to their initial values L after each iteration.\nWe implemented the label propagation algorithm using four different methods of computing affinity matrix and word representations. First, a semantic lexicon-based approach in which the graph is built based upon the semantic relationship between words (Semantic lexicon-based Label propagation or SLLP). Second, a distributional based approach in which vocabulary and weights come from co-occurrence statistics in a corpus (corpus-based label propagation or CLP). Third, a neural word embeddings method (neural word embedding label propagation or NWELP), and fourth, a combination of semantic and distributional methods (semantic neural word embedding label propagation or SNWELP). The following subsections describe these four different methods of label propagation."
    }, {
      "heading" : "3.1.1 Semantic Lexicon-based Label Propagation (SLLP)",
      "text" : "The SLLP algorithm follows the general principle of the graph-based label propagation approach as described in the previous section, but the affinity matrixW is computed using the semantic features obtained from semantic lexicons. Two semantic lexicons were used in this algorithm: WordNet dictionary (WN) (Miller, 1995) and the paraphrase database (PPDB) (Ganitkevitch et al., 2013). The SLLP algorithm constructs the vocabulary V from the words of the dictionaries and computes and normalizes the weight matrix W using the synonyms relationship between words. The semanticbased similarity wi,j of any pair of words xi and xj in the vocabulary V is calculated as follows:\nwi,j =\n{\n1.0 if xj is a synonym of xi 0.0 otherwise (1)"
    }, {
      "heading" : "3.1.2 Corpus-based Label Propagation (CLP)",
      "text" : "Corpus-based label propagation (CLP) is one of the most commonly used methods for sentiment lexicon generation that uses the co-occurrence statistics aggregated from different corpora (news articles, Twitter, etc.) to build the similarity graph in the label propagation algorithms. We used an n-gram features from the signal media (SM) one million news articles dataset which contains\n∼ 265K blog articles and ∼ 734K news articles (Corney et al., 2016) and the North American News (NAN) text corpus (Graff, 1995) which has ∼931K articles from a variety of news sources.\nThe co-occurrence matrix R was computed on a window size of four words. Bigrams with stop words, words less than three letters, proper nouns, non-alpha words, and the bigrams that do not occur more than ten times were filtered out. These heuristics reduce the set into ∼ 80k and ∼ 40k, for SM and NAN corpora, respectively. We constructed the word vectors by computing the smoothed positive point-wise mutual information(SPPMI) (Levy et al., 2015) of the co-occurrence matrix R. This smoothing technique reduces the PMI’s bias towards rare words and found to improve the performance of NLP tasks (Levy et al., 2015).\nSPPMIij = max{(log2 p(wi,wj)\np(wi)pα(wj) ,0)}\n(2)\nwhere p(wi,wj) is the empirical co-occurrence probability of a pair of words wi and wj and p(wi) and pα(wj) are the marginal probability of wi and the smoothed marginal probability of wj, respectively. We use α = 0.75 as it is found to give better results (Levy et al., 2015) (Mikolov et al., 2013) and we also experiment with the unsmoothed PPMI. The SPPMI matrix is then factorized with truncated Singular Value Decomposition (SVD) (Eckart and Young, 1936) as follows:\nSPPMI = U ∗Σ ∗VT (3)\nWe take the top k rows of U as the word representation or word vector (we used k=300):\nWSVD = Uk (4)\nThe affinity matrix is then computed as:\nwij = cos(vi,vj) = vivj\n‖vi‖‖vj‖ , ∀vi,vj ∈ WSVD\n(5)"
    }, {
      "heading" : "3.1.3 Neural Word Embeddings",
      "text" : "Label-propagation (NWELP)\nThis method uses word embeddings (wordvectors) that capture syntactic and semantic properties. We use two pre-trained word embedding models that are trained on co-occurrence\nstatistics. We used skip-gram word vector (SG) (Mikolov et al., 2013) that is trained on a skip-gram model of co-occurrence statistics aggregated from Google News dataset and Global vector for word representation(GloVe) (Pennington et al., 2014) which have been trained on co-occurrence statistics aggregated from Wikipedia. The vocabulary V in this algorithm is all words in the word embeddings set (we filtered out non-alpha words and words that contain digits), and the affinity matrix W is computed using the cosine similarity (Equation 5) between word vectors (each vi ∈ V is a 300 dimensional vector)."
    }, {
      "heading" : "3.1.4 Semantic and Neural Word Embeddings Label-propagation (SNWELP)",
      "text" : "To improve the results of the NWELP algorithm, we propose the SNWELP, a model that combines both semantic and distributional information obtained from the neural word embedding models and a semantic lexicon (a dictionary). The SNWELP algorithm constructs the affinity matrix W using the neural word embeddings features (SG or GloVe) and semantic features obtained from a semantic lexicon (WN or PPDB). In this case, V is intersection between the words in lexicon and the word in the filtered embeddings set, W is the averaged cosine similarity scores (Equation 5) of the neural and the semantic word representations (Equation 1)."
    }, {
      "heading" : "3.2 Sampling Methods",
      "text" : "Choosing the labeled words (also called paradigm or seed words) in the graph-based label propagation methods is one of the critical factors. We used two methods: 1) fixed seed sets (fixed-paradigms), and 2) words sampled from the vocabularies V used in the label propagation algorithm (vocabulary-paradigms). The fixed-paradigms set was chosen from Osgood et al’s (Osgood, 1957) research as shown in Table 1 while the vocabulary-paradigms set was randomly sampled from the corpus’ vocabulary for words with the highest and lowest EPA values (words with E, P or A ≤ −2.5 or ≥ 2.5). The objective is to use words at extremes of each dimension E,P, and A, as paradigm words in order to propagate these highly influencing EPA throughout the graph. The seed words contribute to no more than 1% of all words in each algorithm. We tested with\nthe fixed-paradigms sets, but the results of the vocabulary-paradigms were significantly better."
    }, {
      "heading" : "3.3 Evaluation Metrics",
      "text" : "To evaluate the effectiveness of the algorithm in generating a multidimensional sentiment lexicon, we chose the most recent manually-annotated affective dictionary (Warriner et al., 2013) as baseline. We use the (Warriner et al., 2013) dictionary in the lexicon induction procedure by sampling the paradigm words from it and we compare the generated lexicon against it. We randomly divided the (Warriner et al., 2013) affective dictionary (original-EPA) into EPA-training (third of the set equal to 5566 words) and EPA-testing (two-thirds of the set equal to 8349 words). The seed words for all algorithms are sampled from the EPA-training set only, and all results are presented on the EPA-testing set.\nThe EPA scores of (Warriner et al., 2013) initially range ∈ [1, 9] and we rescaled them to ∈ [−4.3,+4.3] to follow the same EPA scale used in the other lexicons we have considered (Heise, 2010). The [−4.3,+4.3] scale is the standard scale used by most of the researchers in the sociology field who study or measure individuals’ emotions towards terms.\nFour evaluation metrics were used to compare the induced EPA (EPA-induced) against the manually annotated EPA (EPA-testing): mean absolute error (MAE), Kendall τ rank correlation, F1-binary (positive and negative), and F1-ternary (positive, neutral, and negative). We used F1binary to evaluate the binary classification performance of the model (positive ≥ 0 and negative < 0 ) and similar to most recently proposed studies in the field (Hamilton et al., 2016), we computed F1-ternary to measure the ternary classification accuracy: positive ∈ (1, 4.3], neutral ∈ [−1, 1], and negative ∈ [−4.3,−1). To calculate the F1-ternary, we used the class-mass normalization (CMN) methods (Zhu et al., 2003) that rescale the predicted label (ŷi,l) for a point xi by\nincorporating the class prior as follows :\nargmax l wl ŷi,l\nwhere wl is the label mass normalization which is equal to pl/ml where pl is the prior probability of a label l (computed from the labeled data), and ml is the estimated weight of label l over the unlabeled sets. This scaling method is known to improve the results in comparison with the typical decision function argmaxl ŷi,l."
    }, {
      "heading" : "3.4 Baseline and State-of-the-art Comparison",
      "text" : "We compared our induced results against some of the standard state-of-art algorithms for inducing the valence (evaluation scores). We implemented the PMI-IR algorithm proposed by (Turney et al., 2003) which estimates the sentiment orientation (either positive or negative) of a word by computing the difference between the strength of the word associations with positive paradigm words and with negative paradigm words using the cooccurrence statistics aggregated from search engines’ results. We also compare our results against the reported results of (Rothe et al., 2016)’s orthogonal transformation of word vectors, and a label spreading algorithm trained on ( a domainspecific) SVDword vector model (Hamilton et al., 2016). We also experimented with the retrofitted word vector model that improves the neural word embedding vectors using semantic features obtained from the lexical resources (WN, PPDB) (Faruqui et al., 2014).\nTo make a fair comparison, we implemented our label propagation algorithm and the retrofitted word vector approach (Faruqui et al., 2014) to recreate the General Inquirer lexicon (Stone et al., 1966) with valence score ∈ R from (Warriner et al., 2013) lexicon to compare our results to (Hamilton et al., 2016) and (Rothe et al., 2016).We also ignored the neutral class and used the same seed set used by (Hamilton et al., 2016) and other researchers in the field. We also compare all the results against the EPA scores obtained from a supervised learning algorithm. We trained a support vector regression (SVR) model on a co-occurrence statistics model derived from the skip-gram word embedding model (SG) (Mikolov et al., 2013) and sentiment lexicon resource (Warriner et al., 2013). The SVR model uses RBF kernel with C = 10, and\nγ = 0.0 for training and is trained on the full training set (EPA-training)."
    }, {
      "heading" : "4 Results",
      "text" : "In this section, we present the results of comparing the induced EPA scores using the label propagation algorithms against their corresponding values in the EPA-testing. As shown in Table 2, using SVD word embeddings in the CLP algorithm generated the lowest ranking correlation τ and the highest error rate (MAE) in comparison with the other label propagation methods. The results of comparing the induced EPA scores against their true values in the testing set show that the MAE ranged between 0.99 and 1.3 and the ranking correlation 2 τ was less than 0.2 using cosine similarity and hard clamping ( α = 1.0) assumption. We also experimented with the unsmoothed pointwise mutual information (PPMI), but there was not a significant difference between the smooth and the unsmoothed PMI. We also tried different dimension of the SVD word vector k=100 and k=300, but there was no significant difference between them as well.\nThe results of the SLLP algorithm that uses the semantic features obtained from either WN or PPDB lexicons generated a total of ∼50K words, where ∼ 4K words are in the testing set (EPAtesting). The results of comparing the induced EPA scores to their corresponding values in the testing set (EPA-testing) show that the MAE less than 1.0, F1-binary greater than 0.70, F1-ternary greater than or equal to 0.60, and the ranking correlation τ ≥ 0.2 suggesting that there is a reasonable degree of agreement between the induced EPA score using dictionary-based features and the manually labeled EPA values.\nThe τ correlations scores show that neural word embedding label propagating NWELP outperformed the semantic based, and corpus-based label propagation algorithms, as shown in Table 2. The MAE and F1-scores of the semanticbased and neural word embedding label propagation were close. The MAE of the neural word embedding ranged from 0.84 to 1.09, F-1 scores were between 0.67 and 0.80, and τ ranged from 0.1 to 0.44. Comparing the results of the two pre-trained neural word embedding shows that the skip-gram based (SG) model performed better than (GloVe).\n2The p-value for all the reported τ scores are less than 0.001\nWe experimented with different thresholds (0.0, 0.3, and 0.5) of the cosine similarities and the result using different threshold varied a lot in respect to the number of induced words and the accuracy. Higher thresholds provided more accurate results and less noise in the results, but with less number of induced words. The reported results in Table 2 and 3 are using cosine similarity threshold equal to 0.0 since the adjacency matrix of both SG and Glove contain negative values. Combining the semantic and neural word embedding features improved the results with τ ranged between 0.43 and 0.51 and MAE≤ 1.1 for the evaluation scores (E). The results of the supervised SVR model significantly outperformed the results obtained from the semi-supervised method with τ equal to 0.628, 0.422 and 0.500 for E, P, and A, respectively, F1 scores equal to 0.83, 0.84, and 0.78, and MAE close to 0.6, but the results of the SNWELP were comparable.\nComparing the results across the different affective dimensions (E,P, and A) shows that the rank correlation τ of comparing the potency (P) to their counterpart scores in testing set was low in comparison with the scores of evaluation (E) and activity (A) in both the semi-supervised algorithms and the supervised algorithm. While the rank correlation τ of the evaluation (E) scores were the highest in all the algorithms which indicate that words with similar word embeddings have a similar evaluation score. Table 4 shows some of the induced EPA scores and their corresponding values\nin (Warriner et al., 2013) dataset. The table also shows some examples of the same words and their induced EPA scores using different word representations. Comparing our induced evaluation scores (E) with some of the state-of-the-art methods, as shown in Table 3, indicates that our label propagation algorithms significantly performed better than (Turney and Littman, 2002)’s unsupervised method. The result also shows that semantic neural word embedding (SNWELP) model outperformed (Rothe et al., 2016) and (Hamilton et al., 2016) approaches. Also, the neural word embedding and semantic neural word embedding algorithms perform better than the label propagation that uses the retrofitted word vector (the reported results are of the improved skip-gram model (SG) using semantic features obtained from wordnet (WN)) (Faruqui et al., 2014)."
    }, {
      "heading" : "5 Discussion",
      "text" : "Sentiment analysis is a feature engineering problem in which sentiment lexicons play a significant role in improving the model accuracy. One of the challenges of sentiment analysis is the increasing number of new words and terms in the social media or news resources (e.g., selfie, sexting, photobomb,etc.) that do not have a sentiment score attached to them. Also, there is a need to measure the variance in human attitudes towards some terms over a period of time (e.g., homosexuality, abortion) and to explore other dimensions of humans’ emotions. To overcome these limitations, reduce the cost of manual annotation, and increase the number of the annotated terms, we propose an extension and an evaluation of corpus and thesaurus-based algorithms to automatically induce a three-dimensional sentiment lexicon.\nSimilar to any NLP applications, the vast majority of the work in lexicon induction uses distributional word representations (corpus-based statistics). In this study, the corpus-based label propagation (CLP) algorithm generated the least accurate results. Also, despite the viability of distributional word representations, exactly what syntactic and semantic information it captures is hard to determine, and not clear whether it is relevant for sentiment at all.\nThe semantic lexicon-based label propagation (SLLP) was better than CLP. However, there are also some limitations of using the dictionary based approach 1) the synonym relationship can only be computed between words of the same part of speech, 2) the dictionary has a limited number of\nwords and does not include words that are used in the social media and internet in general.\nOnly one study have experimented with neural word embedding label propagation to expand the one-dimensional sentiment lexicon (Hamilton et al., 2016) with only reporting the result of using SVD word embedding model. In our study, we report the results of using different neural word embedding models. The results show that our neural word embedding model performed better than the SVDword vector approach. These findings require further analysis and assessment on different corpora.\nThe results of combining both the semantic and neural word embedding (NWELP) was better than the corpus-based or semantic lexicon-based algorithms. The semantic neural word embedding provided a higher rank correlation scores and a slighting lower MAE in comparison with the semantic lexicon and neural word embedding-based algorithms. The results of the semantic neural label propagation algorithm are also comparable with those generated using a supervised learning algorithm (SVR) trained on word embeddings and a sentiment lexicon. Using the semi-supervised algorithm; however, does not require a large training dataset and allows to annotate the words independently from the previously human-coded lexica."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this study, we propose an extension to the graph-based lexicon induction algorithms to expand sentiment lexicons and explore other dimensions of sentiments. This study to the best of our knowledge is the first work that expands a multidimension sentiment lexicon and the first to incorporates both the semantic and neural word representations in the label propagation algorithm. We also provided an extensive evaluation of label propagation algorithms using a variety of word representations that have been found to provide higher accuracy in many NLP tasks in comparison with other standard methods. The results show that the word semantic neural word embedding label propagation generates the highest correlations compared with the corpus-based, semantic lexicon-based, and neural word embedding algorithms."
    } ],
    "references" : [ {
      "title" : "Good news or bad news: Using affect control theory to analyze readers reaction towards news articles",
      "author" : [ "Areej Alhothali", "Jesse Hoey." ],
      "venue" : "Proc. Conference of the North American Chapter of the Association for Computational Linguistics - Human",
      "citeRegEx" : "Alhothali and Hoey.,? 2015",
      "shortCiteRegEx" : "Alhothali and Hoey.",
      "year" : 2015
    }, {
      "title" : "Semantic tag extraction from wordnet glosses",
      "author" : [ "Alina Andreevskaia", "Sabine Bergler." ],
      "venue" : "Proceedings of 5th International Conference on Language Resources and Evaluation (LREC06). Citeseer.",
      "citeRegEx" : "Andreevskaia and Bergler.,? 2006",
      "shortCiteRegEx" : "Andreevskaia and Bergler.",
      "year" : 2006
    }, {
      "title" : "Inesc-id: A regression model for large scale twitter sentiment lexicon induction",
      "author" : [ "Ramon F Astudillo", "Silvio Amir", "Wang Ling", "Bruno Martins", "Mário Silva", "Isabel Trancoso", "Rua Alves Redol." ],
      "venue" : "SemEval-2015 page 613.",
      "citeRegEx" : "Astudillo et al\\.,? 2015",
      "shortCiteRegEx" : "Astudillo et al\\.",
      "year" : 2015
    }, {
      "title" : "Sentiwordnet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining",
      "author" : [ "Stefano Baccianella", "Andrea Esuli", "Fabrizio Sebastiani" ],
      "venue" : "In LREC",
      "citeRegEx" : "Baccianella et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Baccianella et al\\.",
      "year" : 2010
    }, {
      "title" : "Neural probabilistic language models",
      "author" : [ "Yoshua Bengio", "Holger Schwenk", "Jean-Sébastien Senécal", "Fréderic Morin", "Jean-Luc Gauvain." ],
      "venue" : "Innovations in Machine Learning, Springer, pages 137–186.",
      "citeRegEx" : "Bengio et al\\.,? 2006",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2006
    }, {
      "title" : "New Directions in Contemporary Sociological Theories",
      "author" : [ "Joseph Berger", "Morris Zelditch." ],
      "venue" : "Rowman & Littlefield.",
      "citeRegEx" : "Berger and Zelditch.,? 2002",
      "shortCiteRegEx" : "Berger and Zelditch.",
      "year" : 2002
    }, {
      "title" : "Retrieval time from semantic memory",
      "author" : [ "Allan M Collins", "M Ross Quillian." ],
      "venue" : "Journal of verbal learning and verbal behavior 8(2):240–247.",
      "citeRegEx" : "Collins and Quillian.,? 1969",
      "shortCiteRegEx" : "Collins and Quillian.",
      "year" : 1969
    }, {
      "title" : "What do a million news articles look like",
      "author" : [ "David Corney", "Dyaa Albakour", "Miguel Martinez", "Samir Moussa" ],
      "venue" : "In Proceedings of the First International Workshop on Recent Trends in News Information Retrieval",
      "citeRegEx" : "Corney et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Corney et al\\.",
      "year" : 2016
    }, {
      "title" : "The approximation of one matrix by another of lower rank",
      "author" : [ "Carl Eckart", "Gale Young." ],
      "venue" : "Psychometrika 1(3):211–218.",
      "citeRegEx" : "Eckart and Young.,? 1936",
      "shortCiteRegEx" : "Eckart and Young.",
      "year" : 1936
    }, {
      "title" : "Sentiwordnet: A publicly available lexical resource for opinion mining",
      "author" : [ "Andrea Esuli", "Fabrizio Sebastiani." ],
      "venue" : "Proceedings of LREC. volume 6, pages 417–422.",
      "citeRegEx" : "Esuli and Sebastiani.,? 2006",
      "shortCiteRegEx" : "Esuli and Sebastiani.",
      "year" : 2006
    }, {
      "title" : "Retrofitting word vectors to semantic lexicons",
      "author" : [ "Manaal Faruqui", "Jesse Dodge", "Sujay K Jauhar", "Chris Dyer", "Eduard Hovy", "Noah A Smith." ],
      "venue" : "arXiv preprint arXiv:1411.4166 .",
      "citeRegEx" : "Faruqui et al\\.,? 2014",
      "shortCiteRegEx" : "Faruqui et al\\.",
      "year" : 2014
    }, {
      "title" : "A synopsis of linguistic theory, 1930-1955",
      "author" : [ "John Rupert Firth" ],
      "venue" : null,
      "citeRegEx" : "Firth.,? \\Q1957\\E",
      "shortCiteRegEx" : "Firth.",
      "year" : 1957
    }, {
      "title" : "The world of emotions is not two-dimensional",
      "author" : [ "Johnny RJ Fontaine", "Klaus R Scherer", "Etienne B Roesch", "Phoebe C Ellsworth." ],
      "venue" : "Psychological science 18(12):1050–1057.",
      "citeRegEx" : "Fontaine et al\\.,? 2007",
      "shortCiteRegEx" : "Fontaine et al\\.",
      "year" : 2007
    }, {
      "title" : "Ppdb: The paraphrase database",
      "author" : [ "Juri Ganitkevitch", "Benjamin Van Durme", "Chris Callison-Burch." ],
      "venue" : "HLT-NAACL. pages 758–764.",
      "citeRegEx" : "Ganitkevitch et al\\.,? 2013",
      "shortCiteRegEx" : "Ganitkevitch et al\\.",
      "year" : 2013
    }, {
      "title" : "Large-scale sentiment analysis for news and blogs",
      "author" : [ "Namrata Godbole", "Manja Srinivasaiah", "Steven Skiena." ],
      "venue" : "ICWSM 7.",
      "citeRegEx" : "Godbole et al\\.,? 2007",
      "shortCiteRegEx" : "Godbole et al\\.",
      "year" : 2007
    }, {
      "title" : "Inducing domain-specific sentiment lexicons from unlabeled corpora",
      "author" : [ "William L Hamilton", "Kevin Clark", "Jure Leskovec", "Dan Jurafsky." ],
      "venue" : "arXiv preprint arXiv:1606.02820 .",
      "citeRegEx" : "Hamilton et al\\.,? 2016",
      "shortCiteRegEx" : "Hamilton et al\\.",
      "year" : 2016
    }, {
      "title" : "Distributional structure",
      "author" : [ "Zellig S Harris." ],
      "venue" : "Springer.",
      "citeRegEx" : "Harris.,? 1981",
      "shortCiteRegEx" : "Harris.",
      "year" : 1981
    }, {
      "title" : "Expressive order: Confirming sentiments in social actions",
      "author" : [ "David R Heise." ],
      "venue" : "Springer.",
      "citeRegEx" : "Heise.,? 2007",
      "shortCiteRegEx" : "Heise.",
      "year" : 2007
    }, {
      "title" : "Surveying Cultures: Discovering Shared Conceptions and Sentiments",
      "author" : [ "David R. Heise." ],
      "venue" : "Wiley.",
      "citeRegEx" : "Heise.,? 2010",
      "shortCiteRegEx" : "Heise.",
      "year" : 2010
    }, {
      "title" : "Mining and summarizing customer reviews",
      "author" : [ "Minqing Hu", "Bing Liu." ],
      "venue" : "Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, pages 168– 177.",
      "citeRegEx" : "Hu and Liu.,? 2004",
      "shortCiteRegEx" : "Hu and Liu.",
      "year" : 2004
    }, {
      "title" : "Principal component analysis",
      "author" : [ "Ian Jolliffe." ],
      "venue" : "Wiley Online Library.",
      "citeRegEx" : "Jolliffe.,? 2002",
      "shortCiteRegEx" : "Jolliffe.",
      "year" : 2002
    }, {
      "title" : "Using wordnet to measure semantic orientation of adjectives",
      "author" : [ "Jaap KAMPS." ],
      "venue" : "Proceedings of the 4th International Conference on Language Resources and Evaluation (LREC 2004). pages 1115– 1118.",
      "citeRegEx" : "KAMPS.,? 2004",
      "shortCiteRegEx" : "KAMPS.",
      "year" : 2004
    }, {
      "title" : "A solution to plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge",
      "author" : [ "Thomas K Landauer", "Susan T Dumais." ],
      "venue" : "Psychological review 104(2):211.",
      "citeRegEx" : "Landauer and Dumais.,? 1997",
      "shortCiteRegEx" : "Landauer and Dumais.",
      "year" : 1997
    }, {
      "title" : "Improving distributional similarity with lessons learned from word embeddings",
      "author" : [ "Omer Levy", "Yoav Goldberg", "Ido Dagan." ],
      "venue" : "Transactions of the Association for Computational Linguistics 3:211–225.",
      "citeRegEx" : "Levy et al\\.,? 2015",
      "shortCiteRegEx" : "Levy et al\\.",
      "year" : 2015
    }, {
      "title" : "Mean affective ratings of 2, 294 concepts by guelph university undergraduates, ontario, canada",
      "author" : [ "Neil J. MacKinnon." ],
      "venue" : "2001-3 [Computer file].",
      "citeRegEx" : "MacKinnon.,? 2006",
      "shortCiteRegEx" : "MacKinnon.",
      "year" : 2006
    }, {
      "title" : "Recurrent neural network based language model",
      "author" : [ "Tomas Mikolov", "Martin Karafiát", "Lukas Burget", "Jan Cernockỳ", "Sanjeev Khudanpur." ],
      "venue" : "INTERSPEECH. pages 1045–1048.",
      "citeRegEx" : "Mikolov et al\\.,? 2010",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2010
    }, {
      "title" : "Extensions of recurrent neural network language model",
      "author" : [ "Tomas Mikolov", "Stefan Kombrink", "Lukas Burget", "JH Cernocky", "Sanjeev Khudanpur." ],
      "venue" : "Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on.",
      "citeRegEx" : "Mikolov et al\\.,? 2011",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2011
    }, {
      "title" : "The measurement",
      "author" : [ "Charles Egerton Osgood" ],
      "venue" : null,
      "citeRegEx" : "41",
      "shortCiteRegEx" : "41",
      "year" : 1957
    }, {
      "title" : "The general inquirer: A",
      "author" : [ "DM Ogilvie" ],
      "venue" : null,
      "citeRegEx" : "Ogilvie.,? \\Q1968\\E",
      "shortCiteRegEx" : "Ogilvie.",
      "year" : 1968
    }, {
      "title" : "Verbs semantics and lexical selection",
      "author" : [ "ZhibiaoWu andMartha Palmer." ],
      "venue" : "Proceedings of the 32nd annual meeting on Association for Computational Linguistics. Association for Computational Linguistics, pages 133–138.",
      "citeRegEx" : "Palmer.,? 1994",
      "shortCiteRegEx" : "Palmer.",
      "year" : 1994
    }, {
      "title" : "Learning with local and global consistency",
      "author" : [ "Dengyong Zhou", "Olivier Bousquet", "Thomas Navin Lal", "Jason Weston", "Bernhard Schölkopf." ],
      "venue" : "Advances in neural information processing systems 16(16):321–328.",
      "citeRegEx" : "Zhou et al\\.,? 2004",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2004
    }, {
      "title" : "Learning from labeled and unlabeled data with label propagation",
      "author" : [ "Xiaojin Zhu", "Zoubin Ghahramani." ],
      "venue" : "Technical report, Technical Report CMUCALD-02-107, Carnegie Mellon University.",
      "citeRegEx" : "Zhu and Ghahramani.,? 2002",
      "shortCiteRegEx" : "Zhu and Ghahramani.",
      "year" : 2002
    }, {
      "title" : "Semi-supervised learning using gaussian fields and harmonic functions",
      "author" : [ "Xiaojin Zhu", "Zoubin Ghahramani", "John Lafferty" ],
      "venue" : "In ICML",
      "citeRegEx" : "Zhu et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2003
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : ", 1968) or real-valued onedimensional scores (Baccianella et al., 2010) to the words.",
      "startOffset" : 45,
      "endOffset" : 71
    }, {
      "referenceID" : 12,
      "context" : "It is well known; however, that one dimension is insufficient to adequately characterise the complexity of emotion (Fontaine et al., 2007).",
      "startOffset" : 115,
      "endOffset" : 138
    }, {
      "referenceID" : 18,
      "context" : "sentiment lexicons have been manually labeled using surveys in different countries (Heise, 2010).",
      "startOffset" : 83,
      "endOffset" : 96
    }, {
      "referenceID" : 5,
      "context" : "3 (infinitely good, powerful, or lively) (Berger and Zelditch, 2002; Heise, 2007).",
      "startOffset" : 41,
      "endOffset" : 81
    }, {
      "referenceID" : 17,
      "context" : "3 (infinitely good, powerful, or lively) (Berger and Zelditch, 2002; Heise, 2007).",
      "startOffset" : 41,
      "endOffset" : 81
    }, {
      "referenceID" : 18,
      "context" : "combinations of identities, behaviours, adjectives and institutions) (Heise, 2010) on 5item scales ranging from ”Infinitely negative (e.",
      "startOffset" : 69,
      "endOffset" : 82
    }, {
      "referenceID" : 16,
      "context" : "The most commonly used word representation is the distributional word embeddings representing word based on the co-occurrence statistics with other words in a document or corpora (Harris, 1981; Firth, 1957).",
      "startOffset" : 179,
      "endOffset" : 206
    }, {
      "referenceID" : 11,
      "context" : "The most commonly used word representation is the distributional word embeddings representing word based on the co-occurrence statistics with other words in a document or corpora (Harris, 1981; Firth, 1957).",
      "startOffset" : 179,
      "endOffset" : 206
    }, {
      "referenceID" : 8,
      "context" : "The dimensionality of this sparse representation can be reduced using Singular value decomposition (Eckart and Young, 1936), Latent Semantic Analysis (Landauer and Dumais, 1997) or Principal Component Analysis (Jolliffe, 2002).",
      "startOffset" : 99,
      "endOffset" : 123
    }, {
      "referenceID" : 22,
      "context" : "The dimensionality of this sparse representation can be reduced using Singular value decomposition (Eckart and Young, 1936), Latent Semantic Analysis (Landauer and Dumais, 1997) or Principal Component Analysis (Jolliffe, 2002).",
      "startOffset" : 150,
      "endOffset" : 177
    }, {
      "referenceID" : 20,
      "context" : "The dimensionality of this sparse representation can be reduced using Singular value decomposition (Eckart and Young, 1936), Latent Semantic Analysis (Landauer and Dumais, 1997) or Principal Component Analysis (Jolliffe, 2002).",
      "startOffset" : 210,
      "endOffset" : 226
    }, {
      "referenceID" : 4,
      "context" : "Similar to distributional word embeddings, neural word embeddings are usually based upon co-occurrence statistics, but they are more compact, less sensitive to data sparsity, and able to represent an exponential number of word clusters (Bengio et al., 2006) (Mikolov et al.",
      "startOffset" : 236,
      "endOffset" : 257
    }, {
      "referenceID" : 6,
      "context" : "Thesaurus-based methods use the lexical relationship such as the depth of a concept in taxonomy tree (Wu and Palmer, 1994) or edge counting (Collins and Quillian, 1969) to build sentiment lexicons.",
      "startOffset" : 140,
      "endOffset" : 168
    }, {
      "referenceID" : 21,
      "context" : "Similar to Turney’s PMI approach (KAMPS, 2004) they use WordNet based relatedness metric between words and given seed words.",
      "startOffset" : 33,
      "endOffset" : 46
    }, {
      "referenceID" : 9,
      "context" : ", word-net synonym, antonym) (Strapparava and Valitutti, 2004; Esuli and Sebastiani, 2006).",
      "startOffset" : 29,
      "endOffset" : 90
    }, {
      "referenceID" : 14,
      "context" : "Some researchers have developed a weighted label propagation algorithm that propagates a continuous sentiment score from seed words to lexically related words (Godbole et al., 2007).",
      "startOffset" : 159,
      "endOffset" : 181
    }, {
      "referenceID" : 2,
      "context" : "gram 600 word embedding to create a Twitterbased sentiment lexicon (Astudillo et al., 2015).",
      "startOffset" : 67,
      "endOffset" : 91
    }, {
      "referenceID" : 15,
      "context" : "A recent study has also proposed a label propagation based model that uses word embedding, built using singular value decomposition (SVD) and PMI, to induce a domain-specific sentiment lexicon (Hamilton et al., 2016).",
      "startOffset" : 193,
      "endOffset" : 216
    }, {
      "referenceID" : 21,
      "context" : "(KAMPS, 2004) use a WordNet-based metric to elicit semantic orientation of adjectives.",
      "startOffset" : 0,
      "endOffset" : 13
    }, {
      "referenceID" : 0,
      "context" : "A three-dimensional sentiment lexicon was extended using a thesaurus-based label propagation algorithm based upon WordNet similarity (Alhothali and Hoey, 2015), and their results were compared against the Ontario dataset (MacKinnon, 2006).",
      "startOffset" : 133,
      "endOffset" : 159
    }, {
      "referenceID" : 24,
      "context" : "A three-dimensional sentiment lexicon was extended using a thesaurus-based label propagation algorithm based upon WordNet similarity (Alhothali and Hoey, 2015), and their results were compared against the Ontario dataset (MacKinnon, 2006).",
      "startOffset" : 221,
      "endOffset" : 238
    }, {
      "referenceID" : 19,
      "context" : "Expanding sentiment lexicons using graph-based propagation algorithms was pursued previously and found to give higher accuracy in comparison with other standard methods (Hu and Liu, 2004; Andreevskaia and Bergler, 2006; Rao and Ravichandran, 2009).",
      "startOffset" : 169,
      "endOffset" : 247
    }, {
      "referenceID" : 1,
      "context" : "Expanding sentiment lexicons using graph-based propagation algorithms was pursued previously and found to give higher accuracy in comparison with other standard methods (Hu and Liu, 2004; Andreevskaia and Bergler, 2006; Rao and Ravichandran, 2009).",
      "startOffset" : 169,
      "endOffset" : 247
    }, {
      "referenceID" : 31,
      "context" : "To evaluate the effectiveness of graph-based approaches in expanding multidimensional sentiment lexicons, in this paper, we use the label propagation algorithm (Zhu and Ghahramani, 2002; Zhou et al., 2004), combined with four methods for computing words vectors and word similarities.",
      "startOffset" : 160,
      "endOffset" : 205
    }, {
      "referenceID" : 30,
      "context" : "To evaluate the effectiveness of graph-based approaches in expanding multidimensional sentiment lexicons, in this paper, we use the label propagation algorithm (Zhu and Ghahramani, 2002; Zhou et al., 2004), combined with four methods for computing words vectors and word similarities.",
      "startOffset" : 160,
      "endOffset" : 205
    }, {
      "referenceID" : 13,
      "context" : "Two semantic lexicons were used in this algorithm: WordNet dictionary (WN) (Miller, 1995) and the paraphrase database (PPDB) (Ganitkevitch et al., 2013).",
      "startOffset" : 125,
      "endOffset" : 152
    }, {
      "referenceID" : 7,
      "context" : "one million news articles dataset which contains ∼ 265K blog articles and ∼ 734K news articles (Corney et al., 2016) and the North American News (NAN) text corpus (Graff, 1995) which has ∼931K articles from a variety of news sources.",
      "startOffset" : 95,
      "endOffset" : 116
    }, {
      "referenceID" : 23,
      "context" : "We constructed the word vectors by computing the smoothed positive point-wise mutual information(SPPMI) (Levy et al., 2015) of the co-occurrence matrix R.",
      "startOffset" : 104,
      "endOffset" : 123
    }, {
      "referenceID" : 23,
      "context" : "This smoothing technique reduces the PMI’s bias towards rare words and found to improve the performance of NLP tasks (Levy et al., 2015).",
      "startOffset" : 117,
      "endOffset" : 136
    }, {
      "referenceID" : 23,
      "context" : "75 as it is found to give better results (Levy et al., 2015) (Mikolov et al.",
      "startOffset" : 41,
      "endOffset" : 60
    }, {
      "referenceID" : 8,
      "context" : "The SPPMI matrix is then factorized with truncated Singular Value Decomposition (SVD) (Eckart and Young, 1936) as follows:",
      "startOffset" : 86,
      "endOffset" : 110
    }, {
      "referenceID" : 18,
      "context" : "3] to follow the same EPA scale used in the other lexicons we have considered (Heise, 2010).",
      "startOffset" : 78,
      "endOffset" : 91
    }, {
      "referenceID" : 15,
      "context" : "We used F1binary to evaluate the binary classification performance of the model (positive ≥ 0 and negative < 0 ) and similar to most recently proposed studies in the field (Hamilton et al., 2016), we computed F1-ternary to measure the ternary classification accuracy: positive ∈ (1, 4.",
      "startOffset" : 172,
      "endOffset" : 195
    }, {
      "referenceID" : 32,
      "context" : "To calculate the F1-ternary, we used the class-mass normalization (CMN) methods (Zhu et al., 2003) that rescale the predicted label (ŷi,l) for a point xi by",
      "startOffset" : 80,
      "endOffset" : 98
    }, {
      "referenceID" : 15,
      "context" : ", 2016)’s orthogonal transformation of word vectors, and a label spreading algorithm trained on ( a domainspecific) SVDword vector model (Hamilton et al., 2016).",
      "startOffset" : 137,
      "endOffset" : 160
    }, {
      "referenceID" : 10,
      "context" : "We also experimented with the retrofitted word vector model that improves the neural word embedding vectors using semantic features obtained from the lexical resources (WN, PPDB) (Faruqui et al., 2014).",
      "startOffset" : 179,
      "endOffset" : 201
    }, {
      "referenceID" : 10,
      "context" : "To make a fair comparison, we implemented our label propagation algorithm and the retrofitted word vector approach (Faruqui et al., 2014) to recreate the General Inquirer lexicon (Stone et al.",
      "startOffset" : 115,
      "endOffset" : 137
    }, {
      "referenceID" : 15,
      "context" : ", 2013) lexicon to compare our results to (Hamilton et al., 2016) and (Rothe et al.",
      "startOffset" : 42,
      "endOffset" : 65
    }, {
      "referenceID" : 15,
      "context" : "by (Hamilton et al., 2016) and other researchers in the field.",
      "startOffset" : 3,
      "endOffset" : 26
    }, {
      "referenceID" : 15,
      "context" : "94 (Hamilton et al., 2016) 0.",
      "startOffset" : 3,
      "endOffset" : 26
    }, {
      "referenceID" : 10,
      "context" : "91 (Faruqui et al., 2014) 0.",
      "startOffset" : 3,
      "endOffset" : 25
    }, {
      "referenceID" : 15,
      "context" : ", 2016) and (Hamilton et al., 2016) approaches.",
      "startOffset" : 12,
      "endOffset" : 35
    }, {
      "referenceID" : 10,
      "context" : "results are of the improved skip-gram model (SG) using semantic features obtained from wordnet (WN)) (Faruqui et al., 2014).",
      "startOffset" : 101,
      "endOffset" : 123
    }, {
      "referenceID" : 15,
      "context" : "Only one study have experimented with neural word embedding label propagation to expand the one-dimensional sentiment lexicon (Hamilton et al., 2016) with only reporting the result of using SVD word embedding model.",
      "startOffset" : 126,
      "endOffset" : 149
    } ],
    "year" : 2017,
    "abstractText" : "In this paper, we propose an extension to graph-based sentiment lexicon induction methods by incorporating distributed and semantic word representations in building the similarity graph to expand a threedimensional sentiment lexicon. We also implemented and evaluated the label propagation using four different word representations and similarity metrics. Our comprehensive evaluation of the four approaches was performed on a single data set, demonstrating that all four methods can generate a significant number of new sentiment assignments with high accuracy. The highest correlations (τ = 0.51) and the lowest error (mean absolute error < 1.1%), obtained by combining both the semantic and the distributional features, outperformed the distributional-based and semantic-based label-propagation models and approached a supervised algorithm.",
    "creator" : "LaTeX with hyperref package"
  }
}