{
  "name" : "1609.02748.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "INSIGHT-1 at SemEval-2016 Task 5: Deep Learning for Multilingual Aspect-based Sentiment Analysis",
    "authors" : [ "Sebastian Ruder", "Parsa Ghaffari", "John G. Breslin" ],
    "emails" : [ "firstname.lastname@insight-centre.org", "firstname@aylien.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n60 9.\n02 74\n8v 2\n[ cs\n.C L\n] 2\n2 Se"
    }, {
      "heading" : "1 Introduction",
      "text" : "With access to the Internet becoming more prevalent, an inreasing number of people express their opinions online in a plethora of lan-\nguages. Sentiment analysis (Liu, 2012) enables us to derive shallow insights from these opinions related to their overall polarity. Often, however, e.g. in reviews, people do not express their opinion towards the entity as a whole, but refer to specific aspects such as the service in a restaurant.\nAspect-based sentiment analysis allows us to go deeper and determine sentiment towards such aspects of an entity. Past research in aspect-based sentiment analysis has largely focused on the English language, while SemEval 2016 Task 5 (Pontiki et al., 2016) for the first time provides a forum for multilingual aspectbased sentiment analysis.\nRecently, deep learning-based approaches have demonstrated remarkable results for text classification and sentiment analysis (Kim, 2014). A cascade of non-linearities allows them to model complex functions such as sentiment compositionality, while their ability to process raw signals renders them language and domain independent. In spite of these factors, they have largely gone untested for aspect-based sentiment analysis, particularly in the multilingual setting.\nIn this paper, we introduce our deep-learning\nbased approach to aspect-based sentiment analysis as part of our participation in SemEval2016 Task 5 Aspect-based Sentiment Analysis Slot 1 (Aspect Category Detection) and Slot 3 (Sentiment Polarity) ."
    }, {
      "heading" : "2 Related work",
      "text" : "Aspect-based sentiment analysis is traditionally split into an aspect extraction and a sentiment analysis subtask.\nPrevious approaches to aspect extraction framed the task as a multiclass classification problem and relied mostly on CRS that leveraged a plethora of common features, e.g. NER, POS tagging, parsing, semantic analysis, bagof-words, as well as domain-dependent ones, such as word clusters learnt from Amazon and Yelp data, while previous sentiment analysis approaches have used different classifiers with a wide range of features based on ngrams, POS, negation words, and a large array of sentiment lexica (Pontiki et al., 2014; Pontiki et al., 2015).\nPast deep learning-based approaches have focused mostly on the sentiment analysis subtask: Tang et al. (2015) use a target-dependent LSTM to determine sentiment towards a target word, while Nguyen and Shirai (2015) use a recursive neural network that leverages both constituency as well as dependency trees.\nIn contrast to previous approaches, our model neither relies on expensive feature engineering, availability of a parser, nor positional information, but solely on a language’s input signals."
    }, {
      "heading" : "3 Model",
      "text" : "The model architecture we use is an extension of the CNN structure used by Collobert et al. (2011), which has been successfully used by many others (Kim, 2014).\nThe model takes as input a text, which is padded to length n. We represent the text as a concatentation of its word embeddings x1:n where xi ∈ Rk is the k-dimensional vector of the i-th word in the text.\nThe convolutional layer slides filters of different window sizes over the input embeddings. Each filter with weights w ∈ Rhk generates a new feature ci for a window of h words according to the following operation:\nci = f(w · xi:i+h−1 + b) (1)\nNote that b ∈ R is a bias term and f is a nonlinear function, ReLU (Nair and Hinton, 2010) in our case. The application of the filter over each possible window of h words or characters in the sentence produces the following feature map:\nc = [c1, c2, ..., cn−h+1] (2)\nMax-over-time pooling in turn condenses this feature vector to its most important feature by taking its maximum value and naturally deals with variable input lengths.\nA final softmax layer takes the concatenation of the maximum values of the feature maps produced by all filters and outputs a probability distribution over all output classes."
    }, {
      "heading" : "4 Methodology",
      "text" : ""
    }, {
      "heading" : "4.1 Preprocessing",
      "text" : "We lower-case and tokenize the corpus, where applicable, keeping the 10,000 most frequent words as the vocabulary for each language and domain. For Chinese, in preparation for the previous step, we additionally segment the corpus using the mmseg Python library.\nListing 1: Example sentence with aspect and sentiment annotations for the English laptops domain.\n1 < s e n t e n c e i d =\" 347 : 0 \"> 2 < t e x t > I b o u g h t i t f o r r e a l l y cheap a l s o and i t s AMAZING. < / t e x t > 3 < Op in io n s > 4 <Op in io n c a t e g o r y=\"LAPTOP#PRICE \" p o l a r i t y =\" p o s i t i v e \" / > 5 <Op in io n c a t e g o r y=\"LAPTOP#GENERAL\" p o l a r i t y =\" p o s i t i v e \" / > 6 < / Op in io n s > 7 < / s e n t e n c e>"
    }, {
      "heading" : "4.2 Hyperparameters",
      "text" : "We randomly split off 20% of each training data set as a validation set. We use this to optimize hyperparameters via random search over a wide range of values.\nFor both tasks and all languages and domains, we use the following hyperparameters, which are similar to those reported by Kim (2014): mini-batch size of 10, maximum sentence length of 100 tokens, word embedding size of 300, dropout rate of 0.5, and 100 filter maps. We use filter lengths of 3, 4, and 5, and of 4, 5, and 6 for aspect extraction and aspect-based sentiment analysis respectively since these produced good results for the respective task.\nEnglish word embeddings are initialized with 300-dimensional GloVe vectors (Pennington et al., 2014) trained on 840B tokens of the Common Crawl corpus for the unconstrained submission. Word embeddings for the constrained submission, for all other languages, as well as for words not present in the pre-trained set of words are initialized randomly.\nWe train for 15 epochs using mini-batch stochastic gradient descent, the Adadelta update rule (Zeiler, 2012), and early stopping."
    }, {
      "heading" : "4.3 Aspect Category Detection",
      "text" : "To extract aspects, e.g. LAPTOP#PRICE and LAPTOP#GENERAL from sentences as in Listing 1, we cast aspect extraction as a multi-label classification problem and train a convolutional neural network (CNN) to output probability distributions over aspects, minimizing the crossentropy loss. To model multi-label output as a probability distribution, we define an aspect a’s probability p given a sentence s as p(a|s) = 1/n if a appears in s and s contains n aspects, otherwise p(a|s) = 0. We define a threshold f and discard all aspects with p(a|s) < f . After training, we select f maximizing the F1 score on the validation set.\nWe observe that aspect distributions vary significantly depending on the domain and language. For instance, the English laptops domain contains 82 aspects, while the restaurants domain only contains 13 aspects.\nIn every domain, we thus replace all aspects that occur less than 5 times with an OTHER aspect.1 E.g. this produces 51 aspects covering 98% of occurrences and all 13 aspects for the English laptops and restaurants domain respectively. For instance, in the English laptops domain, as-\n1We found that replacing all aspects with fewer than 5 occurrences yields the best trade-off between accuracy and recall.\npects such as HARDWARE#MISCELLANEOUS and BATTERY#USABILITY, which occur less than 5 times are replaced with OTHER during training. We add a NONE aspect to each sentence containing no aspect to enable the CNN to make no aspect prediction. During inference, every time the model predicts OTHER, the most frequent aspect replaced by OTHER for each domain is output instead. For the English laptops domain, this can be one of several aspects, e.g. SOFTWARE#QUALITY occurring four times. Finally, we discard all predicted NONE aspects.\nWe experimented with producing representations for the preceding and subsequent sentence to take context information into account, but this did not improve results."
    }, {
      "heading" : "4.4 Sentiment Polarity",
      "text" : "For aspect-based sentiment analysis, we feed the aspect vector together with the word embeddings of the input sentence into a CNN. To obtain the aspect vector, we follow an approach similar to the one used by Socher et al. (2013) to represent named entities: We split each aspect into its constituent tokens, e.g. RESTAURANT#GENERAL → restaurant, general. We embed the tokens of all aspects in an embedding space. We then look up the embedding of each of the tokens and average them to retrieve the aspect vector. This way, the model should learn that aspects sharing the same entity, e.g. restaurant are correlated without the need to train several tiered models to classify between entities (restaurant) and attributes (general).\nFor aspect-based sentiment analysis in the English language, we embed aspect tokens in the same embedding space as word tokens to exploit the semantics of pre-trained embeddings. For all other languages, we keep the embedding spaces separate, as aspect tokens are\nin English and word tokens are in the respective languages. Translating aspect tokens into the source language did not provide any benefits, but the use of pre-trained embeddings in the source language could ameliorate this.\nWe have experimented with different variants of adding aspect embeddings to our model: We evaluated summation, concatenation, and multiplication of word vectors and aspect vectors before the convolution, and multiplication and concatenation of the max-pooled sentence vector and the aspect vector after the convolution. We find that concatenating each word vector with the aspect vector before the convolution yields the best results.\nTo summarize, for the sentence in Listing 1 and the aspect LAPTOP#PRICE, our model first pads the input sentence, then looks up the embeddings of each of the input words. It creates the aspect vector by splitting LAPTOP#PRICE into the aspect tokens laptop and price. For these, it looks up the embeddings in the aspect embedding space (which is the same as for word embeddings for English) and averages both embeddings. The resulting aspect vector is then concatenated with each word vector, which are then concatenated to produce a 100x600 sentence matrix. Convolutions, maxpooling and softmax are applied to this matrix as described in section 3.\nWe observe for some languages an incremental performance improvement when using additional average-pooling as reported by Tang et al. (2014). We further note that simply using a low-dimensional embedding space to embed aspects leads to superior results on a few occasions when no pre-trained word embeddings are used."
    }, {
      "heading" : "SP REST 61.370 70.588 5/9",
      "text" : ""
    }, {
      "heading" : "5 Evaluation",
      "text" : "We have participated for all domains and languages in Slot 1: Aspect Category Detection and Slot 3: Sentiment Polarity. We report results for aspect extraction in Table 1 and results for aspect-based sentiment analysis in Table 2."
    }, {
      "heading" : "5.1 Aspect Category Detection",
      "text" : "Despite using only the input sentence as data, our system is able to achieve competitive performance for multilingual aspect extraction, placing first or second in 5 out of 11 languagedomain pairs. However, for English, Spanish, French, and Turkish, the differential with regard to the best performing system still remains large. We observe that initializing the system with general-purpose pre-trained embeddings\nprovides a significant performance boost, which is most pronounced in the English restaurants domain.\nConsequently, we hypothesize that the most straightforward way to overcome this performance differential is to initialize the system with embeddings trained on a large monolingual corpus in the target language. Incorporating domain information used by bestperforming systems (Pontiki et al., 2015) by pre-training on a large in-domain corpus such as the dataset released as part of the Yelp Dataset Challenge2 should further improve results.\nThe multi-label condition is currently enforced only after prediction through the application of a threshold, which may potentially discard promising aspects or retain erroneous ones, while the current normalization of aspect probabilities might lead to the loss of signals. To make the model more robust, the multi-label condition can be integrated more naturally into the architecture, e.g. by adapting the error function and using a trainable thresholding function as in (Zhang et al., 2006)."
    }, {
      "heading" : "5.2 Sentiment Polarity",
      "text" : "We report convincing results for multilingual aspect-based sentiment analysis, placing first or second for 7 out of 11 language-domain pairs. Again, the difference in performance compared to the best-performing system is largest for English, Spanish, French, and Turkish. To mitigate this, pre-trained word embeddings as described in 5.1 can be used.\nHowever, without relying on dependency and constituency trees (Nguyen and Shirai, 2015) or positional information (Tang et al., 2015), the model falls short of being able to reliably triangulate aspects, particularly in sentences with\n2https://www.yelp.com/dataset_challenge"
    }, {
      "heading" : "DU PHNS 83.333 83.333 1/3",
      "text" : ""
    }, {
      "heading" : "CH CAME 78.170 80.457 2/5",
      "text" : ""
    }, {
      "heading" : "CH PHNS 72.401 73.346 2/5",
      "text" : "opposing sentiments toward different aspects. Simply concatenating each word vector with the aspect vector does not seem to provide the model with enough expressiveness to model truly aspect-dependent sentiment. Training the model to associate different surface forms with their aspect instantiations should help to ameliorate this."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this paper, we have presented a deep learning-based approach to aspect-based sentiment analysis, which employs a convolutional neural network for aspect extraction and sentiment analysis as part of our submission to SemEval-2016 Task 5. We have demonstrated convincing results in the multilingual setting, which is particularly appropriate for neural networks due to their language and domain independence. We have evaluated our model, outlining weaknesses and potential future improvements."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This project has emanated from research conducted with the financial support of the Irish Research Council (IRC) under Grant Number EBPPG/2014/30 and with Aylien Ltd. as Enterprise Partner. This publication has emanated from research supported in part by a research grant from Science Foundation Ireland (SFI) under Grant Number SFI/12/RC/2289."
    } ],
    "references" : [ {
      "title" : "Koray Kavukcuoglu",
      "author" : [ "Ronan Collobert", "Jason Weston", "Leon Bottou", "Michael Karlen" ],
      "venue" : "and Pavel Kuksa.",
      "citeRegEx" : "Collobert et al.2011",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Convolutional Neural Networks for Sentence Classification",
      "author" : [ "Yoon Kim" ],
      "venue" : "Proceedings of the Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Kim.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kim.",
      "year" : 2014
    }, {
      "title" : "Sentiment Analysis and Opinion Mining",
      "author" : [ "Bing Liu" ],
      "venue" : "Synthesis Lectures on Human Language Technologies,",
      "citeRegEx" : "Liu.,? \\Q2012\\E",
      "shortCiteRegEx" : "Liu.",
      "year" : 2012
    }, {
      "title" : "Rectified Linear Units Improve Restricted Boltzmann Machines",
      "author" : [ "Nair", "Hinton2010] Vinod Nair", "Geoffrey E Hinton" ],
      "venue" : "Proceedings of the 27th International Conference on Machine Learning,",
      "citeRegEx" : "Nair et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Nair et al\\.",
      "year" : 2010
    }, {
      "title" : "PhraseRNN : Phrase Recursive Neural Network for Aspect-based Sentiment Analysis",
      "author" : [ "Nguyen", "Shirai2015] Thien Hai Nguyen", "Kiyoaki Shirai" ],
      "venue" : null,
      "citeRegEx" : "Nguyen et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Nguyen et al\\.",
      "year" : 2015
    }, {
      "title" : "Richard Socher",
      "author" : [ "Jeffrey Pennington" ],
      "venue" : "and Christopher D Manning.",
      "citeRegEx" : "Pennington et al.2014",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Ion Androutsopoulos",
      "author" : [ "Maria Pontiki", "Dimitrios Galanis", "John Pavlopoulos", "Haris Papageorgiou" ],
      "venue" : "and Suresh Manandhar.",
      "citeRegEx" : "Pontiki et al.2014",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Suresh Manandhar",
      "author" : [ "Maria Pontiki", "Dimitris Galanis", "Haris Papageorgiou" ],
      "venue" : "and Ion Androutsopoulos.",
      "citeRegEx" : "Pontiki et al.2015",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Jiménez-Zafra, and Gülşen Eryiğit",
      "author" : [ "Marianna Apidianaki", "Xavier Tannier", "Natalia Loukachevitch", "Evgeny Kotelnikov", "Nuria Bel", "Salud María" ],
      "venue" : null,
      "citeRegEx" : "Apidianaki et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Apidianaki et al\\.",
      "year" : 2016
    }, {
      "title" : "and Andrew Y",
      "author" : [ "Richard Socher", "Danqi Chen", "Christopher D. Manning" ],
      "venue" : "Ng.",
      "citeRegEx" : "Socher et al.2013",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Ting Liu",
      "author" : [ "Duyu Tang", "Furu Wei", "Nan Yang", "Ming Zhou" ],
      "venue" : "and Bing Qin.",
      "citeRegEx" : "Tang et al.2014",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Xiaocheng Feng",
      "author" : [ "Duyu Tang", "Bing Qin" ],
      "venue" : "and Ting Liu.",
      "citeRegEx" : "Tang et al.2015",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Zhi-hua Zhou",
      "author" : [ "Min-ling Zhang" ],
      "venue" : "and Senior Member.",
      "citeRegEx" : "Zhang et al.2006",
      "shortCiteRegEx" : null,
      "year" : 2006
    } ],
    "referenceMentions" : [ ],
    "year" : 2016,
    "abstractText" : "This paper describes our deep learningbased approach to multilingual aspectbased sentiment analysis as part of SemEval 2016 Task 5. We use a convolutional neural network (CNN) for both aspect extraction and aspect-based sentiment analysis. We cast aspect extraction as a multi-label classification problem, outputting probabilities over aspects parameterized by a threshold. To determine the sentiment towards an aspect, we concatenate an aspect vector with every word embedding and apply a convolution over it. Our constrained system (unconstrained for English) achieves competitive results across all languages and domains, placing first or second in 5 and 7 out of 11 language-domain pairs for aspect category detection (slot 1) and sentiment polarity (slot 3) respectively, thereby demonstrating the viability of a deep learning-based approach for multilingual aspect-based sentiment analysis.",
    "creator" : "LaTeX with hyperref package"
  }
}