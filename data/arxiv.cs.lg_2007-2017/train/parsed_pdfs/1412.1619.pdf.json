{
  "name" : "1412.1619.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Learning by Transferring from Auxiliary Hypotheses",
    "authors" : [ "Ilja Kuzborskij" ],
    "emails" : [ "ilja.kuzborskij@idiap.ch", "francesco@orabona.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "√ m). We also observe that if the combination is perfect,\nour theory formally backs up the intuition that learning is not necessary. On the other hand, if the source hypotheses combination is a misfit for the target task, we recover the usual learning rate. As a byproduct of our study, we also prove a new bound on the Rademacher complexity of the smooth loss class under weaker assumptions compared to previous works."
    }, {
      "heading" : "1 Introduction",
      "text" : "In the standard supervised machine learning setting the learner receives a set of labeled examples, known as a training set. However, in practice, very often we have additional information at hand that can be beneficial to the learning process. One such example is the use of unlabeled data drawn from the marginal distributions, that gives rise to the semi-supervised learning setting [Chapelle et al., 2006]. Another example is when the training data is coming from a related problem, as in multi-task learning [Caruana, 1998], domain adaptation [Ben-David et al., 2010, Mansour et al., 2009], and transfer learning [Pan and Yang, 2010, Taylor and Stone, 2009]. Among others, there is the use of structural information, such as taxonomy, or different views on the same data [Blum and Mitchell, 1998], or even a sort of privileged information [Vapnik and Vashist, 2009]. In the recent years all these directions have received a considerable empirical and theoretical attention. In this work we focus on a less theoretically studied direction in the use of supplementary information – learning with auxiliary hypotheses, that is classifiers or regressors originating from another tasks. More in details, the learner is supplied with a collection of hypotheses in addition to the training set. The goal of the learner is to figure out which ones are helpful and combine them to improve the performance\nar X\niv :1\n41 2.\n16 19\nv1 [\ncs .L\nG ]\n4 D\non the target problem. We will call these auxiliary hypotheses the source hypotheses and we will say that helpful ones accelerate the learning on the target task.\nIndeed, the scenario described above is a case of transfer learning, or learning effectively from possibly small amount of data by reusing the prior knowledge. A first formalization and theoretical treatment of transfer learning under the use of source hypothesis was given by Kuzborskij and Orabona [2013b] in the Hypothesis Transfer Learning (HTL) framework, albeit empirically it has already been extensively exploited in the past [Yang et al., 2007, Orabona et al., 2009, Tommasi et al., 2010, Luo et al., 2011, Kuzborskij et al., 2013]. The attractive quality of an HTL framework is the fact that we assume very little about source hypotheses, that is, we do not need any knowledge about the tasks they come from, or any information about the model or the way they were trained. In other words, we treat each hypothesis as a “black box”. This offers an advantage compared to transfer learning and domain adaptation frameworks where one requires access to the data of the source domain. For example, in domain adaptation [Ben-David et al., 2010], one employs large unlabeled samples to estimate the relatedness of source and target domains to perform adaptation. Even if unlabeled data are abundant, the estimation of adaptation parameters can be computationally prohibitive. A hypothetical example is a large number of domains involved, or, for instance, when one acquires new domains incrementally. There, keeping unlabeled data from all the domains and re-estimating the parameters is a necessity. In HTL we naturally alleviate these practical limitations through indirect access to the source domain by means of a source hypothesis.\nAs was mentioned above, the first theoretical analysis of HTL was carried out by Kuzborskij and Orabona [2013b], where we analyzed HTL problem cast as a regularized least-squares with a single fixed, unweighted, source hypothesis. There, we came up with polynomial generalization bound that depends on the performance of that fixed source hypothesis on the target task. In this work we considerably extend and generalize the theory of HTL. Our contributions. We formulate a general Hypothesis Transfer Learning problem through regularized Empirical Risk Minimization (ERM) with respect to any non-negative smooth loss function and any strongly convex regularizer. For its solution we prove high-probability generalization bounds that exhibit fast rate, i.e. O(1/m), of convergence whenever any weighted combination of multiple source hypotheses performs well on the target task. In other words, we show theoretically that HTL facilitates a faster generalization. In addition, we show that, if the combination is perfect, the error on the training set becomes the error we observe on the whole target distribution. Furthermore, we analyze an excess risk of our formulation, and conclude that a good source hypothesis also speeds up the convergence to the performance of the bestin-the-class. As a byproduct of our study, we prove an upper bound on the Rademacher complexity of a smooth loss class that provides extra information compared to that of Lipschitz loss classes. This is an alternative to the analysis of Srebro et al. [2010a] that holds under much weaker assumptions, and it might be of independent interest.\nThe rest of the paper is organized as follows. In the next section we make a brief review of the previous work. Next, we formally state the HTL in Section 4 and present main results right after in Section 5. In Section 5.1 we discuss the implications and compare them to the body of literature in transfer learning and learning with the fast rates. Next, in Section 6, we present the proofs of our main results. Section 7 concludes the paper."
    }, {
      "heading" : "2 Related Work",
      "text" : "The framework of Hypothesis Transfer Learning (HTL) that we address in this paper was first formally introduced and studied theoretically by Kuzborskij and Orabona [2013b]. It was shown that the generalization ability of the regularized least-squares HTL algorithm improves if supplied source hypothesis performs well on the target task. More specifically, we proposed a key criterion, the risk of the source on the target domain, that captures the relatedness of the source and target domains. Later, Ben-David and Urner [2013] showed a similar bound, but with different source performance criterion. Instead of considering general source hypothesis, they have confined their analysis to the linear hypothesis class. This allowed them to\nshow that the target hypothesis generalizes better when it is close to the good source hypothesis. From this perspective it is easy to interpret the source hypothesis as an initialization point in the hypothesis class. Naturally, given a starting position that is close to the best in the class, one generalizes well.\nPrior to these works there were few studies trying to understand the learning with auxiliary hypotheses subject to different conditions. Li and Bilmes [2007] have analyzed a Bayesian approach to HTL. Employing a PAC-Bayes analysis they showed that given a prior on the hypothesis class, the generalization ability of logistic regression improves if the prior is informative on the target task. Mansour et al. [2008] analyzed a setting of multiple source hypotheses combination. There, in addition to the source hypotheses, the learner receives unlabeled samples drawn from the source distributions, that are used to weight and combine these source hypotheses. They have studied the possibility of learning in such a scenario, however, did not address generalization properties of any particular algorithm.\nUnlike these works, we focus on the generalization ability of a large family of HTL algorithms, that generates the target predictor given a set of multiple source hypotheses. In particular, we analyze HTL through Regularized Empirical Risk Minimization with the choice of any non-negative smooth loss and any strongly convex regularizer. Thus our analysis covers a wide range of algorithms, including many empirically successful ones. One category of those, prevalent in computer vision [Kienzle and Chellapilla, 2006, Yang et al., 2007, Tommasi et al., 2010, Aytar and Zisserman, 2011, Kuzborskij et al., 2013, Tommasi et al., 2014], employs the principle of biased regularization [Schölkopf et al., 2001]. For example, instead of penalizing large weights by introducing norm ‖w‖2 into the objective function, one enforces them to be close to some “prior” model, that is ‖w−wprior‖2. This principle also found its applications in other fields, such as NLP [Daumé III, 2007, Daumé III et al., 2010], and electromyography classification [Orabona et al., 2009, Tommasi et al., 2013]. Many empirical works have also investigated the use of the source hypotheses in a “black box” sense rather than biased regularization, sometimes not even posing the problem as a transfer learning [Duan et al., 2009, Li et al., 2010, Luo et al., 2011, Bergamo and Torresani, 2014], and recently in conjunction with deep neural networks [Hoffman et al., 2013, Oquab et al., 2014].\nIn the literature there are several other machine learning directions conceptually similar to the one we consider in this work. Arguably, the most well known one is the Domain Adaptation (DA) problem. The standard machine learning assumption is that the training and the testing sets are sampled from the same probability distribution. In such case, we expect that a hypothesis generated by the learner from that training set will lead to sensible predictions on the testing set. The difficulty arises when training and testing distributions differ, that is we have a training set sampled from the source domain and testing set from the target domain. Clearly, the hypothesis generated from the source domain can perform arbitrarily badly on the target one. A paradigm of DA, addressing this issue has received a lot of attention in recent years [Ben-David et al., 2010, Mansour et al., 2009]. Although, this framework is different from the one we study in this work, we identify similarities and compare our findings with the theory of learning from different domains in Section 5.2."
    }, {
      "heading" : "3 Definitions",
      "text" : "In this section we introduce the definitions used in the rest of the paper.\nWe denote random variables by capital letters. The expected value of a random variable distributed according to a probability distributionD is denoted by EX∼D[X] and the variance is denoted by VarX∼D[X]. The small and capital bold letters will stand respectively for the vectors and matrices, e.g. x = [x1, . . . , xd]> and A ∈ Rd1×d2 . Denoting by X and Y respectively the input and output space of the learning problem, the training set is S = {(xi, yi)}mi=1, drawn i.i.d. from the probability distribution D defined over X ×Y . Without the loss of generality we will have X = {x : ‖x‖ ≤ 1} and we will focus on the problems where Y = [−C,C].\nTo measure the accuracy of a learning algorithm, we introduce a non-negative loss function `(h(x), y), which measures the cost incurred predicting h(x) instead of y. The risk of a hypothesis h, with respect to\na probability distribution D, and the empirical risk measured on the sample S are then defined as\nR(h) := E (x,y)∼D\n[`(h(x), y)], and R̂S(h) := 1\nm m∑ i=1 `(h(xi), yi).\nIn the following, the risk is measured with respect to the probability distribution of the target domain, unless stated otherwise. We capture the smoothness of the loss function via following definition.\nH-smooth loss function. We say a non-negative loss function ` : Y × Y 7→ R+ is H-smooth iff,\n∀t, r ∈ R,∀y ∈ Y, |∇t`(t, y)−∇r`(r, y)| ≤ H|t− r|.\nIn this work we will make use of strongly convex regularizers, functions that are defined as follows.\nStrongly convex function. A function Ω is σ-strongly convex w.r.t. a norm ‖ · ‖ iff for all w,v, and α ∈ (0, 1) we have\nΩ(αw + (1− α)v) ≤ αΩ(w) + (1− α)Ω(v)− σ 2 α(1− α)‖w − v‖2.\nWe will quantify the complexity of a hypothesis class by the means of Rademacher complexity [Bartlett and Mendelson, 2003]. In particular, the empirical Rademacher complexity of the hypothesis classH measured on the sample S and its expectation are defined as\nR̂S(H) := E ε [ sup h∈H 1 m m∑ i=1 εih(xi) ] and R(H) := E S [ R̂S(H) ] .\nHere, εi is a random variable such that P(εi = 1) = P(εi = −1) = 12 . Similarly, as in the case of the risk, the Rademacher complexity is measured with respect to the probability distribution of the target domain, unless stated otherwise."
    }, {
      "heading" : "4 Transferring from Auxiliary Hypotheses",
      "text" : "In the following we will try to capture and generalize transfer learning problems that employ a collection of hypotheses as a prior knowledge, which originate from the different tasks. We will refer to the set of these given hypotheses as source hypotheses. These problems typically involve some criterion for source hypothesis selection and combination with the goal to increase performance on the target task. Indeed, some source hypotheses might come from tasks similar to the target task and the goal of a transfer learning algorithm is to select only relevant ones. In this work we will focus on the Regularized ERM formulations of such transfer learning problems. As an example of such, consider a common transfer learning problem: the least squares with biased regularization [Schölkopf et al., 2001].\nLeast-Squares with Biased Regularization. Given the target training set S = {(xi, yi)}mi=1, source hypotheses {wsrci }ni=1 ⊂ H, parameters β ∈ Rn and λ ∈ R+, the algorithm generates the target hypothesis h(x) = 〈ŵ,x〉, where\nŵ = argmin w∈H\n{ 1\nm m∑ i=1 (〈w,xi〉 − yi)2 + λ ‖w −Wsrcβ‖22\n} . (1)\nThis problem has a simple intuitive interpretation: minimize the training error on the target training set while keeping the solution close to the linear combination of the source hypotheses. Indeed, one can naturally arrive at (1) from a probabilistic perspective. The solution ŵ is a maximum a posteriori estimate when the conditional distribution is Gaussian and the prior is a Wsrcβ-mean, 1λI-covariance Gaussian distribution. Similar reasoning can be applied to other distributions from the exponential family, giving rise to different regularizers. Even though biased regularization is quite a simple idea, it found success in many transfer learning applications, ranging from computer vision [Kienzle and Chellapilla, 2006, Yang et al., 2007, Tommasi et al., 2010, Aytar and Zisserman, 2011, Kuzborskij et al., 2013, Tommasi et al., 2014] to NLP [Daumé III, 2007], to electromyography [Orabona et al., 2009, Tommasi et al., 2013].\nAlbeit practically appealing, the formulation (1) is limited in the fact that the source hypotheses must be a linear predictor living in the same space of the target predictor. Instead, we will assume that any source hypothesis shares only the input and output space with our target problem. In other words, we will see the source hypothesis as a “black box”. Next, we formulate the general HTL problem that we will consider in the rest of the paper.\nHaving the source hypothesis set {hsrci }ni=1, we will be interested in target hypotheses of the form\nhw,β(x) := 〈w,x〉+ hsrcβ (x), (2)\nwhere\nhsrcβ (x) := n∑ i=1 βih src i (x) .\nIn the following we will pay special attention to a quantity that captures the performance of the source hypothesis combination hsrcβ (x) on the target domain\nRsrc := R(hsrcβ ).\nNow we formulate a problem that generalizes the biased regularization approach for transfer learning. We also generalize it in the choice of any non-negative smooth loss function and any strongly-convex regularizer. This puts our problem into the class of the ones that can be solved efficiently, yet endowed with interesting properties.\nHypothesis Transfer Learning through Regularized ERM. Let ` : Y × Y 7→ R+ be an H-smooth loss function and let Ω : H 7→ R+ be a σ-strongly convex function w.r.t. a norm ‖ · ‖. Given the target training set S = {(xi, yi)}mi=1, source hypotheses {hsrci }ni=1, parameters β obeying Ω(β) ≤ ρ, and λ ∈ R+, the algorithm generates the target hypothesis hŵ,β, such that\nŵ = argmin w∈H\n{ 1\nm m∑ i=1 ` ( 〈w,xi〉+ hsrcβ (x), yi ) + λΩ(w)\n} . (3)\nClaim 1 Least-Squares with Biased Regularization is a special case of Hypothesis Transfer Learning through Regularized ERM.\nProof Introduce w′, such that w′ = w −Wsrcβ. Then we have that problem (1) is equivalent to\nmin w∈H\n{ 1\nm m∑ i=1 (〈w′ + Wsrcβ,xi〉 − yi) 2 + λ ‖w′‖22\n} ,\nthat in turn is a special version of (3) when hsrci (x) = 〈wsrci ,x〉, we use the square loss, and ‖ · ‖22 as regularizer.\nNote that (3) is minimized only w.r.t. w, that is we do not analyze any particular algorithm that searches for the optimal weights of the source hypotheses. However, we assume that Ω(β) ≤ ρ, that is we constrain β through a strongly convex function. Thus, we take into account potential regularized algorithms generating β, which include most of the empirical work in this field.\nOur analysis will focus on the generalization properties of hŵ,β, that is the solution to this HTL problem. In particular, our main goal will be to understand the impact of the source hypothesis combination on the performance of the target hypothesis. In our analysis we will discuss various regimes of interest, for example considering the perfect and arbitrarily bad source hypothesis. Our discussion will touch scenarios where the hypothesis transfer accelerates the learning and the conditions when we can provably expect perfect generalization. Finally, we will consider the consistency of Hypothesis Transfer Learning algorithms and pinpoint conditions when we achieve faster convergence to the performance of the best-in-the-class."
    }, {
      "heading" : "5 Main Results",
      "text" : "In this section, we present the main results of this work: generalization and excess risk bounds for the Hypothesis Transfer Learning through Regularized ERM. In the next section we discuss in detail the implications of these results, while we defer the proofs to the subsequent sections.\nThe first bound demonstrates the utility of the perfect combination of source hypotheses, while the second lets us observe the dependency on the arbitrary combination. In particular, the first bound explicitates the intuition that given the perfect source hypothesis learning is not required. In other words, whenRsrc = 0 we have that the empirical risk becomes equal to the risk with probability one. In addition, we present the second bound, implying a fast rate of the empirical risk convergence, subject to the quality of the source hypothesis combination.\nTheorem 2 Let hŵ,β be generated by HTL through Regularized ERM, given the m-sized training set S sampled i.i.d. from the target domain, source hypotheses {hsrci : ‖hsrci ‖∞ ≤ 1}ni=1, any source weights β obeying Ω(β) ≤ ρ, and λ ∈ R+. Assume that `(hŵ,β(x), y) ≤ M for any (x, y) and any training set. Then, denoting κ = Hσ and assuming that λ ≤ κ, we have with probability at least 1− e −η, ∀η ≥ 0\nR(hŵ,β) ≤ R̂S(hŵ,β) +O Rsrcκ√mλ + √ Rsrcρκ2 mλ +\nMη\nm log ( 1 + √ Mη usrc\n)  (4)\n≤ R̂S(hŵ,β) +O ( κ√ m ( Rsrc λ + √ Rsrcρ λ ) + κ m (√ RsrcMη λ + √ ρ λ )) , (5)\nwhere usrc = Rsrc ( m+ κ √ m λ ) + κ √ Rsrcmρ λ .\nNow we focus on the consistency of the HTL. Specifically, we show an upper bound on the excess risk of the HTL through Regularized ERM, which depends on Rsrc, that is the risk of the combined source hypothesis hsrcβ on the target domain. We observe that for a small R\nsrc, the excess risk shrinks at a fast rate of O(1/m). In other words, a good prior knowledge guarantees not only good generalization, but also the fast recovery of the performance of the best hypothesis in the class.\nThis bound is similar in spirit to the results of localized complexities, as in works of Bartlett et al. [2005], Srebro et al. [2010a], however we focus on the HTL scenario rather than a generic learning setting. Later, in Section 5.1, we compare our bounds to these works and show that our analysis achieves superior results.\nTheorem 3 Let hŵ,β be generated by HTL through Regularized ERM, given the m-sized training set S sampled i.i.d. from the target domain, source hypotheses {hsrci : ‖hsrci ‖∞ ≤ 1}ni=1, any source weights β obeying Ω(β) ≤ ρ, and λ ∈ R+. Then, denoting κ = Hσ , assuming that λ ≤ κ ≤ 1, and setting the regularization parameter\nλ = O √κ τ Rsrc + √ Rsrcρ√ m + √ κ τ √ Rsrc + √ Rsrcρ m1.5  , for any choice of τ ≥ 0, we have with high probability that\nR(hŵ,β)− min Ω(w)≤τ R(hw,β) = O\n(√ Rsrc + 4 √ Rsrcρ\n4 √ m\n√ κτ +\n4 √ Rsrc + 8 √ Rsrcρ\n4 √ m1.5\n4 √ κτ2 +\n√ Rsrc\nm +\n1\nm\n) ."
    }, {
      "heading" : "5.1 Implications",
      "text" : "We start by discussing the effect of the source hypothesis combination on the generalization ability of the HTL algorithm. Intuitively a good source hypothesis combination should facilitate transfer learning, while a reasonable algorithm must not fail if we provide it with the bad one. That said, a natural question to ask here is, what does it make a good or bad source hypothesis? As in previous works in transfer learning and domain adaptation, we capture this notion via a quantity that has two-fold interpretation: (1) the performance of the source hypothesis combination on the target domain; (2) relatedness of source and target domains. In the theorems presented in the previous sections we denoted it by Rsrc, that is the risk of the source hypothesis combination on the target domain. In this section we will consider various regimes of interest with respect to Rsrc. Namely, we will look into scenarios when it is a bad fit for the target task, a good one, and a perfect match, that is Rsrc = 0.\nWhen the source is a bad fit. First consider the case when the source hypothesis combination hsrcβ is useless for the purpose of transfer learning, for example, hsrcβ (x) = 0 for all x. This corresponds to learning with no auxiliary information. Then we can assume that Rsrc ≤ M , and from Theorem 2 we obtain R(hŵ) − R̂S(hŵ) ≤ O (1/( √ mλ)). This rate matches the one in the analysis of regularized leastsquares [Vito et al., 2005, Bousquet and Elisseeff, 2002], that is a special case of the smooth loss function that the HTL through Regularized ERM employs. On the other hand, Srebro et al. [2010a] showed a better worst-case rate O(1/ √ mλ). However, their framework builds upon a worst case Rademacher complexity which does not involve the expectation over the sample and does not lead to the dependency on Rsrc we have obtained in Theorem 2. We will discuss this problem in details later.\nWhen the source is a good fit. Here we would like to consider the behavior of the algorithm in the finitesample and asymptotic scenarios. We first look at the case when Rsrc is small for any finite m, in particular Rsrc = O(1/m). The fast rate term will dominate the bound, and we obtain the convergence rate of O(√ρ/(m √ λ)). In other words, we can expect a faster convergence for any finite number of samples when Rsrc is small. Furthermore, if we assume that ρ = O(1/ √ m), the bound switches to the fast rate with the weaker assumptionRsrc = O(1/ √ m). Now consider the asymptotic behavior of the algorithm, particularly\nwhen m goes to infinity. In such case, the algorithm exhibits a rate of O ( Rsrc/ √ mλ+ √ (Rsrcρ)/mλ ) ,\nso Rsrc governs the constant factor of the rate. Hence, the quantity Rsrc governs the finite sample and asymptotic behavior of the algorithm, predicting a faster convergence in both regimes when it is small.\nWhen source is a perfect fit. It is conceivable that the source hypothesis exploited is the perfect one, that is Rsrc = 0. In other words, the source hypothesis combination is a perfect predictor for the target domain. Theorem 2 implies that R(hŵ,β) = R̂S(hŵ,β) with probability one. We note that for many practically used smooth losses, such as square loss, this setting is only realistic if source and target domains match and the problem is noise-free. However, we can observe Rsrc = 0, for example, when the squared hinge loss, `(z, y) = max{0, 1 − zy}2, is used and all target domain examples are classified correctly by the source hypothesis combination, case that is not unthinkable for related domains.\nFast rates. There is a number of works in the literature investigating a rate of convergence faster than 1/ √ m subject to different conditions. In particular, the localized Rademacher complexity bounds of Bartlett et al. [2005] and Bousquet [2002] can be used to obtain results similar to the second inequality of Theorem 2. Indeed, Theorem 7 shows a bound which is very similar to the localized ones, albeit with two differences. The r.h.s. of the first inequality in Theorem 7 vanishes when the loss class has zero variance. Though intuitively trivial, this allows to prove a considerable result in the theory of transfer learning as it quantifies the intuition that no learning is necessary if the source has perfect performance on the target task. Second, by applying the standard localized Rademacher complexity bounds of Bousquet [2002], and assuming the use of the Lipschitz loss function, we do not achieve a fast rate of convergence, as can be seen from Theorem 13, shown in the Appendix. We suspect that assuming the smoothness of the loss function is crucial to prove fast rates in our formulation.\nFast rates for ERM with the smooth loss have been thoroughly analyzed by Srebro et al. [2010a]. Yet, the analysis of our HTL algorithm within their framework would yield a bound that is inferior to ours in two respects. The first concerns the scenario when the combined source hypothesis is perfect, that is Rsrc = 0. The generalization bound of Srebro et al. [2010a] does not offer a way to show that the empirical risk converges to the risk with probability one – instead one can only get a fast rate of convergence. The second problem is in the fact that such bound would depend on the empirical performance of combined source hypothesis. As we have noted before, the quantityRsrc is essential because it captures the degree of relatedness between two domains. In their bounds, one cannot obtain this relationship through the Rademacher complexity term as we did in our analysis. The reason for this is the stronger notion of Rademacher complexity that is employed by that framework, involving a supremum over the sample instead of an expectation. The expectation over the sample of the target distribution is crucial here, because it allows us to quantify how well the source domain is aligned with the target domain, through the source hypothesis acting as a link. However, one can attempt to obtain the bound on the empirical risk in terms ofRsrc. We prove such a bound in the Appendix, Theorem 11, and conclude that if one has a good source hypothesis or even a perfect one, the rate is O(1/ 4 √ m3), which is worse than ours."
    }, {
      "heading" : "5.2 Comparison to Theories of Domain Adaptation and Transfer Learning",
      "text" : "The setting in DA is different from the one we study, however, we will briefly discuss the theoretical relationship between the two. Typically in DA, one trains a hypothesis from an altered source training set, striving to achieve good performance on the target domain. The key question here is how to alter, or to adapt, the source training set. To answer this question, DA literature introduces the notion of domain relatedness, which quantifies the dissimilarities between the distributions of corresponding domains. Practically, in some cases the domain relatedness can be estimated through a large set of unlabeled samples drawn from both source and target domains. Theories of DA [Ben-David et al., 2010, Mansour et al., 2009, Ben-David and Urner, 2012, Mansour et al., 2008, Cortes and Mohri, 2014] have proposed a number of such domain relatedness criteria. Perhaps the most well known are the dH∆H-divergence [Ben-David et al., 2010] and its more general counterpart, the Discrepancy Distance [Mansour et al., 2009]. Typically, this divergence is explicitated in the generalization bound along with other terms controlling the generalization on the target domain. Let RDtrg(h) and RDsrc(h) denote the risks of the hypothesis h, measured w.r.t. the target and source distributions. Then a well-known result of Ben-David et al. [2010] suggests that for all h ∈ H\nRDtrg(h) ≤ RDsrc(h) + dH∆H(Dsrc,Dtrg) + ε?H, (6)\nwhere ε?H = minh∈H {RDtrg(h) +RDsrc(h)}. As we see, this result implies that adaptation is possible given that dH∆H(Dsrc,Dtrg) and ε? are small. One can try to reduce those by controlling the complexity of the class H and by minimizing the divergence dH∆H(Dsrc,Dtrg). In practice the latter can be manipulated through an empirical counterpart on the basis of unlabeled samples. Increasing the complexity ofH indeed reduces ε?, but inflates dH∆H(Dsrc,Dtrg). On the other hand, by minimizing dH∆H(Dsrc,Dtrg) alone puts us under the risk of increasing ε?, since the empirical divergence is reduced without taking the labelling into account.\nClearly, this bound cannot be directly compared to our result, Theorem 2. However, we note the term Rsrc appearing in our results, which plays a role very similar to dH∆H in (6). In fact, by defining H = {x 7→ 〈β,hsrc(x)〉 : Ω(β) ≤ τ}, where hsrc(x) = [hsrc1 (x), . . . , hsrcn (x)]>, and fixing h = hsrcβ ∈ H in (6), we can write\nRsrc = RDtrg(h src β ) ≤ RDsrc(hsrcβ ) + dH∆H(Dsrc,Dtrg) + ε?H.\nThis bound connects HTL criterion of domain relatedness, Rsrc, to that of domain adaptation. Plugging this into HTL generalization bound (5) and assuming that λ ≤ 1 and ρ ≤ 1/λ we have for the target hypothesis h that\nRDtrg(h) ≤ R̂S(h) +O\n( RDsrc(h\nsrc β ) + dH∆H(Dsrc,Dtrg) + ε?H√\nmλ +\n1\nmλ\n) . (7)\nAlbeit this inequality shows the generalization ability of the transfer learning algorithm, comparing to (6), we observe that DA and HTL agree on the fact that the divergence between the domains has to be small to generalize well. In fact, in the HTL formulation we consider, the divergence is controlled in two ways: implicitly, by the choice of hsrc and through the complexity of classH, that is by choosing τ . Second, in DA we expect that hypothesis performs well on the target only if it performs well on the source. In HTL, this requirement is relaxed. As a side note, we observe that (7) captures an intuitive notion that a good source hypothesis has to perform well on its own domain. Finally, in the theory of DA ?H is assumed to be small. Indeed, if ?H is large, there is no hypothesis that is able to perform well on both domains simultaneously, and therefore adaptation is hopeless. In HTL the algorithm can still generalize even with large ?H, however this is due to the supervised nature of the framework.\nWe now turn our attention to the previous theoretical works studying HTL-related settings. Few papers have addressed the theory of transfer learning, where the only information passed from the source domain is the classifier or regressor. Mansour et al. [2008] have addressed the problem of multiple source hypotheses combination, however, in a different HTL setting. Specifically, in addition to the source hypotheses, the learner receives the unlabeled samples drawn from the source distributions, that ared used to weight and combine these source hypotheses. The authors have presented a general theory of such a scenario and did not study the generalization properties of any particular algorithm. The first analysis of the generalization ability of HTL in the similar context we consider here was done by Kuzborskij and Orabona [2013b,a]. The work focused on the L2-regularized least squares and the generalization bound involving the leaveone-out risk instead of the empirical one. The following result, obtained through an algorithmic stability argument [Bousquet and Elisseeff, 2002], holds with probability at least 1− δ\nR(h) ≤ R̂looS (h) +O\n( 4 √ Rsrc√\nmδλ0.75\n) , (8)\nwhere Rsrc is the risk of a single fixed source hypothesis and h is the solution of a Regularized Least Square problem. We first observe that the shape of the bound is similar to the one obtained in this work, although with the number of differences. First, the bound assumes the use of a fixed source hypothesis, that is not even weighted by any coefficient. In practice, this is a very strong assumption, as one can receive an arbitrarily bad source and have no way to exclude it. Our formulation, HTL through Regularized ERM, and the corresponding generalization bounds of Theorem 2 take into account the weighting of multiple source hypotheses. This is a much more powerful concept, since we address HTL algorithms that can select and weight subsets of source hypotheses that are well aligned with a target task. Second, the bound (8) seems to have a vanishing behavior whenever the risk of the source Rsrc is equal to zero. This comes at the cost of the use of a weaker concentration inequality. In Theorem 2 we manage to obtain the same behavior with high probability. Finally, we get a better dependency on Rsrc. In addition to generalization bounds, in this work we also prove novel high probability excess risk bounds."
    }, {
      "heading" : "6 Technical Results and Proofs",
      "text" : "In this section we present general technical results that are used to prove our theorems.\nFirst, we present the Rademacher complexity generalization bound in Theorem 7, which slightly differs from the usual ones. The difference comes in the assumption that the variance of the loss is uniformly bounded over the hypothesis class. This will allow us to state a generalization bound that obeys the fast empirical risk convergence rate subject to the small class complexity. Second, we will also show a generalization bound with the confidence term that vanishes if the complexity of the class is exactly zero. In other words, for the class with zero complexity, the empirical risk becomes equal to the risk with probability one.\nNext, we focus on the Rademacher complexity of the smooth loss function class. Similarly as in previous works [Srebro et al., 2010a], we exploit additional information about the behavior of the hypothesis coming from the gradient of the loss function. This allows us to prove a bound on the empirical Rademacher complexity of a hypothesis class, Lemma 8, that depends on the point-wise bounds on the loss function. This contrasts with Srebro et al. [2010a], who consider smooth losses as well, but use a much more restrictive notion of Rademacher complexity. This novel bound might be of independent interest. Finally, we employ this result to analyze the effect of the source hypotheses on the complexity of the target hypothesis class in Theorem 10."
    }, {
      "heading" : "6.1 Fast Rate Generalization Bound",
      "text" : "The proof of fast-rate and vanishing-confidence-term bounds, Theorem 7, stems from the functional generalization of Bennett’s inequality which is due to Bousquet [2002, Theorem 2.11] and that we report here for completeness.\nTheorem 4 (Bousquet [2002]) Let X1, X2, . . . , Xm be identically distributed random variables according to D. For all D-measurable, square-integrable g ∈ G, with EX [g(X)] = 0, and supg∈G ess sup g ≤ 1, we denote\nZ = sup g∈G m∑ i=1 g(Xi). (9)\nLet σ be a positive real number such that supg∈G VarX∼D[g(X)] ≤ σ2 almost surely. Then for all t ≥ 0, we have that P (Z ≥ E[Z] + t) ≤ exp ( −vu ( t\nv\n)) , (10)\nwhere\nv = mσ2 + 2E[Z], u(y) = (1 + y) log(1 + y)− y.\nThe following technical lemma will be used to invert the right hand side of (10).\nLemma 5 Let a, b > 0 such that b = (1 + a) log(1 + a)− a. Then a ≤ 3b 2 log( √ b+1) .\nProof It is easy to verify that the inverse function f−1(b) of f(a) := (1 + a) log(1 + a)− a is\nf−1(b) = exp [ W ( b− 1 e ) + 1 ] − 1,\nwhere the function W : R+ → R is the Lambert function that satisfies\nx = W (x) exp (W (x)) .\nHence, to obtain an upper bound to a, we need an upper bound to the Lambert function. We use Theorem 2.3 in Hoorfar and Hassani [2008], that says that\nW (x) ≤ log x+ C 1 + log(C) , ∀x > −1 e , C > 1 e .\nSetting C = √ b+1 e , we obtain\na = f−1(b) ≤ e b−1 e +\n√ b+1 e\n1 + log( √ b+1 e )\n− 1 = b+ √ b\nlog( √ b+ 1) − 1 ≤ 3b 2 log( √ b+ 1) ,\nwhere in the last inequality we used the fact that x+ √ x− log( √ x+ 1) ≤ 32x,∀x ≥ 0, as it can be easily verified comparing the derivatives of both terms.\nThe following lemma is a standard tool [Mohri et al., 2012, (3.8)-(3.13)], [Bartlett and Mendelson, 2003], that is used to relate the expected uniform deviation of empirical risk over the hypothesis class to the Rademacher complexity of that class.\nLemma 6 (Symmetrization) For any f ∈ F , given random variables S = {Xi}mi=1, we have\nE S sup f∈F { E X [f(X)]− 1 m m∑ i=1 f(Xi) } ≤ 2R(F),\nE S sup f∈F\n{ 1\nm m∑ i=1 f(Xi)− E X [f(X)]\n} ≤ 2R(F).\nNow we are ready to present the proof of Theorem 7.\nTheorem 7 Consider the non-negative loss function ` : Y × Y 7→ R+, such that 0 ≤ `(h(x), y) ≤ M for any h ∈ H and any (x, y) ∈ X × Y . In addition, let the training set S of size m be sampled i.i.d. from the probability distribution over X ×Y . Also for any r ≥ 0, define the loss class with respect to the hypothesis classH as,\nL := {(x, y) 7→ `(h(x), y) : h ∈ H ∧ R(h) ≤ r} .\nThen we have for all h ∈ H, and any training set S of size m, with probability at least 1− e−η, ∀η ≥ 0\nR(h)− R̂S(h) ≤ 2R(L) + 3Mη\nm log ( 1 + √ 2Mη vm\n) ≤ 2R(L) + 3√vMη 2m + 3Mη 2m ,\nwhere v = 4R(L) + r.\nProof To prove the statement, we will consider the uniform deviations of the empirical risk. Namely, we will show an upper bound on the random variable suph∈H { R(h)− R̂S(h) } . For this purpose, we will use the functional generalization of Bennett’s inequality given by Theorem 4. Consider the random variable\nZ := m\n2M sup h∈H\n{ R(h)− R̂S(h) } .\nUsing Theorem 4, we have P ( m\n2M sup h∈H\n{ R(h)− R̂S(h) } ≥ m 2M E [ sup h∈H { R(h)− R̂S(h) }] + t ) ≤ exp ( −vu ( t v )) , (11)\nwhere,\nv = mσ2 + m M E [ sup h∈H { R(h)− R̂S(h) }] , (12)\nσ2 ≥ sup h∈H Var(x,y)\n[ 1\n2M\n( `(h(x), y)− E\n(x′,y′) [`(h(x′), y′)]\n)] .\nWe now need two things: invert the r.h.s. of (11), treating it as a function of t, and provide an upper-bound on v. For the first part, recall that u(y) = (1 + y) log(1 + y) − y. To give an upper-bound of t, we apply Lemma 5 with a = tv , and b = 1 vη. This leads to the inequalities\nt v ≤ 3η 2v log ( 1 + √ η v ) ≤ 3η 4v + 3 2\n√ η\nv .\nUsing this fact, we have with probability at least 1− e−η with any η ≥ 0\nm\n2M sup h∈H\n{ R(h)− R̂S(h) } ≤ m 2M E [ sup h∈H { R(h)− R̂S(h) }] +\n3η 2 log ( 1 + √ η v ) (13) ≤ m 2M E [ sup h∈H { R(h)− R̂S(h) }] + 3 4 η + 3 2 √ vη. (14)\nNext we prove the bound on v. We first show that the variance of centered loss function, σ2, is uniformly bounded by the Rademacher complexity. From the definition of variance we have\nsup h∈H E (x,y)\n[ 1\n4M2\n( `(h(x), y)− E\n(x′,y′) [`(h(x′), y′)] )2] ≤ sup h∈H 1 4M2 E (x,y) [`(h(x), y)2]\n≤ sup h∈H\n1\n2M E (x,y) [|`(h(x), y)|] = σ2 = sup\nh∈H\n1\n2M R(h) =\nr\n2M . (15)\nLast inequality is due to the fact that `(h(x), y) ≤ M . Now we upper-bound the second term of v by applying Lemma 6,\n1\n2mM E S [ sup h∈H m∑ i=1 ( `(h(xi), yi)− E (x′,y′) [`(h(x′), y′)] )]\n= 1\n2M E S [ sup h∈H {( 1 m m∑ i=1 `(h(xi), yi) ) − E (x′,y′) [`(h(x′), y′)] }] ≤ 1 M R(L).\nWe conclude the proof by upper-bounding the expectation terms in (13) and (14) using Lemma 6, and plugging the upper bound on v,\nv ≤ 2m M R(L) +mσ2 ≤ 2mR(L) M + mr 2M ."
    }, {
      "heading" : "6.2 Rademacher Complexity of Smooth Loss Class",
      "text" : "In this section we study the Rademacher complexity of the hypothesis class populated by functions of the form (2), where the parameters w and β are chosen by an algorithm with a strongly convex regularizer.\nThus our analysis will cover many practically used regularizers in the problems alike (3). For this purpose we employ the results of Kakade et al. [2008, 2012], who studied strongly convex regularizers in a more general setting. Furthermore, we will focus on the use of smooth loss functions, that is with bounded second derivative as done by Srebro et al. [2010a].\nThe proof of the main result of this section, Theorem 10, depends essentially on the following lemma, that bounds the empirical Rademacher complexity of a H-smooth loss class.\nLemma 8 Let ` : Y × Y 7→ R+ be the H-smooth loss function. Then for some function class F , let the loss class be\nL = {(x, y) 7→ `(f(x), y) : f ∈ F} .\nThen having the sample S of size m and the set\n{τi : τi ≥ `(f(xi), yi), ∀(xi, yi) ∈ S ∧ ∀f ∈ F} ,\nwe have that\nR̂S(L) ≤ E ε [ sup f∈F { 2 √ 3H m m∑ i=1 εi √ τif(xi) }] ,\nwhere εi is r.v. such that P(εi = 1) = P(εi = −1) = 12 .\nProof This proof follows a line of reasoning similar to the proof of Talagrand’s lemma for Lipschitz functions, see for instance Mohri et al. [2012, p. 79]. We will also use Lemma B.1 by Srebro et al. [2010b], stating that for any H-smooth non-negative function φ : R 7→ R+ and any x, z ∈ R,\n|φ(x)− φ(z)| ≤ √ 6H(φ(x) + φ(z))|x− z|. (16)\nFix the sample S, then, by definition,\nR̂S(L) = 1\nm E ε [ sup f∈F { m∑ i=1 εi`(f(xi), yi) }]\n= E ε1,...,εm−1 [ E εm [ sup f∈F {um−1(f) + εm`(f(xm), ym)} ]] ,\nwhere um−1(f) = ∑n i=1 εi`(f(xi), yi). By definition of supremum, for any δ > 0, there exist f1, f2 ∈ F such that\num−1(f1) + `(f1(xm), ym) ≥ (1− δ) ( sup f∈F {um−1(f) + `(f(x), y)} )\nand um−1(f2)− `(f2(xm), ym) ≥ (1− δ) ( sup f∈F {um−1(f) + `(f(x), y)} ) .\nThus for any δ > 0, by definition of Eεm ,\n(1− δ) E εm [ sup f∈F {um−1(f) + εm`(f(xm), ym)} ]\n= 1− δ\n2 ( sup f∈F {um−1(f) + `(f(xm), ym)}+ sup f∈F {um−1(f)− `(f(xm), ym)} )\n≤ 1 2\n( um−1(f1) + `(f1(xm), ym) + um−1(f2)− `(f2(xm), ym) ) ≤ 1\n2\n( um−1(f1) + um−1(f2) + sm √ 6H`(f1(xm), ym) + `(f2(xm), ym)(f1(xm)− f2(xm)) ) ≤ 1\n2\n( um−1(f1) + um−1(f2) + sm √ 12Hτm(f1(xm)− f2(xm)) ) ≤ 1\n2 sup f∈F\n( um−1(f) + sm √ 12Hτmf(xm) ) + 1\n2 sup f∈F\n( um−1(f)− sm √ 12Hτmf(xm) ) = E εm [ sup f∈F { um−1(f) + εm √ 12Hτmf(xm) }] .\nTo obtain the second inequality, we applied (16), where sm = SGN(f1(xm)−f2(xm)). Since the inequality holds for all δ > 0, we have\nE εm [ sup f∈F {um−1(f) + εm`(f(xm), ym)} ] ≤ E εm [ sup f∈F { um−1(f) + εm √ 12Hτmf(xm) }] .\nProceeding in the same way for all the other εi, with i 6= m, proves the lemma.\nTo prove Theorem 10 we will also use the following lemma in Kakade et al. [2012, Corollary 4].\nLemma 9 (Kakade et al. [2012]) If Ω is σ strongly convex w.r.t. ‖ · ‖ and Ω?(0) = 0, then, denoting the partial sum ∑ j≤i vj by v1:i, we have for any sequence v1, . . . ,vm and for any u,\nm∑ i=1 〈vi,u〉 − Ω(u) ≤ Ω?(v1:m) ≤ m∑ i=1 〈∇Ω?(v1:i−1),vi〉+ 1 2σ m∑ i=1 ‖vi‖2? .\nNow we are ready to give the proofs of the Rademacher complexity results.\nTheorem 10 Let Ω be a non-negative σ-strongly convex function w.r.t. a norm ‖ · ‖, and let 0 be its minimizer. Let risk and empirical risk be defined w.r.t. an H-smooth loss function ` : Y × Y 7→ R+. Finally, given the set of functions {fi : X 7→ Y}ni=1 with f(x) := [f1(x), . . . , fn(x)]>, a combination fβ(x) = 〈β, f(x)〉, a scalar α > 0, and any sample S drawn i.i.d. from distribution over X × Y , define classes\nW = { w : Ω(w) ≤ αR̂S(fβ) } , V = {β : Ω(β) ≤ ρ} ,\nand the loss class L = {(x, y) 7→ `(〈w,x〉+ fβ(x), y) : w ∈ W ∧ β ∈ V} .\nThen for the loss class L, setting constants supx∈X ‖x‖? ≤ B and supx∈X ‖f(x)‖? ≤ C, we have that\nR(L) ≤ 4 √ 3H(B + C) ( 1 + √ 2HB2α\nσ\n) R(fβ) √ α+ √ R(fβ)ρ√\nmσ .\nProof The core of the proof consists in an application of Lemma 8. In particular, Lemma 8 allows us to introduce additional information about the loss class by providing bounds on the loss at each example. We will bound the loss at each example using the definition of smoothness, extracting the empirical risk of hypothesis R̂S(fβ). The last step is to give an upper-bound on the empirical Rademacher complexity of a class regularized by a strongly convex function. We follow the proof of Kakade et al. [2012, Theorem 7] to accomplish this task. First define the classes\nHW := {x 7→ 〈w,x〉 : w ∈ W} , HV := {fβ : β ∈ V} ,\nand also define altered sample S′ := {√τixi}mi=1, where τi is a quantity independent fromW and V . We apply Lemma 8 with the loss class L and the property of Rademacher complexities for the sums of function classes [Bartlett and Mendelson, 2003] to show that\nR̂S(L) ≤ 2 √ 3H ( R̂S′(HW) + R̂S′(HV) ) .\nHaving this, we will follow the proof of Kakade et al. [2012, Theorem 7] to bound the empirical Rademacher complexities R̂S′(HW) and R̂S′(HV) with quantities of interest. Let t > 0 and apply Lemma 9 with u = w and vi = tεi √ τixi to get\nsup w∈W { m∑ i=1 〈w, tεi √ τixi〉 } ≤ t 2 2σ m∑ i=1 ‖εi √ τixi‖2? + sup w∈W Ω(w) + m∑ i=1 〈∇Ω?(v1:i−1), εi √ τixi〉\n≤ t 2B2\n2σ m∑ i=1 |τi|+ αR̂S(f) + m∑ i=1 〈∇Ω?(v1:i−1), εi √ τixi〉 .\nNow take expectation w.r.t. all the εi on both sides. The left hand side is mtR̂S′(HW) and the last term on the right hand side becomes zero since E[εi] = 0. Denoting r = 1m ∑m i=1 |τi| and multiplying through by\n1 mt , we get\nR̂S′(HW) ≤ B2rt\n2σ +\nα\nmt R̂S(fβ).\nProving analogously for R̂S′(HV), we get that\nR̂S(L) ≤ 2 √ 3H\n( (B2 + C2)rt\nσ + αR̂S(fβ) + ρ mt\n) .\nOptimizing over t gives us\nR̂S(L) ≤ 4 √ 3H(B + C)\n√ r(αR̂S(fβ) + ρ)\nmσ .\nNow focus on the upper bound of r. First we obtain bounds on each τi. We start with the bound on the loss function, exploiting smoothness. Let `(〈w,x〉 + fβ(x), y) = φ(〈w,x〉 + fβ(x)), where φ : R 7→ R is an H-smooth function. From the definition of smoothness [Shalev-Shwartz and Ben-David, 2014, (12.5)], we have that for all w and v\nφ(〈w,x〉+ fβ(x)) ≤ φ(〈v,x〉+ fβ(x)) + φ′(〈v,x〉+ fβ(x)) 〈w − v,x〉+ H\n2 〈w − v,x〉2 ≤ φ(〈v,x〉+ fβ(x)) + 2 √ Hφ(〈v,x〉+ fβ(x))‖w − v‖‖x‖? + H\n2 ‖w − v‖2‖x‖2?. (17)\nTo obtain the last inequality we used the generalized Cauchy-Schwarz inequality and the fact that for an H-smooth non-negative function φ, we have that |φ′(t)| ≤ √ 4Hφ(t), [Srebro et al., 2010a, Lemma 2.1].\nNow recall a property of a σ-strongly-convex function F , that holds for its minimizer v and any w [ShalevShwartz and Ben-David, 2014, Lemma 13.5],\n‖w − v‖2 ≤ 2 σ (F (w)− F (v)).\nSince inequality (17) holds for any v, set v = 0, which is also the minimizer of Ω(·), apply aforementioned property to get\nφ(〈w,x〉+ fβ(x)) ≤ φ(fβ(x)) + 2 √ 2H\nσ φ(fβ(x))Ω(w)‖x‖? +\nH σ Ω(w)‖x‖2?\n⇒ `(〈w,xi〉+ fβ(xi), yi) ≤ τi = `(fβ(xi), yi) + √ 8HB2α\nσ R̂S(fβ)`(fβ(xi), yi) +\nHB2α\nσ R̂S(fβ).\n(18)\nThe last inequality comes from the definition of the classH. Now we consider the average and, by Jensen’s inequality,\nr = 1\nm m∑ i=1 |τi| = R̂S(fβ) + 1 m m∑ i=1\n√ 8HB2α\nσ R̂S(fβ)`(fβ(xi), yi) +\nHB2α\nσ R̂S(fβ)\n≤ R̂S(fβ) + √ 8HB2α\nσ R̂S(fβ) +\nHB2α\nσ R̂S(fβ) ≤\n( 1 + √ 2HB2α\nσ\n)2 R̂S(fβ).\nThis gives us\nR̂S(L) ≤ 4 √ 3H(B + C) ( 1 + √ 2HB2α\nσ\n)√ R̂S(fβ)(αR̂S(fβ) + ρ)\nmσ\n≤ 4 √ 3H(B + C) ( 1 + √ 2HB2α\nσ\n) R̂S(fβ) √ α+ √ R̂S(fβ)ρ\n√ mσ\n.\nTaking expectation w.r.t. the sample on both sides and applying Jensen’s inequality gives the statement."
    }, {
      "heading" : "6.3 Proofs of Main Results",
      "text" : "Proof of Theorem 2. To show the statement we will apply Theorem 7. In particular, we will consider any choice of w and β within the set induced by a strongly-convex function Ω. To apply Theorem 7, we need to upper bound the Rademacher complexity of the loss class L and also the quantity r = supf∈L E(x,y)[f(x, y)].\nWe obtain the bound on Rademacher complexity by applying Theorem 10. First define the loss class L := {(x, y) 7→ `(h, y) : h ∈ H} , and hypothesis class\nH := { x 7→ 〈w,x〉+ hsrcβ (x) : Ω(w) ≤ 1\nλ R̂S(h\nsrc β ) ∧ Ω(β) ≤ ρ ∧ R̂S(hw,β) ≤ R̂S(hsrcβ )\n} .\nTo motivate the choice for the constraints observe that for\nŵ = argmin w\n{ R̂S(hw,β) + λΩ(w) } ,\nwe have Ω(ŵ) ≤ λ−1R̂S(h0,β) = λ−1R̂S(hsrcβ ), and R̂S(hŵ,β) ≤ R̂S(hsrcβ ). That said, by applying Theorem 10 with α = 1λ and fβ = h src β and assuming that λ ≤ κ, we obtain\nR(L) ≤ O ( Rsrcκ√ mλ + √ Rsrcρκ2 mλ ) .\nNext we obtain the bound on r\nr = sup h∈H E (x,y) [`(h(x), y)] = sup h∈H E S\n[ R̂S(h) ] ≤ E\nS [ sup h∈H R̂S(h) ] ≤ E S [R̂S(h src β )] = R src.\nThe last two inequalities come from Jensen’s inequality and the definition of the class H. Plugging the bounds on the Rademacher complexity and r into the statement of Theorem 7, and applying the inequality√ a+ b ≤ √ a+ b\n2 √ a\nto the √ v term, gives the statement.\nProof of Theorem 3. For any choice of β with Ω(β) ≤ ρ, denote the best in the class by\nw? = argmin w : Ω(w)≤τ R(hw,β).\nBy the definition of ŵ, we have\nR̂S(hŵ,β) + λΩ(ŵ) ≤ R̂S(hw?,β) + λΩ(w?). (19)\nNow denote\nZ = κ\n√ Rsrc\nm ( √ Rsrc + √ ρ).\nThen, by following the proof of Theorem 2 until the application of inequality √ a+ b ≤ √ a+ b\n2 √ a , ignoring constants, using the assumption (19), and assuming that λ ≤ κ ≤ 1 we have that\nR(hŵ,β) ≤ R̂S(hw?,β) + λτ + Z\nλ +\n√ Mη\nm\n√ Rsrc + Z\nλ + Mη m\n≤ R̂S(hw?,β) + λτ + Z\nλ +\n√ RsrcMη\nm + √ ZMη√ mλ + Mη m . (20)\nOptimizing the l.h.s. over λ gives\nλ? =\n√ Z\nτ +\n1\nτ\n√ ZMη\nm .\nWe plug it back into (20) to obtain that\nR(hŵ,β) ≤ R̂S(hw?,β) + √ τ √ Z + √ ZMη\nm +\n√ RsrcMη\nm + Mη m\n≤ R̂S(hw?,β) + √ Rsrc + 4 √ Rsrcρ\n4 √ m\n√ κτ +\n4 √ Rsrc + 8 √ Rsrcρ\n4 √ m1.5\n4 √ κτ2Mη +\n√ RsrcMη\nm + Mη m .\n(21)\nAll that is left is to concentrate R̂S(hw?,β) around its mean. Denoting the variance by\nV = E [ m∑ i=1 (`(hw?,β(xi), yi)−R(hw?,β))2 ] ,\nwe apply Bernstein’s inequality\nP ( m∑ i=1 (`(hw?,β(xi), yi)−R(hw?,β)) > t ) ≤ exp ( − t 2/2 V +Mt/3 ) .\nSetting\ne−η = exp ( − t 2/2\nV +Mt/3\n) ,\nwe have that with probability at least 1− e−η, ∀η ≥ 0\nR̂S(hw?,β) ≤ R(hw?,β) + √\n2η E [(`(hw?,β(xi), yi)−R(hw?,β))2] m + 2Mη 3m\n≤ R(hw?,β) + 2 √ R(hw?,β)Mη\nm +\n2Mη\n3m ≤ R(hw?,β) + 2 √ RsrcMη\nm +\n2Mη\n3m .\nThe last inequality comes from the observation that R(hw?,β) ≤ R(h0) = Rsrc. Plugging this result into (21) completes the proof."
    }, {
      "heading" : "7 Conclusions",
      "text" : "In this paper we have formally captured and theoretically analyzed a general family of transfer learning algorithms transferring from multiple supplied source hypotheses, the Hypothesis Transfer Learning through Regularized ERM. In particular, our formulation stems from the regularized Empirical Risk Minimization principle with the choice of any non-negative smooth loss function and any strongly convex regularizer. Theoretically we have analyzed the generalization ability and excess risk of this family of HTL algorithms. Our analysis showed that a good source hypothesis combination facilitates faster generalization, specifically in O(1/m) instead of the usual O(1/ √ m). Furthermore, given a perfect source hypothesis combination, our analysis is consistent with the intuition that learning is not required. As a byproduct of our investigation, we came up with new results in Rademacher complexity analysis of the smooth loss classes, that could be of independent interest.\nOur conclusions suggest the key importance of a source hypothesis selection procedure. Indeed, when the HTL algorithm is provided with enormous pool of source hypotheses, how to select relevant ones on the basis of only few labeled examples? This might sound similar to the feature selection problem under the condition that n m, however, earlier empirical studies by Tommasi et al. [2014] with hundreds of sources did not find much corroboration for this hypothesis when applying L1 regularization. Thus, it remains unclear if having few good sources from hundreds is a reasonable assumption. Another interesting practical possibility is the design of strongly convex regularizers tailored for transfer learning. We note that the effect of such on the generalization ability of HTL could be immediately observed through the strong convexity parameter appearing in our bounds."
    }, {
      "heading" : "A Additional Proofs",
      "text" : "Theorem 11 Let hŵ,β be generated by HTL through Regularized ERM, given the m-sized training set S sampled i.i.d. from the target domain, source hypotheses {hsrci }ni=1, any source weights β obeying Ω(β) ≤ ρ, and λ ∈ R+. Assume that `(hŵ,β(x), y) ≤ M for any (x, y) and any training set. Then, denoting κ = Hσ and assuming that λ ≤ 1, we have with probability at least 1− e −η, ∀η ≥ 0\nR(hŵ,β) ≤ R̂S(hŵ,β) + Õ\n((√ Rsrc\nm +\n4\n√ M2ρ\nm3σ +\n8\n√ M4ρ\nm7λ2σ3\n)(√ Mκ\nλ + √ κρ+\n√ Mη )) .\nProof To prove the statement we will use Theorem 1 of Srebro et al. [2010a]. In particular, we need to obtain bounds on the empirical risk and also to bound the worst case Rademacher complexity of the class\nH = { x 7→ 〈w,x〉+ hsrcβ (x) : Ω(w) ≤ R̂S(h src β )\nλ ∧ Ω(β) ≤ τ\n} .\nThe corresponding loss class is\nL = {(x, y) 7→ `(h(x), y) : h ∈ H ∧ R(h) ≤ Rsrc} .\nA constraint on Ω(β) in H comes from the statement of the theorem, while a constraint on Ω(ŵ) comes from an observation that for\nŵ = argmin w\n{ R̂S(hw,β) + λΩ(w) } ,\nso we have Ω(ŵ) ≤ R̂S(h0,β)λ . The same argument immediately gives us a bound on the empirical risk, that is, R̂S(hŵ,β) ≤ R̂S(h0,β) = R̂S(hsrcβ ). Taking expectation on both sides gives the constraint of L.\nBy applying Theorem 1 of Kakade et al. [2008] and subadditive property of Rademacher complexities [Bartlett and Mendelson, 2003], we have that\nR̂S(H) ≤\n√ 2R̂S(h src β )\nmλσ +\n√ 2ρ mσ ≤ √ 2M mλσ + √ 2ρ mσ . (22)\nNote that the upper bound is the bound on the worst-case Rademacher complexity since no term depends on the sample.\nAll that is left to do is to show the bound on the empirical risk in terms of Rsrc. However, we cannot use Theorem 1 of Srebro et al. [2010a] since it is not symmetric. Instead we will use a similar localized bound of Bartlett et al. [2005, Corollary 3.5]. In order to apply it, we have to obtain an upper bound on the Rademacher complexity of the loss class L that is a sub-root function [Bousquet, 2002, Definition 4.1]. By using the fact that loss function is bounded, we apply Talagrand’s lemma [Mohri et al., 2012], have R̂S(L) ≤ MR̂S(H), upper-bound with the first inequality of (22) and applying Jensen’s inequality w.r.t. E[·] have\nR(L) ≤M √ 2Rsrc\nmλσ +M\n√ 2ρ\nmσ .\nSince upper bound is a sub-root function of Rsrc, we obtain it’s fixed point r? as required by Corollary 3.5 and conclude that r? ≤ √ 2M2ρ\nmσ +\n2M2 mλσ + 2M 4\n√ 8ρ\nm3λ2σ3 .\nNow we apply Corollary 3.5 and for any K > 0 we have with probability at least 1 − e−η, ∀η ≥ 0 the following holds\nR̂S(hŵ,β) ≤ K ( Rsrc + √ M2ρ\nmσ +\nM2\nmλσ +M 4\n√ ρ\nm3λ2σ3 +\n1 + η\nm\n) .\nAll that is left to do is to apply Theorem 1 of Srebro et al. [2010a] to have\nR(hŵ,β) ≤ R̂S(hŵ,β) + Õ\n((√ Rsrc\nm +\n4\n√ M2ρ\nm3σ +\nM\nm √ λσ + 8\n√ M2ρ\nm7λ2σ3 +\n√ 1 + η\nm\n) ×\n×\n(√ Mκ\nλ + √ κρ+\n√ Mη ) + Mκ\nmλ + κρ m\n) .\nUsing the assumption on λ, we get the stated result.\nA.1 Guarantees using Localized Rademacher Complexity Bounds The following theorem is due to Bousquet [2002, Theorem 6.1]. In particular, we state the inequality appearing prior to the last in the proof, as it better serves our purpose.\nTheorem 12 (Bousquet [2002]) Let F be a class of non-negative functions such that ‖f‖∞ ≤ M almost surely. Let φm be a function defined on [0,∞) that is non-negative, non-decreasing, not identically zero, and such that φm(r)/ √ r is non-increasing. Moreover let φm be such that for all r > 0\nR̂S(F) ≤ φm(r).\nDefine r?m as the largest solution of the equation φm(r) = r.Then, for all η > 0, with probability at least 1− e−η for all f ∈ F and any {Xi}mi=1 drawn i.i.d.\nE X [f(X)] ≤ 1 m m∑ i=1 f(Xi)+45r ? m+ √ 8r?m E X [f(X)]+\n√ 4M(η + 6 log logm)EX [f(X)]\nm +\n20M(η + 6 log logm)\nm .\nThe following HTL generalization bound is shown using Theorem 12.\nTheorem 13 Let hŵ,β be generated by HTL through Regularized ERM, given the m-sized training set S sampled i.i.d. from the target domain, source hypotheses {hsrci }ni=1, any source weights β obeying Ω(β) ≤ ρ, and λ ∈ R+. Assume that ` is a L-Lipschitz loss function and `(hŵ,β(x), y) ≤ M for any (x, y) and any training set. Then we have with probability at least 1− e−η, ∀η ≥ 0\nR(hŵ,β) ≤ R̂S(hŵ,β)+Õ\n( L2 + L\nmλσ + L\n√ ρ\nmσ +\n√ Rsrc(L2 + L)\nmλσ + √ Rsrc 4\n√ L2ρ\nmσ +\n√ RsrcMη\nm + Mη m\n) .\n(23)\nProof The core of the proof is an application of Theorem 12. In particular, we have to obtain the fixed point r?m and upper bound R(h) with the risk of the source hypothesis R\nsrc. Considering the L-Lipschitz loss class of Theorem 12 to be L := {(x, y) 7→ `(h(x), y) : h ∈ H}, we have the relationship R̂S(L) ≤ LR̂S(H) via Talagrand’s lemma [Mohri et al., 2012, Lemma 4.2]. Furthermore, let the hypothesis class be\nH = { x 7→ 〈w,x〉+ hsrcβ (x) : Ω(w) ≤ 1\nλ R̂S(h\nsrc β ) ∧ Ω(β) ≤ ρ ∧ R̂S(hw,β) ≤ R̂S(hsrcβ )\n} .\nThe motivation for the choice of constraints comes from the same argument as in the proof of Theorem 2. That said, we obtain the upper bound\nR̂S(L) ≤ L √ 2Rsrc\nmλσ + L\n√ 2ρ\nmσ .\nBoth terms come by applying Theorem 7 by Kakade et al. [2012]. In the first term we set fmax = Rsrc and in the second fmax = ρ. Now define function φm(r) = L √ 2r mλσ + L √ 2ρ mσ , and observe that it verifies the\ncondition of Theorem 12. Next, to obtain the upper bound on r?m, we solve L √ 2r mλσ + 2ρ mσ ≤ r and get\nthat r?m ≤ L(L+1) mλσ + L √ 2ρ mσ . As in Theorem 2, we also get that R(h) ≤ R\nsrc. Plugging r?m and the bound on R(h) into Theorem 12, we have the statement."
    } ],
    "references" : [ {
      "title" : "Tabula rasa: Model transfer for object category detection",
      "author" : [ "Y. Aytar", "A. Zisserman" ],
      "venue" : "In Computer Vision (ICCV),",
      "citeRegEx" : "Aytar and Zisserman.,? \\Q2011\\E",
      "shortCiteRegEx" : "Aytar and Zisserman.",
      "year" : 2011
    }, {
      "title" : "Rademacher and gaussian complexities: Risk bounds and structural results",
      "author" : [ "P.L. Bartlett", "S. Mendelson" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Bartlett and Mendelson.,? \\Q2003\\E",
      "shortCiteRegEx" : "Bartlett and Mendelson.",
      "year" : 2003
    }, {
      "title" : "Local rademacher complexities",
      "author" : [ "P.L. Bartlett", "O. Bousquet", "S. Mendelson" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "Bartlett et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Bartlett et al\\.",
      "year" : 2005
    }, {
      "title" : "On the hardness of domain adaptation and the utility of unlabeled target samples",
      "author" : [ "S. Ben-David", "R. Urner" ],
      "venue" : "In Algorithmic Learning Theory,",
      "citeRegEx" : "Ben.David and Urner.,? \\Q2012\\E",
      "shortCiteRegEx" : "Ben.David and Urner.",
      "year" : 2012
    }, {
      "title" : "Domain adaptation as learning with auxiliary information",
      "author" : [ "S. Ben-David", "R. Urner" ],
      "venue" : "In New Directions in Transfer and Multi-Task - Workshop @ Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Ben.David and Urner.,? \\Q2013\\E",
      "shortCiteRegEx" : "Ben.David and Urner.",
      "year" : 2013
    }, {
      "title" : "A theory of learning from different domains",
      "author" : [ "S. Ben-David", "J. Blitzer", "K. Crammer", "A. Kulesza", "F. Pereira", "J.W. Vaughan" ],
      "venue" : "Machine learning,",
      "citeRegEx" : "Ben.David et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Ben.David et al\\.",
      "year" : 2010
    }, {
      "title" : "Classemes and other classifier-based features for efficient object categorization",
      "author" : [ "A. Bergamo", "L. Torresani" ],
      "venue" : "Pattern Analysis and Machine Intelligence, IEEE Transactions on,",
      "citeRegEx" : "Bergamo and Torresani.,? \\Q2014\\E",
      "shortCiteRegEx" : "Bergamo and Torresani.",
      "year" : 2014
    }, {
      "title" : "Combining labeled and unlabeled data with co-training",
      "author" : [ "A. Blum", "T. Mitchell" ],
      "venue" : "In Conference on Computational learning theory,",
      "citeRegEx" : "Blum and Mitchell.,? \\Q1998\\E",
      "shortCiteRegEx" : "Blum and Mitchell.",
      "year" : 1998
    }, {
      "title" : "Concentration inequalities and empirical processes theory applied to the analysis of learning algorithms",
      "author" : [ "O. Bousquet" ],
      "venue" : "PhD thesis, Ecole Polytechnique,",
      "citeRegEx" : "Bousquet.,? \\Q2002\\E",
      "shortCiteRegEx" : "Bousquet.",
      "year" : 2002
    }, {
      "title" : "Stability and Generalization",
      "author" : [ "O. Bousquet", "A. Elisseeff" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Bousquet and Elisseeff.,? \\Q2002\\E",
      "shortCiteRegEx" : "Bousquet and Elisseeff.",
      "year" : 2002
    }, {
      "title" : "Domain adaptation and sample bias correction theory and algorithm for regression",
      "author" : [ "C. Cortes", "M. Mohri" ],
      "venue" : "Theoretical Computer Science,",
      "citeRegEx" : "Cortes and Mohri.,? \\Q2014\\E",
      "shortCiteRegEx" : "Cortes and Mohri.",
      "year" : 2014
    }, {
      "title" : "Frustratingly easy domain adaptation",
      "author" : [ "H. Daumé III" ],
      "venue" : "In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "III.,? \\Q2007\\E",
      "shortCiteRegEx" : "III.",
      "year" : 2007
    }, {
      "title" : "Frustratingly easy semi-supervised domain adaptation",
      "author" : [ "H. Daumé III", "A. Kumar", "A. Saha" ],
      "venue" : "In Proceedings of the 2010 Workshop on Domain Adaptation for Natural Language Processing,",
      "citeRegEx" : "III et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "III et al\\.",
      "year" : 2010
    }, {
      "title" : "Domain adaptation from multiple sources via auxiliary classifiers",
      "author" : [ "L. Duan", "I.W. Tsang", "D. Xu", "T.-S. Chua" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Duan et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Duan et al\\.",
      "year" : 2009
    }, {
      "title" : "One-shot adaptation of supervised deep convolutional models",
      "author" : [ "J. Hoffman", "E. Tzeng", "J. Donahue", "Y. Jia", "K. Saenko", "T. Darrell" ],
      "venue" : "CoRR, abs/1312.6204,",
      "citeRegEx" : "Hoffman et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Hoffman et al\\.",
      "year" : 2013
    }, {
      "title" : "Inequalities on the lambert w function and hyperpower function",
      "author" : [ "A. Hoorfar", "M. Hassani" ],
      "venue" : "J. Inequal. Pure and Appl. Math,",
      "citeRegEx" : "Hoorfar and Hassani.,? \\Q2008\\E",
      "shortCiteRegEx" : "Hoorfar and Hassani.",
      "year" : 2008
    }, {
      "title" : "On the complexity of linear prediction: Risk bounds, margin bounds, and regularization",
      "author" : [ "S.M. Kakade", "K. Sridharan", "A. Tewari" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Kakade et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Kakade et al\\.",
      "year" : 2008
    }, {
      "title" : "Regularization techniques for learning with matrices",
      "author" : [ "S.M. Kakade", "S. Shalev-Shwartz", "A. Tewari" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Kakade et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Kakade et al\\.",
      "year" : 2012
    }, {
      "title" : "Personalized handwriting recognition via biased regularization",
      "author" : [ "W. Kienzle", "K. Chellapilla" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Kienzle and Chellapilla.,? \\Q2006\\E",
      "shortCiteRegEx" : "Kienzle and Chellapilla.",
      "year" : 2006
    }, {
      "title" : "Correction to ”Stability and Hypothesis Transfer Learning",
      "author" : [ "I. Kuzborskij", "F. Orabona" ],
      "venue" : null,
      "citeRegEx" : "Kuzborskij and Orabona.,? \\Q2013\\E",
      "shortCiteRegEx" : "Kuzborskij and Orabona.",
      "year" : 2013
    }, {
      "title" : "Stability and Hypothesis Transfer Learning",
      "author" : [ "I. Kuzborskij", "F. Orabona" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Kuzborskij and Orabona.,? \\Q2013\\E",
      "shortCiteRegEx" : "Kuzborskij and Orabona.",
      "year" : 2013
    }, {
      "title" : "From N to N+1: multiclass transfer incremental learning",
      "author" : [ "I. Kuzborskij", "F. Orabona", "B. Caputo" ],
      "venue" : "In Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Kuzborskij et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Kuzborskij et al\\.",
      "year" : 2013
    }, {
      "title" : "Object bank: A high-level image representation for scene classification & semantic feature sparsification",
      "author" : [ "L.-J. Li", "H. Su", "E.P. Xing", "L. Fei-Fei" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Li et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2010
    }, {
      "title" : "A bayesian divergence prior for classiffier adaptation",
      "author" : [ "X. Li", "J. Bilmes" ],
      "venue" : "In International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "Li and Bilmes.,? \\Q2007\\E",
      "shortCiteRegEx" : "Li and Bilmes.",
      "year" : 2007
    }, {
      "title" : "Multiclass transfer learning from unconstrained priors",
      "author" : [ "J. Luo", "T. Tommasi", "B. Caputo" ],
      "venue" : "In International Conference on Computer Vision,",
      "citeRegEx" : "Luo et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Luo et al\\.",
      "year" : 2011
    }, {
      "title" : "Domain adaptation with multiple sources",
      "author" : [ "Y. Mansour", "M. Mohri", "A. Rostamizadeh" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Mansour et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Mansour et al\\.",
      "year" : 2008
    }, {
      "title" : "Domain adaptation: Learning bounds and algorithms",
      "author" : [ "Y. Mansour", "M. Mohri", "A. Rostamizadeh" ],
      "venue" : "In The Conference on Learning Theory,",
      "citeRegEx" : "Mansour et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Mansour et al\\.",
      "year" : 2009
    }, {
      "title" : "Foundations of machine learning",
      "author" : [ "M. Mohri", "A. Rostamizadeh", "A. Talwalkar" ],
      "venue" : null,
      "citeRegEx" : "Mohri et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Mohri et al\\.",
      "year" : 2012
    }, {
      "title" : "Learning and transferring mid-level image representations using convolutional neural networks",
      "author" : [ "M. Oquab", "L. Bottou", "I. Laptev", "J. Sivic" ],
      "venue" : "In Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Oquab et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Oquab et al\\.",
      "year" : 2014
    }, {
      "title" : "Model Adaptation with Least-Squares SVM for Adaptive Hand Prosthetics",
      "author" : [ "F. Orabona", "C. Castellini", "B. Caputo", "A.E. Fiorilla", "G. Sandini" ],
      "venue" : "In Robotics and Automation, IEEE International Conference on,",
      "citeRegEx" : "Orabona et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Orabona et al\\.",
      "year" : 2009
    }, {
      "title" : "A survey on transfer learning",
      "author" : [ "S.J. Pan", "Q. Yang" ],
      "venue" : "IEEE Trans. Knowl. Data Eng.,",
      "citeRegEx" : "Pan and Yang.,? \\Q2010\\E",
      "shortCiteRegEx" : "Pan and Yang.",
      "year" : 2010
    }, {
      "title" : "A generalized representer theorem",
      "author" : [ "B. Schölkopf", "R. Herbrich", "A.J. Smola" ],
      "venue" : "In Conference on Computational learning theory,",
      "citeRegEx" : "Schölkopf et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Schölkopf et al\\.",
      "year" : 2001
    }, {
      "title" : "Understanding Machine Learning: From Theory to Algorithms",
      "author" : [ "S. Shalev-Shwartz", "S. Ben-David" ],
      "venue" : null,
      "citeRegEx" : "Shalev.Shwartz and Ben.David.,? \\Q2014\\E",
      "shortCiteRegEx" : "Shalev.Shwartz and Ben.David.",
      "year" : 2014
    }, {
      "title" : "Smoothness, low noise and fast rates",
      "author" : [ "N. Srebro", "K. Sridharan", "A. Tewari" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "Srebro et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Srebro et al\\.",
      "year" : 2010
    }, {
      "title" : "Optimistic rates for learning with a smooth loss",
      "author" : [ "N. Srebro", "K. Sridharan", "A. Tewari" ],
      "venue" : "eprint arXiv:1009.3896,",
      "citeRegEx" : "Srebro et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Srebro et al\\.",
      "year" : 2010
    }, {
      "title" : "Transfer leraning for reinforcement learning domains: A survey",
      "author" : [ "M.E. Taylor", "P. Stone" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Taylor and Stone.,? \\Q2009\\E",
      "shortCiteRegEx" : "Taylor and Stone.",
      "year" : 2009
    }, {
      "title" : "Safety in numbers: Learning categories from few examples with multi model knowledge transfer",
      "author" : [ "T. Tommasi", "F. Orabona", "B. Caputo" ],
      "venue" : "In The Twenty-Third IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Tommasi et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Tommasi et al\\.",
      "year" : 2010
    }, {
      "title" : "Improving control of dexterous hand prostheses using adaptive learning",
      "author" : [ "T. Tommasi", "F. Orabona", "C. Castellini", "B. Caputo" ],
      "venue" : "Robotics, IEEE Transactions on,",
      "citeRegEx" : "Tommasi et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Tommasi et al\\.",
      "year" : 2013
    }, {
      "title" : "Learning categories from few examples with multi model knowledge transfer",
      "author" : [ "T. Tommasi", "F. Orabona", "B. Caputo" ],
      "venue" : "IEEE Trans. Pattern Anal. Mach. Intell.,",
      "citeRegEx" : "Tommasi et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Tommasi et al\\.",
      "year" : 2014
    }, {
      "title" : "A new learning paradigm: Learning using privileged information",
      "author" : [ "V. Vapnik", "A. Vashist" ],
      "venue" : "Neural Networks,",
      "citeRegEx" : "Vapnik and Vashist.,? \\Q2009\\E",
      "shortCiteRegEx" : "Vapnik and Vashist.",
      "year" : 2009
    }, {
      "title" : "Model selection for regularized least-squares algorithm in learning theory",
      "author" : [ "E. De Vito", "A. Caponnetto", "L. Rosasco" ],
      "venue" : "Foundations of Computational Mathematics,",
      "citeRegEx" : "Vito et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Vito et al\\.",
      "year" : 2005
    }, {
      "title" : "Cross-Domain Video Concept Detection Using Adaptive SVMs",
      "author" : [ "J. Yang", "R. Yan", "A.G. Hauptmann" ],
      "venue" : "In Proceedings of the 15th international conference on Multimedia,",
      "citeRegEx" : "Yang et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2007
    }, {
      "title" : "In order to apply it, we have to obtain an upper bound on the Rademacher complexity of the loss class L that is a sub-root function [Bousquet",
      "author" : [ "Bartlett" ],
      "venue" : "Definition",
      "citeRegEx" : "Bartlett,? \\Q2002\\E",
      "shortCiteRegEx" : "Bartlett",
      "year" : 2002
    }, {
      "title" : "In the first term we set fmax = Rsrc and in the second fmax = ρ. Now define function φm(r) = L",
      "author" : [ "Kakade" ],
      "venue" : null,
      "citeRegEx" : "Kakade,? \\Q2012\\E",
      "shortCiteRegEx" : "Kakade",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : "Among others, there is the use of structural information, such as taxonomy, or different views on the same data [Blum and Mitchell, 1998], or even a sort of privileged information [Vapnik and Vashist, 2009].",
      "startOffset" : 112,
      "endOffset" : 137
    }, {
      "referenceID" : 39,
      "context" : "Among others, there is the use of structural information, such as taxonomy, or different views on the same data [Blum and Mitchell, 1998], or even a sort of privileged information [Vapnik and Vashist, 2009].",
      "startOffset" : 180,
      "endOffset" : 206
    }, {
      "referenceID" : 5,
      "context" : "For example, in domain adaptation [Ben-David et al., 2010], one employs large unlabeled samples to estimate the relatedness of source and target domains to perform adaptation.",
      "startOffset" : 34,
      "endOffset" : 58
    }, {
      "referenceID" : 18,
      "context" : "A first formalization and theoretical treatment of transfer learning under the use of source hypothesis was given by Kuzborskij and Orabona [2013b] in the Hypothesis Transfer Learning (HTL) framework, albeit empirically it has already been extensively exploited in the past [Yang et al.",
      "startOffset" : 117,
      "endOffset" : 148
    }, {
      "referenceID" : 5,
      "context" : "For example, in domain adaptation [Ben-David et al., 2010], one employs large unlabeled samples to estimate the relatedness of source and target domains to perform adaptation. Even if unlabeled data are abundant, the estimation of adaptation parameters can be computationally prohibitive. A hypothetical example is a large number of domains involved, or, for instance, when one acquires new domains incrementally. There, keeping unlabeled data from all the domains and re-estimating the parameters is a necessity. In HTL we naturally alleviate these practical limitations through indirect access to the source domain by means of a source hypothesis. As was mentioned above, the first theoretical analysis of HTL was carried out by Kuzborskij and Orabona [2013b], where we analyzed HTL problem cast as a regularized least-squares with a single fixed, unweighted, source hypothesis.",
      "startOffset" : 35,
      "endOffset" : 762
    }, {
      "referenceID" : 5,
      "context" : "For example, in domain adaptation [Ben-David et al., 2010], one employs large unlabeled samples to estimate the relatedness of source and target domains to perform adaptation. Even if unlabeled data are abundant, the estimation of adaptation parameters can be computationally prohibitive. A hypothetical example is a large number of domains involved, or, for instance, when one acquires new domains incrementally. There, keeping unlabeled data from all the domains and re-estimating the parameters is a necessity. In HTL we naturally alleviate these practical limitations through indirect access to the source domain by means of a source hypothesis. As was mentioned above, the first theoretical analysis of HTL was carried out by Kuzborskij and Orabona [2013b], where we analyzed HTL problem cast as a regularized least-squares with a single fixed, unweighted, source hypothesis. There, we came up with polynomial generalization bound that depends on the performance of that fixed source hypothesis on the target task. In this work we considerably extend and generalize the theory of HTL. Our contributions. We formulate a general Hypothesis Transfer Learning problem through regularized Empirical Risk Minimization (ERM) with respect to any non-negative smooth loss function and any strongly convex regularizer. For its solution we prove high-probability generalization bounds that exhibit fast rate, i.e. O(1/m), of convergence whenever any weighted combination of multiple source hypotheses performs well on the target task. In other words, we show theoretically that HTL facilitates a faster generalization. In addition, we show that, if the combination is perfect, the error on the training set becomes the error we observe on the whole target distribution. Furthermore, we analyze an excess risk of our formulation, and conclude that a good source hypothesis also speeds up the convergence to the performance of the bestin-the-class. As a byproduct of our study, we prove an upper bound on the Rademacher complexity of a smooth loss class that provides extra information compared to that of Lipschitz loss classes. This is an alternative to the analysis of Srebro et al. [2010a] that holds under much weaker assumptions, and it might be of independent interest.",
      "startOffset" : 35,
      "endOffset" : 2186
    }, {
      "referenceID" : 17,
      "context" : "The framework of Hypothesis Transfer Learning (HTL) that we address in this paper was first formally introduced and studied theoretically by Kuzborskij and Orabona [2013b]. It was shown that the generalization ability of the regularized least-squares HTL algorithm improves if supplied source hypothesis performs well on the target task.",
      "startOffset" : 141,
      "endOffset" : 172
    }, {
      "referenceID" : 3,
      "context" : "Later, Ben-David and Urner [2013] showed a similar bound, but with different source performance criterion.",
      "startOffset" : 7,
      "endOffset" : 34
    }, {
      "referenceID" : 31,
      "context" : ", 2014], employs the principle of biased regularization [Schölkopf et al., 2001].",
      "startOffset" : 56,
      "endOffset" : 80
    }, {
      "referenceID" : 13,
      "context" : "Li and Bilmes [2007] have analyzed a Bayesian approach to HTL.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 13,
      "context" : "Li and Bilmes [2007] have analyzed a Bayesian approach to HTL. Employing a PAC-Bayes analysis they showed that given a prior on the hypothesis class, the generalization ability of logistic regression improves if the prior is informative on the target task. Mansour et al. [2008] analyzed a setting of multiple source hypotheses combination.",
      "startOffset" : 0,
      "endOffset" : 279
    }, {
      "referenceID" : 1,
      "context" : "We will quantify the complexity of a hypothesis class by the means of Rademacher complexity [Bartlett and Mendelson, 2003].",
      "startOffset" : 92,
      "endOffset" : 122
    }, {
      "referenceID" : 31,
      "context" : "As an example of such, consider a common transfer learning problem: the least squares with biased regularization [Schölkopf et al., 2001].",
      "startOffset" : 113,
      "endOffset" : 137
    }, {
      "referenceID" : 2,
      "context" : "This bound is similar in spirit to the results of localized complexities, as in works of Bartlett et al. [2005], Srebro et al.",
      "startOffset" : 89,
      "endOffset" : 112
    }, {
      "referenceID" : 2,
      "context" : "This bound is similar in spirit to the results of localized complexities, as in works of Bartlett et al. [2005], Srebro et al. [2010a], however we focus on the HTL scenario rather than a generic learning setting.",
      "startOffset" : 89,
      "endOffset" : 135
    }, {
      "referenceID" : 8,
      "context" : ", 2005, Bousquet and Elisseeff, 2002], that is a special case of the smooth loss function that the HTL through Regularized ERM employs. On the other hand, Srebro et al. [2010a] showed a better worst-case rate O(1/ √ mλ).",
      "startOffset" : 8,
      "endOffset" : 177
    }, {
      "referenceID" : 2,
      "context" : "In particular, the localized Rademacher complexity bounds of Bartlett et al. [2005] and Bousquet [2002] can be used to obtain results similar to the second inequality of Theorem 2.",
      "startOffset" : 61,
      "endOffset" : 84
    }, {
      "referenceID" : 2,
      "context" : "In particular, the localized Rademacher complexity bounds of Bartlett et al. [2005] and Bousquet [2002] can be used to obtain results similar to the second inequality of Theorem 2.",
      "startOffset" : 61,
      "endOffset" : 104
    }, {
      "referenceID" : 2,
      "context" : "In particular, the localized Rademacher complexity bounds of Bartlett et al. [2005] and Bousquet [2002] can be used to obtain results similar to the second inequality of Theorem 2. Indeed, Theorem 7 shows a bound which is very similar to the localized ones, albeit with two differences. The r.h.s. of the first inequality in Theorem 7 vanishes when the loss class has zero variance. Though intuitively trivial, this allows to prove a considerable result in the theory of transfer learning as it quantifies the intuition that no learning is necessary if the source has perfect performance on the target task. Second, by applying the standard localized Rademacher complexity bounds of Bousquet [2002], and assuming the use of the Lipschitz loss function, we do not achieve a fast rate of convergence, as can be seen from Theorem 13, shown in the Appendix.",
      "startOffset" : 61,
      "endOffset" : 699
    }, {
      "referenceID" : 2,
      "context" : "In particular, the localized Rademacher complexity bounds of Bartlett et al. [2005] and Bousquet [2002] can be used to obtain results similar to the second inequality of Theorem 2. Indeed, Theorem 7 shows a bound which is very similar to the localized ones, albeit with two differences. The r.h.s. of the first inequality in Theorem 7 vanishes when the loss class has zero variance. Though intuitively trivial, this allows to prove a considerable result in the theory of transfer learning as it quantifies the intuition that no learning is necessary if the source has perfect performance on the target task. Second, by applying the standard localized Rademacher complexity bounds of Bousquet [2002], and assuming the use of the Lipschitz loss function, we do not achieve a fast rate of convergence, as can be seen from Theorem 13, shown in the Appendix. We suspect that assuming the smoothness of the loss function is crucial to prove fast rates in our formulation. Fast rates for ERM with the smooth loss have been thoroughly analyzed by Srebro et al. [2010a]. Yet, the analysis of our HTL algorithm within their framework would yield a bound that is inferior to ours in two respects.",
      "startOffset" : 61,
      "endOffset" : 1061
    }, {
      "referenceID" : 2,
      "context" : "In particular, the localized Rademacher complexity bounds of Bartlett et al. [2005] and Bousquet [2002] can be used to obtain results similar to the second inequality of Theorem 2. Indeed, Theorem 7 shows a bound which is very similar to the localized ones, albeit with two differences. The r.h.s. of the first inequality in Theorem 7 vanishes when the loss class has zero variance. Though intuitively trivial, this allows to prove a considerable result in the theory of transfer learning as it quantifies the intuition that no learning is necessary if the source has perfect performance on the target task. Second, by applying the standard localized Rademacher complexity bounds of Bousquet [2002], and assuming the use of the Lipschitz loss function, we do not achieve a fast rate of convergence, as can be seen from Theorem 13, shown in the Appendix. We suspect that assuming the smoothness of the loss function is crucial to prove fast rates in our formulation. Fast rates for ERM with the smooth loss have been thoroughly analyzed by Srebro et al. [2010a]. Yet, the analysis of our HTL algorithm within their framework would yield a bound that is inferior to ours in two respects. The first concerns the scenario when the combined source hypothesis is perfect, that is Rsrc = 0. The generalization bound of Srebro et al. [2010a] does not offer a way to show that the empirical risk converges to the risk with probability one – instead one can only get a fast rate of convergence.",
      "startOffset" : 61,
      "endOffset" : 1334
    }, {
      "referenceID" : 5,
      "context" : "Perhaps the most well known are the dH∆H-divergence [Ben-David et al., 2010] and its more general counterpart, the Discrepancy Distance [Mansour et al.",
      "startOffset" : 52,
      "endOffset" : 76
    }, {
      "referenceID" : 26,
      "context" : ", 2010] and its more general counterpart, the Discrepancy Distance [Mansour et al., 2009].",
      "startOffset" : 67,
      "endOffset" : 89
    }, {
      "referenceID" : 3,
      "context" : ", 2009, Ben-David and Urner, 2012, Mansour et al., 2008, Cortes and Mohri, 2014] have proposed a number of such domain relatedness criteria. Perhaps the most well known are the dH∆H-divergence [Ben-David et al., 2010] and its more general counterpart, the Discrepancy Distance [Mansour et al., 2009]. Typically, this divergence is explicitated in the generalization bound along with other terms controlling the generalization on the target domain. Let RDtrg(h) and RDsrc(h) denote the risks of the hypothesis h, measured w.r.t. the target and source distributions. Then a well-known result of Ben-David et al. [2010] suggests that for all h ∈ H RDtrg(h) ≤ RDsrc(h) + dH∆H(D,D) + εH, (6) where εH = minh∈H {RDtrg(h) +RDsrc(h)}.",
      "startOffset" : 8,
      "endOffset" : 617
    }, {
      "referenceID" : 9,
      "context" : "The following result, obtained through an algorithmic stability argument [Bousquet and Elisseeff, 2002], holds with probability at least 1− δ",
      "startOffset" : 73,
      "endOffset" : 103
    }, {
      "referenceID" : 21,
      "context" : "Mansour et al. [2008] have addressed the problem of multiple source hypotheses combination, however, in a different HTL setting.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 33,
      "context" : "Similarly as in previous works [Srebro et al., 2010a], we exploit additional information about the behavior of the hypothesis coming from the gradient of the loss function. This allows us to prove a bound on the empirical Rademacher complexity of a hypothesis class, Lemma 8, that depends on the point-wise bounds on the loss function. This contrasts with Srebro et al. [2010a], who consider smooth losses as well, but use a much more restrictive notion of Rademacher complexity.",
      "startOffset" : 32,
      "endOffset" : 378
    }, {
      "referenceID" : 8,
      "context" : "1 Fast Rate Generalization Bound The proof of fast-rate and vanishing-confidence-term bounds, Theorem 7, stems from the functional generalization of Bennett’s inequality which is due to Bousquet [2002, Theorem 2.11] and that we report here for completeness. Theorem 4 (Bousquet [2002]) Let X1, X2, .",
      "startOffset" : 186,
      "endOffset" : 285
    }, {
      "referenceID" : 15,
      "context" : "3 in Hoorfar and Hassani [2008], that says that",
      "startOffset" : 5,
      "endOffset" : 32
    }, {
      "referenceID" : 1,
      "context" : "13)], [Bartlett and Mendelson, 2003], that is used to relate the expected uniform deviation of empirical risk over the hypothesis class to the Rademacher complexity of that class.",
      "startOffset" : 6,
      "endOffset" : 36
    }, {
      "referenceID" : 16,
      "context" : "For this purpose we employ the results of Kakade et al. [2008, 2012], who studied strongly convex regularizers in a more general setting. Furthermore, we will focus on the use of smooth loss functions, that is with bounded second derivative as done by Srebro et al. [2010a]. The proof of the main result of this section, Theorem 10, depends essentially on the following lemma, that bounds the empirical Rademacher complexity of a H-smooth loss class.",
      "startOffset" : 42,
      "endOffset" : 274
    }, {
      "referenceID" : 27,
      "context" : "Proof This proof follows a line of reasoning similar to the proof of Talagrand’s lemma for Lipschitz functions, see for instance Mohri et al. [2012, p. 79]. We will also use Lemma B.1 by Srebro et al. [2010b], stating that for any H-smooth non-negative function φ : R 7→ R+ and any x, z ∈ R, |φ(x)− φ(z)| ≤ √ 6H(φ(x) + φ(z))|x− z|.",
      "startOffset" : 129,
      "endOffset" : 209
    }, {
      "referenceID" : 16,
      "context" : "To prove Theorem 10 we will also use the following lemma in Kakade et al. [2012, Corollary 4]. Lemma 9 (Kakade et al. [2012]) If Ω is σ strongly convex w.",
      "startOffset" : 60,
      "endOffset" : 125
    }, {
      "referenceID" : 1,
      "context" : "We apply Lemma 8 with the loss class L and the property of Rademacher complexities for the sums of function classes [Bartlett and Mendelson, 2003] to show that",
      "startOffset" : 116,
      "endOffset" : 146
    }, {
      "referenceID" : 36,
      "context" : "Indeed, when the HTL algorithm is provided with enormous pool of source hypotheses, how to select relevant ones on the basis of only few labeled examples? This might sound similar to the feature selection problem under the condition that n m, however, earlier empirical studies by Tommasi et al. [2014] with hundreds of sources did not find much corroboration for this hypothesis when applying L1 regularization.",
      "startOffset" : 281,
      "endOffset" : 303
    } ],
    "year" : 2017,
    "abstractText" : "In this work we consider the learning setting where in addition to the training set, the learner receives a collection of auxiliary hypotheses originating from other tasks. This paradigm, known as Hypothesis Transfer Learning (HTL), has been successfully exploited in empirical works, but only recently has received a theoretical attention. Here, we try to understand when HTL facilitates accelerated generalization – the goal of the transfer learning paradigm. Thus, we study a broad class of algorithms, a Hypothesis Transfer Learning through Regularized ERM, that can be instantiated with any non-negative smooth loss function and any strongly convex regularizer. We establish generalization and excess risk bounds, showing that if the algorithm is fed with a good source hypotheses combination, generalization happens at the fast rate O(1/m) instead of usual O(1/ √ m). We also observe that if the combination is perfect, our theory formally backs up the intuition that learning is not necessary. On the other hand, if the source hypotheses combination is a misfit for the target task, we recover the usual learning rate. As a byproduct of our study, we also prove a new bound on the Rademacher complexity of the smooth loss class under weaker assumptions compared to previous works.",
    "creator" : "LaTeX with hyperref package"
  }
}