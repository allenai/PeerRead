{
  "name" : "1612.00092.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Computer Assisted Composition with Recurrent Neural Networks",
    "authors" : [ "Christian Walder", "Dongwoo Kim" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Computer Assisted Composition with Recurrent Neural Networks Christian Walder1 and Dongwoo Kim2 1)Data61 at CSIRO, Australia. 2)Australian National University.\nSequence modeling with neural networks has lead to powerful models of symbolic music data. We address the problem of exploiting these models to reach creative musical goals. To this end we generalise previous work, which sampled Markovian sequence models under the constraint that the sequence belong to the language of a given finite state machine. We consider more expressive non-Markov models, thereby requiring approximate sampling which we provide in the form of an efficient sequential Monte Carlo method. In addition we provide and compare with a beam search strategy for conditional probability maximisation.\nOur algorithms are capable of convincingly re-harmonising famous musical works. To demonstrate this we provide visualisations, quantitative experiments, a human listening test and illustrative audio examples. We find both the sampling and optimisation procedures to be effective, yet complementary in character. For the case of highly permissive constraint sets, we find that sampling is to be preferred due to the overly regular nature of the optimisation based results.\nI. INTRODUCTION\nAlgorithmic music composition has intrigued a wide range of thinkers, from times as distant as the earliest days of modern computing, and beyond. As early as 1843, Ada Lovelace famously speculated that a computer “might compose elaborate and scientific pieces of music of any degree of complexity or extent”.15 Still earlier, around 1757, there famously existed a number of musical games which used dice along with precise instructions, to generate musical compositions.18\nMuch work has focused on machine learning methods, which combine musical examples with generic inductive principles, to generate new examples. For a survey of more recent advancements we recommend e.g. Fernandez and Vico 11 , Nierhaus 18 . By restricting to a highly structured and regular musical form, and providing a large number of training examples, we may obtain convincing musical results even with rather music-agnostic models.23 Alternatively, with smaller datasets, more structured models are required, such as the hidden Markov model investigated by Allan and Williams 1 . Both of these methods lead to relatively convincing musical results. However, the plausibility of the results comes at the cost of variety. It is unreasonable to expect such approaches to give rise to interesting new musical forms.\nUnmanned machine learning algorithms capable of divining interesting new musical forms may arise, but this is far from a reality at the time of writing. In any case, hybrid man-machine systems are a promising avenue for exploration. The high level goal is a system for the partial specification of music, which relinquishes precise control for other gains.1\nIn this work, we investigate the combination of machine learning music models with human input. Our approach"
    }, {
      "heading" : "1 I will argue that today’s composers are more frequently gardeners than architects and, further, that the composer as architect",
      "text" : "metaphor was a transitory historical blip — Eno 10 .\nis most closely related to the work of19, who impose the constraint that the resulting sequence be a member of a language defined by a finite state machine. This is a rather general constraint which provides a rich language for human creativity via the partial specifications of music. In that work, the close relationship between Markov models and finite state machines was used to derive a simple but exact belief propagation algorithm. Here, we relax the constraint that our underlying sequence model be Markovian, and instead make use of approximate sampling techniques, namely that of sequential Monte Carlo8,12 (SMC). In addition to sampling, we compare and contrast with a maximum conditional probability approach based on a beam search. (given the finite state machine constraint).\nBy allowing models which are not low-order Markovian, we obtain algorithms which are applicable to a broad range of more expressive probabilistic models of music. Concretely, here we adopt the Long Short-Term Memory (LSTM) based neural network sequence model13. By exploiting advances in massively parallel computing architectures, this class of models has proven remarkably effective in a range of domains, most notably natural language modeling (see e.g. Chelba et al. 5). Given the similarities between music and natural language21, it is unsurprising that the LSTM is a natural model for music data. This was noted by Hochreiter and Schmidhuber 13 in the original LSTM paper, and subsequently investigated by Eck and Schmidhuber 9 . This line of work has been taken up by a number of researchers, with recent examples including Boulanger-Lewandowski et al. 2 , Walder 25 , the latter approach being the one we adopt here.\nIn section II we set the problem up and motivate our solution. Section III provides results including a visual illustration, quantitative investigation, human listening test, and audio examples. We conclude in section IV, and provide technical details on SMC in Appendix A.\nar X\niv :1\n61 2.\n00 09\n2v 1\n[ cs\n.A I]\n1 D\nec 2\n01 6\n2"
    }, {
      "heading" : "II. SET-UP AND MOTIVATION",
      "text" : ""
    }, {
      "heading" : "A. Required Background",
      "text" : "The algorithms we present are applicable to any model explicitly factorised as (1) below. We employ the model of Walder 25 , however, which is a necessary accompaniment to the present work for those wishing to replicate our results. Furthermore, the finite state machine constraint formalism we adopt is closely related to Papadopoulos et al. 19 , which we recommend as background material."
    }, {
      "heading" : "B. Assumed Music Model",
      "text" : "We represent a musical composition by a set of n triples, {(xi, ti, di)}ni=1 ⊂ X×T ×T . Here, X represents the set of possible pitches (from the 12-tet western musical system, including both the note name and octave). The set T represents time, with ti and di denoting the start time and duration of the i-th note event, respectively.\nFollowing Walder 25 , we assume throughout that the timing information is given, so that we have a model p({xi}ni=1 |n, {(ti, di)} n i=1). This model p is based on a reduction to a non-Markov sequence model. This means that (for some particular ordering of the indices, see Walder 25) neglecting the conditioning on {(ti, di)}mi=1, we have\np(x1:n) = n∏ i=1 fi(xi|x1:i−1), (1)\nwhere the conditionals fi are represented explicitly."
    }, {
      "heading" : "C. Assumption of Fixed Rhythmic Information",
      "text" : "Ideally we would model the rhythmic structure, but this turns out to be rather challenging. Indeed, modeling pitches given the rhythmic structure is already rather non-trivial, so it is reasonable to subdivide the problem. Note that some authors have modeled timing information, by assuming e.g.\n1. A simple note on/off representation on a uniform temporal grid as in e.g. Boulanger-Lewandowski et al. 2 , which assumed a temporal resolution of an eighth note. This has the drawback of not distinguishing, say, two eighth notes from a single quarter note (which has length two eighth notes).\n2. A simplified set of possible durations, as in Colombo et al. 6 , Mozer 16 . This approach is sound, but requires more sophisticated machinery to model a realistic range of note durations and start times. Furthermore, sampling conditionally given constraints as is our present focus, becomes more technical. Indeed, point process machinery7 and novel sampling\ntechniques are required, due to the variable dimension of the parameter space (that is, the number of notes). This is the subject of ongoing work, and beyond the present scope.\nShort of developing the second approach, assuming fixed timing has the advantage of allowing arbitrarily complex rhythmic structures. In this way, rather than fully modeling an overly simplistic music representation, we partially model a more realistic music representation. Once we have satisfactorily modeled timing, our methods and findings may be generalised to the full joint distribution, as we can always employ the chain rule of probability to decompose\np (n, {xi, tt, di}ni=1) = p({xi}ni=1 |n, {(ti, di)} n i=1)× p(n, {(ti, di)} n i=1),\nwhere the first factor on the r.h.s. is our present concern."
    }, {
      "heading" : "D. Finite State Machine Constraint Formulation",
      "text" : "Following Papadopoulos et al. 19 we formulate human input as the constraint that the sequence x1:n belong to the language of an arbitrary finite state machine, A. This has the advantage of being extremely general, yet amenable to belief propagation (for Markov p as in Papadopoulos et al. 19), and particle filtering/beam search (as in the present work).\nThe experiments in section III utilise a fraction of the generality of this approach. We envisage a rich set of constraints which users may experiment with to discover interesting new music, and our algorithms facilitate this. For example, one might enforce the repetition of patterns, transform relationships between sub-sequences (such as inversion / retrograde), and so on, with arbitrarily complex implications such as odd poly-metrical patterns, etc. Concrete examples may be found in Roy and Pachet 22 , which imposes metrical structure, and Papadopoulos et al. 20 , which avoids the exact repetition of training sub-sequences of a certain length."
    }, {
      "heading" : "E. Harmonisation by Sampling / Optimisation",
      "text" : "We consider two alternative means of incorporating the constraint of the previous subsection. The first is conditional sampling, from p(x1:n|x1:n ∈ L(A)), where L(A) is the set of sequences A can generate. The second is maximisation of the same conditional probability with respect to x1:n.\nThe nature of (1) is such that given a partial sequence x1:i−1 we may keep track of a latent variable (the LSTM state vector) in order to compute fi(xi|x1:i−1) in time independent of i. The state vector may also be updated in time independent of i, when a new element xi is appended. This structure makes SMC the natural choice for sampling,\n3 Particle Filter\n40 50 60 70 80 m id\ni\npart 1\n40 50 60 70 80 part 2 40 50 60 70 80 part 3\n0 10 20 30 40 50 time (quarter notes)\n40 50 60 70 80 part 4 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 fin al pa th d ist rib ut io n\nBeam Search\n50 60 70 80 m id\ni\npart 1\n50 60 70 80 part 2 50 60 70 80 part 3\n0 10 20 30 40 50 time (quarter notes)\n50 60 70 80 part 4 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 fin al pa th d ist rib ut io n\nFIG. 1. Visualisation of the final state of the particle filter (upper) and beam search (lower), with S = 2048 paths. In both cases the fourth part (lowest sub figure) is fixed to the original from Corelli’s Trio Op 1 No 1 (which actually contains four midi tracks) provided by2. The pitches (vertical axis) of Parts 1–3 were unconstrained, to be determined by our algorithms. For the particle filter, we plot the filtering distribution through a sequential Monte Carlo. For the beam search, we plot the distribution of a set of final candidate paths.\nsince it only ever requires extending a given sequence by a single element. Appendix A provides the details.\nSimilarly, beam search methods are natural for maximising the conditional probability. In this case we maintain S candidate paths, and for each time step we extend each path by one new element, for all feasible continuations, retaining the S most probable resulting sequences."
    }, {
      "heading" : "III. RESULTS",
      "text" : "Throughout the experiments, we employ finite state machines which merely fix certain notes xi to given values, and prevent unison intervals from occurring within a single part (which would be impossible to play on the piano, for example).\nA. Visualisation\nA well known difficulty in particle filtering is the collapsing of the particles to a small number of unique paths. We observe little evidence of this in our setting however, which features a reasonable degree of uncertainty in the final particle distribution (Figure 1, upper plot). As ex-\npected, the beam search (Figure 1, lower plot) behaves very differently, with the majority of the solutions being identical other than at a few isolated temporal intervals."
    }, {
      "heading" : "B. Quantitative Investigation",
      "text" : "Both the beam search and particle filter algorithms have a single key free parameter S, the number of paths to store, which trades accuracy for computation time. In this subsection, we investigate this trade-off. For computational reasons, we restricted our analysis to the sixteen shortest pieces (by number of notes) from those pieces in the MuseData test set from Boulanger-Lewandowski et al. 2 which consist of exactly four voices (all of which turned out to be Bach chorales). For each piece, and for m ∈ {1, 2, 3, 4}, we fixed m of the voices to that of the original piece, and applied our algorithms to choose the remaining (4−m) parts. We did this in all C4m possible ways, plotting in Figure 2 the mean and standard error over all 16×C4m possibilities of two different quantities, as a function of the number of paths S. The two quantities depicted are:\n1. Figure 2 l.h.s.: the filtering probability of the fixed parts — in the notation of section A 2 the mean of the log of\nS∑ s=1 w(x (s) <i )∑S s′=1 w(x (s′) <i ) fi(xi|x(s)<i ),\nfor all i corresponding to the fixed voices. For the particle filter, this is an SMC approximation of the probability of the ground truth for the fixed voices conditional on previous observed values for the fixed voices. For the beam search we let w(x (s) <i ) = 1, and the quantity heuristically measures how well the set of candidate paths agree with the fixed voices on average.\n2. Figure 2 r.h.s.: the probability of the best complete harmonisation found by the algorithm. For the beam search, this is the log probability of the final path obtained by the search algorithm. For the particle filter, we take the most likely solution from the final set of particles.\nQualitatively, the results are as expected, with each algorithm dominating in the setting it is intended for: the particle filter is superior in terms of the filtering probability, while the beam search produces more probable solutions.\nInterestingly, given sufficiently large S, the best log probability exceeds the ground truth (4/4 fixed parts, red line). This indicates that either we are doing better than the original composer, or the probability under p does not fully reflect what we perceive as good music. The listening test in section III C indicates that the latter is of course\n4 most likely, although this does not necessarily imply that p is inadequate — see section III C 2 and footnote 3.\nQuantitatively, we see that around S = 500 paths are necessary in order to obtain good solutions (that the particle filter continues to improve for greater S in terms of the Figure 2 right hand side plot is to be expected since the algorithm does not explicitly search for the most likely sequence). The required number of paths is significantly more than reported by Sutskever et al. 24 , who observed marginal gains for S > 2. Broadly speaking, this difference is to be expected: locally, the best musical harmonisation may be highly ambiguous, requiring many paths to be maintained in order for the future fixed (or observed) notes to have a reasonable chance of resolving well in the musical sense. The machine translation problem considered by Sutskever et al. 24 is less ambiguous, and furthermore future symbols (words) are not conditioned on as in our setting."
    }, {
      "heading" : "C. Human Listening Test",
      "text" : "We also investigated the effectiveness of our algorithms as judged by human evaluators, via an on-line survey using the Amazon Mechanical Turk2."
    }, {
      "heading" : "1. Methodology",
      "text" : "As the basis for the survey, we generated harmonisations as in the previous section III B, with the number of candidate paths S set to 4096. We excluded two of the sixteen pieces previously considered, due to the extended use of pure pedal point (repeating of a single note) in three out of four voices of the original compositions, leading to uninteresting constraints in the majority of cases. For each of the remaining fourteen pieces and for m ∈ {0, 1, 2, 3}, we harmonised the pieces with m out of four voices fixed. As before, we did this all C4m possible ways, for both the particle filter and beam search methods. Each such output was compared with the original piece from which it was derived. This comparison was performed by ten unique human subjects per piece (not the same ten for each piece; there were 67 unique respondents). This gave\na grand total of 16× ∑3\nn=0 C 4 n × 2× 10 = 4200 compar-\nisons, or — figuring conservatively at one minute per test — approximately 72 hours of listening. Each experiment presented the original and derived piece in random order, and asked the participant to decide which sounds better overall?\nTo improve data quality we included two participant filters. First, we presented pieces with random notes drawn from uniform distribution over a four octave range, and excluded subjects who failed to do better than random\n2 http://www.mturk.com\nat selecting the original piece over the random piece. Second, we tried to filter insincere subjects who responded in a very short time."
    }, {
      "heading" : "2. Findings",
      "text" : "A summary of the results is presented in Figure 3, which presents frequentist confidence intervals on the unknown binomial probability3 of choosing our pieces as preferable to the original. Note that a probability (vertical axis) of greater than one half would imply that our results are on average preferable to the original piece.\nThe most clear result is that with no fixed parts, the beam search algorithm tends to produce inferior results. This is in line with the experience of our informal experimentation with the beam search approach which, in the absence of fixed notes to condition on, tended to produce overly regular patterns featuring long sequences of repeated notes, etc. This is not an erroneous result, but rather a natural consequence of maximising the sequence probability, rather than sampling from the distribution.3 It is worth noting that this overly regular behaviour of the beam search may be observed not only in that case of unconditional sampling, but also in pieces where the parts being conditioned on feature sufficiently protracted periods of inactivity, during which the beam search is relatively unconstrained (see the illustrative examples in section III D).\nThe particle filter does only slightly worse than parity with no fixed parts, but this effect does not appear to be significant based on these tests. Our own informal experimentation indicated that the fully unconstrained (no fixed parts) particle filter tends to occasionally finish pieces with poorly resolved harmonic movements — an observation partially corroborated by Figure 3. Interestingly, both algorithms perform well given even a single fixed part to condition on (with the particle filter performing slightly but consistently better overall). This is a broadly satisfactory result, which indicates the possibility of constructing convincing musical results by specifying a single voice (out of four), along with the rhythmic structure of the piece."
    }, {
      "heading" : "3. Discussion",
      "text" : "One goal of our work is to create tools which facilitate the advancement of the musical art form. We believe that this may be be possible, through creative manipulation\n3 Informal thought experiment: consider the stochastic relationship yi = xi + N (0, 1). We imagine the analogy that the xi represent the timing and the yi the pitch of a set of notes making up a piece. The most likely yi given an xi is exactly yi = xi, and yet for a set {xi}ni=1 such a perfect straight line is in some sense highly atypical of a sample from the conditional {yi}ni=1|{xi}ni=1.\nof the constraint set by skilled humans. Ideally, we would leverage the super human ability of the computer to sift through large numbers of possible solutions. In this way, constraint sets which are too complex and tightly coupled for human investigation may yield new musical forms. Evaluating such a scheme is beyond the present scope, likely requiring an entirely different approach than the simple listening test undertaken here14. These experiments are intended merely to verify that our system can produce feasible solutions given a simplistic constraint\nset.\nD. Illustrative Example\nFinally, in Table I we provide sample audio output from both the particle filter and beam search algorithms. To demonstrate the nature of the algorithms, we took a string quartet by Mozart, fixed the melody line, and reharmonised the three remaining parts given their original rhythmic structure (note onset and offset times). The results are typical of the behaviour of the algorithms. In particular, the beam search produces more repetitive results than the particle filter.\n6 function ConstrainedSMC(S, {(fi, hi)}ni=1): for s← 1, 2, . . . , S do\nx(s) ← () initialise with empty sequence end for i← 1, 2, . . . , n do\nfor s← 1, 2, . . . , S do sample x\n(s) i ∼ q (A)(· |x(s)1:i−1) proposal (A4) x(s) ← (x(s), x(s)i ) append ws ← ∑ x̃i∈X fi(x̃i|x (s) 1:i−1)hi(x̃ (s) 1:i ) (A5)\nend k1:S = SystematicResample(w1:S)\n(x(s))Ss=1 ← (x(ks))Ss=1 duplicate/delete particles end return x(1:S) approximate samples x(1:S) from p(A) of (A3)\nfunction SystematicResample(w1:S): ω1:S ← w1:S/ ∑S\ns=1 ws sample u ∼ Uniform([0, 1]) ū← u/S; j ← 1; Sω ← ω1 for l← 1, 2, . . . , S do\nwhile Sω < ū do j ← j + 1 Sω ← Sω + ωj end kl ← j ū← ū + 1/S\nend return k1:S\nAlgorithm 1: Sequential Monte Carlo for constrained non-Markov sequences, with systematic re-sampling4."
    }, {
      "heading" : "IV. CONCLUSIONS",
      "text" : "We presented algorithms for combining sophisticated probabilistic models of polyphonic music with human input. We represent human input as a finite state machine which accepts allowed compositions. Such a constraint is rather general, and yet amenable both to sequential Monte Carlo (for sampling from implied conditional distribution) and beam search optimisation (for maximising the same conditional distribution). We demonstrated the efficacy of the methods both quantitatively and through a listening experiment with human subjects. When the constraints are highly permissive, it seems that conditional sampling should be preferred to probability maximisation, as the latter tends to produce overly regular results in this case. Furthermore, the two approaches lead to a different style of musical result, as exemplified by the included audio example."
    }, {
      "heading" : "Appendix A: Sequential Monte Carlo Details",
      "text" : "We describe detailed sequential sampling methods. In section A 1 we consider partially observed sequences, and then in section A 2 we extend this to more general constraints represented by an arbitrary finite state machine."
    }, {
      "heading" : "1. Partially Observed Sequences",
      "text" : "Let x1:n be a random sequence drawn from the nonMarkov process with discrete state-space X . Then:\np(x1:n) ∝ γn(x1:n) = n∏\ni=1\nfi(xi|x1:i−1),\nwhere γn is an un-normalised probability distribution which can be factorised into the conditionals fi, each of which depends on all of the previous states x1:i−1.\nOur goal is to compute the posterior of x1:n with partial observations x̃. Let zi be 1 if xi is observed and 0 otherwise. The posterior distribution of the unobserved part is:\np({xi}i:zi=0|{x̃j}j:zj=1, z1:n) = p(x1:n)\np({xj}j:zj=1) ∝ ∏n\ni=1 fi(xi|x1:i−1)∫ ·· · ∫ ∏n i=1 fi(xi|x1:i−1) ∏ i:zi=0 dxi ,\nwhere we let x1:n be the full sequence with the partial observations. The computational cost increases exponentially with respect to the number of unobserved notes.\nOne can approximate the posterior of x1:n by importance sampling with fully factorised proposal distribution q(x1:n) = ∏n i=1 qi(xi|x<i), so that\nγn(x1:n) = wn(x1:n)q(x1:n)\np(x1:n) = wn(x1:n)q(x1:n)∫\nwn(x1:n)q(x1:n)dx1:n .\nWith S samples x (s) 1:n from q(x1:n), the posterior can be approximated by an empirical measure of the samples:\np(x1:n|z1:n) = S∑\ns=1\nwn(x (s) 1:n)∑S\ns′=1 wn(x (s′) 1:n )\nδ x (s) 1:n (x1:n).\nThe weight of each sample x (s) 1:n ∼ q(x1:n) is:\nwn(x (s) 1:n) =\nγn(x (s) 1:n)\nq(x (s) 1:n)\n=\n∏n i=1 fi(x (s) i |x (s) 1:i−1)\nq(x (s) 1:n)\n= γ̃n−1(x\n(s) 1:n−1)\nq(x (s) 1:n−1)︸ ︷︷ ︸ ,wn−1(x (s) 1:n−1)\n× fn(x\n(s) n |x(s)1:n−1) qn(x (s) n |x(s)<n)︸ ︷︷ ︸\n,wn(x (s) n )\n. (A1)\nThrough the above recursion we can apply the sequential importance sampling scheme8.\nTo incorporate the partial observations x̃ into the sequential importance sampling scheme, we define the proposal\nqi(xi|x<i, z1:i) = [ δx̃i(xi) ]zi[ fi(xi|x(s)1:i−1) ]1−zi ,\n7 so that our proposal always proposes x̃j whenever the sampler encounters the partial observation. Then from (A1) we have that the weight of sample x (s) i ∼ qi(xi) is\nwi(x (s) i ) ∝\n{ 1 zi = 0\nfi(x (s) i |x (s) 1:i−1) zi = 1\n(A2)\nThis expression is intuitive — we re-weight our particles only when encountering an observed symbol, giving more weight to those particles whose history better agrees with the observed symbol, as measured by the conditional fi."
    }, {
      "heading" : "2. General Constraints",
      "text" : "Here we extend the set-up of the previous section to the more general case of sampling with the hard constraint that the sampled sequence lie within a given regular language. In particular, we wish to sample from\np(A)(x1:n) , p(x1:n|x1:n ∈ L(A))\n∝ { p(x1:n) x1:n ∈ Ln(A), 0 otherwise.\n(A3)\nHere, Ln(A) is the set of sequences of length n generated by finite state machine A = 〈Q,Σ, δ, q0, F 〉, where Q is a set of states, Σ an alphabet of actions, δ the transition function linking a state q ∈ Q and a label a ∈ Σ to the successor state q′ = δ(q, a), q0 ∈ Q the initial state, and F ⊆ Q the set of accepting terminate states. We have X = Σ in our case. Similarly to Papadopoulos et al. 19 we rewrite p(A) as\np(A)(x1:n) ∝ n∏\ni=1\nfi(xi|x1:i−1)hi(x1:i),\nwhere\nhi(x1:i) = { 1 x1:i ∈ Li(A), 0 otherwise,\nPapadopoulos et al. 19 transformed their similar problem into one involving the Cartesian product of the domain of the xi and the state space of A to obtain a tree structured factor graph for which belief propagation is efficient. We are interested in non-Markov p and hence have nothing to gain from such a transformation. Instead, we simply encode the language membership constraint via the general functions hi and proceed with SMC as usual. 4\nNow, we generalise section A 1 by choosing\nq(A)(xi|x1:i−1) = 1\nZq(x1:i−1) fi(xi|x1:i−1)hi(x1:i),(A4)\n4 However, since we only ever append an xi to a given particle, we may leverage the finite state machine formulation of A in order to evaluate the hi, typically in O(1).\nwhere Zq(x1:i−1) = ∑\nxi∈X fi(xi|x1:i−1)hi(x1:i). The re-sampling weights may be derived similarly to (A1), yielding (up to an irrelevant constant factor to which SystematicResample of algorithm 1 is invariant):\nw (A) i (x (s) i ) ∝ Zq(x1:i−1) (A5)\nHence (A5) reduces to (A2) for unary constraints. The above expression is intuitive — if all the hi(x̃ (s) 1:i ) are one, the weights are one so we sample from p without modification. In other cases, we give greater weight to those particles for which the total probability mass of admissible continuations is greater.\nExample 1 (Sequence with unary constraints). We can impose unary constraint at each time i of length-n sequence. Take A = 〈Q,Σ, δ, q0, F 〉, where each time stamp i has one-to-one corresponding state qi. Let Xi be a set of acceptable notes at time i. With transition function δ(qi, xi+1) = qi+1 if xi+1 ∈ Xi+1, we only allow notes in Xn+1 and rejects notes not in Xi+1. Furthermore choose the terminal states F to be {qn}. For example, in the partially observed sequence case, Xi = {x̃i} if zi = 1 and Xi = X if zi = 0. Note that this partially observed constraint is a special case of general unary constraints where Xi consists of single observation x̃i.\nExample 2 (Prohibit repeated notes). One can disallow consecutive positions from having the same note as follows. Let the number of states be equal to the number of possible notes |X |, and each state qx be indexed by x. Choose the transition function δ(qxi , xj) = qxj if xi 6= xj, A.\n8 1Moray Allan and Christopher K. I. Williams. Harmonising chorales by probabilistic inference. Advances in Neural Information Processing Systems 17, 2005. 2Nicolas Boulanger-Lewandowski, Yoshua Bengio, and Pascal Vincent. Modeling temporal dependencies in high-dimensional sequences: Application to polyphonic music generation and transcription. In International Conference on Machine Learning, 2012. 3Lawrence D. Brown, T. Tony Cai, and Anirban DasGupta. Confidence intervals for a binomial proportion and asymptotic expansions. Annals of Statistics, 30(1):160–201, 2 2002. doi: 10.1214/aos/1015362189. 4J. Carpenter, P. Clifford, and P. Fearnhead. Improved particle filter for nonlinear problems. IEE Proc., Radar Sonar Navig., 146(1):2, 1999. 5Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, and Phillipp Koehn. One billion word benchmark for measuring progress in statistical language modeling. CoRR, abs/1312.3005, 2013. 6Florian Colombo, Samuel P. Muscinelli, Alex Seeholzer, Johanni Brea, and Wulfram Gerstner. Algorithmic composition of melodies with deep recurrent neural networks. In Computer Simulation of Musical Creativity, 2016. 7D. J. Daley and D. Vere-Jones. An Introduction to the Theory of Point Processes Volume I: Elementary Theory and Methods. Springer, second edition, 2003. ISBN 0-387-95541-0. 8Arnaud Doucet and Adam M Johansen. A tutorial on particle filtering and smoothing: Fifteen years later. Handbook of nonlinear filtering, 12(656-704):3, 2009. 9Douglas Eck and Juergen Schmidhuber. A first look at music composition using LSTM recurrent neural networks. Istituto Dalle Molle Di Studi Sull Intelligenza Artificiale, 2002.\n10Brian Eno. Composers as gardeners. https://vimeo.com/ 55969912, 2011. Accessed: July 2015 (transcripts can be found online). 11Jose D. Fernandez and Francisco J. Vico. Ai methods in algorithmic composition: A comprehensive survey. J. Artif. Intell. Res. (JAIR), 48:513–582, 2013. 12N.J. Gordon, D.J. Salmond, and A.F.M. Smith. Novel approach to nonlinear/non-Gaussian Bayesian state estimation. IEEE Proceedings F, Radar and Signal Processing, 140(2):107–113, 1993. 13Sepp Hochreiter and Jürgen Schmidhuber. Long short-term mem-\nory. Neural Computation, 9(8):1735–1780, November 1997. ISSN 0899-7667. doi:10.1162/neco.1997.9.8.1735. 14Roisin Loughran and Michael O’Neill. Generative music evaluation: Why do we limit to “human”? In Computer Simulation of Musical Creativity, 2016. 15Luigi Federico Menabrea and Ada Lovelace. Sketch of the analytical engine invented by charles babbage. Scientific Memoirs, pages 666–731, 1843. 16Michael C. Mozer. Neural network music composition by prediction: exploring the benefits of psychoacoustic constraints and multi-scale processing. In Connection Science, pages 247–280, 1994. 17Lawrence M. Murray, Anthony Lee, and Pierre E. Jacob. Parallel resampling in the particle filter. Journal of Computational and Graphical Statistics, 2015. 18Gerhard Nierhaus. Algorithmic Composition: Paradigms of Automated Music Generation. Springer Publishing Company, Incorporated, 1st edition, 2008. ISBN 321175539X, 9783211755396. 19Alexandre Papadopoulos, Francois Pachet, Pierre Roy, and Jason Sakellariou. Exact sampling for regular and markov constraints with belief propagation. In Principles and Practice of Constraint Programming, pages 341–350, 2015. 20Alexandre Papadopoulos, Francois Pachet, and Pierre Roy. Generating non-plagiaristic markov sequences with max order sampling. In Creativity and Universality in Language, 2016. 21A.D. Patel. Music, Language, and the Brain. Oxford University Press, 2010. ISBN 9780198028772. 22Pierre Roy and Francois Pachet. Enforcing meter in finite-length markov sequences. In AAAI Conference on Artificial Intelligence, 2013. 23Bob Sturm, João Felipe Santos, and Iryna Korshunova. Folk music style modelling by recurrent neural networks with long short term memory units. In 16th International Society for Music Information Retrieval Conference, late-breaking demo session, page 2, 2015. 24Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems 27, pages 3104–3112. 2014. 25Christian Walder. Modelling symbolic music: Beyond the piano roll. In The 8th Asian Conference on Machine Learning (ACML), pages 174–189, 2016."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2016,
    "abstractText" : "Sequence modeling with neural networks has lead to powerful models of symbolic music data. We address the problem of exploiting these models to reach creative musical goals. To this end we generalise previous work, which sampled Markovian sequence models under the constraint that the sequence belong to the language of a given finite state machine. We consider more expressive non-Markov models, thereby requiring approximate sampling which we provide in the form of an efficient sequential Monte Carlo method. In addition we provide and compare with a beam search strategy for conditional probability maximisation. Our algorithms are capable of convincingly re-harmonising famous musical works. To demonstrate this we provide visualisations, quantitative experiments, a human listening test and illustrative audio examples. We find both the sampling and optimisation procedures to be effective, yet complementary in character. For the case of highly permissive constraint sets, we find that sampling is to be preferred due to the overly regular nature of the optimisation based results.",
    "creator" : "LaTeX with hyperref package"
  }
}