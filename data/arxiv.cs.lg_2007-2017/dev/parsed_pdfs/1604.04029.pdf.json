{
  "name" : "1604.04029.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Multi-Source Multi-View Clustering via Discrepancy Penalty",
    "authors" : [ "Weixiang Shao", "Jiawei Zhang", "Lifang He", "Philip S. Yu" ],
    "emails" : [ "wshao4@uic.edu,", "jzhan9@uic.edu,", "psyu@uic.edu", "lifanghescut@gmail.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "I. INTRODUCTION\nWith the advance of technology, most of entities can be observed in multiple views. Multiple views containing different types of features can be used for clustering. Multiple view clustering [1]–[3] aims to enhance clustering performance by integrating different views. Moreover, as the information explodes, we can get information from multiple sources. Each source can contain multiple views that are fully aligned and available for clustering. Combining data from multiple sources, multiple views may help us get better clustering performance.\nHowever, several difficulties prevent us from combining different sources and views. First, the views may be very heterogeneous. Different views may have different feature spaces and distributions. Second, the instances mapping between different sources may be incomplete and partially observed. Different sources may have different instance sets, which means the instance mapping between different sources is not fully mapped. Also in real-world problems, the instance mapping is often partially observed. We may only get part of the instance mapping between different sources. Third, the views in one source are generally more cohesive than the\nviews across different sources. This is very different from the traditional multi-view clustering problem.\nA good example is the social networks shown in Fig. 1. People usually use several social network services simultaneously, e.g., those provided by Twitter and Foursquare. Each social network is an independent source containing several views that describe different aspects of the social network. We can use the profile information of users for both Twitter and Foursquare (view 3 in Fig. 1), the social connections of users (view 2 in Fig. 1), location check-in history, etc. Views in a single source describe the characters of the same set of users and focus on different aspects. The views in Twitter focus on the social activity aspects, while the views in Foursquare focus more on the location based aspects. Since not all people use both Twitter and Foursquare, the user mapping between Twitter and Foursquare is incomplete and not one-to-one. Furthermore, not all the shared users link their Twitter accounts with their Foursquare accounts. We can only observe part of the mapping between Twitter and Foursquare.\nMulti-view clustering [1], [4], [5] aims to utilize the multiple representations of instances in different features spaces to get better clustering performance. However, most of the previous methods are based on the assumption that all the views are fully mapped/aligned. They cannot deal with the multi-source multi-view scenario with incomplete/partial mapping across sources. Although there are some previous studies on dealing with multiple incomplete view clustering [6]–[9], none of them are suitable for multi-source multi-view scenario. They either cannot extend to more than two views or they do not treat the views in one source as a cohesive set. All of the previous methods only use the known mapping information. Furthermore, none of them try to extend the known mapping information to help improve clustering. These challenges and emerging applications call for novel clustering methods which can deal with multiple sources multiple views situations. In this paper, we propose MMC (Multi-source Multi-view Clustering) to integrate multiple sources for better clustering results. The main contributions of this paper are summarized as follows:\n1) This paper is the first one to investigate the multi-source multi-view clustering problem, where multiple sources containing multiple views are available for clustering and the instance mappings between sources are incom-\nar X\niv :1\n60 4.\n04 02\n9v 2\n[ cs\n.L G\n] 1\n9 A\npr 2\n01 6\nplete and partially known. 2) We develop a method MMC, based on collective spectral\nclustering with discrepancy penalty within and between sources. MMC appreciates the cohesiveness of all the views in each source by pushing the latent feature matrice of the views in one source to a consensus for each source. MMC also considers the cross-source discrepancy by minimize the difference among the consensus latent feature matrices of different sources. 3) The proposed MMC not only generates clusters from multiple sources but also tries to infer the unknown instance similarity mapping between sources to improve the clustering performance. By using the learned consensus latent feature matrices of different sources and the incomplete/partial mappings across sources, MMC generates the instance similarities across different sources, which will in turn help enhance the clustering performance. 4) The proposed MMC is not limited to the multiple sources multiple views problem. In real-world applications, we may have multiple partial aligned views, i.e., views with different numbers of instances. We can group the views into groups where each group has the same set of instances. Thus each group can be viewed as a source and our method can be applied.\nThe experiment results on three groups of real data show the effectiveness of the proposed method by comparing it with other state-of-art methods."
    }, {
      "heading" : "II. PROBLEM FORMULATION",
      "text" : "In this section, we will first define the problem of multisource multi-view clustering. Then we will start from the single source problem to develop the objective function for multi-source multi-view clustering problem."
    }, {
      "heading" : "A. Problem Definition",
      "text" : "Before we define the problem of multi-source multi-view clustering, we summarize the notations in Table I. Let S = {Sk}Kk=1 denote the set of the K available sources. For each Sk, we have Sk = {Xki } vk i=1, where X k i denotes the i-th view in source k and vk denotes the number of views in source k. We assume that source k (1 ≤ k ≤ K) contains nk instances. Let M = {M (i,j)}1≤i,j≤K be the known instance mappings between sources, where M (i,j) ∈ Rni×nj+ denotes the instance mapping between sources i and j, and its element is defined by\nM (i,j) a,b =  1 instance a in source i is mapped to\ninstance b in source j. 0 otherwise.\n(1)\nOur goal is to cluster the instances into ck clusters for each source k, while considering the other sources by using the cross-source mapping in M."
    }, {
      "heading" : "B. Single Source Multiple Views Clustering",
      "text" : "Clustering with multiple views within a single source can be seen as traditional multi-view clustering problem. However, in order to incorporate the cross-source disagreement, we would like to get a consensus clustering solution for each source. Let Kki be the positive semi-definite similarity matrix or kernel matrix for view i in source k. The corresponding normalized graph Laplacian will be Lki = D k i −1/2 Kki D k i −1/2, where Dki is a diagonal matrix with the diagonal elements be the row sums of Kki . To perform spectral clustering for a single view i in source k, as shown in [3], [10], [11], we only need to solve the following optimization problem for the normalized graph Laplacian Lki :\nmax Uki ∈Rnk×ck\ntr((Uki ) TLki U k i ), s.t.(U k i ) TUki = I , (2)\nwhere tr denotes the matrix trace, and Uki can be seen as a latent feature matrix of view i in source k, which can be given to the k-means algorithm to obtain cluster memberships. For clustering multiple views in the same source, we have to consider the disagreement between different views. We enforce the learned latent feature matrix for each view to look similar by regularizing them towards a common consensus.\nSimilar to the regularization in [3], we define the discrepancy/dissimilarity between two latent feature matrices as:\nD(Uki , U k j ) = ‖KUki −KUkj ‖ 2 F , (3)\nwhere KUki and KUkj are the similarity/kernel matrices for U k i and Ukj , and ‖.‖F denotes the Frobenius norm of the matrix. We use linear kernel as the similarity measure in Eq. (3). Thus, we get KUki = U k i (U k i ) T and KUkj = U k j (U k j ) T . Using the properties of trace and the fact that (Uki ) TUki = I , (Ukj ) TUkj = I , Eq. (3) can be rewritten as follows:\nD(Uki , U k j ) = ‖Uki (Uki )T − Ukj (Ukj )T ‖2F = tr ( Uki (U k i ) T + Ukj (U k j ) T − 2Uki (Uki )TUkj (Ukj )T ) = 2ck − 2tr(Uki (Uki )TUkj (Ukj )T ). (4)\nIgnoring the constant terms, we can get the discrepancy between two different latent feature matrices:\nD(Uki , U k j ) = −tr(Uki (Uki )TUkj (Ukj )T ). (5)\nThen D(Uki , U k∗) = −tr(Uki (Uki )TUk∗(Uk∗)T ) is the discrepancy between the i-th view and the consensus. Considering the discrepancy/dissimilarity between each view and the consensus in the same source, the objective function for clustering all the views within the k-th source will be as follows:\nmax {Uki },U k∗ Jk = vk∑ i=1 ( tr(Uki T Lki U k i ) + α k i tr(U k i U k i T Uk∗Uk∗ T ) ) ,\ns.t. Uki T Uki = I , ∀1 ≤ i ≤ vk, Uk∗ T Uk∗ = I\n(6)\nwhere Jk is the objective function for source k, Uki is the latent feature matrix for the i-th view in source k, Uk∗ is the consensus latent feature matrix for source k, and αki is the relative importance of view i in source k."
    }, {
      "heading" : "C. Multiple Sources Multiple Views Clustering",
      "text" : "In this section, we will derive the objective function for Multi-source Multi-view Clustering problem. We model it as a joint matrix optimization problem.\nIn order to incorporate multiple sources, we need to add penalty between sources to the single source multiple views clustering objective function. To appreciate the cohesiveness of the views within one source, we learn the consensus latent feature matrix for each source and only penalize the discrepancy between those consensus latent feature matrices.\nSimilar to the discrepancy function across views within one source, the penalty function across sources should consider the discrepancy between the consensus clustering results for the sources. Since the mappings between sources are incomplete and partially known, we cannot directly apply the same penalty function as in the single source multiple views clustering objective function. However, by using the mapping matrices, we can project the learned latent feature matrix from one source to other sources, M (i,j) T U i∗ can be seen as projection of the instance in source i to the instances in source j. The\npenalty function for discrepancy between source i and source j is as follow:\nD̃(U i∗, U j∗) = ‖M (i,j) T U i∗(M (i,j) T U i∗)T − U j∗U j∗T ‖2F . (7)\nObserving that the known mapping between two sources is one-to-one and it is reasonable to assume that the unknown part is one-to-at-most-one, which means one instance in one source can be mapped to at most one instance in the other source. So we can approximately assume that M (i,j)M (i,j)T = I to help simplify the penalty function. The Eq. 7 can be expressed as:\ntr (( M (i,j) T U i∗U i∗TM (i,j) − U j∗U j∗T )( M (i,j) T U i∗U i∗TM (i,j) − U j∗U j∗T )) (8)\nUsing the fact that U i∗TU i∗ = I , tr(U i∗TU i∗) = ci, and M (i,j)M (i,j)T = I , and ignoring the constant terms, the penalty function for discrepancy is\nD̃(U i∗, U j∗) = −tr ( U j∗U j∗TM (i,j) T U i∗U i∗TM (i,j) ) (9)\nAdding the penalty function between sources to the single source multiple views clustering objective function for all the sources, we get\nmax Uk i ,Uk∗(1≤k≤K,1≤i≤vk)\nO = K∑\nk=1\nJk − ∑\ni6=j,1≤i,j≤K\nβ (i,j) D̃(U i∗ , U j∗ ),\nwhere β(i,j) ≥ 0 is a parameter controlling the balance between the objective function for individual source and the inconsistency across sources."
    }, {
      "heading" : "III. OPTIMIZATION AND MMC FRAMEWORK",
      "text" : "The proposed MMC framework simultaneously optimizes the latent featue matrices in multiple sources and infers the cross-source instance similarity mappings to help enhance the clustering performance. To optimize the objective function in Eq. (10), we employ an alternating scheme, that is, we optimize the objective function with respect to one variable while fixing others. Basically, we optimize the objective function using two stages. First, maximizing O over Uki s with fixed Uk∗s. Second, maximizing O over Uk∗s with fixed Uki s. We repeat these two steps, until it converges.\nSolving the optimization, we can iteratively learn the latent feature matrices for each source. However, when only a small portion of instances mapping between sources are observed, the clustering performance is affected by the incompleteness of the instance mapping across sources. Inferring the exact instance mapping is really challenging and usually additional information is required to infer such anchor link [12], [13]. However, instead of inferring the exact instance mapping, we can try to infer the similarity mapping. This idea is based on the Principle of Transitivity on Similarity:\nTheorem III.1. Instances similar to the same instance in different sources should be similar.\nFig. 2 illustrates the principle. In Fig. 2, B and C are both similar to A in different sources. Given the known mapping\nbetween A in the two sources, we can infer that B and C are similar. MMC tries to use the similarity transitivity principle to help infer the cross-source instance similarity and to help improve the clustering performance. Next, we will talk about each step in the MMC framework in detail.\nA. Initialization\nSince the efficiency of the iterative optimization is affected by the initialization step, in this paper, we learn the initial value of Uki and U\nk∗ rather than random initialization. For each Uki , we solve Eq. (2) to get an initial value. As we described in the previous section, Eq. (2) is just the objective function for single view clustering, without considering the relation among views and sources. The solution Uki is given by the top-ck eigenvectors of the Laplacian Lki .\nFor each Uk∗, we just solve Eq. (6) to get the initialization value with the initial Uki . The objective function can be written as:\nmax Uk∗ tr\n( Uk∗ T ( vk∑ i=1 αki U k i U k i T ) Uk∗ ) ,\ns.t. Uk∗ T Uk∗ = I\n(10)\nThe solution is given by the top-ck eigenvectors of the modified Laplacian ∑vk i=1 α k i U k i U k i T .\nIn the previous section, we assume that the mapping matrix between two sources is semi-orthogonal, i.e., M (i,j)M (i,j)T = I . However, we can only get part of the mapping information due to the incompleteness and partial known property of realworld problem. The mapping matrix between two sources can be expressed in two parts M (i,j)0 and M i,j 1 :\nM (i,j) =\n[ M\n(i,j) 0 0 0 M (i,j) 1\n] , (11)\nwhere M (i,j)0 represents the known mapping between source i and j and M (i,j)1 represents the unknown part.\nIt is easy to find that M (i,j)0 M (i,j)T 0 = I . We only need to initialize the unknown part to make it semi-orthogonal. One natural way to estimate the unknown mapping is to use the instance similarity among the instances between two sources. Using the similarity transitivity principle, the instance similarity between two sources i and j can be estimated by:\nKUi∗\n[ M\n(i,j) 0 0 0 0\n] KUj∗ , where KUi∗ and KUj∗ are the two\nkernel matrices for latent feature U i∗ and U j∗. It is worth to note that the kernel matrices KUi∗ and KUj∗ can be seen as the similarity matrices of the latent features for sources i and j. The similarity transitivity principle allows us to use the known instance mapping as a bridge to connects unmapped instances in two sources. Using Fig. 2 as an example, (KUi∗)b,a provides the similarity between instances B and A in source i, while (KUj∗)a,c provides the similarity between instances A and C in source j. If both instances A in both sources i and\nj are mapped through M (i,j), ( KUi∗ [ M\n(i,j) 0 0 0 0\n] KUj∗ ) b,c ,\ndenoted by (M̃ (i,j))b,c, will provide the estimated similarity between instance B in source i and instance C in source j. So for the unmapped instances, we can have an estimated similarity mapping M̃ (i,j)1 . Then we can orthogonalize it using SVD or other orthogonalization methods.\nB. Maximizing O over Uki s with fixed Uk∗s With fixed Uk∗s, for each Uki we only need to maximize part of Jk.\nmax Uki\nL = tr(Uki T Lki U k i ) + α k i tr(U k i U k i T Uk∗Uk∗ T )\n= tr(Uki T (Lki + α k i U k∗Uk∗ T )Uki )\ns.t. Uki T Uki = I\n(12)\nThis is a standard spectral clustering objective on source k view i with modified graph Laplacian Lki + α k i U k∗Uk∗ T . According to [11], the solution Uki is given by the top-ck eigenvectors of this modified Laplacian. With fixed Uk∗s, we can calculate each Uki to maximize the objective function.\nC. Maximizing O over Uk∗s with fixed Uki s With fixed Uki s, for each U k∗, we only need to maximize:\nmax Uk∗ Q = vk∑ i=1 αki tr(U k i U k i T Uk∗Uk∗ T )− ∑ 1≤j≤K,j 6=k β(k,j)D̃(Uk∗, U j∗)\n= vk∑ i=1 αki tr(U k i U k i T Uk∗Uk∗ T )\n+ ∑ j 6=k β(k,j)tr ( U j∗U j∗TM (k,j) T Uk∗Uk∗TM (k,j) ) = tr ( Uk∗TLk∗Uk∗\n) s.t. Uk∗ T Uk∗ = I .\n(13) where\nLk∗ = vk∑ i=1 αki U k i U k i T + ∑ j 6=k β(k,j)M (k,j)U j∗U j∗TM (k,j) T\nThe solution Uk∗ is given by the top-ck eigenvectors of this modified Laplacian Lk∗. Thus, with fixed Uki s, we can calculate the consensus Uk∗ for each of the sources to maximize the objective function.\nD. Infer the Similarity Mapping between Sources\nUsing above optimization method, we can iteratively learn the latent feature matrices for each source. However, when the number of partially observed mapping is limited, i.e., when only a small number of instances mapping between sources are observed, the estimated initial similarity mapping between two sources may not be accurate. Hence the improvement of clustering performance is limited.\nBased on the similarity transitivity principle, MMC proposes to use the learned latent feature matrices for multiple sources to help infer the similarity mapping across sources.\nSimilar to the initialization, we use the instance mapping between two sources as a bridge to help transfer the similarity. The new estimated instance similarities between two sources can be written as:\nM̃ (i,j) = KUi∗M (i,j)KUj∗ , (14)\nwhere KUi∗ and KUj∗ are the two kernel matrices for latent feature matrices U i∗ and U j∗. In our experiment, we use linear kernel for the latent feature matrices.\nThis estimated similarity mapping includes every instance across two sources. However, we want to preserve the already known instance mapping and only update the instance similarity mapping for instances whose mappings are unknown. We introduce the indicator matrix W i,j , which has the same dimension as M (i,j) and was initialized with only 0 and 1. W\n(i,j) ab equals to 1 if the mapping between a-th instance from source i and the b-th instance from source j is known, and 0 if unknown. The similarity mapping between source i and source j and is updated as follows:\nM (i,j) ←W (i,j) ◦M (i,j) + (1−W (i,j)) ◦ M̃ (i,j), (15)\nwhere ◦ indicates the element-wise multiplication, M̃ (i,j) = U i∗U i∗ T M (i,j)U j∗U j∗\nT and 1 is an all-one matrix. By using the indicator matrix W (i,j) and element-wise multiplication, we can only update the unknown part of the mapping, and preserve the known part. Once we have a better mapping across sources, it will help learn better latent feature matrices. The better latent feature matrices will in-turn help infer the similarity mapping. This iteration continues until it converges."
    }, {
      "heading" : "E. MMC framework",
      "text" : "The algorithm for the MMC framework is shown as Algorithm 1. We first calculate the kernel matrices and the corresponding normalized graph Laplacian matrices for every view. In all the experiments throughout the paper, we use Gaussian kernel for computing the similarities unless mentioned otherwise. The standard deviation of the kernel is set equal to the median of the pair-wise Euclidean distances between the data points. We then initialize the latent feature matrices {Uki }, {Uk∗} and the instance mappings M (i,j). Then we iteratively update Uki s, U\nk∗s and M (i,j)s until they all converge.\nAlgorithm 1 MMC framework\nInput: Data matrices for every view from each source {Xki }. Instance mappings between sources {M (i,j)}. Indicator matrices {W (i,j)}. The number of clusters for each source {ck}. Parameters {αki } and {β(i,j)}. Output: Clustering results for each source. 1: Calculate Kki and L k i for every k and i.\n2: Initialize Uki for every k and i. 3: Initialize Uk∗ by solving Eq.(10). 4: repeat 5: repeat 6: Update each Uki by solving Eq. (12). 7: Update each Uk∗ by solving Eq. (13). 8: until objective function O converges. 9: Update the similarity mappings using Eq. (15).\n10: until mappings between sources converge. 11: Apply k-means on Uk∗ for every source k."
    }, {
      "heading" : "IV. EXPERIMENTS AND RESULTS",
      "text" : "In this section, we compare MMC framework with a number of baselines on three real-world data sets."
    }, {
      "heading" : "A. Comparison Methods",
      "text" : "We compare the proposed MMC method with several stateof-art methods. Since no previous methods can be directly applied to the multi-source multi-view situation, in order to compare with the previous methods, we make some changes. The details of comparison methods is as follows: • MMC: MMC is the clustering framework proposed in\nthis paper, which applies collective spectral clustering with discrepancy penalty across sources. The parameter α is set to 0.1 and β is set to 1 for all the views and sources throughout the experiment. • Concat: Feature concatenation is one way to integrate all the views. We concatenate views within each source, so each source is a concatenated view. Since the instances between sources are not fully aligned, we extend each source by adding pseudo instances (average features). Thus, sources are fully aligned after extension. We then apply PCA and k-means to get the clustering results. • Sym-NMF Symmetric non-negative matrix factorization is proposed in [14] as a general framework for clustering. It factorizes a symmetric matrix containing pairwise similarity values. To apply Sym-NMF to multi-source multi-view situation, we apply Sym-NMF to every view from each source to get the latent feature matrices. Then we combine all the latent feature matrices in the same source to produce the final clustering results. • MultiNMF: MultiNMF is one of the state-of-art multiview clustering methods based on joint nonnegative matrix factorization [15]. MultiNMF formulates a joint matrix factorization process with the constraint that pushes clustering solution of each view towards a common consensus instead of fixing it directly. Throughout the\nexperiment, the parameter λv is set to 0.01 as in the original paper. • CoReg: CoReg is the centroid based multi-view clustering method proposed in [3]. It aims to get clusters that are consistent across views by co-regularizing the clustering hypotheses. Throughout the experiment, the parameter λv is set to 0.01 as suggested in the original paper. • CGC: CGC [8] is the most recent work that deals with many-to-many instance relationship, which is similar to incomplete instance mapping. In order to run the CGC algorithm, we generated the relations between views within one source, which is complete one-to-one mapping. We also generate the relations between views across sources, which is incomplete and partially known. We run CGC on all the views across sources and report the best performance for each source. In the experiment, the parameter λ is set to 1 as suggested in the original paper.\nIt is worth to note that the two multi-view clustering methods MultiNMF and Co-Reg only work with views that are fully aligned. In our experiments, only views from the same source are fully mapped/aligned. Views across different sources are partially mapped. We apply MultiNMF and CoReg in two ways.\nThe first way is to apply them to every single source, denoted as MultiNMF-S and CoReg-S. Thus, both MultiNMF-S and Co-Reg-S only co-regularize the views within a source without considering the discrepancy between sources. The second way is to apply MultiNMF and Co-Reg to multiple sources, denoted as MultiNMF-M and CoReg-M. However, the views across sources are not fully mapped/aligned. In order to apply MultiNMF and Co-Reg, we align the sources by adding average pseudo instances to every source. As shown in Fig. 3, for the unmapped instances in one source, we created the corresponding pseudo instances in other sources. Thus, the instances from different sources are fully mapped. MultiNMF and Co-Reg are then applied to all the aligned views, and performance is reported for every source.\nIn our experiments, we use Gaussian kernel for computing the similarities unless mentioned otherwise. The standard deviation of the kernel is set equal to the median of the pairwise Euclidean distances between the data points. K-means is\nused to get clustering results for all the methods. For each setting, we run k-means 20 times and report the average performance."
    }, {
      "heading" : "B. Dataset",
      "text" : "In this paper, three groups of real-world data sets are used to evaluate the proposed MMC method. The important statistics of them are summarized in Table II. • Dutch-USPS This data set comes from two sources, UCI\nHandwritten Dutch digit numbers and USPS digit data. The first source, Dutch1, consists of 2000 examples of handwritten numbers ’0’-’9’ (200 examples per class) extracted from a collection of Dutch utility maps. All the examples have been digitized in binary images. Each example is represented in the following six views: (1) 76 Fourier coefficients of the character shapes, (2) 216 profile correlations, (3) 64 Karhunen-Love coefficients, (4) 240 pixel averages in 2× 3 windows, (5) 47 Zernike moments, and (6) 6 morphological features. The second source, USPS2, consists of digit images with size 16×16 for numbers ‘0’-‘9’. We randomly select 2000 examples corresponding to the examples in first source. From USPS data, we extract two views, the original pixel feature with dimension of 256 and the Gaussian similarity matrix between examples with dimension of 2000. • English-Translations: This data contains two sources, the original Reuters news documents written in English, and the machine translations in other four languages (French, German, Spanish and Italian) in 6 topics [16]. From the first source, English, we use the document-term matrix and the cosine similarity matrix of the documents as two views. From the second source, Translation, we extract the document-term matrices from French and German as two views. We randomly sample 1200 documents from the first source in a balanced manner, with each category having 200 documents. We then select the corresponding 1200 documents from the second source. • News Text data3: This news data has three sources: BBC, Reuters, and The Guardian. In total there are 948 news articles covering 416 distinct news stories from the period February to April 2009. Thus, the articles from these three sources are naturally partially mapped. Of these distinct stories, 169 were reported in all three sources, 194 in\n1https://archive.ics.uci.edu/ml/datasets/Multiple+Features 2http://www.cs.nyu.edu/ roweis/data.html 3http://mlg.ucd.ie/datasets/3sources.html\ntwo sources, and 53 appeared in a single news source. Each story was annotated with one of the six topical labels: business, entertainment, health, politics, sport, technology. From each source, we extract two views, the document-term matrix and the cosine similarity matrix. From the three sources, we create three sets of data, BBCReuters (239 mapped instances), BBC-Guardian (250 mapped instances) and Reuters-Guadian (212 mapped instances).\nIt is worth noting that both Dutch-USPS and EnglishTranslation data are one-to-one fully mapped. In our experiments, we randomly delete part of the mappings across different sources."
    }, {
      "heading" : "C. Results",
      "text" : "The results for Dutch-USPS data and English-Translation data are shown in Table III. The results are obtained under 60% known mappings . We report the NMI (Normalized Mutual Information) for each source in Table III.\nFrom Table III, we can observe that the proposed MMC framework outperforms all the other comparison methods on both Dutch-USPS and English-Translation data. For the Dutch-USPS data, although CoReg-M and CGC are close to MMC (less than 4%) on Dutch, MMC outperforms the other methods on USPS by at least 10 %. We can also observe that MultiNMF-M and CoReg-M perform better than MultiNMF-S and CoReg-S on Dutch-USPS. However, the performance of the multi-source methods is worse than single-source method on English-Translation. This suggests that combining multiple sources only using the incomplete instance mappings may even hurt the performance. The proposed MMC methods, however, iteratively discovers the similarity among unmapped instances and uses the similarity to help learning.\nWe also reported the results on three sets of the news text data (BBC-Reuters, BBC-Guardian and Reuters-Guardian) in Table IV. From Table IV, we can see that MMC outperforms other comparison methods by a large margin in most cases. On Reuters-Guardian, although MultiNMF-M is slightly better than MMC on Guardian, MMC is still better than all the other baselines on Reuters.\nFrom Table III and Table IV, we can observe that MMC outperforms other comparison methods in most cases for all the three groups of data. We can also conclude that MMC reduces the impact of negative transfer by iteratively learns the similarity among unmapped instances and takes advantage of it. The other reason why MMC can have a better performance for all the sources is that MMC treats the views within each source as a cohesive unit for clustering while considering discrepancy/disagreements between sources."
    }, {
      "heading" : "D. Parameter Study",
      "text" : "There are two sets of parameters in the proposed MMC method: {αki }, the relative importance of view i in source k, and {β(i,j)}, the weight of the discrepancy penalty between source i and j. Here we explore the influence of the view importance weights and the discrepancy penalty weights. We\nfirst fix {β(i,j)} to be 1, and run the proposed MMC method with various {αki } values (10−3 to 103). We then fix {αki } to be 0.1, and run the proposed MMC method with various {β(i,j)} values (10−3 to 103). Due to the limit of space, we only report the results on Dutch-USPS data with 60% known mapping in Fig. 4 and Fig. 5. From Fig. 4, we can see that the performance is stable with αki smaller than 100, and the best performance is achieved when αki is around 0.1. In Fig. 5, The performance is stable with β(i,j) between 0.1 and 100. The best performance is achieved when β(i,j) is near 1."
    }, {
      "heading" : "V. DISCUSSION",
      "text" : "In this section, we aim at analyzing MMC more in detail in order to answer the following four questions:\n(1) How does the difficulty of the clustering problem affect the performance of these methods? (2) How does percentage of known mappings between sources affect the performance of MMC? (3) How good is the inferred similarity mapping? (4) How fast does MMC converge?\nTo show the performance for clustering problem with different difficulties, we apply MMC to Dutch-USPS data but with different number of clusters (2 to 10). The difficulty of the clustering problem increases as the number of clusters increases. The results are shown in Table V. The percentage of known mappings is also set to 60%. The NMIs for both sources are reported in separate rows (MMC(D) for Dutch and MMC(U) for USPS).\nFrom Table V, we can observe that as the number of clusters increases (the difficulty of the problem increases), the performance for all of the methods decrease. The proposed MMC outperforms other comparison methods in almost all cases with one exception. CoReg-M outperforms MMC on USPS when the cluster number is 4. However, the proposed MMC achieved the second best performance in that case.\nTo answer the second question, we apply MMC on both Dutch-USPS data and English-Translation data with various percentages of known mapping between 30% to 100 % (10% interval). The results are shown in Tables VI and VII. It is worth noting that Sym-NMF, CoReg-S and MultiNMFS do not utilize the instance mapping across sources. Thus, the performance of these three methods remain the same for different percentages.\nIn Table VI and Table VII, the proposed MMC outperforms the other comparison methods for both sources in almost all of the different parameter settings. It is important to notice that even with 100% mapping available, the proposed MMC is still better than other multi-view clustering methods. This is because MMC will treat views within one source as a cohesive set while other multi-view clustering algorithms treat the views from different sources equally.\nFrom the results, we can conclude that MMC works for various percentages of known mapping across sources. The\nreason why MMC performs better is not only because it appreciates the cohesiveness of the views, but also for every iteration, MMC tries to infer the instance similarity mapping between different sources. Although the instance similarity mapping is not as the same as the instance mapping, it provides extra information about the partially known instance mapping. Thus the inferred instance similarity mappings will help improve clustering in the next iteration.\nTo show how good the inference of similarity mapping is, we perform another set of experiments to measure the quality of the inferred similarity mappings among those not-aligned instances. For each not-aligned instance, we select the most similar instance mapped by the similarity mapping. Then we check if the two instances are in the same class. We test the accuracy for different number of clusters on Dutch-USPS data. Here the percentage of known instance mapping is set to 60%. We reported the number of instances that are not aligned by the known mapping, and the number of class matches by the inference of similarity mappings in Table VIII.\nFrom Table VIII, we can clearly observe that when the number of clusters is 4, the inference of similarity mapping can get an accuracy as high as 0.8094. As the number of clusters increases, the accuracy of the inference drops. However, even with the number of clusters being as high as 10, we still get an accuracy of 0.6525 for the inference. To have a better understanding of the inferred similarity mapping, we plot the similarity mapping among the unmapped instances from the Dutch-USPS data with four clusters and 60% known mapping in Fig. 6. The instances are sorted by the class label for both sources (the first 80 instances belongs to class 1, the second 80 instances belong to class 2, etc,.) The X axis indicates the instances in Dutch, while the Y axis indicates the instances in USPS. From the figure, we can clearly see that there are four dark squares of width 80 on the diagonal line, which indicates that the same class of instances are more likely to be mapped together.\nTo show how fast MMC converges, we report the number of outer iterations until convergence for Dutch-USPS data in Table IX. From the table we can see that the method converges fast (less than 20 iterations)."
    }, {
      "heading" : "VI. RELATED WORKS",
      "text" : "Multi-view learning [3], [4], [17], [18], is proposed to learn from instances which have multiple representations in different feature space. For example, [1] developed and stud-\nTABLE VII: NMI for English-Translation data with various percentages of known mapping\nMethod 30% known 40% known 50% known 60% known 70% known 80% known 90% known 100% known Concat(E) 0.1144 0.1334 0.1617 0.1914 0.2037 0.2488 0.2495 0.2498\nSym-NMF(E) 0.2783 0.2783 0.2783 0.2783 0.2783 0.2783 0.2783 0.2783 MultiNMF-S(E) 0.3413 0.3413 0.3413 0.3413 0.3413 0.3413 0.3413 0.3413 MultiNMF-M(E) 0.3253 0.3289 0.3339 0.3420 0.3689 0.3531 0.3523 0.3508\nCoReg-S(E) 0.3381 0.3381 0.3381 0.3381 0.3381 0.3381 0.3381 0.3381 CoReg-M(E) 0.2060 0.2088 0.2098 0.2187 0.2194 0.2193 0.2184 0.2182\nCGC(E) 0.2388 0.2550 0.2656 0.2636 0.2707 0.2680 0.2737 0.2740 MMC(E) 0.3436 0.3485 0.3571 0.3528 0.3576 0.3595 0.3558 0.3637 Concat(T) 0.1044 0.1124 0.1397 0.1621 0.1685 0.2098 0.2072 0.2198 Sym-NMF(T) 0.1527 0.1527 0.1527 0.1527 0.1527 0.1527 0.1527 0.1527 MultiNMF-S(T) 0.2708 0.2708 0.2708 0.2708 0.2708 0.2708 0.2708 0.2708 MultiNMF-M(T) 0.1945 0.2063 0.2146 0.2164 0.2146 0.2223 0.2541 0.2581\nCoReg-S(T) 0.2874 0.2874 0.2874 0.2874 0.2874 0.2874 0.2874 0.2874 CoReg-M(T) 0.2160 0.2146 0.2178 0.2198 0.2171 0.2213 0.2210 0.2299\nCGC(T) 0.2288 0.2450 0.2556 0.2536 0.2607 0.2580 0.2637 0.2640 MMC(T) 0.3028 0.3075 0.3072 0.3073 0.3090 0.3144 0.3196 0.3230\nFig. 6: Scatter plot of the inferred similarity mapping for Dutch-USPS data with four clusters.\nied partitioning and agglomerative, hierarchical multi-view clustering algorithms for text data. [3], [5] are among the first works proposed to solve clustering problem via spectral projection. [15] proposed to solve multi-view clustering by joint non-negative matrix factorization. [6], [7], [19], [20] are among the first works to solve the multi-view clustering with\npartial/incomplete views. However, none of the previous multiview clustering methods can deal with incomplete and partial known mapping between sources/views. Further more, All the previous methods fail to treat the views within one source as a cohesive unit.\nConsensus clustering [21], [22] is also related to the proposed MMC framework. It deals with the situation in which a number of different clustering results have been obtained for a particular dataset and it is desired to find a single consensus clustering which is a better fit in some sense than the existing ones. [23] gives a report about consensus clustering algorithms comparison and refinement. [24] proposes a bayesian consensus clustering method. However, consensus clustering aims to find a single consensus clustering from fully mapped clustering solutions. None of the previous methods works for the incomplete and partially unknown mappings between the instances."
    }, {
      "heading" : "VII. CONCLUSION",
      "text" : "This paper is the first to investigate the problem of clustering with multiple sources and multiple views. The proposed MMC framework treats views in the same source as a cohesive group for clustering by learning consensus latent feature matrices\nfrom the views within one source. It also incorporates multiple sources by using cross-source discrepancy penalty to enhance the clustering performance. MMC also uses the learned latent features to infer the cross-source unknown similarity mapping, which in turn will help improve the performance of clustering. Extensive experiments conducted on three groups of realworld data sets show the effectiveness of MMC comparing with other state-of-arts methods."
    } ],
    "references" : [ {
      "title" : "Multi-view clustering",
      "author" : [ "S. Bickel", "T. Scheffer" ],
      "venue" : "ICDM, 2004.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Multi-view clustering via canonical correlation analysis",
      "author" : [ "K. Chaudhuri", "S.M. Kakade", "K. Livescu", "K. Sridharan" ],
      "venue" : "ICML, 2009.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Co-regularized multi-view spectral clustering",
      "author" : [ "A. Kumar", "P. Rai", "H.D. III" ],
      "venue" : "NIPS, 2011.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "A general model for multiple view unsupervised learning.",
      "author" : [ "B. Long", "P.S. Yu", "Z.M. Zhang" ],
      "venue" : "in SDM. SIAM,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2008
    }, {
      "title" : "A co-training approach for multi-view spectral clustering",
      "author" : [ "A. Kumar", "H.D. III" ],
      "venue" : "ICML, 2011.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Clustering on multiple incomplete datasets via collective kernel learning",
      "author" : [ "W. Shao", "X. Shi", "P.S. Yu" ],
      "venue" : "ICDM, 2013.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Partial multi-view clustering",
      "author" : [ "S. Li", "Y. Jiang", "Z. Zhou" ],
      "venue" : "AAAI, 2014, pp. 1968–1974.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Clustering on multi-source incomplete data via tensor modeling and factorization",
      "author" : [ "W. Shao", "L. He", "P.S. Yu" ],
      "venue" : "PAKDD, 2015.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "A tutorial on spectral clustering",
      "author" : [ "U. Luxburg" ],
      "venue" : "Statistics and Computing, vol. 17, no. 4, pp. 395–416, Dec. 2007.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "On spectral clustering: Analysis and an algorithm",
      "author" : [ "A.Y. Ng", "M.I. Jordan", "Y. Weiss" ],
      "venue" : "NIPS, 2002, pp. 849–856.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Inferring anchor links across multiple heterogeneous social networks",
      "author" : [ "X. Kong", "J. Zhang", "P.S. Yu" ],
      "venue" : "CIKM, 2013, pp. 179–188.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Meta-path based multi-network collective link prediction",
      "author" : [ "J. Zhang", "P.S. Yu", "Z.-H. Zhou" ],
      "venue" : "KDD, 2014.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Symmetric nonnegative matrix factorization for graph clustering.",
      "author" : [ "D. Kuang", "H. Park", "C.H.Q. Ding" ],
      "venue" : "in SDM,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2012
    }, {
      "title" : "Multi-view clustering via joint nonnegative matrix factorization",
      "author" : [ "J. Liu", "C. Wang", "J. Gao", "J. Han" ],
      "venue" : "SDM, 2013.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Learning from multiple partially observed views - an application to multilingual text categorization",
      "author" : [ "M.R. Amini", "N. Usunier", "C. Goutte" ],
      "venue" : "NIPS, 2009.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Combining labeled and unlabeled data with co-training",
      "author" : [ "A. Blum", "T. Mitchell" ],
      "venue" : "COLT, 1998.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Analyzing the effectiveness and applicability of co-training",
      "author" : [ "K. Nigam", "R. Ghani" ],
      "venue" : "CIKM, 2000.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Multiview clustering with incomplete views",
      "author" : [ "A. Trivedi", "P. Rai", "H. Daumé III", "S. DuVall" ],
      "venue" : "NIPS Workshop, 2010.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Multiple incomplete views clustering via weighted nonnegative matrix factorization with L2,1 regularization",
      "author" : [ "W. Shao", "L. He", "P.S. Yu" ],
      "venue" : "ECML PKDD, 2015.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Consensus clustering: A resampling-based method for class discovery and visualization of gene expression microarray data",
      "author" : [ "S. Monti", "P. Tamayo", "J. Mesirov", "T. Golub" ],
      "venue" : "Mach. Learn., 2003.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Weighted Consensus Clustering",
      "author" : [ "T. Li", "C. Ding" ],
      "venue" : "SDM, 2008.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Consensus Clustering Algorithms: Comparison and Refinement",
      "author" : [ "A. Goder", "V. Filkov" ],
      "venue" : "9th Workshop on Algorithm Engineering and Experiments, 2008.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Bayesian consensus clustering.",
      "author" : [ "E.F. Lock", "D.B. Dunson" ],
      "venue" : "Bioinformatics, vol. 29,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Multiple view clustering [1]–[3] aims to enhance clustering performance by integrating different views.",
      "startOffset" : 25,
      "endOffset" : 28
    }, {
      "referenceID" : 2,
      "context" : "Multiple view clustering [1]–[3] aims to enhance clustering performance by integrating different views.",
      "startOffset" : 29,
      "endOffset" : 32
    }, {
      "referenceID" : 0,
      "context" : "Multi-view clustering [1], [4], [5] aims to utilize the multiple representations of instances in different features spaces to get better clustering performance.",
      "startOffset" : 22,
      "endOffset" : 25
    }, {
      "referenceID" : 3,
      "context" : "Multi-view clustering [1], [4], [5] aims to utilize the multiple representations of instances in different features spaces to get better clustering performance.",
      "startOffset" : 27,
      "endOffset" : 30
    }, {
      "referenceID" : 4,
      "context" : "Multi-view clustering [1], [4], [5] aims to utilize the multiple representations of instances in different features spaces to get better clustering performance.",
      "startOffset" : 32,
      "endOffset" : 35
    }, {
      "referenceID" : 5,
      "context" : "Although there are some previous studies on dealing with multiple incomplete view clustering [6]–[9], none of them are suitable for multi-source multi-view scenario.",
      "startOffset" : 93,
      "endOffset" : 96
    }, {
      "referenceID" : 7,
      "context" : "Although there are some previous studies on dealing with multiple incomplete view clustering [6]–[9], none of them are suitable for multi-source multi-view scenario.",
      "startOffset" : 97,
      "endOffset" : 100
    }, {
      "referenceID" : 2,
      "context" : "To perform spectral clustering for a single view i in source k, as shown in [3], [10], [11], we only need to solve the following optimization problem for the normalized graph Laplacian Li :",
      "startOffset" : 76,
      "endOffset" : 79
    }, {
      "referenceID" : 8,
      "context" : "To perform spectral clustering for a single view i in source k, as shown in [3], [10], [11], we only need to solve the following optimization problem for the normalized graph Laplacian Li :",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 9,
      "context" : "To perform spectral clustering for a single view i in source k, as shown in [3], [10], [11], we only need to solve the following optimization problem for the normalized graph Laplacian Li :",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 2,
      "context" : "Similar to the regularization in [3], we define the discrepancy/dissimilarity between two latent feature matrices as:",
      "startOffset" : 33,
      "endOffset" : 36
    }, {
      "referenceID" : 10,
      "context" : "instance mapping is really challenging and usually additional information is required to infer such anchor link [12], [13].",
      "startOffset" : 112,
      "endOffset" : 116
    }, {
      "referenceID" : 11,
      "context" : "instance mapping is really challenging and usually additional information is required to infer such anchor link [12], [13].",
      "startOffset" : 118,
      "endOffset" : 122
    }, {
      "referenceID" : 9,
      "context" : "According to [11], the solution U i is given by the top-ck eigenvectors of this modified Laplacian.",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 12,
      "context" : "• Sym-NMF Symmetric non-negative matrix factorization is proposed in [14] as a general framework for clustering.",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 13,
      "context" : "• MultiNMF: MultiNMF is one of the state-of-art multiview clustering methods based on joint nonnegative matrix factorization [15].",
      "startOffset" : 125,
      "endOffset" : 129
    }, {
      "referenceID" : 2,
      "context" : "• CoReg: CoReg is the centroid based multi-view clustering method proposed in [3].",
      "startOffset" : 78,
      "endOffset" : 81
    }, {
      "referenceID" : 14,
      "context" : "• English-Translations: This data contains two sources, the original Reuters news documents written in English, and the machine translations in other four languages (French, German, Spanish and Italian) in 6 topics [16].",
      "startOffset" : 215,
      "endOffset" : 219
    }, {
      "referenceID" : 2,
      "context" : "Multi-view learning [3], [4], [17], [18], is proposed to learn from instances which have multiple representations in different feature space.",
      "startOffset" : 20,
      "endOffset" : 23
    }, {
      "referenceID" : 3,
      "context" : "Multi-view learning [3], [4], [17], [18], is proposed to learn from instances which have multiple representations in different feature space.",
      "startOffset" : 25,
      "endOffset" : 28
    }, {
      "referenceID" : 15,
      "context" : "Multi-view learning [3], [4], [17], [18], is proposed to learn from instances which have multiple representations in different feature space.",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 16,
      "context" : "Multi-view learning [3], [4], [17], [18], is proposed to learn from instances which have multiple representations in different feature space.",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 0,
      "context" : "For example, [1] developed and stud-",
      "startOffset" : 13,
      "endOffset" : 16
    }, {
      "referenceID" : 2,
      "context" : "[3], [5] are among the first works proposed to solve clustering problem via spectral projection.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 4,
      "context" : "[3], [5] are among the first works proposed to solve clustering problem via spectral projection.",
      "startOffset" : 5,
      "endOffset" : 8
    }, {
      "referenceID" : 13,
      "context" : "[15] proposed to solve multi-view clustering by joint non-negative matrix factorization.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 5,
      "context" : "[6], [7], [19], [20] are among the first works to solve the multi-view clustering with partial/incomplete views.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 6,
      "context" : "[6], [7], [19], [20] are among the first works to solve the multi-view clustering with partial/incomplete views.",
      "startOffset" : 5,
      "endOffset" : 8
    }, {
      "referenceID" : 17,
      "context" : "[6], [7], [19], [20] are among the first works to solve the multi-view clustering with partial/incomplete views.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 18,
      "context" : "[6], [7], [19], [20] are among the first works to solve the multi-view clustering with partial/incomplete views.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 19,
      "context" : "Consensus clustering [21], [22] is also related to the proposed MMC framework.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 20,
      "context" : "Consensus clustering [21], [22] is also related to the proposed MMC framework.",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 21,
      "context" : "[23] gives a report about consensus clustering algorithms comparison and refinement.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 22,
      "context" : "[24] proposes a bayesian consensus clustering method.",
      "startOffset" : 0,
      "endOffset" : 4
    } ],
    "year" : 2016,
    "abstractText" : "With the advance of technology, entities can be observed in multiple views. Multiple views containing different types of features can be used for clustering. Although multi-view clustering has been successfully applied in many applications, the previous methods usually assume the complete instance mapping between different views. In many real-world applications, information can be gathered from multiple sources, while each source can contain multiple views, which are more cohesive for learning. The views under the same source are usually fully mapped, but they can be very heterogeneous. Moreover, the mappings between different sources are usually incomplete and partially observed, which makes it more difficult to integrate all the views across different sources. In this paper, we propose MMC (Multisource Multi-view Clustering), which is a framework based on collective spectral clustering with a discrepancy penalty across sources, to tackle these challenges. MMC has several advantages compared with other existing methods. First, MMC can deal with incomplete mapping between sources. Second, it considers the disagreements between sources while treating views in the same source as a cohesive set. Third, MMC also tries to infer the instance similarities across sources to enhance the clustering performance. Extensive experiments conducted on real-world data demonstrate the effectiveness of the proposed approach.",
    "creator" : "LaTeX with hyperref package"
  }
}