{
  "name" : "1310.0509.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ "fidaner@alternatifbilisim.org", "taylan.cemgil@boun.edu.tr" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n31 0.\n05 09\nv4 [\ncs .L\nG ]\n2 5\nN ov"
    }, {
      "heading" : "1 Introduction",
      "text" : "Clustering aims to summarize observed data by grouping its elements according to their similarities. Depending on the application, clusters may represent words belonging to topics, genes belonging to metabolic processes or any other relation assumed by the deployed approach. Infinite mixture models provide a general solution by allowing a potentially unlimited number of mixture components. These models are based on nonparametric priors such as Dirichlet process (DP) [1, 2], its superclass Poisson-Dirichlet process (PDP) [3, 4] and constructions such as Chinese restaurant process (CRP) [5] and stick-breaking process [6] that enable formulations of efficient inference methods [7]. Studies on infinite mixture models inspired the development of several other models [8, 9] including Indian buffet process (IBP) for infinite feature models [10, 11] and fragmentation-coagulation process for sequence data [12] all of which belong to Bayesian nonparametrics [13].\nIn making inference on infinite mixture models, a sample set of partitionings can be obtained from the posterior.1 If the posterior is peaked around a single partitioning, then the maximum a posteriori solution will be quite informative. However, in some cases the posterior is more diffuse and one needs to extract statistical information about the random partitioning induced by the model. This problem to ‘summarize’ the samples from the infinite mixture posterior was raised in bioinformatics literature in 2002 by Medvedovic and Sivaganesan for clustering gene expression profiles [14]. But the question proved difficult and they ‘circumvented’ it by using a heuristic linkage algorithm based on pairwise occurence probabilities [15, 16]. In this paper, we approach this problem and propose basic methodology for summarizing sample sets of partitionings as well as feature allocations.\nNemenman et al. showed in 2002 that the entropy [17] of a DP posterior was strongly determined by its prior hyperparameters [18]. Archer et al. recently elaborated these results with respect to PDP [19]. In other work, entropy was generalized to partitionings by interpreting partitionings as probability distributions [20, 21]. Therefore, entropy emerges as an important statistic for our problem, but new definitions will be needed for quantifying information in feature allocations.\n1In methods such as collapsed Gibbs sampling, slice sampling, retrospective sampling, truncation methods\nIn the following sections, we define the problem and introduce cumulative statistics for representing partitionings and feature allocations. Then, we develop an interpretation for entropy function in terms of per-element information in order to quantify segmentation among their elements. Finally, we describe entropy agglomeration (EA) algorithm that generates dendrograms to summarize sample sets of partitionings and feature allocations. We demonstrate EA on infinite mixture posteriors for synthetic and real datasets as well as on a real dataset directly interpreted as a feature allocation."
    }, {
      "heading" : "2 Basic definitions and the motivating problem",
      "text" : "We begin with basic definitions. A partitioning of a set of elements [n] = {1, 2, . . . , n} is a set of blocks Z = {B1, . . . , B|Z|} such that Bi ⊂ [n] and Bi 6= ∅ for all i ∈ {1, . . . , n}, Bi ∩ Bj = ∅ for all i 6= j, and ∪iBi = [n].2 We write Z ⊢ [n] to designate that Z is a partitioning of [n].3 A sample set E = {Z(1), . . . , Z(T )} from a distribution π(Z) over partitionings is a multiset such that Z(t) ∼ π(Z) for all t ∈ {1, . . . , T }. We are required to extract information from this sample set.\nOur motivation is the following problem: a set of observed elements (x1, . . . , xn) are clustered by an infinite mixture model with parameters θ(k) for each component k and mixture assignments (z1, . . . , zn) drawn from a two-parameter CRP prior with concentration α and discount d [5].\nz ∼ CRP (z;α, d) θ(k) ∼ p(θ) xi | zi, θ ∼ F (xi | θ (zi)) (1)\nIn the conjugate case, all θ(k) can be integrated out to get p(zi | z−i, x) for sampling zi [22]:\np(zi | z−i, x) ∝\n∫\np(z, x, θ) dθ ∝\n\n\n\nnk−d n−1+α\n∫\nF (xi|θ) p(θ|x−i, z−i) dθ if k ≤ K+\nα+dK+ n−1+α ∫ F (xi|θ) p(θ) dθ otherwise (2)\nThere are K+ non-empty components and nk elements in each component k. In each iteration, xi will either be put into an existing component k ≤ K+ or it will be assigned to a new component. By sampling all zi repeatedly, a sample set of assignments z(t) are obtained from the posterior p(z | x) = π(Z). These z(t) are then represented by partitionings Z(t) ⊢ [n]. The induced sample set contains information regarding (1) CRP prior over partitioning structure given by the hyperparameters (α, d) and (2) integrals over θ that capture the relation among the observed elements (x1, . . . , xn).\nIn addition, we aim to extract information from feature allocations, which constitute a superclass of partitionings [11]. A feature allocation of [n] is a multiset of blocks F = {B1, . . . , B|F |} such that Bi ⊂ [n] and Bi 6= ∅ for all i ∈ {1, . . . , n}. A sample set E = {F (1), . . . , F (T )} from a distribution π(F ) over feature allocations is a multiset such that F (t) ∼ π(F ) for all t. Current exposition will focus on partitionings, but we are also going to show how our statistics apply to feature allocations.\nAssume that we have obtained a sample set E of partitionings. If it was obtained by sampling from an infinite mixture posterior, then its blocks B ∈ Z(t) correspond to the mixture components. Given a sample set E, we can approximate any statistic f(Z) over π(Z) by averaging it over the set E:\nZ(1), . . . , Z(T ) ∼ π(Z) ⇒ 1\nT\nT ∑\nt=1\nf(Z(t)) ≈ 〈 f(Z) 〉π(Z) (3)\nWhich f(Z) would be a useful statistic for Z? Three statistics commonly appear in the literature:\nFirst one is the number of blocks |Z|, which has been analyzed theoretically for various nonparametric priors [2, 5]. It is simple, general and exchangable with respect to the elements [n], but it is not very informative about the distribution π(Z) and therefore is not very useful in practice.\nA common statistic is pairwise occurence, which is used to extract information from infinite mixture posteriors in applications like bioinformatics [14]. For given pairs of elements {a, b}, it counts the number of blocks that contain these pairs, written ∑\ni[{a, b} ⊂ Bi]. It is a very useful similarity measure, but it cannot express information regarding relations among three or more elements.\nAnother statistic is exact block size distribution (referred to as ‘multiplicities’ in [11, 19]). It counts the partitioning’s blocks that contain exactly k elements, written ∑\ni[|Bi| = k]. It is exchangable with respect to the elements [n], but its weighted average over a sample set is difficult to interpret.\n2We use the term ‘partitioning’ to indicate a ‘set partition’ as distinguished from an integer ‘partition’. 3The symbol ‘⊢’ is usually used for integer partitions, but here we use it for partitionings (=set partitions).\nLet us illustrate the problem by a practical example, to which we will return in the formulations:\nZ(1) = {{1, 3, 6, 7}, {2}, {4, 5}} S1 = {1, 2, 3, 4}\nE3 = {Z (1), Z(2), Z(3)} Z(2) = {{1, 3, 6}, {2, 7}, {4, 5}} S2 = {1, 3, 6, 7}\nZ(3) = {{1, 2, 3, 6, 7}, {4, 5}} S3 = {1, 2, 3}\nSuppose that E3 represents interactions among seven genes. We want to compare the subsets of these genes S1, S2, S3. The projection of a partitioning Z ⊢ [n] onto S ⊂ [n] is defined as the set of non-empty intersections between S and B ∈ Z . Projection onto S induces a partitioning of S.\nPROJ(Z, S) = {B ∩ S}B∈Z\\{∅} ⇒ PROJ(Z, S) ⊢ S (4)\nLet us represent gene interactions in Z(1) andZ(2) by projecting them onto each of the given subsets:\nPROJ(Z(1), S1) = {{1, 3}, {2}, {4}} PROJ(Z (2), S1) = {{1, 3}, {2}, {4}}\nPROJ(Z(1), S2) = {{1, 3, 6, 7}} PROJ(Z (2), S2) = {{1, 3, 6}, {7}}\nPROJ(Z(1), S3) = {{1, 3}, {2}} PROJ(Z (2), S3) = {{1, 3}, {2}}\nComparing S1 to S2, we can say that S1 is ‘more segmented’ than S2, and therefore genes in S2 should be more closely related than those in S1. However, it is more subtle and difficult to compare S2 to S3. A clear understanding would allow us to explore the subsets S ⊂ [n] in an informed manner. In the following section, we develop a novel and general approach based on block sizes that opens up a systematic method for analyzing sample sets over partitionings and feature allocations."
    }, {
      "heading" : "3 Cumulative statistics to represent structure",
      "text" : "We define cumulative block size distribution, or ‘cumulative statistic’ in short, as the function φk(Z) = ∑\ni[|Bi| ≥ k], which counts the partitioning’s blocks of size at least k. We can rewrite the previous statistics: number of blocks as φ1(Z), exact block size distribution as φk(Z) − φk+1(Z), and pairwise occurence as φ2(PROJ(Z, {a, b})). Moreover, cumulative statistics satisfy the following property: for partitionings of [n], φ(Z) always sums up to n, just like a probability mass function that sums up to 1. When blocks of Z are sorted according to their sizes and the indicators [|Bi| ≥ k] are arranged on a matrix as in Figure 1a, they form a Young diagram, showing that φ(Z) is always the conjugate partition of the integer partition of Z . As a result, φ(Z) as well as weighted averages over several φ(Z) always sum up to n, just like taking averages over probability mass functions (Figure 2). Therefore, cumulative statistics of a random partitioning ‘conserve mass’. In the\ncase of feature allocations, since elements can be omitted or repeated, this property does not hold.\nZ ⊢ [n] ⇒ n ∑\nk=1\nφk(Z) = n ⇒ n ∑\nk=1\n〈 φk(Z) 〉π(Z) = n (5)\nWhen we project the partitioning Z onto a subset S ⊂ [n], the resulting vector φ(PROJ(Z, S)) will then sum up to |S| (Figure 1b). A ‘taller’ Young diagram implies a ‘more segmented’ subset.\nWe can form a partitioning Z by inserting elements 1, 2, 3, 4, . . . into its blocks (Figure 3a). In such a scheme, each step brings a new element and requires a new decision that will depend on all previous decisions. It would be better if we could determine the whole path by few initial decisions.\nNow suppose that we know Z from the start and we generate an incremental sequence of subsets S1 = {1}, S2 = {1, 2}, S3 = {1, 2, 3}, S4 = {1, 2, 3, 4}, . . . according to a permutation of [n]: σ = (1, 2, 3, 4, . . . ). We can then represent any path in Figure 3a by a sequence of PROJ(Z, Si) and determine the whole path by two initial parameters: Z and σ. The resulting tree can be simplified by representing the partitionings by their cumulative statistics instead of their blocks (Figure 3b).\nBased on this concept, we define cumulative occurence distribution (COD) as the triangular matrix of incremental cumulative statistic vectors, written ∆i,k(Z, σ) = φk(PROJ(Z, Si)) whereZ ⊢ [n], σ is a permutation of [n] and Si = {σ1, . . . , σi} for i ∈ {1, . . . , n}. COD matrices for two extreme paths (Figure 3c, 3e) and for the example partitioning Z(1) (Figure 3d) are shown. For partitionings, ith row of a COD matrix always sums up to i, even when averaged over a sample set as in Figure 4.\nZ ⊢ [n] ⇒ i ∑\nk=1\n∆i,k(Z, σ) = i ⇒ i ∑\nk=1\n〈 ∆i,k(Z, σ) 〉π(Z) = i (6)\nExpected COD matrix of a random partitioning expresses (1) cumulation of elements by the differences between its rows, and (2) cumulation of block sizes by the differences between its columns.\nAs an illustrative example, consider π(Z) = CRP (Z|α, d). Since CRP is exchangable and projective, its expected cumulative statistic 〈φ(Z)〉π(Z) for n elements depends only on its hyperparameters (α, d). As a result, its expected COD matrix ∆ = 〈∆(Z, σ)〉π(Z) is independent of σ, and it\nsatisfies an incremental formulation with the parameters (α, d) over the indices i ∈ N, k ∈ Z+:\n∆0,k = 0 ∆i+1,k = ∆i,k +\n\n\n\nα+d∆i,k i+α if k = 1 (k−1−d)(∆i,k−1−∆i,k) i+α otherwise\n(7)\nBy allowing k = 0 and setting ∆i,0 = −αd , and ∆0,k = 0 for k > 0 as the two boundary conditions, the same matrix can be formulated by a difference equation over the indices i ∈ N, k ∈ N:\n(∆i+1,k −∆i,k)(i + α) = (∆i,k−1 −∆i,k)(k − 1− d) (8)\nBy setting ∆ = ∆(0) we get an infinite sequence of matrices ∆(m) that satisfy the same equation:\n(∆ (m) i+1,k −∆ (m) i,k )(i + α) = (∆ (m) i,k−1 −∆ (m) i,k )(k − 1− d) = ∆ (m+1) i,k (9)\nTherefore, expected COD matrix of a CRP-distributed random partitioning is at a constant ‘equilibrium’ determined by α and d. This example shows that the COD matrix can reveal specific information about a distribution over partitionings; of course in practice we encounter non-exchangeable and almost arbitrary distributions over partitionings (e.g., the posterior distribution of an infinite mixture), therefore in the following section we will develop a measure to quantify this information."
    }, {
      "heading" : "4 Entropy to quantify segmentation",
      "text" : "Shannon’s entropy [17] can be an appropriate quantity to measure ‘segmentation’ with respect to partitionings, which can be interpreted as probability distributions [20, 21]. Since this interpretation does not cover feature allocations, we will make an alternative, element-based definition of entropy.\nHow does a block B inform us about its elements? Each element has a proportion 1/|B|, let us call this quantity per-element segment size. Information is zero for |B| = n, since 1/n is the minimum possible segment size. If |B| < n, the block supplies positive information since the segment size is larger than minimum, and we know that its segment size could be smaller if the block were larger. To quantify this information, we define per-element information for a block B as the integral of segment size 1/s over the range [|B|, n] of block sizes that make this segment smaller (Figure 5).\npein(B) =\n∫ n\n|B|\n1 s ds = log n |B| (10)\nIn pein(B), n is a ‘base’ that determines the minimum possible per-element segment size. Since segment size expresses the significance of elements, the function integrates segment sizes over the block sizes that make the elements less significant. This definition is comparable to the well-known p-value, which integrates probabilities over the values that make the observations more significant.\n2 4 6 8 10 12 0\n0.5\n1\n1.5\n2\n2.5\nnumber of elements n\npa rt\niti on\ne nt\nro py\nH (Z\n)\nFigure 7: H(Z) in incremental construction of Z\nWe can then compute the per-element information supplied by a partitioningZ , by taking a weighted average over its blocks, since each block B ∈ Z supplies information for a different proportion |B|/n of the elements being partitioned. For large n, weighted per-element information reaches its maximum near |B| ≈ n/2 (Figure 6). Total weighted information for Z gives Shannon’s entropy function [17] which can be written in terms of the cumulative statistics (assuming φn+1 = 0):\nH(Z) =\n|Z| ∑\ni=1\n|Bi|\nn pein(Bi) =\n|Z| ∑\ni=1\n|Bi|\nn log\nn\n|Bi| =\nn ∑\nk=1\n(φk(Z)− φk+1(Z)) k\nn log\nn k (11)\nEntropy of a partitioning increases as its elements become more segmented among themselves. A partitioning with a single block has zero entropy, and a partitioning with n blocks has the maximum entropy log n. Nodes of the tree we examined in the previous section (Figure 3b) were vertically arranged according to their entropies. On the extended tree (Figure 7), nth column of nodes represent the possible partitionings of n. This tree serves as a ‘grid’ for both H(Z) and φ(Z), as they are linearly related with the general coefficient ( k\nn log n k − k−1 n log n k−1 ). A similar grid for feature allocations can be generated by inserting nodes for cumulative statistics that do not conserve mass.\nTo quantify the segmentation of a subset S, we compute projection entropy H(PROJ(Z, S)). To understand this function, we compare it to subset occurence in Figure 8. Subset occurence acts as a ‘score’ that counts the ‘successful’ blocks that contain all of S, whereas projection entropy acts as a ‘penalty’ that quantifies how much S is being divided and segmented by the given blocks B ∈ Z .\nA partitioning Z and a permutation σ of its elements induce an entropy sequence (h1, . . . , hn) such that hi(Z, σ) = H(PROJ(Z, Si)) where Si = {σ1, . . . , σi} for i ∈ {1, . . . , n}. To find subsets of elements that are more closely related, one can seek permutations σ that keep the entropies low. The generated subsets Si will be those that are less segmented by B ∈ Z . For the example problem, the permutation 1, 3, 6, 7, . . . keeps the expected entropies lower, compared to 1, 2, 3, 4, . . . (Figure 4)."
    }, {
      "heading" : "5 Entropy agglomeration and experimental results",
      "text" : "We want to summarize a sample set using the proposed statistics. Permutations that yield lower entropy sequences can be meaningful, but a feasible algorithm can only involve a small subset of the n! permutations. We define entropy agglomeration (EA) algorithm, which begins from 1-element subsets, and merges in each iteration the pair of subsets that yield the minimum expected entropy:\nEntropy Agglomeration Algorithm:\n1. Initialize Ψ ← {{1}, {2}, . . . , {n}}.\n2. Find the subset pair {Sa, Sb} ⊂ Ψ that minimizes the entropy 〈H(PROJ(Z, Sa ∪ Sb)) 〉π(Z).\n3. Update Ψ ← (Ψ\\{Sa, Sb}) ∪ {Sa ∪ Sb}.\n4. If |Ψ| > 1 then go to 2.\n5. Generate the dendrogram of chosen pairs by plotting minimum entropies for every split.\nThe resulting dendrogram for the example partitionings are shown in Figure 9a. The subsets {4, 5} and {1, 3, 6} are shown in individual nodes, because their entropies are zero. Besides using this dendrogram as a general summary, one can also generate more specific dendrograms by choosing specific elements or specific parts of the data. For a detailed element-wise analysis, entropy sequences of particular permutations σ can be assessed. Entropy Agglomeration is inspired by ‘agglomerative clustering’, a standard approach in bioinformatics [23]. To summarize partitionings of gene expressions, [14] applied agglomerative clustering by pairwise occurences. Although very useful and informative, such methods remain ‘heuristic’ because they require a ‘linkage criterion’ in merging subsets. EA avoids this drawback, since projection entropy is already defined over subsets.\nTo test the proposed algorithm, we apply it to partitionings sampled from infinite mixture posteriors. In the first three experiments, data is modeled by an infinite mixture of Gaussians, where α = 0.05, d = 0, p(θ) = N (θ|0, 5) and F (x|θ) = N (x|θ, 0.15) (see Equation 1). Samples from the posterior are used to plot the histogram over the number of blocks, pairwise occurences, and the EA dendrogram. Pairwise occurences are ordered according to the EA dendrogram. In the fourth experiment, EA is directly applied on the data. We describe each experiment and make observations:\n1) Synthetic data (Figure 9b): 30 points on R2 are arranged in three clusters. Plots are based on 450 partitionings from the posterior. Clearly separating the three clusters, EA also reflects their qualitative differences. The dispersedness of the first cluster is represented by distinguishing ‘inner’ elements 1, 10, from ‘outer’ elements 6, 7. This is also seen as shades of gray in pairwise occurences.\n2) Iris flower data (Figure 9c): This well-known dataset contains 150 points on R4 from three flower species [24]. Plots are based on 150 partitionings obtained from the posterior. For convenience, small subtrees are shown as single leaves and elements are labeled by their species. All of 50 A points appear in a single leaf, as they are clearly separated from B and C. The dendrogram automatically scales to cover the points that are more uncertain with respect to the distribution.\n3) Galactose data (Figure 9d): This is a dataset of gene expressions by 820 genes in 20 experimental conditions [25]. First 204 genes are chosen, and first two letters of gene names are used for labels. Plots are based on 250 partitionings from the posterior. 70 RP (ribosomal protein) genes and 12 HX (hexose transport) genes appear in individual leaves. In the large subtree on the top, an ‘outer’ grouping of 19 genes (circles in data plot) is distinguished from the ‘inner’ long tail of 68 genes.\n4) IGO (Figure 9e): This is a dataset of intergovernmental organizations (IGO) [26,v2.1] that contains IGO memberships of 214 countries through the years 1815-2000. In this experiment, we take a different approach and apply EA directly on the dataset interpreted as a sample set of single-block feature allocations, where the blocks are IGO-year tuples and elements are the countries. We take the subset of 138 countries that appear in at least 1000 of the 12856 blocks. With some exceptions, the countries display a general ordering of continents. From the ‘outermost’ continent to the ‘innermost’ continent they are: Europe, America-Australia-NZ, Asia, Africa and Middle East."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this paper, we developed a novel approach for summarizing sample sets of partitionings and feature allocations. After presenting the problem, we introduced cumulative statistics and cumulative occurence distribution matrices for each of its permutations, to represent a sample set in a systematic manner. We defined per-element information to compute entropy sequences for these permutations. We developed entropy agglomeration (EA) algorithm that chooses and visualises a small subset of these entropy sequences. Finally, we experimented with various datasets to demonstrate the method.\nEntropy agglomeration is a simple algorithm that does not require much knowledge to implement, but it is conceptually based on the cumulative statistics we have presented. Since we primarily aimed to formulate a useful algorithm, we only made the essential definitions, and several points remain to be elucidated. For instance, cumulative statistics can be investigated with respect to various nonparametric priors. Our definition of per-element information can be developed with respect to information theory and hypothesis testing. Last but not least, algorithms like entropy agglomeration can be designed for summarization tasks concerning various types of combinatorial sample sets."
    }, {
      "heading" : "Acknowledgements",
      "text" : "We thank Ayça Cankorur, Erkan Karabekmez, Duygu Dikicioğlu and Betül Kırdar from Boğaziçi University Chemical Engineering for introducing us to this problem by very helpful discussions. This work was funded by TÜBİTAK (110E292) and BAP (6882-12A01D5)."
    } ],
    "references" : [ {
      "title" : "A Bayesian analysis of some nonparametric problems",
      "author" : [ "T.S. Ferguson" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1973
    }, {
      "title" : "The two-parameter Poisson–Dirichlet distribution derived from a stable subordinator",
      "author" : [ "J. Pitman", "M. Yor" ],
      "venue" : "Annals of Probability,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1997
    }, {
      "title" : "Combinatorial Stochastic Processes",
      "author" : [ "J. Pitman" ],
      "venue" : "Lecture Notes in Mathematics",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2006
    }, {
      "title" : "A constructive definition of Dirichlet priors",
      "author" : [ "J. Sethuraman" ],
      "venue" : "Statistica Sinica,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1994
    }, {
      "title" : "Markov chain sampling methods for Dirichlet process mixture models, Journal of Computational and Graphical Statistics, 9:249–265",
      "author" : [ "R.M. Neal" ],
      "venue" : null,
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2000
    }, {
      "title" : "Modelling dyadic data with binary latent factors",
      "author" : [ "E. Meeds", "Z. Ghahramani", "R. Neal", "S. Roweis" ],
      "venue" : "In Advances in Neural Information Processing",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2007
    }, {
      "title" : "Hierarchical Dirichlet processes",
      "author" : [ "Y.W. Teh", "M.I. Jordan", "M.J. Beal", "D.M. Blei" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2006
    }, {
      "title" : "The Indian buffet process: An introduction and review",
      "author" : [ "T.L. Griffiths", "Z. Ghahramani" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2011
    }, {
      "title" : "Feature allocations, probability functions, and paintboxes",
      "author" : [ "T. Broderick", "J. Pitman", "M.I. Jordan" ],
      "venue" : "arXiv preprint arXiv:1301.6647",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2013
    }, {
      "title" : "Modelling genetic variations with fragmentationcoagulation processes",
      "author" : [ "Y.W. Teh", "C. Blundell", "L.T. Elliott" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2011
    }, {
      "title" : "Bayesian Nonparametric Models",
      "author" : [ "P. Orbanz", "Y.W. Teh" ],
      "venue" : "In Encyclopedia of Machine Learning",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2010
    }, {
      "title" : "Bayesian infinite mixture model based clustering of gene expression profiles",
      "author" : [ "M. Medvedovic", "S. Sivaganesan" ],
      "venue" : null,
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2002
    }, {
      "title" : "Bayesian mixture model based clustering of replicated microarray data",
      "author" : [ "M. Medvedovic", "K. Yeung", "R. Bumgarner" ],
      "venue" : "Bioinformatics",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2004
    }, {
      "title" : "Contextspecific infinite mixtures for clustering gene expression profiles across diverse microarray",
      "author" : [ "Liu X", "S. Sivanagesan", "K.Y. Yeung", "J. Guo", "R.E. Bumgarner", "M. Medvedovic" ],
      "venue" : null,
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2006
    }, {
      "title" : "A Mathematical Theory of Communication",
      "author" : [ "C.E. Shannon" ],
      "venue" : "Bell System Technical Journal 27(3):379–423",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 1948
    }, {
      "title" : "Entropy and inference, revisited",
      "author" : [ "I. Nemenman", "F. Shafee", "W. Bialek" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2002
    }, {
      "title" : "Bayesian Entropy Estimation for Countable Discrete Distributions",
      "author" : [ "E. Archer", "I.M. Park", "J. Pillow" ],
      "venue" : "arXiv preprint arXiv:1302.0328",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2013
    }, {
      "title" : "On Generalized Entropy and Entropic Metrics",
      "author" : [ "D. Simovici" ],
      "venue" : "Journal of Multiple Valued Logic and Soft Computing,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2007
    }, {
      "title" : "Counting distinctions: on the conceptual foundations of Shannon’s information theory",
      "author" : [ "D. Ellerman" ],
      "venue" : null,
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2009
    }, {
      "title" : "Bayesian mixture modeling, in Maximum Entropy and Bayesian Methods",
      "author" : [ "R.M. Neal" ],
      "venue" : "Proceedings of the 11th International Workshop on Maximum Entropy and Bayesian Methods of Statistical Analysis, Seattle,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 1992
    }, {
      "title" : "Cluster analysis and display of genomewide expression patterns",
      "author" : [ "M.B. Eisen", "P.T. Spellman", "P.O. Brown", "D. Botstein" ],
      "venue" : "Proceedings of the National Academy of Sciences,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 1998
    }, {
      "title" : "The use of multiple measurements in taxonomic problems",
      "author" : [ "R.A. Fisher" ],
      "venue" : "Annals of Eugenics,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 1936
    }, {
      "title" : "Integrated genomic and proteomic analyses of a systematically perturbed metabolic",
      "author" : [ "T. Ideker", "V. Thorsson", "J.A. Ranish", "R. Christmas", "J. Buhler", "J.K. Eng", "R. Bumgarner", "D.R. Goodlett", "R. Aebersold", "L. Hood" ],
      "venue" : null,
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2001
    }, {
      "title" : "The COW-2 International Organizations Dataset Version 2.0",
      "author" : [ "J.C. Pevehouse", "T. Nordstrom", "K. Warnke" ],
      "venue" : "Conflict Management and Peace Science 21(2):101-119",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2004
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "These models are based on nonparametric priors such as Dirichlet process (DP) [1, 2], its superclass Poisson-Dirichlet process (PDP) [3, 4] and constructions such as Chinese restaurant process (CRP) [5] and stick-breaking process [6] that enable formulations of efficient inference methods [7].",
      "startOffset" : 78,
      "endOffset" : 84
    }, {
      "referenceID" : 1,
      "context" : "These models are based on nonparametric priors such as Dirichlet process (DP) [1, 2], its superclass Poisson-Dirichlet process (PDP) [3, 4] and constructions such as Chinese restaurant process (CRP) [5] and stick-breaking process [6] that enable formulations of efficient inference methods [7].",
      "startOffset" : 133,
      "endOffset" : 139
    }, {
      "referenceID" : 2,
      "context" : "These models are based on nonparametric priors such as Dirichlet process (DP) [1, 2], its superclass Poisson-Dirichlet process (PDP) [3, 4] and constructions such as Chinese restaurant process (CRP) [5] and stick-breaking process [6] that enable formulations of efficient inference methods [7].",
      "startOffset" : 199,
      "endOffset" : 202
    }, {
      "referenceID" : 3,
      "context" : "These models are based on nonparametric priors such as Dirichlet process (DP) [1, 2], its superclass Poisson-Dirichlet process (PDP) [3, 4] and constructions such as Chinese restaurant process (CRP) [5] and stick-breaking process [6] that enable formulations of efficient inference methods [7].",
      "startOffset" : 230,
      "endOffset" : 233
    }, {
      "referenceID" : 4,
      "context" : "These models are based on nonparametric priors such as Dirichlet process (DP) [1, 2], its superclass Poisson-Dirichlet process (PDP) [3, 4] and constructions such as Chinese restaurant process (CRP) [5] and stick-breaking process [6] that enable formulations of efficient inference methods [7].",
      "startOffset" : 290,
      "endOffset" : 293
    }, {
      "referenceID" : 5,
      "context" : "Studies on infinite mixture models inspired the development of several other models [8, 9] including Indian buffet process (IBP) for infinite feature models [10, 11] and fragmentation-coagulation process for sequence data [12] all of which belong to Bayesian nonparametrics [13].",
      "startOffset" : 84,
      "endOffset" : 90
    }, {
      "referenceID" : 6,
      "context" : "Studies on infinite mixture models inspired the development of several other models [8, 9] including Indian buffet process (IBP) for infinite feature models [10, 11] and fragmentation-coagulation process for sequence data [12] all of which belong to Bayesian nonparametrics [13].",
      "startOffset" : 84,
      "endOffset" : 90
    }, {
      "referenceID" : 7,
      "context" : "Studies on infinite mixture models inspired the development of several other models [8, 9] including Indian buffet process (IBP) for infinite feature models [10, 11] and fragmentation-coagulation process for sequence data [12] all of which belong to Bayesian nonparametrics [13].",
      "startOffset" : 157,
      "endOffset" : 165
    }, {
      "referenceID" : 8,
      "context" : "Studies on infinite mixture models inspired the development of several other models [8, 9] including Indian buffet process (IBP) for infinite feature models [10, 11] and fragmentation-coagulation process for sequence data [12] all of which belong to Bayesian nonparametrics [13].",
      "startOffset" : 157,
      "endOffset" : 165
    }, {
      "referenceID" : 9,
      "context" : "Studies on infinite mixture models inspired the development of several other models [8, 9] including Indian buffet process (IBP) for infinite feature models [10, 11] and fragmentation-coagulation process for sequence data [12] all of which belong to Bayesian nonparametrics [13].",
      "startOffset" : 222,
      "endOffset" : 226
    }, {
      "referenceID" : 10,
      "context" : "Studies on infinite mixture models inspired the development of several other models [8, 9] including Indian buffet process (IBP) for infinite feature models [10, 11] and fragmentation-coagulation process for sequence data [12] all of which belong to Bayesian nonparametrics [13].",
      "startOffset" : 274,
      "endOffset" : 278
    }, {
      "referenceID" : 11,
      "context" : "This problem to ‘summarize’ the samples from the infinite mixture posterior was raised in bioinformatics literature in 2002 by Medvedovic and Sivaganesan for clustering gene expression profiles [14].",
      "startOffset" : 194,
      "endOffset" : 198
    }, {
      "referenceID" : 12,
      "context" : "But the question proved difficult and they ‘circumvented’ it by using a heuristic linkage algorithm based on pairwise occurence probabilities [15, 16].",
      "startOffset" : 142,
      "endOffset" : 150
    }, {
      "referenceID" : 13,
      "context" : "But the question proved difficult and they ‘circumvented’ it by using a heuristic linkage algorithm based on pairwise occurence probabilities [15, 16].",
      "startOffset" : 142,
      "endOffset" : 150
    }, {
      "referenceID" : 14,
      "context" : "showed in 2002 that the entropy [17] of a DP posterior was strongly determined by its prior hyperparameters [18].",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 15,
      "context" : "showed in 2002 that the entropy [17] of a DP posterior was strongly determined by its prior hyperparameters [18].",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 16,
      "context" : "recently elaborated these results with respect to PDP [19].",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 17,
      "context" : "In other work, entropy was generalized to partitionings by interpreting partitionings as probability distributions [20, 21].",
      "startOffset" : 115,
      "endOffset" : 123
    }, {
      "referenceID" : 18,
      "context" : "In other work, entropy was generalized to partitionings by interpreting partitionings as probability distributions [20, 21].",
      "startOffset" : 115,
      "endOffset" : 123
    }, {
      "referenceID" : 2,
      "context" : ", zn) drawn from a two-parameter CRP prior with concentration α and discount d [5].",
      "startOffset" : 79,
      "endOffset" : 82
    }, {
      "referenceID" : 19,
      "context" : "z ∼ CRP (z;α, d) θ ∼ p(θ) xi | zi, θ ∼ F (xi | θ i) (1) In the conjugate case, all θ can be integrated out to get p(zi | z−i, x) for sampling zi [22]: p(zi | z−i, x) ∝ ∫",
      "startOffset" : 145,
      "endOffset" : 149
    }, {
      "referenceID" : 8,
      "context" : "In addition, we aim to extract information from feature allocations, which constitute a superclass of partitionings [11].",
      "startOffset" : 116,
      "endOffset" : 120
    }, {
      "referenceID" : 2,
      "context" : "t=1 f(Z) ≈ 〈 f(Z) 〉π(Z) (3) Which f(Z) would be a useful statistic for Z? Three statistics commonly appear in the literature: First one is the number of blocks |Z|, which has been analyzed theoretically for various nonparametric priors [2, 5].",
      "startOffset" : 236,
      "endOffset" : 242
    }, {
      "referenceID" : 11,
      "context" : "A common statistic is pairwise occurence, which is used to extract information from infinite mixture posteriors in applications like bioinformatics [14].",
      "startOffset" : 148,
      "endOffset" : 152
    }, {
      "referenceID" : 8,
      "context" : "Another statistic is exact block size distribution (referred to as ‘multiplicities’ in [11, 19]).",
      "startOffset" : 87,
      "endOffset" : 95
    }, {
      "referenceID" : 16,
      "context" : "Another statistic is exact block size distribution (referred to as ‘multiplicities’ in [11, 19]).",
      "startOffset" : 87,
      "endOffset" : 95
    }, {
      "referenceID" : 14,
      "context" : "4 Entropy to quantify segmentation Shannon’s entropy [17] can be an appropriate quantity to measure ‘segmentation’ with respect to partitionings, which can be interpreted as probability distributions [20, 21].",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 17,
      "context" : "4 Entropy to quantify segmentation Shannon’s entropy [17] can be an appropriate quantity to measure ‘segmentation’ with respect to partitionings, which can be interpreted as probability distributions [20, 21].",
      "startOffset" : 200,
      "endOffset" : 208
    }, {
      "referenceID" : 18,
      "context" : "4 Entropy to quantify segmentation Shannon’s entropy [17] can be an appropriate quantity to measure ‘segmentation’ with respect to partitionings, which can be interpreted as probability distributions [20, 21].",
      "startOffset" : 200,
      "endOffset" : 208
    }, {
      "referenceID" : 14,
      "context" : "Total weighted information for Z gives Shannon’s entropy function [17] which can be written in terms of the cumulative statistics (assuming φn+1 = 0):",
      "startOffset" : 66,
      "endOffset" : 70
    }, {
      "referenceID" : 20,
      "context" : "Entropy Agglomeration is inspired by ‘agglomerative clustering’, a standard approach in bioinformatics [23].",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 11,
      "context" : "To summarize partitionings of gene expressions, [14] applied agglomerative clustering by pairwise occurences.",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 21,
      "context" : "2) Iris flower data (Figure 9c): This well-known dataset contains 150 points on R from three flower species [24].",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 22,
      "context" : "3) Galactose data (Figure 9d): This is a dataset of gene expressions by 820 genes in 20 experimental conditions [25].",
      "startOffset" : 112,
      "endOffset" : 116
    } ],
    "year" : 2013,
    "abstractText" : "Infinite mixture models are commonly used for clustering. One can sample from the posterior of mixture assignments by Monte Carlo methods or find its maximum a posteriori solution by optimization. However, in some problems the posterior is diffuse and it is hard to interpret the sampled partitionings. In this paper, we introduce novel statistics based on block sizes for representing sample sets of partitionings and feature allocations. We develop an element-based definition of entropy to quantify segmentation among their elements. Then we propose a simple algorithm called entropy agglomeration (EA) to summarize and visualize this information. Experiments on various infinite mixture posteriors as well as a feature allocation dataset demonstrate that the proposed statistics are useful in practice.",
    "creator" : "LaTeX with hyperref package"
  }
}