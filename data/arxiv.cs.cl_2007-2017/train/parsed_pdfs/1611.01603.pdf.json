{
  "name" : "1611.01603.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "MACHINE COMPREHENSION", "Minjoon Seo", "Aniruddha Kembhavi", "Ali Farhadi" ],
    "emails" : [ "minjoon@cs.washington.edu,", "ali@cs.washington.edu,", "hannaneh@cs.washington.edu,", "anik@allenai.org" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "The tasks of machine comprehension (MC) and question answering (QA) have gained significant popularity over the past few years within the natural language processing and computer vision communities. Systems trained end-to-end now achieve promising results on a variety of tasks in the text and image domains. The two significant contributors towards this progress are the advent of deep neural architectures and the availability of massive datasets on the order of 100,000 questions (Rajpurkar et al., 2016; Hermann et al., 2015).\nMore recently, the addition of attention mechanisms to neural architectures (e.g., in machine translation (Bahdanau et al., 2015)) have led to a significant improvement in their performance. Attention mechanisms have also been successfully extended to machine comprehension (attention between the query and the context paragraph) and visual question answering (attention between the query and image). These mechanisms (e.g., Weston et al. (2015); Antol et al. (2015); Xiong et al. (2016)) typically have one or more of the following characteristics. First, the computed attention weight vectors are often used to combine the constituents into a single vector per modality, which are then fed to the output module of the system. Second, they are usually dynamic, whereby the attention weights at the current time step are a function of the query, context and the previous attention weights. Third, they are often uni-directional, wherein the query vector attends on the set of embedded vectors representing the context (words in the language domain and image patches in the vision domain).\nIn this paper, we introduce the Bi-Directional Attention Flow (BIDAF) network that is a hierarchical multi-stage architecture for modeling representations of the context at different levels of granularity (Figure 1). BIDAF includes character-level, word-level, and phrase-level embeddings, and uses bidirectional attention flow to allow for query-aware context representation. Our attention mechanism offers following improvements to the previously popular attention paradigms. First, our attention layer is not used to summarize the two modalities into a single feature vector. Instead, attention vectors at each time step, along with embeddings from previous layers are allowed to flow through to the subsequent modelling layer. This reduces the information loss caused by early summarization. Second, we use a memory-less attention mechanism, whereby the attention at each time step is a function of only the query and context vectors and not an explicit function of the attention at the previous time step. This allows the attention layer to recover from incorrect attendances at previous\n∗The majority of the work was done while the author was interning at AI2.\nar X\niv :1\n61 1.\n01 60\n3v 1\n[ cs\n.C L\n] 5\nN ov\n2 01\n6\ntime steps, simplifies the attention layer, and our experiments show that it improves performance as compared to using dynamic attention. Finally we use a bi-directional attention mechanism from the query text to the context as well as the context back to the query. Our ablation studies show that the two attention directions provide complimentary information to the model and using both improves the results.\nOur Bi-Directional Attention Flow model1 lies at the second position of the hugely competitive Stanford QA (SQuAD) test set leader-board. The same model architecture achieves state of the art results on the CNN and Daily Mail datasets. On the CNN dataset, our single run model outperforms the previous best single run model by 2.2% and our ensemble model beats the previous best ensemble by 0.7%. When applied to the Daily Mail dataset our single run model outperforms both the previous best single run as well as ensemble models by 2.8% and 1.3% percent respectively.\nFinally, we provide an in depth ablation study of our model on the SQuAD development set, visualize the intermediate feature spaces in our model, analyse its performance as compared to a more traditional language model for machine comprehension (Rajpurkar et al., 2016) and present an analysis of the questions answered incorrectly by our model."
    }, {
      "heading" : "2 MODEL",
      "text" : "Our machine comprehension model is a hierarchical multi-stage process and consists of six layers (Figure 1):\n1. Character Embedding Layer embeds each word into a vector space using characrter level CNNs (Kim (2014))\n2. Word Embedding Layer embeds each word into a vector space 3. Phrase Embedding Layer utilizes contextual cues from surrounding words to refine the\nembedding of tokens. These first three layers are applied to both the query and context.\n4. Attention Flow Layer couples the query and context vectors and produces a set of query aware feature vectors for each step in the context\n5. Modeling Layer employs a Recurrent Neural Network to scan the context 6. Output Layer provides an answer to the query\n1Our code is public at allenai.github.io/bi-att-flow/\n1. Character Embedding Layer Character embedding layer is responsible for mapping each word to a high-dimensional vector space. Let {x1, . . .xT } and {q1, . . . qJ} represent the words in the input context paragraph and query, respectively. Following Kim (2014), we obtain the characterlevel embedding of each word using Convolutional Neural Networks (CNN). Characters are embedded into vectors, which can be considered as 1D input to the CNN, and whose size is the input channel size of the CNN. The outputs of the CNN is max-pooled over the entire width to obtain a fixed-size vector for each word.\n2. Word Embedding Layer Word embedding layer also maps each word to a high-dimensional vector space. We use pre-trained word vectors, GloVe (Pennington et al., 2014), to obtain the fixed word embedding of each word.\nThe concatenation of the character and word embedding vectors are passed to a two-layer Highway Network (Srivastava et al., 2015). The outputs of the Highway Network are two sequences of ddimensional vectors, or more conveniently, two matrices: X ∈ Rd×T for the context and Q ∈ Rd×J for the query.\n3. Phrase Embedding Layer In the phrase embedding layer, we use a Long Short-Term Memory Network (LSTM) (Hochreiter & Schmidhuber, 1997) on top of the embeddings provided by the previous layers, to model the temporal interactions between words. We place an LSTM in both directions, and concatenate the outputs of the two LSTMs. Hence we obtain H ∈ R2d×T from the context word vectors X, and U ∈ R2d×J from query word vectors Q. Note that each column vector of H and U is 2d-dimensional because of the concatenation of the outputs of the forward and backward LSTMs, each with d-dimensional output.\nIt is interesting to note that the first three layers of the model are computing features from the query and context at different levels of granularity, akin to the multi-stage feature computation interpretation of convolutional neural networks in the computer vision field.\n4. Attention Flow Layer Attention flow layer is responsible for linking and fusing information from both of the context and the query words. Unlike, previously popular attention mechanisms, the attention flow layer is not used to summarize the query and context into single feature vectors. Instead, the attention vectors at each time step, along with embeddings from previous layers are allowed to flow through to the subsequent modelling layer. This reduces the information loss caused by early summarization.\nThe inputs to the layer are phrase-level vector representations of the context H and the query U. The outputs of the layer are the query-aware vector representations of the context words, G, along with the phrase-level embeddings from the previous layer.\nIn this layer, we compute a bi-directional attention from context-to-query as well as query-tocontext. These attentions are derived from the similarity matrix, S ∈ RT×J , between the the phraselevel embeddings of the context (H) and the query (U), where Stj indicates the similarity between t-th context word and j-th query word. The similarity matrix is computed by\nStj = α(H:t,U:j) ∈ R (1)\nα is a scalar function that encodes the similarity between its two input vectors, H:t is t-th column vector of H, and U:j is j-th column vector of U, We choose α(h,u) = w>(S)[h;u;h ◦ u], where w(S) ∈ R6d is a trainable weight, ◦ is elementwise multiplication, [; ] is vector concatenation across row, and implicit multiplication is matrix multiplication.\nContext-to-query Attention. Context-to-query (C2Q) attention signifies which query words are most relevant to each context word. Let at ∈ RJ represent the attention weights on the query words by t-th context word, ∑ atj = 1 for all t. The attention weight is computed by at = softmax(St:),\nand subsequently each attended query vector is Ũ:t = ∑\nj atjU:j . Hence Ũ is a 2d-by-T matrix containing the attended query vectors for the entire context.\nQuery-to-context Attention. Query-to-context (Q2C) attention signifies which context words have the closest similarity to one of the question words and are hence critical for answering the query. We obtain the attention weights on the context words by b = softmax(maxcol(S)) ∈ RT , where the maximum function (maxcol) is performed across the column. Then the attended context vector is h̃ = ∑ t btH:t ∈ R2d. This vector indicates the weighted sum of the most important\nwords in the context with respect to the query. h̃ is tiled T times across the column, thus giving H̃ ∈ R2d×T . Finally, the phrase-level embeddings and the attention vectors are combined together to yield G, where each column vector can be considered as a query-aware representation of each context word. We define G by G:t = β(H:t, Ũ:t, H̃:t) ∈ RdG (2) where G:t is the t-th column vector (corresponding to t-th context word), β is a vector function that fuses its (three) input vectors, and dG is the output dimension of the β function. While the β function can be an arbitrary trainable neural network, such as multi-layer perceptron, we found that a simple concatenation as following still shows good performance: β(h, ũ, h̃) = [h; ũ;h◦ũ;h◦h̃] ∈ R8d×T (i.e., dG = 8d).\n5. Modeling Layer The input to the modeling layer is G, which encodes the query-aware representations of context words. The output of the layer captures the interaction among the context words conditioned on the query. This is different from the phrase embedding layer, which captures the interaction among context words independent of the query. We use two layers of bi-directional LSTM, with output size of d for each direction. Hence we obtain M ∈ R2d×T matrix, which is passed onto the output layer to predict the answer. Each column vector of M is expected to contain contextual information about the word with respect to the entire context paragraph and query.\n6. Output layer The output layer is application-specific. The modular nature of our architecture allows us to easily swap out output layers based on the task, with the rest of the architecture remaining exactly the same. Here, we describe the output layer for the QA task. In section 5, we use a slight modifications of this output layer for Cloze-style comprehension.\nThe QA task requires the model to find a sub-phrase of the paragraph to answer the query. The phrase is derived by predicting the start and the end indices of the phrase in the paragraph. We obtain the probability distribution of the start index over the entire paragraph by\np1 = softmax(w>(p1)[G;M]), (3)\nwhere w(p1) ∈ R10d is a trainable weight vector. For the end index of the answer phrase, we pass M to another bidirectional LSTM layer and obtain M2 ∈ R2d×T . Then we use M2 to obtain the probability distribution of the end index in a similar manner:\np2 = softmax(w>(p2)[G;M 2]) (4)\nTraining. We define the training loss as the sum of the log probabilities of the true start and end indices by the predicted distributions over all examples:\nL(θ) = − N∑ i log(p1y1i ) + log(p2y2i ) (5)\nwhere θ is the set of all trainable weights in the model (the weights and biases of CNN filters and LSTM cells, w(S), w(p1) and w(p2)), N is the number of examples in the dataset, y1i and y 2 i are the true start and end index of i-th example, respectively, and pk indicates k-th value of the vector p.\nTest. The answer span (k, l) with the maximum value of p1kp2l is chosen, which can be computed in linear time with dynamic programming."
    }, {
      "heading" : "3 RELATED WORK",
      "text" : "Machine comprehension A significant contributor to the advancement of MC models has been the availability of large datasets. Early datasets such as MCTest (Richardson et al., 2013) were too small to train end to end neural models. Massive cloze datasets (CNN & DailyMail by Hermann et al. (2015) and Childrens Book Test by Hill et al. (2016)), enabled the application of deep neural architectures to this task. More recently, Rajpurkar et al. (2016) released the Stanford Question Answering (SQuAD) dataset with over 100,000 questions. We evaluate the performance of our comprehension system on both SQuAD and CNN & DailyMail datasets.\nPrevious work in end-to-end machine comprehension uses attention mechanisms in three distinct ways. The first group (largely inspired by Bahdanau et al. (2015)) uses a dynamic attention mechanism, in which the attention weights are updated dynamically given the query and the context as well as the previous attention. Hermann et al. (2015) argue that the dynamic attention model performs better than using a single fixed query vector to attend on context words on CNN & DailyMail datasets. Chen et al. (2016) show that simply using bilinear term for computing the attention weights in the same model drastically improves the accuracy. Wang & Jiang (2016) reverse the direction of the attention (attending on query words as the context RNN progresses) for SQuAD. In contrast to these models, BIDAF uses a memory-less attention mechanism.\nThe second group computes the attention weights once, which are then fed into an output layer for final prediction (e.g., Kadlec et al. (2016)). Attention-over-attention model (Cui et al., 2016) uses a 2D similarity matrix between the query and context words (similar to Equation 1) to compute the weighted average of query-to-context attention. In contrast to these models, BIDAF does not summarize the two modalities in the attention layer and instead lets the attention vectors flow into the modeling layer.\nThe third group (considered as variants of Memory Network (Weston et al., 2015)) repeats computing an attention vector between the query and the context through multiple layers, typically referred to as multi-hop. (e.g., Sordoni et al. (2016); Dhingra et al. (2016)). Shen et al. (2016) combine Memory Networks with Reinforcement Learning in order to dynamically control the number of hops. One can also extend our BIDAF model to incorporate multiple hops.\nVisual question answering The task of question answering has also gained a lot of interest in the computer vision community. Early works on visual question answering (VQA) involved encoding the question using an RNN, encoding the image using a CNN and combining them to answer the question (Antol et al., 2015; Malinowski et al., 2015). Attention mechanisms have also been successfully employed for the VQA task and can be broadly clustered based on the granularity of their attention and the approach to construct the attention matrix. At the coarse level of granularity, the question attends to different patches in the image (Zhu et al., 2016; Xiong et al., 2016). At a finer level, each question word attends to each image patch and the highest attention value for each spatial location (Xu & Saenko, 2016) is adopted. A hybrid approach is to combine questions representations at multiple levels of granularity (unigrams, bigrams, trigrams) (Yang et al., 2015). Several approaches to constructing the attention matrix have been used including element-wise product, element-wise sum, concatenation and Multimodal Compact Bilinear Pooling (Fukui et al., 2016).\nLu et al. (2016) have recently shown that in addition to attending from the question to image patches, attending from the image back to the question words provides an improvement on the VQA task. This finding in the visual domain is consistent with our finding in the language domain, where our bi-directional attention between the query and context provides improved results. Their model, however, uses the attention weights directly into the output layer and does not take advantage of the attention flow to the modeling layer."
    }, {
      "heading" : "4 QUESTION ANSWERING EXPERIMENTS",
      "text" : "In this section, we evaluate our model on the task of question answering using the recently released SQuAD (Rajpurkar et al., 2016), which has gained a lot of attention over a few short months. In the next section, we evaluate our model on the task of Cloze-style reading comprehension. Dataset SQuAD is a machine comprehension dataset on a large set of Wikipedia articles, with more than 100,000 questions. The answer to each question is always a span in the context. The model is given credit if the answer matches one of the human written answers. Two metrics are used to evaluate models: Exact Match (EM) and a softer metric, F1 score, which measures the weighted average of the precision and recall rate at a character level. The dataset consists of 90k/10k train/dev question-context tuples with a large hidden test set. It is the largest available MC dataset with human written questions and serves as a great test bed for our model. Model Details The model architecture used for this task is depicted in Figure 1. Each paragraph and question are tokenized by a regular-expression-based word tokenizer (PTB Tokenizer) and fed into the model. We use 100 1D filters for CNN char embedding, each with a width of 5. The hidden state size (d) of the model is 100. We use the AdaDelta (Zeiler, 2012) optimizer, using a minibatch size of 60 and an initial learning rate of 0.5, for 10 epochs. A dropout (Srivastava et al., 2014) rate\nof 0.2 is used for the CNN, all LSTM layers, and the linear transformation before the softmax for the answers. The training process takes roughly 10 hours on a single Titan X GPU. We also train an ensemble model consisting of 10 training runs with the identical architecture and hyper-parameters. At test time, we choose the answer amongst the 10 runs, with the highest confidence score. Results The results of our model and competing approaches on the hidden test are summarized in Table 1a. BIDAF∗ achieves an EM score of 69.9 and an F1 score of 78.1. It lies at position 2 on the leaderboard amongst a pool of 12 approaches.\nAblations Table 1b shows the performance of our model and its ablations on the SQuAD dev set. Both char-level and word-level embeddings contribute towards the model’s performance. We conjecture that word-level embedding is better at representing the semantics of each word as a whole, while char-level embedding can better handle out-of-vocab (OOV) or rare words. To evaluate bidirectional attention, we remove C2Q and Q2C attentions. For ablating C2Q attention, we replace the attended question vector Ũ with the average of the output vectors of the question’s phrase embed layer (LSTM). C2Q attention proves to be critical with a drop of more than 10 points on both metrics. For ablating Q2C attention, the output of the attention layer, G, does not include terms that have the attended Q2C vectors, H̃. To evaluate the attention flow, we study a dynamic attention model, where the attention is dynamically computed within the modeling layer’s LSTM, following previous work (Bahdanau et al., 2015; Wang & Jiang, 2016). This is in contrast with our approach, where the attention is pre-computed before flowing to the modeling layer. Note that it is more fair to compare the dynamic attention model’s results to that of No Q2C attention, since Q2C attention is not implemented in the dynamic attention model. Despite being a simpler attention mechanism, our proposed static attention outperforms the dynamically computed attention by more than 3 points. We conjecture that separating out the attention layer results in a richer set of features computed in the first 4 layers which are then incorporated by the modeling layer.\nVisualizations We now provide a qualitative analysis of our model on the SQuAD dev set. First, we visualize the feature spaces after the Word and Phrase Embedding Layers. These two layers are responsible for aligning the embeddings between the query and context words which are the inputs to the subsequent Attention Layer. To visualize the embeddings, we choose a few frequent query words in the dev data and look at the context words that have the highest cosine similarity to the query words (Table 2). At the Word Layer, query words such as When, Where and Who are not well aligned to possible answers in the context, but this dramatically changes in the Phrase Layer which has access to context from surrounding words and is just 1 layer below the Attention Layer. When begins to match years, Where matches locations, and Who matches names. We also show examples where the query word matches similar entities in the Word Layer. This is expected behaviour since half the vector is composed of a GloVe embedding. Such matches are retained at the Phrase Layer.\nWe also visualize these two feature spaces using t-SNE in Figure 2. t-SNE is performed on a large fraction of dev data but we only plot data points corresponding to the months of the year. A curious pattern emerges in the Word space, where May is separated from the rest of the months because\nMay has multiple meanings in the English language. The Phrase Layer uses contextual cues from surrounding words and is able to separate the usages of the word May. Finally we visualize the attention matrices for some question-context tuples in the dev data in Figure 3. In the first example, Where matches locations and in the second example, many matches quantities and numerical symbols. Also, entities in the question typically attend to the same entity in the context, thus providing a feature for the model to localize possible answers. Discussion We analysed the performance of our our model with a more traditional language feature based baseline (Rajpurkar et al., 2016). Figure 2b shows a Venn diagram of the dev set questions correctly answered by the models. Our model is able to answer more than 86% of the questions correctly answered by the baseline. We analysed the 14% that were incorrectly answered, and found no clear pattern. This suggests that neural architectures are able to exploit much of the information captured by the language features. We also broke this comparison down by frequent first words in the questions (Figure 2c). Our model beats the baseline comfortably in all the buckets. Error Analysis We randomly selected 50 incorrect questions (based on EM) and categorized them into 6 classes. 50% of errors were due to the imprecise boundaries of the answers, 28% involved syntactic complications and ambiguities, 14% were paraphrase problems, 4% required external knowledge, 2% needed multiple sentences to answer, and 2% were due to mistakes during tokenization. See Appendix for the examples of the error modes."
    }, {
      "heading" : "5 CLOZE EXPERIMENTS",
      "text" : "We also evaluate our model on the task of Cloze-style reading comprehension using the CNN and Daily Mail datasets (Hermann et al., 2015). Dataset In a Cloze test, the reader is asked to fill in words that have been removed from a passage, for measuring one’s ability to comprehend text. Hermann et al. (2015) have recently compiled a mas-\nsive Cloze-style comprehension dataset, consisting of 300k/4k/3k and 879k/65k/53k (train/dev/test) examples from CNN and DailyMail news articles, respectively. Each example has a news article and an incomplete sentence extracted from the summary of the article. To distinguish this task from language modeling and force one to refer to the article to predict the correct missing word, the missing word is always a named entity, anonymized with random IDs. Model Details The model architecture used for this task is very similar to the one used previously with only a few small changes to adapt it to the Cloze task. Since the answer in the CNN & DailyMail datasets is always a single answer, we only need to predict the start index (p1) and the prediction for the second index (p2) is dropped from the loss function. Also, we mask out all non-entity words in the final classification layer so that they are forced to be excluded from possible answers. Another important difference from the SQuAD dataset is that the answer entity might appear more than once in the context paragraph. To address this , we follow a similar strategy from Kadlec et al. (2016). After we obtain p1 during training, we sum all probability values of entities in the context that correspond to the correct answer. Then the loss function is computed from the sum. We use a minibatch size of 30 and train for 5 epochs. Inspired by the window-based method (Hill et al., 2016), in order to speed up the training process and reduce GPU memory usage, we only consider words within 7 word-distance of any entity, which we found to be sufficient in most cases. The entire training process takes roughly 60 hours on two Titan X GPUs. The other hyper-parameters are identical to the model described in Section 4. Results The results of our single run and ensemble models and competing approaches on the CNN and DailyMail datasets are summarized in Table 3. ∗ indicates ensemble methods. We achieve the state of the arts on both datasets for both val and test data. On the CNN dataset, our single-run model outperforms all previous single-run models, and our ensemble method outperforms all previous ensemble methods. On the DailyMail dataset, our single-run model outperforms all previous models including previous ensemble methods, and ensembling provides us with an additional improvement.\nThe next best approaches for the single model and ensemble runs, GAReader (Dhingra et al., 2016) and ReasoNet (Shen et al., 2016), both compute attention dynamically over several hops of a recurrent layer. Our results provide strong evidence that our proposed attention flow is strongly competitive with dynamic attention."
    }, {
      "heading" : "6 CONCLUSION",
      "text" : "In this paper, we introduce BIDAF, a multi-stage hierarchical process that represents the context at different levels of granularity and uses a bi-directional attention flow mechanism to achieve a query-\naware context representation without early summarization. The experimental evaluations show that our model achieves the state-of-the-art results in Stanford QA (SQuAD) and CNN/DailyMail Cloze Test datasets. The ablation analyses demonstrate the importance of each component in our model. The analysis and visualizations show that our model is learning a suitable representation for MC and is capable of answering complex questions by attending to correct locations in the given paragraph. Future work will extend our approach to incorporate multiple hops."
    }, {
      "heading" : "A ERROR ANALYSIS",
      "text" : "The Table below summarizes the modes of errors by BIDAF and shows examples for each category of error."
    } ],
    "references" : [ {
      "title" : "Vqa: Visual question answering",
      "author" : [ "Stanislaw Antol", "Aishwarya Agrawal", "Jiasen Lu", "Margaret Mitchell", "Dhruv Batra", "C Lawrence Zitnick", "Devi Parikh" ],
      "venue" : "In ICCV,",
      "citeRegEx" : "Antol et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Antol et al\\.",
      "year" : 2015
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio" ],
      "venue" : null,
      "citeRegEx" : "Bahdanau et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2015
    }, {
      "title" : "A thorough examination of the cnn/daily mail reading comprehension",
      "author" : [ "Danqi Chen", "Jason Bolton", "Christopher D. Manning" ],
      "venue" : null,
      "citeRegEx" : "Chen et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2016
    }, {
      "title" : "Attention-overattention neural networks for reading comprehension",
      "author" : [ "Yiming Cui", "Zhipeng Chen", "Si Wei", "Shijin Wang", "Ting Liu", "Guoping Hu" ],
      "venue" : "arXiv preprint arXiv:1607.04423,",
      "citeRegEx" : "Cui et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Cui et al\\.",
      "year" : 2016
    }, {
      "title" : "Gated-attention readers for text comprehension",
      "author" : [ "Bhuwan Dhingra", "Hanxiao Liu", "William W Cohen", "Ruslan Salakhutdinov" ],
      "venue" : "arXiv preprint arXiv:1606.01549,",
      "citeRegEx" : "Dhingra et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Dhingra et al\\.",
      "year" : 2016
    }, {
      "title" : "Multimodal compact bilinear pooling for visual question answering and visual grounding",
      "author" : [ "Akira Fukui", "Dong Huk Park", "Daylen Yang", "Anna Rohrbach", "Trevor Darrell", "Marcus Rohrbach" ],
      "venue" : null,
      "citeRegEx" : "Fukui et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Fukui et al\\.",
      "year" : 2016
    }, {
      "title" : "Teaching machines to read and comprehend",
      "author" : [ "Karl Moritz Hermann", "Tomás Kociský", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Hermann et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Hermann et al\\.",
      "year" : 2015
    }, {
      "title" : "The goldilocks principle: Reading children’s books with explicit memory representations",
      "author" : [ "Felix Hill", "Antoine Bordes", "Sumit Chopra", "Jason Weston" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "Hill et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Hill et al\\.",
      "year" : 2016
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jurgen Schmidhuber" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? \\Q1997\\E",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Text understanding with the attention sum reader network",
      "author" : [ "Rudolf Kadlec", "Martin Schmid", "Ondrej Bajgar", "Jan Kleindienst" ],
      "venue" : "In ACL,",
      "citeRegEx" : "Kadlec et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Kadlec et al\\.",
      "year" : 2016
    }, {
      "title" : "Convolutional neural networks for sentence classification",
      "author" : [ "Yoon Kim" ],
      "venue" : "In EMNLP,",
      "citeRegEx" : "Kim.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kim.",
      "year" : 2014
    }, {
      "title" : "Dynamic entity representation with max-pooling improves machine reading",
      "author" : [ "Sosuke Kobayashi", "Ran Tian", "Naoaki Okazaki", "Kentaro Inui" ],
      "venue" : "In NAACL-HLT,",
      "citeRegEx" : "Kobayashi et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Kobayashi et al\\.",
      "year" : 2016
    }, {
      "title" : "Hierarchical question-image co-attention for visual question answering",
      "author" : [ "Jiasen Lu", "Jianwei Yang", "Dhruv Batra", "Devi Parikh" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Lu et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2016
    }, {
      "title" : "Ask your neurons: A neural-based approach to answering questions about images",
      "author" : [ "Mateusz Malinowski", "Marcus Rohrbach", "Mario Fritz" ],
      "venue" : "In ICCV,",
      "citeRegEx" : "Malinowski et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Malinowski et al\\.",
      "year" : 2015
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher D Manning" ],
      "venue" : "In EMNLP,",
      "citeRegEx" : "Pennington et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Squad: 100,000+ questions for machine comprehension of text",
      "author" : [ "Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang" ],
      "venue" : "In EMNLP,",
      "citeRegEx" : "Rajpurkar et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Rajpurkar et al\\.",
      "year" : 2016
    }, {
      "title" : "Mctest: A challenge dataset for the open-domain machine comprehension of text",
      "author" : [ "Matthew Richardson", "Christopher JC Burges", "Erin Renshaw" ],
      "venue" : "In EMNLP,",
      "citeRegEx" : "Richardson et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Richardson et al\\.",
      "year" : 2013
    }, {
      "title" : "Reasonet: Learning to stop reading in machine comprehension",
      "author" : [ "Yelong Shen", "Po-Sen Huang", "Jianfeng Gao", "Weizhu Chen" ],
      "venue" : "arXiv preprint arXiv:1609.05284,",
      "citeRegEx" : "Shen et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2016
    }, {
      "title" : "Iterative alternating neural attention for machine reading",
      "author" : [ "Alessandro Sordoni", "Phillip Bachman", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1606.02245,",
      "citeRegEx" : "Sordoni et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Sordoni et al\\.",
      "year" : 2016
    }, {
      "title" : "Dropout: a simple way to prevent neural networks from overfitting",
      "author" : [ "Nitish Srivastava", "Geoffrey E. Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov" ],
      "venue" : null,
      "citeRegEx" : "Srivastava et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Srivastava et al\\.",
      "year" : 2014
    }, {
      "title" : "Natural language comprehension with the epireader",
      "author" : [ "Adam Trischler", "Zheng Ye", "Xingdi Yuan", "Kaheer Suleman" ],
      "venue" : "In EMNLP,",
      "citeRegEx" : "Trischler et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Trischler et al\\.",
      "year" : 2016
    }, {
      "title" : "Machine comprehension using match-lstm and answer pointer",
      "author" : [ "Shuohang Wang", "Jing Jiang" ],
      "venue" : "arXiv preprint arXiv:1608.07905,",
      "citeRegEx" : "Wang and Jiang.,? \\Q2016\\E",
      "shortCiteRegEx" : "Wang and Jiang.",
      "year" : 2016
    }, {
      "title" : "Dynamic memory networks for visual and textual question answering",
      "author" : [ "Caiming Xiong", "Stephen Merity", "Richard Socher" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Xiong et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Xiong et al\\.",
      "year" : 2016
    }, {
      "title" : "Ask, attend and answer: Exploring question-guided spatial attention for visual question answering",
      "author" : [ "Huijuan Xu", "Kate Saenko" ],
      "venue" : "In ECCV,",
      "citeRegEx" : "Xu and Saenko.,? \\Q2016\\E",
      "shortCiteRegEx" : "Xu and Saenko.",
      "year" : 2016
    }, {
      "title" : "Stacked attention networks for image question answering",
      "author" : [ "Zichao Yang", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Alex Smola" ],
      "venue" : "arXiv preprint arXiv:1511.02274,",
      "citeRegEx" : "Yang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2015
    }, {
      "title" : "End-to-end reading comprehension with dynamic answer chunk ranking",
      "author" : [ "Yang Yu", "Wei Zhang", "Kazi Hasan", "Mo Yu", "Bing Xiang", "Bowen Zhou" ],
      "venue" : "arXiv preprint arXiv:1610.09996,",
      "citeRegEx" : "Yu et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2016
    }, {
      "title" : "Adadelta: an adaptive learning rate method",
      "author" : [ "Matthew D Zeiler" ],
      "venue" : "arXiv preprint arXiv:1212.5701,",
      "citeRegEx" : "Zeiler.,? \\Q2012\\E",
      "shortCiteRegEx" : "Zeiler.",
      "year" : 2012
    }, {
      "title" : "Visual7w: Grounded question answering in images",
      "author" : [ "Yuke Zhu", "Oliver Groth", "Michael S. Bernstein", "Li Fei-Fei" ],
      "venue" : null,
      "citeRegEx" : "Zhu et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 15,
      "context" : "The two significant contributors towards this progress are the advent of deep neural architectures and the availability of massive datasets on the order of 100,000 questions (Rajpurkar et al., 2016; Hermann et al., 2015).",
      "startOffset" : 174,
      "endOffset" : 220
    }, {
      "referenceID" : 6,
      "context" : "The two significant contributors towards this progress are the advent of deep neural architectures and the availability of massive datasets on the order of 100,000 questions (Rajpurkar et al., 2016; Hermann et al., 2015).",
      "startOffset" : 174,
      "endOffset" : 220
    }, {
      "referenceID" : 1,
      "context" : ", in machine translation (Bahdanau et al., 2015)) have led to a significant improvement in their performance.",
      "startOffset" : 25,
      "endOffset" : 48
    }, {
      "referenceID" : 0,
      "context" : ", in machine translation (Bahdanau et al., 2015)) have led to a significant improvement in their performance. Attention mechanisms have also been successfully extended to machine comprehension (attention between the query and the context paragraph) and visual question answering (attention between the query and image). These mechanisms (e.g., Weston et al. (2015); Antol et al.",
      "startOffset" : 26,
      "endOffset" : 365
    }, {
      "referenceID" : 0,
      "context" : "(2015); Antol et al. (2015); Xiong et al.",
      "startOffset" : 8,
      "endOffset" : 28
    }, {
      "referenceID" : 0,
      "context" : "(2015); Antol et al. (2015); Xiong et al. (2016)) typically have one or more of the following characteristics.",
      "startOffset" : 8,
      "endOffset" : 49
    }, {
      "referenceID" : 15,
      "context" : "Finally, we provide an in depth ablation study of our model on the SQuAD development set, visualize the intermediate feature spaces in our model, analyse its performance as compared to a more traditional language model for machine comprehension (Rajpurkar et al., 2016) and present an analysis of the questions answered incorrectly by our model.",
      "startOffset" : 245,
      "endOffset" : 269
    }, {
      "referenceID" : 10,
      "context" : "Character Embedding Layer embeds each word into a vector space using characrter level CNNs (Kim (2014)) 2.",
      "startOffset" : 92,
      "endOffset" : 103
    }, {
      "referenceID" : 10,
      "context" : "Following Kim (2014), we obtain the characterlevel embedding of each word using Convolutional Neural Networks (CNN).",
      "startOffset" : 10,
      "endOffset" : 21
    }, {
      "referenceID" : 14,
      "context" : "We use pre-trained word vectors, GloVe (Pennington et al., 2014), to obtain the fixed word embedding of each word.",
      "startOffset" : 39,
      "endOffset" : 64
    }, {
      "referenceID" : 16,
      "context" : "Early datasets such as MCTest (Richardson et al., 2013) were too small to train end to end neural models.",
      "startOffset" : 30,
      "endOffset" : 55
    }, {
      "referenceID" : 6,
      "context" : "Massive cloze datasets (CNN & DailyMail by Hermann et al. (2015) and Childrens Book Test by Hill et al.",
      "startOffset" : 43,
      "endOffset" : 65
    }, {
      "referenceID" : 6,
      "context" : "Massive cloze datasets (CNN & DailyMail by Hermann et al. (2015) and Childrens Book Test by Hill et al. (2016)), enabled the application of deep neural architectures to this task.",
      "startOffset" : 43,
      "endOffset" : 111
    }, {
      "referenceID" : 6,
      "context" : "Massive cloze datasets (CNN & DailyMail by Hermann et al. (2015) and Childrens Book Test by Hill et al. (2016)), enabled the application of deep neural architectures to this task. More recently, Rajpurkar et al. (2016) released the Stanford Question Answering (SQuAD) dataset with over 100,000 questions.",
      "startOffset" : 43,
      "endOffset" : 219
    }, {
      "referenceID" : 1,
      "context" : "The first group (largely inspired by Bahdanau et al. (2015)) uses a dynamic attention mechanism, in which the attention weights are updated dynamically given the query and the context as well as the previous attention.",
      "startOffset" : 37,
      "endOffset" : 60
    }, {
      "referenceID" : 1,
      "context" : "The first group (largely inspired by Bahdanau et al. (2015)) uses a dynamic attention mechanism, in which the attention weights are updated dynamically given the query and the context as well as the previous attention. Hermann et al. (2015) argue that the dynamic attention model performs better than using a single fixed query vector to attend on context words on CNN & DailyMail datasets.",
      "startOffset" : 37,
      "endOffset" : 241
    }, {
      "referenceID" : 1,
      "context" : "The first group (largely inspired by Bahdanau et al. (2015)) uses a dynamic attention mechanism, in which the attention weights are updated dynamically given the query and the context as well as the previous attention. Hermann et al. (2015) argue that the dynamic attention model performs better than using a single fixed query vector to attend on context words on CNN & DailyMail datasets. Chen et al. (2016) show that simply using bilinear term for computing the attention weights in the same model drastically improves the accuracy.",
      "startOffset" : 37,
      "endOffset" : 410
    }, {
      "referenceID" : 1,
      "context" : "The first group (largely inspired by Bahdanau et al. (2015)) uses a dynamic attention mechanism, in which the attention weights are updated dynamically given the query and the context as well as the previous attention. Hermann et al. (2015) argue that the dynamic attention model performs better than using a single fixed query vector to attend on context words on CNN & DailyMail datasets. Chen et al. (2016) show that simply using bilinear term for computing the attention weights in the same model drastically improves the accuracy. Wang & Jiang (2016) reverse the direction of the attention (attending on query words as the context RNN progresses) for SQuAD.",
      "startOffset" : 37,
      "endOffset" : 556
    }, {
      "referenceID" : 3,
      "context" : "Attention-over-attention model (Cui et al., 2016) uses a 2D similarity matrix between the query and context words (similar to Equation 1) to compute the weighted average of query-to-context attention.",
      "startOffset" : 31,
      "endOffset" : 49
    }, {
      "referenceID" : 8,
      "context" : ", Kadlec et al. (2016)).",
      "startOffset" : 2,
      "endOffset" : 23
    }, {
      "referenceID" : 0,
      "context" : "Early works on visual question answering (VQA) involved encoding the question using an RNN, encoding the image using a CNN and combining them to answer the question (Antol et al., 2015; Malinowski et al., 2015).",
      "startOffset" : 165,
      "endOffset" : 210
    }, {
      "referenceID" : 13,
      "context" : "Early works on visual question answering (VQA) involved encoding the question using an RNN, encoding the image using a CNN and combining them to answer the question (Antol et al., 2015; Malinowski et al., 2015).",
      "startOffset" : 165,
      "endOffset" : 210
    }, {
      "referenceID" : 27,
      "context" : "At the coarse level of granularity, the question attends to different patches in the image (Zhu et al., 2016; Xiong et al., 2016).",
      "startOffset" : 91,
      "endOffset" : 129
    }, {
      "referenceID" : 22,
      "context" : "At the coarse level of granularity, the question attends to different patches in the image (Zhu et al., 2016; Xiong et al., 2016).",
      "startOffset" : 91,
      "endOffset" : 129
    }, {
      "referenceID" : 24,
      "context" : "A hybrid approach is to combine questions representations at multiple levels of granularity (unigrams, bigrams, trigrams) (Yang et al., 2015).",
      "startOffset" : 122,
      "endOffset" : 141
    }, {
      "referenceID" : 5,
      "context" : "Several approaches to constructing the attention matrix have been used including element-wise product, element-wise sum, concatenation and Multimodal Compact Bilinear Pooling (Fukui et al., 2016).",
      "startOffset" : 175,
      "endOffset" : 195
    }, {
      "referenceID" : 13,
      "context" : ", Sordoni et al. (2016); Dhingra et al.",
      "startOffset" : 2,
      "endOffset" : 24
    }, {
      "referenceID" : 3,
      "context" : "(2016); Dhingra et al. (2016)).",
      "startOffset" : 8,
      "endOffset" : 30
    }, {
      "referenceID" : 3,
      "context" : "(2016); Dhingra et al. (2016)). Shen et al. (2016) combine Memory Networks with Reinforcement Learning in order to dynamically control the number of hops.",
      "startOffset" : 8,
      "endOffset" : 51
    }, {
      "referenceID" : 15,
      "context" : "In this section, we evaluate our model on the task of question answering using the recently released SQuAD (Rajpurkar et al., 2016), which has gained a lot of attention over a few short months.",
      "startOffset" : 107,
      "endOffset" : 131
    }, {
      "referenceID" : 26,
      "context" : "We use the AdaDelta (Zeiler, 2012) optimizer, using a minibatch size of 60 and an initial learning rate of 0.",
      "startOffset" : 20,
      "endOffset" : 34
    }, {
      "referenceID" : 19,
      "context" : "A dropout (Srivastava et al., 2014) rate",
      "startOffset" : 10,
      "endOffset" : 35
    }, {
      "referenceID" : 15,
      "context" : "Table 1: (1a) The performance of our model BIDAF∗ and competing approaches (Rajpurkar et al., 2016), (Wang & Jiang, 2016) and (Yu et al.",
      "startOffset" : 75,
      "endOffset" : 99
    }, {
      "referenceID" : 25,
      "context" : ", 2016), (Wang & Jiang, 2016) and (Yu et al., 2016) on the SQuAD test set.",
      "startOffset" : 34,
      "endOffset" : 51
    }, {
      "referenceID" : 1,
      "context" : "To evaluate the attention flow, we study a dynamic attention model, where the attention is dynamically computed within the modeling layer’s LSTM, following previous work (Bahdanau et al., 2015; Wang & Jiang, 2016).",
      "startOffset" : 170,
      "endOffset" : 213
    }, {
      "referenceID" : 15,
      "context" : "(b) Venn diagram of the questions answered correctly by our model and the more traditional baseline (Rajpurkar et al., 2016).",
      "startOffset" : 100,
      "endOffset" : 124
    }, {
      "referenceID" : 15,
      "context" : "Discussion We analysed the performance of our our model with a more traditional language feature based baseline (Rajpurkar et al., 2016).",
      "startOffset" : 112,
      "endOffset" : 136
    }, {
      "referenceID" : 6,
      "context" : "We also evaluate our model on the task of Cloze-style reading comprehension using the CNN and Daily Mail datasets (Hermann et al., 2015).",
      "startOffset" : 114,
      "endOffset" : 136
    }, {
      "referenceID" : 6,
      "context" : "We also evaluate our model on the task of Cloze-style reading comprehension using the CNN and Daily Mail datasets (Hermann et al., 2015). Dataset In a Cloze test, the reader is asked to fill in words that have been removed from a passage, for measuring one’s ability to comprehend text. Hermann et al. (2015) have recently compiled a mas-",
      "startOffset" : 115,
      "endOffset" : 309
    }, {
      "referenceID" : 7,
      "context" : "Inspired by the window-based method (Hill et al., 2016), in order to speed up the training process and reduce GPU memory usage, we only consider words within 7 word-distance of any entity, which we found to be sufficient in most cases.",
      "startOffset" : 36,
      "endOffset" : 55
    }, {
      "referenceID" : 8,
      "context" : "To address this , we follow a similar strategy from Kadlec et al. (2016). After we obtain p during training, we sum all probability values of entities in the context that correspond to the correct answer.",
      "startOffset" : 52,
      "endOffset" : 73
    }, {
      "referenceID" : 4,
      "context" : "The next best approaches for the single model and ensemble runs, GAReader (Dhingra et al., 2016) and ReasoNet (Shen et al.",
      "startOffset" : 74,
      "endOffset" : 96
    }, {
      "referenceID" : 17,
      "context" : ", 2016) and ReasoNet (Shen et al., 2016), both compute attention dynamically over several hops of a recurrent layer.",
      "startOffset" : 21,
      "endOffset" : 40
    }, {
      "referenceID" : 6,
      "context" : "CNN DailyMail val test val test Attentive Reader (Hermann et al., 2015) 61.",
      "startOffset" : 49,
      "endOffset" : 71
    }, {
      "referenceID" : 7,
      "context" : "0 MemNN (Hill et al., 2016) 63.",
      "startOffset" : 8,
      "endOffset" : 27
    }, {
      "referenceID" : 9,
      "context" : "8 - AS Reader (Kadlec et al., 2016) 68.",
      "startOffset" : 14,
      "endOffset" : 35
    }, {
      "referenceID" : 2,
      "context" : "9 Stanford AR (Chen et al., 2016) 68.",
      "startOffset" : 14,
      "endOffset" : 33
    }, {
      "referenceID" : 11,
      "context" : "9 DER Network (Kobayashi et al., 2016) 71.",
      "startOffset" : 14,
      "endOffset" : 38
    }, {
      "referenceID" : 18,
      "context" : "9 - Iterative Attention (Sordoni et al., 2016) 72.",
      "startOffset" : 24,
      "endOffset" : 46
    }, {
      "referenceID" : 20,
      "context" : "3 - EpiReader (Trischler et al., 2016) 73.",
      "startOffset" : 14,
      "endOffset" : 38
    }, {
      "referenceID" : 4,
      "context" : "0 - GAReader (Dhingra et al., 2016) 73.",
      "startOffset" : 13,
      "endOffset" : 35
    }, {
      "referenceID" : 3,
      "context" : "7 AoA Reader (Cui et al., 2016) 73.",
      "startOffset" : 13,
      "endOffset" : 31
    }, {
      "referenceID" : 17,
      "context" : "4 - ReasoNet (Shen et al., 2016) 72.",
      "startOffset" : 13,
      "endOffset" : 32
    }, {
      "referenceID" : 7,
      "context" : "6 MemNN∗ (Hill et al., 2016) 66.",
      "startOffset" : 9,
      "endOffset" : 28
    }, {
      "referenceID" : 9,
      "context" : "4 - ASReader∗ (Kadlec et al., 2016) 73.",
      "startOffset" : 14,
      "endOffset" : 35
    }, {
      "referenceID" : 18,
      "context" : "7 Iterative Attention∗ (Sordoni et al., 2016) 74.",
      "startOffset" : 23,
      "endOffset" : 45
    }, {
      "referenceID" : 3,
      "context" : "7 - GA Reader∗ (Cui et al., 2016) 76.",
      "startOffset" : 15,
      "endOffset" : 33
    } ],
    "year" : 2016,
    "abstractText" : "Machine Comprehension (MC), answering questions about a given context, requires modeling complex interactions between the context and the query. Recently, attention mechanisms have been successfully extended to MC. Typically these mechanisms use attention to summarize the query and context into a single vector, couple attentions temporally, and often form a uni-directional attention. In this paper we introduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stage hierarchical process that represents the context at different levels of granularity and uses a bi-directional attention flow mechanism to achieve a query-aware context representation without early summarization. Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford QA (SQuAD) and CNN/DailyMail Cloze Test datasets.",
    "creator" : "LaTeX with hyperref package"
  }
}