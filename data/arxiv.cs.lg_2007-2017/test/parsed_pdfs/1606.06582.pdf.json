{
  "name" : "1606.06582.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Augmenting Supervised Neural Networks with Unsupervised Objectives for Large-scale Image Classification",
    "authors" : [ "Yuting Zhang", "Kibok Lee", "Honglak Lee" ],
    "emails" : [ "YUTINGZH@UMICH.EDU", "KIBOK@UMICH.EDU", "HONGLAK@EECS.UMICH.EDU" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Unsupervised and supervised learning have been two associated key topics in deep learning. One important application of deep unsupervised learning over the past decade was to pretrain a deep neural network, which was then finetuned with supervised tasks (such as classification). Many deep unsupervised models were proposed, such as stacked (denoising) autoencoders (Bengio et al., 2007; Vin-\ncent et al., 2010), deep belief networks (Hinton et al., 2006; Lee et al., 2009), sparse encoder-decoders (Ranzato et al., 2007; Kavukcuoglu et al., 2010), and deep Boltzmann machines (Salakhutdinov & Hinton, 2009). These approaches significantly improved the performance of neural networks on supervised tasks when the amount of available labels were not large.\nHowever, over the past few years, supervised learning without any unsupervised pretraining has achieved even better performance, and it has become the dominating approach to train deep neural networks for real-world tasks, such as image classification (Krizhevsky et al., 2012) and object detection (Girshick et al., 2016). Purely supervised learning allowed more flexibility of network architectures, e.g., the inception unit (Szegedy et al., 2015) and the residual structure (He et al., 2016), which were not limited by the modeling assumptions of unsupervised methods. Furthermore, the recently developed batch normalization (BN) method (Ioffe & Szegedy, 2015) has made the neural network learning further easier. As a result, the once popular framework of unsupervised pretraining has become less significant and even overshadowed (LeCun et al., 2015) in the field.\nSeveral attempts (e.g., Ranzato & Szummer (2008); Larochelle & Bengio (2008); Sohn et al. (2013); Goodfellow et al. (2013)) had been made to couple the unsupervised and supervised learning in the same phase, making unsupervised objectives able to impact the network training after supervised learning took place. These methods unleashed new potential of unsupervised learning, but they have not yet been shown to scale to large amounts of labeled and unlabeled data. Rasmus et al. (2015) recently proposed an architecture that is easy to couple with a classification network by extending the stacked denoising autoencoder with lateral connections, i.e., from encoder to the same stages of the decoder, and their methods showed promising semi-supervised learning results. Nonetheless, the existing validations (Rasmus et al., 2015; Pezeshki et al., 2016) were mostly on small-scale datasets like MNIST. Recently, Zhao et al. (2015) proposed the “whatar X\niv :1\n60 6.\n06 58\n2v 1\n[ cs\n.L G\n] 2\n1 Ju\nn 20\n16\nwhere” autoencoder (SWWAE) by extending the stacked convolutional autoencoder using Zeiler et al. (2011)’s “unpooling” operator, which recovers the locational details (which was lost due to max-pooling) using the pooling switches from the encoder. While achieving promising results on the CIFAR dataset with extended unlabeled data (Torralba et al., 2008), SWWAE has not been demonstrated effective for larger-scale supervised tasks.\nIn this paper, inspired by the recent trend toward simultaneous supervised and unsupervised neural network learning, we augment challenge-winning neural networks with decoding pathways for reconstruction, demonstrating the feasibility of improving high-capacity networks for largescale image classification. Specifically, we take a segment of the classification network as the encoder and use the mirrored architecture as the decoding pathway to build several autoencoder variants. The autoencoder framework is easy to construct by augmenting an existing network without involving complicated components. Decoding pathways can be trained either separately from or together with the encoding/classification pathway by the standard stochastic gradient descent methods without special tricks, such as noise injection and activation normalization.\nThis paper first investigates reconstruction properties of the large-scale deep neural networks. Inspired by Dosovitskiy & Brox (2016), we use the auxiliary decoding pathway of the stacked autoencoder to reconstruct images from intermediate activations of the pretrained classification network. Using SWWAE, we demonstrate better image reconstruction qualities compared to the autoencoder using the unpooling operators with fixed switches, which upsamples an activation to a fixed location within the kernel. This result suggests that the intermediate (even high-level) feature representations preserve nearly all the information of the input images except for the locational details “neutralized” by max-pooling layers.\nBased on the above observations, we further improve the quality of reconstruction, an indication of the mutual information between the input and the feature representations (Vincent et al., 2010), by finetuning the entire augmented architecture with supervised and unsupervised objectives. In this setting, the image reconstruction loss can also impact the classification pathway. To the contrary of conventional beliefs in the field, we demonstrate that the unsupervised learning objective posed by the auxiliary autoencoder is an effective way to help the classification network obtain better local optimal solutions for supervised tasks. To the best of our knowledge, this work is the first to show that unsupervised objective can improve the image classification accuracy of deep convolutional neural networks on largescale datasets, such as ImageNet (Deng et al., 2009). We summarize our main contributions as follows:\n• We show that the feature representations learned by high-capacity neural networks preserve the input information extremely well, despite the spatial invariance induced by pooling. Our models can perform high-quality image reconstruction (i.e., “inversion”) from intermediate activations with the unpooling operator using the known switches from the encoder.\n• We successfully improve the large-scale image classification performance of a state-of-the-art classification network by finetuning the augmented network with a reconstructive decoding pathway to make its intermediate activations preserve the input information better.\n• We study several variants of the resultant autoencoder architecture, including instances of SWWAE and more basic versions of autoencoders, and provide insight on the importance of the pooling switches and the layer-wise reconstruction loss."
    }, {
      "heading" : "2. Related work",
      "text" : "In terms of using image reconstruction to improve classification, our work is related to supervised sparse coding and dictionary learning work, which is known to extract sparse local features from image patches by sparsityconstrained reconstruction loss functions. The extracted sparse features are then used for classification purposes. Mairal et al. (2009) proposed to combine the reconstruction loss of sparse coding and the classification loss of sparse features in a unified objective function. Yang et al. (2010) extended this supervised sparse coding with max-pooling to obtain translation-invariant local features.\nZeiler et al. (2010) proposed deconvolutional networks for unsupervised feature learning that consist of multiple layers of convolutional sparse coding with max-pooling. Each layer is trained to reconstruct the output of the previous layer. Zeiler et al. (2011) further introduced the “unpooling with switches” layer to deconvolutional networks to enable end-to-end training.\nAs an alternative to sparse coding and discriminative convolutional networks, autoencoders (Bengio, 2009) are another class of models for representation learning, in particular for the non-linear principal component analysis (Dong & McAvoy, 1996; Scholz & Vigário, 2002) by minimizing the reconstruction errors of a bottlenecked neural network. The stacked autoencoder (SAE) (Bengio et al., 2007) is amenable for hierarchical representation learning. With pooling-induced sparsity bottlenecks (Makhzani & Frey, 2015), the convolutional SAE (Masci et al., 2011) can learn features from middle-size images. In these unsupervised feature learning studies, sparsity is the key regularizer to induce meaningful features in a hierarchy.\nBy injecting noises or corruptions to the input, denoising autoencoders (Vincent et al., 2008; 2010) can learn robust filters to recover the uncorrupted input. Valpola (2015) further added noises to intermediate layers of denoising autoencoders with lateral connections, which was called “ladder network”. Rasmus et al. (2015) combined a classification task with the ladder network for semi-supervised learning, and they showed improved classification accuracy on MNIST and CIFAR-10. Here, supervision from the labeled data is the critical objective that prevents the autoencoder from learning trivial features.\nZhao et al. (2015) proposed the SWWAE, a convolutional autoencoder with unpooling layer, and combined it with classification objective for semi-supervised learning. This model integrates a discriminative convolutional network (for classification) and a deconvolutional network (for reconstruction) and can be regarded as a unification of deconvolutional networks, autoencoders and discriminative convolutional networks. They demonstrated promising results on small scale datasets such as MNIST, SVHN and STL10.\nImproving representation learning with auxiliary tasks is not new (Suddarth & Kergosien, 1990). The idea behind is that the harder the tasks are, the better representations a network can learn. As an alternative to the autoencoder, Lee et al. (2015)’s “deeply supervised network” incorporated classification objectives for intermediate layers, was able to improve the top-layer classification accuracy for reasonably large-scale networks (Wang et al., 2015). In earlier work, Ranzato & Szummer (2008) conducted layerwise training by both classification and reconstruction objectives. Recently, more task-specific unsupervised objectives for image and video representation learning were developed by using spatial context (Doersch et al., 2015) and video continuity (Wang & Gupta, 2015). In contrast, autoencoder-based methods are applicable in more general scenarios."
    }, {
      "heading" : "3. Methods",
      "text" : "In this section, we describe the training objectives and architectures of the proposed augmented network. In Section 3.1, we briefly review the architectures of recent networks for vision tasks, and present the general form of our method. In Section 3.2, we augment the classification network with auxiliary pathways composed of deconvolutional architectures to build fully mirrored autoencoders, on which we specify the auxiliary objective functions."
    }, {
      "heading" : "3.1. Unsupervised loss for intermediate representations",
      "text" : "Deep neural networks trained with full supervision achieved the state-of-the-art image classification performance. Commonly used network architectures\n(Krizhevsky et al., 2012) contain a single pathway of convolutional layers succeeded by nonlinear activation functions and interleaved with max-pooling layers to gradually transform features into high-level representations and gain spatial invariance at different scales. Recent networks (Simonyan & Zisserman, 2015; Szegedy et al., 2015; He et al., 2016; Szegedy et al., 2016) often nest a group of convolutional layers before applying a max-pooling layer. As these layers work together as the feature extractor for a particular scale, we refer to the group as a macro-layer (see the left half of Figure 1). Fully-connected innerproduct layer and/or global average-pooling layer follow the convolution-pooling macro-layers to feed the top-layer classifier. A network of L convolution-pooling macrolayers is defined as\nal = fl(al−1;φl), for l = 1, 2, . . . , L+ 1, (1)\nwhere a0 = x is the input, fl(l = 1, 2, . . . , L) with the parameter φl is the lth macro-layer, and fL+1 denotes the rest of the network, including the inner-product and classification layers. The classification loss isC(x, y) = `(aL+1, y), where y is the ground truth label, and ` is the cross-entropy loss when using a softmax classifier.\nLet x1, x2, . . . , xN denote a set of training images associated with categorical labels y1, y2, . . . , yN . The neural network is trained by minimizing 1N ∑N i=1 C(xi, yi), where we omit the L2-regularization term on the parameters. Though this objective can effectively learn a largescale network by gradient descent with a huge amount of labeled data, it has two limitations. On the one hand, the training of lower intermediate layers might be problematic, because the gradient signals from the top layer can become vanished (Hochreiter et al., 2001) on its way to the bottom layer. Regularization by normalization (Ioffe & Szegedy, 2015) can alleviate this problem, but will also lead to large yet noisy gradients when networks are deep (He et al., 2016). On the other hand, the data space is infor-\ntoencoders. : nodes; : encoder macro-layer; : decoder macro-layer; : inner-product layer; : reconstruction loss; : classification loss.\nmative by itself, but the fully supervised objective guides the representation learning purely by the labels.\nA solution to both problems is to incorporate auxiliary unsupervised training objectives to the intermediate layers. More specifically, the objective function becomes\n1\nN N∑ i=1 (C(xi, yi) + λU(xi)) , (2)\nwhere U(·) is the unsupervised objective function associating with one or more auxiliary pathways that are attached to the convolution-pooling macro-layers in the original classification network."
    }, {
      "heading" : "3.2. Network augmentation with autoencoders",
      "text" : "Given the network architecture for classification defined in Eq. (1), we take the sub-network composed of all the convolution-pooling macro-layers as the encoding pathway, and generate a fully mirrored decoder network as an auxiliary pathway of the original network. The innerproduct layers close to the top-level classifier may be excluded from the autoencoder, since they are supposed to be more task-relevant.\nTaking a network of five macro-layers as an example (e.g., VGGNet), Figure 2a shows the network augmented with a stacked autoencoder. The decoding starts from the pooled\nnodes; : noisy nodes; : encoder macro-layer; : decoder macro-layer; : inner-product layer; : reconstruction loss; : classification loss; : parameter tying.\nfeature map from the 5th macro-layer (pool5) all the way down to the image input. Reconstruction errors are measured at the network input (i.e., the first layer) so that we term the model as “SAE-first”. More specifically, the decoding pathway is\nâL = aL, âl−1 = f dec l (âl;ψl), x̂ = â0. (3)\nwith the loss USAE-first(x) = ‖x̂ − x‖22. Here, ψl’s are decoder parameters.\nThe auxiliary training signals of SAE-first emerge from the bottom of the decoding pathway, and they get merged with the top-down signals for classification at the last convolution-pooling macro-layer into the encoder pathway. To allow more gradient to flow directly into the preceding macro-layers, we propose the “SAE-all” model by replacing the unsupervised loss by USAE-all(x) = ∑L−1 l=0 γl‖âl − al‖22 , which makes the autoencoder have an even better mirrored architecture by matching activations for all the macro-layer (illustrated in Figure 2b).\nIn Figure 2c, we propose one more autoencoder variant with layer-wise decoding architecture, termed “SAElayerwise”. It reconstructs the output activations of every macro-layer to its input. The auxiliary loss of SAElayerwise is the same as SAE-all, i.e., USAE-layerwise(x) = USAE-all(x), but the decoding pathway is replaced by âl−1 = f dec l (al;ψl).\nSAE-first/all encourages top-level convolution features to preserve as much information as possible. In contrast, the auxiliary pathways in SAE-layerwise focus on inverting the clean intermediate activations (from the encoder) to the input of the associated macro-layer, admitting parallel layerwise training. We investigated both in Section 4.3 and take SAE-layerwise decoders as architectures for efficient pretraining.\nIn Figure 1, we illustrate the detailed architecture of f3(·) and fdec3 (·) for Simonyan & Zisserman (2015)’s 16-layer VGGNet. Inspired by Zeiler et al. (2011), we use Zhao\net al. (2015)’s SWWAE as the default for the microarchitecture. More specifically, we record the pooling switches (i.e., the locations of the local maxima) in the encoder, and unpool activations by putting the elements at the recorded locations and filling the blanks with zeros. Unpooling with known switches can recover the local spatial variance eliminated by the max-pooling layer, avoiding the auxiliary objectives from deteriorating the spatial invariance of the encoder filters, which is arguably important for classification. We studied the autoencoders with fixed and known unpooling switch, respectively. In Section 4.2 we efficiently trained the autoencoders augmented from a pretrained deep non-BN network, where the decoder is hard to learn from scratch.\nRasmus et al. (2015)’s ladder network (Figure 3) is a more sophisticated way to augment existing sequential architectures with autoencoders. It is featured by the lateral connections (vertical in Figure 3) and the combinator functions that merge the lateral and top-down activations. Due to the lateral connections, noise must be added to the encoder; otherwise, the combinator function can trivially copy the clean activations from the encoder. In contrast, no autoencoder variant used in our work has “lateral\" connections, which makes the overall architectures of our models simpler and more standard. In SWWAE, the pooling switch connections do not bring the encoder input directly to the decoder, so they cannot be taken as the lateral connections like in the “ladder network”. Moreover, noise injection is also unnecessary for our models. We leave it as an open question whether denoising objectives can help with the augmented (what-where) autoencoder for large-scale data."
    }, {
      "heading" : "4. Experiments",
      "text" : "In this section, we evaluated different variants of the augmented network for image reconstruction and classification on ImageNet ILSVRC 2012 dataset, using the training set for training, and validation set for evaluation. Our experiments were mainly based on the 16-layer VGGNet (Simonyan & Zisserman, 2015).1 To compare with existing methods on inverting neural networks (Dosovitskiy & Brox, 2016), we also partially used Krizhevsky et al. (2012)’s network, termed AlexNet, trained on ILSVRC2012 training set. Our code and trained models can be obtained at http://www.ytzhang.net/ software/recon-dec/"
    }, {
      "heading" : "4.1. Training procedure",
      "text" : "Training a deep neural network is non-trivial. Therefore, we propose the following strategy to make the networks\n1The pretrained network was obtained from http://www. robots.ox.ac.uk/~vgg/research/very_deep/.\naugmented from the classification network efficiently trainable.\n1. We initialized the encoding pathway with the pretrained classification network, and the decoding pathways with Gaussian random initialization.\n2. For any variant of the augmented network, we fixed the parameters for the classification pathway and trained the layer-wise decoding pathways of the SAElayerwise network.\n3. For SAE-first/all, we initialized the decoding pathway with the pretrained SAE-layerwise parameters and finetuned the decoder. (Skip this step for SAElayerwise.)\n4. We finetuned all the decoding and the encoding/classification pathways together with a reduced learning rate.\nUp to Step 3, we trained the decoding pathways with the classification pathway fixed. For all the four steps, we trained the networks by mini-batch stochastic gradient descent (SGD) with the momentum 0.9.\nIn Step 2, the SAE-layerwise model has separate subpathways for decoding, so the training can be done in parallel for every macro-layer. The decoding sub-network for each macro-layer was relatively “shallow” so that it is easy to learn. We found the learning rate annealing not critical for SAE-layerwise pretraining. Proper base learning rates could make it sufficiently converged within 1 epoch. The chosen layer-wise learning rates VGGNet were summarized in Appendix A1 (Table A-1). We used a small mini-batch size of 16 for SGD.\nFor very deep networks, training the decoding pathways of SAE-first/all from random initialization is difficult when batch normalization is absent (e.g., in the VGGNet). Initializing with SAE-layerwise as in Step 3 is critical to efficiently train the stacked decoding pathways of SAE-first and SAE-all.\nFor SAE-all (Step 3, 4) and SAE-layerwise (Step 4), we balanced the reconstruction loss among different macrolayer, where the criterion was to make the weighted loss for every layer comparable to each other. We summarized the balancing weights for VGGNet in Appendix A1 (Table A1). The SGD mini-batch size was set to a larger value (here, 64) in Step 4 for better stability.\nWe adopted commonly used data augmentation schemes. As to VGGNet, we randomly resized the image to [256, 512] pixels with respect to the shorter edge, and then randomly cropped a 224 × 224 patch (or its horizontally mirrored image) to feed into the network. As to AlexNet,\nwe followed Krizhevsky et al. (2012)’s data augmentation scheme, cropping an image at the center to make it square with the shorter edge unchanged, resizing the square to 256×256, and randomly sampling a 227×227 patch or its horizontally mirrored counterpart to feed the network. We ignored the RGB color jittering so as to always take ground truth natural images as the reconstruction targets.\nOur implementation was based on the Caffe framework (Jia et al., 2014)."
    }, {
      "heading" : "4.2. Image reconstruction via decoding pathways",
      "text" : "Using reconstructive decoding pathways, we can visualize the learned hierarchical features by inverting a given classification network, which is a useful way to understand the learned representations. The idea of reconstructing the encoder input from its intermediate activations was first explored by Dosovitskiy & Brox (2016), in contrast to visualizing a single hidden node (Zeiler & Fergus, 2014) and dreaming out images (Mahendran & Vedaldi, 2015). As the best existing method for inverting neural networks with no skip link, it used unpooling with fixed switches to upsample the intermediate activation maps. This method demonstrated how much information the features produced by each layer could preserve for the input. As shown in Figure 4 (the top row), not surprisingly, the details of the input image gradually diminished as the representations went through higher layers.\nThe commonly used classification network mainly consists of convolution/inner-product and max-pooling operators. Based only on Dosovitskiy & Brox (2016)’s visualization, it is hard to tell how much the two types of operators contribute to the diminishing of image details, respectively. Note that our SAE-first architecture is comparable to Dosovitskiy & Brox (2016)’s model except for the better mirrored architectures between the encoder and decoder, which allow extending to SWWAE. Using the SWWAEfirst network (“what-where” version of SAE-first), we were able to revert the max-pooling more faithfully, and to study the amount of information that the convolutional filters and inner-product coefficients preserved.\nTo compare with Dosovitskiy & Brox (2016), we augmented AlexNet to the corresponding SWWAE-first architecture.2 Unlike in Section 3, we built SWWAE-first network starting from every layer, i.e., decoding pathway could start from conv1 to fc8. Each macro-layer in AlexNet included exactly one convolutional or innerproduct layer. We trained the decoding pathway with the encoding/classification pathway fixed.\nAs shown in Figure 4, the images reconstructed from any\n2The decoding pathway almost fully mirrored the classification network except the first layer (conv1). This convolutional layer used the stride 4 rather than 1, which approximates two additional 2× 2 pooling layers. Therefore, we used three deconvolutional layers to inverse the conv1 layer.\nlayer, even including the top 1000-way classification layer, were almost visually perfect.3 Only the local contrast and color saturation became slightly different from the original images as the layer went higher. The surprisingly good reconstruction quality suggests that the features produced by AlexNet preserved nearly all the information of the input except for the spatial invariance gained by the max-pooling layers.\nAs commonly believed, learning task-relevant features for classification and preserving information were conflicting to some extent, since the “nuisance” should be removed for supervised tasks. According to our experiments, the locational details in different scales were almost the only information significantly neutralized by the deep neural network. For the convolutional and inner-product layers, it seems important to encode the input into a better (e.g., taskrelevant) form without information loss.\nWe conducted similar experiments based on the 16-layer VGGNet. As no results using the unpooling with fixed switches had been reported yet, we trained the decoding pathways for both SAE-first (with fixed unpooling switches) and SWWAE-first (with known unpooling switches). We described the detailed training strategy in Section 4.3. In Figure 5, we showed the reconstruction examples up to the 5th macro-layer (the 13th layer). Images reconstructed by SAE-first were blurry for higher layers. In contrast, SWWAE-first could well recover the shape details from the pool5 features. In addition, the SWWAE-first model could also reasonably reconstruct non-ImageNet and even non-natural images like text screenshots, depth maps, and cartoon pictures, as shown in Appendix A2.5 (Figure A-3). These results suggest that the high-level feature representations were also adaptable to other domains.\nSince the architecture was much deeper than AlexNet, VGGNet resulted in noisier reconstruction. Assuming the ability of preserving information as a helpful property for deep neural network, we took the reconstruction loss as an auxiliary objective function for training the classification network, as will be described in Section 4.3."
    }, {
      "heading" : "4.3. Image classification with augmented architectures",
      "text" : "We took as the baseline the 16-layer VGGNet (Simonyan & Zisserman (2015)’s Model D), one of the best open source convolutional neural networks for large-scale image classification.\nWe needed only to use the classification pathway for testing. We report results with the following two schemes for sampling patches to show both more ablative and more\n3For the fc6 and fc7 layers, we applied inner-product followed by relu nonlinearity; for the fc8 layer, we applied only inner-product, but not softmax nonlinearity.\npractical performance on single networks.\nSingle-crop We resized the test image, making its shorter edge 256 pixels, and used only the single 224 × 224 patch (without mirroring) at the center to compute the classification score. It allowed us to examine the tradeoff between training and validation performance without complicated post-processing.\nConvolution We took the VGGNet as a fully convolutional network and used a global average-pooling to fuse the classification scores obtained at different locations in the grid. The test image was resized to 256 pixels for the shorter edge and mirrored to go through the convolution twice. It was a replication of Section 3.2 of (Simonyan & Zisserman, 2015).\nWe report the experimental results in Table 1. Several VGGNet (classification pathway only) results are presented to justify the validity of our baseline implementation. As a replication of Simonyan & Zisserman (2015)’s “singlescale” method, our second post-processing scheme could achieve similar comparable accuracy. Moreover, finetuning the pretrained VGGNet model further without the augmented decoding network using the same training procedure did not lead to significant performance change.\nAs a general trend, all of the networks augmented with autoencoders outperformed the baseline VGGNet by a noticeable margin. In particular, compared to the VGGNet baseline, the SWWAE-all model reduced the top-1 errors by 1.66% and 1.18% for the single-crop and convolution schemes, respectively. It also reduced the top-5 errors by 1.01% and 0.81%, which are 10% and 9% relative to the baseline errors.\nTo the best of our knowledge, this work provides the first experimental results to demonstrate the effectiveness of unsupervised learning objectives for improving the state-ofthe-art image classification performance on large-scale realistic datasets. For SWWAE-all, the validation accuracy in Table 1 was achieved in ∼16 epochs, which took 4~5 days on a workstation with 4 Nvidia Titan X GPUs. Taking pretrained VGGNet as the reference, 75% of the relative accuracy improvement (∼1.25% absolute top-1 accuracy improvement) could be achieved in ∼4 epochs (∼1 day).\nApart from the general performance gain due to reconstructive decoding pathways, the architecture changes could result in relatively small differences. Compared to SWWAElayerwise, SWWAE-all led to slightly higher accuracy, suggesting the usefulness of posing a higher requirement on the top convolutional features for preserving the input information. The slight performance gain of SWWAEall over SAE-all with fixed unpooling switches indicates that the switch connections could alleviate the difficulty\nof learning a stacked convolutional autoencoder. In the meanwhile, it also suggests that, without pooling switches, the decoding pathway can benefit the classification network learning similarly. Using the unpooling with fixed switches, the decoding pathway may not be limited for reconstruction, but can also be designed for the structured outputs that are not locationally aligned with the input images (e.g, adjacent frames in videos, another viewpoint of the input object).\nTo figure out whether the performance gain was due to the potential regularization effects of the decoding pathway or not, we evaluated the networks on 50,000 images randomly chosen from the training set. Interestingly, the networks augmented with autoencoders achieved lower training errors than the baseline VGGNet. Hence, rather than regularizing, it is more likely that the auxiliary unsupervised loss helped the CNN to find better local optima in supervised learning. Compared to SAE/SWWAE-all, SAE/SWWAEfirst led to lower training errors but higher validation errors, a typical symptom of slight overfitting. Thus, incorporating layer-wise reconstruction loss was an effective way to regularize the network training.\nWe provide more discussion for the decoding pathways in Appendix A2, including image reconstruction results after finetuning the augmented networks (Appendix A2.5), training curves (Appendix A2.2), and comparison between the pretrained and finetuned convolution filters (Appendix A2.1).\n4In our experiments, the 16-layer VGGNet (Simonyan & Zisserman (2015)’s Model D) achieved 10.07% for the single-crop scheme and 8.94% for the convolution scheme (in a single scale), which is comparable to 8.8% in Table 3 of (Simonyan & Zisserman, 2015). In that table, the best reported number for the Model D was 8.1%, but it is trained and tested using a different resizing and cropping method, thus not comparable to our results."
    }, {
      "heading" : "5. Conclusion",
      "text" : "We proposed a simple and effective way to incorporate unsupervised objectives into large-scale classification network learning by augmenting the existing network with reconstructive decoding pathways. Using the resultant autoencoder for image reconstruction, we demonstrated the ability of preserving input information by intermediate representation as an important property of modern deep neural networks trained for large-scale image classification. We leveraged this property further by training the augmented network composed of both the classification and decoding pathways. This method improved the performance of the 16-layer VGGNet, one of the best existing networks for image classification by a noticeable margin. We investigated different variants of the autoencoder, and showed that 1) the pooling switch connections between the encoding and decoding pathways were helpful, but not critical for improving the performance of the classification network in largescale settings; 2) the decoding pathways mainly helped the supervised objective reach a better optimum; and 3) the layer-wise reconstruction loss could effectively regularize the solution to the joint objective. We hope this paper will inspire further investigations on the use of unsupervised learning in a large-scale setting."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This work was funded by Software R&D Center, Samsung Electronics Co., Ltd; ONR N00014-13-1-0762; and NSF CAREER IIS-1453651. We also thank NVIDIA for donating K40c and TITAN X GPUs. We thank Jimei Yang, Seunghoon Hong, Ruben Villegas, Wenling Shang, Kihyuk Sohn, and other collaborators for helpful discussions."
    }, {
      "heading" : "A1. Parameters for VGGNet-based models",
      "text" : "We report the learning parameters for 16-layer VGGNet-based model in Table A-1. We chose the learning rates that lead to the largest decrease in the reconstruction loss in the first 2000 iterations for each layer. The “loss weighting” are balancing factors for reconstruction losses in different layers varied to make them comparable in magnitude. In particular, we computed image reconstruction loss against RGB values normalized to [0,1], which are different in scale from intermediate features. We also did not normalize the reconstruction loss with feature dimensions for any layer."
    }, {
      "heading" : "A2. More experimental results and discussions",
      "text" : "A2.1. Learned filters\nCompared to the baseline VGGNet, the finetuned SWWAE-all model demonstrated ∼ 35% element-wise relative change of the filter weights on average for all the layers. A small portion of the filters showed stronger contrast after finetuning. Qualitatively, the finetuned filters kept the pretrained visual shapes. In Figure A-1, we visualize the first-layer 3 × 3 convolution filters.\nA2.2. Training curve\nIn Figure A-2, we report the training curves of validation accuracy for SWWAE-all, where the pretrained VGGNet classification network and decoder network were taken as the starting point.\nTop-1 Validation Accuracy\nTop-5 Validation Accuracy\nA2.3. Selection of different model variants\nThe performance for different variants of the augmented network are comparable, but we can still choose the best available one. In particular, we provide following discussions.\n• Since the computational costs were similar for training and the same for testing, we can use the best available architecture depending on tasks. For example, when using decoding pathways for spatially corresponded tasks like reconstruction (as in our paper) and segmentation, we can use the SWWAE. For more general objectives like predicting next frames, where pooling switches are non-transferrable, we can still use ordinary SAEs to get competitive performance.\n• S(WW)AE-first has less hyper-parameters than S(WW)AE-all, and can be trained first for quick parameter search. It can be switched to *-all for better performance.\nA2.4. Ladder networks\nWe tried training a ladder network following the same procedures of pretraining auxiliary pathways and finetuning the whole network as for our models, which is also similar to Rasmus et al. (2015)’s strategy. We used the augmented multilayer perceptron (AMLP) combinator, which Pezeshki et al. (2016) proposed as the best combinator function. Different from the previous work conducted on the variants of MNIST dataset, the pretrained VGGNet does not have batch normalization (BN) layers, which pushed us to remove the BN layers from the ladder network. However, BN turned out to be critical for proper noise injection, and the non-BN ladder network did not perform well. It might suggest that our models are easier to pair with a standard convolutional network and train on large-scale datasets.\nA2.5. Image reconstruction\nIn Figure A-3, we visualize the images reconstructed by the pretrained decoder of SWWAE-first and the final models for SWWAE-first/all, and reported the L2 reconstruction loss on the validation set. Finetuning the entire networks also resulted in better reconstruction quality, which is consistent with our assumption that enhancing the ability of preserving input information can lead to better features for image classification. Since the shape details had already been well recovered by the pretrained decoder, the finetuned SWWAE-first/all mainly improved the accuracy of colors. Note that the decoder learning is more difficult for SWWAE-all than SWWAE-first, which explains its slightly higher reconstruction loss and better regularization ability.\nIn Figure A-4 and A-5, we showed more examples for reconstructing input images from pretrained neural network features for AlexNet and VGGNet.\nLayer image pool1 pool2 pool3 pool4 pool5\nSAE-first (fixed unpooling switches)\nSWWAE-first (known unpooling\nswitches)\nSAE-first (fixed unpooling switches)\nSWWAE-first (known unpooling\nswitches)\nSAE-first (fixed unpooling switches)\nSWWAE-first (known unpooling\nswitches)\nSAE-first (fixed unpooling switches)\nSWWAE-first (known unpooling\nswitches)\nSAE-first (fixed unpooling switches)\nSWWAE-first (known unpooling\nswitches)\nLayer image pool1 pool2 pool3 pool4 pool5\nSAE-first (fixed unpooling switches)\nSWWAE-first (known unpooling\nswitches)\nSAE-first (fixed unpooling switches)\nSWWAE-first (known unpooling\nswitches)\nSAE-first (fixed unpooling switches)\nSWWAE-first (known unpooling\nswitches)\nSAE-first (fixed unpooling switches)\nSWWAE-first (known unpooling\nswitches)\nSAE-first (fixed unpooling switches)\nSWWAE-first (known unpooling\nswitches)"
    } ],
    "references" : [ {
      "title" : "Learning deep architectures for ai",
      "author" : [ "Y. Bengio" ],
      "venue" : "Foundation and Trends in Machine Learning,",
      "citeRegEx" : "Bengio,? \\Q2009\\E",
      "shortCiteRegEx" : "Bengio",
      "year" : 2009
    }, {
      "title" : "Greedy layer-wise training of deep networks",
      "author" : [ "Y. Bengio", "P. Lamblin", "D. Popovici", "H. Larochelle" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Bengio et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2007
    }, {
      "title" : "Imagenet: A large-scale hierarchical image database",
      "author" : [ "J. Deng", "W. Dong", "R. Socher", "Li", "L.-J", "K. Li", "L. Fei-Fei" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "Deng et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Deng et al\\.",
      "year" : 2009
    }, {
      "title" : "Unsupervised visual representation learning by context prediction",
      "author" : [ "C. Doersch", "A. Gupta", "A.A. Efros" ],
      "venue" : "In ICCV,",
      "citeRegEx" : "Doersch et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Doersch et al\\.",
      "year" : 2015
    }, {
      "title" : "Nonlinear principal component analysis based on principal curves and neural networks",
      "author" : [ "D. Dong", "T.J. McAvoy" ],
      "venue" : "Computers & Chemical Engineering,",
      "citeRegEx" : "Dong and McAvoy,? \\Q1996\\E",
      "shortCiteRegEx" : "Dong and McAvoy",
      "year" : 1996
    }, {
      "title" : "Inverting visual representations with convolutional networks",
      "author" : [ "A. Dosovitskiy", "T. Brox" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "Dosovitskiy and Brox,? \\Q2016\\E",
      "shortCiteRegEx" : "Dosovitskiy and Brox",
      "year" : 2016
    }, {
      "title" : "Region-based convolutional networks for accurate object detection and segmentation",
      "author" : [ "R. Girshick", "J. Donahue", "T. Darrell", "J. Malik" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "Girshick et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Girshick et al\\.",
      "year" : 2016
    }, {
      "title" : "Multi-prediction deep boltzmann machines",
      "author" : [ "I. Goodfellow", "M. Mirza", "A. Courville", "Y. Bengio" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Goodfellow et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Goodfellow et al\\.",
      "year" : 2013
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "K. He", "X. Zhang", "S. Ren", "J. Sun" ],
      "venue" : null,
      "citeRegEx" : "He et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2016
    }, {
      "title" : "A fast learning algorithm for deep belief nets",
      "author" : [ "G.E. Hinton", "S. Osindero", "Teh", "Y.-W" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "Hinton et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2006
    }, {
      "title" : "Gradient flow in recurrent nets: the difficulty of learning long-term dependencies",
      "author" : [ "S. Hochreiter", "Y. Bengio", "P. Frasconi", "J. Schmidhuber" ],
      "venue" : "In A Field Guide to Dynamical Recurrent Networks",
      "citeRegEx" : "Hochreiter et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Hochreiter et al\\.",
      "year" : 2001
    }, {
      "title" : "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "author" : [ "S. Ioffe", "C. Szegedy" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Ioffe and Szegedy,? \\Q2015\\E",
      "shortCiteRegEx" : "Ioffe and Szegedy",
      "year" : 2015
    }, {
      "title" : "Caffe: Convolutional architecture for fast feature embedding",
      "author" : [ "Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell" ],
      "venue" : null,
      "citeRegEx" : "Jia et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Jia et al\\.",
      "year" : 2014
    }, {
      "title" : "Fast inference in sparse coding algorithms with applications to object recognition",
      "author" : [ "K. Kavukcuoglu", "M.A. Ranzato", "Y. LeCun" ],
      "venue" : null,
      "citeRegEx" : "Kavukcuoglu et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Kavukcuoglu et al\\.",
      "year" : 2010
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "A. Krizhevsky", "I. Sutskever", "G.E. Hinton" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Krizhevsky et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Krizhevsky et al\\.",
      "year" : 2012
    }, {
      "title" : "Classification using discriminative restricted boltzmann machines",
      "author" : [ "H. Larochelle", "Y. Bengio" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Larochelle and Bengio,? \\Q2008\\E",
      "shortCiteRegEx" : "Larochelle and Bengio",
      "year" : 2008
    }, {
      "title" : "Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations",
      "author" : [ "H. Lee", "R. Grosse", "R. Ranganath", "A.Y. Ng" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Lee et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2009
    }, {
      "title" : "Understanding deep image representations by inverting them",
      "author" : [ "A. Mahendran", "A. Vedaldi" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "Mahendran and Vedaldi,? \\Q2015\\E",
      "shortCiteRegEx" : "Mahendran and Vedaldi",
      "year" : 2015
    }, {
      "title" : "Supervised dictionary learning",
      "author" : [ "J. Mairal", "J. Ponce", "G. Sapiro", "A. Zisserman", "F.R. Bach" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Mairal et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Mairal et al\\.",
      "year" : 2009
    }, {
      "title" : "Winner-take-all autoencoders",
      "author" : [ "A. Makhzani", "B.J. Frey" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Makhzani and Frey,? \\Q2015\\E",
      "shortCiteRegEx" : "Makhzani and Frey",
      "year" : 2015
    }, {
      "title" : "Stacked convolutional auto-encoders for hierarchical feature extraction",
      "author" : [ "J. Masci", "U. Meier", "D. Cireşan", "J. Schmidhuber" ],
      "venue" : "In International Conference on Artificial Neural Networks,",
      "citeRegEx" : "Masci et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Masci et al\\.",
      "year" : 2011
    }, {
      "title" : "Semi-supervised learning of compact document representations with deep networks",
      "author" : [ "M.A. Ranzato", "M. Szummer" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Ranzato and Szummer,? \\Q2008\\E",
      "shortCiteRegEx" : "Ranzato and Szummer",
      "year" : 2008
    }, {
      "title" : "Unsupervised learning of invariant feature hierarchies with applications to object recognition",
      "author" : [ "M.A. Ranzato", "F.J. Huang", "Boureau", "Y.-L", "Y. LeCun" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "Ranzato et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Ranzato et al\\.",
      "year" : 2007
    }, {
      "title" : "Semi-supervised learning with ladder network",
      "author" : [ "A. Rasmus", "H. Valpola", "M. Honkala", "M. Berglund", "T. Raiko" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Rasmus et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Rasmus et al\\.",
      "year" : 2015
    }, {
      "title" : "Deep boltzmann machines",
      "author" : [ "R. Salakhutdinov", "G.E. Hinton" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "Salakhutdinov and Hinton,? \\Q2009\\E",
      "shortCiteRegEx" : "Salakhutdinov and Hinton",
      "year" : 2009
    }, {
      "title" : "Nonlinear pca: a new hierarchical approach",
      "author" : [ "M. Scholz", "R. Vigário" ],
      "venue" : "In ESANN,",
      "citeRegEx" : "Scholz and Vigário,? \\Q2002\\E",
      "shortCiteRegEx" : "Scholz and Vigário",
      "year" : 2002
    }, {
      "title" : "Very deep convolutional networks for large-scale image recognition",
      "author" : [ "K. Simonyan", "A. Zisserman" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "Simonyan and Zisserman,? \\Q2015\\E",
      "shortCiteRegEx" : "Simonyan and Zisserman",
      "year" : 2015
    }, {
      "title" : "Learning and selecting features jointly with point-wise gated Boltzmann machines",
      "author" : [ "K. Sohn", "G. Zhou", "C. Lee", "H. Lee" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Sohn et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Sohn et al\\.",
      "year" : 2013
    }, {
      "title" : "Rule-injection hints as a means of improving network performance and learning time",
      "author" : [ "S. Suddarth", "Y. Kergosien" ],
      "venue" : "Neural Networks,",
      "citeRegEx" : "Suddarth and Kergosien,? \\Q1990\\E",
      "shortCiteRegEx" : "Suddarth and Kergosien",
      "year" : 1990
    }, {
      "title" : "Rethinking the inception architecture for computer",
      "author" : [ "C. Szegedy", "V. Vanhoucke", "S. Ioffe", "J. Shlens", "Z. Wojna" ],
      "venue" : null,
      "citeRegEx" : "Szegedy et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Szegedy et al\\.",
      "year" : 2016
    }, {
      "title" : "80 million tiny images: A large data set for nonparametric object and scene recognition",
      "author" : [ "A. Torralba", "R. Fergus", "W. Freeman" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "Torralba et al\\.,? \\Q1958\\E",
      "shortCiteRegEx" : "Torralba et al\\.",
      "year" : 1958
    }, {
      "title" : "From neural PCA to deep unsupervised learning",
      "author" : [ "H. Valpola" ],
      "venue" : "In Advances in Independent Component Analysis and Learning Machines (Chapter",
      "citeRegEx" : "Valpola,? \\Q2015\\E",
      "shortCiteRegEx" : "Valpola",
      "year" : 2015
    }, {
      "title" : "Extracting and composing robust features with denoising autoencoders",
      "author" : [ "P. Vincent", "H. Larochelle", "Y. Bengio", "P.A. Manzagol" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Vincent et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Vincent et al\\.",
      "year" : 2008
    }, {
      "title" : "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion",
      "author" : [ "P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "Manzagol", "P.-A" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Vincent et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Vincent et al\\.",
      "year" : 2010
    }, {
      "title" : "Training deeper convolutional networks with deep supervision",
      "author" : [ "L. Wang", "Lee", "C.-Y", "Z. Tu", "S. Lazebnik" ],
      "venue" : null,
      "citeRegEx" : "Wang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2015
    }, {
      "title" : "Unsupervised learning of visual representations using videos",
      "author" : [ "X. Wang", "A. Gupta" ],
      "venue" : "In ICCV,",
      "citeRegEx" : "Wang and Gupta,? \\Q2015\\E",
      "shortCiteRegEx" : "Wang and Gupta",
      "year" : 2015
    }, {
      "title" : "Supervised translationinvariant sparse coding",
      "author" : [ "J. Yang", "K. Yu", "T. Huang" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "Yang et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2010
    }, {
      "title" : "Visualizing and understanding convolutional networks",
      "author" : [ "M.D. Zeiler", "R. Fergus" ],
      "venue" : "In ECCV,",
      "citeRegEx" : "Zeiler and Fergus,? \\Q2014\\E",
      "shortCiteRegEx" : "Zeiler and Fergus",
      "year" : 2014
    }, {
      "title" : "Adaptive deconvolutional networks for mid and high level feature learning",
      "author" : [ "M. Zeiler", "G. Taylor", "R. Fergus" ],
      "venue" : "In ICCV,",
      "citeRegEx" : "Zeiler et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Zeiler et al\\.",
      "year" : 2011
    }, {
      "title" : "Image reconstruction from pool5 features to images. The reconstruction loss is computed on the ILSVRC2012 validation set and measured with L2-distance with the ground truth (RGB values are in [0, 1]). The first 2 example images are from the ILSVRC2012 validation set (excluding the 100 categories). The rest are not in ImageNet",
      "author" : [ ],
      "venue" : null,
      "citeRegEx" : "A.3.,? \\Q2012\\E",
      "shortCiteRegEx" : "A.3.",
      "year" : 2012
    }, {
      "title" : "AlexNet reconstruction on ImageNet ILSVRC2012 validation set. (Best viewed when zoomed in on a screen",
      "author" : [ "Figure A" ],
      "venue" : null,
      "citeRegEx" : "A.4.,? \\Q2012\\E",
      "shortCiteRegEx" : "A.4.",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "Many deep unsupervised models were proposed, such as stacked (denoising) autoencoders (Bengio et al., 2007; Vincent et al., 2010), deep belief networks (Hinton et al.",
      "startOffset" : 86,
      "endOffset" : 129
    }, {
      "referenceID" : 33,
      "context" : "Many deep unsupervised models were proposed, such as stacked (denoising) autoencoders (Bengio et al., 2007; Vincent et al., 2010), deep belief networks (Hinton et al.",
      "startOffset" : 86,
      "endOffset" : 129
    }, {
      "referenceID" : 9,
      "context" : ", 2010), deep belief networks (Hinton et al., 2006; Lee et al., 2009), sparse encoder-decoders (Ranzato et al.",
      "startOffset" : 30,
      "endOffset" : 69
    }, {
      "referenceID" : 16,
      "context" : ", 2010), deep belief networks (Hinton et al., 2006; Lee et al., 2009), sparse encoder-decoders (Ranzato et al.",
      "startOffset" : 30,
      "endOffset" : 69
    }, {
      "referenceID" : 22,
      "context" : ", 2009), sparse encoder-decoders (Ranzato et al., 2007; Kavukcuoglu et al., 2010), and deep Boltzmann machines (Salakhutdinov & Hinton, 2009).",
      "startOffset" : 33,
      "endOffset" : 81
    }, {
      "referenceID" : 13,
      "context" : ", 2009), sparse encoder-decoders (Ranzato et al., 2007; Kavukcuoglu et al., 2010), and deep Boltzmann machines (Salakhutdinov & Hinton, 2009).",
      "startOffset" : 33,
      "endOffset" : 81
    }, {
      "referenceID" : 14,
      "context" : "However, over the past few years, supervised learning without any unsupervised pretraining has achieved even better performance, and it has become the dominating approach to train deep neural networks for real-world tasks, such as image classification (Krizhevsky et al., 2012) and object detection (Girshick et al.",
      "startOffset" : 252,
      "endOffset" : 277
    }, {
      "referenceID" : 6,
      "context" : ", 2012) and object detection (Girshick et al., 2016).",
      "startOffset" : 29,
      "endOffset" : 52
    }, {
      "referenceID" : 8,
      "context" : ", 2015) and the residual structure (He et al., 2016), which were not limited by the modeling assumptions of unsupervised methods.",
      "startOffset" : 35,
      "endOffset" : 52
    }, {
      "referenceID" : 23,
      "context" : "Nonetheless, the existing validations (Rasmus et al., 2015; Pezeshki et al., 2016) were mostly on small-scale datasets like MNIST.",
      "startOffset" : 38,
      "endOffset" : 82
    }, {
      "referenceID" : 0,
      "context" : ", Ranzato & Szummer (2008); Larochelle & Bengio (2008); Sohn et al.",
      "startOffset" : 41,
      "endOffset" : 55
    }, {
      "referenceID" : 0,
      "context" : ", Ranzato & Szummer (2008); Larochelle & Bengio (2008); Sohn et al. (2013); Goodfellow et al.",
      "startOffset" : 41,
      "endOffset" : 75
    }, {
      "referenceID" : 0,
      "context" : ", Ranzato & Szummer (2008); Larochelle & Bengio (2008); Sohn et al. (2013); Goodfellow et al. (2013)) had been made to couple the unsupervised and supervised learning in the same phase, making unsupervised objectives able to impact the network training after supervised learning took place.",
      "startOffset" : 41,
      "endOffset" : 101
    }, {
      "referenceID" : 0,
      "context" : ", Ranzato & Szummer (2008); Larochelle & Bengio (2008); Sohn et al. (2013); Goodfellow et al. (2013)) had been made to couple the unsupervised and supervised learning in the same phase, making unsupervised objectives able to impact the network training after supervised learning took place. These methods unleashed new potential of unsupervised learning, but they have not yet been shown to scale to large amounts of labeled and unlabeled data. Rasmus et al. (2015) recently proposed an architecture that is easy to couple with a classification network by extending the stacked denoising autoencoder with lateral connections, i.",
      "startOffset" : 41,
      "endOffset" : 466
    }, {
      "referenceID" : 0,
      "context" : ", Ranzato & Szummer (2008); Larochelle & Bengio (2008); Sohn et al. (2013); Goodfellow et al. (2013)) had been made to couple the unsupervised and supervised learning in the same phase, making unsupervised objectives able to impact the network training after supervised learning took place. These methods unleashed new potential of unsupervised learning, but they have not yet been shown to scale to large amounts of labeled and unlabeled data. Rasmus et al. (2015) recently proposed an architecture that is easy to couple with a classification network by extending the stacked denoising autoencoder with lateral connections, i.e., from encoder to the same stages of the decoder, and their methods showed promising semi-supervised learning results. Nonetheless, the existing validations (Rasmus et al., 2015; Pezeshki et al., 2016) were mostly on small-scale datasets like MNIST. Recently, Zhao et al. (2015) proposed the “whatar X iv :1 60 6.",
      "startOffset" : 41,
      "endOffset" : 909
    }, {
      "referenceID" : 37,
      "context" : "where” autoencoder (SWWAE) by extending the stacked convolutional autoencoder using Zeiler et al. (2011)’s “unpooling” operator, which recovers the locational details (which was lost due to max-pooling) using the pooling switches from the encoder.",
      "startOffset" : 84,
      "endOffset" : 105
    }, {
      "referenceID" : 33,
      "context" : "Based on the above observations, we further improve the quality of reconstruction, an indication of the mutual information between the input and the feature representations (Vincent et al., 2010), by finetuning the entire augmented architecture with supervised and unsupervised objectives.",
      "startOffset" : 173,
      "endOffset" : 195
    }, {
      "referenceID" : 2,
      "context" : "To the best of our knowledge, this work is the first to show that unsupervised objective can improve the image classification accuracy of deep convolutional neural networks on largescale datasets, such as ImageNet (Deng et al., 2009).",
      "startOffset" : 214,
      "endOffset" : 233
    }, {
      "referenceID" : 18,
      "context" : "Mairal et al. (2009) proposed to combine the reconstruction loss of sparse coding and the classification loss of sparse features in a unified objective function.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 18,
      "context" : "Mairal et al. (2009) proposed to combine the reconstruction loss of sparse coding and the classification loss of sparse features in a unified objective function. Yang et al. (2010) extended this supervised sparse coding with max-pooling to obtain translation-invariant local features.",
      "startOffset" : 0,
      "endOffset" : 181
    }, {
      "referenceID" : 18,
      "context" : "Mairal et al. (2009) proposed to combine the reconstruction loss of sparse coding and the classification loss of sparse features in a unified objective function. Yang et al. (2010) extended this supervised sparse coding with max-pooling to obtain translation-invariant local features. Zeiler et al. (2010) proposed deconvolutional networks for unsupervised feature learning that consist of multiple layers of convolutional sparse coding with max-pooling.",
      "startOffset" : 0,
      "endOffset" : 306
    }, {
      "referenceID" : 18,
      "context" : "Mairal et al. (2009) proposed to combine the reconstruction loss of sparse coding and the classification loss of sparse features in a unified objective function. Yang et al. (2010) extended this supervised sparse coding with max-pooling to obtain translation-invariant local features. Zeiler et al. (2010) proposed deconvolutional networks for unsupervised feature learning that consist of multiple layers of convolutional sparse coding with max-pooling. Each layer is trained to reconstruct the output of the previous layer. Zeiler et al. (2011) further introduced the “unpooling with switches” layer to deconvolutional networks to enable end-to-end training.",
      "startOffset" : 0,
      "endOffset" : 547
    }, {
      "referenceID" : 0,
      "context" : "As an alternative to sparse coding and discriminative convolutional networks, autoencoders (Bengio, 2009) are another class of models for representation learning, in particular for the non-linear principal component analysis (Dong & McAvoy, 1996; Scholz & Vigário, 2002) by minimizing the reconstruction errors of a bottlenecked neural network.",
      "startOffset" : 91,
      "endOffset" : 105
    }, {
      "referenceID" : 1,
      "context" : "The stacked autoencoder (SAE) (Bengio et al., 2007) is amenable for hierarchical representation learning.",
      "startOffset" : 30,
      "endOffset" : 51
    }, {
      "referenceID" : 20,
      "context" : "With pooling-induced sparsity bottlenecks (Makhzani & Frey, 2015), the convolutional SAE (Masci et al., 2011) can learn features from middle-size images.",
      "startOffset" : 89,
      "endOffset" : 109
    }, {
      "referenceID" : 32,
      "context" : "By injecting noises or corruptions to the input, denoising autoencoders (Vincent et al., 2008; 2010) can learn robust filters to recover the uncorrupted input.",
      "startOffset" : 72,
      "endOffset" : 100
    }, {
      "referenceID" : 30,
      "context" : "Valpola (2015) further added noises to intermediate layers of denoising autoencoders with lateral connections, which was called “ladder network”.",
      "startOffset" : 0,
      "endOffset" : 15
    }, {
      "referenceID" : 23,
      "context" : "Rasmus et al. (2015) combined a classification task with the ladder network for semi-supervised learning, and they showed improved classification accuracy on MNIST and CIFAR-10.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 34,
      "context" : "(2015)’s “deeply supervised network” incorporated classification objectives for intermediate layers, was able to improve the top-layer classification accuracy for reasonably large-scale networks (Wang et al., 2015).",
      "startOffset" : 195,
      "endOffset" : 214
    }, {
      "referenceID" : 3,
      "context" : "Recently, more task-specific unsupervised objectives for image and video representation learning were developed by using spatial context (Doersch et al., 2015) and video continuity (Wang & Gupta, 2015).",
      "startOffset" : 137,
      "endOffset" : 159
    }, {
      "referenceID" : 15,
      "context" : "As an alternative to the autoencoder, Lee et al. (2015)’s “deeply supervised network” incorporated classification objectives for intermediate layers, was able to improve the top-layer classification accuracy for reasonably large-scale networks (Wang et al.",
      "startOffset" : 38,
      "endOffset" : 56
    }, {
      "referenceID" : 15,
      "context" : "As an alternative to the autoencoder, Lee et al. (2015)’s “deeply supervised network” incorporated classification objectives for intermediate layers, was able to improve the top-layer classification accuracy for reasonably large-scale networks (Wang et al., 2015). In earlier work, Ranzato & Szummer (2008) conducted layerwise training by both classification and reconstruction objectives.",
      "startOffset" : 38,
      "endOffset" : 307
    }, {
      "referenceID" : 14,
      "context" : "(Krizhevsky et al., 2012) contain a single pathway of convolutional layers succeeded by nonlinear activation functions and interleaved with max-pooling layers to gradually transform features into high-level representations and gain spatial invariance at different scales.",
      "startOffset" : 0,
      "endOffset" : 25
    }, {
      "referenceID" : 8,
      "context" : "Recent networks (Simonyan & Zisserman, 2015; Szegedy et al., 2015; He et al., 2016; Szegedy et al., 2016) often nest a group of convolutional layers before applying a max-pooling layer.",
      "startOffset" : 16,
      "endOffset" : 105
    }, {
      "referenceID" : 29,
      "context" : "Recent networks (Simonyan & Zisserman, 2015; Szegedy et al., 2015; He et al., 2016; Szegedy et al., 2016) often nest a group of convolutional layers before applying a max-pooling layer.",
      "startOffset" : 16,
      "endOffset" : 105
    }, {
      "referenceID" : 10,
      "context" : "On the one hand, the training of lower intermediate layers might be problematic, because the gradient signals from the top layer can become vanished (Hochreiter et al., 2001) on its way to the bottom layer.",
      "startOffset" : 149,
      "endOffset" : 174
    }, {
      "referenceID" : 8,
      "context" : "Regularization by normalization (Ioffe & Szegedy, 2015) can alleviate this problem, but will also lead to large yet noisy gradients when networks are deep (He et al., 2016).",
      "startOffset" : 155,
      "endOffset" : 172
    }, {
      "referenceID" : 23,
      "context" : "Ladder network architectures Rasmus et al. (2015). : nodes; : noisy nodes; : encoder macro-layer; : decoder macro-layer; : inner-product layer; : reconstruction loss; : classification loss; : parameter tying.",
      "startOffset" : 29,
      "endOffset" : 50
    }, {
      "referenceID" : 38,
      "context" : "Inspired by Zeiler et al. (2011), we use Zhao",
      "startOffset" : 12,
      "endOffset" : 33
    }, {
      "referenceID" : 23,
      "context" : "Rasmus et al. (2015)’s ladder network (Figure 3) is a more sophisticated way to augment existing sequential architectures with autoencoders.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 14,
      "context" : "1 To compare with existing methods on inverting neural networks (Dosovitskiy & Brox, 2016), we also partially used Krizhevsky et al. (2012)’s network, termed AlexNet, trained on ILSVRC2012 training set.",
      "startOffset" : 115,
      "endOffset" : 140
    }, {
      "referenceID" : 14,
      "context" : "we followed Krizhevsky et al. (2012)’s data augmentation scheme, cropping an image at the center to make it square with the shorter edge unchanged, resizing the square to 256×256, and randomly sampling a 227×227 patch or its horizontally mirrored counterpart to feed the network.",
      "startOffset" : 12,
      "endOffset" : 37
    }, {
      "referenceID" : 12,
      "context" : "Our implementation was based on the Caffe framework (Jia et al., 2014).",
      "startOffset" : 52,
      "endOffset" : 70
    } ],
    "year" : 2016,
    "abstractText" : "Unsupervised learning and supervised learning are key research topics in deep learning. However, as high-capacity supervised neural networks trained with a large amount of labels have achieved remarkable success in many computer vision tasks, the availability of large-scale labeled images reduced the significance of unsupervised learning. Inspired by the recent trend toward revisiting the importance of unsupervised learning, we investigate joint supervised and unsupervised learning in a large-scale setting by augmenting existing neural networks with decoding pathways for reconstruction. First, we demonstrate that the intermediate activations of pretrained large-scale classification networks preserve almost all the information of input images except a portion of local spatial details. Then, by end-to-end training of the entire augmented architecture with the reconstructive objective, we show improvement of the network performance for supervised tasks. We evaluate several variants of autoencoders, including the recently proposed “what-where\" autoencoder that uses the encoder pooling switches, to study the importance of the architecture design. Taking the 16-layer VGGNet trained under the ImageNet ILSVRC 2012 protocol as a strong baseline for image classification, our methods improve the validation-set accuracy by a noticeable margin.",
    "creator" : "LaTeX with hyperref package"
  }
}