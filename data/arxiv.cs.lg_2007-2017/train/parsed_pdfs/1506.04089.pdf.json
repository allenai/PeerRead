{
  "name" : "1506.04089.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Listen, Attend, and Walk: Neural Mapping of Navigational Instructions to Action Sequences",
    "authors" : [ "Hongyuan Mei", "Mohit Bansal", "Matthew R. Walter" ],
    "emails" : [ "hongyuan@ttic.edu", "mbansal@ttic.edu", "mwalter@ttic.edu" ],
    "sections" : [ {
      "heading" : "Introduction",
      "text" : "Robots must be able to understand and successfully execute natural language navigational instructions if they are to work seamlessly alongside people. For example, someone using a voice-commandable wheelchair might direct it to “Take me to the room across from the kitchen,” or a soldier may command a micro aerial vehicle to “Fly down the hallway into the second room on the right.” However, interpreting such free-form instructions (especially in unknown environments) is challenging due to their ambiguity and complexity, such as uncertainty in their interpretation (e.g., which hallway does the instruction refer to), long-term dependencies among both the instructions and the actions, differences in the amount of detail given, and the diverse ways in which the language can be composed. Figure 1 presents an example instruction that our method successfully follows.\nPrevious work in this domain (Chen and Mooney 2011; Chen 2012; Kim and Mooney 2012; 2013; Artzi and Zettlemoyer 2013; Artzi, Das, and Petrov 2014) largely requires specialized resources like semantic parsers, seed lexicons, and re-rankers to interpret ambiguous, free-form natural language instructions. In contrast, the goal of our work is to learn to map instructions to actions in an end-to-end fashion that assumes no prior linguistic knowledge. Instead, our model learns the meaning of all the words, spatial relations,"
    }, {
      "heading" : "Objects",
      "text" : ""
    }, {
      "heading" : "Wall paintings Tower",
      "text" : "syntax, and compositional semantics from just the raw training sequence pairs, and learns to to translate the free-form instructions to an executable action sequence.\nWe propose a recurrent neural network with long shortterm memory (LSTM) (Hochreiter and Schmidhuber 1997) to both encode the navigational instruction sequence bidirectionally and to decode the representation to an action sequence, based on a representation of the current world state. LSTMs are well-suited to this task, as they have been shown to be effective in learning the temporal de-\nar X\niv :1\n50 6.\n04 08\n9v 4\n[ cs\n.C L\n] 1\n7 D\nec 2\n01 5\npendencies that exist over such sequences, especially for the tasks of image captioning, machine translation, and natural language generation (Kiros, Salakhutdinov, and Zemel 2014; Donahue et al. 2014; Chen and Zitnick 2015; Karpathy and Fei-Fei 2015; Vinyals et al. 2015; Sutskever, Vinyals, and Lee 2014; Rush, Chopra, and Weston 2015; Wen et al. 2015). Additionally, we learn the correspondences between words in the input navigational instruction and actions in the output sequence using an alignment-based LSTM (Bahdanau, Cho, and Bengio 2014; Xu et al. 2015). Standard alignment methods only consider high-level abstractions of the input (language), which sacrifices information important to identifying these correspondences. Instead, we introduce a multi-level aligner that empowers the model to use both high- and low-level input representations and, in turn, improves the accuracy of the inferred directions.\nWe evaluate our model on a benchmark navigation dataset (MacMahon, Stankiewicz, and Kuipers 2006) and achieve the best results reported to-date on the singlesentence task (which contains only 2000 training pairs), without using any additional resources such as semantic parsers, seed lexicons, or rerankers used in previous work. On the multi-sentence task of executing a full paragraph, where the amount of training pairs is even smaller (just a few hundred pairs), our model performs better than several existing methods and is competitive with the state-of-the-art, all of which use specialized linguistic resources, extra annotation, or reranking. We perform a series of ablation studies in order to analyze the primary components of our model, including the encoder, multi-level representations, alignment, and bidirectionality."
    }, {
      "heading" : "Related Work",
      "text" : "A great deal of attention has been paid of late to algorithms that allow robots and autonomous agents to follow free-form navigational route instructions (MacMahon, Stankiewicz, and Kuipers 2006; Kollar et al. 2010; Chen and Mooney 2011; Chen 2012; Kim and Mooney 2012; 2013; Kong et al. 2014; Hemachandra et al. 2015). These methods solve what Harnad (1990) refers to as the symbol grounding problem, that of associating linguistic elements with their corresponding manifestation in the external world. Initial research in natural language symbol grounding focused on manually-prescribed mappings between language and sets of predefined environment features and actions (Winograd 1970; MacMahon, Stankiewicz, and Kuipers 2006). More recent work in statistical language understanding learns to convert free-form instructions into their referent symbols by observing the use of language in a perceptual context (Mooney 2008). These methods represent natural language grounding in terms of manually defined linguistic, spatial, and semantic features (Kollar et al. 2010; Matuszek, Fox, and Koscher 2010; Tellex et al. 2011). They learn the model parameters from natural language corpora, often requiring expensive annotation to pair each phrase to its corresponding grounding.\nOne class of grounded language acquisition methods treats the language understanding problem as one of learning a parser that maps free-form language into its formal lan-\nguage equivalent. For example, Matuszek, Fox, and Koscher (2010) assume no prior linguistic knowledge and employ a general-purpose supervised semantic parser learner. Alternatively, Chen and Mooney (2011) parse free-form route instructions into formal action specifications that a robot control process can then execute. They learn the parser in a weakly supervised manner from natural language instruction and action sequence pairs, together with the corresponding world representation. Alternatively, Kim and Mooney (2012) frame grounded language learning as probabilistic context free grammar (PCFG) induction and use learned lexicons (Chen and Mooney 2011) to control the space of production rules, which allows them to scale PCFGs to the navigation domain. Kim and Mooney (2013) improve upon the accuracy by adding a subsequent re-ranking step that uses a weakly supervised discriminative classifier. Meanwhile, Artzi and Zettlemoyer (2013) learn a combinatory categorical grammar (CCG)-based semantic parser to convert freeform navigational instructions to their manifestation in a lambda-calculus representation. As with Kim and Mooney (2013), they improve upon the accuracy through re-ranking. Artzi, Das, and Petrov (2014) extend their CCG parser learning by using statistics of the corpus to control the size of the lexicon, resulting in improved multi-sentence accuracy.\nA second class of grounded language learning techniques function by mapping free-form utterances to their corresponding object, location, and action referents in the agent’s world model. These methods learn a probabilistic model that expresses the association between each word in the instruction and its matching referent in the world model. The problem of interpreting a new instruction then becomes one of inference in this learned model. Kollar et al. (2010) build a generative model over the assumed flat, sequential structure of language that includes a combination of pre-specified and learned models for spatial relations, adverbs, and verbs. Tellex et al. (2011) later propose a discriminative model that expresses the hierarchical, compositional structure of language. They factor the probability distribution according to the parse structure of the free-form command and employ a log-linear factor graph to express the learned correspondence between linguistic elements and the space of groundings (objects, locations, and actions).\nWe adopt an alternative formulation and treat the problem of interpreting route instructions as a sequence-to-sequence learning problem. We learn this mapping in an end-to-end fashion using a neural network, without using any prior linguistic structure, resources, or annotation, which improves generalizability. Our method is inspired by the recent success of such sequence-to-sequence methods for machine translation (Sutskever, Vinyals, and Lee 2014; Bahdanau, Cho, and Bengio 2014; Cho et al. 2014), image and video caption synthesis (Kiros, Salakhutdinov, and Zemel 2014; Mao et al. 2014; Donahue et al. 2014; Vinyals et al. 2015; Chen and Zitnick 2015; Karpathy and Fei-Fei 2015), and natural language generation (Rush, Chopra, and Weston 2015; Wen et al. 2015), which similarly adopt an encoderdecoder approach. Our model encodes the input free-form route instruction and then decodes the embedding to identify the corresponding output action sequence based upon the lo-\ncal, observable world state (which we treat as a third sequence type that we add as an extra connection to every decoder step). Moreover, our decoder also includes alignment to focus on the portions of the sentence relevant to the current action, a technique that has proven effective in machine translation (Bahdanau, Cho, and Bengio 2014) and machine vision (Mnih et al. 2014; Ba, Mnih, and Kavukcuoglu 2014; Xu et al. 2015). However, unlike the standard alignment techniques, our model learns to align based not only on the high-level input abstraction, but also the low-level representation of the input instruction, which improves performance. Recently, Andreas and Klein (2015) use a conditional random field model to learn alignment between instructions and actions; our LSTM-based aligner performs substantially better than this approach."
    }, {
      "heading" : "Task Definition",
      "text" : "We consider the problem of mapping natural language navigational instructions to action sequences based only on knowledge of the local, observable environment. These instructions may take the form of isolated sentences (singlesentence) or full paragraphs (multi-sentence). We are interested in learning this mapping from corpora of training data of the form (x(i), a(i), y(i)) for i = 1, 2, . . . , n, where x(i) is a variable length natural language instruction, a(i) is the corresponding action sequence, and y(i) is the observable environment representation. The model learns to produce the correct action sequence a(i) given a previously unseen (x(i), y(i)) pair. The challenges arise from the fact that the instructions are free-form and complex, contain numerous spelling and grammatical errors, and are ambiguous in their meaning. Further, the model is only aware of the local environment in the agent’s line-of-sight.\nIn this paper, we consider the route instruction dataset generated by MacMahon, Stankiewicz, and Kuipers (2006). The data includes free-form route instructions and their corresponding action sequences within three different virtual worlds. The environments (Fig. 1) consist of interconnected hallways with a pattern (grass, brick, wood, gravel, blue, flower, or yellow octagons) on each hallway floor, a painting (butterfly, fish, or Eiffel Tower) on the walls, and objects (hat rack, lamp, chair, sofa, barstool, and easel) at intersections. After having explored an environment, instruc-\ntors were asked to give written commands that describe how to navigate from one location to another without subsequent access to the map. Each instruction was then given to several human followers who were tasked with navigating in the virtual world without a map, and their paths were recorded. Many of the raw instructions include spelling and grammatical errors, others are incorrect (e.g., misusing “left” and “right”), approximately 10% of the single sentences have no associated action, and 20% have no feasible path."
    }, {
      "heading" : "The Model",
      "text" : "We formulate the problem of interpreting natural language route instructions as inference over a probabilistic model P (a1:T |y1:T , x1:N ), where a1:T = (a1, a2, . . . , aT ) is the action sequence, yt is the world state at time t, and x1:N = (x1, x2, . . . , xN ) is the natural language instruction,\na∗1:T = arg max a1:T P (a1:T |y1:T , x1:N ) (1a)\n= arg max a1:T T∏ t=1 P (at|a1:t−1, yt, x1:N ) (1b)\nThis problem can be viewed as one of mapping the given instruction sequence x1:N to the action sequence a1:T . An effective means of learning this sequence-to-sequence mapping is to use a neural encoder-decoder architecture. We first use a bidirectional recurrent neural network model to encode the input sentence\nhj = f(xj , hj−1, hj+1) (2a) zt = c(h1, h2, . . . hN ), (2b)\nwhere hj is the encoder hidden state for word j ∈ {1, . . . , N}, and f and c are nonlinear functions that we define shortly. Next, the context vector zt (computed by the aligner) encodes the language instruction at time t ∈ {1, . . . , T}. Next, another RNN decodes the context vector zt to arrive at the desired likelihood (1)\nP (a1:T |y1:T , x1:N ) = T∏\nt=1\nP (at|a1:t−1, yt, x1:N ) (3a)\nP (at|a1:t−1, yt, x1:N ) = g(st−1, zt, yt), (3b) where st−1 is the decoder hidden state at time t−1, and g is a nonlinear function. Inference then follows by maximizing this posterior to determine the desired action sequence.\nOur model (Fig. 2) employs LSTMs as the nonlinear functions f and g due to their ability to learn long-term dependencies that exist over the instruction and action sequences, without suffering from exploding or vanishing gradients. Our model also integrates multi-level alignment to focus on parts of the instruction that are more salient to the current action at multiple levels of abstraction. We next describe each component of our network in detail.\nEncoder Our encoder takes as input the natural language route instruction represented as a sequence x1:N = (x1, x2, . . . , xN ), where x1 and xN are the first and last words in the sentence, respectively. We treat each word xi as a K-dimensional one-hot vector, where K is the vocabulary size. We feed this sequence into an LSTM-RNN that summarizes the temporal relationships between previous words and returns a sequence of hidden annotations h1:N = (h1, h2, . . . , hN ), where the annotation hj summarizes the words up to and including xj .\nWe adopt an LSTM encoder architecture (Fig. 3) similar to that of Graves, Abdel-rahman, and Hinton (2013),\niej fej oej gej  =  σσσ tanh T e( xjhj−1 )\n(4a)\ncej = f e j cej−1 + iej gej (4b) hj = o e j tanh(cej) (4c)\nwhere T e is an affine transformation, σ is the logistic sigmoid that restricts its input to [0, 1], iej , f e j , and o e j are the input, output, and forget gates of the LSTM, respectively, and cej is the memory cell activation vector. The memory cell cej summarizes the LSTM’s previous memory c e j−1 and the current input, which are modulated by the forget and input gates, respectively. The forget and input gates enable the LSTM to regulate the extent to which it forgets its previous memory and the input, while the output gate regulates the degree to which the memory affects the hidden state.\nOur encoder employs bidirectionality, encoding the sentences in both the forward and backward directions, an approach that has been found to be successful in speech recognition and machine translation (Graves, Abdel-rahman, and Hinton 2013; Bahdanau, Cho, and Bengio 2014; Cho et al. 2014). In this way, the hidden annotations\nhj = ( −→ h >j ; ←− h >j ) > concatenate forward −→ h j and backward annotations ←− h j , each determined using Equation 4c.\nMulti-level Aligner The context representation of the instruction is computed as a weighted sum of the word vectors xj and encoder states hj . Whereas most previous work align based only on the hidden annotations hj , we found that also including the original input word xj in the aligner improves performance. This multi-level representation allows the decoder to not just reason over the high-level, context-based representation of the input sentence hj , but to also consider the original low-level word representation xj . By adding xj , the model offsets information that is lost in the high-level abstraction of the instruction. Intuitively, the model is able to better match the salient words in the input sentence (e.g., “easel”) directly to the corresponding landmarks in the current world state yt used in the decoder. The context vector then takes the form\nzt = ∑ j αtj ( xj hj ) (5)\nThe weight αtj associated with each pair (xj , hj) is αtj = exp(βtj)/ ∑ j exp(βtj), (6)\nwhere the alignment term βtj = f(st−1, xj , hj) weighs the extent to which the word at position j and those around it match the output at time t. The alignment is modelled as a one-layer neural perceptron\nβtj = v > tanh(Wst−1 + Uxj + V hj), (7)\nwhere v, W , U , and V are learned parameters.\nDecoder Our architecture uses an LSTM decoder (Fig. 3) that takes as input the current world state yt, the context of the instruction zt, and the LSTM’s previous hidden state st−1. The output is the conditional probability distribution Pa,t = P (at|a1:t−1, yt, x1:N ) over the next action (3), represented as a deep output layer (Pascanu et al. 2014)\nidt fdt odt gdt  =  σσσ tanh T d(Eytst−1 zt ) (8a)\ncdt = f d t cdt−1 + idt gdt (8b)\nst = o d t tanh(cdt ) (8c)\nqt = L0(Eyt + Lsst + Lzzt) (8d) Pa,t = softmax (qt) (8e)\nwhere E is an embedding matrix and L0, Ls, and Lz are parameters to be learned.\nTraining We train the encoder and decoder models so as to predict the action sequence a∗1:T according to Equation 1 for a given instruction x1:N and world state y1:T from the\ntraining corpora. We use the negative log-likelihood of the demonstrated action at each time step t as our loss function,\nL = − logP (a∗t |yt, x1:N ). (9)\nAs the entire model is a differentiable function, the parameters can be learned by back-propagation.\nInference Having trained the model, we generate action sequences by finding the maximum a posteriori actions under the learned model (1). One action sequence is completed when the “stop” action is emitted. For the single-sentence task, we perform this search using standard beam search to maintain a list of the current k best hypotheses.1 We iteratively consider the k-best sequences up to time t as candidates to generate sequences of size t + 1 and keep only the resulting best k of them. For multi-sentence, we perform the search sentence-by-sentence, and initialize the beam of the next sentence with the list of previous k best hypotheses. As a common denoising method in deep learning (Sutskever, Vinyals, and Lee 2014; Zaremba, Sutskever, and Vinyals 2014; Vinyals et al. 2015), we perform inference over an ensemble of randomly initialized models.2"
    }, {
      "heading" : "Experimental Setup",
      "text" : "Dataset We train and evaluate our model using the publicly available SAIL route instruction dataset collected by MacMahon, Stankiewicz, and Kuipers (2006). We use the raw data in its original form (e.g., we do not correct any spelling errors). The dataset contains 706 non-trivial navigational instruction paragraphs, produced by six instructors for 126 unique start and end position pairs spread evenly across three virtual worlds. These instructions are segmented into individual sentences and paired with an action sequence. See corpus statistics in Chen and Mooney (2011).\nWorld State The world state yt encodes the local, observable world at time t. We make the standard assumption that the agent is able to observe all elements of the environment that are within line-of-sight. In the specific domain that we consider for evaluation, these elements include the floor patterns, wall paintings, and objects that are not occluded by walls. We represent the world state as a concatenation of a simple bag-of-words vector for each direction (forward, left, and right). The choice of bag-of-words representation avoids manual domain-specific feature-engineering and the combinatoriality of modeling exact world configurations.\nEvaluation Metrics We evaluate our end-to-end model on both the single-sentence and multi-sentence versions of the corpus. For the single-sentence task, following previous work, the strict evaluation metric deems a trial to be successful iff the final position and orientation exactly match those of the original demonstration. For multi-sentence, we disregard the final orientation, as previous work does. However,\n1We use a beam width of 10 to be consistent with existing work. 2At each time step t, we generate actions using the avg. of the\nposterior likelihoods of 10 ensemble models, as in previous work.\nthis setting is still more challenging than single-sentence due to cascading errors over individual sentences.\nTraining Details We follow the same procedure as Chen and Mooney (2011), training with the segmented data and testing on both single- and multi-sentence versions. We train our models using three-fold cross-validation based on the three maps. In each fold, we retain one map as test and partition the two-map training data into training (90%) and validation (10%) sets, the latter of which is used to tune hyperparameters.3 We repeat this process for each of the three folds and report (size-weighted) average test results over these folds. We later refer to this training procedure as “vDev.” Additionally, some previous methods (p.c.) adopted a slightly different training strategy whereby each fold trains on two maps and uses the test map to decide on the stopping iteration. In order to compare against these methods, we also train a separate version of our model in this way, which we refer to as “vTest.”\nFor optimization, we found Adam (Kingma and Ba 2015) to be very effective for training with this dataset. The training usually converges within 50 epochs. We performed early stopping based on the validation task metric. Similar to previous work (Xu et al. 2015), we found that the validation log-likelihood is not well correlated with the task metric."
    }, {
      "heading" : "Results and Analysis",
      "text" : "In this section, we compare the overall performance of our model on the single- and multi-sentence benchmarks against previous work. We then present an analysis of our model through a series of ablation studies.\nPrimary Result We first investigate the ability to navigate to the intended destination for a given natural language instruction. Figure 1 illustrates an output example for which our model successfully executes the input natural language instruction. Table 1 reports the overall accuracy of our model for both the single- and multi-sentence settings. We report two statistics with our model (vDev and vTest) in order to directly compare with existing work.4\n3We only tuned the no. of hidden units and the drop-out rate (Srivastava et al. 2014; Zaremba, Sutskever, and Vinyals 2014).\n4Subsequent evaluations are on vDev unless otherwise noted.\nAs we can see from Table 1, we surpass state-of-the-art results on the single-sentence route instruction task (for both vDev and vTest settings), despite using no linguistic knowledge or resources. Our multi-sentence accuracy, which is working with a really small amount of training data (a few hundred paragraph pairs), is competitive with state-of-theart and outperforms several previous methods that use additional, specialized resources in the form of semantic parsers, logical-form lexicons, and re-rankers.5 We note that our model yields good results using only greedy search (beam width of one). For vDev, we achieve 68.05 on singlesentence and 23.93 on multi-sentence, while for vTest, we get 70.56 on single-sentence and 27.91 on multi-sentence.\nDistance Evaluation Our evaluation required that the action sequence reach the exact desired destination. It is of interest to consider how close the model gets to the destination when it is not reached. Table 3 displays the fraction of test results that reach within d nodes of the destination. Often, the method produces action sequences that reach points close to the desired destination.\nMulti-level Aligner Ablation Unlike most existing methods that align based only on the hidden annotations hj , we adopt a different approach by also including the original input word xj (Eqn. 5). As shown in Table 2, the multi-level representation (“Full Model”) significantly improves performance over a standard aligner (“High-level Aligner”). Figure 4 visualizes the alignment of words to actions in the map environment for several sentences from the instruction paragraph depicted in Figure 1.\nAligner Ablation Our model utilizes alignment in the decoder as a means of focusing on word “regions” that are more salient to the current world state. We analyze the effect of the learned alignment by training an alternative model\n5Note that with no ensemble, we are still state-of-the-art on single-sentence and better than all comparable approaches on multi-sentence: Artzi et al. (2013, 2014) use extra annotations with a logical-form lexicon and Kim and Mooney (2013) use discriminative reranking, techniques that are orthogonal to our approach and should likely improve our results as well.\nin which the context vector zt is an unweighted average (Eqn. 5). As shown in the Table 2, learning the alignment does improve the accuracy of the resulting action sequence. Note that the “No Aligner” model still maintains all connections between the instruction and actions, but is just using non-learned, uniform weights.\nBidirectionality Ablation We train an alternative model that uses only a unidirectional (forward) encoder. As shown in Table 2, the bidirectional encoder (“Full Model”) significantly improves accuracy.\nEncoder Ablation We further evaluate the benefit of encoding the input sentence and consider an alternative model that directly feeds word vectors as randomly initialized embeddings into the decoder and relies on the alignment model to choose the salient words. Table 2 presents the results with and without the encoder and demonstrates that there is a substantial gain in encoding the input sentence into its context representation. We believe the difference is due to the RNN’s ability to incorporate sentence-level information into the word’s representation as it processes the sentence sequentially (in both directions). This advantage helps resolve ambiguities, such as “turn right before . . . ” versus “turn right after . . . ”."
    }, {
      "heading" : "Conclusion",
      "text" : "We presented an end-to-end, sequence-to-sequence approach to mapping natural language navigational instructions to action plans given the local, observable world state, using a bidirectional LSTM-RNN model with a multilevel aligner. We evaluated our model on a benchmark route instruction dataset and demonstrates that it achieves a new state-of-the-art on single-sentence execution and yields competitive results on the more challenging multi-sentence domain, despite working with very small training datasets and using no specialized linguistic knowledge or resources. We further performed a number of ablation studies to elucidate the contributions of our primary model components."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank Yoav Artzi, David Chen, Oriol Vinyals, and Kelvin Xu for their helpful comments. This work was supported in part by the Robotics Consortium of the U.S. Army Research Laboratory under the Collaborative Technology Alliance Program, Cooperative Agreement W911NF-10-20016, and by an IBM Faculty Award."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2004,
    "abstractText" : "We propose a neural sequence-to-sequence model for direction following, a task that is essential to realizing effective autonomous agents. Our alignment-based encoder-decoder model with long short-term memory recurrent neural networks (LSTM-RNN) translates natural language instructions to action sequences based upon a representation of the observable world state. We introduce a multi-level aligner that empowers our model to focus on sentence “regions” salient to the current world state by using multiple abstractions of the input sentence. In contrast to existing methods, our model uses no specialized linguistic resources (e.g., parsers) or taskspecific annotations (e.g., seed lexicons). It is therefore generalizable, yet still achieves the best results reported to-date on a benchmark single-sentence dataset and competitive results for the limited-training multi-sentence setting. We analyze our model through a series of ablations that elucidate the contributions of the primary components of our model.",
    "creator" : "TeX"
  }
}