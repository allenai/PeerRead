{
  "name" : "1509.03242.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Gibbs Sampling Strategies for Semantic Perception of Streaming Video Data",
    "authors" : [ "Yogesh Girdhar", "Gregory Dudek" ],
    "emails" : [ "yogi@whoi.edu", "dudek@cim.mcgill.ca" ],
    "sections" : [ {
      "heading" : null,
      "text" : "I. INTRODUCTION\nMaking decisions based on the environmental context of a robot’s locations requires that we first model the context of the robot observations, which in turn might correspond to various semantic or conceptually higher level entities that compose the world. If we are given an observation model of these entities that compose the world then it is easy to describe a given scene in terms of these entities using this model; likewise, if we are given a labeling of the world in terms of these entities, then it is easy to compute the observation model for each individual entity. The challenge comes from doing these two tasks together, unsupervised, and with no prior information. ROST [1] , a realtime online spatiotemporal topic modeling framework attempt to solve this problem of assigning high level labels to low level streaming observations.\nTopic modeling techniques were originally developed for unsupervised semantic modeling of text documents [2] [3]. These algorithms automatically discover the main themes (topics) that underly these documents, which can then be used to compare these documents based on their semantic content.\nTopic modeling of observation data captured by a mobile robot faces additional challenges compared to topic modeling of a collection of text documents, or images that are mutually independent. • Robot observations are generally dependent on its lo-\ncation in space and time, and hence the corresponding semantic descriptor must take into account the location of the observed visual words during the refinement, and use it to compute topic priors that are sensitive to changes in time and the location of the robot. • The topic model must be updated online and in realtime, since the observations are generally made continuously at regular intervals. When computing topic labels for a\n1Yogesh Girdhar is at Deep Submergence Laboratory, Woods Hole Oceanographic Institution, Woods Hole, MA 02542, USA yogi@whoi.edu\n2Gregory Dudek is at Center for Intelligent Machines, McGill University, Montreal, QC H3A0E9, Canada dudek@cim.mcgill.ca\nnew observation, we must also update topic labels for previous observations in the light on new incoming data.\nROST[1] extends previous work on text and image topic modeling to make it suitable for processing streaming sensor data such as video and audio observed by a robot, and presents approximations for posterior inferencing that work in realtime. Topics in this case model the latent causes that produce these observations. ROST has been used for building semantic maps [4] and for modeling curiosity in a mobile robot, for the purpose of information theoretic exploration [5]. ROST uses Gibbs sampling to continuously refine the topic labels for the observed data. In this paper we present various variants of Gibbs sampling that can be used to keep the topic labels converged under realtime constraints."
    }, {
      "heading" : "II. PREVIOUS WORK",
      "text" : ""
    }, {
      "heading" : "A. Topic Modeling of Spatiotemporal Data",
      "text" : "Given images of scenes with multiple objects, topic modeling has been used to discover objects in these images in an unsupervised manner. Bosch et al. [6] used PLSA and a SIFT based [7] visual vocabulary to model the content of\nar X\niv :1\n50 9.\n03 24\n2v 1\n[ cs\n.R O\n] 1\n0 Se\np 20\n15\nimages, and used a nearest neighbor classifier to classify the images.\nFei-Fei et al. [8] have demonstrated the use of LDA to provide an intermediate representation of images, which was then used to learn an image classifier over multiple categories.\nInstead of modeling the entire image as a document, Spatial LDA (SLDA) [9] models a subset of words, close to each other in an image as a document, resulting in a better encoding of the spatial structure. The assignment of words to documents is not done a priori, but is instead modeled as an additional hidden variable in the generative process.\nGeometric LDA (gLDA) [10] models the LDA topics using words that are augmented with spatial position. Each topic in gLDA can be visualized as a pin-board where the visual words are pinned at their relatively correct positions. A document is assumed to be generated by first sampling a distribution over topics, and then for each word, sampling a topic label from this distribution, along with the transformation from the latent spatial model to the document (image). These transformations are all assumed to be affine, to model the change in viewpoints.\nLDA has been extended to learn a hierarchical representation of image content. Sivic et al.[11] used hierarchical LDA (hLDA) [12] for automatic generation of meaningful object hierarchies. Like LDA, hLDA also models documents as a mixture of topics; however, instead of the flat topics used in LDA, topics in hLDA correspond to a path in a tree. These topics become more specialized as they travel farther down from the root of the tree."
    }, {
      "heading" : "III. SPATIOTEMPORAL TOPIC MODEL",
      "text" : "An observation word is a discrete observation made by a robot. Given the observation words and their location, we would like to compute the posterior distribution of topics at this location. Let w be the observed word at location x. We assume the following probabilistic model for the observation words:\n1) word distribution for each topic k:\nφk ∼ Dirichlet(β),\n2) topic distribution for words at location x :\nθx ∼ Dirichlet(α+H(x)),\n3) topic label for w:\nz ∼ Discrete(θx),\n4) word label: w ∼ Discrete(φz),\nwhere y ∼ Y implies that random variable y is sampled from distribution Y , z is the topic label for the word observation w, and H(x) is the distribution of topics in the neighborhood of location x. Each topic is modeled by distribution φk over V possible word in the observation vocabulary.\nφk(v) = P(w = v|z = k) =∝ nvk + β, (1)\nwhere nvk is the number of times we have observed word v taking topic label k, and β is the Dirichlet prior hyperparameter. Topic model Φ = {φk} is a K × V matrix that encodes the global topic description information shared by all locations.\nThe main difference between this generative process and the generative process of words in a text document as proposed by LDA [2], [3] is in step 2. The context of words in LDA is modeled by the topic distribution of the document, which is independent of other documents in the corpora. We relax this assumption and instead propose the context of an observation word to be defined by the topic distribution of its spatiotemporal neighborhood. This is achieved via the use of a kernel. The posterior topic distribution at location x is thus defined as:\nθx(k) = P(z = k|x) ∝ (∑ y K(x− y)nky ) + α, (2)\nwhere K(·) is the kernel, α is the Dirichlet prior hyperameter and, nky is the number of times we observed topic k at location y."
    }, {
      "heading" : "IV. APPROXIMATING NEIGHBORHOODS USING CELLS",
      "text" : "The generative process defined above models the clustering behavior of observations from a natural scene well, but is difficult to implement because it requires keeping track of the topic distribution at every location in the world. This is computationally infeasible for any large dataset. For the special case when the kernel is a uniform distribution over a finite region, we can assume a cell decomposition of the world, and approximate the topic distribution around a location by summing over topic distribution of cells in and around the location.\nLet the world be decomposed into C cells, in which each cell c ∈ C is connected to its neighboring cells G(c) ⊆ C. Let c(x) be the cell that contains points x. In this paper we only experiment with a grid decomposition of the world in which each cell is connected to its six nearest neighbors, 4 spatial and 2 temporal. However, the general ideas presented here are applicable to any other topological decomposition of spacetime.\nInitialize ∀i, zi ∼ Uniform({1, . . . ,K}) while true do\nforeach cell c ∈ C do foreach word wi ∈ c do\nzi ∼ P(zi = k|wi = v, xi) Update Θ,Φ given the new zi by updating nvk and n k G\nend end\nend Algorithm 1: Batch Gibbs sampling\nThe topic distribution around x can then be approximated using cells as:\nθx(k) ∝  ∑ c′∈G(c(x)) nkc′ + α (3) Due to this approximation, the following properties emerge: 1) θx = θy if c(x) = c(y), i.e., all the points in a cell\nshare the same neighborhood topic distribution. 2) The topic distribution of the neighborhood is computed\nby summing over the topic distribution of the neighboring cells rather than individual points.\nWe take advantage of these properties while doing inference in realtime."
    }, {
      "heading" : "V. REALTIME INFERENCE USING GIBBS SAMPLING",
      "text" : "Given a word observation wi, its location xi, and its neighborhood Gi = G(c(xi)), we use a Gibbs sampler to assign a new topic label to the word, by sampling from the posterior topic distribution:\nP(zi = k|wi = v, xi) ∝ nvk,−i + β∑V\nv=1(n v k,−i + β)\n·\nnkGi,−i + α∑K k=1(n k Gi,−i + α) ,\n(4)\nwhere nwk,−i counts the number of words of type w in topic k, excluding the current word wi, nkGi,−i is the number of words with topic label k in neighborhood Gi, excluding the current word wi, and α, β are the Dirichlet hyper-parameters. Note that for a neighborhood size of 0, the above Gibbs sampler is equivalent to the LDA Gibbs sampler proposed by Griffiths et al.[3], where each cell corresponds to a document. Algorithm 1 shows a simple iterative technique to compute the topic labels for the observed words in batch mode.\nIn the context of robotics we are interested in the online refinement of observation data. After each new observation, we only have a constant amount of time to do topic label refinement. Hence, any online refinement algorithm that has computational complexity which increases with new data, is not useful. Moreover, if we are to use the topic labels of an incoming observation for making realtime decisions, then\nwhile true do Add new observed words to their corresponding cells. T ← 0 (current time) Initialize ∀i ∈MT , zi ∼ Uniform({1, . . . ,K}) while no new observation do\nt ∼ P(t|T ) foreach cell c ∈Mt do\nforeach word wi ∈ c do zi ∼ P(zi = k|wi = v, xi) Update Θ,Φ given the new zi by updating nvk and n k G\nend end end T ← T + 1\nend Algorithm 2: Realtime Gibbs sampler\nit is essential that the topic labels for the last observation converge before the next observation arrives.\nSince the total amount of data collected grows linearly with time, we must use a refinement strategy that efficiently handles global (previously observed) data and local (recently observed) data.\nOur general strategy is described by Algorithm 2. At each time step we add the new observations to the model, and then randomly pick observation times t ∼ P(t|T ), where T is the current time, for which we resample the topic labels and update the topic model.\nWe discuss the choice of P(t|T ) in the following sections."
    }, {
      "heading" : "A. Now Gibbs Sampling",
      "text" : "The simplest way of processing streaming observation data to ensure that the topic labels from the last observation have converged is to only refine topics from the last observation till the next observation has arrived.\nP(t|T ) = { 1, if t = T 0, otherwise\n(5)\nWe call this the Now Gibbs sampler. This is analogous to o-LDA approach by Banerjee and Basu [13].\nIf R is our computation budget, defined as the expected number of observation time-steps our system can refine between the arrival times of two consecutive observations, and r(t) be the number of times observations in Mt have been refined after time T , then this approach gives each observation R amount of resources.\nE{r(t)} = R (6)\nAlthough this sounds fair, the problem is that no information from the future is used to improve the understanding of the past data."
    }, {
      "heading" : "B. Uniform Gibbs Sampling",
      "text" : "A conceptually opposite strategy is to uniform randomly pick an observation from all the observations thus far, and refine the topic labels for all the words in this observation.\nP(t|T ) = 1/T (7)\nThis is analogous to the incremental Gibbs sampler for LDA proposed by Canini et al.[14].\nLet Mt be the set of cell containing observations at time t, R be the number of observations our system can refine between two observations, and r(t) be the number of times observations in Mt have been refined after time T . The expected value of r(t) is then:\nE{r(t)} = R ( 1\nt +\n1 t+ 1 + · · ·+ 1 T\n) (8)\n≈ R(log T − log t). (9)\nWe see that older observations are sampled disproportionally higher than newer observations, and topic labels of new observations might take a long time to converge. In fact, if τR is the expected number of iterations it takes for topic labels of an observation to converge, where τ < 1 is a constant, then all observations after time t′ = 1/τ would never be able to converge in the time before the next observation arrives. This is a big problem for a realtime system, where we need the topic labels of the last observations to actuate the robot."
    }, {
      "heading" : "C. Age Proportional Gibbs Sampling",
      "text" : "A seemingly good in-between approach might be to bias the random sampling of observations to be refined in favor of picking recent observations, with probability proportional to its timestamp.\nP(t|T ) = t∑T i=1 i\n(10)\nThen, the expected number of times this observation is refined is given by:\nE{r(t)} = R ( t∑t i=1 i + t∑t+1 i=1 i + · · ·+ t∑T i=1 i ) (11)\n≈ 2R (T − t) T . (12)\nWhen a new observation is made, the expected number of refinements it will gets before the next observation arrives is Rt/ ∑ t ≈ 2R/t, which implies that if t′ is the time after which it will not have sufficient number of refinements, then:\n2R\nt′ = τR (13)\n=⇒ t′ = 2 τ\n(14)\nHence, we see that this strategy, although better than uniform random sampling (for which we computed t′ = 1/τ ), is still not useful for long term operating of the robot."
    }, {
      "heading" : "D. Exponential Gibbs Sampling",
      "text" : "Using a geometric distribution we can define the probability of refinement of timestep t, at current time T\nP(t|T ) = q(1− q)T−t, (15)\nwhere 0 < q < 1 is a parameter. Using this distribution for picking refinement samples ensures that on average qR number of refinements are spent on refining the most recent observations, and the remaining (q − 1)R refinement iterations are spent on refining other recent observations. In the limit T → ∞, observations in each time-step are refined E{r(t)} = R number of times, similar to Now Gibbs Sampler. This approach, however, allows new information to influence some of the recent past observations, resulting in lower global perplexity of the learned model."
    }, {
      "heading" : "E. Mixed Gibbs Sampling",
      "text" : "We expect both Now and Exponential Gibbs samplers to be good at ensuring the topic labels for the last observation converges quickly (to a locally optimal solution), before the next observation arrives, whereas Uniform and Ageproportional Gibbs samplers are better at finding globally optimal results.\nOne way to balance both these performance goals is to combine these global and a local strategies. We consider four such approaches in this paper:\nUniform+Now:\nP(t|T ) = { η, if t = T (1− η)/(T − 1), otherwise (16)\nAgeProportional+Now:\nP(t|T ) = { η, if t = T (1− η) t∑T−1\ni=1 i , otherwise\n(17)\nUniform+Exp:\nP(t|T ) = ηq(1− q)T−t + (1− η)/T (18)\nAgeProportional+Exp:\nP(t|T ) = ηq(1− q)T−t + (1− η) t∑T i=1 i (19)\nHere 0 ≤ η ≤ 1 is the mixing proportion between the local and the global strategies."
    }, {
      "heading" : "VI. EXPERIMENTS",
      "text" : "1) Dataset: We evaluated the performance on ROST in analyzing videos using three different datasets with millions of visual words. We used a mixed vocabulary to describe each frame, with 5000 ORB words, 256 intensity words (pixel intensity), and 180 hue words (pixel hue), for a total vocabulary size of 5436. Although it is difficult to substantiate the optimality of the vocabulary, our experiments have suggested that once the vocabulary size is sufficiently large, there is limited sensitivity to its precise value [15].\nSome key statistics for these datasets is shown in Table I.\nThe 2objects dataset show a simple scenario in which two different objects appear on a textured (wood) background randomly, first individually and finally together.\nThe aerial dataset was collected using Unicorn UAV over a coastal region. The UAV performs a zig-zag coverage pattern over buildings, forested areas and ocean.\nThe underwater dataset was collected using Aqua as it swims over a coral reef. The dataset contains a variety of complex underwater terrain such as different coral species, rocks, sand, and divers.\nThe video files corresponding to these datasets, and some examples of ROST in action are available at 1.\nTo focus on analyzing the effects of spatiotemporal neighborhoods, and various Gibbs samplers, we fixed all other parameters of the system. We used cells of size 64x64 pixels with temporal width of 1 time step, Dirichlet parameters α = 0.1, β = 0.5, number of topics K = 16."
    }, {
      "heading" : "A. Realtime Gibbs Samplers",
      "text" : "To evaluate the proposed realtime Gibbs samplers on real data, we performed the following experiment. For each video dataset, and for each Gibbs sampler, we computed the topic labels and perplexity online, with 10 random restarts. We then compared the mean perplexity of words, one time step after their arrival (instantaneous), and after all observations have been made (final), with the perplexity of topic labels computed in batch. For a fair comparison, we used the same refinement time per time step (TR) for both batch and online cases. The resulting perplexity plots are shown in Figures 3, 4, and 5. The mean perplexity scores for the entire datasets are shown in Tables III (instantaneous perplexity), and II (final perplexity). Note that instantaneous perplexity is computed on a new image, given the model learnt online from all previous data. Hence this perplexity score serves the same purpose as computing perplexity on held out data when evaluating topic modeling on batch data.\nFrom our experiments we find that although Uniform and Age Proportional Gibbs samplers perform well when it comes to final perplexity of the dataset, they however perform poorly when measuring instantaneous perplexity. Low instantaneous perplexity, which is measured one time step after an observation is made, is essential for use of topic modeling in robotic applications. We would like to make decisions based on current observations, and hence low instantaneous perplexity is crucial. We find that the mixed Gibbs samplers such as Uniform+Now perform consistently well. Note that all experiments with the mixed Gibbs samplers were performed with a fixed mixing ratio η = 0.5,\n1http://cim.mcgill.ca/mrl/girdhar/\ngiving equal weight to local and global refinement. We are confident that better tuning of this variable will result in even better performance of ROST."
    }, {
      "heading" : "VII. CONCLUSION",
      "text" : "Topic modeling techniques such as ROST, model the latent context of the streaming spatiotemporal observation, such as image and other sensor data collected by a robot. In this paper we compared the performance of several Gibbs samplers for realtime spatiotemporal topic modeling, including those proposed by o-LDA and incremental LDA.\nWe measured how well the topic labels converge, globally for the entire data, and for individually for an observation, one time step after its observation time. The latter measurement criterion is useful in evaluating the performance of the proposed technique in the context of robotics, where we need to make instantaneous decisions. We showed that the proposed mixed Gibbs samplers such as Uniform+Now\nperform consistently better than other samplers, which just focus on recent observation, or which refine all observation with equal probability."
    }, {
      "heading" : "ACKNOWLEDGMENT",
      "text" : "This work was supported by the Natural Sciences and Engineering Research Council (NSERC) through the NSERC Canadian Field Robotics Network (NCFRN). Yogesh Girdhar is currently supported by the Postdoctoral Scholar Program at the Woods Hole Oceanographic Institution, with funding provided by the Devonshire Foundation and the J. Seward Johnson Fund. Authors would like to thank Julian Straub at MIT for helpful discussion."
    } ],
    "references" : [ {
      "title" : "Autonomous adaptive exploration using realtime online spatiotemporal topic modeling,",
      "author" : [ "Y. Girdhar", "P. Giguere", "G. Dudek" ],
      "venue" : "The International Journal of Robotics Research,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2013
    }, {
      "title" : "Latent dirichlet allocation,",
      "author" : [ "D.M. Blei", "A.Y. Ng", "M.I. Jordan" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2003
    }, {
      "title" : "Finding scientific topics,",
      "author" : [ "T.L. Griffiths", "M. Steyvers" ],
      "venue" : "Proceedings of the National Academy of Sciences of the United States of America,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2004
    }, {
      "title" : "Curiosity Based Exploration for Learning Terrain Models,",
      "author" : [ "Y. Girdhar", "D. Whitney", "G. Dudek" ],
      "venue" : "IEEE International Conference on Robotics and Automation,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2014
    }, {
      "title" : "Exploring Underwater Environments with Curiosity,",
      "author" : [ "Y. Girdhar", "G. Dudek" ],
      "venue" : "Canadian Conference on Computer and Robot Vision. Montreal: IEEE,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2014
    }, {
      "title" : "Scene Classification Via pLSA,",
      "author" : [ "A. Bosch", "A. Zisserman", "X. Muñoz" ],
      "venue" : "ser. Lecture Notes in Computer Science,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2006
    }, {
      "title" : "Distinctive image features from scale-invariant keypoints,",
      "author" : [ "D.G. Lowe" ],
      "venue" : "International Journal of Computer Vision,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2004
    }, {
      "title" : "A Bayesian Hierarchical Model for Learning Natural Scene Categories,",
      "author" : [ "L. Fei-Fei", "P. Perona" ],
      "venue" : "IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’05),",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2005
    }, {
      "title" : "Spatial Latent Dirichlet Allocation,",
      "author" : [ "X. Wang", "E. Grimson" ],
      "venue" : "Advances in Neural Information Processing Systems,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2007
    }, {
      "title" : "Geometric LDA: A Generative Model for Particular Object Discovery,",
      "author" : [ "J. Philbin", "J. Sivic", "A. Zisserman" ],
      "venue" : "Proceedings of the British Machine Vision Conference,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2008
    }, {
      "title" : "Unsupervised discovery of visual object class hierarchies,",
      "author" : [ "J. Sivic", "B.C. Russell", "A. Zisserman", "W.T. Freeman", "A.A. Efros" ],
      "venue" : "IEEE Conference on Computer Vision and Pattern Recognition",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2008
    }, {
      "title" : "Hierarchical topic models and the nested Chinese restaurant process,",
      "author" : [ "D.M. Blei", "T.L. Griffiths", "M.I. Jordan", "J.B. Tenenbaum" ],
      "venue" : "Advances in neural information processing systems (NIPS),",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2004
    }, {
      "title" : "Topic Models over Text Streams: A Study of Batch and Online Unsupervised Learning,",
      "author" : [ "A. Banerjee", "S. Basu" ],
      "venue" : "SIAM International Conference on Data Mining,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2007
    }, {
      "title" : "Online Inference of Topics with Latent Dirichlet Allocation,",
      "author" : [ "K.R. Canini", "L. Shi", "T.L. Griffiths" ],
      "venue" : "Proceedings of the International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1999
    }, {
      "title" : "Online Visual Vocabularies,",
      "author" : [ "Y. Girdhar", "G. Dudek" ],
      "venue" : "CRV",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "ROST [1] , a realtime online spatiotemporal topic modeling framework attempt to solve this problem of assigning high level labels to low level streaming observations.",
      "startOffset" : 5,
      "endOffset" : 8
    }, {
      "referenceID" : 1,
      "context" : "Topic modeling techniques were originally developed for unsupervised semantic modeling of text documents [2] [3].",
      "startOffset" : 105,
      "endOffset" : 108
    }, {
      "referenceID" : 2,
      "context" : "Topic modeling techniques were originally developed for unsupervised semantic modeling of text documents [2] [3].",
      "startOffset" : 109,
      "endOffset" : 112
    }, {
      "referenceID" : 0,
      "context" : "ROST[1] extends previous work on text and image topic modeling to make it suitable for processing streaming sensor data such as video and audio observed by a robot, and presents approximations for posterior inferencing that work in realtime.",
      "startOffset" : 4,
      "endOffset" : 7
    }, {
      "referenceID" : 3,
      "context" : "ROST has been used for building semantic maps [4] and for modeling curiosity in a mobile robot, for the purpose of information theoretic exploration [5].",
      "startOffset" : 46,
      "endOffset" : 49
    }, {
      "referenceID" : 4,
      "context" : "ROST has been used for building semantic maps [4] and for modeling curiosity in a mobile robot, for the purpose of information theoretic exploration [5].",
      "startOffset" : 149,
      "endOffset" : 152
    }, {
      "referenceID" : 5,
      "context" : "[6] used PLSA and a SIFT based [7] visual vocabulary to model the content of ar X iv :1 50 9.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 6,
      "context" : "[6] used PLSA and a SIFT based [7] visual vocabulary to model the content of ar X iv :1 50 9.",
      "startOffset" : 31,
      "endOffset" : 34
    }, {
      "referenceID" : 7,
      "context" : "[8] have demonstrated the use of LDA to provide an intermediate representation of images, which was then used to learn an image classifier over multiple categories.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 8,
      "context" : "Instead of modeling the entire image as a document, Spatial LDA (SLDA) [9] models a subset of words, close to each other in an image as a document, resulting in a better encoding of the spatial structure.",
      "startOffset" : 71,
      "endOffset" : 74
    }, {
      "referenceID" : 9,
      "context" : "Geometric LDA (gLDA) [10] models the LDA topics using words that are augmented with spatial position.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 10,
      "context" : "[11] used hierarchical LDA (hLDA) [12] for automatic generation of meaningful object hierarchies.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "[11] used hierarchical LDA (hLDA) [12] for automatic generation of meaningful object hierarchies.",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 1,
      "context" : "The main difference between this generative process and the generative process of words in a text document as proposed by LDA [2], [3] is in step 2.",
      "startOffset" : 126,
      "endOffset" : 129
    }, {
      "referenceID" : 2,
      "context" : "The main difference between this generative process and the generative process of words in a text document as proposed by LDA [2], [3] is in step 2.",
      "startOffset" : 131,
      "endOffset" : 134
    }, {
      "referenceID" : 2,
      "context" : "[3], where each cell corresponds to a document.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 12,
      "context" : "This is analogous to o-LDA approach by Banerjee and Basu [13].",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 13,
      "context" : "[14].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 14,
      "context" : "Although it is difficult to substantiate the optimality of the vocabulary, our experiments have suggested that once the vocabulary size is sufficiently large, there is limited sensitivity to its precise value [15].",
      "startOffset" : 209,
      "endOffset" : 213
    } ],
    "year" : 2015,
    "abstractText" : "Topic modeling of streaming sensor data can be used for high level perception of the environment by a mobile robot. In this paper we compare various Gibbs sampling strategies for topic modeling of streaming spatiotemporal data, such as video captured by a mobile robot. Compared to previous work on online topic modeling, such as o-LDA and incremental LDA, we show that the proposed technique results in lower online and final perplexity, given the realtime constraints.",
    "creator" : "LaTeX with hyperref package"
  }
}