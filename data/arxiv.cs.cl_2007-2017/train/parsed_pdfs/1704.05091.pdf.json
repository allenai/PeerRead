{
  "name" : "1704.05091.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "FEUP at SemEval-2017 Task 5: Predicting Sentiment Polarity and Intensity with Financial Word Embeddings",
    "authors" : [ "Pedro Saleiro", "Eduarda Mendes Rodrigues", "Carlos Soares", "Eugénio Oliveira" ],
    "emails" : [ "pssc@fe.up.pt", "eduarda@fe.up.pt", "csoares@fe.up.pt", "eco@fe.up.pt" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n70 4.\n05 09\n1v 1\n[ cs\n.C L\n] 1\n7 A\npr 2\n01 7\nThis paper presents the approach developed at the Faculty of Engineering of University of Porto, to participate in SemEval 2017, Task 5: Fine-grained Sentiment Analysis on Financial Microblogs and News. The task consisted in predicting a real continuous variable from -1.0 to +1.0 representing the polarity and intensity of sentiment concerning companies/stocks mentioned in short texts. We modeled the task as a regression analysis problem and combined traditional techniques such as pre-processing short texts, bag-of-words representations and lexical-based features with enhanced financial specific bag-ofembeddings. We used an external collection of tweets and news headlines mentioning companies/stocks from S&P 500 to create financial word embeddings which are able to capture domain-specific syntactic and semantic similarities. The resulting approach obtained a cosine similarity score of 0.69 in sub-task 5.1 - Microblogs and 0.68 in sub-task 5.2 - News Headlines."
    }, {
      "heading" : "1 Introduction",
      "text" : "Sentiment Analysis on financial texts has received increased attention in recent years (Nardo et al., 2016). Neverthless, there are some challenges yet to overcome (Smailović et al., 2014). Financial texts, such as microblogs or newswire, usually contain highly technical and specific vocabulary or jargon, making the develop of specific lexical and machine learning approaches necessary. Most of the research in Sentiment Analysis in the financial domain has focused in analyzing subjective text, labeled with explicitly expressed sentiment.\nHowever, it is also common to express financial sentiment in an implicit way. Business news stories often refer to events that might indicate a positive or negative impact, such as in the news title “company X will cut 1000 jobs”. Economic indicators, such as unemployment and future state modifiers such as drop or increase can also provide clues on the implicit sentiment (Musat and Trausan-Matu, 2010). Contrary to explicit expressions (subjective utterances), factual text types often contain objective statements that convey a desirable or undesirable fact (Liu, 2012).\nRecent work proposes to consider all types of implicit sentiment expressions (Van de Kauter et al., 2015). The authors created a fine grained sentiment annotation procedure to identify polar expressions (implicit and explicit expressions of positive and negative sentiment). A target (company of interest) is identified in each polar expression to identify the sentiment expressions that are relevant. The annotation procedure also collected information about the polarity and the intensity of the sentiment expressed towards the target. However, there is still no automatic approach, either lexical-based or machine learning based, that tries to model this annotation scheme.\nIn this work, we propose to tackle the aforementioned problem by taking advantage of unsupervised learning of word embeddings in financial tweets and financial news headlines to construct a domain-specific syntactic and semantic representation of words. We combine bag-ofembeddings with traditional approaches, such as pre-processing techniques, bag-of-words and financial lexical-based features to train a regressor for sentiment polarity and intensity. We study how different regression algorithms perform using all features in two different sub-tasks at SemEval-\n2017 Task 5: microblogs and news headlines mentioning companies/stocks. Moreover, we compare how different combinations of features perform in both sub-tasks. The system source code and word embeddings developed for the competition are publicly available.1\nThe remainder of the paper is organized as follows. We start by describing SemEval-2017 Task 5 and how we created financial-specific word embeddings. In Section 4 we present the implementation details of the system created for the competition followed by the experimental setup. We then present the experimental results and analysis, ending with the conclusions of this work."
    }, {
      "heading" : "2 Task Description",
      "text" : "The task 5 of SemEval 2017 (Cortis et al., 2017) consisted of fine-grained sentiment analysis of financial short texts and it was divided in two subtasks based on the type of text. Sub-task 5.1 – Microblogs – consisted of stocktwits and tweets focusing on stock market events and assessments from investors and traders. Companies/stocks were identified using stock symbols, the so called cashtags, e.g.“$AMZN” for the company Amazon.com, Inc. Sub-task 5.2 – News Headlines – consisted of sentences extracted from Yahoo Finance and other financial news sources on the internet. In this case, companies/stocks were identified using their canonical name and were previously annotated by the task organizers.\nThe goal of both sub-tasks was the following: predict the sentiment polarity and intensity for each of the companies/stocks mentioned in a short text instance (microblog message or news sentence). The sentiment score is a real continuous variable in the range of -1.0 (very negative/bearish) to +1.0 (very positive/bullish), with 0.0 designating neutral sentiment. Table 1 presents two examples from the training set. Task organizers provided 1700 microblog messages for training and 800 messages for testing in sub-task 5.1, while in sub-task 5.2, 1142 news sentences were provided for training and 491 for testing. Sub-\n1 https://github.com/saleiro/Financial-Sentiment-Analysis\nmissions were evaluated using the cosine similarity (Cortis et al., 2017)."
    }, {
      "heading" : "3 Financial Word Embeddings",
      "text" : "Mikolov et al. (2013a) created word2vec, a computational efficient method to learn distributed representation of words, where each word is represented by a distribution of weights (embeddings) across a fixed set of dimensions. Furthermore, Mikolov et al. (2013b) showed that this representation is able to encode syntactic and semantic similarities in the embedding space.\nThe training objective of the skip-gram model, defined by Mikolov et al. (2013b), is to learn the target word representation (embeddings) that maximize the prediction of its surrounding words in a context window. Given the wt word in a vocabulary the objective is to maximize the average log probability:\n1\nT\nT∑\nt=1\n∑\n−c≤j≤c,j 6=0\nlog P (wt+j |wt) (1)\nwhere c is the size of the context window, T is the total number of words in the vocabulary and wt+j is a word in the context window of wt. After training, a low dimensionality embedding matrix E encapsulates information about each word in the vocabulary and its use (surrounding contexts).\nWe used word2vec to learn word embeddings in the context of financial texts using unlabeled tweets and news headlines mentioning companies/stocks from S&P 500. Tweets were collected using the Twitter streaming API with cashtags of stocks titles serving as request parameters. Yahoo Finance API was used for requesting financial news feeds by querying the canonical name of companies/stocks. The datasets comprise a total of 1.7M tweets and 626K news titles.\nWe learned separate word embeddings for tweets and news headlines using the skip-gram model. We tried several configurations of word2vec hyperparameters. The setup resulting in the best performance in both sub-tasks was skipgram with 50 dimensions, removing words occurr ng less than 5 times, using a context window of\n5 words and 25 negative samples per positive example.\nEven though the text collections for training embeddings were relatively small, the resulting embedding space exhibited the ability to capture semantic word similarities in the financial context. We performed simple algebraic operations to capture semantic relations between words, as described in Mikolov et al. (2013c). For instance, the skip-gram model trained on tweets shows that vector (“bearish”) - vector(“loss”) + vector(“gain”) results in vector (“bullish”) as most similar word representation."
    }, {
      "heading" : "4 Approach",
      "text" : "In this section we describe the implementation details of the proposed approach."
    }, {
      "heading" : "4.1 Pre-Processing",
      "text" : "A set of pre-processing operations are applied to every microblog message and news sentence in the training/test sets of sub-tasks 5.1 and 5.2, as well as in the external collections for training word embeddings:\n• Character encoding and stopwords: every message and headline was encoded in UTF-\n8. Standard english stopword removal is also applied.\n• Company/stock and cash obfuscation: both cashtags and canonical company\nnames strings were replaced by the string\ncompany . Dollar or Euro signs followed by numbers were replaced by the string\ncash amount .\n• Mapping numbers and signs: numbers were mapped to strings using bins (0-10, 10-\n20, 20-50, 50-100, >100). Minus and plus signs were coverted to minus and plus, “B” and “M” to billions and millions, respectively. The % symbol was converted to percent. Question and exclamation marks were also converted to strings.\n• Tokenization, punctuation, lowercasing: tokenization was performed using Twok-\nenizer (Gimpel et al., 2011), the remaining punctuation was removed and all characters were converted to lowercase."
    }, {
      "heading" : "4.2 Features",
      "text" : "We combined three different group of features: bag-of-words, lexical-based features and bag-ofembeddings.\n• Bag-of-words: we apply standard bag-ofwords as features. We tried unigrams, bi-\ngrams and tri-grams with unigrams proving to obtain higher cosine similarity in both subtasks.\n• Sentiment lexicon features: we incorporate knowledge from manually curated sentiment\nlexicons for generic Sentiment Analysis as well as lexicons tailored for the financial domain. The Laughran-Mcdonald financial sentiment dictionary (Bodnaruk et al., 2015) has several types of word classes: positive, negative, constraining, litigious, uncertain and modal. For each word class we create a binary feature for the match with a word in a microblog/headline and a polarity score feature (positive - negative normalized by the text span length). As a general-purpose sentiment lexicon we use MPQA (Wilson et al., 2005) and created binary features for positive, negative and neutral words, as well as, the polarity score feature.\n• Bag-of-Embeddings: we create bag-ofembeddings by taking the average of word\nvectors for each word in a text span. We used the corresponding embedding matrix trained on external Twitter and Yahoo Finance collections for sub-task 5.1 and sub-task 5.2, respectively."
    }, {
      "heading" : "5 Experimental Setup",
      "text" : "In order to avoid overfitting we created a validation set from the original training datasets provided by the organizers. We used a 80%-20% split and sampled the validation set using the same distribution as the original training set. We sorted the examples in the training set by the target variable values and skipped every 5 examples. Results are evaluated using Cosine similarity (Cortis et al., 2017) and Mean Average Error (MAE). The former gives more importance to differences in the polarity of the predicted sentiment while the latter is concerned with how well the system predicts the intensity of the sentiment.\nWe opted to model both sub-tasks as single regression problems. Three different regressors\nwere applied: Random Forests (RF), Support Vector Machines (SVM) and MultiLayer Perceptron (MLP). Parameter tuning was carried using 10 fold cross validation on the training sets."
    }, {
      "heading" : "6 Results and Analysis",
      "text" : "In this section we present the experimental results obtained in both sub-tasks. We provide comparison of different learning algorithms using all features, as well as, a comparison of different subsets of features, to understand the information contained in each of them and also how they complement each other."
    }, {
      "heading" : "6.1 Task 5.1 - Microblogs",
      "text" : "Table 2 presents the results obtained using all features in both validation set and test sets. Results in the test set are worse than in the validation set with the exception to MLP. The official score obtained in sub-task 5.1 was 0.6948 using Random Forests (RF), which is the regressor that achieves higher cosine similarity and lower MAE in both training and validation set.\nWe compared the results obtained with different subsets of features using the best regressor, RF, as depicted in Table 3. Interestingly, bag-ofwords (BoW) and bag-of-embeddings (BoE) complement each other, obtaining better cosine similarity than the system using all features. Financial word embeddings (BoE) capture relevant information regarding the target variables. As a single group of features it achieves a cosine similarity of 0.6118 and MAE of 0.2322. It is also able to boost the overall performance of BoW with gains of more than 0.06 in cosine similarity and reducing MAE more than 0.03.\nThe individual group of features with best performance is Bag-of-words while the worst is a system trained using Lex (only lexical-based features). While Lex alone exhibits poor perfor-\nmance, having some value but marginal, when combined with another group of features, it improves the results of the latter, as in the case of BoE + Lex and BoW + Lex."
    }, {
      "heading" : "6.2 Task 5.2 - News Headlines",
      "text" : "Results obtained in news headlines are very different from the ones of the previous sub-task, proving that predicting sentiment polarity and intensity in news headlines is a complete different problem compared to microblogs. Table 4 shows that MLP obtains the best results in the test set using both metrics while SVR obtains the best performance in the validation set. The best regressor of sub-task 5.1, RF is outperformed by both SVR and MLP. The official result obtained at sub-task 5.2 was a cosine similarity of 0.68 using MLP.\nTable 5 shows the results of the different groups of features in sub-task 5.2 for MLP regressor. The most evident observation is that word embeddings are not effective in this scenario. On the other hand, lexical based features have significantly better performance in news headlines than in microblogs. Despite this, the best results are obtained using all features."
    }, {
      "heading" : "6.3 Analysis",
      "text" : "Financial word embeddings were able to encapsulate valuable information in sub-task 5.1 - Mi-\ncroblogs but not so much in the case of sub-task 5.2 - News Headlines. We hypothesize that as we had access to a much smaller dataset (∼ 600K) for training financial word embeddings for news headlines, this resulted in reduced ability to capture semantic similarities in the financial domain. Other related works in Sentiment Analysis usually take advantage of a much larger dataset for training word embeddings (Deriu et al., 2016).\nOn the other hand, lexical features showed poor performance in microblog texts but seem to be very useful using news headlines. The fact that microblogs have poor grammatically constructed texts, slang and informal language reveals that financial lexicals created using well written and formal financial reports, result better in news headlines rather than in microblog texts."
    }, {
      "heading" : "7 Conclusions",
      "text" : "Work reported in this paper is concerned with the problem of predicting sentiment polarity and intensity of financial short texts. Previous work showed that sentiment is often depicted in an implicit way in this domain. We created financialspecific continuous word representations in order to obtain domain specific syntactic and semantic relations between words. We combined traditional bag-of-words and lexical-based features with bagof-embeddings to train a regressor of both sentiment polarity and intensity. Results show that different combination of features attained different performances on each sub-task. Future work will consist on collecting larger external datasets for training financial word embeddings of both microblogs and news headlines. We also have planned to perform the regression analysis using Deep Neural Networks."
    } ],
    "references" : [ {
      "title" : "Using 10-k text to gauge financial constraints",
      "author" : [ "Andriy Bodnaruk", "Tim Loughran", "Bill McDonald." ],
      "venue" : "Journal of Financial and Quantitative Analysis 50(04).",
      "citeRegEx" : "Bodnaruk et al\\.,? 2015",
      "shortCiteRegEx" : "Bodnaruk et al\\.",
      "year" : 2015
    }, {
      "title" : "Semeval-2017 task 5: Fine-grained sentiment analysis on financial microblogs and news",
      "author" : [ "Keith Cortis", "Andre Freitas", "Tobias Daudert", "Manuela Huerlimann", "Manel Zarrouk", "Brian Davis" ],
      "venue" : "Proceedings of SemEval ",
      "citeRegEx" : "Cortis et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Cortis et al\\.",
      "year" : 2017
    }, {
      "title" : "Swisscheese at semeval-2016 task 4: Sentiment classification using an ensemble of convolutional neural networks with distant supervision",
      "author" : [ "J. Deriu", "M. Gonzenbach", "F. Uzdilli", "A. Lucchi", "V. De Luca", "M. Jaggi." ],
      "venue" : "Proceedings of SemEval .",
      "citeRegEx" : "Deriu et al\\.,? 2016",
      "shortCiteRegEx" : "Deriu et al\\.",
      "year" : 2016
    }, {
      "title" : "Part-ofspeech tagging for twitter: Annotation, features, and experiments",
      "author" : [ "K. Gimpel", "N. Schneider", "B. O’Connor", "D. Das", "D. Mills", "J. Eisenstein", "M. Heilman", "D. Yogatama", "J. Flanigan", "Noah A Smith" ],
      "venue" : "In ACL HLT: short papers-Volume",
      "citeRegEx" : "Gimpel et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Gimpel et al\\.",
      "year" : 2011
    }, {
      "title" : "Sentiment analysis and opinion mining",
      "author" : [ "Bing Liu." ],
      "venue" : "Synthesis lectures on human language technologies 5(1).",
      "citeRegEx" : "Liu.,? 2012",
      "shortCiteRegEx" : "Liu.",
      "year" : 2012
    }, {
      "title" : "Efficient estimation of word representations in vector space",
      "author" : [ "Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean." ],
      "venue" : "arXiv preprint arXiv:1301.3781 .",
      "citeRegEx" : "Mikolov et al\\.,? 2013a",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean." ],
      "venue" : "NIPS.",
      "citeRegEx" : "Mikolov et al\\.,? 2013b",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Linguistic regularities in continuous space word representations",
      "author" : [ "Tomas Mikolov", "Wen-tau Yih", "Geoffrey Zweig." ],
      "venue" : "Hlt-naacl. volume 13.",
      "citeRegEx" : "Mikolov et al\\.,? 2013c",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "The impact of valence shifters on mining implicit economic opinions",
      "author" : [ "Claudiu Musat", "Stefan Trausan-Matu." ],
      "venue" : "International Conference on Artificial Intelligence: Methodology, Systems, and Applications. Springer.",
      "citeRegEx" : "Musat and Trausan.Matu.,? 2010",
      "shortCiteRegEx" : "Musat and Trausan.Matu.",
      "year" : 2010
    }, {
      "title" : "Walking down wall street with a tablet: A survey of stock market predictions using the web",
      "author" : [ "Michela Nardo", "Marco Petracco-Giudici", "Mins Naltsidis." ],
      "venue" : "Journal of Economic Surveys 30(2).",
      "citeRegEx" : "Nardo et al\\.,? 2016",
      "shortCiteRegEx" : "Nardo et al\\.",
      "year" : 2016
    }, {
      "title" : "Stream-based active learning for sentiment analysis in the financial domain",
      "author" : [ "Jasmina Smailović", "Miha Grčar", "Nada Lavrač", "Martin Žnidaršič." ],
      "venue" : "Information Sciences 285.",
      "citeRegEx" : "Smailović et al\\.,? 2014",
      "shortCiteRegEx" : "Smailović et al\\.",
      "year" : 2014
    }, {
      "title" : "Fine-grained analysis of explicit and implicit sentiment in financial news articles",
      "author" : [ "Marjan Van de Kauter", "Diane Breesch", "Véronique Hoste." ],
      "venue" : "Expert Systems with applications 42(11).",
      "citeRegEx" : "Kauter et al\\.,? 2015",
      "shortCiteRegEx" : "Kauter et al\\.",
      "year" : 2015
    }, {
      "title" : "Recognizing contextual polarity in phraselevel sentiment analysis",
      "author" : [ "Theresa Wilson", "Janyce Wiebe", "Paul Hoffmann." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Wilson et al\\.,? 2005",
      "shortCiteRegEx" : "Wilson et al\\.",
      "year" : 2005
    } ],
    "referenceMentions" : [ {
      "referenceID" : 9,
      "context" : "Sentiment Analysis on financial texts has received increased attention in recent years (Nardo et al., 2016).",
      "startOffset" : 87,
      "endOffset" : 107
    }, {
      "referenceID" : 10,
      "context" : "Neverthless, there are some challenges yet to overcome (Smailović et al., 2014).",
      "startOffset" : 55,
      "endOffset" : 79
    }, {
      "referenceID" : 8,
      "context" : "Economic indicators, such as unemployment and future state modifiers such as drop or increase can also provide clues on the implicit sentiment (Musat and Trausan-Matu, 2010).",
      "startOffset" : 143,
      "endOffset" : 173
    }, {
      "referenceID" : 4,
      "context" : "Contrary to explicit expressions (subjective utterances), factual text types often contain objective statements that convey a desirable or undesirable fact (Liu, 2012).",
      "startOffset" : 156,
      "endOffset" : 167
    }, {
      "referenceID" : 1,
      "context" : "The task 5 of SemEval 2017 (Cortis et al., 2017) consisted of fine-grained sentiment analysis of financial short texts and it was divided in two subtasks based on the type of text.",
      "startOffset" : 27,
      "endOffset" : 48
    }, {
      "referenceID" : 1,
      "context" : "com/saleiro/Financial-Sentiment-Analysis missions were evaluated using the cosine similarity (Cortis et al., 2017).",
      "startOffset" : 93,
      "endOffset" : 114
    }, {
      "referenceID" : 5,
      "context" : "We performed simple algebraic operations to capture semantic relations between words, as described in Mikolov et al. (2013c). For instance, the skip-gram model trained on tweets shows that vector (“bearish”) - vector(“loss”) + vector(“gain”) results in vector (“bullish”) as most similar word representation.",
      "startOffset" : 102,
      "endOffset" : 125
    }, {
      "referenceID" : 3,
      "context" : "• Tokenization, punctuation, lowercasing: tokenization was performed using Twokenizer (Gimpel et al., 2011), the remaining punctuation was removed and all characters were converted to lowercase.",
      "startOffset" : 86,
      "endOffset" : 107
    }, {
      "referenceID" : 0,
      "context" : "The Laughran-Mcdonald financial sentiment dictionary (Bodnaruk et al., 2015) has several types of word classes: positive, negative, constraining, litigious, uncertain and modal.",
      "startOffset" : 53,
      "endOffset" : 76
    }, {
      "referenceID" : 12,
      "context" : "As a general-purpose sentiment lexicon we use MPQA (Wilson et al., 2005) and created binary features for positive, negative and neutral words, as well as, the polarity score feature.",
      "startOffset" : 51,
      "endOffset" : 72
    }, {
      "referenceID" : 1,
      "context" : "Results are evaluated using Cosine similarity (Cortis et al., 2017) and Mean Average Error (MAE).",
      "startOffset" : 46,
      "endOffset" : 67
    }, {
      "referenceID" : 2,
      "context" : "Other related works in Sentiment Analysis usually take advantage of a much larger dataset for training word embeddings (Deriu et al., 2016).",
      "startOffset" : 119,
      "endOffset" : 139
    } ],
    "year" : 2017,
    "abstractText" : "This paper presents the approach developed at the Faculty of Engineering of University of Porto, to participate in SemEval 2017, Task 5: Fine-grained Sentiment Analysis on Financial Microblogs and News. The task consisted in predicting a real continuous variable from -1.0 to +1.0 representing the polarity and intensity of sentiment concerning companies/stocks mentioned in short texts. We modeled the task as a regression analysis problem and combined traditional techniques such as pre-processing short texts, bag-of-words representations and lexical-based features with enhanced financial specific bag-ofembeddings. We used an external collection of tweets and news headlines mentioning companies/stocks from S&P 500 to create financial word embeddings which are able to capture domain-specific syntactic and semantic similarities. The resulting approach obtained a cosine similarity score of 0.69 in sub-task 5.1 Microblogs and 0.68 in sub-task 5.2 News Headlines.",
    "creator" : "LaTeX with hyperref package"
  }
}