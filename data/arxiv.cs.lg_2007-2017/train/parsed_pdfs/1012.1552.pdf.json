{
  "name" : "1012.1552.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Bridging the Gap between Reinforcement Learning and Knowledge Representation: A Logical Off- and On-Policy Framework",
    "authors" : [ "Emad Saad" ],
    "emails" : [ "saad.e@gust.edu.kw" ],
    "sections" : [ {
      "heading" : null,
      "text" : "reinforcement learning. In this paper, we bridge the gap between reinforcement learning and knowledge representation, by providing a rich knowledge representation framework, based on normal logic programs with answer set semantics, that is capable of solving model-free reinforcement learning problems for more complex domains and exploits the domain-specific knowledge. We prove the correctness of our approach. We show that the complexity of finding an offline and online policy for a model-free reinforcement learning problem in our approach is NP-complete. Moreover, we show that any model-free reinforcement learning problem in MDP environment can be encoded as a SAT problem. The importance of that is model-free reinforcement learning problems can be now solved as SAT problems."
    }, {
      "heading" : "1 Introduction",
      "text" : "Reinforcement learning is the problem of learning to act by trial and error interaction in dynamic environments. Under the assumption that a complete model of the environment is known, a reinforcement learning problem is modeled as a Markov Decision Process (MDP), in which an optimal policy can be learned. Operation research methods, in particular dynamic programming by value iteration, have been extensively used to learn the optimal policy for a reinforcement learning problem in MDP environment. However, an agent may not know the model of the environment. In addition, an agent may not be able to consider all possibilities and use its knowledge to plan ahead, because of the agent’s limited computational abilities to consider all states systematically [4]. Therefore, Q-learning [4] and SARSA [17] are proposed as model-free reinforcement learning algorithms that learn optimal policies without the need for the agent to know the model of the environment. Q-learning and SARSA are incremental dynamic programming algorithms, that learns optimal policy from actual experience from interaction with the environment, where to guarantee convergence the following assumptions must hold; the action-value function is represented as a look-up table; the environment is a deterministic MDP; for each starting state and action, there are an infinite number of episodes; and the learning rate is decreased appropriately over time. However, these assumptions imply that all actions are tried in every possible state and every state must be visited infinitely many times, which leads to a slow convergence, although, it is sufficient for the agent to try all possible actions in every possible state only 1 Department of Computer Science, Gulf University for Science and Tech-\nnology, Kuwait, email: saad.e@gust.edu.kw\nonce to learn about the reinforcements resulting from executing actions in states. In addition, in some situations it is not possible for the agent to visit a state more than once. Consider a deer that eats in an area where a cheetah appears and the deer flees and survived. If the deer revisits this area again it will be eaten and does not learn anymore. This is unavoidable in Q-learning and SARSA because of the iterative dynamic programming approach they adopt and their convergence assumptions. Moreover, dynamic programming methods use primitive representation of states and actions and do not exploit domain-specific knowledge of the problem domain, in addition they solve MDP with relatively small domain sizes [16]. However, using richer knowledge representation frameworks for MDP allow to efficiently find optimal policies in more complex and larger domains. A logical framework to model-based reinforcement learning has been proposed in [19] that overcomes the representational limitations of dynamic programming methods and capable of representing domain specific knowledge. The framework in [19] is based on the integration of model-based reinforcement learning in MDP environment with normal hybrid probabilistic logic programs with probabilistic answer set semantics [23] that allows representing and reasoning about a variety of fundamental probabilistic reasoning problems including probabilistic planning [18], contingent probabilistic planning [21], the most probable explanation in belief networks, and the most likely trajectory [20]. In this paper we integrate model-free reinforcement learning with normal logic programs with answer set semantics and SAT, providing a logical framework to model-free reinforcement learning using Qlearning and SARSA update rules to learn the optimal off- and onpolicy respectively. This framework is considered a model-free extension to the model-based reinforcement learning framework of [19]. The importance of the proposed framework is twofold. First, the proposed framework overcomes the representational limitations of dynamic programming methods to model-free reinforcement learning and capable of representing domain-specific knowledge, and hence bridges the gap between reinforcement learning and knowledge representation. Second, it eliminates the requirement of visiting every state infinitely many times which is required for the convergence of the Q-learning and SARSA. This integration is achieved by encoding the representation of a model-free reinforcement learning problem in a new high level action\nlanguage we develop in this paper called, , into normal logic\nprogram with answer set semantics, where all actions are tried in every state only once. We show the correctness of the translation. We prove that the complexity of finding an off- and on-policy in our ap-\nproach is NP-complete. In addition, we show that any model-free reinforcement learning problem in MDP environment can be encoded as SAT problem."
    }, {
      "heading" : "2 Preliminaries",
      "text" : "As in the underlying assumptions of the original Q-learning and SARSA, the subsequent results in the rest of this paper assume that the considered MDPs are deterministic. Normal logic programs [7] and Q-learning [4] and SARSA [17] are reviewed in this section."
    }, {
      "heading" : "2.1 Normal Logic Programs",
      "text" : "Let be a first-order language with finitely many predicate\nsymbols, function symbols, constants, and infinitely many variables. The Herbrand base of is denoted by . A Herbrand\ninterpretation is a subset of the Herbrand base . A normal logic\nprogram is a finite set of rules of the form\nWhere are atoms and is the negation-as-failure. A normal logic program is ground if no variables appear in any of its rules. Let be a ground normal logic program and be a Herbrand interpretation, then, we say that the above rule is satisfied by iff , whenever and , or for some ,\nA Herbrand model of is a Herbrand interpretation that satisfies every rule in . A Herbrand interpretation of a normal logic program is said to be an answer set of if is the minimal Herbrand model (with respect to the set inclusion) of the reduct, denoted by , where"
    }, {
      "heading" : "2.2 Q-learning and SARSA",
      "text" : "Q-learning learns the optimal Q-function, , from the agent’s experience (set of episodes) by repeatedly estimating the optimal Qvalue for every state-action pair . The Q-value, , given a policy (a mapping from states to actions), is defined as the expected sum of discounted rewards resulting from executing the action in a state and then following the policy thereafter. Given\n, an optimal policy, , can be determined by identifying the optimal action in every state, where is optimal in a state , i.e., and is executable in . An episode is an exploration of the environment which is a sequence of state-action-reward-state of the form\n, where for each means that an agent executed\naction in state and rests in state where it received reward . denotes an initial state and is a terminal (goal) state. Given that the agent sufficiently explored the environment, the optimal Qvalues are repeatedly estimated by the following algorithm:\ninitialize arbitrary Repeat forever for each episode\nSelect the initial state of an episode\nRepeat\nChoose an action for the current state Execute the action in Observe the subsequent state Receive an immediate reward\nSet Until is the end of an episode\nwhere is the learning rate, is the discount factor, and\nis the reward received in from executing in . Q-learning is an offline algorithm that learns the optimal Qfunction while executing another policy. Under the same convergence assumptions as in Q-learning, SARSA [17] has been developed as an online model-free reinforcement learning algorithm, that learns optimal Q-function while exploring the environment. Similar to Q-learning, SARSA is an iterative dynamic programming algorithm whose update rule is given by:\nIn addition, SARSA converges slowly to Q∗, since it requires every state to be visited infinitely many times with all actions are tried. Although, it is sufficient for an agent to try all possible actions in every possible state only once to learn about the reinforcements resulting from executing every possible action in every possible state. This assumption could not be eliminated in Qlearning and SARSA, since both are iterative dynamic programming algorithms. However, under the assumption that the environment is finite-horizon Markov decision process with finite length episodes, estimating the optimal Q-function, for Qlearning, can be simply computed recursively as:\nSimilarly, the estimate of the optimal Q-function for SARSA can be described as:\nEquations (1) and (2) show that it is sufficient to consider the rewards collected from the set of all episodes, , only once to calculate estimate of the optimal Q-function, , which eliminates the need to visit every possible state infinitely many times.\nUnlike Q-learning, our estimate of , can be computed online as well as offline. It can be computed online by accumulating estimate of\nduring the exploration of the environment. On the other hand, it can be computed offline by first exploring the environment and collecting the set of all possible episodes, then computing estimate of ."
    }, {
      "heading" : "3 Action Language",
      "text" : "This section develops the syntax and semantics of the action lan-\nguage, , that allows the representation of model-free reinforce-\nment learning problems, which extends the action language [8]."
    }, {
      "heading" : "3.1 Language syntax",
      "text" : "A fluent is a predicate, which may contain variables, that describes a property of the environment. Let be a set of fluents and be a set of action names that can contain variables. A fluent literal is either a fluent , the negation of . Conjunctive fluent formula is a conjunction of fluent literals of the form , where are fluent literals. Sometimes we abuse the notation and refer to a conjunctive fluent formula as a set of fluent literals ( ). An action theory, is a tuple of the form , where is a proposition of the form (3), is a set of propositions from (4-6), and is a discount factor as follows:\nwhere is a fluent literal, are conjunctive fluent formulas, is an action, and is a real number in . Proposition (3) represents the set of possible initial states. Proposition (4) states that an action is executable in any state in which holds, where each variable that appears in also appears in . Indirect effect of action is described by proposition (5), which says that holds in every state in which also holds. A proposition of the form (6) represents the conditional effects of an action along with the rewards received in a state resulting from executing . All variables that appear in also appear in and . Proposition (6) says that causes to hold with reward is received in a successor state to a state in which is executed and holds. An action theory is ground if it does not contain any variables. Example 1 Consider an elevator of n-story building domain adapted from [5] that is represented by an action theory, is described by (7) ( is a particular value in ) and is represented by (8)-(14).\nThe actions in the elevator domain are for move up to floor , for move down to floor , and for closing the elevator door. The predicates are\nfluents represent respectively that the elevator current floor is ,\nlight of floor is on, and elevator door is opened. The target is to\nget all floors serviced and is true for all ."
    }, {
      "heading" : "3.2 Semantics",
      "text" : "We say a set of ground literals is consistent if it does not contain a pair of complementary literals. If a literal , then we say holds in , and does not hold in if . A set of literals holds in if is contained in , otherwise, does not hold in . We say that a set of literals satisfies an indirect effect of action of the form (5), if belongs to whenever is contained in or is not contained in . Let be an action theory in and be a set of literals. Then is the smallest set of literals that contains and satisfies all indirect effects of actions propositions in . A state is a complete and consistent set of literals that satisfies all the indirect effects of actions propositions in . Definition 1 Let be a ground action theory in ,\nbe a state, be a proposition in . Then, is the state resulting from executing in , given\nthat is executable in , where is defined as:\nwhere the reward received in .\nAn episode in is an expression of the form\n, where for each\nDefinition 2 Let be a ground action theory and be the set of all episodes in . Then, for\n, the optimal Q-function, , for Q-learning and SARSA are respectively estimated by\nConsidering SARSA, the optimal Q-function can be computed incrementally as follows. For any episode in , the optimal Q-value for the initial state-action pair is estimated by\nthat is calculated online during the exploration of the environment. Then, for any state-action pair, , in the episode, , is calculated from by\nHowever, for Q-learning, can be computed incrementally as well by first computing incrementally using (15), then (16) is used as an update rule only once, where for\nNotice that, unlike [4], by using (15) and (16), Q-learning can be computed online during the exploration of the environment as well as offline."
    }, {
      "heading" : "4 Off- and On-Policy Model-free Reinforcement Learning Using Answer Set Programming",
      "text" : "We provide a translation from any action theory , a representation of a model-free reinforcement learning problem into\na normal logic program with answer set semantics , where the rules in encode (1) the set of possible initial states , (2) the transition function , (3) the set of propositions in , (4) and the discount factor . The answer sets of correspond to episodes in\n, with associated estimated optimal Q-values. This translation follows some related translations described in [24, 18, 19].\nWe assume the environment is a finite-horizon Markov decision process, where the length of each episode is known and finite. We use the predicates; to represent a literal holds at time moment ; for action executes at time ;\nfor reward received at time after executing is ; says the estimate of the optimal Q-value of the initial state-\naction pair, in a given episode, steps from the initial state is ; and\nfor the discount factor. We use lower case letters to\nrepresent constants and upper case letters to represent variables. Let be the normal logic program translation of\nthat contains a set of rules described as follows. To\nsimplify the presentation, given is a predicate and\nbe a set of literals, we use to denote\nFor each action includes the set of facts\nLiterals describe states of the world are encoded by\nwhere is a set of facts that describe the properties of the\nworld. To specify that are contrary literals the\nfollowing rules are added to .\nThe reward received at time after executing at time\ngiven that is executable is encoded by\nEstimate of the optimal Q-value of an initial state-action pair, in a given episode, steps away from the initial state, is equal to the estimate of the optimal Q-value of the same initial state-action pair, in the same episode, steps away from the initial state added\nto the discounted reward (by ) received at time , where\nThe following rule says that holds at the time moment if it holds at the time moment and its contrary does not hold at the time moment .\nA literal and its negation cannot hold at the same time is\nencoded in by\nRules that generate actions occurrences once at a time are\nencoded by\nThe set of initial states, , is encoded as\nfollows. Let be the set of initial states, where for\n. Moreover, let\n,\nIntuitively, for any literal belongs to , then\ncontains only . For each literal includes\nwhich says holds at time 0. Literals in belong to every initial\nstate. For each includes\nwhich says that (similarly ) holds at time 0, if (similarly )\ndoes not hold at the time 0.\nEach proposition of the form (4) is encoded in as\nLet be a goal expression, then is encoded\nin as\nEstimates of the optimal Q-value of initial state-action pair, , is represented in , where represents the estimate of at the end of episode of length . These Q-values, , can be computed online during the exploration of the environment as well as offline after the exploration of the environment. Moreover, the action generation rules (31) and (32) in our translation, choose actions greedily at random. However, other action selection strategies can be encoded instead. Example 2 The normal logic program encoding, , of the elevator domain described in Example 1 is given as follows, where consists of the following rules, along with the rules (18), (19), (20), (21), (29), (30), (31), (32):\nEach is encoded as\n, which says that if occurs at time\nholds at the same time moment, then holds at time\nfor The atoms\ndescribe properties of the world that for are encoded\nas\nThe initial state is encoded as follows, where , for\nand for some\nThe executability conditions of actions, for , are encoded as\nEffects, rewards, and the Q-value of the initial state-action pair\nresulting after executing the actions and ,\nfor , are given by\nwhere is a fact,\n, and\nEffects of the action is given by\nThe reward received after executing is given by\nQ-value of the initial state-action pair is given by the following rule, where is a fact.\nThe goal is encoded by the following rule for some"
    }, {
      "heading" : "5 Correctness",
      "text" : "This section shows the correctness of our translation. We prove that the answer sets of the normal logic program translation of an action theory, in , correspond to episodes in , associated with estimates of the optimal Q-values. Moreover, we show that the complexity of finding a policy for in our approach\nis NP-complete. Let the domain of be . Let be a\ntransition function associated with is an initial state, and be a set of actions in . An episode in is stateaction-reward-state sequence of the form , such that\n, are states, is an action,\n, and .\nTheorem 1 Let be an action theory representing a model-free\nreinforcement learning problem in . Then,\nis an episode in iff\nTheorem 1 says that an action theory, , in , can be translated into a normal logic program, , such that an answer set of is equivalent to an episode in .\nTheorem 2 Let be an action theory in be an answer set of\nbe the set of all episodes in . Let be a set such\nthat iff\nTheorem 2 asserts that, given an action theory and by\nconsidering Q-learning update rule, the expected sum of discounted rewards resulting after executing an action in a state and\nfollowing the optimal policy thereafter, , is equal to the\nmaximum over , appearing in which is satisfied by\nevery answer set for which\nis\nalso satisfied. However, by considering the update rule of SARSA,\nis equal to in that is satisfied by some\nanswer set of for which is also satisfied. For any and in\nSARSA, is calculated from by (15), where\nBut, for Q-learning,\nfor any and , is calculated from by (15), then (16) is used as an update rule only once. In addition, we show that any model-free reinforcement learning problem in MDP environment can be encoded as SAT problem. Hence, state-of-the-art SAT solvers can be used to solve model-free reinforcement learning problems. Any normal logic program, ,\ncan be translated into a SAT problem, , where the models of\nare equivalent to the answer sets of [13]. Hence, the normal logic\nprogram encoding of a model-free reinforcement learning problem\ncan be translated into an equivalent SAT problem, where the\nmodels of S correspond to episodes in . Theorem 3 Let be an action theory in and be the normal logic program encoding of . Then, the models of the SAT encoding of are equivalent to valid episodes in . For SAT encoding, the optimal Q-function is computed in a similar way as in the normal logic program encoding of model-free reinforcement learning problems. The transformation step from nor-mal logic program encoding of a model-free reinforcement learning problem into SAT can be avoided, by encoding a modelfree reinforcement learning problem directly into SAT [22]. The following corollary shows any model-free reinforcement learning problem can be encoded directly as SAT problem. Corollary 1 Let be an action theory in . Then, can be\ndirectly encoded as a SAT formula where the models of are\nequivalent to valid episodes in . Normal logic programs with answer set semantics find optimal policies for model-free reinforcement learning problems in finite horizon MDP environments using the flat representation of the problem domains.\nThe flat representation of reinforcement learning problem domains is the explicit enumeration of world states [14]. Hence, Theorem 5 follows directly from Theorem 4 [14]. Theorem 4 The stationary policy existence problem for finitehorizon MDP in the flat representation is NP-complete. Theorem 5 The policy existence problem for a model-free\nreinforcement learning problem in MDP environment using normal\nlogic pro-grams with answer set semantics and SAT is NP-complete."
    }, {
      "heading" : "6 Conclusions and Related Work",
      "text" : "We described a high level action language called that allows\nthe representation of model-free reinforcement learning problems in MDP environments. In addition, we introduced online and offline logical framework to model-free reinforcement learning by relating model-free reinforcement learning in MDP environment to normal logic programs with answer set semantics and SAT. The translation from an action theory in into a normal logic program builds on similar translations described in [24, 18, 19]. The literature is rich with action languages that are capable of represent-ing and reasoning about MDPs and actions with probabilistic effects, which include [1, 2, 6, 9, 12]. The main difference between these languages and is that allows the factored characterization of MDP for model-free reinforcement learning. Many approaches for solving MDP to find the optimal policy for both reinforcement learning and probabilistic planning have been presented. These approaches can be classified into two main categories of approaches; dynamic programming approaches and the search-based approaches (a detailed survey on these approaches can be found in [10, 2]). However, dynamic programming approaches use primitive domain knowledge representation. On the other hand, the search-based approaches mainly rely on search heuristics which have limited knowledge representation capabilities to represent and use domain-specific knowledge. A logic based approach for solving MDP, for probabilistic plan-ning, has been presented in [15]. The approach of [15] converts MDP specification of a probabilistic planning problem into a stochastic satisfiability problem and solving the stochastic satisfiability problem instead. First-order logic representation of MDP for model-based reinforcement learning has been described in [11] based on first-order logic programs without nonmonotonic negations. Similar to the firstorder representation of MDP in [11], allows objects and relations.\nHowever, unlike , [11] finds policies in the abstract level. A more\nexpressive first-order representation of MDP than [11] has been presented in [3] that is a probabilistic extension to Reiter’s situation calculus. Although more expressive, it is more complex than [11]. Unlike the logical model-based reinforcement learning frame-work of [19] that uses normal hybrid probabilistic logic programs to encode model-based reinforcement learning problems, normal logic program with answer set semantics is used to encode model-free reinforcement learning problems."
    } ],
    "references" : [ {
      "title" : "Reasoning about actions in a probabilistic setting",
      "author" : [ "C. Baral", "N. Tran", "L.C. Tuan" ],
      "venue" : "AAAI, ",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Decision-theoretic planning: structural assumptions and computational leverage",
      "author" : [ "C. Boutilier", "T. Dean", "S. Hanks" ],
      "venue" : "Journal of AI Research, 11(1), ",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Symbolic dynamic programming for first-order mdps",
      "author" : [ "C. Boutilier", "R. Reiter", "B. Price" ],
      "venue" : "17th IJCAI, ",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Learning from delayed rewards",
      "author" : [ "Christopher C. Watkins" ],
      "venue" : "Ph.D. dissertation, University of Cambridge,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1989
    }, {
      "title" : "Improving elevator performance using reinforcement learning",
      "author" : [ "R. Crites", "A. Barto" ],
      "venue" : "Advances in Neural Information Processing, ",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "probabilistic reasoning about actions in nonmonotonic causal theories",
      "author" : [ "T. Eiter", "T. Lukasiewicz" ],
      "venue" : "19th Conference on UAI, ",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "The stable model semantics for logic programming",
      "author" : [ "M. Gelfond", "V. Lifschitz" ],
      "venue" : "ICSLP. MIT Pres, ",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 1988
    }, {
      "title" : "Action languages",
      "author" : [ "M. Gelfond", "V. Lifschitz" ],
      "venue" : "Electronic Transactions on AI, 3(16), 193–210, ",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Reasoning about actions with sensing under qualitative and probabilistic uncertainty",
      "author" : [ "L. Iocchi", "T. Lukasiewicz", "D. Nardi", "R. Rosati" ],
      "venue" : "16th European Conference on Artificial Intelligence , ",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Reinforcement learning: A survey",
      "author" : [ "L. Kaelbling", "M. Littman", "A. Moore" ],
      "venue" : "JAIR, 4, 237–285, ",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "Logical markov decision programs and the convergence of logical td(λ)",
      "author" : [ "K. Kersting", "L. De Raedt" ],
      "venue" : "14th International Conference on Inductive Logic Programming, ",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "An algorithm for probabilistic planning",
      "author" : [ "N. Kushmeric", "S. Hanks", "D. Weld" ],
      "venue" : "Artificial Intelligenc , 76(1-2), 239–286, ",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "Assat: Computing answer sets of a logic program by sat solvers",
      "author" : [ "F. Lin", "Y. Zhao" ],
      "venue" : "Artificial Intelligence , 157(1-2), 115–137, ",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "The computational com-plexity of probabilistic planning",
      "author" : [ "M. Littman", "J. Goldsmith", "M. Mundhenk" ],
      "venue" : "Journal of Artificial Intelligence Re-search, 9, 1–36, ",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Maxplan: A new approach to probabilistic planning",
      "author" : [ "S. Majercik", "M. Littman" ],
      "venue" : "Fourth International Conference on Artificial Intelligence Planning, pp. 86–93, ",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Contingent planning under uncertainty via stochastic satisfiability",
      "author" : [ "S. Majercik", "M. Littman" ],
      "venue" : "Artificial Intelligence , 147(1-2), 119– 162, ",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Online q-learning using connectionist systems",
      "author" : [ "G. Rummery", "M. Niranjan" ],
      "venue" : "Technical report, CUED/F-INFENG/TR166, Cambridge University, ",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "Probabilistic planning in hybrid probabilistic logic programs",
      "author" : [ "E. Saad" ],
      "venue" : "1st International Conference on Scalable Uncertainty Management, ",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "A logical framework to reinforcement learning using hybrid probabilistic logic programs",
      "author" : [ "E. Saad" ],
      "venue" : "Second International Conference on Scalable Uncertainty Management, ",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "On the relationship between hybrid probabilistic logic programs and stochastic satisfiability",
      "author" : [ "E. Saad" ],
      "venue" : "Second International Conference on Scalable Uncertainty Management, ",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Probabilistic planning with imperfect sensing actions using hybrid probabilistic logic programs",
      "author" : [ "E. Saad" ],
      "venue" : "Third International Conference on Scalable Uncertainty Management, ",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Probabilistic reasoning by sat solvers",
      "author" : [ "E. Saad" ],
      "venue" : "Tenth ECSQARU, ",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "A new approach to hybrid probabilistic logic programs",
      "author" : [ "E. Saad", "E. Pontelli" ],
      "venue" : "Annals of Mathematics and Artificial Intelligence , 48(3- 4), 187–243, ",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Domain-dependent knowl-edge in answer set planning",
      "author" : [ "T. Son", "C. Baral", "T. Nam", "S. McIlraith" ],
      "venue" : "ACM Transactions on Computational Logic, 7(4), 613–657, ",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2006
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "of the agent’s limited computational abilities to consider all states systematically [4].",
      "startOffset" : 85,
      "endOffset" : 88
    }, {
      "referenceID" : 3,
      "context" : "Therefore, Q-learning [4] and SARSA [17] are proposed as model-free reinforcement learning algorithms that learn",
      "startOffset" : 22,
      "endOffset" : 25
    }, {
      "referenceID" : 16,
      "context" : "Therefore, Q-learning [4] and SARSA [17] are proposed as model-free reinforcement learning algorithms that learn",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 15,
      "context" : "Moreover, dynamic programming methods use primitive representation of states and actions and do not exploit domain-specific knowledge of the problem domain, in addition they solve MDP with relatively small domain sizes [16].",
      "startOffset" : 219,
      "endOffset" : 223
    }, {
      "referenceID" : 18,
      "context" : "A logical framework to model-based reinforcement learning has been proposed in [19] that overcomes the representational limitations of dynamic programming methods and capable of representing domain specific knowledge.",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 18,
      "context" : "The framework in [19] is based on the integration of model-based reinforcement learning in MDP environment with normal hybrid probabilistic logic programs with probabilistic answer set semantics [23] that allows representing and reasoning about a variety of fundamental probabilistic reasoning problems including probabilistic planning [18], contingent probabilistic planning [21], the most probable explanation in belief networks, and the most likely trajectory [20].",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 22,
      "context" : "The framework in [19] is based on the integration of model-based reinforcement learning in MDP environment with normal hybrid probabilistic logic programs with probabilistic answer set semantics [23] that allows representing and reasoning about a variety of fundamental probabilistic reasoning problems including probabilistic planning [18], contingent probabilistic planning [21], the most probable explanation in belief networks, and the most likely trajectory [20].",
      "startOffset" : 195,
      "endOffset" : 199
    }, {
      "referenceID" : 17,
      "context" : "The framework in [19] is based on the integration of model-based reinforcement learning in MDP environment with normal hybrid probabilistic logic programs with probabilistic answer set semantics [23] that allows representing and reasoning about a variety of fundamental probabilistic reasoning problems including probabilistic planning [18], contingent probabilistic planning [21], the most probable explanation in belief networks, and the most likely trajectory [20].",
      "startOffset" : 336,
      "endOffset" : 340
    }, {
      "referenceID" : 20,
      "context" : "The framework in [19] is based on the integration of model-based reinforcement learning in MDP environment with normal hybrid probabilistic logic programs with probabilistic answer set semantics [23] that allows representing and reasoning about a variety of fundamental probabilistic reasoning problems including probabilistic planning [18], contingent probabilistic planning [21], the most probable explanation in belief networks, and the most likely trajectory [20].",
      "startOffset" : 376,
      "endOffset" : 380
    }, {
      "referenceID" : 19,
      "context" : "The framework in [19] is based on the integration of model-based reinforcement learning in MDP environment with normal hybrid probabilistic logic programs with probabilistic answer set semantics [23] that allows representing and reasoning about a variety of fundamental probabilistic reasoning problems including probabilistic planning [18], contingent probabilistic planning [21], the most probable explanation in belief networks, and the most likely trajectory [20].",
      "startOffset" : 463,
      "endOffset" : 467
    }, {
      "referenceID" : 18,
      "context" : "[19].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 6,
      "context" : "Normal logic programs [7] and Q-learning [4] and SARSA [17] are reviewed in this section.",
      "startOffset" : 22,
      "endOffset" : 25
    }, {
      "referenceID" : 3,
      "context" : "Normal logic programs [7] and Q-learning [4] and SARSA [17] are reviewed in this section.",
      "startOffset" : 41,
      "endOffset" : 44
    }, {
      "referenceID" : 16,
      "context" : "Normal logic programs [7] and Q-learning [4] and SARSA [17] are reviewed in this section.",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 16,
      "context" : "Under the same convergence assumptions as in Q-learning, SARSA [17] has been developed as an online model-free reinforcement learning algorithm, that learns optimal Q-function while exploring the environment.",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 7,
      "context" : "ment learning problems, which extends the action language [8].",
      "startOffset" : 58,
      "endOffset" : 61
    }, {
      "referenceID" : 4,
      "context" : "Example 1 Consider an elevator of n-story building domain adapted from [5] that is represented by an action theory, is described by (7) ( is a particular value in ) and is represented by (8)-(14).",
      "startOffset" : 71,
      "endOffset" : 74
    }, {
      "referenceID" : 3,
      "context" : "Notice that, unlike [4], by using (15) and (16), Q-learning can be computed online during the exploration of the environment as well as offline.",
      "startOffset" : 20,
      "endOffset" : 23
    }, {
      "referenceID" : 23,
      "context" : "This translation follows some related translations described in [24, 18, 19].",
      "startOffset" : 64,
      "endOffset" : 76
    }, {
      "referenceID" : 17,
      "context" : "This translation follows some related translations described in [24, 18, 19].",
      "startOffset" : 64,
      "endOffset" : 76
    }, {
      "referenceID" : 18,
      "context" : "This translation follows some related translations described in [24, 18, 19].",
      "startOffset" : 64,
      "endOffset" : 76
    }, {
      "referenceID" : 12,
      "context" : "are equivalent to the answer sets of [13].",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 21,
      "context" : "The transformation step from nor-mal logic program encoding of a model-free reinforcement learning problem into SAT can be avoided, by encoding a modelfree reinforcement learning problem directly into SAT [22].",
      "startOffset" : 205,
      "endOffset" : 209
    }, {
      "referenceID" : 13,
      "context" : "The flat representation of reinforcement learning problem domains is the explicit enumeration of world states [14].",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 13,
      "context" : "Hence, Theorem 5 follows directly from Theorem 4 [14].",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 23,
      "context" : "The translation from an action theory in into a normal logic program builds on similar translations described in [24, 18, 19].",
      "startOffset" : 113,
      "endOffset" : 125
    }, {
      "referenceID" : 17,
      "context" : "The translation from an action theory in into a normal logic program builds on similar translations described in [24, 18, 19].",
      "startOffset" : 113,
      "endOffset" : 125
    }, {
      "referenceID" : 18,
      "context" : "The translation from an action theory in into a normal logic program builds on similar translations described in [24, 18, 19].",
      "startOffset" : 113,
      "endOffset" : 125
    }, {
      "referenceID" : 0,
      "context" : "The literature is rich with action languages that are capable of represent-ing and reasoning about MDPs and actions with probabilistic effects, which include [1, 2, 6, 9, 12].",
      "startOffset" : 158,
      "endOffset" : 174
    }, {
      "referenceID" : 1,
      "context" : "The literature is rich with action languages that are capable of represent-ing and reasoning about MDPs and actions with probabilistic effects, which include [1, 2, 6, 9, 12].",
      "startOffset" : 158,
      "endOffset" : 174
    }, {
      "referenceID" : 5,
      "context" : "The literature is rich with action languages that are capable of represent-ing and reasoning about MDPs and actions with probabilistic effects, which include [1, 2, 6, 9, 12].",
      "startOffset" : 158,
      "endOffset" : 174
    }, {
      "referenceID" : 8,
      "context" : "The literature is rich with action languages that are capable of represent-ing and reasoning about MDPs and actions with probabilistic effects, which include [1, 2, 6, 9, 12].",
      "startOffset" : 158,
      "endOffset" : 174
    }, {
      "referenceID" : 11,
      "context" : "The literature is rich with action languages that are capable of represent-ing and reasoning about MDPs and actions with probabilistic effects, which include [1, 2, 6, 9, 12].",
      "startOffset" : 158,
      "endOffset" : 174
    }, {
      "referenceID" : 9,
      "context" : "These approaches can be classified into two main categories of approaches; dynamic programming approaches and the search-based approaches (a detailed survey on these approaches can be found in [10, 2]).",
      "startOffset" : 193,
      "endOffset" : 200
    }, {
      "referenceID" : 1,
      "context" : "These approaches can be classified into two main categories of approaches; dynamic programming approaches and the search-based approaches (a detailed survey on these approaches can be found in [10, 2]).",
      "startOffset" : 193,
      "endOffset" : 200
    }, {
      "referenceID" : 14,
      "context" : "A logic based approach for solving MDP, for probabilistic plan-ning, has been presented in [15].",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 14,
      "context" : "The approach of [15] converts MDP specification of a probabilistic planning problem into a stochastic satisfiability problem and solving the stochastic satisfiability problem instead.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 10,
      "context" : "First-order logic representation of MDP for model-based reinforcement learning has been described in [11] based on first-order logic programs without nonmonotonic negations.",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 10,
      "context" : "Similar to the firstorder representation of MDP in [11], allows objects and relations.",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 10,
      "context" : "However, unlike , [11] finds policies in the abstract level.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 10,
      "context" : "expressive first-order representation of MDP than [11] has been presented in [3] that is a probabilistic extension to Reiter’s situation calculus.",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 2,
      "context" : "expressive first-order representation of MDP than [11] has been presented in [3] that is a probabilistic extension to Reiter’s situation calculus.",
      "startOffset" : 77,
      "endOffset" : 80
    }, {
      "referenceID" : 10,
      "context" : "Although more expressive, it is more complex than [11].",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 18,
      "context" : "Unlike the logical model-based reinforcement learning frame-work of [19] that uses normal hybrid probabilistic logic programs to encode model-based reinforcement learning problems, normal logic program with answer set semantics is used to encode model-free reinforcement learning problems.",
      "startOffset" : 68,
      "endOffset" : 72
    } ],
    "year" : 2010,
    "abstractText" : "Knowledge Representation is important issue in reinforcement learning. In this paper, we bridge the gap between reinforcement learning and knowledge representation, by providing a rich knowledge representation framework, based on normal logic programs with answer set semantics, that is capable of solving model-free reinforcement learning problems for more complex domains and exploits the domain-specific knowledge. We prove the correctness of our approach. We show that the complexity of finding an offline and online policy for a model-free reinforcement learning problem in our approach is NP-complete. Moreover, we show that any model-free reinforcement learning problem in MDP environment can be encoded as a SAT problem. The importance of that is model-free reinforcement learning problems can be now solved as SAT problems.",
    "creator" : "Microsoft® Office Word 2007"
  }
}