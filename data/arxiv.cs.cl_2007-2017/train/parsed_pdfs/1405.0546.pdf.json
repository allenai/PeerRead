{
  "name" : "1405.0546.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Kaggle LSHTC4 Winning Solution",
    "authors" : [ "Antti Puurula", "Jesse Read", "Albert Bifet" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Kaggle LSHTC4 Winning Solution\nAntti Puurula1, Jesse Read2, and Albert Bifet3\n1 Department of Computer Science, The University of Waikato, Private Bag 3105, Hamilton 3240, New Zealand\n2 Department of Information and Computer Science, Aalto University, FI-00076 Aalto, Espoo, Finland 3 Huawei Noah’s Ark Lab, Hong Kong Science Park, Shatin, Hong Kong, China"
    }, {
      "heading" : "1 Overview",
      "text" : "Our winning submission to the 2014 Kaggle competition for Large Scale Hierarchical Text Classification (LSHTC) consists mostly of an ensemble of sparse generative models extending Multinomial Naive Bayes. The base-classifiers consist of hierarchically smoothed models combining document, label, and hierarchy level Multinomials, with feature pre-processing using variants of TF-IDF and BM25. Additional diversification is introduced by different types of folds and random search optimization for different measures. The ensemble algorithm optimizes macroFscore by predicting the documents for each label, instead of the usual prediction of labels per document. Scores for documents are predicted by weighted voting of base-classifier outputs with a variant of Feature-Weighted Linear Stacking. The number of documents per label is chosen using label priors and thresholding of vote scores.\nThis document describes the models and software used to build our solution. Reproducing the results for our solution can be done by running the scripts included in the Kaggle package4. A package omitting precomputed result files is also distributed5. All code is open source, released under GNU GPL 2.0, and GPL 3.0 for Weka and Meka dependencies."
    }, {
      "heading" : "2 Data Segmentation",
      "text" : "Source files: MAKE FILES, nfold sample corpus.py, fast sample corpus.py, shuffle data.py, count labelsets2.py\nTraining data segmentation is done by the script MAKE FILES, included in the code package. This segments the original training dataset train.txt by random sampling into portions for base-classifier training and for ensemble training.\n4 https://kaggle2.blob.core.windows.net/competitions/kaggle/3634/media/ LSHTC4 winner solution.zip 5 https://kaggle2.blob.core.windows.net/competitions/kaggle/3634/media/ LSHTC4 winner solution omit resultsfiles.zip\nar X\niv :1\n40 5.\n05 46\nv1 [\ncs .A\nI] 3\nM ay\n2 01\n4\nII\n2,341,782 documents are segmented for the former portion and 23,654 documents for the latter. The base-classifier training dataset dry train.txt is further sampled into 10 different folds, each with a 1000 document held-out portion dry dev.txt for parameter optimization. Folds 0-2 have exclusive and different sampled sets for dry dev.txt. Folds 3-5 sample dry train.txt randomly into 3 exlusive training subsets, with a shared optimization portion. Folds 6-9 segment dry train.txt in the original data order into 4 exclusive training subsets, with a shared optimization portion. For all folds, the training datasets are further shuffled to improve the online pruning of parameters in training."
    }, {
      "heading" : "3 Base-classifiers",
      "text" : "Source files: SGM-45l/, SGM-45l je/, Metaopt2.py, Make templates.py, results/, RUN DEVS, RUN EVALS, meka.jar\nThe base-classifiers consist mostly of sparse generative model extensions of Multinomial Naive Bayes (MNB). These extend MNB by introducing constrained finite mixtures at the document and hierarchy level nodes, and performing inference from the Multinomial node-conditional models using hierarchical smoothing, and kernel densities in case of document-conditional nodes. A special case is models using BM25 for kernel densities and no hierarchical smoothing. The models are stored in a sparse precomputed format, and inference using inverted indices is used to reduce the inference complexity according to the sparsity of the model. The constrained mixture modeling and sparse inference makes the models as scalable for text modeling as Naive Bayes and KNN, but with higher modelling accuracy. A detailed description of basic models of this type are given in [1, 2]. Since the LSHTC models can contain up to 100 million parameters for word counts, the models are provided as configuration files in the package. Estimating the models from training data takes negligible time more compared to reading saved model files.\nA development version of the SGMWeka toolkit6 was customized to implement the models. The customized version is included as the Java source directory SGM-45l, and the program SGM Tests.java used for training and testing the models can be compiled without external dependencies. The documentation for SGMWeka version 1.4.47 is accurate, but the development version contains additional functionalities. A modified version is in the directory SGM-45l je. This includes the Meka toolkit8 for doing multi-label decomposition used by one of the base-classifiers.\n6 http://sourceforge.net/projects/sgmweka/ 7 http://sourceforge.net/p/sgmweka/wiki/SGMWeka%20Documentation%20v.1.4.4/ 8 http://meka.sourceforge.net/\nIII\nThe script Metaopt2.py optimizes a base-classifier on a development set according to a chosen performance measure, by iteratively estimating the classifier and classifying the development data portion. The script RUN DEVS runs the development and compresses the log files. The configuration files for Metaopt2.py describes all the parameters provided to a SGM Tests call, as well as the optimization measure to extract from the last line of the SGM Tests log file. Metaopt2.py performs a Gaussian Random Search [3] for the chosen parameters, constrained and transformed according to the configuration file. The directories results * contain the first and last parameter configuration file for each base-classifier type, after a 40x8 iteration random search. Some classifiers were constructed by copying the parameters for similar folds (3,4,5), and some used manually chosen parameter configurations. These classifiers have the final iteration parameter file wikip large X params.txt 39 0, but not the initial file wikip large X params.txt. The script Make templates.py makes the parameter template files as specified in the global variable ”configs”.\nThe template files describe the model by suffixing the file name with modifications. For example, ”mnb mafs2 s8 lp u jm2 bm18ti pct0 ps5 thr16.template” modifies a Multinomial Naive Bayes by optimizing the parameters for a modified version of macro-Fscore ( mafs2), uses data fold 8 ( s8), the Label Powerset method for multi-label classification ( lp), smoothing by a uniform background distribution ( u), a BM25 variant for feature weighting ( bm18ti), uses a safe pruning of pre-computed parameters ( pct0), constrains the scaling of label prior ( ps5) and uses 16 threads for parallel classification.\nSome of the modifications have little influence on the results, such as thr16 that instructs SGM Tests to use 16 threads. More detailed explanations of the important modifications are given in the following sections. A total of 54 baseclassifiers are used in the ensemble, selected down to 42 base-classifiers by model selection. Table 1 shows the base-classifiers sorted according to comb dev.txt macro-averaged Fscore. It should be noted that the parameter ranges for some of the modifications were adjusted during the competition, and the parameter ranges in the individual template files can differ from those in Make templates.py.\nThe word count vectors for LSHTC were preprocessed by the organizers to remove common words, stopwords and short words, as can be seen from looking at the distributions of words in the vectors. This causes problems for some models such as Multinomial models of text, that assume word vectors to distribute normally. Feature transforms and weighting can be used to correct this. Feature weighting is done by each base-classifier separately, using variants of TF-IDF and BM25. All models use 1-3 parameters to optimize the feature weighting on the dry dev.txt portion of the fold. A variant of BM25 that proved most successful has the suffix ” bm18ti”. As seen in TFIDF.java, this combines the term count normalization of BM25 with the parameterized length normalization and\nIV\nV idf weighting from TF-IDF that has been used earlier [3].\nThe Multinomials use hierarchical smoothing with a uniform background distribution [2]. The variant ” uc1” uses a uniform distribution interpolated with a collection model, improving the accuracy by a small amount. All models use Jelinek-Mercer ” jmX” for smoothing label and hierarchy level Multinomials, and Dirichlet Prior smoothing ” kdpX” for smoothing kernel density document models. The feature selection done by the organizers cause very unusual smoothing parameter configurations to be optimal. With Jelinek-Mercer values less than a heavy amount such as 0.98 become rapidly worse, with some models using a smoothing coefficient of 0.999.\nParameter pruning is chosen by the modifiers ” mcX”, ” pciX”, ” pctX”, ” mlcX”. ” mcX” prunes word features based on their frequency. ” pciX” selects on-line pruning of conditional parameters, ” pctX” performs mostly safe pruning of precomputed conditional parameters, ” mlcX” prunes labels based on their frequency.\nOne special classifier is the variant using the modifer ” je”. This requires a development version of the Meka toolkit and the other files in the directory /SGM-45l je. This model does classification with label powersets decomposed into meta-labels, and transforms the meta-labels back into labelsets after classification. The labelset decompositions are stored in a precomputed file loaded by the modified version of SGM Tests.\nKernel densities are selected with the modifier ” kd”, passing -kernel densities to SGM Tests. This constructs document-conditional models, and computes label-conditional probabilities using the document-conditionals as kernel densities [2]. The modifiers ” csX” load the LSHTC4 label hierarchy, and use random parent nodes to smooth the label-conditional Multinomials. The Label Powerset method for mapping a multi-label problem into a multi-class problem is done by the modifier ” lp”, passing -label powerset to SGM Tests.\nThe modifier ” nobo” combined with ” kd” produces models for document instances with no back-off smoothing by label-conditional models. The modifiers ” bm25X” use BM25 instead of Multinomial distances. Combined with ” kd” and ” nobo”, this produces a model that uses BM25 for kernel densities of each label.\nThe modifiers ” ndcg5”, ” mjac”, ” mifs” and ” mafsX” choose the optimization measure for MetaOpt2.py. These correspond to NDCG@5, Mean of Jaccard scores per instance, micro-averaged Fscore, macro-averaged Fscore and surrogate measures for maFscore. It was noticed early in the competition that computing and optimizing maFscore is problematic, since not all labels are present in the training set, and any subset chosen for optimization will contain only a tiny fraction of the 325k+ labels, with the rest being missing labels. Since most la-\nVI\nbels are missing labels, and any number of false positives for a missing label will equal an fscore of 0, optimizing maFscore becomes problematic. The ” mafsX” surrogates used two attempts to penalize for false positives of missing labels, but these was abandoned for a method that allows optimizing macroFscore better without producing too many instances per label.\nThe modifiers ” iwX” select a method developed in this competition. This causes the base-classifier to predict instants per label, instead of labels per instance. A sorted list of the best scores for each label is stored, and for each classified instance the lists for labels are updated. A full distribution of labels is computed for each instance, and the label→instance scores are computed from the rank of the label for each instance. After classification of the dataset, the sparse label→instances scores are transposed and outputted and evaluated in the instance→labels format. The arguments -instantiate weight X and -instantiate threshold X passed to SGM Tests control the number of top scoring instances stored for each label. The ensemble combination uses transposed prediction of the same kind to do the classification."
    }, {
      "heading" : "4 Ensemble Model",
      "text" : "Source files: RUN METACOMB, MetaComb2.java, TransposeFile.py, SelectClassifiers.py, SelectDevLabels.py, comb dev results/, eval results/, weka.jar\nThe ensemble model is built on our earlier LSHTC3 ensemble [3], but performs classification by predicting instances per label. The classifier vote weight prediction is a case of Feature-Weighted Linear Stacking [4], but the regression models are trained separately for each base-classifier, using reference weights that approximate optimal weights per label in a development set.\nThe base-classifier result files are tranposed from a document→labels per line format to a label→documents per line format. After prediction the ensemble result file is transposed back to the document→labels per line .csv format used by the competition. The script RUN METACOMB performs all the required steps, using the result files stored in /comb dev results for training the ensemble and /eval results to do the classifier combination.\nMetacomb2.java perfoms the ensemble classification. The ensemble uses linear regression models to predict the weight of each base-classifier, using metafeatures computed from label information and classifier outputs to predict the optimal classifier weight for each label. The most useful metafeatures in the LSHTC3 submission used labelset correlation features between the base-classifiers for each document instance [3]. This ensemble uses instance-set correlation features for each label analogously.\nVII\nTable 2 shows the metafeatures used by MetaComb2.java. For efficiency and memory use, MetaComb2 adds the correlation metafeatures to each baseclassifier before predicting the vote weights, and doesn’t keep all possible metafeatures in memory at any time. This keeps the memory complexity of the ensemble combination linear in the number of base-classifiers. Functions constuctData(), pruneGlobalFeatures() and addLocalFeatures() in MetaComb2.java show how the features are constructed as Weka [5] Instances.\nThe regression models use Weka LinearRegression for implementing the variant of Feature-Weighted Linear Stacking. For each label in comb dev.txt, optimal reference weights are approximated by distributing a weight of 1 uniformly to the base-classifiers that score highest on the performance measure. Initially fscore was used as the measure, as averaging the fscores across the labels gives maFscore. This however doesn’t use rank information in the instance sets. A small improvement in maFscore was gained by using a similar measure that takes rank information into account. approximateOracleWeights() and updateEvaluationResults() in MetaComb2.java show how the reference vote weights are constructed.\nFollowing vote weight prediction, the label→instances scores are summed for each instance from the weighted votes in the function voteFold(). A combination of label prior information and thresholding similar to one used in the base-classifiers is used to choose the number of instances per label. The label prior information selects a number of instances for the label proportional to the\nVIII\nlabel frequency in training data, multiplied by the parameter 0.95 passed to set instantiate(). The thresholding then includes to the set all instances with score more than 0.5 of the mean of the initial instance set scores. Figure 1 illustrates the ensemble combination and selection of instances.\nDevelopment of the ensemble by n-fold cross-validation can be done by changing the global variable ”developmentRun” in MetaComb2.java to 1. Selection of base-classifiers can be done by giving the classifiers to remove as integer arguments to MetaComb2. The list of removed classifiers used in the final evaluation run in RUN METACOMB was developed by running the classifier selection script SelectClassifiers.py with the n-fold crossvalidation. SelectClassifiers.py performs hill-climbing searches, maximizing the output of MetaComb2 by removing and adding classifiers to the ensemble."
    }, {
      "heading" : "5 How to Generate the Solution",
      "text" : "The programs and scripts described above can be run to produce the winning submission file. Some of the programs can take considerable computing resources to produce. Both optimizing the base-classifier parameters and classifying the 452k document test set can take several days or more, depending on the model. We used a handful of quadcore i7-2600 CPU processors with 16GB RAM over\nIX\nthe competition period to develop and optimize the models. At least 16GB RAM is required to store the word counts reaching 100M parameters. Ensemble combination takes less than 8GB memory, and can be computed from the provided .results files. The base-classifier result files are included in the distribution, as computing these takes considerable time.\nFor optimizing base-classifiers, compile SGM Tests.java with javac, configure Make templates.py or copy an existing template, and run RUN DEVS. For classifying the comb dev.txt and test.txt results with a base-classifier, configure and run RUN EVALS. For combining the base-classifier results with the ensemble, run RUN METACOMB. The global variables in each script can be modified to change configurations."
    }, {
      "heading" : "6 What Gave us the First Place?",
      "text" : "The competition posed a number of complications different from usual Kaggle competitions. Most of our tools were developed over the last LSHTC challenges, and this gave us a big advantage. The biggest complication in the competition was scalability of both the base-classifiers and ensemble. Our solution uses sparse storage and inverted indices for inference, a modeling idea that enabled us to use an ensemble of tens of base-classifiers. With the SGMWeka toolkit we could combine parameterized feature weighting [3], hierarchical smoothing [2], kernel densities [2], model-based feedback [6], etc. Other participants used KNN with inverted indices, but our solution provides a diversity of structured probabilistic base-classifiers with much better modeling accuracies.\nAnother complication was the preprocessed pruned feature vectors. This made usual Multinomial or Language Model solutions usable only with very untypical and heavy use of linear interpolation smoothing. The commonly used TF-IDF feature transforms also corrected the problem only somewhat. Our solutions for smoothing and feature weighting with a customized BM25 variant took extensive experimentation to discover, but improved the accuracy considerably. It is likely that the other teams had less sophisticated text similarity measures available, and the ones having good measures scored better in the contest.\nThe most difficult complication in the contest was the choice of maFscore for evaluation measure, in contrast to earlier LSHTC competitions. What surprised the contestants was that optimization of maFscore with high numbers of labels is problematic, since most labels will be missing. With maFscore a label occurring once is just as important as one occurring 1000 times, and a label never predicted and one predicted by a 1000 false positives have the same effect on the score. Combined with most labels missing, normal optimization of classifiers proved difficult. It took us some time to figure out the right way to solve this problem, but the solution made it possible for us to compete for the win. Before\nX 0.1 0.14 0.18 0.22 0.26 0.3 0.34\nm ac\nro -a\nve ra\nge d\nF sc\no re\ndeveloping the transposed prediction used in both the base-classifiers and the ensemble, our leaderboard score was around 22%. A couple of simple corrections for maximizing maFscore correctly brought the ensemble combination close to 27%, and using the transposed prediction with a larger and more diverse ensemble gave us the final score close to 34%. Other participants noticed this problem of optimizing maFscore, but likely most of them did not find a good solution.\nThe use of metafeature regression in the ensemble instead of majority voting proved a moderate improvement of about 0.5%, and this much was needed for the win. It is likely that the metafeatures optimized on the 23k comb dev.txt documents looked different from the metafeatures computed for the 452k test.txt documents, even though the metafeatures were chosen or normalized to be stable to change in the number of documents. The optimal amount of regularization for the Weka LinearRegression was untypically high at 1000. More complicated Weka regression models for the vote weight prediction failed to improve the test set score, likely due to overfitting the somewhat unreliable features. Another reason could be the small size of the comb dev.txt for ensemble combination. The ensemble fits the parameters for 55 metafeatures to predict the vote weight of each of the 42 base-classifiers, using only 23k points of data shared by the 42 regression models. The improvement from Feature Weighted Linear Stacking could have been considerably larger, if a larger training set had been segmented for the ensemble.\nXI"
    }, {
      "heading" : "7 Acknowledgements",
      "text" : "We’d like to thank Kaggle and the LSHTC organizers for their work in making the competition a success, and the machine learning group at the University of Waikato for the computers we used for our solution."
    } ],
    "references" : [ {
      "title" : "Scalable text classification with sparse generative modeling",
      "author" : [ "A. Puurula" ],
      "venue" : "Proceedings of the 12th Pacific Rim International Conference on Trends in Artificial Intelligence. PRICAI’12,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2012
    }, {
      "title" : "Integrated instance- and class-based generative modeling for text classification",
      "author" : [ "A. Puurula", "S.H. Myaeng" ],
      "venue" : "Australasian Document Computing Symposium",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2013
    }, {
      "title" : "Combining modifications to multinomial naive bayes for text classification",
      "author" : [ "A. Puurula" ],
      "venue" : "Information Retrieval Technology. Volume 7675 of Lecture Notes in Computer Science",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2012
    }, {
      "title" : "Feature-weighted linear stacking",
      "author" : [ "J. Sill", "G. Takcs", "L. Mackey", "D. Lin" ],
      "venue" : "CoRR abs/0911.0460",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2009
    }, {
      "title" : "The weka data mining software: An update",
      "author" : [ "M. Hall", "E. Frank", "G. Holmes", "B. Pfahringer", "P. Reutemann", "I.H. Witten" ],
      "venue" : "SIGKDD Explor. Newsl",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2009
    }, {
      "title" : "Cumulative progress in language models for information retrieval",
      "author" : [ "A. Puurula" ],
      "venue" : "Australasian Language Technology Workshop",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "A detailed description of basic models of this type are given in [1, 2].",
      "startOffset" : 65,
      "endOffset" : 71
    }, {
      "referenceID" : 1,
      "context" : "A detailed description of basic models of this type are given in [1, 2].",
      "startOffset" : 65,
      "endOffset" : 71
    }, {
      "referenceID" : 2,
      "context" : "py performs a Gaussian Random Search [3] for the chosen parameters, constrained and transformed according to the configuration file.",
      "startOffset" : 37,
      "endOffset" : 40
    }, {
      "referenceID" : 2,
      "context" : "idf weighting from TF-IDF that has been used earlier [3].",
      "startOffset" : 53,
      "endOffset" : 56
    }, {
      "referenceID" : 1,
      "context" : "The Multinomials use hierarchical smoothing with a uniform background distribution [2].",
      "startOffset" : 83,
      "endOffset" : 86
    }, {
      "referenceID" : 1,
      "context" : "This constructs document-conditional models, and computes label-conditional probabilities using the document-conditionals as kernel densities [2].",
      "startOffset" : 142,
      "endOffset" : 145
    }, {
      "referenceID" : 2,
      "context" : "The ensemble model is built on our earlier LSHTC3 ensemble [3], but performs classification by predicting instances per label.",
      "startOffset" : 59,
      "endOffset" : 62
    }, {
      "referenceID" : 3,
      "context" : "The classifier vote weight prediction is a case of Feature-Weighted Linear Stacking [4], but the regression models are trained separately for each base-classifier, using reference weights that approximate optimal weights per label in a development set.",
      "startOffset" : 84,
      "endOffset" : 87
    }, {
      "referenceID" : 2,
      "context" : "The most useful metafeatures in the LSHTC3 submission used labelset correlation features between the base-classifiers for each document instance [3].",
      "startOffset" : 145,
      "endOffset" : 148
    }, {
      "referenceID" : 4,
      "context" : "java show how the features are constructed as Weka [5] Instances.",
      "startOffset" : 51,
      "endOffset" : 54
    }, {
      "referenceID" : 2,
      "context" : "With the SGMWeka toolkit we could combine parameterized feature weighting [3], hierarchical smoothing [2], kernel densities [2], model-based feedback [6], etc.",
      "startOffset" : 74,
      "endOffset" : 77
    }, {
      "referenceID" : 1,
      "context" : "With the SGMWeka toolkit we could combine parameterized feature weighting [3], hierarchical smoothing [2], kernel densities [2], model-based feedback [6], etc.",
      "startOffset" : 102,
      "endOffset" : 105
    }, {
      "referenceID" : 1,
      "context" : "With the SGMWeka toolkit we could combine parameterized feature weighting [3], hierarchical smoothing [2], kernel densities [2], model-based feedback [6], etc.",
      "startOffset" : 124,
      "endOffset" : 127
    }, {
      "referenceID" : 5,
      "context" : "With the SGMWeka toolkit we could combine parameterized feature weighting [3], hierarchical smoothing [2], kernel densities [2], model-based feedback [6], etc.",
      "startOffset" : 150,
      "endOffset" : 153
    } ],
    "year" : 2014,
    "abstractText" : "Our winning submission to the 2014 Kaggle competition for Large Scale Hierarchical Text Classification (LSHTC) consists mostly of an ensemble of sparse generative models extending Multinomial Naive Bayes. The base-classifiers consist of hierarchically smoothed models combining document, label, and hierarchy level Multinomials, with feature pre-processing using variants of TF-IDF and BM25. Additional diversification is introduced by different types of folds and random search optimization for different measures. The ensemble algorithm optimizes macroFscore by predicting the documents for each label, instead of the usual prediction of labels per document. Scores for documents are predicted by weighted voting of base-classifier outputs with a variant of Feature-Weighted Linear Stacking. The number of documents per label is chosen using label priors and thresholding of vote scores.",
    "creator" : "LaTeX with hyperref package"
  }
}