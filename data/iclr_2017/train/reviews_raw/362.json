{"conference": "ICLR 2017 conference submission", "title": "Learning to Optimize", "abstract": "Algorithm design is a laborious process and often requires many iterations of ideation and validation. In this paper, we explore automating algorithm design and present a method to learn an optimization algorithm. We approach this problem from a reinforcement learning perspective and represent any particular optimization algorithm as a policy. We learn an optimization algorithm using guided policy search and demonstrate that the resulting algorithm outperforms existing hand-engineered algorithms in terms of convergence speed and/or the final objective value.", "reviews": [{"TITLE": "ICLR committee final decision", "OTHER_KEYS": "ICLR 2017 pcs", "comments": "The authors propose an approach to learning optimization algorithms by framing the problem as a policy search task, then using the guided policy search algorithm. The method is a nice contribution to the \"learning to learn\" framework, and actually was developed simultaneously to a few papers that have since already been published. It's definitely a useful addition to this space.\n \n The biggest issue with this paper is that the results simply aren't that compelling. The methodology and proposed approach is nice, and the text is improved upon a previous version posted to Arxiv, but it seems that for most problems the results aren't that much better than some of the more common off-the-shelf optimization approaches that _don't_ require learning anything. Furthermore, in the one domain where the method does seem to (marginally) outperform the other methods, the neural net domain, it's unclear why we'd want to use the proposed approaches instead of SGD-based methods and their like (which of course everyone actually does for optimization).\n \n Pros:\n + Nice contribution to the learning to learn framework\n + Takes a different approach from past (concurrent) work, namely one based upon policy search\n \n Cons:\n - Experiments are not particularly compelling\n \n Overall, this work is a little borderline. Still, the PCs have determined that it was deserving of appearing at the conference. We hope the authors can strengthen the empirical validation for the camera ready version.", "IS_META_REVIEW": false, "DATE": "06 Feb 2017", "is_meta_review": false}, {"TITLE": "related work on learning to optimize", "OTHER_KEYS": "Yann LeCun", "comments": "There is a series of somewhat-related papers on learning to optimize for sparse coding and other L1/L2 optimization problems.\nInference in sparse coding is often performed with the Iterative Shrinkage and Thresholding Algorithm (ISTA). \nISTA can be viewed as a kind of recurrent net in which the matrices and non-linearities are specified by the reconstruction matrix.\nThe main idea is to train the matrices in this recurrent net to produce an approximate solution faster than ISTA.\n\nHere are two examples:\n[Gregor & LeCun, ICML 2010] ", "IS_META_REVIEW": false, "DATE": "20 Dec 2016", "is_meta_review": false}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Review", "is_meta_review": false, "comments": "\nThis papers adds to the literature on learning optimizers/algorithms that has gained popularity recently. The authors choose to use the framework of guided policy search at the meta-level to train the optimizers. They also opt to train on random objectives and assess transfer to a few simple tasks.\n\nAs pointed below, this is a useful addition.\n\nHowever, the argument of using RL vs gradients at the meta-level that appears below is not clear or convincing. I urge the authors to run an experiment comparing the two approaches and to present comparative results. This is a very important question, and the scalability of this approach could very well hinge on this fact. Indeed, demonstrating both scaling to large domains and transfer to those domains is the key challenge in this domain.\n\nIn summary, the idea is a good one, but the experiments are weak.\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "No Title", "is_meta_review": false, "comments": "The current version of the paper is improved w.r.t. the original arXiv version from June. While the results are exactly the same, the text does not oversell them as much as before. You may also consider to avoid words like \"mantra\", etc. \nI believe that my criticism given in my comment from 3 Dec 2016 about \"randomly generated task\" is valid and you answer is not.", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016 (modified: 20 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "Very nice methodological contribution, though I'm not quite convinced it is as general as claimed", "is_meta_review": false, "comments": "This paper proposes an approach to learning a custom optimizer for a given class optimization problems. I think in the case of training machine learning algorithms, a class would represent a model like \u201clogistic regression\u201d. The authors cleverly cast this as a reinforcement learning problem and use guided policy search to train a neural network to map the current location and history onto a step direction/magnitude. Overall I think this is a great idea and a very nice contribution to the fast growing meta-learning literature. However, I think that there are some aspects that could be touched on to make this a stronger paper.\n\nMy first thought is that the authors claim to train the method to learn the regularities of an entire class of optimization problems, rather than learning to exploit regularities in a given set of tasks. The distinction here is not terribly clear to me. For example, in learning an optimizer for logistic regression, the authors seem to claim that learning on a randomly sampled set of logistic regression problems will allow the model to learn about logistic regression itself. I am not convinced of this, because there is bias in the randomly sampled data itself. From the paper in this case, \u201cThe instances are drawn randomly from two multivariate Gaussians with random means and covariances, with half drawn from each.\u201d It seems the optimizer is then trained to optimize instances of logistic regression *given this specific family of training inputs* and not logistic regression problems in general. A simple experiment to prove the method works more generally would be to repeat the existing experiments, but where the test instances are drawn from a completely different distribution. It would be even more interesting to see how this changes as the test distribution deviates further from the training distribution.\n\nCan the authors comment on the choice of architecture used here? Why one layer with 50 hidden units and softplus activations specifically? Why not e.g., 100 units, 2 layers and ReLUs?\nPresumably this is to prevent overfitting, but given the limited capacity of the network, how do these results look when the dimensionality of the input space increases beyond 2 or 3?\n\nI would love to see what kind of policy the network learns on e.g., a 2D function using a contour plot. What do the steps look like on a random problem instance when compared to other hand-engineered optimizers?\n\nOverall I think this a really interesting paper with a great methodological contribution. My main concern is that the results may be oversold as the problems are still relatively simple and constrained. However, if the authors can demonstrate that this approach produces robust policies for a very general set of problems then that would be truly spectacular.\n\nMinor notes below.\n\nSection 3.1 should you be using \\pi_T^* to denote the optimal policy? You use \\pi_t^* and \\pi^* currently.\n\nAre the problems here considered noiseless? That is, is the state transition given an action deterministic? It would be very interesting to see this on noisy problems.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "A good contribution to learning to learn", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "\nThis work appeared on arxiv just before we were able to release our learning to learn by gradient descent by gradient descent paper. \nIn our case, we were motivated by neural art and a desire to replace the lBFGS optimizer with a neural Turing machine (both have the same equation forms - see appendix of our arxiv version). In the end, we settled on an LSTM optimizer that is also learned by SGD. \n\nThis paper, on the other hand, uses guided policy search to choose the parameter updates (policy). Importantly, this paper also emphasizes the importance of transfer of optimizers to novel tasks.\n\nThis is undoubtedly a timely, good contribution to the learning to learn literature, which has gained great popularity in recent months.", "IS_META_REVIEW": false, "DATE": "13 Dec 2016", "is_meta_review": false}, {"TITLE": "RL vs supervised training of optimizers and discrete/continuous optimization spaces", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "", "IS_META_REVIEW": false, "DATE": "13 Dec 2016", "is_meta_review": false}, {"TITLE": "randomly generated task", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "", "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "is_meta_review": false}, {"TITLE": "Number of objective functions", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "", "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "is_meta_review": false}, {"TITLE": "Questions about experiments and algorithm", "OTHER_KEYS": "(anonymous)", "comments": "I have some questions about the details of your experiments and algorithm. The paper presents a method using Guided Policy Search (GPS) techniques for learning optimization algorithm (\\pi). Since the contribution is mainly empirical, convincing experimental analysis is expected. Currently, all of three experiments were conducted on toy datasets. The number of parameters to be optimized is so small (only no more than 10). I suggest you evaluate your method on some standard datasets on middle or large scale network structures. Besides, other popular optimization algorithms, such as Adam, RMSprop, etc. should be compared. \n \nAnother point I was wondering is whether your method is suitable for practical use. Both the value and the gradient of the objective function at the most recent H steps (H=25 in the paper) are encoded as state space. Suppose that the dimension of the parameters to be optimized is N, the dimension of the state space would be (1+N)*H. Then they are fed to a one layer network with 50 hidden units to model the policy (\\pi), yielding N outputs to update parameters. But in practical scenario, the computational complexity and memory used for optimization algorithm should better not exceed the complexity of the training algorithm. Your policy network takes the gradients of all the parameters as input, which means policy network itself has more parameters to learn than the parameters of the problem of interest.\n", "IS_META_REVIEW": false, "DATE": "28 Nov 2016", "is_meta_review": false}], "authors": "Ke Li, Jitendra Malik", "KEYWORDS": "We explore learning an optimization algorithm automatically.", "accepted": true, "id": ""}
