{
  "name" : "1606.00739.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Stochastic Structured Prediction under Bandit Feedback",
    "authors" : [ "Artem Sokolov", "Julia Kreutzer", "Christopher Lo", "Stefan Riezler" ],
    "emails" : [ "sokolov@cl.uni-heidelberg.de", "kreutzer@cl.uni-heidelberg.de", "riezler@cl.uni-heidelberg.de", "chris.aa.lo@gmail.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n60 6.\n00 73\n9v 2\n[ cs\n.C L\n] 2"
    }, {
      "heading" : "1 Introduction",
      "text" : "We present algorithms for stochastic structured prediction under bandit feedback that obey the following learning protocol: On each of a sequence of iterations, the learner receives an input, predicts an output structure, and receives partial feedback in form of a task loss evaluation of the predicted structure. In contrast to the full-information batch learning scenario, the gradient cannot be averaged over the complete input set. Furthermore, in contrast to standard stochastic learning, the correct output structure is not revealed to the learner. We present algorithms that use this feedback to “banditize” expected loss minimization approaches to structured prediction [18, 25]. The algorithms follow the structure of performing simultaneous exploration/exploitation by sampling output structures from a log-linear probability model, receiving feedback to the sampled structure, and conducting an update in the negative direction of an unbiased estimate of the gradient of the respective full information objective. The algorithms apply to situations where learning proceeds online on a sequence of inputs for which gold standard structures are not available, but feedback to predicted structures can be elicited from users. A practical example is interactive machine translation where instead of human generated reference translations only translation quality judgments on predicted translations are used for learning [20]. The example of machine translation showcases the complexity of the problem: For each input sentence, we receive feedback for only a single predicted translation out of a space that is exponential in sentence length, while the goal is to learn to predict the translation with the smallest loss under a complex evaluation metric.\n[19] showed that partial feedback is indeed sufficient for optimization of feature-rich linear structured prediction over large output spaces in various natural language processing (NLP) tasks. Their experiments follow the standard online-to-batch conversion practice in NLP applications where the\n∗ The work for this paper was done while the authors were at Heidelberg University.\n30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nmodel with optimal task performance on development data is selected for final evaluation on a test set. The contribution of our paper is to analyze these algorithms as stochastic first-order (SFO) methods in the framework of [7] and investigate the connection of optimization for task performance with optimization-theoretic concepts of convergence.\nOur analysis starts with revisiting the approach to stochastic optimization of a non-convex expected loss criterion presented by [20]. The iteration complexity of stochastic optimization of a non-convex objective J(wt) can be analyzed in the framework of [7] as O(1/ǫ2) in terms of the number of iterations needed to reach an accuracy of ǫ for the criterion E[‖∇J(wt)‖2] ≤ ǫ. [19] attempt to improve convergence speed by introducing a cross-entropy objective that can be seen as a (strong) convexification of the expected loss objective. The known best iteration complexity for strongly convex stochastic optimization is O(1/ǫ) for the suboptimality criterion E[J(wt)] − J(w∗) ≤ ǫ. Lastly, we analyze the pairwise preference learning algorithm introduced by [19]. This algorithm can also be analyzed as an SFO method for non-convex optimization. To our knowledge, this is the first SFO approach to stochastic learning form pairwise comparison feedback, while related approaches fall into the area of gradient-free stochastic zeroth-order (SZO) approaches [24, 1, 7, 4]. Convergence rate for SZO methods depends on the dimensionality d of the function to be evaluated, for example, the non-convex SZO algorithm of [7] has an iteration complexity of O(d/ǫ2). SFO algorithms do not depend on d which is crucial if the dimensionality of the feature space is large as is common in structured prediction.\nFurthermore, we present a comparison of empirical and theoretical convergence criteria for the NLP tasks of machine translation and noun-phrase chunking. We compare the empirical convergence criterion of optimal task performance on development data with the theoretically motivated criterion of minimal squared gradient norm. We find a correspondence of fastest convergence of pairwise preference learning on both tasks. Given the standard analysis of asymptotic complexity bounds, this result is surprising. An explanation can be given by measuring variance and Lipschitz constant of the stochastic gradient, which is smallest for pairwise preference learning and largest for cross-entropy minimization by several orders of magnitude. This offsets the possible gains in asymptotic convergence rates for strongly convex stochastic optimization, and makes pairwise preference learning an attractive method for fast optimization in practical interactive scenarios."
    }, {
      "heading" : "2 Related Work",
      "text" : "The methods presented in this paper are related to various other machine learning problems where predictions over large output spaces have to be learned from partial information.\nReinforcement learning has the goal of maximizing the expected reward for choosing an action at a given state in a Markov Decision Process (MDP) model, where unknown rewards are received at each state, or once at the final state. The algorithms in this paper can be seen as one-state MDPs with context where choosing an action corresponds to predicting a structured output. Most closely related are recent applications of policy gradient methods to exponential output spaces in NLP problems [22, 3, 15]. Similar to our expected loss minimization approaches, these approaches are based on nonconvex models, however, convergence rates are rarely a focus in the reinforcement learning literature. One focus of our paper is to present an analysis of asymptotic convergence and convergence rates of non-convex stochastic first-order methods.\nContextual one-state MDPs are also known as contextual bandits [11, 13] which operate in a scenario of maximizing the expected reward for selecting an arm of a multi-armed slot machine. Similar to our case, the feedback is partial, and the models consist of a single state. While bandit learning is mostly formalized as online regret minimization with respect to the best fixed arm in hindsight, we characterize our approach in an asymptotic convergence framework. Furthermore, our highdimensional models predict structures over exponential output spaces. Since we aim to train these models in interaction with real users, we focus on the ease of elicitability of the feedback and on speed of convergence. In the spectrum of stochastic versus adversarial bandits, our approach is semi-adversarial in making stochastic assumptions on inputs, but not on rewards [12].\nPairwise preference learning has been studied in the full information supervised setting [8, 10, 6] where given preference pairs are assumed. Work on stochastic pairwise learning has been formalized as derivative-free stochastic zeroth-order optimization [24, 1, 7, 4]. To our knowledge, our approach\nAlgorithm 1 Bandit Structured Prediction 1: Input: sequence of learning rates γt 2: Initialize w0 3: for t = 0, . . . , T do 4: Observe xt 5: Sample ỹt ∼ pwt(y|xt) 6: Obtain feedback ∆(ỹt) 7: wt+1 = wt − γt st 8: Choose a solution ŵ from the list {w0, . . . , wT }\nto pairwise preference learning from partial feedback is the first SFO approach to learning from pairwise preferences in form of relative task loss evaluations."
    }, {
      "heading" : "3 Expected Loss Minimization for Structured Prediction",
      "text" : "[18, 25] introduce the expected loss criterion for structured prediction as the minimization of the expectation of a given task loss function with respect to the conditional distribution over structured outputs. Let X be a structured input space, let Y(x) be the set of possible output structures for input x, and let ∆y : Y → [0, 1] quantify the loss ∆y(y′) suffered for predicting y′ instead of the gold standard structure y. In the full information setting, for a given (empirical) data distribution p(x, y), the learning problem is defined as\nmin w∈Rd\nEp(x,y)pw(y′|x) [∆y(y ′)] = min\nw∈Rd\n∑\nx,y\np(x, y) ∑\ny′∈Y(x)\n∆y(y ′)pw(y ′|x), (1)\nwhere pw(y|x) = exp(w ⊤φ(x, y))/Zw(x) (2)\nis a Gibbs distribution with joint feature representation φ : X ×Y → Rd, weight vector w ∈ Rd, and normalization constant Zw(x). Despite being a highly non-convex optimization problem, positive results have been obtained by gradient-based optimization with respect to\n∇Ep(x,y)pw(y′|x) [∆y(y ′)] = Ep(x,y)pw(y′|x)\n[\n∆y(y ′) ( φ(x, y′)− Epw(y′|x)[φ(x, y ′)] )\n]\n. (3)\nUnlike in the full information scenario, in structured learning under bandit feedback the gold standard output structure y with respect to which the objective function is evaluated is not revealed to the learner. Thus we can neither evaluate the task loss ∆ nor calculate the gradient (3) as in the full information case. A solution to this problem is to pass the evaluation of the loss function to the user, i.e, we access the loss directly through user feedback without assuming existence of a fixed reference y. In the following, we will drop the subscript referring to the gold standard structure in the definition of ∆ to indicate that the feedback is in general independent of gold standard outputs. In particular, we allow ∆ to be equal to 0 for several outputs."
    }, {
      "heading" : "4 Stochastic Structured Prediction under Partial Feedback",
      "text" : "Algorithm Structure. Algorithm 1 shows the structure of the methods analyzed in this paper. It assumes a sequence of input structures xt, t = 0, . . . , T that are generated by a fixed, unknown distribution p(x) (line 4). For each randomly chosen input, an output ỹt is sampled from a Gibbs model to perform simultaneous exploitation (use the current best estimate) / exploration (get new information) on output structures (line 5). Then, feedback ∆(ỹt) to the predicted structure is obtained (line 6). An update is performed by taking a step in the negative direction of the stochastic gradient st, at a rate γt (line 7). As a post-optimization step, a solution ŵ is chosen from the list of vectors wt ∈ {w0, . . . , wT } (line 8).\nGiven Algorithm 1, we can formalize the notion of “banditization” of objective functions by presenting different instantiations of the vector st, and showing them to be unbiased estimates of the gradients of corresponding full information objectives.\nExpected Loss Minimization. [20] presented an algorithm that minimizes the following expected loss objective. It is non-convex for the specific instantiations in this paper:\nEp(x)pw(y|x) [∆(y)] = ∑\nx\np(x) ∑\ny∈Y(x)\n∆(y)pw(y|x). (4)\nThe vector st used in their algorithm can be seen as a stochastic gradient of this objective, i.e., an evaluation of the full gradient at a randomly chosen input xt and output ỹt:\nst = ∆(ỹt) ( φ(xt, ỹt)− Epwt (y|xt)[φ(xt, y)] ) . (5)\nInstantiating st in Algorithm 1 to the stochastic gradient in equation (5) yields an update that compares the sampled feature vector to the average feature vector, and performs a step into the opposite direction of this difference, the more so the higher the loss of the sampled structure is. In the following, we refer to the algorithm for expected loss minimization defined by the update (5) as Algorithm EL.\nPairwise Preference Learning. Decomposing complex problems into a series of pairwise comparisons has been shown to be advantageous for human decision making [23]. For the example of machine translation, this means that instead of requiring numerical assessments of translation quality from human users, only a relative preference judgement on a pair of translations needs to be elicited. This idea is formalized in [19] as an expected loss objective with respect to a conditional distribution of pairs of structured outputs. Let P(x) = {〈yi, yj〉 |yi, yj ∈ Y(x)} denote the set of output pairs for an input x, and let ∆(〈yi, yj〉) : P(x) → [0, 1] denote a task loss function that specifies a dispreference of yi compared to yj . In the experiments reported in this paper, we simulate two types of pairwise feedback. Firstly, continuous pairwise feedback is computed as\n∆(〈yi, yj〉) =\n{\n∆(yi)−∆(yj) if ∆(yi) > ∆(yj), 0 otherwise.\n(6)\nA binary feedback function is computed as\n∆(〈yi, yj〉) =\n{\n1 if ∆(yi) > ∆(yj), 0 otherwise.\n(7)\nFurthermore, we assume a feature representation φ(x, 〈yi, yj〉) = φ(x, yi) − φ(x, yj) and a Gibbs model on pairs of output structures\npw(〈yi, yj〉 |x) = ew\n⊤(φ(x,yi)−φ(x,yj))\n∑\n〈yi,yj〉∈P(x)\new ⊤(φ(x,yi)−φ(x,yj))\n= pw(yi|x)p−w(yj |x). (8)\nThe factorization of this model into the product pw(yi|x)p−w(yj |x) allows efficient sampling and calculation of expectations. Instantiating objective (4) to the case of pairs of output structures defines the following objective that is again non-convex in the use cases in this paper:\nEp(x)pw(〈yi,yj〉|x) [∆(〈yi, yj〉)] = ∑\nx\np(x) ∑\n〈yi,yj〉∈P(x)\n∆(〈yi, yj〉) pw(〈yi, yj〉 |x). (9)\nLearning from partial feedback on pairwise preferences will ensure that the model finds a ranking function that assigns low probabilities to discordant pairs with respect the the observed preference pairs. Stronger assumptions on the learned ranking can be made if asymmetry and transitivity of the observed ordering of pairs is required.2 An algorithm for pairwise preference learning can be defined by instantiating Algorithm 1 to sampling output pairs 〈ỹi, ỹj〉t, receiving feedback ∆(〈ỹi, ỹj〉t), and performing a stochastic gradient update using\nst = ∆(〈ỹi, ỹj〉t) ( φ(xt, 〈ỹi, ỹj〉t)− Epwt (〈yi,yj〉|xt)[φ(xt, 〈yi, yj〉)] ) . (10)\nThe algorithms for pairwise preference ranking defined by update (10) are referred to as Algorithms PR(bin) and PR(cont), depending on the use of binary or continuous feedback.\n2See [2] for an overview of bandit learning from consistent and inconsistent pairwise comparisons.\nCross-Entropy Minimization. The standard theory of stochastic optimization predicts considerable improvements in convergence speed depending on the functional form of the objective. This motivated the formalization of a convex upper bounds on expected normalized loss in [19]. If a normalized gain function ḡ(y) = g(y)\nZg(x) is used where Zg(x) =\n∑\ny∈Y(x) g(y), and g = 1−∆, the\nobjective can be seen as the cross-entropy of model pw(y|x) with respect to ḡ(y):\nEp(x)ḡ(y) [− log pw(y|x)] = − ∑\nx\np(x) ∑\ny∈Y(x)\nḡ(y) log pw(y|x). (11)\nFor a proper probability distribution ḡ(y), an application of Jensen’s inequality to the convex negative logarithm function shows that objective (11) is a convex upper bound on objective (4). However, normalizing the gain function is prohibitive in a partial feedback setting since it would require to elicit user feedback for each structure in the output space. [19] thus work with an unnormalized gain function g(y) that preserves convexity. This can be seen by rewriting the objective as the sum of a linear and a convex function in w:\nEp(x)g(y) [− log pw(y|x)] =− ∑\nx\np(x) ∑\ny∈Y(x)\ng(y)w⊤φ(x, y) (12)\n+ ∑\nx\np(x)(log ∑\ny∈Y(x)\nexp(w⊤φ(x, y)))α(x),\nwhere α(x) = ∑\ny∈Y(x) g(y) is a constant factor not depending on w. Instantiating Algorithm 1 to the following stochastic gradient st of this objective yields an algorithm for cross-entropy minimization:\nst = g(ỹt)\npwt(ỹt|xt)\n( − φ(xt, ỹt) + Epwt [φ(xt, yt)] ) . (13)\nNote that the ability to sample structures from pwt(ỹt|xt) comes at the price of having to normalize st by 1/pwt(ỹt|xt). While minimization of this objective will assign high probabilities to structures with high gain, as desired, each update is affected by a probability that changes over time and is unreliable when training is started. This further increases the variance already present in stochastic optimization. We deal with this problem by clipping too small sampling probabilities to p̂wt(ỹt|xt) = max{pwt(ỹt|xt), k} for a constant k [9]. The algorithm for cross-entropy minimization based on the stochastic gradient (13) is referred to as Algorithm CE in the following."
    }, {
      "heading" : "5 Convergence Analysis",
      "text" : "To analyze convergence, we describe Algorithms EL, PR, and CE as stochastic first-order (SFO) methods in the framework of [7]. We assume lower bounded, differentiable objective functions J(w) with Lipschitz continuous gradient ∇J(w) satisfying\n‖∇J(w + w′)−∇J(w)‖ ≤ L‖w′‖ ∀w,w′, ∃L ≥ 0. (14)\nFor an iterative process of the form wt+1 = wt − γt st, the conditions to be met concern unbiasedness of the gradient estimate\nE[st] = ∇J(wt), ∀t ≥ 0, (15)\nand boundedness of the variance of the stochastic gradient\nE[||st −∇J(wt)|| 2] ≤ σ2, ∀t ≥ 0. (16)\nCondition (15) is met for all three Algorithms by taking expectations over all sources of randomness, i.e., over random inputs and output structures. Assuming ‖φ(x, y)‖ ≤ R, ∆(y) ∈ [0, 1] and g(y) ∈ [0, 1] for all x, y, and since the ratio g(ỹt)\np̂wt (ỹt|xt) is bounded, the variance in condition (16) is bounded.\nNote that the analysis of [7] justifies the use of constant learning rates γt = γ, t = 0, . . . , T .\nConvergence speed can be quantified in terms of the number of iterations needed to reach an accuracy of ǫ for a gradient-based criterion E[‖∇J(wt)‖2] ≤ ǫ. For stochastic optimization of non-convex objectives, the iteration complexity with respect to this criterion is analyzed as O(1/ǫ2) in [7]. This complexity result applies to our Algorithms EL and PR.\nThe iteration complexity of stochastic optimization of (strongly) convex objectives has been analyzed as at best O(1/ǫ) for the suboptimality criterion E[J(wt)] − J(w∗) ≤ ǫ for decreasing learning rates [14].3 Strong convexity of objective (12) can be achieved easily by adding an ℓ2 regularizer λ 2 ‖w‖\n2 with constant λ > 0. Algorithm CE is then modified to use the following regularized update rule wt+1 = wt − γt (st + λT wt).\nThis standard analysis shows two interesting points: First, Algorithms EL and PR can be analyzed as SFO methods where the latter only requires relative preference feedback for learning, while enjoying an iteration complexity that does not depend on the dimensionality of the function as in gradientfree stochastic zeroth-order (SZO) approaches. Second, the standard asymptotic complexity bound of O(1/ǫ2) for non-convex stochastic optimization hides the constants L and σ2 in which iteration complexity increases linearly. As we will show, these constants have a substantial influence, possibly offsetting the advantages in asymptotic convergence speed of Algorithm CE."
    }, {
      "heading" : "6 Experiments",
      "text" : "Measuring Numerical Convergence and Task Loss Performance. In the following, we will present an experimental evaluation for two complex structured prediction tasks from the area of NLP, namely statistical machine translation and noun phrase chunking. Both tasks involve dynamic programming over exponential output spaces, large sparse feature spaces, and non-linear non-decomposable task loss metrics. Training for both tasks was done by simulating bandit feedback by evaluating ∆ against gold standard structures which are never revealed to the learner. We compare the empirical convergence criterion of optimal task performance on development data with numerical results on theoretically motivated convergence criteria.\nFor the purpose of measuring convergence with respect to optimal task performance, we report an evaluation of convergence speed on a fixed set of unseen data as performed in [19]. This instantiates the selection criterion in line (8) in Algorithm 1 to an evaluation of the respective task loss function ∆(ŷwt(x)) under MAP prediction ŷw(x) = argmaxy∈Y(x) pw(y|x) on the development data. This corresponds to the standard practice of online-to-batch conversion where the model selected on the development data is used for final evaluation on a further unseen test set. For bandit structured prediction algorithms, final results are averaged over three runs with different random seeds.\nFor the purpose of obtaining numerical results on convergence speed, we compute estimates of the expected squared gradient norm E[‖∇J(wt)‖2], the Lipschitz constant L and the variance σ2 in which the asymptotic bounds on iteration complexity grow linearly.4 We estimate the squared gradient norm by the squared norm of the stochastic gradient ‖sT ‖2 at a fixed time horizon T . The Lipschitz constant L in equation (14) is estimated by maxi,j\n‖si−sj‖ ‖wi−wj‖\nfor 500 pairs wi and wj randomly drawn from the weights produced during training. The variance σ2 in equation (16) is computed as the empirical variance of the stochastic gradient, taken at regular intervals after each epoch of size D, yielding the estimate 1\nK\n∑K\nk=1 ‖skD − 1 K\n∑K\nk=1 skD‖ 2 where K = ⌊ T D ⌋. All\nestimates include multiplication of the stochastic gradient with the learning rate. For comparability of results across different algorithms, we use the same T and the same constant learning rates for all algorithms.5\nStatistical Machine Translation. In this experiment, an interactive machine translation scenario is simulated where a given machine translation system is adapted to user style and domain based on feedback to predicted translations. Domain adaptation from Europarl to NewsCommentary domains using the data provided at the WMT 2007 shared task is performed for French-to-English translation.6\n3For constant learning rates, [21] show even faster convergence in the search phase of strongly-convex stochastic optimization.\n4For example, these constants appear as O(L ǫ + Lσ\n2\nǫ2 ) in the complexity bound for non-convex stochastic\noptimization of [7]. 5Note that the squared gradient norm upper bounds the suboptimality criterion s.t. ‖∇J(wt)‖2 ≥ 2λJ(wt)]− J(w ∗) for strongly convex functions. Together with the use of constant learning rates this means that we measure convergence to a point near an optimum for strongly convex objectives. 6http://www.statmt.org/wmt07/shared-task.html\nThe MT experiments are based on the synchronous context-free grammar decoder cdec [5]. The models use a standard set of dense and lexicalized sparse features, including an out-of and an indomain language model. The out-of-domain baseline model has around 200k active features. The pre-processing, data splits, feature sets and tuning strategies are described in detail in [19]. The difference in the task loss evaluation between out-of-domain (BLEU: 0.2651) and in-domain (BLEU: 0.2831) models gives the range of possible improvements (1.8 BLEU points) for bandit learning.\nLearning under bandit feedback starts at the learned weights of the out-of-domain median models. It uses parallel in-domain data (news-commentary, 40,444 sentences) to simulate bandit feedback, by evaluating the sampled translation against the reference using as loss function ∆ a smoothed per-sentence 1 − BLEU (zero n-gram counts being replaced with 0.01). After each update, the hypergraph is re-decoded and all hypotheses are re-ranked. Training is distributed across 38 shards using a multitask-based feature selection algorithm [17].\nNoun-phrase Chunking. The experimental setting for chunking is the same as in [19]. Following [16], conditional random fields (CRF) are applied to the noun phrase chunking task on the CoNLL2000 dataset7. The implemented set of feature templates is a simplified version of [16] and leads to around 2M active features. Training under full information with a log-likelihood objective yields 0.935 F1. In difference to machine translation, training with bandit feedback starts from w0 = 0, not from a pre-trained model.\nTask Loss Evaluation. Table 1 lists the results of the task loss evaluation for machine translation and chunking as performed in [19], together with the optimal meta-parameters and the number of iterations needed to find an optimal result on the development set. Note that the pairwise feedback type (cont or bin) is treated as a meta-parameter for Algorithm PR in our simulation experiment. We found that bin is preferable for machine translation and cont for chunking in order to obtain the highest task scores.\nFor machine translation, all bandit learning runs show significant improvements in BLEU score over the out-of-domain baseline. Early stopping by task performance on the development led to the selection of algorithm PR(bin) at a number of iterations that is by a factor of 2-4 smaller compared to Algorithms EL and CE.\nFor the chunking experiment, the F1-score results obtained for bandit learning are close to the fullinformation baseline. The number of iterations needed to find an optimal result on the development set is smallest for Algorithm PR(cont), compared to Algorithms EL and CE. However, the best F1-score is obtained by Algorithm EL.\nNumerical Convergence Results. Estimates of E[‖∇J(wt)‖2], L and σ2 for three runs of each algorithm and task with different random seeds are listed in Table 2.\nFor machine translation, at time horizon T , the estimated squared gradient norm for Algorithm PR is several orders of magnitude smaller than for Algorithms EL and CE. Furthermore, the estimated\n7http://www.cnts.ua.ac.be/conll2000/chunking/\nLipschitz constantL and the estimated variance σ2 are smallest for Algorithm PR. Since the iteration complexity increases linearly with respect to these terms, smaller constants L and σ2 and a smaller value of the estimate E[‖∇J(wt)‖2] at the same number of iterations indicates fastest convergence for Algorithm PR. This theoretically motivated result is consistent with the practical convergence criterion of early stopping on development data: Algorithm PR which yields the smallest squared gradient norm at time horizon T also needs the smallest number of iterations to achieve optimal performance on the development set. In the case of machine translation, Algorithm PR even achieves the nominally best BLEU score on test data.\nFor the chunking experiment, after T iterations, the estimated squared gradient norm and either of the constants L and σ2 for Algorithm PR are several orders of magnitude smaller than for Algorithm CE, but similar to the results for Algorithm EL. The corresponding iteration counts determined by early stopping on development data show an improvement of Algorithm PR over Algorithms CE and EL, however, by a smaller factor than in the machine translation experiment.\nNote that for comparability across algorithms, the same constant learning rates were used in all runs. However, we obtained similar relations between algorithms by using the meta-parameter settings chosen on development data as shown in Table 1. Furthermore, the above tendendencies hold for both settings of the meta-parameter bin or cont of Algorithm PR."
    }, {
      "heading" : "7 Conclusion",
      "text" : "We presented learning objectives and algorithms for stochastic structured prediction under bandit feedback. The presented methods “banditize” well-known approaches to probabilistic structured prediction such as expected loss minimization, pairwise preference ranking, and cross-entropy minimization. We presented a comparison of practical convergence criteria based on early stopping with theoretically motivated convergence criteria based on the squared gradient norm. Our experimental results showed fastest convergence speed under both criteria for pairwise preference learning. Our numerical evaluation showed smallest variance for pairwise preference learning, which possibly explains fastest convergence despite the underlying non-convex objective. Furthermore, since this algorithm requires only easily obtainable relative preference feedback for learning, it is an attractive choice for practical interactive learning scenarios.\nAcknowledgments.\nThis research was supported in part by the German research foundation (DFG), and in part by a research cooperation grant with the Amazon Development Center Germany."
    } ],
    "references" : [ {
      "title" : "Optimal algorithms for online convex optimization with multi-point bandit feedback. In COLT",
      "author" : [ "A. Agarwal", "O. Dekel", "L. Xiao" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2010
    }, {
      "title" : "A survey of preference-based online learning with bandit algorithms. In ALT",
      "author" : [ "R. Busa-Fekete", "E. Hüllermeier" ],
      "venue" : null,
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2014
    }, {
      "title" : "Learning to search better than your teacher",
      "author" : [ "Chang", "K.-W", "A. Krishnamurthy", "A. Agarwal", "H. Daume", "J. Langford" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2015
    }, {
      "title" : "Optimal rates for zero-order convex optimization: The power of two function evaluations",
      "author" : [ "J.C. Duchi", "M.I. Jordan", "M.J. Wainwright", "A. Wibisono" ],
      "venue" : "IEEE Translactions on Information",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2015
    }, {
      "title" : "cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models",
      "author" : [ "C. Dyer", "A. Lopez", "J. Ganitkevitch", "J. Weese", "F. Ture", "P. Blunsom", "H. Setiawan", "V. Eidelman", "P. Resnik" ],
      "venue" : "In ACL Demo",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2010
    }, {
      "title" : "An efficient boosting algorithm for combining preferences",
      "author" : [ "Y. Freund", "R. Iyer", "R.E. Schapire", "Y. Singer" ],
      "venue" : null,
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2003
    }, {
      "title" : "Stochastic first- and zeroth-order methods for nonconvex stochastic programming",
      "author" : [ "S. Ghadimi", "G. Lan" ],
      "venue" : "SIAM J. on Optimization,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2012
    }, {
      "title" : "Large margin rank boundaries for ordinal regression",
      "author" : [ "R. Herbrich", "T. Graepel", "K. Obermayer" ],
      "venue" : "In Advances in Large Margin Classifiers,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2000
    }, {
      "title" : "Truncated importance sampling",
      "author" : [ "E.L. Ionides" ],
      "venue" : "J. of Comp. and Graph",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2008
    }, {
      "title" : "Optimizing search engines using clickthrough data",
      "author" : [ "T. Joachims" ],
      "venue" : "In KDD",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2002
    }, {
      "title" : "The epoch-greedy algorithm for contextual multi-armed bandits",
      "author" : [ "J. Langford", "T. Zhang" ],
      "venue" : "In NIPS",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2007
    }, {
      "title" : "Learning with stochastic inputs and adversarial outputs",
      "author" : [ "A. Lazaric", "R. Munos" ],
      "venue" : "Journal of Computer and System Sciences,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2012
    }, {
      "title" : "A contextual-bandit approach to personalized news article recommendation",
      "author" : [ "L. Li", "W. Chu", "J. Langford", "R.E. Schapire" ],
      "venue" : null,
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2010
    }, {
      "title" : "Introduction to Optimization",
      "author" : [ "B.T. Polyak" ],
      "venue" : "Optimization Software, Inc.,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1987
    }, {
      "title" : "Sequence level training with recurrent neural networks. In ICLR",
      "author" : [ "M. Ranzato", "S. Chopra", "M. Auli", "W. Zaremba" ],
      "venue" : null,
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2016
    }, {
      "title" : "Shallow parsing with conditional random fields",
      "author" : [ "F. Sha", "F. Pereira" ],
      "venue" : "In NAACL",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2003
    }, {
      "title" : "Joint feature selection in distributed stochastic learning for large-scale discriminative training in SMT",
      "author" : [ "P. Simianer", "S. Riezler", "C. Dyer" ],
      "venue" : null,
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2012
    }, {
      "title" : "Linguistic Structure Prediction",
      "author" : [ "N.A. Smith" ],
      "venue" : null,
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2011
    }, {
      "title" : "Learning structured predictors from bandit feedback for interactive NLP",
      "author" : [ "A. Sokolov", "J. Kreutzer", "C. Lo", "S. Riezler" ],
      "venue" : null,
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2016
    }, {
      "title" : "Bandit structured prediction for learning from user feedback in statistical machine translation",
      "author" : [ "A. Sokolov", "S. Riezler", "T. Urvoy" ],
      "venue" : "In MT Summit XV",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2015
    }, {
      "title" : "Incremental gradient algorithms with stepsizes bounded away from zero",
      "author" : [ "M.V. Solodov" ],
      "venue" : "Computational Optimization and Applications,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 1998
    }, {
      "title" : "Policy gradient methods for reinforcement learning with function approximation",
      "author" : [ "R.S. Sutton", "D. McAllester", "S. Singh", "Y. Mansour" ],
      "venue" : null,
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2000
    }, {
      "title" : "A law of comparative judgement",
      "author" : [ "L.L. Thurstone" ],
      "venue" : "Psychological Review,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 1927
    }, {
      "title" : "Interactively optimizing information retrieval systems as a dueling bandits problem",
      "author" : [ "Y. Yue", "T. Joachims" ],
      "venue" : null,
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2009
    }, {
      "title" : "Probabilistic models of vision and max-margin methods",
      "author" : [ "A. Yuille", "X. He" ],
      "venue" : "Frontiers of Electrical and Electronic Engineering,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 17,
      "context" : "We present algorithms that use this feedback to “banditize” expected loss minimization approaches to structured prediction [18, 25].",
      "startOffset" : 123,
      "endOffset" : 131
    }, {
      "referenceID" : 24,
      "context" : "We present algorithms that use this feedback to “banditize” expected loss minimization approaches to structured prediction [18, 25].",
      "startOffset" : 123,
      "endOffset" : 131
    }, {
      "referenceID" : 19,
      "context" : "A practical example is interactive machine translation where instead of human generated reference translations only translation quality judgments on predicted translations are used for learning [20].",
      "startOffset" : 194,
      "endOffset" : 198
    }, {
      "referenceID" : 18,
      "context" : "[19] showed that partial feedback is indeed sufficient for optimization of feature-rich linear structured prediction over large output spaces in various natural language processing (NLP) tasks.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 6,
      "context" : "The contribution of our paper is to analyze these algorithms as stochastic first-order (SFO) methods in the framework of [7] and investigate the connection of optimization for task performance with optimization-theoretic concepts of convergence.",
      "startOffset" : 121,
      "endOffset" : 124
    }, {
      "referenceID" : 19,
      "context" : "Our analysis starts with revisiting the approach to stochastic optimization of a non-convex expected loss criterion presented by [20].",
      "startOffset" : 129,
      "endOffset" : 133
    }, {
      "referenceID" : 6,
      "context" : "The iteration complexity of stochastic optimization of a non-convex objective J(wt) can be analyzed in the framework of [7] as O(1/ǫ) in terms of the number of iterations needed to reach an accuracy of ǫ for the criterion E[‖∇J(wt)‖] ≤ ǫ.",
      "startOffset" : 120,
      "endOffset" : 123
    }, {
      "referenceID" : 18,
      "context" : "[19] attempt to improve convergence speed by introducing a cross-entropy objective that can be seen as a (strong) convexification of the expected loss objective.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 18,
      "context" : "Lastly, we analyze the pairwise preference learning algorithm introduced by [19].",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 23,
      "context" : "To our knowledge, this is the first SFO approach to stochastic learning form pairwise comparison feedback, while related approaches fall into the area of gradient-free stochastic zeroth-order (SZO) approaches [24, 1, 7, 4].",
      "startOffset" : 209,
      "endOffset" : 222
    }, {
      "referenceID" : 0,
      "context" : "To our knowledge, this is the first SFO approach to stochastic learning form pairwise comparison feedback, while related approaches fall into the area of gradient-free stochastic zeroth-order (SZO) approaches [24, 1, 7, 4].",
      "startOffset" : 209,
      "endOffset" : 222
    }, {
      "referenceID" : 6,
      "context" : "To our knowledge, this is the first SFO approach to stochastic learning form pairwise comparison feedback, while related approaches fall into the area of gradient-free stochastic zeroth-order (SZO) approaches [24, 1, 7, 4].",
      "startOffset" : 209,
      "endOffset" : 222
    }, {
      "referenceID" : 3,
      "context" : "To our knowledge, this is the first SFO approach to stochastic learning form pairwise comparison feedback, while related approaches fall into the area of gradient-free stochastic zeroth-order (SZO) approaches [24, 1, 7, 4].",
      "startOffset" : 209,
      "endOffset" : 222
    }, {
      "referenceID" : 6,
      "context" : "Convergence rate for SZO methods depends on the dimensionality d of the function to be evaluated, for example, the non-convex SZO algorithm of [7] has an iteration complexity of O(d/ǫ).",
      "startOffset" : 143,
      "endOffset" : 146
    }, {
      "referenceID" : 21,
      "context" : "Most closely related are recent applications of policy gradient methods to exponential output spaces in NLP problems [22, 3, 15].",
      "startOffset" : 117,
      "endOffset" : 128
    }, {
      "referenceID" : 2,
      "context" : "Most closely related are recent applications of policy gradient methods to exponential output spaces in NLP problems [22, 3, 15].",
      "startOffset" : 117,
      "endOffset" : 128
    }, {
      "referenceID" : 14,
      "context" : "Most closely related are recent applications of policy gradient methods to exponential output spaces in NLP problems [22, 3, 15].",
      "startOffset" : 117,
      "endOffset" : 128
    }, {
      "referenceID" : 10,
      "context" : "Contextual one-state MDPs are also known as contextual bandits [11, 13] which operate in a scenario of maximizing the expected reward for selecting an arm of a multi-armed slot machine.",
      "startOffset" : 63,
      "endOffset" : 71
    }, {
      "referenceID" : 12,
      "context" : "Contextual one-state MDPs are also known as contextual bandits [11, 13] which operate in a scenario of maximizing the expected reward for selecting an arm of a multi-armed slot machine.",
      "startOffset" : 63,
      "endOffset" : 71
    }, {
      "referenceID" : 11,
      "context" : "In the spectrum of stochastic versus adversarial bandits, our approach is semi-adversarial in making stochastic assumptions on inputs, but not on rewards [12].",
      "startOffset" : 154,
      "endOffset" : 158
    }, {
      "referenceID" : 7,
      "context" : "Pairwise preference learning has been studied in the full information supervised setting [8, 10, 6] where given preference pairs are assumed.",
      "startOffset" : 89,
      "endOffset" : 99
    }, {
      "referenceID" : 9,
      "context" : "Pairwise preference learning has been studied in the full information supervised setting [8, 10, 6] where given preference pairs are assumed.",
      "startOffset" : 89,
      "endOffset" : 99
    }, {
      "referenceID" : 5,
      "context" : "Pairwise preference learning has been studied in the full information supervised setting [8, 10, 6] where given preference pairs are assumed.",
      "startOffset" : 89,
      "endOffset" : 99
    }, {
      "referenceID" : 23,
      "context" : "Work on stochastic pairwise learning has been formalized as derivative-free stochastic zeroth-order optimization [24, 1, 7, 4].",
      "startOffset" : 113,
      "endOffset" : 126
    }, {
      "referenceID" : 0,
      "context" : "Work on stochastic pairwise learning has been formalized as derivative-free stochastic zeroth-order optimization [24, 1, 7, 4].",
      "startOffset" : 113,
      "endOffset" : 126
    }, {
      "referenceID" : 6,
      "context" : "Work on stochastic pairwise learning has been formalized as derivative-free stochastic zeroth-order optimization [24, 1, 7, 4].",
      "startOffset" : 113,
      "endOffset" : 126
    }, {
      "referenceID" : 3,
      "context" : "Work on stochastic pairwise learning has been formalized as derivative-free stochastic zeroth-order optimization [24, 1, 7, 4].",
      "startOffset" : 113,
      "endOffset" : 126
    }, {
      "referenceID" : 17,
      "context" : "[18, 25] introduce the expected loss criterion for structured prediction as the minimization of the expectation of a given task loss function with respect to the conditional distribution over structured outputs.",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 24,
      "context" : "[18, 25] introduce the expected loss criterion for structured prediction as the minimization of the expectation of a given task loss function with respect to the conditional distribution over structured outputs.",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 0,
      "context" : "Let X be a structured input space, let Y(x) be the set of possible output structures for input x, and let ∆y : Y → [0, 1] quantify the loss ∆y(y) suffered for predicting y instead of the gold standard structure y.",
      "startOffset" : 115,
      "endOffset" : 121
    }, {
      "referenceID" : 19,
      "context" : "[20] presented an algorithm that minimizes the following expected loss objective.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 22,
      "context" : "Decomposing complex problems into a series of pairwise comparisons has been shown to be advantageous for human decision making [23].",
      "startOffset" : 127,
      "endOffset" : 131
    }, {
      "referenceID" : 18,
      "context" : "This idea is formalized in [19] as an expected loss objective with respect to a conditional distribution of pairs of structured outputs.",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 0,
      "context" : "Let P(x) = {〈yi, yj〉 |yi, yj ∈ Y(x)} denote the set of output pairs for an input x, and let ∆(〈yi, yj〉) : P(x) → [0, 1] denote a task loss function that specifies a dispreference of yi compared to yj .",
      "startOffset" : 113,
      "endOffset" : 119
    }, {
      "referenceID" : 1,
      "context" : "See [2] for an overview of bandit learning from consistent and inconsistent pairwise comparisons.",
      "startOffset" : 4,
      "endOffset" : 7
    }, {
      "referenceID" : 18,
      "context" : "This motivated the formalization of a convex upper bounds on expected normalized loss in [19].",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 18,
      "context" : "[19] thus work with an unnormalized gain function g(y) that preserves convexity.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 8,
      "context" : "We deal with this problem by clipping too small sampling probabilities to p̂wt(ỹt|xt) = max{pwt(ỹt|xt), k} for a constant k [9].",
      "startOffset" : 124,
      "endOffset" : 127
    }, {
      "referenceID" : 6,
      "context" : "To analyze convergence, we describe Algorithms EL, PR, and CE as stochastic first-order (SFO) methods in the framework of [7].",
      "startOffset" : 122,
      "endOffset" : 125
    }, {
      "referenceID" : 0,
      "context" : "Assuming ‖φ(x, y)‖ ≤ R, ∆(y) ∈ [0, 1] and g(y) ∈ [0, 1] for all x, y, and since the ratio g(ỹt) p̂wt (ỹt|xt) is bounded, the variance in condition (16) is bounded.",
      "startOffset" : 31,
      "endOffset" : 37
    }, {
      "referenceID" : 0,
      "context" : "Assuming ‖φ(x, y)‖ ≤ R, ∆(y) ∈ [0, 1] and g(y) ∈ [0, 1] for all x, y, and since the ratio g(ỹt) p̂wt (ỹt|xt) is bounded, the variance in condition (16) is bounded.",
      "startOffset" : 49,
      "endOffset" : 55
    }, {
      "referenceID" : 6,
      "context" : "Note that the analysis of [7] justifies the use of constant learning rates γt = γ, t = 0, .",
      "startOffset" : 26,
      "endOffset" : 29
    }, {
      "referenceID" : 6,
      "context" : "For stochastic optimization of non-convex objectives, the iteration complexity with respect to this criterion is analyzed as O(1/ǫ) in [7].",
      "startOffset" : 135,
      "endOffset" : 138
    }, {
      "referenceID" : 13,
      "context" : "The iteration complexity of stochastic optimization of (strongly) convex objectives has been analyzed as at best O(1/ǫ) for the suboptimality criterion E[J(wt)] − J(w) ≤ ǫ for decreasing learning rates [14].",
      "startOffset" : 202,
      "endOffset" : 206
    }, {
      "referenceID" : 18,
      "context" : "For the purpose of measuring convergence with respect to optimal task performance, we report an evaluation of convergence speed on a fixed set of unseen data as performed in [19].",
      "startOffset" : 174,
      "endOffset" : 178
    }, {
      "referenceID" : 20,
      "context" : "6 For constant learning rates, [21] show even faster convergence in the search phase of strongly-convex stochastic optimization.",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 6,
      "context" : "For example, these constants appear as O( ǫ + Lσ 2 ǫ ) in the complexity bound for non-convex stochastic optimization of [7].",
      "startOffset" : 121,
      "endOffset" : 124
    }, {
      "referenceID" : 18,
      "context" : "002 1e-4 Table 1: Test set evaluation for stochastic learning under bandit feedback from [19], for chunking under F1-score, and for machine translation under BLEU.",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 4,
      "context" : "The MT experiments are based on the synchronous context-free grammar decoder cdec [5].",
      "startOffset" : 82,
      "endOffset" : 85
    }, {
      "referenceID" : 18,
      "context" : "The pre-processing, data splits, feature sets and tuning strategies are described in detail in [19].",
      "startOffset" : 95,
      "endOffset" : 99
    }, {
      "referenceID" : 16,
      "context" : "Training is distributed across 38 shards using a multitask-based feature selection algorithm [17].",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 18,
      "context" : "The experimental setting for chunking is the same as in [19].",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 15,
      "context" : "Following [16], conditional random fields (CRF) are applied to the noun phrase chunking task on the CoNLL2000 dataset7.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 15,
      "context" : "The implemented set of feature templates is a simplified version of [16] and leads to around 2M active features.",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 18,
      "context" : "Table 1 lists the results of the task loss evaluation for machine translation and chunking as performed in [19], together with the optimal meta-parameters and the number of iterations needed to find an optimal result on the development set.",
      "startOffset" : 107,
      "endOffset" : 111
    } ],
    "year" : 2016,
    "abstractText" : "Stochastic structured prediction under bandit feedback follows a learning protocol where on each of a sequence of iterations, the learner receives an input, predicts an output structure, and receives partial feedback in form of a task loss evaluation of the predicted structure. We present applications of this learning scenario to convex and non-convex objectives for structured prediction and analyze them as stochastic first-order methods. We present an experimental evaluation on problems of natural language processing over exponential output spaces, and compare convergence speed across different objectives under the practical criterion of optimal task performance on development data and the optimization-theoretic criterion of minimal squared gradient norm. Best results under both criteria are obtained for a non-convex objective for pairwise preference learning under bandit feedback.",
    "creator" : "LaTeX with hyperref package"
  }
}