{
  "name" : "1611.06423.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Incorporating Pass-Phrase Dependent Background Models for Text-Dependent Speaker Verification",
    "authors" : [ "A. K. Sarkar", "Zheng-Hua Tan" ],
    "emails" : [ "akc@es.aau.dk,", "zt@es.aau.dk" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n61 1.\n06 42\n3v 2\n[ cs\n.C L\n] 1\n2 Ju\nn 20\n17\nIn this paper, we propose pass-phrase dependent background models (PBMs) for text-dependent (TD) speaker verification (SV) to integrate the pass-phrase identification process into the conventional TD-SV system, where a PBM is derived from a textindependent background model through adaptation using the utterances of a particular pass-phrase. During training, pass-phrase specific target speaker models are derived from the particular PBM using the training data for the respective target model. While testing, the best PBM is first selected for the test utterance in the maximum likelihood (ML) sense and the selected PBM is then used for the log likelihood ratio (LLR) calculation with respect to the claimant model. The proposed method incorporates the pass-phrase identification step in the LLR calculation, which is not considered in conventional standalone TD-SV systems. The performance of the proposed method is compared to conventional text-independent backgroundmodel based TD-SV systems using either Gaussian mixture model (GMM)-universal background model (UBM) or Hidden Markov model (HMM)-UBM or i-vector paradigms. In addition, we consider two approaches to build PBMs: speaker-independent and speaker-dependent. We show that the proposed method significantly reduces the error rates of text-dependent speaker verification for the non-target types: target-wrong and imposter-wrong while it maintains comparable TD-SV performance when imposters speak a correct utterance with respect to the conventional system. Experiments are conducted on the RedDots challenge and the RSR2015 databases that consist of short utterances.\nKey words: Pass-phrase dependent background models (PBMs), GMM-UBM, HMM-UBM, i-vector, text-dependent, speaker verification"
    }, {
      "heading" : "1. Introduction",
      "text" : "Speaker verification (SV) [1, 2] is the process of authentication of a person’s claimed identity by analyzing his/her speech signal. It is a binary pattern recognition problem where a SV system makes the decision by calculating the log-likelihood ratio (LLR) between the claimant and background models (also called alternative/negative hypothesis) for the test signal. If the LLR value is greater than a pre-defined threshold, the claimant is accepted and otherwise it is rejected.\nSpeaker verification systems are broadly divided into two categories: text-independent (TI) and text-dependent (TD). In TI-SV, speakers are free to speak any sentences, i.e. phrases, during the enrollment as well as the test phases. It does not impose any constraint that enrollment and test utterances are to be the same phrase. However, TD-SV systems require speakers to speak within pre-defined sentences, i.e. fixed pass-phrases during the speaker enrollment and test phases.\nIn real-life applications, we need a speaker verification system that is accurate on short utterances. In this regard, TDSV systems are the ideal choice. Since speakers use the same pass-phrase during both the enrollment and test phases, it provides a well matched phonetic content between the enrollment and test phases. Therefore, TD-SV systems are more accurate compared to their TI-SV counterparts. Over the last decades,\nmany techniques have been introduced in literature to improve the performance of TD-SV on short utterances. Examples are deep neural network (DNN) [3, 4, 5], i-vector [4, 6], hierarchical multi-Layer acoustic model (HiLAM) [2, 7], phonedependent Hidden Markov model (HMM) [8, 9], domain adaptation [10] and phonetic higher level maximum likelihood linear regression (MLLR) super-vector based features [11]. In [3, 4], phonetic information is incorporated into an i-vector system by accumulating statistics from speech with respect to a pre-defined phonetic class through an DNN based automatic speech recognition (ASR) system. In [5], the intermediate output of the DNN layers are used to vectorize characterization of speech data. HiLAM builds a HMM model by concatenating the speech segment-wise adapted models from the Gaussian mixture model- universal background model (GMMUBM) [2]. In domain adaptation [10], the mismatch between the text-independent and the text-dependent data is reduced by transforming the text-independent data to better match the textdependent task (using the a-priori transcription knowledge of the text-dependent data). In conventional HMM based TDSV systems [8, 9], phoneme (context) dependent speaker models are built using the knowledge of speech transcriptions. In [11], a speech signal is represented by a super-vector concatenation of MLLR transformations estimated with respect to a\nSubmitted to .....\npre-defined phonetic class (e.g. vowel and consonant) using automatic speech recognition (ASR).\nAll of these techniques need a background model as an alternative/negative hypothesis for TD-SV. A single textindependent background model (either gender dependent or independent) is commonly used in literature where target speakers are represented by models (say in GMM-UBM framework) derived from the background model. In [12, 13, 14], a multiple background model concept is proposed to improve the performance of the conventional speaker verification system, by training background models (BMs) based on the vocal tract length (VTL) characteristic of target speakers as in [14] and passphrases as in [12, 13] for text-independent and text-dependent speaker verification, respectively. During enrollment, target speaker models are derived from the BM based on VTL in [14] and pass-phrase of target data in [12, 13]). In the test phase, a test utterance is scored against the claimant and background models specific for the claimant (defined during the enrollment phase) in order to calculate the log-likelihood ratio. However, this [12, 13, 14] does not incorporate the passphrase identification process to address the following two nontarget types: target-wrong and imposter-wrong. Recently, in [15], the authors proposed a fusion system which combines the score/decision of an utterance verification system with a conventional SV system to improve the performance of the TD-SV system against target/imposter-wrong non-target trials.\nIn this paper, we propose pass-phrase specific background models (PBMs) for TD-SV to integrate the utterance identification process (without an extra separate system) into the conventional SV system, aiming at rejecting more non-target types: target-wrong and imposter-wrong than the conventional SV system while maintaining the performance for the impostercorrect non-target type. In the proposed method, PBMs are derived from the text-independent background model by pooling the training data for a particular pass-phrase from many speakers. During enrollment, pass-phrase specific target speaker models are derived from the particular PBM with MAP adaptation by using the training data for the respective target model. In the test phase, the best PBM is selected for a particular test utterance in the maximum likelihood (ML) sense and the best selected PBM is used as an alternative hypothesis for the loglikelihood ratio calculation with respect to the claimant model, which differs from [12, 13, 14]. Furthermore two strategies are considered for building the PBM: one is called speakerindependent (SI) and the other is speaker-dependent (SD) again in contrast to [12, 13, 14] where only SI-PBM concept is considered. In the SI-PBM case, PBMs are built using data from the non-target speakers who are not participating in the evaluation task, which reflects the scenario when no priori knowledge about the particular target speaker is given. It generates a global set of PBMs which are common to all target speakers and used during their training and testing phases. In the SDPBM case, PBMs are built by using the particular target speaker training data together with the non-target speakers data. This yields a separate set of PBMs for the particular target speakers that are used during their training and test phases. Since SDPBM incorporates a priori knowledge about the particular tar-\nget speaker in the PBMs, we called it speaker-dependent. The main salient feature of the proposed method is that it incorporates the utterance identification in the LLR calculation process in the conventional approach in contrast to [12, 13, 14].\nWe study the performance of the proposed PBM based TDSV system under the GMM and recently proposed HMM modeling [16] and i-vector paradigms. Experiments are conducted on the RedDots Challenge [17] and the RSR2015 [18] databases. The proposed PBM methods yield significantly better results than those of the conventional text-independent background model based TD-SV method for non-target types: target-wrong and imposter-wrong while providing a comparable performance for imposter-correct.\nThe reminder of this paper is organized as follows: Section 2 describes the GMM-UBM, HMM-UBM and i-vector based TD-SV methods. Section 3 describes the proposed methods. Experimental setup is given in Section 4. Results and discussion are presented in Section 5. Finally, the paper is concluded in Section 6."
    }, {
      "heading" : "2. Text-dependent speaker verification methods",
      "text" : "In this section, we briefly describe the GMM-UBM, HMMUBM and i-vector techniques for text-dependent speaker verification, which are considered for comparing the performance of the proposed methods with the existing ones."
    }, {
      "heading" : "2.1. Gaussian mixture model- universal background model based method",
      "text" : "In this method, a largeGaussian mixturemodel called GMMUBM [2] is built using data with different textual contents from non-target speakers. It represents a large acoustic space that covers all sorts of attributes available in the training data. Then, pass-phrase-specific target speaker models are derived from the text-independent GMM-UBM with maximum a posteriori (MAP) adaptation using the training data for the particular target model. In the test phase, a test utterance X = {x1, x2, . . . , xL} is scored against the claimant λr and GMM-UBM λUBM for log likelihood ratio calculation,\nΛ(X) = 1\nL\nL ∑\nt=1\n{ log p(xt|λr) − log p(xt|λUBM) }\n(1)\nwhere p(xt|λr) denotes the likelihood value for a given feature vector xt with respect to the model λr. The GMM-UBM can be either gender-dependent or gender-independent. In genderdependent case, speaker models are derived from the GMMUBM with respect to their gender. It is well established that the gender-dependentGMM-UBM system shows slightly better performance than the gender-independent one. Fig.1 illustrates how a GMM-UBM speaker verification system works."
    }, {
      "heading" : "2.2. Hidden Markov model - universal background model based method",
      "text" : "In this method [16], an HMM called ‘HMM-UBM’ is built using data from many non-target speakers without any speech\ntranscription. A dummyword (such as ‘Hi’) is used as the label (without phonetic level break up) for all training speech data during the HMM training. HMM-UBM is trained by pooling all training data together and iteratively updating its parameters using the Baum-Welch re-estimation algorithm. Since no transcriptions are considered during HMM training, state transition probabilities of the HMM-UBM [19] will inherently reflect the speaker-independent temporal information available within the data. However, this temporal information is not accounted for in the conventional GMM-UBM based TD-SV systems.\nSimilarly to the GMM-UBM TD-SV system, pass-phrasespecific target speaker models are derived from the textindependent HMM-UBM with MAP adaptation [20] using the training data for the particular target model. In test, a test utterance is forced-aligned against the claimant and the HMMUBMmodels for the LLR score calculation. Fig.2 illustrates the training of HMM-UBM in an un-supervised manner for textdependent speaker verification.\nIt should be noted that the HMM-UBM approach is different from the HiLAM proposed in [7] in two ways: First, HMMUBM is trained by pooling all training data together using the Baum-Welch re-estimation algorithm, and then target speaker models are derived from the HMM-UBM with MAP using the training data of particular target speaker model. On the other hand, HiLAM builds an target specific HMM by concatenation of speech segment-wise adapted GMM models as state (from GMM-UBM) and uniformly assigning the transition probabilities across the states of HMM. Secondly, in the HMM-UBM method, both claimant and background models are considered under the same HMM modeling paradigm. HiLAM, however, uses HMM for target speaker modeling and GMM-UBM as a negative hypothesis during test."
    }, {
      "heading" : "2.3. i-vector based method",
      "text" : "In this method, a speech utterance is represented by a vector in a low dimensional subspace in the GMM-UBM super-vector\ndomain called total variability space where speaker and channel information is assumed dense. It is generally expressed as,\nM = m + Tw (2)\nwhere w is called an i-vector. M, m and T denote the utterance dependent GMM super-vector, the speaker-independent GMM super-vector obtained by concatenating the mean vectors from the GMM-UBM and the total variability space, respectively. The following steps are involved during i-vector estimation for a given speech signal X = {x1, x2, . . . , xL} using the GMM-UBM λUBM and T space.\n- Estimate sufficient statistics,\nPr(c|xt) = ωcpc(xt)\n∑C j=1 ω jp j(xt)\n(3)\n(0th order)Nc =\nL ∑\nt=1\nPr(c|xt, λUBM) (4)\n(1storder) Fc =\nL ∑\nt=1\nPr(c|xt, λUBM)xt (5)\n- Centralize Fc statistics w.r.t GMM-UBM\nF̃c =\nL ∑\nt=1\nPr(c|xt, λUBM)(xt − µc) (6)\n- Obtain i-vector w for X using the statistics,\nw = (I + T ′ Σ−1N(X)T )−1.T ′ Σ−1F̃(X) (7)\nwhere c, N(X) and ˜F(X) represent the cth mixtures of GMMUBM, the CF × CF block diagonal matrix, and the CF × 1 super-vector obtained by concatenating the first order statistics F̃C for the utterance X, respectively. Pr(c|xt) denotes the posteriori alignment of feature vector xt corresponding to the mixture component c. ωc and pc indicate the weight and Gaussian density function of cth mixture component of GMM-UBM, respectively. Σ represents a diagonal covariance matrix of dimension CF × CF estimated during factor analysis training. C and F are respectively, the number of mixtures in GMM-UBM and the dimension of feature vector. µc denotes the mean vector corresponding to the cth mixture of GMM-UBM. (′) is the matrix transpose operation. I denotes the identity matrix. The total variability space T is trained using data from many nontarget speakers in the expectation-maximization (EM) sense. For more details about the i-vector see [6].\nDuring enrollment, each target is represented by an average i-vector. The average i-vector is computed over the i-vectors of each speech file available for training the particular target model. An i-vector for a particular speech file is extracted using sufficient statistics with respect to the GMM-UBM, followed by projection onto the total variability space T . In the test phase, the score between the i-vector of the test utterance and the claimant specific is calculated using probabilistic linear\ndiscriminate analysis (PLDA). PLDA is a generative modeling approach which decomposes the i-vector into several components with a joint factor analysis (JFA) [21] framework as:\nw = µw + Φy + Γz + ǫ (8)\nwhere Φ and Γ are matrices denoting the eigen voice and eigen channel subspaces, respectively. y and z are the speaker and channel factors, respectively, with a priori normal distribution. ǫ represents the residual noise. Φ, Γ and ǫ are iteratively updated during training process by pooling a numbers of i-vectors per speaker class from many speakers. During test, the score between two i-vectors (w1, w2) is calculated as:\nscore(w1,w2) = log p(w1,w2|θtar)\np(w1,w1|θnon) (9)\nhypothesis θtar states that w1 and w2 come from the same speaker, and hypothesis θnon states that they are from different speakers. For more details about the PLDA based scoring see [22, 23, 24].\nPrior to PLDA, i-vectors are post-processed for session variability compensation using an iterative conditioning algorithm called spherical normalization (Sph) proposed in [22]. It has been shown in [22] that Sph improves the speaker verification performance of PLDA based systems when compared to other conventional approaches."
    }, {
      "heading" : "3. Proposed methods",
      "text" : ""
    }, {
      "heading" : "3.1. Gaussian mixture model- universal background model based PBM TD-SV system",
      "text" : "In this case, pass-phrase specific background models are derived from the text-independent GMM-UBM with MAP adaptation by pooling the data of the particular pass-phrase of many speakers. Concerning the real scenarios, we consider two approaches: one is called speaker-independent and the other is speaker-dependent.\n• Speaker-independent PBM (SI-PBM): PBMs are built\nusing data from non-target speakers who do not participate in the evaluation task. It gives a global set of PBMs common to all target speakers. Since this PBM approach does not use any data/information from a particular target speaker, we call it speaker-independent.\n• Speaker-dependent PBM (SD-PBM): In this case, each\ntarget speaker specific PBMs are built by pooling the passphrase specific training data of both the particular target speaker and non-target speakers. This creates PBMs specific for the particular target speaker. Since SD-PBMs use the training data from a particular target speaker, therefore contains priori information about the particular target speaker and thus call it speaker-dependent.\nAfter building the PBMs, pass-phrase specific target speaker models are derived from the particular PBM with MAP adaptation by using his/her training data for the particular target model. Algorithm 1 describes the enrollment phase of\nthe target speaker model using PBMs.\nAlgorithm 1: Enrollment phase\nInitial: load the PBMs\nStep1: Read all the enrollment data Xr of pass-phrase specific target model, r\nStep2: Choose the PBM as per enrollment data\nStep3: Derive the target model λr from the chosen PBM with MAP adaptation using the enrollment data Xr\nStep4: Repeat the Step 1 to 3 for all target models\nIn the test phase, the best PBM for the particular test utterance Y = {y1, y2, . . . , yL} is first selected from the PBMs obtained in training phase in the ML sense. Then, the selected PBM is used as an alternative hypothesis for the log likelihood ratio calculation with respect to the claimant model, λr. Algorithm 2 presents the test phase of the proposed method.\nAlgorithm 2: Test phase\nStep1: Load the feature vectors of a test utterance Y = {y1, y2, . . . , yL}, PBMs and the target model λr for the claimant, r\nStep2: Find the best PBM in the ML sense for the test utterance,\nîPBM = arg max i p(Y |λPBMi) (10)\nStep3: Calculate the log likelihood ratio as,\nΛ(Y) = 1\nL\nL ∑\nt=1\n{ log p(yt|λr) − log p(yt|λîPBM ) }\n(11)\nBased on speaker independent/dependent PBMs, it yields two sub-systems: GMM-UBM speaker-independent PBM (GMM-UBM-SI-PBM) and GMM-UBM speaker-dependent PBM (GMM-UBM-SD-PBM) TD-SV systems. Fig.3 shows the block diagram of the proposed PBM based text-dependent speaker verification system in the GMM-UBM paradigm."
    }, {
      "heading" : "3.2. HiddenMarkov model- universal backgroundmodel based PBM TD-SV system",
      "text" : "This system is similar to the GMM-UBM based PBM TD-SV system. The main difference is that HMM-UBM is considered instead of GMM-UBM.\nAs we also consider both speaker independent and dependent training strategies for building the PBMs, we get two sub-systems called HMM-UBM speaker-independent PBM (HMM-UBM-SI-PBM) and HMM-UBM speaker-dependent PBM (HMM-UBM-SD-PBM ) TD-SV systems."
    }, {
      "heading" : "3.3. i-vector based PBM TD-SV system",
      "text" : "We further investigate the GMM-UBM PBM based TD-SV system in i-vector paradigm. In the enrollment phase, zero and first order statistics are estimated with respect to the particular PBM based on the target model of specific pass-phrase for all training speech files that individually belongs to the respective target model. Then, statistics are projected onto the total variability space, T to get an i-vector for each speech file. After that, the target is represented by an average i-vector computed over the speech file-wise i-vector. Before projecting the first-order statistics onto the T space, they are centralized with respect to the text-independent GMM-UBM i.e. both the proposed method and conventional system use the same T space. Algorithm 3 describes the enrollment phase of the target speakers in the PBM based i-vector system. It is also noted that PBMs are derived from the GMM-UBM with MAP only updating the Gaussian mean vectors and hence all the PBMs and GMM-UBM share the same Gaussian weight and covariance matrix (except mean) parameters across the Gaussian components.\nAlgorithm 3: Enrollment phase\nInitial: Load GMM-UBM, T matrix and PBMs\nStep1: Read all enrollment data Xr for the target model, r (for a particular pass-phrase)\nStep2: Choose the PBM (e.g. λPBM) as per pass-phrase of enrollment data\nStep3: Estimate sufficient statistics for ath speech file ǫ Xr, say Xar = {x1, x2, . . . , xL} w.r.t λPBM model,\n(0th order)Nc =\nL ∑\nt=1\nPr(c|xt, λPBM) (12)\n(1storder) Fc =\nL ∑\nt=1\nPr(c|xt, λPBM)xt (13)\n- Centralized Fc statistics w.r.t GMM-UBM\nF̃c =\nL ∑\nt=1\nPr(c|xt, λPBM)(xt − µc) (14)\n- Obtained i-vector war using the statistics for X a r as,\nwar = (I + T ′ Σ−1N(Xar )T ) −1.T ′ Σ−1F̃(Xar ) (15)\nStep4: Repeat the Step 3 for (#) number of speech files\nStep5: Compute average i-vector for target model, r\nwr = 1\n#\n# ∑\na=1\nwar (16)\nStep6: Repeat the Step 1 to 5 fo all target models\nIn the test phase, the best PBM is first selected for the test utterance in the ML sense (similarly to Eqn(10)) and following sufficient statistics are computed with respect to the selected PBM (similarly to Eqn(12-14)). Finally, statistics are projected onto the T space for i-vector extraction (Eqn.15) for the test utterance. The score between the two i-vectors: claimant specific and the test utterance is calculated with PLDA.\nBased on the PBM training strategies, it gives us two subsystems: i-vector speaker-independent PBM (i-vector-SI-PBM) and i-vector speaker-dependent PBM (i-vector-SD-PBM) for TD-SV, respectively.\nIt is important to note that T space, i-vectors for training PLDA and EFR are extracted using the statistics with respect to the text-independent GMM-UBM i.e. i-vector baseline system represented in Sec.2, and also reused in the proposed method. It gives an identical setup for comparison the proposed method with the baseline system."
    }, {
      "heading" : "4. Experimental setup",
      "text" : "Experiments are performed on male speakers in two databases: RedDots challenge (task m-part-01) and RSR2015 (evaluation set of part1 i.e. 3sess-pwd eval m task) as per protocols in [17] and [18], respectively. The respective task in each database consists of the speakers data recorded on common pass-phrases (i.e. sentences) over many sessions for textdependent speaker verification. There are three recording sessions to train the particular pass-phrase-wise target speaker model. The utterances are of very short duration on an average of 2-3s per speech signal. Test trials are divided into three types of non-target for system performance evaluation:\n• target wrong: when a target speaker speaks a wrong sen-\ntence i.e. a different pass-phrase, in the testing phase as compared to their enrollment phase.\n• imposter correct: imposter speaks a correct sentence, i.e.\nthe same pass-phrase as that of the target enrollment sessions.\n• imposter wrong: imposter speaks a wrong sentence i.e. a\ndifferent pass-phrase from target enrollment phrase.\nIn the RSR2015 database, there are 1708 target models for training and evaluation. Data from 50 development speakers (disjoint from the evaluation set) are used for training SI-PBMs. In case of the RedDots database, a disjoint set of nine speakers’ data (excluded from the evaluation) are used for SI-PBMs (approximately 148 files per pass-phrase) and the remaining speakers are considered for the evaluation [15]. This gives 248 target models for training and evaluation trials from 40 different speakers. Table 1 shows the number of trials available for the system evaluation on different types of non-target scenarios on the respective databases.\nFor SV systems on RSR2015, text-independent GMM-UBM and HMM-UBM are trained by pooling data consisting of various textual contents from the TIMIT database across 438 male non-target speakers (approximately 4380 utterances). The GMM/HMM-UBM training data are reused for the training of total variability space. For PLDA and Sph, training data used for GMM/HMM-UBM and SI-PBMs (50 development speakers who are disjoint from the evaluation set) are used. As part 1 of RSR2015 database contains recordings of speaker data over 30 common pass-phrases in 9 sessions. This gives 30 PBMs (approximately 450 files per pass-phrase) in the proposedmethods. In PLDA training, utterances having the same pass-phrase of a particular speaker (of 50 development speakers from the RSR2015 database) are treated as belonging to the individual speaker class. This gives a total of 1938 classes in PLDA (438 from TIMIT and 1500 from the development set of RSR2015).\nFor SV systems on RedDots, GMM-UBM and HMM-UBM are trained using data of various textual contents from the RSR2015 database [18] over 157 male non-target speakers (approximately 42000 utterances). Since the data of the speakers in the m-part-01 task of RedDots database were recorded over 10 common pass-phrases, it yields 10 PBMs in the proposed PBM based systems. The GMM/HMM-UBM training data are reused for the training of total variability space, PLDA and Sph algorithm. In PLDA training, utterances having the same passphrase of a particular speaker are treated as belonging to the individual speaker class. This gives a total of 157 ∗ 30 = 4710 speakers (classes) in PLDA (each class having on average 9 examples).\nGMM based systems consist of GMM-UBM with 512 Gaussian mixtures. In case of HMM-UBM, 14 states and different numbers of Gaussians per state are considered as per [16] and inspired from [25]. A left to right modeling concept is followed in the HMM-UBM modeling. HTK toolkit [26] and ALIZE toolkit [27] are used for implementing the HMM-UBM and GMM-UBM based systems, respectively. Both GMM- and HMM-UBM have diagonal covariance matrices. During the construction of GMM-UBM PBMs only Gaussian mean vectors are updated with MAP adaptation of GMM-UBM. For HMM-UBM PBMs, both Gaussian mean vectors and transition parameters are updated during MAP adaptation of HMMUBM. When enrolling a target, only Gaussian mean vectors of the GMM/HMM-UBM/PBMs are updated duringMAP adaptation. Three iterations are used in MAP with a value of relevance factor 10 in all cases.\nIn i-vector based systems, channel and speaker factors in PLDA are kept equal to the dimension of i-vector. In Sph, two iterations are followed. We consider i-vector of 400 dimensions. An i-vector is extracted for each single utterance. During enrollment, each target is represented by an average ivector computed over their respective training example-wise ivector. It is noted that i-vectors for training both Sph and PLDA are extracted from the GMM-UBM rather than from PBM. In other words, total variability space, PLDA and Sph parts are the same for both conventional and the proposed method. The BOB toolkit [28] is used for implementing the i-vector system.\nFor cepstral analysis, 57 dimensional MFCC [29] (19 static+∆+∆∆) feature vectors are extracted from speech signals using 20ms hamming window with 10ms overlap of adjacent frames. RASTA [30] filtering is applied on the features. An energy based voice activity detection (VAD) is used to discard the low energy frames. High energy frames are normalized to fit zero-mean and unity variance at the utterance level. System performances are evaluated in terms of equal error rate (EER) and minimum detection cost function (MinDCF) [31]."
    }, {
      "heading" : "5. Results and Discussion",
      "text" : ""
    }, {
      "heading" : "5.1. Systems based on Gaussian mixture model",
      "text" : "Tables 2a and 2b compare the performance of the proposed PBM methods with the baseline system for text-dependent speaker verification on the GMM-UBM paradigm. It can be seen that the proposed PBM techniques give significantly lower EER andMinDCF values for the non-target types: target-wrong and imposter-wrong than the baseline system on both databases. At the same time, the performance of the proposed methods for the imposter-correct type is comparable/closer to the baseline. This shows the effectiveness of the proposed PBM methods which incorporates the pass-phrase identification process in LLR during the test phase. To provide inside information about the significant performance improvement for target-wrong and imposter-wrong, we calculate the LLR score difference between the baseline and the proposed SI- and SD-PBM based systems i.e. Y(x) = LLRbaseline(x) − LLRPBM(x) where x denotes a trial for the target-wrong and imposter-wrong trials. It shows that\n99.67% and 99.86% of the target-wrong and imposter-wrong trials show lower LLR score values, respectively for the SIPBM and SD-PBM systems than the baseline system in the RedDots database. Similar observation is also seen on the RSR2015 database. This further demonstrates that the proposed PBM based methods are able to reject target-wrong and imposter-wrong non-target types more than the conventional TD-SV method.\nThe SD-PBM system shows slightly lower error rates than SI-PBM for the non-target type: target-wrong. This is expected due to the fact that SD-PBMs contain the priori information about the target speakers and hence it would give a lower passphrase identification error rate during test. It is also well known in automatic speech recognition (ASR) that a SD system gives higher accuracies than the SI counterpart.\nTable 3 shows the pass-phrase identification accuracies on the evaluation data for different PBM based systems on both databases. From Table 3, we can see that SD-PBM systems show higher pass-phrase identification accuracies than SI-PBM systems on the evaluation set, which is also reflected by the speaker verification performance in the Tables 2a (SD-PBM shows lower error rates for non-target type target-wrong than SI-PBM). On RSR2015 (Table 2b), both SI- and SD-PBM systems show equal (and saturated) value of pass-phrase identification accuracy. Both PBM systems also show very close speaker verification performance to each other. Table 3 further shows that the system gives an approximately 4% pass-phrase identification error rate on the evaluation data of the RedDots database. However, the proposed method still shows significantly lower error rates for non-target types: the target/imposter-wrongcases while maintaining a performance for the imposter-correct nontarget type comparable to the baseline SV system.\nIn case of the non-target type: imposter-correct in the RedDots database, the SD-PBM system shows slightly higher speaker verification error rates than the SI-PBM and the base-\nline. The reason for higher error rates in the SD-PBM could be due to the fact that both SI-PBM and baseline systems do not use the target speaker data during training the PBM and GMMUBM, respectively. Hence, SI-PBM and GMM-UBM theoretically satisfy the fully non-target hypothesis than the SD-PBM with respect to the targets. Therefore, the false rejection rate is expected to be increased on a certain operating region of the SD-PBM system with respect to the SI-PBM and the baseline. This impacts on the equal error rate. However, there is always a trade-off between the false acceptance and false rejection rate based on the intended application."
    }, {
      "heading" : "5.2. Systems based on Hidden Markov model",
      "text" : "Tables 4a and 4b compare the performance of the proposed PBM method with the baseline system on the HMM paradigm for different numbers of mixtures per state in HMM-UBMs. Similarly to the GMM-UBM based PBM systems, we can observe from Tables 4a -4b that the PBM system shows lower error rates compared to the baseline for non-target types: targetwrong and imposter-wrong in both databases. For impostercorrect in the RedDots database, SI-PBM systems show better or comparable performance to the baseline over various numbers of mixtures in the HMM-UBM state. However, SV\nerror rates of the SD-PBM systems for imposter-correct increase when the number of Gaussian components per state in the HMM-UBM increases with compared to the SI-PBM and baseline systems. This could be due to the same reason as explained in section 5.1 (for the GMM-UBM-SD-PBM system) i.e. SD-PBM uses target data during its training phase. Hence, it does not fully satisfies the non-target hypothesis with respect to the target model in contrast to SI-PBM and HMM-UBM. It is also noted that pass-phrase identification accuracies of the HMM-UBM based PBM systems are similar to the systems in GMM-UBM paradigm, which is not shown in the paper."
    }, {
      "heading" : "5.3. Systems based on i-vector",
      "text" : "Tables 5a and 5b show the performance of the proposed PBM methods incorporated into an i-vector paradigm and that of an i-vector baseline for text dependent speaker verification. From Tables 5a and 5b, it can be observed that the proposed PBM methods on the i-vector framework showmuch lower error rates for the target-wrong and imposter-wrong; and comparable error rates for imposter-correct with compared to the baseline. This again indicates the usefulness of the proposed PBM methods of incorporating the pass-phrase identification process. Now if we compare the PBM based system on the i-vector paradigm\nwith GMM and HMM, it can be observed from Tables 2, 4 and 5 that the PBM system based on GMM and HMM more significantly reduces the error rate for target-wrong and imposterwrong. This could be due to the nature of very short duration utterances in both the RedDots and RSR2015 databases, and short utterances are challenging for the i-vector paradigm. For the non-target imposter-correct type in RedDots database, the error rates of the SI-PBM on the i-vector framework are marginally higher compared to the method in GMM and HMM paradigms. It could be due to the fact that a selection (i.e. switching) of the PBM during the test phase introduces another variability in the i-vector system, i.e. the known fact that i-vector based SV systems are very sensible to the session variability."
    }, {
      "heading" : "6. Conclusion and future work",
      "text" : "In this paper, we proposed pass-phrase dependent background models (PBMs) for text-dependent speaker verification (TD-SV) to integrate the pass-phrase identification process into conventional TD-SV systems, by deriving PBMs from the text-independent background model with adaptation using the data for a particular pass-phrase across the speakers. During training, a pass-phrase specific target speaker model is derived from the corresponding PBM using his/her training data of the particular pass-phrase. During test, the best PBM for the particular test utterance is selected in the maximum likelihood sense and used for a log likelihood ratio score calculation with respect to the claimant. The effectiveness of the proposed techniques were comparedwith the conventional textdependent speaker verification systems under GMM-UBM and HMM-UBM paradigms. We further showed the proposed concept can be incorporated into an i-vector based TD-SV system. Experimental results were demonstrated on the recent RedDots challenge and RSR2015 databases. We showed that incorporation of pass-phase identification in the test phase significantly\nreduces the speaker verification error rates, especially for the non-target types: target-wrong and imposter-wrong while giving better or comparable performances for the imposter-correct case. Future work includes better techniques for incorporating PBM into the i-vector paradigm in order to further improve the speaker verification performance."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work is partly supported by the iSocioBot project, funded by the Danish Council for Independent Research - Technology and Production Sciences (#1335-00162) and OCTAVE Project (#647850), funded by the Research European Agency (REA) of the European Commission, in its framework programme Horizon 2020. The views expressed in this paper are those of the authors and do not engage any official position of the European Commission."
    } ],
    "references" : [ {
      "title" : "Speaker Verification using Adapted Gaussian Mixture Models",
      "author" : [ "D.A. Reynolds", "T.F. Quatieri", "R.B. Dunn" ],
      "venue" : "Digital Signal Processing",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2000
    }, {
      "title" : "A Novel Scheme for Speaker Recognition using a Phonetically- Aware Deep Neural Network",
      "author" : [ "Y. Lei", "N. Scheffer", "L. Ferrer", "M. McLaren" ],
      "venue" : "in: Proc. of IEEE Int. Conf. Acoust. Speech Signal Processing (ICASSP),",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2014
    }, {
      "title" : "Content Matching for Short Duration Speaker Recognition",
      "author" : [ "N. Scheffer", "Y. Lei" ],
      "venue" : "in: Proc. of Interspeech,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2014
    }, {
      "title" : "Deep Feature for Textdependent Speaker Verification",
      "author" : [ "Y. Liu", "Y. Qian", "N. Chen", "T. Fu", "Y. Zhang", "K. Yu" ],
      "venue" : "Speech Communication",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2015
    }, {
      "title" : "Front-End Factor Analysis for Speaker Verification",
      "author" : [ "N. Dehak", "P. Kenny", "R. Dehak", "P. Ouellet", "P. Dumouchel" ],
      "venue" : "IEEE Trans. on Audio, Speech and Language Processing",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2011
    }, {
      "title" : "RSR2015: Database for Textdependent Speaker Verification using Multiple Pass-phrases",
      "author" : [ "A. Larcher", "K.A. Lee", "B. Ma", "H. Li" ],
      "venue" : "in: Proc. of Interspeech,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2012
    }, {
      "title" : "Speaker Verification Based on Broad Phonetic Categories",
      "author" : [ "S. Kajarekar", "H. Hermansky" ],
      "venue" : "in: Proc. of Odyssey Speaker and Language Recognition Workshop,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2001
    }, {
      "title" : "Improving a GMM Speaker Verification System by Phonetic Weighting",
      "author" : [ "R. Auckenthaler", "E. Parris", "M. Carey" ],
      "venue" : "in: Proc. of IEEE Int. Conf. Acoust. Speech Signal Processing (ICASSP),",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1999
    }, {
      "title" : "Domain Adaptation for Text Dependent Speaker Verification",
      "author" : [ "H. Aronowitz", "A. Rendel" ],
      "venue" : "in: Proc. of Interspeech,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2014
    }, {
      "title" : "Multiple Background Models for Speaker Verification using the Concept of Vocal Tract Length and MLLR Super-vector",
      "author" : [ "A. Sarkar", "S. Umesh" ],
      "venue" : "International Journal of Speech Technology",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2012
    }, {
      "title" : "Utterance Verification for Text-dependent Speaker Recognition: A Comparative Assessment using the RedDots Corpus",
      "author" : [ "T. Kinnunen", "M. Sahidullah", "I. Kukanov", "H. Delgado", "M. Todisco", "A. Sarkar", "N.B. Thomsen", "V. Hautamaki", "N. Evans", "Z.-H. Tan" ],
      "venue" : "in: in Proc. of Interspeech,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2016
    }, {
      "title" : "Text Dependent Speaker Verification using Unsuper-vised HMM-UBM and Temporal GMM-UBM",
      "author" : [ "A.K. Sarkar", "Z.-H. Tan" ],
      "venue" : "in: in Proc. of Interspeech,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2016
    }, {
      "title" : "Text-dependent Speaker Verification: Classifiers, Databases and RSR2015",
      "author" : [ "A. Larcher", "K.A. Lee", "B. Ma", "H. Li" ],
      "venue" : "Speech Communication",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2014
    }, {
      "title" : "A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition",
      "author" : [ "L.R. Rabiner" ],
      "venue" : "Proc. of the IEEE",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 1989
    }, {
      "title" : "Maximum a Posteriori Estimation for Multivariate Gaussian Mixture Observations of Markov Chains",
      "author" : [ "J.-L. Gauvain", "C.-H. Lee" ],
      "venue" : "IEEE Trans. on Speech and Audio Processing",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 1994
    }, {
      "title" : "Variance-Spectra Based Normalization for i-vector Standard and Probabilistic Linear Discriminant Anal ysis",
      "author" : [ "P.M. Bousquet" ],
      "venue" : "in: Proc. of Odyssey Speaker and Language Recognition Workshop",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2012
    }, {
      "title" : "Computer Vision: Models Learning and Inference",
      "author" : [ "S.J. Prince" ],
      "venue" : null,
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2012
    }, {
      "title" : "Mixture of PLDA Models in I-Vector Space for Gender-Independent Speaker Recognition",
      "author" : [ "M. Senoussaoui" ],
      "venue" : "in: Proc. of Interspeech,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2011
    }, {
      "title" : "Low-complexity Variable Frame Rate Analysis for Speech Recognition and Voice Activity Detection",
      "author" : [ "Z.-H. Tan", "B. Lindbeg" ],
      "venue" : "IEEE Journal of Selected Topics in Signal Processing",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2010
    }, {
      "title" : "A Free Signal Processing and Machine Learning Toolbox for Researchers, in: 20th ACMConference onMultimedia Systems",
      "author" : [ "A. Anjos", "L.E. Shafey", "R. Wallace", "M. Günther", "C. McCool", "S. Marcel", "Bob" ],
      "venue" : null,
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2012
    }, {
      "title" : "Comparison of Parametric Representations for Monosyllabic Word Recognition in Continuously Spoken Sentences",
      "author" : [ "S.B. Davis", "P. Mermelstein" ],
      "venue" : "IEEE Trans. Acoust. Speech Signal Processing",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 1980
    }, {
      "title" : "The Det Curve in Assessment of Detection Task Performance",
      "author" : [ "A. Martin", "G. Doddington", "T. Kamm", "M. Ordowskiand", "M. Przybocki" ],
      "venue" : "in: Proc. of Eur. Conf. Speech Commun. and Tech. (Eurospeech),",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 1997
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Speaker verification (SV) [1, 2] is the process of authentication of a person’s claimed identity by analyzing his/her speech signal.",
      "startOffset" : 26,
      "endOffset" : 32
    }, {
      "referenceID" : 1,
      "context" : "Examples are deep neural network (DNN) [3, 4, 5], i-vector [4, 6], hierarchical multi-Layer acoustic model (HiLAM) [2, 7], phonedependent Hidden Markov model (HMM) [8, 9], domain adaptation [10] and phonetic higher level maximum likelihood linear regression (MLLR) super-vector based features [11].",
      "startOffset" : 39,
      "endOffset" : 48
    }, {
      "referenceID" : 2,
      "context" : "Examples are deep neural network (DNN) [3, 4, 5], i-vector [4, 6], hierarchical multi-Layer acoustic model (HiLAM) [2, 7], phonedependent Hidden Markov model (HMM) [8, 9], domain adaptation [10] and phonetic higher level maximum likelihood linear regression (MLLR) super-vector based features [11].",
      "startOffset" : 39,
      "endOffset" : 48
    }, {
      "referenceID" : 3,
      "context" : "Examples are deep neural network (DNN) [3, 4, 5], i-vector [4, 6], hierarchical multi-Layer acoustic model (HiLAM) [2, 7], phonedependent Hidden Markov model (HMM) [8, 9], domain adaptation [10] and phonetic higher level maximum likelihood linear regression (MLLR) super-vector based features [11].",
      "startOffset" : 39,
      "endOffset" : 48
    }, {
      "referenceID" : 2,
      "context" : "Examples are deep neural network (DNN) [3, 4, 5], i-vector [4, 6], hierarchical multi-Layer acoustic model (HiLAM) [2, 7], phonedependent Hidden Markov model (HMM) [8, 9], domain adaptation [10] and phonetic higher level maximum likelihood linear regression (MLLR) super-vector based features [11].",
      "startOffset" : 59,
      "endOffset" : 65
    }, {
      "referenceID" : 4,
      "context" : "Examples are deep neural network (DNN) [3, 4, 5], i-vector [4, 6], hierarchical multi-Layer acoustic model (HiLAM) [2, 7], phonedependent Hidden Markov model (HMM) [8, 9], domain adaptation [10] and phonetic higher level maximum likelihood linear regression (MLLR) super-vector based features [11].",
      "startOffset" : 59,
      "endOffset" : 65
    }, {
      "referenceID" : 0,
      "context" : "Examples are deep neural network (DNN) [3, 4, 5], i-vector [4, 6], hierarchical multi-Layer acoustic model (HiLAM) [2, 7], phonedependent Hidden Markov model (HMM) [8, 9], domain adaptation [10] and phonetic higher level maximum likelihood linear regression (MLLR) super-vector based features [11].",
      "startOffset" : 115,
      "endOffset" : 121
    }, {
      "referenceID" : 5,
      "context" : "Examples are deep neural network (DNN) [3, 4, 5], i-vector [4, 6], hierarchical multi-Layer acoustic model (HiLAM) [2, 7], phonedependent Hidden Markov model (HMM) [8, 9], domain adaptation [10] and phonetic higher level maximum likelihood linear regression (MLLR) super-vector based features [11].",
      "startOffset" : 115,
      "endOffset" : 121
    }, {
      "referenceID" : 6,
      "context" : "Examples are deep neural network (DNN) [3, 4, 5], i-vector [4, 6], hierarchical multi-Layer acoustic model (HiLAM) [2, 7], phonedependent Hidden Markov model (HMM) [8, 9], domain adaptation [10] and phonetic higher level maximum likelihood linear regression (MLLR) super-vector based features [11].",
      "startOffset" : 164,
      "endOffset" : 170
    }, {
      "referenceID" : 7,
      "context" : "Examples are deep neural network (DNN) [3, 4, 5], i-vector [4, 6], hierarchical multi-Layer acoustic model (HiLAM) [2, 7], phonedependent Hidden Markov model (HMM) [8, 9], domain adaptation [10] and phonetic higher level maximum likelihood linear regression (MLLR) super-vector based features [11].",
      "startOffset" : 164,
      "endOffset" : 170
    }, {
      "referenceID" : 8,
      "context" : "Examples are deep neural network (DNN) [3, 4, 5], i-vector [4, 6], hierarchical multi-Layer acoustic model (HiLAM) [2, 7], phonedependent Hidden Markov model (HMM) [8, 9], domain adaptation [10] and phonetic higher level maximum likelihood linear regression (MLLR) super-vector based features [11].",
      "startOffset" : 190,
      "endOffset" : 194
    }, {
      "referenceID" : 1,
      "context" : "In [3, 4], phonetic information is incorporated into an i-vector system by accumulating statistics from speech with respect to a pre-defined phonetic class through an DNN based automatic speech recognition (ASR) system.",
      "startOffset" : 3,
      "endOffset" : 9
    }, {
      "referenceID" : 2,
      "context" : "In [3, 4], phonetic information is incorporated into an i-vector system by accumulating statistics from speech with respect to a pre-defined phonetic class through an DNN based automatic speech recognition (ASR) system.",
      "startOffset" : 3,
      "endOffset" : 9
    }, {
      "referenceID" : 3,
      "context" : "In [5], the intermediate output of the DNN layers are used to vectorize characterization of speech data.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 0,
      "context" : "HiLAM builds a HMM model by concatenating the speech segment-wise adapted models from the Gaussian mixture model- universal background model (GMMUBM) [2].",
      "startOffset" : 150,
      "endOffset" : 153
    }, {
      "referenceID" : 8,
      "context" : "In domain adaptation [10], the mismatch between the text-independent and the text-dependent data is reduced by transforming the text-independent data to better match the textdependent task (using the a-priori transcription knowledge of the text-dependent data).",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 6,
      "context" : "In conventional HMM based TDSV systems [8, 9], phoneme (context) dependent speaker models are built using the knowledge of speech transcriptions.",
      "startOffset" : 39,
      "endOffset" : 45
    }, {
      "referenceID" : 7,
      "context" : "In conventional HMM based TDSV systems [8, 9], phoneme (context) dependent speaker models are built using the knowledge of speech transcriptions.",
      "startOffset" : 39,
      "endOffset" : 45
    }, {
      "referenceID" : 9,
      "context" : "In [12, 13, 14], a multiple background model concept is proposed to improve the performance of the conventional speaker verification system, by training background models (BMs) based on the vocal tract length (VTL) characteristic of target speakers as in [14] and passphrases as in [12, 13] for text-independent and text-dependent speaker verification, respectively.",
      "startOffset" : 3,
      "endOffset" : 15
    }, {
      "referenceID" : 9,
      "context" : "In [12, 13, 14], a multiple background model concept is proposed to improve the performance of the conventional speaker verification system, by training background models (BMs) based on the vocal tract length (VTL) characteristic of target speakers as in [14] and passphrases as in [12, 13] for text-independent and text-dependent speaker verification, respectively.",
      "startOffset" : 255,
      "endOffset" : 259
    }, {
      "referenceID" : 9,
      "context" : "During enrollment, target speaker models are derived from the BM based on VTL in [14] and pass-phrase of target data in [12, 13]).",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 9,
      "context" : "However, this [12, 13, 14] does not incorporate the passphrase identification process to address the following two nontarget types: target-wrong and imposter-wrong.",
      "startOffset" : 14,
      "endOffset" : 26
    }, {
      "referenceID" : 10,
      "context" : "Recently, in [15], the authors proposed a fusion system which combines the score/decision of an utterance verification system with a conventional SV system to improve the performance of the TD-SV system against target/imposter-wrong non-target trials.",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 9,
      "context" : "In the test phase, the best PBM is selected for a particular test utterance in the maximum likelihood (ML) sense and the best selected PBM is used as an alternative hypothesis for the loglikelihood ratio calculation with respect to the claimant model, which differs from [12, 13, 14].",
      "startOffset" : 271,
      "endOffset" : 283
    }, {
      "referenceID" : 9,
      "context" : "Furthermore two strategies are considered for building the PBM: one is called speakerindependent (SI) and the other is speaker-dependent (SD) again in contrast to [12, 13, 14] where only SI-PBM concept is considered.",
      "startOffset" : 163,
      "endOffset" : 175
    }, {
      "referenceID" : 9,
      "context" : "The main salient feature of the proposed method is that it incorporates the utterance identification in the LLR calculation process in the conventional approach in contrast to [12, 13, 14].",
      "startOffset" : 176,
      "endOffset" : 188
    }, {
      "referenceID" : 11,
      "context" : "We study the performance of the proposed PBM based TDSV system under the GMM and recently proposed HMM modeling [16] and i-vector paradigms.",
      "startOffset" : 112,
      "endOffset" : 116
    }, {
      "referenceID" : 12,
      "context" : "Experiments are conducted on the RedDots Challenge [17] and the RSR2015 [18] databases.",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 0,
      "context" : "In this method, a largeGaussian mixturemodel called GMMUBM [2] is built using data with different textual contents from non-target speakers.",
      "startOffset" : 59,
      "endOffset" : 62
    }, {
      "referenceID" : 11,
      "context" : "In this method [16], an HMM called ‘HMM-UBM’ is built using data from many non-target speakers without any speech",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 13,
      "context" : "Since no transcriptions are considered during HMM training, state transition probabilities of the HMM-UBM [19] will inherently reflect the speaker-independent temporal information available within the data.",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 14,
      "context" : "Similarly to the GMM-UBM TD-SV system, pass-phrasespecific target speaker models are derived from the textindependent HMM-UBM with MAP adaptation [20] using the training data for the particular target model.",
      "startOffset" : 146,
      "endOffset" : 150
    }, {
      "referenceID" : 5,
      "context" : "It should be noted that the HMM-UBM approach is different from the HiLAM proposed in [7] in two ways: First, HMMUBM is trained by pooling all training data together using the Baum-Welch re-estimation algorithm, and then target speaker models are derived from the HMM-UBM with MAP using the training data of particular target speaker model.",
      "startOffset" : 85,
      "endOffset" : 88
    }, {
      "referenceID" : 4,
      "context" : "For more details about the i-vector see [6].",
      "startOffset" : 40,
      "endOffset" : 43
    }, {
      "referenceID" : 15,
      "context" : "For more details about the PLDA based scoring see [22, 23, 24].",
      "startOffset" : 50,
      "endOffset" : 62
    }, {
      "referenceID" : 16,
      "context" : "For more details about the PLDA based scoring see [22, 23, 24].",
      "startOffset" : 50,
      "endOffset" : 62
    }, {
      "referenceID" : 17,
      "context" : "For more details about the PLDA based scoring see [22, 23, 24].",
      "startOffset" : 50,
      "endOffset" : 62
    }, {
      "referenceID" : 15,
      "context" : "Prior to PLDA, i-vectors are post-processed for session variability compensation using an iterative conditioning algorithm called spherical normalization (Sph) proposed in [22].",
      "startOffset" : 172,
      "endOffset" : 176
    }, {
      "referenceID" : 15,
      "context" : "It has been shown in [22] that Sph improves the speaker verification performance of PLDA based systems when compared to other conventional approaches.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 12,
      "context" : "3sess-pwd eval m task) as per protocols in [17] and [18], respectively.",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 10,
      "context" : "In case of the RedDots database, a disjoint set of nine speakers’ data (excluded from the evaluation) are used for SI-PBMs (approximately 148 files per pass-phrase) and the remaining speakers are considered for the evaluation [15].",
      "startOffset" : 226,
      "endOffset" : 230
    }, {
      "referenceID" : 12,
      "context" : "For SV systems on RedDots, GMM-UBM and HMM-UBM are trained using data of various textual contents from the RSR2015 database [18] over 157 male non-target speakers (approximately 42000 utterances).",
      "startOffset" : 124,
      "endOffset" : 128
    }, {
      "referenceID" : 11,
      "context" : "In case of HMM-UBM, 14 states and different numbers of Gaussians per state are considered as per [16] and inspired from [25].",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 18,
      "context" : "In case of HMM-UBM, 14 states and different numbers of Gaussians per state are considered as per [16] and inspired from [25].",
      "startOffset" : 120,
      "endOffset" : 124
    }, {
      "referenceID" : 19,
      "context" : "The BOB toolkit [28] is used for implementing the i-vector system.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 20,
      "context" : "For cepstral analysis, 57 dimensional MFCC [29] (19 static+∆+∆∆) feature vectors are extracted from speech signals using 20ms hamming window with 10ms overlap of adjacent frames.",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 21,
      "context" : "System performances are evaluated in terms of equal error rate (EER) and minimum detection cost function (MinDCF) [31].",
      "startOffset" : 114,
      "endOffset" : 118
    } ],
    "year" : 2017,
    "abstractText" : "In this paper, we propose pass-phrase dependent background models (PBMs) for text-dependent (TD) speaker verification (SV) to integrate the pass-phrase identification process into the conventional TD-SV system, where a PBM is derived from a textindependent background model through adaptation using the utterances of a particular pass-phrase. During training, pass-phrase specific target speaker models are derived from the particular PBM using the training data for the respective target model. While testing, the best PBM is first selected for the test utterance in the maximum likelihood (ML) sense and the selected PBM is then used for the log likelihood ratio (LLR) calculation with respect to the claimant model. The proposed method incorporates the pass-phrase identification step in the LLR calculation, which is not considered in conventional standalone TD-SV systems. The performance of the proposed method is compared to conventional text-independent backgroundmodel based TD-SV systems using either Gaussian mixture model (GMM)-universal background model (UBM) or Hidden Markov model (HMM)-UBM or i-vector paradigms. In addition, we consider two approaches to build PBMs: speaker-independent and speaker-dependent. We show that the proposed method significantly reduces the error rates of text-dependent speaker verification for the non-target types: target-wrong and imposter-wrong while it maintains comparable TD-SV performance when imposters speak a correct utterance with respect to the conventional system. Experiments are conducted on the RedDots challenge and the RSR2015 databases that consist of short utterances.",
    "creator" : "LaTeX with hyperref package"
  }
}