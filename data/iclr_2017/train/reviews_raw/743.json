{"conference": "ICLR 2017 conference submission", "title": "Universality in halting time", "abstract": "The authors present empirical distributions for the halting time (measured by the number of iterations to reach a given accuracy) of optimization algorithms applied to two random systems: spin glasses and deep learning. Given an algorithm, which we take to be both the optimization routine and the form of the random landscape, the fluctuations of the halting time follow a distribution that remains unchanged even when the input is changed drastically. We observe two main classes, a Gumbel-like distribution that appears in Google searches, human decision times, QR factorization and spin glasses, and a Gaussian-like distribution that appears in conjugate gradient method, deep network with MNIST input data and deep network with random input data. This empirical evidence suggests presence of a class of distributions for which the halting time is independent of the underlying distribution under some conditions.", "reviews": [{"is_meta_review": true, "comments": "The idea of universality that is independent of input distribution and dimension, depending only on the algorithm is an appealing one. However, as an empirical study, this paper comes up somewhat short:\n\n1. Exactly one algorithm is shown for the deep learning example. It would have been more convincing to compare distributions with one or more algorithms. \n\n2. The definition (1),  and much of the work of Section 2.1 seems to have already been covered in Deift (2014), Section 1.3. In that paper, a number of different algorithms for the solution of linear systems are considered, and then the concept of universality becomes more plausible. I do not see enough of such algorithmic comparisons in this paper (same problem setup, different algorithms).\n\n3.  It seems to me that what practitioners might care about in practice are both the mean and variance in running times; these quantities are buried in (1). So I question how useful the distribution itself might be for algorithm tuning. \n\nAt the least, many more empirical comparisons should be provided to convince me that the universality holds across a broad range of algorithms.", "IS_META_REVIEW": true}, {"TITLE": "ICLR committee final decision", "OTHER_KEYS": "ICLR 2017 pcs", "comments": "My overall conclusion is that the paper needs more work to be sufficiently convincing. I think the reviews are sufficiently careful. My recommendation is based on the fact that none of the reviewers supports acceptance.", "IS_META_REVIEW": false, "DATE": "06 Feb 2017", "is_meta_review": false}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "TITLE": "interesting idea, shortcomings in evaluation and clarity", "is_meta_review": false, "comments": "The authors explore whether the halting time distributions for various algorithms in various settings exhibit \"universality\", i.e. after rescaling to zero mean and unit variance, the distribution does not depend on stopping parameter, dimensionality and ensemble.\n\nThe idea of the described universality is very interesting. However I see several shortcomings in the paper:\n\nIn order to be of practical relevance, the actual stopping time might be more relevant than the scaled one. The discussion of exponential tailed halting time distributions is a good start, but I am not sure how often this might be actually helpful. Still, the findings in the paper might be interesting from a theoretical point of view.\n\nEspecially for ICLR, I think it would have been more interesting to look into comparisons between stochastic gradient descent, momentum, ADAM etc on different deep learning architectures. Over which of those parameters does universality hold?. How can different initializations influence the halting time distribution? I would expect a sensible initialization to cut of part of the right tail of the distribution.\n\nAdditionally, I found the paper quite hard to read. Here are some clarity issues:\n\n- abstract: \"even when the input is changed drastically\": From the abstract I'm not sure what \"input\" refers to, here\n- I. Introduction: \"where the stopping condition is, essentially, the time to find the minimum\": this doesn't seem to make sense, a condition is not a time. I guess the authors wanted to say that the stopping condition is that the minimum has been reached?\n- I.1 the notions of dimension N, epsilon and ensemble E are introduced without any clarification what they are. From the later parts of the paper I got some ideas and examples, but here it is very hard to understand what these parameters should be (just some examples would be already helpful)\n- I.3 \"We use x^\\ell for \\ell \\in Z=\\{1, \\dots, S\\} where Z is a random sample from of training samples\" This formulation doesn't make sense. Either Z is a random sample, or Z={1, ..., S}.\n- II.1 it took me a long time to find the meaning of M. As this parameter seems to be crucial for universality in this case, it would be very helpful to point out more explicitly what it refers to.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "03 Jan 2017", "REVIEWER_CONFIDENCE": 3}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Unusual research. Unsound methodology. conclusions are not supported by the content.", "is_meta_review": false, "comments": "Summary\n\n\nFor several algorithms, previous research has shown that the halting time follows a two-parameter distribution (the so-called universal property investigated by the authors). In this work, the authors extend the investigation to new algorithms (spin-glass, gradient descent in deep learning).\n\nAn algorithm is considered to satisfy the universality property when the centered/scaled halting time fluctuations (empirical distribution of halting times) depend on the algorithm but do not depend on the target accuracy epsilon, an intrinsic measure of dimension N, the probability distribution/random ensemble. (This is clear from Eq 1 where on the left the empirical halting time distribution depends on epsilon, N, A, E and on the right, the approximation only depends on the algorithm)\n\nThe authors argue that empirically, the universal property is observed when both algorithms (spin glass and deep learning) perform well and that it is not observed when they do not perform well.\n\nA moment-based indicator is introduced to assess whether universality is observed.\n\n\nReview\n\n\nThis paper presents several problems.\n\n\n\npage 2: \u201c[\u2026] for sufficiently large N and eps = eps(N)\u201d\n\nThe dependence of epsilon on N is troubling.\n\n\n\npage 3: \u201cUniversality is a measure of stability in an algorithm [\u2026] For example [\u2026] halting time for the power method [\u2026] has infinite expectation and hence this type of universality is *not* present. One could use this to conclude that the power method is naive. Therefore the presence of universality is a desirable feature of a numerical method\u201d\n\nNo. An algorithm is naive if there are better ways to answer the problem. One could not conclude from a halting time with infinite expectation (e.g. solving a problem extremely quickly 99% of the time, and looping forever in 1% of cases) or infinite variance, that the algorithm is naive.\u2028\nMoreover the universal property is more restrictive than having a finite halting time expectation. Even if in many specific cases, having a finite halting time expectation is a desirable property, showing that the presence of universality is desirable would require a demonstration that the other more restrictive aspects are also desirable.\u2028\nAlso, the paragraph only concerns one algorithm. why would the conclusions generalise to all numerical methods ?\u2028\nEven if the universality property is arguably desirable (i.e. event if the conclusion of this paragraph is assumed correct), the paragraph does not support the given conclusion.\n\n\n\nComparing Eq 1 and figures 2,3,4,5\u2028\nFrom Eq 1, universality means that the centered/scaled halting time fluctuations (which depend on A, epsilon, N, E) can be approximated by a distribution that only depends on A (not on epsilon, N, E) but in the experiments only E varies (figures 2,3,4,5). The validity of the approximation with varying epsilon or N is never tested\n\n\n\nThe ensembles/distributions parameter E (on which halting fluctuations should not depend) and the algorithm A (on which halting fluctuations are allowed to depend) are not well defined, especially w.r.t. the common use of the words. In the optimisation setting we are told that the functional form of the landscape function is part of A (in answer to the question of a reviewer) but what is part of the functional form ? what about computations where the landscape has no known functional form (black box) ?\n\n\n\nThe conclusion claims that the paper \u201cattempts to exhibit cases\u201d where one can answer 5 questions in a robust and quantitative way.\n\nQuestion 1: \u201cWhat are the conditions on the ensembles and the model that lead to such universality ?\u201d\u2028The only quantitative way would be to use the moments based indicator however there is only one example of universality not being observed which concerns only one algorithm (conjugate gradient) and one type of failure (when M = N). This does not demonstrate robustness of the method.\n\nQuestion 2: \u201cWhat constitutes a good set of hyper parameters for a given algorithm ?\u201d\nThe proposed way to choose would be to test whether universality is observed. If it is then the hyper parameters are good, if not the hyper parameters are bad. The correspondance between bad hyper-parameters and observing no universality concerns only one algorithm and one type of failure. Other algorithms may fail in the universal regime or perform well in the non universal regime. The paper does not show how to answer this question in a robust way.\n\nQuestion 3: \"How can we go beyond inspection when tuning a system ?\u2028\"\nThe question is too vague and general and there is probably no robust and quantitative way to answer it at all.\n\nQuestion 4: \"How can we infer if an algorithm is a good match to the system at hand ?\u2028\"\nThe paper fails to demonstrate convincingly that universality is either a good or robust way to approach the very few studied algorithms. The suggested generalisation to all systems and algorithms is extremely far fetched.\n\nQuestion 5: \"What is the connection between the universal regime and the structure of the landscape ?\"\n\u2028Same as before, the question is extremely vague and cannot be answered in a robust or quantitative way at all. The fact that what corresponds to A and what corresponds to E is not clear does not help.\n\n\nIn the conclusion it is written that the paper validates the claim that universality is present in all or nearly all sensible computation. It does not. The paper does not properly test whether universality is present (only 1 parameter in 3 that should not vary is tested). The paper does not properly test whether universality is lost when the computation is no longer sensible (only one failure case tested). Finally the experiments do not apply to all or nearly all computations but only to very few  specific algorithms.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 2, "DATE": "22 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Hard to see universality yet.", "is_meta_review": false, "comments": "The idea of universality that is independent of input distribution and dimension, depending only on the algorithm is an appealing one. However, as an empirical study, this paper comes up somewhat short:\n\n1. Exactly one algorithm is shown for the deep learning example. It would have been more convincing to compare distributions with one or more algorithms. \n\n2. The definition (1),  and much of the work of Section 2.1 seems to have already been covered in Deift (2014), Section 1.3. In that paper, a number of different algorithms for the solution of linear systems are considered, and then the concept of universality becomes more plausible. I do not see enough of such algorithmic comparisons in this paper (same problem setup, different algorithms).\n\n3.  It seems to me that what practitioners might care about in practice are both the mean and variance in running times; these quantities are buried in (1). So I question how useful the distribution itself might be for algorithm tuning. \n\nAt the least, many more empirical comparisons should be provided to convince me that the universality holds across a broad range of algorithms.", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "18 Dec 2016 (modified: 19 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "From ICLR 2016", "OTHER_KEYS": "(anonymous)", "comments": "It is an interesting paper. However, I would like to describe a hypothesis/counter-example and I hope you will\neasily reject it. Otherwise, I will fail to see any universality in the discussed \"universality in halting time\".\n\nI would like to present a toy minimization problem where f(x) is defined in [0,1] and has two global\noptima in 0.0 and 1.0 respectively. f(x) has its maximum in 0.5 but f(x) decays somewhat differently to the \"left\"\nand to the \"right\" optima. These somewhat different decays (shapes of f(x)) impact the way some optimizer A minimizes\nf(x). My A can be e.g. a deterministic local search algorithm such that if it is initialized in the basin of attraction [0,0.5], it will stay/search there. Thus, the starting position will determine in which global optima it will end up. To observe the halting distribution, I run A starting from different starting positions generated uniformly at random in [0,1]. I expect the halting distribution to have two peaks: the first peak corresponds to a typical number of iterations required to reach some epsilon around the \"left\" optima, the second peak would correspond to the \"right\" optima. Question: Why there are two peaks and not one? Answer: Because, as mentioned before, the shapes/decays of f(x) on the left and right side are different and this affects our algorithm A, i.e., the number of iterations to reach some epsilon precision. Question: Is this a problem to have two peaks in the halting distribution not like the ones shown in figures? Answer: No, it is perfectly fine. What is not fine is that when I run the same algorithm A on another problem which is unimodal, then I will get a completely different halting distribution, e.g., like the \"one peak\" ones shown in the paper. Question: Where is the problem? Answer: The problem is that these observations contradict the discussed universality in halting time distribution which is an intrinsic thing to A and does not depend on the problem. Question: This was stated for large dimension N and not one-dimensional problem. Answer: Increase the dimensionality of the toy problem preserving the idea of the two (multiple) optima with, e.g., basins of attraction of equal size (keep in mind the  course of dimensionality). Question: There is a restriction on the class of ensembles E. Answer: If we restrict our-self to unimodal problems then I barely see \"universality\". Moreover, unimodal non-convex problems may exhibit the same properties as multi-modal ones, e.g., make a horizontal one-dimensional slice of 2-dimensional Rosenbrock function. Question: Why we see these nice-looking distribution for very different problems? Answer: because for some problems the multi-modality is negligeable and the dynamical system behind the optimizer may behave similarly for different values of N, especially for the large ones. Nevertheless, to claim it to be the \"universal law\" is misleading since it depends on the properties of the problem at hand and is not strictly intrinsic to the optimizer. Moreover, for the same (class of) problem the landscape may qualitatively change with increasing N. Rosenbrock function is a good example, it is uni-modal for 2 variables and multi-modal for multiple variables which would affect the halting time distribution. The \"threshold\" dimension value 2 can be turned to 2 billion if needed.", "IS_META_REVIEW": false, "DATE": "16 Dec 2016", "is_meta_review": false}, {"TITLE": "Figure 1", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "", "IS_META_REVIEW": false, "DATE": "09 Dec 2016", "is_meta_review": false}, {"TITLE": "Why", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "", "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "is_meta_review": false}], "SCORE": 5, "authors": "Levent Sagun, Thomas Trogdon, Yann LeCun", "KEYWORDS": "Normalized halting time distributions are independent of the input data distribution.", "accepted": false, "id": ""}
