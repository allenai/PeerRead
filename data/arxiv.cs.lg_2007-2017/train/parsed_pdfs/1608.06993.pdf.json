{
  "name" : "1608.06993.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "DENSELY CONNECTED CONVOLUTIONAL NETWORKS",
    "authors" : [ "Gao Huang", "Zhuang Liu" ],
    "emails" : [ "gh349@cornell.edu", "liuzhuang13@mails.tsinghua.edu.cn", "kqw4@cornell.edu" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "Convolutional neural networks (CNNs) have become the dominant machine learning approach for visual object recognition. Although they have been originally introduced over 20 years ago (LeCun et al., 1989) only recent improvements in computer hardware and network structure have enabled the training of truly deep CNNs. The original LeNet5 (LeCun et al., 1998) consisted of 5 layers, VGG-net featured 19 (Russakovsky et al., 2015), and only last year Highway Networks (Srivastava et al., 2015) and Residual Networks (ResNets) (He et al., 2015b) have surpassed the 100 layers barrier.\nAs CNNs become increasingly deep, a new research problem emerges: as information about the input or gradient passes through many layers, it can vanish and “wash out” by the time it reaches the end (or beginning) of the network. Many recent publications address this or related problems. ResNets (He et al., 2015b) and Highway Networks (Srivastava et al., 2015) bypass signal from one layer to the next via identity connections. Stochastic Depth (Huang et al., 2016) shortens ResNets by randomly dropping layers during training to allow better information and gradient flow. Recently, Larsson et al. (2016) introduced FractalNets, which repeatedly combine several parallel layer sequences with different number of convolutional blocks to obtain a large nominal depth, while main-\n∗Authors contribute equally.\nar X\niv :1\n60 8.\n06 99\n3v 1\n[ cs\n.C V\n] 2\n5 A\ntaining many short paths in the network. Although these different approaches vary in network topology and training procedure, we observe a key characteristic shared by all of them: they create short paths from earlier layers near the input to those later layers near the output.\nIn this paper we propose an architecture that distills this insight into a simple and clean connectivity pattern. The idea is straight-forward, yet compelling: to ensure maximum information flow between layers in the network, we connect all layers directly with each other. To preserve the feed-forward nature, each layer obtains additional inputs from all preceding layers and passes on its own feature maps to all subsequent layers. Figure 1 illustrates this layout schematically. Crucially, in contrast to ResNets, we never combine features through summation before they are passed into a layer, instead we provide them all as separate inputs. For example, the `th layer has ` inputs, consisting of the feature maps of all preceding convolutional blocks. Its own feature maps are passed on to all L− ` subsequent layers. This introduces L(L+1)2 connections in an L-layer network, instead of just L, as in traditional feed-forward architectures. Because of its dense connectivity pattern, we refer to our approach as Dense Convolutional Network (DenseNet).\nA possibly counter-intuitive feature of the DenseNet is that it requires fewer parameters than traditional networks. Although the number of connections grows quadratically with depth, the topology encourages heavy feature reuse. Prior work (Huang et al., 2016) has shown that there is great redundancy within the feature maps of the individual layers in ResNets. In DenseNets, all layers have direct access to every feature map from all preceding layers, which means that there is no need to re-learn redundant feature maps. Consequently, DenseNet layers are very narrow (on the order of 12 feature maps per layer) and only add a small set of feature maps to the “collective knowledge” of the whole network.\nTraditional feed-forward architectures can be viewed as algorithms with a state, which is passed on from layer to layer. Each layer reads the state from its previous layer and writes to the subsequent layer. It changes the states but also passes on information that needs to be preserved. ResNets (He et al., 2015b) make this information preservation explicit through additive identity transformations. Recent variations of ResNets (Huang et al., 2016) show that many layers make only very small changes, and in fact can be randomly dropped during training. This makes the state of ResNets more akin to (unrolled) recurrent neural networks (Liao & Poggio, 2016), however the number of parameters of ResNets is substantially larger because each layer still has its own weights. Our proposed DenseNet architecture explicitly differentiates between information that is added to the network and information that is preserved. Each layer only adds a small set of feature maps to the\ncollective knowledge of the network and keeps the remaining feature maps unchanged. The final classifier makes a decision based on the entire knowledge of the network.\nAlthough they follow a simple generative rule, DenseNets are very general and easy to train. One big advantage of the DenseNet is its improved flow of information and gradients throughout the network. Each layer has direct access to the gradients from the loss function and the original input signal. This improves training, especially of deeper network architectures. Finally, we also observe that the short connections have a regularizing effect on the network, which tends to avoid overfitting in settings with smaller datasets (or those without data augmentation).\nWe evaluate DenseNets on several highly competitive benchmark datasets (CIFAR-10, CIFAR-100, SVHN) with and without data augmentation. Our models tend to require fewer parameters than existing algorithms with comparable accuracy. Further, we significantly outperform the current state-of-the-art results on all 5 highly competitive benchmark tasks."
    }, {
      "heading" : "2 RELATED WORK",
      "text" : "The exploration of different network architectures has been a part of neural network research ever since their initial discovery. The recent resurgence in popularity has also revived this research domain. The increasing number of layers in modern neural networks amplifies the differences between architectures and motivates the exploration of different connectivity patterns and the revisiting of some older research ideas.\nA cascade structure similar to our proposed dense network layout has already been studied in the neural networks literature over a quarter of a century ago by Fahlman & Lebiere (1989). Their pioneering work focuses on fully connected multi-layer perceptrons trained in a layer-by-layer manner. More recently Wilamowski & Yu (2010) proposed fully connected cascade networks to be trained with batch gradient descent. Although effective on small datasets, this approach only scales to networks with a few hundred parameters.\nHighway Networks (Srivastava et al., 2015) were amongst the first architectures that provided a means to effectively train end-to-end networks with more than 100 layers. By introducing the bypassing layers along with gating units, Srivastava et al. (2015) showed that Highway Networks with hundreds of layers can be optimized with SGD effectively. The bypassing paths are perhaps the key factor that eases the training of very deep networks. This point is further supported by ResNet (He et al., 2015b), in which pure identity mappings are used as bypassing layers. ResNets have achieved impressive (often record breaking) achievements on most challenging image recognition, localization and detection tasks, such as ImageNet and COCO object detection dataset (He et al., 2015b). Recently, Huang et al. (2016) successfully trained a 1202-layer ResNet with stochastic depth (Huang et al., 2016). Stochastic depth is a mechanism to improve the training of deep residual networks by dropping layers randomly. It also shows that not all layers may be needed and highlights that there is a great amount of redundancy in deep (residual) networks. This paper was in part inspired by this observation. He et al. (2016) introduces ResNets with pre-activation, which similar to stochastic depth allow state-of-the-art performance with > 1000 layers.\nAn orthogonal approach to making networks deeper (e.g., with the help of skip connections) is to increase the network width. The GoogLeNet (Szegedy et al., 2015a;b) uses an “Inception module” which concatenates feature maps produced by filters of different sizes. Targ et al. (2016) propose a variant of ResNets with wide generalized residual blocks. In fact, simply increasing the number of filers in each layer of ResNets can improve its performance provided the depth is sufficient (Zagoruyko & Komodakis, 2016). The FractalNet proposed by Larsson et al. (2016) also achieves competitive results on several benchmark datasets using a wide network structure.\nInstead of drawing the representational power from extremely deep or wide architectures, DenseNets fully explore the potential of the network through feature reuse, yielding condensed models that are straight-forward to train and highly parameter efficient. Concatenating feature maps learned by different layers tends to increase the variety of the input of subsequent layers and improves efficiency. This constitutes a major difference between DenseNets and ResNets. Compared to Inception networks, Szegedy et al. (2015a;b), which also concatenate features from different layers, DenseNets are simpler and more efficient.\nThere are other notable network architecture innovations which have yielded competitive results. The Network in Network (NIN) (Lin et al., 2013) structure includes micro multi-layer perceptrons into the filters of convolutional layers to extract more complicated features. In Deeply Supervised Network (DSN) (Lee et al., 2015), internal layers are directly supervised by auxiliary classifiers, which can strengthen the gradients received by earlier layers. The Ladder Networks (Rasmus et al., 2015; Pezeshki et al., 2015) introduce lateral connections into autoencoders, leading to impressive accuracy on semi-supervised learning tasks ."
    }, {
      "heading" : "3 DENSENETS",
      "text" : "Let us consider the setting of a single image x0 that is passed through our convolutional neural network. Our network is made out of L layers, each of which consists of a non-linear transformation H`(·), where ` indexes the layer. H`(·) can be a composite function of operations such as Batch Normalization (BN) (Ioffe & Szegedy, 2015), rectified linear units (ReLU) (Glorot et al., 2011), Pooling (LeCun et al., 1998), or Convolution (Conv). We denote the output of the `th layer as x`.\nResNets. Traditional convolutional feed-forward networks connect the output of the `th layer as input to the (`+1)th layer (Krizhevsky et al., 2012), which gives rise to the following layer transition x` = H`(x`−1). Recently ResNets (He et al., 2015b) proposed to add a skip-connection that bypass the non-linear transformations with an identity function. The resulting layer transition becomes\nx` = H`(x`−1) + x`−1. (1)\nOne advantage of ResNets is that the gradient flows directly through the identity function from later layers to the earlier layers. However, the identity function and the output of H` are combined in summation, which could impedes the forward information flow of earlier layers to later layers.\nDense connectivity. To further improve the information flow between layers we propose a different connectivity pattern: we introduce connections from any layer to any other layer. Figure 1 illustrates the layout of a DenseNet schematically. To ensure that the network stays feed-forward, we pass the feature maps of the earlier layer as inputs into the later layer. Consequently, the `th layer receives the feature maps of all preceding layers, x0, . . . ,x`−1, as input, i.e.,\nx` = H`([x0,x1, . . . ,x`−1]), (2)\nwhere [x0,x1, . . . ,x`−1] refers to the concatenation of the feature maps produced in layers 0, . . . , ` − 1. Because of its dense connectivity we refer to our network architecture as Dense Convolutional Network (DenseNet). For ease of implementation , we concatenate the multiple inputs of H`() in eq. (2) into a single tensor.\nComposite function. Motivated by He et al. (2016) we define H`(·) as a composite function of three consecutive operations: Batch Normalization (BN) (Ioffe & Szegedy, 2015), followed by a rectified linear unit (ReLU) (Glorot et al., 2011), and then Convolution (Conv). Other choices may be possible, but for simplicity we only consider the BN-ReLU-Conv version of H` for all layers ` (except dedicated pooling layers).\nPooling layers. The concatenation operation, as used in eq. (2) is not viable when the size of feature maps changes. However, an essential part of convolutional neural networks are pooling layers, which do change the size of feature maps. To facilitate pooling in our architecture we divide the network into multiple densely connected dense blocks (see Figure 2). We refer to layers between blocks as transition layers, which do convolution and pooling. The transition layers used in our\nexperiments consist of a 1×1 convolutional layer followed by a 2×2 average pooling layer. For simplicity, we preserve the number of feature maps across transition layers, i.e., if the first dense block contains m feature maps, we let the first transition layer generate m output feature maps. There are no connections across dense blocks other than the transition layer. We typically use three dense blocks in our architectures.\nGrowth rate. If each function H` produces k feature maps as output, it follows that the `th layer has k × (` − 1) + k0 input feature maps (including the k0 channels of the input). To prevent the network from growing too wide and to improve the parameter efficiency we limit k to a small integer (e.g., k = 12). We refer to this hyper-parameter k as the growth rate of the network. We show in Section 4 that in fact a relatively small growth rate is sufficient to obtain state-of-the-art results on the datasets that we tested on. One explanation for this is that each layer has access to all the preceding feature maps in its block and therefore its “collective knowledge”. One can view the feature maps as the global state of the network. Each layer adds k feature maps of its own to this state. The growth rate regulates how much new information each layer contributes to the global state. This global state, once written, can be accessed from everywhere within the network and different from traditional network architectures there is no need to replicate it from layer to layer.\nImplementation Details. The DenseNet used in our experiments has three dense blocks with equal numbers of layers. Before entering the first dense block, a convolution with 16 output channels (comparable with the growth rates we use) is performed on the input images. Within each dense block, all the convolutional layers use filters with kernel size 3×3, and each side of the inputs is zero-padded by one pixel to keep the feature map size fixed. We use 1×1 convolution followed by 2×2 average pooling as transition layers between two contiguous dense blocks. At the end of the last dense block, a global average pooling is performed and then a softmax classifier is attached. On all the datasets we used, the feature map sizes in the three dense blocks are 32× 32, 16×16, and 8×8, respectively. We experimented with two growth rate values, k=12 and k=24 and networks with L=40, and L=100 layers."
    }, {
      "heading" : "4 EXPERIMENTS",
      "text" : "We empirically demonstrate the effectiveness of DenseNet on several benchmark datasets and compare it with state-of-the-art network architectures, especially with ResNet and its variants. The implementation is in Torch 7 (Collobert et al., 2011). The code to reproduce the results is available at https://github.com/liuzhuang13/DenseNet."
    }, {
      "heading" : "4.1 DATASETS",
      "text" : "CIFAR. The two CIFAR datasets (Krizhevsky & Hinton, 2009) consist of colored natural scene images, with 32×32 pixels each. CIFAR-10 (C10) images are drawn from 10 and CIFAR-100 (C100) images from 100 classes. The train and test sets contain 50,000 and 10,000 images respectively, and we hold out 5,000 training images as the validation set. A standard data augmentation scheme is widely used for this dataset (Lin et al., 2013; Romero et al., 2014; Lee et al., 2015; Springenberg et al., 2014; Srivastava et al., 2015; He et al., 2015b; Huang et al., 2016; Larsson et al., 2016): the images are first zero-padded with 4 pixels on each side, then randomly cropped to again produce 32×32 images; half of the images are then horizontally mirrored. We denote this augmentation scheme by a “+” mark at the end of the dataset name (e.g., C10+). For data preprocessing, we normalize the data using the channel means and standard deviations. We evaluate our algorithm on all four datasets: C10, C100, C10+, C100+. For the final run we use all 50,000 training images and report the final test error at the end of training.\nSVHN. The Street View House Numbers (SVHN) dataset (Netzer et al., 2011) contains 32×32 colored digit images coming from Google Street View. The task is to classify the central digit into the correct one of the 10 digit classes. There are 73,257 images in the training set, 26,032 images in the test set, and 531,131 images for additional training. Following common practice (Sermanet et al., 2012; Goodfellow et al., 2013; Lin et al., 2013; Lee et al., 2015; Huang et al., 2016) we use all the training data without any data augmentation, and a validation set with 6,000 images is split from the training set. We select the model with the lowest validation error during training and report\nthe test error. For preprocessing, we simply divide the raw input pixel values by its maximum, 255, to let them fall into the range [0, 1], as in Zagoruyko & Komodakis (2016).\nTraining Setting. The network is trained using SGD for 300 epochs on CIFAR and 40 epochs on SVHN, both with a mini-batch size of 64. The initial learning rate is set to 0.1, and is divided by 10 at 0.5 and 0.75 fraction of the total number of training epochs. Following Gross & Wilber (2016), we use a weight decay of 1e-4, momentum of 0.9 and Nesterov momentum (Sutskever et al., 2013) with 0 dampening. We adopt the weight initialization introduced by He et al. (2015a), and also equip the network with batch normalization (Ioffe & Szegedy, 2015). Specifically, each layer performs batch normalization, ReLU and convolution (BN-ReLU-Conv) in sequence. For the three datasets without data augmentation, i.e., C10, C100 and SVHN, we add a Dropout (Srivastava et al., 2014) layer after each convolutional layer, and the drop rate is set to 0.2. The final test errors were only evaluated once for each task and model."
    }, {
      "heading" : "4.2 CLASSIFICATION RESULTS.",
      "text" : "We train DenseNets with different depths, L, and growth rates, k. The main results are shown in Table 1. To highlight general trends, we mark all results that outperform the existing state-of-the-art in boldface and the overall best result in blue.\nAccuracy. Possibly the most noticeable trend may originate from the bottom row of Table 1, which shows that DenseNet with L=100 and k=24 outperforms the existing state-of-the-art consistently on all five datasets. Its error rate of 3.74% on C10+ (with data augmentation) is significantly lower than that achieved by wide ResNet architecture (Zagoruyko & Komodakis, 2016). On SVHN, with Dropout, it also surpasses the current best result achieved by wide ResNet. However, even when the growth rate is lowered to k=12, which reduces the number of learned parameters to roughly 1/4, it still performs highly competitively. In fact, this architecture achieves the lowest error on C10, and outperforms all existing state-of-the-art results across the four CIFAR datasets.\nCapacity. There is a general trend that DenseNets perform better as L and k increase. We attribute this primarily to the corresponding parameters growth. This is best demonstrated by the column of\nC10+ and C100+. On C10+, the error drops from 5.24% to 4.10% and finally to 3.74% as the number of parameters increases from 1.0M, over 7.0M to 27.2M. On C100+, we observe a similar trend. This suggests that DenseNets can utilize the increased representational power of bigger and deeper models. It also indicates that they do not suffer from overfitting or the optimization difficulties pointed out by He et al. (2015b). In fact, it is possible that a deeper DenseNet with more parameters could achieve even better results.\nParameter Efficiency. The results in Table 1 indicate that DenseNets utilize parameters more effectively than prior work (in particular ResNets). As a comparison, the DenseNet with 1.0M parameters achieves better or comparable results than the ResNet (pre-activation) with 1.7M parameters (He et al., 2016). Also, the DenseNet with 7.0M parameters consistently outperforms ResNets (pre-activation) with 10.2M parameters (e.g., 4.10% vs 4.62% error on C10+, 20.20% vs 22.71% error on C100+). It even obtains lower error rates than a 36.5M-parameter wide ResNet on C10+ and C100+. Figure 3 shows the training loss and test error of DenseNet with 7.0M parameters (L=100 and k=12) compared to a 1001-layer pre-activation ResNet with 10.2M parameters on C10+. The 1001-layer deep ResNet converges to a lower training loss value but a higher test error. We will analyze this effect in more detail in following paragraph.\nOverfitting. One positive side-effect of the more efficient use of parameters may be a tendency of DenseNets to be less prone to overfitting. We did observe that typically Dropout is not necessary with DenseNet architectures, although we did use it for datasets without data augmentation (which are substantially smaller and where overfitting is more of a nuisance). We observe that on these datasets the improvements of DenseNet architectures over prior work are particularly pronounced. On C10 the improvement denotes a 20% relative reduction in error from 7.33% to 5.83%. On C100 the reduction is about 17% from 28.20% to 23.42%. Out of curiosity, we did evaluate DenseNet with L = 40 and k = 12 on C10, C100 without Dropout, and the resulting error rates are 8.85% and 32.53%, which are better than all previous methods except FractalNets with Dropout/Drop-path regularization. There may be a single setting where we may observe overfitting: on C10 a 4× growth of parameters, caused by increasing k=12 to k=24, leads to a modest increase in error, from 5.77% to 5.83%. As this increase is within the range of natural fluctuations it is ultimately not clear if it is caused by overfitting."
    }, {
      "heading" : "5 DISCUSSION",
      "text" : "Superficially, DenseNets may be considered quite similar to ResNets. Eq.(2) differs from eq. (1) only in that the inputs to H`(·) are concatenated and not added. However, the implications of this seemingly small modification lead to substantially different behaviors amongst the two network architectures.\nParameter Efficiency. As a direct consequence of the input concatenation the feature maps learned by any of the DenseNet layers can be accessed by all subsequent layers. This encourages feature reuse throughout the network.\nFigure 5 shows the result of an experiment aimed to show the improved parameter efficiency of DenseNets over ResNets. We train multiple small networks with varying depths on C10+ and compare their test accuracies. In comparison with other popular network architectures, like AlexNet (Krizhevsky et al., 2012) or VGG-net (Russakovsky et al., 2015), ResNets with pre-activation use fewer parameters while typically achieving better results (He et al., 2016). Hence we compare DenseNet (k = 12) against this architecture. The training setting for DenseNet is kept the same as in the previous section.\nOne immediate observation from this graph is that with similar number of parameters, DenseNets yield significantly better results compared to ResNets. To achieve the same level of accuracy, DenseNets only require approximately 1/2 of the parameters of ResNets.\nImplicit Deep Supervision. One explanation for the improved accuracy of dense convolutional networks may be that individual layers receive additional supervision from the loss function through the shorter connections. Lee et al. (2015) propose deeply-supervised nets (DSN), which have classifiers attached to every hidden layer, enforcing the intermediate layers to learn discriminative features. The authors demonstrate that such an architecture can be beneficial to train deep networks. One can interpret DenseNets to perform a similar deep supervision, however in an implicit yet elegant fashion. Only one single classifier at the end provides direct supervision to all layers (through at most two transition layers). In contrast to DSNs, the loss function and gradients are substantially less complicated and the training objective of all layers align.\nFeature Reuse. By design, DenseNets allow layers access to feature maps from all of its preceding layers (although sometimes through transition layers). Is access to all of these layers really necessary? Would it be sufficient for a layer to only have access to m earlier feature maps? To answer this question, we conduct two experiments with DenseNets on C10+.\nOur first experiment aims to discover if layers make use of all the earlier feature maps in practice. We first train a DenseNet on C10+ with L=124 and, for visualization purposes, growth rate k=1. For each convolutional layer we separately compute the l2-norm of the filter weights of each input feature map. The weight norm serves as an approximate for the dependency of a convolutional layer on its preceding layers. The results are shown in the heat map in Figure 4, in which columns correspond to individual layers, rows correspond feature maps, and each pixel encodes the norm of weights associated with a convolutional layer and a feature map. Orange/yellow dots indicate that this layer (x-axis) makes strong use of this particular feature map (y-axis), dark blue dots indicate that this feature map was not used in this layer. For better visualization we normalize the norms within each column and divide by its maximum value. From Figure 4, several interesting observations can be made.\n1. Every layer spreads its weights over many of its inputs. It shows that features extracted by very earlier layers are indeed directly used by deep layers throughout the DenseNet.\n2. The right triangle regions in the low part of each dense block correspond to within-block feature use. These tend to be brighter than other regions, indicating a bias towards the use of within block features. However among these features, there is no clear trend that later ones are more heavily used.\n3. The two transition layers and the classification layer, which are highlighted by red boxes in the heat map and shown enlarged on the right rely more on features from deeper layers. This may suggest that later layers extract more discriminative features.\nUltimately we conclude from this first experiment that convolutional layers tend to use features from all over the network. Our second experiment investigates if this is necessary. We modify the structure of a DenseNet and remove all “long-distance” connections, such that for each layer only feature maps from the m closest preceding feature maps remain as inputs. Within each dense block we set m to be the initial number of feature maps in this block (lower values of m would render some outputs of the transition layer inaccessible). We refer to this setup as a partial DenseNet. We train two networks with L= 40 and k= 12, one with partial connectivity, the other with all dense connections. For the partial DenseNet, we have m = 16 for the first dense block, as the initial convolutional layer produces 16 output feature maps. For block 2 and 3 the values of m are 160 and 304, respectively. The corresponding training curves are shown in Figure 6. The graph shows that removing the direct “long-distance” connections between layers, which are far from each other, degrades the performance significantly. This seems to suggest that long distance dependencies may indeed be necessary to produce the low error rates shown in Section 4."
    }, {
      "heading" : "6 CONCLUSION",
      "text" : "We proposed a new convolutional neural network architecture, which we refer to as Dense Convolutional Network (DenseNet). It introduces direct connections within all layers of a “dense block” in the network. We showed that DenseNets scale naturally to over one hundred layers, while exhibiting no optimization difficulties. In our experiments, DenseNets tend to yield consistent improvement in accuracy with growing number of parameters, without any signs of performance degradation or overfitting. Under multiple settings it achieved state-of-the-art results across several datasets.\nAlthough following a simple connectivity rule, DenseNets naturally integrate the nice properties of identity mappings, deep supervision and diversified depth. They allow feature reuse throughout the networks and can consequently learn more compact and, according to our experiments, more accurate models. Because of their compact internal representations and reduced feature redundancy, DenseNets could be beneficial as feature extractors for a wide range of computer vision tasks that build upon deep convolutional features, e.g., Gardner et al. (2015) or Gatys et al. (2015). Finally, we believe that there may be further gains in accuracy obtainable through more detailed fine-tuning of hyper-parameters or learning rate schedules."
    }, {
      "heading" : "ACKNOWLEDGMENTS",
      "text" : "The authors are supported in part by the, III-1618134, III-1526012, IIS-1149882 grants from the National Science Foundation. Gao Huang is supported by the International Postdoctoral Exchange Fellowship Program of China Postdoctoral Council (No.20150015). Zhuang Liu is supported by the National Basic Research Program of China Grants 2011CBA00300, 2011CBA00301, the National Natural Science Foundation of China Grant 61361136003. We also thank Geoff Pleiss and Yu Sun for many insightful discussions."
    } ],
    "references" : [ {
      "title" : "Torch7: A matlab-like environment for machine learning",
      "author" : [ "Ronan Collobert", "Koray Kavukcuoglu", "Clément Farabet" ],
      "venue" : "In BigLearn, NIPS Workshop, number EPFL-CONF-192376,",
      "citeRegEx" : "Collobert et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Collobert et al\\.",
      "year" : 2011
    }, {
      "title" : "The cascade-correlation learning architecture",
      "author" : [ "Scott E Fahlman", "Christian Lebiere" ],
      "venue" : null,
      "citeRegEx" : "Fahlman and Lebiere.,? \\Q1989\\E",
      "shortCiteRegEx" : "Fahlman and Lebiere.",
      "year" : 1989
    }, {
      "title" : "Deep manifold traversal: Changing labels with convolutional features",
      "author" : [ "Jacob R Gardner", "Matt J Kusner", "Yixuan Li", "Paul Upchurch", "Kilian Q Weinberger", "John E Hopcroft" ],
      "venue" : "arXiv preprint arXiv:1511.06421,",
      "citeRegEx" : "Gardner et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Gardner et al\\.",
      "year" : 2015
    }, {
      "title" : "A neural algorithm of artistic style",
      "author" : [ "Leon A Gatys", "Alexander S Ecker", "Matthias Bethge" ],
      "venue" : "arXiv preprint arXiv:1508.06576,",
      "citeRegEx" : "Gatys et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Gatys et al\\.",
      "year" : 2015
    }, {
      "title" : "Deep sparse rectifier neural networks",
      "author" : [ "Xavier Glorot", "Antoine Bordes", "Yoshua Bengio" ],
      "venue" : "In Aistats,",
      "citeRegEx" : "Glorot et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Glorot et al\\.",
      "year" : 2011
    }, {
      "title" : "Training and investigating residual nets, 2016",
      "author" : [ "Sam Gross", "Michael Wilber" ],
      "venue" : "URL http:// torch.ch/blog/2016/02/04/resnets.html",
      "citeRegEx" : "Gross and Wilber.,? \\Q2016\\E",
      "shortCiteRegEx" : "Gross and Wilber.",
      "year" : 2016
    }, {
      "title" : "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun" ],
      "venue" : "In Proceedings of the IEEE International Conference on Computer Vision,",
      "citeRegEx" : "He et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2015
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun" ],
      "venue" : "arXiv preprint arXiv:1512.03385,",
      "citeRegEx" : "He et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2015
    }, {
      "title" : "Identity mappings in deep residual networks",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun" ],
      "venue" : "arXiv preprint arXiv:1603.05027,",
      "citeRegEx" : "He et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2016
    }, {
      "title" : "Deep networks with stochastic depth",
      "author" : [ "Gao Huang", "Yu Sun", "Zhuang Liu", "Daniel Sedra", "Kilian Weinberger" ],
      "venue" : "arXiv preprint arXiv:1603.09382,",
      "citeRegEx" : "Huang et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2016
    }, {
      "title" : "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "author" : [ "Sergey Ioffe", "Christian Szegedy" ],
      "venue" : "arXiv preprint arXiv:1502.03167,",
      "citeRegEx" : "Ioffe and Szegedy.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ioffe and Szegedy.",
      "year" : 2015
    }, {
      "title" : "Learning multiple layers of features from tiny images",
      "author" : [ "Alex Krizhevsky", "Geoffrey Hinton" ],
      "venue" : null,
      "citeRegEx" : "Krizhevsky and Hinton.,? \\Q2009\\E",
      "shortCiteRegEx" : "Krizhevsky and Hinton.",
      "year" : 2009
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing",
      "author" : [ "Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton" ],
      "venue" : null,
      "citeRegEx" : "Krizhevsky et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Krizhevsky et al\\.",
      "year" : 2012
    }, {
      "title" : "Fractalnet: Ultra-deep neural networks without residuals",
      "author" : [ "Gustav Larsson", "Michael Maire", "Gregory Shakhnarovich" ],
      "venue" : "arXiv preprint arXiv:1605.07648,",
      "citeRegEx" : "Larsson et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Larsson et al\\.",
      "year" : 2016
    }, {
      "title" : "Backpropagation applied to handwritten zip code recognition",
      "author" : [ "Yann LeCun", "Bernhard Boser", "John S Denker", "Donnie Henderson", "Richard E Howard", "Wayne Hubbard", "Lawrence D Jackel" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "LeCun et al\\.,? \\Q1989\\E",
      "shortCiteRegEx" : "LeCun et al\\.",
      "year" : 1989
    }, {
      "title" : "Gradient-based learning applied to document recognition",
      "author" : [ "Yann LeCun", "Léon Bottou", "Yoshua Bengio", "Patrick Haffner" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "LeCun et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "LeCun et al\\.",
      "year" : 1998
    }, {
      "title" : "Bridging the gaps between residual learning, recurrent neural networks and visual cortex",
      "author" : [ "Qianli Liao", "Tomaso Poggio" ],
      "venue" : "arXiv preprint arXiv:1604.03640,",
      "citeRegEx" : "Liao and Poggio.,? \\Q2016\\E",
      "shortCiteRegEx" : "Liao and Poggio.",
      "year" : 2016
    }, {
      "title" : "Reading digits in natural images with unsupervised feature learning",
      "author" : [ "Yuval Netzer", "Tao Wang", "Adam Coates", "Alessandro Bissacco", "Bo Wu", "Andrew Y Ng" ],
      "venue" : "In NIPS Workshop on Deep Learning and Unsupervised Feature Learning,",
      "citeRegEx" : "Netzer et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Netzer et al\\.",
      "year" : 2011
    }, {
      "title" : "Deconstructing the ladder network architecture",
      "author" : [ "Mohammad Pezeshki", "Linxi Fan", "Philemon Brakel", "Aaron Courville", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1511.06430,",
      "citeRegEx" : "Pezeshki et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Pezeshki et al\\.",
      "year" : 2015
    }, {
      "title" : "Semisupervised learning with ladder networks",
      "author" : [ "Antti Rasmus", "Mathias Berglund", "Mikko Honkala", "Harri Valpola", "Tapani Raiko" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Rasmus et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Rasmus et al\\.",
      "year" : 2015
    }, {
      "title" : "Fitnets: Hints for thin deep nets",
      "author" : [ "Adriana Romero", "Nicolas Ballas", "Samira Ebrahimi Kahou", "Antoine Chassang", "Carlo Gatta", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1412.6550,",
      "citeRegEx" : "Romero et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Romero et al\\.",
      "year" : 2014
    }, {
      "title" : "Imagenet large scale visual recognition challenge",
      "author" : [ "Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael Bernstein" ],
      "venue" : "International Journal of Computer Vision,",
      "citeRegEx" : "Russakovsky et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Russakovsky et al\\.",
      "year" : 2015
    }, {
      "title" : "Convolutional neural networks applied to house numbers digit classification",
      "author" : [ "Pierre Sermanet", "Soumith Chintala", "Yann LeCun" ],
      "venue" : "In Pattern Recognition (ICPR),",
      "citeRegEx" : "Sermanet et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Sermanet et al\\.",
      "year" : 2012
    }, {
      "title" : "Striving for simplicity: The all convolutional net",
      "author" : [ "Jost Tobias Springenberg", "Alexey Dosovitskiy", "Thomas Brox", "Martin Riedmiller" ],
      "venue" : "arXiv preprint arXiv:1412.6806,",
      "citeRegEx" : "Springenberg et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Springenberg et al\\.",
      "year" : 2014
    }, {
      "title" : "Dropout: a simple way to prevent neural networks from overfitting",
      "author" : [ "Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Srivastava et al\\.,? \\Q1929\\E",
      "shortCiteRegEx" : "Srivastava et al\\.",
      "year" : 1929
    }, {
      "title" : "On the importance of initialization and momentum in deep learning",
      "author" : [ "Ilya Sutskever", "James Martens", "George Dahl", "Geoffrey Hinton" ],
      "venue" : "In Proceedings of the 30th international conference on machine learning",
      "citeRegEx" : "Sutskever et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2013
    }, {
      "title" : "Going deeper with convolutions",
      "author" : [ "Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Szegedy et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Szegedy et al\\.",
      "year" : 2015
    }, {
      "title" : "Rethinking the inception architecture for computer vision",
      "author" : [ "Christian Szegedy", "Vincent Vanhoucke", "Sergey Ioffe", "Jonathon Shlens", "Zbigniew Wojna" ],
      "venue" : "arXiv preprint arXiv:1512.00567,",
      "citeRegEx" : "Szegedy et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Szegedy et al\\.",
      "year" : 2015
    }, {
      "title" : "Resnet in resnet: Generalizing residual architectures",
      "author" : [ "Sasha Targ", "Diogo Almeida", "Kevin Lyman" ],
      "venue" : "arXiv preprint arXiv:1603.08029,",
      "citeRegEx" : "Targ et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Targ et al\\.",
      "year" : 2016
    }, {
      "title" : "Neural network learning without backpropagation",
      "author" : [ "Bogdan M Wilamowski", "Hao Yu" ],
      "venue" : "IEEE Transactions on Neural Networks,",
      "citeRegEx" : "Wilamowski and Yu.,? \\Q2010\\E",
      "shortCiteRegEx" : "Wilamowski and Yu.",
      "year" : 2010
    }, {
      "title" : "Wide residual networks",
      "author" : [ "Sergey Zagoruyko", "Nikos Komodakis" ],
      "venue" : "arXiv preprint arXiv:1605.07146,",
      "citeRegEx" : "Zagoruyko and Komodakis.,? \\Q2016\\E",
      "shortCiteRegEx" : "Zagoruyko and Komodakis.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 14,
      "context" : "Although they have been originally introduced over 20 years ago (LeCun et al., 1989) only recent improvements in computer hardware and network structure have enabled the training of truly deep CNNs.",
      "startOffset" : 64,
      "endOffset" : 84
    }, {
      "referenceID" : 15,
      "context" : "The original LeNet5 (LeCun et al., 1998) consisted of 5 layers, VGG-net featured 19 (Russakovsky et al.",
      "startOffset" : 20,
      "endOffset" : 40
    }, {
      "referenceID" : 21,
      "context" : ", 1998) consisted of 5 layers, VGG-net featured 19 (Russakovsky et al., 2015), and only last year Highway Networks (Srivastava et al.",
      "startOffset" : 51,
      "endOffset" : 77
    }, {
      "referenceID" : 9,
      "context" : "Stochastic Depth (Huang et al., 2016) shortens ResNets by randomly dropping layers during training to allow better information and gradient flow.",
      "startOffset" : 17,
      "endOffset" : 37
    }, {
      "referenceID" : 6,
      "context" : ", 2015) and Residual Networks (ResNets) (He et al., 2015b) have surpassed the 100 layers barrier. As CNNs become increasingly deep, a new research problem emerges: as information about the input or gradient passes through many layers, it can vanish and “wash out” by the time it reaches the end (or beginning) of the network. Many recent publications address this or related problems. ResNets (He et al., 2015b) and Highway Networks (Srivastava et al., 2015) bypass signal from one layer to the next via identity connections. Stochastic Depth (Huang et al., 2016) shortens ResNets by randomly dropping layers during training to allow better information and gradient flow. Recently, Larsson et al. (2016) introduced FractalNets, which repeatedly combine several parallel layer sequences with different number of convolutional blocks to obtain a large nominal depth, while main∗Authors contribute equally.",
      "startOffset" : 41,
      "endOffset" : 704
    }, {
      "referenceID" : 9,
      "context" : "Prior work (Huang et al., 2016) has shown that there is great redundancy within the feature maps of the individual layers in ResNets.",
      "startOffset" : 11,
      "endOffset" : 31
    }, {
      "referenceID" : 9,
      "context" : "Recent variations of ResNets (Huang et al., 2016) show that many layers make only very small changes, and in fact can be randomly dropped during training.",
      "startOffset" : 29,
      "endOffset" : 49
    }, {
      "referenceID" : 9,
      "context" : "(2016) successfully trained a 1202-layer ResNet with stochastic depth (Huang et al., 2016).",
      "startOffset" : 70,
      "endOffset" : 90
    }, {
      "referenceID" : 19,
      "context" : "Highway Networks (Srivastava et al., 2015) were amongst the first architectures that provided a means to effectively train end-to-end networks with more than 100 layers. By introducing the bypassing layers along with gating units, Srivastava et al. (2015) showed that Highway Networks with hundreds of layers can be optimized with SGD effectively.",
      "startOffset" : 18,
      "endOffset" : 256
    }, {
      "referenceID" : 6,
      "context" : "This point is further supported by ResNet (He et al., 2015b), in which pure identity mappings are used as bypassing layers. ResNets have achieved impressive (often record breaking) achievements on most challenging image recognition, localization and detection tasks, such as ImageNet and COCO object detection dataset (He et al., 2015b). Recently, Huang et al. (2016) successfully trained a 1202-layer ResNet with stochastic depth (Huang et al.",
      "startOffset" : 43,
      "endOffset" : 368
    }, {
      "referenceID" : 6,
      "context" : "This point is further supported by ResNet (He et al., 2015b), in which pure identity mappings are used as bypassing layers. ResNets have achieved impressive (often record breaking) achievements on most challenging image recognition, localization and detection tasks, such as ImageNet and COCO object detection dataset (He et al., 2015b). Recently, Huang et al. (2016) successfully trained a 1202-layer ResNet with stochastic depth (Huang et al., 2016). Stochastic depth is a mechanism to improve the training of deep residual networks by dropping layers randomly. It also shows that not all layers may be needed and highlights that there is a great amount of redundancy in deep (residual) networks. This paper was in part inspired by this observation. He et al. (2016) introduces ResNets with pre-activation, which similar to stochastic depth allow state-of-the-art performance with > 1000 layers.",
      "startOffset" : 43,
      "endOffset" : 769
    }, {
      "referenceID" : 6,
      "context" : "This point is further supported by ResNet (He et al., 2015b), in which pure identity mappings are used as bypassing layers. ResNets have achieved impressive (often record breaking) achievements on most challenging image recognition, localization and detection tasks, such as ImageNet and COCO object detection dataset (He et al., 2015b). Recently, Huang et al. (2016) successfully trained a 1202-layer ResNet with stochastic depth (Huang et al., 2016). Stochastic depth is a mechanism to improve the training of deep residual networks by dropping layers randomly. It also shows that not all layers may be needed and highlights that there is a great amount of redundancy in deep (residual) networks. This paper was in part inspired by this observation. He et al. (2016) introduces ResNets with pre-activation, which similar to stochastic depth allow state-of-the-art performance with > 1000 layers. An orthogonal approach to making networks deeper (e.g., with the help of skip connections) is to increase the network width. The GoogLeNet (Szegedy et al., 2015a;b) uses an “Inception module” which concatenates feature maps produced by filters of different sizes. Targ et al. (2016) propose a variant of ResNets with wide generalized residual blocks.",
      "startOffset" : 43,
      "endOffset" : 1181
    }, {
      "referenceID" : 6,
      "context" : "This point is further supported by ResNet (He et al., 2015b), in which pure identity mappings are used as bypassing layers. ResNets have achieved impressive (often record breaking) achievements on most challenging image recognition, localization and detection tasks, such as ImageNet and COCO object detection dataset (He et al., 2015b). Recently, Huang et al. (2016) successfully trained a 1202-layer ResNet with stochastic depth (Huang et al., 2016). Stochastic depth is a mechanism to improve the training of deep residual networks by dropping layers randomly. It also shows that not all layers may be needed and highlights that there is a great amount of redundancy in deep (residual) networks. This paper was in part inspired by this observation. He et al. (2016) introduces ResNets with pre-activation, which similar to stochastic depth allow state-of-the-art performance with > 1000 layers. An orthogonal approach to making networks deeper (e.g., with the help of skip connections) is to increase the network width. The GoogLeNet (Szegedy et al., 2015a;b) uses an “Inception module” which concatenates feature maps produced by filters of different sizes. Targ et al. (2016) propose a variant of ResNets with wide generalized residual blocks. In fact, simply increasing the number of filers in each layer of ResNets can improve its performance provided the depth is sufficient (Zagoruyko & Komodakis, 2016). The FractalNet proposed by Larsson et al. (2016) also achieves competitive results on several benchmark datasets using a wide network structure.",
      "startOffset" : 43,
      "endOffset" : 1463
    }, {
      "referenceID" : 19,
      "context" : "The Ladder Networks (Rasmus et al., 2015; Pezeshki et al., 2015) introduce lateral connections into autoencoders, leading to impressive accuracy on semi-supervised learning tasks .",
      "startOffset" : 20,
      "endOffset" : 64
    }, {
      "referenceID" : 18,
      "context" : "The Ladder Networks (Rasmus et al., 2015; Pezeshki et al., 2015) introduce lateral connections into autoencoders, leading to impressive accuracy on semi-supervised learning tasks .",
      "startOffset" : 20,
      "endOffset" : 64
    }, {
      "referenceID" : 4,
      "context" : "H`(·) can be a composite function of operations such as Batch Normalization (BN) (Ioffe & Szegedy, 2015), rectified linear units (ReLU) (Glorot et al., 2011), Pooling (LeCun et al.",
      "startOffset" : 136,
      "endOffset" : 157
    }, {
      "referenceID" : 15,
      "context" : ", 2011), Pooling (LeCun et al., 1998), or Convolution (Conv).",
      "startOffset" : 17,
      "endOffset" : 37
    }, {
      "referenceID" : 12,
      "context" : "Traditional convolutional feed-forward networks connect the output of the ` layer as input to the (`+1) layer (Krizhevsky et al., 2012), which gives rise to the following layer transition x` = H`(x`−1).",
      "startOffset" : 110,
      "endOffset" : 135
    }, {
      "referenceID" : 4,
      "context" : "(2016) we define H`(·) as a composite function of three consecutive operations: Batch Normalization (BN) (Ioffe & Szegedy, 2015), followed by a rectified linear unit (ReLU) (Glorot et al., 2011), and then Convolution (Conv).",
      "startOffset" : 173,
      "endOffset" : 194
    }, {
      "referenceID" : 5,
      "context" : "Motivated by He et al. (2016) we define H`(·) as a composite function of three consecutive operations: Batch Normalization (BN) (Ioffe & Szegedy, 2015), followed by a rectified linear unit (ReLU) (Glorot et al.",
      "startOffset" : 13,
      "endOffset" : 30
    }, {
      "referenceID" : 0,
      "context" : "The implementation is in Torch 7 (Collobert et al., 2011).",
      "startOffset" : 33,
      "endOffset" : 57
    }, {
      "referenceID" : 20,
      "context" : "A standard data augmentation scheme is widely used for this dataset (Lin et al., 2013; Romero et al., 2014; Lee et al., 2015; Springenberg et al., 2014; Srivastava et al., 2015; He et al., 2015b; Huang et al., 2016; Larsson et al., 2016): the images are first zero-padded with 4 pixels on each side, then randomly cropped to again produce 32×32 images; half of the images are then horizontally mirrored.",
      "startOffset" : 68,
      "endOffset" : 237
    }, {
      "referenceID" : 23,
      "context" : "A standard data augmentation scheme is widely used for this dataset (Lin et al., 2013; Romero et al., 2014; Lee et al., 2015; Springenberg et al., 2014; Srivastava et al., 2015; He et al., 2015b; Huang et al., 2016; Larsson et al., 2016): the images are first zero-padded with 4 pixels on each side, then randomly cropped to again produce 32×32 images; half of the images are then horizontally mirrored.",
      "startOffset" : 68,
      "endOffset" : 237
    }, {
      "referenceID" : 9,
      "context" : "A standard data augmentation scheme is widely used for this dataset (Lin et al., 2013; Romero et al., 2014; Lee et al., 2015; Springenberg et al., 2014; Srivastava et al., 2015; He et al., 2015b; Huang et al., 2016; Larsson et al., 2016): the images are first zero-padded with 4 pixels on each side, then randomly cropped to again produce 32×32 images; half of the images are then horizontally mirrored.",
      "startOffset" : 68,
      "endOffset" : 237
    }, {
      "referenceID" : 13,
      "context" : "A standard data augmentation scheme is widely used for this dataset (Lin et al., 2013; Romero et al., 2014; Lee et al., 2015; Springenberg et al., 2014; Srivastava et al., 2015; He et al., 2015b; Huang et al., 2016; Larsson et al., 2016): the images are first zero-padded with 4 pixels on each side, then randomly cropped to again produce 32×32 images; half of the images are then horizontally mirrored.",
      "startOffset" : 68,
      "endOffset" : 237
    }, {
      "referenceID" : 17,
      "context" : "The Street View House Numbers (SVHN) dataset (Netzer et al., 2011) contains 32×32 colored digit images coming from Google Street View.",
      "startOffset" : 45,
      "endOffset" : 66
    }, {
      "referenceID" : 22,
      "context" : "Following common practice (Sermanet et al., 2012; Goodfellow et al., 2013; Lin et al., 2013; Lee et al., 2015; Huang et al., 2016) we use all the training data without any data augmentation, and a validation set with 6,000 images is split from the training set.",
      "startOffset" : 26,
      "endOffset" : 130
    }, {
      "referenceID" : 9,
      "context" : "Following common practice (Sermanet et al., 2012; Goodfellow et al., 2013; Lin et al., 2013; Lee et al., 2015; Huang et al., 2016) we use all the training data without any data augmentation, and a validation set with 6,000 images is split from the training set.",
      "startOffset" : 26,
      "endOffset" : 130
    }, {
      "referenceID" : 23,
      "context" : "35 All-CNN (Springenberg et al., 2014) - - 9.",
      "startOffset" : 11,
      "endOffset" : 38
    }, {
      "referenceID" : 13,
      "context" : "39 FractalNet (Larsson et al., 2016) 21 38.",
      "startOffset" : 14,
      "endOffset" : 36
    }, {
      "referenceID" : 9,
      "context" : "01 ResNet with Stochastic Depth (Huang et al., 2016) 110 1.",
      "startOffset" : 32,
      "endOffset" : 52
    }, {
      "referenceID" : 8,
      "context" : "64 ResNet (pre-activation) (He et al., 2016) 164 1.",
      "startOffset" : 27,
      "endOffset" : 44
    }, {
      "referenceID" : 6,
      "context" : "87 ResNet (He et al., 2015b) 110 1.7M - 6.61 - - ResNet (reported by Huang et al. (2016)) 110 1.",
      "startOffset" : 11,
      "endOffset" : 89
    }, {
      "referenceID" : 25,
      "context" : "9 and Nesterov momentum (Sutskever et al., 2013) with 0 dampening.",
      "startOffset" : 24,
      "endOffset" : 48
    }, {
      "referenceID" : 6,
      "context" : "We adopt the weight initialization introduced by He et al. (2015a), and also equip the network with batch normalization (Ioffe & Szegedy, 2015).",
      "startOffset" : 49,
      "endOffset" : 67
    }, {
      "referenceID" : 6,
      "context" : "It also indicates that they do not suffer from overfitting or the optimization difficulties pointed out by He et al. (2015b). In fact, it is possible that a deeper DenseNet with more parameters could achieve even better results.",
      "startOffset" : 107,
      "endOffset" : 125
    }, {
      "referenceID" : 8,
      "context" : "7M parameters (He et al., 2016).",
      "startOffset" : 14,
      "endOffset" : 31
    }, {
      "referenceID" : 12,
      "context" : "In comparison with other popular network architectures, like AlexNet (Krizhevsky et al., 2012) or VGG-net (Russakovsky et al.",
      "startOffset" : 69,
      "endOffset" : 94
    }, {
      "referenceID" : 21,
      "context" : ", 2012) or VGG-net (Russakovsky et al., 2015), ResNets with pre-activation use fewer parameters while typically achieving better results (He et al.",
      "startOffset" : 19,
      "endOffset" : 45
    }, {
      "referenceID" : 8,
      "context" : ", 2015), ResNets with pre-activation use fewer parameters while typically achieving better results (He et al., 2016).",
      "startOffset" : 99,
      "endOffset" : 116
    }, {
      "referenceID" : 2,
      "context" : ", Gardner et al. (2015) or Gatys et al.",
      "startOffset" : 2,
      "endOffset" : 24
    }, {
      "referenceID" : 2,
      "context" : ", Gardner et al. (2015) or Gatys et al. (2015). Finally, we believe that there may be further gains in accuracy obtainable through more detailed fine-tuning of hyper-parameters or learning rate schedules.",
      "startOffset" : 2,
      "endOffset" : 47
    } ],
    "year" : 2016,
    "abstractText" : "Recent work has shown that convolutional networks can be substantially deeper, more accurate and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper we embrace this observation and introduce the Dense Convolutional Network (DenseNet), where each layer is directly connected to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections, one between each layer and its subsequent layer (treating the input as layer 0), our network has L(L+1) 2 direct connections. For each layer, the feature maps of all preceding layers are treated as separate inputs whereas its own feature maps are passed on as inputs to all subsequent layers. Our proposed connectivity pattern has several compelling advantages: it alleviates the vanishing gradient problem and strengthens feature propagation; despite the increase in connections, it encourages feature reuse and leads to a substantial reduction of parameters; its models tend to generalize surprisingly well. We evaluate our proposed architecture on five highly competitive object recognition benchmark tasks. The DenseNet obtains significant improvements over the state-of-the-art on all five of them (e.g., yielding 3.74% test error on CIFAR-10, 19.25% on CIFAR-100 and 1.59% on SVHN).",
    "creator" : "LaTeX with hyperref package"
  }
}