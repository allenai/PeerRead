{
  "name" : "1611.10052.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Performance Tuning of Hadoop MapReduce: A Noisy Gradient Approach",
    "authors" : [ "Sandeep Kumar", "Sindhu Padakandla" ],
    "emails" : [ "iisc.csa.priyank.parihar}@gmail.com,", "shalabh}@csa.iisc.ernet.in" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Keywords Hadoop performance tuning, Simultaneous Perturbation Stochastic Approximation"
    }, {
      "heading" : "1. INTRODUCTION",
      "text" : "ACM ISBN 978-1-4503-2138-9.\nDOI: 10.1145/1235\nWe are in the era of big data and huge volumes of data are generated in various domains like social media, financial markets, transportation and health care. Faster analysis of such big unstructured data is a key requirement for achieving success in these domains. Popular instances of such cases include distributed pattern-based searching, distributed sorting, web link-graph reversal, singular value decomposition, web access log stats, inverted index construction and document clustering. Extracting hidden patterns, unknown correlations and other useful information is critical for making better decisions. Many industrial organisations like Yahoo!, Facebook, Amazon etc. need to handle and process large volumes of data and their product success hinges on this ability. Thus, there is a need for parallel and distributed processing/programming methodologies that can handle big data using resources built out of commodity hardware. Currently available parallel processing systems are database systems [24] like Teradata, Aster Data, Vertica etc., which are quite robust and are high-performance computing platforms. However, there is a need for a parallel processing system which can handle large volumes of data using low-end servers and which is easy-to-use. MapReduce[10] is one-such programming model.\nMapReduce computation over input data goes through two phases namely map and reduce. At the start of the map phase, the job submitted by the client is split into multiple map-reduce tasks that are to be executed by various worker nodes. The map phase then creates the key-value pairs from the input dataset according to the user defined map. The reduce phase makes use of the key-value pairs and aggregates according to user specified function to produce the output.\nApache Hadoop[31] is an open-source implementation of MapReduce written in Java for distributed storage and processing of very large data sets on clusters built using commodity hardware. The Hadoop framework gives various parameter (knobs) that need to be tuned according to the program, input data and hardware resources. It is important to tune these parameters to obtain best performance for a given MapReduce job. The problem of Hadoop performance being limited by the parameter configuration was recognized in [17]. Unlike SQL, MapReduce jobs cannot be modeled using a small and finite space of relational operators [24]. Thus, it is not straight forward to quantify the effect of these various parameters on the performance and hence it is difficult to compute the best parameter configuration\nar X\niv :1\n61 1.\n10 05\n2v 2\n[ cs\n.D C\n] 1\n6 D\nec 2\n01 6\napriori. In addition, difficulty in tuning these parameters also arises due to two other important reasons. Firstly, due to the presence of a large number of parameters (about 200, encompassing a variety of functionalities) the search space is large and complex. Secondly, there is a pronounced effect of cross-parameter interactions, i.e., the knobs are not independent of each other. For instance, increasing the parameter corresponding to map-buffer size will decrease the I/O cost, however, the overall job performance may degrade because sorting cost may increase (in quick sort, sorting cost is proportional to the size of data). The complex search space along with the cross-parameter interaction does not make Hadoop amenable to manual tuning.\nThe necessity for tuning of Hadoop parameters was first emphasized in [17], which proposed a method to determine the optimum configuration given a set of computing resources. Recent efforts in the direction of automatic tuning of the Hadoop parameters include Starfish[15], AROMA[21], MROnline[22], PPABS [32] and JellyFish [11]. We observe that collecting statistical data to create virtual profiles and estimating execution time using mathematical model (as in [17, 15, 11, 21, 22, 32]) requires significant level of expertise which might not be available always. In addition, since Hadoop MapReduce is evolving continuously with a number of interacting parts, the mathematical model also has to be updated and in the worst case well-defined mathematical model might not be available for some of its parts due to which a model-based approach might fail. Further, given the presence of cross-parameter interaction it is a good idea to retain as many parameters as possible (as opposed to reducing the parameters [32]) in the tuning phase.\nIn this paper, we present a novel tuning methodology based on a noisy gradient method known as the simultaneous perturbation stochastic approximation (SPSA) algorithm [27]. The SPSA algorithm is a black-box stochastic optimization technique which has been applied to tune parameters in a variety of complex systems. An important feature of the SPSA is that it utilizes observations from the real system as feedback to tune the parameters. Also, the SPSA algorithm is dimensionality free, i.e., it needs only 2 or fewer observations per iteration irrespective of the number of parameters involved. In this paper, we adapt the SPSA algorithm to tune the parameters used by Hadoop to allocate resources for program execution."
    }, {
      "heading" : "1.1 Our Contribution",
      "text" : "Our aim is to introduce the practitioners to a new method that is different in flavour from the prior methods, simple to implement and effective at the same time. The highlights of our SPSA based approach are as follows:\n• Mathematical model: The methodology we propose utilizes the observations from the Hadoop system and does not need a mathematical model. This is desirable since mathematical models developed for older versions might not carry over to newer versions of Hadoop.\n• Dimension free nature: SPSA is designed to handle complex search spaces. Thus, unlike [7] reducing the search space is not a requirement.\n• Parametric dependencies: Unlike a host of blackbox optimization methods that depend on clever heuristics, our SPSA based method computes the gradient\nand hence takes into account the cross parameter interactions in the underlying problem.\n• Performance: Using the SPSA algorithm we tune 11 parameters simultaneously. Our method provides a 66% decrease in execution time of Hadoop jobs on an average, when compared to the default configuration. Further, we also observe a reduction of 45% in execution times, when compared to prior [15] methods."
    }, {
      "heading" : "1.2 Organisation of the Paper",
      "text" : "In the next section, we describe the Hadoop architecture, its data flow analysis and point out the importance and role of some of the configuration parameters. Following it, in Section 3 we discuss the related work and contrast it with our approach. We provide a detailed description of our SPSAbased approach in Section 4. In Section 5 we discuss the specific details in implementing the SPSA algorithm to tune the Hadoop parameters. We describe the experimental setup and present the results in Section 6. Section 7 concludes the paper and suggests future enhancements."
    }, {
      "heading" : "2. HADOOP",
      "text" : "Hadoop is an open source implementation of the MapReduce[10], which has gained a huge amount of popularity in recent years as it can be used over commodity hardware. Hadoop has two main components namely MapReduce and Hadoop Distributed File System(HDFS). The HDFS is used for storing data and MapReduce is used for performing computations over the data. We first discuss the HDFS and then MapReduce. Following this, we describe the data flow analysis in Hadoop with an aim to illustrate the importance of the various parameters."
    }, {
      "heading" : "2.1 Hadoop Distributed File System",
      "text" : "Hadoop uses HDFS to store input and output data for the MapReduce applications. HDFS provides interfaces for applications to move themselves closer [2] to where the data is located because data movement will be costly as compared to movement of small MapReduce code. It is fault tolerant and is optimized for storing large data sets.\nA HDFS cluster (see [31]) consists of a single NameNode, a master server, and multiple slave DataNodes. The DataNodes, usually one per node, store the actual data used for computation. These manage the storage attached to the nodes that they run on. Internally, a file is split into one or more data blocks (block size is controlled by dfs.block.size) and these blocks are stored in a set of DataNodes. They are responsible for serving read and write requests from the file system’s clients. NameNode manages the file system namespace and regulates access to files by clients. It has the following functions:\n• Store HDFS metadata and execute file systems operations on HDFS\n• Mapping data blocks to DataNodes\n• Periodically monitor the performance of DataNodes"
    }, {
      "heading" : "2.2 MapReduce",
      "text" : "A client application submits a MapReduce job. It is then split into various map and reduce tasks that are to be executed in the various cluster nodes. In MapReduce version 1\n(v1), the JobTracker, usually running on a dedicated node, is responsible for execution and monitoring of jobs in the cluster. It schedules map and reduce tasks to be run on the nodes in the cluster, which are monitored by a corresponding TaskTracker running on that particular node. Each TaskTracker sends the progress of the corresponding map or reduce task to JobTracker at regular intervals. Hadoop MapReduce version 2 (v2, also known as Yet Another Resource Negotiator (YARN)[30]) has a different architecture. It has a ResourceManager and NodeManager instead of JobTracker and TaskTracker. The tasks of resource and job management are distributed among resource manager and application master (a process spawned for every job) respectively. The job submitted by a client application (for e.g., Terasort, WordCount benchmark applications) is associated with a NodeManager, which starts an “Application Master” in a container (a container is a Unix process, which runs on a node). The container architecture utilizes cluster resources better, since YARN manages a pool of resources that can be allocated based on need. This is unlike MapReduce v1 where each TaskTracker is configured with an inflexible map/reduce slot. A map slot can only be used to run a map task and same with reduce slots."
    }, {
      "heading" : "2.3 MapReduce Data Flow Analysis",
      "text" : "Map and Reduce are the two main phases of job processing (see Fig. 1). The function of these phases is illustrated with the following simple example:\nExample 1. The objective is to count the number of times each word appears in a file whose content is given by,\n“This is an apple. That is an apple”.\nThe output of the Map operation is then given by,\n< This, 1 >< is, 1 >< an, 1 >< apple, 1 > < That, 1 >< is, 1 >< an, 1 >< apple, 1 >,\nfollowing which the Reduce operation outputs\n< This, 1 >< That, 1 >< is, 2 >< an, 2 >< apple, 2 >.\nThus we obtain the count for each of the words.\nMap and Reduce phases can perform complex computations in contrast to the above example. The efficiency of these computations and phases is controlled by various system parameters. We describe the parameters our algorithm tunes (see Section 6) and show how they influence the map and reduce phases of computation.\n2.3.1 Map Phase Input data is split according to the input format (text, zip\netc.) and split size is controlled by the parameter dfs.block.size. For each split, a corresponding mapper task is created.\nThe Map function retrieves data (records) from the input split with the help of the record reader. The record reader provides the key-value pair (record) to the mapper according to the input format. The Map outputs < key, value > and the meta-data (corresponding partition number of the key) according to the logic written in the map function. This output is written to a circular buffer, whose size is controlled by parameter mapreduce.task.io.sort.mb. When the data in the buffer reaches a threshold, defined by mapreduce.map.sort.spill.percent, data is spilled to the local disk\nof a mapper node. The Map outputs will continue to be written to this buffer while the spill takes place. If any time the buffer becomes full, the Map task is blocked till spill finishes.\nSorting (default Quick Sort) and combine operations are performed in the memory buffer prior to the data spill onto the disk. So increasing the buffer size of mapper decreases I/O cost but sorting cost will increase. Combine executes on a subset of < key, value > pairs. Combine is used for reducing data written to the disk.\nThe merge phase starts once the Map and the Spill phases complete. In this phase, all the spilled files from a mapper are merged together to form a single output file. Number of streams to be merged is controlled by the parameter mapreduce.task.io.sort.factor (a higher value means more number of open file handles and a lower value implies multiple merging rounds in the merge phase). After merging, there could be multiple records with same key in the merged file, so combiner could be used again.\n2.3.2 Reduce Phase Reducers are executed in parallel to the mappers if the\nfraction of map task completed is more than the value of mapreduce.job.reduce.slowstart.completedmaps parameter, ot herwise reducers execute after mappers. The number of reducers for a work is controlled by mapreduce.job.reducers. A Reducer fetches its input partition from various mappers via HTTP or HTTPS. The total amount of memory allocated to a reducer is set by mapreduce.reduce.memory.totalbytes and the fraction of memory allocated for storing data fetched from mappers is set by mapreduce.reduce.shuffle.input.buffer. percent. In order to create a single sorted data based on the key, the merge phase is executed in order to collate the keys obtained from different partitions. The number of map outputs needed to start this merge process is determined by mapreduce.reduce.merge.inmem.threshold. The threshold for spilling the merged map outputs to disk is controlled by mapreduce.reduce.shuffle.merge.percent. Subsequent to all merge operations, reduce code is executed and its output is saved to the HDFS (as shown in Figure 1).\n2.3.3 Cross-Parameter Interaction The parameters io.sort.mb,reduce.input.buffer.percent and\nshuffle.input.buffer.percent control the number of spills written to disk. Increasing the memory allocated will reduce the number of spill records in both Map and Reduce phases. When io.sort.mb is high, the spill percentage of Map (controlled by sort.spill.percent) should be set to a high value. In the Reduce phase, the map outputs are merged and spilled to disk when either the merge.inmem.threshold or shuffle.merg e.percent is reached. Similarly, the task.io.sort.factor determines the minimum number of streams to be merged at once, during sorting. So, on the reducer side, if there are say 40 mapper outputs and this value is set to 10, then there will be 5 rounds of merging (on an average 10 files for merge round).\nThe above examples indicate that changing one function in the reduce/map phase, affects other characteristics of the system. This implies that Hadoop system parameters cannot be tuned in isolation. Each parameter has to be tuned by taking note of the values of related parameters. Our SPSAbased method takes into account such relations between the parameters and appropriately tunes them to achieve en-\nhanced performance. In the next section, we discuss the existing work in the literature which suggest techniques to enhance the performance of Hadoop."
    }, {
      "heading" : "3. RELATED WORK",
      "text" : "Some early works [12, 18] have focussed on analysing the MapReduce performance and not addressed the problem of parameter tuning. The authors in [12] develop models for predicting performance of Hive queries and ETL (Extract Transform Load) kind of MapReduce jobs. This work uses KCCA (Kernel Canonical Correlation Analysis) and nearest neighbor for modeling and prediction respectively. KCCA provides dimensionality reduction and preserves the neighborhood relationship even after projecting onto a lower dimensional space. It uses a number of training sets to build a single model for multiple performance metrics. Hadoop job log for a period of six months is used for training. Input data characteristic (like byte read locally, byte read from HDFS and byte input to a map stage), configuration parameters, job count (number and configuration of map and reduce to be executed by a given Hadoop job), query operator count etc. are used as features for comparison and prediction about new job.\nMapReduce logs of a M45 supercomputing cluster (released by Yahoo!) are analysed in [18]. This analysis characterizes job patterns, completion times, job failures and resource utilization patterns based on the logs. Jobs are characterized into map-only, reduce-only, reduce-mostly etc. Based on this categorization, [18] suggests improvements in Hadoop MapReduce which can mitigate performance bottlenecks and reduce job failures.\nAttempts toward building an optimizer for hadoop performance started with Starfish[15]. In Starfish [15, 14], a Profiler collects detailed statistical information (like data flow and cost statistics) from unmodified Mapreduce program during full or partial execution. Then, a What-if engine estimates the cost of a new job without executing it on real system using mathematical models, simulation, data flow and cost statitics. The Cost-based optimizer (CBO) uses the what-if engine and recursive random search (RSS) for tuning the parameters for a new Mapreduce job.\nWorks following Starfish are [21, 32]. These methods collect information about the jobs executed on hadoop, a process known as profiling. Job “signatures”, i.e., the resource utilization patterns of the jobs are used for profiling. In\nthe offline phase, using a training set, the jobs are clustered (using variants of k-means) according to their respective signatures. In the online phase [21] trains a SVM which makes accurate and fast prediction of a job’s performance for various configuration parameters and input data sizes. For any new job, its signature is matched with the profiles of one of the clusters, after which that cluster’s optimal parameter configuration is used. In [32], the optimal parameter configuration for every cluster is obtained through simulated annealing, albeit for a reduced parameter search space.\nAn online MapReduce performance tuner (MROnline) is developed in [22]. It is desgined and implemented on YARN [30] (described in Section 2). MROnline consists of a centralized master component which is the online tuner. It is a daemon process that runs on the same machine as the resource manager of YARN or on a dedicated machine. Online tuner controls slave components that run within the node managers on the slave nodes of the YARN cluster. It consists of three components: a monitor, a tuner and a dynamic configurator. The monitor works together with the per-node slave monitors to periodically monitor application statistics. These statistics are sent to the centralized monitor. The centralized monitor then aggregates, analyzes and passes the information to the tuner. The tuner implements hill climbing algorithm to tune parameter values. The tuned parameter values are distributed to the slave configurators by the dynamic configurator. The slave configurators activate the new parameter values for the tasks that are running on their associated nodes.\nIndustry and MapReduce vendors also provide guides [3, 1] on parameter tuning which help in finding suitable values for the client applications. However, these guides are heuristic and the end-users are still faced with the challenge of manually trying out multiple parameter configurations."
    }, {
      "heading" : "3.1 Motivation for Our Approach",
      "text" : "The contrast between prior approaches to parameter tuning in Hadoop and our approach is shown in Figure 2. In [14], the optimization is based on the what-if engine which uses a mix of simulation and model-based estimation. Here, the cost model F is high-dimensional, nonlinear, non-convex and multimodal. In [32], authors make use of available knowledge from literature in order to reduce the parameter space and they make use of simulated annealing to find the right parameter setting in the reduced space. We observe\nthat\n1. Collecting statistics and building an accurate model requires certain level of expertise. Also, mathematical models developed for an older version may fail for the newer versions since Hadoop is evolving continuously. In the worst case, mathematical models might not be well defined for some components of Hadoop.\n2. The effect of cross-parameter interactions are significant and hence it might be a good idea to have the search space as big as possible.\nWith the above two points in mind, we suggest a more direct approach (see Figure 2), i.e., we suggest a method that directly utilizes the data from the real system and tunes the parameters via feedback. Thus, we are motivated to adapt SPSA algorithm to tune the parameters. We believe that the SPSA based scheme is of interest to practitioners because it does not require any model building and it uses only the gradient estimate at each step. Through the gradient estimate, it takes the cross parameter interaction into account. Further, the SPSA algorithm is not limited by the parameter dimension and requires only 2 measurements per iteration."
    }, {
      "heading" : "4. AUTOMATIC PARAMETER TUNING",
      "text" : "The performance of various complex systems such as traffic control [25], unmanned aerial vehicle (UAV) control [6], remote sensing [9], communication in satellites [13] and airlines [16] depends on a set of tunable parameters (denoted by θ). Parameter tuning in such cases is difficult because of bottlenecks namely the black-box nature of the problem and the curse-of-dimensionality i.e., the complexity of the search space. In this section, we discuss the general theme behind the methods that tackle these bottlenecks and their relevance to the problem of tuning the Hadoop parameters."
    }, {
      "heading" : "4.1 Bottlenecks in Parameter Tuning",
      "text" : "In many complex systems, the exact nature of the dependence of the performance on the parameters is not known explicitly i.e., the performance cannot be expressed as an analytical function of the parameters. As a result, the parameter setting that offers the best performance cannot be computed apriori. However, the performance of the system can be observed for any given parameter setting either from the system or a simulator of the system. In such a scenario, one can resort to black-box/simulation-based optimization methods that tune the parameters based on the output observed from the system/simulator without knowing its internal functioning. Figure 3 is a schematic to illustrate the black-box optimization procedure. Here, the black-box optimization scheme sets the current value of the parameter\nbased on the past observations. The way in which past observation is used to compute the current parameter setting varies across methods.\nAn important issue in the context of black-box optimization is the number of observations and the cost of obtaining an observation from the system/simulator. The term curse-of-dimensionality denotes the exponential increase in the size of the search space as the number of dimensions increases. In addition, in many applications, the parameter θ belongs to a subset X of Rn (for some positive integer n > 0). Since it is computationally expensive to search such a large and complex parameter space, it is important for black-box optimization methods to make as fewer observations as possible.\nHadoop MapReduce exhibits the above described black box kind of behavior because it is not well structured like SQL. In addition, cross-parameter interactions also affect the performance, and hence it is not possible to treat the parameters independent of each other. Besides, the problem is also afflicted by the curse-of-dimesionality."
    }, {
      "heading" : "4.2 Noisy Gradient based optimization",
      "text" : "In order to take the cross-parameter interactions into account, one has to make use of the sensitivity of the performance measure with respect to each of the parameters at a given parameter setting. This sensitivity is formally known as the gradient of the performance measure at a given setting. It is important to note that it takes only O(n) observations to compute the gradient of a function at a given point. However, even O(n) computations are not desirable if each observation is itself costly.\nConsider the noisy gradient scheme given in (1) below. θn+1 = θn − αn ( ∇fn +Mn ) , (1)\nwhere n = 1, 2 . . . denotes the iteration number, ∇fn ∈ Rn is the gradient of function f , Mn ∈ Rn is a zero-mean noise sequence and αn is the step-size. Fig. 4 presents an intuitive picture of how a noisy gradient algorithm works. Here, the algorithm starts at θ0 and needs to move to θ\n∗ which is the desired solution. The green lines denote the true gradient step (i.e., αn∇fn) and the dotted circles show the region of uncertainty due to the noise term αnMn. The red line denotes the fact that the true gradient is disturbed and the iterates are pushed to a different point within the region of uncertainty. The idea here is to use diminishing step-sizes\nto filter the noise and eventually move towards θ∗. The simultaneous perturbation stochastic approximation (SPSA) algorithm is a noisy gradient algorithm which works as illustrated in Figure 4. It requires only 2 observations per iteration. We adapt it to tune the parameters of Hadoop. By adaptively tuning the Hadoop parameters, we intend to optimize the Hadoop job execution time, which is the performance metric (i.e., f(θ)) in our experiments. Note that we can also have other performance metrics - like number of records spilled to disk, Memory and heap usage or number of failed jobs. Next, we provide a detailed description of SPSA."
    }, {
      "heading" : "4.3 Simultaneous Perturbation Stochastic Approximation (SPSA)",
      "text" : "We use the following notation:\n1. θ ∈ X ⊂ Rn denotes the tunable parameter. Here n is the dimension of the parameter space. Also, X is assumed to be a compact and convex subset of Rn.\n2. Let x ∈ Rn be any vector then x(i) denotes its ith co-ordinate, i.e., x = (x(1), . . . , x(n)).\n3. f(θ) denotes the performance of the system for parameter θ. Let f be a smooth and differentiable function of θ.\n4. ∇f(θ) = ( ∂f ∂θ(1) , . . . , ∂f ∂θ(n) ) is the gradient of the func-\ntion, and ∂f ∂θ(i) is the partial derivative of f with respect to θ(i).\n5. ei ∈ Rn is the standard n-dimensional unit vector with 1 in the ith co-ordinate and 0 elsewhere.\nFormally the gradient is given by\n∂f\n∂θ(i) = lim h→0 f(θ + hei)− f(θ) h . (2)\nIn (2), the ith partial derivative is obtained by perturbing the ith co-ordinate of the parameter alone and keeping rest of the co-ordinates the same. Thus, the number of operations required to compute the gradient once via perturbations is of n+1. This can be a shortcoming in cases when it is costly (i.e., computationally expensive) to obtain measurements of f and the number of parameters is large.\nThe SPSA algorithm [27] computes the gradient of a function with only 2 or fewer perturbations. Thus the SPSA algorithm is extremely useful in cases when the dimensionality is high and the observations are costly. The idea behind the SPSA algorithm is to perturb not just one co-ordinate at a time but all the co-ordinates together simultaneously in a random fashion. However, one has to carefully choose these random perturbations so as to be able to compute the\ngradient. Formally, a random perturbation ∆ ∈ Rn should satisfy the following assumption.\nAssumption 1. For any i 6= j, i = 1, . . . , n, j = 1, . . . , n, the random variables ∆(i) and ∆(j) are zero-mean,independent, and the random variable Zij given by Zij = ∆(i) ∆(j) is such that E[Zij ] = 0 and it has finite second moment.\nWe now provide an example of random perturbations that satisfies the Assumption 1.\nExample 2. ∆ ∈ Rn is such that, each of its co-ordinates ∆(i)s are independent Bernoulli random variables taking values −1 or +1 with equal probability, i.e., Pr{∆(i) = 1} = Pr{∆(i) = −1} = 1\n2 for all i = 1, . . . , n."
    }, {
      "heading" : "4.4 Noisy Gradient Recovery from Random Perturbations",
      "text" : "Let ∇̂fθ denote the gradient estimate, and let ∆ ∈ Rn be any perturbation vector satisfying Assumption 1. Then for any small positive constant δ > 0, the one-sided SPSA algorithm [23, 29] obtains an estimate of the gradient according to equation (3) given below.\n∇̂fθ(i) = f(θ + δ∆)− f(θ)\nδ∆(i) . (3)\nWe now look at the expected value of ∇̂fθ(i), i.e., E[∇̂fθ(i)|θ] = E [ f(θ) + δ∆>∇f(θ) + o(δ2)− f(θ) δ∆(i) |θ ]\n= ∂f\n∂θ(i) + E  n∑ j=1,j 6=i ∂f ∂θ(j) ∆(j) ∆(i) |θ + o(δ) = ∂f\n∂θ(i) + o(δ). (4)\nThe third equation follows from the second by noting that E [\n∂f ∂θ(j) ∆(j) ∆(i) |θ ] = 0, a fact that follows from the property of ∆ in Assumption 1. Thus E[∇̂fθ(i)]→ ∇fθ(i) as δ → 0. Notice that in order to compute the gradient ∇fθ at the point θ the SPSA algorithm requires only 2 measurements namely f(θ) and f(θ + δ∆). An extremely useful consequence is that the gradient estimate is not affected by the number of dimensions.\nAlgorithm 1 Simultaneous Perturbation Stochastic Approximation\n1: Let initial parameter setting be θ0 ∈ X ⊂ Rn 2: for n = 1, 2 . . . , N do 3: Observe the performance of system f(θn). 4: Generate a random perturbation vector ∆n ∈ Rn. 5: Observe the performance of system f(θn + δ∆n). 6: Compute the gradient estimate ∇̂fn(i) = f(θn+δ∆n)−f(θn)\nδ∆n(i) .\n7: Update the parameter in the negative gradient direction θn+1(i) = Γ ( θn(i)− αn f(θn+δ∆n)−f(θn)δ∆n(i) ) . 8: end for 9: return θN+1\nThe complete SPSA algorithm is shown in Algorithm 1, where {αn} is the step-size schedule and Γ is a projection\noperator that keeps the iterates within X. We now briefly discuss the conditions and nature of the convergence of the SPSA algorithm."
    }, {
      "heading" : "4.5 Convergence Analysis",
      "text" : "The SPSA algorithm (Algorithm 1) makes use of a noisy gradient estimate (in line 6) and at each iteration takes a step in the negative gradient direction so as to minimize the cost function. The noisy gradient update can be re-written as\nθn+1 = Γ ( θn − αn ( E[∇̂fn|θn] + ∇̂fn −E[∇̂fn|θn]) ) (5)\n= Γ ( θn − αn ( ∇fn +Mn+1 + n) ) where Mn+1 = ∇̂fn − E[∇̂fn|θn] is an associated martingale difference sequence under the sequence of σ-fields Fn = σ(θm,m ≤ n,∆m,m < n), n ≥ 1 and n is a small bias due to the o(δ) term in (4).\nThe iterative update in (5) is known as a stochastic approximation [8] recursion. As per the theory of stochastic approximation, in order to filter out the noise, the step-size schedule {αn} needs to satisfy the conditions below.\n∞∑ n=0 αn =∞, ∞∑ n=0 α2n <∞. (6)\nWe now state the convergence result.\nTheorem 1. As n → ∞ and δ → 0, the iterates in (5) (i.e., line 7 of Algorithm 1) converge to a set A = {θ|Γ(∇f(θ)) = 0}, where for any continuous J : Rn → Rn, Γ̂(J(x)) = lim\nη↓0\n( Γ(x+ηJ(x))−Γ(x)\nη\n) .\nProof. The proof is similar to Theorem 3.3.1, pp. 191- 196 of [19].\nTheorem 1 guarantees the convergence of the iterates to local minima. However, in practice local minima corresponding to small valleys are avoided due either to the noise inherent to the update or one can periodically inject some noise so as to let the algorithm explore further. Also, though the result stated in Theorem 1 is only asymptotic in nature, in most practical cases convergence is observed in a finite number of steps. In the following section, we adapt SPSA to the problem of parameter tuning for enhancing the performance of Hadoop."
    }, {
      "heading" : "5. APPLYING SPSA TO HADOOP PARAMETER TUNING",
      "text" : "The SPSA algorithm was presented in its general form in Algorithm 1. We now discuss the specific details involved\nin suitably applying the SPSA algorithm to the problem of parameter tuning in Hadoop."
    }, {
      "heading" : "5.1 Mapping the Parameters",
      "text" : "The SPSA algorithm needs each of the parameter components to be real-valued i.e., in Algorithm 1, θ ∈ X ⊂ Rn. However, most of the Hadoop parameters that are of interest are not Rn-valued. Thus, on the one hand we need a set of Rn-valued parameters that the SPSA algorithm can tune and a mapping that takes these Rn-valued parameters to the Hadoop parameters. In order to make things clear we introduce the following notation:\n1. The Hadoop parameters are denoted by θH and the Rn-valued parameters tuned by SPSA are denoted by θA 1.\n2. Si denotes the set of values that the i th Hadoop pa-\nrameter can assume. θminH (i), θ max H (i) and θ d H(i) denote the minimum, maximum and default values that the ith Hadoop parameter can assume.\n3. θA ∈ X ⊂ Rn and θH ∈ S1 × . . .× Sn.\n4. θH = µ(θA), where µ is the function that maps θA ∈ X ⊂ Rn to θH ∈ S1 × . . .× Sn.\nIn this paper, we choose X = [0, 1]n, and µ is defined as µ(θA)(i) = b(θmaxH (i)−θminH (i))θA(i)+θminH (i)c and µ(θA)(i) = (θmaxH (i)− θminH (i))θA(i) + θminH (i) for integer and real-valued Hadoop parameters respectively."
    }, {
      "heading" : "5.2 Perturbation Sequences and Step-Sizes",
      "text" : "We chose δ∆n ∈ Rn be independent random variables, such that Pr{δ∆n(i) = − 1θmax H (i)−θmin H (i) } = Pr{δ∆n(i) = + 1 θmax H (i)−θmin H (i) } = 1 2 . This perturbation sequence ensures that the Hadoop parameters assuming only integer values change by a magnitude of at least 1 in every perturbation. Otherwise, using a perturbation whose magnitude is less than 1\nθmax H (i)−θmin H\n(i) might not cause any change to the\ncorresponding Hadoop parameter resulting in an incorrect gradient estimate.\nThe conditions for the step-sizes in (6) are asymptotic in nature and are required to hold in order to be able to arrive at the result in Theorem 1. However, in practice, a constant step size can be used since one reaches closer to the desired value in a finite number of iterations. We know apriori that the parameters tuned by the SPSA algorithm belong to the interval [0, 1] and it is enough to have step-sizes of the order of mini(\n1 θmax H (i)−θmin H (i) ) (since any finer step-size\nused to update the SPSA parameter θA(i) will not cause a change in the corresponding Hadoop parameter θH(i)). In our experiments, we chose αn = 0.01,∀n ≥ 0 and observed convergence in about 20 iterations."
    }, {
      "heading" : "6. EXPERIMENTAL EVALUATION",
      "text" : "We use Hadoop versions 1.0.3 and 2.7 in our experiments. The SPSA algorithm described in Sections 4,5 is implemented as a process which executes in the Resource Manager (and/or NameNode). First we justify the selection of parameters to be tuned in our experiments, following which we give details about the implementation. 1Here subscripts A and H are abbreviations of the keywords Algorithm and Hadoop respectively"
    }, {
      "heading" : "6.1 Parameter Selection",
      "text" : "As discussed in Section 2, based on the data flow analysis of Hadoop MapReduce and the Hadoop manual [4], we identify 11 parameters which are found to critically affect the operation of HDFS and the Map/Reduce operations. The list of important parameters that emerged by analyzing the MapReduce implementation are listed in Table 1. Numerous parameters of Hadoop deal with book keeping related tasks, whereas some other parameters are connected with the performance of underlying operating system tasks. For e.g., mapred.child.java.opts is a parameter related to the Java virtual machine (JVM) of Hadoop. We avoid tuning such parameters, which are best left for low-level OS optimization. Instead we tune parameters which are directly Hadoop dependent, for e.g., number of reducers, I/O utilization parameter etc. However, even with 11 parameters, the search space is still large and complex. To see this, if each parameter can assume say 10 different values then the search space contains 1011 possible parameter settings. Some parameters have been left out because either they are backward incompatible or they incur additional overhead in implementation."
    }, {
      "heading" : "6.2 Cluster Setup",
      "text" : "Our Hadoop cluster consists of 25 nodes. Each node has a 8 core Intel Xeon E3, 2.50 GHz processor, 3.5 TB HDD, 16 GB memory, 1 MB L2 Cache, 8MB L3 Cache. One node works as the NameNode and the rest of the nodes are used as DataNodes. For optimization and evaluation purpose we set the number of map slots to 3 and reduce slots to 2 per node. Hence, in a single wave of Map jobs processing, the cluster can process 24 × 3 = 72 map tasks and 24 × 2 = 48 reduce tasks (for more details see [31]). HDFS block replication was set to 2. We use a dedicated Hadoop cluster in our experiments, which is not shared with any other application."
    }, {
      "heading" : "6.3 Benchmark Applications",
      "text" : "In order to evaluate the performance of tuning algorithm, we adapt representative MapReduce applications. The applications we use are listed in Table 1. Terasort application takes as input a text data file and sorts it. It has three components - TeraGen - which generates the input data for sorting algorithm, TeraSort - algorithm that implements sorting and TeraValidate - validates the sorted output data. The Grep application searches for a particular pattern in a given input file. The Word Cooccurrence application counts the number of occurrences of a particular word in an input file (can be any text format). Bigram application counts all unique sets of two consecutive words in a set of documents, while the Inverted index application generates word to document indexing from a list of documents. Word Cooccurrence is a popular Natural Language Processing program which computes the word co-occurrence matrix of a large text collection. As can be inferred, Grep and Bigram applications are CPU intensive, while the Inverted Index and TeraSort applications are both CPU and memory intensive. These benchmark applications can be further categorized as map-intensive, reduce-intensive etc."
    }, {
      "heading" : "6.4 SPSA Iterations",
      "text" : "SPSA is an iterative algorithm and it runs a Hadoop job with different configurations. We refer to these iterations as the optimization or the learning phase. The algorithm even-\ntually converges to an optimal value of the configuration parameters. The performance metric (the job execution time) corresponding to the converged parameter vector is optimal for the corresponding application. During our evaluations we have seen that SPSA converges within 20 - 30 iterations and within each iteration it makes two observations, i.e. it executes Hadoop job twice taking the total count of Hadoop runs during the optimization phase to 40 - 60. It is of utmost importance that the optimization phase is fast otherwise it can overshadow the benefits which it provides.\nIn order to ensure that the optimization phase is fast, we execute the Hadoop jobs on a subset of the workload. This is done, since SPSA takes considerable time when executed on large workloads. Deciding the size of this “partial workload” is very important as the run time on a small work load will be eclipsed by the job setup and cleanup time. We then consider the technique involved in processing done by Hadoop system to find a suitable workload size. Hadoop splits the input data based on the block size of HDFS. It then spawns a map for each of the splits and processes each of the maps in parallel. The number of the map tasks that can run at a given time is upper bounded by the total map slots available in the cluster. Using this fact, the size of the partial data set which we use is equal to twice the number of map slots in the cluster multiplied by the data block size. Hadoop finishes the map task in two waves of the maps jobs which allows the SPSA to capture the statistics with a single wave and the cross relations between two successive waves.\nOur claim is that the value of configuration parameters which optimize these two waves of Hadoop job also optimize all the subsequent waves as those are repetitions of similar map jobs. However, the number of reducers to run is completely optimized by SPSA, albeit for a partial workload size. For the larger (actual) workload, the number of reducers decided is based on the ratio of partial work load size to the actual size of workload. An advantage with SPSA iterations is that these can be halted at any parameter configuration (for e.g., need for executing a production job on the cluster) and later resumed at the same parameter configuration where the iterations were halted."
    }, {
      "heading" : "6.5 Optimization Settings",
      "text" : "For evaluating performance of SPSA on different benchmarks, two waves of map tasks during job execution were ensured. Further, we selected workloads such that the execution time with default configuration is at least 10 minutes. This was done to avoid the scenario where the job setup and cleanup time overshadows the actual running time and there is practically nothing for SPSA to optimize.\nIn the cases of Bigram and Inverted Index benchmark executions, we observed that even with small amount of data, the job completion time is high (since they are reduceintensive operations). So, due to this reason, we used small sized input data files. Using small sized input data files resulted in the absence of two waves of map tasks. However, since in these applications, reduce operations take precedence, the absence of two waves of map tasks did not create much of a hurdle.\nWe optimize Terasort using a partial data set of size 30GB, Grep on 22GB, Word Co-occurrence on 85GB, Inverted Index on 1GB and Bigram count on 200MB of data set. In optimization (or learning) phases, for each benchmark, we use the default configuration as the initial point for the opti-\nmization. Table 1 indicates the default values and the values provided by SPSA for the parameters we tune. For greater sizes of data, we used Wikipedia dataset[5](≈ 100GB) for Word co-occurrence, Grep and Bigram benchmarks\nThe SPSA algorithm is terminated when either the change in gradient estimate is negligible or the maximum number of iterations have been reached. An important point to note is that Hadoop parameters can take values only in a fixed range. We take care of this by projecting the tuned parameter values into the range set (component-wise). A penaltyfunction can also be used instead. If noise level in the function evaluation is high then, it is useful to average several SP gradient estimates (each with independent values of ∆k) at a given θk. Theoretical justification for net improvements to efficiency by such gradient averaging is given in [28]. We can also use a one evaluation variant of SPSA, which can reduce the per iteration cost of SPSA. But it has been shown that standard two function measurement form, which we use in our work is more efficient (in terms of total number of loss function measurements) to obtain a given level of accuracy in the θ iterate."
    }, {
      "heading" : "6.6 Comparison with Related Work",
      "text" : "We compare our method with prior works in the literature on Hadoop performance tuning. Specifically, we look at Starfish[15] as well as Profiling and Performance Analysisbased System (PPABS) [32] frameworks. We briefly describe these methods in Section 3. Starfish is designed for Hadoop version 1 only, whereas PPABS works with the recent versions also. Hence, in our experiments we use both versions of Hadoop. To run Starfish, we use the executable hosted by the authors of [15] to profile the jobs run on partial workloads. Then execution time of new jobs is obtained by running the jobs using parameters provided by Starfish. For testing PPABS, we collect datasets as described in [32], cluster them and find optimized parameters (using simulated annealing) for each cluster. Each new job is then assigned to one cluster and executed with the parameters optimized for that cluster."
    }, {
      "heading" : "6.7 Discussion of Results",
      "text" : "Our method starts optimizing with the default configuration, hence the first entry in Fig. 6 show the execution time of Hadoop jobs for the default parameter setting. It is important to note that the jumps in the plots are due to the noisy nature of the gradient estimate and they eventually die down after sufficiently large number of iterations. As can be observed from Fig. 6, SPSA reduces the execution time of Terasort benchmark by 60− 63% when compared to default settings and by 40 − 60% when compared to Starfish optimizer. For Inverted Index benchmark the reduction is 80% when compared to default settings. In the case of word cooccurrence, the observed reduction is 22% when compared to default settings and 2% when compared to Starfish.\nSPSA finds the optimal configuration while keeping the relation among the parameters in mind. For Terasort, a small value (0.14) of io.sort.spill.percent will generate a lot of spilled files of small size. Because of this, the value of io.sort.factor has been increased to 475 from the default value of 10. This ensures the combination of number of spilled files to generate the partitioned and sorted files. A small value of shuffle.input.buffer.percent (0.14) and a large value of inmem.merge.threshold (9513) may be confusing as\nboth of them act as a threshold beyond which in-memory merge of files (output by map) is triggered. However, map outputs a total bytes of 100 GB and a total of 2, 000, 000, 000 files are spilled to disk which, effectively make each spilled file of size 50 bytes. Thus filling 0.14% of the memory allocated to Reduce makes 50 bytes files of which there will be 9513. Default value of number of reducers (i.e., 1) generally does not work in practical situations. However, increasing it to a very high number also creates an issue as it results in more network and disk overhead. As can be observed in Table 1, mapred.compress.map.output is set to true for Terasort benchmark. This is because, the output data of Map phase has same size as the input data (which might be huge). Thus, in such scenarios, it is beneficial if the Map output is compressed. Grep benchmark, on the other hand produces very little map output, and even smaller sized data to be shuffled. Hence io.sort.mb value is reduced to 50 from default 100 (see Table 1) and number of reducers is set to 1. Further, value of inmem.merge.threshold has been reduced to 681 from 1000 as there is not much data to work on.\nBigram and Inverted Index are computationally expensive operations. Hence io.sort.mb is increased to 751 and 617 respectively. Both of these applications also generate a reasonable size of data during the map phase, which implies a lot of spilled files are generated. Thus, inmem.merge.threshold has been increased to 4201 and 3542 respectively."
    }, {
      "heading" : "6.8 Advantages of SPSA",
      "text" : "The above discussion indicates that SPSA performs well in optimizing Hadoop parameters. We highlight other advantages (also see Table 2) of using our proposed method:\n1. Most of the profiling-based methods (Starfish, MROnline etc), use the internal Hadoop(source code) structure to place “markers” for precisely profiling a job. Starfish observes the time spent in each function by using btrace. Small change in the source code make this unusable (clearly Starfish only support Hadoop versions < 1.0.3). SPSA does not rely on the internal structure of hadoop and only observes the final execution time of the job which can be accessed easily.\n2. Independent of Hadoop version: As mentioned previously, profiling-based methods are highly dependent on the MapReduce version and any changes in the source code of Hadoop will require a version upgrade of these methods. In contrast, our SPSA-based method does not rely on any specific Hadoop version.\n3. Pause and resume: SPSA optimizes the parameters iteratively. It starts at a given point (default setting in our case) and then progressively finds a better configuration (by estimating gradient). Such a process can be paused at any iteration and then resumed using the same parameter configuration, where the iteration was stopped. This is unlike the profiling-based methods, which need to profile jobs in one go.\n4. SPSA takes into consideration multiple values of execution time of a job for the same parameter setting (randomness in execution time). This is not the case in other methods, which profile a job only once. Multiple observations helps SPSA to remove the randomness in the job which arise due to the underlying hardware.\n5. Parameters can be easily added and removed from the set of tunable parameters, which make our method suitable for scenarios where the user wants to have control over the parameters to be tuned.\n6. Profiling overhead: Profiling of takes a long time (since job run time is not yet optimized during profiling) which adds an extra overhead for these methods like Starfish, PPABS, MROnline etc. For e.g., in our experiments, Starfish profiling executed for 4 hours, 38 minutes (= 16680 seconds) in the case of Word cooccurrence benchmark on Wikipedia data of size 4 GB. Also, Starfish profiled Terasort on 100 GB of synthetic data for > 2 hours. In contrast, our method does not incur additional “profiling” time. The SPSA optimization time is justified, since each iteration results in learning a better parameter configuration."
    }, {
      "heading" : "7. CONCLUSIONS AND FUTURE WORK",
      "text" : "Hadoop framework presents the user with a large set of tunable parameters. Though default setting is known for these parameters, it is important to tune these parameters in order to obtain better performance. However, manual tuning of these parameters is difficult owing to the complex nature of the search space and the pronounced effect of cross-parameter interactions. This calls for an automatic tuning mechanism. Prior attempts at automatic tuning have adopted a mathematical model based approach and have resorted to parameter reduction prior to optimization. Since, Hadoop is continuously evolving, the mathematical models may fail for later versions and given the level of cross\nparameter interaction, it is a good idea to retain as many parameters as possible.\nIn this paper, we suggested a tuning method based on the simultaneous perturbation stochastic approximation (SPSA) algorithm. The salient features of the SPSA based scheme included its ability to use observations from a real system and its insensitivity to the number of parameters. Also, the SPSA algorithm took the cross-parameter interaction into account by computing the gradient at each point. Using the SPSA scheme, we tuned as many as 11 parameters and observed an improvement in execution time on real system. In particular, our experiments on benchmark applications such as Terasort, Grep, Bigram, Word Co-occurrence and Inverted Index showed that the parameters obtained using the SPSA algorithm yielded a decrease of 45-66% in execution times on a realistic 25 node cluster.\nOur aim here was to introduce the practitioners to an algorithm which was different in flavor, simple to implement and was as effective as the previous methods. In this work we considered only Hadoop parameters, however, the SPSA algorithm based tuning can include parameters from other layers such OS, System, Hardware etc. This will go a long way in providing a holistic approach to performance tuning of Hadoop MapReduce. Further, other simulation optimization algorithms like [20, 26] can be applied to the problem of Hadoop parameter tuning."
    }, {
      "heading" : "8. REFERENCES",
      "text" : "[1] Cloudera, 7 tips for improving MapReduce\nperformance. http://blog.cloudera.com/blog/2009/12/ 7-tips-for-improving-mapreduce-performance/.\n[2] HDFS Architecture. https://hadoop.apache.org/docs/r2.4.1/ hadoop-project-dist/hadoop-hdfs/HdfsDesign.html.\n[3] Microsoft, Hadoop Job Optimization. https://msdn.microsoft.com/en-us/dn197899.aspx.\n[4] Parameter Manual. https://hadoop.apache.org/docs/ r2.6.0/hadoop-mapreduce-client/ hadoop-mapreduce-client-core/mapred-default.xml.\n[5] PUMA Benchmarks and Dataset downloads. https://engineering.purdue.edu/˜puma/datasets.htm.\n[6] C. Antal, O. Granichin, and S. Levi. Adaptive autonomous soaring of multiple uavs using simultaneous perturbation stochastic approximation. In 49th IEEE Conference on Decision and Control (CDC), pages 3656–3661. IEEE, 2010.\n[7] S. Babu. Towards automatic optimization of mapreduce programs. In In SoCC, pages 137–142, 2010.\n[8] V. S. Borkar. Stochastic Approximation: A Dynamical Systems Viewpoint. TRIM, 2008.\n[9] A. A. Cole-Rhodes, K. L. Johnson, J. LeMoigne, and I. Zavorin. Multiresolution registration of remote sensing imagery by optimization of mutual information using a stochastic gradient. Image Processing, IEEE Transactions on, 12(12):1495–1511, 2003.\n[10] J. Dean and S. Ghemawat. MapReduce: Simplified Data Processing on Large Clusters. Commun. ACM, 51(1):107–113, Jan 2008.\n[11] X. Ding, Y. Liu, and D. Qian. Jellyfish: Online performance tuning with adaptive configuration and elastic container in hadoop yarn. In Parallel and Distributed Systems (ICPADS), 2015 IEEE 21st International Conference on, pages 831–836. IEEE, 2015.\n[12] A. S. Ganapathi. Predicting and Optimizing System Utilization and Performance via Statistical Machine Learning. PhD thesis, EECS Department, University of California, Berkeley, Dec 2009.\n[13] L. Hao and M. Yao. SPSA-based step tracking algorithm for mobile {DBS} reception. Simulation Modelling Practice and Theory, 19(2):837 – 846, 2011.\n[14] H. Herodotou and S. Babu. Profiling, what-if analysis, and cost-based optimization of mapreduce programs. PVLDB, 4(11):1111–1122, 2011.\n[15] H. Herodotou, H. Lim, G. Luo, N. Borisov, L. Dong, F. B. Cetin, and S. Babu. Starfish: A self-tuning system for big data analytics. In In CIDR, pages 261–272, 2011.\n[16] D. W. Hutchison and S. D. Hill. Simulation optimization of airline delay with constraints. In Simulation Conference, 2001. Proceedings of the Winter, volume 2, pages 1017–1022. IEEE, 2001.\n[17] K. Kambatla, A. Pathak, and H. Pucha. Towards optimizing hadoop provisioning in the cloud. HotCloud, 9:12, 2009.\n[18] S. Kavulya, J. Tan, R. Gandhi, and P. Narasimhan. An analysis of traces from a production mapreduce cluster. In Proceedings of the 2010 10th IEEE/ACM\nInternational Conference on Cluster, Cloud and Grid Computing, CCGRID ’10, pages 94–103, Washington, DC, USA, 2010. IEEE Computer Society.\n[19] D. Kushner, H.J.and Clark. Stochastic Approximation Methods for Constrained and Unconstrained Systems. Springer-Verlag New York, 1978.\n[20] K. Lakshmanan and S. Bhatnagar. Quasi-newton smoothed functional algorithms for unconstrained and constrained simulation optimization. Computational Optimization and Applications, pages 1–24, 2016.\n[21] P. Lama and X. Zhou. Aroma: Automated resource allocation and configuration of mapreduce environment in the cloud. In Proceedings of the 9th International Conference on Autonomic Computing, ICAC ’12, pages 63–72, New York, NY, USA, 2012. ACM.\n[22] M. Li, L. Zeng, S. Meng, J. Tan, L. Zhang, A. R. Butt, and N. Fuller. MROnline: MapReduce Online Performance Tuning. In Proceedings of the 23rd International Symposium on High-performance Parallel and Distributed Computing, HPDC ’14, pages 165–176, New York, NY, USA, 2014. ACM.\n[23] Y. Li and H.-F. Chen. Robust adaptive pole placement for linear time-varying systems. IEEE transactions on automatic control, 41(5):714–719, 1996.\n[24] A. Pavlo, E. Paulson, A. Rasin, D. J. Abadi, D. J. DeWitt, S. Madden, and M. Stonebraker. A comparison of approaches to large-scale data analysis. In Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data, SIGMOD ’09, pages 165–178, New York, NY, USA, 2009. ACM.\n[25] L. Prashanth and S. Bhatnagar. Threshold tuning using stochastic optimization for graded signal control. Vehicular Technology, IEEE Transactions on, 61(9):3865–3880, 2012.\n[26] L. A. Prashanth, S. Bhatnagar, M. C. Fu, and S. Marcus. Adaptive system optimization using random directions stochastic approximation. IEEE Transactions on Automatic Control, PP(99):1–1, 2016.\n[27] J. C. Spall. Multivariate stochastic approximation using a simultaneous perturbation gradient approximation. Automatic Control, IEEE Transactions on, 37(3):332–341, 1992.\n[28] J. C. Spall. Multivariate stochastic approximation using a simultaneous perturbation gradient approximation. IEEE Transactions on Automatic Control, 37:332–341, 1992.\n[29] Q.-Y. Tang, H.-F. Chen, and Z.-J. Han. Convergence rates of perturbation-analysis-robbins-monro-single-run algorithms for single server queues. IEEE Transactions on Automatic Control, 42(10):1442–1447, Oct 1997.\n[30] V. K. Vavilapalli, A. C. Murthy, C. Douglas, S. Agarwal, M. Konar, R. Evans, T. Graves, J. Lowe, H. Shah, S. Seth, B. Saha, C. Curino, O. O’Malley, S. Radia, B. Reed, and E. Baldeschwieler. Apache Hadoop YARN: Yet Another Resource Negotiator. In Proceedings of the 4th Annual Symposium on Cloud Computing, SOCC ’13, pages 5:1–5:16, New York, NY, USA, 2013. ACM.\n[31] T. White. Hadoop: The Definitive Guide, Storage and\nAnalysis at Internet Scale, 4th Edition. O’Reilly Media, March 2015.\n[32] D. Wu and A. S. Gokhale. A self-tuning system based on application profiling and performance analysis for optimizing hadoop mapreduce cluster configuration. In 20th Annual International Conference on High Performance Computing, HiPC, 2013, pages 89–98. IEEE Computer Society, 2013."
    } ],
    "references" : [ {
      "title" : "Adaptive autonomous soaring of multiple uavs using simultaneous perturbation stochastic approximation",
      "author" : [ "C. Antal", "O. Granichin", "S. Levi" ],
      "venue" : "IEEE Conference on Decision and Control (CDC),",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2010
    }, {
      "title" : "Towards automatic optimization of mapreduce programs",
      "author" : [ "S. Babu" ],
      "venue" : "In In SoCC, pages 137–142,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2010
    }, {
      "title" : "Stochastic Approximation: A Dynamical Systems Viewpoint",
      "author" : [ "V.S. Borkar" ],
      "venue" : "TRIM,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2008
    }, {
      "title" : "Multiresolution registration of remote sensing imagery by optimization of mutual information using a stochastic gradient",
      "author" : [ "A.A. Cole-Rhodes", "K.L. Johnson", "J. LeMoigne", "I. Zavorin" ],
      "venue" : "Image Processing, IEEE Transactions on,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2003
    }, {
      "title" : "MapReduce: Simplified Data",
      "author" : [ "J. Dean", "S. Ghemawat" ],
      "venue" : "Processing on Large Clusters. Commun. ACM,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2008
    }, {
      "title" : "Jellyfish: Online performance tuning with adaptive configuration and elastic container in hadoop yarn",
      "author" : [ "X. Ding", "Y. Liu", "D. Qian" ],
      "venue" : "In Parallel and Distributed Systems (ICPADS),",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2015
    }, {
      "title" : "Predicting and Optimizing System Utilization and Performance via Statistical Machine Learning",
      "author" : [ "A.S. Ganapathi" ],
      "venue" : "PhD thesis, EECS Department,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2009
    }, {
      "title" : "SPSA-based step tracking algorithm for mobile {DBS} reception",
      "author" : [ "L. Hao", "M. Yao" ],
      "venue" : "Simulation Modelling Practice and Theory,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2011
    }, {
      "title" : "Profiling, what-if analysis, and cost-based optimization of mapreduce programs",
      "author" : [ "H. Herodotou", "S. Babu" ],
      "venue" : "PVLDB, 4(11):1111–1122,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2011
    }, {
      "title" : "Starfish: A self-tuning system for big data analytics",
      "author" : [ "H. Herodotou", "H. Lim", "G. Luo", "N. Borisov", "L. Dong", "F.B. Cetin", "S. Babu" ],
      "venue" : "In In CIDR,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2011
    }, {
      "title" : "Simulation optimization of airline delay with constraints",
      "author" : [ "D.W. Hutchison", "S.D. Hill" ],
      "venue" : "In Simulation Conference,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2001
    }, {
      "title" : "Towards optimizing hadoop provisioning",
      "author" : [ "K. Kambatla", "A. Pathak", "H. Pucha" ],
      "venue" : "in the cloud. HotCloud,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2009
    }, {
      "title" : "An analysis of traces from a production mapreduce cluster",
      "author" : [ "S. Kavulya", "J. Tan", "R. Gandhi", "P. Narasimhan" ],
      "venue" : "In Proceedings of the 2010 10th IEEE/ACM  International Conference on Cluster, Cloud and Grid Computing,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2010
    }, {
      "title" : "Stochastic Approximation Methods for Constrained and Unconstrained Systems",
      "author" : [ "H.J.D. Kushner", "Clark" ],
      "venue" : null,
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 1978
    }, {
      "title" : "Quasi-newton smoothed functional algorithms for unconstrained and constrained simulation optimization",
      "author" : [ "K. Lakshmanan", "S. Bhatnagar" ],
      "venue" : "Computational Optimization and Applications,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2016
    }, {
      "title" : "Aroma: Automated resource allocation and configuration of mapreduce environment in the cloud",
      "author" : [ "P. Lama", "X. Zhou" ],
      "venue" : "In Proceedings of the 9th International Conference on Autonomic Computing,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2012
    }, {
      "title" : "MROnline: MapReduce Online Performance Tuning",
      "author" : [ "M. Li", "L. Zeng", "S. Meng", "J. Tan", "L. Zhang", "A.R. Butt", "N. Fuller" ],
      "venue" : "In Proceedings of the 23rd International Symposium on High-performance Parallel and Distributed Computing,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2014
    }, {
      "title" : "Robust adaptive pole placement for linear time-varying systems",
      "author" : [ "Y. Li", "H.-F. Chen" ],
      "venue" : "IEEE transactions on automatic control,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 1996
    }, {
      "title" : "A comparison of approaches to large-scale data analysis",
      "author" : [ "A. Pavlo", "E. Paulson", "A. Rasin", "D.J. Abadi", "D.J. DeWitt", "S. Madden", "M. Stonebraker" ],
      "venue" : "In Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2009
    }, {
      "title" : "Threshold tuning using stochastic optimization for graded signal control",
      "author" : [ "L. Prashanth", "S. Bhatnagar" ],
      "venue" : "Vehicular Technology, IEEE Transactions on,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2012
    }, {
      "title" : "Adaptive system optimization using random directions stochastic approximation",
      "author" : [ "L.A. Prashanth", "S. Bhatnagar", "M.C. Fu", "S. Marcus" ],
      "venue" : "IEEE Transactions on Automatic Control,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2016
    }, {
      "title" : "Multivariate stochastic approximation using a simultaneous perturbation gradient approximation",
      "author" : [ "J.C. Spall" ],
      "venue" : "Automatic Control, IEEE Transactions on,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 1992
    }, {
      "title" : "Multivariate stochastic approximation using a simultaneous perturbation gradient approximation",
      "author" : [ "J.C. Spall" ],
      "venue" : "IEEE Transactions on Automatic Control,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 1992
    }, {
      "title" : "Convergence rates of perturbation-analysis-robbins-monro-single-run algorithms for single server queues",
      "author" : [ "Q.-Y. Tang", "H.-F. Chen", "Z.-J. Han" ],
      "venue" : "IEEE Transactions on Automatic Control,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 1997
    }, {
      "title" : "Baldeschwieler. Apache Hadoop YARN: Yet Another Resource Negotiator",
      "author" : [ "V.K. Vavilapalli", "A.C. Murthy", "C. Douglas", "S. Agarwal", "M. Konar", "R. Evans", "T. Graves", "J. Lowe", "H. Shah", "S. Seth", "B. Saha", "C. Curino", "O. O’Malley", "S. Radia", "B. Reed" ],
      "venue" : "In Proceedings of the 4th Annual Symposium on Cloud Computing,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2013
    }, {
      "title" : "Hadoop: The Definitive Guide, Storage and  Analysis at Internet Scale, 4th Edition",
      "author" : [ "T. White" ],
      "venue" : null,
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2015
    }, {
      "title" : "A self-tuning system based on application profiling and performance analysis for optimizing hadoop mapreduce cluster configuration",
      "author" : [ "D. Wu", "A.S. Gokhale" ],
      "venue" : "In 20th Annual International Conference on High Performance Computing,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 18,
      "context" : "Currently available parallel processing systems are database systems [24] like Teradata, Aster Data, Vertica etc.",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 4,
      "context" : "MapReduce[10] is one-such programming model.",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 25,
      "context" : "Apache Hadoop[31] is an open-source implementation of MapReduce written in Java for distributed storage and processing of very large data sets on clusters built using commodity hardware.",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 11,
      "context" : "The problem of Hadoop performance being limited by the parameter configuration was recognized in [17].",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 18,
      "context" : "Unlike SQL, MapReduce jobs cannot be modeled using a small and finite space of relational operators [24].",
      "startOffset" : 100,
      "endOffset" : 104
    }, {
      "referenceID" : 11,
      "context" : "The necessity for tuning of Hadoop parameters was first emphasized in [17], which proposed a method to determine the optimum configuration given a set of computing resources.",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 9,
      "context" : "Recent efforts in the direction of automatic tuning of the Hadoop parameters include Starfish[15], AROMA[21], MROnline[22], PPABS [32] and JellyFish [11].",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 15,
      "context" : "Recent efforts in the direction of automatic tuning of the Hadoop parameters include Starfish[15], AROMA[21], MROnline[22], PPABS [32] and JellyFish [11].",
      "startOffset" : 104,
      "endOffset" : 108
    }, {
      "referenceID" : 16,
      "context" : "Recent efforts in the direction of automatic tuning of the Hadoop parameters include Starfish[15], AROMA[21], MROnline[22], PPABS [32] and JellyFish [11].",
      "startOffset" : 118,
      "endOffset" : 122
    }, {
      "referenceID" : 26,
      "context" : "Recent efforts in the direction of automatic tuning of the Hadoop parameters include Starfish[15], AROMA[21], MROnline[22], PPABS [32] and JellyFish [11].",
      "startOffset" : 130,
      "endOffset" : 134
    }, {
      "referenceID" : 5,
      "context" : "Recent efforts in the direction of automatic tuning of the Hadoop parameters include Starfish[15], AROMA[21], MROnline[22], PPABS [32] and JellyFish [11].",
      "startOffset" : 149,
      "endOffset" : 153
    }, {
      "referenceID" : 11,
      "context" : "We observe that collecting statistical data to create virtual profiles and estimating execution time using mathematical model (as in [17, 15, 11, 21, 22, 32]) requires significant level of expertise which might not be available always.",
      "startOffset" : 133,
      "endOffset" : 157
    }, {
      "referenceID" : 9,
      "context" : "We observe that collecting statistical data to create virtual profiles and estimating execution time using mathematical model (as in [17, 15, 11, 21, 22, 32]) requires significant level of expertise which might not be available always.",
      "startOffset" : 133,
      "endOffset" : 157
    }, {
      "referenceID" : 5,
      "context" : "We observe that collecting statistical data to create virtual profiles and estimating execution time using mathematical model (as in [17, 15, 11, 21, 22, 32]) requires significant level of expertise which might not be available always.",
      "startOffset" : 133,
      "endOffset" : 157
    }, {
      "referenceID" : 15,
      "context" : "We observe that collecting statistical data to create virtual profiles and estimating execution time using mathematical model (as in [17, 15, 11, 21, 22, 32]) requires significant level of expertise which might not be available always.",
      "startOffset" : 133,
      "endOffset" : 157
    }, {
      "referenceID" : 16,
      "context" : "We observe that collecting statistical data to create virtual profiles and estimating execution time using mathematical model (as in [17, 15, 11, 21, 22, 32]) requires significant level of expertise which might not be available always.",
      "startOffset" : 133,
      "endOffset" : 157
    }, {
      "referenceID" : 26,
      "context" : "We observe that collecting statistical data to create virtual profiles and estimating execution time using mathematical model (as in [17, 15, 11, 21, 22, 32]) requires significant level of expertise which might not be available always.",
      "startOffset" : 133,
      "endOffset" : 157
    }, {
      "referenceID" : 26,
      "context" : "Further, given the presence of cross-parameter interaction it is a good idea to retain as many parameters as possible (as opposed to reducing the parameters [32]) in the tuning phase.",
      "startOffset" : 157,
      "endOffset" : 161
    }, {
      "referenceID" : 21,
      "context" : "In this paper, we present a novel tuning methodology based on a noisy gradient method known as the simultaneous perturbation stochastic approximation (SPSA) algorithm [27].",
      "startOffset" : 167,
      "endOffset" : 171
    }, {
      "referenceID" : 1,
      "context" : "Thus, unlike [7] reducing the search space is not a requirement.",
      "startOffset" : 13,
      "endOffset" : 16
    }, {
      "referenceID" : 9,
      "context" : "Further, we also observe a reduction of 45% in execution times, when compared to prior [15] methods.",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 4,
      "context" : "Hadoop is an open source implementation of the MapReduce[10], which has gained a huge amount of popularity in recent years as it can be used over commodity hardware.",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 25,
      "context" : "A HDFS cluster (see [31]) consists of a single NameNode, a master server, and multiple slave DataNodes.",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 24,
      "context" : "Hadoop MapReduce version 2 (v2, also known as Yet Another Resource Negotiator (YARN)[30]) has a different architecture.",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 6,
      "context" : "Some early works [12, 18] have focussed on analysing the MapReduce performance and not addressed the problem of parameter tuning.",
      "startOffset" : 17,
      "endOffset" : 25
    }, {
      "referenceID" : 12,
      "context" : "Some early works [12, 18] have focussed on analysing the MapReduce performance and not addressed the problem of parameter tuning.",
      "startOffset" : 17,
      "endOffset" : 25
    }, {
      "referenceID" : 6,
      "context" : "The authors in [12] develop models for predicting performance of Hive queries and ETL (Extract Transform Load) kind of MapReduce jobs.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 12,
      "context" : "MapReduce logs of a M45 supercomputing cluster (released by Yahoo!) are analysed in [18].",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 12,
      "context" : "Based on this categorization, [18] suggests improvements in Hadoop MapReduce which can mitigate performance bottlenecks and reduce job failures.",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 9,
      "context" : "Attempts toward building an optimizer for hadoop performance started with Starfish[15].",
      "startOffset" : 82,
      "endOffset" : 86
    }, {
      "referenceID" : 9,
      "context" : "In Starfish [15, 14], a Profiler collects detailed statistical information (like data flow and cost statistics) from unmodified Mapreduce program during full or partial execution.",
      "startOffset" : 12,
      "endOffset" : 20
    }, {
      "referenceID" : 8,
      "context" : "In Starfish [15, 14], a Profiler collects detailed statistical information (like data flow and cost statistics) from unmodified Mapreduce program during full or partial execution.",
      "startOffset" : 12,
      "endOffset" : 20
    }, {
      "referenceID" : 15,
      "context" : "Works following Starfish are [21, 32].",
      "startOffset" : 29,
      "endOffset" : 37
    }, {
      "referenceID" : 26,
      "context" : "Works following Starfish are [21, 32].",
      "startOffset" : 29,
      "endOffset" : 37
    }, {
      "referenceID" : 15,
      "context" : "In the online phase [21] trains a SVM which makes accurate and fast prediction of a job’s performance for various configuration parameters and input data sizes.",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 26,
      "context" : "In [32], the optimal parameter configuration for every cluster is obtained through simulated annealing, albeit for a reduced parameter search space.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 16,
      "context" : "An online MapReduce performance tuner (MROnline) is developed in [22].",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 24,
      "context" : "It is desgined and implemented on YARN [30] (described in Section 2).",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 8,
      "context" : "In [14], the optimization is based on the what-if engine which uses a mix of simulation and model-based estimation.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 26,
      "context" : "In [32], authors make use of available knowledge from literature in order to reduce the parameter space and they make use of simulated annealing to find the right parameter setting in the reduced space.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 19,
      "context" : "The performance of various complex systems such as traffic control [25], unmanned aerial vehicle (UAV) control [6], remote sensing [9], communication in satellites [13] and airlines [16] depends on a set of tunable parameters (denoted by θ).",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 0,
      "context" : "The performance of various complex systems such as traffic control [25], unmanned aerial vehicle (UAV) control [6], remote sensing [9], communication in satellites [13] and airlines [16] depends on a set of tunable parameters (denoted by θ).",
      "startOffset" : 111,
      "endOffset" : 114
    }, {
      "referenceID" : 3,
      "context" : "The performance of various complex systems such as traffic control [25], unmanned aerial vehicle (UAV) control [6], remote sensing [9], communication in satellites [13] and airlines [16] depends on a set of tunable parameters (denoted by θ).",
      "startOffset" : 131,
      "endOffset" : 134
    }, {
      "referenceID" : 7,
      "context" : "The performance of various complex systems such as traffic control [25], unmanned aerial vehicle (UAV) control [6], remote sensing [9], communication in satellites [13] and airlines [16] depends on a set of tunable parameters (denoted by θ).",
      "startOffset" : 164,
      "endOffset" : 168
    }, {
      "referenceID" : 10,
      "context" : "The performance of various complex systems such as traffic control [25], unmanned aerial vehicle (UAV) control [6], remote sensing [9], communication in satellites [13] and airlines [16] depends on a set of tunable parameters (denoted by θ).",
      "startOffset" : 182,
      "endOffset" : 186
    }, {
      "referenceID" : 21,
      "context" : "The SPSA algorithm [27] computes the gradient of a function with only 2 or fewer perturbations.",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 17,
      "context" : "Then for any small positive constant δ > 0, the one-sided SPSA algorithm [23, 29] obtains an estimate of the gradient according to equation (3) given below.",
      "startOffset" : 73,
      "endOffset" : 81
    }, {
      "referenceID" : 23,
      "context" : "Then for any small positive constant δ > 0, the one-sided SPSA algorithm [23, 29] obtains an estimate of the gradient according to equation (3) given below.",
      "startOffset" : 73,
      "endOffset" : 81
    }, {
      "referenceID" : 2,
      "context" : "The iterative update in (5) is known as a stochastic approximation [8] recursion.",
      "startOffset" : 67,
      "endOffset" : 70
    }, {
      "referenceID" : 13,
      "context" : "191196 of [19].",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 25,
      "context" : "Hence, in a single wave of Map jobs processing, the cluster can process 24 × 3 = 72 map tasks and 24 × 2 = 48 reduce tasks (for more details see [31]).",
      "startOffset" : 145,
      "endOffset" : 149
    }, {
      "referenceID" : 22,
      "context" : "Theoretical justification for net improvements to efficiency by such gradient averaging is given in [28].",
      "startOffset" : 100,
      "endOffset" : 104
    }, {
      "referenceID" : 9,
      "context" : "Specifically, we look at Starfish[15] as well as Profiling and Performance Analysisbased System (PPABS) [32] frameworks.",
      "startOffset" : 33,
      "endOffset" : 37
    }, {
      "referenceID" : 26,
      "context" : "Specifically, we look at Starfish[15] as well as Profiling and Performance Analysisbased System (PPABS) [32] frameworks.",
      "startOffset" : 104,
      "endOffset" : 108
    }, {
      "referenceID" : 9,
      "context" : "To run Starfish, we use the executable hosted by the authors of [15] to profile the jobs run on partial workloads.",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 26,
      "context" : "For testing PPABS, we collect datasets as described in [32], cluster them and find optimized parameters (using simulated annealing) for each cluster.",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 14,
      "context" : "Further, other simulation optimization algorithms like [20, 26] can be applied to the problem of Hadoop parameter tuning.",
      "startOffset" : 55,
      "endOffset" : 63
    }, {
      "referenceID" : 20,
      "context" : "Further, other simulation optimization algorithms like [20, 26] can be applied to the problem of Hadoop parameter tuning.",
      "startOffset" : 55,
      "endOffset" : 63
    } ],
    "year" : 2016,
    "abstractText" : "Hadoop MapReduce is a framework for distributed storage and processing of large datasets that is quite popular in big data analytics. It has various configuration parameters (knobs) which play an important role in deciding the performance i.e., the execution time of a given big data processing job. Default values of these parameters do not always result in good performance and hence it is important to tune them. However, there is inherent difficulty in tuning the parameters due to two important reasons firstly, the parameter search space is large and secondly, there are cross-parameter interactions. Hence, there is a need for a dimensionality-free method which can automatically tune the configuration parameters by taking into account the cross-parameter dependencies. In this paper, we propose a novel Hadoop parameter tuning methodology, based on a noisy gradient algorithm known as the simultaneous perturbation stochastic approximation (SPSA). The SPSA algorithm tunes the parameters by directly observing the performance of the Hadoop MapReduce system. The approach followed is independent of parameter dimensions and requires only 2 observations per iteration while tuning. We demonstrate the effectiveness of our methodology in achieving good performance on popular Hadoop benchmarks namely Grep, Bigram, Inverted Index, Word Co-occurrence and Terasort. Our method, when tested on a 25 node Hadoop cluster shows 66% decrease in execution time of Hadoop jobs on an average, when compared to the default configuration. Further, we also observe a reduction of 45% in execution times, when compared to prior methods.",
    "creator" : "LaTeX with hyperref package"
  }
}