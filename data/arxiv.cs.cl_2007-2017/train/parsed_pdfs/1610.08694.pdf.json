{
  "name" : "1610.08694.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "CogALex-V Shared Task: LexNET - Integrated Path-based and Distributional Method for the Identification of Semantic Relations",
    "authors" : [ "Vered Shwartz", "Ido Dagan" ],
    "emails" : [ "vered1986@gmail.com", "dagan@cs.biu.ac.il" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n61 0.\n08 69\n4v 1\n[ cs\n.C L\n] 2\n7 O\nct 2\n01 6\nWe present a submission to the CogALex 2016 shared task on the corpus-based identification of semantic relations, using LexNET (Shwartz and Dagan, 2016), an integrated path-based and distributional method for semantic relation classification. The reported results in the shared task bring this submission to the third place on subtask 1 (word relatedness), and the first place on subtask 2 (semantic relation classification), demonstrating the utility of integrating the complementary path-based and distributional information sources in recognizing semantic relatedness. Combined with a common similarity measure, LexNET performs fairly good on the word relatedness task (subtask 1). The relatively lower performance of LexNET and the various other systems on subtask 2, however, confirms the difficulty of the semantic relation classification task, and stresses the need to develop additional methods for this task."
    }, {
      "heading" : "1 Introduction",
      "text" : "Discovering whether words are semantically related and identifying the specific semantic relation that holds between them is a key component in many NLP applications, such as question answering and recognizing textual entailment (Dagan et al., 2013). Automated methods for semantic relation identification are commonly corpus-based, and mainly rely on the distributional representation of each word.\nThe CogALex shared task on the corpus-based identification of semantic relations consists of two subtasks. In the first task, the system needs to identify for a word pair whether the words are semantically related or not (e.g. True:(dog, cat), False:(dog, fruit)). In the second task, the goal is to determine the specific semantic relation that holds for a given pair, if any (PART OF:(tail, cat), HYPER:(cat, animal)).\nIn this paper we describe our approach and system setup for the shared task. We use LexNET (Shwartz and Dagan, 2016), an integrated path-based and distributional method for semantic relation classification, available at https://github.com/vered1986/LexNET. LexNET was the system with the overall best performance on subtask 2, and was ranked third on subtask 1, demonstrating the utility of integrating the complementary path-based and distributional information sources in recognizing semantic relatedness.\nTo aid in recognizing whether a pair of words are related at all (subtask 1), we combine LexNET with a common similarity measure (cosine similarity), achieving fairly good performance, and a slight improvement upon using cosine similarity alone. Subtask 2, however, has shown to be extremely difficult, with LexNET and all other systems achieving relatively low F1 scores. The conflict between the mediocre performance and the recent success of distributional methods on several other common datasets for semantic relation classification (Baroni et al., 2012; Weeds et al., 2014; Roller et al., 2014) could be explained by the strict evaluation setup in this subtask, which is supposed to demonstrate more closely real-world application. The difficulty of the semantic relation classification task emphasizes the need to develop additional and improved methods for this task.\nThis work is licenced under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons.org/licenses/by/4.0/"
    }, {
      "heading" : "2 Background",
      "text" : ""
    }, {
      "heading" : "2.1 Word Relatedness",
      "text" : "Recognizing word relatedness is typically addressed by distributional methods, i.e., to determine to what extent x and y are related, a vector similarity/distance measure is applied to their distributional representations: sim(~vwx , ~vwy). This is a straightforward application of the distributional hypothesis (Harris, 1954), according to which related words occur in similar contexts, hence have similar vector representations.\nMost commonly, vector cosine is adopted as a similarity measure (Turney et al., 2010). Many other measures exist, including but not limited to Euclidean distance, KL divergence (Cover and Thomas, 2012), Jaccard’s coefficient (Salton and McGill, 1986), and more recently neighbor rank (Hare et al., 2009; Lapesa and Evert, 2013) and APSyn (Santus et al., 2016a).1 To turn this task into a binary classification task, where x and y are classified as either related or not, one can set a threshold to separate similarity scores of related and unrelated word pairs."
    }, {
      "heading" : "2.2 Semantic Relation Classification",
      "text" : "Recognizing lexical semantic relations between words is valuable for many NLP applications, such as ontology learning, question answering, and recognizing textual entailment. Most corpus-based methods classify the relation between a pair of words x and y based on the distributional representation of each word (Baroni et al., 2012; Roller et al., 2014; Fu et al., 2014; Weeds et al., 2014). Earlier methods utilized the dependency paths that connect the co-occurrences of x and y in the corpus as a cue to the relation between the words (Hearst, 1992; Snow et al., 2004; Nakashole et al., 2012). Recently, Shwartz and Dagan (2016) presented LexNET, a method that integrates both path-based and distributional information for semantic relation classification, which outperformed individual approaches on several common datasets (Baroni and Lenci, 2011; Necşulescu et al., 2015; Santus et al., 2015; Santus et al., 2016b)."
    }, {
      "heading" : "3 System Description",
      "text" : "We use LexNET (Shwartz and Dagan, 2016), an integrated path-based and distributional method for semantic relation classification. In LexNET, a word-pair (x, y) is represented as a feature vector, consisting of both distributional and path-based features: ~vxy = [~vwx , ~vpaths(x,y), ~vwy ], where ~vwx and ~vwy are x and y’s word embeddings, providing their distributional representation, and ~vpaths(x,y) is the average embedding vector of all the dependency paths that connect x and y in the corpus. Dependency paths are embedded using a LSTM (Hochreiter and Schmidhuber, 1997), as described in Shwartz et al. (2016). This vector is then fed into a neural network that outputs the class distribution ~c, and the pair is classified to the relation with the highest score r:\n~c = softmax(MLP(~vxy)) (1a)\nr = argmaxi ~c[i] (1b)\nMLP stands for Multi Layer Perceptron, and could be computed with or without a hidden layer (equations 2 and 3, respectively):\n~h = tanh(W1 · ~vxy + b1) (2a)\nMLP(~vxy) = W2 · ~h+ b2 (2b)\nMLP(~vxy) = W1 · ~vxy + b1 (3)\nwhere Wi and bi are the network parameters and ~h is the hidden layer.\n1See Lee (1999) for an extensive list of such measures."
    }, {
      "heading" : "3.1 A Note About Word Relatedness",
      "text" : "While path-based approaches have been commonly used for semantic relation classification (Hearst, 1992; Snow et al., 2004; Nakashole et al., 2012; Necşulescu et al., 2015), they are never used for word relatedness, which is considered a “classical” task for distributional methods. We argue that path-based information can improve performance of word relatedness tasks as well (see Section 4.1). We train LexNET to distinguish between two classes: RELATED and UNRELATED, and combine it with the common cosine similarity measure to solve subtask 1."
    }, {
      "heading" : "4 Experimental Settings",
      "text" : "The shared task organizers provided a dataset extracted from EVALution 1.0 (Santus et al., 2015), which was split into training and test sets. As instructed, we trained and tuned our method on the training set, and evaluated it once on the test set. To tune the hyper-parameters, we split the training set to 90% train and 10% validation sets. Since the dataset contains only 318 different words (in the x slot), we performed the split such that the train and the validation contain distinct x words.2\nLexNET has several tunable hyper-parameters. As an underlying corpus, we used the Wikipedia corpus from Shwartz and Dagan (2016). Similarly to Shwartz et al. (2016), we initialized the network’s word embeddings with the 50-dimensional pre-trained GloVe word embeddings (Pennington et al., 2014), trained on Wikipedia. We fixed this hyper-parameter due to computational limitations with higher-dimensional embeddings. For each subtask, we tuned LexNET’s hyper-parameters on the validation set: the number of hidden layers (0 or 1), the number of training epochs, and the word dropout rate (see Shwartz et al. (2016) for technical details). Table 1 displays the best performing hyper-parameters in each subtask, along with the performance on the validation set, which is detailed below."
    }, {
      "heading" : "4.1 Subtask 1: Word Relatedness",
      "text" : "We tuned LexNET’s hyper-parameters on the validation set, disregarding the similarity measure at this point, and then chose the model that performed best on the validation set and combined it with the similarity measure.\nWe computed cosine similarity for each (x, y) pair in the dataset: cos(~vwx , ~vwy) = ~vwx ·~vwy\n‖~vwx‖·‖~vwy ‖ , and\nnormalized it to the range [0, 1]. We scored each (x, y) pair by a combination of LexNET’s score for the RELATED class and the cosine similarity score:\nRel(x, y) = wC · cos(~vwx , ~vwy) + wL · ~c[RELATED] (4)\nwhere wC , wL are the weights assigned to cosine similarity and LexNET’s scores respectively, such that wC + wL = 1. We tuned the weights and a threshold t using the validation set, and classified (x, y) as related if Rel(x, y) ≥ t. The word vectors used to compute the cosine similarity scores were chosen among several available pre-trained embeddings.3 For completeness we also report the performance of two baselines: cosine similarity (wC = 1) and LexNET (wL = 1, fixed t = 0.5).\n2A random split yielded perfect results on the validation set, which were due to lexical memorization (Levy et al., 2015). 3word2vec (300 dimensions, SGNS, trained on GoogleNews) (Mikolov et al., 2013), GloVe (50-300 dimensions, trained on\nWikipedia) (Pennington et al., 2014), and dependency-based embeddings (Levy and Goldberg, 2014)."
    }, {
      "heading" : "4.2 Subtask 2: Semantic Relation Classification",
      "text" : "The subtask’s train set is highly imbalanced towards random instances (roughly 10 times more than any other relation), and training any supervised method leads to overfitting to the random class. We therefore trained the model only on the related classes (excluding RANDOM pairs), for which the classes are more balanced. During inference time, we used the model from subtask 1 to assign a relatedness score to each pair, Rel(x, y), and computed the class distribution using the model from subtask 2, only for pairs that were related according to this score.\nFinally, we applied a fix that if for a word pair (x, y), the difference in scores between the top scoring classes is low (< 0.2), and one of the classes is SYN, then it is only classified as SYN if the number of paths is smaller than 3. This is due to the fact that synonyms are hard to recognize with both distributional and path-based approaches (Shwartz and Dagan, 2016), but it is known that they do not tend to co-occur.\nTo compare LexNET’s performance on the validation set with other methods’ performances, we adapted the distributional baseline employed by Shwartz et al. (2016) and Shwartz and Dagan (2016), where a classifier is trained on the combination of x and y’s word embeddings. We experimented with several combination methods (concatenation (Baroni et al., 2012), difference (Fu et al., 2014; Weeds et al., 2014), and ASYM (Roller et al., 2014)), regularization factors, and pre-trained word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014). This time, we used cosine similarity (subtask 1) to separate related from unrelated pairs, and trained the classifier only to distinguish between the related classes. Similarly to subtask 1, we tune LexNET and the baseline’s hyper-parameters on the validation set. The best performance is reported in Table 1."
    }, {
      "heading" : "5 Results and Analysis",
      "text" : "Table 2 displays the performance of our methods and the baselines on the test set. In addition to the two baselines provided by the shared task organizers (majority and random), we report also the results of our baselines detailed in Section 4. The majority baseline classifies all the instances as UNREALTED (subtask 1) or RANDOM (subtask 2). Since these labels are excluded from the averaged F1 computation, this baseline’s performance metrics are all zero.\nSubtask 1: Word Relatedness Cos achieves fairly good performance (F1 = 0.747), and LexNET+Cos slightly improves upon it. To better understand LexNET’s contribution, we examined pairs that were correctly classified by LexNET+Cos while being incorrectly classified by Cos. Out of the 57 pairs that were true negative in LexNET and false positive in Cos, we judged only one as somehow related ((death, man)).\nWe sampled 25 (from the 184) true positive pairs in LexNET+Cos that were false negatives in Cos, and found that they were all connected via paths in the corpus, suggesting that LexNET’s contribution comes from the path-based component, rather than from additional distributional information. 12 of the pairs contained a polysemous term, for which the relation holds in a specific sense (e.g. (fire, shoot)). 5 other pairs had a weak relation, e.g. (compact, car). While a car can be compact, non of these words is one of the most related words to the other.4 As noted by Shwartz and Dagan (2016), these are cases\n4car is mostly related to driver, cars, and race, and compact to compactness and locally.\nin which distributional methods may fail to identify the relation between the words, while even a single meaningful path connecting x and y can capture the relation.\nSubtask 2: Semantic Relation Classification We note that the overall results on this task are low, in contrast to the success of several methods on common datasets (Baroni et al., 2012; Weeds et al., 2014; Roller et al., 2014; Shwartz and Dagan, 2016). One possible explanation is the strict evaluation, that considers the RANDOM class as noise, discarding it from the F1 average.5 Additionally, the dataset is lexically split, disabling lexical memorization (Levy et al., 2015). However, the strict evaluation spots a light on the difficulty of this task, which was somewhat obfuscated by the strong results published so far.\nFigure 1 displays LexNET’s per relation F1 scores on the test set, with the corresponding confusion matrix. While the F1 scores of individual classes are relatively low, the confusion matrix shows that pairs were always classified to the correct relation more than to any other class. A common error comes from subtask 1’s model: while most unrelated pairs were classified as unrelated, many related pairs were also classified as unrelated. Among the other relations, the performance on synonyms was the worst. The path-based component is weak in recognizing synonyms, which do not tend to co-occur. The distributional information causes confusion between synonyms and antonyms, since both tend to occur in the same contexts. Moreover, synonyms were also sometimes mistaken with hypernyms, as the difference between the two relations is often subtle (Shwartz et al., 2016)."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We have presented our submission to the CogALex 2016 shared task on the corpus-based identification of semantic relations. Our system is based on LexNET (Shwartz and Dagan, 2016), an integrated pathbased and distributional method for semantic relation classification. LexNET was the best-performing system on subtask 2, demonstrating the utility of integrating the complementary path-based and distributional information sources in recognizing semantic relatedness.\nWe have shown that subtask 1 (word relatedness) reaches reasonable performance with cosine similarity, and is slightly improved when combined with LexNET, especially when the relation between the words is non-prototypical. The performance on subtask 2, however, was relatively low for all systems that participated in the shared task, including LexNET. This demonstrates the difficulty of the semantic relation classification task, and emphasizes the need to develop additional and improved methods for this task."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work was partially supported by an Intel ICRI-CI grant, the Israel Science Foundation grant 880/12, and the German Research Foundation through the German-Israeli Project Cooperation (DIP, grant DA 1600/1-1).\n5When the random class is included in the averaged F1 score, the results are: P = 0.780, R = 0.786, F1 = 0.781."
    } ],
    "references" : [ {
      "title" : "How we blessed distributional semantic evaluation",
      "author" : [ "Baroni", "Lenci2011] Marco Baroni", "Alessandro Lenci" ],
      "venue" : "In GEMS Workshop on GEometrical Models of Natural Language Semantics",
      "citeRegEx" : "Baroni et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Baroni et al\\.",
      "year" : 2011
    }, {
      "title" : "Entailment above the word level in distributional semantics",
      "author" : [ "Baroni et al.2012] Marco Baroni", "Raffaella Bernardi", "Ngoc-Quynh Do", "Chung-chieh Shan" ],
      "venue" : null,
      "citeRegEx" : "Baroni et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Baroni et al\\.",
      "year" : 2012
    }, {
      "title" : "Elements of information theory",
      "author" : [ "Cover", "Thomas2012] Thomas M Cover", "Joy A Thomas" ],
      "venue" : null,
      "citeRegEx" : "Cover et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Cover et al\\.",
      "year" : 2012
    }, {
      "title" : "Recognizing textual entailment",
      "author" : [ "Dagan et al.2013] Ido Dagan", "Dan Roth", "Mark Sammons" ],
      "venue" : null,
      "citeRegEx" : "Dagan et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Dagan et al\\.",
      "year" : 2013
    }, {
      "title" : "Learning semantic hierarchies via word embeddings",
      "author" : [ "Fu et al.2014] Ruiji Fu", "Jiang Guo", "Bing Qin", "Wanxiang Che", "Haifeng Wang", "Ting Liu" ],
      "venue" : null,
      "citeRegEx" : "Fu et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Fu et al\\.",
      "year" : 2014
    }, {
      "title" : "Activating event knowledge",
      "author" : [ "Hare et al.2009] Mary Hare", "Michael Jones", "Caroline Thomson", "Sarah Kelly", "Ken McRae" ],
      "venue" : null,
      "citeRegEx" : "Hare et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Hare et al\\.",
      "year" : 2009
    }, {
      "title" : "Automatic acquisition of hyponyms from large text corpora",
      "author" : [ "Marti A Hearst" ],
      "venue" : null,
      "citeRegEx" : "Hearst.,? \\Q1992\\E",
      "shortCiteRegEx" : "Hearst.",
      "year" : 1992
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "Jürgen Schmidhuber" ],
      "venue" : "Neural computation",
      "citeRegEx" : "Hochreiter et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Hochreiter et al\\.",
      "year" : 1997
    }, {
      "title" : "Evaluating neighbor rank and distance measures as predictors of semantic priming",
      "author" : [ "Lapesa", "Evert2013] Gabriella Lapesa", "Stefan Evert" ],
      "venue" : "In ACL Workshop on Cognitive Modeling and Computational Linguistics (CMCL",
      "citeRegEx" : "Lapesa et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Lapesa et al\\.",
      "year" : 2013
    }, {
      "title" : "Measures of distributional similarity",
      "author" : [ "Lillian Lee" ],
      "venue" : null,
      "citeRegEx" : "Lee.,? \\Q1999\\E",
      "shortCiteRegEx" : "Lee.",
      "year" : 1999
    }, {
      "title" : "Dependency-based word embeddings",
      "author" : [ "Levy", "Goldberg2014] Omer Levy", "Yoav Goldberg" ],
      "venue" : null,
      "citeRegEx" : "Levy et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Levy et al\\.",
      "year" : 2014
    }, {
      "title" : "Do supervised distributional methods really learn lexical inference relations",
      "author" : [ "Levy et al.2015] Omer Levy", "Steffen Remus", "Chris Biemann", "Ido Dagan" ],
      "venue" : null,
      "citeRegEx" : "Levy et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Levy et al\\.",
      "year" : 2015
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Ilya Sutskever", "Kai Chen", "Gregory S Corrado", "Jeffrey Dean" ],
      "venue" : null,
      "citeRegEx" : "Mikolov et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Patty: a taxonomy of relational patterns with semantic types",
      "author" : [ "Gerhard Weikum", "Fabian Suchanek" ],
      "venue" : "In EMNLP and CoNLL",
      "citeRegEx" : "Nakashole et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Nakashole et al\\.",
      "year" : 2012
    }, {
      "title" : "Reading between the lines: Overcoming data sparsity for accurate classification of lexical relationships. *SEM",
      "author" : [ "Núria Bel", "Sara Mendes", "David Jurgens", "Roberto Navigli" ],
      "venue" : null,
      "citeRegEx" : "Necşulescu et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Necşulescu et al\\.",
      "year" : 2015
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Richard Socher", "Christopher D. Manning" ],
      "venue" : null,
      "citeRegEx" : "Pennington et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Inclusive yet selective: Supervised distributional hypernymy detection",
      "author" : [ "Katrin Erk", "Gemma Boleda" ],
      "venue" : "In COLING",
      "citeRegEx" : "Roller et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Roller et al\\.",
      "year" : 2014
    }, {
      "title" : "Introduction to modern information retrieval",
      "author" : [ "Salton", "McGill1986] Gerard Salton", "Michael J McGill" ],
      "venue" : null,
      "citeRegEx" : "Salton et al\\.,? \\Q1986\\E",
      "shortCiteRegEx" : "Salton et al\\.",
      "year" : 1986
    }, {
      "title" : "Evalution 1.0: an evolving semantic dataset for training and evaluation of distributional semantic models",
      "author" : [ "Santus et al.2015] Enrico Santus", "Frances Yung", "Alessandro Lenci", "Chu-Ren Huang" ],
      "venue" : null,
      "citeRegEx" : "Santus et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Santus et al\\.",
      "year" : 2015
    }, {
      "title" : "2016a. Testing apsyn against vector cosine on similarity estimation. PACLIC",
      "author" : [ "Emmanuele Chersoni", "Alessandro Lenci", "Chu-Ren Huang", "Philippe Blache" ],
      "venue" : null,
      "citeRegEx" : "Santus et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Santus et al\\.",
      "year" : 2016
    }, {
      "title" : "2016b. Nine features in a random forest to learn taxonomical semantic relations",
      "author" : [ "Alessandro Lenci", "Tin-Shing Chiu", "Qin Lu", "Chu-Ren Huang" ],
      "venue" : null,
      "citeRegEx" : "Santus et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Santus et al\\.",
      "year" : 2016
    }, {
      "title" : "Path-based vs. distributional information in recognizing lexical semantic relations",
      "author" : [ "Shwartz", "Dagan2016] Vered Shwartz", "Ido Dagan" ],
      "venue" : "Proceedings of the 5th Workshop on Cognitive Aspects of the Lexicon (CogALex-V),",
      "citeRegEx" : "Shwartz et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Shwartz et al\\.",
      "year" : 2016
    }, {
      "title" : "Improving hypernymy detection with an integrated path-based and distributional method",
      "author" : [ "Yoav Goldberg", "Ido Dagan" ],
      "venue" : null,
      "citeRegEx" : "Shwartz et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Shwartz et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning syntactic patterns for automatic hypernym discovery",
      "author" : [ "Snow et al.2004] Rion Snow", "Daniel Jurafsky", "Andrew Y Ng" ],
      "venue" : null,
      "citeRegEx" : "Snow et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Snow et al\\.",
      "year" : 2004
    }, {
      "title" : "From frequency to meaning: Vector space models of semantics. JAIR",
      "author" : [ "Patrick Pantel" ],
      "venue" : null,
      "citeRegEx" : "Turney and Pantel,? \\Q2010\\E",
      "shortCiteRegEx" : "Turney and Pantel",
      "year" : 2010
    }, {
      "title" : "Learning to distinguish hypernyms and co-hyponyms",
      "author" : [ "Weeds et al.2014] Julie Weeds", "Daoud Clarke", "Jeremy Reffin", "David Weir", "Bill Keller" ],
      "venue" : "In COLING",
      "citeRegEx" : "Weeds et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Weeds et al\\.",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "Discovering whether words are semantically related and identifying the specific semantic relation that holds between them is a key component in many NLP applications, such as question answering and recognizing textual entailment (Dagan et al., 2013).",
      "startOffset" : 229,
      "endOffset" : 249
    }, {
      "referenceID" : 1,
      "context" : "The conflict between the mediocre performance and the recent success of distributional methods on several other common datasets for semantic relation classification (Baroni et al., 2012; Weeds et al., 2014; Roller et al., 2014) could be explained by the strict evaluation setup in this subtask, which is supposed to demonstrate more closely real-world application.",
      "startOffset" : 165,
      "endOffset" : 227
    }, {
      "referenceID" : 25,
      "context" : "The conflict between the mediocre performance and the recent success of distributional methods on several other common datasets for semantic relation classification (Baroni et al., 2012; Weeds et al., 2014; Roller et al., 2014) could be explained by the strict evaluation setup in this subtask, which is supposed to demonstrate more closely real-world application.",
      "startOffset" : 165,
      "endOffset" : 227
    }, {
      "referenceID" : 16,
      "context" : "The conflict between the mediocre performance and the recent success of distributional methods on several other common datasets for semantic relation classification (Baroni et al., 2012; Weeds et al., 2014; Roller et al., 2014) could be explained by the strict evaluation setup in this subtask, which is supposed to demonstrate more closely real-world application.",
      "startOffset" : 165,
      "endOffset" : 227
    }, {
      "referenceID" : 5,
      "context" : "Many other measures exist, including but not limited to Euclidean distance, KL divergence (Cover and Thomas, 2012), Jaccard’s coefficient (Salton and McGill, 1986), and more recently neighbor rank (Hare et al., 2009; Lapesa and Evert, 2013) and APSyn (Santus et al.",
      "startOffset" : 197,
      "endOffset" : 240
    }, {
      "referenceID" : 1,
      "context" : "Most corpus-based methods classify the relation between a pair of words x and y based on the distributional representation of each word (Baroni et al., 2012; Roller et al., 2014; Fu et al., 2014; Weeds et al., 2014).",
      "startOffset" : 136,
      "endOffset" : 215
    }, {
      "referenceID" : 16,
      "context" : "Most corpus-based methods classify the relation between a pair of words x and y based on the distributional representation of each word (Baroni et al., 2012; Roller et al., 2014; Fu et al., 2014; Weeds et al., 2014).",
      "startOffset" : 136,
      "endOffset" : 215
    }, {
      "referenceID" : 4,
      "context" : "Most corpus-based methods classify the relation between a pair of words x and y based on the distributional representation of each word (Baroni et al., 2012; Roller et al., 2014; Fu et al., 2014; Weeds et al., 2014).",
      "startOffset" : 136,
      "endOffset" : 215
    }, {
      "referenceID" : 25,
      "context" : "Most corpus-based methods classify the relation between a pair of words x and y based on the distributional representation of each word (Baroni et al., 2012; Roller et al., 2014; Fu et al., 2014; Weeds et al., 2014).",
      "startOffset" : 136,
      "endOffset" : 215
    }, {
      "referenceID" : 6,
      "context" : "Earlier methods utilized the dependency paths that connect the co-occurrences of x and y in the corpus as a cue to the relation between the words (Hearst, 1992; Snow et al., 2004; Nakashole et al., 2012).",
      "startOffset" : 146,
      "endOffset" : 203
    }, {
      "referenceID" : 23,
      "context" : "Earlier methods utilized the dependency paths that connect the co-occurrences of x and y in the corpus as a cue to the relation between the words (Hearst, 1992; Snow et al., 2004; Nakashole et al., 2012).",
      "startOffset" : 146,
      "endOffset" : 203
    }, {
      "referenceID" : 13,
      "context" : "Earlier methods utilized the dependency paths that connect the co-occurrences of x and y in the corpus as a cue to the relation between the words (Hearst, 1992; Snow et al., 2004; Nakashole et al., 2012).",
      "startOffset" : 146,
      "endOffset" : 203
    }, {
      "referenceID" : 14,
      "context" : "Recently, Shwartz and Dagan (2016) presented LexNET, a method that integrates both path-based and distributional information for semantic relation classification, which outperformed individual approaches on several common datasets (Baroni and Lenci, 2011; Necşulescu et al., 2015; Santus et al., 2015; Santus et al., 2016b).",
      "startOffset" : 231,
      "endOffset" : 323
    }, {
      "referenceID" : 18,
      "context" : "Recently, Shwartz and Dagan (2016) presented LexNET, a method that integrates both path-based and distributional information for semantic relation classification, which outperformed individual approaches on several common datasets (Baroni and Lenci, 2011; Necşulescu et al., 2015; Santus et al., 2015; Santus et al., 2016b).",
      "startOffset" : 231,
      "endOffset" : 323
    }, {
      "referenceID" : 0,
      "context" : "Most corpus-based methods classify the relation between a pair of words x and y based on the distributional representation of each word (Baroni et al., 2012; Roller et al., 2014; Fu et al., 2014; Weeds et al., 2014). Earlier methods utilized the dependency paths that connect the co-occurrences of x and y in the corpus as a cue to the relation between the words (Hearst, 1992; Snow et al., 2004; Nakashole et al., 2012). Recently, Shwartz and Dagan (2016) presented LexNET, a method that integrates both path-based and distributional information for semantic relation classification, which outperformed individual approaches on several common datasets (Baroni and Lenci, 2011; Necşulescu et al.",
      "startOffset" : 137,
      "endOffset" : 457
    }, {
      "referenceID" : 21,
      "context" : "Dependency paths are embedded using a LSTM (Hochreiter and Schmidhuber, 1997), as described in Shwartz et al. (2016). This vector is then fed into a neural network that outputs the class distribution ~c, and the pair is classified to the relation with the highest score r:",
      "startOffset" : 95,
      "endOffset" : 117
    }, {
      "referenceID" : 9,
      "context" : "See Lee (1999) for an extensive list of such measures.",
      "startOffset" : 4,
      "endOffset" : 15
    }, {
      "referenceID" : 6,
      "context" : "1 A Note About Word Relatedness While path-based approaches have been commonly used for semantic relation classification (Hearst, 1992; Snow et al., 2004; Nakashole et al., 2012; Necşulescu et al., 2015), they are never used for word relatedness, which is considered a “classical” task for distributional methods.",
      "startOffset" : 121,
      "endOffset" : 203
    }, {
      "referenceID" : 23,
      "context" : "1 A Note About Word Relatedness While path-based approaches have been commonly used for semantic relation classification (Hearst, 1992; Snow et al., 2004; Nakashole et al., 2012; Necşulescu et al., 2015), they are never used for word relatedness, which is considered a “classical” task for distributional methods.",
      "startOffset" : 121,
      "endOffset" : 203
    }, {
      "referenceID" : 13,
      "context" : "1 A Note About Word Relatedness While path-based approaches have been commonly used for semantic relation classification (Hearst, 1992; Snow et al., 2004; Nakashole et al., 2012; Necşulescu et al., 2015), they are never used for word relatedness, which is considered a “classical” task for distributional methods.",
      "startOffset" : 121,
      "endOffset" : 203
    }, {
      "referenceID" : 14,
      "context" : "1 A Note About Word Relatedness While path-based approaches have been commonly used for semantic relation classification (Hearst, 1992; Snow et al., 2004; Nakashole et al., 2012; Necşulescu et al., 2015), they are never used for word relatedness, which is considered a “classical” task for distributional methods.",
      "startOffset" : 121,
      "endOffset" : 203
    }, {
      "referenceID" : 18,
      "context" : "0 (Santus et al., 2015), which was split into training and test sets.",
      "startOffset" : 2,
      "endOffset" : 23
    }, {
      "referenceID" : 15,
      "context" : "(2016), we initialized the network’s word embeddings with the 50-dimensional pre-trained GloVe word embeddings (Pennington et al., 2014), trained on Wikipedia.",
      "startOffset" : 111,
      "endOffset" : 136
    }, {
      "referenceID" : 17,
      "context" : "0 (Santus et al., 2015), which was split into training and test sets. As instructed, we trained and tuned our method on the training set, and evaluated it once on the test set. To tune the hyper-parameters, we split the training set to 90% train and 10% validation sets. Since the dataset contains only 318 different words (in the x slot), we performed the split such that the train and the validation contain distinct x words.2 LexNET has several tunable hyper-parameters. As an underlying corpus, we used the Wikipedia corpus from Shwartz and Dagan (2016). Similarly to Shwartz et al.",
      "startOffset" : 3,
      "endOffset" : 558
    }, {
      "referenceID" : 17,
      "context" : "0 (Santus et al., 2015), which was split into training and test sets. As instructed, we trained and tuned our method on the training set, and evaluated it once on the test set. To tune the hyper-parameters, we split the training set to 90% train and 10% validation sets. Since the dataset contains only 318 different words (in the x slot), we performed the split such that the train and the validation contain distinct x words.2 LexNET has several tunable hyper-parameters. As an underlying corpus, we used the Wikipedia corpus from Shwartz and Dagan (2016). Similarly to Shwartz et al. (2016), we initialized the network’s word embeddings with the 50-dimensional pre-trained GloVe word embeddings (Pennington et al.",
      "startOffset" : 3,
      "endOffset" : 594
    }, {
      "referenceID" : 15,
      "context" : "(2016), we initialized the network’s word embeddings with the 50-dimensional pre-trained GloVe word embeddings (Pennington et al., 2014), trained on Wikipedia. We fixed this hyper-parameter due to computational limitations with higher-dimensional embeddings. For each subtask, we tuned LexNET’s hyper-parameters on the validation set: the number of hidden layers (0 or 1), the number of training epochs, and the word dropout rate (see Shwartz et al. (2016) for technical details).",
      "startOffset" : 112,
      "endOffset" : 457
    }, {
      "referenceID" : 11,
      "context" : "A random split yielded perfect results on the validation set, which were due to lexical memorization (Levy et al., 2015).",
      "startOffset" : 101,
      "endOffset" : 120
    }, {
      "referenceID" : 12,
      "context" : "word2vec (300 dimensions, SGNS, trained on GoogleNews) (Mikolov et al., 2013), GloVe (50-300 dimensions, trained on Wikipedia) (Pennington et al.",
      "startOffset" : 55,
      "endOffset" : 77
    }, {
      "referenceID" : 15,
      "context" : ", 2013), GloVe (50-300 dimensions, trained on Wikipedia) (Pennington et al., 2014), and dependency-based embeddings (Levy and Goldberg, 2014).",
      "startOffset" : 57,
      "endOffset" : 82
    }, {
      "referenceID" : 1,
      "context" : "We experimented with several combination methods (concatenation (Baroni et al., 2012), difference (Fu et al.",
      "startOffset" : 64,
      "endOffset" : 85
    }, {
      "referenceID" : 4,
      "context" : ", 2012), difference (Fu et al., 2014; Weeds et al., 2014), and ASYM (Roller et al.",
      "startOffset" : 20,
      "endOffset" : 57
    }, {
      "referenceID" : 25,
      "context" : ", 2012), difference (Fu et al., 2014; Weeds et al., 2014), and ASYM (Roller et al.",
      "startOffset" : 20,
      "endOffset" : 57
    }, {
      "referenceID" : 16,
      "context" : ", 2014), and ASYM (Roller et al., 2014)), regularization factors, and pre-trained word embeddings (Mikolov et al.",
      "startOffset" : 18,
      "endOffset" : 39
    }, {
      "referenceID" : 12,
      "context" : ", 2014)), regularization factors, and pre-trained word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014).",
      "startOffset" : 66,
      "endOffset" : 138
    }, {
      "referenceID" : 15,
      "context" : ", 2014)), regularization factors, and pre-trained word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014).",
      "startOffset" : 66,
      "endOffset" : 138
    }, {
      "referenceID" : 15,
      "context" : "To compare LexNET’s performance on the validation set with other methods’ performances, we adapted the distributional baseline employed by Shwartz et al. (2016) and Shwartz and Dagan (2016), where a classifier is trained on the combination of x and y’s word embeddings.",
      "startOffset" : 139,
      "endOffset" : 161
    }, {
      "referenceID" : 15,
      "context" : "To compare LexNET’s performance on the validation set with other methods’ performances, we adapted the distributional baseline employed by Shwartz et al. (2016) and Shwartz and Dagan (2016), where a classifier is trained on the combination of x and y’s word embeddings.",
      "startOffset" : 139,
      "endOffset" : 190
    }, {
      "referenceID" : 1,
      "context" : "Subtask 2: Semantic Relation Classification We note that the overall results on this task are low, in contrast to the success of several methods on common datasets (Baroni et al., 2012; Weeds et al., 2014; Roller et al., 2014; Shwartz and Dagan, 2016).",
      "startOffset" : 164,
      "endOffset" : 251
    }, {
      "referenceID" : 25,
      "context" : "Subtask 2: Semantic Relation Classification We note that the overall results on this task are low, in contrast to the success of several methods on common datasets (Baroni et al., 2012; Weeds et al., 2014; Roller et al., 2014; Shwartz and Dagan, 2016).",
      "startOffset" : 164,
      "endOffset" : 251
    }, {
      "referenceID" : 16,
      "context" : "Subtask 2: Semantic Relation Classification We note that the overall results on this task are low, in contrast to the success of several methods on common datasets (Baroni et al., 2012; Weeds et al., 2014; Roller et al., 2014; Shwartz and Dagan, 2016).",
      "startOffset" : 164,
      "endOffset" : 251
    }, {
      "referenceID" : 11,
      "context" : "5 Additionally, the dataset is lexically split, disabling lexical memorization (Levy et al., 2015).",
      "startOffset" : 79,
      "endOffset" : 98
    }, {
      "referenceID" : 21,
      "context" : "Moreover, synonyms were also sometimes mistaken with hypernyms, as the difference between the two relations is often subtle (Shwartz et al., 2016).",
      "startOffset" : 124,
      "endOffset" : 146
    } ],
    "year" : 2017,
    "abstractText" : "We present a submission to the CogALex 2016 shared task on the corpus-based identification of semantic relations, using LexNET (Shwartz and Dagan, 2016), an integrated path-based and distributional method for semantic relation classification. The reported results in the shared task bring this submission to the third place on subtask 1 (word relatedness), and the first place on subtask 2 (semantic relation classification), demonstrating the utility of integrating the complementary path-based and distributional information sources in recognizing semantic relatedness. Combined with a common similarity measure, LexNET performs fairly good on the word relatedness task (subtask 1). The relatively lower performance of LexNET and the various other systems on subtask 2, however, confirms the difficulty of the semantic relation classification task, and stresses the need to develop additional methods for this task.",
    "creator" : "LaTeX with hyperref package"
  }
}