{
  "name" : "1606.07230.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Deep Learning Markov Random Field for Semantic Segmentation",
    "authors" : [ "Ziwei Liu", "Xiaoxiao Li", "Ping Luo", "Chen Change Loy", "Xiaoou Tang" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Index Terms—Semantic Image/Video Segmentation, Markov Random Field, Convolutional Neural Network.\nF"
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "S EMANTIC segmentation is a fundamental and long-standingproblem in computer vision. It is defined as a multi-label classification problem, aiming to assign each pixel with a category label. There are two widely adopted research realms, namely semantic image segmentation [1] and semantic video segmentation [2], with static image and video sequence as input respectively. The obtained per-pixel segmentation results are extremely useful for several applications like smart editing [3], scene understanding [4] and automated driving [5].\nSince pixels in natural images or videos generally exhibit strong correlation, jointly modeling label distribution in all locations is desirable. To capture these contextual information, Markov random field (MRF) and conditional random field (CRF) [6] are commonly used as classic frameworks for semantic segmentation. They model the joint distribution of labels by defining both unary term and pairwise terms. Unary term reflects the per-pixel confidence of assigning labels while pairwise terms capture the inter-pixel constraints.\nMost previous studies focus on designing pairwise terms that possess strong expressive power. For example, Krähenbühl et al. [7] attained accurate segmentation boundary by inferring on a fully-connected graph. Vineet et al. [8] extended [7] by defining both high-order and long-range terms between pixels. Global or local semantic contexts between labels were also investigated by [9]. However, their performance are limited by the relatively shallow models (e.g. SVM or Adaboost) used as unary term. As deep learning gradually takes over in many image recognition fields [10], researchers have also explored the possibility of designing effective deep architecture for semantic segmentation. For instance, Long et al. [11] transformed fully-connected layers\n• Z. Liu, X. Li, P. Luo, C. C. Loy and X. Tang are with the Department of Information Engineering, The Chinese University of Hong Kong. E-mail: {lz013, lx015, pluo, ccloy, xtang}@ie.cuhk.edu.hk • * indicates shared first authorship.\nof CNN into convolutional layers, making accurate per-pixel classification possible using contemporary CNN architectures that were pre-trained on ImageNet [12]. Chen et al. [13] improved [11] by feeding the outputs of CNN into a MRF with simple pairwise potentials, but it treated CNN and MRF as separate components. A recent advance was made in joint training CNN and MRF by passing the error of MRF inference backward into CNN [14]. Nonetheless, an iterative inference of MRF such as the mean field algorithm (MF) [15] is required for each training image during the back-propagation (BP). Zheng et al. [16] further showed that the procedure of MF inference can be represented as a Recurrent Neural Network (RNN), but their computational costs are similar to that of [14].\nWe observed that a direct combination of CNN and MRF as above is inefficient, because CNN typically has millions of parameters while MRF infers thousands of latent variables; and even worse, incorporating complex pairwise terms into a MRF becomes impractical, limiting the performance of the entire system. In this study, we propose a novel Deep Parsing Network (DPN), which is an end-to-end system enabling jointly training of CNN and complex pairwise terms. DPN has several appealing properties:\n(1) DPN solves MRF with a single feed-forward pass, reducing computational cost and meanwhile maintaining high performance. Specifically, DPN models unary terms by extending the VGG-16 network (VGG16) [10] pre-trained on ImageNet, while additional layers are carefully designed to model complex pairwise terms. The learning of these terms is transformed into deterministic endto-end computation by BP, instead of embedding MF into BP as [14], [17] did. Although MF can be represented by a RNN [16], it needs to recurrently compute the forward pass so as to achieve good performance and thus the process is time-consuming, e.g. each forward pass contains hundred thousand of weights. DPN approximates MF by using only one iteration of inference. This\nar X\niv :1\n60 6.\n07 23\n0v 1\n[ cs\n.C V\n] 2\n3 Ju\nn 20\n16\n2 is made possible by joint learning strong unary terms and rich pairwise information.\n(2) Pairwise terms determine the graphical structure. In previous studies, if the former is changed, so is the latter as well as its inference procedure. But with DPN, modifying the complexity of pairwise terms, e.g. range of pixels and contexts, is as simple as modifying the receptive fields of convolutions, without varying BP. Furthermore, DPN is capable of representing multiple types of pairwise terms, making many previous works [13], [14], [16] as its special cases.\n(3) DPN approximates MF with convolutional and pooling operations, which can be speeded up by low-rank approximation [18] and easily parallelized [19] in a Graphical Processing Unit (GPU).\nOur contributions are summarized as below. (1) We propose the novel DPN to jointly train VGG16 for unary terms with rich pairwise information, i.e. mixture of label contexts and highorder relations. In comparison to existing deep models, DPN approximates MF with only one iteration of inference, reducing computational cost but still maintaining high performance. (2) We show that multiple types of MRFs can be represented in DPN, making many previous works such as RNN [16] and DeepLab [13] as its special cases. (3) We conduct extensive experiments to investigate which component of DPN is crucial to achieve high performance. We demonstrate the generalizability of DPN model by showing its state-of-the-art performance on several standard semantic image/video segmentation benchmarks, including PASCAL VOC 2012 [20], CityScapes dataset [5] and CamVid dataset [21].\nIn comparison to our earlier version of this work [1], we propose a generic deep learning framework, Deep Parsing Network (DPN) to model and solve N -D high-order Markov Random Field (MRF). Our previous study [1] only shows the possibility on 2-D image segmentation problem. Specifically, we employ dynamic node linking to construct graph in N -D space, which results in a model of N -D high-order MRF. To solve this highdimensional and high-order MRF, we re-formulate the mean field (MF) update process into a feed-forward pass of Convolutional Neural Network (CNN). N -D local and global convolutional layers are designed to approximate different terms in a MF solver. Apart from the methodology, the paper was also substantially improved by providing more technical details and more extensive experimental evaluations."
    }, {
      "heading" : "2 RELATED WORK",
      "text" : "Existing studies [4], [11], [22], [23], [24], [25], [26], [27], [28] on semantic segmentation focus on either constructing specific graph structure so that contextual information and long-term dependencies can be captured, or designing suitable network architecture to leverage the power of deep learning. In the following, we summarize recent research advances with respect to these two aspects.\nMarkov Random Field. Markov Random Field (MRF) or Conditional Random Field (CRF) has achieved great successes in semantic image segmentation, which is one of the most challenging problems in computer vision. Researchers improved labeling accuracy by exploring rich information to define the pairwise functions, including long-range dependencies [7], [29], high-order potentials [8], [30], and semantic label contexts [9], [31], [32]. For example, Krähenbühl et al. [7] attained accurate\nsegmentation boundary by inferring on a fully-connected graph. Vineet et al. [8] extended [7] by defining both high-order and longrange terms between pixels. Global or local semantic contexts between labels were also investigated by [9]. Although they accomplished promising results, they modeled the unary terms as SVM or Adaboost, whose learning capacity becomes a bottleneck. The learning and inference of complex pairwise terms are often expensive.\nMRF and CRF have also been utilized in semantic video segmentation by extending their graph structure to spatio-temporal domain. For example, Wang et al. [33] unified foreground object segmentation, tracking and occlusion reasoning into a carefully designed MRF model. Optical flow based long-term trajectories [2] were also exploited to discover moving objects. Liu et al. [34] employed fully-connected CRF augmented with object potentials for efficient multi-class inference. However, these methods are based on hand-crafted features, thus lacking suffcient learning capacity. Convolutional Neural Network. More recently, Convolutional Neural Network (CNN) has been leveraged as a strong unary classifier. With deep models, exising works [11], [13], [14], [16], [17], [28], [35], [36], [37] demonstrated encouraging segmentation results through using just simple definition of the pairwise function or even neglecting it. For instance, Long et al. [11] transformed fully-connected layers of CNN into convolutional layers, making accurate per-pixel classification possible using the contemporary CNN architectures that were pre-trained on ImageNet [12]. Chen et al. [13] improved [11] by feeding the outputs of CNN into a MRF with simple pairwise potentials, but it treated CNN and MRF as separated components. A recent advance was obtained by [14], which jointly trained CNN and MRF by passing the error of MRF inference backward into CNN, but iterative inference of MRF such as the mean field algorithm (MF) [15] is required for each training image during the back-propagation (BP). Zheng et al. [16] further showed that the procedure of MF inference can be represented as a Recurrent Neural Network (RNN), but their computational costs are similar to that of [14].\nLittle attempts have been made to develop unified deep learning framework for semantic video segmentation. Recent efforts in this direction include SegNet [38], which adopted an encoderdecoder architecture but did not take temporal relationships into consideration. Here we extend DPN to further include temporal voxels into the joint learning and inference process, which results in an end-to-end trainable system with rich spatio-temporal information encoded."
    }, {
      "heading" : "3 OUR APPROACH",
      "text" : "We develop a unified framework, DPN, for modeling and solving high-order MRF. The architecture of DPN is shown in Fig. 1(a). Our model imposes no restrictions on the dimension of MRF considered. It can be either 2-D (e.g. images), 3-D (e.g. videos) or N -D (e.g. sensor data). DPN learns MRF by extending VGG16 to model unary terms and additional layers are carefully designed for pairwise terms. In the following, we describe the formulation of DPN in terms of 3-D MRF. Note that it can be easily resorted to 2-D cases or extended to N -D cases by removing or adding the node linking.\nMarkov Random Field. MRF [39] is an undirected graph where each node represents a voxel in a video I, and each edge represents relation between voxels, as shown in Fig. 1(b). Each node is\n3 \uD835\uDC56 \uD835\uDC57(b) (c) (a)\n\uD835\uDC56 \uD835\uDC57\nUnary Term Pairwise Term\nTriple Penalty Label Contexts\nMax PoolingLocal ConvolutionConvolution Deconvolution Min Pooling\n\uD835\uDC61\nFig. 1: (a) The network architecture of a deep parsing network (DPN). (b) DPN extends a contemporary CNN architecture to model unary terms and additional layers are carefully devised to approximate the mean field algorithm (MF) for pairwise terms. (c) DPN enables dynamic linking of nodes in Markov Random Field (MRF) by incorporating domain knowledge.\nassociated with a binary latent variable, yui ∈ {0, 1}, indicating whether a voxel i = [i ti] has label u, where i is its spatial index and ti is its temporal index. We have ∀u ∈ L = {1, 2, ..., l}, representing a set of l labels. The energy function of MRF is written as\nE(y) = ∑ ∀i∈V Φ(yui ) + ∑ ∀(i,j)∈E Ψ(yui , y v j ), (1)\nwhere y, V , and E denote a set of latent variables, nodes, and edges, respectively. Φ(yui ) is the unary term, measuring the cost of assigning label u to voxel i. For instance, if voxel i belongs to the first category other than the second one, we should have Φ(y1i ) < Φ(y 2 i ). Moreover, Ψ(y u i , y v j ) is the pairwise term that measures the penalty of assigning labels u, v to voxel pair (i, j) respectively.\nDynamic Node Linking. Traditional approaches [40] usually define the edges E on rectangular grid in 3-D space. However, when large motion exists, the actual temporal trajectory for certain pixel will not reside inside a rigid cube, as assumed by this rectangular grid structure. To better preserve the contextual information in a spatio-temporal space, we employ dynamic node linking to construct edges E in DPN. Specifically, we keep the 2-D structure in the spatial domain, as illustrated in Fig. 1(c). In the temporal domain, the neighboring voxels i = [i ti] and j = [j tj ] are defined as those lie on the same temporal trajectories ∆i→j, which can be estimated by standard optical flow techniques [41]. As such, the formulation of edges Et in the temporal domain is\n(i, j) ∈ Et ⇐⇒ j = i + ∆i→j. (2)\nIn this setting, adjacent nodes in the temporal space would be more likely to belong to the same category. Moreover, the label contexts are also more localized to capture.\nUnary and Pairwise Terms. Intuitively, the unary terms represent per-voxel classifications, while the pairwise terms represent a set of smoothness constraints. The unary term in Eqn. (1) is typically defined as\nΦ(yui ) = − ln p(yui = 1|I), (3)\nwhere p(yui = 1|I) indicates the probability of the presence of label u at voxel i, modeling by VGG16. To simplify discussions, we abbreviate it as pui . The smoothness term can be formulated as\nΨ(yui , y v j ) = µ(u, v)d(i, j), (4)\nwhere the first term learns the penalty of global co-occurrence between any pair of labels, e.g. the output value of µ(u, v) is large if u and v should not coexist, while the second term calculates the distances between voxels, e.g.\nd(i, ti, j, tj) = ω1‖Ii − Ij‖2 + ω2‖[i ti]− [j tj ]‖2. (5)\nHere, Ii indicates a feature vector such as RGB values extracted from the input video, [i ti] denote coordinates of voxels’ positions, and ω1, ω2 are the constant weights. Eqn. (4) implies that if two voxels are close and look similar, they are encouraged to have labels that are compatible. This formulation has been adopted by most of the recent deep models [13], [14], [16] for semantic image segmentation.\nHowever, Eqn. (4) has two main drawbacks. First, its first term captures the co-occurrence frequency of two labels in the training data, but neglects the spatial context between objects. For example, ‘person’ may appear beside ‘table’, but not at its bottom. This spatial context is a mixture of patterns, as different object configurations may appear in different images. Second, it defines only the pairwise relations between pixels, missing their high-order interactions.\nTo resolve these issues, we define the smoothness term by leveraging rich information between voxels, which is one of the advantages of DPN over existing deep models. We have\nΨ(yui , y v j ) = K∑ k=1 λkµk(i, u, j, v) ∑ ∀z∈Nj d(j, z)pvz. (6)\nThe first term in Eqn. (6) learns a mixture of local label contexts, penalizing label assignment in a local cube, where K is the number of components in mixture and λk is an indicator, determining which component is activated. We define λk ∈ {0, 1} and ∑K k=1 λk = 1. An intuitive illustration is given in Fig. 2(a),\n4 (a) (b)\n(c) (d)\n\uD835\uDF07(i,\uD835\uDC62, j,\uD835\uDC63), j ∈ \uD835\uDC41i \uD835\uDC5B×\uD835\uDC5B×\uD835\uDC47\uD835\uDC5B\n(j,\uD835\uDC63), j ∈ \uD835\uDEEE\uD835\uDEEEi\n(i,\uD835\uDC62)\n�\uD835\uDC51(j, z)\uD835\uDC5Dz\uD835\uDC63 z∈\uD835\uDEEEj\n\uD835\uDF07\uD835\uDC58(i,\uD835\uDC62, j,\uD835\uDC63) (z,\uD835\uDC63)\n(j,\uD835\uDC63)\n(i,\uD835\uDC62)\n\uD835\uDC5A \uD835\uDC5A\n\uD835\uDC47\uD835\uDC5A\uD835\uDC44\uD835\uDC63 j\n\uD835\uDC51(j, z)\uD835\uDC5Ej\uD835\uDC63, z ∈ \uD835\uDC41j \uD835\uDC5A×\uD835\uDC5A×\uD835\uDC47\uD835\uDC5A\n\uD835\uDC5B \uD835\uDC5B\n\uD835\uDC47\uD835\uDC5B\uD835\uDC44\uD835\uDC63′ i\n\uD835\uDF07(i,\uD835\uDC62, j,\uD835\uDC63), j ∈ \uD835\uDC41i \uD835\uDC5B×\uD835\uDC5B×\uD835\uDC47\uD835\uDC5B\nFig. 2: (a) Illustration of the pairwise terms in DPN. (b) explains the label contexts. (c) and (d) show that the mean field update of DPN corresponds to convolutions.\nwhere the dots in red and blue represent a center voxel i and its neighboring voxels j, i.e. j ∈ Ni, and (i, u) indicates assigning label u to voxel i. Here, µ(i, u, j, v) outputs labeling cost between (i, u) and (j, v) with respect to their relative positions. For instance, if u, v represent ‘person’ and ‘table’, the learned penalties of positions j that are at the bottom of center i should be large. The second term of Eqn. (6) basically models a triple penalty, which involves voxels i, j, and j’s neighbors, implying that if (i, u) and (j, v) are compatible, then (i, u) should be also compatible with j’s nearby pixels (z, v), ∀z ∈ Nj, as shown in Fig. 2(b).\nLearning parameters (i.e. weights of VGG16 and costs of label contexts) in Eqn. (1) requires us to minimize the distances between ground-truth label map and y, which needs to be inferred subject to the smoothness constraints.\nInference Overview. Inference of Eqn. (1) can be obtained by the mean field (MF) algorithm [15], which estimates the joint distribution of MRF\nP (y) = 1\nZ exp{−E(y)}, (7)\nby using a fully-factorized proposal distribution Q(y) = ∏ ∀i∈V ∏ ∀u∈L qui , (8)\nwhere each qui is a variable we need to estimate, indicating the predicted probability of assigning label u to voxel i. The KullbackLeibler divergence between them is then calculated as:\nDKL(Q‖P ) = ∑ y Q(y) ln ( Q(y) P (y) ) = ∑ y Q(y)E(y) + ∑ y Q(y) lnQ(y) + lnZ,\n(9)\nSince lnZ is a constant, minimizing the Kullback-Leibler divergence between Q(y) and P (y) is equivalent to minimizing the\nformer terms in Eqn. (9), which is denoted as free energy F (Q) [6]. To simplify the discussion, we denote Φ(yui ) and Ψ(y u i , y v j ) as Φui and Ψ uv ij , respectively. And we can further substitute Eqn.(1) into F (Q):\nF (Q) = ∑ y Q(y)E(y) + ∑ y Q(y) lnQ(y)\n= ∑ ∀i∈V ∑ ∀u∈L qui Φ u i + ∑ ∀i,j∈E ∑ ∀u∈L ∑ ∀v∈L qui q v j Ψ uv ij\n+ ∑ ∀i∈V ∑ ∀u∈L qui ln q u i .\n(10)\nSpecifically, the first term in Eqn. (10) characterizes the cost of each voxel’s predictions, while the second term characterizes the consistencies of predictions between voxels. The last term denotes the entropy, measuring the confidences of predictions. Then, a constrained optimization problem regarding qui could be formulated:\nminimize qui F (Q) subject to ∑ u qui = 1, ∀i ∈ V, (11)\nTo solve this minimization problem, we define J(Q) = F (Q) +∑ i λi( ∑ u q u i − 1) by introducing Lagrange multipliers λi. The final closed-form solution can be obtained by differentiating J(Q) w.r.t. to qui and equating the resulting expression to zero:\nqui ∝ exp { − (Φui + ∑ ∀j∈Ni ∑ ∀v∈L qvj Ψ uv ij ) } , (12)\nsuch that the predictions for each voxel is independently attained by repeating Eqn. (12), which implies whether voxel i have label u is proportional to the estimated probabilities of all its neighboring voxels, weighted by their corresponding smoothness penalties. Substituting Eqn. (6) into Eqn. (12), we have\nqui ∝ exp { − Φui − K∑ k=1 λk ∑ ∀v∈L\n(13)\n∑ ∀j∈Ni µk(i, u, j, v) ∑ ∀z∈Nj d(j, z)qvj q v z } ,\nwhere each qui is initialized by the corresponding p u i in Eqn. (3), which is the unary prediction of VGG16. Eqn. (13) satisfies the smoothness constraints.\nIn the following, DPN approximates one iteration of Eqn. (13) by decomposing it into two steps. Let Qv be a predicted label map of the v-th category. In the first step as shown in Fig. 2(c), we calculate the triple penalty term in Eqn. (13) by applying a m × m×Tm filter on each position j, where each element of this filter equals d(j, z)qvj , resulting in Q\nv ′. Apparently, this step smoothes the prediction of voxel j with respect to the distances between it and its neighborhood. In the second step as illustrated in Fig. 2(d), the labeling contexts can be obtained by convolving Qv ′ with a n × n × Tn filter, each element of which equals µk(i, u, j, v), penalizing the triple relations as shown in Fig. 2(a).\nFig. 3 depicts the semantic meaning of the triple penalty and the mixture of local label contexts term in the spatial-temporal domain. From Fig. 3(a) we can see that the triple penalty term tracks the movement of local pixels, such as ‘pole’, ‘car’ and ‘road’. These temporal trajectories combined with local regions are subsequently used to smooth original predictions. Fig. 3(b) demonstrates that a mixture of local label contexts term not only\n5 Car Pole\nRoad\nSky\nTree\nPedestrian\nRoad\n(a) (b)\n\uD835\uDC61 = 1,2, … ,\uD835\uDC47\uD835\uDC5A \uD835\uDC61 = 1,2, … ,\uD835\uDC47\uD835\uDC5B\nFig. 3: Illustrative depiction of (a) triple penalty term and (b) mixture of local label contexts term.\ncaptures label co-occurrence in a single image, but also encodes the change of label configurations along time. For example, as the observer vehicle drives, ‘tree’ will move backward and get nearer to ‘road’."
    }, {
      "heading" : "4 DEEP PARSING NETWORK",
      "text" : "This section describes the implementation of Eqn. (13) in a Deep Parsing Network (DPN). DPN extends VGG16 to model the unary term and with additional layers to approximate one iteration of MF inference as the pairwise term. The hyper-parameters of VGG16 and DPN are compared in Table 1.\nAs listed in Table 1, the first row represents the name of layer and ‘x-y’ in the second row represents the size of the receptive field and the stride of convolution, respectively. For instance, ‘3-1’ in the convolutional layer implies that the receptive field of each filter is 3×3 and it is applied on every single pixel of an input feature map, while ‘2-2’ in the max-pooling layer indicates each feature map is pooled over every other pixel within a 2×2 local region. On the other hand, ‘3-50-1’ implies a 3D convolution of size 50×50×3, where ‘3’ suggests that this 3D filter is applied on three consecutive frames. The last three rows show the number of the output feature maps, activation functions, and the size of output feature maps, respectively. T represents the number of frames in the underlying video sequence.\nAs summarized in Table 1(a), VGG16 contains thirteen convolutional layers, five max-pooling layers, and three fully-connected layers. These layers can be partitioned into twelve groups, each of which covers one or more homogenous layers. For example, the first group comprises two convolutional layers with 3×3 receptive field and 64 output feature maps, each of which is 224×224."
    }, {
      "heading" : "4.1 Modeling Unary Terms",
      "text" : "To make full use of VGG16, which is pre-trained by ImageNet, we adopt all its parameters to initialize the filters of the first ten groups of DPN. To simplify the discussions, we take PASCAL VOC 2012 (VOC12) [20] as an example. Note that DPN can be easily adapted to any other semantic image segmentation dataset by modifying its hyper-parameters. VOC12 contains 21 categories and each image is rescaled to 512×512 in training. Therefore, DPN needs to predict a total of 512×512×21 labels, i.e. one label for each pixel. To this end, we extends VGG16 in two aspects.\nIn particular, let ai and bi denote the i-th group in Table 1(a) and (b), respectively. First, we increase the resolution of VGG16 by removing its max pooling layers at a8 and a10, since most of the information is lost after pooling, e.g. a10 reduces the\ninput size by 32 times, i.e. from 224×224 to 7×7. As a result, the smallest size of feature map in DPN is 64×64, keeping much more information compared with VGG16. Note that the filters of b8 are initialized as the filters of a9, but the 3×3 receptive field is padded into 5×5 as shown in Fig. 4(a), where the cells in white are the original values of the a9’s filter and the cells in gray are zeros. This step is performed because a8 is not presented in DPN, therefore each filter in a9 should be convolved on every other pixel of a7. To maintain the convolution with one stride, we pad the filters with zeros. Furthermore, the feature maps in b11 are up-sampled to 512×512 by bilinear interpolation. Since DPN is trained with label maps of the entire images, the missing information in the preceding layers of b11 can be recovered by BP. The supervision signals in the interpolated pixels will guide the feature learning for full-resolution images.\nSecond, two fully-connected layers at a11 are transformed into two convolutional layers at b9 and b10, respectively. As shown in Table 1(a), the first ‘fc’ layer learns 7×7×512×4096 parameters, which can be altered to 4096 filters in b9, each of which is 25×25×512. Since a8 and a10 have been removed, the 7×7 receptive field is padded into 25×25 similar as above and shown in Fig.4 (b). The second ‘fc’ layer learns a 4096×4096 weight matrix, corresponding to 4096 filters in b10. Each filter is 1×1×4096.\nOverall, b11 generates the unary labeling results, producing twenty-one 512×512 feature maps, each of which represents the probabilistic label map of each category."
    }, {
      "heading" : "4.2 Modeling Smoothness Terms",
      "text" : "The last four layers of DPN, i.e. from b12 to b15, are carefully designed to smooth the unary labeling results. • b12. As listed in Table 1 (b), ‘lconv’ in b12 indicates a 3D locally convolutional layer. A counterpart of it (i.e. 2D locally convolutional layer) is widely used in face recognition [43], [44], [45] to capture different information from different\n6\nTABLE 1: A comparison between the network architectures of VGG16 and DPN. (a) VGG16: 224×224×3 input image; 1×1000 output labels\n1 2 3 4 5 6 7 8 9 10 11 12 layer fi.-st. #ch. act. size 2×conv 3-1 64 relu 224 max 2-2 64 idn 112 2×conv 3-1 128 relu 112 max 2-2 128 idn 56 3×conv 3-1 256 relu 56 max 2-2 256 idn 28 3×conv 3-1 512 relu 28 max 2-2 512 idn 14 3×conv 3-1 512 relu 14 max 2-2 512 idn 7 2×fc - 1 relu 4096 fc - 1 soft 1000\n(b) DPN: T×512×512×3 input image; T×512×512×L output label maps 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15\nlayer fi.-st. #ch. act. size 2×conv 3-1 64 relu T -512\nmax 2-2 64 idn\nT -256 2×conv 3-1 128 relu T -256\nmax 2-2 128 idn\nT -128 3×conv 3-1 256 relu T -128 max 2-2 256 idn T -64 3×conv 3-1 512 relu T -64 3×conv 5-1 512 relu T -64\nconv 25-1 4096 relu T -64\nconv 1-1 4096 relu T -64 conv 1-1 L sigm T -512 lconv-3D 3-50-1 L lin T -512 conv-3D 3-9-1 L× k lin T -512 bmin 1-1 L idn T -512\nsum 1-1 L\nsoft T -512\nAs shown in (a) and (b) respectively. Each table contains five rows, ‘layer’, ‘fi.-st.’, ‘#ch.’, ‘act.’, and ‘size’ represent the ‘name of layer’, ‘receptive field of filter’−‘stride’, ‘number of output feature maps’, ‘activation function’, and ‘size of output feature maps’, respectively. Furthermore, ‘conv’, ‘lconv-3D’, ‘conv-3D’, ‘max’, ‘bmin’, ‘fc’, and ‘sum’ represent the convolution, 3D convolution, 3D local convolution, max pooling, block min pooling, fully connection, and summation, respectively. Moreover, ‘relu’, ‘idn’, ‘soft’, ‘sigm’, and ‘lin’ represent the activation functions, including rectified linear unit [42], identity, softmax, sigmoid, and linear, respectively. T , L, and k represent the length of frames, number of categories, and number of mixture filters.\nfacial positions. Similarly, distinct spatial positions of b12 have different filters, and each filter is shared across 21 input channels, as shown in Fig. 4(c). It can be formulated as\no12(j,v) = lin(k(j,v) ∗ o11(j,v)), (14)\nwhere lin(x) = ax+b representing the linear activation function, ‘∗’ is the convolutional operator, and k(j,v) is a 50×50×3×1 filter at position j of channel v. The choice and effect of filter size will be discussed in the experiments (Sec.5.1). We have k(j,1) = k(j,2) = ... = k(j,21) shared across 21 channels. o11(j,v) indicates a local cube in b11, while o12(j,v) is the corresponding output of b12. Since b12 has a stride of one, the result of kj ∗ o11(j,v) is scalar. In summary, b12 has 512×512 different filters and produces 21 output feature maps.\nEqn. (14) implements the triple penalty of Eqn. (13). Recall that each output feature map of b11 indicates a probabilistic label map of a specific object appearing in the frame. As a result, Eqn. (14) suggests that the probability of object v presented at position j is updated by weighted averaging over the probabilities at its nearby positions. Thus, as shown in Fig. 2(c), o11(j,v) corresponds to a cube of Qv centered at j, which has values pvz , ∀z ∈ N 50×50×3j . Similarly, k(j,v) is initialized by d(j, z)pvj , implying each filter captures dissimilarities between positions. These filters remain fixed during BP, other than learned as in conventional CNN1. • b13. As shown in Table 1(b) and Fig. 5(a), b13 is a 3D global convolutional layer that generates 105 feature maps by using 105 filters of size 9×9×3×21. For example, the value of (i, u = 1) is attained by applying a 9×9×3×21 filter at positions {(j, v = 1, ..., 21)}. In other words, b13 learns a filter for each category to penalize the probabilistic label maps of b12, corresponding to the local label contexts in Eqn.(13) by assuming K = 5 and n = 9, as shown in Fig.2 (d). • b14. As illustrated in Table 1 and Fig. 5(b), b14 is a block min pooling layer that pools over every 1×1 region with one stride\n1. Each filter in b12 actually represents a distance metric between pixels in a specific region. In VOC12, the patterns of all the training images in a specific region are heterogenous, because of various object shapes. Therefore, we initialize each filter with Euclidean distance. Nevertheless, Eqn. (14) is a more general form than the triple penalty in Eqn. (13), i.e. filters in Eqn. (14) can be automatically learned from data, if the patterns in a specific region are homogenous, such as face or human images, which have more regular shapes than images in VOC12.\nacross every 5 input channels, leading to 21 output channels, i.e. 105÷5=21. Layer b14 activates the contextual pattern with the smallest penalty. • b15. This layer combines both the unary and smoothness terms by summing the outputs of b11 and b14 in an element-wise manner similar to Eqn. (13),\no15(i,u) = exp\n{ ln(o11(i,u))− o14(i,u) }∑21 u=1 exp { ln(o11(i,u))− o14(i,u)\n} , (15) where probability of assigning label u to voxel i is normalized over all the labels.\nRelation to Previous Deep Models. Many existing deep models such as [13], [14], [16] employed Eqn. (4) as the pairwise terms, which are the special cases of Eqn. (13). To see this, let K = 1, j = i and omit t (i.e. tz = tj = ti), the right hand side of Eqn. (13) reduces to\nexp{−Φui − ∑ v∈L λ1µ1(i, u, i, v) ∑ z∈Ni d(i, z)pvi p v z}\n= exp{−Φui − ∑ v∈L µ(u, v) ∑ z∈Ni,z 6=i d(i, z)pvz}, (16)\nwhere µ(u, v) and d(i, z) represent the global label co-occurrence and pairwise pixel similarity of Eqn. (4), respectively. This is because λ1 is a constant, d(i, i) = 0, and µ(i, u, i, v) = µ(u, v). Eqn. (16) is the corresponding MF update equation of (4)."
    }, {
      "heading" : "4.3 Learning Algorithms",
      "text" : "We describe the training strategy of DPN and also its time complexity and efficient implementation.\n7\nLearning. The first ten groups of DPN are initialized by VGG162, while the last four groups can be initialized randomly. DPN is then fine-tuned in an incremental manner with four stages. During finetuning, all these stages solve the pixelwise softmax loss [11], but updating different sets of parameters.\nFirst, we add a loss function to b11 and fine-tune the weights from b1 to b11 without the last four groups, in order to learn the unary terms. Second, to learn the triple relations, we stack b12 on top of b11 and update its parameters (i.e. ω1, ω2 in the distance measure), but the weights of the preceding groups (i.e. b1∼b11) are fixed. Third, b13 and b14 are stacked onto b12 and similarly, their weights are updated with all the preceding parameters fixed, so as to learn the local label contexts. Finally, all the parameters are jointly fine-tuned.\nComplexity. DPN transforms Eqn. (13) into convolutions and poolings in the groups from b12 to b15, such that filtering at each pixel can be performed in a parallel manner. Assume we have f input and f ′ output feature maps, N × N pixels, filters with s× s receptive field, and a mini-batch with M samples. b12 takes a total f · N2 · s2 ·M operations, b13 takes f · f ′ · N2 · s2 ·M operations, while both b14 and b15 require f · N2 ·M operations. For example, when M=10 as in our experiment, we have 21×5122×502×10=1.3×1011 operations in b12, which has the highest complexity in DPN. If we parallelize these operations using matrix multiplication on GPU as [19] did, the operation in b12 can be computed within 30ms. The total runtime of the last four layers of DPN is 75ms. Note that convolutions in DPN can be further speeded up by low-rank decompositions [18] of the filters and model compressions [46].\nIn contrast, existing works [13], [16] employ fast Gaussian filtering [47] to accelerate the direct calculation of Eqn. (13). For a mini-batch of ten 512×512 images, a recently optimized implementation [7] takes 12 seconds on CPU to compute one iteration of (13). In DPN, simultaneous message passing is enabled over every training image, in the sense that parallelization is obtained in image level. Therefore, DPN makes (13) easier to be parallelized and speeded up.\nEfficient Implementation. As mentioned in Eqn. (4), the local filters in b12 are computed by the distances between RGB values of the pixels. XY coordinates are omitted here because they could be pre-computed. To accelerate the computation of local convolution, the lookup table-based filtering approach is employed. Specifically, we first construct a lookup table storing distances between any two pixel intensities (ranging from 0 to 255), which results in a 256 × 256 matrix. Then when we perform local convolution, the kernels’ coefficients can be obtained efficiently by just looking up the table."
    }, {
      "heading" : "5 EXPERIMENTS",
      "text" : "In this section, we demonstrate the effectiveness of DPN and benchmark it against other state-of-the-art semantic segmentation methods. Below, we give an overview of the dataset and evaluation metrics used in these experiments. Representative methods are also introduced. In the following experiments, we denote 2-D DPN as DPN and 3-D DPN as spatial-temporal DPN.\nDataset. We compare DPN with the state-of-the-art methods on PASCAL VOC 2012 (VOC12) [20] , Cityscapes [5] and CamVid\n2. We use the released VGG16 model, which is publicly available at http: //www.robots.ox.ac.uk/∼vgg/research/very deep/\n[48] datasets. VOC12 is a well-known benchmark for generic image segmentation and Cityscapes dataset focuses on parsing urban street scenes. We choose those two benchmarks to evaluate the original DPN. On the other hand, CamVid dataset is composed of several video sequences, which is suitable for the evaluation of spatial-temporal DPN. We summarize the information of all datasets we used in Table 2.\nEvaluation Metrics. All existing works employed mean pixelwise intersection-over-union (denoted as mIoU) [11] to evaluate their performance. To fully examine the effectiveness of DPN, we introduce another three metrics, including tagging accuracy (TA), localization accuracy (LA), and boundary accuracy (BA). (1) TA compares the predicted image-level tags with the ground truth tags, calculating the accuracy of multi-class image classification. (2) LA evaluates the IoU between the predicted object bounding boxes3 and the ground truth bounding boxes (denoted as bIoU), measuring the precision of object localization. (3) For those objects that have been correctly localized, we compare the predicted object boundary with the ground truth boundary, measuring the precision of semantic boundary similar to [49].\nComparisons. DPN is compared with the state-of-the-art segmentation methods, including FCN [11], Zoom-out [28], DeepLab [13], WSSL [37], BoxSup [50], Piecewise [17], RNN [16], SuperParsing [51], Dilation10 [52], ALE [53], Multiclass [34], and SegNet [38]. All these methods are based on CNNs or MRFs. They can be grouped according to different aspects: (1) jointtrain: Piecewise and RNN; (2) w/o joint-train: DeepLab, WSSL, FCN, and BoxSup; (3) pre-train on COCO: RNN, WSSL, and BoxSup. The first and the second groups are the methods with and without joint training CNNs and MRFs, respectively. Methods in the last group also employed MS-COCO [54] to pre-train deep models. To conduct a comprehensive comparison, the performance of DPN are reported on both settings, i.e. , with and without pretraining on COCO.\nIn the following, Sec. 5.1 investigates the effectiveness of different components of DPN on the VOC12. Sec. 5.2 evaluates the spatial-temporal DPN on the CamVid. Sec. 5.3 provides detailed analysis of the DPN system and its performance. Sec. 5.4 compares DPN with the state-of-the-art methods on the benchmarks."
    }, {
      "heading" : "5.1 Effectiveness of DPN",
      "text" : "All the models evaluated in this section are trained on the training set and tested on the validation set of VOC12.\nTriple Penalty. The receptive field of b12 indicates the range of triple relations for each pixel. We examine different settings of the receptive fields, including ‘10×10’, ‘50×50’, and ‘100×100’, as shown in Table 3(a), where ‘50×50’ achieves the best mIoU, which is sightly better than ‘100×100’. For a 512×512 image, this result implies that 50×50 neighborhood is sufficient to capture\n3. They are the bounding boxes of the predicted segmentation regions.\n8\nrelations between pixels, while smaller or larger regions tend to under-fit or over-fit the training data. Moreover, all models of triple relations outperform the ‘baseline’ method that models dense pairwise relations, i.e. VGG16+denseCRF [7].\nLabel Contexts. Receptive field of b13 indicates the range of local label context. To evaluate its effectiveness, we fix the receptive field of b12 as 50×50. As summarized in Table 3(b), ‘9×9 mixtures’ improves preceding settings by 1.7, 0.5, and 0.2 percent respectively. We observe large gaps exist between ‘1×1’ and ‘5×5’. Note that the 1×1 receptive field of b13 corresponds to learning a global label co-occurrence without considering local spatial contexts. Table 3(c) shows that the pairwise terms of DPN are more effective than DSN and DeepLab4.\nMore importantly, mIoU of all the categories can be improved through increasing the size of receptive field and learning a mixture. Specifically, for each category, the improvements of the last three settings in Table 3(b) over the first one are 1.2±0.2, 1.5±0.2, and 1.7±0.3, respectively.\nWe also visualize the learned label compatibilities and contexts in Fig. 6(a) and (b), respectively. Fig. 6(a) is obtained by summing each filter in b13 over a 9×9 region, indicating how likely a column object would present when a row object is presented. Blue color represents high favorability. It is worth pointing out that Fig. 6(a) is non-symmetry. For example, when a ‘horse’ is presented, a ‘person’ is more likely to present than the other objects. On the other hand, when a ‘person’ presents in an image, it is more likely to find a ‘bike’ (than a ‘horse’). The ‘bkg’ (background) is compatible with all the objects. Fig. 6(b) visualizes some contextual patterns, where ‘A:B’ indicates that\n4. The other deep models such as RNN and Piecewise did not report the exact imrprovements after combining unary and pairwise terms.\nwhen ‘A’ is presented, where ‘B’ is more likely to present. For example, ‘bkg’ is around ‘train’, ‘motor bike’ is below ‘person’, and ‘person’ is sitting on ‘chair’."
    }, {
      "heading" : "5.2 Effectiveness of Spatial-temporal DPN",
      "text" : "In this section, we quantitatively investigate the effectiveness of the pairwise term in spatial-temporal DPN. CamVid train are used for training and performance are reported on CamVid test. Images in CamVid are captured from a moving vehicle, thus exhibiting certain temporal regularization.\n3D Convolution. In spatial-temporal DPN, the b12 is a 3D local convolutional layer and b13 is a 3D global convolutional layer. In Table 4, we evaluate the performance gain of each stage on DPN. According to Table 3(a), we set the receptive field of b12 as 50×50 and b13 as 9×9 in 2D setting. In 3D pairwise term, the receptive field of b12 is 50×50×3 and b13 is 7×7×3. We observe that applying 2D and 3D pairwise terms on unary term both improves the performance. Since 3D pairwise terms capture the information between successive frames, it performs slightly better than 2D pairwise terms.\nTemporal Regularization. In Fig. 7, we visualize the 3D learned label compatibilities and contexts across frames.\nFig. 7(a) depicts the possibility of column object would present in the T -th frame when a row object is presented in the T − 1-th frame. Blue color indicates high favorability. For instance, when ‘pavement’ is presented in the T − 1-th frame, ‘road’ is more likely to present around in the T -th frame. However, if ‘sky’ is presented in the T − 1-th frame, the pairwise term will penalize the probability which ‘road’ is presented in the T -th frame.\nFig. 7(b) visualizes the spatial−temporal contextual patterns. In each row, ‘A:B’ indicates how ‘A’ influences ‘B’.The figures suggest that when ‘A’ is presented in the T -th or the T − 1-th frame, where ‘B’ is more likely to present in the T -th frame. In the first row, (1) represents that when ‘sky’ is presented in the T -th frame, its neighborhood is more likely to be ‘sky’. (2) means when ‘sky’ is presented in the T−1-th frame, it is more likely presented above this position in the T -th frame. As shown in Fig. 8, (a) and\n9\n(b) are neighboring video frames, when the vehicle moves forward from (a) to (b), ‘sky’ region (yellow) shifts upward in the scene. So compared with the favor region in Fig. 7(b)-(1), the favor region also shifts upward in (2). In the second row of Fig. 7(b), (3) and (4) represent that when ‘pedestrian’ is presented in the T -th or the T − 1-th frame, ‘building’ is more likely presented above in the T -th frame. We observe that the shifting from (3) to (4) is similar with the shifting of ‘pedestrian’(orange) in Fig. 8 from (a) to (b)."
    }, {
      "heading" : "5.3 Further Analysis",
      "text" : "All the analysis in this section are conducted on the validation set of VOC12.\nIncremental Learning. As discussed in Sec. 4.3, DPN is trained in an incremental manner. The right hand side of Table 5(a) demonstrates that each stage leads to performance gain compared to its previous stage. For instance, ‘triple penalty’ improves ‘unary term’ by 2.3 percent, while ‘label contexts’ improves ‘triple penalty’ by 1.8 percent. More importantly, joint fine-tuning all the components (i.e. unary terms and pairwise terms) in DPN achieves another gain of 1.3 percent. A step-by-step visualization is provided in Fig. 9.\nWe also compare ‘incremental learning’ with ‘joint learning’, which fine-tunes all the components of DPN at the same time. The training curves of them are plotted in Fig.10 (a), showing that the former leads to higher and more stable accuracies with respect to different iterations, while the latter may get stuck at local minima. This difference is easy to understand – incremental learning only introduces new parameters until all existing parameters have been fine-tuned.\nOne-iteration MF. DPN approximates one iteration of MF. Fig. 10(b) illustrates that DPN reaches a good accuracy with one MF iteration. A CRF [7] with dense pairwise edges needs more than 5 iterations to converge. It also has a large gap compared to DPN. Note that the existing deep models such as [13], [14], [16] required 5∼10 iterations to converge as well. Per-stage Analysis. We further evaluate DPN using three metrics. The results are given in Fig. 11. For example, (a) illustrates that the tagging accuracy can be improved in the third stage, as it captures label co-occurrence with a mixture of contextual patterns. However, TA decreases a little after the final stage. Since joint tuning maximizes segmentation accuracies by optimizing all components together, extremely small objects, which rarely occur in VOC training set, are discarded. As shown in (b), accuracies of object localization are significantly improved in the second and the final stages. This is intuitive because the unary prediction can be refined by long-range and high-order pixel relations, and joint training further improves results. (c) discloses that the second stage also captures object boundary, since it measures dissimilarities between pixels.\nPer-class Analysis. Table 5(a) reports the per-class accuracies of four evaluation metrics, where the first four rows represent the mIoU of four stages, while the last three rows represent TA, LA, and BA, respectively. We have several valuable observations, which motivate future researches. (1) Joint training benefits most of the categories, except animals such as ‘bird’, ‘cat’, and ‘cow’. Some instances of these categories are extremely small so that joint training discards them for smoother results. (2) Training DPN with pixelwise label maps implicitly models image-level tags, since it achieves a high averaged TA of 96.4%. (3) Object localization always helps. However, for objects with complex boundary such as ‘bike’, its mIoU is low even it can be localized, e.g. ‘bike’ has high LA but low BA and mIoU. (4) Failures of different categories have different factors. With these three metrics, they can be easily identified. For example, the failures of ‘chair’, ‘table’, and ‘plant’ are caused by the difficulties to\n10\naccurately capture their bounding boxes and boundaries. Although ‘bottle’ and ‘tv’ are also difficult to localize, they achieve moderate mIoU because of their regular shapes. In other words, mIoU of ‘bottle’ and ‘tv’ can be significantly improved if they can be accurately localized."
    }, {
      "heading" : "5.4 Benchmarks",
      "text" : "We evaluate the performance of DPN and spatio-temporal DPN on several standard semantic segmentation benchmarks."
    }, {
      "heading" : "5.4.1 Pascal VOC12",
      "text" : "The VOC12 dataset is one of the most popular benchmarks for semantic image segmentation. This dataset contains 20 indoor and outdoor object categories and one background category. As previously mentioned, we employ 10, 582 images for training, 1, 449 images for validation, and 1, 456 images for testing. Results are given Table 5 (b), we compare DPN with the bestperforming methods5 on VOC12 test set based on two settings,\n5. The results of these methods were presented in either the published papers or arXiv pre-prints.\ni.e. with and without pre-training on COCO. The approaches pretrained on COCO are marked with ‘†’. We evaluate DPN on several scales of the images and then average the results following [13], [17].\nDPN outperforms all the existing methods that were trained on VOC12, but DPN needs only one MF iteration to solve MRF, other than 10 iterations of RNN, DeepLab, and Piecewise. By averaging the results of two DPNs, we achieve 74.1% accuracy on VOC12 without outside training data. As discussed in Sec.4.3, MF iteration is the most complex step even when it is implemented as convolutions. Therefore, DPN at least reduces 10× runtime compared to previous works.\nFollowing [16], [50], we pre-train DPN with COCO, where 20 object categories that are also presented in VOC12 are selected for training. A single DPN† has achieved 77.5% mIoU on VOC12 test set. As shown in Table 5 (b), we observe that DPN† achieves best performances on more than half of the object classes."
    }, {
      "heading" : "5.4.2 Cityscapes",
      "text" : "Cityscapes dataset focuses on street scenes segmentation. All images are captured from a moving vehicle in various seasons and cities. This dataset is highly different from VOC12. Because\n11\nof the great depth of images, the street scenes have large scale variations even in a single image. It defines 19 object categories for evaluation. There are 2975 training, 500 validation and 1525 testing images with fine pixel-level annotations and extra 20000 images with cores annotations. To conduct a fair comparison, we only use the images with fine pixel-level annotations in our experiment.\nAs shown in Table 6, DPN achieves 66.8% on Cityscapes dataset, which is the second best method, and it is close to the first place 67.1% [52]. [52] takes a multi-scale aggregation strategy, which can be easily integrated into DPN to further boost the performance. Our method shows advantages on objects with arbitrary shape (i.e. ‘road’, ‘building’, ‘vegetation, ‘terrain’ and ‘sky’) which can be easily confused with other objects. Thanks to our high-order and long-range pairwise term, DPN captures their appearance and segments them from other objects accurately."
    }, {
      "heading" : "5.4.3 CamVid",
      "text" : "CamVid dataset consists of 367 training and 233 testing images with 11 classes annotations. All images are extracted from three video sequences at 1Hz. Similar to Cityscapes dataset, these video sequences are also captured from a moving vehicle. As shown in Table 7, DPN outperforms all existing methods, and spatial-temporal pairwise term further improves the performance to 60.25%. Considering the relatively sparse sampling rate (one frame per second) and strong unary term used, our 3-D MRF can indeed leverage temporal contextual information for joint inference.\nWe can observe that DPN achieves much better performance than other methods, especially on narrow and small objects (i.e. ‘pole’ and ‘sign’), which are very difficult in segmentation task. Since our triple penalty captures the appearance of pixels, we can predict more accurate boundary on those objects. The spatial-temporal pairwise term further improves the performance. Spatial-temporal DPN encodes the relationship between successive frames, which improve the performance on some specific categories like’tree’, ‘sky’ and ‘road’, because they have flexible shape and are continuous in successive frames. It is also worthwhile to note that our spatial-temporal DPN achieves best performances on 7 object categories out of 11."
    }, {
      "heading" : "5.5 Visual Quality Comparisons",
      "text" : "In the following, we inspect visual quality of obtained label maps. Fig. 12 demonstrates the comparisons of DPN with FCN [11] and DeepLab [13]. We use the publicly released model6 to regenerate label maps of FCN while the results of DeepLab are extracted from their published paper. DPN generally makes more accurate predictions in both image-level and instance-level. For example, in Fig. 12 (row 2), DPN is able to discover all persons available while still recovering sharp boundaries of the aeroplane. Even in the challenging case of Fig. 12 (row 4) due to complex reflectance, the integrity of the bus is maintained by DPN. More examples of DPN label maps are shown in Fig. 14. We observe that learning local label contexts helps differentiate confusing objects and learning triple penalty facilitates the capturing of intrinsic object boundaries.\nWe also include some typical failure modes of DPN in Fig. 13. In the first case, the object with atypical pose is hard to be found.\n6. http://dl.caffe.berkeleyvision.org/fcn-8s-pascal.caffemodel\nFurther considering well-trained object detector as additional unary potential might be a potential solution to this challenge. The second case suggests that scale and illumination are also important factors that influence the performance. This problem can be partially alleviated by augmenting or adding the variance of training data."
    }, {
      "heading" : "6 CONCLUSION",
      "text" : "We proposed Deep Parsing Network (DPN) to address semantic image/video segmentation. DPN has several appealing properties. First, DPN unifies the inference and learning of unary term and pairwise terms in a single convolutional network. No iterative inference is required during back-propagation. Second, high-order relations and mixtures of label contexts are incorporated to its pairwise terms modeling, making existing works as special cases. Third, DPN is built upon conventional operations of CNN, thus easy to be parallelized and speeded up.\nDPN achieves state-of-the-art performance on VOC12, Cityscapes and CamVid datasets. Multiple valuable facts about semantic segmentation are revealed through extensive experiments, such as the interplay between tagging, localization and boundary accuracies along different processing stages. Future directions include investigating the generalizability of DPN to more challenging scenarios with large number of object classes and substantial appearance/scale variations."
    } ],
    "references" : [ {
      "title" : "Semantic image segmentation via deep parsing network",
      "author" : [ "Z. Liu", "X. Li", "P. Luo", "C.-C. Loy", "X. Tang" ],
      "venue" : "ICCV, 2015, pp. 1377–1385.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Track to the future: Spatiotemporal video segmentation with long-range motion cues",
      "author" : [ "J. Lezama", "K. Alahari", "J. Sivic", "I. Laptev" ],
      "venue" : "CVPR, 2011.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Scene collaging: Analysis and synthesis of natural images with semantic layers",
      "author" : [ "P. Isola", "C. Liu" ],
      "venue" : "ICCV. IEEE, 2013, pp. 3048–3055.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Learning hierarchical features for scene labeling",
      "author" : [ "C. Farabet", "C. Couprie", "L. Najman", "Y. LeCun" ],
      "venue" : "PAMI, vol. 35, no. 8, pp. 1915–1929, 2013.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 1915
    }, {
      "title" : "The cityscapes dataset for semantic urban scene understanding",
      "author" : [ "M. Cordts", "M. Omran", "S. Ramos", "T. Rehfeld", "M. Enzweiler", "R. Benenson", "U. Franke", "S. Roth", "B. Schiele" ],
      "venue" : "CVPR, 2016.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "An introduction to variational methods for graphical models",
      "author" : [ "M.I. Jordan", "Z. Ghahramani", "T.S. Jaakkola", "L.K. Saul" ],
      "venue" : "Machine learning, vol. 37, no. 2, pp. 183–233, 1999.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Efficient inference in fully connected crfs with gaussian edge potentials",
      "author" : [ "P. Krähenbühl", "V. Koltun" ],
      "venue" : "NIPS, 2011.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Filter-based mean-field inference for random fields with higher-order terms and product label-spaces",
      "author" : [ "V. Vineet", "J. Warrell", "P.H. Torr" ],
      "venue" : "ECCV, 2012, pp. 31–44.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Context driven scene parsing with attention to rare classes",
      "author" : [ "J. Yang", "B. Price", "S. Cohen", "M.-H. Yang" ],
      "venue" : "CVPR, 2014, pp. 3294–3301.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Very deep convolutional networks for large-scale image recognition",
      "author" : [ "K. Simonyan", "A. Zisserman" ],
      "venue" : "ICLR, 2015.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Fully convolutional networks for semantic segmentation",
      "author" : [ "J. Long", "E. Shelhamer", "T. Darrell" ],
      "venue" : "CVPR, 2015, pp. 3431–3440.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Imagenet: A large-scale hierarchical image database",
      "author" : [ "J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei-Fei" ],
      "venue" : "CVPR, 2009, pp. 248–255.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Semantic image segmentation with deep convolutional nets and fully connected crfs",
      "author" : [ "L.-C. Chen", "G. Papandreou", "I. Kokkinos", "K. Murphy", "A.L. Yuille" ],
      "venue" : "ICLR, 2015.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Fully connected deep structured networks",
      "author" : [ "A.G. Schwing", "R. Urtasun" ],
      "venue" : "arXiv:1503.02351v1, 9 Mar 2015.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "From naive mean field theory to the tap equations",
      "author" : [ "M. Opper", "O. Winther" ],
      "venue" : "2001.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Conditional random fields as recurrent neural networks",
      "author" : [ "S. Zheng", "S. Jayasumana", "B. Romera-Paredes", "V. Vineet", "Z. Su", "D. Du", "C. Huang", "P. Torr" ],
      "venue" : "arXiv:1502.03240v2, 30 Apr 2015.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Efficient piecewise training of deep structured models for semantic segmentation",
      "author" : [ "G. Lin", "C. Shen", "I. Reid", "A. Hengel" ],
      "venue" : "arXiv:1504.01013v2, 23 Apr 2015.  12 (a) input image (b) ground truth (c) FCN (d) DeepLab (e) DPN Fig. 12: Visual quality comparison of different semantic image segmentation methods: (a) input image (b) ground truth (c) FCN [11] (d) DeepLab [13] and (e) DPN.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Speeding up convolutional neural networks with low rank expansions",
      "author" : [ "M. Jaderberg", "A. Vedaldi", "A. Zisserman" ],
      "venue" : "BMVC, 2014.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "cudnn: Efficient primitives for deep learning",
      "author" : [ "S. Chetlur", "C. Woolley", "P. Vandermersch", "J. Cohen", "J. Tran", "B. Catanzaro", "E. Shelhamer" ],
      "venue" : "NIPS Deep Learning Workshop, 2014.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "The pascal visual object classes (voc) challenge",
      "author" : [ "M. Everingham", "L. Van Gool", "C.K. Williams", "J. Winn", "A. Zisserman" ],
      "venue" : "IJCV, vol. 88, no. 2, pp. 303–338, 2010.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Semantic object classes in video: A high-definition ground truth database",
      "author" : [ "G.J. Brostow", "J. Fauqueur", "R. Cipolla" ],
      "venue" : "Pattern Recognition Letters, 2008.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Normalized cuts and image segmentation",
      "author" : [ "J. Shi", "J. Malik" ],
      "venue" : "PAMI, vol. 22, no. 8, pp. 888–905, 2000.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Learning a classification model for segmentation",
      "author" : [ "X. Ren", "J. Malik" ],
      "venue" : "ICCV, 2003, pp. 10–17.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Efficient belief propagation for early vision",
      "author" : [ "P.F. Felzenszwalb", "D.P. Huttenlocher" ],
      "venue" : "IJCV, vol. 70, no. 1, pp. 41–54, 2006.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Learning crfs using graph cuts",
      "author" : [ "M. Szummer", "P. Kohli", "D. Hoiem" ],
      "venue" : "ECCV, 2008, pp. 582–595.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Class segmentation and object localization with superpixel neighborhoods",
      "author" : [ "B. Fulkerson", "A. Vedaldi", "S. Soatto" ],
      "venue" : "ICCV, 2009, pp. 670– 677.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Contour detection and hierarchical image segmentation",
      "author" : [ "P. Arbelaez", "M. Maire", "C. Fowlkes", "J. Malik" ],
      "venue" : "PAMI, vol. 33, no. 5, pp. 898–916, 2011.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Feedforward semantic segmentation with zoom-out features",
      "author" : [ "M. Mostajabi", "P. Yadollahpour", "G. Shakhnarovich" ],
      "venue" : "CVPR, 2015, pp. 3376–3385.",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Parameter learning and convergent inference for dense random fields",
      "author" : [ "P. Krähenbühl", "V. Koltun" ],
      "venue" : "ICML, 2013, pp. 513–521.",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Posefield: An efficient mean-field based method for joint estimation of human pose, segmentation, and depth",
      "author" : [ "V. Vineet", "G. Sheasby", "J. Warrell", "P.H. Torr" ],
      "venue" : "Energy Minimization Methods in Computer Vision and Pattern Recognition. Springer, 2013, pp. 180–194.",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Nonparametric scene parsing via label transfer",
      "author" : [ "C. Liu", "J. Yuen", "A. Torralba" ],
      "venue" : "PAMI, vol. 33, no. 12, pp. 2368–2382, 2011.",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "The role of context for object detection and semantic segmentation in the wild",
      "author" : [ "R. Mottaghi", "X. Chen", "X. Liu", "N.-G. Cho", "S.-W. Lee", "S. Fidler", "R. Urtasun", "A. Yuille" ],
      "venue" : "CVPR, 2014, pp. 891–898.",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Segmentation, ordering and multi-object tracking using graphical models.",
      "author" : [ "C. Wang", "M. de La Gorce", "N. Paragios" ],
      "venue" : "in ICCV,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2009
    }, {
      "title" : "Multiclass semantic video segmentation with objectlevel active inference",
      "author" : [ "B. Liu", "X. He" ],
      "venue" : "CVPR, 2015, pp. 4286–4294.",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Hierarchical face parsing via deep learning",
      "author" : [ "P. Luo", "X. Wang", "X. Tang" ],
      "venue" : "CVPR, 2012, pp. 2480–2487.",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Pedestrian parsing via deep decompositional network",
      "author" : [ "——" ],
      "venue" : "ICCV, 2013, pp. 2648–2655.",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Weakly-and semi-supervised learning of a dcnn for semantic image segmentation",
      "author" : [ "G. Papandreou", "L.-C. Chen", "K. Murphy", "A.L. Yuille" ],
      "venue" : "arXiv:1502.02734v2, 8 May 2015.",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Segnet: A deep convolutional encoder-decoder architecture for image segmentation",
      "author" : [ "V. Badrinarayanan", "A. Kendall", "R. Cipolla" ],
      "venue" : "arXiv preprint arXiv:1511.00561, 2015.",
      "citeRegEx" : "38",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Learning low-level vision",
      "author" : [ "W.T. Freeman", "E.C. Pasztor", "O.T. Carmichael" ],
      "venue" : "IJCV, vol. 40, no. 1, pp. 25–47, 2000.",
      "citeRegEx" : "39",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Semantic video segmentation: Exploring inference efficiency",
      "author" : [ "S. Tripathi", "S. Belongie", "Y. Hwang", "T. Nguyen" ],
      "venue" : "arXiv preprint arXiv:1509.02441, 2015.",
      "citeRegEx" : "40",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Sift flow: Dense correspondence across scenes and its applications",
      "author" : [ "C. Liu", "J. Yuen", "A. Torralba" ],
      "venue" : "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 33, no. 5, pp. 978–994, 2011.",
      "citeRegEx" : "41",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "A. Krizhevsky", "I. Sutskever", "G.E. Hinton" ],
      "venue" : "NIPS, 2012, pp. 1097–1105.",
      "citeRegEx" : "42",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Deep learning face representation by joint identification-verification",
      "author" : [ "Y. Sun", "X. Wang", "X. Tang" ],
      "venue" : "NIPS, 2014.",
      "citeRegEx" : "43",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Deepface: Closing the gap to human-level performance in face verification",
      "author" : [ "Y. Taigman", "M. Yang", "M. Ranzato", "L. Wolf" ],
      "venue" : "CVPR, 2014, pp. 1701–1708.",
      "citeRegEx" : "44",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Deep learning face attributes in the wild",
      "author" : [ "Z. Liu", "P. Luo", "X. Wang", "X. Tang" ],
      "venue" : "ICCV, 2015, pp. 3730–3738.",
      "citeRegEx" : "45",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Distilling the knowledge in a neural network",
      "author" : [ "G.E. Hinton", "O. Vinyals", "J. Dean" ],
      "venue" : "NIPS Deep Learning Workshop, 2014.",
      "citeRegEx" : "46",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Fast high-dimensional filtering using the permutohedral lattice",
      "author" : [ "A. Adams", "J. Baek", "M.A. Davis" ],
      "venue" : "Computer Graphics Forum, vol. 29, no. 2, 2010, pp. 753–762.",
      "citeRegEx" : "47",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Segmentation and 13 (a) input image  (b) ground truth  (c) DPN Fig. 13: Failure cases: (a) input image (b) ground truth (c) DPN. recognition using structure from motion point clouds",
      "author" : [ "G.J. Brostow", "J. Shotton", "J. Fauqueur", "R. Cipolla" ],
      "venue" : "ECCV, 2008, pp. 44–57.",
      "citeRegEx" : "48",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Semantic contours from inverse detectors",
      "author" : [ "B. Hariharan", "P. Arbeláez", "L. Bourdev", "S. Maji", "J. Malik" ],
      "venue" : "ICCV, 2011, pp. 991–998.",
      "citeRegEx" : "49",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Boxsup: Exploiting bounding boxes to supervise convolutional networks for semantic segmentation",
      "author" : [ "J. Dai", "K. He", "J. Sun" ],
      "venue" : "arXiv:1503.01640v2, 18 May 2015.",
      "citeRegEx" : "50",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Superparsing: scalable nonparametric image parsing with superpixels",
      "author" : [ "J. Tighe", "S. Lazebnik" ],
      "venue" : "Computer Vision–ECCV 2010. Springer, 2010, pp. 352–365.",
      "citeRegEx" : "51",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Multi-scale context aggregation by dilated convolutions",
      "author" : [ "F. Yu", "V. Koltun" ],
      "venue" : "arXiv preprint arXiv:1511.07122, 2015.",
      "citeRegEx" : "52",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Associative hierarchical crfs for object class image segmentation",
      "author" : [ "C. Russell", "P. Kohli", "P.H. Torr" ],
      "venue" : "Computer Vision, 2009 IEEE 12th International Conference on. IEEE, 2009, pp. 739–746.",
      "citeRegEx" : "53",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Microsoft coco: Common objects in context",
      "author" : [ "T.-Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Dollár", "C.L. Zitnick" ],
      "venue" : "ECCV, 2014, pp. 740–755.  14 (a) input image (b) ground truth (c) DPN (a) input image (b) ground truth (c) DPN (a) input image  (b) ground truth  (c) DPN (a) input image  (b) ground truth  (c) DPN  Pa  sc  al  V  O  C  12 C  ity  sc  ap  es C  am  Vi d Fig. 14: Visual quality of DPN label maps: (a) input image (b) ground truth (white labels indicating ambiguous regions) and (c) DPN.",
      "citeRegEx" : "54",
      "shortCiteRegEx" : null,
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "There are two widely adopted research realms, namely semantic image segmentation [1] and semantic video segmentation [2], with static image and video sequence as input respectively.",
      "startOffset" : 81,
      "endOffset" : 84
    }, {
      "referenceID" : 1,
      "context" : "There are two widely adopted research realms, namely semantic image segmentation [1] and semantic video segmentation [2], with static image and video sequence as input respectively.",
      "startOffset" : 117,
      "endOffset" : 120
    }, {
      "referenceID" : 2,
      "context" : "The obtained per-pixel segmentation results are extremely useful for several applications like smart editing [3], scene understanding [4] and automated driving [5].",
      "startOffset" : 109,
      "endOffset" : 112
    }, {
      "referenceID" : 3,
      "context" : "The obtained per-pixel segmentation results are extremely useful for several applications like smart editing [3], scene understanding [4] and automated driving [5].",
      "startOffset" : 134,
      "endOffset" : 137
    }, {
      "referenceID" : 4,
      "context" : "The obtained per-pixel segmentation results are extremely useful for several applications like smart editing [3], scene understanding [4] and automated driving [5].",
      "startOffset" : 160,
      "endOffset" : 163
    }, {
      "referenceID" : 5,
      "context" : "To capture these contextual information, Markov random field (MRF) and conditional random field (CRF) [6] are commonly used as classic frameworks for semantic segmentation.",
      "startOffset" : 102,
      "endOffset" : 105
    }, {
      "referenceID" : 6,
      "context" : "[7] attained accurate segmentation boundary by inferring on a fully-connected graph.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 7,
      "context" : "[8] extended [7] by defining both high-order and long-range terms between pixels.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 6,
      "context" : "[8] extended [7] by defining both high-order and long-range terms between pixels.",
      "startOffset" : 13,
      "endOffset" : 16
    }, {
      "referenceID" : 8,
      "context" : "Global or local semantic contexts between labels were also investigated by [9].",
      "startOffset" : 75,
      "endOffset" : 78
    }, {
      "referenceID" : 9,
      "context" : "As deep learning gradually takes over in many image recognition fields [10], researchers have also explored the possibility of designing effective deep architecture for semantic segmentation.",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 10,
      "context" : "[11] transformed fully-connected layers",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "of CNN into convolutional layers, making accurate per-pixel classification possible using contemporary CNN architectures that were pre-trained on ImageNet [12].",
      "startOffset" : 155,
      "endOffset" : 159
    }, {
      "referenceID" : 12,
      "context" : "[13] improved [11] by feeding the outputs of CNN into a MRF with simple pairwise potentials, but it treated CNN and MRF as separate components.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 10,
      "context" : "[13] improved [11] by feeding the outputs of CNN into a MRF with simple pairwise potentials, but it treated CNN and MRF as separate components.",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 13,
      "context" : "A recent advance was made in joint training CNN and MRF by passing the error of MRF inference backward into CNN [14].",
      "startOffset" : 112,
      "endOffset" : 116
    }, {
      "referenceID" : 14,
      "context" : "Nonetheless, an iterative inference of MRF such as the mean field algorithm (MF) [15] is required for each training image during the back-propagation (BP).",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 15,
      "context" : "[16] further showed that the procedure of MF inference can be represented as a Recurrent Neural Network (RNN), but their computational costs are similar to that of [14].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 13,
      "context" : "[16] further showed that the procedure of MF inference can be represented as a Recurrent Neural Network (RNN), but their computational costs are similar to that of [14].",
      "startOffset" : 164,
      "endOffset" : 168
    }, {
      "referenceID" : 9,
      "context" : "Specifically, DPN models unary terms by extending the VGG-16 network (VGG16) [10] pre-trained on ImageNet, while additional layers are carefully designed to model complex pairwise terms.",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 13,
      "context" : "The learning of these terms is transformed into deterministic endto-end computation by BP, instead of embedding MF into BP as [14], [17] did.",
      "startOffset" : 126,
      "endOffset" : 130
    }, {
      "referenceID" : 16,
      "context" : "The learning of these terms is transformed into deterministic endto-end computation by BP, instead of embedding MF into BP as [14], [17] did.",
      "startOffset" : 132,
      "endOffset" : 136
    }, {
      "referenceID" : 15,
      "context" : "Although MF can be represented by a RNN [16], it needs to recurrently compute the forward pass so as to achieve good performance and thus the process is time-consuming, e.",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 12,
      "context" : "Furthermore, DPN is capable of representing multiple types of pairwise terms, making many previous works [13], [14], [16] as its special cases.",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 13,
      "context" : "Furthermore, DPN is capable of representing multiple types of pairwise terms, making many previous works [13], [14], [16] as its special cases.",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 15,
      "context" : "Furthermore, DPN is capable of representing multiple types of pairwise terms, making many previous works [13], [14], [16] as its special cases.",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 17,
      "context" : "(3) DPN approximates MF with convolutional and pooling operations, which can be speeded up by low-rank approximation [18] and easily parallelized [19] in a Graphical Processing Unit (GPU).",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 18,
      "context" : "(3) DPN approximates MF with convolutional and pooling operations, which can be speeded up by low-rank approximation [18] and easily parallelized [19] in a Graphical Processing Unit (GPU).",
      "startOffset" : 146,
      "endOffset" : 150
    }, {
      "referenceID" : 15,
      "context" : "(2) We show that multiple types of MRFs can be represented in DPN, making many previous works such as RNN [16] and DeepLab [13] as its special cases.",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 12,
      "context" : "(2) We show that multiple types of MRFs can be represented in DPN, making many previous works such as RNN [16] and DeepLab [13] as its special cases.",
      "startOffset" : 123,
      "endOffset" : 127
    }, {
      "referenceID" : 19,
      "context" : "We demonstrate the generalizability of DPN model by showing its state-of-the-art performance on several standard semantic image/video segmentation benchmarks, including PASCAL VOC 2012 [20], CityScapes dataset [5] and CamVid dataset [21].",
      "startOffset" : 185,
      "endOffset" : 189
    }, {
      "referenceID" : 4,
      "context" : "We demonstrate the generalizability of DPN model by showing its state-of-the-art performance on several standard semantic image/video segmentation benchmarks, including PASCAL VOC 2012 [20], CityScapes dataset [5] and CamVid dataset [21].",
      "startOffset" : 210,
      "endOffset" : 213
    }, {
      "referenceID" : 20,
      "context" : "We demonstrate the generalizability of DPN model by showing its state-of-the-art performance on several standard semantic image/video segmentation benchmarks, including PASCAL VOC 2012 [20], CityScapes dataset [5] and CamVid dataset [21].",
      "startOffset" : 233,
      "endOffset" : 237
    }, {
      "referenceID" : 0,
      "context" : "In comparison to our earlier version of this work [1], we propose a generic deep learning framework, Deep Parsing Network (DPN) to model and solve N -D high-order Markov Random Field (MRF).",
      "startOffset" : 50,
      "endOffset" : 53
    }, {
      "referenceID" : 0,
      "context" : "Our previous study [1] only shows the possibility on 2-D image segmentation problem.",
      "startOffset" : 19,
      "endOffset" : 22
    }, {
      "referenceID" : 3,
      "context" : "Existing studies [4], [11], [22], [23], [24], [25], [26], [27], [28] on semantic segmentation focus on either constructing specific graph structure so that contextual information and long-term dependencies can be captured, or designing suitable network architecture to leverage the power of deep learning.",
      "startOffset" : 17,
      "endOffset" : 20
    }, {
      "referenceID" : 10,
      "context" : "Existing studies [4], [11], [22], [23], [24], [25], [26], [27], [28] on semantic segmentation focus on either constructing specific graph structure so that contextual information and long-term dependencies can be captured, or designing suitable network architecture to leverage the power of deep learning.",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 21,
      "context" : "Existing studies [4], [11], [22], [23], [24], [25], [26], [27], [28] on semantic segmentation focus on either constructing specific graph structure so that contextual information and long-term dependencies can be captured, or designing suitable network architecture to leverage the power of deep learning.",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 22,
      "context" : "Existing studies [4], [11], [22], [23], [24], [25], [26], [27], [28] on semantic segmentation focus on either constructing specific graph structure so that contextual information and long-term dependencies can be captured, or designing suitable network architecture to leverage the power of deep learning.",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 23,
      "context" : "Existing studies [4], [11], [22], [23], [24], [25], [26], [27], [28] on semantic segmentation focus on either constructing specific graph structure so that contextual information and long-term dependencies can be captured, or designing suitable network architecture to leverage the power of deep learning.",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 24,
      "context" : "Existing studies [4], [11], [22], [23], [24], [25], [26], [27], [28] on semantic segmentation focus on either constructing specific graph structure so that contextual information and long-term dependencies can be captured, or designing suitable network architecture to leverage the power of deep learning.",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 25,
      "context" : "Existing studies [4], [11], [22], [23], [24], [25], [26], [27], [28] on semantic segmentation focus on either constructing specific graph structure so that contextual information and long-term dependencies can be captured, or designing suitable network architecture to leverage the power of deep learning.",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 26,
      "context" : "Existing studies [4], [11], [22], [23], [24], [25], [26], [27], [28] on semantic segmentation focus on either constructing specific graph structure so that contextual information and long-term dependencies can be captured, or designing suitable network architecture to leverage the power of deep learning.",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 27,
      "context" : "Existing studies [4], [11], [22], [23], [24], [25], [26], [27], [28] on semantic segmentation focus on either constructing specific graph structure so that contextual information and long-term dependencies can be captured, or designing suitable network architecture to leverage the power of deep learning.",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 6,
      "context" : "Researchers improved labeling accuracy by exploring rich information to define the pairwise functions, including long-range dependencies [7], [29], high-order potentials [8], [30], and semantic label contexts [9], [31], [32].",
      "startOffset" : 137,
      "endOffset" : 140
    }, {
      "referenceID" : 28,
      "context" : "Researchers improved labeling accuracy by exploring rich information to define the pairwise functions, including long-range dependencies [7], [29], high-order potentials [8], [30], and semantic label contexts [9], [31], [32].",
      "startOffset" : 142,
      "endOffset" : 146
    }, {
      "referenceID" : 7,
      "context" : "Researchers improved labeling accuracy by exploring rich information to define the pairwise functions, including long-range dependencies [7], [29], high-order potentials [8], [30], and semantic label contexts [9], [31], [32].",
      "startOffset" : 170,
      "endOffset" : 173
    }, {
      "referenceID" : 29,
      "context" : "Researchers improved labeling accuracy by exploring rich information to define the pairwise functions, including long-range dependencies [7], [29], high-order potentials [8], [30], and semantic label contexts [9], [31], [32].",
      "startOffset" : 175,
      "endOffset" : 179
    }, {
      "referenceID" : 8,
      "context" : "Researchers improved labeling accuracy by exploring rich information to define the pairwise functions, including long-range dependencies [7], [29], high-order potentials [8], [30], and semantic label contexts [9], [31], [32].",
      "startOffset" : 209,
      "endOffset" : 212
    }, {
      "referenceID" : 30,
      "context" : "Researchers improved labeling accuracy by exploring rich information to define the pairwise functions, including long-range dependencies [7], [29], high-order potentials [8], [30], and semantic label contexts [9], [31], [32].",
      "startOffset" : 214,
      "endOffset" : 218
    }, {
      "referenceID" : 31,
      "context" : "Researchers improved labeling accuracy by exploring rich information to define the pairwise functions, including long-range dependencies [7], [29], high-order potentials [8], [30], and semantic label contexts [9], [31], [32].",
      "startOffset" : 220,
      "endOffset" : 224
    }, {
      "referenceID" : 6,
      "context" : "[7] attained accurate segmentation boundary by inferring on a fully-connected graph.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 7,
      "context" : "[8] extended [7] by defining both high-order and longrange terms between pixels.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 6,
      "context" : "[8] extended [7] by defining both high-order and longrange terms between pixels.",
      "startOffset" : 13,
      "endOffset" : 16
    }, {
      "referenceID" : 8,
      "context" : "Global or local semantic contexts between labels were also investigated by [9].",
      "startOffset" : 75,
      "endOffset" : 78
    }, {
      "referenceID" : 32,
      "context" : "[33] unified foreground object segmentation, tracking and occlusion reasoning into a carefully designed MRF model.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 1,
      "context" : "Optical flow based long-term trajectories [2] were also exploited to discover moving objects.",
      "startOffset" : 42,
      "endOffset" : 45
    }, {
      "referenceID" : 33,
      "context" : "[34] employed fully-connected CRF augmented with object potentials for efficient multi-class inference.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 10,
      "context" : "With deep models, exising works [11], [13], [14], [16], [17], [28], [35], [36], [37] demonstrated encouraging segmentation results through using just simple definition of the pairwise function or even neglecting it.",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 12,
      "context" : "With deep models, exising works [11], [13], [14], [16], [17], [28], [35], [36], [37] demonstrated encouraging segmentation results through using just simple definition of the pairwise function or even neglecting it.",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 13,
      "context" : "With deep models, exising works [11], [13], [14], [16], [17], [28], [35], [36], [37] demonstrated encouraging segmentation results through using just simple definition of the pairwise function or even neglecting it.",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 15,
      "context" : "With deep models, exising works [11], [13], [14], [16], [17], [28], [35], [36], [37] demonstrated encouraging segmentation results through using just simple definition of the pairwise function or even neglecting it.",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 16,
      "context" : "With deep models, exising works [11], [13], [14], [16], [17], [28], [35], [36], [37] demonstrated encouraging segmentation results through using just simple definition of the pairwise function or even neglecting it.",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 27,
      "context" : "With deep models, exising works [11], [13], [14], [16], [17], [28], [35], [36], [37] demonstrated encouraging segmentation results through using just simple definition of the pairwise function or even neglecting it.",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 34,
      "context" : "With deep models, exising works [11], [13], [14], [16], [17], [28], [35], [36], [37] demonstrated encouraging segmentation results through using just simple definition of the pairwise function or even neglecting it.",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 35,
      "context" : "With deep models, exising works [11], [13], [14], [16], [17], [28], [35], [36], [37] demonstrated encouraging segmentation results through using just simple definition of the pairwise function or even neglecting it.",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 36,
      "context" : "With deep models, exising works [11], [13], [14], [16], [17], [28], [35], [36], [37] demonstrated encouraging segmentation results through using just simple definition of the pairwise function or even neglecting it.",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 10,
      "context" : "[11] transformed fully-connected layers of CNN into convolutional layers, making accurate per-pixel classification possible using the contemporary CNN architectures that were pre-trained on ImageNet [12].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "[11] transformed fully-connected layers of CNN into convolutional layers, making accurate per-pixel classification possible using the contemporary CNN architectures that were pre-trained on ImageNet [12].",
      "startOffset" : 199,
      "endOffset" : 203
    }, {
      "referenceID" : 12,
      "context" : "[13] improved [11] by feeding the outputs of CNN into a MRF with simple pairwise potentials, but it treated CNN and MRF as separated components.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 10,
      "context" : "[13] improved [11] by feeding the outputs of CNN into a MRF with simple pairwise potentials, but it treated CNN and MRF as separated components.",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 13,
      "context" : "A recent advance was obtained by [14], which jointly trained CNN and MRF by passing the error of MRF inference backward into CNN, but iterative inference of MRF such as the mean field algorithm (MF) [15] is required for each training image during the back-propagation (BP).",
      "startOffset" : 33,
      "endOffset" : 37
    }, {
      "referenceID" : 14,
      "context" : "A recent advance was obtained by [14], which jointly trained CNN and MRF by passing the error of MRF inference backward into CNN, but iterative inference of MRF such as the mean field algorithm (MF) [15] is required for each training image during the back-propagation (BP).",
      "startOffset" : 199,
      "endOffset" : 203
    }, {
      "referenceID" : 15,
      "context" : "[16] further showed that the procedure of MF inference can be represented as a Recurrent Neural Network (RNN), but their computational costs are similar to that of [14].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 13,
      "context" : "[16] further showed that the procedure of MF inference can be represented as a Recurrent Neural Network (RNN), but their computational costs are similar to that of [14].",
      "startOffset" : 164,
      "endOffset" : 168
    }, {
      "referenceID" : 37,
      "context" : "Recent efforts in this direction include SegNet [38], which adopted an encoderdecoder architecture but did not take temporal relationships into consideration.",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 38,
      "context" : "MRF [39] is an undirected graph where each node represents a voxel in a video I, and each edge represents relation between voxels, as shown in Fig.",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 39,
      "context" : "Traditional approaches [40] usually define the edges E on rectangular grid in 3-D space.",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 40,
      "context" : "In the temporal domain, the neighboring voxels i = [i ti] and j = [j tj ] are defined as those lie on the same temporal trajectories ∆i→j, which can be estimated by standard optical flow techniques [41].",
      "startOffset" : 198,
      "endOffset" : 202
    }, {
      "referenceID" : 12,
      "context" : "This formulation has been adopted by most of the recent deep models [13], [14], [16] for semantic image segmentation.",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 13,
      "context" : "This formulation has been adopted by most of the recent deep models [13], [14], [16] for semantic image segmentation.",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 15,
      "context" : "This formulation has been adopted by most of the recent deep models [13], [14], [16] for semantic image segmentation.",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 14,
      "context" : "(1) can be obtained by the mean field (MF) algorithm [15], which estimates the joint distribution of MRF",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 5,
      "context" : "(9), which is denoted as free energy F (Q) [6].",
      "startOffset" : 43,
      "endOffset" : 46
    }, {
      "referenceID" : 19,
      "context" : "To simplify the discussions, we take PASCAL VOC 2012 (VOC12) [20] as an example.",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 42,
      "context" : "2D locally convolutional layer) is widely used in face recognition [43], [44], [45] to capture different information from different",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 43,
      "context" : "2D locally convolutional layer) is widely used in face recognition [43], [44], [45] to capture different information from different",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 44,
      "context" : "2D locally convolutional layer) is widely used in face recognition [43], [44], [45] to capture different information from different",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 41,
      "context" : "Moreover, ‘relu’, ‘idn’, ‘soft’, ‘sigm’, and ‘lin’ represent the activation functions, including rectified linear unit [42], identity, softmax, sigmoid, and linear, respectively.",
      "startOffset" : 119,
      "endOffset" : 123
    }, {
      "referenceID" : 12,
      "context" : "Many existing deep models such as [13], [14], [16] employed Eqn.",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 13,
      "context" : "Many existing deep models such as [13], [14], [16] employed Eqn.",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 15,
      "context" : "Many existing deep models such as [13], [14], [16] employed Eqn.",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 10,
      "context" : "During finetuning, all these stages solve the pixelwise softmax loss [11], but updating different sets of parameters.",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 18,
      "context" : "If we parallelize these operations using matrix multiplication on GPU as [19] did, the operation in b12 can be computed within 30ms.",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 17,
      "context" : "Note that convolutions in DPN can be further speeded up by low-rank decompositions [18] of the filters and model compressions [46].",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 45,
      "context" : "Note that convolutions in DPN can be further speeded up by low-rank decompositions [18] of the filters and model compressions [46].",
      "startOffset" : 126,
      "endOffset" : 130
    }, {
      "referenceID" : 12,
      "context" : "In contrast, existing works [13], [16] employ fast Gaussian filtering [47] to accelerate the direct calculation of Eqn.",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 15,
      "context" : "In contrast, existing works [13], [16] employ fast Gaussian filtering [47] to accelerate the direct calculation of Eqn.",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 46,
      "context" : "In contrast, existing works [13], [16] employ fast Gaussian filtering [47] to accelerate the direct calculation of Eqn.",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 6,
      "context" : "For a mini-batch of ten 512×512 images, a recently optimized implementation [7] takes 12 seconds on CPU to compute one iteration of (13).",
      "startOffset" : 76,
      "endOffset" : 79
    }, {
      "referenceID" : 19,
      "context" : "We compare DPN with the state-of-the-art methods on PASCAL VOC 2012 (VOC12) [20] , Cityscapes [5] and CamVid",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 4,
      "context" : "We compare DPN with the state-of-the-art methods on PASCAL VOC 2012 (VOC12) [20] , Cityscapes [5] and CamVid",
      "startOffset" : 94,
      "endOffset" : 97
    }, {
      "referenceID" : 47,
      "context" : "[48] datasets.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 10,
      "context" : "All existing works employed mean pixelwise intersection-over-union (denoted as mIoU) [11] to evaluate their performance.",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 48,
      "context" : "(3) For those objects that have been correctly localized, we compare the predicted object boundary with the ground truth boundary, measuring the precision of semantic boundary similar to [49].",
      "startOffset" : 187,
      "endOffset" : 191
    }, {
      "referenceID" : 10,
      "context" : "DPN is compared with the state-of-the-art segmentation methods, including FCN [11], Zoom-out [28], DeepLab [13], WSSL [37], BoxSup [50], Piecewise [17], RNN [16], SuperParsing [51], Dilation10 [52], ALE [53], Multiclass [34], and SegNet [38].",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 27,
      "context" : "DPN is compared with the state-of-the-art segmentation methods, including FCN [11], Zoom-out [28], DeepLab [13], WSSL [37], BoxSup [50], Piecewise [17], RNN [16], SuperParsing [51], Dilation10 [52], ALE [53], Multiclass [34], and SegNet [38].",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 12,
      "context" : "DPN is compared with the state-of-the-art segmentation methods, including FCN [11], Zoom-out [28], DeepLab [13], WSSL [37], BoxSup [50], Piecewise [17], RNN [16], SuperParsing [51], Dilation10 [52], ALE [53], Multiclass [34], and SegNet [38].",
      "startOffset" : 107,
      "endOffset" : 111
    }, {
      "referenceID" : 36,
      "context" : "DPN is compared with the state-of-the-art segmentation methods, including FCN [11], Zoom-out [28], DeepLab [13], WSSL [37], BoxSup [50], Piecewise [17], RNN [16], SuperParsing [51], Dilation10 [52], ALE [53], Multiclass [34], and SegNet [38].",
      "startOffset" : 118,
      "endOffset" : 122
    }, {
      "referenceID" : 49,
      "context" : "DPN is compared with the state-of-the-art segmentation methods, including FCN [11], Zoom-out [28], DeepLab [13], WSSL [37], BoxSup [50], Piecewise [17], RNN [16], SuperParsing [51], Dilation10 [52], ALE [53], Multiclass [34], and SegNet [38].",
      "startOffset" : 131,
      "endOffset" : 135
    }, {
      "referenceID" : 16,
      "context" : "DPN is compared with the state-of-the-art segmentation methods, including FCN [11], Zoom-out [28], DeepLab [13], WSSL [37], BoxSup [50], Piecewise [17], RNN [16], SuperParsing [51], Dilation10 [52], ALE [53], Multiclass [34], and SegNet [38].",
      "startOffset" : 147,
      "endOffset" : 151
    }, {
      "referenceID" : 15,
      "context" : "DPN is compared with the state-of-the-art segmentation methods, including FCN [11], Zoom-out [28], DeepLab [13], WSSL [37], BoxSup [50], Piecewise [17], RNN [16], SuperParsing [51], Dilation10 [52], ALE [53], Multiclass [34], and SegNet [38].",
      "startOffset" : 157,
      "endOffset" : 161
    }, {
      "referenceID" : 50,
      "context" : "DPN is compared with the state-of-the-art segmentation methods, including FCN [11], Zoom-out [28], DeepLab [13], WSSL [37], BoxSup [50], Piecewise [17], RNN [16], SuperParsing [51], Dilation10 [52], ALE [53], Multiclass [34], and SegNet [38].",
      "startOffset" : 176,
      "endOffset" : 180
    }, {
      "referenceID" : 51,
      "context" : "DPN is compared with the state-of-the-art segmentation methods, including FCN [11], Zoom-out [28], DeepLab [13], WSSL [37], BoxSup [50], Piecewise [17], RNN [16], SuperParsing [51], Dilation10 [52], ALE [53], Multiclass [34], and SegNet [38].",
      "startOffset" : 193,
      "endOffset" : 197
    }, {
      "referenceID" : 52,
      "context" : "DPN is compared with the state-of-the-art segmentation methods, including FCN [11], Zoom-out [28], DeepLab [13], WSSL [37], BoxSup [50], Piecewise [17], RNN [16], SuperParsing [51], Dilation10 [52], ALE [53], Multiclass [34], and SegNet [38].",
      "startOffset" : 203,
      "endOffset" : 207
    }, {
      "referenceID" : 33,
      "context" : "DPN is compared with the state-of-the-art segmentation methods, including FCN [11], Zoom-out [28], DeepLab [13], WSSL [37], BoxSup [50], Piecewise [17], RNN [16], SuperParsing [51], Dilation10 [52], ALE [53], Multiclass [34], and SegNet [38].",
      "startOffset" : 220,
      "endOffset" : 224
    }, {
      "referenceID" : 37,
      "context" : "DPN is compared with the state-of-the-art segmentation methods, including FCN [11], Zoom-out [28], DeepLab [13], WSSL [37], BoxSup [50], Piecewise [17], RNN [16], SuperParsing [51], Dilation10 [52], ALE [53], Multiclass [34], and SegNet [38].",
      "startOffset" : 237,
      "endOffset" : 241
    }, {
      "referenceID" : 53,
      "context" : "Methods in the last group also employed MS-COCO [54] to pre-train deep models.",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 13,
      "context" : "Pairwise Terms DSN [14] DeepLab [13] DPN",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 12,
      "context" : "Pairwise Terms DSN [14] DeepLab [13] DPN",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 6,
      "context" : "VGG16+denseCRF [7].",
      "startOffset" : 15,
      "endOffset" : 18
    }, {
      "referenceID" : 15,
      "context" : "DPN pairwise terms denseCRF [16] (a) (b)",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 6,
      "context" : "A CRF [7] with dense pairwise edges needs more than 5 iterations to converge.",
      "startOffset" : 6,
      "endOffset" : 9
    }, {
      "referenceID" : 12,
      "context" : "Note that the existing deep models such as [13], [14], [16] required 5∼10 iterations to converge as well.",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 13,
      "context" : "Note that the existing deep models such as [13], [14], [16] required 5∼10 iterations to converge as well.",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 15,
      "context" : "Note that the existing deep models such as [13], [14], [16] required 5∼10 iterations to converge as well.",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 10,
      "context" : "FCN [11] 76.",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 27,
      "context" : "2 Zoom-out [28] 85.",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 16,
      "context" : "6 Piecewise [17] 87.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 12,
      "context" : "7 DeepLab [13] 84.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 15,
      "context" : "6 RNN [16] 87.",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 36,
      "context" : "0 WSSL† [37] 89.",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 15,
      "context" : "9 RNN† [16] 90.",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 49,
      "context" : "7 BoxSup† [50] 89.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 53,
      "context" : "The approaches pre-trained on COCO [54] are marked with †.",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 51,
      "context" : "Dilation10 [52] no no no 97.",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 16,
      "context" : "1 Piecewise [17] no no no 97.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 10,
      "context" : "3 FCN [11] no no no 97.",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 36,
      "context" : "3 WSSL [37] yes no 2 97.",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 12,
      "context" : "8 DeepLab [13] no no 2 97.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 15,
      "context" : "1 RNN [16] no no 2 96.",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 52,
      "context" : "ALE [53] 73.",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 50,
      "context" : "59 SuperParsing [51] 70.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 39,
      "context" : "[40] 74.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 33,
      "context" : "18 Liu and He [34] 66.",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 37,
      "context" : "2 SegNet [38] 68.",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 12,
      "context" : "We evaluate DPN on several scales of the images and then average the results following [13], [17].",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 16,
      "context" : "We evaluate DPN on several scales of the images and then average the results following [13], [17].",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 15,
      "context" : "Following [16], [50], we pre-train DPN with COCO, where 20 object categories that are also presented in VOC12 are selected for training.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 49,
      "context" : "Following [16], [50], we pre-train DPN with COCO, where 20 object categories that are also presented in VOC12 are selected for training.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 51,
      "context" : "1% [52].",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 51,
      "context" : "[52] takes a multi-scale aggregation strategy, which can be easily integrated into DPN to further boost the performance.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 10,
      "context" : "12 demonstrates the comparisons of DPN with FCN [11] and DeepLab [13].",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 12,
      "context" : "12 demonstrates the comparisons of DPN with FCN [11] and DeepLab [13].",
      "startOffset" : 65,
      "endOffset" : 69
    } ],
    "year" : 2017,
    "abstractText" : "Semantic segmentation tasks can be well modeled by Markov Random Field (MRF). This paper addresses semantic segmentation by incorporating high-order relations and mixture of label contexts into MRF. Unlike previous works that optimized MRFs using iterative algorithm, we solve MRF by proposing a Convolutional Neural Network (CNN), namely Deep Parsing Network (DPN), which enables deterministic end-to-end computation in a single forward pass. Specifically, DPN extends a contemporary CNN to model unary terms and additional layers are devised to approximate the mean field (MF) algorithm for pairwise terms. It has several appealing properties. First, different from the recent works that required many iterations of MF during back-propagation, DPN is able to achieve high performance by approximating one iteration of MF. Second, DPN represents various types of pairwise terms, making many existing models as its special cases. Furthermore, pairwise terms in DPN provide a unified framework to encode rich contextual information in high-dimensional data, such as images and videos. Third, DPN makes MF easier to be parallelized and speeded up, thus enabling efficient inference. DPN is thoroughly evaluated on standard semantic image/video segmentation benchmarks, where a single DPN model yields state-of-the-art segmentation accuracies on PASCAL VOC 2012, Cityscapes dataset and CamVid dataset.",
    "creator" : "LaTeX with hyperref package"
  }
}