{
  "name" : "1602.06561.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Deep Learning in Finance",
    "authors" : [ "J. B. Heaton", "N. G. Polson", "J. H. Witte" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "We explore the use of deep learning hierarchical models for problems in financial prediction and classification. Financial prediction problems – such as those presented in designing and pricing securities, constructing portfolios, and risk management – often involve large data sets with complex data interactions that currently are difficult or impossible to specify in a full economic model. Applying deep learning methods to these problems can produce more useful results than standard methods in finance. In particular, deep learning can detect and exploit interactions in the data that are, at least currently, invisible to any existing financial economic theory.\nKey Words: Deep Learning, Machine Learning, Big Data, Artificial Intelligence, LSTM Models, Finance, Asset Pricing, Volatility\n∗Bartlit Beck Herman Palenchar & Scott LLP, and GreyMaths Inc. †Booth School of Business, University of Chicago, and GreyMaths Inc. ‡Mathematical Institute, University of Oxford, and GreyMaths Inc.\nar X\niv :1\n60 2.\n06 56\n1v 1\n[ cs\n.L G\n] 2\n1 Fe\nb 20\n16"
    }, {
      "heading" : "1 Introduction",
      "text" : "Financial prediction problems are of great practical and theoretical interest. They are also quite daunting. Theory suggests that much information relevant to financial prediction problems may be spread throughout available economic and other data, an idea that also gains support from the many disparate data sources that different market participants watch for clues on future price movements.\nDealing with this variety of data sources is difficult. The collection of possibly relevant data is very large, while the importance of the data and the potentially complex non-linear interactions in the data are not well specified by financial economic theory. In practice, this results in a plethora of predictive models, many with little theoretical justification and subject to over-fitting and poor predictive out-of-sample performance.\nWhat is needed is a method able to learn those complex features of the data inputs which lead to good predictions of the target output variables (such as an asset or portfolio return).\nIn this paper, we introduce deep learning hierarchical decision models for problems in financial prediction and classification. The deep learning predictor has a number of advantages over traditional predictors, which include that\n• input data can be expanded to include all items of possible relevance to the prediction problem,\n• non-linearities and complex interactions among input data are accounted for, which can help increase in-sample fit versus traditional models,\n• over-fitting is more easily avoided.\nOur paper continues as follows. Section 2 introduces the deep learning framework. Section 3 presents three finance applications of the deep learning framework. Section 4 presents an example. Section 5 concludes.\nA guiding principle throughout our paper is the construction of predictive models whose inputs are high-dimensional. See Breiman (2001) for a discussion that contrasts predictive algorithmic modelling with traditional statistical approaches."
    }, {
      "heading" : "2 Deep Learning",
      "text" : "We begin by introducing the general theoretical deep learning framework as well as several specifications."
    }, {
      "heading" : "2.1 Architecture",
      "text" : "Deep learning is a form of machine learning. Machine learning is using data to train a model and then using the trained model to make predictions from new data. The fundamental machine\nlearning problem is to find a predictor of an output Y given an input X. A learning machine is defined as an input-output mapping Y = F (X), where the input space is high-dimensional and we write\nY = F (X) where X = (X1, . . . , Xp),\nand a predictor is denoted by Ŷ (X) := F (X). The output T can be continuous, discrete as in classification, or mixed. For example, in a classification problem, we need to learn a mapping F : X → Y , where Y ∈ {1, . . . ,K} indexes categories.\nAs a form of machine learning, deep learning trains a model on data to make predictions, but is distinguished by passing learned features of data through different layers of abstraction. Raw data is entered at the bottom level, and the desired output is produced at the top level, the result of learning through many levels of transformed data. Deep learning is hierarchical in the sense that, in every layer, the algorithm extracts features into factors, and a deeper level’s factors become the next level’s features.\nSpecifically, a deep learning architecture can be described as follows. Let f1, . . . , fL be given univariate activation functions for each of the L layers. Activation functions are non-linear transformations of weighted data. A semi-affine activation rule is then defined by\nfW,bl := fl  Nl∑ j=1 WljXj + bl  = fl(WlXl + bl) , 1 ≤ l ≤ L, which implicitly needs the specification of the number of hidden units Nl. Our deep predictor, given the number of layers L, then becomes the composite map\nŶ (X) := F (X) = ( fW1,b11 ◦ . . . ◦ f WL,bL L ) (X) .\nPut simply, we model a high dimensional mapping, F , via the superposition of univariate semi-affine functions. (Similar to a classic basis decomposition, the deep approach uses univariate activation functions to decompose a high dimensional X.)\nWe let Z(l) denote the l-th layer, and so X = Z(0). The final output is the response Y , which can be numeric or categorical. The explicit structure of a deep prediction rule is then\nZ(1) = f (1) ( W (0)X + b(0) ) ,\nZ(2) = f (2) ( W (1)Z(1) + b(1) ) ,\n. . . Z(L) = f (L) ( W (L−1)Z(L−1) + b(L−1) ) ,\nŶ (X) = W (L)Z(L) + b(L) .\nHere, W (l) are weight matrices, and b(l) are threshold or activation levels. Designing a good predictor depends crucially on the choice of univariate activation functions f (l).\nThe Z(l) are hidden features (or factors) which the algorithm extracts. One particular feature is that\nthe weight matrices Wl ∈ RNl×Nl−1 are matrix valued. This gives the predictor great flexibility to uncover non-linear features of the data – particularly so in finance data, since the estimated hidden features Z(l) can represent portfolios of payouts. The choice of the dimension Nl is key, however, since if a hidden unit (columns of Wl) is dropped at layer l, then it eliminates all terms above it in the layered hierarchy.\nPut differently, the deep approach employs hierarchical predictors comprising of a series of L nonlinear transformations applied to X. Each of the L transformations is referred to as a layer, where the original input is X, the output of the first transformation is the first layer, and so on, with the output Ŷ as the (L+ 1)-th layer. We use l ∈ {1, . . . , L} to index the layers from 1 to L, which are called hidden layers. The number of layers L represents the depth of our architecture.\nCommonly used activation functions are sigmoidal (e.g., 1/(1 + exp(−x)), cosh(x), or tanh(x)), heaviside gate functions (e.g., I(x > 0)), or rectified linear units (ReLU) max{x, 0}. ReLU’s especially have been found to lend themselves well to rapid dimension reduction. A deep learning predictor is a data reduction scheme that avoids the curse of dimensionality through the use of univariate activation functions. See Kolmorogov (1957), Lorenz (1976), Gallant and White (1988), Hornik et al. (1989), and Poggio and Girosi (1990) for further discussion."
    }, {
      "heading" : "2.2 Training a Deep Architecture",
      "text" : "Constructing a deep learner requires a number of steps. It is common to split the data-set into three subsets, namely training, validation, and testing. The training set is used to adjust the weights of the network. The validation set is used to minimize the over-fitting and relates to the architecture design (a.k.a. model selection). Finally, testing is used to confirm the actual predictive power of a learner.\nOnce the activation functions, size, and depth of the learning routine have been chosen, we need to solve the training problem of finding (Ŵ , b̂), where"
    }, {
      "heading" : "Ŵ = (Ŵ0, . . . , ŴL) and b̂ = (b̂0, . . . , b̂L)",
      "text" : "denote the learning parameters which we compute during training. To do this, we need a training dataset D = {Y (i), X(i)}Ti=1 of input-output pairs and a loss function L(Y, Ŷ ) at the level of the output signal. In its simplest form, we solve\narg minW,b 1\nT T∑ i=1 L(Yi, Ŷ W,b(Xi)) . (1)\nOften, the L2-norm for a traditional least squares problem is chosen as error measure, and if we then minimize the loss function\nL(Yi, Ŷ (Xi)) = ‖Yi − Ŷ (Xi)‖22 ,\nour target function (1) becomes the mean-squared error (MSE) over the training dataset D = {Y (i), X(i)}Ti=1.\nIt is common to add a regularization penalty, denoted by φ(W, b), to avoid over-fitting and to stabilize our predictive rule. We combine this with the loss function via a parameter λ > 0, which gauges the overall level of regularization. We then need to solve\narg minW,b 1\nT T∑ i=1 L(Yi, Ŷ W,b(Xi)) + λφ(W, b) . (2)\nThe choice of the amount of regularization, λ, is a key parameter. This gauges the trade-off present in any statistical modelling that too little regularization will lead to over-fitting and poor out-of-sample performance.\nIn many cases, we will take a separable penalty, φ(W, b) = φ(W ) + φ(b). The most useful penalty is the ridge or L2-norm, which can be viewed as a default choice, namely\nφ(W ) = ‖W‖22 = T∑ l=1 W>i Wi.\nOther norms include the lasso, which corresponds to an L1-norm, and which can be used to induce sparsity in the weights and/or off-sets. The ridge norm is particularly useful when the amount of regularization, λ, has itself to be learned. This is due to the fact that there are many good predictive generalization results for ridge-type predictors. When sparsity in the weights is paramount, it is common to use a lasso L1-norm penalty."
    }, {
      "heading" : "2.2.1 Probabilistic Interpretation",
      "text" : "In a traditional probabilistic setting, we could view the output Y as a random variable generated by a probability model p(Y |Y W,b(X)), where the conditioning is on the predictor Ŷ (X). The corresponding loss function is then\nL(Y, Ŷ ) = − log p(Y |Y Ŵ ,b̂(X)),\nnamely the negative log-likelihood. For example, when predicting the probability of default, we have a multinomial logistic regression model which leads to a cross-entropy loss function. For multivariate normal models in particular (which includes many financial time series), the L2-norm becomes a suitable error measure.\nProbabilistically, the regularization term, λφ(W, b), can be viewed as a negative log-prior distribution over parameters, namely\n− log p(φ(W, b)) = λφ(W, b), p(φ(W, b)) = C exp(−λφ(W, b)),\nwhere C is a suitable normalization constant. This framework then provides a correspondence with Bayes learning. Our deep predictor is simply a regularized maximum a posteriori (MAP) estimator.\nWe can show this using Bayes rule as\np(W, b|D) ∝ p(Y |Y W,b(X))p(W, b) ∝ exp ( − log p(Y |Y W,b(X))− log p(W, b) ) ,\nand the deep learning predictor satisfies\nŶ := Y Ŵ ,b̂(X) where (Ŵ , b̂) := arg minW,b log p(W, b|D),\nand\n− log p(W, b|D) = T∑ i=1 L(Y (i), Y W,b(X(i))) + λφ(W, b)\nis the log-posterior distribution over parameters given the training data, D = {Y (i), X(i)}Ti=1."
    }, {
      "heading" : "2.2.2 Cross Validation",
      "text" : "Cross validation is a technique by which we split our training data into complementary subset to then conduct analysis and validation on different sets, aiming to reduce over-fitting and increase out-of-sample performance.\nIn particular, when training on time series, we may split our training data into disjoint time periods of identical length, which is particularly desirable in financial applications where reliable time consistent predictors are hard to come by and have to be trained and tested extensively.\nCross validation also provides a tool to decide what levels of regularization lead to good generalization (i.e., prediction), which is the classic variance-bias trade-off. A key advantage of cross validation (over traditional statistical metrics such as t-ratios and p-values) is that it also allows us to assess the size and depth of the hidden layers, that is, solve the model selection problem of choosing L and Nl for 1 ≤ l ≤ L . This ability to pragmatically and seamlessly solve the model selection and estimation problems is one of the reasons for the current widespread use of machine learning methods."
    }, {
      "heading" : "2.2.3 Back-propagation",
      "text" : "The common numerical approach for the solution of (2) is a form of stochastic gradient descent, which adapted to a deep learning setting is usually called back-propagation. One caveat of backpropagation in this context is the multi-modality of the system to be solved (and the resulting slow convergence properties), which is the main reason why deep learning methods heavily rely on the availability of large computational power.\nOne of the advantages of using a deep network is that first-order derivative information is directly available. There are tensor libraries available that directly calculate\n∇W,bL(Yi, Ŷ W,b(Xi))\nusing the chain rule across the training data-set. For ultra-large data-sets, we use mini-batches and stochastic gradient descent (SGD) to perform this optimization, see LeCun et al. (2012). An active area of research is the use of this information within a Langevin MCMC algorithm that allows sampling from the full posterior distribution of the architecture. The deep learning model by its very design is highly multi-modal, and the parameters are high dimensional and in many cases unidentified in the traditional sense. Traversing the objective function is the desired problem, and handling the multi-modal and slow convergence of traditional decent methods can be alleviated with proximal algorithms such as the alternating method of multipliers (ADMM), as has been discussed in Polson et al. (2015 a, b)."
    }, {
      "heading" : "2.3 Predictive Performance",
      "text" : "There are two key training problems that can be addressed using the predictive performance of an architecture.\n(i) How much regularization to add to the loss function. As indicated before, one approach is to use cross validation and to teach the algorithm to calibrate itself to a training data. An independent hold-out data set is kept separately to perform an out-of-sample measurement of the training success in a second step. As we vary the amount of regularization, we obtain a regularization path and choose the level of regularization to optimize out-of-sample predictive loss. Another approach is to use Stein’s unbiased estimator of risk (SURE).\n(ii) A more challenging problem is to train the size and depth of each layer of the architecture, i.e., to determine L and N = (N1, . . . , NL). This is known as the model selection problem. In the next subsection, we will describe a technique known as dropout, which solves this problem.\nStein’s unbiased estimator of risk (SURE) proceeds as follows. For a stable predictor, Ŷ , we can define the degrees of freedom of a predictor by df = E (∑T\ni=1 ∂Ŷi/∂Yi\n) . Then, given the scalability\nof our algorithm, the derivative ∂Ŷ /∂Y is available using the chain rule for the composition of the L layers.\nNow let the in-sample MSE be given by err = ||Y − Ŷ ||22 and, for a future observation Y ?, the out-of-sample predictive MSE is\nErr = EY ? ( ||Y ? − Ŷ ||22 ) .\nIn expectation, we then have\nE (Err) = E ( err + 2Var(Ŷ , Y ) ) .\nThe latter term can be written in terms of df as a covariance. Stein’s unbiased risk estimate then becomes\nÊrr = ||Y − Ŷ ||2 + 2σ2 n∑\ni=1\n∂Ŷi ∂Yi .\nAs before, models with the best predictive MSE are favoured."
    }, {
      "heading" : "2.4 Dropout for Model Selection",
      "text" : "Dropout is a model selection technique. It is designed to avoid over-fitting in the training process, and does so by removing input dimensions in X randomly with a given probability p. In a simple model with one hidden layer, we replace the network\nY (l) i = f(Z (l) i ), Z (l) i = W (l) i X (l) + b (l) i ,\nwith the dropout architecture\nD (l) i ∼ Ber(p), Ỹ (l) i = D (l) ? X(l), Y (l) i = f(Z (l) i ), Z (l) i = W (l) i X (l) + b (l) i .\nIn effect, this replaces the input X by D ? X, where ? denotes the element-wise product and D is a matrix of independent Bernoulli Ber(p) distributed random variables.\nIt is instructive to see how this affects the underlying loss function and optimization problem. For example, suppose that we wish to minimise MSE, L(Y, Ŷ ) = ‖Y − Ŷ ‖22, then, when marginalizing over the randomness, we have a new objective\narg minW ED∼Ber(p)‖Y −W (D ?X)‖22 ,\nwhich is equivalent to arg minW ‖Y − pWX‖22 + p(1− p)‖ΓW‖22 , where Γ = (diag(X>X)) 1 2 . We can also interpret this last expression as a Bayesian ridge regression with a g-prior. Put simply, dropout reduces the likelihood of over-reliance on small sets of input data in training. See Hinton and Salakhutdinov (2006) and Srivastava et al. (2014). Lopes and West (2004) provide a fully Bayesian approach to factor selection. Dropout can be viewed as the optimization version of the traditional spike-and-slab prior that has proven so popular in Bayesian model averaging.\nAnother application of dropout regularization is the choice of the number of hidden units in a layer. (This can be achieved if we drop units of the hidden rather than the input layer and then establish which probability p gives the best results). It is worth recalling though, as we have stated before, that one of the dimension reduction properties of a network structure is that once a variable from a layer is dropped, all terms above it in the network also disappear. This is just the nature of a composite structure for the deep predictor in (2.1).\nWe now turn to describing three widely used architecture designs that have become commonplace in applications of machine learning, namely auto-encoders, rectified neural networks (RNNs), and long short term memory (LTSM) models. In Section 4, we provide an application of auto-encoders to smart asset indexing problems."
    }, {
      "heading" : "2.5 Auto-encoder",
      "text" : "An auto-encoder is a deep learning routine which trains the architecture to approximate X by itself (i.e., X = Y ) via a bottleneck structure. This means we select a model FW,b(X) which aims to concentrate the information required to recreate X. Put differently, an auto-encoder creates a more cost effective representation of X.\nFor example, under an L2-loss function, we wish to find\narg minW,B ‖FW,b(X)−X‖22\nsubject to a regulariziation penalty on the weights and offsets.\nIn an auto-encoder, for a training data set {X1, X2, . . .}, we set the target values as Yi = Xi. A static auto-encoder with two linear layers, akin to a traditional factor model, can be written as a\ndeep learner as\nz(2) = W (1)X + b(1),\na(2) = f2(z (2)), z(3) = W (2)a(2) + b(2),\nY = FW,b(X) = a(3) = f3(z (3)),\nwhere a(2), a(3) are activation levels. It is common to set a(1) = X. The goal is to learn the weight matrices W (1),W (2). If X ∈ RN , then W (1) ∈ RM,N and W (1) ∈ RN,M , where M N provides the auto-encoding at a lower dimensional level.\nIf W2 is estimated from the structure of the training data matrix, then we have a traditional factor model, and the W1 matrix provides the factor loadings. (We note that PCA in particular falls into this category, as we have seen in (3).) If W2 is estimated based on the pair X̂ = {Y,X} = X (which means estimation of W2 based on the structure of the training data matrix with the specific autoencoder objective), then we have a sliced inverse regression model. If W1 and W2 are simultaneously estimated based on the training data X, then we have a two layer deep learning model.\nA dynamic one layer auto-encoder for a financial time series (Yt) can, for example, be written as a coupled system of the form\nYt = WxXt +WyYt−1 and ( Xt Yt−1 ) = WYt .\nWe then need to learn the weight matrices Wx and Wy. Here, the state equation encodes and the matrix W decodes the Yt vector into its history Yt−1 and the current state Xt.\nThe auto-encoder demonstrates nicely that in deep learning we do not have to model the variancecovariance matrix explicitly, as our model is already directly in predictive form. (Given an estimated non-linear combination of deep learners, there is an implicit variance-covariance matrix, but that is not the driver of the method.)"
    }, {
      "heading" : "2.6 Long Short Term Memory Models (LSTMs)",
      "text" : "Traditional rectified neural nets (RNNs) can learn complex temporal dynamics via the set of deep recurrence equations\nZt = f(WxzXt +Wzz + bx),\nYt = f(WhzZt + bz),\nwhere Xt is the input, Zt is the hidden layer with N hidden units, and Yt is the output at time t. For a length T input sequence, the updates are computed sequentially.\nThough RNNs have proven successful on tasks such as speech recognition and text generation (see Dean et al. 2012 and Lake et al. 2016), they have difficulty in learning long-term dynamics, due in part to the vanishing and exploding gradients that can result from propagating the gradients down\nthrough the many layers (corresponding to time) of the recurrent network.\nLong-short-term-memories (LSTMs) are a particular form of recurrent network which provide a solution by incorporating memory units. This allows the network to learn when to forget previous hidden states and when to update hidden states given new information. Models with hidden units with varying connections within the memory unit have been proposed in the literature with great empirical success. Specifically, in addition to a hidden unit Zt, LSTMs include an input gate, a forget gate, an input modulation gate, and a memory cell. The memory cell unit combines the previous memory cell unit which is modulated by the forget and input modulation gate together with the previous hidden state, modulated by the input gate. These additional cells enable an LSTM architecture to learn extremely complex long-term temporal dynamics that a vanilla RNN is not capable of. Additional depth can be added to LSTMs by stacking them on top of each other, using the hidden state of the LSTM as the input to the next layer.\nAn architecture for an LSTM model might be\nFt = σ(W T f [Zt−1, Xt] + bf ),\nIt = σ(W T i [Zt−1, Xt] + bi),\nC̄t = tanh(W T c [Zt−1, Xt] + bc), Ct = Ft ⊗ Ct−1 + It ⊗ C̄t, Zt = Ot ⊗ tanh(Ct).\nThe key addition, compared to an RNN, is the hidden state Ct, the information is added or removed from the memory state via layers defined via a sigmoid function σ(x) = (1 + e−x)−1 and point-wise multiplication ⊗. The first gate Ft ⊗ Ct−1, called the forget gate, allows to throw away some data from the previous cell state. The next gate, It ⊗ C̄t, called the input gate, decides which values will be updated. Then the new cell state is a sum of the previous cell state, passed through the forgot gate selected components of the [Zt−1, Xt] vector. This provides a mechanism for dropping irrelevant information from the past and adding relevant information from the current time step. Finally, the output layer, Ot⊗ tanh(Ct), returns tanh applied to the hidden state with some of the entries removed.\nAn LSTM model might potentially improve predictors by utilizing data from the past by memorizing volatility patterns from previous periods. The LSTM model allows to automate the identification of the temporal relations in the data, at the cost of larger sets of parameters to be trained.\nThere are numerous finance applications of LTSM models. They provide a new class of volatility models that are capable of capturing long-memory effects in the underlying structure of asset return movements."
    }, {
      "heading" : "3 Finance Applications",
      "text" : "We now come to discuss deep learning specifically in the context of finance. For areas of finance applications, see Fama and French (1992, 2008), Engle (1982), Campbell, Lo, and MacKinley (1997), Singleton (2006), and Daniel and Titman (2006). Hutchison, Lo, and Poggio (1994) provide a shallow learner for option pricing."
    }, {
      "heading" : "3.1 Deep Factor Models versus Shallow Factor Models",
      "text" : "Almost all shallow data reduction techniques can be viewed as consisting of a low dimensional auxiliary variable Z and a prediction rule specified by a composition of functions\nŶ = fW1,b11 (f2(W2X + b2) )\n= fW1,b11 (Z), where Z := f2(W2X + b2).\nIn this formulation, we also recognize the previously introduced deep learning structure (2.1). The problem of high dimensional data reduction in general is to find the Z-variable and to estimate the\nlayer functions (f1, f2) correctly. In the layers, we want to uncover the low-dimensional Z-structure in a way that does not disregard information about predicting the output Y .\nPrincipal component analysis (PCA), reduced rank regression (RRR), linear discriminant analysis (LDA), project pursuit regression (PPR), and logistic regression are all shallow learners. See Wold (1956), Diaconis and Shahshahani (1984), Ripley (1996), Cook (2007), and Hastie et al. (2009) for further discussion.\nFor example, PCA reduces X to f2(X) using a singular value decomposition of the form\nZ = f2(X) = W >X + b , (3)\nwhere the columns of the weight matrix W form an orthogonal basis for directions of greatest variance (which is in effect an eigenvector problem). Similarly, for the case of X = (x1, . . . , xp), PPR reduces X to f2(X) by setting\nZ = f2(X) = N1∑ i=1 fi(Wi1X1 + . . .+WipXp) .\nAs stated before, these types of dimension reduction are independent of y and can easily discard information that is valuable for predicting the desired output. Sliced inverse regression (SIR) overcomes this drawback somewhat by estimating the layer function f2 using data on both, Y and\nX, but still operates independently of f1.\nDeep learning overcomes many classic drawbacks by jointly estimating f1 and f2 based on the full training data X̂ = {Yi, Xi}Ti=1, using information on Y and X as well as their relationships, and by using L > 2 layers. If we choose to use non-linear layers, we can view a deep learning routine as a hierarchical non-linear factor model, or, more specifically, as a generalized linear model (GLM) with recursively defined non-linear link functions.\nStock and Watson have used a similar approach when forecasting a single time series on inflation based on a large numbers of predictors. Another obvious application is cost effective indexing replication, where we are trying to create a small sub-portfolio which dynamics similar to the main index (see also Section 4)."
    }, {
      "heading" : "3.2 Default Probabilities",
      "text" : "Another area of great application of deep learning is credit risk analysis. The goal of a deep learning model is a feature representation of a high dimensional input space. For example, in image processing, one can think of the layers as first representing objects, then object parts (faces), then edges, and finally pixels. A similar feature map can be found for the credit-worthiness of companies. We can combine financial asset return data with text data (earnings calls) and accounting data (book values, etc.) to obtain an image of the health of a firm.\nSpecifically, suppose that our observations Yi represents a multi-class 1-of-K indicator vector. We equate classes via Y = k for 1 ≤ k ≤ K. For example, the K classes might correspond to bond ratings. At the extreme, we might have an indicator yi ∈ {0, 1} which indicates bankrupt or not.\nWe need to model the probability of default. Suppose that Yi ∈ {−1, 1} is categorical. Given the output X, it is common to model the probability of default via a soft-max (or logic) activation function\np(Yi |W, b,X) = 1\n1 + eŶ W,b(X) ,\nwhere Ŷ W,b(X) = W1f1(. . .+ b1). We define p̂(Xi) = arg max W,b p(Yi |W, b,Xi) as the maximum probability estimator.\nGiven a multinomial likelihood p(Y, Ŷ ), this then leads to a cross-entropy loss function\nL(Y, Ŷ ) = − log p(Y, Ŷ ) = −Y log p̂(X).\nAlternatively, in its natural parameter space, we have the log-odds as a deep predictor Ŷi = log(pi/(1− pi). We have a multi-class predictor\np(Yi = k|Ŷ Ŵ ,b̂i (X)) = σk(W1Z1),\nwhere σ(x) = 1/1 + ex. The negative log-likelihood is given by\nL(Yi, Ŷi) = − log p(Yi|Ŷi) = − log K∏ k=1 (Ŷi,k) Yi,k = − K∑ k=1 Yi,k log Ŷi,k.\nTherefore, minimizing the cross-entropy cost functional is equivalent to a multi-class logistic likelihood function The gain from a deep learning approach is our ability to include the kitchen-sink into the input space X. Feature extraction is the main output from a deep learner and these non-linear contrast of input variables provide summaries of the tendency for firms to default."
    }, {
      "heading" : "3.3 Event Studies",
      "text" : "Let y ∈ S denote an observed output variable, where S = RN for regression and S = {1, . . . ,K} for classification. We have a high dimensional input/covariate variable given by X = {Xt}Tt=1 and Xt ∈ RN×M .\nWe can build a deep learner for event study analysis as follows. Given a series of input event embeddings X = (X1, . . . , Xn), we can use a weight matrix W1 ∈ Rl to extract the l-possible events. For example, l = 4 for earnings announcements during the year, and n = 252 for number of trading days. We now construct a hidden factor\nZj = W > 1 Xj−l+1,\nso we can measure the effect of the l previous events on today’s return.\nWe might use a max-pooling activation function if we think that the effect is based solely on the largest value (i.e., maxZj), in which case the model ignores all other and focuses on the largest."
    }, {
      "heading" : "4 Example: Smart Indexing",
      "text" : "When aiming to replicate (or approximate) a stock index through a subset of stocks, there are two conceptual approaches we can choose from.\n(i) Identify a small group of stocks which historically have given a performance very similar to that of the observed index.\n(ii) Identify a small group of stocks which historically have represented an over-proportionally large part of the total aggregate information of all the stocks the index comprises of.\nWhile, on the face of things, (i) and (ii) may appear very similar, they characterize, in fact, very different methodologies.\nMany classic approaches to index replication are essentially rooted in linear-regression, which is part of group (i). Frequently, by trial and error, we are trying to find a small subset of stocks which in-sample gives a reasonable linear approximation of the considered index.\nThe deep learning version of (i) allows translating the input data through a hierarchical sequence of adaptive linear layers into a desired output, which means that, in training, even non-linear relationships can be readily identified. Since every hidden layer provides a new interpretation of the input features, we refer to the resulting strategy for approximation (or prediction) as a deep feature policy (DFP), an example of which is given in Figure 1.\nThe availability of tailored non-linear relationships in deep learning makes the conventional objective of (i), namely good in-sample approximation, an easily achieved triviality, and takes the focus straight to training for out-of-sample performance (which brings us back to cross-validation and dropout, see Section 2).\nAnother weakness of (i) is that it fits the finished (and through aggregation diluted) product. A deep auto-encoder avoids this problem by directly (rather than indirectly) approximating the aggregate information contained in the considered family of index stocks. In Figure 2, a deep autoencoder for a small set of ten stocks is depicted. In Figure 3, we see the stock paths before and after compression.∗\nThe bottleneck structure of an auto-encoder creates a compressed set of information from which all stocks are re-created (through linear and non-linear relationships). Thus, for indexing, the stocks which are closest to the compressed core of the index can be interpreted as a non-linear basis of the aggregate information of the considered family of stocks.\nIn Figure 4 at the top, we auto-encoded all stocks of the S&P500 over the period 2014/15. We then ranked the stocks by how close they were to their own auto-encoded version; the closer, the higher the communal information content of a stock. On the top right, we see the approximation of the S&P500 obtained by simply investing in the ten stocks with the highest communal information content.\nWe notice that, while the ten stock auto-encoder basis is reasonable, the approximation is a little off, particularly in the last seven months of the training period. It is instructive to observe how in-sample this deviation can easily be avoided by using a DFP index approximation.\nIn Figure 4 at the bottom, we combined the two sets of ten stocks with the highest and lowest communal information, respectively, and then trained a deep learning routine over the same period 2014/15 to approximate the S&P500 index based on this expanded basis of twenty stocks.† For every combination of inputs from the selected twenty stocks, the DFP gives us, based on the hierarchical composition of non-linear features extracted from the input data, an optimized action for the approximation of the desired index.\nGiven sufficient diversity of the input data, a DFP can often be trained to approximate the target data to almost arbitrarily accuracy, an improvement we now notice for the last six months of the training period in the bottom right chart in Figure 4.\nIn short, many classic models have had to focus on the wrong thing, namely in-sample approximation quality, due to their shortcomings in that area, while deep learning naturally addresses\n∗We use an auto-encoder with one hidden layer of 4 units and a sparsity constraint of ρ = 0.01 (to avoid training the identity function). †We use a deep neural net with (4,2) hidden layers.\nout-of-sample performance as optimization target.\nIn Figure 5, we apply our two example index trackers to the period 2010/14 as an out-of-sample test. We notice how the previously superior DFP approximation is unreliable, while the simple auto-encoder basis (made up of ten stocks rich in communal information) provides a consistent index replication. We conclude that, for index replication, auto-encoding as suggested by (ii) seems to be the more robust approach, and that the superior learning abilities of a DFP require careful handling in training to achieve the desired result."
    }, {
      "heading" : "5 Conclusion",
      "text" : "Deep learning presents a general framework for using large data sets to optimize predictive performance. As such, deep learning frameworks are well-suited to many problems – both practical and theoretical – in finance. This paper presents deep learning hierarchical decision models for problems in financial prediction and classification. As we have demonstrated, deep learning has the potential to improve – sometimes dramatically – on predictive performance in conventional applications. Our example on smart indexing in Section 4 presents just one way to implement deep learning models in finance. Many other applications remain for development.\nAt the same time, deep learning is likely to present significant challenges to current thinking in finance, including, most notably, the concept of market efficiency. Because it can model complex non-linearities in the data, deep learning may be able to price assets to within arbitrarily small pricing errors. Will this imply that markets are informationally efficient, or will new tests of market efficiency be necessary? Overall, it is unlikely that any theoretical models built from existing axiomatic foundations will be able to compete with the predictive performance of deep learning models. What this means for the future of financial economics remains to be seen.\nIn the meantime, deep learning models are likely to exert greater and greater influence in the practice of finance, particularly where prediction is paramount."
    } ],
    "references" : [ {
      "title" : "Statistical modeling: the two cultures (with comments and a rejoinder by the author)",
      "author" : [ "L. Breiman" ],
      "venue" : "Statistical Science,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2001
    }, {
      "title" : "A",
      "author" : [ "J.Y. Campbell" ],
      "venue" : "W. Lo and A. C. MacKinley: The econonmetrics of financial markets. Princeton University Press",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Titman: Market reactions to tangible and intangible information",
      "author" : [ "S.K. Daniel" ],
      "venue" : "Journal of Finance,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2006
    }, {
      "title" : "R",
      "author" : [ "J. Dean", "G. Corrado" ],
      "venue" : "Monga, et al.: Large scale distributed deep networks, Advances in Neural Information Processing Systems, pp. 1223-1231",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "On non-linear functions of linear combinations",
      "author" : [ "P. Diaconis", "M. Shahshahani" ],
      "venue" : "SIAM Journal on Scientific and Statistical Computing,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1984
    }, {
      "title" : "Autoregressive conditional heteroscedasticity with estimates of the variance of United Kingdom inflation, Econometrika",
      "author" : [ "R. Engle" ],
      "venue" : null,
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1982
    }, {
      "title" : "The cross-section of expected stock returns",
      "author" : [ "E.F. Fama", "K.R. French" ],
      "venue" : "Journal of Finance,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1992
    }, {
      "title" : "There exists a neural network that does not make avoidable mistakes",
      "author" : [ "A.R. Gallant", "H. White" ],
      "venue" : "IEEE International Conference on Neural Networks,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1988
    }, {
      "title" : "and J",
      "author" : [ "T. Hastie", "R. Tibshirani" ],
      "venue" : "Friedman: The elements of statistical learning, Vol 2",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Reducing the dimensionality of data with neural networks",
      "author" : [ "G.E. Hinton", "R.R. Salakhutdinov" ],
      "venue" : "Science, Vol",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2006
    }, {
      "title" : "and H",
      "author" : [ "K. Hornik", "M. Stinchcombe" ],
      "venue" : "White: Multilayer feedforward networks are universal 19 approximators, Neural networks, Vol. 2(5), pp. 359-366",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 1989
    }, {
      "title" : "and T",
      "author" : [ "J.M. Hutchinson", "A.W. Lo" ],
      "venue" : "Poggio: A Nonparametric approach to pricing and hedging derivative securities via learning networks, Journal of Finance, Vol. 48(3), pp. 851-889",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "The representation of continuous functions of many variables by superposition of continuous functions of one variable and addition, Dokl",
      "author" : [ "A. Kolmogorov" ],
      "venue" : "Akad. Nauk SSSR,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 1957
    }, {
      "title" : "Human-level concept learning through probabilistic program induction",
      "author" : [ "B.M. Lake", "R. Salakhutdinov", "J.B. Tenenbaum" ],
      "venue" : "Science, Vol. 3560,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2015
    }, {
      "title" : "G",
      "author" : [ "Y.A. LeCun", "L. Bottou" ],
      "venue" : "B. Orr, and K.-R. Muller: Efficient backprop, Neural networks: Tricks of the trade, pp. 948",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Bayesian Model Assessment in Factor Analysis",
      "author" : [ "H.F. Lopes", "M. West" ],
      "venue" : "Statistica Sinica,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2004
    }, {
      "title" : "Lorentz: The 13th problem of Hilbert",
      "author" : [ "G. G" ],
      "venue" : "Proceedings of Symposia in Pure Mathematics, American Mathematical Society,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 1976
    }, {
      "title" : "Girosi: Networks for approximation and learning",
      "author" : [ "F.T. Poggio" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 1990
    }, {
      "title" : "and B",
      "author" : [ "N.G. Polson", "J.G. Scott" ],
      "venue" : "T. Willard: Proximal algorithms in statistics and machine learning, Statistical Science, 30, 559-581",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "and M",
      "author" : [ "N.G. Polson", "B.T. Willard" ],
      "venue" : "Heidari: A statistical theory for Deep Learning",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Ripley: Pattern recognition and neural networks",
      "author" : [ "D. B" ],
      "venue" : null,
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 1996
    }, {
      "title" : "Singelton: Empirical Dynamic Asset Pricing",
      "author" : [ "J. K" ],
      "venue" : null,
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2006
    }, {
      "title" : "Dropout: a simple way to prevent neural networks from overfitting",
      "author" : [ "Srivastava" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2014
    }, {
      "title" : "Causal inference from observational data: a review of end and means, Journal of the Royal Statistical Society, Series A (General)",
      "author" : [ "H. Wold" ],
      "venue" : null,
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 1956
    } ],
    "referenceMentions" : [ ],
    "year" : 2016,
    "abstractText" : "We explore the use of deep learning hierarchical models for problems in financial prediction and classification. Financial prediction problems – such as those presented in designing and pricing securities, constructing portfolios, and risk management – often involve large data sets with complex data interactions that currently are difficult or impossible to specify in a full economic model. Applying deep learning methods to these problems can produce more useful results than standard methods in finance. In particular, deep learning can detect and exploit interactions in the data that are, at least currently, invisible to any existing financial economic theory.",
    "creator" : "LaTeX with hyperref package"
  }
}