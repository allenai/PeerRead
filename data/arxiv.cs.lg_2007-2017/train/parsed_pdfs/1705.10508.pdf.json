{
  "name" : "1705.10508.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Implications of Decentralized Q-learning Resource Allocation in Wireless Networks",
    "authors" : [ "Francesc Wilhelmi", "Boris Bellalta", "Cristina Cano", "Anders Jonsson" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n70 5.\n10 50\n8v 2\n[ cs\n.N I]\n2 9\nA ug\n2 01\nI. INTRODUCTION\nReinforcement Learning (RL) has recently spread use in the wireless communications field to solve many kinds of problems such as Access Point (AP) association [1], channel selection [2] or transmit power adjustment [3], as it allows learning good-performing configurations only from the observed results. Among these, Q-learning has been applied to dynamic channel assignment in mobile networks in [4] and to automatic channel selection in Femto Cell networks in [5]. However, to the best of our knowledge, the case of a fully decentralized scenario where nodes do not have knowledge from each other, has not yet been considered.\nIn this work we propose a stateless variation of Q-learning in which nodes select the transmission power and channel to use solely based on their resulting throughput. We concentrate on a fully decentralized scenario where no information about the actions and resulting performance of the other nodes is available to the learners. Note that inferring the throughput of neighbouring nodes allocated to different channels is costly as periodic sensing in the other channels would then be needed. We aim to characterize the performance of Q-learning in such scenarios, obtaining insight on the most played actions (i.e., channel and transmit power selected) and the resulting performance. We observe that when no information about the neighbours is available to the learners, these will tend to apply selfish strategies that result in alternating good/poor performance depending on the actions of the others. In such scenarios, we show that the use of Q-learning allows each\nnetwork to find the best-performing actions, though without reaching a steady solution. Note that achieving a steady solution in a decentralized environment relies in finding a Nash Equilibrium, a concept used in Game Theory to define a set of individual strategies that maximize the profits of each player in a non-cooperative game, regardless of the others’ strategy. Formally, a set of best player actions a∗ = (a∗1, ..., a ∗ n) ∈ A leads to a Nash Equilibrium if a∗i ∈ Bi(a∗−i), ∀i ∈ N , where Bi(a−i) is the best response to the others actions (a−i). Thus, the consequences of not reaching a Nash Equilibrium can have an impact on performance variability.\nIn addition, we look at the resulting performance in terms of throughput when varying several parameters intrinsic to the learning algorithm, which helps in understanding the interactions between the degree of exploration and learning rate, and the variability of the resulting performance.\nThe remaining of this document is structured as follows: Section II introduces the simulation scenario and considerations. Then, Section III presents our Stateless variation of Q-learning and its practical implementation for the resource allocation problem in Wireless Networks (WNs). Simulation results are later discussed in Section IV. Finally, some final remarks are provided in Section V."
    }, {
      "heading" : "II. SYSTEM MODEL",
      "text" : "For the remainder of this work, we consider a scenario in which several WNs are placed in a 3D-map (with parameters described later in Section IV-A), each one formed by an Access Point (AP) transmitting to a single Station (STA) in downlink manner."
    }, {
      "heading" : "A. Channel modelling",
      "text" : "Path-loss and shadowing effects are modelled using the log-distance model for indoor communications. The path-loss between WN i and j is given by\nPLi,j = Ptx,i − Prx,j =\n= PL0 + 10αPL log10(di,j) + Gs + di,j dobs Go,\nwhere Ptx,i is the transmitted power in dBm by WN i, Prx,j is the power in dBm received in WN j, PL0 is the path-loss at one meter in dB, αPL is the path-loss exponent, di,j is the distance between the transmitter and the receiver in meters, Gs\nis the shadowing loss in dB, and Go is the obstacles loss in dB. Note that we include the factor dobs, which is the distance between two obstacles in meters."
    }, {
      "heading" : "B. Throughput calculation",
      "text" : "By using the power received and the interference, we calculate the maximum theoretical throughput of each WN i at time t ∈ {1, 2...} by using the Shannon Capacity.\nΓi,t = B log2(1 + SINRi,t),\nwhere B is the channel bandwidth and the experienced Signal to Interference plus Noise Ratio (SINR) is given by:\nSINRi,t = Pi,t\nIi,t + N ,\nwhere Pi,t and Ii,t are the received power and the sum of the interference at WN i at time t, respectively, and N is the floor noise power. For each STA in a WN, the interference is considered to be the total power received from all the APs of the other coexisting WNs as if they were continuously transmitting. Adjacent channel interference is also considered in Ii,t, i ∈ {1, ..,W}, where W is the number of neighbouring WNs. We consider that the transmitted power leaked to adjacent channels is 20 dBm lower for each channel separation."
    }, {
      "heading" : "III. DECENTRALIZED STATELESS Q-LEARNING FOR ENHANCING SPATIAL REUSE IN WNS",
      "text" : "Q-learning [6, 7] is an RL technique that enables an agent to learn the optimal policy to follow in a given environment. A set of possible states describing the environment and actions are defined in this model. In particular, an agent maintains an estimate of the expected long-term discounted reward for each state-action pair, and selects actions with the aim of maximizing it. The expected cumulative reward Vπ(s) is given by:\nVπ(s) = lim N→∞ E\n( N ∑\nt=1\nrπt (s) ) ,\nwhere rπt (s) is the reward obtained at iteration t after starting from state s and by following policy π. Since the reward may easily get unbounded, a discount factor parameter (γ < 1) is used. The optimal policy π∗ that maximizes the total expected reward is given by the Bellman’s Optimality Equation [6]:\nQ∗(s, a) = E { rt+1 + γmaxa′Q ∗(st+1, a ′)|st = s, at = a } .\nHenceforth, Q-learning receives information about the current state-action tuple (st, at), the generated reward rt and the next state st+1, in order to update the Q-table: Q̂(st, at)← (1−αt)Q̂(st, at)+αt ( rt+γ (\nmax a′\nQ̂(st+1, a ′) )\n)\n,\nwhere αt is the learning rate at time t, and max a′\nQ̂(st+1, a ′)\nis the best estimated value for the next state st+1. The optimal solution is theoretically achieved with probability 1 if ∑∞\nt=0 αt = ∞, and ∑∞ t=0 α 2 t < ∞, which satisfies that\nlim t→∞\nQ̂(s, a) = Q∗(s, a). Since we focus on a completely\ndecentralized scenario where no information about the other nodes is available, the system can then be fully described by the set of actions and rewards.1 Thus, we propose using a stateless variation of the original Q-learning algorithm. To implement decentralized learning to the resource allocation problem, we consider each WN to be an agent running Stateless Q-learning through an ε-greedy action-selection strategy, so that actions a ∈ A correspond to all the possible configurations that can be chosen with respect to the channel and transmit power. During the learning process we assume that WNs select actions sequentially, so that at each learning iteration, every agent takes an action in an ordered way. The order at which WNs choose an action at each iteration is randomly selected at the beginning of it. The reward after choosing an action is set as:\nri,t = Γi,t Γ∗i ,\nwhere Γi,t is the experienced throughput at time t by WN i ∈ {1, ..., n}, being n the number of WNs in the scenario, and Γ∗i = B log2(1 + SNRi) is WN i maximum achievable throughput (i.e., when it uses the maximum transmission power and there is no interference). Each WN applies the Stateless Q-learning as follows:\n• Initially, it sets the estimates of its actions k ∈ {1, ...,K} to 0: Q̂(ak) = 0. • At each iteration, it applies an action by following the εgreedy strategy, i.e., it selects the best-rewarding action\nwith probability 1 − εt, and a random one (uniformly distributed) the rest of the times. • After choosing action ak, it observes the generated reward (the relative experienced throughput), and updates the\nestimated value Q̂(ak). • Finally, εt is updated to follow a decreasing sequence:\nεt = ε0√ t .\nNote, as well, that the optimal policy cannot be derived for the presented scenario, but it can be approximated to enhance spatial reuse. This is due to the nature of the presented environment, as well as WNs decisions affect the others performance. Formally, the implementation details of Stateless Q-learning are described in Algorithm 1. The presented learning approach is intended to operate at the PHY level, allowing the operation of the current MAC-layer communication standards (e.g., in IEEE 802.11 WLANs, the channel access is governed by the CSMA/CA operation, so that Stateless Q-learning may contribute to improve spatial reuse at the PHY level)."
    }, {
      "heading" : "IV. PERFORMANCE EVALUATION",
      "text" : "In this section we introduce the simulation parameters and\ndescribe the experiments.2 Then, we show the main results.\n1We note that local information such as the observed instantaneous channel quality could be incorporated in the state definition. However, such a description of the system entails increased complexity.\n2The code used for simulations can be found at https://github.com/wn-upf/Decentralized Qlearning Resource Allocation in WNs.git (Commit: eb4042a1830c8ea30b7eae3d72a51afe765a8d86).\nAlgorithm 1: Stateless Q-learning\n1 Function Stateless Q-learning (SINR,A); Input : SINR: Signal-to-Interference-plus-Noise Ratio\nsensed at the STA\nA: set of possible actions in {1, ..., K} Output: Γ: Mean throughput experienced in the WN\n2 initialize: t = 0, Q̂(ak) = 0, ∀ak ∈ A 3 while active do\n4 Select ak\n\n\n\nargmax k=1,...,K\nQ̂(ak), with prob 1− ε\ni ∼ U(1,K), otherwise 5 Observe reward rak = Γak,t\nΓ∗\n6 Q̂(ak)← Q̂(ak) + α · ( rak + γ ·max Q̂− Q̂(ak) )\n7 εt ← ε0/ √ t 8 t← t+ 1 9 end"
    }, {
      "heading" : "A. Simulation Parameters",
      "text" : "According to [8], a typical high-density scenario for residential buildings contains 0.0033APs/m3. We then consider a map scenario with dimensions 10× 5 × 10 m containing 4 WNs that form a grid topology in which STAs are placed at the maximum possible distance from the other networks. This toy scenario allows us to study the performance of Stateless Q-learning in a controlled environment , which is useful to check the applicability of RL in WNs by only using local information 3. We consider that the number of channels is equal to half the number of coexisting WNs, so that we can study a challenging situation regarding the spatial reuse. Table I details the parameters used."
    }, {
      "heading" : "B. Optimal solution",
      "text" : "We first identify the optimal solutions that maximize: i) the aggregate throughput, and ii) the proportional fairness, which is computed as the logarithmic sum of the throughput experienced by each WN, i.e., PF = max k∈A ∑ i log(Γi,k). The\n3The analysis of the presented learning mechanisms in more congested scenarios is left as future work.\noptimal solutions are listed in Table II. Note that, since the considered scenario is symmetric, there are two equivalent solutions. Note, as well, that in order to maximize the aggregate network throughput two of the WNs sacrifice themselves by choosing a lower transmit power. This result is then not likely to occur in an adversarial selfish setting.\nC. Input Parameters Analysis\nWe first analyse the effects of modifying α (the learning rate), γ (the discount factor) and ε0 (the initial exploration coefficient of the ε-greedy update rule) with respect to the achieved network throughput. We run simulations of 10000 iterations and capture the results of the last 5000 iterations to ensure that the initial transitory phase has ended. Each simulation is repeated 100 times for averaging purposes. Figure 1 shows the average aggregate throughput achieved for each of the proposed combinations. It can be observed that the best results with respect to the aggregate throughput, regarding both average and variance, are achieved when α = 1, γ = 0.95 and ε0 = 1. This means that for achieving the best results (i.e., high average aggregate throughput and low variance), the immediate reward of a given action must be considered rather than any previous information (α = 1). We see that the difference between the pay-off offered by the best action and the current one must also be high (γ = 0.95). In addition, exploration must be highly boosted at the beginning (ε0 = 1). For this setting, the resulting throughput (902.739 Mbps) represents 80.29% of the one provided by the optimal configuration that maximizes the aggregate throughput (shown in Table II). Regarding proportional fairness, the algorithm’s resulting throughput is only 1.32% higher than the optimal. We also evaluate the relationship between different values of α and γ in the average aggregate throughput and standard deviation (shown in Figure 2). We observe a remarkably higher aggregate throughput when α > γ. We also see that the variability between different simulation runs is much lower when the average throughput is higher. Additionally, we note a peak in the standard deviation when γ ≈ α and γ > α. To further understand the effects of modifying each of the aforementioned parameters, we show for different ε0, α and γ: i) the individual throughput experienced by each WN during the total 10000 iterations of a single simulation run (Figure 3),\n= 0.5 = 0.5\nii) the average throughput experienced by each WN for the last 5000 iterations, also for a single simulation run (Figure 4), and iii) the probability of choosing each action at each WN (Figure 5). We observe the following aspects:\n• In Figure 3 a high variability of the throughput experienced by each WN can be observed, specially if\nǫ0 is high (as in Figures 3(a), 3(c)). A high degree of exploration allows WNs to discover changes in the resulting performance of their actions due to the activity of the other nodes, which at the same time generates more variability (WN adapt to changes in the environment). • Despite the variability generated, we obtain fairer results for high ǫ0 (Figure 4). Henceforth, there is a relationship between the variability generated and the average\nthroughput fairness.\n• Finally, in Figures 5(a) and 5(c) we observe that for the former, there are two favourite actions that are being\nplayed the most, but for the latter there is only one preferred action. The lower the learning rate (α), and consequently the discount factor (γ), the higher the probability of choosing a unique action, which results to be the one that provided the best performance in the past. The opposite occurs for higher α and γ values, since giving more importance to the immediate reward allows for a reaction only to the recently-played actions of the neighbouring nodes: the algorithm is short-sighted."
    }, {
      "heading" : "V. CONCLUSIONS",
      "text" : "Decentralized Q-learning can be used to improve spatial reuse in dense wireless networks, enhancing performance as a result of exploiting the most rewarding actions. We have shown in this article, by means of a toy scenario, that Stateless Qlearning in particular allows finding good-performing configurations that achieve close-to-optimal (in terms of throughput maximization and proportional fairness) solutions.\nHowever, the competitiveness of the presented fullydecentralized environment involves the non-existence of a Nash Equilibrium. Thus, we have also identified high variability in the experienced individual throughput due to the constant changes of the played actions, motivated by the fact that the reward generated by each action changes according to the opponents’ ones. We have evaluated the impact of the parameters intrinsic to the learning algorithm on this variability showing that it can be reduced by decreasing the exploration degree and learning rate. The individual reduction on the throughput variability occurs at the expense of losing aggregate performance.\nThis variability can potentially result in negative effects on the overall WN’s performance. The effects of such a fluctuation in higher layers of the protocol stack can have severe consequences depending on the time scale at which they occur. For example, noticing high throughput fluctuations may trigger congestion recovery procedures in TCP (Transmission Control Protocol), which would harm the experienced performance.\nWe left for future work to further extend the decentralized approach in order to find collaborative algorithms that allow the neighbouring WNs to reach an equilibrium that grants acceptable individual performance. Acquiring any kind of knowledge about the neighbouring WNs is assumed to solve the variability issues arisen from decentralization. This information may be directly exchanged or inferred from observations. Furthermore, other learning approaches are intended\nto be analysed in the future for performance comparison in the resource allocation problem."
    }, {
      "heading" : "ACKNOWLEDGMENT",
      "text" : "This work has been partially supported by the Spanish Ministry of Economy and Competitiveness under the Maria de Maeztu Units of Excellence Programme (MDM-2015-0502), and by the European Regional Development Fund under grant TEC2015-71303-R (MINECO/FEDER)."
    } ],
    "references" : [ {
      "title" : "A distributed access point selection algorithm based on no-regret learning for wireless access networks",
      "author" : [ "Chen", "May" ],
      "venue" : "In Vehicular Technology Conference (VTC 2010-Spring),",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2010
    }, {
      "title" : "Channel selection for networkassisted D2D communication via no-regret bandit learning with calibrated forecasting",
      "author" : [ "S. Maghsudi", "S. Staczak" ],
      "venue" : "IEEE Transactions on Wireless Communications,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2015
    }, {
      "title" : "Joint channel selection and power control in infrastructureless wireless networks: A multiplayer multiarmed bandit framework",
      "author" : [ "S. Maghsudi", "S. Staczak" ],
      "venue" : "IEEE Transactions on Vehicular Technology,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2015
    }, {
      "title" : "A Q-learning-based dynamic channel assignment technique for mobile communication systems",
      "author" : [ "J. Nie", "S. Haykin" ],
      "venue" : "IEEE Transactions on Vehicular Technology,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1999
    }, {
      "title" : "December). A Q-learning based approach to interference avoidance in self-organized femtocell networks",
      "author" : [ "M. Bennis", "D. Niyato" ],
      "venue" : "In GLOBECOM Workshops (GC Wkshps),",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2010
    }, {
      "title" : "Reinforcement learning: An introduction (Vol",
      "author" : [ "R.S. Sutton", "A.G. Barto" ],
      "venue" : null,
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1998
    }, {
      "title" : "IEEE 802.11 ax: High-efficiency WLANs.",
      "author" : [ "B. Bellalta" ],
      "venue" : "IEEE Wireless Communications",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "INTRODUCTION Reinforcement Learning (RL) has recently spread use in the wireless communications field to solve many kinds of problems such as Access Point (AP) association [1], channel selection [2] or transmit power adjustment [3], as it allows learning good-performing configurations only from the observed results.",
      "startOffset" : 172,
      "endOffset" : 175
    }, {
      "referenceID" : 1,
      "context" : "INTRODUCTION Reinforcement Learning (RL) has recently spread use in the wireless communications field to solve many kinds of problems such as Access Point (AP) association [1], channel selection [2] or transmit power adjustment [3], as it allows learning good-performing configurations only from the observed results.",
      "startOffset" : 195,
      "endOffset" : 198
    }, {
      "referenceID" : 2,
      "context" : "INTRODUCTION Reinforcement Learning (RL) has recently spread use in the wireless communications field to solve many kinds of problems such as Access Point (AP) association [1], channel selection [2] or transmit power adjustment [3], as it allows learning good-performing configurations only from the observed results.",
      "startOffset" : 228,
      "endOffset" : 231
    }, {
      "referenceID" : 3,
      "context" : "Among these, Q-learning has been applied to dynamic channel assignment in mobile networks in [4] and to automatic channel selection in Femto Cell networks in [5].",
      "startOffset" : 93,
      "endOffset" : 96
    }, {
      "referenceID" : 4,
      "context" : "Among these, Q-learning has been applied to dynamic channel assignment in mobile networks in [4] and to automatic channel selection in Femto Cell networks in [5].",
      "startOffset" : 158,
      "endOffset" : 161
    }, {
      "referenceID" : 5,
      "context" : "DECENTRALIZED STATELESS Q-LEARNING FOR ENHANCING SPATIAL REUSE IN WNS Q-learning [6, 7] is an RL technique that enables an agent to learn the optimal policy to follow in a given environment.",
      "startOffset" : 81,
      "endOffset" : 87
    }, {
      "referenceID" : 5,
      "context" : "The optimal policy π∗ that maximizes the total expected reward is given by the Bellman’s Optimality Equation [6]: Q∗(s, a) = E {",
      "startOffset" : 109,
      "endOffset" : 112
    }, {
      "referenceID" : 6,
      "context" : "Simulation Parameters According to [8], a typical high-density scenario for residential buildings contains 0.",
      "startOffset" : 35,
      "endOffset" : 38
    } ],
    "year" : 2017,
    "abstractText" : "Reinforcement Learning is gaining attention by the wireless networking community due to its potential to learn goodperforming configurations only from the observed results. In this work we propose a stateless variation of Q-learning, which we apply to exploit spatial reuse in a wireless network. In particular, we allow networks to modify both their transmission power and the channel used solely based on the experienced throughput. We concentrate in a completely decentralized scenario in which no information about neighbouring nodes is available to the learners. Our results show that although the algorithm is able to find the best-performing actions to enhance aggregate throughput, there is high variability in the throughput experienced by the individual networks. We identify the cause of this variability as the adversarial setting of our setup, in which the most played actions provide intermittent good/poor performance depending on the neighbouring decisions. We also evaluate the effect of the intrinsic learning parameters of the algorithm on this variability.",
    "creator" : "LaTeX with hyperref package"
  }
}