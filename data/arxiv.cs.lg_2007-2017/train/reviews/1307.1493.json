{"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Jul-2013", "title": "Dropout Training as Adaptive Regularization", "abstract": "Dropout and other feature noising schemes control overfitting by artificially corrupting the training data. For generalized linear models, dropout performs a form of adaptive regularization. Using this viewpoint, we show that the dropout regularizer is first-order equivalent to an L2 regularizer applied after scaling the features by an estimate of the inverse diagonal Fisher information matrix. We also establish a connection to AdaGrad, an online learner, and find that a close relative of AdaGrad operates by repeatedly solving linear dropout-regularized problems. By casting dropout as regularization, we develop a natural semi-supervised algorithm that uses unlabeled data to create a better adaptive regularizer. We apply this idea to document classification tasks, and show that it consistently boosts the performance of dropout training, improving on state-of-the-art results on the IMDB reviews dataset.", "histories": [["v1", "Thu, 4 Jul 2013 21:33:56 GMT  (62kb,D)", "https://arxiv.org/abs/1307.1493v1", "10 pages"], ["v2", "Fri, 1 Nov 2013 17:56:35 GMT  (67kb,D)", "http://arxiv.org/abs/1307.1493v2", "11 pages. Advances in Neural Information Processing Systems (NIPS), 2013"]], "COMMENTS": "10 pages", "reviews": [], "SUBJECTS": "stat.ML cs.LG stat.ME", "authors": ["stefan wager", "sida i wang", "percy liang"], "accepted": true, "id": "1307.1493"}
