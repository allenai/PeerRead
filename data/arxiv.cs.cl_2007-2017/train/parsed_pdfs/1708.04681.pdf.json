{
  "name" : "1708.04681.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Identifying Harm Events in Clinical Care  through Medical Narratives",
    "authors" : [ "Arman Cohan", "Allan Fong", "Raj Ratwani", "Nazli Goharian" ],
    "emails" : [ "arman@ir.cs.georgetown.edu", "allan.fong@medstar.net", "raj.m.ratwani@medstar.net", "nazli@ir.cs.georgetown.edu", "permissions@acm.org." ],
    "sections" : [ {
      "heading" : null,
      "text" : "KEYWORDS Natural Language Processing; Medical Text; Deep Learning"
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "Preventable medical errors have been shown to be a major cause of injury and death in the United States [9, 24, 34]. Medical errors are estimated to be the 3rd leading causes of death in the U.S. [24, 30], which translates to an estimated incidence of 210,000 to 400,000 deaths annually [17, 24]. To address these major concerns, healthcare systems have adopted reporting systems in clinical care to help track and trend hazards and errors in patient care [25, 34]. The data from these systems are later used to identify the causes of harm and actions that should be taken to prevent similar situations. These reporting systems allow frontline clinicians to report events that are relevant to patient care including both near misses and serious safety events. Near misses are events or situations where a hazard was identi ed before a patient could be harmed. For example,\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. ACM-BCB’17, August 20-23, 2017, Boston, MA, USA. © 2017 ACM. 978-1-4503-4722-8/17/08. . . $15.00 DOI: http://dx.doi.org/10.1145/3107411.3107485\na wrong medication order that was never administered to a patient would be considered a near miss, hence reported as a no-harm event. Serious safety events on the other hand are situations where a patient was harmed. The above example would be considered a patient harm event if the nurse had actually administered the medication causing additional treatment, monitoring, or irreversible e ects on the patient.\nAlthough reporting systems have been implemented with the goal of improving patient safety and patient care, hospital sta are faced with many challenges in analyzing and understanding these reports [23, 25]. These reports which are narratives in natural language are generated by frontline sta and vary widely in content, structure, language used, and style. These reports include a textual eld where the clinicians describe the safety event and its details in free-form text. While these texts provide valuable information about the safety event, it is challenging to perform large scale analysis of these narratives to identify important safety events. In this paper, we propose and evaluate Natural Language Processing (NLP) methods to identify cases that caused harm to the patient based on medical narratives.\nOne important aspect in patient care is to identify events that have contributed to or resulted in harm to the patient [34]. There have been many e orts in characterizing harm to the patients based on their severity. The most common is a harm categorical system that indicates the severity of the harm to the patient [1]. These categories range from an unsafe condition (which describes an event where there was no error, but had capacity to cause harm) to death (which is an event where an error has caused or contributed to the death of a patient). These harm categories are described in Table 1 in detail [1].\nThe patient incident narratives can be complex and it is challenging to identify the cases of harm from these reports. These reports often consist of multiple events. For example, consider a case where a patient is found on the oor in the emergency department (ED) with no physical signs of injury. This is initially entered as a no-harm case. However, later when the patient is transferred to the radiology for an x-ray as a precaution, a small fracture is discovered from the x-ray. Therefore, while the ED sta originally entered the event as a no-harm event, the radiology department would revise this as a harm event.\nFor these reasons, reporting harm is often miscategorized. While most events are eventually recategorized by a department manager\nar X\niv :1\n70 8.\n04 68\n1v 1\n[ cs\n.C L\n] 1\n5 A\nug 2\nor patient safety o cers who have a more global perspective of events, this recategorization incurs additional time, resources, and expenses leading to missed opportunities to address the actual event in a timely fashion. We present a method for identifying the severity of harm from narratives regarding incidents in patient care. While there is a growing number of work in categorizing patient safety reports, none has looked at the modeling of general harm across all event types [12, 26]. Our method is based on a neural network model consisting of several layers including a convolutional layer, a recurrent layer, and an attention mechanism to improve the performance of the recurrent layer. Our method is designed to capture local signi cant features as well as the interactions and dependencies between the features in long sequences. Traditional methods in general and domain speci c NLP rely heavily on engineering a set of representative features for the task and utilizing external knowledge and resources. While these models have been shown to work reasonably well for di erent tasks, their success relies on the type of features that they utilize. Apart from the feature engineering e orts, these approaches usually model the problem with respect to certain selected features and ignore other indicators and signals that might improve prediction. In contrast, our approach only relies on the text in the patient incident narratives and it does not rely on any features or external resources, making it generalizable. Through extensive evaluation on two large datasets, we show that our proposed method is able to signi cantly outperform the existing approaches of identifying harm in clinical care. E ective identi cation of harm can help the hospital sta save time both during analysis and reporting. Furthermore, a more accurate and immediate classi cation of harm can also help to better prioritize resources to address safety incidents, which subsequently improves general patient care."
    }, {
      "heading" : "2 RELATEDWORK",
      "text" : "There has been a growing number of work in categorizing patient incident and safety narratives in clinical care. Fong et al. [12] explored both the unstructured free-text and structured data elements in safety reports to identify and rank similar events. They evaluated di erent search methods utilizing bag of words features, structured elements, and topic modeling features to rank and identify similar events. In another work, Ong et al. [26] explored the similar problem of identifying extreme-risk events in patient safety and\nincident reports using Naive Bayes and SVM classi ers with bag of words features. In contrast to these works, we focus on the problem of identifying harm and categorizing the harm based on its severity in medical narratives. We present neural network methods that are able to capture information from the complex narratives regarding safety events without utilizing any external features. We compare our results with feature based methods and show that our proposed methods are signi cantly superior in identifying and classifying harm in patient incident reports.\nThe problem of identifying harm in patient safety reports is a type of text classi cation problem. Traditional approaches in text classi cation include methods to extract features from text and then use the feature vector as an input to a classi er such as SVM [2]. More recently, neural networks have shown success in many NLP tasks including text classi cation. Two of the more widely used neural network architectures have been Convolutional Neural Networks (CNN) [21] and Recurrent Neural Networks (RNN) [11]. Collobert et al. [7] were one of the rst to utilize CNNs in many NLP tasks including text classi cation. In particular, they proposed a CNN architecture which operated on one-hot encodings of words; their model was based on the original CNN architecture of LeCun et al. [21] with adaptations to the NLP domain and showed improvements on several NLP tasks. Later CNNs were further explored for sentence modeling and classi cation tasks [10, 18, 19].\nIn the biomedical domain, there have been many e orts in classifying biomedical text and narratives based on di erent tasks. Many works have looked at the speci c problem of indexing biomedical literature using MeSH1 terms. These works have mostly used supervised learning frameworks with bag of words features, namedentities, and ontology speci c features [37]. More recently, Rios and Kavuluru [28] utilized CNNs for this task; they showed that CNNs are more e ective than feature based methods in biomedical indexing. The authors in [6] used a neural network architecture using CNN and RNNs to classify patient safety report. Xu et al. [35] used a CNN architecture, with multiple sources of word embeddings and evaluated its e ectiveness on the tasks of biomedical literature indexing and clinical note annotation. Our method, in contrast, is based on an extension of CNNs with recurrent layer as well as an attention model to improve performance on longer\n1Medical Subject Heading\nsequences. We compare our methods with a CNN baseline and show that our methods can signi cantly outperform the baselines. Our focus is on the challenging task of identifying harm in patient incident reports where the incident narratives are often complex, consisting of multiple chained events in a single narrative. Our proposed model, is designed to capture these complexities."
    }, {
      "heading" : "3 METHODS",
      "text" : "We present a general neural network architecture for identifying harm in patient safety reports. As explained in §1, the narratives regarding patient safety can be complex and identifying harm to the patient is challenging in these reports. To be able to perform this task e ectively, we will need a model that is able to capture both local features as well as the language usage in the entire report. To achieve this goal, we propose a neural network consisting of several layers where each layer is designed to address the aforementioned challenges. Our approach does not require feature engineering and it learns to identify signi cant features from the raw text automatically. We rst describe the general outline of our model and then we describe each component in more detail.\nThe outline of the model. The proposed architecture is shown in the Figure 1. The input report is rst pre-processed and represented as a matrix corresponding to word embeddings. Word emdeddings or distributed representations of words aim to embed (represent) words with dense vectors such that words with similar properties have similar vectors [4]. These embeddings can be general and pre-trained or can be trained according to the task at hand. Then a convolutional layer extracts the signi cant local features that are helpful for identifying harm in the report. Next, a recurrent layer captures the interactions of the local features along the entire sequence of the words in the report. In the next layer, we propose an attention model which serves to overcome the problem of recurrent networks in compressing an entire sequence in a single vector by focusing the attention to the important timesteps (steps of the sequence) in the recurrent layer. Finally, the output of the attention model is a vector which is passed to a fully connected layer and a\nsoftmax classi er identi es the level of harm associated with the report. We now explain each of these layers in detail."
    }, {
      "heading" : "3.1 Embedding layer",
      "text" : "This layer pre-processes the raw text corresponding to the medical report and represents it as a matrix of real valued numbers. This matrix consists of embeddings of the words in the report. We tokenize the text using a simple white space tokenizer and we lowercase all the words. We then transform the input sequence of tokens into a sequence of dense distributional vectors. Speci cally, given a sequence of tokensW whereW = 〈w1,w2, ...,wn〉 and wi ’s are the input sequence tokens, the embedding layer represents each token wi as a d dimensional vector xi , and the sequenceW will be represented as a matrix of real valued numbers X with dimensions of X ∈ R(nmax×d ) where nmax is the maximum sequence length. Text inputs with length larger than nmax will be cropped and text inputs shorter than the nmax are padded with zeros. The value of nmax is determined empirically."
    }, {
      "heading" : "3.2 Convolutional layer",
      "text" : "Convolutional layer is repsonsible for extracting local features from the input text. Convolutional Neural Networks (CNNs) [21] have been previously used in sentence modeling and classi cation tasks [18, 19]. A CNN is a neural network that consists of two main operations: convolution and pooling. A convolution is an operation between two functions f and д where f is the primary vector and д is the lter. The convolution operation between f and д, evaluated at entry n is represented as: (f ∗ д)[n] = ∑Ki=−K f [n − i] × д[i] Where ∗ denotes the convolution operation and L = 2K − 1 is the length of the lter. Here, f is the input to the convolution (word vectors obtained from the embedding layer).\nFeatures are extracted by convolution of the input text with a number of linear lters, adding a bias term and applying a nonlinearity. The result is called a feature map. The trained weights in these lters correspond to a linguistic feature detector that learns to recognize a speci c class of n-grams where L ≤ n. A max-pooling\noperation is used after the convolution to extract the signi cant features.\nMultiple feature maps. Similar to convolutional networks for object recognition [21], we use multiple feature maps with di erent lters to capture various aspects of the input sequence. Figure 2 illustrates how the feature maps are constructed from the input. First the convolution and non-linearity are applied to the input and then the max pooling derives the resulting feature maps. The nal output of this layer at each time-step is the concatenation of the feature maps at that time-step."
    }, {
      "heading" : "3.3 Recurrent layer",
      "text" : "The result of the convolution layer is a sequence of vectors each of which is the concatenation of the feature maps at corresponding time-step. Convolutional layer is able to extract signi cant local features that are important for our task. However, the interactions between the words are not captured specially if the words are distant from each others. Recurrent Neural Networks (RNNs) are a family of neural networks that are designed to process a sequence of values. We use an RNN on top of the result of the convolution layer to capture interactions along the entire sequence of words. RNNs are an extension of multilayer perceptrons in which the output of each step is used as an additional input to the next step. Speci cally, the activations arrive at the hidden layer of the network from both the current external input and the hidden layer activations one step back in time. The general formulation of an RNN is as follows:\nh(t) = д(W (h)h(t − 1) +W (x )x(t)) (1a) ŷ(t) = so max(W (s)h(t)) (1b)\nWhere h(t) shows the hidden state of the RNN in time step t , x(t) is the input sequence at time step t ,W (h),W (x ), andW (s) are the weights associated with the hidden state, input, and softmax, respectively, and д is an activation function such as RELU [8].\nIn sequence modeling tasks, the nal hidden state of the network can represent the whole sequence and can be used for making predictions [13]. This nal hidden state, in theory, can capture all the information in the entire sequence. This is because the output of each timestep is used as an input to the subsequent timestep in the network. Figure 3 illustrates the prediction made at the last hidden state of the network.\n3.3.1 RNN variants. Training the general formulation of RNNs in practice is di cult due to the exploding and vanishing gradient problems (gradients becoming exceedingly high or become exceedingly close to 0 after only a few timesteps) [27]. For the exploding gradient problem, a common solution is to cap the gradient value at a speci c maximum threshold. There has been some variants\nof RNNs that assist the gradient ow and mitigate the vanishing gradient problem. Most notable are the Long Short Term Memory (LSTM) [16] and Gated Recurrent Unit (GRU) [5].\nLSTM adds additional gates to the regular hidden layer of a recurrent network to assist the gradient ow and allow the network to be e ectively trained. These gates control the amount of information to be forgotten or preserved throughout the sequence. Concretely, we are referring to the formulation of Graves [15] for LSTM.\nGRU proposed by Cho et al. [5] makes each recurrent unit to adaptively capture dependencies of di erent time scales and similar to LSTM, GRU also has gating units that control the ow of information through the computational graph. The di erence with LSTM is that the GRU does not have a separate memory cell. We use the exact formulation of Cho et al. [5] for GRU.\nBidirectional RNNs. In order to also capture the backward dependencies and interactions between di erent parts of a sequence, a backward RNN is also trained which can encode the information from the future time steps [14, 29]. The hidden states of the backward RNN are then considered along with the corresponding hidden states of the forward RNN (e.g. by concatenation) at each time step and used in the subsequent layers.\nLet x = 〈x1, ...,xn〉 be the input to the Recurrent layer. Then the bidirectional RNN over the time steps t = 1, ...,n is as follows:\nht = 〈 −→ ht ; ←− ht 〉 (2)\nwhere “〈·; ·〉” shows the concatenation operation and −→ht ( ←− ht ) is\nthe forward (backward) RNN de ned as follows:\n−→ ht = −−−→ RNN (xt ); ←− ht = ←−−− RNN (xt ) (3)\nWhere RNN(·) is the feed forward RNN cell in the general form, LSTM, or GRU."
    }, {
      "heading" : "3.4 Attention model",
      "text" : "We use an attention model on top of our recurrent layer to be able to capture the local features that are more important in the task at hand. The limitation of using the regular recurrent network for the classi cation task is that the last time step of recurrent network loses some information about the sequence, specially when the sequence length becomes large [5]. This will not be a signi cant problem in short sentence classi cation tasks, but in our problem, the reports can have several sentences and the sequence length can be long. While in theory, the last step of the RNN is able to encode all the important information in the entire sequence, in practice it tends to focus more on the more recent time steps [31] and therefore loses some information specially about the earlier time steps. Using a bidirectional RNN can partially mitigate this problem where the last state of the backward RNN along with the last state of the forward RNN are able to capture the information in beginning and the end of the sequence. However, bidirectional RNNs are still su ering from the same information loss problem.\nInspired by recent work in machine translation [3] and document modeling [36], we propose to address this problem using a soft attention mechanism. Attention mechanism helps in constructing a context vector over the input that automatically incorporates the important parts of the input. The attention mechanism is shown in\nThe states −→ hi and ←− hi show forward and backwardhidden state of the RNN respectively. hi ’s are the concatenation of the forward and backward RNN states. z is a context vector that attends to important time steps and αi are the weights associated with each hidden state hi . The gure shows an example where darker colors for the hi showmore importance in constructing c which is used as input to the next layer.\nFigure 4. Particularly, instead of only considering the last hidden state of the RNN (hn ), the attention model attends to the important timesteps by introducing additional weights (α ’s):\nc = N∑ t=1 αtht (4)\nwhere t are the time steps in the input sequence and the weights α are learned according to the following softmax function:\nαt = so max(uᵀt z) (5)\nwhere z is a context vector that helps in nding the weight importance of the local states ht . This context vector can be seen as an input memory representation in memory networks and is jointly trained with the network. ut is a feed forward function which we will de ne later and for a set of scores si , the softmax function returns a probability distribution over the scores:\nso max(si ) = exp(si/β)∑ j exp(sj/β) ; (6)\nwith β being a parameter controlling the smoothness of the resulting distribution.\nut in equation 5 is the result of applying a regular feed forward network over the hidden state ht with weights U and biases b:\nut = F (ht ) = tanh(Uht + b) (7)\nFinally, our model at the top layer has a fully connected layer followed by a softmax non-linear layer that predicts the probability distribution of harm severity given an input report."
    }, {
      "heading" : "3.5 Training",
      "text" : "Let Θ denote all the parameters in the network which includes the weights associated with each of the layers described in previous sections. The entire network is then trained to minimize the following loss function:\nJ (Θ) = − C∑ c=1 1[y∗ = c] log Pr(Y = c |x) (8)\nwhere C is the number of harm severity classes and y* is the ground truth label for the input report x, 1[·] is the indicator function, and the probability of each harm severity class is estimated through the network."
    }, {
      "heading" : "4 EXPERIMENTS",
      "text" : ""
    }, {
      "heading" : "4.1 Data",
      "text" : "We use two large scale datasets consisting of patient safety and incidents reports sampled from various healthcare systems. These reports are sometimes referred to as “patient safety reports” in the health informatics literature, but in general they are meant to identify and characterize errors in patient care.\nThis study was approved by the MedStar Health Research Institute Institutional Review Board (protocol 2014-101). The characteristics of the datasets are outlined in Table 2. We observe that one of the datasets (DS2) is larger than the other one (DS1) and the length of reports are rather di erent between them. Each dataset consists of reports regarding di erent categories in patient care. The statistics about each of the harm levels are shown in Table 3. The harm events (right side of the Table) are usually much less frequent than the events with no actual harm (left side of the Table). We divide each dataset to 3 subsets of training, validation, and test with respective distribution of 60%, 20%, and 20% of the entire data. The hyper-parameters of the models were chosen empirically based on the performance on the validation set and the test set is preserved for evaluation."
    }, {
      "heading" : "4.2 Evaluation",
      "text" : "We evaluate the e ectiveness of our models in identifying harm by using standard classi cation evaluation metrics namely precision, recall, F-1 and Area Under the Curve (AUC)."
    }, {
      "heading" : "4.3 Baselines",
      "text" : "For comparing the performance of the proposed methods, we consider the following baselines: • SVM bow - SVM with linear kernel with n-gram bag of words\n(bow) features [33]. We experiment with three types of features, n-grams of size {1}, {1,2}, and {1,2,3} (we respectively abbreviate the resulting models with bow1, bow2, and bow3). • MNB bow - We also experiment with Multinomial Naive Bayes method for classi cation where Wang and Manning [33] show its e ectiveness in many text classi cation tasks. We used scikitlearn (http://scikit-learn.org/) implementation of SVM and MNB. • CNN - We consider CNN model for text classi cation which has shown good results in both general domain [18, 19] and biomedical domain [28]. • LSTM - We also compare against RNN (LSTM) classi er which is similar to the models used in [22, 32] (see Figure 3).\nThese methods form strong baselines with which we compare the performance of our models."
    }, {
      "heading" : "4.4 Model variants",
      "text" : "We evaluate several variants of our models. The rst variant is our model architecture which is the entire model presented in §3 minus the attention model. We consider two types of recurrent networks, GRU and LSTM, as well as their bidirectional variants (Bi-GRU and Bi-LSTM). We then evaluate our complete model which utilizes the attention mechanism. When considering attention, we\nevaluated both GRU and LSTM as the underlying recurrent layer. We abbreviate these models based on the layers from top to bottom. For example “ATT GRU CNN ” corresponds to our attention model with GRU unit, while example “ATT Bi-LSTM CNN ” corresponds to our attention model with bidirectional LSTM in the recurrent layer.\nDesign decisions and hyperparameters. We empirically made the following design choices and hyperparameter selection: We used embedding size of 100 for word vectors and we set the maximum sequence length to 100 words (smaller sequences are padded with zero vectors and larger sequences are cropped). For convolution, we used lters of length 2 to 5 with 128 channels each, max pooling of length 4, and merge the output of the lters by concatenation. For RNN, we use LSTM and GRU with hidden size of 100. We used dropout rate of 0.25 after convolution. Training was done with batch size of 128 and through 2 and 6 epochs for the larger and smaller datasets, respectively. Adam [20] was used as optimizer and early stopping was applied by monitoring accuracy on the validation set."
    }, {
      "heading" : "4.5 Results",
      "text" : "We rst consider the problem of identifying harm cases in the patient reports. That is, we classify a report as indicating some signs of harm to the patient (a harm case) or not (a no-harm case). The main results of our methods in identifying harm are illustrated in Table 4. The metrics are Precision, Recall, F-1 score for the harm category as well as the Area under the curve. We observe that our attention models (starting with ATT in the Table) are the best performing methods in both datasets evaluated by F-1 scores. In particular, the attention model using an LSTM recurrent unit (ATT LSTM CNN) achieves the highest F-1 of 72.9% on the rst dataset and the attention model using a bidirectional GRU (ATT Bi-GRU CNN) achieves F-1 of 77.9% on the second dataset. While the results ranges are similar between the two datasets, in general we can see that the results on the second dataset are slightly higher. This is due to the datasets being generated at di erent healthcare systems and thus there are qualitative and quantitative di erence between the datasets. As far as the baselines, we can see that in general, in terms of F-1 scores, traditional bag of words approaches [33] are not\nquite competitive. In terms of precision, the Multinommial Naive Bayes method using up to 3gram features (MNB bow3) achieves the highest overall scores on rst dataset; however, its recall is very low, making it relatively ine ective. The SVM baselines work generally better on the second dataset compared with the rst dataset, and they outperform the performance of CNN and LSTM baselines. For example, the best F-1 score on the second dataset is 73.9% which is for the SVM bow3 baseline. Our methods are still able to significantly improve over this baseline (compare the performance of ATT Bi-GRU CNN with SVM bow 3). Another trend that is worth noting is the signi cantly higher recall performance of our proposed models in comparison with the baselines. Recall is important in the task of harm detection, as any harm case can impact the patient and the method should minimize false negatives. We then compare the result of our method using a recurrent model on top of a convolutional model and observe how it can improve both the CNN and LSTM baselines. This suggests that while CNNs are e ective in capturing the information in longer sequences, there is also some additional information that is captured when considering the interactions between the words along the entire sequence. We also observe that using a recurrent layer on top of a convoluational layer improves the performance (compare LSTM with our models in the Table), suggesting that local features captured by CNN are important in the nal prediction.\nNext, we evaluate the performance of our top models in negrain classi cation of harm severity on patients compared with the top baselines. Table 5 shows the performance on 4 levels of harm: Temporary or permenant harm, event that reached the patient but did not cause harm, near miss events, unsafe events. For description of these categories refer to Section 1 and Table 1. We observe that our method variant ATT Bi-GRU CNN achieves the best overall performance with average respective F-1 scores of 68.9% and 75.3% in datasets 1 and 2."
    }, {
      "heading" : "4.6 Analysis",
      "text" : "To better evaluate the performance of our system and study the errors that it makes, we analyze the performance on each dataset based on each category of incident reports. The incident reports are categorized into several categories and there are often qualitative\ndi erences between the narratives in di erent categories. Tables 6 and 7 show the breakdown of results based on top common categories in dataset 1 and 2, respectively. We report the results of the best performing model variant on each dataset (i.e. ATT LSTM CNN for dataset 1 and ATT Bi-GRU CNN for dataset 2). On dataset 1 (Table 6) we observe that the model achieves very high scores in identifying harm in Skin/Tissue category with F-1 of 92.9% in identifying harm. Results on some other categories such as Surgury/Procedure, Seclusion Injury, Airway Management, and Blood bank are also relatively high. However, we observe that on some categories such as Patient ID/Documentation and Lab/Specimen the performance is low. We attribute the low performance in these categories to three main reasons: the total number of data in each category, the relative number of harm cases in the category, and the diversity of the type of reports in each category. We analyzed the distribution of the harm cases in each category. Some categories are more balanced in terms of harm and no-harm cases, while other categories are extremely unbalanced. We calculate the class ratios of harm in each category and compare the results based on these ratios. Figure 5 illustrates the performance of our method on each category and the ratio of harm cases in that category. Each data point shows the performance results in terms of F-1 based on the ratio of harm cases in that category. We observe that as the ratio of harm cases increases, the performance generally tends to increase. This is expected, as training the model on highly unbalanced datasets prevents the model to learn the appropriate weights associated with the positive class. The two categories at the"
    }, {
      "heading" : "1 based on each category. Each data point shows the results in a speci c category as well as the ratio of harm cases in",
      "text" : "that category. The x-axis shows the ratio of harm cases.\nbottom left side of Figure 5 are the categories with lowest results in Table 6. The respective ratio of harm cases in these categories are 0.009 and 0.027, while the ratio of harm cases in Skin/Tissue (the point on top right side of the Figure) is 0.53.\nWe also performed qualitative analysis on the reports in each category by inspecting the type of incidents in each category. We investigate the types of incidents in the best performing category in dataset 1 Skin/Tissue and most of the events are regarding pressure ulcer and wounds. On the other hand, looking at the Lab/Specimen category, there are many diverse types of errors and harm in this category such as collection issues, documentation problems, labeling issues, ordering issues, etc, that are very di erent in description, making it di cult for the model to learn all the nuances in this category. This reason, coupled with relative low number of harm cases in the dataset in this category, results in low performance. We believe that having more data would help improving the performance of the model."
    }, {
      "heading" : "5 CONCLUSIONS",
      "text" : "In this paper, we presented a neural network model for identifying harm in safety narratives related to clinical care. We used a multilayer network with convolutional, recurrent, and soft attention mechanism layers. We argued that convolutional layer is important in nding the local features and the recurrent layer with attention is e ective in nding the interactions and dependencies along the sequence. We demonstrated that our methods can signi cantly improve the performance over existing methods in identifying harm safety cases. The impact of the methods and results presented in this paper is substantial to patient care. More accurate methods in the identi cation of harm can help the data analysis and reporting process, prevent harm to patients, better prioritize resources to address safety incidents, and subsequently improve general patient care."
    }, {
      "heading" : "ACKNOWLEDGMENTS",
      "text" : "We thank the four anonymous reviewers for their helpful comments and suggestions. We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Titan X Pascal GPU used for this research. This project was funded under contract/grant number Grant R01 HS023701-02 from the Agency for Healthcare Research and Quality (AHRQ), U.S. Department of Health and Human Services. The opinions expressed in this document are those of the authors and do not necessarily re ect the o cial position of AHRQ or the U.S. Department of Health and Human Services."
    } ],
    "references" : [ {
      "title" : "A survey of text classi cation algorithms",
      "author" : [ "Charu C Aggarwal", "ChengXiang Zhai" ],
      "venue" : "In Mining text data",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2012
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2014
    }, {
      "title" : "Representation learning: A review and new perspectives",
      "author" : [ "Yoshua Bengio", "Aaron Courville", "Pascal Vincent" ],
      "venue" : "IEEE transactions on pattern analysis and machine intelligence 35,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2013
    }, {
      "title" : "On the properties of neural machine translation: Encoder-decoder approaches",
      "author" : [ "Kyunghyun Cho", "Bart Van Merriënboer", "Dzmitry Bahdanau", "Yoshua Bengio" ],
      "venue" : null,
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2014
    }, {
      "title" : "A Neural Attention Model for Categorizing Patient Safety Events",
      "author" : [ "Arman Cohan", "Allan Fong", "Nazli Goharian", "Raj Ratwani" ],
      "venue" : "In Advances in Information Retrieval: 39th European Conference on IR Research,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2017
    }, {
      "title" : "Natural Language Processing (Almost) from Scratch",
      "author" : [ "Ronan Collobert", "Jason Weston", "Léon Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa" ],
      "venue" : "J. Mach. Learn. Res",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2011
    }, {
      "title" : "Improving deep neural networks for LVCSR using recti ed linear units and dropout",
      "author" : [ "George E Dahl", "Tara N Sainath", "Geo rey E Hinton" ],
      "venue" : "In ICASSP",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2013
    }, {
      "title" : "Deep Convolutional Neural Networks for Sentiment Analysis of Short Texts",
      "author" : [ "Cícero Nogueira dos Santos", "Maira Gatti" ],
      "venue" : "In COLING",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2014
    }, {
      "title" : "Finding structure in time",
      "author" : [ "Je rey L Elman" ],
      "venue" : "Cognitive science 14,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1990
    }, {
      "title" : "Exploring methods for identifying related patient safety events using structured and unstructured data",
      "author" : [ "Allan Fong", "A Zachary Hettinger", "Raj M Ratwani" ],
      "venue" : "Journal of biomedical informatics",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2015
    }, {
      "title" : "A Primer on Neural Network Models for Natural Language Processing",
      "author" : [ "Yoav Goldberg" ],
      "venue" : "Journal of Arti cial Intelligence Research (JAIR)",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2016
    }, {
      "title" : "Supervised sequence labelling with recurrent neural networks",
      "author" : [ "Alex Graves" ],
      "venue" : "Ph.D. thesis, Technische UniversitÂĺat MÂĺunchen",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2008
    }, {
      "title" : "Generating sequences with recurrent neural networks",
      "author" : [ "Alex Graves" ],
      "venue" : "arXiv preprint arXiv:1308.0850",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2013
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber" ],
      "venue" : "Neural computation 9,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1997
    }, {
      "title" : "A New, Evidence-based Estimate of Patient Harms Associated with Hospital Care",
      "author" : [ "John T. James" ],
      "venue" : "Journal of Patient Safety",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2013
    }, {
      "title" : "A Convolutional Neural Network for Modelling Sentences",
      "author" : [ "Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom" ],
      "venue" : "In ACL",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2014
    }, {
      "title" : "Convolutional Neural Networks for Sentence Classi cation",
      "author" : [ "Yoon Kim" ],
      "venue" : "In EMNLP",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2014
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik Kingma", "Jimmy Ba" ],
      "venue" : "arXiv preprint arXiv:1412.6980",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2014
    }, {
      "title" : "Gradientbased learning applied to document recognition",
      "author" : [ "Yann LeCun", "Léon Bottou", "Yoshua Bengio", "Patrick Ha ner" ],
      "venue" : "Proc. IEEE 86,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 1998
    }, {
      "title" : "Recurrent neural network for text classi cation with multi-task learning",
      "author" : [ "Pengfei Liu", "Xipeng Qiu", "Xuanjing Huang" ],
      "venue" : null,
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2016
    }, {
      "title" : "The problem with incident reporting",
      "author" : [ "Carl Macrae" ],
      "venue" : "BMJ Quality & Safety",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2016
    }, {
      "title" : "Medical errorâĂŤthe third leading cause of death in the US",
      "author" : [ "Martin A Makary", "Michael Daniel" ],
      "venue" : "Bmj 353 (2016),",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2016
    }, {
      "title" : "Patient safety incident reporting: a qualitative study of thoughts and perceptions of experts 15 years after âĂŸ To Err is Human ",
      "author" : [ "Imogen Mitchell", "Anne Schuster", "Katherine Smith", "Peter Pronovost", "Albert Wu" ],
      "venue" : "BMJ Quality & Safety",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2016
    }, {
      "title" : "Automated identi cation of extreme-risk events in clinical incident reports",
      "author" : [ "Mei-Sing Ong", "Farah Magrabi", "Enrico Coiera" ],
      "venue" : "Journal of the American Medical Informatics Association 19,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2012
    }, {
      "title" : "On the di culty of training recurrent neural networks",
      "author" : [ "Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio" ],
      "venue" : "ICML",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2013
    }, {
      "title" : "Convolutional neural networks for biomedical text classi cation: application in indexing biomedical articles",
      "author" : [ "Anthony Rios", "Ramakanth Kavuluru" ],
      "venue" : "In ACM-BCB",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2015
    }, {
      "title" : "Bidirectional recurrent neural networks",
      "author" : [ "Mike Schuster", "Kuldip K Paliwal" ],
      "venue" : "IEEE Transactions on Signal Processing 45,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 1997
    }, {
      "title" : "Sequence to sequence learning with neural networks. In Advances in neural information processing",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc V Le" ],
      "venue" : null,
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2014
    }, {
      "title" : "Document Modeling with Gated Recurrent Neural Network for Sentiment Classi cation",
      "author" : [ "Duyu Tang", "Bing Qin", "Ting Liu" ],
      "venue" : null,
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2015
    }, {
      "title" : "Baselines and bigrams: Simple, good sentiment and topic classi cation",
      "author" : [ "Sida Wang", "Christopher D Manning" ],
      "venue" : "In ACL",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2012
    }, {
      "title" : "Opportunities for incident reporting",
      "author" : [ "Huw Williams", "Alison Cooper", "Andrew Carson-Stevens" ],
      "venue" : "BMJ Quality & Safety",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2016
    }, {
      "title" : "Text Classi cation with Topic-based Word Embedding and Convolutional Neural Networks",
      "author" : [ "Haotian Xu", "Ming Dong", "Dongxiao Zhu", "Alexander Kotov", "April Idalski Carcone", "Sylvie Naar-King" ],
      "venue" : "In ACM-BCB",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 2016
    }, {
      "title" : "Hierarchical Attention Networks for Document Classi cation",
      "author" : [ "Zichao Yang", "Diyi Yang", "Chris Dyer", "Xiaodong He", "Alex Smola", "Eduard Hovy" ],
      "venue" : null,
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 2016
    }, {
      "title" : "Comparison and combination of several MeSH indexing approaches",
      "author" : [ "Antonio Jose Jimeno Yepes", "James G Mork", "Dina Demner-Fushman", "Alan R Aronson" ],
      "venue" : "In AMIA annual symposium proceedings,",
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 21,
      "context" : "Preventable medical errors have been shown to be a major cause of injury and death in the United States [9, 24, 34].",
      "startOffset" : 104,
      "endOffset" : 115
    }, {
      "referenceID" : 30,
      "context" : "Preventable medical errors have been shown to be a major cause of injury and death in the United States [9, 24, 34].",
      "startOffset" : 104,
      "endOffset" : 115
    }, {
      "referenceID" : 21,
      "context" : "[24, 30], which translates to an estimated incidence of 210,000 to 400,000 deaths annually [17, 24].",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 14,
      "context" : "[24, 30], which translates to an estimated incidence of 210,000 to 400,000 deaths annually [17, 24].",
      "startOffset" : 91,
      "endOffset" : 99
    }, {
      "referenceID" : 21,
      "context" : "[24, 30], which translates to an estimated incidence of 210,000 to 400,000 deaths annually [17, 24].",
      "startOffset" : 91,
      "endOffset" : 99
    }, {
      "referenceID" : 22,
      "context" : "To address these major concerns, healthcare systems have adopted reporting systems in clinical care to help track and trend hazards and errors in patient care [25, 34].",
      "startOffset" : 159,
      "endOffset" : 167
    }, {
      "referenceID" : 30,
      "context" : "To address these major concerns, healthcare systems have adopted reporting systems in clinical care to help track and trend hazards and errors in patient care [25, 34].",
      "startOffset" : 159,
      "endOffset" : 167
    }, {
      "referenceID" : 20,
      "context" : "Although reporting systems have been implemented with the goal of improving patient safety and patient care, hospital sta are faced with many challenges in analyzing and understanding these reports [23, 25].",
      "startOffset" : 198,
      "endOffset" : 206
    }, {
      "referenceID" : 22,
      "context" : "Although reporting systems have been implemented with the goal of improving patient safety and patient care, hospital sta are faced with many challenges in analyzing and understanding these reports [23, 25].",
      "startOffset" : 198,
      "endOffset" : 206
    }, {
      "referenceID" : 30,
      "context" : "One important aspect in patient care is to identify events that have contributed to or resulted in harm to the patient [34].",
      "startOffset" : 119,
      "endOffset" : 123
    }, {
      "referenceID" : 9,
      "context" : "While there is a growing number of work in categorizing patient safety reports, none has looked at the modeling of general harm across all event types [12, 26].",
      "startOffset" : 151,
      "endOffset" : 159
    }, {
      "referenceID" : 23,
      "context" : "While there is a growing number of work in categorizing patient safety reports, none has looked at the modeling of general harm across all event types [12, 26].",
      "startOffset" : 151,
      "endOffset" : 159
    }, {
      "referenceID" : 9,
      "context" : "[12] explored both the unstructured free-text and structured data elements in safety reports to identify and rank similar events.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 23,
      "context" : "[26] explored the similar problem of identifying extreme-risk events in patient safety and incident reports using Naive Bayes and SVM classi ers with bag of words features.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 0,
      "context" : "Traditional approaches in text classi cation include methods to extract features from text and then use the feature vector as an input to a classi er such as SVM [2].",
      "startOffset" : 162,
      "endOffset" : 165
    }, {
      "referenceID" : 18,
      "context" : "Two of the more widely used neural network architectures have been Convolutional Neural Networks (CNN) [21] and Recurrent Neural Networks (RNN) [11].",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 8,
      "context" : "Two of the more widely used neural network architectures have been Convolutional Neural Networks (CNN) [21] and Recurrent Neural Networks (RNN) [11].",
      "startOffset" : 144,
      "endOffset" : 148
    }, {
      "referenceID" : 5,
      "context" : "[7] were one of the rst to utilize CNNs in many NLP tasks including text classi cation.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 18,
      "context" : "[21] with adaptations to the NLP domain and showed improvements on several NLP tasks.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 7,
      "context" : "Later CNNs were further explored for sentence modeling and classi cation tasks [10, 18, 19].",
      "startOffset" : 79,
      "endOffset" : 91
    }, {
      "referenceID" : 15,
      "context" : "Later CNNs were further explored for sentence modeling and classi cation tasks [10, 18, 19].",
      "startOffset" : 79,
      "endOffset" : 91
    }, {
      "referenceID" : 16,
      "context" : "Later CNNs were further explored for sentence modeling and classi cation tasks [10, 18, 19].",
      "startOffset" : 79,
      "endOffset" : 91
    }, {
      "referenceID" : 33,
      "context" : "These works have mostly used supervised learning frameworks with bag of words features, namedentities, and ontology speci c features [37].",
      "startOffset" : 133,
      "endOffset" : 137
    }, {
      "referenceID" : 25,
      "context" : "More recently, Rios and Kavuluru [28] utilized CNNs for this task; they showed that CNNs are more e ective than feature based methods in biomedical indexing.",
      "startOffset" : 33,
      "endOffset" : 37
    }, {
      "referenceID" : 4,
      "context" : "The authors in [6] used a neural network architecture using CNN and RNNs to classify patient safety report.",
      "startOffset" : 15,
      "endOffset" : 18
    }, {
      "referenceID" : 31,
      "context" : "[35] used a CNN architecture, with multiple sources of word embeddings and evaluated its e ectiveness on the tasks of biomedical literature indexing and clinical note annotation.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 2,
      "context" : "Word emdeddings or distributed representations of words aim to embed (represent) words with dense vectors such that words with similar properties have similar vectors [4].",
      "startOffset" : 167,
      "endOffset" : 170
    }, {
      "referenceID" : 18,
      "context" : "Convolutional Neural Networks (CNNs) [21] have been previously used in sentence modeling and classi cation tasks [18, 19].",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 15,
      "context" : "Convolutional Neural Networks (CNNs) [21] have been previously used in sentence modeling and classi cation tasks [18, 19].",
      "startOffset" : 113,
      "endOffset" : 121
    }, {
      "referenceID" : 16,
      "context" : "Convolutional Neural Networks (CNNs) [21] have been previously used in sentence modeling and classi cation tasks [18, 19].",
      "startOffset" : 113,
      "endOffset" : 121
    }, {
      "referenceID" : 18,
      "context" : "Similar to convolutional networks for object recognition [21], we use multiple feature maps with di erent lters to capture various aspects of the input sequence.",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 6,
      "context" : "Where h(t) shows the hidden state of the RNN in time step t , x(t) is the input sequence at time step t ,W (h),W (x ), andW (s) are the weights associated with the hidden state, input, and softmax, respectively, and д is an activation function such as RELU [8].",
      "startOffset" : 257,
      "endOffset" : 260
    }, {
      "referenceID" : 10,
      "context" : "In sequence modeling tasks, the nal hidden state of the network can represent the whole sequence and can be used for making predictions [13].",
      "startOffset" : 136,
      "endOffset" : 140
    }, {
      "referenceID" : 24,
      "context" : "Training the general formulation of RNNs in practice is di cult due to the exploding and vanishing gradient problems (gradients becoming exceedingly high or become exceedingly close to 0 after only a few timesteps) [27].",
      "startOffset" : 215,
      "endOffset" : 219
    }, {
      "referenceID" : 13,
      "context" : "Most notable are the Long Short Term Memory (LSTM) [16] and Gated Recurrent Unit (GRU) [5].",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 3,
      "context" : "Most notable are the Long Short Term Memory (LSTM) [16] and Gated Recurrent Unit (GRU) [5].",
      "startOffset" : 87,
      "endOffset" : 90
    }, {
      "referenceID" : 12,
      "context" : "Concretely, we are referring to the formulation of Graves [15] for LSTM.",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 3,
      "context" : "[5] makes each recurrent unit to adaptively capture dependencies of di erent time scales and similar to LSTM, GRU also has gating units that control the ow of information through the computational graph.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 3,
      "context" : "[5] for GRU.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 11,
      "context" : "In order to also capture the backward dependencies and interactions between di erent parts of a sequence, a backward RNN is also trained which can encode the information from the future time steps [14, 29].",
      "startOffset" : 197,
      "endOffset" : 205
    }, {
      "referenceID" : 26,
      "context" : "In order to also capture the backward dependencies and interactions between di erent parts of a sequence, a backward RNN is also trained which can encode the information from the future time steps [14, 29].",
      "startOffset" : 197,
      "endOffset" : 205
    }, {
      "referenceID" : 3,
      "context" : "The limitation of using the regular recurrent network for the classi cation task is that the last time step of recurrent network loses some information about the sequence, specially when the sequence length becomes large [5].",
      "startOffset" : 221,
      "endOffset" : 224
    }, {
      "referenceID" : 27,
      "context" : "While in theory, the last step of the RNN is able to encode all the important information in the entire sequence, in practice it tends to focus more on the more recent time steps [31] and therefore loses some information specially about the earlier time steps.",
      "startOffset" : 179,
      "endOffset" : 183
    }, {
      "referenceID" : 1,
      "context" : "Inspired by recent work in machine translation [3] and document modeling [36], we propose to address this problem using a soft attention mechanism.",
      "startOffset" : 47,
      "endOffset" : 50
    }, {
      "referenceID" : 32,
      "context" : "Inspired by recent work in machine translation [3] and document modeling [36], we propose to address this problem using a soft attention mechanism.",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 29,
      "context" : "• SVM bow - SVM with linear kernel with n-gram bag of words (bow) features [33].",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 29,
      "context" : "• MNB bow - We also experiment with Multinomial Naive Bayes method for classi cation where Wang and Manning [33] show its e ectiveness in many text classi cation tasks.",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 15,
      "context" : "• CNN - We consider CNN model for text classi cation which has shown good results in both general domain [18, 19] and biomedical domain [28].",
      "startOffset" : 105,
      "endOffset" : 113
    }, {
      "referenceID" : 16,
      "context" : "• CNN - We consider CNN model for text classi cation which has shown good results in both general domain [18, 19] and biomedical domain [28].",
      "startOffset" : 105,
      "endOffset" : 113
    }, {
      "referenceID" : 25,
      "context" : "• CNN - We consider CNN model for text classi cation which has shown good results in both general domain [18, 19] and biomedical domain [28].",
      "startOffset" : 136,
      "endOffset" : 140
    }, {
      "referenceID" : 19,
      "context" : "• LSTM - We also compare against RNN (LSTM) classi er which is similar to the models used in [22, 32] (see Figure 3).",
      "startOffset" : 93,
      "endOffset" : 101
    }, {
      "referenceID" : 28,
      "context" : "• LSTM - We also compare against RNN (LSTM) classi er which is similar to the models used in [22, 32] (see Figure 3).",
      "startOffset" : 93,
      "endOffset" : 101
    }, {
      "referenceID" : 17,
      "context" : "Adam [20] was used as optimizer and early stopping was applied by monitoring accuracy on the validation set.",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 29,
      "context" : "As far as the baselines, we can see that in general, in terms of F-1 scores, traditional bag of words approaches [33] are not",
      "startOffset" : 113,
      "endOffset" : 117
    } ],
    "year" : 2017,
    "abstractText" : "Preventable medical errors are estimated to be among the leading causes of injury and death in the United States. To prevent such errors, healthcare systems have implemented patient safety and incident reporting systems. These systems enable clinicians to report unsafe conditions and cases where patients have been harmed due to errors in medical care. These reports are narratives in natural language and while they provide detailed information about the situation, it is non-trivial to perform large scale analysis for identifying common causes of errors and harm to the patients. In this work, we present a method based on attentive convolutional and recurrent networks for identifying harm events in patient care and categorize the harm based on its severity level. We demonstrate that our methods can signi cantly improve the performance over existing methods in identifying harm in clinical care.",
    "creator" : "LaTeX with hyperref package"
  }
}