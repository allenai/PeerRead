{
  "name" : "1511.06744.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "EFFICIENT IMAGE CLASSIFICATION",
    "authors" : [ "Yani Ioannou", "Duncan Robertson", "Jamie Shotton", "Roberto Cipolla", "Antonio Criminisi" ],
    "emails" : [ "yai20@cam.ac.uk,", "rc10001@cam.ac.uk,", "a-durobe@microsoft.com", "jamiesho@microsoft.com", "antcrim@microsoft.com" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "Convolutional neural networks (CNNs) have been used increasingly succesfully to solve challenging computer vision problems such as image classification (Krizhevsky et al., 2012), object detection (Ren et al., 2015), and human pose estimation (Tompson et al., 2015). However, recent improvements in recognition accuracy have come at the expense of increased model size and computational complexity. These costs can be prohibitive for deployment on low-power devices, or for fast analysis of videos and volumetric medical images.\nOne promising aspect of CNNs, from a memory and computational efficiency standpoint, is their use of convolutional filters (a.k.a. kernels). Such filters usually have limited spatial extent and their learned weights are shared across the image spatial domain to provide translation invariance (Fukushima, 1980; LeCun et al., 1998). Thus, as illustrated in Fig. 1, in comparison with fully connected network layers (Fig. 1a), convolutional layers have a much sparser connection structure and use fewer parameters (Fig. 1b). This leads to faster training and test, better generalization, and higher accuracy.\nThis paper focuses on reducing the computational complexity of the convolutional layers of CNNs by further sparsifying their connection structures. Specifically, we show that by representing convolutional filters using a basis space comprising groups of filters of different spatial dimensions (examples shown in Fig. 1c and d), we can significantly reduce the computational complexity of existing state-of-the-art CNNs without compromising classification accuracy.\nOur contributions include a novel method of learning a set of small basis filters that are combined to represent larger filters efficiently. Rather than approximating previously trained networks, we\nar X\niv :1\n51 1.\n06 74\n4v 1\n[ cs\n.C V\n] 2\n0 N\nov 2\n01 5\ntrain networks from scratch and show that our convolutional layer representation can improve both efficiency and classification accuracy. We further describe how to initialize connection weights effectively for training networks with composite convolutional layers containing groups of differentlyshaped filters, which we found to be of critical importance to our training method."
    }, {
      "heading" : "1.1 RELATED WORK",
      "text" : "There has been much previous work on increasing the test-time efficiency of CNNs. Some promising approaches work by making use of more hardware-efficient representations. For example Gupta et al. (2015) and Vanhoucke et al. (2011) achieve training- and test-time compute savings by further quantization of network weights that were originally represented as 32 bit floating point numbers. However, more relevant to our work are approaches that depend on new network connection structures, efficient approximations of previously trained networks, and learning low rank filters.\nEfficient Network Connection Structures. LeCun et al. (1989) suggest a method of pruning unimportant connections within networks. However this requires repeated network re-training and may be infeasible for modern, state-of-the-art CNNs requiring weeks of training time. Lin et al. (2013) show that the geometric increase in the number and dimensions of filters with deeper networks can be managed using low-dimensional embeddings. The same authors show that global average-pooling may be used to decrease model size in networks with fully connected layers. Simonyan & Zisserman (2014) show that stacked filters with small spatial dimensions (e.g. 3×3), can operate on the effective receptive field of larger filters (e.g. 5×5) with less computational complexity.\nLow-Rank Filter Approximations. Rigamonti et al. (2013) approximate previously trained CNNs with low-rank filters for the semantic segmentation of curvilinear structures within volumetric medical imagery. They discuss two approaches: enforcing an L1-based regularization to learn approximately low rank filters, which are later truncated to enforce a strict rank, and approximating a set of pre-learned filters with a tensor-decomposition into many rank-1 filters. Neither approach learns low rank filters directly, and indeed the second approach proved the more successful.\nThe work of Jaderberg et al. (2014) also approximates the existing filters of previously trained networks. They find separable 1D filters through an optimization minimizing the reconstruction error of the already learned full rank filters. They achieve a 4.5× speed-up with a loss of accuracy of 1% in a text recognition problem. However since the method is demonstrated only on text recognition, it is not clear how well it would scale to larger data sets or more challenging problems. A key insight\nof the paper is that filters can be represented by low rank approximations not only in the spatial domain but also in the channel domain.\nBoth of these methods show that, at least for their respective applications, low rank approximations of full-rank filters learned in convolutional networks can increase test time efficiency significantly. However, being approximations of pre-trained networks, they are unlikely to improve test accuracy, and can only increase the computational requirements during training.\nLearning Separable Filters. Mamalet & Garcia (2012) propose training networks with separable filters on the task of digit recognition with the MNIST dataset. They train networks with sequential convolutional layers of horizontal and vertical 1D filters, achieving a speed-up factor of 1.6×, but with a relative increase in test error of 13% (1.45% v.s. 1.28%). Our approach generalizes this, allowing both horizontal and vertical 1D filters (and other shapes too) at the same layer and avoiding issues with ordering. We also demonstrate a decrease in error and on more challenging datasets."
    }, {
      "heading" : "2 USING LOW-RANK FILTERS IN CNNS",
      "text" : ""
    }, {
      "heading" : "2.1 CONVOLUTIONAL FILTERS",
      "text" : "The convolutional layers of a CNN produce output ‘images’ (usually called feature maps) by convolving input images with one or more learned filters. In a typical convolutional layer, as illustrated in Fig. 2a, a c-channel input image of sizeH×W pixels is convolved with d filters of size h×w×c\nto create a d-channel output image. Each filter is represented by hwc independent weights. Therefore the computational complexity for the convolution of the filter with a c-channel input image is O(dwhc) (per pixel in the output feature map). In what follows, we describe schemes for modifying the architecture of the convolutional layers so as to reduce computational complexity. The idea is to replace full-rank convolutional layers with modified versions that represent the same number of filters by linear combinations of basis vectors, i.e. as lower rank representations of the full rank originals."
    }, {
      "heading" : "2.2 SEQUENTIAL SEPARABLE FILTERS",
      "text" : "An existing scheme for reducing the computational complexity of convolutional layers (Jaderberg et al., 2014) is to replace each one with a sequence of two regular convolutional layers but with filters that are rectangular in the spatial domain, as shown in Fig 2b. The first convolutional layer has m filters of sizew×1×c, producing an output feature map withm channels. The second convolutional layer has d filters of size 1×h×m, producing an output feature map with d channels. By this means the full rank original convolutional filter bank is represented by a low rank approximation formed from a linear combination of a set of separable w × h basis filters. The computational complexity of this scheme is O(mcw) for the first layer of horizontal filters and O(dmh) for the second layer of vertical filters, with a total of O(m(cw + dh)). Note that Jaderberg et al. (2014) use this scheme to approximate existing full rank filters belonging to previously trained networks using a retrospective fitting step. In this work, by contrast, we train networks containing convolutional layers with this architecture from scratch. In effect, we learn the separable basis filters and their combination weights simultaneously during network training."
    }, {
      "heading" : "2.3 FILTERS AS LINEAR COMBINATIONS OF BASES",
      "text" : "In this work we introduce another scheme for reducing convolutional layer complexity. This works by representing convolutional filters as linear combinations of basis filters as illustrated in Fig. 2c. This scheme uses composite layers comprising several sets of filters where the filters in each set have different spatial dimensions (see Fig. 5). The outputs of these basis filters may be combined in a subsequent layer containing filters with spatial dimensions 1× 1. This is illustrated in Fig. 2c. Here, our composite layer contains horizontal w× 1 and vertical 1× h filters, the outputs of which are concatenated in the channel dimension, resulting in an intermediate m-channel feature map. These filter responses are then linearly combined by the next layer of d 1 × 1 filters to give a d-channel output feature map. In this case, the filters are applied on the input feature map with c channels and followed by a set of m 1×1 filters over the m output channels of the basis filters. If the number of horizontal and vertical filters is the same, the computational complexity is O(m(wc/2 + hc/2 + d)). Interestingly, the configuration of Fig. 2c gives rise to linear combinations of horizontal and vertical filters that are cross-shaped in the spatial domain. This is illustrated in Fig. 3 for filters learned in the first convolutional layer of the‘vgg-gmp-lr-join’ model that is described in the Results section when it is trained using ILSVRC dataset.\nNote that, in general, more than two different sizes of basis filter might be used in the composite layer. For example, Fig. 2d shows a combination of three sets of filters with spatial dimensions w× 1, 1×h, and w×h. Also note that an interesting option is to omit the 1× 1 linear combination layer and instead allow the connection weights in a subsequent network layer to learn to combine the basis filters of the preceding layer (despite any intermediate non-linearity, e.g. ReLUs). This possibility is explored in practice in the Results section.\nIn that our method uses a combination of filters in a composite layer, it is similar to the ‘GoogLeNet’ of Szegedy et al. (2014) which uses ‘inception’ modules comprising several (square) filters of different sizes ranging from 1×1 to 5×5. In our case, however, we are implicitly learning linear combinations of less computationally expensive filters with different orientations (e.g. 3×1 and 1×3 filters), rather than combinations of filters of different sizes. Amongst networks with similar computational requirements, GoogLeNet is one of the most accurate for large scale image classification tasks (see Fig. 4), partly due to the use of heterogeneous filters in the inception modules, but also the use of low-dimensional embeddings and global pooling."
    }, {
      "heading" : "3 TRAINING CNNS WITH MIXED-SHAPE LOW-RANK FILTERS",
      "text" : "To determine the standard deviations to be used for weight initialization, we use an approach similar to that described by Glorot & Bengio (2010) (with the adaptation described by He et al. (2015) for layers followed by a ReLU). In Appendix A, we show the details of our derivation, generalizing the approach of He et al. (2015) to the initialization of ‘composite’ layers comprising several groups of filters of different spatial dimensions (see Appendix A, Fig. 5). This is one of the main contributions of this work.\nWe find that a composite layer of heterogeneously-shaped filter groups, where each filter group i has w[i]h[i]d[i] outgoing connections should be initialized as if it is a single layer with n̂ = ∑ w[i]h[i]d[i]. Thus in the case of a ReLU non-linearity, we find that such a composite layer should be initialized with a zero-mean Gaussian distribution with standard deviation:\nσ = √ 2∑\nw[i]h[i]d[i] . (1)"
    }, {
      "heading" : "4 RESULTS AND COMPARISONS",
      "text" : "To validate our approach, we show that we can replace the filters used in existing state-of-the-art network architectures with low-rank representations as described above to reduce computational complexity without reducing accuracy. Here we characterize the computational complexity of a CNN using the number of multiply accumulate operations required for a forward pass (which depends on the size of the filters in each convolutional layer as well as the input image size and stride). However, we have observed strong correlation between multiply-accumulate counts and runtime for both CPU and GPU implementations of the networks described here (as shown in Appendix B, Fig. 6). Note that the Caffe timings differ more for the initial convolutional layers where the input sizes are much smaller (3-channels) as BLAS is less efficient for the relatively small matrices being multiplied.\nMethodology. We augment our training set with randomly cropped and mirrored images, but do not use any scale or photometric augmentation, or over-sampling. This allows us to compare the efficiency of different network architectures without having to factor in the computational cost of the various augmentation methods used elsewhere. During training, for every model except GoogLeNet, we adjust the learning rate according to the schedule γt = γ0(1 + γ0λt)−1, where γ0, γt and λ are the initial learning rate, learning rate at iteration t, and weight decay respectively (Bottou, 2012).\nWhen the validation accuracy levels off we manually reduce the learning rate by further factors of 10 until the validation accuracy no longer increases. Unless otherwise indicated, aside from changing the standard deviation of the normally distributed weight initialization, as explained in §3, we used the standard hyper-parameters for each given model. Our results use no test-time augmentation."
    }, {
      "heading" : "4.1 VGG-11 ARCHITECTURES FOR ILSVRC OBJECT CLASSIFICATION AND MIT PLACES SCENE CLASSIFICATION",
      "text" : "We evaluated classification accuracy of the VGG-11 based architectures using two datasets, ImageNet Large Scale Visual Recognition Challenge 2012 (‘ILSVRC’) and MIT Places. The ILSVRC dataset comprises 1.2M training images of 1000 object classes, commonly evaluated by top-1 and top-5 accuracy on the 50K image validation set. The MIT Places dataset comprises 2.4M training images from 205 scene classes, evaluated with top-1 and top-5 accuracy on the 20K image validation set.\nVGG-11 (‘VGG-A’) is an 11-layer convolutional network introduced by Simonyan & Zisserman (2014). It is in the same family of network architectures used by Simonyan & Zisserman (2014); He et al. (2015) to obtain the state-of-the-art accuracy for ILSVRC, but uses fewer convolutional layers and therefore fits on a single GPU during training. During training of our VGG-11 based models, we used the standard hyperparameters as detailed by Simonyan & Zisserman (2014) and the initialization of He et al. (2015).\nIn what follows, we compare the accuracy of a number of different network architectures detailed in Appendix E, Table 6. Results for ILSVRC are given in Table 1, and plotted in Fig. 4. Results for MIT Places are given in Table 2, and plotted in Fig. 9.\nBaseline (Global Max Pooling). Compared to the version of the network described in (Simonyan & Zisserman, 2014), we use a variant that replaces the final 2× 2 max pooling layer before the first fully connected layer with a global max pooling operation, similar to the global average pooling used by Lin et al. (2013); Szegedy et al. (2014). We evaluated the accuracy of the baseline VGG-11 network with global max-pooling (vgg-gmp) and without (vgg-11) on the two datasets. We trained these networks at stride 1 on the ILSVRC dataset and at stride 2 on the larger MIT Places dataset. This globally max-pooled variant of VGG-11 uses over 75% fewer parameters than the original network and gives consistently better accuracy – almost 3 percentage points lower top-5 error on ILSVRC than the baseline VGG-11 network on ILSVRC (see Table 1). We used this network as the baseline for the rest of our experiments.\nSeparable Filters. To evaluate the separable filter approach described in §2.2 (illustrated in Fig. 2b), we replaced each convolutional layer in VGG-11 with a sequence of two layers, the first containing horizontally oriented 1× 3 filters and the second containing vertically oriented 3× 1 filters (vgg-gmp-sf). These filters applied in sequence represent 3×3 kernels using a low dimensional basis space. Unlike Jaderberg et al. (2014), we trained this network from scratch instead of approximating the full-rank filters in a previously trained network. Compared to the original VGG-11 network, the separable filter version requires approximately 14% less compute. Results are shown in Table 1 for ILSVRC and Table 2 for MIT Places. Accuracy for this network is approx. 0.8% lower than that of the baseline vgg-11-gmp network for ILSVRC and broadly comparable for MIT Places. This approach does not give such a significant reduction in computational complexity as what follows, but it is nonetheless interesting that separable filters are capable of achieving quite high classification accuracy on such challenging tasks.\nSimple Horizontal/Vertical Basis. To demonstrate the efficacy of the simple low rank filter representation illustrated in Fig. 2c, we created a new network architecture (vgg-gmp-lr-join) by replacing each of the convolutional layers in VGG-11 (original filter dimensions were 3 × 3) with a sequence of two layers. The first layer comprises half 1×3 filters and half 3×1 filters whilst the second layer comprises the same number of 1× 1 filters. The resulting network is approximately 49% faster than the original and yet it gives broadly comparable accuracy (within 1 percentage point) for both the ILSVRC and MIT Places datasets.\nFull-Rank Mixture. An interesting question concerns whether combining a small proportion of 3× 3 with the horizontal and vertical filters used in ‘vgg-gmp-lr-join’. To answer this question, we\ntrained a network, vgg-gmp-lr-join-wfull, with a mixture of 25% 3 × 3 and 75% 1 × 3 and 3 × 1 filters, while preserving the total number of filters of the baseline network (as illustrated in Fig. 2d). This network was significantly more accurate than both ‘vgg-gmp-lr-join’ and the baseline, with a top-5 center crop accuracy of 89.7% on ILSVRC, with a computational savings of approx. 16% over our baseline. We note that the accuracy is approx. 1 percentage point higher than GoogLeNet.\nImplicitly Learned Combinations. In addition, we try a network similar to vgg-gmp-lr-join but without the 1×1 convolutional layer (as shown in Fig. 2c) used to sum the contributions of 3×1 and 1×3 filters (vgg-gmp-lr). Interestingly, because of the elimination of the extra 1×1 layers, this gives an additional compute saving such that this model is is only 1/3rd of the compute of our baseline, with no reduction in accuracy. This seems to be a consequence of the fact that the subsequent convolutional layer is itself capable of learning effective combinations of filter responses even after the intermediate ReLU non-linearity.\nWe also trained such a network with double the number of convolutional filters (vgg-gmp-lr-2x), i.e. with an equal number of 1× 3 and 3× 1 filters, or 2c filters as shown in Fig. 2c. We found this to increase accuracy further (88.9% Top-5 on ILSVRC) while still being approximately 58% faster than our baseline network.\nLow-Dimensional Embeddings. We attempted to reduce the computational complexity of our ‘gmp-lr’ network further in the vgg-gmp-lr-lde network by using a stride of 2 in the first convolutional layer, and adding low-dimensional embeddings, as in Lin et al. (2013); Szegedy et al. (2014). We reduced the number of output channels by half after each convolutional layer using 1× 1 convolutional layers, as detailed in Appendix E, Table 6. While this reduces computation significantly, by approx. 86% compared to our baseline, we saw a decrease in top-5 accuracy on ILSVRC of 1.2 percentage points. We do note however, that this network remains 2.5 percentage points more accurate than the original VGG-11 network, but is 87% faster."
    }, {
      "heading" : "4.2 GOOGLENET FOR ILSVRC OBJECT CLASSIFICATION",
      "text" : "GoogLeNet, introduced by Szegedy et al. (2014), is the most efficient network for ILSVRC, getting close to state-of-the-art results with a fraction of the compute and model size of even VGG-11. The\nGoogLeNet inception module is a composite layer of 5 homogeneously-shaped filters, 1× 1, 3× 3, 5× 5, and the output of a 3x3 average pooling operations. All of these are concatenated and used as input for successive layers.\nFor the googlenet-lr network, within only the inception modules we replaced each the 3 × 3 filters with low-rank 3 × 1 and 1 × 3 filters, and replaced the layer of 5 × 5 filters with a set of low-rank 5 × 1 and 1 × 5 filters. For the googlenet-lr-conv1 network, we similarly replaced the first and second layer convolutional layers with 7× 1 / 1× 7 and 3× 1 / 1× 3 layers respectively. Results are shown in Table 3. Due to the intermediate losses used for training, which contain the only fully-connected layers in GoogLeNet, test time model size is significantly smaller than training time model size. Table 3 also reports test time model size. The low-rank network delivers comparable classification accuracy using 26% less compute. No other networks produce comparable accuracy within an order of magnitude of compute. We note that although the Caffe pre-trained GoogLeNet model (Jia et al., 2014) has a top-5 accuracy of 0.889, our training of the same network using the given model definition, including the hyper-parameters and training schedule, but a different random initialization had a top-5 accuracy of 0.883."
    }, {
      "heading" : "4.3 NETWORK-IN-NETWORK FOR CIFAR-10 OBJECT CLASSIFICATION",
      "text" : "The CIFAR-10 dataset consists of 60,000 32 × 32 images in 10 classes, with 6000 images per class. This is split into standard sets of 50,000 training images, and 10,000 test images (Krizhevsky, 2009). As a baseline for the CIFAR-10 dataset, we used the Network in Network architecture (Lin et al., 2013), which has a published test-set error of 8.81%. We also used random crops during training, with which the network has an error of 8.1%. Like most state of the art CIFAR results, this was with ZCA pre-processed training and test data (Goodfellow et al., 2013), training time mirror augmentation and random sub-crops. The results of our CIFAR experiments are listed in Table 4 and plotted in Fig. 11.\nThis architecture uses 5× 5 filters in some layers. We found that we could replace all of these with 3 × 3 filters, with comparable accuracy. As suggested by Simonyan & Zisserman (2014), stacked 3× 3 filters have the effective receptive field of larger filters with less computational complexity. In\nthis nin-c3 network, we replaced the first convolutional layer with one 3 × 3 layer, and the second convolutional layer with two 3× 3 layers. This network is 26% faster than the standard NiN model, with only 54% of the model parameters. Using our low-rank filters in this network, we trained the nin-c3-lr network, which is of similar accuracy (91.8% v.s. 91.9%) but is approximately 54% of the original network’s computational complexity, with only 45% of the model parameters."
    }, {
      "heading" : "5 DISCUSSION",
      "text" : "It is somewhat surprising that networks based on learning filters with less representational ability are able to do as well, or better, than CNNs with full k× k filters on the task of image classification. However, a lot of interesting small-scale image structure is well-characterized by low-rank filters, e.g. edges and gradients. Our experiments training a separable (rank-1) model (‘vgg-gmp-sf’) on ILSVRC and MIT Places show surprisingly high accuracy on what are considered challenging problems – approx. 88% top-5 accuracy on ILSVRC – but not enough to obtain comparable accuracies to the models on which they are based.\nGiven that most discriminative filters learned for image classification appear to be low-rank, we instead structure our architectures with a set of basis filters in the way illustrated in Fig. 2d. This allows our networks to learn the most effective combinations of complex (e.g. k×k) and simple (e.g. 1 × k, k × 1) filters. Furthermore, in restricting how many complex spatial filters may be learned, this architecture prevents over-fitting, and helps improve generalization. Even in our models where we do not use square k× k filters, we obtain comparable accuracies to the baseline model, since the rank-2 cross-shaped filters effectively learned as a combination of 3× 1 and 1× 3 filters are capable of representing more complex local pixel relations than rank-1 filters."
    }, {
      "heading" : "6 CONCLUSION",
      "text" : "This paper has presented a method to train convolutional neural networks from scratch using lowrank filters. This is made possible by a new way of initializing the networks weights which takes into consideration the presence of differently shape filters in composite layers. Validation on image classification in three popular datasets confirms similar or higher accuracy than state of the art, with much greater computational efficiency.\nRecent advances in state-of-the-art accuracy with CNNs for image classification have come at the cost of increasingly large and computational complex models. We believe our results to show that learning computationally efficient models with fewer, more relevant parameters, can prevent overfitting, increase generalization and thus also increase accuracy.\nFUTURE WORK\nThis paper has addressed the spatial extents of convolutional filters in CNNs, however the channel extents also exhibit some redundancy, as highlighted by Jaderberg et al. (2014), and exploited in the form of low-dimensional embeddings by Lin et al. (2013); Szegedy et al. (2014). We intend to further explore how our methods can be extended to learn and combine even smaller basis filters and filters with more diverse shapes."
    }, {
      "heading" : "B MULTIPLY-ACCUMULATE OPERATIONS AND CAFFE CPU/GPU TIMINGS.",
      "text" : "We have characterized the computational complexity of a CNN using the number of multiply accumulate operations required for a forward pass (which depends on the size of the filters in each convolutional layer as well as the input image size and stride), to give as close as possible to a hardware and implementation independent evaluation the computational complexity of our method. However, we have observed strong correlation between multiply-accumulate counts and runtime for both CPU and GPU implementations of the networks described here (as shown in Fig. 6). Note that the Caffe timings differ more for the initial convolutional layers where the input sizes are much smaller (3-channels), and BLAS is less efficient for the relatively small matrices being multiplied."
    }, {
      "heading" : "C COMPARING WITH STATE OF THE ART NETWORKS FOR ILSVRC",
      "text" : "Figures 7 and 8 compare published top-5 ILSVRC validation error v.s. multiply-accumulate operations and number of model parameters (respectively) for several state-of-the-art networks (Simonyan & Zisserman, 2014; Szegedy et al., 2014; He et al., 2015). The error rates for these networks are only reported as obtained with different combinations of computationally expensive training and\ntest time augmentation methods, including scale, photometric, ensembles (multi-model), and multiview/dense oversampling. This can makes it difficult to compare model architectures, especially with respect to computational requirements.\nState-of-the-art networks, such as MSRA-C, VGG-19 and oversampled GoogLeNet are orders of magnitude larger in computational complexity than our networks. From Fig. 7, where the multiplyaccumulate operations are plotted on a log scale, increasing the model size and/or computational complexity of test-time augmentation of CNNs appears to have diminishing returns for decreasing validation error. Our models without training or test time augmentation show comparable accuracy to networks such as VGG-13 with training and test time augmentation, while having far less computational complexity and model size. In particular, the ‘googlenet-lr’ model has a much smaller test-time model size than any network of comparable accuracy."
    }, {
      "heading" : "D PLOTS OF RESULTS",
      "text" : "Following are several plots of results, which for reasons of space consideration are not in the main section of the paper. These include the results for VGG-derived models on MIT Places (Fig. 9), GoogLeNet-derived models on ILSVRC (Fig. 10), and finally the results for Network-in-Networkderived models on CIFAR-10 (Fig. 11).\nE VGG-DERIVED MODEL TABLE\nFigure 6 shows the architectual details of the VGG-11 derived models used in §4.1.\n18"
    } ],
    "references" : [ {
      "title" : "Stochastic gradient descent tricks",
      "author" : [ "Bottou", "Léon" ],
      "venue" : "In Neural Networks: Tricks of the Trade,",
      "citeRegEx" : "Bottou and Léon.,? \\Q2012\\E",
      "shortCiteRegEx" : "Bottou and Léon.",
      "year" : 2012
    }, {
      "title" : "Neocognitron: A Self-Organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position",
      "author" : [ "Fukushima", "Kunihiko" ],
      "venue" : "Biological Cybernetics,",
      "citeRegEx" : "Fukushima and Kunihiko.,? \\Q1980\\E",
      "shortCiteRegEx" : "Fukushima and Kunihiko.",
      "year" : 1980
    }, {
      "title" : "Understanding the difficulty of training deep feedforward neural networks",
      "author" : [ "Glorot", "Xavier", "Bengio", "Yoshua" ],
      "venue" : "In International conference on artificial intelligence and statistics,",
      "citeRegEx" : "Glorot et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Glorot et al\\.",
      "year" : 2010
    }, {
      "title" : "Maxout Networks",
      "author" : [ "Goodfellow", "Ian", "Warde-farley", "David", "Mirza", "Mehdi", "Courville", "Aaron", "Bengio", "Yoshua" ],
      "venue" : "In Proceedings of the 30th International Conference on Machine Learning",
      "citeRegEx" : "Goodfellow et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Goodfellow et al\\.",
      "year" : 2013
    }, {
      "title" : "Deep Learning with Limited Numerical Precision, February 2015. URL http://arxiv.org/abs/1502.02551v1;http:// arxiv.org/pdf/1502.02551v1",
      "author" : [ "Gupta", "Suyog", "Agrawal", "Ankur", "Gopalakrishnan", "Kailash", "Narayanan", "Pritish" ],
      "venue" : null,
      "citeRegEx" : "Gupta et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Gupta et al\\.",
      "year" : 2015
    }, {
      "title" : "Delving Deep into Rectifiers: Surpassing Human-Level",
      "author" : [ "He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Jian Sun" ],
      "venue" : "Performance on ImageNet Classification. CoRR,",
      "citeRegEx" : "He et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2015
    }, {
      "title" : "Gradient Flow in Recurrent Nets: the Difficulty of Learning Long-Term Dependencies",
      "author" : [ "Hochreiter", "Sepp", "Bengio", "Yoshua", "Frasconi", "Paolo", "Schmidhuber", "Jürgen" ],
      "venue" : null,
      "citeRegEx" : "Hochreiter et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Hochreiter et al\\.",
      "year" : 2001
    }, {
      "title" : "Speeding up Convolutional Neural Networks with Low Rank Expansions",
      "author" : [ "Jaderberg", "Max", "Vedaldi", "Andrea", "Zisserman", "Andrew" ],
      "venue" : "CoRR, abs/1405.3866,",
      "citeRegEx" : "Jaderberg et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Jaderberg et al\\.",
      "year" : 2014
    }, {
      "title" : "Caffe: Convolutional Architecture for Fast Feature Embedding",
      "author" : [ "Jia", "Yangqing", "Shelhamer", "Evan", "Donahue", "Jeff", "Karayev", "Sergey", "Long", "Jonathan", "Girshick", "Ross", "Guadarrama", "Sergio", "Darrell", "Trevor" ],
      "venue" : "arXiv preprint arXiv:1408.5093,",
      "citeRegEx" : "Jia et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Jia et al\\.",
      "year" : 2014
    }, {
      "title" : "Learning Multiple Layers of Features from Tiny Images",
      "author" : [ "Krizhevsky", "Alex" ],
      "venue" : "Technical report, University of Toronto,",
      "citeRegEx" : "Krizhevsky and Alex.,? \\Q2009\\E",
      "shortCiteRegEx" : "Krizhevsky and Alex.",
      "year" : 2009
    }, {
      "title" : "ImageNet Classification with Deep Convolutional Neural Networks",
      "author" : [ "Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "Krizhevsky et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Krizhevsky et al\\.",
      "year" : 2012
    }, {
      "title" : "Optimal brain damage",
      "author" : [ "LeCun", "Yann", "Denker", "John S", "Solla", "Sara A", "Howard", "Richard E", "Jackel", "Lawrence D" ],
      "venue" : "In NIPs,",
      "citeRegEx" : "LeCun et al\\.,? \\Q1989\\E",
      "shortCiteRegEx" : "LeCun et al\\.",
      "year" : 1989
    }, {
      "title" : "Gradient-Based Learning Applied to Document Recognition",
      "author" : [ "LeCun", "Yann", "Bottou", "Léon", "Bengio", "Yoshua", "Haffner", "Patrick" ],
      "venue" : "In Proceedings of the IEEE,",
      "citeRegEx" : "LeCun et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "LeCun et al\\.",
      "year" : 1998
    }, {
      "title" : "Simplifying convnets for fast learning",
      "author" : [ "Mamalet", "Franck", "Garcia", "Christophe" ],
      "venue" : "In Artificial Neural Networks and Machine Learning–ICANN",
      "citeRegEx" : "Mamalet et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Mamalet et al\\.",
      "year" : 2012
    }, {
      "title" : "Object Detection Networks on Convolutional Feature Maps",
      "author" : [ "Ren", "Shaoqing", "He", "Kaiming", "Girshick", "Ross", "Zhang", "Xiangyu", "Sun", "Jian" ],
      "venue" : "arXiv preprint arXiv:1504.06066,",
      "citeRegEx" : "Ren et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ren et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning Separable Filters",
      "author" : [ "Rigamonti", "Roberto", "Sironi", "Amos", "Lepetit", "Vincent", "Fua", "Pascal" ],
      "venue" : "In CVPR, pp. 2754–2761",
      "citeRegEx" : "Rigamonti et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Rigamonti et al\\.",
      "year" : 2013
    }, {
      "title" : "Very Deep Convolutional Networks for Large-Scale Image Recognition",
      "author" : [ "Simonyan", "Karen", "Zisserman", "Andrew" ],
      "venue" : "CoRR, abs/1409.1556,",
      "citeRegEx" : "Simonyan et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Simonyan et al\\.",
      "year" : 2014
    }, {
      "title" : "Efficient Object Localization Using Convolutional Networks",
      "author" : [ "Tompson", "Jonathan", "Goroshin", "Ross", "Jain", "Arjun", "LeCun", "Yann", "Bregler", "Christoph" ],
      "venue" : null,
      "citeRegEx" : "Tompson et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Tompson et al\\.",
      "year" : 2015
    }, {
      "title" : "Improving the speed of neural networks on CPUs",
      "author" : [ "Vanhoucke", "Vincent", "Senior", "Andrew", "Mao", "Mark Z" ],
      "venue" : "In Proc. Deep Learning and Unsupervised Feature Learning NIPS Workshop,",
      "citeRegEx" : "Vanhoucke et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Vanhoucke et al\\.",
      "year" : 2011
    }, {
      "title" : "2015), in that a layer with n̂l connections followed by a ReLU activation function should be initialized with a zero-mean Gaussian distribution with standard deviation",
      "author" : [ "He" ],
      "venue" : null,
      "citeRegEx" : "He,? \\Q2015\\E",
      "shortCiteRegEx" : "He",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 10,
      "context" : "Convolutional neural networks (CNNs) have been used increasingly succesfully to solve challenging computer vision problems such as image classification (Krizhevsky et al., 2012), object detection (Ren et al.",
      "startOffset" : 152,
      "endOffset" : 177
    }, {
      "referenceID" : 14,
      "context" : ", 2012), object detection (Ren et al., 2015), and human pose estimation (Tompson et al.",
      "startOffset" : 26,
      "endOffset" : 44
    }, {
      "referenceID" : 17,
      "context" : ", 2015), and human pose estimation (Tompson et al., 2015).",
      "startOffset" : 35,
      "endOffset" : 57
    }, {
      "referenceID" : 12,
      "context" : "Such filters usually have limited spatial extent and their learned weights are shared across the image spatial domain to provide translation invariance (Fukushima, 1980; LeCun et al., 1998).",
      "startOffset" : 152,
      "endOffset" : 189
    }, {
      "referenceID" : 4,
      "context" : "For example Gupta et al. (2015) and Vanhoucke et al.",
      "startOffset" : 12,
      "endOffset" : 32
    }, {
      "referenceID" : 4,
      "context" : "For example Gupta et al. (2015) and Vanhoucke et al. (2011) achieve training- and test-time compute savings by further quantization of network weights that were originally represented as 32 bit floating point numbers.",
      "startOffset" : 12,
      "endOffset" : 60
    }, {
      "referenceID" : 11,
      "context" : "LeCun et al. (1989) suggest a method of pruning unimportant connections within networks.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 11,
      "context" : "LeCun et al. (1989) suggest a method of pruning unimportant connections within networks. However this requires repeated network re-training and may be infeasible for modern, state-of-the-art CNNs requiring weeks of training time. Lin et al. (2013) show that the geometric increase in the number and dimensions of filters with deeper networks can be managed using low-dimensional embeddings.",
      "startOffset" : 0,
      "endOffset" : 248
    }, {
      "referenceID" : 11,
      "context" : "LeCun et al. (1989) suggest a method of pruning unimportant connections within networks. However this requires repeated network re-training and may be infeasible for modern, state-of-the-art CNNs requiring weeks of training time. Lin et al. (2013) show that the geometric increase in the number and dimensions of filters with deeper networks can be managed using low-dimensional embeddings. The same authors show that global average-pooling may be used to decrease model size in networks with fully connected layers. Simonyan & Zisserman (2014) show that stacked filters with small spatial dimensions (e.",
      "startOffset" : 0,
      "endOffset" : 545
    }, {
      "referenceID" : 14,
      "context" : "Rigamonti et al. (2013) approximate previously trained CNNs with low-rank filters for the semantic segmentation of curvilinear structures within volumetric medical imagery.",
      "startOffset" : 0,
      "endOffset" : 24
    }, {
      "referenceID" : 7,
      "context" : "The work of Jaderberg et al. (2014) also approximates the existing filters of previously trained networks.",
      "startOffset" : 12,
      "endOffset" : 36
    }, {
      "referenceID" : 7,
      "context" : "(b) Sequential separable filters (Jaderberg et al., 2014).",
      "startOffset" : 33,
      "endOffset" : 57
    }, {
      "referenceID" : 7,
      "context" : "2 SEQUENTIAL SEPARABLE FILTERS An existing scheme for reducing the computational complexity of convolutional layers (Jaderberg et al., 2014) is to replace each one with a sequence of two regular convolutional layers but with filters that are rectangular in the spatial domain, as shown in Fig 2b.",
      "startOffset" : 116,
      "endOffset" : 140
    }, {
      "referenceID" : 7,
      "context" : "2 SEQUENTIAL SEPARABLE FILTERS An existing scheme for reducing the computational complexity of convolutional layers (Jaderberg et al., 2014) is to replace each one with a sequence of two regular convolutional layers but with filters that are rectangular in the spatial domain, as shown in Fig 2b. The first convolutional layer has m filters of sizew×1×c, producing an output feature map withm channels. The second convolutional layer has d filters of size 1×h×m, producing an output feature map with d channels. By this means the full rank original convolutional filter bank is represented by a low rank approximation formed from a linear combination of a set of separable w × h basis filters. The computational complexity of this scheme is O(mcw) for the first layer of horizontal filters and O(dmh) for the second layer of vertical filters, with a total of O(m(cw + dh)). Note that Jaderberg et al. (2014) use this scheme to approximate existing full rank filters belonging to previously trained networks using a retrospective fitting step.",
      "startOffset" : 117,
      "endOffset" : 908
    }, {
      "referenceID" : 7,
      "context" : "2 SEQUENTIAL SEPARABLE FILTERS An existing scheme for reducing the computational complexity of convolutional layers (Jaderberg et al., 2014) is to replace each one with a sequence of two regular convolutional layers but with filters that are rectangular in the spatial domain, as shown in Fig 2b. The first convolutional layer has m filters of sizew×1×c, producing an output feature map withm channels. The second convolutional layer has d filters of size 1×h×m, producing an output feature map with d channels. By this means the full rank original convolutional filter bank is represented by a low rank approximation formed from a linear combination of a set of separable w × h basis filters. The computational complexity of this scheme is O(mcw) for the first layer of horizontal filters and O(dmh) for the second layer of vertical filters, with a total of O(m(cw + dh)). Note that Jaderberg et al. (2014) use this scheme to approximate existing full rank filters belonging to previously trained networks using a retrospective fitting step. In this work, by contrast, we train networks containing convolutional layers with this architecture from scratch. In effect, we learn the separable basis filters and their combination weights simultaneously during network training. 2.3 FILTERS AS LINEAR COMBINATIONS OF BASES In this work we introduce another scheme for reducing convolutional layer complexity. This works by representing convolutional filters as linear combinations of basis filters as illustrated in Fig. 2c. This scheme uses composite layers comprising several sets of filters where the filters in each set have different spatial dimensions (see Fig. 5). The outputs of these basis filters may be combined in a subsequent layer containing filters with spatial dimensions 1× 1. This is illustrated in Fig. 2c. Here, our composite layer contains horizontal w× 1 and vertical 1× h filters, the outputs of which are concatenated in the channel dimension, resulting in an intermediate m-channel feature map. These filter responses are then linearly combined by the next layer of d 1 × 1 filters to give a d-channel output feature map. In this case, the filters are applied on the input feature map with c channels and followed by a set of m 1×1 filters over the m output channels of the basis filters. If the number of horizontal and vertical filters is the same, the computational complexity is O(m(wc/2 + hc/2 + d)). Interestingly, the configuration of Fig. 2c gives rise to linear combinations of horizontal and vertical filters that are cross-shaped in the spatial domain. This is illustrated in Fig. 3 for filters learned in the first convolutional layer of the‘vgg-gmp-lr-join’ model that is described in the Results section when it is trained using ILSVRC dataset. Note that, in general, more than two different sizes of basis filter might be used in the composite layer. For example, Fig. 2d shows a combination of three sets of filters with spatial dimensions w× 1, 1×h, and w×h. Also note that an interesting option is to omit the 1× 1 linear combination layer and instead allow the connection weights in a subsequent network layer to learn to combine the basis filters of the preceding layer (despite any intermediate non-linearity, e.g. ReLUs). This possibility is explored in practice in the Results section. In that our method uses a combination of filters in a composite layer, it is similar to the ‘GoogLeNet’ of Szegedy et al. (2014) which uses ‘inception’ modules comprising several (square) filters of different sizes ranging from 1×1 to 5×5.",
      "startOffset" : 117,
      "endOffset" : 3459
    }, {
      "referenceID" : 5,
      "context" : "To determine the standard deviations to be used for weight initialization, we use an approach similar to that described by Glorot & Bengio (2010) (with the adaptation described by He et al. (2015) for layers followed by a ReLU).",
      "startOffset" : 180,
      "endOffset" : 197
    }, {
      "referenceID" : 5,
      "context" : "To determine the standard deviations to be used for weight initialization, we use an approach similar to that described by Glorot & Bengio (2010) (with the adaptation described by He et al. (2015) for layers followed by a ReLU). In Appendix A, we show the details of our derivation, generalizing the approach of He et al. (2015) to the initialization of ‘composite’ layers comprising several groups of filters of different spatial dimensions (see Appendix A, Fig.",
      "startOffset" : 180,
      "endOffset" : 329
    }, {
      "referenceID" : 5,
      "context" : "It is in the same family of network architectures used by Simonyan & Zisserman (2014); He et al. (2015) to obtain the state-of-the-art accuracy for ILSVRC, but uses fewer convolutional layers and therefore fits on a single GPU during training.",
      "startOffset" : 87,
      "endOffset" : 104
    }, {
      "referenceID" : 5,
      "context" : "It is in the same family of network architectures used by Simonyan & Zisserman (2014); He et al. (2015) to obtain the state-of-the-art accuracy for ILSVRC, but uses fewer convolutional layers and therefore fits on a single GPU during training. During training of our VGG-11 based models, we used the standard hyperparameters as detailed by Simonyan & Zisserman (2014) and the initialization of He et al.",
      "startOffset" : 87,
      "endOffset" : 368
    }, {
      "referenceID" : 5,
      "context" : "It is in the same family of network architectures used by Simonyan & Zisserman (2014); He et al. (2015) to obtain the state-of-the-art accuracy for ILSVRC, but uses fewer convolutional layers and therefore fits on a single GPU during training. During training of our VGG-11 based models, we used the standard hyperparameters as detailed by Simonyan & Zisserman (2014) and the initialization of He et al. (2015). In what follows, we compare the accuracy of a number of different network architectures detailed in Appendix E, Table 6.",
      "startOffset" : 87,
      "endOffset" : 411
    }, {
      "referenceID" : 7,
      "context" : "Unlike Jaderberg et al. (2014), we trained this network from scratch instead of approximating the full-rank filters in a previously trained network.",
      "startOffset" : 7,
      "endOffset" : 31
    }, {
      "referenceID" : 7,
      "context" : "Accuracy, multiply-accumulate operations, and number of parameters for the baseline ‘vgg-11-gmp’ network, separable filter network as described by Jaderberg et al. (2014), and more efficient models created by the methods described in this paper.",
      "startOffset" : 147,
      "endOffset" : 171
    }, {
      "referenceID" : 8,
      "context" : "We note that although the Caffe pre-trained GoogLeNet model (Jia et al., 2014) has a top-5 accuracy of 0.",
      "startOffset" : 60,
      "endOffset" : 78
    }, {
      "referenceID" : 3,
      "context" : "Like most state of the art CIFAR results, this was with ZCA pre-processed training and test data (Goodfellow et al., 2013), training time mirror augmentation and random sub-crops.",
      "startOffset" : 97,
      "endOffset" : 122
    }, {
      "referenceID" : 3,
      "context" : "Like most state of the art CIFAR results, this was with ZCA pre-processed training and test data (Goodfellow et al., 2013), training time mirror augmentation and random sub-crops. The results of our CIFAR experiments are listed in Table 4 and plotted in Fig. 11. This architecture uses 5× 5 filters in some layers. We found that we could replace all of these with 3 × 3 filters, with comparable accuracy. As suggested by Simonyan & Zisserman (2014), stacked 3× 3 filters have the effective receptive field of larger filters with less computational complexity.",
      "startOffset" : 98,
      "endOffset" : 449
    }, {
      "referenceID" : 7,
      "context" : "FUTURE WORK This paper has addressed the spatial extents of convolutional filters in CNNs, however the channel extents also exhibit some redundancy, as highlighted by Jaderberg et al. (2014), and exploited in the form of low-dimensional embeddings by Lin et al.",
      "startOffset" : 167,
      "endOffset" : 191
    }, {
      "referenceID" : 7,
      "context" : "FUTURE WORK This paper has addressed the spatial extents of convolutional filters in CNNs, however the channel extents also exhibit some redundancy, as highlighted by Jaderberg et al. (2014), and exploited in the form of low-dimensional embeddings by Lin et al. (2013); Szegedy et al.",
      "startOffset" : 167,
      "endOffset" : 269
    }, {
      "referenceID" : 7,
      "context" : "FUTURE WORK This paper has addressed the spatial extents of convolutional filters in CNNs, however the channel extents also exhibit some redundancy, as highlighted by Jaderberg et al. (2014), and exploited in the form of low-dimensional embeddings by Lin et al. (2013); Szegedy et al. (2014). We intend to further explore how our methods can be extended to learn and combine even smaller basis filters and filters with more diverse shapes.",
      "startOffset" : 167,
      "endOffset" : 292
    } ],
    "year" : 2017,
    "abstractText" : "We propose a new method for creating computationally efficient convolutional neural networks (CNNs) by using low-rank representations of convolutional filters. Rather than approximating filters in previously-trained networks with more efficient versions, we learn a set of small basis filters from scratch; during training, the network learns to combine these basis filters into more complex filters that are discriminative for image classification. To train such networks, a novel weight initialization scheme is used. This allows effective initialization of connection weights in convolutional layers composed of groups of differently-shaped filters. We validate our approach by applying it to several existing CNN architectures and training these networks from scratch using the CIFAR, ILSVRC and MIT Places datasets. Our results show similar or higher accuracy than conventional CNNs with much less compute. Applying our method to an improved version of VGG11 network using global max-pooling, we achieve comparable validation accuracy using 41% less compute and only 24% of the original VGG-11 model parameters; another variant of our method gives a 1 percentage point increase in accuracy over our improved VGG-11 model, giving a top-5 center-crop validation accuracy of 89.7% while reducing computation by 16% relative to the original VGG-11 model. Applying our method to the GoogLeNet architecture for ILSVRC, we achieved comparable accuracy with 26% less compute and 41% fewer model parameters. Applying our method to a near state-of-the-art network for CIFAR, we achieved comparable accuracy with 46% less compute and 55% fewer parameters.",
    "creator" : "LaTeX with hyperref package"
  }
}