{
  "name" : "1301.6732.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Graphical Representations of Consensus Belief",
    "authors" : [ "David M. Pennock", "Michael P. Wellman" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Graphical models based on conditional indepen dence support concise encodings of the subjec tive belief of a single agent. A natural ques tion is whether the consensus belief of a group of agents can be represented with equal parsimony. We prove, under relatively mild assumptions, that even if everyone agrees on a common graph topology, no method of combining beliefs can maintain that structure. Even weaker conditions rule out local aggregation within conditional probability tables. On a more positive note, we show that if probabilities are combined with the logarithmic opinion pool (LogOP), then com monly held Markov independencies are main tained. This suggests a straightforward proce dure for constructing a consensus Markov net work. We describe an algorithm for computing the LogOP with time complexity comparable to that of exact Bayesian inference.\n1 Introduction\nSuppose that you are charged with the task of modeling the effect of interest rates on the inflation rate. For sim plicity, assume. that there are only two relevant binary un certain events: interest rates rise and inflation rates rise. A full joint probability distribution describing this situa tion would assign a probability to each of the four possi ble combinations of outcomes. As the number of modeled events increases, the size of the joint distribution grows ex ponentially. Yet often, the probabilistic relationships can be specified more naturally and compactly in terms of lo cal probabilistic dependencies among events. Graphical models offer a language for describing a joint distribution in terms of events and the conditional dependence between them [Jensen, 1996, P earl, 1988, Whittaker, 1990]. Expert systems based on such models are among the most success ful and practical products to emerge from artificial intelli-\ngence (AI) research. One of their key features is the abil ity to efficiently encode an otherwise unmanageably large joint distribution. Indeed, if sufficient conditional indepen dencies (Cis) exist, then memory requirements are expo nentially reduced.\nFrom an AI perspective, a graphical model typically en codes the subjective belief of a single agent. We address the more general task of compactly representing the con sensus or combined belief of a group of agents.\nFor a modeler, decisions about how to combine beliefs are almost unavoidable. For example, in pursuit of an accurate distribution over interest and inflation rates, you may wish to consult several economists. In a larger model, each ex pert might be a specialist in some subset of the complete domain. When several related models already exist, it may be desirable to conglomerate their knowledge into a single, more general representation. Even when consulting only one expert to construct only one model, the designer's be liefs inevitably play a role- for example, he or she may choose to correct for typical biases of those unfamiliar with probability theory-in fact, choosing not to correct for bias itself may distort the expert's true beliefs.\nCan the success of graphical models within the single-agent framework be extended to this multiagent setting? More specifically, given each of the agents' beliefs, and some reasonable aggregation rule, will the combined belief have enough structure to warrant a graphical representation?\nDecades of research have yielded a variety of pre scriptions for aggregating beliefs, which we survey briefly in Section 2. We distinguish between two prevailing methodologies. The first, which enjoys a rich history within statistics and the decision sci ences, defines aggregation over joint distributions [Dalkey, 1975, Genest and Zidek, 1986, Wagner, 1984, Madansky, 1964]. The second, more recent and more popular within the AI community, focuses on combining graphical models [Matzkevich and Abramson, 1992, Ng and Abramson, 1994, Xiang, 1996].\nThis paper demonstrates that common assumptions regard-\n532 Pennock and Wellman\ning the aggregation of joint distributions imply severe lim itations for combining graphical topologies. Most propos als for combining graphical models assume that, at a min imum, if all agents' beliefs conform to a particular struc ture, then the consensus model should mirror that struc ture. Yet, as we see in Section 3, almost all proposed sta tistical aggregation methods violate this property. In fact, we prove that no combination function can preserve unani mous structures while simultaneously satisfying other nat ural and desirable properties. We also demonstrate that essentially no combination function can operate within each conditional probability table separately, and also de pend only on the underlying joint distributions. In Sec tion 4, we show that, although it cannot maintain arbitrary structures, a weighted geometric average aggregation rule, called the LogOP, does maintain all unanimously agreed upon Markov structures. We then describe procedures for generating consensus structures that are consistent with the LogOP. Section 5 presents an algorithm for computing the LogOP that, if the consensus structure is sufficiently sparse, can run exponentially faster than a brute force approach.\n2 Background: Belief and Consensus Belief\nSection 2.1 describes background and related work on sub jective probability and belief aggregation. In Section 2.2, we cover relevant material on two graphical models for representing probability distributions-Bayesian networks (BNs) and Markov networks (MNs)-and discuss their ap plicability for encoding both individual and multiagent be lief.\n2.1 Opinion pools and aggregation properties\nSuppose that n agents are uncertain about m binary events, A1, Az, . . . , Am, and thus do not know which of the 2m possible joint outcomes or atomic states will eventually ob-. I { tam. Let Z = A1, Az, ... , Am} be the set of events, and Jet (2 = {wl, Wz, ... , Wzm} be the Set Of all 2m (exclusive, exhaustive) atomic states. We refer to the Aj as the primary events, to distinguish them from the other 22m - m possi ble sets of atomic states, each of which is also an event. A joint probability distribution Pr associates a probability with each atomic state.\nA designer of a probabilistic expert system must assign, implicitly or explicitly, all 2m probabilities. In many situations-for example, when the modeled events encom pass a domain broader than any one expert's specialty more than one source is consulted for probabilities. There is a large body of work in the statistics literature which ad dresses the aggregation of experts' beliefs into a single, co herent representation.\n1 An example of an atomic state is A1 A A2 A A3 1\\ · · · 1\\ Am or, written more simply, A,A2A3 ... Am.\nIf each of the n experts, including the system designer, holds a subjective belief Pr;, then a consensus joint proba bility distribution Pro is any function f of the Pr;:\n(I)\nwhere Pro is itself a legal joint probability distribution. The combination function f is often called an opinion pool. Many pooling functions over many years have been proposed; Genest and Zidek [1986] provide an excellent overview of the various kinds and discuss their relative merits. The two most common and well-studied are the linear and logarithmic opinion pools (LinOP, LogOP). The LinOP is a weighted arithmetic mean of the members' probabilities for atomic events,\nN Pro(Wj) = L a;Pr;(wj ),\ni=l\nand the LogOP is a weighted geometric mean,\n(2)\n(3)\nwhere the a;, called expert weights, are nonnegative num bers that sum to one. A third pooling method iden tifies one distinguished individual h (real or fictitious, within or outside the group) as a so-called supra Bayesian [Lindley, 1985]. The consensus is then defined as the supra Bayesian's posterior distribution, given the \"evidence\" pro vided by all of the experts' opinions:\nPra(w/Prt, ... , PrN) ex: Prh(Pr1, ... , PrN/w)Prh(w). (4) Because this approach takes a single agent's perspective, it is well grounded in normative Bayesian theory. Implement ing it requires that we choose the supra Bayesian, or assess its prior belief if it is fictitious [Genest and Zidek, 1986]. Computing the posterior further requires that the supra Bayesian specify a joint distribution over all other agents' beliefs.\nAttempts to justify more symmetric opinion pools often proceed by posing axioms on the combi nation function, and arguing that they represent desirable properties [Dalkey, 1975, Genest, 1984c, Genest, 1984b, Genest, 1984a, Genest and Zidek, 1986, Genest et al. , 1986, Genest and Wagner, 1987, Wagner, 1984]. Researchers have proved that certain pooling formulae are implied by certain sets of prop erties. We begin with two seemingly incontrovertible assumptions.\nProperty 1 (Unanimity (UNAM)) !fPrh(w) = Pr;(w) for all agents h and i, and for all states w E 0, then Pro(w) = Pr1(w).\nGraphical Representations of Consensus Belief 533\nProperty 2 (Nondictatorship (ND)) There is no single agent i such that Pro(w) = Pr;(w) for all w E 0, and regardless of the agents' beliefs.\nUNAM states that if everyone's assessments are in com plete agreement, then the consensus agrees as well. ND simply ensures that what is inherently a multiagent prob lem is not reduced to the single-agent case.\nProperty 3 (Marginalization property (MP)) Let E be an arbitrary event, that is, any subset ofO. Then,\nf(Prt, Pr2, ... , Prn)(E) = f(Prt(E) , Pr2(E), ... , Prn(E)).\nProperty 4 (Externally Bayesian (EB)) Let E and F be arbitrary events. Then,\nf(Prt, Pr2, ... , Prn)(EIF) =\nf(PrtiF, Pr2IF, ... , Prn IF)( E).\nMP and EB require consistency for probabilistic oper ations performed before and after pooling. MP states that we obtain the same probability for an event E whether we pool the opinions first, and then compute Pro(E) = LwEE Pro(w ), or if we first compute Pr;(E) = LwEE Pr;(w) for each agent i, and then pool their opin ions only over E. Similarly, EB holds that we obtain the same Pr0(EIF) whether we combine opinions first and condition on F second, or condition on F first and com bine opinions second. It has been shown that any f satisfy ing both MP and UNAM is a LinOP [Genest, 1984c], and any satisfying EB and UNAM is a LogOP [Genest, 1984a]. Genest [1984b] also shows that f cannot simultaneously satisfy MP, EB, UNAM, and ND.\nProperty 5 (Proportional dependence on states (PDS))\nPro(w) <X f(Prt(w) , Pr2(w), ... , Prn(w)).\nPDS is sometimes called independence of irrelevant states, or termed a likelihood principle. It assures that the con sensus likelihood ratio between two states does not depend on the agents' assessments of any other \"irrelevant\" state. The LinOP, LogOP, and most other proposed opinion pools satisfy PDS.\nProperty 6 (Independence erty (IPP)) Let E and If Pr;(EIF) Pr;(E) Pro(EIF) = Pro(E). preservation F be arbitrary for all agents i, prop events. then\nIPP requires that all unanimously held independencies are preserved in the consensus. Advocates of IPP reason that identifying the independencies in a model is central to un derstanding the underlying phenomena, and that complete agreement on this dimension should be embraced. On the\nother hand, Genest and Wagner [ 1987] make a compelling case against the use of IPP by proving that no aggregation function whatsoever can satisfy it along with PDS and ND, when 1111 2: 5.\nOne might argue that IPP is overly strong. It requires preservation of, for example, a unanimous independence between the events E = A3:ih and F = A2A4 V A7. This kind of independence seems of little descriptive value to a modeler, and indeed cannot be represented with a BN. The designer may be willing to forgo preserving all indepen dencies, being content to preserve independencies among the primary events, A1, A2, .. , Am. With this in mind, we define a weaker independence property.\nProperty 7 (Event independence preservation property (EIPP)) If Pr;(A1 IAk) = Pr;(Aj) for all agents i, then Pro(Aj IAk) = Pro(Aj ).\nIn Section 3, we see that substituting EIPP for IPP does ad mits a possibility that is consistent with both PDS and ND, though not a very satisfactory one. In search of a nontrivial possibility, we define two even weaker independence con ditions.\nProperty 8 (Markov event independence preservation property (MEIPP)) IfPr;(AJIWAk) = Pr;(AJIW)for all agents i and for all W <;; Z (including W = 0 ), then Pro(Aj IAk) = Pro(Aj)·\nProperty 9 (Non-Markov event independence preser vation property (NMEIPP)) If Pr;(Aj IAk) = Pr;(Aj) for all agents i, and Prh(Aj IW Ak) f Prh(Aj IW), for some agent h and some W <;; Z, then Pro(Aj IAk) = Pro(Aj ).\nThese two properties are purposely constructed so that EIPP ¢} (MEIPP 1\\ NMEIPP). We see in Section 3 that the source of the impossibility lies entirely within the lat ter. Finally, we define a stronger version of the MEIPP.\nProperty 10 (Markov independence preservation prop erty (MIPP)) Let W, X <;; Z- Aj be disjoint sets of events such thatA1UWUX = Z. IfPr;(AJIWX) = Pr;(AJIW) for a// agents i, then Pr0(A1 IW X) = Pro(Aj IW).\nThe relative strengths of these various independence con ditions can be summarized as follows:\nIPP =? EIPP ¢} (MEIPP 1\\ NMEIPP)\nMIPP =? MEIPP\n2.2 Graphical models for belief and consensus belief\nThe Bayesian network (BN) has proved invaluable as a language for compactly encoding a joint probability distri bution [Jensen, 1996]. Conciseness is achieved by factor ing atomic states into primary events, and exploiting con ditional independence among these events. Consider the\n534 Pennock and Wellman\nevent Ak E Z, with events A,, A2, . . . , Ak-l preceding it in index order. Suppose that, given the outcomes of a subset pa(Ak) S::: {A1, A2, ... , Ak-d of its preceding events called Ak 's parents-the event Ak is conditionally inde pendent of all other preceding events. This structure can be depicted graphically as a directed acyclic graph: each event is a node in the graph, and there is a directed edge from node Aj to node Ak if and only if Aj is a parent of Ak. We also refer to Ak as the child of Aj, and Ak U pa( Ak) as the family of Ak. We can write the joint probability distribution in a (usually) more compact form:\nm Pr(A1A2 ··· Am) = IT Pr(Ak!pa(Ak)).\nk=l\nFor each event A., we record a conditional probability ta ble (CPT), which contains probabilities Pr(Ak lpa(Ak)) for all possible combinations of outcomes of events in pa(Ak)· Thus it is possible to implicitly represent the full joint with 0(2qm) probabilities, instead of 0(2m), where q is the maximum number of parents of any node in the network.\nA Markov network (MN) is another graphical lan guage for modeling conditional independence and for im plicitly describing a joint distribution [Whittaker, 1990, Darroch et al., 1980]. Events are again associated with nodes in a graph, and edges encode probabilistic dependen cies. However, as opposed to BNs, the underlying structure of a MN is an undirected graph. Given the outcomes of its direct neighbors, an event Aj is conditionally indepen dent of every other event in the network, not just preceding events. The neighbors of an event form a Markov blanket around it, \"shielding\" it from direct influence from the rest of the events [Pearl, 1988]. We call the node Aj and the set of nodes X S::: Z - Aj Markov independent, given another set W S::: Z- X - Aj, if Pr(Aj !W X) = Pr(Aj !W) and Aj U W U X = Z. Thus a node is Markov independent of all other nodes, given its blanket. Encoding the joint dis tribution implied by a MN involves assigning a potential probability to each clique [Neapolitan, 1990, Pearl, 1988].\nThe Markov blanket of a node in a BN consists of its direct parents, its direct children, and its children's direct parents [Pearl, 1988]. Therefore a BN can be converted into a MN by moralizing the network, or fully connecting (\"marry ing\") each node's parents, and dropping edge directionality [Lauritzen and Spiegelhalter, 1988, Neapolitan, 1990]. A MN can be converted into a BN by filling in or triangulating [Kloks, 1994] the graph, and adding directionality according to the fill-in order ing [Jensen, 1996, Lauritzen and Spiegelhalter, 1988, Neapolitan, 1990, Pearl, 1988]. Both transformations are sound with respect to independence, but neither is complete. A filled-in BN is also called decompos able [Chyu, 1991, Darroch et al., 1980, Pearl, 1988, Shachter et al. , 1991].\nAlthough most BN research concerns modeling a sin gle agent's belief, some researchers have examined the use of BNs in a multiagent context. Ng and Abram son [ 1994] describe an architecture called the probabilis tic multi-knowledge-base system, which consists of a col lection of BNs, each encoding the knowledge of a single expert. The authors choose to keep the BNs separate and combine probabilities at run time with a variable-weight variant of the LinOP. They address a variety of engineer ing issues, including the elicitation and propagation of ex pert confidence information, and build a working proto type to diagnose pathologies of the lymph system. Xiang [ 1996] describes conditions under which multiply sectioned Bayesian networks, originally developed for single agent reasoning, can represent the combined beliefs of multiple agents. The main assumption is that, whenever two agents' BNs contain some of the same events, they must agree on the joint distribution over these common events. Bondu elle [ 1987] prescribes both normative and behavioral tech niques for a decision maker (DM) to identify and reconcile differences of opinion among experts. When those opin ions are expressed as graphical models, he suggests that the DM first choose a consensus topology, and then calculate aggregate probabilities. Jacobs [ 1995] compares the LinOP and supra Bayesian approaches as methods for combining the multiple feature analyzers found in real and artificial neural systems.\nMatzkevich and Abramson [ 1992] give an algorithm for ex plicitly combining two BN DAGs into a single DAG, or fusing the two topologies. The algorithm transfers one arc at a time from the second DAG to the first, possibly re versing the arc in order to remain consistent with the cur rent partial ordering. Reversing arcs may add new arcs to the second DAG [Shachter, 1988], which would in turn need to be transferred. In a second paper, the same au thors show [ 1993] that the task of minimizing the number of arcs in their combined DAG is NP-hard, as are several other related tasks. They argue that, intuitively, the consen sus model should capture independencies agreed upon by at least c S: n of the agents; in particular, when c = n and the orderings are mutually consistent, the consensus DAG should be a union of the individual DAGs. In both of these papers, and in Bonduelle's work, it is essentially assumed that the EIPP, or a stronger version thereof, should hold.\nThough Matzkevich and Abramson make no commitment on how to combine probabilities, they do give an example [1992] where the LinOP is applied locally, or separately within each CPT. We say that such a localized aggregator satisfies the family aggregation (FA) property.\nProperty 11 (Family aggregation (FA))\nPro(Aj lpa(Aj )) = f(Pr,(Ai lpa(Ai )), ... , Prn(Aj lpa(Aj ))).\n--;\nGraphical Representations of Consensus Belief 535\nPr1\n(a) ,At! 1Az1\n(b) :At' 1A21\n(c) rAt) \" \" IA2: 1A31 :\n(d) 1At1 1Az;\n... , i.A31\nPr2\niAt I 1A21\nrAt! 'Az'\n(At! \" \"\n'A2! :A31\n:.At1 Az) ... , IA3:\nPro\nI At 1., Az) LinOP\nIAti rA2 ... LogOP \\ /\n!At) \" \"\n:A21 iA31\n, At I llo\\ A2) ... , 1A3)\nSuppose that two agents agree that two primary events, A1 and A2, are independent, as pictured in Figure l(a), but disagree on the associated marginal probabilities. For concreteness, let the first agent hold beliefs Pr1(Al) = Pr1(A2) = 0.5, and the second Pr2(Al) = 0.8 and Prz(Az) = 0.6. Thus,\nPr1(A1A2) = 0.25 Pr1(A1A2) = 0.25 Pr1(A1A2) = 0.25 Pr1(A1Az) = 0.25\nPr2(A1A2) = 0.48 Pr2(A1A2) = 0.32 Pr2(A1A2) = 0.12 Pr2(A1Az) = 0.08.\nNow if we apply the LinOP (2) with, say, equal weights of w1 = w2 = 0.5, we get:\nPro(A1A2) = 0.365 Pro(A!Az) = 0.41 Pro(A1A2) = 0.185 Pro(AlAz) = 0.165.\nIn particular, Pro(Al) Pro(Az) f. Pro(A1A2). and so the two events are not independent in the consensus.2 Even though the precondition of the EIPP is met, the postcondi tion is not: a BN representation of the derived consensus would have to include an edge between A1 and A2. o\nExample 2 (EIPP and the LogOP)\nSuppose that two agents' beliefs over two primary events are as described in Example I. If we apply the LogOP with equal weights, we get:\nPro(AlAz) = 0.367007 Pro(A1A2) = 0.29966 Pro(A1A2) = 0.183503 Pro(A1A2) = 0.14983.\nIn this case, Pro(A1) Pr0(A2) = Pro(A1Az), and the two events remain independent, as shown in Figure I (b). This is not a numerical coincidence; in fact, indepen dence between only two events is always maintained by the LogOP [Genest and Wagner, 1987]. Now suppose that among three primary events, both agents agree that As is independent of Az given A1. That is, both agents agree that dependencies conform to a tree structure, with A1 the parent of both A2 and As. as depicted in Figure l(c). Then once again, the LogOP will maintain this structure. One might conjecture that the LogOP maintains all BN struc tures, but this is not the case. For example, suppose that, among three primary events, the two agents agree that A1 and Az are mutually independent, and that A3 depends on both A1 and Az. That is, both agents agree on the polytree structure in Figure !(d). In this case, when we compute the consensus with the LogOP, A1 and Az will in general be come mutually dependent, the EIPP is not satisfied, and a consensus BN will require an arc between the two nodes. 0\nHaving seen that both the LinOP and the LogOP violate the EIPP, we seek a more general characterization of the class of functions that do obey it. We begin by showing that Lemma 3.2 in [Genest and Wagner, 1987], originally proved with respect to the IPP, is also applicable under the weaker EIPP.\nLemma 1 (Adapted from [Genest and Wagner, 1987]) If f obeys EIPP and PDS, then there exist constants a1, az, . . . , an, and c such that\nn Pro(wj) = L a;Pr;(wj) +c.\ni=l (5)\n2 As early as Yule [1903] it was recognized that averaging two distributions may mask a commonly held independence.\n536 Pennock and Wellman\nProof (sketch). Consider three events At. A2. and A3, with agents' beliefs described as follows:\nPr;(AtA2A3) = Pr;(AtA2A3)\nPr;(AtA2A3) Pr;(AtA2A3)\n(1- z;)2\n4(1+z;) 1- z;\n4 X;\n(6)\nwhere z; = x; + y; for all i. I n this case, all agents agree that At and A2 are independent and, as long as z; < 1, these equations describe a legal probability distribution. Since f obeys PDS, there must be some function g such that,\nand similarly for Pro(AtA2�). Now imagine a second situation exactly as in (6), except with Pr;(AtA2A3) =xi and Pr;(AtA2A3) = Yi· Genest and Wagner show that, as long as x; + y; =xi + Yi < 1, then\ng(Xt, X2, · · · , Xn) + g(yt, Y2, · · ·, Yn) == g(x�, x� , ... , x� ) + g(y�, y�, ... , y�). (7)\nFrom here, they show that since x; andy; can be chosen arbitrarily (as long as their sum is less than one), then f must have the form specified. 0\nGenest and Wagner go on to show, without further assump tion, that f must be a dictatorship. However, that proof does not carry through under the weaker condition EIPP. This can be seen via a simple counterexample. Let f al ways ignore the agents' opinions, and simply assign a uni form distribution over all w E fl. I n this case, the consensus distribution holds that all primary events Aj are indepen dent, and thus any agreed upon independencies are trivially maintained. One might wonder whether EIPP admits any other, more appealing, aggregation functions. The follow ing proposition essentially establishes that it does not.\nProposition 1 No aggregation function f can simultane ously satisfy EJPP, PDS, UNAM, and ND.\nProof. With the addition of UNAM, it is clear that c must be zero in (5), and thus f must have the form of a standard LinOP (2). From Example I, we know that the LinOP does not maintain independence even between just two events. The fact that the LinOP cannot satisfy both IPP and ND is proved formally by several authors [Genest, 1984c, Lehrer and Wagner, 1983, Wagner, 1984]. Their proofs are applicable to EIPP as well, since they hold even when 1111 = 4, in which case EIPP and IPP coincide. 0\nA careful examination of the proof of Lemma I also sug gests one more possibility when the full generality of IPP is relaxed. Suppose that all agents agree that all three events, At. A2, and A3, are completely independent. Then it can be shown that Pr;(AtA2A3) = z;/(1 + z; ) + y; and, fur thermore, that x; = y; for all i. I n this case, (7) holds only vacuously, since xi = x; and Yi = y;. Moreover, since x; and y; are no longer arbitrary, the proof does not go through. Thus, under this fully independent condition, the conclusion of Lemma I is no longer valid.\nThis insight leads us to characterize the inherent impossi bility more sharply, by dividing EIPP into two, weaker con ditions, NMEIPP and MEIPP, and showing that the former retains the impossibility while the latter does not.\nCorollary 1 No aggregation function f can simultane ously satisfy NME/PP, PDS, UNAM, and ND.\nProof. The proof of Lemma I still follows under NMEIPP, and thus so does the proof of Proposition I. 0\nSection 4 demonstrates that in fact, MEIPP is perfectly con sistent with PDS, UNAM, and ND in a nontrivial way. In deed, the stronger MIPP is consistent as well.\n3.2 Family Aggregation\nExample 3 (Family aggregation)\nConsider two agents, each with a BN consisting of two pri mary events, with At the parent of A2 and with beliefs as follows:\nPrt (At) = 0.2 Prt(A21At) = 0.4 Prt(A21At) = 0.6\nP r2(At) = 0.8 P rz(A2IAt) = 0.8 Pr2(A2IAt) = 0.3\nWe compute each consensus CPT as an average of the cor responding individual CPTs. That is, Pr0(At) = (.2 + .8)/2 = .5, Pr0(A21At) = (.4 + .8)/2 = .6, etc. This results in the following consensus joint distribution:\nPro(AtA2) = 0.3 Pro(AtA2) = 0.2 Pro(A1A2) = 0.225 Pro(AtA2) = 0.275.\nNext suppose that both agents reverse their edge between the two events, such that A2 is the parent of A1, but that their joint distributions remain unchanged. Now the agents' CPTs are:\nPrt(A2) = 0.56 Pr1(A1IA2) = 0.142857 Prl(A1IA2) = 0.272727\nPr2(A2) = 0.7 Pr2(AtiA2) = 0.914286 Pr2(A1IA2) = 0.533333\nGraphical Representations of Consensus Belief 537\nand if we average locally within each CPT, we get a differ ent consensus distribution:\nPro(AtA2) = 0.333 Pro(AtA2) = 0.149121 Pro(At A2) = 0.297 Pro(AtA2) = 0.220878.\nThus averaging only within each family of the BN violates the form of the opinion pool itself (I), which insists that the consensus joint distribution depend only on the underlying joint distributions of the agents involved. D\nWe now show that this inconsistency is not confined solely to the averaging aggregator.\nProposition 2 No aggregation function f can simultane ously satisfy FA, UNAM, and ND.\nProof (sketch). Let the first event in the consensus BN be Aj, the second Ah, ... , and the last Aj m. The FA property requires both of the following:\nPro(Aj,) f(Prt (Aj, ), Pr2(Aj,), ... , Prn (Ail)) (8)\nPro(Ajm\\Z- Ajm) f(Prt (Aim \\Z- Aim), . .. , Prn(Aim \\Z- Aim tV.l\nBy the definition of an opinion pool (I), the consensus belief depends only on the agents' underlying joint dis tributions, and not on the particular ordering of events in each BN. Thus, we must arrive at the same consensus dis tribution as long as {it, h, . .. , im} is some permutation of {1, 2, . . . , m}. Consider two permutations, one where }t = 1 and one where im = 1. Then (8) and (9) become:\nPro(At) = f(Prt(At), Pr2(A1), ... , Prn(At)) (10)\nPro(A1\\Z- A1) f(Prl(At\\Z- A1 ), ... , Prn(AJ\\Z- A!))(! I)\nDalkey [1975] proves that no function can simultaneously satisfy (I 0), (II), UNAM, and ND. Alternatively, the two equations essentially require that f satisfy both MP and EB, which Genest [ 1984b] shows are incompatible with UNAM and ND. D\n4 The LogOP and Consensus Markov Networks\nThe results in Section 3 suggest that insisting upon general event independence preservation has rather severe conse quences. In this section, we see that preserving Markov in dependencies is in fact compatible with PDS, UNAM, and ND. Let Aj be a primary event, and W � Z - A1 and\nX = Z - W - Aj be sets of events. Then Aj is Markov independent of X given W ifPr(Aj\\WX) = Pr(Aj\\W).\nProposition 3 The LogOP satisfies MIPP.\nProof. Since the LogOP is defined in terms of atomic states w, we make use of the following two identities:\np (A\\WX) _ Pr0(AWX) ro = Pr0(AWX)+Pro(AWX) _\nLxPro(AWX) Pro(A\\W) = Lx Pro(AW X)+ Lx Pro(AW X)\nwhere Lx represents a sum over all possible combinations of outcomes of events in the set X. Then we have that,\nPro (A I W X) = \"\"\"'r-:::.....,-\"�\":::----;=m-::::-=:-::::-:-\nIT\n= -\"---IT�[-P •-, (�A-W�)�P .�; l( WlXl)�]�\"'�\"��rr��p1, �(A�W� )�P�,1�(W� X-)l<•�, L....t x P r1(w) + � X P r1(W) _ Lxfl[Pr,(AWX)]\"• - Lx WPr,(AW X)]•• + Lx II[Pr,(AW X)]•• Pro(AWX) = Lx Pr0(AW X)+ x Pr0(AW X) = Pro(A\\W) D\nSuppose that each agent's belief is given as a MN, and we wish to generate a consensus MN structure that can en code the results of the LogOP. As discussed in Section 2.2, graph connectivity in a MN represents probabilistic depen dence, and the neighborhood relation represents direct in fluence. For each node Aj, the set of its neighbors plays the role of W in Proposition 3, and all other nodes consti tute the set X. The proposition ensures that, if all agents agree on a common MN structure, then the consensus dis tribution derived by the LogOP will respect the same struc ture. When agents are not in complete agreement on the structure, then the consensus can be represented as a MN defined by the union of all the individual MNs. In other words, there is an edge between Aj and Ak in the consen sus MN if and only if there is an edge between those two nodes in at least one of the agents' MNs.\nPearl [1988] gives axiomatic descriptions of both MNs and BNs. Only the former includes an axiom called strong union, which states that if Pr(Aj \\Ak) = Pr(Aj ), then Pr(Aj\\WAk) = Pr(A1\\W) for all W � Z. Notice that, if\n538 Pennock and Wellman\nthe precondition of the EIPP is met, and strong union holds for all agents, then the precondition of the MEIPP must also hold. This axiom is the key distinction that allows common MN structures to be maintained in the LogOP consensus, whereas common BN structures in general are not.\nGiven a collection of BNs, generating a consensus BN structure that is consistent with the LogOP is also rel atively straightforward. We first convert each BN into a MN by moralizing the graphs, or fully connect ing each node's parents and dropping edge directional ity [Lauritzen and Spiegelhalter, 1988, Neapolitan, 1990]. Next, we compute the union of the individual MNs, and finally we convert the resulting consensus MN back into a BN by filling in or triangulating the net work, reintroducing directionality according to the fill in order3 [Jensen, 1996, Lauritzen and Spiegelhalter, 1988, Neapolitan, 1990, P earl, 1988].\nWe have outlined how to derive consensus MN or BN structures; what of computing the associated probabilities? In Section 5, we give an algorithm for computing the prob abilities in a consensus BN that is polynomial in the size of its CPTs. Note that, even when all agents agree on a BN structure, the size of the final representation may grow exponentially during fill-in, and computing the union of the intermediate MNs when agents disagree will only ex acerbate this problem. Nevertheless, even a decomposable representation can be exponentially smaller than the full joint distribution, and the most popular algorithms for ex act Bayesian inference do operate on decomposable models in practice.\n5 Computing LogOP and LinOP\nSince the LinOP (2) and LogOP (3) are defined over atomic states, computing, for example, the consensus marginal probability of a single event involves in the worst case a summation over 2m -I terms. Moreover, even computing the LogOP consensus for a single state requires a normal ization factor that is itself a sum over all 2m states. I n this section, we see that if each agent's belief is represented as a BN, the LinOP and LogOP consensus for any probabilistic query can be computed more efficiently. In particular, for the LogOP, we can compute the CPTs of a consensus BN with time complexity 0( nm22q), where q is the maximum number of parents of any node in the consensus structure.\nWe focus first on the task of generating a LogOP-consistent consensus BN. We compute its structure as described in Section 4. Consider computing the CPT at Aj, that is, Pro(AJ lpa(AJ )) for all combinations of outcomes of\n3We do not claim that these consensus structures are correct or minimal in any sense, or even that LogOP is the preferred ag gregation method. Our goal is more to guide a modeler's decision process by delineating what representations are consistent under what circumstances.\nFigure 2: Two potential sections of a decomposable BN. Aj 's children can be either in the same clique or in separate cliques.\nevents in pa(AJ ). From P roposition 2, we know that sim ply combining each agent's assessment of this conditional probability will not succeed in general. However, we can compute the last CPT, Pr0(Am lpa(Am)), in terms of only the Pr;(Am lpa(Am)), by computing the LogOP over the single event Am: Pro(Amlpa(Am)) =\n[1;'::.1 [Pr;(Am lpa(Am))]\"' f1 [Pr;(Am lpa(Am))]\"• + f1 [Pr;(Am lpa(Am))J\"'\n. (12)\nBecause the LogOP satisfies EB, if we condition on all other events Z - Am in the network, then the LogOP over just Am will return the same result as if we had computed the LogOP over all events, and then con ditioned on Z - Am. Equation 12 also reflects the fact that Pro(Amlpa(Am)) = Pro(AmiZ- Am) and Pr;(Amlpa(Am)) = Pr;(AmiZ- Am). by the semantics of the BNs.\nWe can compute the remainder of the CPTs in reverse in dex order. Assume that the CPTs Pro ( Ak lpa( Ak)) have been calculated for all k > j, and that next we need to cal culate Pr0( Aj lpa( Aj)). To simplify the discussion, let Aj have exactly two children, Ak and A�o with j < k < l; the analysis generalizes easily to more children (or one child). Since the BN is decomposable, its topology is a tree of cliques [Chyu, 1991, Pearl, 1988, Shachter et al., 1991], and Ak and A1 can either be in the same clique or in sepa rate cliques, as depicted in Figure 2. Note that decompos ability also ensures that Aj's neighbors, A1 U Ak U pa(Aj ) , constitute its Markov blanket. We can query each of the agent's BNs for the probabilities Pr;(Aj IA1UAk Upa(AJ)) using a standard BN inference algorithm. From these, we can compute the corresponding consensus probability as a LogOP only over Aj, as before:\nPro(Aj IA1 U Ak U pa(Aj )) N\n<X IT [Pr;(Aj IA1 U Ak U pa(Aj ))]\"'. i=l\nWe now need only eliminate the conditioning on A1 and\nGraphical Representations of Consensus Belief 539\nAk. By Bayes's rule, we have that Pro(AiiAt U Ak U pa(Aj)) Pro(Aj I At U Ak U pa(Aj ))\nPro(At U AkiAi U pa(Aj)) . Pro(AiiPa(Aj)) Pro(At U AkiAi U pa(Aj)) Pro(AiiPa(Aj)) Pro(AtiAk U Aj U pa(Aj )) . Pro(Ak IAi U pa(Aj )) Pro(AtiAk U Aj U pa(Aj)) Pro(AkiAi Upa(Ai))\nPr0(Aj jpa(Aj )) Pro(Aj lpa(Aj )) ·\nBecause the BN is decomposable, and regardless of whether Ak and At are in the same or different cliques, Pro(AtiAk U Aj U pa(Aj)) = Pro(Atlpa(At)) and Pro(AkiAi U pa(Aj)) = Pro(Aklpa(Ak)), both of which have already been computed. Therefore we can calculate the CPT at Ai as follows:\nPro(Ai jpa(Ai)) Pro(Aj jpa(Ai ))\n= Pro(Aj I At U Ak U pa(Aj )) Pro(Aj I At U Ak U pa(Aj ))\nPro(Aklp-a(Ak)) Pro(Aklpa(Ak))'\nPro(AtiP'a(At)) Pro(Adpa(At))\n(13)\nwhere pa(Ak) and pa(At) contain Ai, and pa(Ak) and pa( At) contain Ai. Once we compute the likelihood ratio on the LHS of (13 ), the desired probabilities are uniquely determined, since Pr0(Ailpa(Aj)) + Pr0(Ajlpa(Aj)) = 1.\nA consensus BN consistent with the LinOP would in gen eral be fully connected, and thus not an object of particular value. However, if all agents' beliefs are given as BNs, we can retain their separation and still compute LinOP queries more efficiently. We exploit the fact that the LinOP obeys the marginalization property, and thus that the LinOP of any compound, marginal event can be computed as a LinOP over only that event. For example,\nn\ni=l\nwhere the terms on the RHS are calculated using a standard algorithm for Bayesian inference. Any conditional proba bility can be computed as the division of two compound, marginal probabilities.\nFinally, we characterize the computational complexity of LinOP when all input models are BNs. Clearly, comput ing an arbitrary query Pr0(EIF) is NP-hard. Proposition 4 establishes that, even when all topologies agree, and even when only computing the LinOP of a CPT entry, the prob lem remains intractable.\nProposition 4 Let all input BNs have identical topologies. Then computing Pr0(Ai jpa(Aj )) consistent with LinOP is NP-hard.\nProof. (sketch) Suppose that n = 2. Let Pr1 be an ar bitrary BN and let Pr2 have an identical topology, but en code a uniform distribution-that is, Pr2(w) = l/2m. We have shown that, if Pro(Amlpa(Am)) were computable in polynomial time, then Pr1 (Am) could be inferred in polynomial time. Computing the later query is NP-hard [Cooper, 1990], and so the former must be as well. D\n6 Conclusions\nGraphical representations of a single agent's subjective be lief form the core of many successful applications of un certain reasoning. We examine the problem of combining several graphical models, to form a consensus model. Two intuitively reasonable assumptions in this context, made a priori by other authors, are ( 1) if all agents agree on a sin gle topology, then that structure should be maintained, and (2) probability aggregation can be isolated within each con ditional probability table (CPT). We demonstrate that each of these properties leads to an impossibility theorem when combined with other reasonable, oft-invoked assumptions. We prove that the logarithmic opinion pool (LogOP) main tains all agreed-upon Markov independencies, and describe procedures for constructing consensus Markov networks and consensus Bayesian networks that are consistent with the LogOP. We provide an algorithm for computing the CPTs of a LogOP-consistent consensus BN that takes ad vantage of available structure.\nWe consider the main contribution of this work to be an ex tension of known results on aggregating joint distributions to the case of combining graphical models. The results en tail serious pitfalls for a modeler wishing to take into ac count the divergent opinions of multiple sources. Coherent combination of multiple models requires careful interpre tation of the models to be combined, and deliberate consid eration of the desired properties of the result.\nAcknowledgments\nThanks to Eric Horvitz and the anonymous referees for in sightful comments and pointers to related research. Part of this investigation was conducted while the first author was at Microsoft Research. Work at the University of Michigan was supported by AFOSR Grant F49620-97-0175.\nReferences\n[Bonduelle, 1987] Yann Bonduelle. Aggregating expert opinions by resolving sources of disagreement. PhD the sis, Stanford University, 1987.\n[Chyu, 1991] C. C. Chyu. Decomposable probabilistic in fluence diagrams. Probability in the Engineering and Informational Sciences, 5:229-243, 1991.\n540 Pennock and Wellman\n[Cooper, 1990] G. Cooper. The computational complexity of probabilistic inference using Bayes belief networks. Artificial Intelligence, 42:393-405, 1990.\n[Dalkey, 1975] Norman C. Dalkey. Toward a theory of group estimation. In H. A. Linstone and M. Turoff, edi tors, The Delphi Method: Techniques and Applications, pages 236-261. Addison-Wesley, Reading, MA, 1975.\n[Darroch et al., 1980] J. N. Darroch, S. L. Lauritzen, and T. P. Speed. Markov fields and log-linear interaction models for contingency tables. Annals of Statistics, 8(3):522-539, May 1980.\n[Genest and Wagner, 1987] Christian Genest and Carl G. Wagner. Further evidence against independence preser vation in expert judgement synthesis. Aequationes Mathematicae, 32(1):74-86, 1987.\n[Genest and Zidek, 1986] Christian Genest and James V. Zidek. Combining probability distributions: A cri tique and an annotated bibliography. Statistical Science, 1(1):114-148, 1986.\n[Genest et a/., 1986] Christian Genest, Kevin J. Me Conway, and Mark J. Schervish. Characterization of externally Bayesian pooling operators. Annals of Statis tics, 14(2):487-501, June 1986.\n[Genest, 1984a] Christian Genest. A characterization the orem for externally Bayesian groups. Annals of Statis tics, 12(3):1100-1105, September 1984.\n[Genest, 1984b] Christian Genest. A conflict between two axioms for combining subjective distributions. Journal of the Royal Statistical Society, 46(3):403-405, 1984.\n[Genest, 1984c] Christian Genest. Pooling operators with the marginalization property. Canadian Journal of Statistics, 12(2): 153-163, 1984.\n[Jacobs, 1995] Robert A. Jacobs. Methods for combining experts' probability assessments. Neural Computation, 7(5):867-888, September 1995.\n[Jensen, 1996] Finn V. Jensen. An Introduction to Bayes ian Networks. Springer, New York, 1996.\n[Kloks, 1994] Ton Kloks. Treewidth: Computations and Approximations. Springer-Verlag, Berlin, 1994.\n[Lauritzen and Spiegelhalter, 1988] Steffen L. Lauritzen and David J. Spiegelhalter. Local computations with probabilities on graphical structures and their applica tion to expert systems. Journal of the Royal Statistical Society, Series B, 50:157-224, 1988.\n[Lehrer and Wagner, 1983] Keith Lehrer and Carl G. Wag ner. Probability amalgamation and the independence is sue: A reply to Laddaga. Synthese. An International\nJournal for Epistemology, Methodology and Philosophy of Science, 55(3 ):339-346, 1983.\n[Lindley, 1985] Dennis V. Lindley. Reconciliation of dis crete probability distributions. In Bayesian Statistics 2, pages 375-390, Amsterdam, 1985. North Holland.\n[Madansky, 1964] A. Madansky. Externally Bayesian groups. RM-4141-PR, The RAND Corporation, Santa Monica, CA, 1964.\n[Matzkevich and Abramson, 1992] Izhar Matzkevich and Bruce Abramson. The topological fusion of Bayes nets. In Proceedings of the 8th Annual Conference on Uncer tainty in Artificial Intelligence, pages 152-158, 1992.\n[Matzkevich and Abramson, 1993] Izhar Matzkevich and Bruce Abramson. Some complexity considerations in the combination of belief networks. In Proceedings of the 9th Annual Conference on Uncertainty in Artificial Intelligence, pages 152-158, 1993.\n[Neapolitan, 1990] Richard E. Neapolitan. Probabilistic Reasoning in Expert Systems: Theory and Algorithms. John Wiley and Sons, New York, 1990.\n[Ng and Abramson, 1994] Keung-Chi Ng and Bruce Abramson. Probabilistic multi-knowledge-base systems. Applied Intelligence, 4(2):219-236, 1994.\n[Pearl, 1988] J. Pearl. Probabilistic Reasoning in Intelli gent Systems. Morgan Kaufmann, 1988.\n[Shachter et al., 1991] Ross D. Shachter, Stig K. Ander sen, and Kim L. Poh. Directed reduction algorithms and decomposable graphs. In Uncertainty in Artificial Intelligence, volume 6, pages 197-208. North Holland, Amsterdam, 1991.\n[Shachter, 1988] Ross D. Shachter. Probabilistic infer ence and influence diagrams. Operations Research, 36( 4 ):589-604, 1988.\n[Wagner, 1984] Carl Wagner. Aggregating subjective probabilities: Some limitative theorems. Notre Dame Journal of Formal Logic, 25(3):233-240, July 1984.\n[Whittaker, 1990] Joe Whittaker. Graphical Models in Ap plied Multivariate Statistics. Wiley, Chichester, Eng land; New York, 1990.\n[Xiang, 1996] Y. Xi an g. A probabilistic framework for co operative multi-agent distributed interpretation and op timization of communication. Artificial Intelligence, 87(1-2):295-342, November 1996.\n[Yule, 1903] G. Udny Yule. Notes on the theory of associ ation of attributes in statistics. Biometrika, 2:121-134, 1903."
    } ],
    "references" : [ {
      "title" : "PhD the­ sis",
      "author" : [ "Yann Bonduelle. Aggregating expert opinions by resolving sources of disagreement" ],
      "venue" : "Stanford University,",
      "citeRegEx" : "Bonduelle. 1987",
      "shortCiteRegEx" : null,
      "year" : 1987
    }, {
      "title" : "Decomposable probabilistic in­ fluence diagrams",
      "author" : [ "C.C. Chyu" ],
      "venue" : "Probability in the Engineering and Informational Sciences, 5:229-243",
      "citeRegEx" : "Chyu. 1991",
      "shortCiteRegEx" : null,
      "year" : 1991
    }, {
      "title" : "The computational complexity of probabilistic inference using Bayes belief networks",
      "author" : [ "G. Cooper" ],
      "venue" : "Artificial Intelligence, 42:393-405",
      "citeRegEx" : "Cooper. 1990",
      "shortCiteRegEx" : null,
      "year" : 1990
    }, {
      "title" : "edi­ tors",
      "author" : [ "Norman C. Dalkey. Toward a theory of group estimation. In H.A. Linstone", "M. Turoff" ],
      "venue" : "The Delphi Method: Techniques and Applications, pages 236-261. Addison-Wesley, Reading, MA,",
      "citeRegEx" : "Dalkey. 1975",
      "shortCiteRegEx" : null,
      "year" : 1975
    }, {
      "title" : "8(3):522-539",
      "author" : [ "J.N. Darroch", "S.L. Lauritzen", "T.P. Speed. Markov fields", "log-linear interaction models for contingency tables. Annals of Statistics" ],
      "venue" : "May",
      "citeRegEx" : "Darroch et al.. 1980",
      "shortCiteRegEx" : null,
      "year" : 1980
    }, {
      "title" : "Further evidence against independence preser­ vation in expert judgement synthesis",
      "author" : [ "Christian Genest", "Carl G. Wagner" ],
      "venue" : "Aequationes Mathematicae, 32(1):74-86,",
      "citeRegEx" : "Genest and Wagner. 1987",
      "shortCiteRegEx" : null,
      "year" : 1987
    }, {
      "title" : "Combining probability distributions: A cri­ tique and an annotated bibliography",
      "author" : [ "Christian Genest", "James V. Zidek" ],
      "venue" : "Statistical Science, 1(1):114-148,",
      "citeRegEx" : "Genest and Zidek. 1986",
      "shortCiteRegEx" : null,
      "year" : 1986
    }, {
      "title" : "Me­ Conway",
      "author" : [ "Christian Genest", "Kevin J" ],
      "venue" : "and Mark J. Schervish. Characterization of externally Bayesian pooling operators. Annals of Statis­ tics, 14(2):487-501, June",
      "citeRegEx" : "Genest et a... 1986",
      "shortCiteRegEx" : null,
      "year" : 1986
    }, {
      "title" : "A characterization the­ orem for externally Bayesian groups",
      "author" : [ "Christian Genest" ],
      "venue" : "Annals of Statis­ tics, 12(3):1100-1105, September",
      "citeRegEx" : "Genest. 1984a",
      "shortCiteRegEx" : null,
      "year" : 1984
    }, {
      "title" : "Journal of the Royal Statistical Society",
      "author" : [ "Christian Genest. A conflict between two axioms for combining subjective distributions" ],
      "venue" : "46(3):403-405,",
      "citeRegEx" : "Genest. 1984b",
      "shortCiteRegEx" : null,
      "year" : 1984
    }, {
      "title" : "Pooling operators with the marginalization property",
      "author" : [ "Genest", "1984c] Christian Genest" ],
      "venue" : "Canadian Journal of Statistics,",
      "citeRegEx" : "Genest and Genest.,? \\Q1984\\E",
      "shortCiteRegEx" : "Genest and Genest.",
      "year" : 1984
    }, {
      "title" : "7(5):867-888",
      "author" : [ "Robert A. Jacobs. Methods for combining experts' probability assessments. Neural Computation" ],
      "venue" : "September",
      "citeRegEx" : "Jacobs. 1995",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "An Introduction to Bayes­ ian Networks",
      "author" : [ "Finn V. Jensen" ],
      "venue" : "Springer, New York,",
      "citeRegEx" : "Jensen. 1996",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "Treewidth: Computations and Approximations",
      "author" : [ "Ton Kloks" ],
      "venue" : "Springer-Verlag, Berlin,",
      "citeRegEx" : "Kloks. 1994",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "Local computations with probabilities on graphical structures and their applica­ tion to expert systems",
      "author" : [ "Steffen L. Lauritzen", "David J. Spiegelhalter" ],
      "venue" : "Journal of the Royal Statistical Society, Series B, 50:157-224,",
      "citeRegEx" : "Lauritzen and Spiegelhalter. 1988",
      "shortCiteRegEx" : null,
      "year" : 1988
    }, {
      "title" : "Wag­ ner. Probability amalgamation and the independence is­ sue: A reply to Laddaga. Synthese",
      "author" : [ "Lehrer", "Wagner", "1983] Keith Lehrer", "Carl G" ],
      "venue" : null,
      "citeRegEx" : "Lehrer et al\\.,? \\Q1983\\E",
      "shortCiteRegEx" : "Lehrer et al\\.",
      "year" : 1983
    }, {
      "title" : "Reconciliation of dis­ crete probability distributions",
      "author" : [ "Dennis V. Lindley" ],
      "venue" : "Bayesian Statistics 2, pages 375-390, Amsterdam,",
      "citeRegEx" : "Lindley. 1985",
      "shortCiteRegEx" : null,
      "year" : 1985
    }, {
      "title" : "Externally Bayesian groups",
      "author" : [ "A. Madansky" ],
      "venue" : "RM-4141-PR, The RAND Corporation, Santa Monica, CA",
      "citeRegEx" : "Madansky. 1964",
      "shortCiteRegEx" : null,
      "year" : 1964
    }, {
      "title" : "In Proceedings of the 8th Annual Conference on Uncer­ tainty in Artificial Intelligence",
      "author" : [ "Izhar Matzkevich", "Bruce Abramson. The topological fusion of Bayes nets" ],
      "venue" : "pages 152-158,",
      "citeRegEx" : "Matzkevich and Abramson. 1992",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "In Proceedings of the 9th Annual Conference on Uncertainty in Artificial Intelligence",
      "author" : [ "Izhar Matzkevich", "Bruce Abramson. Some complexity considerations in the combination of belief networks" ],
      "venue" : "pages 152-158,",
      "citeRegEx" : "Matzkevich and Abramson. 1993",
      "shortCiteRegEx" : null,
      "year" : 1993
    }, {
      "title" : "Probabilistic Reasoning in Expert Systems: Theory and Algorithms",
      "author" : [ "Richard E. Neapolitan" ],
      "venue" : "John Wiley and Sons, New York,",
      "citeRegEx" : "Neapolitan. 1990",
      "shortCiteRegEx" : null,
      "year" : 1990
    }, {
      "title" : "Applied Intelligence",
      "author" : [ "Keung-Chi Ng", "Bruce Abramson. Probabilistic multi-knowledge-base systems" ],
      "venue" : "4(2):219-236,",
      "citeRegEx" : "Ng and Abramson. 1994",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "Probabilistic Reasoning in Intelli­ gent Systems",
      "author" : [ "J. Pearl" ],
      "venue" : "Morgan Kaufmann",
      "citeRegEx" : "Pearl. 1988",
      "shortCiteRegEx" : null,
      "year" : 1988
    }, {
      "title" : "Ander­ sen",
      "author" : [ "Ross D. Shachter", "Stig K" ],
      "venue" : "and Kim L. Poh. Directed reduction algorithms and decomposable graphs. In Uncertainty in Artificial Intelligence, volume 6, pages 197-208. North Holland, Amsterdam,",
      "citeRegEx" : "Shachter et al.. 1991",
      "shortCiteRegEx" : null,
      "year" : 1991
    }, {
      "title" : "Aggregating subjective probabilities: Some limitative theorems",
      "author" : [ "Carl Wagner" ],
      "venue" : "Notre Dame Journal of Formal Logic, 25(3):233-240, July",
      "citeRegEx" : "Wagner. 1984",
      "shortCiteRegEx" : null,
      "year" : 1984
    }, {
      "title" : "Graphical Models in Ap­ plied Multivariate Statistics",
      "author" : [ "Joe Whittaker" ],
      "venue" : "Wiley, Chichester, Eng­ land; New York,",
      "citeRegEx" : "Whittaker. 1990",
      "shortCiteRegEx" : null,
      "year" : 1990
    }, {
      "title" : "Notes on the theory of associ­ ation of attributes in statistics",
      "author" : [ "G. Udny Yule" ],
      "venue" : "[Yule,",
      "citeRegEx" : "Yule.,? \\Q1903\\E",
      "shortCiteRegEx" : "Yule.",
      "year" : 1903
    } ],
    "referenceMentions" : [ {
      "referenceID" : 16,
      "context" : "A third pooling method iden­ tifies one distinguished individual h (real or fictitious, within or outside the group) as a so-called supra Bayesian [Lindley, 1985].",
      "startOffset" : 147,
      "endOffset" : 162
    }, {
      "referenceID" : 6,
      "context" : "Implement­ ing it requires that we choose the supra Bayesian, or assess its prior belief if it is fictitious [Genest and Zidek, 1986].",
      "startOffset" : 109,
      "endOffset" : 133
    }, {
      "referenceID" : 8,
      "context" : "It has been shown that any f satisfy­ ing both MP and UNAM is a LinOP [Genest, 1984c], and any satisfying EB and UNAM is a LogOP [Genest, 1984a].",
      "startOffset" : 129,
      "endOffset" : 144
    }, {
      "referenceID" : 12,
      "context" : "The Bayesian network (BN) has proved invaluable as a language for compactly encoding a joint probability distri­ bution [Jensen, 1996].",
      "startOffset" : 120,
      "endOffset" : 134
    }, {
      "referenceID" : 22,
      "context" : "The neighbors of an event form a Markov blanket around it, \"shielding\" it from direct influence from the rest of the events [Pearl, 1988].",
      "startOffset" : 124,
      "endOffset" : 137
    }, {
      "referenceID" : 22,
      "context" : "The Markov blanket of a node in a BN consists of its direct parents, its direct children, and its children's direct parents [Pearl, 1988].",
      "startOffset" : 124,
      "endOffset" : 137
    }, {
      "referenceID" : 13,
      "context" : "A MN can be converted into a BN by filling in or triangulating [Kloks, 1994] the graph, and adding directionality according to the fill-in order­ ing [Jensen, 1996, Lauritzen and Spiegelhalter, 1988, Neapolitan, 1990, Pearl, 1988].",
      "startOffset" : 63,
      "endOffset" : 76
    }, {
      "referenceID" : 5,
      "context" : "This is not a numerical coincidence; in fact, indepen­ dence between only two events is always maintained by the LogOP [Genest and Wagner, 1987].",
      "startOffset" : 119,
      "endOffset" : 144
    }, {
      "referenceID" : 5,
      "context" : "2 in [Genest and Wagner, 1987], originally proved with respect to the IPP, is also applicable under the weaker EIPP.",
      "startOffset" : 5,
      "endOffset" : 30
    }, {
      "referenceID" : 5,
      "context" : "Lemma 1 (Adapted from [Genest and Wagner, 1987]) If f obeys EIPP and PDS, then there exist constants a1, az, .",
      "startOffset" : 22,
      "endOffset" : 47
    }, {
      "referenceID" : 2,
      "context" : "Computing the later query is NP-hard [Cooper, 1990], and so the former must be as well.",
      "startOffset" : 37,
      "endOffset" : 51
    } ],
    "year" : 2011,
    "abstractText" : "Graphical models based on conditional indepen­ dence support concise encodings of the subjec­ tive belief of a single agent. A natural ques­ tion is whether the consensus belief of a group of agents can be represented with equal parsimony. We prove, under relatively mild assumptions, that even if everyone agrees on a common graph topology, no method of combining beliefs can maintain that structure. Even weaker conditions rule out local aggregation within conditional probability tables. On a more positive note, we show that if probabilities are combined with the logarithmic opinion pool (LogOP), then com­ monly held Markov independencies are main­ tained. This suggests a straightforward proce­ dure for constructing a consensus Markov net­ work. We describe an algorithm for computing the LogOP with time complexity comparable to that of exact Bayesian inference.",
    "creator" : "pdftk 1.41 - www.pdftk.com"
  }
}