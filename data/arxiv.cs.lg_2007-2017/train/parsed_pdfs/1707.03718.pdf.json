{
  "name" : "1707.03718.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "LinkNet: Exploiting Encoder Representations for Efficient Semantic Segmentation",
    "authors" : [ "Abhishek Chaurasia", "Eugenio Culurciello" ],
    "emails" : [ "aabhish@purdue.edu", "euge@purdue.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "LinkNet: Exploiting Encoder Representations for Efficient Semantic Segmentation Abhishek Chaurasia\nSchool of Electrical and Computer Engineering Purdue University\nWest Lafayette, USA Email: aabhish@purdue.edu\nEugenio Culurciello Weldon School of Biomedical Engineering\nPurdue University West Lafayette, USA\nEmail: euge@purdue.edu\nAbstract—Pixel-wise semantic segmentation for visual scene understanding not only needs to be accurate, but also efficient in order to find any use in real-time application. Existing algorithms even though are accurate but they do not focus on utilizing the parameters of neural network efficiently. As a result they are huge in terms of parameters and number of operations; hence slow too. In this paper, we propose a novel deep neural network architecture which allows it to learn without any significant increase in number of parameters. Our network uses only 11.5 million parameters and 21.2 GFLOPs for processing an image of resolution 3 × 640 × 360. It gives state-of-the-art performance on CamVid and comparable results on Cityscapes dataset. We also compare our networks processing time on NVIDIA GPU and embedded system device with existing state-of-the-art architectures for different image resolutions.\nI. INTRODUCTION\nRecent advancement in machines with ability to perform computationally intensive tasks have enabled researchers to tap deeper into neural networks. Convolutional neural networks’ (CNNs) [1], [2] recent success has been demonstrated in image classification [3], [4], [5], [6], [7], [8], localization [9], [10], scene understanding [11], [12] etc. A lot of researchers have shifted their focus towards scene understanding because of the surge in tasks like augmented reality and self-driving vehicle, and one of the main step involved in it is pixel-level classification/semantic segmentation [13], [14].\nInspired by auto-encoders [3], [15], most of the existing techniques for semantic segmentation use encoder-decoder pair as core of their network architecture. Here the encoder encodes information into feature space, and the decoder maps this information into spatial categorization to perform segmentation. Even though semantic segmentation targets application that require real-time operation, ironically most of the current deep networks require excessively large processing time. Networks such as YOLO [16], Fast RCNN [17], SSD [18] focus on real-time object detection but there is very little to no work done in this direction in case of semantic segmentation [19].\nIn our work, we have made an attempt to get accurate instance level prediction without compromising processing time of the network. Generally, spatial information is lost in the encoder due to pooling or strided convolution is recovered by using the pooling indices or by full convolution. We hypothesize and later prove in our paper that instead of the above techniques;\nbypassing spatial information, directly from the encoder to the corresponding decoder improves accuracy along with significant decrease in processing time. In this way, information which would have been otherwise lost at each level of encoder is preserved, and no additional parameters and operations are wasted in relearning this lost information.\nIn Section III we give detailed explanation of our LinkNet architecture. The proposed network was tested on popular datasets: Cityscapes [20] and CamVid [21] and its processing times was recorded on NVIDIA Jetson TX1 Embedded Systems module as well as on Titan X GPU. These results are reported in Section IV, which is followed by conclusion."
    }, {
      "heading" : "II. RELATED WORK",
      "text" : "Semantic segmentation involves labeling each and every pixel of an image and therefore, retaining spatial information becomes utmost important. A neural network architecture used for scene parsing can be subdivided into encoder and decoder networks, which are basically discriminative and generative networks respectively. State-of-the-art segmentation networks, generally use categorization models which are mostly winners of ImageNet Large Scale Visual Recognition Challenge (ILSCRC) as their discriminator. The generator either uses the stored pooling indices from discriminator, or learns the parameters using convolution to perform upsampling. Moreover, encoder and decoder can be either symmetric (same number of layers in encoder and decoder with same number of pooling and unpooling layers), or they can be asymmetric.\nIn [22] a pre-trained VGG was used as discriminator. Pooling indices after every max-pooling step was saved and then later used for upsampling in the decoder. Later on researchers came up with the idea of deep deconvolution network [23], [24], fully convolutional network (FCN) combined with skip architecture [25], which eliminated the need of saving pooling indices. Networks designed for classification and categorization mostly use fully connected layer as their classifier; in FCN they get replaced with convolutional layers. Standard pre-trained encoders such as: AlexNet [4], VGG [5], and GoogLeNet [7] have been used for segmentation. In order to get precise segmentation boundaries, researchers have also tried to cascade their deep convolutional neural network (DCNN) with post-\nar X\niv :1\n70 7.\n03 71\n8v 1\n[ cs\n.C V\n] 1\n4 Ju\nn 20\n17\nprocessing steps, like the use of Conditional Random Field (CRF) [26], [11].\nInstead of using networks which were designed to perform image classification, [27] proposed to use networks specifically designed for dense predictions. Most of these networks failed to perform segmentation in real-time on existing embedded hardware. Apart from this, recently recurrent neural networks (RNNs) were used to get contextual information [28] and to optimize CRF [29]; but the use of RNN in itself makes it computationally very expensive. Some work was also done in designing efficient network [30], [19], where DCNN was optimized to get a faster forward processing time but with a decrease in prediction accuracy."
    }, {
      "heading" : "III. NETWORK ARCHITECTURE",
      "text" : "The architecture of LinkNet is presented in Figure 1. Here, conv means convolution and full-conv means full convolution [25]. Furthermore, /2 denotes downsampling by a factor of 2 which is achieved by performing strided convolution,\nand ∗2 means upsampling by a factor of 2. We use batch normalization between each convolutional layer and which is followed by ReLU non-linearity [31], [32]. Left half of the network shown in Figure 1 is the encoder while the one the right is the decoder. The encoder starts with an initial block which performs convolution on input image with a kernel of size 7×7 and a stride of 2. This block also performs spatial max-pooling in an area of 3 × 3 with a stride of 2. The later portion of encoder consists of residual blocks [6] and are represented as encoder-block(i). Layers within these encoder-blocks are shown in detail in Figure 2. Similarly, layer details for decoder-blocks are provided in Figure 3. Table I contains the information about the feature maps used in each of these blocks. Contemporary segmentation algorithms use networks such as VGG16 (138 million parameters), ResNet101 (45 million parameters) as their encoder which are huge in terms of parameters and GFLOPs. LinkNet uses ResNet18 as its encoder, which is fairly lighter network and still outperforms them as evident from Section\nIV. We use the technique of full-convolution in our decoder as proposed earlier by [25]. Every conv(k × k)(im, om) and full-conv(k × k)(im, om) operations has at least three parameters. Here, (k × k) represent (kernel − size) and (im, om) represent (inputmap, outputmap) respectively.\nUnlike existing neural network architectures which are being used for segmentation, our novelty lies in the way we link each encoder with decoder. By performing multiple downsampling operations in the encoder, some spatial information is lost. It is difficult to recover this lost information by using only the downsampled output of encoder. [22] linked encoder with decoder through pooling indices, which are not trainable parameters. Other methods directly use the output of their encoder and feed it into the decoder to perform segmentation. In this paper, input of each encoder layer is also bypassed to the output of its corresponding decoder. By doing this we aim at recovering lost spatial information that can be used by the decoder and its upsampling operations. In addition, since the decoder is sharing knowledge learnt by the encoder at every layer, the decoder can use fewer parameters. This results in an overall more efficient network when compared to the existing state-of-the-art segmentation networks, and thus real-time operation. Information about trainable parameters and number operations required for each forward pass is provided in detail in Section IV."
    }, {
      "heading" : "IV. RESULTS",
      "text" : "We compare LinkNet with existing architectures on two different metrics:\n1) Performance in terms of speed: • Number of operations required to perform one\nforward pass of the network • Time taken to perform one forward pass\n2) Performace interms of accuracy on Cityscapes [20] and CamVid [21] datasets."
    }, {
      "heading" : "A. Performance Analysis",
      "text" : "We report inference speed of LinkNet on NVIDIA TX1 embedded system module as well as on widely used NVIDIA TitanX. Table II compares inference time for a single input\nframe with varying resolution. As evident from the numbers provided, LinkNet can process very high resolution image at 8.5 fps on GPU. More importantly, it can give real-time performance even on NVIDIA TX1. ’-’ indicates that network was not able to process image at that resolution on the embedded device.\nWe choose 640× 360 as our default image resolution and report number of operations required to process image of this resolution in Table III. Number of operations determine the forward pass time of any network, therefore reduction in it is more vital than reduction in number of parameters. Our approach’s efficiency is evident in the much low number of operations per frame and overall parameters."
    }, {
      "heading" : "B. Benchmarks",
      "text" : "We use Torch7 [33] machine-learning tool for training with RMSProp as the optimization algorithm. The network was trained using four NVIDIA TitanX. Since the classes present in all the datsets are highly imbalanced; we use a custom class weighing scheme defined as wclass = 1ln(1.02+pclass) . This class weighing scheme has been taken from [19] and it gave us better results than mean average frequency. As suggested in Cityscapes [20], we use intersections over union (IoU) and instance-level intersection over union (iIoU) as our performance metric instead of using pixel-wise accuracy. In order to prove that the bypass connections do help, each table contains IoU and iIoU values with as well as without bypass. We also compare LinkNet’s performance with other standard models such as SegNet [24], ENet [19], Dilation8/10 [34], and Deep-Lab CRF [35]. a) Cityscapes: This dataset consists of 5000 fineannotated images, out of which 2975 are available for training, 500 for validation, and the remaining 1525 have been selected as test set [20]. We trained on our network on 19 classes that was provided in the official evaluation scripts [20]. As reported in Table IV, our network outperforms existing models.\nThese performance values were calculated on validation dataset. Input image of resolution 1024 × 512 was used for training the network. A batch size of 10 and initial learning rate of 5e−4 was found to give optimum performance. Figure 4 shows the predicted segmented output on couple of cityscapes test images. b) CamVid: It is another automotive dataset which contains 367 training, 101 validation, and 233 testing images [21]. There are eleven different classes such as building, tree, sky, car, road, etc. while the twelfth class contains unlabeled data, which we ignore during training. The original frame resolution for this dataset is 960× 720 (W,H) but we downsampled the images by a factor of 1.25 before training. Due to hardware constraint, batch size of 8 was used to train the network. In Table V we compare the performance of the proposed algorithm with existing state-of-the-art algorithms on test set. LinkNet outperforms all of them in both IoU and iIoU metrics. Segmented output of LinkNet can be seen in Figure 5"
    }, {
      "heading" : "V. CONCLUSION",
      "text" : "We have proposed a novel neural network architecture designed from the ground up specifically for semantic segmentation. Our main aim is to make efficient use of scarce resources available on embedded platforms, compared to fully fledged deep learning workstations. Our work provides large gains in this task, while matching and at times exceeding existing baseline models, that have an order of magnitude larger computational and memory requirements. The application of proposed network on the NVIDIA TX1 hardware exemplifies real-time portable embedded solutions.\nEven though the main goal was to run the network on mobile devices, we have found that it is also very efficient on high end GPUs like NVIDIA Titan X. This may prove useful in data-center applications, where there is a need of processing large numbers of high resolution images. Our network allows to perform large-scale computations in a much faster and more efficient manner, which might lead to significant savings."
    }, {
      "heading" : "ACKNOWLEDGMENT",
      "text" : "This work was partly supported by the Office of Naval Research (ONR) grants N00014-12-1-0167, N00014-15-1-2791 and MURI N00014-10-1-0278. We gratefully acknowledge the support of NVIDIA Corporation with the donation of the TX1, Titan X, K40 GPUs used for this research."
    } ],
    "references" : [ {
      "title" : "Convolutional networks for images, speech, and time series,",
      "author" : [ "Y. LeCun", "Y. Bengio" ],
      "venue" : "The handbook of brain theory and neural networks,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1998
    }, {
      "title" : "Unsupervised learning of invariant feature hierarchies with applications to object recognition,",
      "author" : [ "M.A. Ranzato", "F.J. Huang", "Y.-L. Boureau", "Y. LeCun" ],
      "venue" : "Computer Vision and Pattern Recognition,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2007
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks,",
      "author" : [ "A. Krizhevsky", "I. Sutskever", "G.E. Hinton" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2012
    }, {
      "title" : "Very deep convolutional networks for large-scale image recognition,",
      "author" : [ "K. Simonyan", "A. Zisserman" ],
      "venue" : "arXiv preprint arXiv:1409.1556,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2014
    }, {
      "title" : "Deep residual learning for image recognition,",
      "author" : [ "K. He", "X. Zhang", "S. Ren", "J. Sun" ],
      "venue" : "arXiv preprint arXiv:1512.03385,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2015
    }, {
      "title" : "Going deeper with convolutions,",
      "author" : [ "C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich" ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2015
    }, {
      "title" : "Inception-v4, inception-resnet and the impact of residual connections on learning,",
      "author" : [ "C. Szegedy", "S. Ioffe", "V. Vanhoucke" ],
      "venue" : "arXiv preprint arXiv:1602.07261,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2016
    }, {
      "title" : "Overfeat: Integrated recognition, localization and detection using convolutional networks,",
      "author" : [ "P. Sermanet", "D. Eigen", "X. Zhang", "M. Mathieu", "R. Fergus", "Y. Le- Cun" ],
      "venue" : "arXiv preprint arXiv:1312.6229,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2013
    }, {
      "title" : "Efficient object localization using convolutional networks,",
      "author" : [ "J. Tompson", "R. Goroshin", "A. Jain", "Y. LeCun", "C. Bregler" ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2015
    }, {
      "title" : "Combining appearance and structure from motion features for road scene understanding,",
      "author" : [ "P. Sturgess", "K. Alahari", "L. Ladicky", "P.H. Torr" ],
      "venue" : "BMVC 2012-23rd British Machine Vision Conference,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2009
    }, {
      "title" : "Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture,",
      "author" : [ "D. Eigen", "R. Fergus" ],
      "venue" : "Proceedings of the IEEE International Conference on Computer Vision,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2015
    }, {
      "title" : "Rgb-(d) scene labeling: Features and algorithms,",
      "author" : [ "X. Ren", "L. Bo", "D. Fox" ],
      "venue" : "Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2012
    }, {
      "title" : "Learning hierarchical features for scene labeling,",
      "author" : [ "C. Farabet", "C. Couprie", "L. Najman", "Y. LeCun" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2013
    }, {
      "title" : "Multimodal deep learning,",
      "author" : [ "J. Ngiam", "A. Khosla", "M. Kim", "J. Nam", "H. Lee", "A.Y. Ng" ],
      "venue" : "Proceedings of the 28th international conference on machine learning (ICML-11),",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2011
    }, {
      "title" : "You only look once: Unified, real-time object detection,",
      "author" : [ "J. Redmon", "S. Divvala", "R. Girshick", "A. Farhadi" ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2016
    }, {
      "title" : "Faster r-cnn: Towards real-time object detection with region proposal networks,” in Advances in neural information processing",
      "author" : [ "S. Ren", "K. He", "R. Girshick", "J. Sun" ],
      "venue" : null,
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2015
    }, {
      "title" : "Ssd: Single shot multibox detector,",
      "author" : [ "W. Liu", "D. Anguelov", "D. Erhan", "C. Szegedy", "S. Reed", "C.-Y. Fu", "A.C. Berg" ],
      "venue" : "European Conference on Computer Vision. Springer,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2016
    }, {
      "title" : "Enet: A deep neural network architecture for real-time semantic segmentation,",
      "author" : [ "A. Paszke", "A. Chaurasia", "S. Kim", "E. Culurciello" ],
      "venue" : "arXiv preprint arXiv:1606.02147,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2016
    }, {
      "title" : "The cityscapes dataset for semantic urban scene understanding,",
      "author" : [ "M. Cordts", "M. Omran", "S. Ramos", "T. Rehfeld", "M. Enzweiler", "R. Benenson", "U. Franke", "S. Roth", "B. Schiele" ],
      "venue" : "in Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2016
    }, {
      "title" : "Segmentation and recognition using structure from motion point clouds,",
      "author" : [ "G.J. Brostow", "J. Shotton", "J. Fauqueur", "R. Cipolla" ],
      "venue" : null,
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2008
    }, {
      "title" : "Segnet: A deep convolutional encoder-decoder architecture for robust semantic pixel-wise labelling,",
      "author" : [ "V. Badrinarayanan", "A. Handa", "R. Cipolla" ],
      "venue" : "arXiv preprint arXiv:1505.07293,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2015
    }, {
      "title" : "Learning deconvolution network for semantic segmentation,",
      "author" : [ "H. Noh", "S. Hong", "B. Han" ],
      "venue" : "Proceedings of the IEEE International Conference on Computer Vision,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2015
    }, {
      "title" : "Segnet: A deep convolutional encoder-decoder architecture for image segmentation,",
      "author" : [ "V. Badrinarayanan", "A. Kendall", "R. Cipolla" ],
      "venue" : "arXiv preprint arXiv:1511.00561,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2015
    }, {
      "title" : "Fully convolutional networks for semantic segmentation,",
      "author" : [ "J. Long", "E. Shelhamer", "T. Darrell" ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2015
    }, {
      "title" : "Semantic image segmentation with deep convolutional nets and fully connected crfs,",
      "author" : [ "L.-C. Chen", "G. Papandreou", "I. Kokkinos", "K. Murphy", "A.L. Yuille" ],
      "venue" : "arXiv preprint arXiv:1412.7062,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2014
    }, {
      "title" : "Multi-scale context aggregation by dilated convolutions,",
      "author" : [ "F. Yu", "V. Koltun" ],
      "venue" : "arXiv preprint arXiv:1511.07122,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2015
    }, {
      "title" : "Reseg: A recurrent neural networkbased model for semantic segmentation,",
      "author" : [ "F. Visin", "M. Ciccone", "A. Romero", "K. Kastner", "K. Cho", "Y. Bengio", "M. Matteucci", "A. Courville" ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2016
    }, {
      "title" : "Conditional random fields as recurrent neural networks,",
      "author" : [ "S. Zheng", "S. Jayasumana", "B. Romera-Paredes", "V. Vineet", "Z. Su", "D. Du", "C. Huang", "P.H. Torr" ],
      "venue" : "Proceedings of the IEEE International Conference on Computer Vision,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2015
    }, {
      "title" : "Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding,",
      "author" : [ "S. Han", "H. Mao", "W.J. Dally" ],
      "venue" : "arXiv preprint arXiv:1510.00149,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2015
    }, {
      "title" : "Rectified linear units improve restricted boltzmann machines,",
      "author" : [ "V. Nair", "G.E. Hinton" ],
      "venue" : "Proceedings of the 27th international conference on machine learning (ICML-10),",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2010
    }, {
      "title" : "Batch normalization: Accelerating deep network training by reducing internal covariate shift,",
      "author" : [ "S. Ioffe", "C. Szegedy" ],
      "venue" : "arXiv preprint arXiv:1502.03167,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2015
    }, {
      "title" : "Multi-scale context aggregation by dilated convolutions,",
      "author" : [ "F. Yu", "V. Koltun" ],
      "venue" : "arXiv preprint arXiv:1511.07122,",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2015
    }, {
      "title" : "Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs,",
      "author" : [ "L.-C. Chen", "G. Papandreou", "I. Kokkinos", "K. Murphy", "A.L. Yuille" ],
      "venue" : "arXiv preprint arXiv:1606.00915,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Convolutional neural networks’ (CNNs) [1], [2] recent success has been demonstrated in image classification [3], [4], [5], [6], [7], [8], localization [9], [10], scene understanding [11], [12] etc.",
      "startOffset" : 38,
      "endOffset" : 41
    }, {
      "referenceID" : 1,
      "context" : "Convolutional neural networks’ (CNNs) [1], [2] recent success has been demonstrated in image classification [3], [4], [5], [6], [7], [8], localization [9], [10], scene understanding [11], [12] etc.",
      "startOffset" : 108,
      "endOffset" : 111
    }, {
      "referenceID" : 2,
      "context" : "Convolutional neural networks’ (CNNs) [1], [2] recent success has been demonstrated in image classification [3], [4], [5], [6], [7], [8], localization [9], [10], scene understanding [11], [12] etc.",
      "startOffset" : 113,
      "endOffset" : 116
    }, {
      "referenceID" : 3,
      "context" : "Convolutional neural networks’ (CNNs) [1], [2] recent success has been demonstrated in image classification [3], [4], [5], [6], [7], [8], localization [9], [10], scene understanding [11], [12] etc.",
      "startOffset" : 118,
      "endOffset" : 121
    }, {
      "referenceID" : 4,
      "context" : "Convolutional neural networks’ (CNNs) [1], [2] recent success has been demonstrated in image classification [3], [4], [5], [6], [7], [8], localization [9], [10], scene understanding [11], [12] etc.",
      "startOffset" : 123,
      "endOffset" : 126
    }, {
      "referenceID" : 5,
      "context" : "Convolutional neural networks’ (CNNs) [1], [2] recent success has been demonstrated in image classification [3], [4], [5], [6], [7], [8], localization [9], [10], scene understanding [11], [12] etc.",
      "startOffset" : 128,
      "endOffset" : 131
    }, {
      "referenceID" : 6,
      "context" : "Convolutional neural networks’ (CNNs) [1], [2] recent success has been demonstrated in image classification [3], [4], [5], [6], [7], [8], localization [9], [10], scene understanding [11], [12] etc.",
      "startOffset" : 133,
      "endOffset" : 136
    }, {
      "referenceID" : 7,
      "context" : "Convolutional neural networks’ (CNNs) [1], [2] recent success has been demonstrated in image classification [3], [4], [5], [6], [7], [8], localization [9], [10], scene understanding [11], [12] etc.",
      "startOffset" : 151,
      "endOffset" : 154
    }, {
      "referenceID" : 8,
      "context" : "Convolutional neural networks’ (CNNs) [1], [2] recent success has been demonstrated in image classification [3], [4], [5], [6], [7], [8], localization [9], [10], scene understanding [11], [12] etc.",
      "startOffset" : 156,
      "endOffset" : 160
    }, {
      "referenceID" : 9,
      "context" : "Convolutional neural networks’ (CNNs) [1], [2] recent success has been demonstrated in image classification [3], [4], [5], [6], [7], [8], localization [9], [10], scene understanding [11], [12] etc.",
      "startOffset" : 182,
      "endOffset" : 186
    }, {
      "referenceID" : 10,
      "context" : "Convolutional neural networks’ (CNNs) [1], [2] recent success has been demonstrated in image classification [3], [4], [5], [6], [7], [8], localization [9], [10], scene understanding [11], [12] etc.",
      "startOffset" : 188,
      "endOffset" : 192
    }, {
      "referenceID" : 11,
      "context" : "shifted their focus towards scene understanding because of the surge in tasks like augmented reality and self-driving vehicle, and one of the main step involved in it is pixel-level classification/semantic segmentation [13], [14].",
      "startOffset" : 219,
      "endOffset" : 223
    }, {
      "referenceID" : 12,
      "context" : "shifted their focus towards scene understanding because of the surge in tasks like augmented reality and self-driving vehicle, and one of the main step involved in it is pixel-level classification/semantic segmentation [13], [14].",
      "startOffset" : 225,
      "endOffset" : 229
    }, {
      "referenceID" : 1,
      "context" : "Inspired by auto-encoders [3], [15], most of the existing techniques for semantic segmentation use encoder-decoder pair as core of their network architecture.",
      "startOffset" : 26,
      "endOffset" : 29
    }, {
      "referenceID" : 13,
      "context" : "Inspired by auto-encoders [3], [15], most of the existing techniques for semantic segmentation use encoder-decoder pair as core of their network architecture.",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 14,
      "context" : "Networks such as YOLO [16], Fast RCNN [17], SSD [18] focus on real-time object detection but there is very little to no work done in this direction in case of semantic segmentation [19].",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 15,
      "context" : "Networks such as YOLO [16], Fast RCNN [17], SSD [18] focus on real-time object detection but there is very little to no work done in this direction in case of semantic segmentation [19].",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 16,
      "context" : "Networks such as YOLO [16], Fast RCNN [17], SSD [18] focus on real-time object detection but there is very little to no work done in this direction in case of semantic segmentation [19].",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 17,
      "context" : "Networks such as YOLO [16], Fast RCNN [17], SSD [18] focus on real-time object detection but there is very little to no work done in this direction in case of semantic segmentation [19].",
      "startOffset" : 181,
      "endOffset" : 185
    }, {
      "referenceID" : 18,
      "context" : "The proposed network was tested on popular datasets: Cityscapes [20] and CamVid [21] and its processing times was recorded on NVIDIA Jetson TX1 Embedded Systems module as well as on Titan X GPU.",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 19,
      "context" : "The proposed network was tested on popular datasets: Cityscapes [20] and CamVid [21] and its processing times was recorded on NVIDIA Jetson TX1 Embedded Systems module as well as on Titan X GPU.",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 20,
      "context" : "In [22] a pre-trained VGG was used as discriminator.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 21,
      "context" : "Later on researchers came up with the idea of deep deconvolution network [23], [24], fully convolutional network (FCN) combined with skip architecture [25], which eliminated the need of saving pooling indices.",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 22,
      "context" : "Later on researchers came up with the idea of deep deconvolution network [23], [24], fully convolutional network (FCN) combined with skip architecture [25], which eliminated the need of saving pooling indices.",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 23,
      "context" : "Later on researchers came up with the idea of deep deconvolution network [23], [24], fully convolutional network (FCN) combined with skip architecture [25], which eliminated the need of saving pooling indices.",
      "startOffset" : 151,
      "endOffset" : 155
    }, {
      "referenceID" : 2,
      "context" : "Standard pre-trained encoders such as: AlexNet [4], VGG [5], and GoogLeNet [7] have been used for segmentation.",
      "startOffset" : 47,
      "endOffset" : 50
    }, {
      "referenceID" : 3,
      "context" : "Standard pre-trained encoders such as: AlexNet [4], VGG [5], and GoogLeNet [7] have been used for segmentation.",
      "startOffset" : 56,
      "endOffset" : 59
    }, {
      "referenceID" : 5,
      "context" : "Standard pre-trained encoders such as: AlexNet [4], VGG [5], and GoogLeNet [7] have been used for segmentation.",
      "startOffset" : 75,
      "endOffset" : 78
    }, {
      "referenceID" : 24,
      "context" : "processing steps, like the use of Conditional Random Field (CRF) [26], [11].",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 9,
      "context" : "processing steps, like the use of Conditional Random Field (CRF) [26], [11].",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 25,
      "context" : "Instead of using networks which were designed to perform image classification, [27] proposed to use networks specifically designed for dense predictions.",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 26,
      "context" : "Apart from this, recently recurrent neural networks (RNNs) were used to get contextual information [28] and to optimize CRF [29]; but the use of RNN in itself makes it computationally very expensive.",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 27,
      "context" : "Apart from this, recently recurrent neural networks (RNNs) were used to get contextual information [28] and to optimize CRF [29]; but the use of RNN in itself makes it computationally very expensive.",
      "startOffset" : 124,
      "endOffset" : 128
    }, {
      "referenceID" : 28,
      "context" : "Some work was also done in designing efficient network [30], [19], where DCNN was optimized to get a faster forward processing time but with a decrease in prediction accuracy.",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 17,
      "context" : "Some work was also done in designing efficient network [30], [19], where DCNN was optimized to get a faster forward processing time but with a decrease in prediction accuracy.",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 23,
      "context" : "Here, conv means convolution and full-conv means full convolution [25].",
      "startOffset" : 66,
      "endOffset" : 70
    }, {
      "referenceID" : 29,
      "context" : "We use batch normalization between each convolutional layer and which is followed by ReLU non-linearity [31], [32].",
      "startOffset" : 104,
      "endOffset" : 108
    }, {
      "referenceID" : 30,
      "context" : "We use batch normalization between each convolutional layer and which is followed by ReLU non-linearity [31], [32].",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 4,
      "context" : "The later portion of encoder consists of residual blocks [6] and are represented as encoder-block(i).",
      "startOffset" : 57,
      "endOffset" : 60
    }, {
      "referenceID" : 23,
      "context" : "We use the technique of full-convolution in our decoder as proposed earlier by [25].",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 20,
      "context" : "[22] linked encoder with decoder through pooling indices, which are not trainable parameters.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 18,
      "context" : "2) Performace interms of accuracy on Cityscapes [20] and CamVid [21] datasets.",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 19,
      "context" : "2) Performace interms of accuracy on Cityscapes [20] and CamVid [21] datasets.",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 17,
      "context" : "This class weighing scheme has been taken from [19] and it gave us better results than mean average frequency.",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 18,
      "context" : "As suggested in Cityscapes [20], we use intersections over union (IoU) and instance-level intersection over union (iIoU) as our performance metric instead of using pixel-wise accuracy.",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 22,
      "context" : "We also compare LinkNet’s performance with other standard models such as SegNet [24], ENet [19], Dilation8/10 [34], and Deep-Lab CRF [35].",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 17,
      "context" : "We also compare LinkNet’s performance with other standard models such as SegNet [24], ENet [19], Dilation8/10 [34], and Deep-Lab CRF [35].",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 31,
      "context" : "We also compare LinkNet’s performance with other standard models such as SegNet [24], ENet [19], Dilation8/10 [34], and Deep-Lab CRF [35].",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 32,
      "context" : "We also compare LinkNet’s performance with other standard models such as SegNet [24], ENet [19], Dilation8/10 [34], and Deep-Lab CRF [35].",
      "startOffset" : 133,
      "endOffset" : 137
    }, {
      "referenceID" : 18,
      "context" : "as test set [20].",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 18,
      "context" : "We trained on our network on 19 classes that was provided in the official evaluation scripts [20].",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 18,
      "context" : "4: LinkNet prediction on Cityscapes [20] test set.",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 19,
      "context" : "5: LinkNet prediction on CamVid [21] test set.",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 19,
      "context" : "b) CamVid: It is another automotive dataset which contains 367 training, 101 validation, and 233 testing images [21].",
      "startOffset" : 112,
      "endOffset" : 116
    } ],
    "year" : 2017,
    "abstractText" : "Pixel-wise semantic segmentation for visual scene understanding not only needs to be accurate, but also efficient in order to find any use in real-time application. Existing algorithms even though are accurate but they do not focus on utilizing the parameters of neural network efficiently. As a result they are huge in terms of parameters and number of operations; hence slow too. In this paper, we propose a novel deep neural network architecture which allows it to learn without any significant increase in number of parameters. Our network uses only 11.5 million parameters and 21.2 GFLOPs for processing an image of resolution 3 × 640 × 360. It gives state-of-the-art performance on CamVid and comparable results on Cityscapes dataset. We also compare our networks processing time on NVIDIA GPU and embedded system device with existing state-of-the-art architectures for different image resolutions.",
    "creator" : "LaTeX with hyperref package"
  }
}