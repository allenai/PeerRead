{
  "name" : "1709.06033.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Sequence to Sequence Learning for Event Prediction",
    "authors" : [ "Dai Quoc Nguyen", "Dat Quoc Nguyen", "Cuong Xuan Chu", "Stefan Thater", "Manfred Pinkal" ],
    "emails" : [ "pinkal}@coli.uni-saarland.de", "dat.nguyen@mq.edu.au", "cxchu@mpi-inf.mpg.de" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "We consider a task of event prediction which aims to generate sentences describing a predicted event from the preceding sentence in a text. The following example presents an instruction in terms of a sequence of contiguous event descriptions for the activity of baking a cake:\nGather ingredients. Turn on oven. Combine ingredients into a bowl. Pour batter in pan. Put pan in oven. Bake for specified time.\nThe task is to predict event description “Put pan in oven” from sentence “Pour batter in pan”, or how to generate the continuation of the story, i.e., the event following “Bake for specified time”, which might be “Remove pan from oven”. Event prediction models an important facet of semantic expectation, and thus will contribute to text understanding as well as text generation. We propose to em-\nploy sequence-to-sequence learning (SEQ2SEQ) for this task.\nSEQ2SEQ have received significant research attention, especially in machine translation (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015), and in other NLP tasks such as parsing (Vinyals et al., 2015; Dong and Lapata, 2016), text summarization (Nallapati et al., 2016) and multi-task learning (Luong et al., 2016). In general, SEQ2SEQ uses an encoder which typically is a recurrent neural network (RNN) (Elman, 1990) to encode a source sequence, and then uses another RNN which we call decoder to decode a target sequence. The goal of SEQ2SEQ is to estimate the conditional probability of generating the target sequence given the encoding of the source sequence. These characteristics of SEQ2SEQ allow us to approach the event prediction task. SEQ2SEQ has been applied to text prediction by Kiros et al. (2015) and Pichotta and Mooney (2016). We also use SEQ2SEQ for prediction of what comes next in a text. However, there are several key differences.\n• We collect a new dataset based on the largest available resource of instructional texts, i.e., WIKIHOW1, consisting of pairs of adjacent sentences, which typically describe contiguous members of an event chain characterizing a complex activity. We also present another dataset based on the DESCRIPT corpus—a crowdsourced corpus of event sequence descriptions (Wanzare et al., 2016). While the WIKIHOW-based dataset helps to evaluate the models in an open-domain setting, the DESCRIPT-based dataset is used to evaluate the models in a closed-domain setting.\n1www.wikihow.com\nar X\niv :1\n70 9.\n06 03\n3v 1\n[ cs\n.C L\n] 1\n8 Se\np 20\n17\n• Pichotta and Mooney (2016) use the BLEU score (Papineni et al., 2002) for evaluation (i.e., the standard evaluation metric used in machine translation), which measures surface similarity between predicted and actual sentences. We complement this evaluation by measuring prediction accuracy on the semantic level. To this purpose, we use the gold paraphrase sets of event descriptions in the DESCRIPT corpus, e.g., “Remove cake”, “Remove from oven” and “Take the cake out of oven” belong to the same gold paraphrase set of taking out oven. The gold paraphrase sets allow us to access the correctness of the prediction which could not be attained by using the BLEU measure.\n• We explore multi-layer RNNs which have currently shown the advantage over single/shallow RNNs (Sutskever et al., 2014; Vinyals et al., 2015; Luong et al., 2015). We use a bidirectional RNN architecture for the encoder and examine the RNN decoder with or without attention mechanism. We achieve better results than previous work in terms of BLEU score."
    }, {
      "heading" : "2 Sequence to Sequence Learning",
      "text" : "Given a source sequence x1, x2, ..., xm and a target sequence y1, y2, ..., yn, sequence to sequence learning (SEQ2SEQ) is to estimate the conditional probability Pr(y1, y2, ..., yn | x1, x2, ..., xm) (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015; Vinyals et al., 2015; Luong et al., 2016). Typically, SEQ2SEQ consists of a RNN encoder and a RNN decoder. The RNN encoder maps the source sequence into a vector representation c which is then fed as input to the decoder for generating the target sequence.\nWe use a bidirectional RNN (BiRNN) architecture (Schuster and Paliwal, 1997) for mapping the source sequence x1, x2, ..., xm into the list of encoder states se1, s e 2, ..., s e m.\nThe RNN decoder is able to work with or without attention mechanism. When not using attention mechanism (Sutskever et al., 2014; Cho et al., 2014), the vector representation c is the last state sem of the encoder, which is used to initialize the decoder. Then, at the timestep i (1 ≤ i ≤ n), the RNN decoder takes into account the hidden state sdi−1 and the previous input yi−1 to output the hidden state sdi and generate the target yi.\nAttention mechanism allows the decoder to attend to different parts of the source sequence at one position of a timestep of generating the target sequence (Bahdanau et al., 2015; Luong et al., 2015; Vinyals et al., 2015). We adapt the attention mechanism proposed by Vinyals et al. (2015) to employ a concatenation of the hidden state sdi and the vector representation c to make predictions at the timestep i.\nWe use two advanced variants of RNNs that replace the cells of RNNs with the Long Sort Term Memory (LSTM) cells (Hochreiter and Schmidhuber, 1997) and the Gated Recurrent Unit (GRU) cells (Cho et al., 2014). We also use a deeper architecture of multi-layers, to model complex interactions in the context. This is different from Kiros et al. (2015) and Pichotta and Mooney (2016) where they only use a single layer. So we in fact experiment with Bidirectional-LSTM multi-layer RNN (BiLSTM) and Bidirectional-GRU multilayer RNN (BiGRU)."
    }, {
      "heading" : "3 Experiments",
      "text" : ""
    }, {
      "heading" : "3.1 Datasets",
      "text" : "WIKIHOW-based dataset: WIKIHOW is the largest collection of “how-to” tasks, created by an online community, where each task is represented by sub-tasks with detailed descriptions and pictorial illustrations, e.g., as shown in Figure 1. We collected 168K articles (e.g., “Bake-a-Cake”) consisting of 238K tasks (e.g., “Making Vanilla Pound Cake”) and approximately 1.59 millions sub-tasks (e.g., “Gather your ingredients”, “Preheat the oven to 325 degrees”), representing a wide variety of activities and events. Then we created a corpus of approximately 1.34 million pairs of subsequent sub-tasks (i.e., source and target\nsentences for the SEQ2SEQ model), for which we have the training set of approximately 1.28 million pairs, the development and test sets of 26,800 pairs in each. This dataset aims to evaluate the models in an open-domain setting where the predictions can go into many kinds of directions.\nDESCRIPT-based dataset: The DESCRIPT corpus (Wanzare et al., 2016) is a crowdsourced corpus of event sequence descriptions on 40 different scenarios with approximately 100 event sequence descriptions per scenario. In addition, the corpus includes the gold paraphrase sets of event descriptions. From the DESCRIPT corpus, we create a new corpus of 29,150 sentence pairs of an event and its next contiguous event. Then, for each 10 sentence pairs, the 5th and 10th pairs are used for the development and test sets respectively, and 8 remaining pairs are used for the training set. Thus, each of the development and test sets has 2,915 pairs, and the training set has 23,320 pairs. This dataset helps to assess the models in a closeddomain setting where the goal is trying to achieve a reasonable accuracy."
    }, {
      "heading" : "3.2 Implementation details",
      "text" : "The models are implemented in TensorFlow (Abadi et al., 2016) and trained with/without attention mechanism using the training sets. Then, given a source sentence describing an event as input, the trained models are used to generate a sentence describing a predicted event. We use the BLEU metric (Papineni et al., 2002) to evaluate the generated sentences against the target sentences corresponding to the source sentences. A SEQ2SEQ architecture using a single layer adapted by Pichotta and Mooney (2016) is treated as the BASELINE model.\nWe found vocabulary sizes of 30,000 and 5,000 most frequent words as optimal for the WIKIHOW and DESCRIPT-based datasets, respectively. Words not occurring in the vocabulary are mapped to a special token UNK. Word embeddings are initialized using the pre-trained 300-dimensional word embeddings provided by Word2Vec (Mikolov et al., 2013) and then updated during training. We use two settings of a single BiLSTM/BiGRU layer (1-LAYERBISEQ2SEQ) and two BiLSTM/BiGRU layers (2- LAYER-BISEQ2SEQ). We use 300 hidden units for both encoder and decoder. Dropout (Srivastava et al., 2014) is applied with probability of 0.5.\nThe training objective is to minimize the crossentropy loss using the Adam optimizer (Kingma and Ba, 2015) and a mini-batch size of 64. The initial learning rate for Adam is selected from {0.0001, 0.0005, 0.001, 0.005, 0.01}. We run up to 100 training epochs, and we monitor the BLEU score after each training epoch and select the best model which produces highest BLEU score on the development set."
    }, {
      "heading" : "3.3 Evaluation using BLEU score",
      "text" : "Table 1 presents our BLEU scores with models trained on WIKIHOW and DESCRIPT-based data on the respective test sets. There are significant differences in attending to the WIKIHOW sentences and the DESCRIPT sentences. The BLEU scores between the two datasets cannot be compared because of the much larger degree of variation in WIKIHOW. The scores reported in Pichotta and Mooney (2016) on WIKIPEDIA are not comparable to our scores for the same reason.\nTable 1 shows that 1-LAYER-BISEQ2SEQ obtains better results than the strong BASELINE. Specifically, 1-LAYER-BISEQ2SEQ improves the baselines with 0.3+ BLEU in both cases of ATT and NON-ATT, indicating the usefulness of using bidirectional architecture. Furthermore, the twolayer architecture produces better scores than the single layer architecture. Using more layers can help to capture prominent linguistic features, that is probably the reason why deeper layers empirically work better.\nAs shown in Table 1, the GRU-based models obtains similar results to the LSTM-based models on the WIKIHOW-based dataset, but achieves lower scores on the DESCRIPT-based dataset. This could show that LSTM cells with memory gate may help to better remember linguistic fea-\ntures than GRU cells without memory gate for the closed-domain setting.\nThe ATT model outperforms the NON-ATT model on the WIKIHOW-based dataset, but not on the DESCRIPT-based dataset. This is probably because neighboring WIKIHOW sentences (i.e., subtask headers) are more parallel in structure (see “Pour batter in pan” and “Put pan in oven” from the initial example), which could be related to the fact that they are in general shorter. Figure 2 shows that the ATT model actually works well for DESCRIPT pairs with a short source sentence, while its performance decreases with longer sentences."
    }, {
      "heading" : "3.4 Evaluation based on paraphrase sets",
      "text" : "BLEU scores are difficult to interpret for the task: BLEU is a surface-based measure as mentioned in (Qin and Specia, 2015), while event prediction is essentially a semantic task. Table 2 shows output examples of the two-layer BILSTM SEQ2SEQ NON-ATT on the DESCRIPT-based dataset. Although the target and predicted sentences have different surface forms, they are perfect paraphrases of the same type of event.\nTo assess the semantic success of the prediction model, we use the gold paraphrase sets of event descriptions provided by the DESCRIPT corpus for 10 of its scenarios. We consider a subset of 682 pairs, for which gold paraphrase information is available, and check, whether a target event and its corresponding predicted event are paraphrases,\ni.e., belong to the same gold paraphrase set. The accuracy results are given in Table 3 for the same LSTM-based models taken from Section 3.3. Accuracy is measured as the percentage of predicted sentences that occur token-identical in the paraphrase set of the corresponding target sentences. Our best model outperforms Pichotta and Mooney (2016)’s BASELINE by 3.4%.\nSince the DeScript gold sets do not contain all possible paraphrases, an expert (computational linguist) checked cases of near misses between Target and Predicted (i.e. similar to the cases shown in Table 2) in a restrictive manner, not counting borderline cases. So we achieve a final average accuracy of about 31%, which is the sum of an average accuracy over 6 models in Table 3 (24%) and an average accuracy (7%) of checking cases of near misses (i.e, Target and Predicted are clearly event paraphrases).\nThe result does not look really high, but the task is difficult: on average, one out of 26 paraphrase sets (i.e., event types) per scenario has to be predicted, the random baseline is about 4% only. Also we should be aware that the task is prediction of an unseen event, not classification of a given event description. Continuations of a story are underdetermined to some degree, which implies that the upper bound for human guessing cannot be 100 %, but must be substantially lower."
    }, {
      "heading" : "4 Conclusions",
      "text" : "In this paper, we explore the task of event prediction, where we aim to predict a next event addressed in a text based on the description of the preceding event. We created the new open-domain and closed-domain datasets based on WIKIHOW and DESCRIPT which are available to the public at: https://github.com/daiquocnguyen/ EventPrediction. We demonstrated that more advanced SEQ2SEQ models with a bidirectional and multi-layer RNN architecture substantially outperform the previous work. We also introduced an alternative evaluation method for event prediction based on gold paraphrase sets, which focuses on semantic agreement between the target and predicted sentences."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This research was funded by the German Research Foundation (DFG) as part of SFB 1102 “Information Density and Linguistic Encoding.” We would like to thank Hannah Seitz for her kind help and support. We thank anonymous reviewers for their helpful comments."
    } ],
    "references" : [ {
      "title" : "TensorFlow: Large-Scale Machine Learning on",
      "author" : [ "jay Vasudevan", "Fernanda B. Viégas", "Oriol Vinyals", "Pete Warden", "Martin Wattenberg", "Martin Wicke", "Yuan Yu", "Xiaoqiang Zheng" ],
      "venue" : null,
      "citeRegEx" : "Vasudevan et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Vasudevan et al\\.",
      "year" : 2016
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "Proceedings of the 3rd International Conference on Learning Representations.",
      "citeRegEx" : "Bahdanau et al\\.,? 2015",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning Phrase Representations using RNN Encoder– Decoder for Statistical Machine Translation",
      "author" : [ "Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio." ],
      "venue" : "Pro-",
      "citeRegEx" : "Cho et al\\.,? 2014",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2014
    }, {
      "title" : "Language to logical form with neural attention",
      "author" : [ "Li Dong", "Mirella Lapata." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 33–43.",
      "citeRegEx" : "Dong and Lapata.,? 2016",
      "shortCiteRegEx" : "Dong and Lapata.",
      "year" : 2016
    }, {
      "title" : "Finding structure in time",
      "author" : [ "Jeffrey L. Elman." ],
      "venue" : "Cognitive Science, pages 179–211.",
      "citeRegEx" : "Elman.,? 1990",
      "shortCiteRegEx" : "Elman.",
      "year" : 1990
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural Computation, pages 1735–1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "Proceedings of the 3rd International Conference on Learning Representations.",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Skip-thought vectors",
      "author" : [ "Ryan Kiros", "Yukun Zhu", "Ruslan Salakhutdinov", "Richard S. Zemel", "Antonio Torralba", "Raquel Urtasun", "Sanja Fidler." ],
      "venue" : "Proceedings of the 28th International Conference on Neural Information Processing Systems, pages",
      "citeRegEx" : "Kiros et al\\.,? 2015",
      "shortCiteRegEx" : "Kiros et al\\.",
      "year" : 2015
    }, {
      "title" : "Multi-task sequence to sequence learning",
      "author" : [ "Minh-Thang Luong", "Quoc V. Le", "Ilya Sutskever", "Oriol Vinyals", "Lukasz Kaiser." ],
      "venue" : "Proceedings of the 4th International Conference on Learning Representations.",
      "citeRegEx" : "Luong et al\\.,? 2016",
      "shortCiteRegEx" : "Luong et al\\.",
      "year" : 2016
    }, {
      "title" : "Effective approaches to attentionbased neural machine translation",
      "author" : [ "Minh-Thang Luong", "Hieu Pham", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1412–1421.",
      "citeRegEx" : "Luong et al\\.,? 2015",
      "shortCiteRegEx" : "Luong et al\\.",
      "year" : 2015
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Gregory S. Corrado", "Jeffrey Dean." ],
      "venue" : "Advances in Neural Information Processing Systems 26: 27th Annual Conference on",
      "citeRegEx" : "Mikolov et al\\.,? 2013",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Abstractive Text Summarization using Sequence-tosequence RNNs and Beyond",
      "author" : [ "Ramesh Nallapati", "Bowen Zhou", "Cicero dos Santos", "Caglar Gulcehre", "Bing Xiang." ],
      "venue" : "Proceedings of the 20th SIGNLL Conference on Computational Natural",
      "citeRegEx" : "Nallapati et al\\.,? 2016",
      "shortCiteRegEx" : "Nallapati et al\\.",
      "year" : 2016
    }, {
      "title" : "Bleu: A method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu." ],
      "venue" : "Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 311–318.",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "Using sentence-level lstm language models for script inference",
      "author" : [ "Karl Pichotta", "Raymond J. Mooney." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 279–289.",
      "citeRegEx" : "Pichotta and Mooney.,? 2016",
      "shortCiteRegEx" : "Pichotta and Mooney.",
      "year" : 2016
    }, {
      "title" : "Truly exploring multiple references for machine translation evaluation",
      "author" : [ "Ying Qin", "Lucia Specia." ],
      "venue" : "Proceedings of the 18th Annual Conference of the European Association for Machine Translation, pages 113–120.",
      "citeRegEx" : "Qin and Specia.,? 2015",
      "shortCiteRegEx" : "Qin and Specia.",
      "year" : 2015
    }, {
      "title" : "Bidirectional recurrent neural networks",
      "author" : [ "M. Schuster", "K.K. Paliwal." ],
      "venue" : "IEEE Transactions on Signal Processing, pages 2673–2681.",
      "citeRegEx" : "Schuster and Paliwal.,? 1997",
      "shortCiteRegEx" : "Schuster and Paliwal.",
      "year" : 1997
    }, {
      "title" : "Dropout: A simple way to prevent neural networks from overfitting",
      "author" : [ "Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov." ],
      "venue" : "Journal of Machine Learning Research, pages 1929–1958.",
      "citeRegEx" : "Srivastava et al\\.,? 2014",
      "shortCiteRegEx" : "Srivastava et al\\.",
      "year" : 2014
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le." ],
      "venue" : "Proceedings of the 27th International Conference on Neural Information Processing Systems, pages 3104–3112.",
      "citeRegEx" : "Sutskever et al\\.,? 2014",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "Grammar as a foreign language",
      "author" : [ "Oriol Vinyals", "Lukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton." ],
      "venue" : "Advances in Neural Information Processing Systems 28, pages 2773– 2781.",
      "citeRegEx" : "Vinyals et al\\.,? 2015",
      "shortCiteRegEx" : "Vinyals et al\\.",
      "year" : 2015
    }, {
      "title" : "Descript: A crowdsourced corpus for the acquisition of highquality script knowledge",
      "author" : [ "Lilian D.A. Wanzare", "Alessandra Zarcone", "Stefan Thater", "Manfred Pinkal." ],
      "venue" : "Proceedings of the Tenth International Conference on Language Re-",
      "citeRegEx" : "Wanzare et al\\.,? 2016",
      "shortCiteRegEx" : "Wanzare et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "SEQ2SEQ have received significant research attention, especially in machine translation (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015), and in other NLP tasks such as parsing (Vinyals et al.",
      "startOffset" : 88,
      "endOffset" : 173
    }, {
      "referenceID" : 17,
      "context" : "SEQ2SEQ have received significant research attention, especially in machine translation (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015), and in other NLP tasks such as parsing (Vinyals et al.",
      "startOffset" : 88,
      "endOffset" : 173
    }, {
      "referenceID" : 1,
      "context" : "SEQ2SEQ have received significant research attention, especially in machine translation (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015), and in other NLP tasks such as parsing (Vinyals et al.",
      "startOffset" : 88,
      "endOffset" : 173
    }, {
      "referenceID" : 9,
      "context" : "SEQ2SEQ have received significant research attention, especially in machine translation (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015), and in other NLP tasks such as parsing (Vinyals et al.",
      "startOffset" : 88,
      "endOffset" : 173
    }, {
      "referenceID" : 11,
      "context" : "pata, 2016), text summarization (Nallapati et al., 2016) and multi-task learning (Luong et al.",
      "startOffset" : 32,
      "endOffset" : 56
    }, {
      "referenceID" : 8,
      "context" : ", 2016) and multi-task learning (Luong et al., 2016).",
      "startOffset" : 32,
      "endOffset" : 52
    }, {
      "referenceID" : 4,
      "context" : "In general, SEQ2SEQ uses an encoder which typically is a recurrent neural network (RNN) (Elman, 1990) to encode a source sequence, and then",
      "startOffset" : 88,
      "endOffset" : 101
    }, {
      "referenceID" : 7,
      "context" : "SEQ2SEQ has been applied to text prediction by Kiros et al. (2015) and Pichotta and Mooney (2016).",
      "startOffset" : 47,
      "endOffset" : 67
    }, {
      "referenceID" : 7,
      "context" : "SEQ2SEQ has been applied to text prediction by Kiros et al. (2015) and Pichotta and Mooney (2016). We also use SEQ2SEQ for prediction of what comes next in a text.",
      "startOffset" : 47,
      "endOffset" : 98
    }, {
      "referenceID" : 19,
      "context" : "We also present another dataset based on the DESCRIPT corpus—a crowdsourced corpus of event sequence descriptions (Wanzare et al., 2016).",
      "startOffset" : 114,
      "endOffset" : 136
    }, {
      "referenceID" : 12,
      "context" : "• Pichotta and Mooney (2016) use the BLEU score (Papineni et al., 2002) for evaluation (i.",
      "startOffset" : 48,
      "endOffset" : 71
    }, {
      "referenceID" : 12,
      "context" : "• Pichotta and Mooney (2016) use the BLEU score (Papineni et al.",
      "startOffset" : 2,
      "endOffset" : 29
    }, {
      "referenceID" : 17,
      "context" : ", xm) (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015; Vinyals et al., 2015; Luong et al., 2016).",
      "startOffset" : 6,
      "endOffset" : 113
    }, {
      "referenceID" : 2,
      "context" : ", xm) (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015; Vinyals et al., 2015; Luong et al., 2016).",
      "startOffset" : 6,
      "endOffset" : 113
    }, {
      "referenceID" : 1,
      "context" : ", xm) (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015; Vinyals et al., 2015; Luong et al., 2016).",
      "startOffset" : 6,
      "endOffset" : 113
    }, {
      "referenceID" : 18,
      "context" : ", xm) (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015; Vinyals et al., 2015; Luong et al., 2016).",
      "startOffset" : 6,
      "endOffset" : 113
    }, {
      "referenceID" : 8,
      "context" : ", xm) (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015; Vinyals et al., 2015; Luong et al., 2016).",
      "startOffset" : 6,
      "endOffset" : 113
    }, {
      "referenceID" : 15,
      "context" : "We use a bidirectional RNN (BiRNN) architecture (Schuster and Paliwal, 1997) for mapping the source sequence x1, x2, .",
      "startOffset" : 48,
      "endOffset" : 76
    }, {
      "referenceID" : 17,
      "context" : "When not using attention mechanism (Sutskever et al., 2014; Cho et al., 2014), the vector representation c is the last state sm of the encoder, which is used to initialize the decoder.",
      "startOffset" : 35,
      "endOffset" : 77
    }, {
      "referenceID" : 2,
      "context" : "When not using attention mechanism (Sutskever et al., 2014; Cho et al., 2014), the vector representation c is the last state sm of the encoder, which is used to initialize the decoder.",
      "startOffset" : 35,
      "endOffset" : 77
    }, {
      "referenceID" : 1,
      "context" : "Attention mechanism allows the decoder to attend to different parts of the source sequence at one position of a timestep of generating the target sequence (Bahdanau et al., 2015; Luong et al., 2015; Vinyals et al., 2015).",
      "startOffset" : 155,
      "endOffset" : 220
    }, {
      "referenceID" : 9,
      "context" : "Attention mechanism allows the decoder to attend to different parts of the source sequence at one position of a timestep of generating the target sequence (Bahdanau et al., 2015; Luong et al., 2015; Vinyals et al., 2015).",
      "startOffset" : 155,
      "endOffset" : 220
    }, {
      "referenceID" : 18,
      "context" : "Attention mechanism allows the decoder to attend to different parts of the source sequence at one position of a timestep of generating the target sequence (Bahdanau et al., 2015; Luong et al., 2015; Vinyals et al., 2015).",
      "startOffset" : 155,
      "endOffset" : 220
    }, {
      "referenceID" : 1,
      "context" : ", 2014; Bahdanau et al., 2015; Vinyals et al., 2015; Luong et al., 2016). Typically, SEQ2SEQ consists of a RNN encoder and a RNN decoder. The RNN encoder maps the source sequence into a vector representation c which is then fed as input to the decoder for generating the target sequence. We use a bidirectional RNN (BiRNN) architecture (Schuster and Paliwal, 1997) for mapping the source sequence x1, x2, ..., xm into the list of encoder states s1, s e 2, ..., s e m. The RNN decoder is able to work with or without attention mechanism. When not using attention mechanism (Sutskever et al., 2014; Cho et al., 2014), the vector representation c is the last state sm of the encoder, which is used to initialize the decoder. Then, at the timestep i (1 ≤ i ≤ n), the RNN decoder takes into account the hidden state si−1 and the previous input yi−1 to output the hidden state si and generate the target yi. Attention mechanism allows the decoder to attend to different parts of the source sequence at one position of a timestep of generating the target sequence (Bahdanau et al., 2015; Luong et al., 2015; Vinyals et al., 2015). We adapt the attention mechanism proposed by Vinyals et al. (2015) to employ a concatenation of the hidden state si and the vector representation c to make predictions at the timestep i.",
      "startOffset" : 8,
      "endOffset" : 1191
    }, {
      "referenceID" : 5,
      "context" : "We use two advanced variants of RNNs that replace the cells of RNNs with the Long Sort Term Memory (LSTM) cells (Hochreiter and Schmidhuber, 1997) and the Gated Recurrent Unit (GRU) cells (Cho et al.",
      "startOffset" : 112,
      "endOffset" : 146
    }, {
      "referenceID" : 2,
      "context" : "We use two advanced variants of RNNs that replace the cells of RNNs with the Long Sort Term Memory (LSTM) cells (Hochreiter and Schmidhuber, 1997) and the Gated Recurrent Unit (GRU) cells (Cho et al., 2014).",
      "startOffset" : 188,
      "endOffset" : 206
    }, {
      "referenceID" : 2,
      "context" : "We use two advanced variants of RNNs that replace the cells of RNNs with the Long Sort Term Memory (LSTM) cells (Hochreiter and Schmidhuber, 1997) and the Gated Recurrent Unit (GRU) cells (Cho et al., 2014). We also use a deeper architecture of multi-layers, to model complex interactions in the context. This is different from Kiros et al. (2015) and Pichotta and Mooney (2016)",
      "startOffset" : 189,
      "endOffset" : 348
    }, {
      "referenceID" : 2,
      "context" : "We use two advanced variants of RNNs that replace the cells of RNNs with the Long Sort Term Memory (LSTM) cells (Hochreiter and Schmidhuber, 1997) and the Gated Recurrent Unit (GRU) cells (Cho et al., 2014). We also use a deeper architecture of multi-layers, to model complex interactions in the context. This is different from Kiros et al. (2015) and Pichotta and Mooney (2016)",
      "startOffset" : 189,
      "endOffset" : 379
    }, {
      "referenceID" : 19,
      "context" : "DESCRIPT-based dataset: The DESCRIPT corpus (Wanzare et al., 2016) is a crowdsourced corpus of event sequence descriptions on 40 different scenarios with approximately 100 event sequence descriptions per scenario.",
      "startOffset" : 44,
      "endOffset" : 66
    }, {
      "referenceID" : 12,
      "context" : "We use the BLEU metric (Papineni et al., 2002) to evaluate the generated sentences against the target sentences corresponding to the source sentences.",
      "startOffset" : 23,
      "endOffset" : 46
    }, {
      "referenceID" : 12,
      "context" : "We use the BLEU metric (Papineni et al., 2002) to evaluate the generated sentences against the target sentences corresponding to the source sentences. A SEQ2SEQ architecture using a single layer adapted by Pichotta and Mooney (2016) is treated as the BASELINE model.",
      "startOffset" : 24,
      "endOffset" : 233
    }, {
      "referenceID" : 10,
      "context" : "Word embeddings are initialized using the pre-trained 300-dimensional word embeddings provided by Word2Vec (Mikolov et al., 2013) and then updated during training.",
      "startOffset" : 107,
      "endOffset" : 129
    }, {
      "referenceID" : 16,
      "context" : "Dropout (Srivastava et al., 2014) is applied with probability of 0.",
      "startOffset" : 8,
      "endOffset" : 33
    }, {
      "referenceID" : 6,
      "context" : "The training objective is to minimize the crossentropy loss using the Adam optimizer (Kingma and Ba, 2015) and a mini-batch size of 64.",
      "startOffset" : 85,
      "endOffset" : 106
    }, {
      "referenceID" : 13,
      "context" : "The scores reported in Pichotta and Mooney (2016) on WIKIPEDIA are not comparable to our scores for the same reason.",
      "startOffset" : 23,
      "endOffset" : 50
    }, {
      "referenceID" : 14,
      "context" : "BLEU scores are difficult to interpret for the task: BLEU is a surface-based measure as mentioned in (Qin and Specia, 2015), while event prediction is essentially a semantic task.",
      "startOffset" : 101,
      "endOffset" : 123
    }, {
      "referenceID" : 13,
      "context" : "Our best model outperforms Pichotta and Mooney (2016)’s BASELINE by 3.",
      "startOffset" : 27,
      "endOffset" : 54
    } ],
    "year" : 2017,
    "abstractText" : "This paper presents an approach to the task of predicting an event description from a preceding sentence in a text. Our approach explores sequence-to-sequence learning using a bidirectional multi-layer recurrent neural network. Our approach substantially outperforms previous work in terms of the BLEU score on two datasets derived from WIKIHOW and DESCRIPT respectively. Since the BLEU score is not easy to interpret as a measure of event prediction, we complement our study with a second evaluation that exploits the rich linguistic annotation of gold paraphrase sets of events.",
    "creator" : "LaTeX with hyperref package"
  }
}