{"conference": "ICLR 2017 conference submission", "title": "Gated-Attention Readers for Text Comprehension", "abstract": "In this paper we study the problem of answering cloze-style questions over documents. Our model, the Gated-Attention (GA) Reader, integrates a multi-hop architecture with a novel attention mechanism, which is based on multiplicative interactions between the query embedding and the intermediate states of a recurrent neural network document reader. This enables the reader to build query-specific representations of tokens in the document for accurate answer selection. The GA Reader obtains state-of-the-art results on three benchmarks for this task--the CNN \\& Daily Mail news stories and the Who Did What dataset. The effectiveness of multiplicative interaction is demonstrated by an ablation study, and by comparing to alternative compositional operators for implementing the gated-attention.", "reviews": [{"is_meta_review": true, "comments": "SUMMARY.\n\nThe paper proposes a machine reading approach for cloze-style question answering.\nThe proposed system first encodes the query and the document using a bidirectional gru. These two representations are combined together using a Gated Attention (GA).\nGA calculates the compatibility of each word in the document and the query as a probability distribution.\nFor each word in the document a gate is calculated weighting the query representation according to the word compatibility.\nUltimately, the gate is applied to the gru-encoded document word.\nThe resulting word vectors are re-encoded with a bidirectional GRU.\nThis process is performed for multiple hops. After k hops, the probability of a word to be part of the answer is calculated by a log-linear model that take as input the last word representations, and the concatenation of the last query representation before and after the cloze token.\nThe probability of a candidate being the answer to the question is given by a linear combination of the single word probabilities.\n\nThe proposed model is tested on 4 different dataset. \nThe authors shown that the proposed model works well (state-of-the-art performance) for 3 out of 4 benchmarks.\n\n\n----------\n\nOVERALL JUDGMENT\nThe main contribution of the paper is the gated attention mechanism, that in my opinion, is a simple and interesting idea.\nThe paper is well thought, and the ablation study on the benefits given by the gated attention are convincing.\nThe GA reader as whole model outperforms previous state-of-the-art models on 3 benchmarks and seems very promising also on the CBT dataset.\nI would have liked to see some discussion on why the model works less well on the CBT dataset, though.\n\n\n----------\n\nDETAILED COMMENTS\n\nminor. In the introduction, Weston et al., 2014 do not use any attention mechanism.", "IS_META_REVIEW": true}, {"TITLE": "ICLR committee final decision", "OTHER_KEYS": "ICLR 2017 pcs", "comments": "The paper proposes several extensions to popular attention-enhanced models for cloze-style QA. The results are near state of the art, and an ablation study hows that the different features (multiplicative interaction, gating) contribute to the model's performance. The main concern is the limited applicability of the model to other machine reading problems. The authors claim that unpublished results show applicability to other problems, but that is not sufficient defence against these concerns in the context of this paper. That said, the authors have addressed most of the other concerns brought up by the reviewers (e.g. controlling for number of hops) in the revised version. Overall however, the PCs believe that this contribution is not broad and not novel enough; we encourage the authors to resubmit.", "IS_META_REVIEW": false, "DATE": "06 Feb 2017", "is_meta_review": false}, {"TITLE": "Request the Reviewers for Feedback", "OTHER_KEYS": "Hanxiao Liu", "comments": "Respected Reviewers,\n\nWe would like to thank you again for the comments and suggestions. We believe that we have addressed most of the concerns raised in the revision posted on 12-21-2016. \n\nWe request you to kindly review the changes and our rebuttal below, and reconsider your scores if satisfied. Otherwise, kindly let us know how we can improve the paper further (before the review period ends on Jan 20 this Friday).\n\nBest Regards,\nAuthors.", "IS_META_REVIEW": false, "DATE": "17 Jan 2017", "is_meta_review": false}, {"TITLE": "Revision #2 (12/21/2016)", "OTHER_KEYS": "Bhuwan Dhingra", "comments": "Based on the reviewer feedback we have added the following:\n\n1. Analysis of the effect of number of hops K on the performance to Table 4.\n2. Ablation study of the components in the current version added on top of GA Reader-- to Table 6.\n\nA discussion of these is added to section 4.4, and also provided in response to the reviews here.\n\nNote - Mistakenly pushed the same revision twice on 12/21/2016.", "IS_META_REVIEW": false, "DATE": "22 Dec 2016", "is_meta_review": false}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Review", "is_meta_review": false, "comments": "Summary:\n\nThe authors propose a multi-hop \"gated attention\" model, which models the interactions between query and document representations, for answering cloze-style questions. The document representation is attended to sequentially over multiple-hops using similarity with the query representation (using a dot-product) as the scoring/attention function. \nThe proposed method improves upon (CNN, Daily Mail, Who-Did-What datasets) or is comparable to (CBT dataset) the state-of-the-art results.\n\n\nPros:\n\n1. Nice idea on heirarchical attention for modulating the context (document) representation by the task-specific (query) representation.\n2. The presentation is clear with thorough experimental comparison with the latest results.\n\n\nComments:\n\n1. The overall system presents a number of architectural elements: (1) attention at multiple layers (multi-hop), (2) query based attention for the context (or gated attention), (3) encoding the query vector at each layer independently.\nIt is important to breakdown the gain in performance due to the above factors: the ablation study presented in section 4.4 helps establish the importance of Gated Attention (#2 above). However, it is not clear:\n\n  (1) how much multiple-hops of gated-attention contribute to the performance.\n  (2) how important is it to have a specialized query encoder for each layer.\n\nUnderstanding the above better, will help simplify the architecture.\n\n\n2. The tokens are represented using L(w) and C(w). It is not clear if C(w) is crucial for the performance of the proposed method.\nThere is a significant performance drop when C(w) is absent (e.g. in \"GA Reader--\"; although there are other changes in \"GA Reader--\" which could affect the performance). Hence, it is not clear how much does the main idea, i.e., gated attention contributes towards the superior performance of the proposed method.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "A simple and interesting idea for iteratively re-weighting word representations ", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "This paper presents an interesting idea for iteratively re-weighting the word representations in a document (hence the GRU-coded doc representation as well) with a simple multiplication operation. As the authors correctly pointed out, such an operation serves as a \"filter\" to reduce the attentions to less relevant parts in the document, hence leading to better performance of the modeling.\n\nThe results are or close to the state-of-the-art for a few Cloze-style QA tasks. This paper would deserve an even higher score, if the following limitations could be addressed better:\n\n1. While interesting and conceptually simple (though with significant increased computational overheads), the architecture proposed in the paper is for a very specific task. \n\n2. The improvement of the main idea of this paper (gated attention) is less significant, comparing GA Reader-- vs. GA Reader, while the latter includes a number of engineering tricks such as adding character embedding and using a word embedding trained from larger corpus (GloVe), as well as some small improvements on the modeling by using token-specific attention in (5).\n\n3. I also wish the authors can shed more lights on what a role the K (number of hops) plays, both intuitively and empirically. I feel more insights could be obtained if we do more deeper analysis of K's impacts to different types of questions for example.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "is_meta_review": false}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "No Title", "is_meta_review": false, "comments": "SUMMARY.\n\nThe paper proposes a machine reading approach for cloze-style question answering.\nThe proposed system first encodes the query and the document using a bidirectional gru. These two representations are combined together using a Gated Attention (GA).\nGA calculates the compatibility of each word in the document and the query as a probability distribution.\nFor each word in the document a gate is calculated weighting the query representation according to the word compatibility.\nUltimately, the gate is applied to the gru-encoded document word.\nThe resulting word vectors are re-encoded with a bidirectional GRU.\nThis process is performed for multiple hops. After k hops, the probability of a word to be part of the answer is calculated by a log-linear model that take as input the last word representations, and the concatenation of the last query representation before and after the cloze token.\nThe probability of a candidate being the answer to the question is given by a linear combination of the single word probabilities.\n\nThe proposed model is tested on 4 different dataset. \nThe authors shown that the proposed model works well (state-of-the-art performance) for 3 out of 4 benchmarks.\n\n\n----------\n\nOVERALL JUDGMENT\nThe main contribution of the paper is the gated attention mechanism, that in my opinion, is a simple and interesting idea.\nThe paper is well thought, and the ablation study on the benefits given by the gated attention are convincing.\nThe GA reader as whole model outperforms previous state-of-the-art models on 3 benchmarks and seems very promising also on the CBT dataset.\nI would have liked to see some discussion on why the model works less well on the CBT dataset, though.\n\n\n----------\n\nDETAILED COMMENTS\n\nminor. In the introduction, Weston et al., 2014 do not use any attention mechanism.", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "15 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Query GRUs and GA Placement", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "", "IS_META_REVIEW": false, "DATE": "03 Dec 2016", "is_meta_review": false}, {"TITLE": "Revision #1 (12-01-2016)", "OTHER_KEYS": "Bhuwan Dhingra", "comments": "Updated Tables 3,4,5 with new comparisons of GA Reader's performance when word embeddings are either held fixed or updated during training. We observe that for smaller datasets (WDW and CBT) keeping the word embeddings fixed leads to substantial improvements (>1%). Also, retuned the character GRU hidden state size to 25 for WDW and CBT, which was previously 50.", "IS_META_REVIEW": false, "DATE": "01 Dec 2016", "is_meta_review": false}, {"TITLE": "character-level embeddings", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "", "IS_META_REVIEW": false, "DATE": "30 Nov 2016", "is_meta_review": false}, {"TITLE": "Confusion relating equation (5) to figure 3.", "OTHER_KEYS": "David McAllester", "comments": "I am confused by the relationship between equation (5) and the heat map in figure 3 (and the other heat maps in the appendix).  In equation (5) the attention alpha_i is indexed by document positions.  But the heat map in figure 3 is indexed by question positions.  I don't understand.", "IS_META_REVIEW": false, "DATE": "16 Nov 2016", "is_meta_review": false}, {"TITLE": "Question about the experimental details", "OTHER_KEYS": "(anonymous)", "comments": "1: in WDW dataset, each candidate answer might has several tokens, how did you deal with it?\n2: Different from the previous GA readers, you have three differences, did you do ablation study on the attention mechanism and char embedding?\n3: I observed the experiment hyper parameters are different from previous GA readers, how much does this hyper parameters matters?\n ", "IS_META_REVIEW": false, "DATE": "14 Nov 2016", "is_meta_review": false}, {"TITLE": "which data used", "OTHER_KEYS": "(anonymous)", "comments": "Hi, for CNN/DailyMail, did you obtain the data via DeepMind's extraction script, or did you use Kyunghyun Cho's preprocessed data?", "IS_META_REVIEW": false, "DATE": "11 Nov 2016", "is_meta_review": false}], "SCORE": 7, "authors": "Bhuwan Dhingra, Hanxiao Liu, Zhilin Yang, William W. Cohen, Ruslan Salakhutdinov", "accepted": false, "id": ""}
