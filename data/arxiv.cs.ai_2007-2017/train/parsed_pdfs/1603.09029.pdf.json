{
  "name" : "1603.09029.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Maximize Pointwise Cost-sensitively Submodular Functions With Budget Constraint",
    "authors" : [ "Nguyen Viet Cuong", "Huan Xu" ],
    "emails" : [ "NVCUONG@NUS.EDU.SG", "ISEXUH@NUS.EDU.SG" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Consider problems where we need to adaptively make a sequence of decisions while taking into account the outcomes of previous decisions. For instance, in the sensor placement problem (Krause & Guestrin, 2007b; Golovin & Krause, 2011), one needs to sequentially place sensors at some prespecified locations, taking into account the working conditions of previously deployed sensors. The aim is to cover as large an area as possible while keeping the cost of placement within a given budget. As another example, in the pool-based active learning problem (McCallum & Nigam, 1998; Cuong et al., 2016), one needs to sequentially select unlabeled examples and query their labels, taking into account the previously observed labels. The aim is to learn a good classifier while ensuring that the cost of querying does not exceed some given budget.\nThese problems can usually be considered under the framework of adaptive optimization with budget constraint. In this framework, the objective is to find a policy for making decisions that maximizes the value of some utility function. With a budget constraint, such a policy must have a cost no higher than the budget given by the problem. Adaptive optimization with budget constraint has been previously studied in the average case (Dean et al., 2004; Asadpour et al., 2008; Golovin & Krause, 2011). In this paper, we shall focus on this problem in the worst case.\nIn contrast to previous works on adaptive optimization with budget constraint (both in the average case and worst case), we consider the setting where the cost is a set function on sets of decisions. For example, in the sensor placement problem above, the cost of a set of deployed sensors may be the weight of the minimum spanning tree connecting those sensors, where the weight of the edge between any two sensors is the distance between them. In this case, the cost of deploying a sensor is not fixed, but depends on the set of previously deployed sensors. This setting allows the cost function to be non-modular, and thus is more general than the setting in previous works, which usually assume the cost to be modular.1\nWith such general cost functions, we focus on a useful class of utility functions that satisfy a novel property called pointwise cost-sensitive submodularity. This property is an extension of a property called cost-sensitive submodularity to the adaptive setting. In essence, cost-sensitive submodularity means the utility function is more submodular than the cost. The usual submodularity property (Nemhauser & Wolsey, 1978) is a special case of cost-sensitive submodularity when the cost is modular. In the adaptive setting, pointwise cost-sensitive submodularity is a generalization of pointwise submodularity, a property of utility functions that has been used in interactive submodular set cover and active learning problems (Guillory & Bilmes, 2010; Cuong\n1A set function c is modular if c(S)= ∑\nx∈S c({x}) for all S.\nar X\niv :1\n60 3.\n09 02\n9v 1\n[ cs\n.A I]\n3 0\nM ar\n2 01\net al., 2014).\nFor the class of pointwise cost-sensitively submodular utilities, we investigate the near-optimality of greedy policies for the worst-case adaptive optimization problem with budget constraint. A policy is near-optimal if its worst-case utility is within a constant factor of the optimal worst-case utility. In this work, we first consider two simple greedy policies: one that maximizes the worst-case utility gain and one that maximizes the worst-case utility gain per unit cost increment at each step. If the cost function is uniform and modular, it is known that these two policies are equivalent and are near-optimal (Cuong et al., 2014). However, in this paper, we show that these policies cannot achieve near-optimality in our general setting, even for the simpler case of non-uniform modular costs.\nDespite this negative result, we can prove that the best between these two greedy policies always achieves nearoptimality. This suggests we can combine the two policies into one greedy policy, and this new policy can achieve near-optimality with respect to the optimal worst-case policy that uses half of the budget. Our proof for this result is built upon the proof techniques for worst-case adaptive optimization with uniform modular costs (Cuong et al., 2014) and non-adaptive optimization with non-uniform modular costs (Khuller et al., 1999) but goes beyond them to handle general, possibly non-uniform and non-modular, costs.\nWe also discuss applications of our theoretical results to the budgeted adaptive coverage problem and the budgeted pool-based active learning problem, both of which can be modeled as worst-case adaptive optimization problems with budget constraint. Lastly, we report experimental results comparing the greedy policies on the budgeted active learning problem.\nIn summary, our paper makes the following theoretical contributions to the study of adaptive optimization with budget constraint:\n• We formalize and consider this problem in the worst case with a general, possibly non-uniform and nonmodular, cost function (Section 2). This general setting has not been considered before our work.\n• We generalize the concept of submodularity to costsensitive submodularity, which can be further extended to pointwise cost-sensitive submodularity in the adaptive setting (Section 3). The class of utility functions satisfying pointwise cost-sensitive submodularity is useful for the adaptive optimization problem with budget constraint.\n• We prove that two simple greedy policies for this problem are not near-optimal, but the best between\nthem is near-optimal. We also propose a combined policy for the problem (Section 4)."
    }, {
      "heading" : "2. Problem Description: Worst-case Adaptive Optimization with Budget Constraint",
      "text" : "We now formalize the framework for worst-case adaptive optimization with budget constraint. LetX be a finite set of items (or decisions) and Y be a finite set of possible states (or outcomes). Each item in X can be in any particular state in Y . Let h : X → Y be a deterministic function that maps each item x ∈ X to its state h(x) ∈ Y . We call h a realization. Let H def= YX = {h | h : X → Y} be the realization set consisting of all possible realizations.\nWe consider the problem where we sequentially select a subset of items from X as follows: we select an item, observe its state, then select the next item, observe its state, etc. After some iterations, our observations so far can be represented as a partial realization, which is a partial function fromX toY . An adaptive strategy to select items takes into account the states of all previous items when deciding the next item to select. Each adaptive strategy can be encoded as a policy for selecting items,2 where a policy is a function from a partial realization to the next item to select. A policy can be represented by a policy tree in which each node is an item to be selected and edges below a node correspond to its states. Figure 1 shows an illustration of a policy tree.\nWe assume there is a cost function c : 2X → R≥0, where 2X is the power set of X . For any set of items S ⊆ X , c(S) is the cost incurred if we select the items in S and observe their states. For simplicity, we also assume c(∅) = 0 and c(S) > 0 for S 6= ∅. If c is modular, then c(S) = ∑ x∈S c({x}) for all S. However, in this paper, we consider a general, possibly non-modular, cost function c.\nFor a policy π, we define the cost of π as the maximum cost incurred by a set of items selected along any path of the policy tree of π. Note that if we fix a realization h, the set of items selected by the policy π is fixed, and we\n2We only consider deterministic policies in this paper.\ndenote this set by xπh. The set x π h corresponds to a path of the policy tree of π, and thus the cost of π can be formally defined as c(π) def= maxh∈H c(xπh).\nIn the worst-case adaptive optimization problem, we have a utility function f : 2X × H → R≥0 that we wish to maximize in the worst case. The utility function f(S, h) depends on a set S of selected items and a realization h that determines the states of all items. Essentially, f(S, h) denotes the value of selecting S, given that the true realization is h. We assume that f(∅, h) = 0 for all h.\nFor a policy π, we define the worst-case utility of π as fworst(π) def= minh∈H f(x π h, h). Given a budget K > 0, our goal is to find a policy π∗ whose cost does not exceed K, and π∗ maximizes fworst. Formally,\nπ∗ def= arg max π fworst(π) subject to c(π) ≤ K.\nWe call this the problem of worst-case adaptive optimization with budget constraint."
    }, {
      "heading" : "3. Cost-sensitive Submodularity and Assumptions on the Utility",
      "text" : "Adaptive optimization with an arbitrary utility function is hard, so we focus on a useful class of utility functions: the pointwise cost-sensitively submodular functions. First, we will formally define cost-sensitive submodularity, a generalization of submodularity that takes into account a general cost on sets of items. Then we will state some assumptions on the utility that include the pointwise cost-sensitive submodularity assumption."
    }, {
      "heading" : "3.1. Cost-sensitive Submodularity",
      "text" : "We recall that a set function g : 2X → R is submodular if it satisfies the following diminishing return property: for all A ⊆ B ⊆ X and x ∈ X \\ B, g(A ∪ {x})− g(A) ≥ g(B ∪ {x})− g(B). Furthermore, g is monotone if g(A) ≤ g(B) for allA ⊆ B. Given a general cost function c, we define cost-sensitive submodularity as follows.\nDefinition 1 (Cost-sensitive Submodularity). A set function g : 2X → R is cost-sensitively submodular with respect to a cost function c if it satisfies: for all A ⊆ B ⊆ X and x ∈ X \\B,\ng(A ∪ {x})− g(A) ∆c(x|A) ≥ g(B ∪ {x})− g(B) ∆c(x|B) .\nIn this definition, for any item x ∈ X and any S ⊆ X , we define ∆c(x|S) def= c(S ∪ {x})− c(S), which is the cost increment of selecting x after S has been selected. Furthermore, for the cost-sensitive submodularity definition to\nbe well-defined, we assume that c is strictly monotone; that is, c(A) < c(B) for all A ⊂ B ⊆ X . Hence, ∆c(x|S) > 0 for all S and x /∈ S. In this paper, we also assume that c satisfies the triangle inequality, i.e., c(A∪B) ≤ c(A) + c(B) for all A,B ⊆ X .\nIn essence, cost-sensitive submodularity says that g is more submodular than the cost c. It is a generalization of submodularity for a general cost. When c is modular, costsensitive submodularity is equivalent to submodularity. Besides, if g is cost-sensitively submodular with respect to a submodular cost function, it will also be submodular.\nSince c satisfies the triangle inequality, it cannot be a supermodular function,3 but it can be non-submodular. For example, if X only contains three items {x1, x2, x3}, we can construct a set function c that is not submodular but satisfies the triangle inequality such that c(∅) = 0, c({x1}) = c({x2}) = c({x3}) = 1, c({x1, x2}) = 2, c({x1, x3}) = c({x2, x3}) = 1.5, and c({x1, x2, x3}) = 2.5. Note that this function is not submodular because c({x3, x2, x1}) − c({x3, x2}) > c({x3, x1})− c({x3}).\nWe now state some useful properties of cost-sensitive submodularity in Theorem 1 below. In this theorem, αg1+βg2 is the function g such that g(S) = αg1(S) + βg2(S) for all S ⊆ X , and αc1 + βc2 is the function c such that c(S) = αc1(S) + βc2(S) for all S ⊆ X . The proof of this theorem is given in Appendix A.\nTheorem 1. The following properties hold for costsensitively submodular functions. (a) If g1 and g2 are cost-sensitively submodular with respect to a cost function c, then αg1 + βg2 is also costsensitively submodular with respect to c for all α, β ≥ 0. (b) If g is cost-sensitively submodular with respect to cost functions c1 and c2, then g is also cost-sensitively submodular with respect to αc1 + βc2 for all α, β ≥ 0 such that α+ β > 0. (c) For any integer n ≥ 1, if g is monotone and c(S) = ∑n i=1 ai(g(S))\ni with non-negative coefficients ai ≥ 0 such that ∑n i=1 ai > 0, then g is cost-sensitively submodular with respect to c. (d) If g is monotone and c(S) = αeg(S) for α > 0, then g is cost-sensitively submodular with respect to c.\nThis theorem specifies various cases where a function g is cost-sensitively submodular with respect to a cost function c. Note that neither g nor c needs to be submodular for this theorem to hold. Parts (a) and (b) of the theorem state that cost-sensitive submodularity is preserved for linear combinations of either g or c. Parts (c) and (d) state that if c is a polynomial of g with non-negative coefficients or if c is an exponential of g, then g is cost-sensitively submodular\n3A set function c is super-modular if for all A ⊆ B ⊆ X and x ∈ X \\B, ∆c(x|A) ≤ ∆c(x|B).\nwith respect to c."
    }, {
      "heading" : "3.2. Pointwise Cost-sensitive Submodularity and Assumptions on the Utility Function",
      "text" : "In our problem, the utility function f(S, h) depends on both the selected items S and the realization h, and we assume it satisfies the pointwise cost-sensitive submodularity property below. Definition 2 (Pointwise Cost-sensitive Submodularity). A utility function f(S, h) is pointwise cost-sensitively submodular with respect to a cost function c if, for all h, the set function fh(S) def= f(S, h) is cost-sensitively submodular with respect to c.\nPointwise cost-sensitive submodularity is an extension of cost-sensitive submodularity to the adaptive setting. It is also a generalization of pointwise submodularity (Guillory & Bilmes, 2010; Golovin & Krause, 2011) for a general cost. Pointwise submodularity is an extension of submodularity to the adaptive setting and it has been used for active learning (Guillory & Bilmes, 2010; Cuong et al., 2014). When the cost c is modular, pointwise cost-sensitive submodularity is equivalent to pointwise submodularity.\nBesides pointwise cost-sensitive submodularity, we also assume the utility function satisfies the pointwise monotonicity and minimal dependency properties below. Definition 3 (Pointwise Monotonicity). A utility function f(S, h) is pointwise monotone if, for all h, the set function fh(S) def= f(S, h) is monotone. Definition 4 (Minimal Dependency). A utility function f(S, h) satisfies minimal dependency if the value of f(S, h) only depends on the items in S and their states (with respect to the realization h).\nPointwise monotonicity is an extension of monotonicity to the adaptive setting. The minimal dependency property is needed to make sure that the value of f only depends on what have already been observed. If we allow f to depend on the states of items that have not been selected, its value may be unpredictable and is hard to be reasoned about. Pointwise monotonicity and minimal dependency were also assumed in (Cuong et al., 2014) for the modular and uniform cost setting. Pointwise cost-sensitive submodularity, pointwise monotonicity, and minimal dependency will be useful for the analyses in the subsequent sections."
    }, {
      "heading" : "4. Greedy Policies and Analyses",
      "text" : "In this paper, we focus on greedy policies for the worst-case adaptive optimization problem with budget constraint. We are interested in a theoretical guarantee for these policies called the near-optimality guarantee. Specifically, a policy is near-optimal if its worst-case utility is within a constant\nAlgorithm 1 Cost-average Greedy Policy (π1) D ← ∅; U ← X ; repeat\nPick x∗ ∈ U that maximizes δ(x∗|D)/∆c(x∗|XD); if c(XD ∪ {x∗}) ≤ K then\nObserve state y∗ of x∗; D ← D ∪ {(x∗, y∗)};\nend if U ← U \\ {x∗};\nuntil U = ∅;\nAlgorithm 2 Cost-insensitive Greedy Policy (π2) D ← ∅; U ← X ; repeat\nPick x∗ ∈ U that maximizes δ(x∗|D); if c(XD ∪ {x∗}) ≤ K then\nObserve state y∗ of x∗; D ← D ∪ {(x∗, y∗)};\nend if U ← U \\ {x∗};\nuntil U = ∅;\nfactor of the optimal worst-case utility. In this section, we consider two most intuitive greedy policies and prove that each of these policies is individually not near-optimal but the best between them will always be near-optimal. We shall also discuss a combined policy and its guarantee in this section."
    }, {
      "heading" : "4.1. Two Simple Greedy Policies",
      "text" : "We consider two greedy policies in Algorithms 1 and 2. In these policies, D is the partial realization that we have observed so far, and XD def= {x ∈ X | (x, y) ∈ D for some y ∈ Y} is the domain of D (i.e., the set of selected items in D). We write δ(x∗|D) to denote the worstcase utility gain if x∗ is selected after we observe D. Formally, for any item x, we define δ(x|D) as:\nδ(x|D) def= min y∈Y {f(XD ∪{x},D∪{(x, y)})−f(XD,D)}.\nIn this definition, note that we have extended the utility function f to take a partial realization as the second parameter (instead of a full realization). This extension is possible because the utility function is assumed to satisfy minimal dependency, and thus its value only depends on the partial realization that we have observed so far.\nThe two greedy policies in Algorithms 1 and 2 are very simple and intuitive. The cost-average policy π1 greedily selects the items that maximize the worst-case utility gain per unit cost increment if they are still affordable by the current remaining budget. On the other hand, the cost-insensitive policy π2 simply ignores the items’ costs\nand greedily selects the affordable items that maximize the worst-case utility gain.\nAnalyses of π1 and π2: Given the two greedy policies, we are interested in their near-optimality: whether they provide a constant factor approximation to the optimal worstcase utility. Unfortunately, we can show that these policies are not near-optimal. These negative results are stated in Theorems 2 and 3 below. The proofs of these theorems construct counter-examples where the two greedy policies are not near-optimal (see Appendices B and C). The counter-examples use a modular cost function, so the results hold even for this simple case. Theorem 2. For any α > 0, there exists a worst-case adaptive optimization problem with a utility function f , a modular cost function c, and a budget K such that f and c satisfy the assumptions in Sections 2 and 3, and fworst(π1)/fworst(π\n∗) < α, where π∗ is the optimal policy for the problem. Theorem 3. For any α > 0, there exists a worst-case adaptive optimization problem with a utility function f , a modular cost function c, and a budget K such that f and c satisfy the assumptions in Sections 2 and 3, and fworst(π2)/fworst(π\n∗) < α, where π∗ is the optimal policy for the problem."
    }, {
      "heading" : "4.2. A Near-optimal Policy",
      "text" : "Although the greedy policies π1 and π2 are not nearoptimal, we now show that the best policy between them is in fact near-optimal. More specifically, let us define a policy π such that:\nπ def= { π1 if fworst(π1) > fworst(π2) π2 otherwise . (1)\nTheorem 4 below states that π is near-optimal for the worstcase adaptive optimization problem with budget constraint. Proving this theorem requires a sophisticated combination of the proof techniques for worst-case adaptive optimization with uniform modular costs (Cuong et al., 2014) and non-adaptive optimization with non-uniform modular costs (Khuller et al., 1999). Unlike the proof in (Khuller et al., 1999), our proof deals with policy trees instead of sets. Furthermore, previous proof techniques were originally used for modular cost functions, so we need to generalize the techniques to use them for general cost functions. The detailed proof is given in Appendix D. Theorem 4. Assume the utility function f and the cost function c satisfy the assumptions in Sections 2 and 3. Let π∗ be the optimal policy for the worst-case adaptive optimization problem with the utility f , cost c, and budget K. The policy π defined by Equation (1) satisfies:\nfworst(π) > 1\n2\n( 1− 1\ne\n) fworst(π ∗).\nThe constant factor 12 (1 − 1/e) in Theorem 4 is the same as the constant factor for the non-adaptive budgeted maximum coverage problem (Khuller et al., 1999). If we apply this theorem to a problem with a uniform modular cost, i.e., c({x}) = c({x′}) for all x and x′, then π1 = π2 and fworst(π) = fworst(π1) = fworst(π2). Thus, from Theorem 4, fworst(π1) = fworst(π2) > 12 (1− 1/e) fworst(π\n∗). Although this implies the greedy policy is near-optimal, the constant factor 12 (1− 1/e) in this case is not as good as the constant factor (1 − 1/e) in (Cuong et al., 2014) for the uniform modular cost setting.\nWe also note that Theorem 4 still holds if we replace the cost-insensitive policy π2 with only the first item that it selects (see the proof of Theorem 4 in Appendix D for details). In other words, we can terminate π2 right after it selects the first item and the near-optimality in the theorem is still guaranteed."
    }, {
      "heading" : "4.3. A Combined Policy",
      "text" : "With Theorem 4, a naive approach to the worst-case adaptive optimization problem with budget constraint is to estimate fworst(π1) and fworst(π2) (without actually running these policies) and use the best between them. However, exact estimation of these quantities is intractable because it would require a consideration of all realizations (an exponential number of them) to find the worst-case realization for these policies. This is very different from the nonadaptive setting where we can easily find the best policy because there is only one realization.\nFurthermore, in the adaptive setting, we cannot roll back once we run a policy. For example, we cannot run π1 and π2 at the same time to determine which one is better without doubling the budget. This is because we have to pay the cost every time we want to observe the state of an item, and the next item selected would depend on the previous states. Thus, the adaptive setting in our paper is more difficult than the non-adaptive setting considered in previous works (Khuller et al., 1999; Leskovec et al., 2007).\nIf we consider a Bayesian setting with some prior on the set of realizations (Golovin & Krause, 2011; Cuong et al., 2013; 2016), we can sample a subset of realizations from the prior to estimate fworst. However, this method does not provide any guarantee for the estimation.\nGiven these difficulties, a more practical approach is to run both π1 and π2 using half of the budget for each policy and combine the selected sets. Denote this combined policy by π1/2. The details of π1/2 are as follows:\n1. Run π1 with budgetK/2 (half of the total budget), and let the set of selected items be S1.\n2. Starting with the empty set, run π2 with budget K/2\nand let the set of items selected in this step be S2. For simplicity, we allow S2 to overlap with S1.\n3. Return S1 ∪ S2.\nUsing Theorem 4, we can show that π1/2 is near-optimal with respect to the optimal worst-case policy that uses half of the budget. Theorem 5 below states this result. The proof of this theorem is given in Appendix E. We note that the theorem still holds if the order of running π1 and π2 is exchanged in the policy π1/2.\nTheorem 5. Assume the same setting as in Theorem 4. Let π∗1/2 be the optimal policy for the worst-case adaptive optimization problem with budget K/2. The policy π1/2 satisfies: fworst(π1/2) > 12 ( 1− 1e ) fworst(π ∗ 1/2).\nSince Theorem 5 only compares π1/2 with the optimal policy π∗1/2 that uses half of the budget, a natural question is whether or not the policies π1 and π2 running with the full budget have a similar guarantee compared to π∗1/2. Using the same counter-example as in the proof of Theorem 3, we can easily show that this guarantee does not hold for the cost-insensitive policy π2. Theorem 6 states this result.\nTheorem 6. For any α > 0, there exists a worst-case adaptive optimization problem with a utility function f , a modular cost function c, and a budget K such that f and c satisfy the assumptions in Sections 2 and 3, and fworst(π2)/fworst(π ∗ 1/2) < α, where π ∗ 1/2 is the optimal policy for the problem with budget K/2.\nAs regards the cost-average policy π1, it remains open whether running π1 with the full budget provides any constant factor approximation to the worst-case utility of π∗1/2. However, in Appendix F, we show that it is not possible to construct a counter-example for this case using the utility and cost functions that are modular. As a result, a counterexample for this case (if there is any) should use a more sophisticated utility or cost function."
    }, {
      "heading" : "5. Applications",
      "text" : "We discuss two applications of adaptive optimization with budget constraint: the budgeted adaptive coverage problem and the budgeted pool-based active learning problem. These problems were considered in (Golovin & Krause, 2011) for the average case with a modular cost, while we study them in the worst case with general costs."
    }, {
      "heading" : "5.1. Budgeted Adaptive Coverage",
      "text" : "In this problem, we are given a set of locations where we need to place some sensors to get the spatial information of the surrounding environment. If sensors are deployed at a set of sensing locations, we have to pay a cost depending\non where the locations are. After a sensor is deployed at a location, it may be in one of a few possible states (e.g., this may be caused by a partial failure of the sensor), leading to various degree of information covered by the sensor. The budgeted adaptive coverage problem can be stated as: given a cost budget K, where should we place the sensors to cover as much spatial information as possible?\nWe can model this problem as a worst-case adaptive optimization problem with budget K. Let X be the set of all possible locations where sensors may be deployed, and let Y be the set of all possible states of the sensors. For each set of locations S ⊆ X , c(S) is the cost of deploying sensors there. For a location x and a state y, let Rx,y be the geometric shape associated with the spatial information covered if we put a sensor at x and its state is y. We can define the utility function f(S, h) = | ⋃ x∈S Rx,h(x)|, which is the cardinality (or volume) of the covered region.\nIf we fix the realization h, the above utility is monotone submodular (Krause & Guestrin, 2007a). Thus, f(S, h) is pointwise monotone and pointwise cost-sensitively submodular with respect to any modular cost function, including uniform modular costs. Since f(S, h) also satisfies minimal dependency, we can apply the policy π1/2 to this problem with the guarantee in Theorem 5. We can also achieve the guarantee if the cost is non-modular but f(S, h) is pointwise cost-sensitively submodular with respect to it."
    }, {
      "heading" : "5.2. Budgeted Pool-based Active Learning",
      "text" : "For pool-based active learning, we are given a finite set (a pool) of unlabeled examples and we need to adaptively query the labels of some selected examples from that set to train a classifier. Every time we query an example, we have to pay a cost, which depends on the set of all selected examples, and then get to see its label. In the next iteration, we can use the labels observed so far to select the next example to query. The budgeted pool-based active learning problem can be stated as: given a cost budget K, which examples should we query to train a good classifier?\nWe can model this problem as a worst-case adaptive optimization problem with budget K. Let X be the pool of unlabeled examples, and let Y be the set of all possible labels. For each set of examples S ⊆ X , c(S) is the cost of querying their labels. In this problem, a realization h is a labeling of all examples in the pool X . For poolbased active learning, previous works (Golovin & Krause, 2011; Cuong et al., 2013; 2014) have shown that the version space reduction utility is pointwise monotone submodular and satisfies minimal dependency. This utility is defined as f(S, h) = ∑ h′:h′(S)6=h(S) p0[h\n′], where p0 is a prior distribution on H and h(S) is the labels of S according to h. Thus, we can apply policy π1/2 to this problem with the guarantee in Theorem 5 if the cost c is modular.\nWith the utility above, the greedy criterion that maximizes δ(x∗|D) in the cost-insensitive policy π2 is equivalent to the well-known least confidence criterion x∗ = arg minx maxy pD[y;x] = arg maxx miny{1− pD[y;x]}, where pD is the posterior after observing D and pD[y;x] is the probability that x has label y. On the other hand, the greedy criterion that maximizes δ(x∗|D)/∆c(x∗|XD) in the cost-average policy π1 is equivalent to:\nx∗ = arg max x\n{ miny{1− pD[y;x]}\n∆c(x|XD)\n} . (2)\nWe prove this equation in Appendix G. Theorem 5 can also be applied if we consider the total generalized version space reduction utility (Cuong et al., 2014) that incorporates an arbitrary loss. This utility was also shown to be pointwise monotone submodular and satisfy minimal dependency (Cuong et al., 2014), and thus the theorem still holds in this case for modular costs."
    }, {
      "heading" : "6. Experiments",
      "text" : "We now present experimental results for the policies discussed in this paper. We focus on the budgeted poolbased active learning problem with various settings that use modular costs. We experiment with 3 binary classification data sets extracted from the 20 Newsgroups data (Joachims, 1996): alt.atheism/comp.graphics (data set 1), comp.sys.mac.hardware/comp.windows.x (data set 2), and rec.motorcycles/rec.sport.baseball (data set 3). We will consider settings where random costs and margindependent costs are put on the training examples. Since we use modular costs in the experiments, the costs are put on individual training examples, and the total cost is the sum of the selected examples’ costs.\nFor each data set, we compare 4 strategies for choosing training examples: passive learning (Passive), costinsensitive greedy policy or least confidence (LC), costaverage greedy policy (AvgLC), and budgeted least confidence (BudgetLC). LC and AvgLC have been discussed in Section 5.2, and BudgetLC is the corresponding policy π1/2 with the version space reduction utility. These three strategies are active learning algorithms. For comparison, we train a logistic regression model with budget 50, 100, 150, and 200, and compute its accuracies on a separate test set. In all the tables, bold numbers indicate the best scores among all the algorithms, and underscores indicate that BudgetLC achieves the second best score among the three active learning algorithms."
    }, {
      "heading" : "6.1. Experiments with Random Costs",
      "text" : "In this setting, costs are put at random to the training examples. We consider 2 scenarios. In the first scenario, some random examples will have a cost drawn from the\ndistribution Gamma(80, 0.1) and the other examples will have cost 1. The results for this scenario are in Table 1, where AvgLC performs better than LC and BudgetLC performs mostly the second best among the active learning algorithms. In the second scenario, all examples with label 1 will have a cost drawn from Gamma(45, 0.1) and the others (examples with label 0) will have cost 1. The results for this scenario are in Table 2, where LC generally performs better than AvgLC. This is because the costs are more biased toward examples with label 0, so AvgLC will prefer examples from that class. In this scenario, BudgetLC also performs mostly the second best among the active learning algorithms, although it is still significantly worse than LC."
    }, {
      "heading" : "6.2. Experiments with Margin-Dependent Costs",
      "text" : "In this setting, costs are put on training examples based on their margins to a classifier trained on the whole data set. Specifically, we first train a logistic regression model on all the data and compute its probabilistic prediction for each training example. The margin of an example is then the scaled distance between 0.5 and its probabilistic prediction.\nWe also consider 2 scenarios. In the first scenario, we put higher costs on examples with lower margins. The results for this scenario are in Table 3, with AvgLC generally performing better than LC. BudgetLC performs better than both AvgLC and LC on data set 2, and performs the second best among the active learning algorithms on data sets 1 and 3. In the second scenario, we put higher costs on examples with larger margins. From the results for this scenario in Table 4, AvgLC performs better than LC on data set 1, while LC performs better than AvgLC on data sets 2 and 3. On all the data sets, BudgetLC generally performs the second best among the active learning algorithms.\nFrom all the experimental results, BudgetLC may be a more robust active learning algorithm in some cases where either LC or AvgLC may perform badly and we do not know which of them is better."
    }, {
      "heading" : "7. Related Work",
      "text" : "Our work is related to (Khuller et al., 1999; Leskovec et al., 2007; Guillory & Bilmes, 2010; Cuong et al., 2014), but we consider a more general case than these works. Cuong et al. (2014) considered a similar worst-case setting as our work, but they assumed the utility is pointwise submodular and the cost function is uniform modular. Our work is more general than theirs in two aspects. First, pointwise cost-sensitive submodularity is a generalization of pointwise submodularity. Second, our cost function is general and may be neither uniform nor modular. These generalizations make the problem more complicated as we have shown in Section 4 that simple greedy policies, which are near-optimal in the uniform modular cost setting (Cuong et al., 2014), will not be near-optimal anymore. Thus, we need to combine two simple greedy policies to obtain a new near-optimal policy.\nGuillory & Bilmes (2010) considered the interactive submodular set cover problem, which also requires the utility to be pointwise submodular. This problem looks for a policy that can achieve at least a certain value of utility with respect to an unknown target realization while at the same time minimizing the cost of this policy. The final utility in this problem is derived from the individual utilities of various realizations and satisfies submodularity. Our work, in contrast, tries to maximize the worst-case utility directly given a cost budget.\nKhuller et al. (1999) considered the budgeted maximum coverage problem, which is the non-adaptive version of our problem with a modular cost. For budgeted maximum coverage, Khuller et al. (1999) showed that the best between two non-adaptive greedy policies can achieve nearoptimality compared to the optimal non-adaptive policy. Similar results were also shown in (Leskovec et al., 2007) for the outbreak detection problem. Our work is a generalization of these works to the adaptive setting with general cost functions, and we can achieve the same constant factor 1 2 (1− 1/e) as theirs. Furthermore, the class of utility functions in our work is even more general than the coverage utilities in (Khuller et al., 1999; Leskovec et al., 2007).\nCost-sensitive submodularity is a generalization of submodularity (Nemhauser & Wolsey, 1978) for general costs. Submodularity has been successfully applied to many applications (Krause & Guestrin, 2007b; 2011; Wei et al., 2015). There are other ways to extend submodularity to the adaptive setting, e.g., adaptive submodularity (Golovin & Krause, 2011) and approximately adaptive submodularity (Kusner, 2014). When the utility is adaptive submodular, Golovin & Krause (2011) proved that the greedy policy that maximizes the average utility gain in each step is near-optimal in both the average and worst cases. However, neither pointwise submodularity implies adaptive submod-\nularity nor vice versa. Thus, our assumptions in this paper, which is more general than the pointwise submodularity assumption, can be applied to a different class of utility functions than those in (Golovin & Krause, 2011).\nThe adaptive optimization problem in our paper is for the worst-case maximum coverage setting, where we look for a policy that maximizes the utility while maintaining its cost within a certain budget. This problem can also be considered in the worst-case min-cost setting (Guillory & Bilmes, 2010), where we look for a policy that can achieve at least a certain value of utility while minimizing the total cost."
    }, {
      "heading" : "8. Conclusion",
      "text" : "We studied worst-case adaptive optimization with budget constraint, where the cost can be a general set function and the utility satisfies pointwise cost-sensitive submodularity. We proved negative results about two intuitive greedy policies for this problem, but also showed a near-optimality result for the best policy between them. We used this result to derive a combined policy that can achieve nearoptimality against the optimal policy that uses half of the budget. We discussed applications of our theoretical results and reported experiments for the greedy policies on the pool-based active learning problem.\nAppendix"
    }, {
      "heading" : "A. Proof of Theorem 1",
      "text" : "A.1. PROOF OF PART (A)\nSince g1 and g2 are cost-sensitively submodular with respect to c, for all A ⊆ B ⊆ X and x ∈ X \\B, we have:\ng1(A ∪ {x})− g1(A) ∆c(x|A) ≥ g1(B ∪ {x})− g1(B) ∆c(x|B) , and\ng2(A ∪ {x})− g2(A) ∆c(x|A) ≥ g2(B ∪ {x})− g2(B) ∆c(x|B) .\nMultiplying α and β into both sides of the first and second inequality respectively, then summing the resulting inequalities, we have:\n(αg1(A ∪ {x}) + βg2(A ∪ {x}))− (αg1(A) + βg2(A)) ∆c(x|A)\n≥ (αg1(B ∪ {x})+βg2(B ∪ {x}))−(αg1(B)+βg2(B)) ∆c(x|B) .\nThus, αg1+βg2 is cost-sensitively submodular with respect to c.\nA.2. PROOF OF PART (B)\nSince g is cost-sensitively submodular with respect to c1, for all A ⊆ B ⊆ X and x ∈ X \\B, we have:\ng(A ∪ {x})− g(A) ∆c1(x|A) ≥ g(B ∪ {x})− g(B) ∆c1(x|B) ,\nwhich implies:\n(g(A ∪ {x})− g(A))(c1(B ∪ {x})− c1(B))\n≥ (g(B ∪ {x})− g(B))(c1(A ∪ {x})− c1(A)).\nMultiplying α into both sides of this inequality, we have:\n(g(A ∪ {x})− g(A))(αc1(B ∪ {x})− αc1(B))\n≥ (g(B ∪ {x})− g(B))(αc1(A ∪ {x})− αc1(A)).\nSimilarly, we also have:\n(g(A ∪ {x})− g(A))(βc2(B ∪ {x})− βc2(B))\n≥ (g(B ∪ {x})− g(B))(βc2(A ∪ {x})− βc2(A)).\nSumming these inequalities, we have:\n(g(A ∪ {x})− g(A))× (αc1(B ∪ {x}) + βc2(B ∪ {x})− αc1(B)− βc2(B)) ≥(g(B ∪ {x})− g(B))×\n(αc1(A ∪ {x}) + βc2(A ∪ {x})− αc1(A)− βc2(A)).\nThus,\ng(A ∪ {x})− g(A) (αc1(A ∪ {x}) + βc2(A ∪ {x}))− (αc1(A) + βc2(A))\n≥ g(B ∪ {x})− g(B) (αc1(B ∪ {x})+βc2(B ∪ {x}))−(αc1(B)+βc2(B)) .\nHence, g is cost-sensitively submodular with respect to αc1 + βc2.\nA.3. PROOF OF PARTS (C) AND (D)\nFirst, we prove the following lemma.\nLemma 1. For any integer k ≥ 1, if c(S) = (g(S))k for all S ⊆ X and g is monotone, then g is cost-sensitively submodular with respect to c.\nProof. If k = 1, this trivially holds. If k ≥ 2, for all A ⊆ B ⊆ X and x ∈ X \\B, we have:\ng(A ∪ {x})− g(A) ∆c(x|A) = g(A ∪ {x})− g(A) (g(A ∪ {x}))k − (g(A))k\n= 1∑k−1\ni=0 (g(A ∪ {x}))k−1−i(g(A))i .\nSimilarly,\ng(B ∪ {x})− g(B) ∆c(x|B) = 1∑k−1\ni=0 (g(B ∪ {x}))k−1−i(g(B))i .\nSince g is monotone, g(A ∪ {x}) ≤ g(B ∪ {x}) and g(A) ≤ g(B). Thus, ∑k−1 i=0 (g(A ∪ {x}))k−1−i(g(A))i ≤∑k−1\ni=0 (g(B ∪ {x}))k−1−i(g(B))i. Hence,\ng(A ∪ {x})− g(A) ∆c(x|A) ≥ g(B ∪ {x})− g(B) ∆c(x|B) ,\nwhich implies that g is cost-sensitively submodular with respect to c.\nApplying part (b) and Lemma 1, we can easily see that part (c) holds. Furthermore, from parts (b), (c), and the Taylor approximation of eg(S), part (d) also holds."
    }, {
      "heading" : "B. Proof of Theorem 2",
      "text" : "We construct a worst-case adaptive optimization problem that satisfies Theorem 2. In this problem, the utility function and the cost function are both modular, i.e., they can be decomposed into the sum of the utilities (or costs) of the individual items. Besides, all the items have only one state, so it is essentially a non-adaptive problem.\nConsider the utility function: f(S, h) = ∑ x∈S w(x, h(x)), (3)\nwhere w : X × Y → R≥0 is the utility function for one item. Intuitively, w(x, y) is the utility obtained by selecting item xwith state y, and f(S, h) is the sum of all the utilities of the items in S with states according to h. It is easy to see that f is pointwise monotone and also satisfies minimal dependency.\nWe now consider the worst-case adaptive optimization problem with two items {x1, x2} and one state {0} such that w(x1, 0) = 1 and w(x2, 0) = p, for some p > 1. Let the cost function be:\nc(∅) = 0, c({x1}) = 1, c({x2}) = p+ 1,\nc({x1, x2}) = c({x1}) + c({x2}) = p+ 2.\nIt is easy to check that the utility f is pointwise costsensitively submodular with respect to this cost function. We let the budget be K = p+ 1. With this budget, a policy is only allowed to select at most one item.\nFor this problem, the policy π1 would select the item x1 because:\nδ(x1|∅) c({x1})− c(∅) = miny f({x1}, {(x1, y)}) c({x1}) = 1\n> p\np+ 1 = miny f({x2}, {(x2, y)}) c({x2}) = δ(x2|∅) c({x2})− c(∅) .\nThus, fworst(π1) = 1. However, the optimal policy π∗ would select x2 to obtain fworst(π∗) = p. Hence, fworst(π1)/fworst(π\n∗) = 1/p. By increasing p, we can have fworst(π1)/fworst(π ∗) < α for any α > 0."
    }, {
      "heading" : "C. Proof of Theorem 3",
      "text" : "We construct a worst-case adaptive optimization problem that satisfies Theorem 3. Similar to the proof of Theorem 2, the utility and cost here are also modular and the items also have only one state.\nIn particular, we consider the worst-case adaptive optimization problem with n + 1 items {x0, x1, . . . , xn} and one state {0}. We will use the utility function f defined by Equation (3) in the proof of Theorem 2 with w(x0, 0) = 2 and w(xi, 0) = 1 for i = 1, . . . , n.\nLet the cost function be:\nc({x0}) = n, c({xi}) = 1, for i = 1, . . . , n,\nand c(S) = ∑ x∈S c({x}), for other subsets of items S.\nSimilar to the proof of Theorem 2, the utility function f is pointwise cost-sensitively submodular with respect to the cost function above. It is also pointwise monotone and satisfies minimal dependency.\nWe let the budget be K = n. With this budget, a policy may select exactly one item x0, or it may ignore x0 and select only the items among {x1, . . . , xn}.\nFor this problem, the policy π2 would select the item x0 because for any i = 1, . . . , n:\nδ(x0|∅) = min y f({x0}, {(x0, y)}) = 2\n> 1 = min y f({xi}, {(xi, y)}) = δ(xi|∅).\nThus, fworst(π2) = 2. However, the optimal policy π∗ would select all the items {x1, . . . , xn} to obtain fworst(π\n∗) = n. Hence, fworst(π2)/fworst(π∗) = 2/n. By increasing n, we can have fworst(π2)/fworst(π∗) < α for any α > 0."
    }, {
      "heading" : "D. Proof of Theorem 4",
      "text" : "Without loss of generality, we assume that each item can be selected by at least one policy given the budget K; otherwise, we can simply remove that item from the item set. First, we consider the policy π1. Let h1 = arg minh f(x π1 h , h) be the worst-case realization of π1. We have fworst(π1) = f(xπ1h1 , h1). Note that h1 corresponds to a path from the root to a leaf of the policy tree\nof π1, and let the items and states along this path (starting from the root) be:\nh1 = {(x1, y1), (x2, y2), . . . , (x|h1|, y|h1|)}.\nAt any item xi along the path h1, imagine that we run the optimal policy π∗ right after selecting xi and then follow the paths consistent with {(x1, y1), . . . , (xi, yi)} down to a leaf of the policy tree of π∗. We consider the following adversary’s path ha = {(xa1 , ya1 ), (xa2 , ya2 ), . . . , (xa|ha|, y a |ha|)} in the policy tree of π∗ that satisfies:\nyaj = arg min y {f({xt}it=1 ∪ {xat } j−1 t=1 ∪ {xaj },\n{yt}it=1 ∪ {yat } j−1 t=1 ∪ {y})\n−f({xt}it=1 ∪ {xat } j−1 t=1 , {yt}it=1 ∪ {yat } j−1 t=1 )}\nif xaj has not appeared in {x1, . . . , xi}. Otherwise, yaj = yt if xaj = xt for some t = 1, . . . , i. In the above, since f satisfies minimal dependency, we write f({xt}it=1, {yt}it=1) to denote the utility obtained after observing {(xt, yt)}it=1.\nAssume we follow the path h1 during the execution of π1. Let r be the number of iterations (the repeat loop) executed in the algorithm for π1 (see Algorithm 1) until the first time an item in the corresponding adversary’s path is considered, but not added to D due to the cost budget. Let (x1, y1), . . . , (xl, yl) be the items selected (i.e., added to D) along the path h1 until iteration r. Furthermore, let xl+1 be the item in the corresponding adversary’s path (i.e., the adversary’s path right after selecting xl) that is considered but not added to D. Consider an arbitrary state yl+1 for xl+1. Also let ji be the iteration where xi (1 ≤ i ≤ l + 1) is considered. For i = 1, 2, . . . , l + 1, define:\nui = f({xt}it=1, {yt}it=1)− f({xt}i−1t=1, {yt} i−1 t=1),\nvi = i∑ t=1 ut, and zi = fworst(π∗)− vi.\nWe first prove the following lemma.\nLemma 2. For i = 1, . . . , l+ 1, after each iteration ji, we have ui ≥ ∆c(xi|{xt}i−1t=1)\nK zi−1.\nProof. For i = 1, . . . , l + 1 and j = 1, . . . , |ha| (note that ha depends on i), we have:\nui\n∆c(xi|{xt}i−1t=1) = f({xt}it=1, {yt}it=1)− f({xt}i−1t=1, {yt} i−1 t=1)\nc({xt}it=1)− c({xt} i−1 t=1)\n≥ min y\nf({xt}i−1t=1∪{xi},{yt} i−1 t=1∪{y})−f({xt} i−1 t=1,{yt} i−1 t=1)\nc({xt}it=1)− c({xt} i−1 t=1)\n≥ min y\nf({xt}i−1t=1∪{x a j },{yt} i−1 t=1∪{y})−f({xt} i−1 t=1,{yt} i−1 t=1)\nc({xt}i−1t=1 ∪ {xaj })− c({xt} i−1 t=1)\n≥ min y\nf({xt}i−1t=1∪{x a t } j−1 t=1∪{x a j },{yt} i−1 t=1∪{y a t } j−1 t=1∪{y})\n−f({xt}i−1t=1∪{x a t } j−1 t=1 ,{yt} i−1 t=1∪{y a t } j−1 t=1 )\nc({xt}i−1t=1∪{x a t } j−1 t=1∪{x a j })−c({xt} i−1 t=1∪{x a t } j−1 t=1 )\n=\nf({xt}i−1t=1∪{x a t } j t=1,{yt} i−1 t=1∪{y a t } j t=1)\n−f({xt}i−1t=1∪{x a t } j−1 t=1 ,{yt} i−1 t=1∪{y a t } j−1 t=1 )\nc({xt}i−1t=1 ∪ {xat } j t=1)− c({xt} i−1 t=1 ∪ {xat } j−1 t=1 )\n,\nwhere the first equality is from the definition of ui, the second inequality is from the greedy criterion and assumption of xl+1, the third inequality is from the pointwise costsensitive submodularity of f and c, and the last equality is from the definition of yaj .\nThus,\nzi−1\n= fworst(π ∗)− vi−1 ≤ f({xt}i−1t=1 ∪ {xat } |ha| t=1 , {yt} i−1 t=1 ∪ {yat } |ha| t=1)\n− f({xt}i−1t=1, {yt} i−1 t=1)\n= |ha|∑ j=1 (f({xt}i−1t=1 ∪ {xat } j t=1, {yt} i−1 t=1 ∪ {yat } j t=1)\n− f({xt}i−1t=1 ∪ {xat } j−1 t=1 , {yt} i−1 t=1 ∪ {yat } j−1 t=1 ))\n≤ |ha|∑ j=1 ui(c({xt}i−1t=1 ∪ {xat } j t=1)− c({xt} i−1 t=1 ∪ {xat } j−1 t=1 )) ∆c(xi|{xt}i−1t=1) = ui\n∆c(xi|{xt}i−1t=1) ×\n|ha|∑ j=1 (c({xt}i−1t=1 ∪ {xat } j t=1)− c({xt} i−1 t=1 ∪ {xat } j−1 t=1 ))\n= ui\n∆c(xi|{xt}i−1t=1) (c({xt}i−1t=1 ∪ {xat } |ha| t=1)− c({xt} i−1 t=1))\n≤ ui ∆c(xi|{xt}i−1t=1) c({xat } |ha| t=1) ≤ ui ∆c(xi|{xt}i−1t=1) K.\nIn the above, the first inequality is from the definition of vi−1 and the pointwise monotonicity of f , the second inequality is from the previous discussion, the third inequality is from the triangle inequality for c, and the last inequal-\nity is from the fact that ha is a path of π∗, whose cost is at most K. Thus, Lemma 2 holds.\nUsing Lemma 2, we now prove the next lemma.\nLemma 3. For i = 1, . . . , l+ 1, after each iteration ji, we have:\nvi ≥ [ 1−\ni∏ t=1\n( 1−\n∆c(xt|{xj}t−1j=1) K\n)] fworst(π ∗).\nProof. We prove this lemma by induction.\nBase case: For i = 1, consider the path hb = {(xb1, yb1), (xb2, yb2), . . . , (xb|hb|, y b |hb|)} in the policy tree of π∗ that satisfies ybi = arg miny f({xbi}, {y}). For all i = 1, 2, . . . , |hb|, we have:\nf({x1}, {y1}) c({x1}) = f({x1}, {y1})− f(∅, ∅)\nc({x1})− c(∅)\n≥ miny{f({x1}, {y})− f(∅, ∅)} c({x1})− c(∅) ≥ miny{f({x b i}, {y})− f(∅, ∅)}\nc({xbi})− c(∅)\n= f({xbi}, {ybi })− f(∅, ∅)\nc({xbi})− c(∅)\n≥ f({xbj}ij=1, {ybj}ij=1)− f({xbj} i−1 j=1, {ybj} i−1 j=1)\nc({xbj}ij=1)− c({xbj} i−1 j=1)\n.\nIn the above, the first equality is due to f(∅, ∅) = 0 and c(∅) = 0, the second inequality is due to the greedy criterion of π1, the second equality is from the definition of ybi , and the last inequality is from the cost-sensitive submodularity of f .\nThus, for all i = 1, 2, . . . , |hb|,\nf({x1}, {y1}) c({x1}) (c({xbj}ij=1)− c({xbj}i−1j=1)) ≥ f({xbj}ij=1, {ybj}ij=1)− f({xbj}i−1j=1, {y b j}i−1j=1).\nSumming the above inequality for all i, we have:\nf({x1}, {y1}) c({x1}) c({xbj} |hb| j=1) ≥ f({x b j} |hb| j=1, {y b j} |hb| j=1).\nSince hb is a path of π∗, we have c({xbj} |hb| j=1) ≤ K and f({xbj} |hb| j=1, {ybj} |hb| j=1) ≥ fworst(π∗). Thus, v1 = f({x1}, {y1}) ≥ c({x1})K fworst(π ∗) and the base case holds.\nInductive step: Now assume the lemma holds for i − 1. We have:\nvi = vi−1 + ui\n≥ vi−1 + ∆c(xi|{xt}i−1t=1)\nK (fworst(π\n∗)− vi−1)\n= (1− ∆c(xi|{xt} i−1 t=1)\nK )vi−1\n+ ∆c(xi|{xt}i−1t=1)\nK fworst(π\n∗)\n≥ (1− ∆c(xi|{xt} i−1 t=1)\nK )×[\n1− i−1∏ t=1\n( 1−\n∆c(xt|{xj}t−1j=1) K\n)] fworst(π ∗)\n+ ∆c(xi|{xt}i−1t=1)\nK fworst(π\n∗)\n= [ 1−\ni∏ t=1\n( 1−\n∆c(xt|{xj}t−1j=1) K\n)] fworst(π ∗),\nwhere the first inequality is from Lemma 2 and the second inequality is from the inductive hypothesis.\nNow we prove Theorem 4. Applying Lemma 3 to iteration jl+1, we have:\nvl+1 ≥ [ 1−\nl+1∏ t=1\n( 1−\n∆c(xt|{xj}t−1j=1) K\n)] fworst(π ∗)\n≥ [ 1−\nl+1∏ t=1\n( 1−\n∆c(xt|{xj}t−1j=1)∑l+1 i=1 ∆c(xi|{xj} i−1 j=1)\n)] ×\nfworst(π ∗)\n≥ [ 1− ( 1− 1\nl + 1\n)l+1] fworst(π ∗)\n> ( 1− 1\ne\n) fworst(π ∗).\nThe second inequality is due to ∑l+1 i=1 ∆c(xi|{xj} i−1 j=1) = c({x1, . . . , xl+1}) > K, and the third inequality is due to the fact that the function 1− ∏n t=1 ( 1− at∑n\ni=1 ai\n) achieves\nits minimum when a1 = . . . = an = ∑ i ai n . Hence,\nvl + ul+1 = vl+1 >\n( 1− 1\ne\n) fworst(π ∗).\nNow consider the first item x selected by the policy π2. Let yw be the state of x in the worst-case path of the policy tree of π2. In the previous arguments, note that yl+1 can be arbitrary, thus without loss of generality, we can set yl+1 =\narg miny f({xl+1}, {y}). Now we have:\nfworst(π2)\n≥ f({x}, {yw}) ≥ min y f({x}, {y})\n≥ min y f({xl+1}, {y}) = f({xl+1}, {yl+1})\n≥ f({xt} l+1 t=1, {yt} l+1 t=1)− f({xt}lt=1, {yt}lt=1)\nc({xt}l+1t=1)− c({xt}lt=1) c({xl+1})\n≥ ul+1,\nwhere the first inequality is from the pointwise monotonicity of f , the third inequality is from the greedy criterion of π2, the fourth inequality is from the pointwise costsensitive submodularity of f , and the last inequality is from the triangle inequality for c.\nFurthermore, fworst(π1) ≥ vl due to the pointwise monotonicity of f and vl is computed along the worst-case path of π1. Hence,\nfworst(π1) + fworst(π2) >\n( 1− 1\ne\n) fworst(π ∗).\nTherefore, fworst(π) = max{fworst(π1), fworst(π2)} > 1\n2 (1− 1 e )fworst(π ∗).\nFrom this proof, we can easily see that the theorem still holds if we replace the policy π2 with only the first item x that it selects. In other words, we can terminate the policy π2 right after it selects the first item and the near-optimality is still guaranteed."
    }, {
      "heading" : "E. Proof of Theorem 5",
      "text" : "Let h1/2 = arg minh f(x π1/2 h , h) be the worst-case realization of π1/2. We have fworst(π1/2) = f(x π1/2 h1/2 , h1/2). Note that x π1/2 h1/2 = xπ1h1/2 ∪ x π2 h1/2 , where xπ1h1/2 and x π2 h1/2 are the sets selected by π1 and π2 respectively in the policy π1/2 under the realization h1/2. Thus, fworst(π1/2) ≥ f(xπ1h1/2 , h1/2) and fworst(π1/2) ≥ f(x π2 h1/2\n, h1/2) due to the pointwise monotonicity of f .\nFrom the definition of fworst, we have f(xπ1h1/2 , h1/2) ≥ minh f(x π1 h , h) = fworst(π1). Hence, fworst(π1/2) ≥ fworst(π1). Similarly, fworst(π1/2) ≥ fworst(π2). From Theorem 4, either fworst(π1) or fworst(π2) must be greater than 1 2 (1− 1/e) fworst(π∗1/2). Therefore, Theorem 5 holds."
    }, {
      "heading" : "F. Discussion on π1 versus π∗1/2",
      "text" : "In this section, we show that it is not possible to construct a counter-example for the policy π1 with full budget compared to π∗1/2 if we use the simple utility and modular cost functions in the proofs of Theorems 2 and 3. This means\nwe will prove that π1 provides a constant factor approximation to π∗1/2 for those utility and modular cost functions. We state and prove this result in the proposition below. Proposition 1. For any utility function f(S, h) def=∑ x∈S w(x, h(x)), where w : X × Y → R≥0, and any\nmodular cost function c such that c(S) = ∑ x∈S c({x}),\nfworst(π1) > 1\n2\n( 1− 1\ne\n) fworst(π ∗ 1/2),\nwhere π1 is run with budget K and π∗1/2 is the optimal worst-case policy with budget K/2.\nProof. For this utility, note that the realization h∗(x) def= arg miny w(x, y) is always the worst-case realization of any policy. Besides, δ(x|D) = w(x, h∗(x)), which means the greedy criterion in policy π1 would always consider the state h∗(x) instead of other states. So, we can fix the realization h∗ in all of our following arguments.\nNow assume that we run π1 with budget K/2 and select x′1, x ′ 2, . . . , x ′ t, x ′ t+1, . . . , x ′ T , while at the same time we run π1 with budget K and select x′1, x ′ 2, . . . , x ′ t, xt+1, where xt+1 is the first item selected by π1 with budget K but could not be selected with budget K/2 due to the budget constraint. From the greedy criterion of π1, it is easy to see that:\nw(xt+1, h ∗(xt+1))\nc({xt+1}) ≥ w(x\n′ i, h ∗(x′i))\nc({x′i}) , for i = t+1, . . . , T.\nThus,\nT∑ i=t+1 w(x′i, h ∗(x′i)) ≤ w(xt+1, h ∗(xt+1)) c({xt+1}) T∑ i=t+1 c({x′i})\n≤ w(xt+1, h∗(xt+1)),\nwhich is due to the fact that ∑T i=t+1 c({x′i}) = c({x′i}Ti=t+1) < c(xt+1). This implies that:\nfworst(π1 with budget K) ≥ fworst(π1 with budget K/2).\nNow let xπ2 be the first item selected if we run π2 with budget K/2. If xπ2 ∈ {x′1, x′2, . . . , x′t, xt+1}, then fworst(π1 with budget K) ≥ w(xπ2 , h∗(xπ2)). If xπ2 /∈ {x′1, x′2, . . . , x′t, xt+1}, then\nw(x′i, h ∗(x′i))\nc({x′i}) ≥ w(xπ2 , h\n∗(xπ2))\nc({xπ2}) , for i = 1, . . . , t\nand w(xt+1, h\n∗(xt+1))\nc({xt+1}) ≥ w(xπ2 , h\n∗(xπ2))\nc({xπ2}) .\nThus, t∑ i=1 w(x′i, h ∗(x′i)) + w(xt+1, h ∗(xt+1))\n≥ w(xπ2 , h ∗(xπ2))\nc({xπ2})\n( t∑ i=1 c({x′i}) + c({xt+1}) ) ≥ w(xπ2 , h∗(xπ2)).\nThis is due to the fact that ∑t i=1 c({x′i}) + c({xt+1}) > K/2 ≥ c({xπ2}). Hence, fworst(π1 with budget K) ≥ w(xπ2 , h ∗(xπ2)).\nSince we always have fworst(π1 with budget K) ≥ max{fworst(π1 with budget K/2), w(xπ2 , h∗(xπ2))}, from the proof of Theorem 4, this implies\nfworst(π1 with budget K) > 1\n2 (1− 1/e)fworst(π∗1/2),\nand the proposition holds.\nG. Proof of Equation (2)\nLet YD be the labels of the items in XD. We have:\nx∗ = arg max x δ(x|D)/∆c(x|XD)\n= arg max x\nmin y∈Y {f(XD ∪ {x},D ∪ {(x, y)})− f(XD,D)}\n∆c(x|XD)\n= arg max x\nmin y∈Y {p0[YD;XD]− p0[YD ∪ {y};XD ∪ {x}]}\n∆c(x|XD)\n= arg max x\nmin y∈Y {1− p0[YD ∪ {y};XD ∪ {x}] p0[YD;XD] }\n∆c(x|XD)\n= arg max x miny∈Y{1− pD[y;x]} ∆c(x|XD) .\nIn the above, the first two equalities are from the definitions of x∗ and δ(x|D), the third equality is from the definition of the version space reduction utility f , the fourth equality is obtained from dividing the numerator by the constant p0[YD;XD], and the last equality is due to the definition of the posterior pD[y;x]."
    } ],
    "references" : [ {
      "title" : "Stochastic submodular maximization",
      "author" : [ "Asadpour", "Arash", "Nazerzadeh", "Hamid", "Saberi", "Amin" ],
      "venue" : "In Internet and Network Economics,",
      "citeRegEx" : "Asadpour et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Asadpour et al\\.",
      "year" : 2008
    }, {
      "title" : "Active learning for probabilistic hypotheses using the maximum Gibbs error criterion",
      "author" : [ "Cuong", "Nguyen Viet", "Lee", "Wee Sun", "Ye", "Nan", "Chai", "Kian Ming A", "Chieu", "Hai Leong" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Cuong et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Cuong et al\\.",
      "year" : 2013
    }, {
      "title" : "Nearoptimal adaptive pool-based active learning with general loss",
      "author" : [ "Cuong", "Nguyen Viet", "Lee", "Wee Sun", "Ye", "Nan" ],
      "venue" : "In Conference on Uncertainty in Artificial Intelligence (UAI),",
      "citeRegEx" : "Cuong et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Cuong et al\\.",
      "year" : 2014
    }, {
      "title" : "Robustness of Bayesian pool-based active learning against prior misspecification",
      "author" : [ "Cuong", "Nguyen Viet", "Ye", "Nan", "Lee", "Wee Sun" ],
      "venue" : "In AAAI Conference on Artificial Intelligence (AAAI),",
      "citeRegEx" : "Cuong et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Cuong et al\\.",
      "year" : 2016
    }, {
      "title" : "Approximating the stochastic knapsack problem: The benefit of adaptivity",
      "author" : [ "Dean", "Brian C", "Goemans", "Michel X", "J. Vondrdk" ],
      "venue" : "In Annual IEEE Symposium on Foundations of Computer Science (FOCS),",
      "citeRegEx" : "Dean et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Dean et al\\.",
      "year" : 2004
    }, {
      "title" : "Adaptive submodularity: Theory and applications in active learning and stochastic optimization",
      "author" : [ "Golovin", "Daniel", "Krause", "Andreas" ],
      "venue" : "Journal of Artificial Intelligence Research (JAIR),",
      "citeRegEx" : "Golovin et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Golovin et al\\.",
      "year" : 2011
    }, {
      "title" : "Interactive submodular set cover",
      "author" : [ "Guillory", "Andrew", "Bilmes", "Jeff" ],
      "venue" : "In International Conference on Machine Learning (ICML), pp",
      "citeRegEx" : "Guillory et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Guillory et al\\.",
      "year" : 2010
    }, {
      "title" : "A probabilistic analysis of the Rocchio algorithm with TFIDF for text categorization",
      "author" : [ "Joachims", "Thorsten" ],
      "venue" : "DTIC Document,",
      "citeRegEx" : "Joachims and Thorsten.,? \\Q1996\\E",
      "shortCiteRegEx" : "Joachims and Thorsten.",
      "year" : 1996
    }, {
      "title" : "The budgeted maximum coverage problem",
      "author" : [ "Khuller", "Samir", "Moss", "Anna", "Naor", "Joseph Seffi" ],
      "venue" : "Information Processing Letters,",
      "citeRegEx" : "Khuller et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Khuller et al\\.",
      "year" : 1999
    }, {
      "title" : "Near-optimal observation selection using submodular functions",
      "author" : [ "Krause", "Andreas", "Guestrin", "Carlos" ],
      "venue" : "In Conference on Artificial Intelligence (AAAI),",
      "citeRegEx" : "Krause et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Krause et al\\.",
      "year" : 2007
    }, {
      "title" : "Nonmyopic active learning of Gaussian processes: An explorationexploitation approach",
      "author" : [ "Krause", "Andreas", "Guestrin", "Carlos" ],
      "venue" : "In International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Krause et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Krause et al\\.",
      "year" : 2007
    }, {
      "title" : "Submodularity and its applications in optimized information gathering",
      "author" : [ "Krause", "Andreas", "Guestrin", "Carlos" ],
      "venue" : "ACM Transactions on Intelligent Systems and Technology (TIST),",
      "citeRegEx" : "Krause et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Krause et al\\.",
      "year" : 2011
    }, {
      "title" : "Approximately adaptive submodular maximization",
      "author" : [ "Kusner", "Matt J" ],
      "venue" : "In NIPS Workshop on Discrete and Combinatorial Problems in Machine Learning,",
      "citeRegEx" : "Kusner and J.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kusner and J.",
      "year" : 2014
    }, {
      "title" : "Cost-effective outbreak detection in networks",
      "author" : [ "Leskovec", "Jure", "Krause", "Andreas", "Guestrin", "Carlos", "Faloutsos", "Christos", "VanBriesen", "Jeanne", "Glance", "Natalie" ],
      "venue" : "In ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD),",
      "citeRegEx" : "Leskovec et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Leskovec et al\\.",
      "year" : 2007
    }, {
      "title" : "Employing EM and pool-based active learning for text classification",
      "author" : [ "McCallum", "Andrew", "Nigam", "Kamal" ],
      "venue" : "In International Conference on Machine Learning (ICML),",
      "citeRegEx" : "McCallum et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "McCallum et al\\.",
      "year" : 1998
    }, {
      "title" : "Best algorithms for approximating the maximum of a submodular set function",
      "author" : [ "G.L. Nemhauser", "L.A. Wolsey" ],
      "venue" : "Mathematics of Operations Research,",
      "citeRegEx" : "Nemhauser and Wolsey,? \\Q1978\\E",
      "shortCiteRegEx" : "Nemhauser and Wolsey",
      "year" : 1978
    }, {
      "title" : "Submodularity in data subset selection and active learning",
      "author" : [ "Wei", "Kai", "Iyer", "Rishabh", "Bilmes", "Jeff" ],
      "venue" : "In International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Wei et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Wei et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "As another example, in the pool-based active learning problem (McCallum & Nigam, 1998; Cuong et al., 2016), one needs to sequentially select unlabeled examples and query their labels, taking into account the previously observed labels.",
      "startOffset" : 62,
      "endOffset" : 106
    }, {
      "referenceID" : 4,
      "context" : "Adaptive optimization with budget constraint has been previously studied in the average case (Dean et al., 2004; Asadpour et al., 2008; Golovin & Krause, 2011).",
      "startOffset" : 93,
      "endOffset" : 159
    }, {
      "referenceID" : 0,
      "context" : "Adaptive optimization with budget constraint has been previously studied in the average case (Dean et al., 2004; Asadpour et al., 2008; Golovin & Krause, 2011).",
      "startOffset" : 93,
      "endOffset" : 159
    }, {
      "referenceID" : 2,
      "context" : "If the cost function is uniform and modular, it is known that these two policies are equivalent and are near-optimal (Cuong et al., 2014).",
      "startOffset" : 117,
      "endOffset" : 137
    }, {
      "referenceID" : 2,
      "context" : "Our proof for this result is built upon the proof techniques for worst-case adaptive optimization with uniform modular costs (Cuong et al., 2014) and non-adaptive optimization with non-uniform modular costs (Khuller et al.",
      "startOffset" : 125,
      "endOffset" : 145
    }, {
      "referenceID" : 8,
      "context" : ", 2014) and non-adaptive optimization with non-uniform modular costs (Khuller et al., 1999) but goes beyond them to handle general, possibly non-uniform and non-modular, costs.",
      "startOffset" : 69,
      "endOffset" : 91
    }, {
      "referenceID" : 2,
      "context" : "Pointwise submodularity is an extension of submodularity to the adaptive setting and it has been used for active learning (Guillory & Bilmes, 2010; Cuong et al., 2014).",
      "startOffset" : 122,
      "endOffset" : 167
    }, {
      "referenceID" : 2,
      "context" : "Pointwise monotonicity and minimal dependency were also assumed in (Cuong et al., 2014) for the modular and uniform cost setting.",
      "startOffset" : 67,
      "endOffset" : 87
    }, {
      "referenceID" : 2,
      "context" : "Proving this theorem requires a sophisticated combination of the proof techniques for worst-case adaptive optimization with uniform modular costs (Cuong et al., 2014) and non-adaptive optimization with non-uniform modular costs (Khuller et al.",
      "startOffset" : 146,
      "endOffset" : 166
    }, {
      "referenceID" : 8,
      "context" : ", 2014) and non-adaptive optimization with non-uniform modular costs (Khuller et al., 1999).",
      "startOffset" : 69,
      "endOffset" : 91
    }, {
      "referenceID" : 8,
      "context" : "Unlike the proof in (Khuller et al., 1999), our proof deals with policy trees instead of sets.",
      "startOffset" : 20,
      "endOffset" : 42
    }, {
      "referenceID" : 8,
      "context" : "The constant factor 12 (1 − 1/e) in Theorem 4 is the same as the constant factor for the non-adaptive budgeted maximum coverage problem (Khuller et al., 1999).",
      "startOffset" : 136,
      "endOffset" : 158
    }, {
      "referenceID" : 2,
      "context" : "Although this implies the greedy policy is near-optimal, the constant factor 12 (1− 1/e) in this case is not as good as the constant factor (1 − 1/e) in (Cuong et al., 2014) for the uniform modular cost setting.",
      "startOffset" : 153,
      "endOffset" : 173
    }, {
      "referenceID" : 8,
      "context" : "Thus, the adaptive setting in our paper is more difficult than the non-adaptive setting considered in previous works (Khuller et al., 1999; Leskovec et al., 2007).",
      "startOffset" : 117,
      "endOffset" : 162
    }, {
      "referenceID" : 13,
      "context" : "Thus, the adaptive setting in our paper is more difficult than the non-adaptive setting considered in previous works (Khuller et al., 1999; Leskovec et al., 2007).",
      "startOffset" : 117,
      "endOffset" : 162
    }, {
      "referenceID" : 1,
      "context" : "If we consider a Bayesian setting with some prior on the set of realizations (Golovin & Krause, 2011; Cuong et al., 2013; 2016), we can sample a subset of realizations from the prior to estimate fworst.",
      "startOffset" : 77,
      "endOffset" : 127
    }, {
      "referenceID" : 1,
      "context" : "For poolbased active learning, previous works (Golovin & Krause, 2011; Cuong et al., 2013; 2014) have shown that the version space reduction utility is pointwise monotone submodular and satisfies minimal dependency.",
      "startOffset" : 46,
      "endOffset" : 96
    }, {
      "referenceID" : 2,
      "context" : "Theorem 5 can also be applied if we consider the total generalized version space reduction utility (Cuong et al., 2014) that incorporates an arbitrary loss.",
      "startOffset" : 99,
      "endOffset" : 119
    }, {
      "referenceID" : 2,
      "context" : "This utility was also shown to be pointwise monotone submodular and satisfy minimal dependency (Cuong et al., 2014), and thus the theorem still holds in this case for modular costs.",
      "startOffset" : 95,
      "endOffset" : 115
    }, {
      "referenceID" : 8,
      "context" : "Our work is related to (Khuller et al., 1999; Leskovec et al., 2007; Guillory & Bilmes, 2010; Cuong et al., 2014), but we consider a more general case than these works.",
      "startOffset" : 23,
      "endOffset" : 113
    }, {
      "referenceID" : 13,
      "context" : "Our work is related to (Khuller et al., 1999; Leskovec et al., 2007; Guillory & Bilmes, 2010; Cuong et al., 2014), but we consider a more general case than these works.",
      "startOffset" : 23,
      "endOffset" : 113
    }, {
      "referenceID" : 2,
      "context" : "Our work is related to (Khuller et al., 1999; Leskovec et al., 2007; Guillory & Bilmes, 2010; Cuong et al., 2014), but we consider a more general case than these works.",
      "startOffset" : 23,
      "endOffset" : 113
    }, {
      "referenceID" : 2,
      "context" : "These generalizations make the problem more complicated as we have shown in Section 4 that simple greedy policies, which are near-optimal in the uniform modular cost setting (Cuong et al., 2014), will not be near-optimal anymore.",
      "startOffset" : 174,
      "endOffset" : 194
    }, {
      "referenceID" : 1,
      "context" : ", 2007; Guillory & Bilmes, 2010; Cuong et al., 2014), but we consider a more general case than these works. Cuong et al. (2014) considered a similar worst-case setting as our work, but they assumed the utility is pointwise submodular and the cost function is uniform modular.",
      "startOffset" : 33,
      "endOffset" : 128
    }, {
      "referenceID" : 13,
      "context" : "Similar results were also shown in (Leskovec et al., 2007) for the outbreak detection problem.",
      "startOffset" : 35,
      "endOffset" : 58
    }, {
      "referenceID" : 8,
      "context" : "Furthermore, the class of utility functions in our work is even more general than the coverage utilities in (Khuller et al., 1999; Leskovec et al., 2007).",
      "startOffset" : 108,
      "endOffset" : 153
    }, {
      "referenceID" : 13,
      "context" : "Furthermore, the class of utility functions in our work is even more general than the coverage utilities in (Khuller et al., 1999; Leskovec et al., 2007).",
      "startOffset" : 108,
      "endOffset" : 153
    }, {
      "referenceID" : 16,
      "context" : "Submodularity has been successfully applied to many applications (Krause & Guestrin, 2007b; 2011; Wei et al., 2015).",
      "startOffset" : 65,
      "endOffset" : 115
    }, {
      "referenceID" : 16,
      "context" : "Submodularity has been successfully applied to many applications (Krause & Guestrin, 2007b; 2011; Wei et al., 2015). There are other ways to extend submodularity to the adaptive setting, e.g., adaptive submodularity (Golovin & Krause, 2011) and approximately adaptive submodularity (Kusner, 2014). When the utility is adaptive submodular, Golovin & Krause (2011) proved that the greedy policy that maximizes the average utility gain in each step is near-optimal in both the average and worst cases.",
      "startOffset" : 98,
      "endOffset" : 363
    } ],
    "year" : 2017,
    "abstractText" : "We study the worst-case adaptive optimization problem with budget constraint. Unlike previous works, we consider the general setting where the cost is a set function on sets of decisions. For this setting, we investigate the near-optimality of greedy policies when the utility function satisfies a novel property called pointwise cost-sensitive submodularity. This property is an extension of cost-sensitive submodularity, which in turn is a generalization of submodularity to general cost functions. We prove that two simple greedy policies for the problem are not near-optimal but the best between them is near-optimal. With this result, we propose a combined policy that is nearoptimal with respect to the optimal worst-case policy that uses half of the budget. We discuss applications of our theoretical results and also report experimental results comparing the greedy policies on the active learning problem.",
    "creator" : "TeX"
  }
}