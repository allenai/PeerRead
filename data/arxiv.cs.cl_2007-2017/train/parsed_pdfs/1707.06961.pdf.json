{
  "name" : "1707.06961.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Mimicking Word Embeddings using Subword RNNs",
    "authors" : [ "Yuval Pinter", "Robert Guthrie", "Jacob Eisenstein" ],
    "emails" : [ "uvp@gatech.edu", "rguthrie3@gatech.edu", "jacobe@gatech.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "One of the key advantages of word embeddings for natural language processing is that they enable generalization to words that are unseen in labeled training data, by embedding lexical features from large unlabeled datasets into a relatively low-dimensional Euclidean space. These low-dimensional embeddings are typically trained to capture distributional similarity, so that information can be shared among words that tend to appear in similar contexts.\nHowever, it is not possible to enumerate the entire vocabulary of any language, and even large unlabeled datasets will miss terms that appear in later applications. The issue of how to handle these out-of-vocabulary (OOV) words poses challenges for embedding-based methods. These challenges are particularly acute when working with lowresource languages, where even unlabeled data may be difficult to obtain at scale. A typical solution is to abandon hope, by assigning a single OOV embedding to all terms that do not appear in the unlabeled data.\nWe approach this challenge from a quasigenerative perspective. Knowing nothing of a word except for its embedding and its written form, we attempt to learn the former from the latter. We train a recurrent neural network (RNN) on the character level with the embedding as the target, and use it later to predict vectors for OOV words in any downstream task. We call this model the MIMICK-RNN, for its ability to read a word’s spelling and mimick its distributional embedding.\nThrough nearest-neighbor analysis, we show that vectors learned via this method capture both word-shape features and lexical features. As a result, we obtain reasonable near-neighbors for OOV abbreviations, names, novel compounds, and orthographic errors. Quantitative evaluation on the Stanford RareWord dataset (Luong et al., 2013) provides more evidence that these character-based embeddings capture word similarity for rare and unseen words.\nAs an extrinsic evaluation, we conduct experiments on joint prediction of part-of-speech tags and morphosyntactic attributes for a diverse set of 23 languages, as provided in the Universal Dependencies dataset (De Marneffe et al., 2014). Our model shows significant improvement\nar X\niv :1\n70 7.\n06 96\n1v 1\n[ cs\n.C L\n] 2\n1 Ju\nl 2 01\n7\nacross the board against a single UNK-embedding backoff method, and obtains competitive results against a supervised character-embedding model, which is trained end-to-end on the target task. In low-resource settings, our approach is particularly effective, and is complementary to supervised character embeddings trained from labeled data. The MIMICK-RNN therefore provides a useful new tool for tagging tasks in settings where there is limited labeled data. Models and code are available at www.github.com/ yuvalpinter/mimick ."
    }, {
      "heading" : "2 Related Work",
      "text" : "Compositional models for embedding rare and unseen words. Several studies make use of morphological or orthographic information when training word embeddings, enabling the prediction of embeddings for unseen words based on their internal structure. Botha and Blunsom (2014) compute word embeddings by summing over embeddings of the morphemes; Luong et al. (2013) construct a recursive neural network over each word’s morphological parse; Bhatia et al. (2016) use morpheme embeddings as a prior distribution over probabilistic word embeddings. While morphology-based approaches make use of meaningful linguistic substructures, they struggle with names and foreign language words, which include out-of-vocabulary morphemes. Character-based approaches avoid these problems: for example, Kim et al. (2016) train a recurrent neural network over words, whose embeddings are constructed by convolution over character embeddings; Wieting et al. (2016) learn embeddings of character ngrams, and then sum them into word embeddings. In all of these cases, the model for composing embeddings of subword units into word embeddings is learned by optimizing an objective over a large unlabeled corpus. In contrast, our approach is a post-processing step that can be applied to any set of word embeddings, regardless of how they were trained. This is similar to the “retrofitting” approach of Faruqui et al. (2015), but rather than smoothing embeddings over a graph, we learn a function to build embeddings compositionally.\nSupervised subword models. Another class of methods learn task-specific character-based word embeddings within end-to-end supervised systems. For example, Santos and Zadrozny (2014) build word embeddings by convolution over char-\nacters, and then perform part-of-speech (POS) tagging using a local classifier; the tagging objective drives the entire learning process. Ling et al. (2015) propose a multi-level long shortterm memory (LSTM; Hochreiter and Schmidhuber, 1997), in which word embeddings are built compositionally from an LSTM over characters, and then tagging is performed by an LSTM over words. Plank et al. (2016) show that concatenating a character-level or bit-level LSTM network to a word representation helps immensely in POS tagging. Because these methods learn from labeled data, they can cover only as much of the lexicon as appears in their labeled training sets. As we show, they struggle in several settings: lowresource languages, where labeled training data is scarce; morphologically rich languages, where the number of morphemes is large, or where the mapping from form to meaning is complex; and in Chinese, where the number of characters is orders of magnitude larger than in non-logographic scripts. Furthermore, supervised subword models can be combined with MIMICK, offering additive improvements.\nMorphosyntactic attribute tagging. We evaluate our method on the task of tagging word tokens for their morphosyntactic attributes, such as gender, number, case, and tense. The task of morpho-syntactic tagging dates back at least to the mid 1990s (Oflazer and Kuruöz, 1994; Hajič and Hladká, 1998), and interest has been rejuvenated by the availability of large-scale multilingual morphosyntactic annotations through the Universal Dependencies (UD) corpus (De Marneffe et al., 2014). For example, Faruqui et al. (2016) propose a graph-based technique for propagating typelevel morphological information across a lexicon, improving token-level morphosyntactic tagging in 11 languages, using an SVM tagger. In contrast, we apply a neural sequence labeling approach, inspired by the POS tagger of Plank et al. (2016)."
    }, {
      "heading" : "3 MIMICK Word Embeddings",
      "text" : "We approach the problem of out-of-vocabulary (OOV) embeddings as a generation problem: regardless of how the original embeddings were created, we assume there is a generative wordformbased protocol for creating these embeddings. By training a model over the existing vocabulary, we can later use that model for predicting the embedding of an unseen word.\nFormally: given a language L, a vocabulary V ⊆ L of size V , and a pre-trained embeddings table W ∈ RV×d where each word {wk}Vk=1 is assigned a vector ek of dimension d, our model is trained to find the function f : L → Rd such that the projected function f |V approximates the assignments f(wk) ≈ ek. Given such a model, a new word wk∗ ∈ L \\ V can now be assigned an embedding ek∗ = f(wk∗).\nOur predictive function of choice is a Word Type Character Bi-LSTM. Given a word with character sequence w = {ci}n1 , a forward-LSTM and a backward-LSTM are run over the corresponding character embeddings sequence {e(c)i }n1 . Let hnf represent the final hidden vector for the forward-LSTM, and let h0b represent the final hidden vector for the backward-LSTM. The word embedding is computed by a multilayer perceptron:\n(1)f(w) = OT · g(Th · [hnf ;h0b ] + bh) + bT ,\nwhere Th, bh and OT , bT are parameters of affine transformations, and g is a nonlinear elementwise function. The model is presented in Figure 1.\nThe training objective is similar to that of Yin and Schütze (2016). We match the predicted embeddings f(wk) to the pre-trained word embeddings ewk , by minimizing the squared Euclidean distance,\n(2)L = ‖f(wk)− ewk‖ 2 2 .\nBy backpropagating from this loss, it is possible to obtain local gradients with respect to the parameters of the LSTMs, the character embeddings, and the output model. The ultimate output of the training phase is the character embeddings matrix C and the parameters of the neural network: M = {C,F,B,Th, bh,OT , bT }, where F,B are the forward and backward LSTM component parameters, respectively."
    }, {
      "heading" : "3.1 MIMICK Polyglot Embeddings",
      "text" : "The pretrained embeddings we use in our experiments are obtained from Polyglot (Al-Rfou et al., 2013), a multilingual word embedding effort. Available for dozens of languages, each dataset contains 64-dimension embeddings for the 100,000 most frequent words in a language’s training corpus (of variable size), as well as an UNK embedding to be used for OOV words. Even with this vocabulary size, querying words from respective UD corpora (train + dev + test) yields high\nOOV rates: in at least half of the 23 languages in our experiments (see Section 5), 29.1% or more of the word types do not appear in the Polyglot vocabulary. The token-level median rate is 9.2%.1\nApplying our MIMICK algorithm to Polyglot embeddings, we obtain a prediction model for each of the 23 languages. Based on preliminary testing on randomly selected held-out development sets of 1% from each Polyglot vocabulary (with error calculated as in Equation 2), we set the following hyper-parameters for the remainder of the experiments: character embedding dimension = 20; one LSTM layer with 50 hidden units; 60 training epochs with no dropout; nonlinearity function g = tanh.2 We initialize character embeddings randomly, and use DyNet to implement the model (Neubig et al., 2017).\nNearest-neighbor examination. As a preliminary sanity check for the validity of our protocol, we examined nearest-neighbor samples in languages for which speakers were available: English, Hebrew, Tamil, and Spanish. Table 1 presents selected English OOV words with\n1Some OOV counts, and resulting model performance, may be adversely affected by tokenization differences between Polyglot and UD. Notably, some languages such as Spanish, Hebrew and Italian exhibit relational synthesis wherein words of separate grammatical phrases are joined into one form (e.g. Spanish del = de + el, ‘from the-masc.sg.’). For these languages, the UD annotations adhere to the sub-token level, while Polyglot does not perform subtokenization. As this is a real-world difficulty facing users of out-of-the-box embeddings, we do not patch it over in our implementations or evaluation.\n2Other settings, described below, were tuned on the supervised downstream tasks.\ntheir nearest in-vocabulary Polyglot words computed by cosine similarity. These examples demonstrate several properties: (a) word shape is learned well (acronyms, capitalizations, suffixes); (b) the model shows robustness to typos (e.g., developiong, corssing); (c) part-of-speech is learned across multiple suffixes (pesky – euphoric, ghastly); (d) word compounding is detected (e.g., lawnmower – bookmaker, postman); (e) semantics are not learned well (as is to be expected from the lack of context in training), but there are surprises (e.g., flatfish – slimy, watery). Table 2 presents examples from Hebrew that show learned properties can be extended to nominal morphosyntactic attributes (gender, number – first two examples) and even relational syntactic subword forms such as genetive markers (third example). Names are learned (fourth example) despite the lack of casing in the script. Spanish examples exhibit wordshape and part-of-speech learning patterns with some loose semantics: for example, the plural adjective form prenatales is similar to other familyrelated plural adjectives such as patrimoniales and generacionales. Tamil displays some semantic similarities as well: e.g. enjineer (‘engineer’) predicts similarity to other professional terms such as kalviyiyal (‘education’), thozhilnutpa (‘technical’), and iraanuva (‘military’).\nStanford RareWords. The Stanford RareWord evaluation corpus (Luong et al., 2013) focuses on predicting word similarity between pairs involving low-frequency English words, predominantly ones with common morphological affixes. As these words are unlikely to be above the cutoff threshold for standard word embedding models, they emphasize the performance on OOV words.\nFor evaluation of our MIMICK model on the RareWord corpus, we trained the Variational Embeddings algorithm (VarEmbed; Bhatia et al., 2016) on a 20-million-token, 100,000- type Wikipedia corpus, obtaining 128-dimension\nword embeddings for all words in the test corpus. VarEmbed estimates a prior distribution over word embeddings, conditional on the morphological composition. For in-vocabulary words, a posterior is estimated from unlabeled data; for outof-vocabulary words, the expected embedding can be obtained from the prior alone. In addition, we compare to FastText (Bojanowski et al., 2016), a high-vocabulary, high-dimensionality embedding benchmark.\nThe results, shown in Table 3, demonstrate that the MIMICK RNN recovers about half of the loss in performance incurred by the original Polyglot training model due to out-of-vocabulary words in the “All pairs” condition. MIMICK also outperforms VarEmbed. FastText can be considered an upper bound: with a vocabulary that is 25 times larger than the other models, it was missing words from only 44 pairs on this data."
    }, {
      "heading" : "4 Joint Tagging of Parts-of-Speech and Morphosyntactic Attributes",
      "text" : "The Universal Dependencies (UD) scheme (De Marneffe et al., 2014) features a minimal set of 17 POS tags (Petrov et al., 2012) and supports tagging further language-specific features using attribute-specific inventories. For example, a verb in Turkish could be assigned a value for the evidentiality attribute, one which is absent from Danish. These additional morphosyntactic attributes are marked in the UD dataset as optional per-token attribute-value pairs.\nOur approach for tagging morphosyntactic attributes is similar to the part-of-speech tagging model of Ling et al. (2015), who attach a projection layer to the output of a sentence-level bidirectional LSTM. We extend this approach to morphosyntactic tagging by duplicating this projection layer for each attribute type. The input to our multilayer perceptron (MLP) projection network is the hidden state produced for each token in the sentence by an underlying LSTM, and the output is\nattribute-specific probability distributions over the possible values for each attribute on each token in the sequence. Formally, for a given attribute a with possible values v ∈ Va, the tagging probability for the i’th word in a sentence is given by:\nPr(awi = v) = (Softmax(φ(hi)))v , (3)\nwith\n(4)φ(hi) = OaW · tanh(Wah · hi + bah) + baW ,\nwhere hi is the i’th hidden state in the underlying LSTM, and φ(hi) is a two-layer feedforward neural network, with weights Wah and O a W . We apply a softmax transformation to the output; the value at position v is then equal to the probability of attribute v applying to token wi. The input to the underlying LSTM is a sequence of word embeddings, which are initialized to the Polyglot vectors when possible, and to MIMICK vectors when necessary. Alternative initializations are considered in the evaluation, as described in Section 5.2.\nEach tagged attribute sequence (including POS tags) produces a loss equal to the sum of negative log probabilities of the true tags. One way to combine these losses is to simply compute the sum loss. However, many languages have large differences in sparsity across morpho-syntactic attributes, as apparent from Table 4 (rightmost column). We therefore also compute a weighted sum\nloss, in which each attribute is weighted by the proportion of training corpus tokens on which it is assigned a non-NONE value. Preliminary experiments on development set data were inconclusive across languages and training set sizes, and so we kept the simpler sum loss objective for the remainder of our study. In all cases, part-of-speech tagging was less accurate when learned jointly with morphosyntactic attributes. This may be because the attribute loss acts as POS-unrelated “noise” affecting the common LSTM layer and the word embeddings."
    }, {
      "heading" : "5 Experimental Settings",
      "text" : "The morphological complexity and compositionality of words varies greatly across languages. While a morphologically-rich agglutinative language such as Hungarian contains words that carry many attributes as fully separable morphemes, a sentence in an analytic language such as Vietnamese may have not a single polymorphemic or inflected word in it. To see whether this property is influential on our MIMICK model and its performance in the downstream tagging task, we select languages that comprise a sample of multiple morphological patterns. Language family and script type are other potentially influential factors in an orthography-based approach such as ours, and so we vary along these parameters as well. We also considered language selection recommendations from de Lhoneux and Nivre (2016) and Schluter and Agić (2017).\nAs stated above, our approach is built on the Polyglot word embeddings. The intersection of the Polyglot embeddings and the UD dataset (version 1.4) yields 44 languages. Of these, many are under-annotated for morphosyntactic attributes; we select twenty-three sufficiently-tagged languages, with the exception of Indonesian.3 Table 4 presents the selected languages and their typological properties. As an additional proxy for mor-\n3Vietnamese has no attributes by design; it is a pure analytic language.\nphological expressiveness, the rightmost column shows the proportion of UD tokens which are annotated with any morphosyntactic attribute."
    }, {
      "heading" : "5.1 Metrics",
      "text" : "As noted above, we use the UD datasets for testing our MIMICK algorithm on 23 languages4 with the supplied train/dev/test division. We measure partof-speech tagging by overall token-level accuracy.\nFor morphosyntactic attributes, there does not seem to be an agreed-upon metric for reporting performance. Dzeroski et al. (2000) report pertag accuracies on a morphosyntactically tagged corpus of Slovene. Faruqui et al. (2016) report macro-averages of F1 scores of 11 languages from UD 1.1 for the various attributes (e.g., part-ofspeech, case, gender, tense); recall and precision were calculated for the full set of each attribute’s values, pooled together.5 Agić et al. (2013) report separately on parts-of-speech and morphosyntactic attribute accuracies in Serbian and Croatian, as well as precision, recall, and F1 scores per tag. Georgiev et al. (2012) report token-level accuracy for exact all-attribute tags (e.g. ‘Ncmsh’ for “Noun short masculine singular definite”) in Bulgarian, reaching a tagset of size 680. Müller et al. (2013) do the same for six other languages. We report micro F1: each token’s value for each attribute is compared separately with the gold labeling, where a correct prediction is a matching non-NONE attribute/value assignment. Recall and\n4When several datasets are available for a language, we use the unmarked corpus.\n5Details were clarified in personal communication with the authors.\nprecision are calculated over the entire set, with F1 defined as their harmonic mean."
    }, {
      "heading" : "5.2 Models",
      "text" : "We implement and test the following models:\nNo-Char. Word embeddings are initialized from Polyglot models, with unseen words assigned the Polyglot-supplied UNK vector. Following tuning experiments on all languages with cased script, we found it beneficial to first back off to the lowercased form for an OOV word if its embedding exists, and only otherwise assign UNK.\nMIMICK. Word embeddings are initialized from Polyglot, with OOV embeddings inferred from a MIMICK model (Section 3) trained on the Polyglot embeddings. Unlike the No-Char case, backing off to lowercased embeddings before using the MIMICK output did not yield conclusive benefits and thus we report results for the more straightforward no-backoff implementation.\nCHAR→TAG. Word embeddings are initialized from Polyglot as in the No-Char model (with lowercase backoff), and appended with the output of a character-level LSTM updated during training (Plank et al., 2016). This additional module causes a threefold increase in training time.\nBoth. Word embeddings are initialized as in MIMICK, and appended with the CHAR→TAG LSTM.\nOther models. Several non-Polyglot embedding models were examined, all performed substantially worse than Polyglot. Two of these\nare notable: a random-initialization baseline, and a model initialized from FastText embeddings (tested on English). FastText supplies 300-dimension embeddings for 2.51 million lowercase-only forms, and no UNK vector.6 Both of these embedding models were attempted with and without CHAR→TAG concatenation. Another model, initialized from only MIMICK output embeddings, performed well only on the language with smallest Polyglot training corpus (Latvian). A Polyglot model where OOVs were initialized using an averaged embedding of all Polyglot vectors, rather than the supplied UNK vector, performed worse than our No-Char baseline on a great majority of the languages.\nLast, we do not employ type-based tagset restrictions. All tag inventories are computed from the training sets and each tag selection is performed over the full set."
    }, {
      "heading" : "5.3 Hyperparameters",
      "text" : "Based on development set experiments, we set the following hyperparameters for all models on all languages: two LSTM layers of hidden size 128, MLP hidden layers of size equal to the number of each attribute’s possible values; momentum stochastic gradient descent with 0.01 learning rate; 40 training epochs (80 for 5K settings) with a dropout rate of 0.5. The CHAR→TAG models use 20-dimension character embeddings and a single hidden layer of size 128."
    }, {
      "heading" : "6 Results",
      "text" : "We report performance in both low-resource and full-resource settings. Low-resource training sets were obtained by randomly sampling training sentences, without replacement, until a predefined token limit was reached. We report the results on the full sets and on N = 5000 tokens in Table 5 (partof-speech tagging accuracy) and Table 6 (morphosyntactic attribute tagging micro-F1). Results for additional training set sizes are shown in Figure 2; space constraints prevent us from showing figures for all languages.\nMIMICK as OOV initialization. In nearly all experimental settings on both tasks, across languages and training corpus sizes, the MIMICK embeddings significantly improve over the Polyglot UNK embedding for OOV tokens on both\n6Vocabulary type-level coverage for the English UD corpus: 55.6% case-sensitive, 87.9% case-insensitive.\nPOS and morphosyntactic tagging. For POS, the largest margins are in the Slavic languages (Russian, Czech, Bulgarian), where word order is relatively free and thus rich word representations are imperative. Chinese also exhibits impressive improvement across all settings, perhaps due to the large character inventory (> 12,000), for which a model such as MIMICK can learn well-informed embeddings using the large Polyglot vocabulary dataset, overcoming both word- and characterlevel sparsity in the UD corpus.7 In morphosyntactic tagging, gains are apparent for Slavic languages and Chinese, but also for agglutinative languages — especially Tamil and Turkish — where the stable morpheme representation makes it easy for subword modeling to provide a type-level signal.8 To examine the effects on Slavic and agglutinative languages in a more fine-grained view, we present results of multiple training-set size experiments for each model, averaged over five repetitions (with different corpus samples), in Figure 2.\nMIMICK vs. CHAR→TAG. In several languages, the MIMICK algorithm fares better than the CHAR→TAG model on part-of-speech tagging in low-resource settings. Table 7 presents the POS tagging improvements that MIMICK achieves over the pre-trained Polyglot models, with and without CHAR→TAG concatenation, with 10,000 tokens of training data. We obtain statistically significant improvements in most languages, even when CHAR→TAG is included. These improvements are particularly substantial for test-set tokens outside the UD training set, as shown in the right two columns. While test set OOVs are a strength of the CHAR→TAG model (Plank et al., 2016), in many languages there are still considerable improvements to be obtained from the application of MIMICK initialization. This suggests that with limited training data, the end-to-end CHAR→TAG model is unable to learn a sufficiently accurate representational mapping from orthography."
    }, {
      "heading" : "7 Conclusion",
      "text" : "We present a straightforward algorithm to infer OOV word embedding vectors from pre-trained,\n7Character coverage in Chinese Polyglot is surprisingly good: only eight characters from the UD dataset are unseen in Polyglot, across more than 10,000 unseen word types.\n8Persian is officially classified as agglutinative but it is mostly so with respect to derivations. Its word-level inflections are rare and usually fusional.\nlimited-vocabulary models, without need to access the originating corpus. This method is particularly useful for low-resource languages and tasks with little labeled data available, and in fact is task-agnostic. Our method improves performance over word-based models on annotated sequence-tagging tasks for a large variety of languages across dimensions of family, orthography, and morphology. In addition, we present a BiLSTM approach for tagging morphosyntactic attributes at the token level. In this paper, the MIMICK model was trained using characters as input, but future work may consider the use of other subword units, such as morphemes, phonemes, or even bitmap representations of ideographic characters (Costa-jussà et al., 2017)."
    }, {
      "heading" : "8 Acknowledgments",
      "text" : "We thank Umashanthi Pavalanathan, Sandeep Soni, Roi Reichart, and our anonymous reviewers for their valuable input. We thank Manaal Faruqui and Ryan McDonald for their help in understanding the metrics for morphosyntactic tagging. The project was supported by project HDTRA1-15-10019 from the Defense Threat Reduction Agency."
    } ],
    "references" : [ {
      "title" : "Lemmatization and morphosyntactic tagging of Croatian and Serbian",
      "author" : [ "Željko Agić", "Nikola Ljubešić", "Danijela Merkler." ],
      "venue" : "4th Biennial International Workshop on Balto-Slavic Natural Language Processing (BSNLP 2013).",
      "citeRegEx" : "Agić et al\\.,? 2013",
      "shortCiteRegEx" : "Agić et al\\.",
      "year" : 2013
    }, {
      "title" : "Polyglot: Distributed word representations for multilingual NLP",
      "author" : [ "Rami Al-Rfou", "Bryan Perozzi", "Steven Skiena." ],
      "venue" : "Proceedings of the Conference on Natural Language Learning (CoNLL), pages 183–192, Sofia, Bulgaria.",
      "citeRegEx" : "Al.Rfou et al\\.,? 2013",
      "shortCiteRegEx" : "Al.Rfou et al\\.",
      "year" : 2013
    }, {
      "title" : "Morphological priors for probabilistic neural word embeddings",
      "author" : [ "Parminder Bhatia", "Robert Guthrie", "Jacob Eisenstein." ],
      "venue" : "Proceedings of Empirical Methods for Natural Language Processing (EMNLP).",
      "citeRegEx" : "Bhatia et al\\.,? 2016",
      "shortCiteRegEx" : "Bhatia et al\\.",
      "year" : 2016
    }, {
      "title" : "Enriching word vectors with subword information",
      "author" : [ "Piotr Bojanowski", "Edouard Grave", "Armand Joulin", "Tomas Mikolov." ],
      "venue" : "arXiv preprint arXiv:1607.04606.",
      "citeRegEx" : "Bojanowski et al\\.,? 2016",
      "shortCiteRegEx" : "Bojanowski et al\\.",
      "year" : 2016
    }, {
      "title" : "Compositional morphology for word representations and language modelling",
      "author" : [ "Jan A Botha", "Phil Blunsom." ],
      "venue" : "(?).",
      "citeRegEx" : "Botha and Blunsom.,? 2014",
      "shortCiteRegEx" : "Botha and Blunsom.",
      "year" : 2014
    }, {
      "title" : "Chinese–spanish neural machine translation enhanced with character and word bitmap fonts",
      "author" : [ "Marta R Costa-jussà", "David Aldón", "José AR Fonollosa." ],
      "venue" : "Machine Translation, pages 1–13.",
      "citeRegEx" : "Costa.jussà et al\\.,? 2017",
      "shortCiteRegEx" : "Costa.jussà et al\\.",
      "year" : 2017
    }, {
      "title" : "Universal stanford dependencies: A cross-linguistic typology",
      "author" : [ "Marie-Catherine De Marneffe", "Timothy Dozat", "Natalia Silveira", "Katri Haverinen", "Filip Ginter", "Joakim Nivre", "Christopher D Manning." ],
      "venue" : "Proceedings of the Language Resources and",
      "citeRegEx" : "Marneffe et al\\.,? 2014",
      "shortCiteRegEx" : "Marneffe et al\\.",
      "year" : 2014
    }, {
      "title" : "Morphosyntactic tagging of slovene: Evaluating taggers and tagsets",
      "author" : [ "Saso Dzeroski", "Tomaz Erjavec", "Jakub Zavrel." ],
      "venue" : "Proceedings of the Language Resources and Evaluation Conference (LREC).",
      "citeRegEx" : "Dzeroski et al\\.,? 2000",
      "shortCiteRegEx" : "Dzeroski et al\\.",
      "year" : 2000
    }, {
      "title" : "Retrofitting word vectors to semantic lexicons",
      "author" : [ "Manaal Faruqui", "Jesse Dodge", "Sujay K Jauhar", "Chris Dyer", "Eduard Hovy", "Noah A Smith." ],
      "venue" : "Proceedings of the North American Chapter of the Association for Computational Linguistics",
      "citeRegEx" : "Faruqui et al\\.,? 2015",
      "shortCiteRegEx" : "Faruqui et al\\.",
      "year" : 2015
    }, {
      "title" : "Morpho-syntactic lexicon generation using graph-based semi-supervised learning",
      "author" : [ "Manaal Faruqui", "Ryan McDonald", "Radu Soricut." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 4:1–16.",
      "citeRegEx" : "Faruqui et al\\.,? 2016",
      "shortCiteRegEx" : "Faruqui et al\\.",
      "year" : 2016
    }, {
      "title" : "Feature-rich part-of-speech tagging for morphologically complex languages: Application to Bulgarian",
      "author" : [ "Georgi Georgiev", "Valentin Zhikov", "Petya Osenova", "Kiril Simov", "Preslav Nakov." ],
      "venue" : "Proceedings of the European Chapter of the Association for",
      "citeRegEx" : "Georgiev et al\\.,? 2012",
      "shortCiteRegEx" : "Georgiev et al\\.",
      "year" : 2012
    }, {
      "title" : "Tagging inflective languages: Prediction of morphological categories for a rich, structured tagset",
      "author" : [ "Jan Hajič", "Barbora Hladká." ],
      "venue" : "Proceedings of the Association for Computational Linguistics (ACL), pages 483–490.",
      "citeRegEx" : "Hajič and Hladká.,? 1998",
      "shortCiteRegEx" : "Hajič and Hladká.",
      "year" : 1998
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural computation, 9(8):1735–1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Character-aware neural language models",
      "author" : [ "Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M Rush." ],
      "venue" : "Proceedings of the National Conference on Artificial Intelligence (AAAI).",
      "citeRegEx" : "Kim et al\\.,? 2016",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2016
    }, {
      "title" : "Ud treebank sampling for comparative parser evaluation",
      "author" : [ "Miryam de Lhoneux", "Joakim Nivre." ],
      "venue" : "The Sixth Swedish Language Technology Conference (SLTC).",
      "citeRegEx" : "Lhoneux and Nivre.,? 2016",
      "shortCiteRegEx" : "Lhoneux and Nivre.",
      "year" : 2016
    }, {
      "title" : "Finding function in form: Compositional character models for open vocabulary word representation",
      "author" : [ "Wang Ling", "Tiago Luı́s", "Luı́s Marujo", "Ramón Fernandez Astudillo", "Silvio Amir", "Chris Dyer", "Alan W Black", "Isabel Trancoso" ],
      "venue" : null,
      "citeRegEx" : "Ling et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ling et al\\.",
      "year" : 2015
    }, {
      "title" : "Better word representations with recursive neural networks for morphology",
      "author" : [ "Minh-Thang Luong", "Richard Socher", "Christopher D Manning." ],
      "venue" : "Proceedings of the Conference on Natural Language Learning (CoNLL).",
      "citeRegEx" : "Luong et al\\.,? 2013",
      "shortCiteRegEx" : "Luong et al\\.",
      "year" : 2013
    }, {
      "title" : "Efficient higher-order CRFs for morphological tagging",
      "author" : [ "Thomas Müller", "Helmut Schmid", "Hinrich Schütze." ],
      "venue" : "Proceedings of Empirical Methods for Natural Language Processing (EMNLP), pages 322–332.",
      "citeRegEx" : "Müller et al\\.,? 2013",
      "shortCiteRegEx" : "Müller et al\\.",
      "year" : 2013
    }, {
      "title" : "DyNet: The dynamic neural network toolkit",
      "author" : [ "Graham Neubig", "Chris Dyer", "Yoav Goldberg", "Austin Matthews", "Waleed Ammar", "Antonios Anastasopoulos", "Miguel Ballesteros", "David Chiang", "Daniel Clothiaux", "Trevor Cohn" ],
      "venue" : null,
      "citeRegEx" : "Neubig et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Neubig et al\\.",
      "year" : 2017
    }, {
      "title" : "Tagging and morphological disambiguation of turkish text",
      "author" : [ "Kemal Oflazer", "Ìlker Kuruöz." ],
      "venue" : "Proceedings of the fourth conference on Applied natural language processing, pages 144–149. Association for Computational Linguistics.",
      "citeRegEx" : "Oflazer and Kuruöz.,? 1994",
      "shortCiteRegEx" : "Oflazer and Kuruöz.",
      "year" : 1994
    }, {
      "title" : "A universal part-of-speech tagset",
      "author" : [ "Slav Petrov", "Dipanjan Das", "Ryan McDonald." ],
      "venue" : "Proceedings of the Language Resources and Evaluation Conference (LREC).",
      "citeRegEx" : "Petrov et al\\.,? 2012",
      "shortCiteRegEx" : "Petrov et al\\.",
      "year" : 2012
    }, {
      "title" : "Multilingual part-of-speech tagging with bidirectional long short-term memory models and auxiliary loss",
      "author" : [ "Barbara Plank", "Anders Søgaard", "Yoav Goldberg." ],
      "venue" : "(?).",
      "citeRegEx" : "Plank et al\\.,? 2016",
      "shortCiteRegEx" : "Plank et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning character-level representations for part-of-speech tagging",
      "author" : [ "Cicero D. Santos", "Bianca Zadrozny." ],
      "venue" : "(?), pages 1818–1826.",
      "citeRegEx" : "Santos and Zadrozny.,? 2014",
      "shortCiteRegEx" : "Santos and Zadrozny.",
      "year" : 2014
    }, {
      "title" : "Empirically sampling universal dependencies",
      "author" : [ "Natalie Schluter", "Željko Agić." ],
      "venue" : "The NoDaLiDa Workshop on Universal Dependencies (UDW 2017).",
      "citeRegEx" : "Schluter and Agić.,? 2017",
      "shortCiteRegEx" : "Schluter and Agić.",
      "year" : 2017
    }, {
      "title" : "Building a tree-bank of modern hebrew text",
      "author" : [ "Khalil Sima’an", "Alon Itai", "Yoad Winter", "Alon Altman", "Noa Nativ" ],
      "venue" : "Traitement Automatique des Langues,",
      "citeRegEx" : "Sima.an et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Sima.an et al\\.",
      "year" : 2001
    }, {
      "title" : "Charagram: Embedding words and sentences via character n-grams",
      "author" : [ "John Wieting", "Mohit Bansal", "Kevin Gimpel", "Karen Livescu." ],
      "venue" : "arXiv preprint arXiv:1607.02789.",
      "citeRegEx" : "Wieting et al\\.,? 2016",
      "shortCiteRegEx" : "Wieting et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning word meta-embeddings",
      "author" : [ "Wenpeng Yin", "Hinrich Schütze." ],
      "venue" : "(?).",
      "citeRegEx" : "Yin and Schütze.,? 2016",
      "shortCiteRegEx" : "Yin and Schütze.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 16,
      "context" : "Quantitative evaluation on the Stanford RareWord dataset (Luong et al., 2013) provides more evidence that these character-based embeddings capture word similarity for rare and unseen words.",
      "startOffset" : 57,
      "endOffset" : 77
    }, {
      "referenceID" : 4,
      "context" : "Botha and Blunsom (2014)",
      "startOffset" : 0,
      "endOffset" : 25
    }, {
      "referenceID" : 15,
      "context" : "compute word embeddings by summing over embeddings of the morphemes; Luong et al. (2013) construct a recursive neural network over each word’s morphological parse; Bhatia et al.",
      "startOffset" : 69,
      "endOffset" : 89
    }, {
      "referenceID" : 2,
      "context" : "(2013) construct a recursive neural network over each word’s morphological parse; Bhatia et al. (2016) use morpheme embeddings as a prior distribu-",
      "startOffset" : 82,
      "endOffset" : 103
    }, {
      "referenceID" : 8,
      "context" : "This is similar to the “retrofitting” approach of Faruqui et al. (2015), but rather than smoothing embeddings over a graph, we learn a function to build embeddings compositionally.",
      "startOffset" : 50,
      "endOffset" : 72
    }, {
      "referenceID" : 22,
      "context" : "For example, Santos and Zadrozny (2014) build word embeddings by convolution over characters, and then perform part-of-speech (POS) tagging using a local classifier; the tagging objective drives the entire learning process.",
      "startOffset" : 13,
      "endOffset" : 40
    }, {
      "referenceID" : 12,
      "context" : "(2015) propose a multi-level long shortterm memory (LSTM; Hochreiter and Schmidhuber, 1997), in which word embeddings are built compositionally from an LSTM over characters, and then tagging is performed by an LSTM over",
      "startOffset" : 51,
      "endOffset" : 91
    }, {
      "referenceID" : 21,
      "context" : "Plank et al. (2016) show that concatenating a character-level or bit-level LSTM network to a word representation helps immensely in POS tagging.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 19,
      "context" : "The task of morpho-syntactic tagging dates back at least to the mid 1990s (Oflazer and Kuruöz, 1994; Hajič and Hladká, 1998), and interest has been rejuvenated by the availability of large-scale multilingual mor-",
      "startOffset" : 74,
      "endOffset" : 124
    }, {
      "referenceID" : 11,
      "context" : "The task of morpho-syntactic tagging dates back at least to the mid 1990s (Oflazer and Kuruöz, 1994; Hajič and Hladká, 1998), and interest has been rejuvenated by the availability of large-scale multilingual mor-",
      "startOffset" : 74,
      "endOffset" : 124
    }, {
      "referenceID" : 6,
      "context" : "phosyntactic annotations through the Universal Dependencies (UD) corpus (De Marneffe et al., 2014). For example, Faruqui et al. (2016) propose a graph-based technique for propagating typelevel morphological information across a lexicon, improving token-level morphosyntactic tagging in 11 languages, using an SVM tagger.",
      "startOffset" : 76,
      "endOffset" : 135
    }, {
      "referenceID" : 6,
      "context" : "phosyntactic annotations through the Universal Dependencies (UD) corpus (De Marneffe et al., 2014). For example, Faruqui et al. (2016) propose a graph-based technique for propagating typelevel morphological information across a lexicon, improving token-level morphosyntactic tagging in 11 languages, using an SVM tagger. In contrast, we apply a neural sequence labeling approach, inspired by the POS tagger of Plank et al. (2016).",
      "startOffset" : 76,
      "endOffset" : 430
    }, {
      "referenceID" : 26,
      "context" : "The training objective is similar to that of Yin and Schütze (2016). We match the predicted em-",
      "startOffset" : 45,
      "endOffset" : 68
    }, {
      "referenceID" : 1,
      "context" : "The pretrained embeddings we use in our experiments are obtained from Polyglot (Al-Rfou et al., 2013), a multilingual word embedding effort.",
      "startOffset" : 79,
      "endOffset" : 101
    }, {
      "referenceID" : 18,
      "context" : "beddings randomly, and use DyNet to implement the model (Neubig et al., 2017).",
      "startOffset" : 56,
      "endOffset" : 77
    }, {
      "referenceID" : 16,
      "context" : "The Stanford RareWord evaluation corpus (Luong et al., 2013) focuses on predicting word similarity between pairs involving low-frequency English words, predominantly ones with common morphological affixes.",
      "startOffset" : 40,
      "endOffset" : 60
    }, {
      "referenceID" : 2,
      "context" : "For evaluation of our MIMICK model on the RareWord corpus, we trained the Variational Embeddings algorithm (VarEmbed; Bhatia et al., 2016) on a 20-million-token, 100,000type Wikipedia corpus, obtaining 128-dimension word embeddings for all words in the test corpus.",
      "startOffset" : 107,
      "endOffset" : 138
    }, {
      "referenceID" : 3,
      "context" : "compare to FastText (Bojanowski et al., 2016), a high-vocabulary, high-dimensionality embedding benchmark.",
      "startOffset" : 20,
      "endOffset" : 45
    }, {
      "referenceID" : 20,
      "context" : ", 2014) features a minimal set of 17 POS tags (Petrov et al., 2012) and supports tagging further language-specific features using attribute-specific inventories.",
      "startOffset" : 46,
      "endOffset" : 67
    }, {
      "referenceID" : 15,
      "context" : "Our approach for tagging morphosyntactic attributes is similar to the part-of-speech tagging model of Ling et al. (2015), who attach a projection layer to the output of a sentence-level bidirectional LSTM.",
      "startOffset" : 102,
      "endOffset" : 121
    }, {
      "referenceID" : 24,
      "context" : "Table 2: Nearest-neighbor examples for Hebrew (Transcriptions per Sima’an et al. (2001)).",
      "startOffset" : 66,
      "endOffset" : 88
    }, {
      "referenceID" : 14,
      "context" : "We also considered language selection recommendations from de Lhoneux and Nivre (2016) and Schluter and Agić (2017).",
      "startOffset" : 62,
      "endOffset" : 87
    }, {
      "referenceID" : 14,
      "context" : "We also considered language selection recommendations from de Lhoneux and Nivre (2016) and Schluter and Agić (2017). As stated above, our approach is built on the Polyglot word embeddings.",
      "startOffset" : 62,
      "endOffset" : 116
    }, {
      "referenceID" : 7,
      "context" : "Dzeroski et al. (2000) report pertag accuracies on a morphosyntactically tagged corpus of Slovene.",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 7,
      "context" : "Dzeroski et al. (2000) report pertag accuracies on a morphosyntactically tagged corpus of Slovene. Faruqui et al. (2016) report macro-averages of F1 scores of 11 languages from",
      "startOffset" : 0,
      "endOffset" : 121
    }, {
      "referenceID" : 0,
      "context" : "5 Agić et al. (2013) report separately on parts-of-speech and morphosyntactic attribute accuracies in Serbian and Croatian, as well as precision, recall, and F1 scores per tag.",
      "startOffset" : 2,
      "endOffset" : 21
    }, {
      "referenceID" : 0,
      "context" : "5 Agić et al. (2013) report separately on parts-of-speech and morphosyntactic attribute accuracies in Serbian and Croatian, as well as precision, recall, and F1 scores per tag. Georgiev et al. (2012) report token-level accuracy for exact all-attribute tags (e.",
      "startOffset" : 2,
      "endOffset" : 200
    }, {
      "referenceID" : 0,
      "context" : "5 Agić et al. (2013) report separately on parts-of-speech and morphosyntactic attribute accuracies in Serbian and Croatian, as well as precision, recall, and F1 scores per tag. Georgiev et al. (2012) report token-level accuracy for exact all-attribute tags (e.g. ‘Ncmsh’ for “Noun short masculine singular definite”) in Bulgarian, reaching a tagset of size 680. Müller et al. (2013) do the same for six other languages.",
      "startOffset" : 2,
      "endOffset" : 383
    }, {
      "referenceID" : 21,
      "context" : "ercase backoff), and appended with the output of a character-level LSTM updated during training (Plank et al., 2016).",
      "startOffset" : 96,
      "endOffset" : 116
    }, {
      "referenceID" : 21,
      "context" : "While test set OOVs are a strength of the CHAR→TAG model (Plank et al., 2016), in many languages there are still considerable improvements to be obtained from the application of MIMICK initialization.",
      "startOffset" : 57,
      "endOffset" : 77
    }, {
      "referenceID" : 21,
      "context" : "* For reference, we copy the reported results of Plank et al. (2016)’s analog to CHAR→TAG.",
      "startOffset" : 49,
      "endOffset" : 69
    }, {
      "referenceID" : 5,
      "context" : "In this paper, the MIMICK model was trained using characters as input, but future work may consider the use of other subword units, such as morphemes, phonemes, or even bitmap representations of ideographic characters (Costa-jussà et al., 2017).",
      "startOffset" : 218,
      "endOffset" : 244
    } ],
    "year" : 2017,
    "abstractText" : "Word embeddings improve generalization over lexical features by placing each word in a lower-dimensional space, using distributional information obtained from unlabeled data. However, the effectiveness of word embeddings for downstream NLP tasks is limited by out-of-vocabulary (OOV) words, for which embeddings do not exist. In this paper, we present MIMICK, an approach to generating OOV word embeddings compositionally, by learning a function from spellings to distributional embeddings. Unlike prior work, MIMICK does not require re-training on the original word embedding corpus; instead, learning is performed at the type level. Intrinsic and extrinsic evaluations demonstrate the power of this simple approach. On 23 languages, MIMICK improves performance over a word-based baseline for tagging part-of-speech and morphosyntactic attributes. It is competitive with (and complementary to) a supervised characterbased model in low-resource settings.",
    "creator" : "LaTeX with hyperref package"
  }
}