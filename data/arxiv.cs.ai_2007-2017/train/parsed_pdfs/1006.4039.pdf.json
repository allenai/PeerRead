{
  "name" : "1006.4039.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n00 6.\n40 39\nv1 [\ncs .L\nG ]\n2 1\nJu n\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n050\n051\n052\n053"
    }, {
      "heading" : "1 Introduction",
      "text" : "Online learning has emerged as an attractive and dominant paradigm in machine learning given the ever increasing amounts of data that is being increasing collected everyday. However, the key underlying assumption here is that all the training data is available at a central location. For many applications this is not the case. For instance, sensor networks may be deployed in rain forests and collect data autonomously. The cost of transmitting the data to a central server can be prohibitively high. Similarly, banks collect credit information about their customers but might not share the information with other financial institutions. Similar privacy concerns might prevent sharing of patient records across hospitals. Our aim in this paper is to devise an online learning algorithm which can work in the distributed data setting.\nSome research effort has been devoted to this problem recently. For instance, [1] cast many machine learning problems as a regularized risk minimization problem. Since the empirical risk is computed by averaging the loss over the entire data set, one can design a decentralized algorithm by letting individual slave processors take charge of a portion of the data. At every iteration, the master communicates the current parameter vector to all the slaves, and they in turn compute the loss and its gradient on the part of the data they own and communicate it back to the master. However, as [1] observe, when the number of processors increases Amdahls law [2] kicks in and the cost of communicating and synchronizing becomes prohibitively expensive.\nAnother notable effort is the work by [3], where they show that one can avoid the expensive synchronization step above. The key idea here is that the slaves periodically poll the master node to receive the latest parameter vector. This is used to compute stochastic gradients which are then fed back to the master node. Even though the gradients received by the master may be out of sync, that is, it may not be computed using the current parameter vector, [3] show that their algorithm suffers small regret.\nUnlike the above case where the processors have to communicate with a master node, we will work in a fully decentralized setting. In our case the individual processors act as autonomous agents. They maintain their individual parameter vectors and periodically communicate with each other and pass information. We call this the cooperative autonomous online learning algorithm. Using tools from convex analysis and a theorem from Markov chain theory, we conduct a theoretical study of\n055\n056\n057\n058\n059\n060\n061\n062\n063\n064\n065\n066\n067\n068\n069\n070\n071\n072\n073\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099\n100\n101\n102\n103\n104\n105\n106\n107\nthis algorithm, and derive regret bounds. If we assume that each local learner processes data at the same rate, then in spite of not having complete information, the new regret bounds guarantee that the predictive performance of autonomous online learners will converge to that of the best parameter vector chosen in hindsight, as fast as a centralized online learner.\nOur paper is structured as follows: In section 2 we introduce notation and some key results from the analysis of finite-state Markov chains. We present our algorithm in section 3 and its analysis in section 4. The paper concludes with a outlook and discussion in section 5."
    }, {
      "heading" : "2 Preliminaries",
      "text" : "Notation: Lower case letters (e.g., w) denote (column) vectors while upper case letters (e.g., A) denote matrices. We will denote the (j, i)-th element of A by Aji and the i-th column of A by Ai. Subscripts with t, t + 1 etc are used for indexing the parameter vector with respect to time while superscripts are used for indexing with respect to a processor. For instance,wit denotes the parameter vector of the i-th processor at time t. We use ei to denote the i-th basis vector (the vector of all zeros except one on the ith position), and e to denote the vector of all ones. Unless specified otherwise, ‖·‖ refers to the Euclidean norm ‖x‖ := (∑\ni x 2 i\n)1/2 , and 〈·, ·〉 denotes the Euclidean dot product\n〈x, x′〉 = ∑\ni xix ′ i.\nSubgradients and Strongly Convex Functions: The subgradient (set) ∂xf(·) of a convex function f(x) at x0 is defined as [4]\ng ∈ ∂f(x0) ⇐⇒ ∀y, f(y)− f(x0) ≥ 〈y − x0, g〉 . (1) A convex function f(·) defined on domain Ω is said to be strongly convex with modulus λ > 0 if and only if [4]\n∀x, y ∈ Ω, f(y)− f(x)− 〈y − x, ∂xf(x)〉 ≥ λ ‖y − x‖2 . (2)\nProjection and its Properties: The Euclidean projection operator onto a set Ω ⊆ Rn is defined as PΩ(w\n′) = argmin w∈Ω ‖w − w′‖ . (3)\nThe projection operator satisfies the following property (see e.g. equation (59) in [5]):\n〈PΩ (ŵ)− ŵ, ŵ − w〉 ≤ −‖PΩ (ŵ)− ŵ‖2 ≤ 0, ∀w ∈ Ω. (4)\nA Result from Analysis of Markov Chains: We shall see that our autonomous learners exchange information with their neighbors. The communication pattern is defined by a weighted directed graph whose adjacency matrix, A, is doubly stochastic. Recall that a matrix is said to be doubly stochastic if and only if all elements of A are non-negative and both rows and columns sum to one.\nA doubly stochastic matrix also occurs in the analysis of Markov chains, where the entryAji defines the probability of transitioning from the ith state to the jth state. A is said to be irreducible if and only if the weighted directed graph whose adjacency matrix is A⊤ is strongly connected. A is called aperiodic if and only if\n∀i, gcd{k ≥ 1 : Akii 6= 0} = 1, where gcd denotes the greatest common divisor. A distribution vector π ∈ Rm is called a stationary distribution if and only if Aπ = π. Whenever A is doubly stochastic its stationary distribution π = 1me. We will use the following standard result from the analysis of finite-state Markov chains that characterizes the limiting behaviors of Ak as k → ∞ in our analysis (see e.g. Chapter 12 of [6]):\nTheorem 1 Let A be the transition matrix of a Markov chain. If A is irreducible, aperiodic, and π is the stationary distribution, then there exists a positive real number β < 1 such that\n∀i, ∑\nj\n|Akji − πj | ≤ 2βk. (5)\nThe connectivity of G affects β. The intuition is with stronger connectivity, transiting from one state to another state will bypass fewer intermediate states, so mixing to the stationary distribution is easier. Therefore, the higher the connectivity, the lower the β.\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161"
    }, {
      "heading" : "3 Cooperative Autonomous Online Learning",
      "text" : "Sequential Online Learning: Online learning usually proceeds in trials. At each trial a data point xt is given to the learner which produces a parameter vectorwt from a convex set Ω ⊆ Rn. One then computes some function of the inner product 〈wt, xt〉 in order to produce a label ŷt. The true label yt is revealed to the learner, which then incurs a convex (but not necessarily smooth) loss l(wt, xt, yt) and adjusts its parameter vector. If we succinctly denote ft(w) := l(w, xt, yt), then online learning is equivalent to solving the following optimization problem in a stochastic fashion:\nmin w∈Ω\nJ(w), where J(w) = T∑\nt=1\nft(w) and Ω ⊆ Rn, (6)\nand the goal is to minimize the regret\nRS = T∑\nt=1\nft(wt)−min w∈Ω J(w). (7)\nCooperative Autonomous Online Learning: Sequential online learning assumes a centralized learner with all the data available to it sequentially. However, for cooperative autonomous online learning, we assume we have m local online learners using only data contained at local sites. At each trial m data points xit with i ∈ {1, 2, . . . ,m} are given and the i-th learner updates model parameters based on the i-th point. The learner produces a parameter vector wit which is used to compute the prediction 〈 wit, x i t 〉 and the corresponding loss f it (w) = l(w, x i t, y i t). The learners then exchange information with a selected set of their neighbors before updating wit to w i t+1. In order to reduce network traffic, the communication pattern amongst processors is assumed to form a strongly (but not necessarily fully) connected graph. In particular, we will assume a directed weighted graph whose adjacency matrix A is doubly stochastic. One can interpret the entry Aji as the importance that learner i places on the parameter vector communicated by learner j. Of course, if Aji = 0 then learners i and j do not communicate.\nThe optimization problem in this case can be written as\nmin w∈Ω\nJ(w), where J(w) = T∑\nt=1\nm∑\ni=1\nf it (w) and Ω ⊆ Rn, (8)\nand regret is measured with respect to the vector wt which is computed by averaging over the wit:\nRCA = T∑\nt=1\nm∑\ni=1\nf it (wt)−min w∈Ω\nJ(w), where wt = 1\nm\nm∑\ni=1\nwit. (9)\nIf we denote ft = ∑m i=1 f i t\n1, our definition of the regret has the same form of the regret in sequential online learning. However, our definition of RCA is distinguished from the sequential regret RS by the following two points\n• Given N data points, there are T = N iterations or trial in sequential online learning. In our case, this number reduces down to T = Nm . • The regret of cooperative autonomous online learning is defined through the average parameters wt which can be regarded as summaries of the learners’ local parameters wit. We shall see in Algorithm 1 that wt need not even be calculated explicitly.\nWe will show the convergence of wt by bounding the regret RCA. In particular, we are interested in generalizing the celebrated √ T and log T bounds [7, 8] of sequential online learning to cooperative autonomous online learning.\nWe present a general online learning algorithm for solving (8) here. Specifically, a local learner propagates the parameter to other learners. After receiving the parameters from other learners, each learner updates its local parameter through a linear combination of the received and its own old parameter. Then the local learner updates the local model parameter based on the data collected and the local subgradient. Via this cooperation, the learners learn a model from distributed data sequentially. The algorithm is summarized in Algorithm 1.\n1We abuse the notation ft hereinafter.\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\nAlgorithm 1 Cooperative Autonomous Online Learning\n1: Input: The number of learners m; initial points w11 , . . . w m 1 ; double stochastic matrix A =\n(Aji) ∈ Rm×m; and maximum iterations T . 2: for t = 1, . . . , T do 3: for each learner i = 1, . . . ,m do 4: git ← ∂wf it (wit). 5: Communicate wit with neighbors (as defined by A) and obtain their parameters. 6: ŵit+1 ← ∑ j Ajiw j t − ηtgit (Local subgradient descent) 7: wit+1 ← PΩ ( ŵit+1 ) (Projection) 8: end for 9: end for\n10: Return: wT+1 ← 1m ∑ i w i T+1"
    }, {
      "heading" : "4 Regret Bounds",
      "text" : "For our analysis we make the following standard assumptions, which are assumed to hold for all the proofs and theorems presented below.\n• Each f it is strongly convex with modulus λ ≥ 02. • Aji 6= 0 if and only if the ith learner communicates with the jth learner. We further assume A is irreducible, aperiodic, and there exists β < 1 as defined in Theorem 1.\n• Ω is a closed convex subset of Rn with non-empty interior. The subgradient ∂wf it (w) can be computed for every w ∈ Ω. • The diameter diam(Ω) = supx,x′∈Ω ‖x− x′‖ of Ω is bounded by F <∞. • The set of optimal solutions of (8) denoted by Ω∗ is non-empty. • The norm of the subgradients of f it is bounded by L, and wi1 are identically initialized.\nWe start from a key result concerning the decomposition of regret is Lemma 2 given below (also see Lemma 5 of [9]). It extends a result that can be found in various guises in many places most notably Lemma 2.1 and 2.2 in [10], Theorem 4.1 and Eq. (4.21) and (4.15) in [11], in the proof of Theorem 1 of [7], as well as Lemma 3 of [12].\nLemma 2 Let wit denote the sequences generated by Algorithm 1. Denote wt = 1\nm ∑m i=1 w i t, and\nḡit = ∂wf i t (wt). For any w ∈ Ω we have\n‖wt+1 − w‖2 ≤ (1− 2ηtλ) ‖wt − w‖2 + 4η2t m2\n( m∑\ni=1\n∥ ∥git ∥ ∥\n)2\n− 2ηt m (ft(wt)− ft(w))\n+ 2ηt m\nm∑\ni=1\n( ∥ ∥git ∥ ∥+ ∥ ∥ḡit ∥ ∥) ∥ ∥wt − wit ∥ ∥+ 2ηt m\nm∑\ni=1\n∥ ∥git ∥ ∥ ∥ ∥wt − ŵit+1 ∥ ∥ (10)"
    }, {
      "heading" : "Proof",
      "text" : "Define\nrit := w i t − ŵit = PΩ ( ŵit ) − ŵit. (11)\nRecall that Ω is assumed to be convex, A is a doubly stochastic matrix, and wjt ∈ Ω for all j. Therefore, Aji ≥ 0, ∑ j Aji = 1, and ∑ j Ajiw j t ∈ Ω for all i. By this observation, the definition of the projection operator (3), and the definition of ŵit+1 in Line 6 of Algorithm 1 we have the following estimate for the norm of rit+1\n∥ ∥rit+1 ∥ ∥ = ∥ ∥PΩ ( ŵit+1 ) − ŵit+1 ∥ ∥ ≤ ∥ ∥ ∥ ∥ ∥ ∥ ∑\nj\nAjiw j t − ŵit+1 ∥ ∥ ∥ ∥ ∥ ∥ = ηt ∥ ∥git ∥ ∥ (12)\n2Note that we allow for λ = 0, in which case f it is just convex, but not strongly convex.\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\nThen, we define the following matrices to simplify the notations.\nWt = [w 1 t , . . . , w m t ], Ŵt = [ŵ 1 t , . . . , ŵ m t ]\nGt = [g 1 t , . . . , g m t ], Rt = [r 1 t , . . . , r m t ]\nSince A is doubly stochastic Ae = 1. Therefore, by using (11) and the update in step 6 of Algorithm 1, we have the relation\nwt+1 = 1\nm Wt+1e =\n1 m (WtA− ηtGt +Rt+1)e (13)\n= 1\nm Wte− ηt m Gte+ 1 m Rt+1e = wt − ηt m\nm∑\ni=1\ngit + 1\nm\n∑\ni=1\nrit+1.\nUsing the above relation we unroll ‖wt+1 − w‖2 by\n‖wt+1 − w‖2 = ‖wt − w‖2 + 1\nm2 ∥ ∥ ∥ ∥ ∥ m∑\ni=1\n( rit+1 + ηtg i t ) ∥ ∥ ∥ ∥ ∥ 2\n− 2ηt m\nm∑\ni=1\n〈 git, wt − w 〉 + 2\nm\nm∑\ni=1\n〈 rit+1, wt − w 〉 . (14)\nIn view of (12)\n1\nm2 ∥ ∥ ∥ ∥ ∥ m∑\ni=1\n( rit+1 + ηtg i t ) ∥ ∥ ∥ ∥ ∥ 2 ≤ 1 m2 ( m∑\ni=1\n∥ ∥rit+1 ∥ ∥+ ηt ∥ ∥git ∥ ∥\n)2\n= 4η2t m2\n( m∑\ni=1\n∥ ∥git ∥ ∥\n)2\n. (15)\nNext we turn our attention to the −∑i 〈 git, wt − w 〉 term which we bound using (1) and (2) as follows: − 〈 git, wt − w 〉 = − 〈 git, wt − wit 〉 − 〈 git, w i t − w 〉\n≤ ∥ ∥git ∥ ∥ ∥ ∥wt − wit ∥ ∥+ f it (w) − f it (wit)− λ ∥ ∥wit − w ∥ ∥ = ∥ ∥git ∥ ∥ ∥ ∥wt − wit ∥ ∥+ f it (wt)− f it (wit)− λ ∥ ∥wit − w ∥ ∥+ f it (w) − f it (wt) ≤ ∥ ∥git ∥ ∥ ∥ ∥wt − wit ∥ ∥+ 〈 ḡit, wt − wit 〉 − λ ∥ ∥wit − wt ∥ ∥− λ ∥ ∥wit − w ∥ ∥+ f it (w) − f it (wt) ≤ (∥ ∥git ∥ ∥+ ∥ ∥ḡit ∥ ∥ ) ∥ ∥wt − wit ∥ ∥− λ ‖wt − w‖+ f it (w)− f it (wt).\nThe last inequality is by using 〈 ḡit, wt − wit 〉 ≤ ∥ ∥ḡit ∥ ∥ ∥ ∥wt − wit ∥ ∥ and ∥ ∥wit − wt ∥ ∥ + ∥ ∥wit − w ∥ ∥ ≥ ‖wt − w‖. Summing up over i = 1, . . . ,m, obtains\n− m∑\ni=1\n〈 git, wt − w 〉 ≤\nm∑\ni=1\n(∥ ∥git ∥ ∥+ ∥ ∥ḡit ∥ ∥ ) ∥ ∥wt − wit ∥ ∥− λm ‖wt − w‖ − (ft(wt)− ft(w)) (16)\nIn order to estimate 〈 rit+1, wt − w 〉 , we use (11), (4), and (12) to write\n〈 rit+1, wt − w 〉 = 〈 rit+1, wt − ŵit+1 〉 + 〈 PΩ ( ŵit+1 ) − ŵit+1, ŵit+1 − w 〉\n≤ 〈 rit+1, wt − ŵit+1 〉 ≤ ηt ∥ ∥git ∥ ∥ ∥ ∥wt − ŵit+1 ∥ ∥ . (17)\nCombining (15), (16) and (17) with (14) completes the proof.\nNext we upper bound the terms ∥ ∥wt − wit ∥ ∥ and ∥ ∥wt − ŵit+1 ∥ ∥ in (10). The convergence rate in Theorem 1 plays a central role in this lemma.\nLemma 3 If the assumptions in section 3 hold, and let β be as in Theorem 1, then\n∥ ∥wt − wit ∥ ∥ ≤ 4L\nt−1∑\nk=1\nηt−kβ k−1 (18)\n∥ ∥wt − ŵit+1 ∥ ∥ ≤ 4L\nt−1∑\nk=0\nηt−kβ k. (19)\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323"
    }, {
      "heading" : "Proof",
      "text" : "Using the notations defined in the proof of Lemma 2, we unroll the relation Wt =Wt−1A− ηtGt−1 +Rt\nwhich is defined through Algorithm 1 yields\nWt =W1A t−1 −\nt−1∑\nk=1\nηt−kGt−kA k−1 +\nt−1∑\nk=1\nRt−k+1A k−1. (20)\nUsing Ake = 1 for all k, (5), (12), and the above relation we can write\n∥ ∥wt − wit ∥ ∥ = ∥ ∥ ∥ ∥ Wt ( 1\nm e− ei\n)∥ ∥ ∥ ∥ = ∥ ∥ ∥ ∥ ∥ ( W1A t−1 − t−1∑\nk=1\nηt−kGt−kA k−1 +\nt−2∑\nk=0\nRt−kA k\n)( 1\nm e− ei ) ∥ ∥ ∥ ∥ ∥\n≤ ∥ ∥w1 − wi1 ∥ ∥+\nt−1∑\nk=1\nηt−k ∥ ∥ ∥ ∥ Gt−k ( 1\nm e −Ak−1i\n)∥ ∥ ∥ ∥ + t−1∑\nk=1\n∥ ∥ ∥ ∥ Rt−k+1 ( 1\nm e−Ak−1i\n)∥ ∥ ∥ ∥\n≤ 4L t−1∑\nk=1\nηt−kβ k−1\nWe omit the proof for (19) which follows along similar lines.\nUsing Lemma 2 and 3, we are now ready to bound the regret for our algorithm.\nLemma 4 Let w∗ ∈ Ω∗ denote the best parameter chosen in hindsight. Then the regret of Algorithm 1 can be bounded via\nT∑\nt=1\nft(wt)− ft(w∗) ≤ mF ( 1\n2ηT − Tλ\n)\n+mCL2 T∑\nt=1\nηt, (21)\nwhere C is an absolute constant defined as\nC = (4 + 12\n1− β ). (22)\nProof The proof is straight-forward, we put it in the supplementary material.\nNow with Lemma 4, we can obtain the following regret bounds.\nTheorem 5 If λ > 0 and we set ηt = 12λt then T∑\nt=1\nft(wt)− ft(w∗) ≤ CL2m\n2λ (1 + log(T )), (23)\nOn the other hand, when λ = 0, if we set ηt = 1 2 √ t then\nT∑\nt=1\nft(wt)− ft(w∗) ≤ m ( F + CL2 )√ T . (24)\nProof First consider λ > 0 with ηt = 12λt . In this case 1 2ηT = Tλ, and consequently (21) in Lemma 4 specializes to T∑\nt=1\nft(wt)− ft(w∗) ≤ CL2m\n2λ\nT∑\nt=1\n1 t ≤ CL\n2m\n2λ (1 + log(T )).\nWhen λ = 0, and we set ηt = 1 2 √ t and to rewrite (21) as\nT∑\nt=1\nft(wt)− ft(w∗) ≤ mF √ T + CL2m T∑\nt=1\n1\n2 √ t ≤ mF\n√ T + CL2m √ T .\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377"
    }, {
      "heading" : "4.1 Interpreting the Bounds",
      "text" : "Whenm = 1, then algorithm 1 reduces to the familiar sequential online learning, and unsurprisingly (24) and (23) recover the classical square root regret of [7] and logarithmic regret of [8]. Whenm > 1 then one has to exercise care when interpreting the bounds. Recall that every time instance t them processors simultaneously processm data points. Therefore in T steps our learners processmT data points. If we letN = mT , then our bounds can be rewritten asO( √ mN) andO(m+m log(N/m)) respectively. Contrast these bounds with the ones obtained by [3] which are of the form O( √ τN ) and O(τ + τ log(N/m)), where τ is the delay in the subgradient calculation.\nAnother interesting exercise is to compare our bounds with that of a sequential online learning algorithm which processes N = mT data points. In this case, the regret of the sequential algorithm is O( √ N) (resp.O(log(N))), while our algorithm suffers a O( √ mN) (resp.O(m+m log(N/m)) regret. It must be borne in mind, however, that our algorithm is handicapped by two factors. First, there is only limited information sharing between different learners and second, by our definition of regret, our algorithm is forced to predict on m data points in one shot with a single parameter vector wt. This is in contrast with the sequential online learner which has access to the full data set and can use different parameter vectors for each of the m data points.\nOn the other end of the spectrum let us consider the most optimistic scenario for our algorithm. If instead of working on m separate data points at every time instance, all the learners receive the same data at every iteration, then the updates in algorithm 1 recover the familiar sequential online learning updates and the corresponding regret bounds. This is in contrast to the algorithm of [3] where the optimistic scenario is achieved for gradients which are completely de-correlated, which in turn means that the data points are de-correlated. These scenarios represent two ends of the spectrum and it is hard to argue which one is more favorable than the other. Moreover, Langford et al.’s optimistic bound O(τ2 + logT ) may not suitable for large delay/number of learners, because the O(τ2) dominates. In such a scenario, the optimistic bound is no better than theO(τ logT ) since logT is a slowly growing function."
    }, {
      "heading" : "4.2 Generalization to Bregman Divergences",
      "text" : "Our proof techniques can be generalized to Bregman divergences. We assume access to a function ψ : Ω → R which is continuously differentiable and strongly convex with modulus of strong convexity σ > 0, and use it to define a Bregman divergence [13, 14]:\n∆ψ(w,w ′) = ψ(w)− ψ(w′)− 〈w − w′,∇ψ(w′)〉 . (25)\nThe diameter of Ω as measured by ∆ψ is given by\ndiamψ(Ω) = max w,w′∈Ω\n∆ψ(w,w ′). (26)\nIn addition to the assumptions made for the analysis of Algorithm 1, we further need to assume that the gradient ∇ψ, and its inverse (∇ψ)−1 = ∇ψ∗ can be computed. Algorithm 2 summarizes the generalization of Algorithm 1. Note that the projection step can be omitted now because the domain of ψ is restricted to Ω. The bounds we prove in Lemma 4 and Theorem 5 can be recovered with minor modifications. We omit these proofs due to lack of space."
    }, {
      "heading" : "5 Outlook and Discussion",
      "text" : "We presented a decentralized online learning algorithm3 and proved regret bounds which are similar to the traditional sequential online learning bounds. Stochastic optimization with autonomous agents is also gaining research attention in optimization [9]. Even though our analysis shares some similarities with these methods, the focus is very different. While we work with projections, Bregman divergences, and focus on regret bounds in the presence and absence of strong convexity they mainly focus on the rates of convergence of stochastic (sub)gradient descent with a fixed step size. These differences are somewhat similar to the key differences between the analysis of stochastic\n3Using very different techniques, Langford, Smola, and Zinkevich independently study an autonomous setting somewhat similar to ours in a paper submitted to NIPS 2010 (personal communication).\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\n400\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\nAlgorithm 2 Cooperative Autonomous Online Learning with Bregman Divergence\n1: Input: The number of learners m; initial points w11 , . . . w m 1 ; double stochastic matrix A =\n(Aji) ∈ Rm×m; and maximum iterations T . 2: for t = 1, . . . , T do 3: for each learner i = 1, . . . ,m do 4: git ← ∂wf it (wit). 5: Communicate wit with neighbors and obtain their parameters. 6: wit+1 ← ∇ψ∗( ∑ j Aji∇ψ(w j t )− ηtgit) (Local gradient descent) 7: end for 8: end for 9: Return: wT+1 ← ∇ψ∗( 1m ∑ i∇ψ(wiT+1))\n(sub)gradient descent methods (see e.g. [15]) and regret bounds in online learning (e.g. [8]). There is also literature in the area of sensor networks which deals with learning in decentralized settings with limited communication. However, the tasks here are rather specific such as probabilistic inference [16] or evolving consensus [17]. A common form of f it (w) is l(y i t, 〈 w, xit 〉 ), where l(yit, z) is the loss function. So the subgradient w.r.t. to wit is g i t = ∂zl(y i t, 〈 wit, x i t 〉 ) xit, which is proportional to x i t. Thus algorithms that transmit subgradients (e.g.the first variant of Langford et al.’s algorithm [3]) may disclose information about data points. In the privacy sensitive applications mentioned in section 1, such as mining patient information across hospitals, we would like to keep the data at local sites and collaboratively complete data mining tasks. Our decentralized algorithm transmits only local parameters between the online learners, which makes it safer against network attacks.\nFor simplicity, we only analyzed the case where the communication pattern matrix A is fixed, and does not evolve over time. Our proofs can be extended to the setting where the communication graph evolves over time, or is selected from a small list of possible candidates at every iteration. This might be of importance in the case of privacy preserving data mining, where a fixed communication graph may be vulnerable to attack by network sniffing. Also, for simplicity, we assumed that nodes communicate with each other at every iteration. This can be relaxed to the case where the nodes only communicate periodically, that is, after having performed several local parameter updates.\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485"
    }, {
      "heading" : "Appendix",
      "text" : ""
    }, {
      "heading" : "Proof of Lemma 4",
      "text" : "Proof Set w = w∗, divide both sides of (10) by 2ηtm and rearrange to obtain\nft(wt)− ft(w∗) ≤ m\n2ηt [(1− 2ηtλ) ‖wt − w∗‖ − ‖wt+1 − w∗‖] + 2 ηt m\n( m∑\ni=1\n∥ ∥git ∥ ∥\n)2\n+ 2L\nm∑\ni=1\n∥ ∥wt − wit ∥ ∥+ L\nm∑\ni=1\n∥ ∥wt − ŵit+1 ∥ ∥\nPlug in the estimate of the subgradients and the bounds (18) and (19).\nft(wt)− ft(w∗) ≤ m\n2ηt [(1− 2ηtλ) ‖wt − w∗‖ − ‖wt+1 − w∗‖]\n+ 2mL2ηt + 8L 2m\nt−1∑\nk=1\nηt−kβ k−1 + 4L2m\nt−1∑\nk=0\nηt−kβ k\n≤ m 2ηt [(1− 2ηtλ) ‖wt − w∗‖ − ‖wt+1 − w∗‖]\n+ 4mL2ηt + 12L 2m\nt−1∑\nk=1\nηt−kβ k−1\nSumming over t = 1, . . . , T\nT∑\nt=1\nft(w − t)− ft(w∗) ≤ m T∑\nt=1\n1\n2ηt [(1 − 2ηtλ) ‖wt − w∗‖ − ‖wt+1 − w∗‖] ︸ ︷︷ ︸\n:=C1\n+ 4mL2 T∑\nt=1\nηt + 12L 2m\nT∑\nt=1\nt−1∑\nk=1\nηt−kβ k−1\n︸ ︷︷ ︸\n:=C2\nSince the diameter of Ω is bounded by F\nC1 =\n( 1 2η1 − λ ) ‖w1 − w∗‖ − 1 2ηT ‖wT+1 − w∗‖+ T∑\nt=2\n‖wt − w∗‖ ( 1\n2ηt − 1 2ηt−1 − λ )\n≤ ( 1 2η1 − λ ) F + T∑\nt=2\nF\n( 1\n2ηt − 1 2ηt−1 − λ ) = F ( 1 2ηT − Tλ )\nLet I(t > k) be the indicator function which is 1 when t > k and 0 otherwise. Then\nC2 =\nT∑\nt=1\nT∑\nk=1\nηt−kβ k−1I(t > k) =\nT∑\nk=1\nβk−1 T∑\nt=k+1\nηt−k\n≤ T∑\nk=1\nβk−1 T∑\nt=1\nηt ≤ 1\n1− β\nT∑\nt=1\nηt\nPlug in the estimate for C1 and C2, to obtain (21)."
    } ],
    "references" : [ {
      "title" : "Bundle methods for regularized risk minimization",
      "author" : [ "Choon Hui Teo", "S.V.N. Vishwanthan", "Alex J. Smola", "Quoc V. Le" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2010
    }, {
      "title" : "Validity of the single processor approach to achieving large-scale computing capabilities",
      "author" : [ "Gene Amdahl" ],
      "venue" : null,
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1967
    }, {
      "title" : "Slow learners are fast",
      "author" : [ "J. Langford", "A.J. Smola", "M. Zinkevich" ],
      "venue" : "arXiv:0911.0491",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Convex Analysis and Minimization Algorithms",
      "author" : [ "J.B. Hiriart-Urruty", "C. Lemaréchal" ],
      "venue" : "I and II, volume 305 and 306. Springer-Verlag",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 1993
    }, {
      "title" : "Tracking the best linear predictor",
      "author" : [ "M. Herbster", "M.K. Warmuth" ],
      "venue" : "Journal of Machine Learning Research, 1:281–309",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Monte Carlo strategies in scientific computing",
      "author" : [ "Jun S. Liu" ],
      "venue" : null,
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2001
    }, {
      "title" : "Online convex programming and generalised infinitesimal gradient ascent",
      "author" : [ "M. Zinkevich" ],
      "venue" : "Proc. Intl. Conf. Machine Learning, pages 928–936",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Logarithmic regret algorithms for online convex optimization",
      "author" : [ "Elad Hazan", "Amit Agarwal", "Satyen Kale" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2007
    }, {
      "title" : "Distributed subgradient methods for multi-agent optimization",
      "author" : [ "Angelia Nedic", "Asu Ozdaglar" ],
      "venue" : "IEEE Trans. on Automatic Control,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2009
    }, {
      "title" : "Subgradient Methods for Convex Minimization",
      "author" : [ "Angelia Nedic" ],
      "venue" : "PhD thesis,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2002
    }, {
      "title" : "Mirror descent and nonlinear projected subgradient methods for convex optimization",
      "author" : [ "Amir Beck", "Marc Teboulle" ],
      "venue" : "Operations Research Letters,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2003
    }, {
      "title" : "Logarithmic regret algorithms for strongly convex repeated games",
      "author" : [ "S. Shalev-Shwartz", "Y. Singer" ],
      "venue" : "Technical report, School of Computer Science, Hebrew University",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Relative loss bounds for on-line density estimation with the exponential family of distributions",
      "author" : [ "K. Azoury", "M.K. Warmuth" ],
      "venue" : "Machine Learning, 43(3):211–246",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Parallel Optimization",
      "author" : [ "Y. Censor", "S.A. Zenios" ],
      "venue" : "Oxford, New York",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Convergence rate of incremental subgradient algorithms",
      "author" : [ "A. Nedich", "D. P Bertsekas" ],
      "venue" : "S. Uryasev and P. M. Pardalos, editors, Stochastic Optimization: Algorithms and Applications, pages 263–304. Kluwer Academic Publishers",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Robust probabilistic inference in distributed systems",
      "author" : [ "Mark Paskin", "Carlos Guestrin" ],
      "venue" : "In Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2004
    }, {
      "title" : "Constrained consensus and optimization in multiagent networks",
      "author" : [ "A. Nedic", "A. Ozdaglar", "P.A. Parrilo" ],
      "venue" : "Automatic Control, IEEE Transactions on, 55",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "For instance, [1] cast many machine learning problems as a regularized risk minimization problem.",
      "startOffset" : 14,
      "endOffset" : 17
    }, {
      "referenceID" : 0,
      "context" : "However, as [1] observe, when the number of processors increases Amdahls law [2] kicks in and the cost of communicating and synchronizing becomes prohibitively expensive.",
      "startOffset" : 12,
      "endOffset" : 15
    }, {
      "referenceID" : 1,
      "context" : "However, as [1] observe, when the number of processors increases Amdahls law [2] kicks in and the cost of communicating and synchronizing becomes prohibitively expensive.",
      "startOffset" : 77,
      "endOffset" : 80
    }, {
      "referenceID" : 2,
      "context" : "Another notable effort is the work by [3], where they show that one can avoid the expensive synchronization step above.",
      "startOffset" : 38,
      "endOffset" : 41
    }, {
      "referenceID" : 2,
      "context" : "Even though the gradients received by the master may be out of sync, that is, it may not be computed using the current parameter vector, [3] show that their algorithm suffers small regret.",
      "startOffset" : 137,
      "endOffset" : 140
    }, {
      "referenceID" : 3,
      "context" : "Subgradients and Strongly Convex Functions: The subgradient (set) ∂xf(·) of a convex function f(x) at x0 is defined as [4] g ∈ ∂f(x0) ⇐⇒ ∀y, f(y)− f(x0) ≥ 〈y − x0, g〉 .",
      "startOffset" : 119,
      "endOffset" : 122
    }, {
      "referenceID" : 3,
      "context" : "(1) A convex function f(·) defined on domain Ω is said to be strongly convex with modulus λ > 0 if and only if [4]",
      "startOffset" : 111,
      "endOffset" : 114
    }, {
      "referenceID" : 4,
      "context" : "equation (59) in [5]):",
      "startOffset" : 17,
      "endOffset" : 20
    }, {
      "referenceID" : 5,
      "context" : "Chapter 12 of [6]):",
      "startOffset" : 14,
      "endOffset" : 17
    }, {
      "referenceID" : 6,
      "context" : "In particular, we are interested in generalizing the celebrated √ T and log T bounds [7, 8] of sequential online learning to cooperative autonomous online learning.",
      "startOffset" : 85,
      "endOffset" : 91
    }, {
      "referenceID" : 7,
      "context" : "In particular, we are interested in generalizing the celebrated √ T and log T bounds [7, 8] of sequential online learning to cooperative autonomous online learning.",
      "startOffset" : 85,
      "endOffset" : 91
    }, {
      "referenceID" : 8,
      "context" : "We start from a key result concerning the decomposition of regret is Lemma 2 given below (also see Lemma 5 of [9]).",
      "startOffset" : 110,
      "endOffset" : 113
    }, {
      "referenceID" : 9,
      "context" : "2 in [10], Theorem 4.",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 10,
      "context" : "15) in [11], in the proof of Theorem 1 of [7], as well as Lemma 3 of [12].",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 6,
      "context" : "15) in [11], in the proof of Theorem 1 of [7], as well as Lemma 3 of [12].",
      "startOffset" : 42,
      "endOffset" : 45
    }, {
      "referenceID" : 11,
      "context" : "15) in [11], in the proof of Theorem 1 of [7], as well as Lemma 3 of [12].",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 6,
      "context" : "Whenm = 1, then algorithm 1 reduces to the familiar sequential online learning, and unsurprisingly (24) and (23) recover the classical square root regret of [7] and logarithmic regret of [8].",
      "startOffset" : 157,
      "endOffset" : 160
    }, {
      "referenceID" : 7,
      "context" : "Whenm = 1, then algorithm 1 reduces to the familiar sequential online learning, and unsurprisingly (24) and (23) recover the classical square root regret of [7] and logarithmic regret of [8].",
      "startOffset" : 187,
      "endOffset" : 190
    }, {
      "referenceID" : 2,
      "context" : "Contrast these bounds with the ones obtained by [3] which are of the form O( √ τN ) and O(τ + τ log(N/m)), where τ is the delay in the subgradient calculation.",
      "startOffset" : 48,
      "endOffset" : 51
    }, {
      "referenceID" : 2,
      "context" : "This is in contrast to the algorithm of [3] where the optimistic scenario is achieved for gradients which are completely de-correlated, which in turn means that the data points are de-correlated.",
      "startOffset" : 40,
      "endOffset" : 43
    }, {
      "referenceID" : 12,
      "context" : "We assume access to a function ψ : Ω → R which is continuously differentiable and strongly convex with modulus of strong convexity σ > 0, and use it to define a Bregman divergence [13, 14]:",
      "startOffset" : 180,
      "endOffset" : 188
    }, {
      "referenceID" : 13,
      "context" : "We assume access to a function ψ : Ω → R which is continuously differentiable and strongly convex with modulus of strong convexity σ > 0, and use it to define a Bregman divergence [13, 14]:",
      "startOffset" : 180,
      "endOffset" : 188
    }, {
      "referenceID" : 8,
      "context" : "Stochastic optimization with autonomous agents is also gaining research attention in optimization [9].",
      "startOffset" : 98,
      "endOffset" : 101
    }, {
      "referenceID" : 14,
      "context" : "[15]) and regret bounds in online learning (e.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 7,
      "context" : "[8]).",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 15,
      "context" : "However, the tasks here are rather specific such as probabilistic inference [16] or evolving consensus [17].",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 16,
      "context" : "However, the tasks here are rather specific such as probabilistic inference [16] or evolving consensus [17].",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 2,
      "context" : "’s algorithm [3]) may disclose information about data points.",
      "startOffset" : 13,
      "endOffset" : 16
    } ],
    "year" : 2017,
    "abstractText" : "Online learning is becoming increasingly popular for training on large datasets. However, the sequential nature of online learning requires a centralized learner to store data and update parameters. In this paper, we consider a fully decentralized setting, cooperative autonomous online learning, with a distributed data source. The learners perform learning with local parameters while periodically communicating with a small subset of neighbors to exchange information. We define the regret in terms of an implicit aggregated parameter of the learners for such a setting and prove regret bounds similar to the classical sequential online learning.",
    "creator" : "LaTeX with hyperref package"
  }
}