{
  "name" : "1704.04133.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Explaining the Unexplained: A CLass-Enhanced Attentive Response (CLEAR) Approach to Understanding Deep Neural Networks",
    "authors" : [ "Devinder Kumar", "Alexander Wong", "Graham W. Taylor" ],
    "emails" : [ "devinder.kumar@uwaterloo.ca", "a28wong@uwaterloo.ca", "gwtaylor@uoguelph.ca" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "In recent years, we have seen tremendous success in the field of artificial intelligence (AI). In particular, many of the recent advances have been related to one particular area of machine learning: deep neural networks (DNNs). DNNs have been shown to outperform previous machine learning techniques for a variety of tasks, such as fine-grained classification [18, 5], self-driving cars [4], captioning and answering questions about images [9, 1], and even defeating human champions at Go [11]. Although DNNs have demonstrated tremendous effectiveness at a wide range of tasks, when they fail, they often fail spectacularly, producing unexplainable and incoherent results that can leave one to wonder what caused the DNN to make such decisions. This lack of transparency and interpretability of DNNs during the decision-making process is largely due to the complex nature of DNNs, where individual neural responses, unlike other interpretable decision-making processes such as decision trees, provide very little insight as to what is actually going on.\nThe lack of transparency in the decision-making process of DNNs is a significant bottleneck in their widespread\n1\nar X\niv :1\n70 4.\n04 13\n3v 1\n[ cs\n.C V\n] 1\n3 A\npr 2\n01 7\nadoption in industry, such as healthcare, defence, cybersecurity, etc., where the error tolerance is very low and the ability to interpret, understand, and trust decisions is critical. As such, a way to peer inside a DNN and see why it made a decision the way it did can have tremendous potential for pushing towards explainable AI, where a human expert gains the ability to understand, interpret and verify the decisions made.\nRecently, a number of researchers have been exploring the understanding and interpretation of decisions made by DNNs, in particular by Deep Convolutional Neural Networks (DCNN), by asking the following question: based on what information in the image is the DCNN making a decision? To tackle this question, much recent work has focused on understanding the decision-making process of networks by leveraging heatmaps that provide information about which areas of the image is used by the DCNN to make a particular decision. These approaches have produced some promising results in revealing what is important to a decision made by a DCNN. More details regarding the relevant works are provided in Section 2. A common limitation with such heatmap-based approaches to understanding the decision-making process of DCNNs is that of decision ambiguity, where one can gain insight into which regions of interest are important for making decisions, but gives no insight as to why such regions of interest are important. As a result, these methods leave the “thought process” of the DCNN largely ambiguous.\nIn an attempt to mitigate the problem of decision ambiguity, we take a step towards “explaining the unexplained”, with regards to the decision-making process of DCNNs, through the introduction of CLass-Enhanced Attentive Response (CLEAR) maps that go beyond what existing heatmap-based approaches [19, 2, 10] can provide. The proposed CLEAR maps allow for the visualization of not only the attentive regions of interest and corresponding attentive levels of DCNNs during the decision-making process, but also the corresponding dominant classes associated with these attentive regions of interest. As such, compared to heatmaps, CLEAR maps are much more effective at conveying where and why certain regions of interest influence the decision-making process. An example of this is shown in Fig. 1. We further demonstrate the effectiveness of the proposed CLEAR maps, both quantitatively and qualitatively, by conducting a number of experiments using three different publicly available datasets."
    }, {
      "heading" : "2. Related Work",
      "text" : "There has been a significant body of work in recent years in the domain of visualizing and understanding DCNNs. This literature can be broadly divided into two groups: first, approaches that focus on understanding the global structure of a trained network [3, 7, 15]; and second, approaches that\nmainly focus on understanding the decision-making process of trained networks for a specific instance [14, 2, 18, 10, 12, 16, 19]. Our present work can be considered as belonging to the second category. Relevant work pertaining to each group is explained below: Global Understanding-Based Methods: Many methods in this domain try to understand the decision-making process of the deep network by measuring its operating characteristics; for example, finding an input that maximizes the response of a particular neuron [6], measuring the network’s invariance to certain kinds of data augmentation [7], or determining global decision structure [3]. Other methods seek to find image instances from a database that maximally activate particular neurons or the posterior class probability of a given network [15]. Instance-Based Methods: These methods are based on interpreting individual decisions made by a DCNN for a particular image instance. One specific instance-based method was proposed by Simonyan et al. in [12], where using backpropagated partial derivatives of the class score with respect to pixel values were used to create class saliency maps. Zeiler & Fergus [16] proposed a deconvolution-based method to project the activations from feature space back to the input space (pixels) recursively. However, the method did not provide any meaning to the assignments other than that they should form a coherent set of interpretable pixels. Springenberg et al. [14] provided another gradientbased visualization method, which restricts the negative gradients from flowing backwards towards the input layer, leading to sharper visualization, still without attributing any meaning to the obtained visualization. However, the study strongly showed the efficacy of networks with global average pooling for image classification and visualization. To visually discern unique features for a particular category of image, Zhou et. al. [18] created a class activation map using DCNNs with global average pooling layers. This class activation map was also used for localising objects within the image. Bach et al. [2] and Montavon et al. [10] aimed at finding a general approach to visualize non-linear classifiers, leading to interesting heatmap generation. Recently, similar to the occlusion-based methodology for creating heatmaps in [16], Zintgraf et al. [19] proposed a method based on multivariate conditional sampling over image patches to visualize and interpret individual decisions of DCNNs as binary saliency maps to represent information that contributes for or against the decision.\nIn our work, instead of only obtaining feature maps, we attribute meaning to each pixel in the back-projected response in the input space using a class-based approach. Also, unlike [2, 10] or [19], that provide heatmaps or binary heatmaps for correctly classified samples, we create CLEAR maps that are more interpretable (Fig. 1) for both correctly or misclassified cases. Finally, compared to the\nper-class maps created in [18], CLEAR maps show multiple class-specific contributions at once."
    }, {
      "heading" : "3. Class Enhanced Attentive Response",
      "text" : "(CLEAR)\nThis section explains the procedure for generating the proposed CLass-Enhanced Attentive Response (CLEAR) maps. The main goal of CLEAR maps is to convey the following information: 1) the attentive regions of interest in the image responsible for the decision made by the DCNN; 2) the attentive levels at these regions of interest so that we understand their level of influence over the decision made by the DCNN; and 3) the dominant class associated with these attentive regions of interest so that we can better understand why a decision was made. The procedure for generating CLEAR maps can be summarized as follows (see Fig. 2). First, individual attentive response maps are computed for each kernel associated with a class by backprojecting activations from the output layer of the DCNN. Based on this set of attentive response maps, two different types of maps are computed: 1) a dominant attentive response map, which shows the dominant attentive level for each location in the image; and 2) a dominant class attentive map, which shows the dominant class involved in\nthe decision-making process at each location. Finally, the dominant attentive response map and the dominant attentive class map are combined visually by using color and intensity to produce the final CLEAR map for a given image.\nInspired by the effectiveness of the ALL-CNN [14] on different datasets, we leveraged a similar network architecture for building the DCNN used for classification in this paper. While, for clarity, we describe the procedure for computing individual attentive response maps based on the ALL-CNN architecture, the procedure will generalize to other DCNNs provided class-specific responses can be computed in input (pixel) space. ALL-CNNs are composed primarily of convolutional, ReLU, and max-pooling layers. Towards the output of the DCNN, the last convolutional layer contains a set of kernels equal to the number of classes, and then global averaging is performed before passing these energy values to the softmax output layer which represents categories. As such, each kernel can be thought of as being associated with a particular class.\nThe first step of CLEAR is to compute a set of individual attentive response maps, one for each of the classes learned by the DCNN, which we will denote as {R(x|c)|1 ≤ c ≤ N}, where N is the number of classes. This is achieved in the current realization of CLEAR by back-propagating the responses of each kernel in the last\nconvolutional layer from feature space to the input space to form each attentive response map, thus extending upon the idea first introduced in [17]. To explain the formulation for the formation of CLEAR maps, first consider a single layer of a DCNN. Let ĥl be the deconvolved output response of the single layer l with K kernel weights w. The deconvolution output response at layer l then can be then obtained by convolving each of the feature maps zl with kernels wl and summing them as:\nĥl = K∑ k=1 zk,l ∗ wk,l. (1)\nHere ∗ represents the convolution operation. For notational brevity, we can combine the convolution and summation operation for layer l into a single convolution matrix Gl. Hence the above equation can be denoted as: ĥl = Glzl.\nFor multi-layered DCNNs, we can extend the above formulation by adding an additional un-pooling operation U as described in [17]. Thus, we can calculate the deconvolved output response from feature space to input space for any layer l in a multi-layer network as:\nRl = G1U1G2U2....Gl−1Ul−1Glzl (2)\nFor CLEAR maps, we specifically calculate the output responses from individual kernels of the last layer of a network. Hence, given a network with last layer L containing K = N kernels, we can calculate the attentive response map; R(x|c) (where x denotes the response back-projected to the input layer, and thus an array the same size as the input) for any class-specific kernel c (1 ≤ c ≤ N ) in the last layer as:\nR(x|c) = G1U1G2U2....GL−1UL−1GcLzL. (3)\nHere GcL represents the convolution matrix operation in which the kernel weights wL are all zero except that at the cth location.\nGiven the set of individual attentive response maps, we then compute the dominant attentive class map, Ĉ(x), by finding the class at each pixel that maximizes the attentive response level, R(x|c), across all classes:\nĈ(x) = argmax c\nR(x|c). (4)\nGiven the dominant attentive class map, Ĉ(x), we can now compute the dominant attentive response map, DĈ(x), by selecting the attentive response level at each pixel based on the identified dominant class, which can be expressed as follows:\nDĈ(x) = R(x|Ĉ). (5)\nTo form the final CLEAR map, we map the dominant class attentive map and the dominant attentive response map in the HSV color space as follows, then transform back into the RGB color space:\nH = F (Ĉ(x)),\nS = 1,\nV = DĈ(x).\n(6)\nHere F (.) is the color map dictionary that assigns an individual color to each dominant attentive class, c. Fig. 2 shows an example of the CLEAR map overlayed on the image."
    }, {
      "heading" : "4. Experiments",
      "text" : "In this section, we illustrate the efficacy of CLEAR maps for understanding and interpreting the decision-making of DCNNs. We conducted qualitative and quantitative experiments on three different datasets: the commonly used benchmarks MNIST and Street View House Numbers (SVHN), and the Stanford Dog dataset [8]. In the following section, we explain the experimental setup."
    }, {
      "heading" : "4.1. Setup",
      "text" : "To conduct experiments on three different datasets, we trained three different DCNN architectures with all convolutional layers1. For training on MNIST and SVHN, we set our network architecture similar to [14], as it has shown to perform very effectively for a variety of datasets. To train these networks, we used the default train and test split. We achieved an accuracy of 99.26% and 92.6% for the MNIST and SVHN datasets, respectively. For training on the Stanford dog dataset, we used a 16 layer VGG net [13], pretrained on ImageNet. We modified the VGG net slightly by removing the two last fully-connected layers and augmenting with two convolutional layers at the end. We fine-tuned the last two layers using the Stanford dog dataset. As with MNIST and SVHN, the default train and test split was applied to this dataset, but instead, we only took 10 different classes for training. We made this decision, as it would be an arduous task to interpret from all 120 classes. For this fine-grained classification task, we achieved an accuracy of 58.74% for 10 classes, whereas the state-of-the art for this dataset with 120 classes is 68%.\nIn all three networks, as the last layer (convolutional layer) was linearly connected to the softmax activation function, each kernel can be considered to represent one separate class. It is important to note that the aim was to understand and interpret the decision of a trained network; hence we did not strive to achieve the best architecture and state-of-the art results for each dataset. Using the previously mentioned setup, we conducted the following experiments.\n1Network architectures are shown in Appendix.\n0 1 2 3 4 5 6 7 8 9\nCorrectly classified Misclassified I II III I II III"
    }, {
      "heading" : "4.2. Qualitative Experiments: Understanding The",
      "text" : "Decision Making Process\nIn this set of experiments, we first create binary heatmaps and the proposed CLEAR maps for individual images in the three different datasets. The binary heatmaps represent which information in the image was used for or against the true class versus other image classes during classification.\nThe binary heatmaps were formed by overlaying the output response from the kernel representing the true class as “hot” regions and response of the rest of the kernels in the last layer, represented by green regions. The response for rest of the kernels is formed by performing max operation across the individual output responses. Thus, in the binary heatmaps, the hot regions and green regions represent the information for and against the actual class respectively, that was used for decision-making by the network. The binary heatmaps are constructed similarly to [19] and [10]. The CLEAR map formation is explained in Section 3 and Fig. 2.\nFor the SVHN and Stanford datasets, we also create an additional binary map. This map replaces the varying values in the binary heatmaps with a constant value. In the binary map, red and blue regions represent the information used for and against the class, respectively. We create these maps for visual clarity, as sometimes it is harder to visualize the green regions in the binary heatmaps. MNIST: Some of the randomly chosen results for the MNIST dataset are shown in Fig. 3. This figure shows examples of correctly classified and misclassified examples by the network. From these results, observations that can be made are: 1) Looking at the example sets for digit 0, although positive support is contributed by the same bottom curved features in both examples, only in one case is the image correctly identified as zero. Looking at the CLEAR maps, we can see the dominant activations for the correctly classified example corresponds to class 0, whereas for the misclassified case they correspond to class 5. 2) Similarly, for digit 7 and 8 it is difficult to interpret the decision output of the DCNN, but looking at the CLEAR maps make them more interpretable. SVHN: Presented similarly to the MNIST dataset, results obtained for the SVHN dataset are shown in Fig. 4. Some interesting observations are as follows: 1) For the misclassifed 0 digit, the heatmap overwhelmingly focuses on the correct curves; but the network still misclassifies it. This is counterintuitive to human interpretation. But when observing the CLEAR maps, we see that almost all the strong activations are for classes other than 0. 2) For the digit 9, it is difficult to interpret the binary heatmaps, as the positive kernel focuses on the digit 1, but it still correctly classifies the digit as 9 with high confidence. Observing the CLEAR maps, we see that most of the dominant activation in the focus areas belong to digit 9, including the ones for digit 1. Stanford Dog dataset: Results for Stanford Dogs are shown in Fig. 5–7. The key observations are the following: 1) Binary heatmaps can be used to find strong identifying features for different classes, as shown in Fig. 6. 2) In the same figure, the rightmost misclassifed cases present an interesting observation. We can observe that the network misclassifies the Chihuahua breed as Shiz-Tzu and the Ridge-\nback breed as Afghan Hound. This happens even when the positive kernels associated with their respective true class focus on the strong discriminating features, as identified by correctly classified images on the left. In Fig. 7, the CLEAR maps show that for Chihuahuas, the strong activations are for the Shih-Tzu, whereas for the Ridgeback, the activations are stronger for Afghan Hounds.\nBased on these results and observations, it is evident that binary maps are not enough for interpreting and explaining the individual decision outputs of a network. There is a strong motivation for class-based maps, such as CLEAR maps, that are more effective for understanding and interpreting the classification decisions made by a DCNN."
    }, {
      "heading" : "4.3. Quantitative Experiments",
      "text" : "To re-validate our observations for the MNIST and SVHN datasets, we conducted two different quantitative ex-\nperiments. In the first experiment, we removed all parts of the image, except for regions responsible for the activations of the kernel associated with the class of the image (positive kernel). We call these regions strong features associated with the class. For the MNIST dataset, we replace the digit with the background and for the SVHN dataset, we replace the region with a gray patch. In the second experi-\nment, we do the opposite: we remove the regions responsible for the kernel associated with the true class of the input\nChihuahua Jap. Spaniel Maltese Pekinese\nShih-Tzu B. Spaniel Papillon Toy Terrier\nR. Ridgeback Afghan Hound\nimage and keep the rest of the image. Results are shown in Table 1, and demonstrate that the identified strong features are vital for correctly classifying a particular class. For the case where the network is still able to classify without the strong features, albeit with half of the accuracy in comparison to the above case, an argument can be made that for these cases, the network focuses again on similar or redundant features. An example is digit 3, where there are redundant strong curve features."
    }, {
      "heading" : "5. Discussion",
      "text" : "This section discusses some general points associated with the CLEAR maps approach: 1) It is interesting to note that in Fig. 2, there is sparsity in the individual response maps from the last layer kernels. We observed the same pattern for all datasets considered. Evidence for classes tends to come from very specific localized regions. 2) For the datasets with a large number of classes, like the Stanford Dog dataset, we selected and created CLEAR maps with only 10 classes. We didn’t strive to show the CLEAR maps for all 120 classes, as doing so would make it extremely difficult to interpret the decision outputs. For such cases, perhaps showing the top 10 most activated class or several different maps with N classes would be a better approach. 3) In the current realization of our approach, we use deconvolution responses with only fully convolutional networks. We would like to point out that even though end-to-end\nlearning in this case is only possible with Fully Convolutional Nets (FCN), our approach can be extended to be used with different network architectures with the use different response methods, such as Layer-wise Relevance Propagation (LRP) [2], Deep Taylor decomposition [10], or prediction differential analysis [19]."
    }, {
      "heading" : "6. Conclusion",
      "text" : "In this work, a novel approach to better understanding and visualizing the decision-making process of DNNs was introduced in the form of CLass-Enhanced Attentive Response (CLEAR) maps. CLEAR maps are designed to enable the visualization of not only the areas of interest that predominantly influence the decision-making process, but also the degree of influence as well as the dominant class of influence in these areas. This multi-faceted look at the decision-making process allows for a better understanding of not only where but why certain decisions are made by DCNNs compared to existing heatmap-based approaches. Experiments using three different publicly available datasets were performed and show the efficacy of CLEAR maps both quantitatively and qualitatively. Furthermore, we demonstrated that strong areas of interest identified with CLEAR maps play a pivotal role in the correct classification of the class. Future work will explore extending CLEAR to facilitate for scenarios characterized by a large number of classes (i.e. greater than 10), as well as exploring CLEAR with different network architectures."
    }, {
      "heading" : "6.3. Stanford Dog",
      "text" : ""
    }, {
      "heading" : "6.2. SVHN",
      "text" : ""
    }, {
      "heading" : "6.1. MNIST",
      "text" : ""
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "In this work, we propose CLass-Enhanced Attentive Response (CLEAR): an approach to visualize and understand the decisions made by deep neural networks (DNNs) given a specific input. CLEAR facilitates the visualization of attentive regions and levels of interest of DNNs during the decision-making process. It also enables the visualization of the most dominant classes associated with these attentive regions of interest. As such, CLEAR can mitigate some of the shortcomings of heatmap-based methods associated with decision ambiguity, and allows for better insights into the decision-making process of DNNs. Quantitative and qualitative experiments across three different datasets demonstrate the efficacy of CLEAR for gaining a better understanding of the inner workings of DNNs during the decision-making process.",
    "creator" : "LaTeX with hyperref package"
  }
}