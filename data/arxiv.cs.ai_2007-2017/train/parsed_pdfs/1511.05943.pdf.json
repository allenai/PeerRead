{
  "name" : "1511.05943.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ "dipanp@andrew.cmu.edu", "msavvid@andrew.cmu.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n51 1.\n05 94\n3v 1\n[ cs\n.L G\n] 1\n8 N\nov 2\n01 5"
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "It is becoming increasingly important to learn well generalizing representations that are invariant to many common transformations of the data. These transformations can give rise to many ‘degrees of freedom’ even in a constrained task such as face recognition (e.g. pose, age-variation, illumination etc.). In fact, explicitly factoring them out leads to improvements in recognition performance as found in Leibo et al. (2014); Hinton (1987). To this end, the study of invariant features is important. Anselmi et al. (2013) showed that features that are explicitly invariant to intra-class transformations allow the sample complexity of the recognition problem to be reduced.\nPrior Art: Invariant Kernels. Kernel methods in machine learning have long been studied to considerable depth. Nonetheless, the study of invariant kernels and techniques to extract invariant features has received less attention. An invariant kernel allows the kernel product to remain invariant under transformations of the inputs. There has been some work on incorporating invariances into popular machinery such as the SVM in Lauer & Bloch (2008). Most instances of incorporating invariances focused on local invariances through regularization and optimization such as Schlkopf et al. (1996; 1998); Decoste & Schölkopf (2002); Zhang et al. (2013). Some other techniques were jittering kernels (Schölkopf & Smola (2002); Decoste & Schölkopf (2002)) and tangent-distance kernels (Haasdonk & Keysers (2002)), both of which sacrificed the positive semidefinite property of its kernels and were computationally expensive. Haasdonk & Burkhardt (2007) had first used group integration to arrive at invariant kernels, however, their approach does not address two important problems that arise in practice (group observation through unlabelled samples and partially observed groups). We will shortly state these problems more concretely and will show that the invariant kernels proposed do in fact solve both problems.\nPrior Art: Invariance through dataset augmentation. Many approaches in the past have enforced invariance through generating transformed training samples in some form such as Poggio & Vetter (1992); Schölkopf & Smola (2002); Schlkopf et al. (1998); Niyogi et al. (1998); Reisert (2008); Haasdonk & Burkhardt (2007). This assumes that one has knowledge about the transformation. The approach presented in this paper however, under the unitarity assumption, can learn the transformations through unlabelled samples and does not need training dataset augmentation. Perhaps, the\nmost popular method for incorporating invariances in SVMs is the virtual support method (VSV) in Schlkopf et al. (1996), which used sequential runs of SVMs in order to find and augment the support vectors with transformed versions of themselves. Loosli et al. (2007) proposed a similar algorithm to generate and prune out examples. Though these methods have had some success, most of them still lack explicit theoretical guarantees towards invariance. The proposed invariant kernel SVM formulation on the other hand, is guaranteed to be invariant. Further, unlike VSV and other approaches to incorporate invariance into the SVM, the proposed invariant kernel SVM solves the common and important practical problems that we will state shortly. To the best of our knowledge, it is the first formulation to do so.\nPrior Art: Linear Invariant Features. Recently, Anselmi et al. (2013) proposed linear groupinvariant features as an explanation for multiple characteristics of the visual cortex. They achieve invariance in a slightly more general way than group integration, utilizing measures of the distribution characterizing an orbit of a sample under the action group. We extend the method to the RKHS using unitary kernels and extend some properties regarding invariance and stability. We also show that the extension can solve both motivating problems (Problem 1 and Problem 2). This leads to a practical way of extracting non-linear invariant features with theoretical guarantees.\nMotivating Problems. We now state the two central problems that this paper tries to address through invariant kernels and features. A common practical problem that one faces utilizing previous methods involving generating transformed samples is the computational expense of generating and processing them (including virtual support vectors). Further, in many cases transformed labelled samples are unavailable.Two important problems that arise when practically applying invariant kernels and features are:\nProblem 1: (Group observed through unlabelled samples) The transformed versions of the training labelled data are not available i.e. one might only have access to transformed versions of unlabelled data outside of the training set (theoretically equivalent to having transformed versions of arbitrary vectors), e.g. only unlabelled transformed images are observed.\nProblem 2: (Partially observed group) Not all members of the group (symmetric set) of transformations G are observed i.e. the group is only partially observed through its actions, e.g. not all transformations of an image are observed. In many practical cases, partial invariance is in fact necessary, when a transformation from one class to another exists.\nGroup Theory and Invariance. Towards this goal, the study of incorporating invariance through group integration seems useful. Group theory is an elegant way to model symmetry. Classical invariant theory provides group integration techniques to enforce invariance. Group integration can also be used to model mean pooling (and max pooling albeit in a different framework as proposed in Anselmi et al. (2013)), which is in implicit use in several areas of machine learning and computer vision. The transformations, in this paper, are modelled as unitary and collectively form the unitarygroup G. Classes of learning problems, such as vision, often have transformations belonging to the unitary-group, that one would like to be invariant towards (such as translation and rotation). The results can also be extended to discrete groups. In practice however, Liao et al. (2013) found that invariance to much more general transformations not captured by this model can been achieved.\nWe will see that given explicit access to G, one can theoretically capitalize on properties such as guaranteed global invariance (as opposed to local invariance in optimization approaches, where the classifier is invariant to only small transformations). However, controlled local invariance can also be achieved. Local invariance is important when the extreme transformation of one class overlaps with another. The unitary property of the group and the unitary restriction on kernels (in Section 2.1), allow the development of theoretical motivation for existing techniques, an invariant kernel and invariant kernel features theoretically addressing Problems 1 and 2.\nContributions. We list our main contributions below:\n1. In contrast to many previous studies on invariant kernels, our focus is to study positive semi-definite unitary-group invariant kernels and features guaranteeing invariance that can address both Problem 1 and Problem 2 .\n2. One of our central results to applying group integration in the RKHS builds on the observation that, under unitary restrictions on the kernel map, group action is preserved in the RKHS.\n3. Using the proposed invariant kernel, we present a theoretically motivated alternate approach to designing a non-linear invariant SVM that can handle both Problem 1 and Problem 2 with explicit invariance guarantees.\n4. We propose kernel unitary-group invariant feature extraction techniques by extending a theory of linear group invariant features presented in Anselmi et al. (2013). We show that the kernel extension addresses both Problem 1 and Problem 2 and preserves properties such as global (and local) invariance and stability.\nOrganization. The paper is broadly organized into two parts. Section 2 and 3 present the proposed invariant kernels and the invariant kernel SVM, whereas Section 4 and 5 present the proposed invariant features extracted using kernels.\nSection 2 and 3 (Unitary-group Invariant Kernels). We first present some important known elementary unitary-group integration properties and present a central result to applying group integration in the RKHS in Section 2. We then present a theoretically motivated alternate approach to designing a non-linear invariant SVM and present a simple albeit important result to reduce computation. In Section 3, we then continue on to develop an invariant kernel which does not require to observe transformed versions of the input arguments whatsoever.\nSection 4 and 5 (Unitary-group Invariant Kernel Features). In Section 4, we propose kernel unitary-group invariant feature extraction techniques by extending a linear invariant feature extraction method (Anselmi et al. (2013)) to the kernel domain. We show that the resultant feature, while addressing Problem 1, preserves important properties such as global invariance and stability. In Section 5, we show that a simple extension of the method can help it to solve both problems (Problem 1 and Problem 2). This leads to a practical way of extracting invariant non-linear features with theoretical guarantees.\nSection 6 presents some experiments illustrating our methods."
    }, {
      "heading" : "2 GLOBALLY GROUP INVARIANT KERNELS: WHEN THE GROUP G IS EXPLICITLY KNOWN",
      "text" : "Premise: Consider a dataset of normalized samples along with labels X = {xi, yi} ∀i ∈ 1...N with x ∈ Rd and y ∈ {+1,−1}. We now introduce into the dataset a number of unitary transformations g part of the locally compact unitary-group G (in general we require local compactness to enable the existence of the Haar measure). Our augmented normalized dataset becomes XG = {gxi, yi} ∀g ∈ G ∀i1. Thus, X ⊆ XG . We assume for now that G is known and accessible completely. Let φ be some mapping to a high dimensional Hilbert space H, i.e. φ : X → H. Once the points are mapped, the problem of learning a separator in that space can be assumed to be linear.\nAn invariant function is defined as follows.\nDefinition 2.1 (G-Invariant Function). For any group G, we define a function f : X → Rn to be G-invariant if f(x) = f(gx) ∀x ∈ X ∀g ∈ G.\nOne method of generating an invariant towards a group is through group integration. Group integration has stemmed from classical invariant theory and its foundational theorem was proved by Haar.\nTheorem 2.1. (Haar) On every locally compact group there exists at least one left invariant integral. Such an integral is unique except for a strictly positive factor of proportionality. One can choose the factor of proportionality such that the group volume equals 1 (i.e. ∫ 1dg = 1, in the case of discrete finite groups each group element would be scaled down by 1|G| ). For compact groups, such an integral converges for any bounded function of the group. For discrete groups, the integral is replaced by a sum. Group integration can be shown to be a projection onto a G-invariant subspace. Such a subspace can be defined for a Hilbert space H by HG = {x ∈ H | x = gx ∀g ∈ G, ∀x ∈ H}. An invariant to any group G can be generated through the following basic (previously) known property (Lemma 2.2) based on group integration.\n1With a slight abuse of notation, we denote by gx the action of group element g ∈ G on x\nLemma 2.2. (Invariance Property) Given a vector ω ∈ Rd, and any group G, for any fixed g′ ∈ G and a normalized Haar measure dg, we have g′ ∫ G gω dg = ∫ G gω dg\nThe Haar measure (dg) exists for every locally compact group and is unique upto a positive multiplicative constant (hence normalized). A similar property holds for discrete groups. The Invariance Property results in global invariance to group G. This property allows one to generate a G-invariant subspace in the inherent space RN .\nThe following two lemmas (Lemma 2.3 and 2.4) showcase (novel) elementary properties of the operator Ψ = ∫ G g dg for the unitary-group G. These properties would prove useful in the analysis of unitary-group invariant kernels and features. Lemma 2.3. If Ψ = ∫ G g dg for unitary G, then Ψ T = Ψ Lemma 2.4. (Unitary Projection) If Ψ = ∫ G g dg for any G, then ΨΨ = Ψ, i.e. it is a projection operator. Further, if G is unitary, then 〈ω,Ψω′〉 = 〈Ψω, ω′〉 ∀ω, ω′ ∈ Rd\nThe proofs of these lemmas utilize elementary properties of groups, invariance of the Haar measure dg and the unitarity of g2.\nSample Complexity and Generalization. On applying the operator Ψ to the dataset X , all points in the set {gx | g ∈ G} for any x ∈ X map to the same point Ψx in the G-invariant subspace. Theoretically, this would drastically reduce sample complexity while preserving linear feasibility (separability). It is trivial to observe that a perfect linear separator learnt in XΨ = {Ψx | x ∈ X} would also be a perfect separator for XG , thus in theory achieving perfect generalization. We prove a similar result for the RKHS case in Section 2.2. This property is theoretically powerful since cardinality of G can be large. A classifier can avoid having to observe transformed versions {gx} of any x and yet generalize."
    }, {
      "heading" : "2.1 GROUP ACTIONS RECIPROCATE IN A REPRODUCING KERNEL HILBERT SPACE",
      "text" : "Group integration provides exact invariance in the domain of X . However, it requires the group structure to be preserved. In the context of kernels, it is imperative that the group relation between the samples x ∈ XG be preserved in the kernel Hilbert space H corresponding to some kernel k. Under the restriction of unitary k, this is possible. We present an elementary albeit important result that allows this after defining unitary kernels in the following sense.\nDefinition 2.2 (Unitary Kernel). We define a kernel k(x, y) = 〈φ(x), φ(y)〉 to be a unitary kernel if, for a unitary group G, the mapping φ(x) : X → H satisfies 〈φ(gx), φ(gy)〉 = 〈φ(x), φ(y)〉 ∀g ∈ G, ∀x, y ∈ X .\nThe unitary condition is fairly general, a common class of unitary kernels is the RBF kernel. We now define an operator gH : φ(x) → φ(gx) ∀φ(x) ∈ H for any g ∈ G where G is unitary. gH thus is a mapping within the RKHS. Under unitary G, we then have the following result.\nTheorem 2.5. (Covariance in the RKHS) If k(x, y) = 〈φ(x), φ(y)〉 is a unitary kernel in the sense of Definition 2.2, then gH is unitary, and the set GH = {gH | gH : φ(x) → φ(gx) ∀g ∈ G} is a unitary-group in H.\nTheorem 2.5 shows that the unitary-group structure is preserved in the RKHS. This provides new theoretically motivated approaches to achieve invariance in the RKHS. Specifically, a theory of invariance which was proposed to utilize unsupervised linear filters can now also utilize non-linear supervised ‘templates’ as we discuss in Section 4."
    }, {
      "heading" : "2.2 INVARIANT NON-LINEAR SVM: AN ALTERNATE APPROACH THROUGH GROUP INTEGRATION",
      "text" : "We present the group integration approach to kernel SVMs before comparing it to other methods. The decision function of SVMs can be written in the general form as fθ(x) = ωTφ(x) + b for some bias b ∈ R (we agglomerate all parameters of f in θ) where φ is the feature map, i.e. φ : X → H.\n2All proofs are presented in the supplementary material\nReviewing the SVM, a maximum margin separator is found by minimizing loss functions such as the hinge loss along with a regularizer. In order to invoke invariance, we can now utilize group integration in the the kernel space H using Theorem 2.5. All points in the set {gx ∈ XG} get mapped to φ(gx) = gHφ(x) for a given g ∈ G. Group integration then results in a G-invariant subspace within H through ΨH = ∫ GH gH dgH using Lemma 2.2. Introducing Lagrange multipliers α = (α1, α2...αN ) ∈ R N , the dual formulation (utilizing Lemma 2.3 and Lemma 2.4) then becomes\nargmin α\n− ∑\ni\nαi + 1\n2\n∑\ni\n∑\nj\nyiyjαiαj〈ΨHφ(xi),ΨHφ(xj)〉 (1)\nunder the constraints ∑\ni αiyi = 0, 0 ≤ αi ≤ 1 N ∀i. The separator is then given by\nω∗H = ∑ i yiαiΨHφ(xi) = ΨHω ∗ thereby existing in the GH-invariant (or equivalently G-invariant) subspace ΨH within H (since g → gH is a bijection). Effectively, the SVM observes samples from XΨH = {x | φ(x) = ΨHφ(u), ∀u ∈ XG}. If G is known, then this provides exact global invariance during testing. Further, ΨHω∗ is a maximum-margin separator of {φ(XG)}. This can be shown by the following result.\nTheorem 2.6. (Generalization) For a unitary group G and unitary kernel k(x, y) = 〈φ(x), φ(y)〉, if ω∗H = ∫ GH gH dgH ω\n∗ = ΨHω∗ is a perfect separator for {ΨHφ(X )} = {ΨHφ(x) | ∀x ∈ X}, then ΨHω∗ is also a perfect separator for {φ(XG)} = {φ(x) | x ∈ XG} with the same margin. Further, a max-margin separator of {ΨHφ(X )} is also a max-margin separator of {φ(XG)}.\nThe invariant non-linear SVM in objective 1, observes samples in the form of ΨHφ(x) and obtains a max-margin separator ΨHω∗. Theorem 2.6 shows that the margins of φ(XG) and {ΨHφ(X )} are deeply related and implies that ΨHφ(x) is a max-margin separator for both datasets. Theoretically, the Invariant non-linear SVM is able to generalize to XG on just observing X and utilizing prior information in the form of G for all unitary kernels k. This is true in practice for linear kernels. For non-linear kernels in practice, however, the invariant SVM still needs to observe and integrate over transformed training inputs. We also present the following result for unitary-group invariant kernels which helps in saving computation. Lemma 2.7. (Invariant Projection) If Ψ = ∫ G g dg for any unitary group G, then for any fixed g′ ∈ G we have 〈Ψω,Ψω′〉 = 〈g′ω,Ψω′〉 ∀ω, ω′ ∈ Rd\nWe provide the proof in the supplementary material. Thus, the kernel in the Invariant SVM formulation can be replaced by the form kΨ(x, y) = 〈φ(x),ΨHφ(y)〉, thereby reducing the number of transformed training samples required to be observed by an order of magnitude. It also allows for the kernel kΨ(x, y) to be invariant to the orbit of x, i.e. {gx} with observing just a single arbitrary point (g′x) on the orbit. Nonetheless, as the formulation stands, it still requires observing the entire orbit of atleast one of the transformed training samples. However, we can get around this fundamental problem as we show in the next section (Section 3).\nNote that for the general kernel, the GH-invariant subspace cannot be explicitly computed, it is only implicitly projected upon through ΨHφ(xi) = ∫ G φ(gxi)dgH. It is important to note that during testing however, the SVM formulation will be invariant to transformations of the test sample regardless of a linear or non-linear kernel. Also, interestingly, ω∗ might be a different decision boundary than ω ′∗ obtained by training the vanilla SVM on XG .\nPositive Semi-Definiteness. The G-invariant kernel map is now of the form kΨ(x, y) = 〈φ(x), ∫ G φ(gy)dgH〉. This preserves the positive semi-definite property of the kernel k while guaranteeing global invariance to unitary transformations., unlike jittering kernels (Schölkopf & Smola (2002); Decoste & Schölkopf (2002)) and tangent-distance kernels (Haasdonk & Keysers (2002)). If we wish to include invariance to scaling however, then we would lose positive-semi-definiteness (it is also not a unitary transform). Nonetheless, Walder & Chapelle (2007) show that conditionally positive definite kernels still exist for transformations including scaling, although we focus of unitary transformations in this paper.\nPartial Invariance. The invariant kernel SVM formulation (objective 1) also supports partial invariance when G is not fully observed (addressing Problem 2), a notion extended to invariant kernel\nmethods in Section 5. Partial invariance gives one control over the degree of invariance over transformation groups, allowing classes that are transformations of one another (such as MNIST classes 6 and 9) to be discriminated.\nRelating the Virtual Support Vector Method (VSV): Consider the popular Virtual Support Vector Method (VSV) (Schlkopf et al. (1996)). Here the support vectors are augmented with small (finite) number of transformed versions of themselves. This assumes that the transformations are explicitly known, thereby failing to address Problem 1. The augmented training set is used to train another SVM with improved invariance. We show in the following section that the Invariant SVM formulation (objective 1), on the other hand, does address Problem 1. The group integration framework provides a theoretical motivation for the VSV, since at minimum, it suggests having transformed versions of the support vectors. The VSV however, can have different αi for different transformed versions of a xi, whereas group integration would force them to be the same because the kernel kΨ(x, y) is G-invariant. For linear kernels we have more benefits. Group integration also suggests building an explicit G-invariant subspace before projecting the training set on it. This approach does not increase computation time (for linear kernels) while allowing the SVM to generalize to G-transformed inputs."
    }, {
      "heading" : "3 GLOBALLY GROUP INVARIANT KERNELS: WHEN ACTION OF GROUP G IS",
      "text" : "OBSERVED only ON unlabelled DATA\nThe previous section introduced a group integration approach to the invariant non-linear SVM. Although the formulation addresses Problem 2, it does not address Problem 1 i.e. the kernel kΨ(x, y) = 〈 ∫ G φ(gx)dgH, ∫ G φ(gy)dgH〉 = 〈ΨHφ(x),ΨHφ(y)〉 still requires observing transformed versions of the labelled input sample namely {gx | gx ∈ XG} (or atleast one of the labelled samples if we utilize Lemma 2.7). We now present an approach to not require the observation of any labelled training sample whatsoever.\nAssume that for every sample x ∈ XG , there exists a vector ux s.t. φ(x) ≈ φ(T )ux, where T is an arbitrary unlabelled set (in the form of a column-major matrix) of M arbitrary templates {ti} (Note that there exist more informed ways of choosing T , however to keep the theory general we work with arbitrary template sets). We assume that we have access to transformed versions of each template ti i.e. we observe G only through {gt | t ∈ T, g ∈ G}. We then have the following result. Theorem 3.1. For a unitary group G, a template set T ∈ Rd×M = {ti} and a unitary kernel k(x, y) = 〈φ(x), φ(y)〉, if φ(x) = φ(T )ux and φ(y) = φ(T )uy, then the G-invariant kernel kΨ(x, y) = 〈ΨHφ(x),ΨHφ(y)〉 can be written as\nkΨ(x, y) =\n∫\nG 〈φ(gT )ux, φ(T )uy〉dgH\nTheorem 3.1 assumes that the points φ(x) lies in the span of φ(T ). It allows the kernel kΨ(x, y) to be G-invariant for x, y i.e. kΨ(x, y) = kΨ(g′x, g′′y) ∀g′, g′′ ∈ G. It achieves this while only observing transformed versions of the unlabelled template set T . This is very useful since the use of Theorem 3.1 solves Problem 1 while guaranteeing invariance. Further, in practice, one does not need to have explicit knowledge of the transformations. In many cases, they can simply store the naturally transforming samples (e.g. transforming images). A constructed kernel can be applied to any dataset directly provided the same group G acts. Coefficients ux required for Theorem 3.1 for any x ∈ XG can be approximated by projecting the sample x onto the space spanned by T in the RKHS i.e. ux = (φ(T )Tφ(T ))−1φ(T )Tφ(x). This assumes that the kernel matrix (φ(T )Tφ(T ))−1 is invertible, a condition that can be satisfied by construction.\nInvariant Non-linear SVM through transformed unlabelled data (comparison with the VSV): The invariant kernel SVM in objective 1 using the invariant kernel kΨ(x, y) achieves invariance through learning the transformation only through observed unlabelled data. Further, it does not need multiple runs as opposed to the VSV which requires the generation of transformed labelled examples. Theorem 3.1 allows an invariant kernel to be used directly without the computational expense of finding potential support vectors, generating transformations of them and then processing the added samples. Further, invariance helps to reduce sample complexity and improve performance given a number of samples, a phenomenon we observe in our experiments."
    }, {
      "heading" : "4 GLOBALLY GROUP INVARIANT KERNEL FEATURES FROM A SINGLE",
      "text" : "SAMPLE: WHEN ACTION OF G IS OBSERVED only ON UNLABELLED DATA\nUp until now we have studied the properties of the proposed unitary group invariant kernels. We now shift our attention to group invariant features. Invariant kernels are a form of an invariant similarity measure and can be used to construct invariant feature maps. Anselmi et al. (2013) proposed linear invariant features that enjoys properties such as global invariance and stability. We extend their method to the RKHS using unitary kernels and extend the invariance and stability properties. We now briefly present their theory of invariance."
    }, {
      "heading" : "4.1 THEORY OF LINEAR INVARIANT FEATURES",
      "text" : "Under G, the orbit of any sample x ∈ X is defined by Ox = {gx | g ∈ G}. As a straightforward albeit elegant observation, the orbit itself is an invariant under G, since Ox = Ogx. Measures of such an orbit also provide invariance, such as the high dimensional distribution Px induced by the group’s action on x. In fact, Anselmi et al. (2013) show Px to be both invariant and unique, i.e. x ∼ x′ ⇔ Ox ∼ Ox′ ⇔ Px ∼ Px′ , where ∼ denotes membership in the same class. Thus, measures of the distribution, through a finite number of one-dimensional projections {P〈x,tk〉} K k=1, can be used as a similarity measure between two orbits 3. Further, the measures are invariant to the action of unitary group G. For unitary group G, normalized dot-products and an arbitrary template tk, an empirical estimate of the 1-dimensional distribution of the projection onto template tk can be expressed as µkn(x) = ∫ G ηn(〈x, gt\nk〉)dg, where ηn ∀n ∈ {1...N}, a non-linearity, can either estimate the n-th bin of the CDF or the n-th moment, the set of which together define P〈x,tk〉. In practice, Liao et al. (2013) found that a few or even one of these moments has been shown to be sufficiently invariant. The final signature or feature vector is ∆(x) = ({µ1n(x)}, {µ 2 n(x)}...{µ K n (x)}, ) ∈ R NK ∀n.\n4.2 GROUP INVARIANT FEATURE EXTRACTION IN KERNEL SPACE FROM A Single SAMPLE\nWe now present a kernel extension of the approach to invariance presented above. We assume access to the set U = {Otk | ∀k ∈ {1...K}}, i.e. the orbits of K arbitrary unlabelled vectors or templates. For simplicity, we also assume a compact unitary G with finite cardinality |G|. Then for every g′ ∈ G, we have template tkg′ = g\n′tk. Similarly, for unitary kernels (Definition 2.2), the templates in the RKHS behave as transformed versions of each other owing to Theorem 2.5. Therefore, tkHg′ = φ(t k g′ ) = g ′ Hφ(t\nk) ∀g′ ∈ G. Thus, {gφ(tk) | g ∈ G} form a set of transformed elements for each k under the action of G. Invariance can then be achieved using a form of Equation 7 in Anselmi et al. (2013).\nΥkn(x) = 1\n|G|\n∑\ng\nηn(〈φ(x), t k Hg〉) (2)\nΥ(x) can extract non-linear kernel features for any single sample x ∈ X that are invariant to the group G without ever needing to observe {gx | g ∈ G \\ I} 4. This also solves Problem 1 listed in the introduction. Recall that ηn can either estimate the CDF or the set of moments. In the case of moments, the first moment leads to mean pooling and the infinite moment results in max pooling. We now show that the kernel feature Υ(x) continues to satisfy useful properties such as stability (in the Lipschitz sense) i.e. a form of a stability result in Anselmi et al. (2013) can be proved using a similar analysis.\nTheorem 4.1. (Stability) If Υ(x) is invariant to a unitary-group G and the non-linearities ηn are Lipschitz continuous with constant Lηn , with Lη = maxn(Lηn) s.t. NLη ≤ 1√ 2 , and we have a normalized unitary kernel k with k(x, x) = 1, ∀x ∈ Rd, then\n‖Υ(x)−Υ(x′)‖22 < 1− k(x, x ′)H ≤ 1− k(x, x ′)\n3This follows from Cramer-Wold theorem along with concentration of measures. 4Note that even though the features extracted are non-linear, invariance generated is purely towards unitary\ntransformations.\nfor all x, x′ ∈ X . Here k(x, x′)H is the kernel distance in the Hausdorff sense in H, i.e. k(x, x′)H = maxg,g′∈G k(gx, g′x′).\nA good representation ideally should be stable and the distance between two points in the feature space should be bounded. Unstable representations can skew the feature space and allow for degenerate results. Theorem 4.1 shows that under Lipschitz continuity for the ηn estimation functions and k(x, x) = 1, ∀x ∈ Rd, the kernel feature distance is bounded by the kernel product.\nDiscriminative templates: Equation 2 can be instantiated to extract discriminative kernel features, by choosing discriminative instead of arbitrary templates. Let UH = φ(U), then for each group element g′ ∈ G, one can train K binary one vs. all classifiers with the kth template (g′tk) labelled as +1 and the rest ({gtk | g ∈ G \\ {g′}}) as −1 for all k. Recall that the separator ωkg′ (for template t k\nas +1 and for group element g′) can be expressed ωkg′ = ∑ i αiyiφ(g ′tk) = g′H( ∑ i αiyiφ(t k)) = g′Hω k I (using Theorem 2.5 and where I is the identity element of G). Thus, {ω k I , ..., ω k g′} form a set of transformed templates for each k under the action of G using which partial invariance can then be achieved through Equation 25."
    }, {
      "heading" : "5 TOWARDS PARTIALLY GROUP INVARIANT KERNELS: WHEN THE GROUP G",
      "text" : "IS partially OBSERVED THROUGH TRANSFORMED SAMPLES\nWe extend the notion of partial invariance to the kernel features extracted similar to Equation 2 following the analysis of Anselmi et al. (2013). Partial invariance arises from partially observing the group G, i.e. observing only a finite group (may not be a subgroup) G0 ⊆ G. In practice, this is the most likely case. However, partial invariance can be obtained over the observed subset G0 through a local kernel feature, which can also be generalized to locally compact groups. A partially invariant kernel feature is Υ̂kn(x) = 1 |G0| ∑ g∈G0 ηn(〈φ(x), ω k g 〉).\nUniqueness: The analysis for uniqueness in Anselmi et al. (2013) can be applied to Υ̂(x) with no significant changes, since the group structure is preserved in H through Theorem 2.5. In summary, any two partial orbits with a common point are identical. Invariance: Theorem 6 from Anselmi et al. (2013) can be applied in H with some modification.\nTheorem 5.1. (Partially Invariance) Let ηn : R → R+ are a set of bijective and positive functions and G be a locally compact group. Further, assuming G0 ⊆ G and supp(〈gx, ωkg 〉) ⊆ G0 (where supp() denotes the support), then ∀g ∈ G and ∀x ∈ Rd, we have Υ̂kn(x) = Υ̂ k n(gx).\nStability: Υ̂(x) is stable (in the Lipschitz sense) following the analysis of Theorem 4.1. In particular, we have the following result.\nTheorem 5.2. (Stability of Partially Invariant Feature) If Υ̂(x) is partially invariant to the group G0 and the non-linearities ηn are Lipschitz continuous with constant Lηn , with Lη = maxn(Lηn) s.t. NLη ≤ 1√ 2 , and we have kernel k with k(x, x) = 1, ∀x ∈ Rd, then for unitary G, if G0 ⊆ G and assuming supp(〈gx, ωkg 〉) ⊆ G0, then ||Υ̂(x) − Υ̂(x ′)||22 ≤ 1 − k(x, x ′) if |G0| = 1. Further, if |G0| > 1 then ||Υ̂(x)− Υ̂(x′)||22 < 1−k(x, x ′)H for all x, x′ ∈ X . Here k(x, x′)H is the kernel distance in the Hausdorff sense over G0 in H, i.e. k(x, x′)H = maxg,g′∈G0⊆G k(gx, g ′x′).\nThus Υ̂(x) can achieve partial invariance provided a limited number of transformations of the unlabelled data. Further, results developed for kernel methods in this section encourage their use in practice since the feature Υ̂(x) now solves both motivating problems mentioned in Section 1. Note that the notion of and results on partial invariance can be easily applied to the invariant kernel kΨ(x, y) proposed in Section 2 and 3 thereby making them practical tools with theoretical guarantees.\n5Since this is agnostic to the selection of α, any classifier which can be expressed as a linear combination of the samples in H (such as the perceptron, SVM, correlation filters) can be used as discriminative templates to generate invariance."
    }, {
      "heading" : "6 EXPERIMENTAL VALIDATION",
      "text" : "Goal: The goal of this section is two fold, to see (1) whether partially invariant kernel features Υ̂(x) and (2) invariant kernel SVM i.e. objective 1 coupled with Theorem 3.1, in practice, are able to address Problem 1 and Problem 2, and (3) whether kernel invariant features offer any advantage over linear invariant features µ(x). We refrain from using discriminative kernel features since our theoretical results does not assume any structure for the templates.\nSet-up and Method: We use 10 normalized datasets (each with number of samples ≥ 200) from the UCI ML repository for this task. We form a random 10-fold cross-validation partition (training (XTr)/testing (XTe)) for each dataset X . In order to enforce Problem 1, we introduce a number of transformations g belonging the a randomly chosen set of unitary transformations G0 into the test data (XTe) thereby multiplying the test data size by a factor of |G0|, thus obtaining XG0Te (we set |G0| = 10)6. However, we do not augment the training data XTr. We instead generate random vectors or templates t ∈ T and augment them using the same unitary transformations G0 as the test data (we set |T | = 100 for all experiments). This enforces Problem 1. Problem 2 is inherently enforced to a large degree since it is practically very difficult to generate an entire group. The transformations we introduce are a subset of the unitary group i.e. G0 ⊂ G with |G0| = 10.\nFor our first experiment, we compute Υ̂(x) using the randomly generated transformed templates TG0 and use the RBF kernel (σ = 1) and the polynomial kernel (k(x, y) = (〈x, y〉+1)d with degree d = 2). We set η to compute the infinite moment equivalent to max-pooling. As an evaluation, to estimate the separability of the data, we train a linear SVM on the unaugmented (not transformed) data (XTr) using (1) raw features (Raw baseline), (2) linear invariant features (µkn(x) baseline), (3) RBF kernel invariant features (Υ̂(x)RBF ) and (4) polynomial kernel invariant features (Υ̂(x)poly). We then test on the augmented corresponding fold (transformed) of the test data (XG0Te) after extracting the corresponding feature. We also report the test accuracy of testing on raw XTe as an illustration of the classification difficulty introduced by the transformations added in. The results are summarized\n6We uniformly set |G0| to a reasonably modest value of 10 in order to keep computational load of multiplying the dataset manageable.\nin Table 1. For our second experiment, we use the same datasets and generate a random 10-fold partition. Here we always train on the raw untransformed (XTr) fold and test on the raw transformed data (XG0Te). We train a standard RBF kernel SVM (σ = 1) and an invariant SVM using the same kernel as described in Theorem 3.1. We also test the standard kernel SVM on the untransformed data (XTe) as an illustration of the classification difficulty introduced due to transformations. The results are summarized in Table 2.\nResults: Our first observation is that in almost all of the datasets, even the modestly (|G0| = 10) added transformations significantly impaired the SVM’s performance (Table 1 and Table 2). Thus we confirm that most of the difficulty in the problem of learning arises from the presence of inherent transformations relating different orbits of the data. Secondly, for both experiments explicitly generating invariance through invariant features (Table 1) and through the invariant kernel (Table 2) helps in performance suggesting that in both cases sample complexity was lowered. We find that invariant kernel features and the invariant kernel (Theorem 3.1), in practice as well, address Problem 1 and Problem 2. Kernel features, in general, modestly outperform linear features in most of these datasets since even though the features are non-linear, the transformation they are invariant to are linear."
    }, {
      "heading" : "7 CONCLUSION",
      "text" : "One of the main handicaps in applying invariant kernel methods was the computational expense in generating and processing additional transformed form of the data. Further, in many cases it is difficult to generate such samples due to the transformation being unknown. However, in many cases, it is easier to obtain transformed unlabelled samples (such as video sequences in vision). The invariant kernels described in this paper can be used to address such issues while theoretically guaranteeing invariance."
    }, {
      "heading" : "8 SUPPLEMENTARY MATERIAL",
      "text" : ""
    }, {
      "heading" : "8.1 PROOF OF LEMMA 2.2",
      "text" : "Proof. We have,\ng′ ∫\nG gω dg =\n∫\nG g′gω dg =\n∫\nG g′′ω dg′′ =\n∫\nG gω dg\nSince the normalized Haar measure is invariant, i.e. dg = dg′. Intuitively, g′ simply rearranges the group integral owing to elementary group properties."
    }, {
      "heading" : "8.2 PROOF OF LEMMA 2.3",
      "text" : "Proof. We have,\nΨT = (\n∫\nG g dg)T =\n∫\nG gT dg =\n∫\nG g−1 dg−1 = Ψ\nUsing the fact g ∈ G ⇒ g−1 ∈ G and dg = dg−1."
    }, {
      "heading" : "8.3 PROOF LEMMA 2.4",
      "text" : "Proof. We have,\nΨΨ =\n∫\nG\n∫\nG gh dg dh =\n∫\nG\n∫\nG g′dg′dh =\n∫\nG dh\n∫\nG g′dg′ = Ψ\nSince the Haar measure is normalized ( ∫ G dg = 1), and invariant. Also for any ω, ω\n′ ∈ Rd, we have 〈ω,Ψω′〉 = ∫ G〈ω, gω ′〉dg = ∫ G〈g −1ω, ω′〉dg−1 = 〈Ψω, ω′〉"
    }, {
      "heading" : "8.4 PROOF OF LEMMA 2.7",
      "text" : "Proof. We have 〈Ψω,Ψω′〉 = 〈 ∫ g gω,Ψω′〉dg = 〈 ∫ g g′ω,Ψω′〉dg = 〈g′ω,Ψω′) ∫ g dg = 〈g′ω,Ψω′〉\nIn the second equality, we fix a group element g′ since the inner-product is invariant using the argument 〈φω,Ψω′〉 = 〈g′ω,Ψω′〉. This is true using Lemma 2.2 and the fact that G is unitary. Further, the final equality utilizes the fact that the Haar measure dg is normalized."
    }, {
      "heading" : "8.5 PROOF OF THEOREM 2.5",
      "text" : "Proof. We have 〈φ(gx), φ(gy)〉 = 〈φ(x), φ(y)〉 = 〈gHφ(x), gHφ(y)〉, since the kernel k is unitary. Here we define gHφ(x) as the action of gH on φ(x). Thus, the mapping gH preserves the dot-product in H while reciprocating the action of g. This is one of the requirements of a unitary operator, however gH needs to be linear. We note that linearity of gH can be derived from the linearity of the inner product and its preservation under gH in H. Specifically for an arbitrary vector p and a scalar α, we have\n||αgHp− gH(αp)|| 2 = 〈αgHp− gH(αp), αgHp− gH(αp)〉 (3)\n= ||αgHp|| 2 + ||gH(αp)|| 2 − 2〈αgHp, gH(αp)〉 (4)\n= |α|||p||2 + ||αp||2 − 2α2〈p, p〉 = 0 (5)\n(6)\nSimilarly for vectors p, q, we have ||gH(p+ q)− (gHp+ gHq)||2 = 0\nWe now prove that the set GH is a group. We start with proving the closure property. We have for any fixed gH, g′H ∈ GH\ngHg ′ Hφ(x) = gHφ(g ′x) = φ(gg′x) = φ(g′′x) = g′′Hφ(x)\nSince g′′ ∈ G therefore g′′H ∈ GH by definition. Also, gHg ′ H = g ′′ H and thus closure is established. Associativity, identity and inverse properties can be proved similarly. The set GH = {gH | gH : φ(x) → φ(gx) ∀g ∈ G} is therefore a unitary-group in H."
    }, {
      "heading" : "8.6 PROOF OF THEOREM 2.6",
      "text" : "Proof. Since ΨHω∗ is a perfect separator for {ΨHφ(X )}, ∃ρ′ > 0, s.t. mini yi(ΨHφ(xi))T (ΨHω∗) ≥ ρ′ ∀{xi, yi} ∈ X .\nUsing Lemma 2.4 and Theorem 2.5, we have for any fixed g′H ∈ GH,\n(ΨHφ(xi)) T (ΨHω ∗) = (g′Hφ(xi)) T (ΨHω ∗)\nHence,\nmin i\nyi(g ′ Hφ(xi)) T (ΨHω ∗) = min\ni yi(ΨHφ(xi))\nT (ΨHω ∗) ≥ ρ′ ∀(g′H ⇒ g) ∈ G\nThus, ΨHω∗ is perfect separator for {φ(XG)} with a margin of at-least ρ′. It also implies that a max-margin separator of {ΨHφ(X )} is also a max-margin separator of {φ(XG)}."
    }, {
      "heading" : "8.7 PROOF OF THEOREM 3.1",
      "text" : "Proof. For any fixed g′H we find, 〈ΨHφ(x),ΨHφ(y)〉 = 〈g ′ Hφ(x),ΨHφ(y)〉 using Lemma 2.4. Choosing g′H to be identity and substituting the expansion of ΨH, φ(x) = φ(T )ux and φ(y) = φ(T )uy we have the desired result."
    }, {
      "heading" : "8.8 PROOF OF THEOREM 4.1",
      "text" : "Proof. Since ηn are Lipschitz continuous ∀n, for each k component of the signature Υkn(x), we have\n||Υk(x)−Υk(x′)||2 RN\n≤ 1\n|G|2\n∑\nn\n  ∑\ng∈GH Lηn |〈φ(x), gHω\nk g 〉 − 〈φ(x ′), gHω k g 〉|\n  2\n(7)\n≤ L2η\n|G|2\n∑\nn\n  ∑\ng∈GH ||φ(x) − φ(x′)||H||gHω k g ||H\n  2\n(8)\n≤ N2L2η||φ(x) − φ(x ′)||H (9)\nwhere we utilize Cauchy-Schwartz, Theorem 2.5 and the fact that for some tk ∈ Rd, we have ||ωkg || 2 2 = ||φ(t k)||22 = 〈φ(t k), φ(tk)〉 = k(tk, tk) = 1. Since Υk(x) is invariant to the action of G (and consequently GH), ||Υk(x) − Υk(x′)||RN ≤ N2L2η mingH,g′H∈GH ||gHφ(x) − g ′ Hφ(x ′)||2H = N2L2η ming,g′∈G ||φ(gx) − φ(g ′x′)||2H = N 2L2η||φ(x) − φ(x ′)||2H = 2N 2L2η(1 − k(x, x ′)H). If NLη < 1√ 2\n, then the map is a contraction and we obtain the desired result by summing over all K components and dividing by K ."
    }, {
      "heading" : "8.9 PROOF OF THEOREM 5.1",
      "text" : "Proof. The proof is very similar to that of Theorem 6 in Anselmi et al. (2013) since Theorem 2.5 allows the group structure of GH to be preserved in H."
    }, {
      "heading" : "8.10 PROOF OF THEOREM 5.2",
      "text" : "Proof. The first condition follows the exact analysis as in Theorem 4.1. For the second condition to hold, we apply Theorem 5.1 and follow an argument very similar to that of Theorem 4.1."
    } ],
    "references" : [ {
      "title" : "Unsupervised learning of invariant representations in hierarchical architectures",
      "author" : [ "Anselmi", "Fabio", "Leibo", "Joel Z", "Rosasco", "Lorenzo", "Mutch", "Jim", "Tacchetti", "Andrea", "Poggio", "Tomaso" ],
      "venue" : "CoRR, abs/1311.4158,",
      "citeRegEx" : "Anselmi et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Anselmi et al\\.",
      "year" : 2013
    }, {
      "title" : "Training invariant support vector machines",
      "author" : [ "Decoste", "Dennis", "Schölkopf", "Bernhard" ],
      "venue" : "Mach. Learn.,",
      "citeRegEx" : "Decoste et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Decoste et al\\.",
      "year" : 2002
    }, {
      "title" : "Tangent distance kernels for support vector machines",
      "author" : [ "B. Haasdonk", "D. Keysers" ],
      "venue" : "In Pattern Recognition,",
      "citeRegEx" : "Haasdonk and Keysers,? \\Q2002\\E",
      "shortCiteRegEx" : "Haasdonk and Keysers",
      "year" : 2002
    }, {
      "title" : "Invariant kernel functions for pattern analysis and machine learning",
      "author" : [ "Haasdonk", "Bernard", "Burkhardt", "Hans" ],
      "venue" : "In Machine Learning,",
      "citeRegEx" : "Haasdonk et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Haasdonk et al\\.",
      "year" : 2007
    }, {
      "title" : "Learning translation invariant recognition in a massively parallel networks",
      "author" : [ "Hinton", "Geoffrey E" ],
      "venue" : "In PARLE Parallel Architectures and Languages Europe,",
      "citeRegEx" : "Hinton and E.,? \\Q1987\\E",
      "shortCiteRegEx" : "Hinton and E.",
      "year" : 1987
    }, {
      "title" : "Incorporating prior knowledge in support vector machines for classification: A review",
      "author" : [ "Lauer", "Fabien", "Bloch", "Gérard" ],
      "venue" : null,
      "citeRegEx" : "Lauer et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Lauer et al\\.",
      "year" : 2008
    }, {
      "title" : "Subtasks of unconstrained face recognition",
      "author" : [ "Leibo", "Joel Z", "Liao", "Qianli", "Poggio", "Tomaso" ],
      "venue" : "In International Joint Conference on Computer Vision, Imaging and Computer Graphics, VISIGRAPP,",
      "citeRegEx" : "Leibo et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Leibo et al\\.",
      "year" : 2014
    }, {
      "title" : "Learning invariant representations and applications to face verification",
      "author" : [ "Q. Liao", "J.Z. Leibo", "T. Poggio" ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Liao et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Liao et al\\.",
      "year" : 2013
    }, {
      "title" : "Training Invariant Support Vector Machines using Selective Sampling",
      "author" : [ "Loosli", "Gaëlle", "Canu", "Stéphane", "Bottou", "Léon" ],
      "venue" : "In Large Scale Kernel Machines,",
      "citeRegEx" : "Loosli et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Loosli et al\\.",
      "year" : 2007
    }, {
      "title" : "Incorporating prior information in machine learning by creating virtual examples",
      "author" : [ "P. Niyogi", "F. Girosi", "T. Poggio" ],
      "venue" : "In Proceedings of the IEEE,",
      "citeRegEx" : "Niyogi et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Niyogi et al\\.",
      "year" : 1998
    }, {
      "title" : "Recognition and structure from one 2d model view: Observations on prototypes, object classes and symmetries",
      "author" : [ "T. Poggio", "T. Vetter" ],
      "venue" : "Laboratory, Massachusetts Institute of Technology,",
      "citeRegEx" : "Poggio and Vetter,? \\Q1992\\E",
      "shortCiteRegEx" : "Poggio and Vetter",
      "year" : 1992
    }, {
      "title" : "Group integration techniques in pattern analysis a kernel view",
      "author" : [ "Reisert", "Marco" ],
      "venue" : "PhD Thesis,",
      "citeRegEx" : "Reisert and Marco.,? \\Q2008\\E",
      "shortCiteRegEx" : "Reisert and Marco.",
      "year" : 2008
    }, {
      "title" : "Prior knowledge in support vector kernels",
      "author" : [ "B. Schlkopf", "P. Simard", "A. Smola", "V. Vapnik" ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Schlkopf et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Schlkopf et al\\.",
      "year" : 1998
    }, {
      "title" : "Incorporating invariances in support vector learning machines",
      "author" : [ "Schlkopf", "Bernhard", "Burges", "Chris", "Vapnik", "Vladimir" ],
      "venue" : null,
      "citeRegEx" : "Schlkopf et al\\.,? \\Q1996\\E",
      "shortCiteRegEx" : "Schlkopf et al\\.",
      "year" : 1996
    }, {
      "title" : "Learning with kernels: Support vector machines, regularization, optimization, and beyond",
      "author" : [ "Schölkopf", "Bernhard", "Smola", "Alexander J" ],
      "venue" : "MIT press,",
      "citeRegEx" : "Schölkopf et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Schölkopf et al\\.",
      "year" : 2002
    }, {
      "title" : "Learning with transformation invariant kernels",
      "author" : [ "Walder", "Christian", "Chapelle", "Olivier" ],
      "venue" : "In Advances in Neural Information Processing Systems, pp",
      "citeRegEx" : "Walder et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Walder et al\\.",
      "year" : 2007
    }, {
      "title" : "Learning with invariance via linear functionals on reproducing kernel hilbert space",
      "author" : [ "Zhang", "Xinhua", "Lee", "Wee Sun", "Teh", "Yee Whye" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Zhang et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : "In fact, explicitly factoring them out leads to improvements in recognition performance as found in Leibo et al. (2014); Hinton (1987).",
      "startOffset" : 100,
      "endOffset" : 120
    }, {
      "referenceID" : 5,
      "context" : "In fact, explicitly factoring them out leads to improvements in recognition performance as found in Leibo et al. (2014); Hinton (1987). To this end, the study of invariant features is important.",
      "startOffset" : 100,
      "endOffset" : 135
    }, {
      "referenceID" : 0,
      "context" : "Anselmi et al. (2013) showed that features that are explicitly invariant to intra-class transformations allow the sample complexity of the recognition problem to be reduced.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 0,
      "context" : "Anselmi et al. (2013) showed that features that are explicitly invariant to intra-class transformations allow the sample complexity of the recognition problem to be reduced. Prior Art: Invariant Kernels. Kernel methods in machine learning have long been studied to considerable depth. Nonetheless, the study of invariant kernels and techniques to extract invariant features has received less attention. An invariant kernel allows the kernel product to remain invariant under transformations of the inputs. There has been some work on incorporating invariances into popular machinery such as the SVM in Lauer & Bloch (2008). Most instances of incorporating invariances focused on local invariances through regularization and optimization such as Schlkopf et al.",
      "startOffset" : 0,
      "endOffset" : 623
    }, {
      "referenceID" : 0,
      "context" : "Anselmi et al. (2013) showed that features that are explicitly invariant to intra-class transformations allow the sample complexity of the recognition problem to be reduced. Prior Art: Invariant Kernels. Kernel methods in machine learning have long been studied to considerable depth. Nonetheless, the study of invariant kernels and techniques to extract invariant features has received less attention. An invariant kernel allows the kernel product to remain invariant under transformations of the inputs. There has been some work on incorporating invariances into popular machinery such as the SVM in Lauer & Bloch (2008). Most instances of incorporating invariances focused on local invariances through regularization and optimization such as Schlkopf et al. (1996; 1998); Decoste & Schölkopf (2002); Zhang et al.",
      "startOffset" : 0,
      "endOffset" : 802
    }, {
      "referenceID" : 0,
      "context" : "Anselmi et al. (2013) showed that features that are explicitly invariant to intra-class transformations allow the sample complexity of the recognition problem to be reduced. Prior Art: Invariant Kernels. Kernel methods in machine learning have long been studied to considerable depth. Nonetheless, the study of invariant kernels and techniques to extract invariant features has received less attention. An invariant kernel allows the kernel product to remain invariant under transformations of the inputs. There has been some work on incorporating invariances into popular machinery such as the SVM in Lauer & Bloch (2008). Most instances of incorporating invariances focused on local invariances through regularization and optimization such as Schlkopf et al. (1996; 1998); Decoste & Schölkopf (2002); Zhang et al. (2013). Some other techniques were jittering kernels (Schölkopf & Smola (2002); Decoste & Schölkopf (2002)) and tangent-distance kernels (Haasdonk & Keysers (2002)), both of which sacrificed the positive semidefinite property of its kernels and were computationally expensive.",
      "startOffset" : 0,
      "endOffset" : 823
    }, {
      "referenceID" : 0,
      "context" : "Anselmi et al. (2013) showed that features that are explicitly invariant to intra-class transformations allow the sample complexity of the recognition problem to be reduced. Prior Art: Invariant Kernels. Kernel methods in machine learning have long been studied to considerable depth. Nonetheless, the study of invariant kernels and techniques to extract invariant features has received less attention. An invariant kernel allows the kernel product to remain invariant under transformations of the inputs. There has been some work on incorporating invariances into popular machinery such as the SVM in Lauer & Bloch (2008). Most instances of incorporating invariances focused on local invariances through regularization and optimization such as Schlkopf et al. (1996; 1998); Decoste & Schölkopf (2002); Zhang et al. (2013). Some other techniques were jittering kernels (Schölkopf & Smola (2002); Decoste & Schölkopf (2002)) and tangent-distance kernels (Haasdonk & Keysers (2002)), both of which sacrificed the positive semidefinite property of its kernels and were computationally expensive.",
      "startOffset" : 0,
      "endOffset" : 895
    }, {
      "referenceID" : 0,
      "context" : "Anselmi et al. (2013) showed that features that are explicitly invariant to intra-class transformations allow the sample complexity of the recognition problem to be reduced. Prior Art: Invariant Kernels. Kernel methods in machine learning have long been studied to considerable depth. Nonetheless, the study of invariant kernels and techniques to extract invariant features has received less attention. An invariant kernel allows the kernel product to remain invariant under transformations of the inputs. There has been some work on incorporating invariances into popular machinery such as the SVM in Lauer & Bloch (2008). Most instances of incorporating invariances focused on local invariances through regularization and optimization such as Schlkopf et al. (1996; 1998); Decoste & Schölkopf (2002); Zhang et al. (2013). Some other techniques were jittering kernels (Schölkopf & Smola (2002); Decoste & Schölkopf (2002)) and tangent-distance kernels (Haasdonk & Keysers (2002)), both of which sacrificed the positive semidefinite property of its kernels and were computationally expensive.",
      "startOffset" : 0,
      "endOffset" : 923
    }, {
      "referenceID" : 0,
      "context" : "Anselmi et al. (2013) showed that features that are explicitly invariant to intra-class transformations allow the sample complexity of the recognition problem to be reduced. Prior Art: Invariant Kernels. Kernel methods in machine learning have long been studied to considerable depth. Nonetheless, the study of invariant kernels and techniques to extract invariant features has received less attention. An invariant kernel allows the kernel product to remain invariant under transformations of the inputs. There has been some work on incorporating invariances into popular machinery such as the SVM in Lauer & Bloch (2008). Most instances of incorporating invariances focused on local invariances through regularization and optimization such as Schlkopf et al. (1996; 1998); Decoste & Schölkopf (2002); Zhang et al. (2013). Some other techniques were jittering kernels (Schölkopf & Smola (2002); Decoste & Schölkopf (2002)) and tangent-distance kernels (Haasdonk & Keysers (2002)), both of which sacrificed the positive semidefinite property of its kernels and were computationally expensive.",
      "startOffset" : 0,
      "endOffset" : 980
    }, {
      "referenceID" : 0,
      "context" : "Anselmi et al. (2013) showed that features that are explicitly invariant to intra-class transformations allow the sample complexity of the recognition problem to be reduced. Prior Art: Invariant Kernels. Kernel methods in machine learning have long been studied to considerable depth. Nonetheless, the study of invariant kernels and techniques to extract invariant features has received less attention. An invariant kernel allows the kernel product to remain invariant under transformations of the inputs. There has been some work on incorporating invariances into popular machinery such as the SVM in Lauer & Bloch (2008). Most instances of incorporating invariances focused on local invariances through regularization and optimization such as Schlkopf et al. (1996; 1998); Decoste & Schölkopf (2002); Zhang et al. (2013). Some other techniques were jittering kernels (Schölkopf & Smola (2002); Decoste & Schölkopf (2002)) and tangent-distance kernels (Haasdonk & Keysers (2002)), both of which sacrificed the positive semidefinite property of its kernels and were computationally expensive. Haasdonk & Burkhardt (2007) had first used group integration to arrive at invariant kernels, however, their approach does not address two important problems that arise in practice (group observation through unlabelled samples and partially observed groups).",
      "startOffset" : 0,
      "endOffset" : 1121
    }, {
      "referenceID" : 0,
      "context" : "Anselmi et al. (2013) showed that features that are explicitly invariant to intra-class transformations allow the sample complexity of the recognition problem to be reduced. Prior Art: Invariant Kernels. Kernel methods in machine learning have long been studied to considerable depth. Nonetheless, the study of invariant kernels and techniques to extract invariant features has received less attention. An invariant kernel allows the kernel product to remain invariant under transformations of the inputs. There has been some work on incorporating invariances into popular machinery such as the SVM in Lauer & Bloch (2008). Most instances of incorporating invariances focused on local invariances through regularization and optimization such as Schlkopf et al. (1996; 1998); Decoste & Schölkopf (2002); Zhang et al. (2013). Some other techniques were jittering kernels (Schölkopf & Smola (2002); Decoste & Schölkopf (2002)) and tangent-distance kernels (Haasdonk & Keysers (2002)), both of which sacrificed the positive semidefinite property of its kernels and were computationally expensive. Haasdonk & Burkhardt (2007) had first used group integration to arrive at invariant kernels, however, their approach does not address two important problems that arise in practice (group observation through unlabelled samples and partially observed groups). We will shortly state these problems more concretely and will show that the invariant kernels proposed do in fact solve both problems. Prior Art: Invariance through dataset augmentation. Many approaches in the past have enforced invariance through generating transformed training samples in some form such as Poggio & Vetter (1992); Schölkopf & Smola (2002); Schlkopf et al.",
      "startOffset" : 0,
      "endOffset" : 1683
    }, {
      "referenceID" : 0,
      "context" : "Anselmi et al. (2013) showed that features that are explicitly invariant to intra-class transformations allow the sample complexity of the recognition problem to be reduced. Prior Art: Invariant Kernels. Kernel methods in machine learning have long been studied to considerable depth. Nonetheless, the study of invariant kernels and techniques to extract invariant features has received less attention. An invariant kernel allows the kernel product to remain invariant under transformations of the inputs. There has been some work on incorporating invariances into popular machinery such as the SVM in Lauer & Bloch (2008). Most instances of incorporating invariances focused on local invariances through regularization and optimization such as Schlkopf et al. (1996; 1998); Decoste & Schölkopf (2002); Zhang et al. (2013). Some other techniques were jittering kernels (Schölkopf & Smola (2002); Decoste & Schölkopf (2002)) and tangent-distance kernels (Haasdonk & Keysers (2002)), both of which sacrificed the positive semidefinite property of its kernels and were computationally expensive. Haasdonk & Burkhardt (2007) had first used group integration to arrive at invariant kernels, however, their approach does not address two important problems that arise in practice (group observation through unlabelled samples and partially observed groups). We will shortly state these problems more concretely and will show that the invariant kernels proposed do in fact solve both problems. Prior Art: Invariance through dataset augmentation. Many approaches in the past have enforced invariance through generating transformed training samples in some form such as Poggio & Vetter (1992); Schölkopf & Smola (2002); Schlkopf et al.",
      "startOffset" : 0,
      "endOffset" : 1709
    }, {
      "referenceID" : 0,
      "context" : "Anselmi et al. (2013) showed that features that are explicitly invariant to intra-class transformations allow the sample complexity of the recognition problem to be reduced. Prior Art: Invariant Kernels. Kernel methods in machine learning have long been studied to considerable depth. Nonetheless, the study of invariant kernels and techniques to extract invariant features has received less attention. An invariant kernel allows the kernel product to remain invariant under transformations of the inputs. There has been some work on incorporating invariances into popular machinery such as the SVM in Lauer & Bloch (2008). Most instances of incorporating invariances focused on local invariances through regularization and optimization such as Schlkopf et al. (1996; 1998); Decoste & Schölkopf (2002); Zhang et al. (2013). Some other techniques were jittering kernels (Schölkopf & Smola (2002); Decoste & Schölkopf (2002)) and tangent-distance kernels (Haasdonk & Keysers (2002)), both of which sacrificed the positive semidefinite property of its kernels and were computationally expensive. Haasdonk & Burkhardt (2007) had first used group integration to arrive at invariant kernels, however, their approach does not address two important problems that arise in practice (group observation through unlabelled samples and partially observed groups). We will shortly state these problems more concretely and will show that the invariant kernels proposed do in fact solve both problems. Prior Art: Invariance through dataset augmentation. Many approaches in the past have enforced invariance through generating transformed training samples in some form such as Poggio & Vetter (1992); Schölkopf & Smola (2002); Schlkopf et al. (1998); Niyogi et al.",
      "startOffset" : 0,
      "endOffset" : 1733
    }, {
      "referenceID" : 0,
      "context" : "Anselmi et al. (2013) showed that features that are explicitly invariant to intra-class transformations allow the sample complexity of the recognition problem to be reduced. Prior Art: Invariant Kernels. Kernel methods in machine learning have long been studied to considerable depth. Nonetheless, the study of invariant kernels and techniques to extract invariant features has received less attention. An invariant kernel allows the kernel product to remain invariant under transformations of the inputs. There has been some work on incorporating invariances into popular machinery such as the SVM in Lauer & Bloch (2008). Most instances of incorporating invariances focused on local invariances through regularization and optimization such as Schlkopf et al. (1996; 1998); Decoste & Schölkopf (2002); Zhang et al. (2013). Some other techniques were jittering kernels (Schölkopf & Smola (2002); Decoste & Schölkopf (2002)) and tangent-distance kernels (Haasdonk & Keysers (2002)), both of which sacrificed the positive semidefinite property of its kernels and were computationally expensive. Haasdonk & Burkhardt (2007) had first used group integration to arrive at invariant kernels, however, their approach does not address two important problems that arise in practice (group observation through unlabelled samples and partially observed groups). We will shortly state these problems more concretely and will show that the invariant kernels proposed do in fact solve both problems. Prior Art: Invariance through dataset augmentation. Many approaches in the past have enforced invariance through generating transformed training samples in some form such as Poggio & Vetter (1992); Schölkopf & Smola (2002); Schlkopf et al. (1998); Niyogi et al. (1998); Reisert (2008); Haasdonk & Burkhardt (2007).",
      "startOffset" : 0,
      "endOffset" : 1755
    }, {
      "referenceID" : 0,
      "context" : "Anselmi et al. (2013) showed that features that are explicitly invariant to intra-class transformations allow the sample complexity of the recognition problem to be reduced. Prior Art: Invariant Kernels. Kernel methods in machine learning have long been studied to considerable depth. Nonetheless, the study of invariant kernels and techniques to extract invariant features has received less attention. An invariant kernel allows the kernel product to remain invariant under transformations of the inputs. There has been some work on incorporating invariances into popular machinery such as the SVM in Lauer & Bloch (2008). Most instances of incorporating invariances focused on local invariances through regularization and optimization such as Schlkopf et al. (1996; 1998); Decoste & Schölkopf (2002); Zhang et al. (2013). Some other techniques were jittering kernels (Schölkopf & Smola (2002); Decoste & Schölkopf (2002)) and tangent-distance kernels (Haasdonk & Keysers (2002)), both of which sacrificed the positive semidefinite property of its kernels and were computationally expensive. Haasdonk & Burkhardt (2007) had first used group integration to arrive at invariant kernels, however, their approach does not address two important problems that arise in practice (group observation through unlabelled samples and partially observed groups). We will shortly state these problems more concretely and will show that the invariant kernels proposed do in fact solve both problems. Prior Art: Invariance through dataset augmentation. Many approaches in the past have enforced invariance through generating transformed training samples in some form such as Poggio & Vetter (1992); Schölkopf & Smola (2002); Schlkopf et al. (1998); Niyogi et al. (1998); Reisert (2008); Haasdonk & Burkhardt (2007).",
      "startOffset" : 0,
      "endOffset" : 1771
    }, {
      "referenceID" : 0,
      "context" : "Anselmi et al. (2013) showed that features that are explicitly invariant to intra-class transformations allow the sample complexity of the recognition problem to be reduced. Prior Art: Invariant Kernels. Kernel methods in machine learning have long been studied to considerable depth. Nonetheless, the study of invariant kernels and techniques to extract invariant features has received less attention. An invariant kernel allows the kernel product to remain invariant under transformations of the inputs. There has been some work on incorporating invariances into popular machinery such as the SVM in Lauer & Bloch (2008). Most instances of incorporating invariances focused on local invariances through regularization and optimization such as Schlkopf et al. (1996; 1998); Decoste & Schölkopf (2002); Zhang et al. (2013). Some other techniques were jittering kernels (Schölkopf & Smola (2002); Decoste & Schölkopf (2002)) and tangent-distance kernels (Haasdonk & Keysers (2002)), both of which sacrificed the positive semidefinite property of its kernels and were computationally expensive. Haasdonk & Burkhardt (2007) had first used group integration to arrive at invariant kernels, however, their approach does not address two important problems that arise in practice (group observation through unlabelled samples and partially observed groups). We will shortly state these problems more concretely and will show that the invariant kernels proposed do in fact solve both problems. Prior Art: Invariance through dataset augmentation. Many approaches in the past have enforced invariance through generating transformed training samples in some form such as Poggio & Vetter (1992); Schölkopf & Smola (2002); Schlkopf et al. (1998); Niyogi et al. (1998); Reisert (2008); Haasdonk & Burkhardt (2007). This assumes that one has knowledge about the transformation.",
      "startOffset" : 0,
      "endOffset" : 1800
    }, {
      "referenceID" : 9,
      "context" : "most popular method for incorporating invariances in SVMs is the virtual support method (VSV) in Schlkopf et al. (1996), which used sequential runs of SVMs in order to find and augment the support vectors with transformed versions of themselves.",
      "startOffset" : 97,
      "endOffset" : 120
    }, {
      "referenceID" : 6,
      "context" : "Loosli et al. (2007) proposed a similar algorithm to generate and prune out examples.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 0,
      "context" : "Recently, Anselmi et al. (2013) proposed linear groupinvariant features as an explanation for multiple characteristics of the visual cortex.",
      "startOffset" : 10,
      "endOffset" : 32
    }, {
      "referenceID" : 0,
      "context" : "Recently, Anselmi et al. (2013) proposed linear groupinvariant features as an explanation for multiple characteristics of the visual cortex. They achieve invariance in a slightly more general way than group integration, utilizing measures of the distribution characterizing an orbit of a sample under the action group. We extend the method to the RKHS using unitary kernels and extend some properties regarding invariance and stability. We also show that the extension can solve both motivating problems (Problem 1 and Problem 2). This leads to a practical way of extracting non-linear invariant features with theoretical guarantees. Motivating Problems. We now state the two central problems that this paper tries to address through invariant kernels and features. A common practical problem that one faces utilizing previous methods involving generating transformed samples is the computational expense of generating and processing them (including virtual support vectors). Further, in many cases transformed labelled samples are unavailable.Two important problems that arise when practically applying invariant kernels and features are: Problem 1: (Group observed through unlabelled samples) The transformed versions of the training labelled data are not available i.e. one might only have access to transformed versions of unlabelled data outside of the training set (theoretically equivalent to having transformed versions of arbitrary vectors), e.g. only unlabelled transformed images are observed. Problem 2: (Partially observed group) Not all members of the group (symmetric set) of transformations G are observed i.e. the group is only partially observed through its actions, e.g. not all transformations of an image are observed. In many practical cases, partial invariance is in fact necessary, when a transformation from one class to another exists. Group Theory and Invariance. Towards this goal, the study of incorporating invariance through group integration seems useful. Group theory is an elegant way to model symmetry. Classical invariant theory provides group integration techniques to enforce invariance. Group integration can also be used to model mean pooling (and max pooling albeit in a different framework as proposed in Anselmi et al. (2013)), which is in implicit use in several areas of machine learning and computer vision.",
      "startOffset" : 10,
      "endOffset" : 2269
    }, {
      "referenceID" : 0,
      "context" : "Recently, Anselmi et al. (2013) proposed linear groupinvariant features as an explanation for multiple characteristics of the visual cortex. They achieve invariance in a slightly more general way than group integration, utilizing measures of the distribution characterizing an orbit of a sample under the action group. We extend the method to the RKHS using unitary kernels and extend some properties regarding invariance and stability. We also show that the extension can solve both motivating problems (Problem 1 and Problem 2). This leads to a practical way of extracting non-linear invariant features with theoretical guarantees. Motivating Problems. We now state the two central problems that this paper tries to address through invariant kernels and features. A common practical problem that one faces utilizing previous methods involving generating transformed samples is the computational expense of generating and processing them (including virtual support vectors). Further, in many cases transformed labelled samples are unavailable.Two important problems that arise when practically applying invariant kernels and features are: Problem 1: (Group observed through unlabelled samples) The transformed versions of the training labelled data are not available i.e. one might only have access to transformed versions of unlabelled data outside of the training set (theoretically equivalent to having transformed versions of arbitrary vectors), e.g. only unlabelled transformed images are observed. Problem 2: (Partially observed group) Not all members of the group (symmetric set) of transformations G are observed i.e. the group is only partially observed through its actions, e.g. not all transformations of an image are observed. In many practical cases, partial invariance is in fact necessary, when a transformation from one class to another exists. Group Theory and Invariance. Towards this goal, the study of incorporating invariance through group integration seems useful. Group theory is an elegant way to model symmetry. Classical invariant theory provides group integration techniques to enforce invariance. Group integration can also be used to model mean pooling (and max pooling albeit in a different framework as proposed in Anselmi et al. (2013)), which is in implicit use in several areas of machine learning and computer vision. The transformations, in this paper, are modelled as unitary and collectively form the unitarygroup G. Classes of learning problems, such as vision, often have transformations belonging to the unitary-group, that one would like to be invariant towards (such as translation and rotation). The results can also be extended to discrete groups. In practice however, Liao et al. (2013) found that invariance to much more general transformations not captured by this model can been achieved.",
      "startOffset" : 10,
      "endOffset" : 2734
    }, {
      "referenceID" : 0,
      "context" : "We propose kernel unitary-group invariant feature extraction techniques by extending a theory of linear group invariant features presented in Anselmi et al. (2013). We show that the kernel extension addresses both Problem 1 and Problem 2 and preserves properties such as global (and local) invariance and stability.",
      "startOffset" : 142,
      "endOffset" : 164
    }, {
      "referenceID" : 0,
      "context" : "In Section 4, we propose kernel unitary-group invariant feature extraction techniques by extending a linear invariant feature extraction method (Anselmi et al. (2013)) to the kernel domain.",
      "startOffset" : 145,
      "endOffset" : 167
    }, {
      "referenceID" : 12,
      "context" : "Relating the Virtual Support Vector Method (VSV): Consider the popular Virtual Support Vector Method (VSV) (Schlkopf et al. (1996)).",
      "startOffset" : 108,
      "endOffset" : 131
    }, {
      "referenceID" : 0,
      "context" : "Anselmi et al. (2013) proposed linear invariant features that enjoys properties such as global invariance and stability.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 0,
      "context" : "In fact, Anselmi et al. (2013) show Px to be both invariant and unique, i.",
      "startOffset" : 9,
      "endOffset" : 31
    }, {
      "referenceID" : 0,
      "context" : "In fact, Anselmi et al. (2013) show Px to be both invariant and unique, i.e. x ∼ x′ ⇔ Ox ∼ Ox′ ⇔ Px ∼ Px′ , where ∼ denotes membership in the same class. Thus, measures of the distribution, through a finite number of one-dimensional projections {P〈x,tk〉} K k=1, can be used as a similarity measure between two orbits 3. Further, the measures are invariant to the action of unitary group G. For unitary group G, normalized dot-products and an arbitrary template t, an empirical estimate of the 1-dimensional distribution of the projection onto template t can be expressed as μn(x) = ∫ G ηn(〈x, gt 〉)dg, where ηn ∀n ∈ {1...N}, a non-linearity, can either estimate the n-th bin of the CDF or the n-th moment, the set of which together define P〈x,tk〉. In practice, Liao et al. (2013) found that a few or even one of these moments has been shown to be sufficiently invariant.",
      "startOffset" : 9,
      "endOffset" : 780
    }, {
      "referenceID" : 0,
      "context" : "Invariance can then be achieved using a form of Equation 7 in Anselmi et al. (2013).",
      "startOffset" : 62,
      "endOffset" : 84
    }, {
      "referenceID" : 0,
      "context" : "a form of a stability result in Anselmi et al. (2013) can be proved using a similar analysis.",
      "startOffset" : 32,
      "endOffset" : 54
    }, {
      "referenceID" : 0,
      "context" : "We extend the notion of partial invariance to the kernel features extracted similar to Equation 2 following the analysis of Anselmi et al. (2013). Partial invariance arises from partially observing the group G, i.",
      "startOffset" : 124,
      "endOffset" : 146
    }, {
      "referenceID" : 0,
      "context" : "Uniqueness: The analysis for uniqueness in Anselmi et al. (2013) can be applied to Υ̂(x) with no significant changes, since the group structure is preserved in H through Theorem 2.",
      "startOffset" : 43,
      "endOffset" : 65
    }, {
      "referenceID" : 0,
      "context" : "Uniqueness: The analysis for uniqueness in Anselmi et al. (2013) can be applied to Υ̂(x) with no significant changes, since the group structure is preserved in H through Theorem 2.5. In summary, any two partial orbits with a common point are identical. Invariance: Theorem 6 from Anselmi et al. (2013) can be applied in H with some modification.",
      "startOffset" : 43,
      "endOffset" : 302
    } ],
    "year" : 2015,
    "abstractText" : "The study of representations invariant to common transformations of the data is important to learning. Most techniques have focused on local approximate invariance implemented within expensive optimization frameworks lacking explicit theoretical guarantees. In this paper, we study kernels that are invariant to the unitary group while having theoretical guarantees in addressing practical issues such as (1) unavailability of transformed versions of labelled data and (2) not observing all transformations. We present a theoretically motivated alternate approach to the invariant kernel SVM. Unlike previous approaches to the invariant SVM, the proposed formulation solves both issues mentioned. We also present a kernel extension of a recent technique to extract linear unitary-group invariant features addressing both issues and extend some guarantees regarding invariance and stability. We present experiments on the UCI ML datasets to illustrate and validate our methods.",
    "creator" : "LaTeX with hyperref package"
  }
}