{
  "name" : "1412.2197.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Liangliang Cao", "Chang Wang" ],
    "emails" : [ "liangliang.cao@us.ibm.edu", "wangchan@us.ibm.edu" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "A synonym is a word having the same or nearly the same meaning as another word. Due to the complexity of natural language, synonym happens very frequently and brings much challenge to many applications. For example, a NLP system in clinical domain should understand that “oxyphenbutazone” and “Tandearil” are of the same medicine, otherwise it may generate misleading interpretation or even answer patients’ questions wrongly.\nHowever, it is far from a trivial task to identify synonyms. Many studies in this direction have started out with intuitively appealing ideas, but in practice, these intuition based approaches always meet exceptions. As discussed in (Norlindh, 2012), sometime it is difficult to extract synonym even for professional lexicographers. Moreover, as we will show in this paper, recognizing synonym on large scale datasets is much more difficult than that in small scales.\nTable 1 shows a few examples of synonym and non-synonym. Extracting such synonyms is important in a number of applications:\n∗The two authors contributed equally.\nar X\niv :1\n41 2.\n21 97\nv1 [\ncs .C\nL ]\n6 D\nec 2\n01 4\n• Searching engine: The vast number of users of search engines may use different terms for the same query concept. Thus a crucial step in search engines is query expansion, for which extraction of synonyms plays an important role (Bernhard, 2010). For example, if one query is “Turks-cap”, the search engine should know it is a flower like “Lilium martagon”.\n• Clinical text: In health and related applications, we also need match different terms used in books and clinical realities (Henriksson et al., 2014). For example, the symptoms from patients’ own description may appear different from the clinical books, but they may refer to the same disease. Moreover, clinical text may change over time, and it is important to recognize synonyms from historical records.\n• Question Answering: Question Answering (QA) aims to automatically answer questions posed by humans in a natural language. Ferrucci et al. (2010) showed it is possible to analyzes natural language questions and content well enough and fast enough to compete and win against champion players at Jeopardy! game. To handle the ambiguous questions, many modules in the QA system (Ferrucci, 2012) must take the synonym into consideration.\nThe purpose of this research is to develop a synonym extractor which can automatically explore a large corpus and complement existing thesauri or synonym annotations. Intuitively, we can ask lexicographers to think or analyze corpora by themselves to find synonyms candidates. However, manual labeling is both costly and challenging, often resulting in low coverage of synonym in large corpus. Even worse, when we come to a new domain, we may not get enough lexicographers’ annotations in time. A better approach is to let lexicographer annotate a training set of synonyms, from which we can train a detector to find more synonyms1.\nThere have been a number of research studies on synonym extraction. Wang & Hirst (2009) explored dictionary definitions to extract synonyms. They compared rule based methods and maximum entropy method, and surprisingly found that rule based method outperforms its counterpart by large margin. Henriksson et al. (2014) studied synonym in medical datasets. One shared limitation in these studies lies in the fact that the dataset is very small. The medical synonym datasets used by Wang & Hirst (2009) is limited 340 synonym pairs. The experiments in (Henriksson et al., 2014) are limited to TOFEL synonym test with 80 questions. Collobert & Weston (2008) considered synonym, as a related task to improve the performance of semantic role labeling, but did not report the performance of synonym extraction in large scales.\nIn this paper, we build a synonym extraction datasets with 3.5M term pairs, which is significantly larger than those in previous work. We test several approaches including linear SVM, multilayer perceptron, and a new verification feature learning based deep neural network. The results show that our feature learning based neural network outperforms the baseline by relatively 100% in the large scale dataset."
    }, {
      "heading" : "2 DATASET",
      "text" : "Our synonym dataset is obtained by expanding the synonyms and antonyms from WordNet (Miller, 1995). After excluding some extremely unpopular words, we first get 35K synonym and antonym pairs. The ratio between synonym and antonym is about 10:1. They are used in our experiments as positive and negative examples.\nIn practice, we need distinguish synonym from a large amount of irrelevant pairs. To model this phenomenon, we introduce a huge amount of unrelated terms as negative pairs. Table 2 demonstrates an example of expanding negative pairs. We choose the ratio of synonym over the whole dataset as 1:100. Finally we get 3.5M term pairs, of which 1% are positive and 99% are negative, including both antonym and irrelevant.\nThe motivation of building such a highly unbalanced datasets is two-fold:\n1For applications with high requirement of accuracy, a post-processing step is employed when lexicographers will look at the synonym proposals from the automatic detector, and weed out bad synonym candidates.\nPositive pairs Negative pairs market vs. grocery market dorsal horn\nmarket vs. instrumentality market vs. supposal\nmarket vs. hybridization market vs. goliath\nmarket vs. revolving charge account market vs. soldierfish\nmarket vs. · · ·\nEvaluation: To evaluate the performance of our synonym extraction, we divide the dataset into two parts: 33% for training, and the other 67% for testing. There are 1.1M training pairs and 2.3M testing pairs, which we believe will cover a wide range of word concepts.\nSince our dataset is highly unbalanced, it makes less sense if we use classification accuracy as the evaluation criteria. Instead, we choose F1 score, which is the harmonic mean of precision recall in the testing set. F1 score is widely used in many QA system modules (Ferrucci, 2012)."
    }, {
      "heading" : "3 BASELINE AND CHALLENGES",
      "text" : "Our baseline approach is to train a SVM model over the distributed representations of words which can be learned from a corpus in an unsupervised manner. One of the earliest use of word representations dates back to 1986 due to Rumelhart et al. (1986). This idea has since been applied to statistical language modeling (Bengio et al., 2003), which motivated a number of research in speech recognition and machine translation, as well as a wide range of NLP tasks, for example (Collobert & Weston, 2008) (Glorot et al., 2011) (Socher et al., 2011) (Mnih & Teh, 2012) (Lebret & Collobert, 2014).\nThe idea of using distributed representation is to explore the co-occurrence of words and phrases to represent semantics (Turney & Pantel, 2010). A number of researcher (Bengio et al., 2003) have employed recurrent neural network to learn a distributed representations of words, which encode many linguistic regularities and patterns in a vector space (Mikolov et al., 2013b). For example, the result of a vector calculation vec(Madrid) - vec(Spain) + vec(France) is closer to vec(Paris) than to any other word vector (Mikolov et al., 2013b).\nAlthough the recurrent neural network can achieve good performance in natural language processing tasks, not all of them are scalable for very large corpus. We prefer an efficient method for arbitrary size of documents, so we employ the Skip-gram model proposed by (Mikolov et al., 2013a) which learn word vectors efficiently by a two layer network from the context words in a sentences. The implementation of Mikolov et al. (2013a) is extremely efficient: we can learn vector representations for billions of words from the whole Wikipedia corpus on a server with 5 CPUs in two hours. We choose dimension=100 for the word representation.\nThe unsupervised learning of distributed representation helps us to develop a synonym classifier for either general purpose or a specified domain. We can first learn the vector representation from a large corpus, and use this presentation as features to learn a synonym extractor. Our baseline is to train a linear SVM with the distributed representation. In practice, we use the liblinear package by Fan et al. (2008). To handle the unbalanced problem of positive pairs, we use a sampling weight of 100 for positive class in the 3.5M dataset.\nWe train the linear SVM model on two datasets. A small dataset has 35K pairs with synonyms and antonyms only. Another dataset has the extended dataset with 3.5M pairs. Table 3 shows the performance of our baseline approach for synonym recognition. It is easy to see that the word vector + SVM approach works very well for the small dataset, but it is really challenging to obtain good performance for the large synonym extraction.\nTable 4 shows some failure examples using our baseline algorithms. We can see that synonym extraction is a difficult problem by its nature. Some words, like “capital”, “insulation”, and “groin” have multiple meanings; Some words, like “‘VIII” are only used in certain documents. When it comes to large scale, the learning problem become even more difficult due to several factors: the learning problem is highly unbalanced, the difference feature x1−x2 is not good enough to capture diversity in large scale word representation. Although models of distributional semantics provide a potential means of supporting development of such resources, the ability to isolate synonymy from other semantic relations in large corpora is limited. In the next section, we will discuss how to overcome these challenges and how to build better synonym extractors."
    }, {
      "heading" : "4 NEW APPROACHES AND RESULTS",
      "text" : "To explain our approach, let’s first denote a pair of words represented as x1 and x2, and the label of this pair is y. We say y = 1 if x1 and x2 is synonym, and y = −1 for non-synonyms. A synonym extractor will generate an estimation ŷ which is hope to be close to y. Before discussing our model, we would like to introduce our cost function:\nC(ŷ, y) = { (ŷ − y)2 if (ŷ − y) ∗ y < 0 0 otherwise (1)\nwhere ŷ denotes the estimation. Our cost function can be viewed as a modified version of mean square error (MSE), whose cost is simply (ŷ − y)2. Such cost function prefers estimations with good F1 scores. However, unlike traditional MSE, our new cost does not require ŷ be exactly as +1 or −1. Given a training sample with ŷ − y > 0 for y = 1 or ŷ − y < 0 for y = −1, we think the estimation is good enough and will count as zero cost. Many popular cost functions, including MSE, cross entropy and hinge loss both suffer from unbalanced problem, for which we have to assign a hyper parameter as sampling weight for unbalanced categories. However, there is no hyper-parameter in our cost function because a majority of samples in the dominant categories are overlooked when they satisfied the constraint (ŷ − y)y > 0. Figure 4 plots the cost function for y = −1 and y = 1, respectively. It is easy to see that our function is also smooth with continuous gradient."
    }, {
      "heading" : "4.1 NEURAL NETWORK WITH HAND-DESIGNED FEATURES",
      "text" : "As shown in previous section, linear SVM cannot model the nonlinear relationship and does not work effectively in large scale. On the other hand, Nonlinear SVMs , are too expensive in large scale datasets, because empirically the number of support vectors grows with the number of training samples. Our first attempt is to employ multilayer perceptron (MLP) to replace SVM. Following the large amount of previous work, we employ a two layer MLP with hyperbolic tangent function as the activation function as the classification model.\nThe input of MLP can be of many choices. Suppose the input is one pair of words represented by x1 and x2. The most straightforward representation is to consider the difference x1−x2, or use the absolute value in every dimension abs(x1−x2). If we only use the difference as the input, the MLP in fact is a nonlinear generalization of Mahalanobis distance (x1 − x2)TM(x1 − x2). However, the difference vector x1 − x2 does not keep all the information for the synonym classification. As suggested by Li et al. (2013), such models in fact impose a global decision threshold without considering the local characteristics of x1 or x2. We propose to add more relationship and obtain more features for the pair, including x1 + x2, x1 ∗ x2, together with x1 and x2 individually. Here +, − and ∗ all denote element-wise operation. As shown in Figure 2 we can concatenate these hand designed vector into a long vector and build a neural network to learn the synonym. Suppose x1 and x2 are both of d dimensional, the concatenated vector will be of 5d. The neural network is trained using stochastic gradient decent (SGD), where the gradient can be computed efficiently using back propagation.\nTable 5 compares the performance of individual features as well as the combined features. It is easy to see that by combining multiple hand-assigned features, we can great improve the performance of synonym extraction."
    }, {
      "heading" : "4.2 DEEP NEURAL NETWORK WITH FEATURES LEARNING",
      "text" : "Neural network has been recognized by its potentials in learning features instead of using handcraft features. We have seen a number of successful examples including unsupervised learning (Coates et al., 2011) or supervised convolutional network (LeCun et al., 1998). In this paper, we propose an automatic approach to learn the feature representation instead hand-assigned features [x1 − x2,x1 + x2,x1 ∗ x2]. As shown in Figure 4.2, we first build a neural network which can take one pair of input features and learn a set of relationship automatically. These learned features are mapped into a nonlinear space and then used as the input of a multi-layer perceptron.\nOur learned feature can be represented in the following form:\nf(wax1 + wbx2 + wcx1 ∗ x2) = f([x1,x2,x1 ∗ x2] [ wa wb wc ] ). (2)\nIt is easy to see that Eq. (2) is a general form of all the hand-assigned features: when wa = 1, wb = −1, wc = 0, this is a function of input difference f(x1 − x2); when wa = wb = 0, wc = 1, this becomes a function of the inner product as x1 ∗ x2. Eq. (2) can be viewed as a special 1 × 1 convolutional filter on the input (x1,x2,x1 ∗ x2). In practice, our network use a three-dimensional tensor as the input. Dimensions of the tensor input are the batch size, the dimension of the distributed representation, and the size of input sample (as 3). Our algorithm is implementation based on Theano (Bergstra et al., 2010), where the layer definition is illustrated as Algorithm 1. To train the model, we use the standard stochastic gradient with a fixed learning rate 0.04.\nOur feature learning based neural network achieves the best performance on our synonym recognition dataset. Table 6 compares the performance with hand-assigned features and SVM baseline (also using [x1 − x2,x1 + x2,x1 ∗ x2]). Our new approach significantly improves the F1 score of the baseline by 97%. Also it is interesting to find out that our feature learning approach outperforms the hand-assigned features in large scale, however, it does not show advantage in the small scale dataset with 35K word pairs. These results suggest that not only powerful models but enough training samples are important in feature learning.\nAlgorithm 1 Definition of our feature learning based deep neural network. layers = [] layers += [InputTensor3Layer(inputshape=[nbatch,nfeature,3])] layers += [HiddenLayerWithoutB(layers[-1], outdim=10, activation = tanh)] layers += [FlattenLayer(layers[-1], flattendim=2)] layers += [HiddenLayer(layers[-1], outdim=100, activation = tanh)] layers += [HiddenLayer( layers[-1], outdim=1, activation = None)]"
    }, {
      "heading" : "5 CONCLUSION",
      "text" : "This paper describe our efforts towards extracting synonyms in large scale settings. We employ the distribute representation to represent word pairs and propose suggest a feature learning neural network with a new cost function to recognize synonyms. The experimental results on a dataset with 3.5 million synonym/non-synonym pairs suggests our approach outperforms the hand-assigned features. Our best performance improves the SVM baseline by relative 97%. Our future work is to merge the synonym extraction into QA systems in different domains."
    } ],
    "references" : [ {
      "title" : "A neural probabilistic language mode",
      "author" : [ "Bengio", "Yoshua", "Ducharme", "Rejean", "Vincent", "Pascal", "Janvin", "Christian" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Bengio et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2003
    }, {
      "title" : "Theano: a CPU and GPU math expression compiler",
      "author" : [ "Bergstra", "James", "Breuleux", "Olivier", "Bastien", "Frédéric", "Lamblin", "Pascal", "Pascanu", "Razvan", "Desjardins", "Guillaume", "Turian", "Joseph", "Warde-Farley", "David", "Bengio", "Yoshua" ],
      "venue" : "In SciPy,",
      "citeRegEx" : "Bergstra et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Bergstra et al\\.",
      "year" : 2010
    }, {
      "title" : "Query expansion based on pseudo relevance feedback from definition clusters",
      "author" : [ "Bernhard", "Delphine" ],
      "venue" : "In COLING, pp",
      "citeRegEx" : "Bernhard and Delphine.,? \\Q2010\\E",
      "shortCiteRegEx" : "Bernhard and Delphine.",
      "year" : 2010
    }, {
      "title" : "An analysis of single-layer networks in unsupervised feature learning",
      "author" : [ "Coates", "Adam", "Ng", "Andrew Y", "Lee", "Honglak" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "Coates et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Coates et al\\.",
      "year" : 2011
    }, {
      "title" : "A unified architecture for natural language processing: deep neural networks with multitask learning",
      "author" : [ "Collobert", "Ronan", "Weston", "Jason" ],
      "venue" : "In ICML, pp",
      "citeRegEx" : "Collobert et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Collobert et al\\.",
      "year" : 2008
    }, {
      "title" : "Liblinear: A library for large linear classification",
      "author" : [ "Fan", "R.-E", "Chang", "K.-W", "Hsieh", "C.-J", "Wang", "X.-R", "Lin" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Fan et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Fan et al\\.",
      "year" : 2008
    }, {
      "title" : "Building watson: An overview of the deepqa project",
      "author" : [ "Ferrucci", "David", "Brown", "Eric", "Chu-Carroll", "Jennifer", "Fan", "James", "Gondek", "Kalyanpur", "Aditya A", "Lally", "Adam", "Murdock", "J William", "Nyberg", "Prager", "John" ],
      "venue" : "AI magazine,",
      "citeRegEx" : "Ferrucci et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Ferrucci et al\\.",
      "year" : 2010
    }, {
      "title" : "Introduction to ”this is watson",
      "author" : [ "Ferrucci", "David A" ],
      "venue" : "IBM Journal of Research and Development,",
      "citeRegEx" : "Ferrucci and A.,? \\Q2012\\E",
      "shortCiteRegEx" : "Ferrucci and A.",
      "year" : 2012
    }, {
      "title" : "Domain adaptation for large-scale sentiment classification: A deep learning approach",
      "author" : [ "Glorot", "Xavier", "Bordes", "Antoine", "Bengio", "Yoshua" ],
      "venue" : null,
      "citeRegEx" : "Glorot et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Glorot et al\\.",
      "year" : 2011
    }, {
      "title" : "Synonym extraction and abbreviation expansion with ensembles of semantic spaces",
      "author" : [ "Henriksson", "Aron", "Moen", "Hans", "Skeppstedt", "Maria", "Daudaravicius", "Vidas", "Duneld", "Martin" ],
      "venue" : "J. Biomedical Semantics,",
      "citeRegEx" : "Henriksson et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Henriksson et al\\.",
      "year" : 2014
    }, {
      "title" : "Word embeddings through Hellinger PCA",
      "author" : [ "R. Lebret", "R. Collobert" ],
      "venue" : "In EACL,",
      "citeRegEx" : "Lebret and Collobert,? \\Q2014\\E",
      "shortCiteRegEx" : "Lebret and Collobert",
      "year" : 2014
    }, {
      "title" : "Gradient based learning applied to document recognition",
      "author" : [ "Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "LeCun et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "LeCun et al\\.",
      "year" : 1998
    }, {
      "title" : "Learning locally-adaptive decision functions for person verification",
      "author" : [ "Li", "Zhen", "Chang", "Shiyu", "Liang", "Feng", "Huang", "Thomas S", "Cao", "Liangliang", "Smith", "John R" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "Li et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2013
    }, {
      "title" : "Efficient estimation of word representations in vector space",
      "author" : [ "Mikolov", "Tomas", "Chen", "Kai", "Corrado", "Greg", "Dean", "Jeffrey" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "Mikolov et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Linguistic regularities in continuous space word representations",
      "author" : [ "Mikolov", "Tomas", "tau Yih", "Wen", "Zweig", "Geoffrey" ],
      "venue" : "NAACL HLT,",
      "citeRegEx" : "Mikolov et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Wordnet: A lexical database for english",
      "author" : [ "Miller", "George A" ],
      "venue" : "Communications of the ACM,",
      "citeRegEx" : "Miller and A.,? \\Q1995\\E",
      "shortCiteRegEx" : "Miller and A.",
      "year" : 1995
    }, {
      "title" : "A fast and simple algorithm for training neural probabilistic language models",
      "author" : [ "A. Mnih", "Y.W. Teh" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Mnih and Teh,? \\Q2012\\E",
      "shortCiteRegEx" : "Mnih and Teh",
      "year" : 2012
    }, {
      "title" : "Extraction of synonyms and semantically related words from chat logs",
      "author" : [ "Norlindh", "Fredrik" ],
      "venue" : null,
      "citeRegEx" : "Norlindh and Fredrik.,? \\Q2012\\E",
      "shortCiteRegEx" : "Norlindh and Fredrik.",
      "year" : 2012
    }, {
      "title" : "Learning representations by backpropagating",
      "author" : [ "Rumelhart", "David E", "Hintont", "Geoffrey E", "Williams", "Ronald J" ],
      "venue" : "errors. Nature,",
      "citeRegEx" : "Rumelhart et al\\.,? \\Q1986\\E",
      "shortCiteRegEx" : "Rumelhart et al\\.",
      "year" : 1986
    }, {
      "title" : "Parsing natural scenes and natural language with recursive neural networks",
      "author" : [ "Socher", "Richard", "Lin", "Cliff C", "Ng", "Andrew Y", "Manning", "Christopher D" ],
      "venue" : null,
      "citeRegEx" : "Socher et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2011
    }, {
      "title" : "From frequency to meaning: Vector space models of semantics",
      "author" : [ "Turney", "Peter D", "Pantel", "Patrick" ],
      "venue" : "Journal of Artificial Intelligence Research,",
      "citeRegEx" : "Turney et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Turney et al\\.",
      "year" : 2010
    }, {
      "title" : "Extracting synonyms from dictionary definitions",
      "author" : [ "Wang", "Tong", "Hirst", "Graeme" ],
      "venue" : "In RANLP, pp",
      "citeRegEx" : "Wang et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 9,
      "context" : "• Clinical text: In health and related applications, we also need match different terms used in books and clinical realities (Henriksson et al., 2014).",
      "startOffset" : 125,
      "endOffset" : 150
    }, {
      "referenceID" : 6,
      "context" : "Ferrucci et al. (2010) showed it is possible to analyzes natural language questions and content well enough and fast enough to compete and win against champion players at Jeopardy! game.",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 9,
      "context" : "The experiments in (Henriksson et al., 2014) are limited to TOFEL synonym test with 80 questions.",
      "startOffset" : 19,
      "endOffset" : 44
    }, {
      "referenceID" : 9,
      "context" : "Henriksson et al. (2014) studied synonym in medical datasets.",
      "startOffset" : 0,
      "endOffset" : 25
    }, {
      "referenceID" : 9,
      "context" : "Henriksson et al. (2014) studied synonym in medical datasets. One shared limitation in these studies lies in the fact that the dataset is very small. The medical synonym datasets used by Wang & Hirst (2009) is limited 340 synonym pairs.",
      "startOffset" : 0,
      "endOffset" : 207
    }, {
      "referenceID" : 9,
      "context" : "Henriksson et al. (2014) studied synonym in medical datasets. One shared limitation in these studies lies in the fact that the dataset is very small. The medical synonym datasets used by Wang & Hirst (2009) is limited 340 synonym pairs. The experiments in (Henriksson et al., 2014) are limited to TOFEL synonym test with 80 questions. Collobert & Weston (2008) considered synonym, as a related task to improve the performance of semantic role labeling, but did not report the performance of synonym extraction in large scales.",
      "startOffset" : 0,
      "endOffset" : 361
    }, {
      "referenceID" : 0,
      "context" : "This idea has since been applied to statistical language modeling (Bengio et al., 2003), which motivated a number of research in speech recognition and machine translation, as well as a wide range of NLP tasks, for example (Collobert & Weston, 2008) (Glorot et al.",
      "startOffset" : 66,
      "endOffset" : 87
    }, {
      "referenceID" : 8,
      "context" : ", 2003), which motivated a number of research in speech recognition and machine translation, as well as a wide range of NLP tasks, for example (Collobert & Weston, 2008) (Glorot et al., 2011) (Socher et al.",
      "startOffset" : 170,
      "endOffset" : 191
    }, {
      "referenceID" : 19,
      "context" : ", 2011) (Socher et al., 2011) (Mnih & Teh, 2012) (Lebret & Collobert, 2014).",
      "startOffset" : 8,
      "endOffset" : 29
    }, {
      "referenceID" : 0,
      "context" : "A number of researcher (Bengio et al., 2003) have employed recurrent neural network to learn a distributed representations of words, which encode many linguistic regularities and patterns in a vector space (Mikolov et al.",
      "startOffset" : 23,
      "endOffset" : 44
    }, {
      "referenceID" : 13,
      "context" : "One of the earliest use of word representations dates back to 1986 due to Rumelhart et al. (1986). This idea has since been applied to statistical language modeling (Bengio et al.",
      "startOffset" : 74,
      "endOffset" : 98
    }, {
      "referenceID" : 0,
      "context" : "This idea has since been applied to statistical language modeling (Bengio et al., 2003), which motivated a number of research in speech recognition and machine translation, as well as a wide range of NLP tasks, for example (Collobert & Weston, 2008) (Glorot et al., 2011) (Socher et al., 2011) (Mnih & Teh, 2012) (Lebret & Collobert, 2014). The idea of using distributed representation is to explore the co-occurrence of words and phrases to represent semantics (Turney & Pantel, 2010). A number of researcher (Bengio et al., 2003) have employed recurrent neural network to learn a distributed representations of words, which encode many linguistic regularities and patterns in a vector space (Mikolov et al., 2013b). For example, the result of a vector calculation vec(Madrid) - vec(Spain) + vec(France) is closer to vec(Paris) than to any other word vector (Mikolov et al., 2013b). Although the recurrent neural network can achieve good performance in natural language processing tasks, not all of them are scalable for very large corpus. We prefer an efficient method for arbitrary size of documents, so we employ the Skip-gram model proposed by (Mikolov et al., 2013a) which learn word vectors efficiently by a two layer network from the context words in a sentences. The implementation of Mikolov et al. (2013a) is extremely efficient: we can learn vector representations for billions of words from the whole Wikipedia corpus on a server with 5 CPUs in two hours.",
      "startOffset" : 67,
      "endOffset" : 1317
    }, {
      "referenceID" : 0,
      "context" : "This idea has since been applied to statistical language modeling (Bengio et al., 2003), which motivated a number of research in speech recognition and machine translation, as well as a wide range of NLP tasks, for example (Collobert & Weston, 2008) (Glorot et al., 2011) (Socher et al., 2011) (Mnih & Teh, 2012) (Lebret & Collobert, 2014). The idea of using distributed representation is to explore the co-occurrence of words and phrases to represent semantics (Turney & Pantel, 2010). A number of researcher (Bengio et al., 2003) have employed recurrent neural network to learn a distributed representations of words, which encode many linguistic regularities and patterns in a vector space (Mikolov et al., 2013b). For example, the result of a vector calculation vec(Madrid) - vec(Spain) + vec(France) is closer to vec(Paris) than to any other word vector (Mikolov et al., 2013b). Although the recurrent neural network can achieve good performance in natural language processing tasks, not all of them are scalable for very large corpus. We prefer an efficient method for arbitrary size of documents, so we employ the Skip-gram model proposed by (Mikolov et al., 2013a) which learn word vectors efficiently by a two layer network from the context words in a sentences. The implementation of Mikolov et al. (2013a) is extremely efficient: we can learn vector representations for billions of words from the whole Wikipedia corpus on a server with 5 CPUs in two hours. We choose dimension=100 for the word representation. The unsupervised learning of distributed representation helps us to develop a synonym classifier for either general purpose or a specified domain. We can first learn the vector representation from a large corpus, and use this presentation as features to learn a synonym extractor. Our baseline is to train a linear SVM with the distributed representation. In practice, we use the liblinear package by Fan et al. (2008). To handle the unbalanced problem of positive pairs, we use a sampling weight of 100 for positive class in the 3.",
      "startOffset" : 67,
      "endOffset" : 1941
    }, {
      "referenceID" : 12,
      "context" : "As suggested by Li et al. (2013), such models in fact impose a global decision threshold without considering the local characteristics of x1 or x2.",
      "startOffset" : 16,
      "endOffset" : 33
    }, {
      "referenceID" : 3,
      "context" : "We have seen a number of successful examples including unsupervised learning (Coates et al., 2011) or supervised convolutional network (LeCun et al.",
      "startOffset" : 77,
      "endOffset" : 98
    }, {
      "referenceID" : 11,
      "context" : ", 2011) or supervised convolutional network (LeCun et al., 1998).",
      "startOffset" : 44,
      "endOffset" : 64
    }, {
      "referenceID" : 1,
      "context" : "Our algorithm is implementation based on Theano (Bergstra et al., 2010), where the layer definition is illustrated as Algorithm 1.",
      "startOffset" : 48,
      "endOffset" : 71
    } ],
    "year" : 2014,
    "abstractText" : "Synonym extraction is an important task in natural language processing and often used as a submodule in query expansion, question answering and other applications. Automatic synonym extractor is highly preferred for large scale applications. Previous studies in synonym extraction are most limited to datasets in small scales. In this paper, we build a large dataset with 3.5 million synonym/nonsynonym pairs to capture the challenges in real world scenarios. To overcome these challenges in large scale in synonym extraction, we proposed (1) a new cost function to accommodate the unbalanced learning problem, and (2) a feature learning based deep neural network to model the complicated relationships in synonym pairs. We compare several different approaches based on SVMs and neural networks, and find out our feature learning based neural network outperforms the methods with hand-assigned features. Specifically, the best performance of our model surpluses the SVM baseline with a significant 97% relative improvement.",
    "creator" : "LaTeX with hyperref package"
  }
}