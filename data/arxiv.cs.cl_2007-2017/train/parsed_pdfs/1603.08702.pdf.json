{
  "name" : "1603.08702.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Nine Features in a Random Forest to Learn Taxonomical Semantic Relations",
    "authors" : [ "Enrico Santus", "Alessandro Lenci", "Tin-Shing Chiu", "Qin Lu", "Chu-Ren Huang" ],
    "emails" : [ "esantus@gmail.com,", "cstschiu@comp.polyu.edu.hk,", "churen.huang}@polyu.edu.hk", "alessandro.lenci@unipi.it" ],
    "sections" : [ {
      "heading" : null,
      "text" : "introduced ROOT13 (Santus et al., 2016). It relies on a Random Forest algorithm and nine unsupervised corpus-based features. We evaluate it with a 10-fold cross validation on 9,600 pairs, equally distributed among the three classes and involving several Parts-Of-Speech (i.e. adjectives, nouns and verbs). When all the classes are present, ROOT9 achieves an F1 score of 90.7%, against a baseline of 57.2% (vector cosine). When the classification is binary, ROOT9 achieves the following results against the baseline: hypernyms-co-hyponyms 95.7% vs. 69.8%, hypernyms-random 91.8% vs. 64.1% and co-hyponyms-random 97.8% vs. 79.4%. In order to compare the performance with the state-of-the-art, we have also evaluated ROOT9 in subsets of the Weeds et al. (2014) datasets, proving that it is in fact competitive. Finally, we investigated whether the system learns the semantic relation or it simply learns the prototypical hypernyms, as claimed by Levy et al. (2015). The second possibility seems to be the most likely, even though ROOT9 can be trained on negative examples (i.e., switched hypernyms) to drastically reduce this bias.\nKeywords: Vector Space Models, VSMs, Distributional Semantic Models, DSMs, Semantic Relations, Taxonomy, Hypernymy, Entailment, Hyponymy, Co-Hyponymy"
    }, {
      "heading" : "1. Introduction",
      "text" : "Distinguishing hypernyms from co-hyponyms and, in turn, discriminating them from semantically unrelated words (henceforth randoms) is a fundamental task in Natural Language Processing (NLP). Hypernymy in fact represents a key organization principle of semantic memory (Murphy, 2002), the backbone of taxonomies and ontologies, and one of the crucial semantic relations supporting lexical entailment (Geffet and Dagan, 2005). Co-hyponymy (or coordination) is instead the relation held by words sharing a close hypernym, which are therefore attributionally similar (Weeds et al., 2014). The ability of discriminating hypernymy, co-hyponymy and random words has potentially infinite applications, including Automatic Thesauri Creation, Paraphrasing, Textual Entailment, Sentiment Analysis and so on (Weeds et al., 2014; Tungthamthiti et al. 2015). For this reason, in the last decades, numerous methods, datasets and shared tasks have been proposed to improve computers’ ability in such discrimination, generally achieving promising results (Santus et al., 2016b; Roller et al., 2014, Weeds et al., 2014; Santus et al. 2014a; Rimmel, 2014; Lenci and Benotto, 2012; Kotlerman et al., 2010; Geffet and Dagan, 2005; Weeds and Weir, 2003). Both supervised and unsupervised approaches have been investigated. The former have been shown to outperform the latter in Weeds et al. (2014), even though Levy et al. (2015) have claimed that these methods may learn whether a term y is a\nprototypical hypernym, regardless of its actual relation with a term x. In this paper we further investigate and revise ROOT13 (Santus et al., 2016b), a supervised method based on a Random Forest algorithm and thirteen corpus-based features. The feature contribution is evaluated with an ablation test, using a 10-fold cross validation on 9,600 pairs randomly extracted from EVALution (Santus et al., 2015) 1 , Lenci/Benotto (Benotto, 2015) and BLESS (Baroni and Lenci, 2011). The ablation test has shown that four out of thirteen features were actually not contributing to the system’s performance, and they were therefore removed, turning ROOT13 into ROOT9. On the 9,600 pairs, ROOT9 achieved an F1 score of 90.7% when the three classes were present, 95.7% when we had to discriminate hypernyms and co-hyponyms, 91.8% for hypernyms and randoms, and 97.8% for co-hyponyms and randoms. In order to compare ROOT9 with the state-of-the-art, we have also evaluated it in the Weeds et al. (2014) datasets. Unfortunately, ROOT9 was not able to cover the full datasets, as several words in their pairs were missing from our Distributional Semantic Model (DSM) because of their low frequency. Nevertheless, the authors kindly provided the results of their models on our subsets, so that the comparison can be considered reliable. Also in\n1 The 9,600 pairs are available at https://github.com/esantus/ROOT9\nrelation to the state of the art, ROOT9 is proved to be competitive, being slightly outperformed in all the datasets only by the svmCAT model (Weeds et al., 2014), which is a Support Vector Machine (SVM) classifier run on the concatenation of the distributional vectors of the words in the pairs. Finally, we carried out an extra test to verify whether the system was actually learning the semantic relation between two word pairs, or simply identifying prototypical hypernyms (Levy et al. 2015). The test consisted in providing to the trained model switched hypernyms (e.g. from “dog HYPER animal” to “dog RANDOM fruit”), and verify how they were classified. Our results show that most of the switched hypernyms were in fact misclassified as hypernyms (especially when the words in the switched hypernyms were the same used to train the model on the real hypernyms), and that the only way to overcome such problem is to explicitly provide the model with bad examples (i.e., switched hypernyms tagged as randoms) during the training."
    }, {
      "heading" : "2. Related Work",
      "text" : "Since the pioneering work of Hearst (1992), who used a pattern based approach for the “automatic acquisition of hyponyms from large text corpora”, a large number of distributional methods were applied to the identification of hypernyms. These methods relied on interpretations of the Distributional Hypothesis (Harris, 1954), according to which the meaning of a linguistic expression can be inferred from its distribution in text corpora, so that linguistic expressions occurring in similar contexts are likely to be similar. These approaches, which can be either supervised or unsupervised, are generally implemented using Vector Space Models (VSMs; also called Distributional Semantic Models, DSMs), where vectors represent words, and their dimensions weight the association between the words and the contexts (Turney and Pantel, 2010). Among the unsupervised methods, Weeds and Weir (2003) proposed the Distributional Inclusion Hypothesis (DIH), according to which less general words tend to occur in a subset of the contexts of their hypernyms. Their measure identified the direction of hypernymy with 71% accuracy on word-pairs extracted from WordNet (Fellbaum, 1998). This result, however, was not significantly better than the frequency baseline, according to which more general words are more frequent. Clarke (2009) extended the DIH, suggesting that generality difference can be calculated as the degree to which the narrower term’s dimensions have lower values than the broader ones, across all the intersected dimensions. Lenci and Benotto (2012) adapted this measure to check not only to which extent the features of the narrower term are included in the features of the broader, but also how the features of the broader are not included in the features of the narrower. Kotlerman et al. (2010) combined Average Precision (AP) with the balancing approach of Szpektor and Dagan (2008), outperforming the above mentioned\nmethods. Herbelot and Ganesalingam (2013) measured the Kullback-Leibler (KL) divergence between the probability distribution over context words for a term, and the background probability distribution, based on the idea that the smaller such KL was, the less informative the word was (and therefore more likely to be a hypernym). Rimmel (2014) considered the top features in a context vector as topics and used a Topic Coherence (TC) measure. Santus et al. (2014a) formulated the Distributional Informativeness Hypothesis (DInH), according to which the generality of a term can be inferred from the informativeness of its most typical linguistic contexts. In their evaluation, the authors have shown that hypernyms’ most typical contexts are in fact less informative than hyponyms’ ones. Among the supervised methods, Baroni et al. (2012) proposed to use an SVM classifier on the concatenation (after having tried also subtraction and division) of the vectors. Roller et al. (2014) used the vectors’ difference, while Weeds et al. (2014) implemented numerous combinations (difference, multiplication, sum, concatenation, etc.), comparing them against the most common unsupervised methods. The authors demonstrated that supervised methods generally perform better than unsupervised ones, but they acknowledge that these methods tend to learn ontological information, re-using it any time a word occur again in the dataset. For this reason, they suggest to adopt a new dataset, where words occur at most twice. Weeds et al. (2014)’s observation was further investigated by Levy et al. (2015), who claimed that supervised methods learn whether a term y is a prototypical hypernym, regardless of its actual relation with a term x."
    }, {
      "heading" : "3. Method",
      "text" : "ROOT13 was firstly introduced in Santus et al. (2016b). It uses the Random Forest algorithm implemented in Weka (Breiman, 2001), with the default settings (i.e., 100 trees, 1 seed, and maxDepth and numFeatures initialized to 0), and relies on thirteen features that are carefully described below. Each of them is automatically extracted from a window-based DSM, trained on a combination of ukWaC and WaCkypedia corpora (about 2.7 billion words), counting word co-occurrences within the 5 nearest content words to the left and right of each target. Only adjectives, nouns and verbs with frequency above 1,000 are included in the DSM. As it will be shown in the evaluation, four out of thirteen features were redundant and were not contributing to the system performance. They were therefore dropped, turning ROOT13 into ROOT9."
    }, {
      "heading" : "3.1 Features",
      "text" : "The feature set was designed to identify several distributional properties characterizing the terms in the pairs. On top of the standard distributional features (e.g., co-occurrence frequency and words frequencies), we have added several information that have been proved to\nbe effective to discriminate paradigmatic semantic relations in vector spaces (Santus et al., 2014a; Santus et al., 2016a). All the features were computed using the above-mentioned DSM and normalized in the range 0-1."
    }, {
      "heading" : "3.1.1 Co-Occurrence",
      "text" : "Cooc is defined as the co-occurrence frequency between the two terms in the pair, within the DSM window. According to the Co-occurrence Hypothesis (Charles and Miller, 1989), this measure is discriminative for synonyms and antonyms: antonyms are in fact expected to occur in the same sentence more often than synonyms. Since co-hyponyms can be often seen as a specific kind of opposition (e.g. “Winter or summer?”; Murphy 2003), this measure should help in discriminating them from hypernyms and randoms (Santus et al., 2014b-c)."
    }, {
      "heading" : "3.1.2 Frequency",
      "text" : "Frequency is an important property of words and it is a very discriminative information. For what concerns our task, Weeds and Weir (2003) have shown that the frequency baseline was very competitive in identifying the directionality of hypernymy-related pairs. We can therefore expect that hypernyms have higher frequency than hyponyms. Frequency is incorporated in our model with three features, namely one for each word involved in the pair (Freq1,2), plus one saving the difference between the frequencies (Diff Freq)."
    }, {
      "heading" : "3.1.3 Entropy",
      "text" : "Entropy is generally used to measure informativeness: the lower the entropy of an event, the higher its informativeness. Words occurrence in a corpus has very low entropy, as every word needs to fulfil certain morphological, syntactic and semantic requirements in order to occur in specific positions (e.g. in “\uD835\uDC65 barks”, it is very likely that \uD835\uDC65 is “dog”, because \uD835\uDC65 is expected to be a noun, and only dogs are known for barking). Nevertheless, words entropy varies according to several factors, such as the generality and prototypicality of the word. As claimed by Murphy (2002), the amount of informativeness in the taxonomies increases, when moving from the superordinate to the subordinate level. We use entropy as an index of word informativeness. It is calculated using the Shannon (1948)’s equation presented below:\n\uD835\uDC3B(\uD835\uDC64) = −∑\uD835\uDC5D(\uD835\uDC50\uD835\uDC56|\uD835\uDC64) ∙ \uD835\uDC59\uD835\uDC5C\uD835\uDC542(\uD835\uDC5D(\uD835\uDC50\uD835\uDC56|\uD835\uDC64))\n\uD835\uDC5B\n\uD835\uDC56=1\nwhere \uD835\uDC5D(\uD835\uDC50\uD835\uDC56|\uD835\uDC64) is the probability of the context \uD835\uDC50\uD835\uDC56 given the word \uD835\uDC64 , computed as the ratio between the co-occurrence frequency of the pair <\uD835\uDC64, \uD835\uDC50\uD835\uDC56> and the total frequency of \uD835\uDC64.\nIn our system, entropy corresponds to three features, namely one for each word in the pair (Entr1,2), plus one saving the difference between the entropies (Diff Entr).\n3.1.4 Shared and APSyn Shared and APSyn (Santus et al., 2016a-b) are two features that do not rely on the full distribution of the words, but on the top \uD835\uDC41 most related contexts to the words in a pair, where \uD835\uDC41 was empirically fixed at 1000. The value of this parameter was tested in other experiments, some of which reported in Santus et al. (2016a). Differently from Santus et al. (2016a-b), where the relation between contexts and words was measured with Local Mutual Information (LMI; Evert, 2005), in the current paper we used Positive Pointwise Mutual Information (PPMI; Levy et al., 2015), as it has shown some improvements:\n\uD835\uDC43\uD835\uDC43\uD835\uDC40\uD835\uDC3C(\uD835\uDC64, \uD835\uDC50) = max(\uD835\uDC43\uD835\uDC40\uD835\uDC3C(\uD835\uDC64, \uD835\uDC50), 0)\n\uD835\uDC43\uD835\uDC40\uD835\uDC3C(\uD835\uDC64, \uD835\uDC50) = log2 ( \uD835\uDC43(\uD835\uDC64, \uD835\uDC50)\n\uD835\uDC43(\uD835\uDC64) × \uD835\uDC43(\uD835\uDC50) ) =  log2 (\n|\uD835\uDC64, \uD835\uDC50| × \uD835\uDC37\n|\uD835\uDC64| × |\uD835\uDC50| )\nwere \uD835\uDC64 is the target word, \uD835\uDC50 is the given context, \uD835\uDC43(\uD835\uDC64, \uD835\uDC50) is the probability of co-occurrence and \uD835\uDC37 is the collection of observed word-context pairs. Once the PPMI values are assigned to all contexts of the target words (i.e. the words in the pair), we rank these contexts in a decreasing order, and consider only the top \uD835\uDC41, with \uD835\uDC41 = 1000. At this point, Shared is the cardinality of the intersection of the top \uD835\uDC41 contexts of the target words. APSyn, instead, is the weighted cardinality of the intersection, where the average ranking of the common features is used as a weight, as shown in the equation below:\n\uD835\uDC34\uD835\uDC43\uD835\uDC46\uD835\uDC66\uD835\uDC5B(\uD835\uDC641, \uD835\uDC642) =  ∑ 1\n(\uD835\uDC5F\uD835\uDC4E\uD835\uDC5B\uD835\uDC581(\uD835\uDC53) + \uD835\uDC5F\uD835\uDC4E\uD835\uDC5B\uD835\uDC582(\uD835\uDC53))/2 \uD835\uDC53∈\uD835\uDC41(\uD835\uDC391)∩\uD835\uDC41(\uD835\uDC392)\nThat is, for every feature \uD835\uDC53 included in the intersection between the top N features of \uD835\uDC641, \uD835\uDC41(\uD835\uDC391), and \uD835\uDC642, \uD835\uDC41(\uD835\uDC392), APSyn will add 1 divided by the average rank of the feature, among the top PPMI ranked features of \uD835\uDC641 , \uD835\uDC5F\uD835\uDC4E\uD835\uDC5B\uD835\uDC581(\uD835\uDC531), and \uD835\uDC642, \uD835\uDC5F\uD835\uDC4E\uD835\uDC5B\uD835\uDC582(\uD835\uDC532)."
    }, {
      "heading" : "3.1.5 Contexts Frequency",
      "text" : "We have noticed that hypernyms tend to occur in more frequent contexts than co-hyponyms and randoms. Our system exploits two features, C-Freq1,2, capturing the frequency of the \uD835\uDC41 top contexts of the target words in the pair."
    }, {
      "heading" : "3.1.6 Contexts Entropy",
      "text" : "Given what mentioned in 3.1.3 and the DIH and DInH (Weeds and Weir, 2003; Santus et al., 2014a), general words are likely to occur in a larger variety of contexts\n(i.e. higher frequency) and in broader ones (i.e. less informative), compared to specific words. In fact, while hypernyms can certainly occur in narrower contexts, specific words are more likely to be chosen in these situations. Consider the following sentences:\na) The X has barked the all night. b) The Y has arrested the thieves.\nAny reader would agree that X is likely to be dog and Y policeman. Of course, X could have also been animal and Y man, or – even – both X and Y could have been mammal, but we expect that such general words are less frequently used in these contexts, as their hyponyms are more appropriate. Adopting a similar approach to Santus et al. (2014a), we have measured the average entropy of the top N mostrelated contexts and used it as an index of generality. The higher the entropy, the less informative the word (i.e. it is more likely to be a hypernym). Our system uses one of these features for each target: C-Entr1,2."
    }, {
      "heading" : "4. Evaluation",
      "text" : ""
    }, {
      "heading" : "4.1 Tasks",
      "text" : "We have performed three tasks: i) an ablation test to evaluate the contribution of the features on our dataset (henceforth, ROOT9 Dataset; see Section 4.2); ii) an evaluation against the state of the art, and – in particular – against the best performant models in Weeds et al. (2014); iii) an evaluation on switched pairs to verify whether the actual semantic relations or the prototypical hypernyms (Levy et al., 2015) were learnt. For what concerns the ablation test, we performed it on a tree-classes classification task (hypernyms, co-hyponyms and randoms), removing each feature at a time and measuring the loss/gain (F1 score is used for the evaluation on a 10-fold cross validation). Thanks to this task, we have found that four of our features were in fact redundant, and we have therefore removed them from the final model, turning ROOT13 into ROOT9. This is discussed in Section 5. Once the best model has been identified, we have performed three binary classification tasks, involving only two classes per time. F1 score on a 10-fold cross validation was chosen as accuracy measure. The second task, which is described in Section 6, consisted in binary classification tasks on the four datasets proposed by Weeds et al. (2014). These datasets are described below, in Section 4.3. The task allowed us to compare ROOT9 against the state of the art models reported in Weeds et al. (2014). The last task is described in Section 7. It was performed on an extended ROOT9 Dataset, including also 3,200 randomly switched hypernyms to verify whether they were classified as hypernyms or as randoms.\n4.2 ROOT9 Dataset We have used 9,600 pairs, randomly extracted from three datasets: EVALution (Santus et al., 2015), Lenci/Benotto (Benotto, 2015) and BLESS (Baroni and Lenci, 2011), which is freely available at https://github.com/esantus/ROOT9. The pairs are equally distributed among the three classes (i.e., hypernyms, co-hyponyms and random words) and involve several Parts-Of-Speech (i.e., adjectives, nouns and verbs). The class of hypernyms contains 2,447 noun pairs, 458 verb pairs and 295 adjective pairs. The class of co-hyponyms has only 3,200 noun pairs, which were completely derived from BLESS, as this relation does not exist in the other two datasets. The class of randoms contains 1,100 noun pairs, 1,050 verb pairs and 1,050 random pairs. The full dataset contains 4,263 terms (2,380 nouns, 958 verbs and 927 adjectives), so that every term occurs on average 4.5 times. Considering only the first word in the pairs, we have 1,265 different terms (987 nouns, 186 verbs and 92 adjectives). Considering instead only the second word, we have 3,665 terms (1,945 nouns, 860 verbs and 862 adjectives). In the third task, we have extended this dataset randomly switching the 3,200 hypernymy pairs (e.g. from “car HYPER vehicle” to “car RANDOM mammal”) to verify whether ROOT9 was able to classify them as randoms.\n4.3 Weeds Dataset In order to compare ROOT9 to the state-of-the-art, we have evaluated it with the datasets created by Weeds et al. (2014). 2 These are four datasets, containing respectively: i) hypernyms versus other relations (extracted from WordNet; henceforth WN Hyper); ii) co-hyponyms versus other relations (extracted from WordNet; henceforth WN Co-Hyp); iii) hypernyms versus other relations (extracted from BLESS; henceforth Bless Hyper); iv) co-hyponyms versus other relations (extracted from BLESS; henceforth Bless Co-Hyp). The WN dataset (Weeds et al., 2014) – meaning both WN Hyper and WN Co-Hyp – in particular, was built after noticing that supervised systems tended to perform well also on random vectors. This happens because they are able to learn ontological information and re-use it whenever the words re-appear in other pairs. For this reason, the authors have constructed a dataset where words occurred at most twice (once on the left and once on the right of the relation). In this dataset, ontological information cannot be learnt and re-used, and indeed the random vectors cannot perform well. Unfortunately our DSM did not cover the whole datasets, because of the chosen frequency threshold (in Table 1, we report the size of our subsets in comparison to the original datasets). However, Weeds et al. (2014) kindly provided\n2 The datasets are freely available at: https://github.com/SussexCompSem/learninghypernyms\nthe results of their models on our subsets, so that the comparison is representative 3 ."
    }, {
      "heading" : "4.4 Baselines and Other Models",
      "text" : "For our internal tests, we have implemented two baselines, which can be used as reference for evaluating the performance of ROOT9: COSINE and RANDOM13. The first baseline simply uses the vector cosine (COSINE) with a Random Forest classifier in the default settings (i.e. 100 trees, 1 seed, and maxDepth and numFeatures initialized to 0). This baseline is supposed to perform particularly well in discriminating similar words (i.e. hypernyms and co-hyponyms) from randoms. In fact, this measure has been extensively used to identify word similarity in vector spaces (Turney and Pantel, 2010) because it verifies the normalized correlation between the vectors of \uD835\uDC641 and \uD835\uDC642:\ncos(\uD835\uDC641, \uD835\uDC642) =  ∑ \uD835\uDC531\uD835\uDC56 × \uD835\uDC5B \uD835\uDC56=1 \uD835\uDC532\uD835\uDC56\n√∑(\uD835\uDC531\uD835\uDC56) 2 × √∑(\uD835\uDC532\uD835\uDC56) 2\nwhere \uD835\uDC53\uD835\uDC65\uD835\uDC56 is the i-th dimension in the vector x. The second baseline (RANDOM13) relies on a default Random Forest classifier, but uses thirteen randomly initialized features, with values between 0 and 1. While the vector cosine achieves a reasonable accuracy, which is anyway far below the results obtained by our model, the random baseline performs much worst. The discrepancy with what found by Weeds et al. (2014) – namely that random vectors perform particularly well when words are re-used in the dataset – may depend on the small number of features, which does not allow the system to identify discriminative random dimensions. In the second task (see Section 6), we have used as baselines the most competitive models reported in Weeds et al. (2014), namely the SVM classifiers trained on the PPMI vector of the second word (svmSINGLE), or on the concatenated (svmCAT), summed (svmADD), multiplied (svmMULT) and subtracted (svmDIFF) PPMI vectors of the words in the pair. Such vectors contain as features all major grammatical dependency relations involving open class Parts Of Speech. Also, the performance of three main unsupervised methods is reported as a reference: cosine (see above in this section), balAPinc (Kotlerman et al., 2010) and invCL (Lenci and Benotto, 2012). A threshold p empirically found in a training set was used in these methods for the decision,\n3 The subsets of Weeds et al. (2014)’s datasets are also available at https://github.com/esantus/ROOT9."
    }, {
      "heading" : "5. Task 1: Ablation Test and Binary Classification",
      "text" : "Table 2 describes the features’ contribution in the ablation test. Given the set of thirteen features of ROOT13 (Santus et al., 2016b), we have removed them one by one and measured the loss (negative) or the gain (positive).\nAs it can be easily seen from the table, most of features are contributing for an increment between 1.12% and 2.46%. The highest contribution comes from the C-Entr1,2, which were inspired at SLQS (Santus et al., 2014a), and the second highest contribute is given by APSyn, which was introduced in Santus et al. (2016a). Interestingly, four out of thirteen features were not contributing, penalizing the performance somewhere between 0.11% and 0.34%. These features are Diff Entr, Diff Freq, Co-Occurrence, and APSyn and Shared, when they are used together (so we kept only APSyn, removing Shared). The main reason why these features seem to affect negatively the results could be that they contain redundant information. If we remove both APSyn and Shared, for example, we have a loss of 1.79%, but when we remove only one of them we have a gain of 0.34%. In a similar way, Diff Entr and Diff Freq can be seen as redundant in respect to the features Entr1,2 and Freq1,2. Perhaps surprisingly, Cooc does not contribute to the final score, and instead penalizes it. Removing the four redundant features (we removed Shared but kept APSyn), ROOT13 turns into ROOT9. This system outperforms all the baselines (COSINE, RANDOM13) and ROOT13. For the sake of completeness, in Table 2 we also report the performance of ROOT9 using Logistic Regression (Cessie, 1992) and\nSMO (Keerthi et al., 2001) classifiers. As it can be seen, the Random Forest version largely outperforms the other classifiers in this dataset. However, it is worth noticing here that such difference disappears with the WN datasets proposed by Weeds et al. (2014). See section 6, and – in particular – Table 4.\nTable 3 describes the results of ROOT9 and the baseline in the binary classification tasks. These results confirm the analysis suggested above."
    }, {
      "heading" : "6. Task 2: ROOT9 vs. State of the Art",
      "text" : "In Table 4, we show ROOT9’s performance compared to the best systems reported by Weeds et al. (2014). The scores are all calculated on subsets of Weeds et al. (2014)’s datasets, as reported in Section 4.3. Considering all the datasets, ROOT9 is the second best performing system, after svmCAT (Weeds et al., 2014), which uses the SVM classifier on the concatenation of PPMI vectors, containing as features all major grammatical dependency relations involving open class Parts Of Speech. The SVM classifier on the sum (svmADD) and the multiplication (svmMULT) of the same PPMI vectors performs better in identifying co-hyponyms, but worst in identifying hypernyms. The SVM on the difference (svmDIFF) and on the second PPMI vector (svmSINGLE) is instead particularly good at identifying hypernyms, while it performs bad at identifying co-hyponyms. Among the unsupervised methods, we report the results for the cosine and the methods of Lenci and Benotto (2012; invCL) and Kotlerman et al. (2010; balAPinc). Such methods classify using the best threshold p observed in the training sets. In general, unsupervised methods are less competitive. Differently from what observed in Section 5, the performance of ROOT9 does not change by adopting a different classifier (i.e., Random Forest, SMO or Logistic Regression) on the WN Hyper and WN Co-Hyp datasets. However, it drastically changes again on the BLESS Hyper and BLESS Co-Hyp datasets. This may depend on the ability of the Random Forest classifier to learn more ontological information than SMO and Logistic Regression, also when the number of features is small."
    }, {
      "heading" : "7. Task 3: Learning Prototypical Hypernyms?",
      "text" : "Finally, we have tried to test Levy et al. (2015)’s claim by evaluating the classifier on a dataset containing 3,200 hypernyms and 3,200 switched hypernyms (e.g. apple RANDOM animal and dog RANDOM fruit). In this evaluation, we have noticed that a large number of the switched hypernyms were indeed misclassified as hypernyms (up to 100% of them, if the words in the testing switched pairs were exactly the same used as hypernyms in the training set). In the attempt of correcting the behavior of the classifier, we extended the original 9,600 pairs dataset with other 3,200 switched hypernyms pairs labeled as randoms. It is relevant to notice that the switched hypernyms (tagged as randoms) contain the same words used in for the real hypernyms, and that in this new dataset, the size of the random class is double the others, including a total of 6,400 pairs. The new 10-fold cross validation test on the three classes registered a significant loss, passing from 90.7% to 84%. However, only 576 out of 6,400 randoms (most of which are likely to be the switched pairs) were misclassified as hypernyms."
    }, {
      "heading" : "8. Conclusions",
      "text" : "In this paper, we have described ROOT9, a classifier for hypernyms, co-hyponyms and random words that is derived from an optimization of ROOT13 (Santus et al., 2016b). The classifier, based on the Random Forest algorithm, uses only nine unsupervised corpus-based features, which have been described, and their contribution assessed. The impressive results in our dataset, developed by randomly extracting 9,600 pairs from EVALution (Santus et al., 2015), Lenci/Benotto (Benotto, 2015) and BLESS (Baroni and Lenci, 2011), were further tested against the state-of-the-art models presented in Weeds et al. (2014). The comparison has\nshown that ROOT9 is in fact competitive with the state of the art, being outperformed on all the datasets only by an SVM trained on concatenated PPMI vectors. Interestingly, while on our dataset and on BLESS the chosen classifier is fundamental for the performance, on the WN Hyper and WN Co-Hyp datasets, Random Forest, SMO and Logistic Regression algorithm achieved a similar performance. Finally, we have noticed the Levy et al. (2015)’s effect. However, we reduced it by training the model also on negative examples, namely switched hypernyms labeled as randoms (e.g. apple RANDOM animal, dog RANDOM fruit). In future experiment, we plan to increase the number of features, investigating new distributional properties that may help in the classification without incurring in memorization effects such as those described by Levy et al. (2015)."
    }, {
      "heading" : "9. Acknowledgements",
      "text" : "We are very thankful to Julie Weeds for having helped us, recalculating the results of the Weeds et al. (2014) models also for our subsets of their datasets. Thanks also to Aristotelis Kostopoulos for the precious suggestions. This work is partially supported by HK PhD Fellowship Scheme under PF12-13656\n10. Main References\nBaroni, M., Lenci, A. (2011). How we BLESSed\ndistributional semantic evaluation”. EMNLP 2011.\nBaroni, M., Bernardi, R., Do, N.-Q., and Shan, C.-C.\n(2012). Entailment above the word level in distributional semantics. In Proceedings of ACL 2012, pages 23–32, Avignon, France. Benotto, Giulia. (2015). Distributional Models for\nSemantic Relations: A Sudy on Hyponymy and Antonymy. PhD Thesis, University of Pisa. Breiman, L. (2001). Random Forests. Machine Learning.\nVol. 45 (1).\nCessie, S., van Houwelingen, J.C. (1992). Ridge\nEstimators in Logistic Regression. In Applied Statistics. 41(1):191-201. Charles, W. G. and Miller G. A. (1989). Contexts of\nantonymous adjectives. In Applied Psychology, 10:357–375. Clarke, D. (2009). Context-theoretic semantics for natural\nlanguage: an overview. In Proceedings of the EACL 2009 Workshop on GEMS: GEometrical Models of Natural Language Semantics, Athens, Greece: 112– 119. Evert, S. (2005). The Statistics of Word Cooccurrences:\nWord Pairs and Collocations. Dissertation, University of Stuttgart. Fellbaum C. (1998). WordNet: An electronic lexical\ndatabase. Cambridge, MA: MIT Press.\nGeffet, M. and Dagan, I. (2005). “The Distributional\nInclusion Hypotheses and Lexical Entailment”. ACL\n2005.\nHarris, Z. (1954). Distributional structure. Word, Vol. 10\n(23). 146-162.\nHearst, M. A. (1992). Automatic Acquisition of\nHyponyms from Large Text Corpora. Proceedings of the 14th International Conference on Computational Linguistics. Nantes, France. 539-545. Herbelot, A. and Ganesalingam, M. (2013). Measuring\nsemantic content in distributional vectors. In Proceedings of ACL2013. Keerthi, S.S., Shevade, S.K., Bhattacharyya, C., and\nMurthy, K.R.K. (2001). Improvements to Platt's SMO Algorithm for SVM Classifier Design. Neural Computation. 13(3):637-649. Kotlerman, L., Dagan, I., Szpektor, I., and\nZhitomirsky-Geffet, M. (2010). Directional Distributional Similarity for Lexical Inference. In Natural Language Engineering, Vol. 16 (4). 359-389 Lenci, A., and Benotto, G. (2012). Identifying hypernyms\nin distributional semantic spaces. In SEM 2012 – The First Joint Conference on Lexical and Computational Semantics, Vol. 2. 75-79. Levy, O., Remus, S., Biemann, C., and Dagan, I. (2015).\nDo Supervised Distributional Methods Really Learn Lexical Inference Relations? In Proceedings of NAACL 2015. Murphy, Gregory L.. (2002). The Big Book of Concepts.\nThe MIT Press, Cambridge, MA.\nMurphy, M. Lynne (2003). Semantic Relations and the\nLexicon Antonymy, Synonymy, and Other Paradigms. Cambridge University Press, Cambridge. Rimmel, L. (2014). Distributional Lexical Entailment by\nTopic Coherence. In Proceedings of EACL 2014.\nRoller, S., Erk K. and Boleda G. (2014). Inclusive yet\nSelective: Supervised Distributional Hypernymy Detection. In Proceedings of COLING 2014. Santus, E., Lenci, A., Lu, Q., and Schulte im Walde, S..\n(2014a). Chasing Hypernyms in Vector Spaces with Entropy. In Proceedings of EACL 2014. Santus, E., Lu, Q., Lenci, A., and Huang, C-R. (2014b).\nUnsupervised Antonym-Synonym Discrimination in Vector Space. CLIC-IT 2014. Santus, E., Lu, Q., Lenci, A., and Huang, C-R. (2014c).\nTaking Antonymy Mask off in Vector Space. PACLIC 2014. Santus, E., Yung, F., Lenci, A., Huang, C-R. (2015).\nEVALution 1.0: an Evolving Semantic Dataset for Training and Evaluation of Distributional Semantic Models. LDL, in ACL-IJCNLP 2015. Santus, E., Chiu, T.-S., Lu, Q., Lenci, A., and Huang,\nC.-R. (2016a). Unsupervised Measure of Word Similarity: How to Outperform Co-occurrence and Vector Cosine in VSMs. In Proceedings of AAAI 2016, Phoenix, Arizona (USA) Santus, E., Chiu, T.-S., Lu, Q., Lenci, A., and Huang,\nC.-R. (2016b). ROOT 13: Spotting Hypernyms, Co-Hyponyms and Randoms. In Proceedings of AAAI 2016, Phoenix, Arizona (USA). Shannon, C. E. (1948). A mathematical theory of\ncommunication. In Bell System Technical Journal, Vol. 27, pages 379-423 and 623-656. Szpektor, I. and Dagan, I. (2008). Learning Entailment\nRules for Unary Templates. In Proceedings of COLING 2008, Manchester, UK: 849–856. Turney, P.D. and Pantel, P. (2010). From Frequency to\nMeaning: Vector Space Models of Semantics. Journal of Articial Intelligence Research, Vol. 37. 141-188. Weeds, J. and Weir, D. (2003). A general framework for\ndistributional similarity. In Proceedings of the 2003 Conference on EMNLP. Sapporo, Japan. 81-88. Weeds, J., Clarke, D., Reffin, J., Weir, D., Keller, B.\n(2014). Learning to Distinguish Hypernyms and Co-Hyponyms. Proceedings of COLING 2014. Tungthamthiti, P., Santus E., Xu, H., Huang C.-R. and\nShirai, K.. (2015). Sentiment Analyzer with Rich Features for Ironic and Sarcastic Tweets. In Proceedings of PACLIC 2015, 178-187.\nLanguage Resource References\nBLESS. In: Baroni, M. and Lenci, A. (2011). How we\nBLESSed distributional semantic evaluation. Proceedings of the EMNLP 2011 Geometrical Models for Natural Language Semantics (GEMS 2011) Workshop. Edinburg, UK. 1-10. BLESS Hyper-Co-Hyp: In: Weeds, J., Clarke, D.,\nReffin, J., Weir, D., Keller, B. 2014. Learning to Distinguish Hypernyms and Co-Hyponyms. Proceedings of COLING 2014. EVALution. In: Santus, E., Yung, F., Lenci, A. and\nHuang C-R. (2015). EVALution 1.0: an Evolving Semantic Dataset for Training and Evaluation of Distributional Semantic Models. Proceedings of the 4th Workshop on Linked Data in Linguistics, ACL-IJCNLP 2015, 64. Lenci/Benotto. In: Benotto, Giulia. (2015).\nDistributional Models for Semantic Relations: A Sudy on Hyponymy and Antonymy. PhD Thesis, University of Pisa. WN Hyper-Co-Hyp: In: Weeds, J., Clarke, D., Reffin, J.,\nWeir, D., Keller, B. 2014. Learning to Distinguish Hypernyms and Co-Hyponyms. Proceedings of COLING 2014."
    } ],
    "references" : [ {
      "title" : "How we BLESSed distributional semantic evaluation",
      "author" : [ "M. BLESS. In: Baroni", "A. Lenci" ],
      "venue" : "Proceedings of the EMNLP",
      "citeRegEx" : "Baroni and Lenci,? \\Q2011\\E",
      "shortCiteRegEx" : "Baroni and Lenci",
      "year" : 2011
    }, {
      "title" : "Learning to Distinguish Hypernyms and Co-Hyponyms",
      "author" : [ "J. BLESS Hyper-Co-Hyp: In: Weeds", "D. Clarke", "J. Reffin", "D. Weir", "B. Keller" ],
      "venue" : "Proceedings of COLING",
      "citeRegEx" : "Weeds et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Weeds et al\\.",
      "year" : 2014
    }, {
      "title" : "EVALution 1.0: an Evolving Semantic Dataset for Training and Evaluation of Distributional Semantic Models",
      "author" : [ "E. EVALution. In: Santus", "F. Yung", "A. Lenci", "Huang C-R" ],
      "venue" : "Proceedings of the 4th Workshop on Linked Data in Linguistics,",
      "citeRegEx" : "Santus et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Santus et al\\.",
      "year" : 2015
    }, {
      "title" : "Distributional Models for Semantic Relations: A Sudy on Hyponymy and Antonymy",
      "author" : [ "Lenci/Benotto. In: Benotto", "Giulia" ],
      "venue" : "PhD Thesis,",
      "citeRegEx" : "Benotto and Giulia.,? \\Q2015\\E",
      "shortCiteRegEx" : "Benotto and Giulia.",
      "year" : 2015
    }, {
      "title" : "Learning to Distinguish Hypernyms and Co-Hyponyms",
      "author" : [ "J. WN Hyper-Co-Hyp: In: Weeds", "D. Clarke", "J. Reffin", "D. Weir", "B. Keller" ],
      "venue" : "Proceedings of COLING",
      "citeRegEx" : "Weeds et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Weeds et al\\.",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "order to compare the performance with the state-of-the-art, we have also evaluated ROOT9 in subsets of the Weeds et al. (2014)",
      "startOffset" : 107,
      "endOffset" : 127
    }, {
      "referenceID" : 1,
      "context" : "Co-hyponymy (or coordination) is instead the relation held by words sharing a close hypernym, which are therefore attributionally similar (Weeds et al., 2014).",
      "startOffset" : 138,
      "endOffset" : 158
    }, {
      "referenceID" : 1,
      "context" : "The ability of discriminating hypernymy, co-hyponymy and random words has potentially infinite applications, including Automatic Thesauri Creation, Paraphrasing, Textual Entailment, Sentiment Analysis and so on (Weeds et al., 2014; Tungthamthiti et al. 2015).",
      "startOffset" : 211,
      "endOffset" : 258
    }, {
      "referenceID" : 1,
      "context" : ", 2014, Weeds et al., 2014; Santus et al. 2014a; Rimmel, 2014; Lenci and Benotto, 2012; Kotlerman et al., 2010; Geffet and Dagan, 2005; Weeds and Weir, 2003). Both supervised and unsupervised approaches have been investigated. The former have been shown to outperform the latter in Weeds et al. (2014), even though Levy et al.",
      "startOffset" : 8,
      "endOffset" : 302
    }, {
      "referenceID" : 1,
      "context" : ", 2014, Weeds et al., 2014; Santus et al. 2014a; Rimmel, 2014; Lenci and Benotto, 2012; Kotlerman et al., 2010; Geffet and Dagan, 2005; Weeds and Weir, 2003). Both supervised and unsupervised approaches have been investigated. The former have been shown to outperform the latter in Weeds et al. (2014), even though Levy et al. (2015) have claimed that these methods may learn whether a term y is a prototypical hypernym, regardless of its actual relation with a term x.",
      "startOffset" : 8,
      "endOffset" : 334
    }, {
      "referenceID" : 2,
      "context" : "The feature contribution is evaluated with an ablation test, using a 10-fold cross validation on 9,600 pairs randomly extracted from EVALution (Santus et al., 2015) 1 , Lenci/Benotto (Benotto, 2015) and BLESS (Baroni and Lenci, 2011).",
      "startOffset" : 143,
      "endOffset" : 164
    }, {
      "referenceID" : 0,
      "context" : ", 2015) 1 , Lenci/Benotto (Benotto, 2015) and BLESS (Baroni and Lenci, 2011).",
      "startOffset" : 52,
      "endOffset" : 76
    }, {
      "referenceID" : 1,
      "context" : "In order to compare ROOT9 with the state-of-the-art, we have also evaluated it in the Weeds et al. (2014) datasets.",
      "startOffset" : 86,
      "endOffset" : 106
    }, {
      "referenceID" : 1,
      "context" : "datasets only by the svmCAT model (Weeds et al., 2014), which is a Support Vector Machine (SVM) classifier run on the concatenation of the distributional vectors of the words in the pairs.",
      "startOffset" : 34,
      "endOffset" : 54
    }, {
      "referenceID" : 2,
      "context" : "Santus et al. (2014a) formulated the Distributional Informativeness Hypothesis (DInH), according to which the generality of a term can be inferred from the informativeness of its most typical linguistic contexts.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 2,
      "context" : "Santus et al. (2014a) formulated the Distributional Informativeness Hypothesis (DInH), according to which the generality of a term can be inferred from the informativeness of its most typical linguistic contexts. In their evaluation, the authors have shown that hypernyms’ most typical contexts are in fact less informative than hyponyms’ ones. Among the supervised methods, Baroni et al. (2012) proposed to use an SVM classifier on the concatenation",
      "startOffset" : 0,
      "endOffset" : 396
    }, {
      "referenceID" : 1,
      "context" : "(2014) used the vectors’ difference, while Weeds et al. (2014) implemented numerous combinations (difference, multiplication, sum, concatenation, etc.",
      "startOffset" : 43,
      "endOffset" : 63
    }, {
      "referenceID" : 1,
      "context" : "(2014) used the vectors’ difference, while Weeds et al. (2014) implemented numerous combinations (difference, multiplication, sum, concatenation, etc.), comparing them against the most common unsupervised methods. The authors demonstrated that supervised methods generally perform better than unsupervised ones, but they acknowledge that these methods tend to learn ontological information, re-using it any time a word occur again in the dataset. For this reason, they suggest to adopt a new dataset, where words occur at most twice. Weeds et al. (2014)’s observation was further investigated by Levy et al.",
      "startOffset" : 43,
      "endOffset" : 554
    }, {
      "referenceID" : 1,
      "context" : "(2014) used the vectors’ difference, while Weeds et al. (2014) implemented numerous combinations (difference, multiplication, sum, concatenation, etc.), comparing them against the most common unsupervised methods. The authors demonstrated that supervised methods generally perform better than unsupervised ones, but they acknowledge that these methods tend to learn ontological information, re-using it any time a word occur again in the dataset. For this reason, they suggest to adopt a new dataset, where words occur at most twice. Weeds et al. (2014)’s observation was further investigated by Levy et al. (2015), who claimed that supervised methods learn whether a term y is a prototypical hypernym, regardless of",
      "startOffset" : 43,
      "endOffset" : 615
    }, {
      "referenceID" : 2,
      "context" : "ROOT13 was firstly introduced in Santus et al. (2016b). It uses the Random Forest algorithm implemented in Weka (Breiman, 2001), with the default settings (i.",
      "startOffset" : 33,
      "endOffset" : 55
    }, {
      "referenceID" : 2,
      "context" : "4 Shared and APSyn Shared and APSyn (Santus et al., 2016a-b) are two features that do not rely on the full distribution of the words, but on the top N most related contexts to the words in a pair, where N was empirically fixed at 1000. The value of this parameter was tested in other experiments, some of which reported in Santus et al. (2016a). Differently from Santus et al.",
      "startOffset" : 37,
      "endOffset" : 345
    }, {
      "referenceID" : 2,
      "context" : "Adopting a similar approach to Santus et al. (2014a), we have measured the average entropy of the top N",
      "startOffset" : 31,
      "endOffset" : 53
    }, {
      "referenceID" : 1,
      "context" : "2); ii) an evaluation against the state of the art, and – in particular – against the best performant models in Weeds et al. (2014); iii) an evaluation on switched pairs to verify whether the actual semantic relations or the prototypical hypernyms",
      "startOffset" : 112,
      "endOffset" : 132
    }, {
      "referenceID" : 1,
      "context" : "proposed by Weeds et al. (2014). These datasets are described below, in Section 4.",
      "startOffset" : 12,
      "endOffset" : 32
    }, {
      "referenceID" : 1,
      "context" : "proposed by Weeds et al. (2014). These datasets are described below, in Section 4.3. The task allowed us to compare ROOT9 against the state of the art models reported in Weeds et al. (2014). The last task is described in Section 7.",
      "startOffset" : 12,
      "endOffset" : 190
    }, {
      "referenceID" : 2,
      "context" : "datasets: EVALution (Santus et al., 2015), Lenci/Benotto (Benotto, 2015) and BLESS (Baroni and Lenci, 2011), which is freely available at https://github.",
      "startOffset" : 20,
      "endOffset" : 41
    }, {
      "referenceID" : 0,
      "context" : ", 2015), Lenci/Benotto (Benotto, 2015) and BLESS (Baroni and Lenci, 2011), which is freely available at https://github.",
      "startOffset" : 49,
      "endOffset" : 73
    }, {
      "referenceID" : 1,
      "context" : "3 Weeds Dataset In order to compare ROOT9 to the state-of-the-art, we have evaluated it with the datasets created by Weeds et al. (2014). 2 These are four datasets, containing respectively:",
      "startOffset" : 117,
      "endOffset" : 137
    }, {
      "referenceID" : 1,
      "context" : "The WN dataset (Weeds et al., 2014) – meaning both WN Hyper and WN Co-Hyp – in particular, was built after noticing that supervised systems tended to perform well also on random vectors.",
      "startOffset" : 15,
      "endOffset" : 35
    }, {
      "referenceID" : 1,
      "context" : "However, Weeds et al. (2014) kindly provided",
      "startOffset" : 9,
      "endOffset" : 29
    }, {
      "referenceID" : 1,
      "context" : "Coverage on Weeds et al. (2014)’s datasets.",
      "startOffset" : 12,
      "endOffset" : 32
    }, {
      "referenceID" : 1,
      "context" : "The discrepancy with what found by Weeds et al. (2014) – namely that random vectors perform particularly well when words are re-used in the dataset – may depend on the small number of features, which does not allow the system to identify discriminative random dimensions.",
      "startOffset" : 35,
      "endOffset" : 55
    }, {
      "referenceID" : 1,
      "context" : "The discrepancy with what found by Weeds et al. (2014) – namely that random vectors perform particularly well when words are re-used in the dataset – may depend on the small number of features, which does not allow the system to identify discriminative random dimensions. In the second task (see Section 6), we have used as baselines the most competitive models reported in Weeds et al. (2014), namely the SVM classifiers trained on the PPMI vector of the second word (svmSINGLE), or on the concatenated (svmCAT), summed (svmADD), multiplied (svmMULT) and subtracted (svmDIFF) PPMI vectors of",
      "startOffset" : 35,
      "endOffset" : 394
    }, {
      "referenceID" : 1,
      "context" : "3 The subsets of Weeds et al. (2014)’s datasets are also available at https://github.",
      "startOffset" : 17,
      "endOffset" : 37
    }, {
      "referenceID" : 2,
      "context" : "The highest contribution comes from the C-Entr1,2, which were inspired at SLQS (Santus et al., 2014a), and the second highest contribute is given by APSyn, which was introduced in Santus et al. (2016a). Interestingly, four out of thirteen features were not contributing, penalizing the performance somewhere between 0.",
      "startOffset" : 80,
      "endOffset" : 202
    }, {
      "referenceID" : 1,
      "context" : "However, it is worth noticing here that such difference disappears with the WN datasets proposed by Weeds et al. (2014). See section 6, and – in particular – Table 4.",
      "startOffset" : 100,
      "endOffset" : 120
    }, {
      "referenceID" : 1,
      "context" : "Considering all the datasets, ROOT9 is the second best performing system, after svmCAT (Weeds et al., 2014), which uses the SVM classifier on the concatenation of",
      "startOffset" : 87,
      "endOffset" : 107
    }, {
      "referenceID" : 1,
      "context" : "In Table 4, we show ROOT9’s performance compared to the best systems reported by Weeds et al. (2014). The scores are all calculated on subsets of Weeds et al.",
      "startOffset" : 81,
      "endOffset" : 101
    }, {
      "referenceID" : 1,
      "context" : "In Table 4, we show ROOT9’s performance compared to the best systems reported by Weeds et al. (2014). The scores are all calculated on subsets of Weeds et al. (2014)’s datasets, as reported in Section 4.",
      "startOffset" : 81,
      "endOffset" : 166
    }, {
      "referenceID" : 1,
      "context" : "STATE OF THE ART (Weeds et al., 2014)",
      "startOffset" : 17,
      "endOffset" : 37
    }, {
      "referenceID" : 2,
      "context" : "The impressive results in our dataset, developed by randomly extracting 9,600 pairs from EVALution (Santus et al., 2015), Lenci/Benotto (Benotto, 2015) and BLESS (Baroni and Lenci, 2011), were further tested against the state-of-the-art models presented in Weeds et al.",
      "startOffset" : 99,
      "endOffset" : 120
    }, {
      "referenceID" : 0,
      "context" : ", 2015), Lenci/Benotto (Benotto, 2015) and BLESS (Baroni and Lenci, 2011), were further tested against the state-of-the-art models presented in Weeds et al.",
      "startOffset" : 49,
      "endOffset" : 73
    }, {
      "referenceID" : 0,
      "context" : ", 2015), Lenci/Benotto (Benotto, 2015) and BLESS (Baroni and Lenci, 2011), were further tested against the state-of-the-art models presented in Weeds et al. (2014). The comparison has",
      "startOffset" : 50,
      "endOffset" : 164
    }, {
      "referenceID" : 1,
      "context" : "We are very thankful to Julie Weeds for having helped us, recalculating the results of the Weeds et al. (2014) models also for our subsets of their datasets.",
      "startOffset" : 91,
      "endOffset" : 111
    } ],
    "year" : 2016,
    "abstractText" : "ROOT9 is a supervised system for the classification of hypernyms, co-hyponyms and random words that is derived from the already introduced ROOT13 (Santus et al., 2016). It relies on a Random Forest algorithm and nine unsupervised corpus-based features. We evaluate it with a 10-fold cross validation on 9,600 pairs, equally distributed among the three classes and involving several Parts-Of-Speech (i.e. adjectives, nouns and verbs). When all the classes are present, ROOT9 achieves an F1 score of 90.7%, against a baseline of 57.2% (vector cosine). When the classification is binary, ROOT9 achieves the following results against the baseline: hypernyms-co-hyponyms 95.7% vs. 69.8%, hypernyms-random 91.8% vs. 64.1% and co-hyponyms-random 97.8% vs. 79.4%. In order to compare the performance with the state-of-the-art, we have also evaluated ROOT9 in subsets of the Weeds et al. (2014) datasets, proving that it is in fact competitive. Finally, we investigated whether the system learns the semantic relation or it simply learns the prototypical hypernyms, as claimed by Levy et al. (2015). The second possibility seems to be the most likely, even though ROOT9 can be trained on negative examples (i.e., switched hypernyms) to drastically reduce this bias.",
    "creator" : "Microsoft® Word 2010"
  }
}