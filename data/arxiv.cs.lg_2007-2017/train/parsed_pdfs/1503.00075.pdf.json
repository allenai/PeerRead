{
  "name" : "1503.00075.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks",
    "authors" : [ "Kai Sheng Tai", "Richard Socher", "Christopher D. Manning" ],
    "emails" : [ "kst@cs.stanford.edu", "richard@metamind.io", "manning@stanford.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Most models for distributed representations of phrases and sentences—that is, models where realvalued vectors are used to represent meaning—fall into one of three classes: bag-of-words models, sequence models, and tree-structured models. In bag-of-words models, phrase and sentence representations are independent of word order; for example, they can be generated by averaging constituent word representations (Landauer and Dumais, 1997; Foltz et al., 1998). In contrast, sequence models construct sentence representations as an order-sensitive function of the sequence of tokens (Elman, 1990; Mikolov, 2012). Finally, in tree-structured models, each phrase and sentence representation is composed from its constituent subphrases following a given syntactic structure over the words and phrases in the sentence (Goller and Kuchler, 1996; Socher et al., 2011).\nIn order to produce sentence representations that fully capture the semantics of natural lan-\nguage, order-insensitive models are insufficient due to their inability to account for differences in meaning as a result of differences in word order or syntactic structure (e.g., “cats climb trees” vs. “trees climb cats”). We therefore turn to ordersensitive sequential or tree-structured models. In particular, tree-structured models are a linguistically attractive option due to their relation to syntactic interpretations of sentence structure. A natural question, then, is the following: to what extent (if at all) can we do better with tree-structured models as opposed to sequential models for sentence representation? In this paper, we work towards addressing this question by directly comparing a type of sequential model that has recently been used to achieve state-of-the-art results in several NLP tasks against its tree-structured generalization.\nDue to their capability for processing arbitrarylength sequences, recurrent neural networks (RNNs) are a natural choice for sequence model-\nar X\niv :1\n50 3.\n00 07\n5v 1\n[ cs\n.C L\n] 2\n8 Fe\nb 20\n15\ning tasks. Recently, RNNs with Long Short-Term Memory (LSTM) units (Hochreiter and Schmidhuber, 1997) have re-emerged as a popular architecture due to their effectiveness at capturing long-term dependencies. LSTM networks, which we review in Sec. 2, have been successfully applied to a variety of sequence modeling and prediction tasks, notably machine translation (Bahdanau et al., 2014; Sutskever et al., 2014), speech recognition (Graves et al., 2013), image caption generation (Vinyals et al., 2014), and program execution (Zaremba and Sutskever, 2014).\nIn this paper, we introduce a generalization of the standard LSTM architecture to tree-structured network topologies and show its superiority for representing sentence meaning over a sequential LSTM. While the standard LSTM composes its hidden state from the input at the current time step and the hidden state of the LSTM unit in the previous time step, the tree-structured LSTM, or Tree-LSTM, composes its state from an input vector and the hidden states of arbitrarily many child units. The standard LSTM can then be considered a special case of the Tree-LSTM where each internal node has exactly one child.\nWe evaluate the Tree-LSTM architecture on two tasks: semantic relatedness prediction on sentence pairs and sentiment classification of sentences drawn from movie reviews."
    }, {
      "heading" : "2 Long Short-Term Memory Networks",
      "text" : ""
    }, {
      "heading" : "2.1 Overview",
      "text" : "Recurrent neural networks (RNNs) are able to process input sequences of arbitrary length via the recursive application of a transition function on a hidden state vector ht. At each time step t, the hidden state ht is a function of the input vector xt that the network receives at time t and its previous hidden state ht−1. For example, the input vector xt could be a vector representation of the t-th word in body of text (Elman, 1990; Mikolov, 2012). The hidden state ht ∈ Rd can be interpreted as a d-dimensional distributed representation of the sequence of tokens observed up to time t.\nCommonly, the RNN transition function is an affine transformation followed by a pointwise nonlinearity such as the hyperbolic tangent function:\nht = tanh (Wxt + Uht−1 + b) .\nUnfortunately, a problem with RNNs with transi-\ntion functions of this form is that during training, components of the gradient vector can grow or decay exponentially over long sequences (Hochreiter, 1998; Bengio et al., 1994). This problem with exploding or vanishing gradients makes it difficult for the RNN model to learn long-distance correlations in a sequence.\nThe LSTM architecture (Hochreiter and Schmidhuber, 1997) addresses this problem of learning long-term dependencies by introducing a memory cell that is able to preserve state over long periods of time. While numerous LSTM variants have been described, here we describe the version used by Zaremba and Sutskever (2014).\nWe define the LSTM unit at each time step t to be a collection of vectors in Rd: an input gate it, a forget gate ft, an output gate ot, a memory cell ct and a hidden state ht. The entries of the gating vectors it, ft and ot are in [0, 1]. We refer to d as the memory dimension of the LSTM. The LSTM transition equations are the following:\nit = σ ( W (i)xt + U (i)ht−1 + b (i) ) , (1)\nft = σ ( W (f)xt + U (f)ht−1 + b (f) ) ,\not = σ ( W (o)xt + U (o)ht−1 + b (o) ) ,\nut = tanh ( W (u)xt + U (u)ht−1 + b (u) ) ,\nct = it ut + ft ct−1, ht = ot tanh(ct),\nwhere xt is the input at the current time step, σ denotes the logistic sigmoid function and denotes elementwise multiplication. Intuitively, the forget gate controls the amount of which each unit of the memory cell is erased, the input gate controls how much each unit is updated, and the output gate controls the exposure of the internal memory state. The hidden state vector in an LSTM unit is therefore a gated, partial view of the state of the unit’s internal memory cell. Since the value of the gating variables vary for each vector element, the model can learn to represent information over multiple time scales."
    }, {
      "heading" : "2.2 Variants",
      "text" : "Two commonly-used variants of the basic LSTM architecture are the Bidirectional LSTM and the Multilayer LSTM (also known as the stacked or deep LSTM).\nBidirectional LSTM. A Bidirectional LSTM (Graves et al., 2013) consists of two LSTMs that are run in parallel: one on the input sequence and the other on the reverse of the input sequence. At each time step, the hidden state of the Bidirectional LSTM is the concatenation of the forward and backward hidden states. This setup allows the hidden state to capture both past and future information.\nMultilayer LSTM. In Multilayer LSTM architectures, the hidden state of an LSTM unit in layer ` is used as input to the LSTM unit in layer `+1 in the same time step (Graves et al., 2013; Sutskever et al., 2014; Zaremba and Sutskever, 2014). Here, the idea is to let the higher layers capture longerterm dependencies of the input sequence.\nThese two variants can be combined as a Multilayer Bidirectional LSTM (Graves et al., 2013)."
    }, {
      "heading" : "3 Tree-Structured LSTMs",
      "text" : "A limitation of the LSTM architectures described in the previous section is that they only allow for strictly sequential information propagation. Here, we propose two natural extensions to the basic LSTM architecture: the Child-Sum Tree-LSTM and the N-ary Tree-LSTM. Both variants allow for richer network topologies where each LSTM unit is able to incorporate information from multiple child units.\nAs in standard LSTM units, each Tree-LSTM unit contains input and output gates ij and oj , a memory cell cj and hidden state hj . The difference between the standard LSTM unit and TreeLSTM units is that gating vectors and memory cell updates are dependent on the states of possibly many child units. Additionally, instead of a single forget gate, the Tree-LSTM unit contains one forget gate fjk for each child k. This allows the Tree-LSTM unit to selectively incorporate information from each child. For example, a Tree-LSTM model can learn to emphasize semantic heads in a semantic relatedness task, or it can learn to preserve the representation of sentimentrich children for sentiment classification.\nAs with the standard LSTM, each Tree-LSTM unit takes an input vector xj . In our applications, each xj is a vector representation of a word in a sentence. The input word at each node depends on the tree structure used for the network. In a Tree-LSTM over a dependency tree, each node in the tree takes the vector corresponding to the\nhead word as input. In a Tree-LSTM over a constituency tree, the leaf nodes take the corresponding word vectors as input."
    }, {
      "heading" : "3.1 Child-Sum Tree-LSTMs",
      "text" : "Given a tree, let C(j) denote the set of children of node j. The Child-Sum Tree-LSTM transition equations are the following:\nh̃j = ∑\nk∈C(j)\nhk, (2)\nij = σ ( W (i)xj + U (i)h̃j + b (i) ) , (3)\nfjk = σ ( W (f)xj + U (f)hk + b (f) ) , (4)\noj = σ ( W (o)xj + U (o)h̃j + b (o) ) , (5)\nuj = tanh ( W (u)xj + U (u)h̃j + b (u) ) , (6)\ncj = ij uj + ∑\nk∈C(j)\nfjk ck, (7)\nhj = oj tanh(cj), (8)\nwhere in Eq. 4, k ∈ C(j). Intuitively, we can interpret each parameter matrix in these equations as encoding correlations between the component vectors of the Tree-LSTM unit, the input xj , and the hidden states hk of the unit’s children. For example, in a dependency tree application, the model can learn parameters W (i) such that the components of the input gate ij have values close to 1 (i.e., “open”) when a semantically important content word (such as a verb) is given as input, and values close to 0 (i.e., “closed”) when the input is a relatively unimportant word (such as a determiner).\nSince the Child-Sum Tree-LSTM unit conditions its components on the sum of child hidden states hk, it is well-suited for trees with high branching factor or whose children are unordered. For example, it is a good choice for dependency trees, where the number of dependents of a head can be highly variable."
    }, {
      "heading" : "3.2 N -ary Tree-LSTMs",
      "text" : "The N -ary Tree-LSTM can be used on tree structures where the branching factor is at most N and where children are ordered, i.e., they can be indexed from 1 to N . For any node j, write the hidden state and memory cell of its kth child as hjk and cjk respectively. The N -ary Tree-LSTM transition equations are the following:\nij = σ ( W (i)xj +\nN∑ k=1 U (i) k hjk + b (i)\n) , (9)\nfjk = σ ( W (f)xj +\nN∑ `=1 U (f) k` hj` + b (f)\n) ,\n(10)\noj = σ ( W (o)xj +\nN∑ k=1 U (o) k hjk + b (o)\n) , (11)\nuj = tanh ( W (u)xj +\nN∑ k=1 U (u) k hjk + b (u)\n) ,\n(12)\ncj = ij uj + N∑ k=1 fjk cjk, (13)\nhj = oj tanh(cj), (14)\nwhere in Eq. 10, k = 1, 2, . . . , N . The introduction of separate parameter matrices for each child k allows the N -ary Tree-LSTM model to learn more fine-grained conditioning on the states of a unit’s children than the ChildSum Tree-LSTM. Consider, for example, a constituency tree application where the left child of a node corresponds to a noun phrase, and the right child to a verb phrase. Suppose that in this case it is advantageous to emphasize the verb phrase in the representation. Then the U (f)k` parameters can be trained such that the components of fj1 are close to 0 (i.e., “forget”), while the components of fj2 are close to 1 (i.e., “preserve”).\nNote that in general the number of parameters scales as O(N2|h|2); this architecture is therefore only practical for smallN . We can naturally apply\nBinary Tree-LSTM units to binarized constituency trees since left and right child nodes are distinguished.\nWhen the tree is simply a chain, both Eqs. 2–8 and Eqs. 9–14 reduce to the standard LSTM transitions, Eqs. 1."
    }, {
      "heading" : "4 Models",
      "text" : "We now describe two specific models that apply the Tree-LSTM architectures described in the previous section."
    }, {
      "heading" : "4.1 Tree-LSTM Classification",
      "text" : "In this setting, we wish to predict labels ŷ from a discrete set of classes Y for some subset of nodes in a tree. For example, the label for a node in a parse tree could correspond to some property of the phrase spanned by that node.\nAt each node j, the label ŷj given the inputs {x}j observed at nodes in the subtree rooted at j is predicted by a softmax classifier that takes the hidden state hj at the node as input:\np̂θ(y | {x}j) = softmax ( W (s)hj + b (s) ) ,\nŷj = argmax y p̂θ (y | {x}j) .\nThe cost function is the negative log-likelihood of the true class labels y(k) at each labeled node:\nJ(θ) = − 1 m m∑ k=1 log p̂θ ( y(k) ∣∣∣ {x}(k))+ λ 2 ‖θ‖22,\nwhere m is the number of labeled nodes in the training set and the superscript k indicates the kth labeled node."
    }, {
      "heading" : "4.2 Semantic Relatedness of Sentence Pairs",
      "text" : "Given a sentence pair, we wish to predict a real-valued similarity score in some range [1,K], where K > 1 is an integer. The sequence {1, 2, . . . ,K} is some ordinal scale of similarity, where higher scores indicate greater degrees of similarity, and we allow real-valued scores to account for ground-truth ratings that are an average over the evaluations of several human annotators.\nWe first produce sentence representations hL and hR for each sentence in the pair using a Tree-LSTM model over each sentence’s parse tree. Given these sentence representations, we predict the similarity score ŷ using a neural network that\nconsiders both the distance and angle between the pair (hL, hR):\nh× = hL hR, (15) h+ = |hL − hR|,\nhs = σ ( W (×)h× +W (+)h+ + b (h) ) ,\np̂θ = softmax ( W (p)hs + b (p) ) ,\nŷ = rT p̂θ,\nwhere rT = [1 2 . . . K] and the absolute value function is applied elementwise.\nWe want the expected rating under the predicted distribution p̂θ given model parameters θ to be close to the gold rating y ∈ [1,K]: ŷ = rT p̂θ ≈ y. We therefore define a sparse target distribution1 p that satisfies y = rT p:\npi =  y − byc, i = byc+ 1 byc − y + 1, i = byc 0 otherwise\nfor 1 ≤ i ≤ K. The cost function is the regularized KL-divergence between p and p̂θ:\nJ(θ) = 1\nm m∑ k=1 KL ( p(k) ∥∥∥ p̂(k)θ )+ λ2 ‖θ‖22, where m is the number of training pairs and the superscript k indicates the kth sentence pair."
    }, {
      "heading" : "5 Experiments",
      "text" : "We evaluate our Tree-LSTM architectures on two tasks: (1) sentiment classification of sentences sampled from movie reviews and (2) predicting the semantic relatedness of sentence pairs.\nIn comparing our Tree-LSTMs against sequential LSTMs, we control for the number of LSTM parameters by varying the dimensionality of the hidden states2. Details for each model variant are summarized in Table 1.\n1In the subsequent experiments, we found that optimizing this “classification” objective yielded better performance than a mean squared error objective.\n2For our Bidirectional LSTMs, the parameters of the forward and backward transition functions are shared. In our experiments, this achieved superior performance to Bidirectional LSTMs with untied weights and the same number of parameters (and therefore smaller hidden vector dimensionality)"
    }, {
      "heading" : "5.1 Sentiment Classification",
      "text" : "In this task, we predict the sentiment of sentences sampled from movie reviews. We use the Stanford Sentiment Treebank (SST) (Socher et al., 2013). There are two subtasks: binary classification of sentences, and fine-grained classification over five classes: very negative, negative, neutral, positive, and very positive. We use the standard train/dev/test splits of 6920/872/1821 for the binary classification subtask and 8544/1101/2210 for the fine-grained classification subtask (there are fewer examples for the binary subtask since neutral sentences are excluded).\nThe sentiment label at each node is predicted using the a softmax classifier, as described in Sec. 4.1. We use Binary Tree-LSTMs (Sec. 3.2) structured according to the constituency parse trees provided with the dataset. For the sequential LSTM models, we predict the sentiment for each phrase using the final hidden state after the entire phrase has been processed. We train the sequential LSTMs on all labeled spans in the training set."
    }, {
      "heading" : "5.2 Semantic Relatedness",
      "text" : "For a given pair of sentences, the semantic relatedness task is to predict a human-generated rating of the similarity of the two sentences in meaning.\nWe use the Sentences Involving Compositional Knowledge (SICK) dataset (Marelli et al., 2014), consisting of 9927 sentence pairs in a 4500/500/4927 train/dev/test split. The sentences are derived from existing image and video description datasets. Each sentence pair is annotated with a relatedness score y ∈ [1, 5], with 1 indicating that the two sentences are completely unrelated, and 5 indicating that the two sentences are very related. Each label is the average of 10 ratings assigned by different human annotators.\nHere, we use the similarity model described in\nSec. 4.2. For the similarity prediction network (Eqs. 15) we use a hidden layer of size 50. We compare two Tree-LSTM architectures for composing sentence representations: the Child-Sum Tree-LSTM architecture (Sec. 3.1) on dependency trees (Chen and Manning, 2014) and the Binary Tree-LSTM (Sec. 3.2) on binarized constituency trees (Klein and Manning, 2003)."
    }, {
      "heading" : "5.3 Hyperparameters and Training Details",
      "text" : "The hyperparameters for our models were tuned on the development set for each task.\nWe initialized our word representations using publicly available 300-dimensional Glove vectors (Pennington et al., 2014). For the sentiment classification task, word representations were fine-tuned during training with a learning rate of 0.1; no finetuning was performed for the semantic relatedness task.\nOur models were trained using AdaGrad (Duchi et al., 2011) with a learning rate of 0.05 and a minibatch size of 25. The model parameters were regularized with a per-minibatch L2 regularization strength of 10−4. The sentiment classifier was additionally regularized using dropout (Hinton et al., 2012)."
    }, {
      "heading" : "6 Results",
      "text" : ""
    }, {
      "heading" : "6.1 Sentiment Classification",
      "text" : "Our results are summarized in Table 2. As was the case with the convolutional neural network model\ndescribed by Kim (2014), we found that tuning word representations yielded a significant boost in performance on the fine-grained classification subtask, in contrast to the minor gains observed on the binary classification subtask. This suggests that fine-tuning helps distinguish positive/negative vs. neutral, strongly positive vs. positive, and strongly negative vs. negative, as opposed to positive vs. negative in the binary case.\nThe Bidirectional LSTM significantly outperformed the standard LSTM on the fine-grained subtask. Note that this result is achieved without introducing any additional parameters in the LSTM transition function since the forward and backward parameters are shared. This indicates that sentence length becomes a limiting factor for the (unidirectional) LSTM on the fine-grained subtask. Somewhat surprisingly, we do not observe a corresponding improvement on the binary subtask (indeed, we achieve similar results on all our single-layer LSTM models). We conjecture that the state that needs to be retained by the network in order to make a correct binary prediction is easily preserved by both the LSTM and Bidirectional LSTM models, whereas the fine-grained case requires more complex interactions between the input word representations and the hidden state of the LSTM unit.\nThe Tree-LSTM over constituency trees outperforms existing systems on the fine-grained classification subtask."
    }, {
      "heading" : "6.2 Semantic Relatedness",
      "text" : "Our results are summarized in Table 3. Following Marelli et al. (2014), we use the Pearson correlation coefficient, the Spearman correlation coefficient and mean squared error as evaluation metrics.\nThe mean vector baseline computes sentence representations as a mean of the representations of the constituent words. In the DT-RNN and SDTRNN models (Socher et al., 2014), the vector representation for each node in a dependency tree is a sum over affine-transformed child vectors, followed by a nonlinearity (the SDT-RNN conditions the affine transformation on the dependency relation with the child node). For each of our baselines, including the LSTM models, we use the similarity model described in Sec. 4.2.\nWe also compare against four of the topperforming systems3 submitted to the SemEval 2014 semantic relatedness shared task: ECNU (Zhao et al., 2014), The Meaning Factory (Bjerva et al., 2014), UNAL-NLP (Jimenez et al., 2014), and Illinois-LH (Lai and Hockenmaier, 2014). These systems are heavily feature engineered, generally using a combination of surface form overlap features and lexical distance features derived from WordNet or the Paraphrase Database (Ganitkevitch et al., 2013).\nOur LSTM models outperform these baseline systems without any additional feature engineering. On this task, the Dependency Tree LSTM model outperformed the sequential LSTM models."
    }, {
      "heading" : "7 Discussion and Qualitative Analysis",
      "text" : ""
    }, {
      "heading" : "7.1 Modeling Semantic Relatedness",
      "text" : "In Table 4, we list nearest-neighbor sentences retrieved from a 1000-sentence sample of the SICK test set. We compare the neighbors ranked by the Dependency Tree LSTM model against a baseline ranking by cosine similarity of the mean word vectors for each sentence.\nWe observe that the Dependency Tree LSTM model exhibits several desirable properties. Note that in the dependency parse of the second query sentence, the word “ocean” is the second-furthest\n3We list the strongest results we were able to find for this task; in some cases, these results are stronger than the official performance by the team on the shared task. For example, the listed result by Zhao et al. (2014) is stronger than their submitted system’s Pearson correlation score of 0.8280. We do not list the StanfordNLP submission since no description of the system is provided in Marelli et al. (2014).\nword from the root (“waving”), with a depth of 4. Regardless, the retrieved sentences are all semantically related to the word “ocean”, which indicates that the Tree-LSTM is able to both preserve and emphasize information from relatively distant nodes. Additionally, the Tree-LSTM model shows greater robustness to differences in sentence length. Given the query “two men are playing guitar”, the Tree-LSTM associates the phrase “playing guitar” with the longer, related phrase “dancing and singing in front of a crowd” (note as well that there is zero token overlap between the two phrases)."
    }, {
      "heading" : "7.2 Effect of Sentence Length",
      "text" : "We investigate the effect of sentence length on the performance of our LSTM models.\nRanking by mean word vector cosine similarity Score\nRanking by Dependency Tree LSTM model Score\nIn Fig. 4, we show the relationship between Pearson correlation with gold ratings and sentence length. The Bidirectional LSTM performs similarly to the standard LSTM for shorter inputs but significantly outperforms on longer sentence pairs. Over all windows except the last, the Dependency Tree LSTM consistently outperforms the sequential LSTM variants. The observation that the Dependency Tree LSTM outperforms even on short sentences suggests that the model benefits from the dependency information captured by the parse tree structure, rather than simply the compression of path lengths to the root.\nWe observe a similar outperformance by the Tree-LSTM on classifying the sentiment of shorter sentences (Fig. 3). Surprisingly, the Bidirectional LSTM also improves on the standard LSTM score in this short-sentence regime; this is perhaps attributable to the fact that the Bidirectional LSTM sentence representation has twice the dimensionality of that of the standard LSTM."
    }, {
      "heading" : "8 Related Work",
      "text" : "Distributed representations of words (Rumelhart et al., 1988; Collobert et al., 2011; Turian et al., 2010; Huang et al., 2012; Mikolov et al., 2013; Pennington et al., 2014) have found wide applicability in a variety of NLP tasks. Following this success, there has been substantial interest in the area of learning distributed phrase and sentence representations (Mitchell and Lapata, 2010; Yessenalina and Cardie, 2011; Grefenstette et al., 2013; Mikolov et al., 2013), as well as distributed\nrepresentations of longer bodies of text such as paragraphs and documents (Srivastava et al., 2013; Le and Mikolov, 2014).\nOur approach builds on recursive neural networks (Goller and Kuchler, 1996; Socher et al., 2011), which we subsequently abbreviate as TreeRNNs in order to avoid confusion with recurrent neural networks. Under the Tree-RNN framework, the vectorial representation associated with each node of a tree is composed as a function of the vectors corresponding to the children of the node. The choice of composition function gives rise to numerous variants of this basic framework. Tree-RNNs have been used to parse images of natural scenes (Socher et al., 2011), compose phrase representations from word vectors (Socher et al., 2012), and classify the sentiment polarity of sentences (Socher et al., 2013)."
    }, {
      "heading" : "9 Conclusion",
      "text" : "In this paper, we introduced a generalization of LSTMs to tree-structured network topologies. The Tree-LSTM architecture can be applied to trees with arbitrary branching factor. We demonstrated the effectiveness of the Tree-LSTM by applying the architecture in two tasks: semantic relatedness and sentiment classification, outperforming existing systems on both. Controlling for model dimensionality, we demonstrated that Tree-LSTM models are able to outperform their sequential counterparts. Our results suggest further lines of work in characterizing the differences between sequential and tree-structured models."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2015,
    "abstractText" : "A Long Short-Term Memory (LSTM) network is a type of recurrent neural network architecture which has recently obtained strong results on a variety of sequence modeling tasks. The only underlying LSTM structure that has been explored so far is a linear chain. However, natural language exhibits syntactic properties that would naturally combine words to phrases. We introduce the Tree-LSTM, a generalization of LSTMs to tree-structured network topologies. TreeLSTMs outperform all existing systems and strong LSTM baselines on two tasks: predicting the semantic relatedness of two sentences (SemEval 2014, Task 1) and sentiment classification (Stanford Sentiment Treebank).",
    "creator" : "LaTeX with hyperref package"
  }
}