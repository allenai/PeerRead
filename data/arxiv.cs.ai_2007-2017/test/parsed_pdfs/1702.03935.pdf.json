{
  "name" : "1702.03935.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Data-Intensive Supercomputing in the Cloud",
    "authors" : [ "Michael S. Warren", "Samuel W. Skillman", "Rick Chartrand", "Tim Kelton", "Ryan Keisler", "David Raleigh", "Matthew Turk" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "I. INTRODUCTION\nOne of the most rapid and remarkable technological advances in history occurred with the steam engine after Watt’s patent expired in 1800. Over a 30 year period, engineers improved the efficiency of the Cornish high-pressure steam engine by a factor of five [11] (a compounded improvement of 5.5% per year). Among computing and information systems today, such a rate of change would be considered very slow. Against the present background of Moore’s Law [10] (loosely representing a doubling in performance every 18 months, or 60% improvement per year) the technological rate of change in computing is 10 times higher than the peak of the industrial revolution. As software replaces physics as the system of the world, these rates of exponential change are becoming apparent in ever-larger aspects of people’s day-today experience.\nTwenty years ago, we were the first to demonstrate a commodity computing cluster (Figure 1) that was competitive with special-purpose HPC architectures [18, 19] and by some accounts [4], led to the demise of the traditional supercomputer. The effort to build our own cluster was a minor perturbation on the effort that had already been expended writing the application software. Today, the cloud is poised to play the same disruptive role with decreasing costs, vanishing barriers to entry, rapidly increasing performance, a stable programming interface, and a rich ecosystem of open-source software libraries and applications to build upon."
    }, {
      "heading" : "II. ORIGINS OF THE CLOUD",
      "text" : "Loki and Hyglac [19] were clusters of Intel processors with Ethernet as a communication fabric, using the Linux operating system, constructed in 1996. The same triad of Intel/Linux/Ethernet describes the majority of cloud computing systems today, and if we allow more exotic communication networks, it describes most of the fastest supercomputers in the world. While other groups were investigating the potential of commodity clusters in the same era [1], our commercial off-the-shelf (COTS) approach distinguished itself through winning the Gordon Bell price/performance prize in 1997. It is also notable that we built the first machine on the TOP500 list which used Linux as the OS in 1998 www.top500.org/ system/166764. Today, 99% of the TOP500 supercomputers run Linux www.top500.org/statistics/details/osfam/1, not to mention its prevalence in other things, from electric cars to nano-satellites [5]. This domination is not accidental; the hardware, operating system and network APIs established at all levels by the Intel/Linux pairing (with the world wide web at WAN scales) that came together in the mid-90s has provided a stable base computing environment for two decades.\nFollowing up the initial demonstration of the BEOWULF project [3], a clear outcome of this work was the democratization of access to HPC resources. A technical group could buy and assemble the components to make their own cluster of machines tailored to their own problem domain. Large dataanalysis problems were also amenable to this approach [17]. However, as the price per CPU continued to fall, the scale of the human effort required to assemble, house, power and maintain such a machine became more than a minor effort. While it was still possible to reach performance among the top 100 supercomputers with mail-order parts and part-time system management in 2002 [21], today it requires a machine with over 30,000 cores. Thus, economies of scale become a dominant factor for significant computing needs, and the cloud has naturally evolved from the fertile environment created from the early work with commodity clusters. The promise of [20] “The software community that develops around these machines (if it remains open and follows the Linux development philosophy) will allow users, vendors, and ar X iv :1\n70 2.\n03 93\n5v 1\n[ cs\n.D C\n] 1\n3 Fe\nb 20\nthe research community to each contribute to a robust software environment” has been realized.\nThere are certainly scientific computing problems outside the realm of cloud computing as it exists today, when dedicated low-latency communication and extreme scaling of tightlycoupled processes are required. Our past work in computational cosmology is one exemplar [14]. Even so, it is likely that a code which can scale efficiently to more than 105 processors on a dedicated supercomputer will perform adequately for smaller problems on a few thousand processors in the cloud. There is also no clear barrier (other than economic) preventing the deployment of HPC technology such as low-latency highperformance networks into the cloud in the future.\nWhile being a “utility” is one of the greatest features of cloud computing, it also is one of the factors that could slow its acceptance. Big computers are institutional status symbols. They serve as an impressive physical artifact to show to visitors who might otherwise have difficulty appreciating the intangible properties of a revolutionary algorithm or wellwritten software. Large computers grow whole ecosystems around themselves, involving procurement, machine rooms, electrical power and support staff. All of those local sources of influence are disrupted in a move to cloud computing, where all that remains is an optical fiber and a monthly bill."
    }, {
      "heading" : "III. ARCHITECTURAL CONSIDERATIONS",
      "text" : "A major advantage of the cloud is almost instant access to vast amounts of computing. In our experience it is the rule rather than the exception that thousands of cores can be spun up and running code within a minute or two. This is in stark contrast to a typical shared supercomputer where jobs can often sit in the queue for hours or days. When time-to-solution\nPUBLISHED GOOGLE CLOUD PLATFORM PRE-EMPTIBLE NODE PRICING AND OUR OWN PERFORMANCE MEASUREMENTS IN SEPTEMBER, 2016. BANDWIDTHS AND CAPACITIES HAVE BEEN CONVERTED TO A COST PER SECOND PER GIGA-UNIT TO FACILITATE TOTAL COST ESTIMATION. FOR EXAMPLE, STORING ONE PETABYTE (1 MILLION GIGABYTES) FOR ONE YEAR (31.5 MILLION SECONDS) IN CLOUD STORAGE COSTS $315,000. ONE DOLLAR CAN CURRENTLY BUY 60 SECONDS OF PROGRAMMING\nLABOR, DELIVER 10 GIGABYTES TO THE INTERNET, STORE 46 GIGABYTES IN DRAM FOR 1 DAY, OR PROVIDE 6× 1015 FLOATING POINT\nOPERATIONS.\nmatters, it matters when a job is done, not how fast it runs after it starts.\nFrom an economic point of view, Table I demonstrates the fundamental lesson that the cost of a programmer must be amortized over an enormous amount of computing. As the price of computing continues to fall, every programmer must contribute to a system which scales to larger and larger processor counts. The yearly salary of an average programmer is now equivalent to 1000 pre-emptible cores running 24 hours per day, 365 days per year. In that context, for any business not utilizing thousands of cores constantly, computing is already free. The converse is that programmers must be as productive as possible. Existing code or libraries that are suited to the task at hand must be used as often as possible, and anything that breaks existing code must be perceived as an enormous expense. Further, understanding the architectural constraints and best approaches for optimization in overall system design requires an additional level of insight and experience (Figure 2)."
    }, {
      "heading" : "A. Everything is a File",
      "text" : "While the cloud solves many of the practical difficulties around having access to computing capability, it does not make architecting and developing application software an easier problem. A particularly large stumbling block for large dataprocessing problems is the object storage API. One of the defining features of Unix and Unix-like operating systems is that “everything is a file.” Object storage does not conform to this fundamental interface, and thereby breaks a vast number of tools, utilities, libraries and application code. For much the same reason, we do not consider the Hadoop [23] ecosystem to be the right model for our data-intensive computing tasks, although this is mostly the fault of HDFS. It is possible that alternate approaches such as Apache Spark could contribute to our software infrastructure, although we have not yet deployed any solutions using that framework.\nPersistent storage in the cloud is generally made available through a RESTful interface where the traditional POSIX open/read/close are instead performed with a HTTP GET operation, while open/write/close is a PUT operation. The hierarchy of a traditional file system is replaced by globally unique identifier. Updating the data in an object requires it to be re-written in its entirety. These interface constraints allow object storage to be more scalable and able to be implemented less expensively than a traditional file system. An additional enormous benefit is that it largely eliminates the complexities associated with data locality, since all data is remote. The price to be paid is higher access latency, but much worse is the additional programming effort to modify code to work with an object store. Often it is easier to copy the data from an object store to an intermediate file on the local filesystem, process it using the normal POSIX I/O interface, and then copy the output back to object storage.\nFor moderate file sizes and low I/O rates, making a copy in local storage can perform reasonably well. The cost of the second read (from the intermediate file into application memory) is usually mostly hidden since the file is cached in memory by the operating system. For larger files at higher data rates, this breaks down and we will run into bandwidth restrictions at the local storage level. Using GCE as an example, standard local storage has a limit of 180 MB/s of read bandwidth and 120 MB/s of write bandwidth. We are then in the paradoxical position of being able to read from\na remote object store at a higher rate than from the (virtual) local disk.\nAs an alternative method, it is possible to read from the object store directly into memory. This can work, but it is also subject to difficulties. If the application reads directly into userspace memory, the ability of the operating system to share those data blocks with other processes is lost. (This sharing happens naturally via the filesystem interface). Additionally, many common programming library interfaces have not been designed to work with a memory interface (for instance, they expect a file name or file pointer, with the actual memory access being private to the library).\nWhen objects are large (approaching the size of the memory available to each processor) and existing code has been designed to read smaller portions of a file and process them individually, there are no good alternatives for interacting with object storage. The data must be accessed via an interface that provides for random read access. The currently favored method for mapping the file abstraction is to use a virtual file system, such as FUSE."
    }, {
      "heading" : "B. Festivus",
      "text" : "FUSE (Filesystem in Userspace) is an interface for userspace programs to export a filesystem to the Linux kernel. For the case of object storage, the job of FUSE is to translate things the kernel understands — system calls like open and read and identifiers like inodes or file names — to things the object store understands — actions like HTTP GET and\nidentifiers like a Uniform Resource Locator (URL). Several implementations of FUSE for cloud object storage have been implemented and are in use (s3fs, gcsfuse).\nNone of the existing FUSE implementations met our needs. The most significant problems were sub-optimal read performance and slow metadata access. Given the importance of a high-performance file system interface on top of cloud object storage, we felt developing our own low-level asynchronous FUSE interface was worth the effort.\nOur FUSE implementation is named festivus (a file system for the rest of us). It has been written from scratch using the low-level threaded asynchronous API of libfuse. libfuse provides the implementation for userspace communication with the FUSE kernel module, reading requests from the kernel which are passed to festivus using callbacks, and then returning the festivus responses to the kernel. Rather than query the object store itself for object metadata, we maintain our own separate scalable in-memory key/value store to perform metadata-related operations (this metadata server is shared by all instances of the file system). We currently use Redis [6], although similar functionality could be provided by other systems. An important optimization for object data access is to increase the Linux kernel parameter FUSE_MAX_PAGES_PER_REQ from its default value of 32 (which limits read chunks to 128k) to a much higher value (the results presented here increase this value to 1024 pages; 4 MB on the Intel Xeon). With this change, the VM_MAX_READAHEAD kernel parameter must also be modified appropriately. This optimization requires the modified kernel to be installed on any node running the festivus filesystem.\nIn addition to large distributed processing tasks, many pieces of production software leverage the POSIX file system. One such application is Mapserver, an open source software package for publishing spatial data and interactive mapping over http [16]. Traditionally, Mapserver is configured to serve data through its local file system, limiting the amount of imagery per node that can be served. Festivus allows processed, compressed, and tiled imagery stored in Google Cloud Storage to be served through the POSIX interface using Mapserver the same as if the data was served natively on attached disks. This allows hundreds of terabytes of imagery to be served on a single instance. Traditional mechanisms for scaling Mapserver would involve replicating the datasets for a specific set of imagery or leveraging large dedicated NFS systems. Mapserver on top of Festivus allows for horizontal scaling without further replication of data while leveraging the economies of scale and ease of data management provided by cloud object storage."
    }, {
      "heading" : "C. Domain Decomposition",
      "text" : "When computing in parallel, a task that must be done well in order to succeed is splitting up the data among processors. This is often referred to as domain decomposition. A single image of the Earth with pixel scales less than about 10km is too large to process efficiently, so the image must be “tiled”, or split into pieces that can be processed independently. For\ncurrent computer architectures and memory storage capacities, a reasonable size for image tiles would be between 256 x 256 and 4096 x 4096 pixels, depending on the application. Note that these tiles are not necessarily individual files, since some image formats support internal tiling, or even further, the API layer may provide virtual tiles which are constructed on-the-fly from the underlying data.\nTwo common map projections that represent the spherical surface of the Earth as a regular grid are the UTM (Universal Transverse Mercator) projection, and the Web Mercator projection. The Web Mercator projection is easily tiled, because the image dimensions are precisely a power of two in both coordinates. The level of the decomposition (L) divides the world into 4L pieces. An appropriate level can be chosen to satisfy various constraints. For instance, that a number of time slices for a given tile can fit into processor memory at one time. Web Mercator is ubiquitous for simple map interfaces, but can not be used for anything beyond simple analysis because the pixel areas are not equal. As a pixel becomes farther from the equator, it represents a smaller and smaller area on the surface of the Earth. Web Mercator has been declared unacceptable for official use by the US government http: //earth-info.nga.mil/GandG/wgs84/web mercator/index.html\nThe UTM projection is not as simple. UTM first splits the world into 60 zones, and within each zone pixels are split into nearly equal areas referenced by their “x” or “Easting” co-ordinate and their “y” or “Northing” co-ordinate. All UTM distances are measured in meters. The number of pixels which span a zone in the East-West direction depends on the distance from the equator.\nFor the most efficient and accurate processing of multiple datasets, they should share a common co-ordinate reference system. Since operations to interpolate pixels to a different map projection or resolution can affect the data quality and require additional computational resources, we seek to minimize the number of such operations. This suggests using UTM as the common map projection, since most data is delivered in UTM co-ordinates.\nThe UTM tiling system we have created is defined by a number of parameters. It is applied to each of the 60 UTM zones with identical parameters, with the zone designated by z. A similar construction can be applied to the polar UPS projection. The parameters are the origin of the tiling system, the number of pixels in the tile x and y dimension, the border (overlap), and the spatial resolution of the pixels. Since a UTM zone is 6 degrees across, that represents 668 km at the equator. For pixel scales larger than about 200 meters, a single tile will cover the east-west extent of a UTM zone. For smaller pixel scales, multiple tiles are required. For 10m resolution, such as the Sentinel-2 satellite, 17 4096-pixel wide tiles would be required.\nIn the y-dimension, the distance from the equator to the pole is near 10000km, so the number of 4096 x 4096 tiles to span that distance is about 10 for a 250m pixel tile, or 244 for a 10m tile. The southern hemisphere can be handled with a similar number of tiles using a negative index referenced from\nthe equator, or referenced by their northing co-ordinate from the south pole using the southern “S” designator for the zone.\nWe store our pre-processed imagery using the JPEG 2000 standard [8, 15] due to its significant advantages in terms of compression and image types as well as its support for internal tiling and a scalable multi-resolution codestream that can be ordered to best fit applications demands. We also use a small portion of the “Part 2” extensions defining the more flexible JPX file format."
    }, {
      "heading" : "IV. PERFORMANCE",
      "text" : ""
    }, {
      "heading" : "100 101 102 103 104 105 106 107",
      "text" : "Size (bytes)\n10-1\n100\n101\n102\n103\n104\nB a n d w\nid th\n( M\nb p s)\n101\n102\n103\n104\nLa te\nn cy\n( µ s)\nFig. 3. Bandwidth (solid) and latency (dashed) for a single thread on a 16- vCPU Google Compute Engine node, plotted against message size on the x-axis. Small message latency is about 40 microseconds, while large message bandwidth reaches 8.6 Gigabits/second. For multiple threads total bandwidth reaches 16 Gigabits/second. Compared to similar measurements from 20 years ago [18] message latency has improved a factor of 5 and bandwidth per HW core by about a factor of 20.\nCluster (2003) Cray XE6 Cloud Performance per core\nSTREAM copy (MB/s) 1203.5 2612.0 1753.9 STREAM add 1237.2 2591.7 1731.4 STREAM scale 1201.8 2820.3 1923.3 STREAM triad 1238.2 2857.9 1953.2 NPB BT (Mop/s) 321.2 1445.1 1244.0 NPB SP 216.5 706.0 613.5 NPB LU 404.3 1599.0 1897.9 NPB MG 385.1 1134.0 940.9 NPB CG 313.1 380.2 299.5 NPB FT 351.0 767.5 923.6 LINPACK (Gflop/s) 3.30 14.94 25.46\nTABLE II A COMPARISON OF 2003 ERA BEOWULF CLUSTER GORDON BELL PRICE/PERFORMANCE PRIZE FINALIST [21] TO A CRAY XE6 NODE AND 2015 CLOUD PERFORMANCE. THE TOP SET OF RESULTS IS MEMORY BANDWIDTH MEASURED IN MBYTES/S BY THE STREAM BENCHMARK [9], MIDDLE IS FROM THE CLASS C NAS PARALLEL BENCHMARKS [2] IN MOP/S, AND LAST LINE IS LINPACK GFLOP/S. BEOWULF RESULTS ARE PER CPU ON A 2.53 GHZ INTEL PENTIUM 4. CLOUD RESULTS ARE PER HW CORE ON A 16-CORE 2.3 GHZ INTEL HASWELL NODE. CRAY RESULTS ARE PER BULLDOZER COMPUTE UNIT OF A DUAL-SOCKET XE6 NODE WITH 2 AMD INTERLAGOS MODEL 6276 2.3 GHZ CPUS (16 CORES TOTAL). THESE DATA MAKE IT CLEAR THAT PERFORMANCE PER CORE HAS PROGRESSED LITTLE IN THE PAST 12 YEARS FOR UN-TUNED APPLICATIONS (BETWEEN FACTORS OF 1 AND 8, WITH MOST OF THESE BENCHMARKS AROUND A FACTOR OF 3).\nA. 1.21 Teraflops for $1/hr ASCI Red, the first Teraflop supercomputer, was deployed at Sandia National Laboratories for a cost of $46M (1996 dollars). Its processors were upgraded in 1999. Disregarding operational costs (which can be substantial), that corresponded to $1,749 per teraflop-hour. We have recently run the LINPACK benchmark on two pre-emptible 64-vCPU GCE nodes (n1-highcpu-64, 32 hardware cores per node and 57.6 GB of memory), and obtained a result of 1.21teraflops. Each of the n1-highcpu-64 nodes cost $0.51 per hour, coming to a total of just $0.84 per teraflop hour, an improvement in price-to-performance of over 2000x. We should also note the incredible ease in which our virtual teraflop cluster was both constructed and decommisioned within a span of several hours, most of which was spent performing a scan of the LINPACK parameter space for peak performance."
    }, {
      "heading" : "B. Festivus bandwidth",
      "text" : "In order to test the scalability of our FUSE-based filesystem, we ran a performance test of reading a random (different on each node) subset of our processed Landsat 8 imagery from Google cloud storage down to a distributed set of 512 16-vCPU n1-standard-16 virtual machines spread over the us-central1-c zone. We measured an aggregate incoming bandwidth to compute nodes across the project of over 231 GB/s. In table III we show the bandwidth as we scale up from a single us-central1-c node to 512 nodes. A 32-vCPU node reaches over 70% of its network capacity (Fig. III). In the transition from 16 to 64 nodes we observe a drop in bandwidth per node from approximately 1 GB/s to 500 MB/s, perhaps due to sharing of network bandwidth between nodes. We have run similar single-node benchmarks of gcsfuse, where we have observed a peak bandwidth of 340 MB/s. However, an important aspect of our festivus implementation is the ability to read smaller (∼ 1 MB) blocks of data from a larger single file. When reading random 4 MB chunks of data from multiple files, we observe a similar peak bandwidth with festivus of 850 MB/s on a single node. A similar experiment with gcsfuse reveals a peak bandwidth of only 47 MB/s (see Table IV)."
    }, {
      "heading" : "V. APPLICATIONS",
      "text" : "A. Initial Processing\nOne of the first major achievements of our satellite imagery pipeline was the processing of over one petabyte of\nLandsat and MODIS imagery in under 16 hours on April 16, 2015. This calculation is described in detail in [22]. Our input dataset consisted of 915.52 × 1012 bytes of Landsat data in 5693003 bzip compressed GeoTIFF files located at gs://earthengine-public/, and 101.83 × 1012 bytes of MODIS Level 1B (2QKM) band 1 (red) and 2 (near infrared) data in 613320 sz compressed HDF4 files (collected from the NASA ftp site and stored in Google Cloud Storage), for a total of 1017.35× 1012 bytes and 6306323 files.\nFor this project, we used Google Compute Engine (GCE), which is the Infrastructure as a Service (IaaS) component of Google’s Cloud Platform. GCE became generally available in December 2013, and offers virtual machines using KVM as the hypervisor.\nThe processing stages for each Landsat image include retrieving it from Google Cloud Storage, uncompressing it, parsing the metadata, identifying the bounding rectangle that contains valid data, cleaning the edges of the image, converting the raw pixel information into meaningful units (calibrated top of atmosphere reflectance using the appropriate constants for each satellite and accounting for solar distance and zenith angle), tiling each image, performing any necessary co-ordinate transformations, compressing the data into JPEG 2000 format, and storing the result back into Cloud Storage.\nSeveral optimizations were necessary to reduce the cost of resources involved in the computation (illuminated by Table I). In particular, we aggressively reduced memory usage to allow us to run on the smallest memory (and least expensive per core) Google Compute Engine nodes, which contain somewhat less than 2 GBytes of memory per hardware core, and used no conventional disk storage on the compute nodes at all (beyond the minimum 10 GB partition required to boot the system), working entirely in memory or from the Linux tmpfs RAM disk. Reducing memory usage is often difficult using Python’s NumPy array objects, since they often require intermediate copies during array operations. We also removed most of the intermediate writes to the local file system, going from memory buffer to memory buffer between application libraries. Memory bandwidth is an important factor in our overall\nperformance, and the measurements we provide in Table II are relevant for this application.\nTo manage the creation of asynchronous tasks for processing millions of scenes across the worker nodes, an asynchronous task queue approach (specifically the Python Celery library [7]) was used. Celery’s API allows multiple asynchronous job queues to be created, manage the list of tasks and their parameters, and insert them into a pluggable backend key-value pair store (Redis [13]). As worker nodes are provisioned and start, they connect to the Celery broker to receive processing tasks in the queue. Work tasks can define data location using cloud object storage’s native location and API’s, additionally, code utilizing POSIX file system can utilize Festivus for accessing data objects just as they would traditional POSIX based datasets. This allows code with dependencies on POSIX based filesystems access to cloud based datasets and leverages the economic and scale of cloud object storage."
    }, {
      "heading" : "B. Field segmentation",
      "text" : "Analysis of satellite imagery that is done at the pixel level fails to take advantage of an obvious source of additional information. The results of such analysis should be highly spatially correlated, as nearby pixels are likely to have the same land use. In the case of agriculture in particular, the land is divided into fields in which the same crop is planted, and which is subsequently subjected to very similar conditions. Identifying the pixels that belong to the same fields thus leads to improved analysis results.\nThe image in Figure 4 shows the segmentation into fields of one of our UTM tiles, covering a portion of Kherson Oblast in southern Ukraine, bordering the Dnieper River. The tile is 6144 x 6144 pixels at 10 m resolution. The segmentation is produced from multispectral imagery from NASA’s Landsat 7 and Landsat 8 satellites collected since 2011, as well as from ESA’s Sentinel 2A satellite collected since the beginning of its mission in June 2015. Using a thin API layer on top of Festivus makes it straightforward to process imagery in parallel from multiple different sensors with their own unique tilings in a consistent, uniform way.\nThe field segmentation process begins with identifying edges. While clouds and other artifacts in any given image can obscure edges and introduce spurious ones, the edges we care about have the property of being persistent in time. Examining temporal edge statistics allows us to use our deep temporal image stack to identify edges of interest. First, for each image we apply a simple cloud mask [12], and remove cloud pixels from the valid data region. We then compute the spatial gradient magnitude, ensuring that only changes across valid pixels produce nonzero gradients. (For example, this keeps the Landsat 7 scan-line corrector artifacts from producing spurious edges.) The magnitude is accumulated over the bands of each image and over the images available in the chosen time interval, along with a count of how many times each pixel contained valid data. These quantities are divided pixelwise to produce a temporal-mean gradient image, which is then thresholded to produce a binary edge map. Morphological operations are used to clean up the edges, and then the non-edge pixels are separated into connected components. These components are labeled and polygonized, and the resulting polygons stored as a GeoJSON file. For display purposes, assigning a random color to each labeled\nregion allows the individual fields to be visually distinguished."
    }, {
      "heading" : "C. Cloud-free composite images",
      "text" : "We have used the virtual file system described above to create a 15-meter, cloud-free composite image of the world using all pre-September 2016 Landsat 8 data (Figure 5). The input for this computation consists of 68 TB of JPEG 2000- compressed imagery and spans 3.4 years of earth observation. The output is a weighted average of this imagery, with higher weight given to cloud-free, verdant input images.\nThe work was easily parallelized by dividing the earth’s surface into 43k square tiles; each tile was processed independently. The computation was distributed across 400 32-vCPU pre-emptible instances and lasted 8 hours, for a total of 100k CPU-hours and a cost of $1000.\nA similar composite image was announced by the Google Maps team in 2016 https://maps.googleblog.com/2016/06/ keeping-earth-up-to-date-and-looking.html. We estimate that our computation requires at least 75X less CPU-hours per day of Landsat imagery than the Google Maps computation."
    }, {
      "heading" : "VI. CONCLUSION",
      "text" : "While the race to exa-scale explores accelerated architectures with their associated breakage of established software interfaces, the cloud offers an alternative convergent architecture which can take full advantage of economies of scale and continue to protect application investment over multiple generations of hardware. While it took over 40 years for Landsat to collect a petabyte of data, next year Planet’s satellite constellation [5] will produce many petabytes. In this work we have demonstrated that cloud computing architectures are suitable for at least one important class of data-intensive scientific analysis."
    }, {
      "heading" : "VII. ACKNOWLEDGMENTS",
      "text" : "We thank Solomon Boulos, Scott Van Woudenberg and Paul Nash from Google for guidance and assistance, and the Google Earth Engine project for providing open access to Landsat data. We thank Jed Sundwall of Amazon Web services for facilitating access to Copernicus Sentinel-2 data. This publication is supported in part by the Gordon and Betty Moore Foundation’s Data-Driven Discovery Initiative through Grant GBMF4561 to Matthew Turk. We thank NASA and USGS for making the full archives of Landsat and MODIS imagery publicly available, and the EU Copernicus program for making Sentinel data available."
    }, {
      "heading" : "VIII. REFERENCES",
      "text" : "[1] T. E. Anderson, D. E. Culler, and D. Patterson. A case for NOW (networks of workstations). IEEE micro, 15 (1):54–64, 1995. [2] D. H. Bailey et al. The NAS Parallel Benchmarks. International Journal of High Performance Computing Applications, 5(3):63–73, Sept. 1991. URL http://hpc. sagepub.com/content/5/3/63. [3] D. J. Becker, T. Sterling, D. Savarese, J. E. Dorband, U. A. Ranawak, and C. V. Packer. BEOWULF: A parallel workstation for scientific computation. In Proceedings, International Conference on Parallel Processing, volume 95, 1995. URL http://www.phy.duke.edu/∼rgb/ brahma/Resources/beowulf/papers/ICPP95/icpp95.html. [4] G. Bell and J. Gray. What’s Next in High-performance Computing? Commun. ACM, 45(2):91–95, Feb. 2002. URL http://doi.acm.org/10.1145/503124.503129. [5] C. Boshuizen, J. Mason, P. Klupar, and S. Spanhake. Results from the Planet Labs Flock Constellation. AIAA/USU Conference on Small Satellites, Aug. 2014. URL http://digitalcommons.usu.edu/smallsat/2014/ PrivEnd/1. [6] J. L. Carlson. Redis in Action. Manning Publications Co., Greenwich, CT, USA, 2013. ISBN 1617290858, 9781617290855. [7] Celery Development Team. Celery - the distributed task queue, 2015. URL http://www.celeryproject.org. [8] ISO/IEC JTC 1/SC 29/WG 1. JPEG 2000 image coding system ISO/IEC 15444-1:2004 | ITU-T Rec. T.800. International Standard, 2004. URL http://www.iso.org/ iso/catalogue detail.htm?csnumber=37674. [9] J. D. McCalpin. Memory bandwidth and machine balance in current high performance computers. 1995. URL http://www.researchgate.net/publication/ 213876927 Memory Bandwidth and Machine Balance in Current High Performance Computers. [10] G. E. Moore. Cramming more components onto integrated circuits. Electronics, 38(8):114–117, Apr. 1965. [11] A. Nuvolari. Open source software development: Some historical perspectives. First Monday, 10(10), 2005. URL http://firstmonday.org/ojs/index.php/fm/ article/view/1284.\n[12] L. Oreopoulos, M. J. Wilson, and T. Várnai. Implementation on Landsat data of a simple cloud-mask algorithm developed for MODIS land bands. IEEE Geoscience and Remote Sensing Letters, 8(4):597–601, July 2011. [13] S. Sanfilippo and P. Noordhuis. Redis, 2010. URL http: //redis.io. [14] S. W. Skillman, M. S. Warren, M. J. Turk, R. H. Wechsler, D. E. Holz, and P. M. Sutter. Dark Sky Simulations: Early Data Release. arXiv:1407.2600 [astro-ph], July 2014. URL http://arxiv.org/abs/1407.2600. arXiv: 1407.2600. [15] D. S. Taubman and M. W. Marcellin. JPEG2000: Image Compression Fundamentals, Standards and Practice. Kluwer Academic Publishers, Norwell, MA, USA, 2001. ISBN 079237519X. [16] R. R. Vatsavai, S. Shekhar, T. E. Burk, and S. Lime. UMN-MapServer: A high-performance, interoperable, and open source web mapping and geo-spatial analysis system. In International Conference on Geographic Information Science, pages 400–417. Springer, 2006. [17] M. S. Warren and J. Wofford. Astronomical data analysis with commodity components. In Proceedings of the ACM/IEEE Conference on Supercomputing, 2007. Winner of the SC ’07 Storage Challenge Award. [18] M. S. Warren, D. J. Becker, M. P. Goda, J. K. Salmon, and T. Sterling. Parallel supercomputing with commodity components. In Proceedings of the International Conference on Parallel and Distributed Processing Techniques and Applications (PDPTA’97), page 1372–1381, 1997. URL http://citeseerx.ist.psu.edu/viewdoc/download?doi= 10.1.1.51.2019&rep=rep1&type=pdf. [19] M. S. Warren, J. K. Salmon, D. J. Becker, M. P. Goda, T. Sterling, and W. Winckelmans. Pentium Pro Inside: I. A Treecode at 430 Gigaflops on ASCI Red, II. Price/Performance of $50/Mflop on Loki and Hyglac. In Supercomputing, ACM/IEEE 1997 Conference, pages 61–61, Nov. 1997. [20] M. S. Warren, T. C. Germann, P. S. Lomdahl, D. M. Beazley, and J. K. Salmon. Avalon: an Alpha/Linux cluster achieves 10 gflops for $150k. In Proceedings of the 1998 ACM/IEEE conference on Supercomputing (CDROM), Supercomputing ’98, page 1–11, Washington, DC, USA, 1998. IEEE Computer Society. ISBN 0-89791-984-X. URL http://dl.acm.org/citation.cfm?id=509058.509130. [21] M. S. Warren, C. L. Fryer, and M. P. Goda. The Space Simulator: Modeling the Universe from Supernovae to Cosmology. In Proceedings of the 2003 ACM/IEEE Conference on Supercomputing, SC ’03, pages 30–, New York, NY, USA, 2003. ACM. ISBN 1-58113-695-1. URL http://doi.acm.org/10.1145/1048935.1050181. [22] M. S. Warren et al. Seeing the earth in the cloud: Processing one petabyte of satellite imagery in one day. In 2015 IEEE Applied Imagery Pattern Recognition Workshop (AIPR), pages 1–12. IEEE, 2015. [23] T. White. Hadoop: The definitive guide. ” O’Reilly Media, Inc.”, 2012."
    } ],
    "references" : [ {
      "title" : "A case for NOW (networks of workstations)",
      "author" : [ "T.E. Anderson", "D.E. Culler", "D. Patterson" ],
      "venue" : "IEEE micro,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1995
    }, {
      "title" : "The NAS Parallel Benchmarks",
      "author" : [ "D.H. Bailey" ],
      "venue" : "International Journal of High Performance Computing Applications,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1991
    }, {
      "title" : "BEOWULF: A parallel workstation for scientific computation",
      "author" : [ "D.J. Becker", "T. Sterling", "D. Savarese", "J.E. Dorband", "U.A. Ranawak", "C.V. Packer" ],
      "venue" : "In Proceedings, International Conference on Parallel Processing,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1995
    }, {
      "title" : "What’s Next in High-performance Computing",
      "author" : [ "G. Bell", "J. Gray" ],
      "venue" : "Commun. ACM,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2002
    }, {
      "title" : "Results from the Planet Labs Flock Constellation",
      "author" : [ "C. Boshuizen", "J. Mason", "P. Klupar", "S. Spanhake" ],
      "venue" : "AIAA/USU Conference on Small Satellites,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2014
    }, {
      "title" : "Redis in Action",
      "author" : [ "J.L. Carlson" ],
      "venue" : "Manning Publications Co., Greenwich, CT, USA,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2013
    }, {
      "title" : "Memory bandwidth and machine balance in current high performance computers",
      "author" : [ "J.D. McCalpin" ],
      "venue" : null,
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1995
    }, {
      "title" : "Cramming more components onto integrated circuits",
      "author" : [ "G.E. Moore" ],
      "venue" : null,
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1965
    }, {
      "title" : "Open source software development: Some historical perspectives",
      "author" : [ "A. Nuvolari" ],
      "venue" : "First Monday,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2005
    }, {
      "title" : "Implementation on Landsat data of a simple cloud-mask algorithm developed for MODIS land bands",
      "author" : [ "L. Oreopoulos", "M.J. Wilson", "T. Várnai" ],
      "venue" : "IEEE Geoscience and Remote Sensing Letters,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2011
    }, {
      "title" : "Dark Sky Simulations: Early Data Release",
      "author" : [ "S.W. Skillman", "M.S. Warren", "M.J. Turk", "R.H. Wechsler", "D.E. Holz", "P.M. Sutter" ],
      "venue" : "[astro-ph],",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2014
    }, {
      "title" : "JPEG2000: Image Compression Fundamentals, Standards and Practice",
      "author" : [ "D.S. Taubman", "M.W. Marcellin" ],
      "venue" : null,
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2001
    }, {
      "title" : "UMN-MapServer: A high-performance, interoperable, and open source web mapping and geo-spatial analysis system",
      "author" : [ "R.R. Vatsavai", "S. Shekhar", "T.E. Burk", "S. Lime" ],
      "venue" : "In International Conference on Geographic Information Science,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2006
    }, {
      "title" : "Astronomical data analysis with commodity components",
      "author" : [ "M.S. Warren", "J. Wofford" ],
      "venue" : "In Proceedings of the ACM/IEEE Conference on Supercomputing,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2007
    }, {
      "title" : "Parallel supercomputing with commodity components",
      "author" : [ "M.S. Warren", "D.J. Becker", "M.P. Goda", "J.K. Salmon", "T. Sterling" ],
      "venue" : "In Proceedings of the International Conference on Parallel and Distributed Processing Techniques and Applications (PDPTA’97),",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 1997
    }, {
      "title" : "Winckelmans. Pentium Pro Inside: I. A Treecode at 430 Gigaflops on ASCI Red, II. Price/Performance of $50/Mflop on Loki and Hyglac",
      "author" : [ "M.S. Warren", "J.K. Salmon", "D.J. Becker", "M.P. Goda", "T. Sterling" ],
      "venue" : "In Supercomputing,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 1997
    }, {
      "title" : "Avalon: an Alpha/Linux cluster achieves 10 gflops for $150k",
      "author" : [ "M.S. Warren", "T.C. Germann", "P.S. Lomdahl", "D.M. Beazley", "J.K. Salmon" ],
      "venue" : "In Proceedings of the 1998 ACM/IEEE conference on Supercomputing (CDROM), Supercomputing",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 1998
    }, {
      "title" : "The Space Simulator: Modeling the Universe from Supernovae to Cosmology",
      "author" : [ "M.S. Warren", "C.L. Fryer", "M.P. Goda" ],
      "venue" : "In Proceedings of the 2003 ACM/IEEE Conference on Supercomputing, SC",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2003
    }, {
      "title" : "Seeing the earth in the cloud: Processing one petabyte of satellite imagery in one day",
      "author" : [ "M.S. Warren" ],
      "venue" : "In 2015 IEEE Applied Imagery Pattern Recognition Workshop (AIPR),",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2015
    }, {
      "title" : "Hadoop: The definitive guide. ",
      "author" : [ "T. White" ],
      "venue" : "O’Reilly Media, Inc.”,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 8,
      "context" : "Over a 30 year period, engineers improved the efficiency of the Cornish high-pressure steam engine by a factor of five [11] (a compounded improvement of 5.",
      "startOffset" : 119,
      "endOffset" : 123
    }, {
      "referenceID" : 7,
      "context" : "Against the present background of Moore’s Law [10] (loosely representing a doubling in performance every 18 months, or 60% improvement per year) the technological rate of change in computing is 10 times higher than the peak of the industrial revolution.",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 14,
      "context" : "Twenty years ago, we were the first to demonstrate a commodity computing cluster (Figure 1) that was competitive with special-purpose HPC architectures [18, 19] and",
      "startOffset" : 152,
      "endOffset" : 160
    }, {
      "referenceID" : 15,
      "context" : "Twenty years ago, we were the first to demonstrate a commodity computing cluster (Figure 1) that was competitive with special-purpose HPC architectures [18, 19] and",
      "startOffset" : 152,
      "endOffset" : 160
    }, {
      "referenceID" : 3,
      "context" : "by some accounts [4], led to the demise of the traditional supercomputer.",
      "startOffset" : 17,
      "endOffset" : 20
    }, {
      "referenceID" : 15,
      "context" : "Loki and Hyglac [19] were clusters of Intel processors with Ethernet as a communication fabric, using the Linux operating system, constructed in 1996.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 0,
      "context" : "While other groups were investigating the potential of commodity clusters in the same era [1], our commercial off-the-shelf (COTS) approach distinguished itself through winning the Gordon Bell price/performance prize in 1997.",
      "startOffset" : 90,
      "endOffset" : 93
    }, {
      "referenceID" : 4,
      "context" : "org/statistics/details/osfam/1, not to mention its prevalence in other things, from electric cars to nano-satellites [5].",
      "startOffset" : 117,
      "endOffset" : 120
    }, {
      "referenceID" : 2,
      "context" : "Following up the initial demonstration of the BEOWULF project [3], a clear outcome of this work was the democratization of access to HPC resources.",
      "startOffset" : 62,
      "endOffset" : 65
    }, {
      "referenceID" : 13,
      "context" : "Large dataanalysis problems were also amenable to this approach [17].",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 17,
      "context" : "top 100 supercomputers with mail-order parts and part-time system management in 2002 [21], today it requires a machine with over 30,000 cores.",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 16,
      "context" : "promise of [20] “The software community that develops around these machines (if it remains open and follows the Linux development philosophy) will allow users, vendors, and ar X iv :1 70 2.",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 15,
      "context" : "This cluster won the Gordon Bell price/performance prize the next year [19].",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 10,
      "context" : "Our past work in computational cosmology is one exemplar [14].",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 19,
      "context" : "For much the same reason, we do not consider the Hadoop [23] ecosystem to be the right model for our data-intensive computing tasks, although this is mostly the fault of HDFS.",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 5,
      "context" : "We currently use Redis [6], although similar functionality could be provided by other systems.",
      "startOffset" : 23,
      "endOffset" : 26
    }, {
      "referenceID" : 12,
      "context" : "One such application is Mapserver, an open source software package for publishing spatial data and interactive mapping over http [16].",
      "startOffset" : 129,
      "endOffset" : 133
    }, {
      "referenceID" : 11,
      "context" : "We store our pre-processed imagery using the JPEG 2000 standard [8, 15] due to its significant advantages in terms of compression and image types as well as its support for internal tiling and a scalable multi-resolution codestream that can be",
      "startOffset" : 64,
      "endOffset" : 71
    }, {
      "referenceID" : 14,
      "context" : "Compared to similar measurements from 20 years ago [18] message latency has improved a factor of 5 and bandwidth per HW core by about a factor of 20.",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 17,
      "context" : "TABLE II A COMPARISON OF 2003 ERA BEOWULF CLUSTER GORDON BELL PRICE/PERFORMANCE PRIZE FINALIST [21] TO A CRAY XE6 NODE AND 2015 CLOUD PERFORMANCE.",
      "startOffset" : 95,
      "endOffset" : 99
    }, {
      "referenceID" : 6,
      "context" : "THE TOP SET OF RESULTS IS MEMORY BANDWIDTH MEASURED IN MBYTES/S BY THE STREAM BENCHMARK [9], MIDDLE IS FROM THE CLASS C NAS PARALLEL BENCHMARKS [2] IN MOP/S, AND LAST LINE IS LINPACK GFLOP/S.",
      "startOffset" : 88,
      "endOffset" : 91
    }, {
      "referenceID" : 1,
      "context" : "THE TOP SET OF RESULTS IS MEMORY BANDWIDTH MEASURED IN MBYTES/S BY THE STREAM BENCHMARK [9], MIDDLE IS FROM THE CLASS C NAS PARALLEL BENCHMARKS [2] IN MOP/S, AND LAST LINE IS LINPACK GFLOP/S.",
      "startOffset" : 144,
      "endOffset" : 147
    }, {
      "referenceID" : 18,
      "context" : "This calculation is described in detail in [22].",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 9,
      "context" : "First, for each image we apply a simple cloud mask [12], and remove cloud pixels from the valid data region.",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 4,
      "context" : "While it took over 40 years for Landsat to collect a petabyte of data, next year Planet’s satellite constellation [5] will produce many petabytes.",
      "startOffset" : 114,
      "endOffset" : 117
    } ],
    "year" : 2017,
    "abstractText" : "We present our experiences using cloud computing to support data-intensive analytics on satellite imagery for commercial applications. Drawing from our background in highperformance computing, we draw parallels between the early days of clustered computing systems and the current state of cloud computing and its potential to disrupt the HPC market. Using our own virtual file system layer on top of cloud remote object storage, we demonstrate aggregate read bandwidth of 230 gigabytes per second using 512 Google Compute Engine (GCE) nodes accessing a USA multi-region standard storage bucket. This figure is comparable to the best HPC storage systems in existence. We also present several of our application results, including the identification of field boundaries in Ukraine, and the generation of a global cloud-free base layer from Landsat imagery.",
    "creator" : "LaTeX with hyperref package"
  }
}