{
  "name" : "1302.4958.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Bayesian Approach to Learning Causal Networks",
    "authors" : [ "David Heckerman" ],
    "emails" : [ "heckerma@microsoft" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Whereas acausal Bayesian networks rep resent probabilistic independence, causal Bayesian networks represent causal relation ships. In this paper, we examine Bayesian methods for learning both types of networks. Bayesian methods for learning acausal net works are fairly well developed. These meth ods often employ assumptions to facilitate the construction of priors, including the as sumptions of parameter independence, pa rameter modularity, and likelihood equiva lence. We show that although these assump tions also can be appropriate for learning causal networks, we need additional assump tions in order to learn causal networks. We introduce two sufficient assumptions, called mechanism independence and component in dependence. We show that these new as sumptions, when combined with parame ter independence, parameter modularity, and likelihood equivalence, allow us to apply methods for learning acausal networks to learn causal networks.\n1 Introduction\nThere has been a great deal of recent interest in Bayesian methods for learning Bayesian networks from data (Spiegelhalter and Lauritzen 1990; Cooper and Herskovits, 1991, 1992; Buntine, 1991, 1994; Spiegel halter et al. 1993; Madigan and Raftery, 1994, Heck erman et al. 1994, 1995). These methods take prior knowledge of a domain and statistical data, and con struct one or more Bayesian-network models of the do main. Most of this work has concentrated on Bayesian networks interpreted as a representation of probabilis tic conditional independence. Nonetheless, several re searchers have proposed a causal interpretation for Bayesian networks (Pearl and Verma 1991; Spirtes et\nal. 1993; Heckerman and Shachter 1994). These re searchers show that having a causal interpretation can be important, because it allows us to predict the affects of interventions in a domain-something that cannot be done without a causal interpretation.\nIn this paper, we extend Bayesian methods for learn ing acausal Bayesian networks to causal Bayesian net works. We offer two contributions. One, we show that acausal and causal Bayesian networks (or acausal and causal networks, for short) are significantly different in their semantics, and that it is inappropriate to blindly apply methods for learning acausal networks to causal networks. Two, despite these differences, we identify circumstances in which methods for learning acausal networks are applicable to learning causal networks.\nIn Section 2, we describe a causal interpretation of Bayesian networks developed by Heckerman and Shachter [1994, 1995] that is consistent with Pearl's causal-theory interpretation (e.g., Pearl and Verma [1991] and Pearl [1995a]). We show that any causal network can be represented as a special type of influ ence diagram. In Section 3, we review Bayesian meth ods for learning acausal networks, showing how var ious assumptions and properties-namely, parameter independence, parameter modularity, and hypothesis equivalence-facilitate the learning task. In Section 4, we show how these methods for learning acausal net works can be adapted to learn ordinary influence di agrams. In Section 5, we identify problems with this approach when learning influence diagrams that cor respond to causal networks. We identify two assump tions, called mechanism independence and component independence that circumvent these problems. In Sec tion 6, we argue that the assumption of parameter modularity is reasonable for learning causal networks, and that the property of hypothesis equivalence should be replaced with a weaker assumption called likelihood equivalence. We show that, given the assumptions of parameter independence, parameter modularity, likeli hood equivalence, mechanism independence, and com ponent independence, we can use methods for learning acausal networks to learn causal networks.\n286 Heckerman\nWe assume that the reader is familiar the concept of random sample, the distinction between subjective and objective probability (which we call probability and physical probability, respectively), and the distinction between chance and decision variables. (We sometimes refer to a decision variable simply as a \"decision.\" ) We consider the problem of modeling relationships in a domain consisting of chance variables U and decision variables D. We use lower-case letters to represent sin gle variables and upper-case letters to represent sets of variables. We write x = k to denote that variable x is in state k. When we observe the state for every vari able in set X, we call this set of observations a state of X, and write X = k. Sometimes, we leave the state of a variable or a set of variables implicit. We use p(X = JIY = k,e) to denote the (subjective) proba bility that X = j given Y = k for a person whose state of information is e; whereas, we use pp(X = JIY = k) to denote the physical probability of this conditional event.\nAn influence diagram for the domain U U D is a model for that domain having a structural component and a probabilistic component. The structure of an influ ence diagram is a directed acyclic graph containing (square) decision and (oval) chance nodes correspond ing to decision and chance variables, respectively, as well as information and relevance arcs. Information arcs, which point to decision nodes, represent what is known at the time decisions are made. Relevance arcs, which point to chance nodes, represent (by their ab sence) assertions of conditional independence. Associ ated with each chance node x in an influence diagram are the probability distributions p(xiPa(x),e), where P a ( x) are the parents of x in the diagram. These dis tributions in combination with the assertions of condi tional independence determine the joint distributions p(UID, e). A special kind of chance node is the deter ministic node (depicted as a double oval). A node x is a deterministic node if its corresponding variable is a deterministic function of its parents. Also, an influ ence diagram may contain a single distinguished node, called a utility node that encodes the decision maker's utility for each state of the node's parents. A utility node is a deterministic function of its predecessors and can have no children. Finally, for an influence diagram to be well formed, its decisions must be totally ordered by the influence-diagram structure. (For more details, see Howard [1981].)\nAn acausal Bayesian network is an influence diagram that contains no decision nodes (and, therefore, no in formation arcs). That is, an acausal Bayesian network represents only assertions of conditional independence. (For more details, see Pearl [1988].)\n(a)\nFigure 1: (a) A causal network. (b) A corresponding influence diagram. Double ovals denote deterministic nodes.\n2 Causal Networks\nIn this section, we describe causal Bayesian networks and how we can represent them as influence diagrams. The influence-diagram representation that we describe is identical to Pearl's causal theory, with one exception to be discussed. Rather than present the representa tion directly, we follow the approach of Heckerman and Shachter (1994 and this proceedings) who define cause and effect, and then develop from this definition the influence-diagram representation of causal networks.\nRoughly speaking, a causal network for a domain of chance variables U is a directed acyclic graph where nodes correspond to the chance variables in U and each nonroot node is the direct causal effect of its parents (Pearl and Verma, 1991). An example of a causal net work is shown in Figure 1a. The diagram indicates that whether or not a car starts is caused by the con dition of its battery and fuel supply, that whether or not a car moves is caused by whether or not it starts, and that (in this model) the condition of the battery and the fuel supply have no causes. In this example, we assume that all variables are binary.\nBefore we develop the influence-diagram representa tion of a causal network, we need to introduce the con cepts of unresponsiveness, set decision, mapping vari able, cause, causal mechanism, and canonical form. To understand the notion of unresponsiveness, con sider the simple decision d of whether or not to bet heads or tails on the outcome of a coin flip c. Let the variable w represent whether or not we win. Thus, w is a deterministic function of d and c: we win if and only if the outcome of the coin matches our bet. Let us assume that the coin is fair (i.e., p(headsle) = 1/2), and that the person who flips the coin does not know how we bet.\nIn this example, we are uncertain whether or not the coin will come up heads, but we are certain that what ever the outcome, it will be the same even if we choose to bet differently. We say that c is unresponsive to d. We cannot make the same claim about the relationship\nA Bayesian Approach to Learning Causal Networks 287\nbetween d and w. Namely, we know that w depends on d in the sense that if we bet differently then w will be different. For example, we know that if we will win by betting heads then we will loose by betting tails. We say that w is responsive to d.\nIn general, to determine whether or not chance vari able x is unresponsive to decision d, we have to an swer the query \"Will the outcome of x be the same no matter how we choose d?\" Queries of this form are a simple type of counterfactual query, discussed in the philosophical literature (e.g., Lewis [1979]). It is inter esting that, in many cases, it is easy to answer such a query, even though we are uncertain about the out come of x. Note that when x is unresponsive to d, x and d must be probabilistically independent; whereas, the converse does not hold.\nTo understand the concept of a set decision, consider the chance variable battery? in our automobile exam ple. Let us assume that it has only two states: \"good\" and \"bad.\" Although battery? is a chance variable, we can imagine taking an action that will force the vari able into one of its possible states. If this action has no side effects on the other variables in the model other than those required by the causal interactions in the domain, we say that we are setting the variable. For example, we can force the battery to fail by blowing up the car. This action, however, will also force the variable fuel? to become empty, and therefore does not qualify as a setting of battery?. In contrast, if we force the battery to fail by emptying the battery fluid on the ground, the only side effects will be those that follow from the causal interactions in the domain. Consequently, this action qualifies as a setting of bat tery?. We can extend the idea of setting a variable to a decision variable. Namely, we set a decision variable simply by choosing one of its alternatives.\nA set decision for chance variable x, denoted x, is a de cision variable whose alternatives are \"set x to k\" for each state k of x and \"do nothing.\" In our example, the set decision corresponding battery? has three al ternatives: \"set the battery to be good,\" \"set the bat tery to be bad,\" and \"do nothing.\" Pearl and Verma ( 1991) introduce the concepts of setting a variable and set decision as primitives. Heckerman and Shachter (in this proceedings) formalize these concepts in terms of unresponsiveness.\nTo understand the concept of a mapping variable, sup pose we have a collection of variables Y (which may in clude both chance and decision variables) and a chance variable x. We can imagine setting Y to each of its states and observing x-that is, observing how Y maps to x. A mapping variable x(Y) is a chance variable whose states correspond to all the possible mappings from Y to x. For example, consider the variables s (start?) and m (move?) in our automobile example. The states of the mapping variable m( s) are shown in\nTable 1: The four states of the mapping variable m( s).\nTable 1. The first state represents ·the normal situa tion. That is, if we make the car start (in the sense of a set action), then it would move; and if we prevent the car from starting, then it would not move. The second state represents the situation where, regardless of whether or not we make the car start, the car not will move. This state would occur, for example, if a parking attendant placed a restraint on one of the car's tires. Note that, by definition, x will always be a de terministic function of the mapping variable x(Y) and the variables Y. For example, if m ( s) =\"state 4\" and s =\"yes,\" then m =\"no\" .\nWe can observe the mapping variable m( s) directly. Namely, we can see if the car moves before and after we start the car. In general, however, mapping vari ables cannot be fully observed. For example, consider the decision x of whether to continue or quit smok ing and the chance variable y representing whether or not we get lung cancer before we reach sixty years of age. In this case, we cannot fully observe the mapping variable y(x), because we cannot observe whether or not we get lung cancer given both possible choices. In general, a mapping variable represents a counter factual set of possible outcomes, only one of which we can actually observe. Rubin (1978) and Howard (1990) define concepts similar to the mapping variable.\nGiven these concepts, Heckerman and Shachter (1994 and this proceedings) say that a set of variables C are causes for x with respect to decisions D if (1) x � C and ( 2) C is a minimal set of variables such that x (C) is unresponsive to D. Roughly speaking, C is a cause for x with respect to D if the way C affects x is not affected by D. This explication of cause is unusual in that it is conditioned on a set of decisions. Heckerman and Shachter discuss the advantages of this approach. When C are causes of x with respect to D, we call the mapping variable x( C) a causal mechanism or simply a mechanism.\nGiven chance variables U and decisions D, Heckerman and Shachter show that we can construct an influence diagram that represents causes for each caused variable in U as follows. First, we add a node to the diagram corresponding to each variable in U U D. Next, we or der the variables x1, . . . , Xn in U so that the variables unresponsive to D come first. Then, for each variable Xi in U in order, if x; is responsive to D we (1) add a causal-mechanism node xi(Ci) to the diagram, where C; � D U {x1, . . . , x;_1}, and (2) make Xi a determin istic function of Ci U Xi ( Ci). Finally, we assess depen dencies among the variables that are unresponsive D.\n288 Heckerman\nThey show that the resulting influence diagram has the following two properties: (1) all chance nodes that are responsive to D are descendants of decision nodes and (2) all nodes that are descendants of decision nodes are deterministic nodes. Influence diagrams that sat isfy these conditions are said to be in canonical form. We note that information arcs and a utility node may be added to canonical form influence diagrams, but these constructs are not needed for the representation of cause and are not used in this discussion.\nWe can use an influence diagram in canonical form to represent the causal relationships depicted in a causal network. Suppose we have a set of chance variables U, a corresponding collection of set decisions (; for U, and a causal network for U. Let Pa(x) be the parents of x in the causal network. Then, we can interpret the causal network to mean that, for all x, Pa(x) U {x} is a set of causes for x with respect 0. Now, if we construct an influence diagram in canonical form as we have described, using an ordering consistent with the causal network, then we obtain an influence diagram where each variable x is a deterministic function of the set decision x, Pa(x), and the causal mechanism x(Pa(x), x). By the definition of a set decision, we can simplify the deterministic relationship by replacing the causal mechanism x(Pa(x), x) with x(Pa(x)), which denotes the mappings from Pa( x) to x when x is set to \"do nothing.\" For example, in our automobile domain, if m(s) =state 4, s =\"do nothing,\" and s =yes, then m =no.\nThe transformation from causal network to canonical form influence diagram for our automobile domain is illustrated in Figure 1. We call the variables in the original causal network domain variables. Each do main variable appears in the influence diagram, and is a function of its set decision x, its parents in the causal network Pa(x), and the mapping variable x(Pa(x)). {Note that x(0) = x when x =\"do nothing\".) The mechanisms and set decisions are independent, be cause, as is required by canonical form, the mecha nisms are unresponsive to the set decisions. Although not required by the canonical-form representation, the mechanisms are mutually independent in this example.\nIn general, this influence-diagram representation of a causal network is identical to Pearl's causal theory, with the exception that Pearl requires the mechanisms {which he calls disturbances) to be independent. One desirable consequence of this restriction is that the variables in the causal network will exhibit the con ditional independencies that we would obtain by in terpreting the causal network as an acausal network (Spirtes et al., 1993; Pearl, 1995a). For example, the independence of causal mechanisms in our example yield the following conditional independencies:\np(flb, �) = p(fl�) p( mlb, f, s, �) = p( mls, �)\nWe obtain these same independencies when we inter pret the causal network in Figure 1a as an acausal network. Nonetheless, as we shall illustrate, depen dent mechanisms cannot be excluded in general.\n3 Learning A causal Networks\nGiven the correspondence in the previous section, we see that learning causal networks is a special case of learning influence diagrams in canonical form. In this section, we review methods for learning acausal Bayesian networks, such as those described by Spiegel halter and Lauritzen (1990), Cooper and Herskovits (1991, 1992), Buntine (1991, 1994), Spiegelhalter et al., {1993), Madigan and Raftery {1994), and Hecker man et al. {1994, 1995). In the following sections, we show how these methods can be extended to learn ar bitrary influence diagrams and influence diagrams in canonical form.\nSuppose we have a domain consisting of chance vari ables U = { x1, . . . , xn } · Also, suppose we have a database of cases C = { C 1, . . . , Cm} where each case Ct contains observations of one or more variables in U. The basic assumption underlying the Bayesian ap proach is that the database C is a random sample from U with joint physical probability distribution pp(U). As is done traditionally, we can characterize this phys ical probability distribution by a finite set of parame ters Gu. For example, if U contains only continuous variables, pp(U) may be a multivariate-Gaussian dis tribution with parameters specifying the distribution's means and covariances. In this paper, we limit our dis cussion to domains containing only discrete variables. Therefore, the parameters Gu correspond exactly to the physical probabilities in the distribution pp(U). (We shall use the e and pp notation interchangeably.)\nIn the general Bayesian approach to learning about these uncertain parameters, we assess prior distribu tions for them, and then compute their posterior distri butions given the database. In the paradigm of learn ing acausal Bayesian networks, we add one twist to this general approach: we assume that the physical proba bility distribution pp( U) is constrained such that it can be encoded in some acausal-network structure whose identity is possibly uncertain.\nTo start with a special case, let us suppose that pp(U) can be encoded in some known acausal-network struc ture B., and that we are uncertain only about the values of the probabilities associated with this net work structure. We say that the database is a ran dom sample from B.. Given this situation, it turns out the database C can be separated into a set of ran dom samples, where these random samples are deter mined by the structure of Bs. For example, consider the domain consisting of two variables x, where each variables has possible states 0 and 1. Then, the asser-\nA Bayesian Approach to Learning Causal Networks 289\n(a) (b)\nFigure 2: (a) Conditional independencies associated with the assertion that the database is a random sam ple from the structure x -+ y, where x and y are binary. (b) The additional assumption of parameter indepen dence.\ntion that the database is a random sample from the structure x -+ y is equivalent to the assertion that the database can be separated into at most three random samples: (1) the observations of x are a binomial sam ple with parameter 8.,=1, (2) the observations of y in those cases (if any) where x = 0 are a binomial sample with parameter By=1ix=o, and (3) the observations of y in those cases (if any) where x = 1 are a binomial sample with parameter By=11x=1. Figure 2a contains an acausal network that illustrates some of the condi tional independencies among the database cases and network parameters for this assertion.\nGiven this decomposition into random samples, we can update each parameter independently under two con ditions: (1) the parameters are independent, an as sumption we call parameter independence, and (2) the database is complete (i.e., every variable is observed in every case). The assumption of parameter indepen dence is illustrated in Figure 2b.\nLet us examine this updating for an arbitrary acausal network structure B, for domain U. We discuss the situation where data may be missing later in this sec tion. Let r; be the number of states of variable x;; and let q; = Ilx1ePa(x;) r1 be the number of states of Pa(x;). Let B;jk denote the parameter corresponding to the physical probability p(x; = kiPa(x;) = j, �) (Bijk > 0; 2:��1 B;jk = 1). In addition, we define e = un uq• 6·· B• - i=1 j=1 •J That is, the parameters 6n, correspond to the physi cal probabilities of the acausal-network structure B,. To illustrate the updating approach, suppose that each variable set 6;j has a Dirichlet distribution:\nr · h IT . N:.k-1 p(6;j lB.,�) = c · B,ji;\nk=1 (1)\nwhere B: is the assertion (or \"hypothesis\" ) that the database is a random sample from the network\nstructure B., and c is some normalization constant. Then, given parameter independence and a complete database, if N;jk is the number of cases in database C in which x; = k and Pa(x;) = j, we obtain\n(e. ·IC Bh C) - . IT BN!jk +Nijk - 1\np •J ' s ' .. c ijk k\n(2)\nwhere c is some other normalization constant. Fur thermore, taking the expectation of B;jk with respect to the distribution for e,j for every i and j, we obtain the probability that each x; = k and Pa(x;) = j in Cm+1 (the next case Cm+1 to be seen after seeing the database):\nn q; N' +N h IT IT ijk ijk p(Cm+11C,B.,�) = N!.+N;· i=1 j=1 •J J\n1 \"\\'ri 1 \"'r; where N;j = l..Jk=1 Nijk and N;j = l..Jk=1 N;jk·\n(3)\nNow, suppose we are not only uncertain about the probabilities, but also uncertain about the structure that encodes them. We express this uncertainty by assigning a prior probability p( B: 1�) to each possible hypothesis B:' and update these probabilities as we see cases. In so doing, we learn about the structure of the domain. From Bayes' theorem, we have\nwhere c is a normalization constant. Also, from the product rule, we have\nm p(CIB�' �) = IIp(CtiC1, ... 'Ct-1' B�' �) (5)\n1=1\nWe can evaluate each term on the right-hand-side of this equation using Equation 3, under the assumption that the database C is complete. For the posterior probability of B: given c, we obtain\n(6)\nUsing these posterior probabilities and Equation 3, we can compute the probability distribution for the next case to be observed after we have seen a database. From the expansion rule, we obtain\np(Cm+IIC,�) = �p(Cm+IIC,B�,�) p(B�IC,�) (7) BZ\nWhen the database contains m1ssmg data, we can compute p(B� IC, �) exactly, by summing the result of Equation 5 over all possible completions of the database (see Section 7). Unfortunately, this approach\n290 Heckerman\nis intractable when many observations are mtssmg. Consequently, we often use approximate methods such as filling in missing data based on the data that is present (Titterington, 1976; Cowell et al., 1995), the EM algorithm (Dempster et al., 1977), and Gibbs sam pling (York, 1992; Madigan and Raftery, 1994).\nWhen we believe that only a few network structures are possible, the approach we have discussed is es sentially all there is to learning network structure. Namely, we directly assess the priors for the possible network structures and their parameters, and subse quently use Equations 3 and 7 or their generalizations for continuous variables and missing data. Nonethe less, the number of network structures for a domain containing n variables is more than exponential in n. Consequently, when we cannot exclude almost all of these network structures, we need efficient methods for assigning priors to structures and parameters (e.g., Buntine [1991], Spiegelhalter et al. [1993], and Beck erman et al. [1995]), as well as search methods for identifying structures that contribute significantly to the sum in Equation 3 (e.g., Cooper and Herskovits [1992] and Heckerman et al. [1995]).\nHere, we review an efficient method described by Beck erman et al. [1995] for assigning priors to the param eters of all possible network structures. In their ap proach, a user assesses a prior network: an acausal Bayesian network for the first case to be seen in database, under the assumption that there are no con straints on the parameters. More formally, this prior network represents the joint probability distribution p(CtlB:c, �), where Bsc is any network . structure con taining no missing arcs. Then, the user assesses an equivalent sample size N' for this prior network. (N' is a measure of the user's confidence in his assessment of the prior network.) Then, for any given network structure B., where x; has parents Pa(x;), we com pute the Dirichlet exponents in Equation 1 using the relation\nN[jk = N' · p( x; = k, Pa( x;) = jJB:c,�) (8)\nwhere the probability is computed from the prior net work.\nHeckerman et al. [1995] derive this approach from the assumption of parameter independence, an additional assumption called parameter modularity, and a prop erty called hypothesis equivalence. The property of hypothesis equivalence stems from the fact that two acausal-network structures can be equivalent-that is, represent exactly the same sets of probability distribu tions (Verma and Pearl, 1990). For example, for the three variable domain { x, y, z}, each of the network structures x -t y -t z, x +-- y -t z, and x +-- y +-- z represents the distributions where x and z are condi tionally independent of y, and are therefore equivalent. Given the definition of the hypothesis B:, it follows\nthat the hypotheses corresponding to two equivalent structures must be the same, which is the property of hypothesis equivalence.\nThe assumption of parameter modularity says that, given two network structures B,1 and B,2, if x; has the same parents in B.t and B,2, then\nfor j = 1, . . . , q;. Heckerman et al. [1995] call this property parameter modularity, because it says that the distributions for parameters e;j depend only on the structure of the network that is local to variable x;-namely, eij only depends on Xj and its parents. In Section 6, we examine the appropriateness of hypothe sis equivalence and parameter modularity for learning causal networks.\n4 Learning Influence Diagrams\nBefore we consider the problem of learning influence diagrams that correspond to causal networks, let us examine the task of learning arbitrary influence dia grams.\nThis task is straightforward once we make the follow ing observations. One, by the definitions of informa tion arc and utility node, information arcs and the predecessors of a utility node are known with certainty by the decision maker and, therefore, are not learned.1 Thus, we need only learn the relevance-arc structure and the physical probabilities associated with chance nodes. Two, by definition of a decision, the states of all decision variables are known by the decision maker in every case. Thus, assuming these decisions are recorded, we have complete data for D in every case of the database.\nGiven these observations, it follows that the problem of learning influence diagrams for the domain UUD re duces to the problem of learning acausal Bayesian net works for U U D, where we interpret the decision vari ables D as chance variables. The only caveat is that the learned relevance-arc structures will be constrained by the influence-diagram semantics. In particular, a relevance-arc structure is eligible to be learned (i.e., has a corresponding hypothesis that can have a non zero prior) if and only if (1) every node in D is a root node and (2) that structure when combined with the information-arc structure declared by the decision maker contains no directed cycles. (Note that both of these constraints are satisfied by canonical-form rep resentations of causal networks.)\n1 For simplicity of presentation, we assume that information-arc and utility-node structure is identical for all cases in the database.\nA Bayesian Approach to Learning Causal Networks 291\nIn this section, we consider aspects of learning influ ence diagrams peculiar to influence diagrams in canon ical form. In this discussion, we assume that the struc ture of the influence diagram is known, and that we need to learn only the parameters of the structure.\nOne difficulty associated with learning influence di agrams in canonical form occurs in domains where we can set variables only once (or a small number of times) so that the mechanisms are not fully observ able. For example, recall our decision to continue or quit smoking where x denotes our decision and y de notes whether or not we get lung cancer before the age of sixty. In this case, we cannot fully observe the map ping variable y( x), because we cannot observe whether or not we get lung cancer for both possible choices. Given any one choice for x and observation of y, we exclude only two of the four states of y(x). Conse quently, it would seem that learning about y(x) would be difficult if not impossible.\nWe can understand this difficulty in another way. Given any mapping variable y( X) where X has q states, we can decompose y( X) into a set of variables y( X = kt), . . . , y( X = kq), where variable y( X = k) represents the variable y when X is set to state k. We call these variables mechanism components. For example, Figure 3a illustrates the components of the mechanism variable y( x), where x is a binary variable. Note that, by the definition of a mechanism compo nent, we have\npp(y( X = k)) = pp(yJ X = k) (9)\nAn analogous equation holds for (subjective) probabil ities.\nGiven this decomposition, the a setting of X and the observation of y is equivalent to the observation of ex actly one of the components of y(X). Thus, if we can set X only once, as in the smoking example, we can\nnot observe multiple mechanism components. Con sequently, we cannot learn about the physical proba bilities that characterize the dependencies among the components.\nTo circumvent this problem, we can assume that mech anism components are independent, an assumption we call component independence.2 If this assumption is in correct, then we will not learn correct counterfactual relationships. Regardless of the assumption's correct ness, however, we can correctly quantify the affects of a single setting action.\nFor example, in the smoking decision, the mechanism components are clearly dependent: Knowing that we quit and got lung cancer (y(x = 0) = 1) makes it more likely that we would have gotten lung cancer had we continued (y(x = 1) = 1). Nonetheless, suppose we assume the components are independent and learn the physical probabilities from a database of cases. Then, although we learn incorrect coun terfactual relationships-namely, that y(x = 0) and y( x = 1) are independent-we can still learn the cor rect marginal physical probabilities associated with both mechanism components. Thus, by Equation 9, we can learn the correct physical probability that that we will get cancer if we continue to smoke as well as the correct physical probability of cancer if we quit smoking.\nA second complication with learning influence dia grams in canonical form is the possible dependency among different mechanisms. For example, suppose we model the voltages in a logic circuit containing two buffers in series as shown in Figure 4a. Here, x and z represent the input and output voltages of the cir cuit, respectively, and y represents the voltage between the two buffers. The causal network for this circuit is x --+ y --+ z. The corresponding influence diagram in canonical form is shown in Figure 4b. The causal mechanism y( x) represents the possible mappings from the input to the output of the first buffer. The possi ble states of y( x) are \"output normal,\" \"output always zero,\" \"output always one,\" and \"output inverted\" . That is, this causal mechanism is a representation of the working status of the buffer. Similarly, the map ping variable z(y) represents the working status of the second buffer. Thus, these mechanisms will be depen dent whenever buffer function is dependent-for ex ample, when it is possible for the circuit to overheat and cause both buffers to fail.\nDependent mechanisms lead to practical problems.\n2We note that, from Equation 9, under the assumption of component independence, we can fill in the probability tables associated with the canonical-form representation of a causal network by copying the probabilities associated with that causal network. Without this assumption, the canonical-form representation requires additional probabil ity assessments.\n292 Heckerman\n(a)\nFigure 4: (a) A logic circuit containing two buffers in series. (b) A causal network for the circuit, represented as an influence diagram in canonical form.\nNamely, given the large number of states typically as sociated with mapping variables, the assessment of pri ors is difficult, and we require vast amounts of data to learn. Fortunately, we can often introduce additional domain variables in order to render mechanisms in dependent. In our circuit example, if we add to our domain the variable t representing the temperature of the circuit, then the new mechanisms y(x, t) and z(y, t) will be independent. This solution sometimes creates a another problem with learning: we may not be able to observe the variables we introduce. We address this issue in Section 7.\nGiven mechanism independence and component inde pendence for all mechanisms, the only chance variables that remain in a canonical form influence diagram are mutually independent mechanism components. Con sequently, if we also assume parameter independence, then the problem of learning a causal network essen tially reduces that of learning an acausal network.\nTo illustrate this equivalence, consider again our two binary-variable domain, and assume that the database is a random sample from an influence diagram corre sponding to the causal network x -+ y. Given the assumptions of mechanism, component, and param eter independence, we have the influence diagram in Figure 5a, where the deterministic functions for x and y are given by\ny = { x= { x(0) k if x = \"do nothing\" if x = \"set x to k\" y(x = j) k if iJ = \"do nothing\" and x = j if iJ = \"set y to k\"\nby the definitions of set decision and mechanism com ponent.\nNow, suppose that all the set decisions are \"do noth ing.\" In this situation, if we integrate out the mech anism variables from the diagram (as discussed in Shachter [1986]), then we obtain the influence diagram\nshown in Figure 5b. This structure is equivalent to the one shown in Figure 2b for learning the acausal net work x -+ y. Thus, we can update the parameters of the causal network x -+ y just as update those for the corresponding acausal network.\nThis result generalizes to arbitrary causal networks. In particular, if all set decisions in a particular case Ct are \"do nothing,\" we say the that observations of the domain variables in Ct are non-experimental data. Otherwise, we say that the observations are experi mental data. Given a case of non-experimental data, we update the parameters of a causal network just as we would the parameters of the corresponding acausal network (assuming mechanism, component, and pa rameter independence).\nThe updating procedure for experimental data is slightly different from that for non-experimental data. In our two-variable example, if we set y and observe x (with x set to \"do nothing\" ), then we obatin the influ ence diagram shown in Figure 5c. Here, the arcs to y are removed, because we have set the variable y. Con sequently, neither By=llx=O nor By=llx=l are updated given this data. In general, to update the parameters for a canonical form influence diagram given experi mental data where we have set x;, we break all arcs to x;, and update the parameters as we would for an acausal network.\n6 Learning Causal-Network Structure\nIn Section 3, we saw that, given the assumptions of parameter independence, parameter modularity, and hypothesis equivalence, we can assess priors for the parameters of all possible acausal-network structures by constructing a single prior network for the first case to be seen in the database and assessing an equivalent sample size (confidence) for this prior network. Thus, given the discussion in the previous section, it follows that we can use this prior-network methodology to establish priors for causal-network learning, provided we assume mechanism independence, component inde pendence, parameter independence, parameter modu larity, and hypothesis equivalence. In this section, we examine the assumptions of parameter modularity and likelihood equivalence for learning causal networks.\nThe assumption of parameter modularity has a com pelling justification in the context of causal networks. Namely, suppose a domain variable x has the same par ents P a(x) in two possible causal-network structures. Then, it is reasonable to believe that the causal mech anism x(Pa(x)) should be the same given either struc ture. It follows that its parameters exiPa(:v) for both structures must have the same prior distributions that is, parameter modularity must hold.\nIn contrast, the property of hypothesis equivalence\ncannot be applied to causal networks. For example, in our two-variable domain, the causal network x --+ y represents the assertion that x causes y, whereas the causal network y --+ x represents the assertion that y causes x. Now, it is possible for both x to cause y and vice versa when the two variables are some how deterministically related (e.g., consider the vari ables pressure and volume in a closed physical system). Barring such deterministic relationships, however, the hypotheses corresponding to these two network struc tures are mutually exclusive. Consequently, hypothe sis equivalence does not hold.\nNonetheless, when we know little about the structure of a domain, we have often found it reasonable to as sume that data cannot help to distinguish between equivalence network structures. To express this as sumption formally, let eu denote the parameters of the joint space, and let c� denote the hypothesis that the database is a random sample from the influence di agram corresponding to the causal-network structure C,. Then, we have\nwhenever the causal-network structures c.l and c.2 are equivalent (when interpreted as acausal networks). We call this assumption likelihood equivalence. Hecker man et al. [1995] show that the prior-network method ology is still justified when we replace the assumption of hypothesis equivalence with that of likelihood equiv alence.\nUnder the assumptions of mechanism, component, and parameter independence, the assumption of likelihood equivalence has an interesting characterization. Con sider again our two-variable domain. Suppose we know nothing about the domain, having uninforma tive Dirichlet priors on the parameters of both net work structures (all Dirichlet exponents arbitrarily close to -1). Further, suppose we adopt the as sumption of likelihood equivalence for the two net work structures x --+ y and y --+ x. Now, suppose we obtain a single case of experimental data where we set x = 1 and observe y = 1. According to\nour updating procedure described in the previous sec tion, for the network structure x --+ y, we update the parameter By=llx=l, but not the parameter Bx=l· In contrast, for the network structure y --+ x, we update the parameter By=l, but not the parameter Bx=lly=l· As a result, our posterior distributions for eu = {Bx=O,y=O, Bx=O,y=l> Bx=l,y=O, Bx=l,y=t} will no longer satisfy likelihood equivalence. One can show that, for any domain, if we have an uninformative Dirichlet prior for that domain and we are given a database containing experimental data, then the re sulting posterior distributions for eu will violate like lihood equivalence. Therefore, we can assess whether or not likelihood equivalence holds by asking ourselves whether or not our prior knowledge is equivalent to having seen only non-experimental data.\nWe note that the assumption of likelihood equivalence tends to be less reasonable for more familiar domains. For example, a doctor may be uncertain as to whether disease d1 causes disease d2 or vice versa, but he may have well-defined hypotheses about why d1 causes dis ease d2 and vice versa. In this case, the assumption of likelihood equivalence would likely be unreasonable.\nWe emphasize that experimental data can be crucial for learning causal structure. In our two-variable do main, suppose we believe that either x causes y or y causes x. Then, if we set x to different states and learn that the probability of y depends on x, then we learn that x causes y. To verify this relation, we can set y to different states and check that the probability of x remains the same. Conversely, if we set y to different states and learn that the probability of x depends on y, then we learn that y causes x. Also, we may need experimental data to quantify the effects of intervention-for example, to learn the phys ical probability distribution pp(yix = 1). Given a causal structure, however, there are situations where we can quantify the effects of intervention using ob servational data only (Pearl and Verma, 1991; Pearl, 1995a).\n294 Heckerman\n7 Learning Hidden Variables\nIn Section 5, we saw that we could often remove de pendencies between causal mechanisms by adding ad ditional domain variables. In many situations, how ever, we can never observe these variables. We say that these variables are hidden.\nAs we have discussed, methods for learning acausal networks with missing data are known (e.g., exact, EM, Gibbs sampling). These methods can be applied to databases containing hidden variables. Thus, un der the assumptions of mechanism independence, com ponent independence, parameter independence, pa rameter modularity, and likelihood equivalence, we can learn causal networks with hidden variables using these methods in conjunction with the prior-network methodology.\nTo illustrate this approach, let us consider a simple medical domain containing two observable variables h and l representing the presence or absence of heart dis ease and lung disease, respectively, and a hidden vari able g representing the presence or absence of a gene that predisposes one to both diseases. Two possible causal-network structures for this domain are shown in Figure 6. In the network structure labeled C.1, h causes l, and g is a hidden common cause of both dis eases. In C,2, the disease variables are related only through the hidden common cause. Suppose that only these two network-structure hypotheses are possible and that they are equally likely a priori. In addi tion, suppose our prior network for this domain is c.2 with the probabilities shown in Figure 6, and N' (the equivalent sample size for this network) is 24. Finally suppose we have a database C containing two cases where-in both cases-all set decisions are \"do noth ing,\" h = 1 (heart disease present), and l = 1 (lung disease present).\nBecause there are only two cases, we can compute the posterior probabilities of both network-structure hy potheses exactly, using Equation 7 (which applies to complete databases), Equation 8, and the relation\np(CJc:, e)= p(gt = 1, ht = 1, Zt = 1, 92 = 1, h2 = 1, !2 = 1JC;, e)+ p(gt = O,ht = 1,lt = 1,g2 = 1,h2 = 1,12 = 1JC:,{) + p(gt = 1,ht = 1,lt = 1,g2 = O,h2 = 1,12 = 1JC:,e) + p(g1 = o, ht = 1,11 = 1, 92 = o, h2 = 1,12 = 11c:, e)\nwhere the subscripts on the variables denote case num bers. For example, from Equations 7 and 8, the first term in this sum for c.l is given by\nPerforming the sums and applying Bayes' theorem, we obtain p(C:1IC,e) = 0.51 and p(C:2IC,e) = 0.49.\nFor domains containing hidden variables, Pearl (1995b) has suggested a generalization of the assump tion of likelihood equivalence, which says that if two causal networks are equivalent with respect to the dis tributions they encode for the observed variables, then the parameters for those observed variables should have identical priors. We call this property strong likelihood equivalence. This property does not hold in our simple medical example. Namely, the two net work structures c.l and c.2 are equivalent with re spect to the variables h and l (i.e., both structures can represent any joint distribution over these variables). Nonetheless, as we saw in the previous example, ob servations can help to discriminate the two network structures. Thus, given the assumptions of mecha nism and component independence, strong likelihood equivalence is not consistent with our prior-network methodology. That is, strong likelihood equivalence is not consistent with the assumptions of parameter in dependence and parameter modularity. Consequently, strong likelihood equivalence may lead to a method for assessing priors on parameters that is an alternative to the prior-network approach.\n8 Learning More General Causal Models\nOur presentation has concentrated on domains where all variables (except root nodes) have causes. We em phasize that this restriction is unnecessary, given the definition of cause given by Heckerman and Shachter (1994 and this proceedings). In particular, as shown by these researchers, the relationships in domains where only some variables have causes can be encoded in canonical form. Consequently, we can often apply the learning methods we have described to these more general domains.\nAcknowledgments\nThis work was motivated by conversations with Max Chickering, Greg Cooper, Dan Geiger, and Judea Pearl. Jack Breese and Max Chickering provided use ful suggestions on earlier versions of this manuscript.\nA Bayesian Approach to Learning Causal Networks 295\nReferences\n[Buntine, 1991] Buntine, W. (1991). Theory refine ment on Bayesian networks. In Proceedings of Sev enth Conference on Uncertainty in Artificial Intelli gence, Los Angeles, CA, pages 52-60. Morgan Kauf mann.\n[Buntine, 1994] Buntine, W. {1994). Operations for learning with graphical models. Journal of Artificial Intelligence Research, 2:159-225.\n[Cooper and Herskovits, 1992] Cooper, G. and Her skovits, E. (1992). A Bayesian method for the induc tion of probabilistic networks from data. Machine Learning, 9:309-347.\n[Cooper and Herskovits, 1991] Cooper, G. and Her skovits, E. (January, 1991). A Bayesian method for the induction of probabilistic networks from data. Technical Report SMI-91-1, Section on Medical In formatics, Stanford University.\n[Cowell et al., 1995] Cowell, R., Dawid, A., and Se. bastiani, P. (1995). A comparison of sequential\nlearning methods for incomplete data. Technical Report 135, Department of Statistical Science, Uni versity College London.\n[Dempster et al., 1977] Dempster, A., Laird, N., and Rubin, D. (1977). Maximum likelihood from incom plete data via the EM algorithm. Journal of the Royal Statistical Society, B 39:1-38.\n[Heckerman et al., 1994] Heckerman, D., Geiger, D., and Chickering, D. (1994). Learning Bayesian net works: The combination of knowledge and statis tical data. In Proceedings of Tenth Conference on Uncertainty in Artificial Intelligence, Seattle, WA, pages 293-301. Morgan Kaufmann.\n[Heckerman et al., 1995] Heckerman, D., Geiger, D., and Chickering, D. (1995). Learning Bayesian net works: The combination of knowledge and statisti cal data. Machine Learning, to appear.\n[Heckerman and Shachter, 1994] Heckerman, D. and Shachter, R. ( 1994). A decision-based view of causality. In Proceedings of Tenth Conference on Uncertainty in Artificial Intelligence, Seattle, WA, pages 302-310. Morgan Kaufmann.\n[Heckerman and Shachter, 1995] Heckerman, D. and Shachter, R. (1995). A definition and graphical rep resentation of causality. In this proceedings.\n[Howard and Matheson, 1981] Howard, R. and Math eson, J. (1981). Influence diagrams. In Howard, R. and Matheson, J., editors, Readings on the Prin ciples and Applications of Decision Analysis, vol ume II, pages 721-762. Strategic Decisions Group, Menlo Park, CA.\n[Lewis, 1978] Lewis, D. (1978). Counterfactual depen dence and time's arrow. N ous, pages 455-476.\n[Madigan and Raftery, 1994] Madigan, D. and Raftery, A. {1994). Model selection and account ing for model uncertainty in graphical models using Occam's window. Journal of the American Statisti cal Association, 89:1535-1546.\n[Pearl, 1988] Pearl, J. {1988). Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Infer ence. Morgan Kaufmann, San Mateo, CA.\n[Pearl, 1995a] Pearl, J. (1995a). Causal diagrams for empirical research. Biometrika, to appear.\n[Pearl, 1995b] Pearl, J. {1995b). Personal communi cation.\n[Pearl and Verma, 1991] Pearl, J. and Verma, T. (1991). A theory of inferred causation. In Allen, J., Fikes, R., and Sandewall, E., editors, Knowl edge Representation and Reasoning: Proceedings of the Second International Conference, pages 441- 452. Morgan Kaufmann, New York.\n[Shachter, 1986] Shachter, R. (1986). Evaluating in fluence diagrams. Operations Research, 34:871-882.\n[Spiegelhalter et al., 1993] Spiegelhalter, D., Dawid, A., Lauritzen, S., and Cowell, R. (1993). Bayesian analysis in expert systems. Statistical Science, 8:219-282.\n[Spiegelhalter and Lauritzen, 1990] Spiegelhalter, D. and Lauritzen, S. (1990). Sequential updating of conditional probabilities on directed graphical struc tures. Networks, 20:579-605.\n[Spirtes et al., 1993] Spirtes, P., Glymour, C., and Scheines, R. (1993). Causation, Prediction, and Search. Springer-Verlag, New York.\n[Titterington, 1976] Titterington, D. (1976). Updat ing a diagnostic system using unconfirmed cases. Applied Statistics, 25:238-247.\n[Verma and Pearl, 1990] Verma, T. and Pearl, J. (1990). Equivalence and synthesis of causal mod els. In Proceedings of Sixth Conference on Uncer tainty in Artificial Intelligence, Boston, MA, pages 220-227. Morgan Kaufmann.\n[York, 1992] York, J. (1992). Bayesian methods for the analysis of misclassified or incomplete multivariate discrete data. PhD thesis, Department of Statistics, University of Washington, Seattle."
    } ],
    "references" : [ {
      "title" : "A Bayesian method for the induc­ tion of probabilistic networks from data",
      "author" : [ "Cooper", "Herskovits", "G. 1992] Cooper", "E. Her­ skovits" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Cooper et al\\.,? \\Q1992\\E",
      "shortCiteRegEx" : "Cooper et al\\.",
      "year" : 1992
    }, {
      "title" : "A comparison of sequential learning methods for incomplete data",
      "author" : [ "Cowell et al", "R. 1995] Cowell", "A. Dawid", "P. Se. bastiani" ],
      "venue" : "Technical Report 135,",
      "citeRegEx" : "al. et al\\.,? \\Q1995\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 1995
    }, {
      "title" : "Maximum likelihood from incom­ plete data via the EM algorithm",
      "author" : [ "Dempster et al", "A. 1977] Dempster", "N. Laird", "D. Rubin" ],
      "venue" : "Journal of the Royal Statistical",
      "citeRegEx" : "al. et al\\.,? \\Q1977\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 1977
    }, {
      "title" : "Learning Bayesian net­ works: The combination of knowledge and statis­ tical data",
      "author" : [ "Heckerman et al", "D. 1994] Heckerman", "D. Geiger", "D. Chickering" ],
      "venue" : "In Proceedings of Tenth Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "al. et al\\.,? \\Q1994\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 1994
    }, {
      "title" : "Learning Bayesian net­ works: The combination of knowledge and statisti­ cal data",
      "author" : [ "Heckerman et al", "D. 1995] Heckerman", "D. Geiger", "D. Chickering" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "al. et al\\.,? \\Q1995\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 1995
    }, {
      "title" : "A decision-based view of causality",
      "author" : [ "Heckerman", "Shachter", "D. 1994] Heckerman", "R. Shachter" ],
      "venue" : "In Proceedings of Tenth Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "Heckerman et al\\.,? \\Q1994\\E",
      "shortCiteRegEx" : "Heckerman et al\\.",
      "year" : 1994
    }, {
      "title" : "A definition and graphical rep­ resentation of causality",
      "author" : [ "Heckerman", "Shachter", "D. 1995] Heckerman", "R. Shachter" ],
      "venue" : null,
      "citeRegEx" : "Heckerman et al\\.,? \\Q1995\\E",
      "shortCiteRegEx" : "Heckerman et al\\.",
      "year" : 1995
    }, {
      "title" : "Influence diagrams",
      "author" : [ "Howard", "Matheson", "R. 1981] Howard", "J. Math­ eson" ],
      "venue" : "Readings on the Prin­ ciples and Applications of Decision Analysis, vol­",
      "citeRegEx" : "Howard et al\\.,? \\Q1981\\E",
      "shortCiteRegEx" : "Howard et al\\.",
      "year" : 1981
    }, {
      "title" : "Model selection and account­ ing for model uncertainty in graphical models using Occam's window",
      "author" : [ "Madigan", "Raftery", "D. 1994] Madigan", "A. Raftery" ],
      "venue" : "Journal of the American Statisti­ cal Association,",
      "citeRegEx" : "Madigan et al\\.,? \\Q1994\\E",
      "shortCiteRegEx" : "Madigan et al\\.",
      "year" : 1994
    }, {
      "title" : "A theory of inferred causation",
      "author" : [ "Pearl", "Verma", "J. 1991] Pearl", "T. Verma" ],
      "venue" : "Proceedings of the Second International Conference,",
      "citeRegEx" : "Pearl et al\\.,? \\Q1991\\E",
      "shortCiteRegEx" : "Pearl et al\\.",
      "year" : 1991
    }, {
      "title" : "Bayesian analysis in expert systems",
      "author" : [ "Spiegelhalter et al", "D. 1993] Spiegelhalter", "A. Dawid", "S. Lauritzen", "R. Cowell" ],
      "venue" : null,
      "citeRegEx" : "al. et al\\.,? \\Q1993\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 1993
    }, {
      "title" : "Sequential updating of conditional probabilities on directed graphical struc­ tures",
      "author" : [ "Spiegelhalter", "Lauritzen", "D. 1990] Spiegelhalter", "S. Lauritzen" ],
      "venue" : null,
      "citeRegEx" : "Spiegelhalter et al\\.,? \\Q1990\\E",
      "shortCiteRegEx" : "Spiegelhalter et al\\.",
      "year" : 1990
    }, {
      "title" : "Causation, Prediction, and Search",
      "author" : [ "Spirtes et al", "P. 1993] Spirtes", "C. Glymour", "R. Scheines" ],
      "venue" : null,
      "citeRegEx" : "al. et al\\.,? \\Q1993\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 1993
    }, {
      "title" : "Equivalence and synthesis of causal mod­ els",
      "author" : [ "Verma", "Pearl", "T. 1990] Verma", "J. Pearl" ],
      "venue" : "In Proceedings of Sixth Conference on Uncer­ tainty in Artificial Intelligence,",
      "citeRegEx" : "Verma et al\\.,? \\Q1990\\E",
      "shortCiteRegEx" : "Verma et al\\.",
      "year" : 1990
    } ],
    "referenceMentions" : [ {
      "referenceID" : 9,
      "context" : ", Buntine [1991], Spiegelhalter et al. [1993], and Beck­ erman et al.",
      "startOffset" : 18,
      "endOffset" : 46
    }, {
      "referenceID" : 9,
      "context" : ", Buntine [1991], Spiegelhalter et al. [1993], and Beck­ erman et al. [1995]), as well as search methods for identifying structures that contribute significantly to the sum in Equation 3 (e.",
      "startOffset" : 18,
      "endOffset" : 77
    }, {
      "referenceID" : 9,
      "context" : ", Buntine [1991], Spiegelhalter et al. [1993], and Beck­ erman et al. [1995]), as well as search methods for identifying structures that contribute significantly to the sum in Equation 3 (e.g., Cooper and Herskovits [1992] and Heckerman et al.",
      "startOffset" : 18,
      "endOffset" : 223
    }, {
      "referenceID" : 5,
      "context" : ", Cooper and Herskovits [1992] and Heckerman et al. [1995]).",
      "startOffset" : 35,
      "endOffset" : 59
    }, {
      "referenceID" : 5,
      "context" : "Heckerman et al. [1995] call this property parameter modularity, because it says that the distributions for parameters e;j depend only on the structure of the network that is local to variable x;-namely, eij only depends on Xj and its parents.",
      "startOffset" : 0,
      "endOffset" : 24
    } ],
    "year" : 2011,
    "abstractText" : "Whereas acausal Bayesian networks rep­ resent probabilistic independence, causal Bayesian networks represent causal relation­ ships. In this paper, we examine Bayesian methods for learning both types of networks. Bayesian methods for learning acausal net­ works are fairly well developed. These meth­ ods often employ assumptions to facilitate the construction of priors, including the as­ sumptions of parameter independence, pa­ rameter modularity, and likelihood equiva­ lence. We show that although these assump­ tions also can be appropriate for learning causal networks, we need additional assump­ tions in order to learn causal networks. We introduce two sufficient assumptions, called mechanism independence and component in­ dependence. We show that these new as­ sumptions, when combined with parame­ ter independence, parameter modularity, and likelihood equivalence, allow us to apply methods for learning acausal networks to learn causal networks.",
    "creator" : "pdftk 1.41 - www.pdftk.com"
  }
}